Paper,description,content,label,keywords
Effects of culling vampire bats on the spatial spread and spillover of rabies virus,"Data and code for Viana, Benavides et al: ""Effects of culling vampire bats on the spatial spread and spillover of rabies virus.""","#libraries
library(R2jags)
library(coda)
library(runjags)
library(boot)

rm(list=ls())


Ndistricts<- read.csv('Ndistricts.csv',header=T)
Ntime<- read.csv('Ntime.csv',header=T)
outbreak_rabies<-as.matrix(data.frame(read.csv('outbreaks_jags.csv',header=T)))
neighbours<-as.matrix(data.frame(read.csv('neighbours.csv', header=T)))
under_reporting<-as.matrix(data.frame(read.csv('under_reporting.csv',header=T)))
culling<-as.matrix(data.frame(read.csv('culling.csv',header=T))) 
vaccination<- as.matrix(data.frame(read.csv('vaccination.csv',header=T)))
culled_neighbours<- as.matrix(data.frame(read.csv('culled_neighbours.csv',header=T)))

#set up
myiterations <- 30000
myburnin <-15000
mythin <- 5

# model parameters
myinitials<- list(list(), list() )

mypars<- c('p_rabies', 'p', 'q', 'outbreak_star',
           'beta0','beta1','beta2','beta3','beta4', 'beta5', 
           'omega',
           'theta0','theta1','theta2','theta3', 'z' ) 

#data
mydat<- list(Ndistricts=Ndistricts, Ntime=Ntime-Init, outbreak_rabies=outbreak_rabies, neighbours=neighbours, 
               under_reporting=under_reporting, vaccination=vaccination, 
               cull_neighbours=culled_neighbours,
               culling=culling )

#model
modelfile <- 'CodeS1_JAGSmodel_ZIPculling.txt'

#RUN MODEL
#note it can take several days for the model to run
prod.jags<- jags(data=mydat, inits=myinitials, parameters.to.save=mypars,
                 model.file=modelfile, n.chains=3, n.iter=myiterations, n.burnin=myburnin,
                 n.thin=mythin, DIC=TRUE)",0,"- gut microbiota
- ecological diversification
- speciation
- adaptive radiations
- trophic ecology
- bacterial communities
- host-specific factors
- crater lakes
- V4 region
- 16s rRNA"
Data from: Parallel and non-parallel changes of the gut microbiota during trophic diversification in repeated young adaptive radiations of sympatric cichlid fish,"Background: Recent increases in understanding the ecological and evolutionary roles of microbial communities has underscored their importance for their hosts' biology. Yet, little is known about gut microbiota dynamics during early stages of ecological diversification and speciation. We sequenced the V4 region of the 16s rRNA gene to study the gut microbiota of extremely young adaptive radiations of Nicaraguan Midas cichlid fish (Amphilophus cf. citrinellus) from two crater lakes to test the hypothesis that parallel divergence in trophic ecology is associated with parallel changes of the gut microbiota.Results: Bacterial communities of the water and guts were highly distinct, indicating that the gut microbiota is shaped by host-specific factors. Across individuals of the same crater lake, differentiation in trophic ecology was associated with gut microbiota differentiation, suggesting that diet, to some extent, affects the gut microbiota. However, differences in trophic ecology were much more pronounced across than within species whereas similar patterns were not observed for taxonomic and functional differences of the gut microbiota. Across two crater lakes, we could not detect evidence for parallel changes of the gut microbiota associated with trophic ecology.Conclusions: Similar cases of non-parallelism have been observed in other recently diverged fish species and might be explained by a lack of clearly differentiated niches during early stages of ecological diversification.","['##Microbiota analysis\r\n\r\nrequire(""ape"")\r\nrequire(""vegan"")\r\nrequire(""reshape2"")\r\n\r\n#Import distance matrix of fish guts & water from Apoyo, Xiloa, Nicaragua & Managua\r\n\r\n#Either weighted/unweighted unifrac or bray-curtis data\r\n\r\ndistmat_complete <- read.csv(""distance-matrix_wuf.tsv"", dec = "","", sep = ""\\t"", as.is = T)\r\n\r\n#distmat_complete <- read.csv(""distance-matrix_wuf.tsv"", dec = "","", sep = ""\\t"", as.is = T)\r\n#distmat_complete <- read.csv(""distance-matrix_uwuf.tsv"", dec = "","", sep = ""\\t"", as.is = T)\r\n#distmat_complete <- read.csv(""distance-matrix_bc.tsv"", dec = "","", sep = ""\\t"", as.is = T)\r\n\r\nrownames(distmat_complete) <- distmat_complete[,1]\r\ndistmat_complete <- distmat_complete[,-1]\r\nnames_vec <- rownames(distmat_complete)\r\ncolnames(distmat_complete) <- names_vec\r\n\r\n#Import metadata file\r\n\r\nids_complete <- read.csv(""complete-samples.txt"", dec = "","", sep = ""\\t"", as.is = T)\r\n\r\n#ids_complete <- read.csv(""samples.txt"", dec = "","", sep = ""\\t"", as.is = T)\r\n\r\nids_complete$pcolor[ids_complete$species==""cit_nic""] <- ""limegreen""\r\nids_complete$pcolor[ids_complete$species==""cit_man""] <- ""darkgreen""\r\nids_complete$pcolor[ids_complete$species==""amarillo""] <- ""mediumblue""\r\nids_complete$pcolor[ids_complete$species==""xiloaensis""] <- ""dodgerblue3""\r\nids_complete$pcolor[ids_complete$species==""sagittae""] <- ""skyblue2""\r\nids_complete$pcolor[ids_complete$species==""astorquii""] <- ""darkred""\r\nids_complete$pcolor[ids_complete$species==""chancho""] <- ""firebrick3""\r\nids_complete$pcolor[ids_complete$species==""globosus""] <- ""indianred2""\r\nids_complete$pcolor[ids_complete$species==""zaliosus""] <- ""darkorange1""\r\nids_complete$pcolor[ids_complete$species==""water""] <- ""black""\r\n\r\nids_complete$lcolor[ids_complete$lake==""nicaragua""] <- ""darkgreen""\r\nids_complete$lcolor[ids_complete$lake==""managua""] <- ""blue""\r\nids_complete$lcolor[ids_complete$lake==""xiloa""] <- ""purple""\r\nids_complete$lcolor[ids_complete$lake==""apoyo""] <- ""orange""\r\n\r\nids_complete$pch[ids_complete$lake==""nicaragua""] <- ""16""\r\nids_complete$pch[ids_complete$lake==""managua""] <- ""18""\r\nids_complete$pch[ids_complete$lake==""xiloa""] <- ""15""\r\nids_complete$pch[ids_complete$lake==""apoyo""] <- ""17""\r\n\r\nids_complete$pch <- as.numeric(ids_complete$pch)\r\n\r\npcoa_complete <- pcoa(distmat_complete)\r\n\r\npcoa_axes <- pcoa_complete$vectors\r\npcoa_axes <- as.data.frame(pcoa_axes)\r\n\r\npcoa_ids <- cbind(ids_complete, pcoa_axes)\r\n\r\nplot(pcoa_axes$Axis.2 ~ pcoa_axes$Axis.1, pch = pcoa_ids$pch, col = pcoa_ids$pcolor)\r\n\r\n#Test whether bacterial community differs between lake water and fish guts\r\nadonis(distmat_complete ~ type, data = pcoa_ids)\r\n\r\n#Taxa barplots\r\n\r\ntaxa <- read.csv(""bacterial_phyla.csv"", dec = "","", sep = "";"", as.is = T)\r\n\r\nrownames(taxa) <- taxa[,1]\r\ntaxa <- taxa[,-1]\r\n\r\nbarplot(as.matrix(taxa), legend = rownames(taxa))\r\n\r\n#Alpha diversity\r\n\r\nadiv <- read.csv(""alpha_div.csv"", dec = "","", sep = "";"", as.is = T)\r\nadiv$type <- as.factor(adiv$type)\r\nadiv$habitat <- as.factor(adiv$habitat)\r\nadiv$species <- factor(adiv$species,levels=c(""water_nic"",""water_man"",""water_apo"", ""water_xil"",""cit_nic"", ""cit_man"", ""astorquii"", ""chancho"", ""globosus"", ""zaliosus"", ""amarillo"", ""xiloaensis"", ""sagittae""))\r\n\r\nplot(adiv$observed_otus ~ adiv$species)\r\n\r\n#Test whether alpha diversity differs between lake water and fish guts\r\nwilcox.test(adiv$observed_otus ~ adiv$type)\r\n\r\nadiv_water <- adiv[1:16,]\r\nadiv_fish <- adiv[17:146,]\r\nadiv_fish_nomanagua <- adiv[c(17:26,37:146),]\r\n\r\n#Test whether alpha diversity differs between great lakes and crater lakes for lake water and fish guts\r\nwilcox.test(adiv_water$observed_otus ~ adiv_water$habitat)\r\nwilcox.test(adiv_fish$observed_otus ~ adiv_fish$habitat)\r\nwilcox.test(adiv_fish_nomanagua$observed_otus ~ adiv_fish_nomanagua$habitat)\r\n\r\n#Test whether alpha diversity differs between species in crater lakes\r\n#Change species names accordingly\r\nwilcox.test(adiv_fish$observed_otus[which(adiv_fish$species == ""amarillo"")], adiv_fish$observed_otus[which(adiv_fish$species == ""xiloaensis"")])\r\n\r\n#Subset: only fish guts\r\n\r\ndistmat_fish <- distmat_complete[1:130,1:130]\r\nids_fish <- ids_complete[1:130,]\r\n\r\nplot(ids_fish$n ~ ids_fish$c, pch = ids_fish$pch, col = ids_fish$pcolor)\r\n\r\nadonis(distmat_fish ~ habitat*lake, data = ids_fish, strate = A:B)\r\n\r\n##SIA data\r\n\r\n#All locations\r\n\r\n#Remove fish with missing SIA data\r\nfish_sia <- subset(ids_fish, !is.na(ids_fish$n))\r\nfish_sia$lake <- as.factor(fish_sia$lake)\r\nfish_sia$habitat <- as.factor(fish_sia$habitat)\r\n\r\n#Differences in Carbon/nitrogen among lakes\r\nkruskal.test(fish_sia$c ~ fish_sia$lake)\r\nkruskal.test(fish_sia$n ~ fish_sia$lake)\r\n\r\n#Subset: only crater lake fish\r\n\r\ndistmat_crater <- distmat_complete[c(2:14,16,17,19:21,23:57,59,61:65,67:90,92:95,98,100:103,107,110,111,114:116,118:127,129,130),c(2:14,16,17,19:21,23:57,59,61:65,67:90,92:95,98,100:103,107,110,111,114:116,118:127,129,130)]\r\nids_crater <- ids_complete[c(2:14,16,17,19:21,23:57,59,61:65,67:90,92:95,98,100:103,107,110,111,114:116,118:127,129,130),]\r\n\r\n#Subset: only crater lake fish with SIA data\r\ndistm', '##Microbiota analysis 2019\r\n\r\nrequire(""ape"")\r\nrequire(""vegan"")\r\nrequire(""reshape2"")\r\nrequire(""ade4"")\r\n\r\n#Import distance matrix of fish guts & water from Apoyo, Xiloa, Nicaragua & Managua\r\n\r\n#Either weighted/unweighted unifrac or bray-curtis data\r\ndistmat_complete <- read.csv(""distance-matrix_wuf.tsv"", dec = "","", sep = ""\\t"", as.is = T)\r\n#distmat_complete <- read.csv(""distance-matrix_uwuf.tsv"", dec = "","", sep = ""\\t"", as.is = T)\r\n#distmat_complete <- read.csv(""distance-matrix_bc.tsv"", dec = "","", sep = ""\\t"", as.is = T)\r\n\r\nrownames(distmat_complete) <- distmat_complete[,1]\r\ndistmat_complete <- distmat_complete[,-1]\r\nnames_vec <- rownames(distmat_complete)\r\ncolnames(distmat_complete) <- names_vec\r\n\r\n#Import metadata file\r\nids_complete <- read.csv(""samples.txt"", dec = "","", sep = ""\\t"", as.is = T)\r\n\r\n#Subset: only crater lakes\r\n\r\ndistmat_crater <- distmat_complete[c(2:14,16,17,19:21,23:57,59,61:65,67:90,92:95,98,100:103,107,110,111,114:116,118:127,129,130),c(2:14,16,17,19:21,23:57,59,61:65,67:90,92:95,98,100:103,107,110,111,114:116,118:127,129,130)]\r\nids_crater <- ids_complete[c(2:14,16,17,19:21,23:57,59,61:65,67:90,92:95,98,100:103,107,110,111,114:116,118:127,129,130),]\r\n\r\n#Xiloa distmat\r\n\r\ndistmat_xiloa <- distmat_crater[c(1,5,8,10:12,14,17,24,26,27,29:31,33,35:37,40:45,47,48,50:52,54:56,58,60:63,65,67:69,71,72,75:77,80:82,89,91,95,98,101,106,110),c(1,5,8,10:12,14,17,24,26,27,29:31,33,35:37,40:45,47,48,50:52,54:56,58,60:63,65,67:69,71,72,75:77,80:82,89,91,95,98,101,106,110)]\r\nids_xiloa <- ids_crater[c(1,5,8,10:12,14,17,24,26,27,29:31,33,35:37,40:45,47,48,50:52,54:56,58,60:63,65,67:69,71,72,75:77,80:82,89,91,95,98,101,106,110),]\r\n\r\n#Xiloa distmat, only SIA samples\r\n\r\ndistmat_xiloa_sia <- distmat_xiloa[c(1:5,7:38,40,42,44:47,49:51,53:56),c(1:5,7:38,40,42,44:47,49:51,53:56)]\r\nids_xiloa_sia <- ids_xiloa[c(1:5,7:38,40,42,44:47,49:51,53:56),]\r\n\r\nids_xiloa_sia$species <- as.factor(ids_xiloa_sia$species)\r\n\r\n#Apoyo distmat\r\n\r\ndistmat_apoyo <- distmat_crater[c(2:4,6,7,9,13,15,16,18:23,25,28,32,34,38,39,46,49,53,57,59,64,66,70,73,74,78,79,83:88,90,92:94,96,97,99,100,102:105,107:109),c(2:4,6,7,9,13,15,16,18:23,25,28,32,34,38,39,46,49,53,57,59,64,66,70,73,74,78,79,83:88,90,92:94,96,97,99,100,102:105,107:109)]\r\nids_apoyo <- ids_crater[c(2:4,6,7,9,13,15,16,18:23,25,28,32,34,38,39,46,49,53,57,59,64,66,70,73,74,78,79,83:88,90,92:94,96,97,99,100,102:105,107:109),]\r\n\r\n#Apoyo distmat, only SIA samples\r\n\r\ndistmat_apoyo_sia <- distmat_apoyo[c(1:45, 47:54),c(1:45, 47:54)]\r\n\r\nids_apoyo_sia <- ids_apoyo[-46,]\r\nids_apoyo_sia$species <- as.factor(ids_apoyo_sia$species)\r\n\r\n#Xiloa: Pairwise distances between individuals, only SIA samples\r\n\r\nD <- as.matrix(distmat_xiloa_sia)\r\nD[upper.tri(D)] <- NA\r\nx <- subset(melt(D), value!=0)\r\nx <- as.data.frame(x)\r\n\r\nrow.names(x) <- NULL\r\n\r\n#Nitrogen\r\nn_dist <- NA\r\n\r\nfor (g in 1:length(x$Var1)){\r\n  id1 <- x$Var1[g]\r\n  id2 <- x$Var2[g]\r\n  dif <- abs(ids_xiloa_sia$n[which(ids_xiloa_sia$id==id1)]-ids_xiloa_sia$n[which(ids_xiloa_sia$id==id2)])\r\n  n_dist[g] <- dif\r\n}\r\n\r\nx$n_dist <- n_dist \r\n\r\n#carbon\r\nc_dist <- NA\r\n\r\nfor (g in 1:length(x$Var1)){\r\n  id1 <- x$Var1[g]\r\n  id2 <- x$Var2[g]\r\n  dif <- abs(ids_xiloa_sia$c[which(ids_xiloa_sia$id==id1)]-ids_xiloa_sia$c[which(ids_xiloa_sia$id==id2)])\r\n  c_dist[g] <- dif\r\n}\r\n\r\nx$c_dist <- c_dist \r\n\r\n#Apoyo: Pairwise distances for between individuals, only SIA samples\r\n\r\nD2 <- as.matrix(distmat_apoyo_sia)\r\nD2[upper.tri(D2)] <- NA\r\nx2 <- subset(melt(D2), value!=0)\r\nx2 <- as.data.frame(x2)\r\n\r\nrow.names(x2) <- NULL\r\n\r\n#Nitrogen\r\nn_dist <- NA\r\n\r\nfor (g in 1:length(x2$Var1)){\r\n  id1 <- x2$Var1[g]\r\n  id2 <- x2$Var2[g]\r\n  dif <- abs(ids_apoyo_sia$n[which(ids_apoyo_sia$id==id1)]-ids_apoyo_sia$n[which(ids_apoyo_sia$id==id2)])\r\n  n_dist[g] <- dif\r\n}\r\n\r\nx2$n_dist <- n_dist \r\n\r\n#carbon\r\nc_dist <- NA\r\n\r\nfor (g in 1:length(x2$Var1)){\r\n  id1 <- x2$Var1[g]\r\n  id2 <- x2$Var2[g]\r\n  dif <- abs(ids_apoyo_sia$c[which(ids_apoyo_sia$id==id1)]-ids_apoyo_sia$c[which(ids_apoyo_sia$id==id2)])\r\n  c_dist[g] <- dif\r\n}\r\n\r\nx2$c_dist <- c_dist \r\n\r\n#Mantel test\r\n\r\nx_wuf <- x[,c(1,2,3)]\r\ndistmat_mb <- acast(x_wuf, Var1 ~ Var2)\r\n\r\nx_n <- x[,c(1,2,4)]\r\ndistmat_n <- acast(x_n, Var1 ~ Var2)\r\n\r\nx_c <- x[,c(1,2,5)]\r\ndistmat_c <- acast(x_c, Var1 ~ Var2)\r\n\r\nx2_wuf <- x2[,c(1,2,3)]\r\ndistmat_mb2 <- acast(x2_wuf, Var1 ~ Var2)\r\n\r\nx2_n <- x2[,c(1,2,4)]\r\ndistmat_n2 <- acast(x2_n, Var1 ~ Var2)\r\n\r\nx2_c <- x2[,c(1,2,5)]\r\ndistmat_c2 <- acast(x2_c, Var1 ~ Var2)\r\n\r\n#Remove 2 samples that occur only in rows or columns \r\ndistmat_mb <- distmat_mb[-18,-25]\r\ndistmat_n <- distmat_n[-18,-25]\r\ndistmat_c <- distmat_c[-18,-25]\r\n\r\ndistmat_mb2 <- distmat_mb2[-4,-36]\r\ndistmat_n2 <- distmat_n2[-4,-36]\r\ndistmat_c2 <- distmat_c2[-4,-36]\r\n\r\ndiag(distmat_n) <- diag(distmat_c) <- diag(distmat_mb) <- diag(distmat_n2) <- diag(distmat_c2) <- diag(distmat_mb2) <- 0\r\n\r\nfor (i in 1:length(colnames(distmat_mb))){\r\n  for (g in 1:length(rownames(distmat_mb))){\r\n    if (is.na(distmat_mb[i,g])){\r\n      distmat_mb[i,g] <- distmat_mb[g,i]\r\n    }\r\n  }\r\n}\r\n\r\nf']",0,"Pronghorn, population genomics, connectivity, core range, long-term persistence, ecological characteristics, social behavior, landscape features, genetic structure, GPS-tracking studies, highways, expected levels, decline, modern threats, landscape modification, North America,"
Pronghorn population genomics show connectivity at the core of their range,"Preserving connectivity in the core of a species' range is crucial for long-term persistence. However, a combination of ecological characteristics, social behavior, and landscape features can reduce connectivity among wildlife populations and lead to genetic structure. Pronghorn (Antilocapra americana), for example, exhibit fluctuating herd dynamics and variable seasonal migration strategies, but GPS-tracking studies show that landscape features such as highways impede their movements, leading to conflicting hypotheses about expected levels of genetic structure. Given that pronghorn populations declined significantly in the early 1900s, have only partially recovered, and are experiencing modern threats from landscape modification, conserving connectivity among populations is important for their long-term persistence in North America. To assess the genetic structure and diversity of pronghorn in the core of their range, we genotyped 4,949 genome-wide single nucleotide polymorphisms and 11 microsatellites from 398 individuals throughout the state of Wyoming. We found no evidence of genetic subdivision and minimal evidence of isolation by distance despite a range that spans hundreds of kilometers, multiple mountain ranges, and three interstate highways. In addition, a rare variant analysis using putatively recent mutations found no genetic division between pronghorn on either side of a major highway corridor. Although we found no evidence that barriers to daily and seasonal movements of pronghorn impede gene flow, we suggest periodic monitoring of genetic structure and diversity as a part of management strategies to identify changes in connectivity.","['#Script to identify loci with excess heterozygotes to remove\n# Script created by Monia Hasselhort and C.A. Buerkle, modified by M.E.F. LaCava\n\n\n#### Identify loci with heterozygosity above a given percentage ####\ngenest <- read.table(""pntest_mean_398_sorted.txt"", header=F)\ntmp.het.count <- apply(genest, 1, function(x) sum(abs(x-1)< 0.05, na.rm=T))\ntmp.af<-apply(genest, 1, mean, na.rm=T)/2\n#plot heterozygote count against allele frequency \n# - peak at center are loci with 0.5 allele frequencing in almost all individuals (unlikely to be biologically real)\nplot(tmp.af,tmp.het.count)\n#plot \ntmp<-apply(genest, 1, function(x) length(which(x==1)))\nplot(tmp.het.count, tmp, xlim=c(0,400), ylim=c(0,400))\n\n#count number of loci with heterozygosity in more than X% of individuals\nlength(which(tmp.het.count > 0.95*398))\n\n#remove improbable loci from dataset\ngenest.subset <- genest[-which(tmp.het.count > 0.95*398),] #32 loci that are hetero in >95% of samples\n\n#correlation between genest and genest.subset covariance matrices\ng.covar<-getcovarmat(genest)\ngsubset.covar<-getcovarmat(genest.subset)\ncor(g.covar[lower.tri(g.covar)], gsubset.covar[lower.tri(gsubset.covar)])\n\n\n\n\n\n\n#### Probabilistically identify loci with excess heterozygosity ####\n\n#import data:\nmyvcf<-read.table(""ph398_maf0.01_miss0.5_ind0.6_dp3_BayesNoOutliers_GT_DP.txt"", stringsAsFactors=F)\n## this looks like this:\n## scaffold_11:53 APH_EE_001 0/0 99,0\n## scaffold_11:53 APH_EE_002 0/0 49,0\n## scaffold_11:53 APH_EE_003 0/0 99,0\n## scaffold_11:53 APH_EE_005 0/0 99,0\n## scaffold_11:53 APH_EE_007 0/0 99,0\n## scaffold_11:53 APH_EE_009 0/0 59,0\n## scaffold_11:53 APH_EE_010 0/0 100,0\n## scaffold_11:53 APH_EE_013 0/0 99,0\n## scaffold_11:53 APH_EE_015 0/0 63,0\n## scaffold_11:53 APH_EE_017 0/0 96,0\n\n\nAD<-matrix(as.numeric(unlist(strsplit(myvcf$V4, "",""))), ncol=2, byrow=T)\nADhetprob<-apply(AD[myvcf$V3 == ""0/1"",],1, function(x) pbinom(x[1], prob=0.5, size=sum(x)))\n\ncor(ADhetprob, rowSums(AD[myvcf$V3==""0/1"",]))\n#theirs: [1] 0.002717052\n#mine: [1] -0.03793837\n\nextreme_ADhetprob_by_contig<-by(ADhetprob, myvcf$V1[myvcf$V3 == ""0/1""], function(x)sum(x>0.95 | x<0.05)/length(x))\nmean_ADhetprob_by_contig<-by(ADhetprob, myvcf$V1[myvcf$V3 == ""0/1""], mean)\nN_ADhetprob_by_contig<-by(ADhetprob, myvcf$V1[myvcf$V3 == ""0/1""], length)\n\nplot(mean_ADhetprob_by_contig ~ by(rowSums(AD[myvcf$V3==""0/1"",]), myvcf$V1[myvcf$V3 == ""0/1""], mean))\n\n\n##  length(ADhetprob)/nrow(myvcf)\n## [1] 0.2529876\n\n#subset genotype matrix according to above calc\nnind <- 398\nnloci<- 4949\ng20<-matrix(scan(""pntest_mean_ph398_maf0.01_miss0.5_ind0.6_dp3_BayesNoOutliers.recode.txt"",n=nind*nloci,sep="" ""),nrow=nloci,ncol=nind,byrow=T)\ng20.subset<-g20[ mean_ADhetprob_by_contig < 0.9 & mean_ADhetprob_by_contig > 0.1,]\n# 3947 loci retained\n\npc20<-do.pca(g20.subset)\npc20Summary<-summary(pc20)\n\npar(mfrow=c(1,1), pty=\'s\')\nplot(pc20$x[,\'PC1\'], pc20$x[,\'PC2\'],col=inds$HRColor ,  pch=19 , cex=0.8,\n     xlab =  paste(""PC1 ("", round(pc20Summary$importance[2,1]*100, 1), ""%)"", sep=""""),\n     ylab =  paste(""PC2 ("", round(pc20Summary$importance[2,2]*100, 1), ""%)"", sep=""""), \n     main=""PCA 3947loci in 398 inds"")\n#this PCA looks very similar to my original PCA - actually looks more clumped, when I would have expected it to \n# look more spread out. but PC importance increased slightly for both PC1 and PC2\nplot(pc20$x[,\'PC2\'], pc20$x[,\'PC3\'],col=inds$HRColor ,  pch=19 , cex=0.8,\n     xlab =  paste(""PC2 ("", round(pc20Summary$importance[2,2]*100, 1), ""%)"", sep=""""),\n     ylab =  paste(""PC3 ("", round(pc20Summary$importance[2,3]*100, 1), ""%)"", sep=""""))\n\n\n\n#check correlation between original covariance matrix and subset matrix\ngetcovarmat<-function(gmat, write.gcov=FALSE, inds=""""){\n  gmn<-apply(gmat,1,mean, na.rm=T)\n  gmnmat<-matrix(gmn,nrow=nrow(gmat),ncol=ncol(gmat))\n  gprime<-gmat-gmnmat ## remove mean\n  \n  gcovarmat<-matrix(NA,nrow=ncol(gmat),ncol=ncol(gmat))\n  for(i in 1:ncol(gmat)){\n    for(j in i:ncol(gmat)){\n      if (i==j){\n        gcovarmat[i,j]<-cov(gprime[,i],gprime[,j], use=""pairwise.complete.obs"")\n      }\n      else{\n        gcovarmat[i,j]<-cov(gprime[,i],gprime[,j], use=""pairwise.complete.obs"")\n        gcovarmat[j,i]<-gcovarmat[i,j]\n      }\n    }\n  }\n  gcovarmat\t\n}\n\ng20.covar<-getcovarmat(g20)\ng20subset.covar<-getcovarmat(g20.subset)\ncor( g20.covar[lower.tri(g20.covar)], g20subset.covar[lower.tri(g20subset.covar)])\n\n', '#PCA with color gradient\n# PCA function created by C.A. Buerkle, rest of script created by M.E.F. LaCava\n\n####Create colors grid ####\n## Test color mixing\n#install.packages(""colorspace"")\nlibrary(colorspace)\nmixcolor(0.5, RGB(1,0,0), RGB(0,0,1))\nbarplot(1:3,col=c(rgb(1, 0, 0),rgb(0.5,0,0.5), rgb(0,0,1)))\n\n#blues <- c(rgb(0,0,1),rgb(.24,.24,1),rgb(.48,.48,1),rgb(.73,.73,1))\nblues <- c(rgb(0,0,1),rgb(.3,.3,1),rgb(.61,.61,1),rgb(.91,.91,1))\nbarplot(1:4,col=blues)\n#reds <- c(rgb(1,0,0),rgb(1,.18,.18),rgb(1,.37,.37),rgb(1,.55,.55))\nreds <- c(rgb(1,0,0),rgb(1,.3,.3),rgb(1,.61,.61),rgb(1,.91,.91))\nbarplot(1:4,col=reds)\n\n## Make matrix of 16 colors for grid\n# used website to pick initial 4 colors in blue and 4 in red\n#   https://meyerweb.com/eric/tools/color-blend/#:::rgbd\ncol.grid <- matrix(nrow=4,ncol=4)\n# two gradients of color\n#blues <- list(c(.73,.73,1),c(.48,.48,1),c(.24,.24,1),c(0,0,1))\nblues <- list(c(.91,.91,1),c(.61,.61,1),c(.3,.3,1),c(0,0,1))\n#reds <- list(c(1,.55,.55),c(1,.37,.37),c(1,.18,.18),c(1,0,0))\nreds <- list(c(1,.91,.91),c(1,.61,.61),c(1,.3,.3),c(1,0,0))\n# mix gradients to create grid\nfor (i in 1:nrow(col.grid)){\n  for (j in 1:ncol(col.grid)){\n    r <- (blues[[i]][1]+reds[[j]][1])/2\n    g <- (blues[[i]][2]+reds[[j]][2])/2\n    b <- (blues[[i]][3]+reds[[j]][3])/2\n    col.grid[i,j] <- rgb(r,g,b)\n  }\n}\ncol.grid\nbarplot(1:16,col=col.grid)\ncol.vect <- as.vector(t(col.grid))\n\n## Import state polygon and sample locations\nstate <- readOGR(dsn=paste(getwd(),""/LandscapeFeatures"",sep=""""), layer=""state"")\nproj4string(state) <- ""+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0""\nlatlong <- read.csv(""LatLong_398ind.csv"", header=F, stringsAsFactors=F)\n#Or microsat samples\n#WD <- ""/Users/melanielacava/Data/PHmsats/""\n#setwd(WD)\n#latlong <- read.csv(""LatLong_274ind.csv"", header=F, stringsAsFactors=F)\n\nnames(latlong) <-c(""id"",""x"",""y"")\ncoordinates(latlong) <- ~ x + y\nproj4string(latlong) <- ""+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0""\nproj4string(latlong)==proj4string(state)\nplot(state)\nplot(latlong,add=T)\n#add grid to state\nbb <- bbox(state)\ncs <- c((bb[1,2]-bb[1,1])/4,(bb[2,2]-bb[2,1])/4) #cell size\ncc <- bb[,1]+(cs/2)\ncd <- ceiling(diff(t(bb))/cs) #number of cells per direction\ngrd <- GridTopology(cellcentre.offset=cc, cellsize=cs, cells.dim=cd)\ngrd\nsp_grd <- SpatialGridDataFrame(grd,\n                               data=data.frame(id=1:prod(cd)),\n                               proj4string=CRS(proj4string(state)))\n#import interstate to overlay on grid\nhighway <- readOGR(dsn=paste(getwd(),""/LandscapeFeatures"",sep=""""), layer=""InterstateOnly"")\n#check it worked\nlibrary(""lattice"")\nspplot(sp_grd, colorkey=F, region=T, col.regions=as.vector(t(col.grid)),\n       panel = function(...) {\n         panel.gridplot(..., border=""black"")\n         #         sp.polygons(state)\n         sp.polygons(highway,lwd=2) #add highways to help orient map\n         sp.points(latlong, cex=1,col=""black"",pch=16)\n         #         panel.text(...)\n       })\n#export as pdf\npdf(file=""samples_gradientmap.pdf"",width=5,height=5,useDingbats=FALSE)\nspplot(sp_grd, colorkey=F, region=T, col.regions=as.vector(t(col.grid)),\n       panel = function(...) {\n         panel.gridplot(..., border=""black"")\n         #         sp.polygons(state)\n         sp.polygons(highway,lwd=2) #add highways to help orient map\n         sp.points(latlong, cex=0.8,col=""black"",pch=16)\n         #         panel.text(...)\n       })\ndev.off()\n\n## Assign colors to points\nover(latlong,sp_grd) #which grid cell is each sample in\nhead(latlong)\nlatlong$cell <- over(latlong,sp_grd)[,1] \nlatlong$color <- NA\nfor (i in 1:nrow(latlong)){\n  latlong$color[i] <- col.vect[latlong$cell[i]]\n}\nhead(latlong)\n#check it looks right on map\nplot(state)\nplot(latlong,add=T,col=latlong$color)\n\n\n#### SNP PCA ####\n#PCA function\ndo.pca<-function(gmat, write.gcov=FALSE, inds=""""){\n  gmn<-apply(gmat,1,mean, na.rm=T) #takes a mean across individuals for each locus\n  gmnmat<-matrix(gmn,nrow=nrow(gmat),ncol=ncol(gmat)) #creates matrix with mean filled in for entire row\n  gprime<-gmat-gmnmat ## remove mean \n  \n  gcovarmat<-matrix(NA,nrow=ncol(gmat),ncol=ncol(gmat))\n  for(i in 1:ncol(gmat)){\n    for(j in i:ncol(gmat)){\n      if (i==j){\n          gcovarmat[i,j]<-cov(gprime[,i],gprime[,j], use=""pairwise.complete.obs"") #only use loci that both samples in pair have a genotype for\n      }\n      else{\n        gcovarmat[i,j]<-cov(gprime[,i],gprime[,j], use=""pairwise.complete.obs"") #only use loci that both samples in pair have a genotype for\n        gcovarmat[j,i]<-gcovarmat[i,j]\n      }\n    }\n  }\n  if(write.gcov==TRUE){\n    inds<-ifelse(inds == """", paste(""i"", 1:ncol(gmat)), inds)\n    write.table(round(gcovarmat,5),file=""gcovarmat.txt"",\n                quote=F,row.names=F,col.names=inds)\n  }\n  prcomp(x=gcovarmat,center=TRUE,scale=FALSE)\n}\n\n#import data\ngeno <- read.table(""pntest_mean_398_sorted.txt"", header=F, stringsAsFactors=F)\n#convert genotypes to matrix\npcatest <- as.matrix(geno)\npca.snp <- do.p', '# This script contains functions for counting how many rare alleles are shared on same side vs. diff side of highway\n# Created by Elizabeth Mandeville, modified by M.E.F. LaCava and C.A. Buerkle\n\n####Count private alleles ####\n# Count # private rare alleles vs. shared across populations \ncount.private <- function(g, mac, group){\n  heterozygote.inds<-apply(g[apply(g, 1, function(x) sum(abs(x-1)< 0.05, na.rm=T)) == mac,], 1, function(x){which(abs(x - 1) <0.05)})\n  #count.diff<-sum(apply(heterozygote.inds, 2, function(x) sum(abs(diff(group[x])))))\n  #again, diff function doesn\'t work properly: group=0,0,1 results in 1 but group=0,1,0 results in 2 -> over-counts loci\n  count.diff<-sum(apply(heterozygote.inds, 2, function(x) var(group[x])!=0)) #var of group=0 when group is all same, so !=0 when diff\n  return(data.frame(count.same=ncol(heterozygote.inds) - count.diff, \n                    count.diff=count.diff, \n                    prop.private=1-(count.diff/ncol(heterozygote.inds))))\n}\n\n####Test on fake data ####\n#make fake genotype data - mac 2-5, 2 loci in each mac (1 same side, 1 diff side, except for mac=6 both diff side)\nfakedata <- matrix(data=0,nrow=10,ncol=10)  # loci are in rows, individuals are in columns\nfakedata[1,1:2] <- 1 #mac=2, same side   \nfakedata[2,1:3] <- 1 #mac=3, same side\nfakedata[3,1:4] <- 1 #mac=4, same side\nfakedata[4,1:5] <- 1 #mac=5, same side\nfakedata[5,1:6] <- 1 #mac=6, diff side\nfakedata[6,5:6] <- 1 #mac=2, diff side\nfakedata[7,5:7] <- 1 #mac=3, diff side\nfakedata[8,5:8] <- 1 #mac=4, diff side\nfakedata[9,5:9] <- 1 #mac=5, diff side\nfakedata[10,5:10] <- 1 #mac=6, diff side\nfakedata\n\n#make fake side data (ind 1-5 on one side, ind 6-10 on other side)\nfakeside <- c(rep(0,5), rep(1,5))\n\n#apply to fake data\nresults <- data.frame(mac=2:6, count.same=rep(0,5), count.diff=rep(0,5), prop.private=rep(0,5))\nfor (i in 2:6) {\n  priv <- count.private(fakedata,i,fakeside)\n  results$count.same[results$mac==i] <- priv$count.same\n  results$count.diff[results$mac==i] <- priv$count.diff\n  results$prop.private[results$mac==i] <- priv$prop.private\n}\nresults$nloci<-results$count.same + results$count.diff\nresults\n\n#### PH data ####\n##Import my data - subset samples around I-80 corridor\ngenest.i80 <- read.table(""pntest_mean_i80_173.sorted.txt"", header=F)\ndb <- read.csv(""ph_i80.csv"", header=T)\n#db$south is the ""group"" data - 1=south of i80, 0=north of i80\n\n##Loop count.private over mac values\nresults <- data.frame(mac=2:10, count.same=rep(0,9), count.diff=rep(0,9), prop.private=rep(0,9))\nfor (i in 2:10) {\n  priv <- count.private(genest.i80,i,db$south)\n  results$count.same[results$mac==i] <- priv$count.same\n  results$count.diff[results$mac==i] <- priv$count.diff\n  results$prop.private[results$mac==i] <- priv$prop.private\n}\nresults$nloci<-results$count.same + results$count.diff\nresults\nresults.private <- results\n\nplot(results.private$mac, results.private$prop.private, ylim=c(0, 1),\n     xlab=""Rare allele count"", ylab=""Proportion private alleles"",\n     main=""Proportion of loci with private alleles (same side of I-80)"")\n\n\n#### Permute to create null model ####\n#Import null model permutations (since it takes a while to run)\n#nullmodel <- read.csv(""nullI80_ind173_mac2-10_100reps.csv"",header=T)\n##Permute group data to generate null model\n# genest.i80 <- read.table(""pntest_mean_i80_173.sorted.txt"", header=F)\n# db <- read.csv(""ph_i80.csv"", header=T)\n# reps <- 100\n# mac <- 2:10\n# null <- data.frame(matrix(nrow=reps*length(mac),ncol=5))\n# names(null) <- c(""iteration"",""mac"",""count.same"",""count.diff"",""prop.private"")\n# null$iteration <- rep(1:reps,each=length(mac))\n# null$mac <- rep(mac,length.out=nrow(null))\n# null[,3:5] <- 0\n# for (j in 1:reps) {\n#   group <- sample(db$south)\n#   for (i in mac) {\n#       priv <- count.private(genest.i80,i,group)\n#       null$count.same[null$iteration==j & null$mac==i] <- priv$count.same\n#       null$count.diff[null$iteration==j & null$mac==i] <- priv$count.diff\n#       null$prop.private[null$iteration==j & null$mac==i] <- priv$prop.private\n#     }\n#   }\n# null\n#write.csv(null,""nullI80_ind173_mac2-10_10reps.csv"",row.names=F,quote=F)\n\n# tmp<-numeric(500)\n# for(i in 1:500){\n#   tmp[i]<-count.private(genest.i80, 2, sample(db$south))$prop.private\n# }\n\n##Plot data with permuted null model\n#calc quantiles\n# for (i in  2:10) {\n#   results.private$lower[results.private$mac==i] <- quantile(nullmodel$prop.private[nullmodel$mac==i],probs=0.025,names=F)\n#   results.private$upper[results.private$mac==i] <- quantile(nullmodel$prop.private[nullmodel$mac==i],probs=0.975,names=F)\n# }\n# results.private\n# #plot\n# plot(results.private$mac, results.private$prop.private, type=""l"", ylim=c(0, 1),\n#      xlab=""Rare allele count"", ylab=""Proportion private alleles"",\n#      main=""Proportion of private alleles (same side of I-80)"")\n# lines(2:10, 2/(2:10 + 1),col=rgb(1,0,0)) # cab: I think this is the correct expectation and the graylines are the uncertainty around it\n# lines(2:10, qbinom(0.975, size=results.private$']",0,"Zieger, et al., Modified denovoLOBGOB pipeline, transcription factor binding sites, de novo mutations, position weight matrices, JASPAR 2020, R scripts, datasets, whole genome sequencing data, trios, nons"
Zieger_et al_Modified denovoLOBGOB pipeline,"This is a modified version of denovoLOBGOB1 to analyze transcription factor binding sites at regions of de novo mutations using position weight matrices from JASPAR 20202.This Zenodo repository contains R-Scripts for the analysis of transcription factor binding sites in datasets of de novo mutations (DNMs) which have been used for the analysis of DNMs in whole genome sequencing data (WGS) of trios with nonsyndromic cleft lip with or without cleft palate (nsCL/P)3. These R scripts are a modified version from denovoLOBGOB, previously denoted as denovoTF, original code available at GitHub: https://github.com/pjshort/denovoTF. 1 https://github.com/pjshort/denovoTF/blob/master/scripts/denovoLOBGOB.RdenovoLOBGOB was used for Short et al., 2018Short, P. J., McRae, J. F., Gallone, G., Sifrim, A., Won, H., Geschwind, D. H., Wright, C. F., Firth, H. V, FitzPatrick, D. R., Barrett, J. C., & Hurles, M. E. (2018). De novo mutations in regulatory elements in neurodevelopmental disorders. Nature, 555(7698), 611616. https://doi.org/10.1038/nature25983 2 https://bioconductor.org/packages/release/data/annotation/html/JASPAR2020.htmlFornes, O., Castro-Mondragon, J.A., Khan, A., van der Lee, R., Zhang, X., Richmond, P.A., Modi, B.P., Correard, S., Gheorghe, M., Baranai, D., et al. (2019). JASPAR 2020: update of the open-access database of transcription factor binding profiles. Nucleic Acids Res. 48, D87D92.3 Zieger, H.K., Weinhold, L., Schmidt, A., Holtgrewe, M., Juranek, S.A., Siewert, A., Scheer, A.B., Thieme, F., Mangold, E., Ishorst, N., Brand, F.U., Welzenbach, J., Beule, D., Paeschke, K., Krawitz, P.M. & Ludwig, K.U., Prioritization of non-coding elements involved in non-syndromic cleft lip with/without cleft palate through genome-wide analysis of de novo mutations, HGG Adv.","['\n\n#Script no.3: Analysis of transcription factor (TF) binding events in two datasets of de novo mutations.\n#This script creates a summary file, summarizing transcription factor binding events\n#for different TF binding sites (motifs represented as position weight matrices).\n#Therefor the ratio of hits for each motif between the two cohorts is calculated, \n#and the strength of binding cange (BC) by the DNM within the TF binding site for the two cohorts is compared.\n#A fisher\'s exact test is used to test for a difference in the number of hits between cohorts for each TF. \n#Log2FC of hits is calculated for all TFs binding motifs with hits in both cohorts with ratio corrected for total number of hits in the respective cohort.\n#A Mann-Whitney-U-Test is used to test for difference in the strength of binding change (for motifs with at least 5 hits, and variance for BC in both cohorts).\n#Log2FC of mean binding change is calculated as the ratio of mean binding change for the specific TF motif (Jaspar_Internal id) in the two cohorts.\n\n#INPUT: \n#File with TF binding events for two datasets of de novo mutations (allPWMDNM_withoutduplis.tsv) \n#de novo file with Cohort and sample information; mandatory columns: ""unique_id"", ""CHROM""/chr"", ""POS""/""pos"", ""REF""/""ref"", ""ALT""/""alt"", ""Cohort"", ""Sample""\n#This file is  output file of the second R-Script: reductionofmultiplehitsperDNMPWM.R \n#This one should include one row per TF binding event for each DNM which hit TF with >=95% match\n\n#OUTPUT: summary file called hitsandchangeofbinding.tsv \n#Analysis of binding events for DNMs in both cohorts. \n\n#Columns in OUTPUT: \n#jaspar_internal:id from Jaspar 2020 for specific PWM, cohort1: number of hits in coh1, cohort2: number of hits in cohort2,\n#cohort1rel: hits corrected for absolute number of hits in cohort; cohort2rel: hits corrected for absolute number of hits in cohort, \n#ratio: ratio of relative number of hits, fisherspvalue: fisher\'s exact test for excess of binding events in coh 1, \n#pvaluemultipl: fisher\'s exact test corrected for multiple testing,\n#log2fc: log2 of ratio of corrected number of hits in cohorts,\n#pvaluemwutest: Mann-Whitney-U-Test for excess of change of binding by DNMs in cohort 1,\n#meancobcohort1: mean change of binding of DNM binding events for specific PWM, cohort 1, \n#meancobcohort2:\tmean change of binding of DNM binding events for specific PWM, cohort 2,\n#ratio_cob: ratio of mean binding change for PWM between cohorts,\n#logBC: log2fc of ration of mean binding changes;\ttfbs_name: name of TF of respective PWM\n\n\n#This R-Script includes analysis of number of hits per PWM and change of binding by DNMs between two cohorts\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n#results from reductionofmultiplehitsperDNMPWM.R are loaded here\n#setwd(""YOURPATH"")\nfread(""allPWMDNM_withoutduplis.tsv"")->newversion\n#suppl. information to DNMs/variants including Cohort of DNMs\nvariants<- read.table(""DNMs_cohortinfo.tsv"", sep = ""\\t"", header = TRUE, stringsAsFactors = FALSE)\nvariants%>%dplyr::select(CHROM, POS, REF, ALT, Cohort, Sample)->variants\n#Change to the following line in case of colnames ""chr"",""pos"",""ref"",""alt"", ""Cohort"", ""Sample\n#variants%>%dplyr::select(chr, pos, ref, alt, Cohort, Sample)->variants\ncolnames(variants)=c(""chr"", ""pos"", ""ref"", ""alt"",  ""Cohort"", ""Sample"")\n\n\n\n#PWMs only hit with cange of binding == 0 so SILENT instead of gain/loss of binding\n#no silent hits in test DNMs\nnewversion%>%filter(result==""SILENT"")%>%distinct(jaspar_internal)\nnewversion%>%filter(result==""SILENT"")%>%distinct(jaspar_internal)->silents \nnewversion%>%filter(result!=""SILENT"")%>%distinct(jaspar_internal)->nonsilents \nsilents$jaspar_internal%in%nonsilents$jaspar_internal #check that all PWMs that were hit also have a change of binding at least once.\n\n\n#Use of jaspar_internal id so that hits are counted per TFBS, i.e. motif, and not for all PWMs of a TF together.\n\nanalysis%>%group_by(Cohort)%>%count(jaspar_internal)%>%tidyr::spread(key = Cohort,value = n)->motifs\nanalysis %>% group_by(tfbs_name) %>% count(Cohort)%>%arrange(desc(n))->x\n\n#number of hits per cohort\nunique(variants$Cohort)->cohort_names\nx%>%filter(Cohort==colnames(motifs)[2])->a\nx%>%filter(Cohort==colnames(motifs)[3])->b\nsum(a$n) # number of hits Cohort1\nsum(b$n) #number of hits Cohort2\nsum(x$n)==(sum(a$n)+sum(b$n))\n\n#number of DNMs that hit TF  \nanalysis%>%filter(Cohort==colnames(motifs)[2])%>%count(unique_id, Sample)%>%nrow()\nanalysis%>%filter(Cohort==colnames(motifs)[3])%>%count(unique_id, Sample)%>%nrow()\n\n\n#TODO add positional information (exonic and intronic) of your DNMs\n#variants%>%dplyr::select(CHROM, POS, REF, ALT,EXON, INTRON, Sample, Cohort)->variants\n#colnames(variants)=c(""chr"", ""pos"", ""ref"", ""alt"", ""EXON"", ""INTRON"", ""Sample"", ""Cohort"")\n#left_join(newversion, variants)->analysis\n#Distribution of TF hits in exonic/intronic/intergenic\n#analysis%>%filter(Cohort==cohort_names[1])%>%count(EXON, INTRON)\n#analysis%>%filter(Cohort==cohort_names[2])%>%count(EXON, INTRON)\n\n', '#This script no.1 , originally from Patrick Short (https://github.com/pjshort/denovoTF/blob/master/scripts/denovoLOBGOB.R)\n#was modified by Hanna K. Zieger (hanna.zieger@uni-bonn.de) for an analysis of transcription factor binding sites \n#(Zieger et al. (2022), Prioritization of non-coding elements involved in non-syndromic cleft lip with/without cleft palate \n#through genome-wide analysis of de novo mutations, HGG Adv.)\n#used Source Scripts are core_minus.R and core_plus.R for binding events on strands separately.\n\n#This is the main script to run in order to annotate a list of de novos mutations (DNMs) with predicted transcription factor (TF) binding sites.\n\n# INPUT:\n# de novo file with mandatory columns: ""unique_id"", ""CHROM""/chr"", ""POS""/""pos"", ""REF""/""ref"", ""ALT""/""alt"" - if no unique_id is provided, then one will be made in the form chr:posref>alt\n# optional: --tf_list -> list of transcription factor motifs (by jaspar_internal id e.g. MA0098.1) in single column with NO HEADER\n# optional: --hg_version -> build for human genome (this is essential to get right as it determines the sequence context used to scan)\n# optional: --min_score -> minimum score cutoff to call binding event (default to 95% i.e. adjusted p-val <0.05)\n\n# OUTPUT of this script:\n# de novo output file with columns ""unique_id"", ""chr"", ""pos"", ""ref"", ""alt"", ""tf_name"", ""jaspar_internal"", ""ref_score"", ""alt_score"" \n# with ONE ROW PER TF BINDING EVENT. the output file will likely have more rows than the input file (many more if score threshold is low)\n#OUTPUT file should then be further modified in the next script:\n#reduction of positions weight matrix (PWM, name: jaspar_internal ids) - DNM - combinations (binding events at DNM positions) with script reductionofmultiplehitsperDNMPWM.R\n\n\n#Make sure you install all needed packages\n#if (!requireNamespace(""BiocManager"", quietly = TRUE))\n#  install.packages(""BiocManager"")\n#BiocManager::install(version = ""3.13"")\n#BiocManager::install(""BSgenomes"")\n#BiocManager::install(""BSgenome.Hsapiens.UCSC.hg19"") # load the hg19 genome build - takes a little while as it is approx. 800 MB\n#BiocManager::install(""Go.db"")\n#BiocManager::install(""TFBSTools"")\n#BiocManager::install(""JASPAR2020"")\n#BiocManager::install(""optparse"")\n\nlibrary(BiocManager)\nlibrary(optparse)\nlibrary(BSgenome.Hsapiens.UCSC.hg19)\nlibrary(TFBSTools)\nlibrary(dplyr)\nlibrary(tidyr)\n\n#Change from JASPAR 2014 used in denovoTF/denovoLOBGOB to JASPAR 2020\n#jaspar 2020 was not available in initializejaspar (#initializeJASPARDB(db, version=""2020""); #db = ""//.../myMatrixDb.sqlite""\n#so install/load via github\n#install.packages(""remote"")\nlibrary(remote)\nremotes::install_github(""da-bar/JASPAR2020"")\nlibrary(JASPAR2020)\n\n#set working directory to folder with scripts if necessary\n#setwd(""YOURPATH/TFbinding_DNMset/"")\nde_novos_original <- read.table(""DNMs.tsv"", sep = ""\\t"", header = TRUE, stringsAsFactors = FALSE)\nde_novos_original%>%dplyr::select(CHROM, POS, REF, ALT)->de_novos_original\n#change to the following line in case of colnames ""chr"",""pos"",""ref"",""alt""\n#de_novos_original%>%dplyr::select(chr, pos, ref, alt)->de_novos_original\ncolnames(de_novos_original)<-c(""chr"", ""pos"", ""ref"", ""alt"")\n\n#Load your dataset of DNMs\n#This table should include columns for chromosome, position, reference, and alternative allele\n#de_novos_original <- read.table(""XXXXXX"", sep = ""\\t"", header = TRUE, stringsAsFactors = FALSE)\n#de_novos_original%>%dplyr::select(CHROM, POS, REF, ALT)->de_novos_original\n#colnames(de_novos_original)<-c(""chr"", ""pos"", ""ref"", ""alt"")\n\n# Remove indels from de novo file \nde_novos_original = de_novos_original[nchar(as.character(de_novos_original$ref)) == 1 & nchar(as.character(de_novos_original$alt)) == 1,]\n\n\n### Check that the input file has columns ""unique_id"", ""chr"", ""pos"", ""ref"", ""alt"". if no ""unique_id"", create one\nreqd_columns <- c(""chr"", ""pos"", ""ref"", ""alt"")\n\n# Throw error if any required column is missing\nif (!all(reqd_columns %in% colnames(de_novos_original))){\n  stop(""One or more of the required column names missing from input de novo file. Requires: \\""chr\\"", \\""pos\\"", \\""ref\\"", \\""alt\\"""")\n}\n\n# unique_id not present, add column in the form chr:posREF>ALT\nif (!(""unique_id"" %in% colnames(de_novos_original))){\n  de_novos_original$unique_id <- paste0(de_novos_original$chr, "":"", de_novos_original$pos, de_novos_original$ref, "">"", de_novos_original$alt)\n}\n\n# NOTE: db is initialized to ../data/myMatrixDb.sqlite after build.R is run\npwm_options = list(""species"" = 9606, ""all_versions"" = TRUE, ""matrixtype"" = ""PWM"") # 9606 = ""homo sapiens""\npwm_list = getMatrixSet(JASPAR2020, pwm_options)\n\nmax_motif_length = max(sapply(pwm_list, function(t) ncol(t@profileMatrix)))\n\nm = max_motif_length\n\n\n### Calculation for minus strand -----\nsource(""core_minus.R"")\n\n\n#We selected a number of DNMs for analyzing position weight matrices (PWMs).\n#We have split the DNMs to sets of rows of DNMs and merged the results to reduce the error rate during the long calculation \n#by saving the results more', '\n\n\n#Core.R from Short et al is separated for plus and minus strand.\n#Changed from denovoLOBGOB calculation, PWM analysis for minus and plus strand is splitted to get all results for both strands.\n#This script sets functions for analysis of PWM DNM binding motifs on minus strand.\n\n#We also changed that in case of hit for PWM by ref allele (>= 95% of max matrix sum of PWM).\n#ALT allele is always checked as well, since this could result in different highest Change of binding (COB).\n\n\nget_sequence <- function(chr, start, stop, version = ""hg19"") {\n  \n  # input: (multiple) chr, start, stop, hg version (defaults to hg19)\n  # output: list of sequences as DNAStrings object for each input\n  \n  if (version == ""hg19""){\n    library(BSgenome.Hsapiens.UCSC.hg19)\n  } else if (version == ""hg18""){\n    library(BSgenome.Hsapiens.UCSC.hg18) # TODO need to add download of hg18 to build.R\n  }\n  \n  if (!all(grepl(pattern = ""^chr"", chr))){  # assert that chromosome column have chr in front\n    warning(""Not all entries in the chromosome column start with \\""chr\\"" - try reformatting this column e.g. \\""chrX\\"" instead of \\""X\\"" with paste0(\\""chr\\"",chr_number"")\n    chr = paste0(""chr"", chr)\n  }\n  \n  seqs = getSeq(Hsapiens, chr, start, stop)\n  return(seqs)\n}\n\nget_alt_sequence <- function(sequence, sub_position, alt) {\n  \n  # input: sequence, positions where alteration has occured, alteration to substitute in\n  # output: new alt_sequence\n  \n  alt_sequence = sequence\n  alt_sequence[sub_position] = alt # TODO: add a test to ensure that sub_position and alt are the same length\n  \n  return(alt_sequence)\n}\n\n### Annotated sequences with predicted TF binding\n\nsingle_sequence_coverage <- function(seq, rel_pos, pwm_list, min.score = ""95%""){\n  \n  # input: DNAString sequence, list of PWMs to query, min.score (optional)\n  # returns: site\n  # returns all regions predicted to have TFB affinity >= min.score\n  \n  # TODO: write test for this section\n  \n  # scan full list of PWMs against the sequence provided\n  site_seq_list = searchSeq(pwm_list, seq, seqname=""ref_sequence"", min.score=min.score, strand=""-"")\n  \n  # keep only the TFs that have a hit greater than min score\n  interval_hits = site_seq_list[which(sapply(site_seq_list, length) > 0)]\n  \n  # keep only the TFs that have a hit in region that overlaps with the de novo\n  overlaps_dn = sapply(interval_hits, function(t) t[(rel_pos >= start(t@views@ranges)) & (rel_pos <= end(t@views@ranges))])\n  \n  # filter list to remove the empty TFs\n  pos_hits = overlaps_dn[which(sapply(overlaps_dn, length) > 0)]\n  \n  return(pos_hits) # returns a (possibly empty) list of SiteSet objects\n}\n\nscan_regions <- function(sequences, rel_positions, pwm_list, min.score = ""95%""){\n  \n  # input: vector of sequences, vector of relative positions of de novo within sequence, list of PWMs to query, minimum binding score (optional)\n  # output: list with one element for each pair of seq, rel_pos that contains predicted de novo binding events (if any)\n  \n  scan_results = mapply(single_sequence_coverage, sequences, rel_positions, MoreArgs = list(""pwm_list"" = pwm_list))\n  \n  return(scan_results)\n}\n\nscan_full_sequence <- function(seq, pwm_list, min.score = ""95%""){\n  \n  # input: DNAString sequence, list of PWMs to query, min.score (optional)\n  # returns: site\n  # returns all regions predicted to have TFB affinity >= min.score\n  \n  # TODO: write test for this section\n  \n  # scan full list of PWMs against the sequence provided\n  site_seq_list = searchSeq(pwm_list, seq, seqname=""ref_sequence"", min.score=min.score, strand=""*"")\n  \n  # keep only the TFs that have a hit greater than min score\n  interval_hits = site_seq_list[which(sapply(site_seq_list, length) > 0)]\n  \n  # filter list to remove the empty TFs\n  pos_hits = interval_hits[which(sapply(interval_hits, length) > 0)]\n  \n  return(pos_hits) # returns a (possibly empty) list of SiteSet objects\n}\n\nLOBGOB_scan <- function(ref_seq, rel_pos, ref, alt, pwm_list, min.score = ""95%""){\n  \n  # input: single ref sequence, single alt sequence, list of PWMs to query, minimum binding score (optional)\n  # returns: SiteSetList of original site that was passed plus any sites that were NOT found with ref (but are found with alt)\n  # stand for \'loss of binding gain of binding scan\'\n  \n  # TODO: alter so rel_pos can be a range instead of a point!\n  \n  if (typeof(ref_seq) != ""DNAString"") { ref_seq = DNAString(ref_seq)}\n  \n  alt_seq = get_alt_sequence(ref_seq, rel_pos, alt)\n  \n  # scan against all PWMs with the reference sequence and alt (after mutation)\n  # the only differences between scan results should be as due to a change in binding affinity due to the mutations\n  ref_results = single_sequence_coverage(ref_seq, rel_pos, pwm_list, min.score = min.score)\n  alt_results = single_sequence_coverage(alt_seq, rel_pos, pwm_list, min.score = min.score)\n  #alt_results = alt_results[!(names(alt_results) %in% names(ref_results))] # only the binding events NOT already spotted in ref\n  #this line was removed from output to get ', '\n\n#Core.R from Short et al is separated for plus and minus strand.\n#Changed from denovoLOBGOB calculation, PWM analysis for minus and plus strand is splitted to get all results for both strands.\n#This script sets functions for analysis of PWM DNM binding motifs on plus strand.\n\n#We also changed that in case of hit for PWM by ref allele (>= 95% of max matrix sum of PWM).\n#ALT allele is always checked as well, since this could result in different highest Change of binding (COB).\n\n\nget_sequence <- function(chr, start, stop, version = ""hg19"") {\n  \n  # input: (multiple) chr, start, stop, hg version (defaults to hg19)\n  # output: list of sequences as DNAStrings object for each input\n  \n  if (version == ""hg19""){\n    library(BSgenome.Hsapiens.UCSC.hg19)\n  } else if (version == ""hg18""){\n    library(BSgenome.Hsapiens.UCSC.hg18) # TODO need to add download of hg18 to build.R\n  }\n  \n  if (!all(grepl(pattern = ""^chr"", chr))){  # assert that chromosome column have chr in front\n    warning(""Not all entries in the chromosome column start with \\""chr\\"" - try reformatting this column e.g. \\""chrX\\"" instead of \\""X\\"" with paste0(\\""chr\\"",chr_number"")\n    chr = paste0(""chr"", chr)\n  }\n  \n  seqs = getSeq(Hsapiens, chr, start, stop)\n  return(seqs)\n}\n\nget_alt_sequence <- function(sequence, sub_position, alt) {\n  \n  # input: sequence, positions where alteration has occured, alteration to substitute in\n  # output: new alt_sequence\n  \n  alt_sequence = sequence\n  alt_sequence[sub_position] = alt # TODO: add a test to ensure that sub_position and alt are the same length  # nchar(toString(alt_sequence))==1\n  return(alt_sequence)\n}\n\n### Annotated sequences with predicted TF binding\n\nsingle_sequence_coverage <- function(seq, rel_pos, pwm_list, min.score = ""95%""){\n  \n  # input: DNAString sequence, list of PWMs to query, min.score (optional)\n  # returns: site\n  # returns all regions predicted to have TFB affinity >= min.score\n  \n  # TODO: write test for this section\n  \n  # scan full list of PWMs against the sequence provided\n  site_seq_list = searchSeq(pwm_list, seq, seqname=""ref_sequence"", min.score=min.score, strand=""+"")\n\n  # keep only the TFs that have a hit greater than min score\n  interval_hits = site_seq_list[which(sapply(site_seq_list, length) > 0)]\n  \n  # keep only the TFs that have a hit in region that overlaps with the de novo\n  overlaps_dn = sapply(interval_hits, function(t) t[(rel_pos >= start(t@views@ranges)) & (rel_pos <= end(t@views@ranges))])\n  \n  # filter list to remove the empty TFs\n  pos_hits = overlaps_dn[which(sapply(overlaps_dn, length) > 0)]\n  \n  return(pos_hits) # returns a (possibly empty) list of SiteSet objects\n}\n\nscan_regions <- function(sequences, rel_positions, pwm_list, min.score = ""95%""){\n  \n  # input: vector of sequences, vector of relative positions of de novo within sequence, list of PWMs to query, minimum binding score (optional)\n  # output: list with one element for each pair of seq, rel_pos that contains predicted de novo binding events (if any)\n  \n  scan_results = mapply(single_sequence_coverage, sequences, rel_positions, MoreArgs = list(""pwm_list"" = pwm_list))\n  \n  return(scan_results)\n}\n\nscan_full_sequence <- function(seq, pwm_list, min.score = ""95%""){\n  \n  # input: DNAString sequence, list of PWMs to query, min.score (optional)\n  # returns: site\n  # returns all regions predicted to have TFB affinity >= min.score\n  \n  # TODO: write test for this section\n  \n  # scan full list of PWMs against the sequence provided\n  site_seq_list = searchSeq(pwm_list, seq, seqname=""ref_sequence"", min.score=min.score, strand=""*"")\n  \n  # keep only the TFs that have a hit greater than min score\n  interval_hits = site_seq_list[which(sapply(site_seq_list, length) > 0)]\n  \n  # filter list to remove the empty TFs\n  pos_hits = interval_hits[which(sapply(interval_hits, length) > 0)]\n  \n  return(pos_hits) # returns a (possibly empty) list of SiteSet objects\n}\n\nLOBGOB_scan <- function(ref_seq, rel_pos, ref, alt, pwm_list, min.score = ""95%""){\n  \n  # input: single ref sequence, single alt sequence, list of PWMs to query, minimum binding score (optional)\n  # returns: SiteSetList of original site that was passed plus any sites that were NOT found with ref (but are found with alt)\n  # stand for \'loss of binding gain of binding scan\'\n  \n  # TODO: alter so rel_pos can be a range instead of a point!\n  \n  if (typeof(ref_seq) != ""DNAString"") { ref_seq = DNAString(ref_seq)}\n  \n  alt_seq = get_alt_sequence(ref_seq, rel_pos, alt)\n  \n  # scan against all PWMs with the reference sequence and alt (after mutation)\n  # the only differences between scan results should be as due to a change in binding affinity due to the mutations\n  ref_results = single_sequence_coverage(ref_seq, rel_pos, pwm_list, min.score = min.score)\n  alt_results = single_sequence_coverage(alt_seq, rel_pos, pwm_list, min.score = min.score)\n  #alt_results = alt_results[!(names(alt_results) %in% names(ref_results))] # only the binding events NOT already spotted in ref\n  #this line wa', '\n\n#Script no.2 for reduction of multiple binding events for position weight matrix (PWM) - de novo mutation (DNM) combination \n#(>=2 hits for one transcription factor (TF) motif (Jaspar_internal id) with one DNM) \n#Therefore PWM-DNM hit with highest change of binding is selected and in case of hits on both strands + strand is preferred over - strand.\n\n#INPUT\n#TFBS (PWM) hits for all DNMs in data set with columns ""unique_id"", ""chr"", ""pos"", ""ref"", ""alt"", ""tf_name"", ""jaspar_internal"", ""ref_score"", ""alt_score"" \n#this is the output file from R script analysisofbindingevents.R , saved as two tsv tables: jaspar2020_results_minusstrand.tsv, jaspar2020_results_plusstrand.tsv\n\n\n#OUTPUT\n#TFBS hits for DNMs in dataset reduced to one row for TF binding event for one DNM\n#saved as ""allPWMDNM_withoutduplis.tsv""\n#Input file for 3rd script with analysis: analysisofbindingevents.R\n\n#load packages \nlibrary(data.table)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# Processing of variants - 1 hit per TFBS motif per DNM ----\n#Set your working directory\n#setwd(""YOURPATH/TFbinding_DNMset/"")\nfread(""jaspar2020_results_minusstrand.tsv"")->minusstrand\nfread(""jaspar2020_results_plusstrand.tsv"")->plusstrand\n\nrbind(minusstrand,plusstrand)->bothstrands\n\n\n##Reduce double hits (1 TF motif: 1 hit per DNM)----\n#Select one hit per one DNM pro TFBSmotif/PWM  \n#Exclusion of rows, with TFBS motif/PWM hit multiple times by 1 DNM (only changed position of DNM in PWM motif)\n#select greatest change of binding\n\n#filter for PWM DNM combinations with multiple hits\n\nbothstrands %>% \n  group_by(chr, pos,  ref, alt, unique_id, tfbs_name, jaspar_internal) %>%\n  filter(n()>1)->test\n\ntest<-data.table(test)\n\n#single PWM-DNM combinations are stored in nodupl\nanti_join(bothstrands,test)->nodupl\nnrow(test)+nrow(nodupl)==nrow(bothstrands)\n\n##all PWM-DNM combinations stored in test(->duplicates) should be reduced to PWM-DNM combination with highest binding change by DNM\n#for this loss of bindings must be rewritten in positive values\n\ntest$diff_pos=0\ntest$diff_pos=abs(test$diff)\n\n#aggregate function for finding maximum of absolute binding change for unique_id (chrom,pos,ref,alt ) und jaspar internal (PWM) combination\ntest.agg <- aggregate(diff_pos ~ unique_id+jaspar_internal+chr+pos+ref+alt+tfbs_name, test, max)\n\n#for these results we add information from duplicated dataset stored in variable ""test"" \nleft_join(test.agg, test)->duplis\n\n#for preferring plusstrand over minusstrand, output is arranged by strand \n#afterwards top result of combination is selected by unique function\nduplis%>%arrange(desc(strand))->duplis\n\n#class(duplis)\nduplis<-data.table(duplis)\n#we prefer unique() to function duplicated() because a table with the shortened results is output directly.\nunique(duplis, by=c(""unique_id"", ""jaspar_internal"", ""diff_pos""))->duplis_final\n\n##these reduced duplicated results are added to PWM-DNM combinations with only one hit which were selected before (nodupl, line 36)\n#in this dataset of unique PWM-DNM combinations with only one results in dataset from beginning we need to add pos value of binding change\nnodupl$diff_pos=0\nnodupl$diff_pos=abs(nodupl$diff)\nrbind(nodupl, duplis_final)->newversion\n\n\n#In our example dataset this reduced output to 309 TFBS for DNMs (from 408 PWM-DNM hits)\n#From Data stored in ""bothstrands"" double hits for one motif(PWM) - position combination were removed\n#and reduced to result with maximum binding change, we preferred plus over minus strand in the event of an equal binding change. \n\n\nduplicated(newversion)%>%sum()\nunique(newversion, by=c(""unique_id"", ""jaspar_internal"", ""diff_pos""))%>%nrow()\n\n\nwrite.table(newversion,file = ""allPWMDNM_withoutduplis.tsv"", row.names = FALSE, sep = ""\\t"", col.names = TRUE, quote = FALSE)\n\n']",0,"Code, analysis, MinION, Reference Consortium, Phase 2 data release, R9.0 chemistry, F1000 Research."
"Code used in analysis titled ""MinION Analysis and Reference Consortium: Phase 2 data release and analysis of R9.0 chemistry""","Code used in the analysis of the publication titled ""MinION Analysis and Reference Consortium: Phase 2 data release and analysis of R9.0 chemistry"" (F1000 Research, 2017).","['usage_msg <- ""\nUsage: Rscript Figure_performancemetrics.R indatafile stylefile outdir outprefix\n""\n\nargs <- commandArgs(trailingOnly = TRUE)\nif (length(args) != 4) {\n    errormsg <- sprintf(\'Error: Invalid number of arguments %d\', length(args))\n    print(errormsg)\n    print(usage_msg)\n    quit(save=""no"", status=1)\n}\ninputfile <- args[1]\nstylefile <- args[2]\noutdir <- args[3]\noutprefix <- args[4]\n\nimagefile <- sprintf(""%s/%s.png"", outdir, outprefix)\n\nlibrary(RColorBrewer)\nlibrary(reshape2)\nlibrary(methods)\nlibrary(ggplot2)\nlibrary(grid)\n\nsource(stylefile)\nplot_width <- 210\nplot_height <- 210\nplot_units <- ""mm""\nplot_resolution <- 200\nline_width <- 0.6\nlegend_key_width <- 1.0\nlegend_key_height <- 1.0\nlegend_width <- 500\nlegend_width_units <- ""mm""\nvert_24h_line_width <- 0.25\nvert_24h_line_type <- ""dashed""\nvert_24h_line_colour <- medgrey\ntext_size <- 13\nfont_family <- ""Helvetica""\ngrid_major_colour <- ""lightgrey""\ngrid_major_size <- 0.30\nplot_margin_bottom <- 0\nplot_title_font <- 10\nstd_point_size <- 4\nsubplotlabel_offset <- -0.06\nlinecolour <- ""black""\n\nstyle <- theme_bw(base_size=text_size, base_family=font_family)\nstyle <- style + theme(panel.grid.major = element_line(\n    colour=grid_major_colour,size=grid_major_size))\nstyle <- style + theme(axis.ticks.x = element_blank())\nstyle <- style + theme(axis.ticks.y = element_blank())\nstyle <- style + theme(plot.margin=unit(c(0.0,0.0,0,0), ""cm""))\n\nConstruct_Figure <- function()\n{\n    data <- read.table(inputfile, header=TRUE, sep=\'\\t\')\n    D <- data[data$valuetype == ""Mean"",]\n    D$Experiment <- factor(D$Experiment, levels=experimentorder)\n    D$metric <- factor(D$metric, levels = \n        c(""Length"", ""Q-score"", ""BQ"", ""GC"", ""GC (1D)"", ""Speed (1D)"", ""Count""))\n    pngpath <- imagefile\n\n    p <- ggplot(D, aes(x=time, y=value, colour=Experiment))\n    p <- p + geom_line(data=D, size=line_width)\n    p <- p + scale_colour_manual(values=exptpalette)\n    p <- p + labs(x=""Time (h)"", y="""")\n    p <- p + style\n    p <- p + theme(legend.position=""bottom"") +\n        guides(colour = guide_legend(override.aes = list(size=3)))\n    p <- p + theme(plot.margin=unit(c(0,0,0,0), ""cm""))\n    p <- p + theme(plot.title = element_text(\n        hjust = subplotlabel_offset, size=plot_title_font)) # -0.071\n    p <- p + theme(axis.text.x = element_text(angle=90, hjust = 1))\n    p <- p + facet_grid(metric ~ Experiment, scales = ""free"")\n    if (max(D$time) > 24) {\n        p <- p + geom_vline(aes(xintercept=24),\n            linetype=vert_24h_line_type, size=vert_24h_line_width,\n            colour=vert_24h_line_colour)\n    }\n    ggsave(pngpath, width=plot_width, height=plot_height, units=plot_units)\n    return(p)\n}\n\nConstruct_Figure()\n', 'usage_msg <- ""\nUsage: Rscript Figure_readlengths.R indatafile stylefile readlenmax outdir outprefix\n""\n\nargs <- commandArgs(trailingOnly = TRUE)\nif (length(args) != 5) {\n    errormsg <- sprintf(\'Error: Invalid number of arguments %d\', length(args))\n    print(errormsg)\n    print(usage_msg)\n    quit(save=""no"", status=1)\n}\ninputfile <- args[1]\nstylefile <- args[2]\nreadlenmax <- args[3]\noutdir <- args[4]\noutprefix <- args[5]\n\nimagefile_counts <- sprintf(""%s/%s.png"", outdir, outprefix)\n\nlibrary(RColorBrewer)\nlibrary(reshape2)\nlibrary(methods)\nlibrary(ggplot2)\nlibrary(grid)\n\nsource(stylefile)\nplot_width <- 210\nplot_height <- 180\nplot_units <- ""mm""\nplot_resolution <- 200\nline_width <- 0.6\nlegend_key_width <- 1.0\nlegend_key_height <- 1.0\nlegend_width <- 500\nlegend_width_units <- ""mm""\nvert_24h_line_width <- 0.25\ntext_size <- 13\nfont_family <- ""Helvetica""\ngrid_major_colour <- ""lightgrey""\ngrid_major_size <- 0.30\nplot_margin_bottom <- 0\nplot_title_font <- 10\nstd_point_size <- 4\nsubplotlabel_offset <- -0.06\nlinecolour <- ""black""\n\nstyle <- theme_bw(base_size=text_size, base_family=font_family)\nstyle <- style + theme(panel.grid.major = element_line(colour=grid_major_colour,size=grid_major_size))\nstyle <- style + theme(axis.ticks.x = element_blank())\nstyle <- style + theme(axis.ticks.y = element_blank())\nstyle <- style + theme(plot.margin=unit(c(0.0,0.0,0,0), ""cm""))\n\nConstruct_Figure <- function()\n{\n    data <- read.table(inputfile, header=TRUE, sep=\'\\t\')\n    D <- data\n    D$Experiment <- factor(D$Experiment, levels=experimentorder)\n    D$readtype <- factor(D$readtype, levels=c(""Template"", ""2D""))\n\n  # Count histogram thing - only reads <= readlenmax bases\n    pngpath <- imagefile_counts\n    p <- ggplot(data=D[data$value <= readlenmax,], aes(x=value/1000, fill=Experiment, colour=Experiment))\n    p <- p + geom_histogram(stat=""bin"", binwidth=1000/1000)\n    p <- p + scale_colour_manual(values=exptpalette)\n    p <- p + scale_fill_manual(values=exptpalette)\n    p <- p + labs(x=""Read length (K)"", y=""Frequency"")\n    p <- p + style\n    p <- p + theme(legend.position=""bottom"")\n    p <- p + guides(colour = guide_legend(override.aes = list(size=1.5)))\n    p <- p + theme(plot.margin=unit(c(0,0,0,0), ""cm""))\n    p <- p + theme(plot.title = element_text(hjust = subplotlabel_offset, size=plot_title_font)) # -0.071\n    p <- p + theme(axis.text.x = element_text(angle=0, hjust = 1))\n    p <- p + scale_y_continuous(breaks = c(2000,4000,6000,8000,10000,12000))\n    p <- p + facet_grid(readtype ~ Experiment)\n    ggsave(pngpath, width=plot_width, height=plot_height, units=plot_units)\n}\n\nConstruct_Figure()\n', 'alpha <- 0.9\nexptpalette <- c(\n    rgb(0.91,0.16,0.54, alpha), # P1b-Lab2-R2-2D : darkpink    #e8288a (232, 40,138)\n    rgb(0.00,0.45,0.70, alpha), #  P2-Lab6-R1-2D : darkblue    #0072b2 (  1,114,178)\n    rgb(0.84,0.37,0.00, alpha), #  P2-Lab7-R1-2D : lightblue   #d55e00 (213, 94,  0)\n    rgb(0.34,0.71,0.87, alpha), #  P2-Lab6-R1-1D : lightblue   #56b4df ( 86,180,223)\n    rgb(0.90,0.62,0.00, alpha)  #  P2-Lab7-R1-1D : lightorange #e69f00 (230,159,  0)\n)\nexperimentorder <- c(\n    ""P1b-Lab2-R2-2D"",\n    ""P2-Lab6-R1-2D"",\n    ""P2-Lab7-R1-2D"",\n    ""P2-Lab6-R1-1D"",\n    ""P2-Lab7-R1-1D""\n)\ndarkgrey <- rgb(0.10,0.10,0.10, alpha)\nmedgrey <- rgb(0.26,0.26,0.26, alpha)\nlibrarytypeorder <- c(""1D"", ""2D"")\nvert_24h_line_type <- ""dashed""\nvert_24h_line_colour <- medgrey\n']",0,"1. Ground reaction forces 
2. Monitor lizards 
3. Varanidae 
4. Scaling 
5. Locomotion 
6. Tetrapods 
7. Geometric scaling 
8. Legged 
9. Ter"
Raw data accompanying: Ground reaction forces in monitor lizards (Varanidae) and the scaling of locomotion in sprawling tetrapods,"Geometric scaling predicts a major challenge for legged, terrestrial locomotion. Locomotor support requirements scale identically with body mass ( M1), while force generation capacity should scale  M2/3 as it depends on muscle cross-sectional area. Mammals compensate with more upright limb postures at larger sizes, but it remains unknown how sprawling tetrapods deal with this challenge. Varanid lizards are an ideal group to address this question because they cover an enormous body size range while maintaining a similar bent-limb posture and body proportions. This study reports the scaling of ground reaction forces and duty factor for varanid lizards ranging from 7 g 37 kg. Impulses (force x time) scaled roughly as predicted by the inverted pendulum model ( M0.99-1.34) while peak forces ( M0.73-1.00) scaled higher than expected. Duty factor scaled  M0.04 and was higher for the hindlimb than the forelimb. The proportion of vertical impulse to total impulse increased with body size, and impulses decreased while peak forces increased with speed. These results provide valuable data into how locomotor forces vary with body size and suggest how other, extinct, sprawling tetrapods may have dealt with the biomechanical challenges associated with generating sufficient locomotor forces at larger body sizes.","['## This code was used to analyze the data published in ""Ground reaction forces in monitor lizards (Varanidae) and the scaling of\r\n## locomotion in sprawling tetrapods,"" submitted to Biology Letters by Robert L. Cieri et al.\r\n\r\n## Set up####\r\n\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\nlibrary(openxlsx)\r\nlibrary(viridis)\r\nlibrary(lme4)\r\nlibrary(lmerTest)\r\nlibrary(ggeffects)\r\nlibrary(scales)\r\nlibrary(MuMIn)\r\nlibrary(ggiraph)\r\nlibrary(jtools)\r\nlibrary(interactions)\r\nlibrary(plotly)\r\nlibrary(reshape2)\r\nlibrary(ggtern)\r\n\r\n#### Import Data ####\r\n\r\nallDataFull <- read.xlsx(\'rawDataSupplemental.xlsx\')\r\n\r\n\r\n##### Data Processing ####\r\n\r\ncolnames(allDataFull)\r\n\r\n# make body-scaled impulses\r\nallDataFull <- allDataFull %>%\r\n  mutate(logGRFintZ = log10(GRFintZ)) %>%\r\n  ## for x positive forces\r\n  mutate(logGRF_PosX = log10(GRFintPosX)) %>%\r\n  ## for x negative forces\r\n  mutate(logGRF_NegX = log10(GRFintNegX)) %>%\r\n  ## for X net forces\r\n  mutate(logGRFx = log10(GRFintX)) %>%\r\n  ## for y positive forces\r\n  mutate(logGRF_PosY = log10(GRFintPosY)) %>%\r\n  ## for y negative forcess\r\n  mutate(logGRF_NegY = log10(GRFintNegY)) %>%\r\n  ## for y net forces\r\n  mutate(logGRFy = log10(GRFintY)) %>%\r\n  # add logmass\r\n  mutate(logMass = log10(kg)) %>%\r\n  # add logSpeed\r\n  mutate(logSpeed = log10(mPerSec))%>%\r\n  # make swing and stance time\r\n  mutate(stanceTime = length/fFPS) %>% \r\n  mutate(logSwing = log10(swingLong)) %>%\r\n  mutate(logStance = log10(supFrame)) %>%\r\n  # make  peak forces\r\n  mutate(logMaxX = log10((GRFmaxX))) %>%\r\n  mutate(logMaxY = log10((GRFmaxY))) %>%\r\n  mutate(logMaxZ = log10((GRFmaxZ))) %>%\r\n  # scale GRFint\r\n  mutate(logGRFall = log10(GRFintAll)) %>%\r\n  # Pos and Negs for the Max Xs and Ys\r\n  mutate(logMaxXpos = log10((GRFmaxXpos))) %>%\r\n  mutate(logMaxXneg = log10(abs(GRFmaxXneg))) %>%\r\n  mutate(logMaxYpos = log10((GRFmaxYpos))) %>%\r\n  mutate(logMaxYneg = log10(abs(GRFmaxYneg)))%>%\r\n  mutate(logMagPeak = log10(GRFmagPeak))%>%\r\n  mutate(logMagInt = log10(GRFmagInt))%>%\r\n  # make percentages\r\n  mutate(ZpercGRF = GRFabsZint / GRFintAll * 100)%>%\r\n  # location of peaks\r\n  mutate(PeakZ = maxIndexZ / length) %>%\r\n  mutate(PeakX = maxIndexX / length) %>%\r\n  mutate(PeakY = maxIndexY / length) %>%\r\n  # make Z peak scaled to BM\r\n  mutate(ZMaxBM = GRFmaxZ / kg) %>%\r\n  mutate(relSpeed = mPerSec/speciesSpeed)%>%\r\n  # make time-averaged vertical forces\r\n  mutate(averageZforce = GRFintZ / stanceTime) %>%\r\n  # check timing\r\n  mutate(checkTiming = strideTime/stanceTime) %>%\r\n  # duty factor logg\r\n  mutate(logDuty = log10(dutyFactor))%>%\r\n  # make dynamic speed\r\n  mutate(dySpeed = mPerSec^2/(9.8*SVL))%>%\r\n  mutate(logDySpeed = log10(dySpeed)) %>%\r\n  mutate(logRelSpeed = log10(relSpeed)) %>%\r\n  mutate(XpercGRF = GRFabsXint / GRFintAll * 100) %>%\r\n  mutate(YpercGRF = GRFabsYint / GRFintAll * 100) %>%\r\n  mutate(ZpercGRF = GRFabsZint / GRFintAll * 100) \r\n\r\n\r\n#### Basic Parameter Values for Results Section ####\r\n\r\n## higest impulses\r\nmean(allDataFull$GRFintZ/allDataFull$kg)\r\nmin(allDataFull$GRFintZ/allDataFull$kg)\r\nmax(allDataFull$GRFintZ/allDataFull$kg)\r\nsd(allDataFull$GRFintZ/allDataFull$kg)/sqrt(length(allDataFull$GRFintZ/allDataFull$kg))\r\n\r\n## highest peak forces\r\nmean(allDataFull$GRFmaxZ/allDataFull$kg)\r\nmin(allDataFull$GRFmaxZ/allDataFull$kg)\r\nmax(allDataFull$GRFmaxZ/allDataFull$kg)\r\nsd(allDataFull$GRFmaxZ/allDataFull$kg)/sqrt(length(allDataFull$GRFmaxZ/allDataFull$kg))\r\n\r\n#### Export Species means for PIC ####\r\n\r\ncolnames(allDataFull)\r\nallDataFull\r\nspeciesGroup <- group_by(allDataFull, species)\r\n#view(speciesGroup)\r\n\r\n## get basic info on species etc ####\r\n\r\nanimalMeans_A <-  filter(allDataFull, foot == ""hind"") %>%\r\n  group_by(species,animal, foot) %>%\r\n  summarise_at(vars(logMass, logSpeed, strideTime, logGRFintZ, logGRF_PosX,\r\n                    logGRF_NegX,  logGRF_PosY, logGRF_NegY, logMaxZ, \r\n                    logMagInt, logMagPeak, logMaxXpos, logMaxXneg, logMaxYpos,\r\n                    logMaxYneg, ZpercGRF), mean, na.rm = TRUE)\r\n#view(animalMeans_A)\r\nspeciesMeans2 <- animalMeans_A %>%\r\n  group_by(species)%>%\r\n  summarise_at(vars(logMass, logSpeed, strideTime, logGRFintZ, logGRF_PosX,\r\n                    logGRF_NegX,  logGRF_PosY, logGRF_NegY, logMaxZ, \r\n                    logMagInt, logMagPeak, logMaxXpos, logMaxXneg, logMaxYpos,\r\n                    logMaxYneg, ZpercGRF), mean, na.rm = TRUE)\r\n\r\n#write.xlsx(speciesMeans2, \'speciesMeansExportHind_A.xlsx\')\r\n\r\n### get basic numbers for table 1 ####\r\n\r\n\r\nspeciesMeans <- group_by(allDataFull, animal)\r\nsummarise_at(speciesMeans, vars(mass), mean)\r\n\r\n#brevicauda \r\nbrevi <- unique (allDataFull$mass[which(allDataFull$species==""brev"")])\r\nmean(brevi)\r\nsd(brevi)/sqrt(length(brevi))\r\n\r\ncaudo <- unique(allDataFull$mass[which(allDataFull$species==""caudo"")])\r\nmean(caudo)\r\nsd(caudo)/sqrt(length(caudo))\r\n\r\nkomodo <- unique(allDataFull$mass[which(allDataFull$species==""komodo"")])\r\nmean(komodo)\r\nsd(komodo)/sqrt(length(komodo))\r\n\r\npanoptes <- unique(allDataFull$mass[which(allDataFull$specie']",0,"Ecological network, community assembly, evolutionary processes, temporal scales, geological chronosequence, Hawaiian Islands, ecosystem development, arthropods, plant taxa, DNA metabarcoding, bipartite networks, interaction evenness, linkage density, vulnerability"
Ecological network structure in response to community assembly processes over evolutionary time,"The dynamical structure of ecological communities results from interactions among taxa that change with shifts in species composition in space and time. However, our ability to study the interplay of ecological and evolutionary processes on community assembly remains relatively unexplored due to the difficulty of measuring community structure over long temporal scales. Here, we made use of a geological chronosequence across the Hawaiian Islands, representing 50 years to 4.15 million years of ecosystem development, to sample 11 communities of arthropods and their associated plant taxa using semi-quantitative DNA metabarcoding. We then examined how ecological communities changed with community age by calculating quantitative network statistics for bipartite networks of arthropod-plant associations. The average number of interactions per species (linkage density), ratio of plant to arthropod species (vulnerability), and uniformity of energy flow (interaction evenness) increased significantly in concert with community age. The index of specialization H2' has a curvilinear relationship with community age. Our analyses suggest that younger communities are characterized by fewer but stronger interactions, while biotic associations become more even and diverse as communities mature. These shifts in structure became especially prominent on East Maui (~0.5 my) and older volcanos, after enough time had elapsed for adaptation and specialization to act on populations in situ. Such natural progression of specialization during community assembly is likely impeded by the rapid infiltration of non-native species, with special risk to younger or more recently disturbed communities that are composed of fewer specialized relationships.","[""library(sp)\nlibrary(raster)\n\n##  function to generate a random sample of size `n' of pixels\n##  for potential use as actual plots\n##    x: site raster with values outsite the site polygon masked\n##    n: number of candidate plots to generate\n##    d: minimum distance between plots\n##    rng: range of acceptable canopy heights\n\nrand.pxl <- function(x,n,d,rng) {\n\tout.note <- 'all good'\n\t# browser()\n\tvals <- values(x)\n\tgood.cells <- which(vals >= min(rng,na.rm=TRUE) & vals <= max(rng,na.rm=TRUE) & !is.na(vals))\n\t\n\tif(length(good.cells) < n) {\n\t\tn <- length(good.cels)\n\t\tout.note <- 'too few cells in canopy height range'\n\t}\n\t\n\tgood.pnts <- xyFromCell(x, good.cells, spatial=TRUE)\n\tgood.pnts.dist <- spDists(good.pnts)\n\t\n\t##  pick points >= d distance appart\n\tpos.pnts <- replicate(100,brut.seq.samp(good.pnts.dist,n,d))\n\t\n\t##  see if we got any sets w/ >= n points\n\t# browser()\n\tnpnts <- sapply(pos.pnts,length)\n\tif(any(npnts >= n)) {\n\t\tout.pnts <- sample(sample(pos.pnts[npnts >= n], 1)[[1]], n)\n\t} else {\n\t\tout.pnts <- pos.pnts[[which.max(npnts)]]\n\t\tif(out.note != 'all good') {\n\t\t\tout.note <- paste(out.note, sprintf('could not find enough pixels >= %s m appart', d), sep='; ')\n\t\t} else {\n\t\t\tout.note <- sprintf('could not find enough pixels >= %s m appart', d)\n\t\t}\n\t\t\n\t}\n\t\n\tout <- good.pnts[out.pnts,]\n\tattr(out,'note') <- out.note\n\treturn(out)\n}\n\n##  functions to generate random samples >= d distance appart\n\n##  best option so far--generates truely random sample\n##  then purges those points that are too close then tries\n##  to add back as many points as possible\nbrut.seq.samp <- function(pdist,n,d) {\n\t# browser()\n\ttry1 <- brut.samp(pdist, n*3, d) # was: n*ceiling(nrow(pdist)/n/2)\n\t\n\tif(length(try1) < n) {\n\t\tpos <- which(apply(pdist[try1,] >= d, 2, all))\n\t\ttry2 <- c(try1,sample(pos,size=ifelse(length(pos) > n-length(try1), n-length(try1), length(pos))))\n\t\t\n\t\treturn(purge2close(pdist < d, try2))\n\t} else {\n\t\treturn(try1)\n\t}\n}\n\n##  just tries to get lucky and find n points >=d appart by chance\nbrut.samp <- function(pdist,n,d) {\n\tpnts <- sample(nrow(pdist),n)\n\t\n\tpurge2close(pdist < d, pnts)\n}\n\n##  gets rid of points in too close of proximity\npurge2close <- function(plogic, pnts) {\n\t# browser()\n\tthis.logic <- plogic[pnts,pnts]\n\t\n\tif(any(this.logic[lower.tri(this.logic)])) {\n\t\tpnts <- pnts[-which.max(rowSums(this.logic))]\n\t\treturn(purge2close(plogic,pnts))\n\t} else {\n\t\treturn(pnts)\n\t}\n}\n\n##  seq.samp is still buggy\nseq.samp <- function(pdist,n,d) {\n\tpnts <- sample(nrow(pdist),1)\n\tok2add <- pdist[pnts,] >= d\n\tfor(i in 1:(n-1)) {\n\t\tif(any(ok2add)) {\n\t\t\tnew.pnt <- sample(which(ok2add),1)\n\t\t\tpnts <- c(pnts,new.pnt)\n\t\t\tok2add <- pdist[new.pnt,] >= d & ok2add\n\t\t\tprint(min(pdist[pnts,pnts][lower.tri(pdist[pnts,pnts])]))\n\t\t} else {\n\t\t\tbreak\n\t\t}\n\t}\n\t\n\treturn(pnts)\n}\n"", ""library(sp)\r\nlibrary(rgdal)\r\nlibrary(raster)\r\nlibrary(maptools)\r\n\r\n##  function to determine acceptable rng of canopy\r\n##  heights based on criteria of:\r\n##    x: the CAO data\r\n##    site: the site polygon\r\n##    q: quantile range we're aiming for (e.g. [0.9,1])\r\n##    r: evaluate that quantile range within `r' distance from site\r\n##    flow: consider all pixels within the flow that contains the site\r\n##          (if flow is NULL, other constraints on pixles are used (e.g. site polygon))\r\n##    all.val: determines if all values are returned or just the quantile specified by `q'\r\n##    return.r: logical, should masked raster be returned\r\n##\r\n##  i'm keeping this function sepparate from the function that selects\r\n##  candidate pixels so that we can evaluate how `r' changes our\r\n##  acceptable range\r\n\r\nfind.rng <- function(x,site,q,r,flow=NULL,all.vals=FALSE,return.r=FALSE) {\r\n\t## determine which polygon to use as a mask for the CAO data\r\n\t# browser()\n\tif(!is.null(flow)) { # use the lava flow as mask\r\n\t\tpoly2mask <- flow\r\n\t\tcat('using flow for mask \\n')\r\n\t} else if(max(apply(bbox(site),1,diff)) < 2*r) { # use a radius as mask\r\n\t\tcPoly <- coordinates(site)[1,]\r\n\t\trPoly <- cbind(cos(seq(0,2,0.02)*pi)*r+cPoly[1], sin(seq(0,2,0.02)*pi)*r+cPoly[2])\r\n\t\tpoly2mask <- SpatialPolygons(\r\n\t\t\tlist(Polygons(list(rPoly=Polygon(rPoly)),ID=0)),\r\n\t\t\tproj4string=CRS(proj4string(site))\r\n\t\t)\r\n\t\tcat('using radius for mask \\n')\r\n\t} else { # use the site polygon as mask\r\n\t\tpoly2mask <- site\r\n\t\tcat('using site for mask \\n')\r\n\t}\r\n\t# browser()\r\n\t## masking is a time-intensive step! cropping speeds it up some\r\n\tx <- crop(x,extent(bbox(poly2mask)))\r\n\t## do the actual masking\r\n\tx <- mask(x,poly2mask)\r\n\t\r\n\t## if raster already masked to site polygon no need for\r\n\t## further masking below\r\n\tif(max(apply(bbox(site),1,diff)) > 2*r & is.null(flow)) {\r\n\t\tr.out <- x\r\n\t}\r\n\t\r\n\t# the `!is.na' part is very important because all the raster\r\n\t# subsetting above is actually just setting out-of-bounds pixles\r\n\t# to NA\r\n\tvals <- values(x)\r\n\tvals <- vals[!is.na(vals)]\r\n\t\n\tif(all.vals) {\r\n\t\tout <- vals\r\n\t} else {\r\n\t\t## `rm.outliers' removes potential erronous pixels at\r\n\t\t## extremes of distribuitons before calculating quantile\r\n\t\trng <- quantile(rm.outliers(vals), prob=q)\r\n\t\tnames(rng) <- NULL\r\n\t\t\r\n\t\tout <- rng\r\n\t}\r\n\t\r\n\t## should the maksed raster be returned?\r\n\tif(return.r) {\r\n\t\t## further clip raster to site polygon if haven't already\r\n\t\tif(!exists('r.out')) {\r\n\t\t\tx <- crop(x,extent(bbox(site)))\r\n\t\t\tcat('further masking to site for output \\n')\r\n\t\t\tr.out <- mask(x,site)\r\n\t\t}\r\n\t\tout <- list(rng=rng,r=r.out)\r\n\t}\r\n\t\r\n\treturn(out)\r\n}\r\n\r\n\r\n##  helper function to remove anamolys LiDAR data. Looks for point mass at 0\r\n##  and also trailing right hand tail. Right had tail criteria is to cut\r\n##  canopy height values that are > 0.9 quantile AND whoes frequencies deviate\r\n##  from an exponential decay from the mode.\r\n\r\nrm.outliers <- function(x) {\r\n\t# browser()\r\n\tx <- x[!is.na(x)]\r\n\t\r\n\t## remove potential point mass near 0\n\ttooSmall <- 0.045 * diff(range(x))\r\n\tif(sum(x < tooSmall) > 1.25*(sum(x < 2*tooSmall) - sum(x < tooSmall))) {\r\n\t\tx <- x[x > tooSmall]\r\n\t}\r\n\t\r\n\t## calculate frequencies excluding top 10% quantile\n\tbrks <- seq(0, max(x)*1.1, by=1.5*tooSmall)\n\tbrks <- c(brks, max(brks) + diff(brks[1:2]))\r\n\tx4tailCalc <- x[x < brks[which.min(abs(brks - quantile(x[x > mean(x)], 0.95)))]]\n\txhist <- data.frame(hist(x4tailCalc, brks, \r\n                             plot=FALSE)[c('mids', 'counts')])\r\n\t\r\n\t## define `tail' as region of distribuiton right of mode\r\n\txtail <- xhist[which.max(xhist[,2]) <= 1:nrow(xhist) & xhist[,2] > 0, ]\r\n\txtail[,2] <- log(xtail[,2])\r\n\t\r\n\t## fit exponential to tail\r\n\ttail.mod <- lm(counts~mids, data=xtail)\r\n\t\r\n\t## calculate frequency for all data\r\n\txhist.all <- data.frame(hist(x, brks, \r\n                                 plot=FALSE)[c('mids', 'counts')])\r\n\txhist.all <- xhist.all[xhist.all$counts > 0, ]\r\n\txhist.all[,2] <- log(xhist.all[,2])\r\n\t\r\n\t## calculate which canopy height classes are outliers\r\n\ttail.line <- predict(tail.mod, newdata=xhist.all[, 1, drop=FALSE])\n\tif(length(tail.mod$residuals) > 2) {\n\t\ttail.line <- tail.line + 2*sqrt(sum(tail.mod$residuals^2)/(length(tail.mod$residuals)-2))\n\t}\n\t\r\n\tbad.mid <- xhist.all[xhist.all[,2] > tail.line & xhist.all[,1] > quantile(x, 0.9), 1]\n\t\r\n\t## remove outliers from `x'\n\tif(length(bad.mid) > 0) {\n\t\tx <- x[x < min(bad.mid)]\n\t}\n\t\r\n\treturn(x)\r\n}\r\n\r\n##  helper function to identify which polygon in large shapefile of flows\r\n##  corresponds to the actual site polygon\r\nfindFlow <- function(site,flows) {\r\n\tID <- over(SpatialPoints(coordinates(site),proj4string=CRS(proj4string(site))),flows)\r\n\t\r\n\treturn(flows[flows$FID_geol_f == ID$FID_geol_f, ])\r\n}\r\n"", ""##  function to generate randomized plots within pre-selected site\n##  polygons. This function creates `n' plots that are minimum `d'\n##  distance appart. Each plot is placed such that it falls within a\n##  specified rang of canopy height values. That range is specified as\n##  a quantile `q' [e.g. c(0.9,1) for the upper 10% quantile] of\n##  canopy heights recorded in the CAO LiDAR data `cao.' The LiDAR\n##  data are clipped to the shape of the lava flow we're interested\n##  in, the lava flow polygon being given by `flow.poly.' Writes the\n##  plot points to a file if argument `file' is provided\n\ngenerate.plots <- function(n, d, q, r, site.poly, cao, flow.poly, file) {\n    if(missing(r)) r <- 0\n    \n    if(is.null(flow.poly)) {\n    \t\tflow2use <- NULL\n    } else {\n    \t\tflow2use <- findFlow(site.poly,flow.poly)\n    }\n    # browser()\n    rng <- find.rng(x=cao, site=site.poly, q=q, r=r,\n                    flow=flow2use, return.r=TRUE)\n    plts <- rand.pxl(x=rng$r, n=n, d=d, rng=rng$rng)\n    \n    cat('\\n', attr(plts,'note'), ';\\n', sprintf('found %s plots',\n                                                length(plts)), '\\n', sep='')\n    \n    vals <- values(rng$r)\n    vals <- vals[!is.na(vals)]\n    \n    if(!missing('file')) {\n        plts <- SpatialPointsDataFrame(plts,\n                                       data=data.frame(type=rep('plot_centroid',\n                                                           length(plts))),\n                                       proj4string=CRS(proj4string(plts)))\n        \n        ## write spatial points to ESRI shapefile\n        writeOGR(plts, file, gsub('.*/','',file), driver='ESRI Shapefile', overwrite_layer=TRUE)\n        \n        ## write raster values to text file\n        write.table(vals, file=paste(file, '/', gsub('.*/','',file), '.txt', sep=''),\n                    sep='\\t', row.names=FALSE, col.names=FALSE)\n                    \n        ## write range of values considered\n        write.table(rng$rng, file=paste(file, '/', gsub('.*/','',file), '_rng.txt', sep=''),\n                    sep='\\t', row.names=FALSE, col.names=FALSE)\n    }\n    \n    out <- list(rng=rng$rng, vals=vals, plts=plts)\n    return(out)\n}\n\n\n##  test it on simulated data\n# setwd('~/Dropbox/hawaiidimensions/SiteSelection')\n# source('find_rng.R')\n# source('find_pixels.R')\n\n# # load simulated CAO data\n# laup.r <- raster('cao/laup_sim.tif')\n\n# # load one of Curtis's site polygons\n# laup.p <- readShapeSpatial('PreliminarySitePolygons/LaupahoehoeHipNet_4-14K.shp',\n                           # proj4string=CRS('+proj=utm +zone=5 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs'))\n\n# # load flow polygons clipped by elevation\n# elevGeo <- readShapeSpatial('elevFlow/geo3to5k.shp',\n                            # proj4string=CRS('+proj=utm +zone=5 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs'))\n\n# ##  specifying the file will make the function write\n# ##  the plots to an ESRI shapefile, you can also omit \n# ##  the file argument and then the function will return\n# ##  the points in memory\n# x <- generate.plots(n=20, d=200, q=c(0.6,0.95), site.poly=laup.p, cao=laup.r,\n                    # flow.poly=elevGeo, file='./laupTest')\n# x <- generate.plots(n=20, d=200, q=c(0.6,0.95), site.poly=laup.p, cao=laup.r,\n                    # flow.poly=NULL, file='./laupTest')\n\n# ##  I've also made the function return the range of\n# ##  canopy heights being used (i.e. across the entire flow)\n# ##  and the range of heights found within the flow\n# x\n"", '### make histograms of site level CAO LiDAR data\r\n## Feb 2014 DSG, revised for new polygons April 2014\r\n## August 2022 getting some site data for Natalie\'s paper\r\n\r\nlibrary(sp)\r\nlibrary(rgdal)\r\nlibrary(raster)\r\nlibrary(maptools)\r\nlibrary(tidyverse)\r\nlibrary(ggplot2)\r\nlibrary(readxl)\r\nlibrary(here)\r\n\r\nsetwd(""G:/My Drive/GrunerGIS/DimensionsHI/plot_selection3/cao"")\r\n\r\nRAST.list <- list(\r\n  ""HAVO_Olaa_Old_CAOMN.tif"",\r\n  ""HAVO_Thurston_2polygons_CAOMN.tif"",\r\n  ""HiloFR_Humuula_2polygons_CAOMN.tif"",\r\n  ""HiloFR_Laupahoehoe_Old_CAOMN.tif"",\r\n  ""NAR_Laupahoehoe_65-250K_CAOMN.tif"",\r\n  ""NAR_Laupahoehoe_HIPPNET_Young_CAOMN.tif"",\r\n  ""NAR_PuuOumi_Old_4polygons_CAOMN.tif"",\r\n  ""NAR_PuuOumi_Young02_CAOMN.tif"",\r\n  ""HAVO_EscapeRd_1973_CAOMN.tif"",\r\n  ""HAVO_EscapeRd_HighStature_CAOMN.tif"",\r\n  ""KauFR_Alili_2polygons_CAOMN.tif"",\r\n  ""NAR_PuuMakaala_Old_2polygons_CAOMN.tif"",\r\n  ""NAR_PuuMakaala_Young_2polygons_CAOMN.tif"",\r\n  ""TNC_Kaiholena_Old_CAOMN.tif"",\r\n  ""TNC_Kaiholena_Young_CAOMN.tif"",\r\n  ""Kauai_8m_CAOMN.tif""\r\n)\r\n\r\n\r\n## output histograms for each raster\r\nfor (i in 1:length(RAST.list)) { \r\n  RAST.i <- raster(RAST.list[[i]])\r\n  File.Name <- paste(names(RAST.i), ""AUG22.jpg"", sep = """")\r\n  jpeg(file = File.Name)\r\n  raster::hist(RAST.i,breaks=20)\r\n  dev.off()\r\n}\r\n\r\n## to obtain values from each cell in each raster\r\nfor (i in 1:length(RAST.list)) { \r\n  RAST.i <- raster(RAST.list[[i]])\r\n  vec.i <- raster::getValues(RAST.i)\r\n  vec.i<-vec.i[!is.na(vec.i)]\r\n  vec.i[complete.cases(vec.i)]  # probably redundant with above\r\n  write.csv(vec.i, file=paste0(names(RAST.i), "".csv""), row.names = FALSE)\r\n}\r\n\r\n# all csv files have leading ""x"" as first row. Don\'t know why\r\n\r\nCSV <- read.csv(""NAR_PuuMakaala_Old_2polygons_CAOMN.csv"")\r\nCSV <- CSV[-1,]\r\nsummary(CSV)\r\nhist(CSV)\r\n\r\n# could write a loop to remove the first column, but also need to extract column names\r\n# easier to paste in excel to create ""SiteValues.xlsx""\r\n# here is the list of files, if proceeding with the for loop approach\r\nCSV.list <- list(\r\n  ""HAVO_Olaa_Old_CAOMN.csv"",\r\n  ""HAVO_Thurston_2polygons_CAOMN.csv"",\r\n  ""HiloFR_Humuula_2polygons_CAOMN.csv"",\r\n  ""HiloFR_Laupahoehoe_Old_CAOMN.csv"",\r\n  ""NAR_Laupahoehoe_65-250K_CAOMN.csv"",\r\n  ""NAR_Laupahoehoe_HIPPNET_Young_CAOMN.csv"",\r\n  ""NAR_PuuOumi_Old_4polygons_CAOMN.csv"",\r\n  ""NAR_PuuOumi_Young02_CAOMN.csv"",\r\n  ""HAVO_EscapeRd_1973_CAOMN.csv"",\r\n  ""HAVO_EscapeRd_HighStature_CAOMN.csv"",\r\n  ""KauFR_Alili_2polygons_CAOMN.csv"",\r\n  ""NAR_PuuMakaala_Old_2polygons_CAOMN.csv"",\r\n  ""NAR_PuuMakaala_Young_2polygons_CAOMN.csv"",\r\n  ""TNC_Kaiholena_Old_CAOMN.csv"",\r\n  ""TNC_Kaiholena_Young_CAOMN.csv"",\r\n  ""Kauai_8m_CAOMN.csv""\r\n)\r\n\r\n\r\n# read in spreadsheet that contains all the CAOMN pixel values by site\r\ndf <- read_xlsx(""SiteValues.xlsx"", sheet = 1, col_names = TRUE, na = """")\r\n\r\n#TMP1 <- data.frame(summary(df))\r\n\r\n# calculate descriptive stats in for loops\r\nMEDIAN <- vector(""double"", ncol(df)) \r\nfor (i in seq_along(df)) {  \r\n  MEDIAN[[i]] <- median(df[[i]],na.rm=TRUE)  \r\n}\r\nMEDIAN\r\n\r\nMEAN <- vector(""double"", ncol(df))\r\nfor (i in seq_along(df)) {\r\n  MEAN[[i]] <- mean(df[[i]],na.rm=TRUE)\r\n}\r\nMEAN\r\n\r\nMIN <- vector(""double"", ncol(df))\r\nfor (i in seq_along(df)) {\r\n  MIN[[i]] <- min(df[[i]],na.rm=TRUE)\r\n}\r\nMIN\r\n\r\nMAX <- vector(""double"", ncol(df))\r\nfor (i in seq_along(df)) {\r\n  MAX[[i]] <- max(df[[i]],na.rm=TRUE)\r\n}\r\nMAX\r\n\r\nSD <- vector(""double"", ncol(df))\r\nfor (i in seq_along(df)) {\r\n  SD[[i]] <- sd(df[[i]],na.rm=TRUE)\r\n}\r\nSD\r\n\r\n#quantile(df[[16]], probs = c(0,0.25,0.40,0.6,0.75,1.0), na.rm = TRUE)\r\nQ40 <- vector(""double"", ncol(df))\r\nfor (i in seq_along(df)) {\r\n  Q40[[i]] <- quantile(df[[i]], probs = 0.6, na.rm = TRUE)\r\n}\r\nQ40\r\n\r\n# get states for kurtosis and skewness in datawizard (see documentation)\r\nrequire(datawizard)\r\nSKEW  <- datawizard::skewness(df,na.rm=TRUE)\r\nKURT  <- datawizard::kurtosis(df,na.rm=TRUE)\r\n\r\n# bind columns for simple stats. Kurtosis and Skewness output separately\r\nCBOUND <- data.frame(\r\n  cbind(MEAN, MEDIAN, MIN, MAX, SD, Q40),\r\n  row.names = c(\r\n    ""HAVO_Olaa_Old"",\r\n    ""HAVO_Thurston"",\r\n    ""HiloFR_Humuula"",\r\n    ""HiloFR_Laupahoehoe_Old"",\r\n    ""NAR_Laupahoehoe_65-250K"",\r\n    ""NAR_Laupahoehoe_HIPPNET"",\r\n    ""NAR_PuuOumi_Old"",\r\n    ""NAR_PuuOumi_Young02"",\r\n    ""HAVO_EscapeRd_1973"",\r\n    ""HAVO_EscapeRd_HighStature"",\r\n    ""KauFR_Alili"",\r\n    ""NAR_PuuMakaala_Old"",\r\n    ""NAR_PuuMakaala_Young"",\r\n    ""TNC_Kaiholena_Old"",\r\n    ""TNC_Kaiholena_Young"",\r\n    ""Kauai_8m""\r\n  )\r\n)\r\n\r\n\r\nwrite_excel_csv(CBOUND, file=""summary.csv"")\r\nwrite_excel_csv(KURT, file=""kurtosis.csv"")\r\nwrite_excel_csv(SKEW, file=""skewness.csv"")\r\n\r\n', '### feed multiple clipped rasters in to match Curtis\' polygons\n### change distance from 200 to 150m because some of these polygons are small\n### mod by DSG Nov 6-11 2013, 6 Dec 2013, 11 Dec 2013, 30 Jan 2014\n### new *.R created using Rominger\'s revisions 4 Feb 2014 to\n### find_pixels.R  &  find_rng.R  &  gnerate_plots.R\n\nrm(list=ls())\n\nsetwd(\'~/Dropbox/plot_selection\')\nsource(\'find_rng.R\')      # function for finding desired canopy height range\nsource(\'find_pixels.R\')   # function for finding random pixels with desired canopy range\nsource(\'generate_plots.R\') # function to make candidate plots\n\n\n## create base directory for appending output directories\nbase <- getwd()\n\n##  Use on Curtis\' polygons with clip of CAO raster (mean height)\n##  List of files in ./SitePolygons_Feb2014\n\n##   RUN FOR EACH SITE\nall.file <- list.files(\'cao\', pattern=\'.tif\')\nall.file <- all.file[!grepl(\'\\\\.xml\', all.file)]\n\nfor(i in all.file) { # first one ran\n\tprint(i)\n\t\n\tbase.name <- gsub(\'_CAOMN.tif\', \'\', i)\n\t\n\tOUT.name <- paste(base.name, \'PTS200\', sep=\'_\')\n\tRAST.name <- paste(\'cao/\', i, sep=\'\')\n\tPOLY.name <- paste(\'SitePolygons_Feb2014/\', base.name, \'.shp\', sep=\'\')\n\t\n\t# create filename for the output\n\tOUT <- paste(base, \'run2014-04-10\', OUT.name, sep=""/"")\n\t\n\t# load CAO raster\n\tRAST <- raster(RAST.name)\n\t\n\t# load one of Curtis\'s site polygons\n\tPOLY <- readShapeSpatial(POLY.name,\n\t    proj4string=CRS(\'+proj=utm +zone=5 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\'))\n\t\n\t# change projection as needed\n\tif(proj4string(POLY) != proj4string(RAST)) {\n\t\tPOLY <- spTransform(POLY, CRS(proj4string(RAST)))\n\t}\n\t\n\tset.seed(103)\n\tx <- generate.plots(n=20, d=200, q=c(0.6,0.95), r=0, site.poly=POLY, cao=RAST,\n    \t                flow.poly=NULL, file=OUT)\n}\n']",0,"Functional connectivity, landscape genomics, wildlife populations, environmental variables, spatial patterns, genomic variation, movement, dispersal, mating behaviors, gene flow, geographic scales, mobile species, mule deer, Odocoileus hemionus,"
"Functional connectivity in a continuously distributed, migratory species as revealed by landscape genomics","Maintaining functional connectivity is critical for the long-term conservation of wildlife populations. Landscape genomics provides an opportunity to assess long-term functional connectivity by relating environmental variables to spatial patterns of genomic variation resulting from generations of movement, dispersal, and mating behaviors. Identifying landscape features associated with gene flow at large geographic scales for highly mobile species is becoming increasingly possible due to more accessible genomic approaches, improved analytical methods, and enhanced computational power. We characterized the genetic structure and diversity of migratory mule deer (Odocoileus hemionus) using 4,051 single nucleotide polymorphisms in 406 individuals sampled across multiple habitats throughout Wyoming, USA. We then identified environmental variables associated with genomic variation within genetic groups and statewide using a stepwise approach to first evaluate nonlinear relationships of landscape resistance with genetic distances and then use mixed-effects modeling to choose top landscape genomic models. We identified three admixed genetic groups of mule deer and found that environmental variables associated with gene flow varied among genetic groups, revealing scale-dependent and regional variation in functional connectivity. At the statewide scale, more gene flow occurred in areas with low elevation and mixed habitat. In the southern genetic group, more gene flow occurred in areas with low elevation. In the northern genetic group, more gene flow occurred in grassland and forest habitats, while highways and energy infrastructure reduced gene flow. In the western genetic group, the null model of isolation by distance best represented genetic patterns. Overall, our findings highlight the role of different seasonal ranges on mule deer genetic connectivity, and show that anthropogenic features hinder connectivity. This study demonstrates the value of combining a large, genome-wide marker set with recent advances in landscape genomics to evaluate functional connectivity in a wide-ranging migratory species.","['#Script to identify loci with excess heterozygotes to remove\n# Script created by Monia Hasselhort and C.A. Buerkle, modified by M.E.F. LaCava\n\n\n#### Identify loci with heterozygosity above a given percentage ####\n#import text file with SNP genotypes\ngenest <- read.table(""genotypes.txt"", header=F)\ntmp.het.count <- apply(genest, 1, function(x) sum(abs(x-1)< 0.05, na.rm=T))\ntmp.af<-apply(genest, 1, mean, na.rm=T)/2\n#plot heterozygote count against allele frequency \n# - peak at center are loci with 0.5 allele frequencing in almost all individuals (unlikely to be biologically real)\nplot(tmp.af,tmp.het.count)\n#plot \ntmp<-apply(genest, 1, function(x) length(which(x==1)))\nplot(tmp.het.count, tmp, xlim=c(0,400), ylim=c(0,400))\n\n#count number of loci with heterozygosity in more than X% of individuals\nlength(which(tmp.het.count > 0.5*398))\n\n#remove improbable loci from dataset\ngenest.subset <- genest[-which(tmp.het.count > 0.5*398),] \n\n#correlation between genest and genest.subset covariance matrices\ng.covar<-getcovarmat(genest)\ngsubset.covar<-getcovarmat(genest.subset)\ncor(g.covar[lower.tri(g.covar)], gsubset.covar[lower.tri(gsubset.covar)])\n\n\n\n#### Probabilistically identify loci with excess heterozygosity ####\n\n#import data:\nmyvcf<-read.table(""genotypes.txt"", stringsAsFactors=F)\n## this looks like this:\n## scaffold_11:53 APH_EE_001 0/0 99,0\n## scaffold_11:53 APH_EE_002 0/0 49,0\n## scaffold_11:53 APH_EE_003 0/0 99,0\n## scaffold_11:53 APH_EE_005 0/0 99,0\n## scaffold_11:53 APH_EE_007 0/0 99,0\n## scaffold_11:53 APH_EE_009 0/0 59,0\n## scaffold_11:53 APH_EE_010 0/0 100,0\n## scaffold_11:53 APH_EE_013 0/0 99,0\n## scaffold_11:53 APH_EE_015 0/0 63,0\n## scaffold_11:53 APH_EE_017 0/0 96,0\n\n\nAD<-matrix(as.numeric(unlist(strsplit(myvcf$V4, "",""))), ncol=2, byrow=T)\nADhetprob<-apply(AD[myvcf$V3 == ""0/1"",],1, function(x) pbinom(x[1], prob=0.5, size=sum(x)))\n\ncor(ADhetprob, rowSums(AD[myvcf$V3==""0/1"",]))\n\nextreme_ADhetprob_by_contig<-by(ADhetprob, myvcf$V1[myvcf$V3 == ""0/1""], function(x)sum(x>0.95 | x<0.05)/length(x))\nmean_ADhetprob_by_contig<-by(ADhetprob, myvcf$V1[myvcf$V3 == ""0/1""], mean)\nN_ADhetprob_by_contig<-by(ADhetprob, myvcf$V1[myvcf$V3 == ""0/1""], length)\n\nplot(mean_ADhetprob_by_contig ~ by(rowSums(AD[myvcf$V3==""0/1"",]), myvcf$V1[myvcf$V3 == ""0/1""], mean))\n\n\n\n#subset genotype matrix according to above calc\nnind <- 398\nnloci<- 4949\ng20<-matrix(scan(""genotypes.txt"",n=nind*nloci,sep="" ""),nrow=nloci,ncol=nind,byrow=T)\ng20.subset<-g20[ mean_ADhetprob_by_contig < 0.9 & mean_ADhetprob_by_contig > 0.1,]\n\npc20<-do.pca(g20.subset)\npc20Summary<-summary(pc20)\n\npar(mfrow=c(1,1), pty=\'s\')\n#plot PC1 x PC2\nplot(pc20$x[,\'PC1\'], pc20$x[,\'PC2\'],col=inds$HRColor ,  pch=19 , cex=0.8,\n     xlab =  paste(""PC1 ("", round(pc20Summary$importance[2,1]*100, 1), ""%)"", sep=""""),\n     ylab =  paste(""PC2 ("", round(pc20Summary$importance[2,2]*100, 1), ""%)"", sep=""""))\n#plot PC2 x PC3\nplot(pc20$x[,\'PC2\'], pc20$x[,\'PC3\'],col=inds$HRColor ,  pch=19 , cex=0.8,\n     xlab =  paste(""PC2 ("", round(pc20Summary$importance[2,2]*100, 1), ""%)"", sep=""""),\n     ylab =  paste(""PC3 ("", round(pc20Summary$importance[2,3]*100, 1), ""%)"", sep=""""))\n\n\n\n#check correlation between original covariance matrix and subset matrix\ngetcovarmat<-function(gmat, write.gcov=FALSE, inds=""""){\n  gmn<-apply(gmat,1,mean, na.rm=T)\n  gmnmat<-matrix(gmn,nrow=nrow(gmat),ncol=ncol(gmat))\n  gprime<-gmat-gmnmat ## remove mean\n  \n  gcovarmat<-matrix(NA,nrow=ncol(gmat),ncol=ncol(gmat))\n  for(i in 1:ncol(gmat)){\n    for(j in i:ncol(gmat)){\n      if (i==j){\n        gcovarmat[i,j]<-cov(gprime[,i],gprime[,j], use=""pairwise.complete.obs"")\n      }\n      else{\n        gcovarmat[i,j]<-cov(gprime[,i],gprime[,j], use=""pairwise.complete.obs"")\n        gcovarmat[j,i]<-gcovarmat[i,j]\n      }\n    }\n  }\n  gcovarmat\t\n}\n\ng20.covar<-getcovarmat(g20)\ng20subset.covar<-getcovarmat(g20.subset)\ncor( g20.covar[lower.tri(g20.covar)], g20subset.covar[lower.tri(g20subset.covar)])\n\n', '#Landscape genetics analysis for mule deer\n#Created by M.E.F. LaCava\n#Code for functional transformations and MLPE modeling adapted from K.A. Zeller et al. 2017\n\n#NOTE: this code is for statewide analysis, but it was adapted for each of the three genetic groups as needed\n\n\n###1. Calculate geographic and genetic distance between samples ####\n\n#Geographic distance matrix\ninstall.packages(""fossil"")\nlibrary(fossil)\ncoords<-read.csv(""AEAcoords.csv"", header=F)\n#convert to lat long for earthdist calculation\ncoordinates(coords) <- ~ V2 + V3\nproj4string(coords) <- ""+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=WGS84 +towgs84=0,0,0,-0,-0,-0,0 +units=m +no_defs""\ncoords <- spTransform(coords,CRS(""+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0""))\ncoords <- as.data.frame(coords)\nlat<-coords[,2:3]\ngeo.dist<-earth.dist(lat, dist=F)\n#remove top half of matrix (including diagonal)\ngeo.dist[upper.tri(geo.dist, diag=TRUE)] <- NA\n#export\nsave(geo.dist,file=""/Users/melanielacava/Data/MD/LGanalysis/geo.dist.rData"")\nload(""/Users/melanielacava/Data/MD/LGanalysis/geo.dist.rData"")\n\n\n#Genetic distance\n# import structure file (convert VCF to structure using PGDSpider)\ninstall.packages(""PopGenReport"")\nlibrary(PopGenReport)\n#calc dist - takes a while (~30-60 min)\nmd370 <- read.structure(""md370.stru"", n.ind=370, n.loc=4069, onerowperind=F, col.lab=1, col.pop=2, ask=F)\nsmouse.dist.370 <- gd.smouse(md370, verbose=TRUE)\nwrite.csv(as.matrix(smouse.dist.370), ""SPgendistance_md370.csv"")\n#OR import genetic distance matrix after making\ngen.dist <- as.matrix(read.table(""SPgendistance_md370.csv"", sep="","", header=T)[,-1])\nrownames(gen.dist) <- colnames(gen.dist)\ngen.dist[1:10,1:10] #check\n#remove top half of matrix (including diagonal)\ngen.dist[upper.tri(gen.dist, diag=TRUE)] <- NA\nsave(gen.dist,file=""/Users/melanielacava/Data/MD/LGanalysis/gen.dist.rData"")\nload(""/Users/melanielacava/Data/MD/LGanalysis/gen.dist.rData"")\n\n\n\n###2. Generate base raster for each variable ####\n\nrequire(raster)\nrequire(rgdal)\nrequire(rgeos)\n\n#NLCD land cover\n# MRLC - https://www.mrlc.gov/data\nWD <- ""/Users/melanielacava/Desktop/WorkingFolder""\nsetwd(WD)\nnlcd <- raster(paste(getwd(),""LandscapeFeatures"",""NLCD_2016_Land_Cover_L48_20190424.img"",sep=""/""))\nplot(nlcd)\nproj4string(nlcd) #""+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=WGS84 +towgs84=0,0,0,-0,-0,-0,0 +units=m +no_defs""\nnlcd_attr <- nlcd@data@attributes[[1]] #save attribute info\nnlcd_attr <- nlcd_attr[nlcd_attr$COUNT!=0,]\nlc_types <- unique(nlcd_attr$NLCD.Land.Cover.Class)\n#crop to 50km buffer around WY\nstate <- readOGR(dsn=paste(getwd(),""/LandscapeFeatures"",sep=""""), layer=""state"")\nproj4string(state) <- ""+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0""\ntemp <- spTransform(state,CRS(""+proj=utm +zone=12 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0""))\nbuf <- gBuffer(temp,width=50000) #buffer state by 50km\nplot(buf,col=""gray"")\nplot(temp,add=T)\nbuf <- spTransform(buf,CRS(""+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=WGS84 +towgs84=0,0,0,-0,-0,-0,0 +units=m +no_defs""))\n#crop to buffered area\nnlcd <- crop(nlcd,buf)\nplot(nlcd)\nwriteRaster(nlcd,""/Users/melanielacava/Data/MD/LGanalysis/Source/nlcd_wy_buf50k"",format=""GTiff"",overwrite=T)\n#after running above once, can just read in WY NLCD layer\nnlcd <- raster(""/Users/melanielacava/Data/MD/LGanalysis/Source/nlcd_wy_buf50k.tif"")\nfreq(nlcd) #gives you cells of each LC type (takes 10 min to run)\n> freq(nlcd)\nvalue     count\n[1,]    11   3421879\n[2,]    12     57619\n[3,]    21   3363141\n[4,]    22   1176536\n[5,]    23    389553\n[6,]    24     84225\n[7,]    31   4986113\n[8,]    41   9761637\n[9,]    42  69598416\n[10,]    43    869109\n[11,]    52 228144404\n[12,]    71 131336970\n[13,]    81   5376205\n[14,]    82  19718009\n[15,]    90   3607178\n[16,]    95   5671118\ntotal cells = 487562112\n% cover in state:\n#forest = 0.1720\n#sage = 0.4679\n#grass = 0.2920\n#ag = 0.040\n#dev = 0.010\n\n\n#simplify into binary layers\n#make matrix with lc values\n# 11 12 21 22 23 24 31 41 42 43 52 71 81 82 90 95\nlc_wy <- sort(unique(getValues(nlcd))) #WY has all types of land cover\nchange <- matrix(c(lc_wy,rep(NA,length(lc_wy))),ncol=2, nrow=length(lc_wy),byrow=F,dimnames=NULL)\n\n#forest/non-forest\n#41, 42, 43, 90 (deciduous forest, evergreen forest, mixed forest, woody wetland)\nchange[,2] <- c(0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,0)\nchange\nforest <- reclassify(nlcd,change)\n#plot(forest)\nnames(forest) <- ""forest""\n\n#sagebrush scrub\n#52 (shrub/scrub)\nchange[,2] <- c(0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0)\nchange\nsage <- reclassify(nlcd,change)\n#plot(sage)\nnames(sage) <- ""sage""\n\n\n#grassland\n#71, 81, 95 (grassland/herbaceous, pasture/hay, emergent herbaceous wetlands)\nchange[,2] <- c(0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1)\nchange\ngrass <- reclassify(nlcd,change)\n#plot(grass)\nnames(grass) <- ""grass""\n\n\n\n\n#Elevation (90m resolution)\n#import state shape\nWD <- ""/Users/melanielacava/Desktop/WorkingFolder""\nsetwd(', '#PCA on SNPs\n# PCA function created by C.A. Buerkle, rest of script created by M.E.F. LaCava\n\n\n#PCA function\ndo.pca<-function(gmat, write.gcov=FALSE, inds=""""){\n  gmn<-apply(gmat,1,mean, na.rm=T) #takes a mean across individuals for each locus\n  gmnmat<-matrix(gmn,nrow=nrow(gmat),ncol=ncol(gmat)) #creates matrix with mean filled in for entire row\n  gprime<-gmat-gmnmat ## remove mean \n  \n  gcovarmat<-matrix(NA,nrow=ncol(gmat),ncol=ncol(gmat))\n  for(i in 1:ncol(gmat)){\n    for(j in i:ncol(gmat)){\n      if (i==j){\n          gcovarmat[i,j]<-cov(gprime[,i],gprime[,j], use=""pairwise.complete.obs"") #only use loci that both samples in pair have a genotype for\n      }\n      else{\n        gcovarmat[i,j]<-cov(gprime[,i],gprime[,j], use=""pairwise.complete.obs"") #only use loci that both samples in pair have a genotype for\n        gcovarmat[j,i]<-gcovarmat[i,j]\n      }\n    }\n  }\n  if(write.gcov==TRUE){\n    inds<-ifelse(inds == """", paste(""i"", 1:ncol(gmat)), inds)\n    write.table(round(gcovarmat,5),file=""gcovarmat.txt"",\n                quote=F,row.names=F,col.names=inds)\n  }\n  prcomp(x=gcovarmat,center=TRUE,scale=FALSE)\n}\n\n#import text file with genotypes\ngeno <- read.table(""genotypes.txt"", header=F, stringsAsFactors=F)\n\n#convert genotypes to matrix\npcatest <- as.matrix(geno)\npca.snp <- do.pca(pcatest)\npca.summary.snp<-summary(pca.snp)\n#quick plot without colors\nplot(pca$x[,\'PC2\']~pca$x[,\'PC1\'],\n     xlab =     paste(""PC1 ("", round(pca.summary$importance[2,1]*100, 1), ""%)"", sep=""""),\n     ylab =     paste(""PC2 ("", round(pca.summary$importance[2,3]*100, 1), ""%)"", sep=""""))\n\n#plot with colors\npops <- read.csv(""groups.csv"") #list of individual assignments to whatever categories you want to color them by\nset <- c(""#1B9E77"",""#FF7F00"",""#7570B3"") #three colors in this case\nfor (i in sort(unique(pops$assign))) {\n  pops$color[pops$assign==i] <- set[i]\n}\nhead(pops)\n#mar = bottom, left, top, right\npar(mar=c(5,5,3,7), xpd=T)\nplot(pca$x[,\'PC2\']~pca$x[,\'PC1\'], cex=0.8, col=pops$color, pch = 16,\n     xlab =     paste(""PC1 ("", round(pca.summary$importance[2,1]*100, 1), ""%)"", sep=""""),\n     ylab =     paste(""PC2 ("", round(pca.summary$importance[2,3]*100, 1), ""%)"", sep=""""))\nlegend(""right"", inset=c(-.35,0), pch=16, legend = c(""Label1"",""Label2"",""Label3""), col=set,\n       title=""Category"", bty=""n"", pt.cex=0.9)\n\n#restore default par()\npar(mar=c(5,4,4,2), xpd=FALSE)\n']",0,"social organisation, elevation, spatial genetic structure, montane ant, population structure, dispersal, demographic processes, conservation decisions, genome-wide SNP markers, Alpine silver ant, Formica selysi, monogyne form, polygyne form"
Effects of social organisation and elevation on spatial genetic structure in a montane ant,"Studying patterns of population structure across the landscape sheds light on dispersal and demographic processes, which helps to inform conservation decisions. Here, we study how social organisation and landscape factors affect spatial patterns of genetic differentiation in an ant species living in mountainous regions. Using genome-wide SNP markers, we assess population structure in the Alpine silver ant, Formica selysi. This species has two social forms controlled by a supergene. The monogyne form has one queen per colony, while the polygyne form has multiple queens per colony. The two social forms co-occur in the same populations. For both social forms, we found a strong pattern of isolation-by-distance across the Alps. Within regions, genetic differentiation between populations was weaker for the monogyne form than for the polygyne form. We suggest that this pattern is due to higher dispersal and effective population sizes in the monogyne form. In addition, we found stronger isolation-by-distance and lower genetic diversity in high elevation populations, compared to lowland populations, suggesting that gene flow between F. selysi populations in the Alps occurs mostly through riparian corridors along lowland valleys. Overall, this survey highlights the need to consider intraspecific polymorphisms when assessing population connectivity and calls for special attention to the conservation of lowland habitats in mountain regions.","['StMichel\n\n\n\n#RSCRIPT TO CALCULATE BETAS WITH BOOTSTRAPPING\n\n####packages\nrequire(tidyr)\nrequire(dplyr)\nlibrary(vcfR)\nlibrary(adegenet)\nrequire(hierfstat)\n\n\n#set wd where I have the files\nsetwd(""/scratch/wally/FAC/FBM/DEE/mchapuis/default/afontcub/POPGEN"")\n\n##vcf\nselysi.asocial4<-read.vcfR(""selysi_compromise.Asocial.vcf.gz"")\n\n\n## Metadata for asocial compromise 152 indvs\ngeod_filt4<-read.table(\'metadata_good_filt_Compromise.csv\', header=T, sep="","", dec= ""."")\n\n\n#create genind object\nasocial4.gd<- vcfR2genind(selysi.asocial4)\n## Define Pop\npop(asocial4.gd)<-geod_filt4$Pop_social\n\n#SAmpling pops\ndplyr::count(geod_filt4,Pop_social)\n# I will hve to remvoe (n =1) Aubenas_P, Dalaas_M, Leuk_P, Riddes_M. \n#and be careful with (n=3) Bussets_P, Hauderes_P, Luette_P, Riddes_P, \n\n##filter small subpops\nasocial4.gd2<-asocial4.gd[!(asocial4.gd@pop==""Aubenas_P""|asocial4.gd@pop==""Dalaas_M""|asocial4.gd@pop==""Leuk_P""|asocial4.gd@pop==""Riddes_M"")]# rm pops < 3\n#asocial4.gd2<-asocial4.gd[!(asocial4.gd@pop==""Aubenas_P""|asocial4.gd@pop==""Dalaas_M""|asocial4.gd@pop==""Leuk_P""|asocial4.gd@pop==""Riddes_M""|asocial4.gd@pop==""Bussets_P""|asocial4.gd@pop==""Hauderes_P""|asocial4.gd@pop==""Luette_P""|asocial4.gd@pop==""Riddes_P"")]#rm pops < 5\ngeno<-genind2hierfstat(asocial4.gd2) #convert to hierfstat\ndim(geno)\n\n#filter metadata\nmeta<-geod_filt4[!(geod_filt4$Pop_social==""Aubenas_P""|geod_filt4$Pop_social==""Dalaas_M""|geod_filt4$Pop_social==""Leuk_P""|geod_filt4$Pop_social==""Riddes_M""),]%>%select( seqIDvcf, Population, Pop_social)\n#meta<-geod_filt4[!(geod_filt4$Pop_social==""Aubenas_P""|geod_filt4$Pop_social==""Dalaas_M""|geod_filt4$Pop_social==""Leuk_P""|geod_filt4$Pop_social==""Riddes_M""|geod_filt4$Pop_social==""Bussets_P""|geod_filt4$Pop_social==""Hauderes_P""|geod_filt4$Pop_social==""Luette_P""|geod_filt4$Pop_social==""Riddes_P""),]%>%select( seqIDvcf, Population, Pop_social)\nnames(meta)<-c(""ID"", ""pop"",""subpop""); dim(meta)\n\ngeno$pop<-meta$subpop#make sure pop factor in hierfstat object is pop social\n\n\n\n#FUNCTION to get matrix of rarefaction sample sizes\npops <- unique(meta$pop)\nsubpops <- unique(meta$subpop)\npops_combinations <- t(combn(pops,2))\npops_resample_sizes <- matrix(data=NA,nrow=length(subpops),ncol=length(subpops))\nrownames(pops_resample_sizes) <- subpops\ncolnames(pops_resample_sizes) <- subpops\n\nfor(row in 1:nrow(pops_combinations)){\n  pop1 <- pops_combinations[row,1]\n  pop2 <- pops_combinations[row,2]\n\n  pops_combination_subset <- as.data.frame(subset(meta, pop == pop1 | pop == pop2))\n\n  subpop_sizes <- c()\n  subpop_names <- unique(pops_combination_subset$subpop)\n  for(subpop_i in subpop_names){\n    subpop_subset <- subset(pops_combination_subset, subpop == subpop_i)\n    n <- nrow(subpop_subset)\n    subpop_sizes <- c(subpop_sizes, n)\n  }\n\n  n_min <- min(subpop_sizes)\n\n  subpops_combinations <- t(combn(subpop_names,2))\n  for(row in 1:nrow(subpops_combinations)){\n    subpop1 <- as.character(subpops_combinations[row,1])\n    subpop2 <-as.character(subpops_combinations[row,2])\n    pops_resample_sizes[subpop1,subpop2] <- n_min\n    pops_resample_sizes[subpop2,subpop1] <- n_min\n  }\n}\n\nprint(pops_resample_sizes)\n\n####FUNCTION to Calculate betas  with BOOTSRTAP iterations\n#(i changed slightly the resampling function using base R instead of dplyr but does the same)\n#test_geno<-geno[,1:21]\n#geno<-test_geno #removed once tested\nnb_ite=100# nb of iterations\nsubpop_names<-unique(geno$pop)\nsubpop_combis=t(combn(subpop_names,2))\n\npairfst <- matrix(data=NA,nrow=length(subpop_names),ncol=length(subpop_names))#matrix for mean beta\nrownames(pairfst) <- subpop_names; colnames(pairfst) <- subpop_names\npairSD <- matrix(data=NA,nrow=length(subpop_names),ncol=length(subpop_names)) #matrix for SD of beta\nrownames(pairSD) <- subpop_names; colnames(pairSD) <- subpop_names\npairMedianFst <- matrix(data=NA,nrow=length(subpop_names),ncol=length(subpop_names)) #matrix for SD of beta\nrownames(pairMedianFst) <- subpop_names; colnames(pairMedianFst) <- subpop_names\n\n#nrow=25\n\nfor (nrow in 1:nrow(subpop_combis)) {\n    subpop1<-as.character(subpop_combis[nrow,1])\n    subpop2<-as.character(subpop_combis[nrow,2])\n      subpop_subset<-geno[(geno$pop==subpop1|geno$pop==subpop2),]\n      n<-pops_resample_sizes[subpop1,subpop2]\n      beta_vec<-NULL\n      #with manual iteration (bootrstraping)\n      for (iteration in 1:nb_ite){\n        subpop_subset$pop<-factor(subpop_subset$pop,levels=as.character(unique(subpop_subset$pop)))#set correct nb of levels\n        resample<-lapply(split(subpop_subset,subpop_subset$pop),function (x) x[sample(1:nrow(x),n),])\n        resample_subset<-do.call(""rbind"",resample) #unlist and make 1 df\n        beta<-betas(resample_subset)$betaW #calculate beta\n        beta_vec<-c(beta_vec,beta)#store in vector\n        }\n      mean_beta<-mean(beta_vec,na.rm=T)#take the mean across nb_ite iterations\n      sd_beta<-sd(beta_vec,na.rm=T)\n      median_beta<-median(beta_vec,na.rm=T)#take median\n\n      pairfst[subpop1,subpop2]<- pairfst[subpop2,subpop1]<-mean_beta\n      pairSD[subpop1,']",0,"1. Astragalus tragacantha
2. Provence
3. Fabaceae
4. realized niche
5. habitat characteristics
6. floristic survey
7. demographic census
8. Outlying Mean Index (OM"
Is a restricted niche the explanation for species vulnerability? Insights from a large field survey of Astragalus tragacantha L. (Fabaceae),"In this study, we examine the realized niche of A. tragacantha in Provence. We focus our fieldwork on habitat characteristics, floristic survey and perform an exhaustive demographic census. We base our analyses on total density and densities of different size stages of A. tragacantha. We delineate its realized ecological niche with the Outlying Mean Index (OMI) multivariate method and estimated the effects of environmental gradients on A. tragacantha populations with the Generalized Joint Attribute Modelling method (GJAM). Analyses based on occurrence data did not support the hypothesis that a too narrow niche by itself explains the vulnerability of A. tragacantha. However, niche modelling revealed differences among size stages supporting a restricted regeneration niche and a wider ecological range in the past.This dataset is accompanying the manuscript published in Flora ""Is a restricted niche the explanation for species vulnerability? Insights from a large field survey of Astragalus tragacantha L. (Fabaceae)""","['library(labdsv)\r\nlibrary(vegan)\r\nlibrary(gjam)\r\nlibrary(ade4)\r\nlibrary(ggplot2)\r\nlibrary(ggcorrplot)\r\nlibrary(reshape)\r\nlibrary(indicspecies)\r\nlibrary(Rmisc)\r\n#Gjam is done here on the OMI axis as supervariables summarizing the main ecological gradients\r\n\r\n########DATA X : Environmental data\r\nmeso<-read.table(""Hab_total_OMI.txt"",h=T)\r\nnames(meso)\r\nelevation<-meso[,2]\r\nslope<-meso[,3]\r\nd2sea<-meso[,4]\r\nslopori<-meso[,5]\r\nwood<-meso[,12]+meso[,13]+meso[,14]\r\nherb<-meso[,15]\r\nrock<-meso[,6]\r\nblock<-meso[,7]\r\nstone<-meso[,8]\r\nsoil<-meso[,9]\r\nlitter<-meso[,10]\r\nenv<-data.frame(elevation,slope,d2sea,slopori,wood,herb,rock,block,stone,soil,litter)\r\n\r\n########DATA Y: floristic data and A. tragcantha variables\r\n#Floristic data\r\nflo<-read.table(""Flore_total_OMI.txt"", h=T)\r\nflo.pa<-decostand(flo[,2:336], method=""pa"")\r\n#A. tragacantha \r\nDemo<-read.table(""Demo_GJAM.txt"", h=T)\r\ndata.frame(Demo$Idp,meso$Idp,flo$Idp)\r\nnames(Demo)\r\nD10<-Demo[,2]\r\nD50<-Demo[,3]\r\nD100<-Demo[,4]\r\nD200<-Demo[,5]\r\nDsup200<-Demo[,6]\r\n#Size stages\r\ndemo_set<-data.frame(D10,D50,D100,D200,Dsup200)\r\n#Abundance\r\nAbun<-Demo[,12]\r\n\r\n#####indiscpecies######\r\nall<-Demo[,12]\r\ngroup<-all>0\r\nindval<-multipatt(flo.pa, group)\r\nsummary(indval, indvalcomp=T)\r\n\r\n############OMI\r\npca<-dudi.pca(env, scannf=F, nf=10)\r\nnichastra<-niche(pca, flo.pa, nf=4, scannf=F)\r\nplot(nichastra)\r\n#inertia\r\nround(nichastra$eig/sum(nichastra$eig),2)\r\n#variables correlations\r\ns.arrow(nichastra$co[,1:2]*7, boxes=F, clab=0.8)\r\n#see the second script on DRYAD to make the final OMI figure\r\n#to obtain niche paramters for each species\r\nwrite.table(niche.param(nichastra),""param.niche.txt"")\r\n#summary of OMI, TOL and RTOL on all data\r\nparam<-niche.param(nichastra)\r\nTol<-param[,3]\r\nsummary(param[Tol>0,5:7])\r\n#we define 4 supervariable for GJAM\r\nOMI1<-nichastra$ls[,1]\r\nOMI2<-nichastra$ls[,2]\r\nOMI3<-nichastra$ls[,3]\r\nOMI4<-nichastra$ls[,4]\r\nenv2<-data.frame(env,OMI1,OMI2,OMI3,OMI4)\r\n#correlation plot to help understanding of the four supervariables\r\nround(cor(env2, method=""spearman""),2)->cor.data\r\nggcorrplot(cor.data, hc.order = F, type = ""lower"", colors = c(""#6D9EC1"", ""white"", ""#E46726""),  lab = TRUE, title=""Heatmap of spearman correlations among habitat variables"")\r\n\r\n########################GJAM\r\n#removing of rare species having a low impact on GJAM models\r\ndataY<-dropspc(flo.pa,25)\r\nnames(dataY)\r\n#GJAM will be performed two times : first with all A. tragcantha size stages and the second with abundance only\r\n#A. tragacantha presence/absence, number 6, is removed\r\ndataY1<-cbind(dataY[,-6],demo_set)\r\ndataY2<-cbind(dataY[,-6],Abun)\r\n\r\n#Model_1 = OMI + Demo set\r\ntypes<-c(rep(\'PA\',94),rep(\'DA\',5))\r\nrl  <- list(r = 2, N =4)\r\nml.par.1<-list(ng = 10000, burnin = 2500, typeNames=types, PREDICTX = T, reductList=rl)\r\nmodel.1<-gjam(~OMI1+OMI2+OMI3+OMI4, env2, dataY1, modelList = ml.par.1)\r\nsummary(model.1)\r\n#graphical outputs saved in a folder\r\npl  <- list(GRIDPLOTS = T, SAVEPLOTS = T, outfolder = \'plots\', SMALLPLOTS = F)\r\ngjamPlot(model.1, plotPars = pl)\r\n#table of beta coefficients\r\nwrite.table(model.1$parameters$betaTable, ""beta_mod_1.txt"")\r\n#Sensitivity Model 1\r\nSensi_model <- gjamSensitivity(model.1)\r\nSensi_10 <- gjamSensitivity(model.1, \'D10\', nsim=500)\r\nSensi_50 <- gjamSensitivity(model.1, \'D50\', nsim=500)\r\nSensi_100 <- gjamSensitivity(model.1, \'D100\', nsim=500)\r\nSensi_200 <- gjamSensitivity(model.1, \'D200\', nsim=500)\r\nSensi_sup200 <- gjamSensitivity(model.1, \'Dsup200\', nsim=500)\r\n\r\n#Model 2 = OMI + Abundance\r\ntypes2<-c(rep(\'PA\',94),\'DA\')\r\nrl  <- list(r = 2, N =4)\r\nml.par.2<-list(ng = 10000, burnin = 2500, typeNames=types2, PREDICTX = T, reductList=rl)\r\nmodel.2<-gjam(~OMI1+OMI2+OMI3+OMI4, env2, dataY2, modelList = ml.par.2)\r\ngjamPlot(model.2, plotPars = pl)\r\nsummary(model.2)\r\n#table of beta coefficients\r\nwrite.table(model.2$parameters$betaTable, ""beta_mod_2.txt"")\r\n#Sensitivity Model 2\r\nSensi_all <- gjamSensitivity(model.2, \'Abun\', nsim=500)\r\n\r\n#Boxplots of senitivity \r\npar(mfrow=c(2,3))\r\npar(mai=c(0.4,0.5,0.2,0.1))\r\nboxplot(Sensi_all , col=""lightgrey"", main=""All_stages"",outline=F)\r\nboxplot(Sensi_10 , col=""lightgrey"", main=""D10"", outline=F)\r\nboxplot(Sensi_50 , col=""lightgrey"", main=""D50"", outline=F)\r\nboxplot(Sensi_100 , col=""lightgrey"", main=""D100"", outline=F)\r\nboxplot(Sensi_200 , col=""lightgrey"", main=""D200"", outline=F)\r\nboxplot(Sensi_sup200 , col=""lightgrey"", main=""Dsup200"", outline=F)\r\n\r\n\r\n\r\n']",0,"1. Genomic variation 
2. Black-throated Green Warbler
3. Setophaga virens
4. Atlantic Coastal Plain 
5. S. v. waynei
6. Whole-genome resequencing"
Genomic variation in the Black-throated Green Warbler (Setophaga virens) suggests divergence in a disjunct Atlantic Coastal Plain population (S. v. waynei),"We used whole-genome resequencing to estimate genetic distinctiveness in the Black-throated Green Warbler (Setophaga virens)including S. v. wayneia putative subspecies that occupies a narrow disjunct breeding range along the Atlantic Coastal Plain. Despite detecting low-global differentiation (FST = 0.027) across the entire species, the principal components analysis of genome-wide differences shows the main axis of variation separates S. v. waynei from all other S. v. virens samples. We also estimated a low-migration rate for S. v. waynei, but found them to be most similar to another disjunct population from the Piedmont of North Carolina, and detected evidence of a historical north-to-south geographic dispersal among the entire species. New World wood warblers (family: Parulidae) can exhibit strong phenotypic differences among species, particularly, in song and plumage; however, within-species variation in these warblersoften designated as subspeciesis much more subtle. The existence of several isolated Black-throated Green Warbler populations across its eastern North American breeding range offers an excellent opportunity to further understand the origin, maintenance, and conservation status of subspecific populations. Our results, combined with previously documented ecological and morphological distinctiveness, support that S. v. waynei be considered a distinct and recognized subspecies worthy of targeted conservation efforts.","['rm( list = ls() )\r\nworkdir <- ""/Users/awwood/Desktop/r_dir/waynyi/pcangsd""\r\nsetwd(workdir)\r\n\r\ncolors <- scan( file = ""population_color_list.txt"", what = ""character"" )\r\n\r\nmatrix <- as.matrix( read.table ( file=""whole_genome.cov"", stringsAsFactors = F ) )\r\ne <- eigen( matrix )\r\n\r\neigen_sum <- sum(e$values)\r\npc3 <- e$values[3] / eigen_sum\r\npc3 <- round(pc3, digits=3)\r\npc3 <- sprintf(""%.1f%%"", 100*pc3)\r\npc4 <- e$values[4] / eigen_sum\r\npc4 <- round(pc4, digits=3)\r\npc4 <- sprintf(""%.1f%%"", 100*pc4)\r\npc2 <- e$values[2] / eigen_sum\r\npc2 <- round(pc2, digits=3)\r\npc2 <- sprintf(""%.1f%%"", 100*pc2)\r\n\r\n# Calculate the percent variance explained by each principal component.\r\n\r\nall_pc <- c(rep(0, length(e$values)) )\r\n\r\nfor (i in 1 : length(all_pc) ){\r\n\r\npc <- e$values[i] / eigen_sum\r\npc <- round(pc, digits=3)\r\n#pc <- 100*pc\r\n\r\nall_pc[i] <- pc\r\n\r\n}\r\n\r\nnames(all_pc) <- as.character(1:29)\r\nbarplot(all_pc, col = ""Black"", names = labels, xlab)\r\n\r\nx <- paste( ""PC2 ("", pc2,"")"", sep = """")\r\ny <- paste( ""PC3 ("", pc3,"")"", sep = """")\r\n\r\nlgnd <- c(""TN"", ""AR"", ""Uwharrie"", ""IN"",""NY"", ""NC [S. v. waynei]"" )\r\nlgnd_fill <- c( ""dodgerblue"", ""cyan"", ""darkorchid"", ""cadetblue"", ""blue"", ""gold"" )\r\n\r\npdf( file = ""pc2_3_population_color_Whole_Genome.pdf"" )\r\npar(mar = c(6,6,2,2))\r\nplot(e$vectors[,2:3],lwd=2,ylab=y,xlab=x,main=""Setophaga virens waynei whole genome"", bg=colors, pch=21, col=""black"", cex=3, cex.lab=1.0, cex.main=1.0 )\r\nlegend(""bottomright"", legend = lgnd , fill = lgnd_fill )\r\ndev.off()\r\n\t\r\n']",0,"Innate immunity, HIV, CD4+ T cells, cell lines, permissiveness, viral infection, transcriptome profiling, gene expression, viral sensing, viral restriction, primary cells, HEK293T, Jurkat, SupT"
Innate immune defects in HIV permissive cell lines,"Background: Primary CD4+ T cells and cell lines differ in their permissiveness to HIV infection. Impaired innate immunity may contribute to this different phenotype.Findings: We used transcriptome profiling of 1503 innate immunity genes in primary CD4+ T cells and permissive cell lines. Two clusters of differentially expressed genes were identified: a set of 249 genes that were highly expressed in primary cells and minimally expressed in cell lines and a set of 110 genes with the opposite pattern. Specific to HIV, HEK293T, Jurkat, SupT1 and CEM cell lines displayed unique patterns of downregulation of genes involved in viral sensing and restriction. Activation of primary CD4+ T cells resulted in reversal of the pattern of expression of those sets of innate immunity genes. Functional analysis of prototypical innate immunity pathways of permissive cell lines confirmed impaired responses identified in transcriptome analyses.Conclusion: Integrity of innate immunity genes and pathways needs to be considered in designing gain/loss functional genomic screens of viral infection.","['# SET UP WORKING DIRECTORY AS THE ONE CONTAINING SUPPLEMENTARY TABLES\n# inputdir=""/pathtoinputdir/""\n# outputdir=""/pathtooutputdir/""\n\nsetwd(inputdir)\n\nlibrary( ""DESeq"" )\nlibrary(""easyRNASeq"")\nlibrary(""RColorBrewer"") \nlibrary(""gplots"") \nlibrary(""Hmisc"")\nlibrary(""vsn"") \nlibrary(""RColorBrewer"") \nlibrary(""gplots"") \nlibrary(""hash"")\nlibrary(cluster)\nlibrary(beanplot)\n\nlistALLInn<-as.character(read.table(""ST1_ListInnateImmunityGenes.txt"",,header=TRUE,sep=""\\t"")$Ensembl.Gene.ID)\ncountTable=read.table(""ST4_RawCounts.txt"", header=TRUE, row.names=1,sep=""\\t"")\nrpkmHraw=read.table(""ST5_rpkmHraw.txt"", header=TRUE, row.names=1,sep=""\\t"")\nsampleTable=read.table(""ST6_LibrariesDescriptionForDESeq.txt"",header=TRUE)\n\ncolnames(countTable)<-substring(colnames(countTable), 2,)\nsummary(colnames(countTable)==sampleTable$Sample)\ncdsHraw = newCountDataSet( countTable, sampleTable$Condition)\n\ncolnames(rpkmHraw)<-substring(colnames(rpkmHraw), 2,)\n###################################################################\nrpkmHraw_lg10N<-log10((rpkmHraw*mean( colSums (countTable[,1:13]))/1000000)+1)\n#----------------------------------------------\n# # Optionally we can change gene Ensembl identifiers to HGCN symbols\n# rpkmHraw_lg10N_GeneNames<-as.matrix(rpkmHraw_lg10N)\n# GeneNames<-read.table(""EnsemblGeneListFromCountMatrices_BioMartEnsembl70Names.txt"",header=TRUE,sep=""\\t"")\n# GeneNamesEnsemblID<-as.vector(GeneNames$Ensembl.Gene.ID)\n# names(GeneNamesEnsemblID)<-GeneNames$Associated.Gene.Name\n# head(GeneNames)\n# GeneNames_Ordered<-GeneNames[order(GeneNames$Ensembl.Gene.ID),]\n# summary(GeneNames_Ordered$Ensembl.Gene.ID==rownames(rpkmHraw_lg10N)[1:(length(rownames(rpkmHraw_lg10N))-1)])\n# rownames(rpkmHraw_lg10N_GeneNames)[1:(length(rownames(rpkmHraw_lg10N))-1)]<-as.character(GeneNames_Ordered$Associated.Gene.Name)\n# head(GeneNames_Ordered$Associated.Gene.Name)\n# head(rownames(rpkmHraw_lg10N))\n# head(rownames(rpkmHraw_lg10N_GeneNames))\n# #write.table(rpkmHraw_lg10N_GeneNames , file=""rpkmHraw_lg10N_GeneNames.txt"",append = FALSE, quote=FALSE, sep=""\\t"")\n#====================================================================\n# Here we restrict to the genes expressed in primary or cell lines (regardless the activated CD328s)\nrsH = rowSums ( countTable[,1:13])\nuseH = (rsH > 0)\nsummary(rownames(rpkmHraw)==names(useH))\nsummary(rownames(rpkmHraw_lg10N)==names(useH))\nrpkmHrawH<-rpkmHraw[ useH, ]\nrpkmHrawH_lg10N<-rpkmHraw_lg10N[ useH, ]\n#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\ncdsHNoAct = cdsHraw[ useH, 1:13]\ncdsHNoAct = estimateSizeFactors( cdsHNoAct )\ncdsHNoAct_Blindlocal = estimateDispersions( cdsHNoAct, method=""blind"" ,fitType = c(""local""))\nvsdHNoAct_Blindlocal = varianceStabilizingTransformation( cdsHNoAct_Blindlocal )\n\ncdsH = cdsHraw[ useH, ]\ncdsH = estimateSizeFactors( cdsH )\ncdsH_Blindlocal = estimateDispersions( cdsH, method=""blind"" ,fitType = c(""local""))\nvsdH_Blindlocal = varianceStabilizingTransformation( cdsH_Blindlocal )\n\n# NOTICE!!!: without including CD328s\ngenes.cor <- cor( t(exprs(vsdHNoAct_Blindlocal)[rownames(exprs(vsdHNoAct_Blindlocal)) %in% listALLInn,]), method=""pearson"")\ngenes.cor.dist <- as.dist(1-genes.cor)\ngenes.tree <- hclust(genes.cor.dist,method=\'complete\')\n\nsamples.cor <- cor( (exprs(vsdHNoAct_Blindlocal)[rownames(exprs(vsdHNoAct_Blindlocal)) %in% listALLInn,]), method=""pearson"")\nsamples.cor.dist <- as.dist(1-samples.cor)\nsamples.tree <- hclust(samples.cor.dist,method=\'complete\')\nplot(samples.tree)\n\npng(""Fig1.png"",bg = ""white"",title =\'\',width = 20, height = 20 ,units=""cm"",res=300)\ncolors = colorRampPalette(c(\'green\', \'red\'))(1000)\nmyMatrix<-as.matrix(rpkmHrawH[rownames(rpkmHrawH) %in% listALLInn,1:13])\nsummary(rownames(myMatrix)==rownames(exprs(vsdHNoAct_Blindlocal)[rownames(exprs(vsdHNoAct_Blindlocal)) %in% listALLInn,]))\nsummary(rownames(myMatrix)==genes.tree$labels)\ncolnames(myMatrix)\nfor (i in 1:length(colnames(myMatrix))){\n  if(grepl(""CD4J3"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""CD4_J3""}\n  if(grepl(""CD4J4"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""CD4_J4""}\n  if(grepl(""293TMo"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""293T_Mock""}\n  if(grepl(""293TLV"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""293T_LV""}\n  if(grepl(""SupMo"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""SupT1_Mock""}\n  if(grepl(""SupHI"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""SupT1_hiLV""}\n  if(grepl(""SupLV"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""SupT1_LV""}\n  if(grepl(""JurMo"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""Jurkat_Mock""}\n  if(grepl(""JurHI"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""Jurkat_hiLV""}\n  if(grepl(""JurLV"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""Jurkat_LV""}\n  if(grepl(""CemMo"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""CEM_Mock""}\n  if(grepl(""CemHI"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""CEM_hiLV""}\n  if(grepl(""CemLV"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""CEM_LV""}\n  if(grepl(""50_"",colnames(myMatrix)[i])){colnames(myMatrix)[i]<-""Act8h_Mock""}\n  if(gre']",0,"Quantitative proteomics, postmating response, female reproductive tract, sibling species, interactions, male and female reproductive proteins, fertility, postmating changes, behavior, morphology, physiology, coevolution, reproductive incompatibilities, prezy"
Data from: Quantitative proteomics reveals rapid divergence in the postmating response of female reproductive tracts among sibling species,"Fertility depends, in part, on interactions between male and female reproductive proteins inside the female reproductive tract (FRT) that mediate postmating changes in female behavior, morphology, and physiology. Coevolution between interacting proteins within species may drive reproductive incompatibilities between species, yet the mechanisms underlying postmating-prezygotic isolating barriers remain poorly resolved. Here, we used quantitative proteomics in sibling Drosophila species to investigate the molecular composition of the FRT environment and its role in mediating species-specific postmating responses. We found that (1) FRT proteomes in D. simulans and D. mauritiana virgin females express unique combinations of secreted proteins and are enriched for distinct functional categories, (2) mating induces substantial changes to the FRT proteome in D. mauritiana but not in D. simulans, and (3) the D. simulans FRT proteome exhibits limited postmating changes irrespective of whether females mate with conspecific or heterospecific males, suggesting an active female role in mediating reproductive interactions. Comparisons with similar data in the closely related outgroup species D. melanogaster suggest that divergence is concentrated on the D. simulans lineage. Our study suggests that divergence in the FRT extracellular environment and postmating response contribute to previously described patterns of postmating-prezygotic isolation and the maintenance of species boundaries.","['library(Biobase)\nlibrary(MSnbase)\nlibrary(limma)\n\nf <- ""female_list.csv""\ne <- c(2:11)\nx <- readMSnSet2(f, e, fnames=1)\n\nSample<-gsub(""(w*).(d)"", ""2.1"", colnames(exprs(x)))\n# for pattern ""generic word [.] digit"", replace with "".2 or .1"", for exprs(x)\n\ntmp <- data.frame(do.call(rbind, strsplit(Sample, ""[.]"")))\n#string split at "".""\n\nnames(tmp) <- c(""Sample"",""Replicate"")\n\nsampleNames(x)<-paste(tmp$Sample, tmp$Replicate, sep = """")\nrownames(tmp) <- paste(tmp$Sample, tmp$Replicate, sep = """")\npData(x) <- tmp\n\nx <- log(x,2)\nx <- normalise(x,""diff.median"")\nboxplot(exprs(x))\nwrite.csv(exprs(x),""norm_female_proteins.csv"")\n\n#########\n#sim mated to virgin comparison\n#need to add [.] before 1 or 2 in column headers\nf <- ""norm_female_proteins.csv""\ne <- c(8:11)\nx <- readMSnSet2(f, e, fnames=1)\n\nSample<-gsub(""(w*).(d)"", ""2.1"", colnames(exprs(x)))\ntmp <- data.frame(do.call(rbind, strsplit(Sample, ""[.]"")))\nnames(tmp) <- c(""Sample"",""Replicate"")\n\nsampleNames(x)<-paste(tmp$Sample, tmp$Replicate, sep = """")\nrownames(tmp) <- paste(tmp$Sample, tmp$Replicate, sep = """")\npData(x) <- tmp\n\nboxplot(exprs(x))\n\nmat.design <- model.matrix(~ 0 + pData(x)$Sample)\ncolnames(mat.design) <- c(\'SS\',\'SV\')\n\ncontrast.matrix <- makeContrasts(SS-SV,\n                                 levels = mat.design)\n\nfit <- lmFit(exprs(x),design=mat.design)\nfit2 <- contrasts.fit(fit, contrast.matrix)\nfit3 <- eBayes(fit2)\ntt <- topTable(fit3, adjust.method = ""BH"", n = Inf,coef=1)\nwrite.csv(tt,""sssv_female.csv"")\n\n#mau mated to virgin comparison\nf <- ""norm_female_proteins.csv""\ne <- c(2:5)\nx <- readMSnSet2(f, e, fnames=1)\n\nSample<-gsub(""(w*).(d)"", ""2.1"", colnames(exprs(x)))\ntmp <- data.frame(do.call(rbind, strsplit(Sample, ""[.]"")))\nnames(tmp) <- c(""Sample"",""Replicate"")\n\nsampleNames(x)<-paste(tmp$Sample, tmp$Replicate, sep = """")\nrownames(tmp) <- paste(tmp$Sample, tmp$Replicate, sep = """")\npData(x) <- tmp\n\nboxplot(exprs(x))\n\nmat.design <- model.matrix(~ 0 + pData(x)$Sample)\ncolnames(mat.design) <- c(\'MM\',\'MV\')\n\ncontrast.matrix <- makeContrasts(MM-MV,\n                                 levels = mat.design)\n\nfit <- lmFit(exprs(x),design=mat.design)\nfit2 <- contrasts.fit(fit, contrast.matrix)\nfit3 <- eBayes(fit2)\ntt <- topTable(fit3, adjust.method = ""BH"", n = Inf,coef=1)\nwrite.csv(tt,""mmmv_female.csv"")\n\n#########\n#virgins\n#rearrange columns of norm_female_proteins to: SV, MV, SM, SS, MM\n#new arrangement is ""norm_female_proteins_reordered""\n\nf <- ""norm_female_proteins_reordered.csv""\ne <- c(2:5)\nx <- readMSnSet2(f, e, fnames=1)\n\nSample<-gsub(""(w*).(d)"", ""2.1"", colnames(exprs(x)))\ntmp <- data.frame(do.call(rbind, strsplit(Sample, ""[.]"")))\nnames(tmp) <- c(""Sample"",""Replicate"")\n\nsampleNames(x)<-paste(tmp$Sample, tmp$Replicate, sep = """")\nrownames(tmp) <- paste(tmp$Sample, tmp$Replicate, sep = """")\npData(x) <- tmp\n\nboxplot(exprs(x))\n\nmat.design <- model.matrix(~ 0 + pData(x)$Sample)\ncolnames(mat.design) <- c(\'SV\',\'MV\')\n\ncontrast.matrix <- makeContrasts(SV-MV,\n                                 levels = mat.design)\n\nfit <- lmFit(exprs(x),design=mat.design)\nfit2 <- contrasts.fit(fit, contrast.matrix)\nfit3 <- eBayes(fit2)\ntt <- topTable(fit3, adjust.method = ""BH"", n = Inf,coef=1)\nwrite.csv(tt,""svmv_female.csv"")\n\n#########\n#mated\n\nf <- ""norm_female_proteins_reordered.csv""\ne <- c(8:11)\nx <- readMSnSet2(f, e, fnames=1)\n\nSample<-gsub(""(w*).(d)"", ""2.1"", colnames(exprs(x)))\ntmp <- data.frame(do.call(rbind, strsplit(Sample, ""[.]"")))\nnames(tmp) <- c(""Sample"",""Replicate"")\n\nsampleNames(x)<-paste(tmp$Sample, tmp$Replicate, sep = """")\nrownames(tmp) <- paste(tmp$Sample, tmp$Replicate, sep = """")\npData(x) <- tmp\n\nboxplot(exprs(x))\n\nmat.design <- model.matrix(~ 0 + pData(x)$Sample)\ncolnames(mat.design) <- c(\'SS\',\'MM\')\n\ncontrast.matrix <- makeContrasts(SS-MM,\n                                 levels = mat.design)\n\nfit <- lmFit(exprs(x),design=mat.design)\nfit2 <- contrasts.fit(fit, contrast.matrix)\nfit3 <- eBayes(fit2)\ntt <- topTable(fit3, adjust.method = ""BH"", n = Inf,coef=1)\nwrite.csv(tt,""ssmm_female.csv"")\n\n#########\n#het vs ss\n#rearrange columns of norm_female_proteins to: SV, MV, SS, SM, MM\n#new arrangement is ""norm_female_proteins_reordered2""\n\nf <- ""norm_female_proteins_reordered2.csv""\ne <- c(6:9)\nx <- readMSnSet2(f, e, fnames=1)\n\nSample<-gsub(""(w*).(d)"", ""2.1"", colnames(exprs(x)))\ntmp <- data.frame(do.call(rbind, strsplit(Sample, ""[.]"")))\nnames(tmp) <- c(""Sample"",""Replicate"")\n\nsampleNames(x)<-paste(tmp$Sample, tmp$Replicate, sep = """")\nrownames(tmp) <- paste(tmp$Sample, tmp$Replicate, sep = """")\npData(x) <- tmp\n\nboxplot(exprs(x))\n\nmat.design <- model.matrix(~ 0 + pData(x)$Sample)\ncolnames(mat.design) <- c(\'SS\',\'SM\')\n\ncontrast.matrix <- makeContrasts(SS-SM,\n                                 levels = mat.design)\n\nfit <- lmFit(exprs(x),design=mat.design)\nfit2 <- contrasts.fit(fit, contrast.matrix)\nfit3 <- eBayes(fit2)\ntt <- topTable(fit3, adjust.method = ""BH"", n = Inf,coef=1)\nwrite.csv(tt,""sssm_female.csv"")\n\n#########\n#het vs mm\n\nf <- ""norm_female_proteins_reordered2.csv""\ne <- c(8:11)\nx <- readMSnSet2(f, e, fname']",0,"transposable elements, TE abundance, de novo pipeline, EDTA, DeepTE, non-model species, teleost, genomic input, transcriptomic input, species specific, underestimates, library choice, repbase, homology, transpos"
Data from: Transposable element annotation in non-model species - on the benefits of species specific repeat libraries using semi-automated EDTA and DeepTE de novo pipelines,"Transposable elements (TEs) are significant genomic components which can be detected either through sequence homology against existing databases or de novo, with the latter potentially reducing underestimates of TE abundance. Here, we describe the semi-automated generation of a de-novo TE library which combines the newly described EDTA pipeline and DeepTE classifier in a non-model teleost (Corydoras sp. C115). We assess performance using both genomic and transcriptomic input by five metrics: (i) abundance (ii) composition (iii) fragmentation (iv) age distributions and (v) capture of potential horizontally transferred TEs. We identified notable differences in these metrics between different TE libraries, and highlight how library choice can have a major impact on TE content estimates in non-model species.This repository incorporates six raw (unparsed) Repeat Masker (RM) output files for two genomes (Corydoras sp. c115 and Corydoras maculifer) one transcriptome (C. maculifer), two Repeat Libraries (one based on the RepBase Danio rerio library and one de novo library build on the C. sp. c115 genome). The RM ouput files correspond to one homology based transposon search using the D. rerio library and one species specific search using the de novo library. It also includes a script to acompany horizontal transfer analysis and a transposable element renamins script.","['#Horizontal Transfer Analysis - broadHT\n##Author = Chris Butler\n##Date = 02/12/20\n\n##Reset\nrm(list = ls())\npar(mfrow=c(1,1))\n\n##Load libraries##\n\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(seqinr)\nlibrary(splitstackshape)  \nlibrary(tigger)\nlibrary(tidyr)\nlibrary(sjPlot)\nlibrary(xtable)\nlibrary(forcats)\nlibrary(dabestr)\nlibrary(dplyr)\nlibrary(KRIS)\n\n##Example production of BED file for genome file :)##\n\n#load custom functions\nread_csv_filename <- function(filename){\n  ret <- read.csv(filename, fill = TRUE, row.names=NULL, stringsAsFactors = FALSE, header = FALSE,  sep = ""\\t"")\n  ret$Source <- filename #EDIT\n  ret\n}\n\n#set working directory \nsetwd(\'/Users/chrisbutler/Documents/technical_paper/broad_HT_test/maculifer_genome/\')\nnoisonosim <- read.csv(\'Maculifer_C115_v1.1lib_noisonosim copy.csv\')\n\n\n######Adding TE type and class column#####\nnoisonosim$TE_type[grepl(""TcMar"", noisonosim$matching_class)] <- ""TIR TC1-Mariner-like""\n\n\n#filter just Mariner\nnoisonosim <- noisonosim %>% filter(TE_type == \'TIR TC1-Mariner-like\')\nnoisonosim <- mutate(noisonosim, perc_frag = (mergedfraglength/reference_length)*100)\n\n#filter so fragments are greater than 80% of total RepBase length\nnoisonosim <- filter(noisonosim, perc_frag >80)\n\nnoisonosim <- noisonosim %>% dplyr::select(\'qry_id\', \'lowextremety\', \'highextremety\', \'repeat_id\')\nwrite.table(noisonosim, file = ""mariner_maculifer_genome_denovo.bed"", col.names = FALSE, sep = \'\\t\', quote = FALSE, row.names = FALSE)\n\n######\n##Example production of BED file for transcriptome file :)##\n########\n\n\nsetwd(\'/Users/chrisbutler/Documents/technical_paper/danio_deepTE/noasterisk/\')\nsample56_danio <- read.csv(\'noasterisk_daniodeepte_Sample56.out_RM_TRIPS.csv\', stringsAsFactors = FALSE)\n\n######Adding TE type and class column#####\nsample56_danio$TE_type[grepl(""TcMar"", sample56_danio$matching_class)] <- ""TIR TC1-Mariner-like""\n\n\n#filter just Mariner\nsample56_danio <- sample56_danio %>% filter(TE_type == \'TIR TC1-Mariner-like\')\n\n\n\n# 2) import list of every transcript name - contains length of every transcript\n\nsetwd(\'/Users/chrisbutler/Documents/assembled_transcripts/sixteen\')\nmyfiles = list.files(pattern=""*.headers.csv"", full.names=TRUE) #myfiles is now a list of every headers csv file\n\n\nall_sequencenames = lapply(myfiles, read_csv_filename) #read_csv_filenames of the myfiles list\ncombined_sequencenames <- do.call(rbind, all_sequencenames) #combined all the csv files into one dataframe\ncombined_sequencenames$Source <- gsub(\'./\', \'\', combined_sequencenames$Source) #clean up file name column\ncombined_sequencenames$Source <- gsub(\'headers.csv\', \'\', combined_sequencenames$Source) #clean up file name column\ncombined_sequencenames$V1 <- gsub(\'>\', \'\', combined_sequencenames$V1) #clean up transcript name column - remove fasta \'>\' \n\ncombined_sequencenames$lengths <- combined_sequencenames$V1 #create a duplicate length column\ncombined_sequencenames$lengths <- sub("".*? "", """", combined_sequencenames$lengths ) #clean - obtain just the length ""bit"" of the transcript header\ncombined_sequencenames$lengths <- sub("" .*"", """", combined_sequencenames$lengths ) \ncombined_sequencenames$lengths <- sub(""len="", """", combined_sequencenames$lengths )\ncombined_sequencenames <- dplyr::rename(combined_sequencenames, qry_id = V1) #renaming\ncombined_sequencenames <- dplyr::rename(combined_sequencenames, Sample = Source) #renaming\ncombined_sequencenames$Sample <- gsub(\'S\', \'s\', combined_sequencenames$Sample) #clean\ncombined_sequencenames$qry_id <- sub(\' .*\',\'\', combined_sequencenames$qry_id) #clean\ncombined_sequencenames <- rename(combined_sequencenames, \'Transcript_L\' = lengths)\ncombined_sequencenames$Transcript_L <- as.numeric(combined_sequencenames$Transcript_L)\n\nsample56_danio$Sample <- \'sample56\'\nsample56_danio <- merge(sample56_danio, combined_sequencenames, by = c(\'Sample\', \'qry_id\'))\nsample56_danio <- mutate(sample56_danio, perc_TE_transcript = (mergedfraglength/Transcript_L)*100)\n\ntest_80 <- filter(sample56_danio, perc_TE_transcript >= 80) # extracting Mariner elements over 80% of length of transcript \n\n###\n#example - writing a .bed file\n###\n\nsetwd(\'/Users/chrisbutler/Documents/technical_paper/broad_HT_test/maculifer_transcriptome/\')\ntest_80 <- test_80 %>% dplyr::select(\'qry_id\', \'merged_qrystart\', \'merged_qryend\', \'repeat_id\')\nwrite.table(test_80, file = ""mariner_sample56_daniodeepte.bed"", col.names = FALSE, sep = \'\\t\', quote = FALSE, row.names = FALSE)\n\n\n\n###################################################\n#part 2 - choosing pool of HT elements and plot\n###################################################\n\n#create a data frame with repeatname and species it originates from\nsetwd(\'/Users/chrisbutler/Documents/Horizontal_Transfer\')\nrepeat_ids <- read.csv(\'repeatmasker_headers.csv\', header = FALSE)\nrepeat_ids <- repeat_ids %>% dplyr::select(V1, V2)\nrepeat_ids$V2 <- gsub(\'@\', \'\', repeat_ids$V2)\nrepeat_ids <- rename(repeat_ids, repeat_id = V1)\nrepeat_ids <- rename(repeat_ids, Species = V2)\nrepeat_ids$repeat_id ']",0,"tree species richness, chemical composition, leaves, roots, root exudates, subtropical trees, sampling, raw data, R code, sunburst plots, classyFire, biodiversity, plant biochemistry, ecological interactions."
"Tree species richness differentially affects the chemical composition of leaves, roots and root exudates in four subtropical tree species  - Sampling Raw Data","Sampling Raw Data for the manuscript ""Tree species richness differentially affects the chemical composition of leaves, roots and root exudates in four subtropical tree species "" R Code for producing the sunburst plots from the data obtained by classyFire","['\r\n#### Get PubChem CID ####\r\n\r\n# install libraries\r\n\r\n# load libraries\r\nlibrary(webchem)\r\nlibrary(reshape2)\r\n#---------------------------------#\r\n#read in metaboscape output\r\n#---------------------------------#\r\ndat <- read.csv(file.choose(), sep="","", header = T, stringsAsFactors = F, check.names = F)\r\n\r\n# -------------------------------- #\r\n# Get CIDs from molecule names ; credits to Linnea Catherine Smith for solving the curl issue\r\n# -------------------------------- #\r\n# Do it in chunks of 50 because for some reason when running the whole thing at once it gives the error: \r\n# ""Error in curl::curl_fetch_memory(url, handle = handle) : \r\n#  Error in the HTTP2 framing layer""  but in smaller batches that doesn\'t happen\r\n\r\ncid.from.name <- get_cid(dat$Name[1:50], from = ""name"", match = ""first"", verbose = T)\r\ncid.from.name <- rbind(cid.from.name,get_cid(dat$Name[51:100], from = ""name"", match = ""first"", verbose = T))\r\nfor(i in seq(nrow(cid.from.name)+1,nrow(dat),by=50)){\r\n  cid.from.name <- rbind(cid.from.name,get_cid(dat$Name[i:(i+49)], from = ""name"", match = ""first"", verbose = T))\r\n  print(paste(i,""/"",NROW(dat$Name),sep=""""))\r\n}\r\ncid.from.name <- cid.from.name[1:nrow(dat),]\r\ndat$CID.fromName <- cid.from.name$cid\r\nsapply(dat, class)\r\nwrite.csv(dat,""path/to/your/data.csv"",row.names=F)\r\n\r\n# ------------------------ #\r\n# get inchikey and smiles from pubchem\r\n# ------------------------ #\r\n\r\n# read in or directly use data from previous step\r\n\r\npubchemlist <- dat$cid\r\npubchemlist <- trimws(pubchemlist)\r\nprop_list <- pc_prop(pubchemlist, properties = c(""InchiKey"", ""Inchi"" ,""CanonicalSMILES""),verbose=T)\r\nwrite.csv(prop_list, ""path/to/your/data.csv"",row.names=F)\r\n\r\n############# Classyfire ##################\r\n# read in from previous step\r\n\r\nlibrary(RAMClustR)\r\n\r\nsmiles_info <- getSmilesInchi(dat) # data table from previous step\r\n\r\n# get rid of the list history if applicable\r\ninput <- smiles_info[-7]\r\nclassyfired <- getClassyFire(input, get.all = T, max.wait = 10, posts.per.minute = 5)\r\nresults <- subset(classyfired, select=c(""cmpd.name"", ""CID"", ""inchikey"", ""smiles"", ""classyfire""))\r\n\r\n#results[is.na(results)] <- 0\r\nsetwd(choose.dir())\r\nwrite.csv(results, ""path/to/your/data.csv"",row.names=F)\r\n\r\n# merging of classyfire output with original data table with merge function\r\n\r\n# sum up intensities per group, done by hand\r\n\r\n############ setup data for sunburst plots ###################\r\n\r\n\r\n\r\n#install the packages if necessary\r\nif(!require(""tidyverse"")) install.packages(""tidyverse"")\r\nif(!require(""fs"")) install.packages(""fs"")\r\nif(!require(""readxl"")) install.packages(""readxl"")\r\nif(!require(""writexl"")) install.packages(""writexl"")\r\n\r\n#load packages\r\nlibrary(tidyverse)\r\nlibrary(fs)\r\nlibrary(readxl)\r\nlibrary(writexl)\r\n\r\npath <- choose.files()\r\n\r\nmad <- path %>%\r\n  excel_sheets() %>%\r\n  set_names() %>%\r\n  map(read_excel,\r\n      path = path)\r\n\r\nroots.sum <- aggregate(mad$`Cgla root`$`Summed up intensities`, \r\n                       by=list(classyfire.kingdom=mad$`Cgla root`$classyfire.kingdom,\r\n                               classyfire.superclass=mad$`Cgla root`$classyfire.superclass,\r\n                               classyfire.class=mad$`Cgla root`$classyfire.class), FUN=sum)\r\nleaves.sum <- aggregate(mad$`Cgla leaf`$`Summed up intensities`, \r\n                        by=list(classyfire.kingdom=mad$`Cgla leaf`$classyfire.kingdom,\r\n                                classyfire.superclass=mad$`Cgla leaf`$classyfire.superclass,\r\n                                classyfire.class=mad$`Cgla leaf`$classyfire.class), FUN=sum)\r\nexudate.sum <- aggregate(mad$`Cgla exudate`$`Summed up intensities`, \r\n                         by=list(classyfire.kingdom=mad$`Cgla exudate`$classyfire.kingdom,\r\n                                 classyfire.superclass=mad$`Cgla exudate`$classyfire.superclass,\r\n                                 classyfire.class=mad$`Cgla exudate`$classyfire.class), FUN=sum)\r\n\r\nx <- list(exudate.sum, leaves.sum, roots.sum)\r\nwrite_xlsx(x, path = ""path/to/your/data.xlsx"", col_names=T)']",0,"vaginal bacteria, metagenomic data, metatranscriptomic data, ecology, integrative analyses, datasets, code, figures, France et al 2022."
Insight into the ecology of vaginal bacteria through integrative analyses of metagenomic and metatranscriptomic data,"Datasets and code used to analyze and and to prepare figures for: ""Insight into the ecology of vaginal bacteria through integrative analyses of metagenomic and metatranscriptomic data"", France et al 2022.","['library(\'variancePartition\')\nlibrary(\'edgeR\')\nlibrary(\'BiocParallel\')\n\nsetwd(""~/bioinformatics_analysis/PGATES/MT/MT_core_diff/"")\n\ndata_test <- read.csv(""MT_UMD_KEGGsums.txt"",sep=""\\t"",row.names = 1)\nfilter = rowSums(cpm(data_test)>0.1) >= 5\n\ngeneExpr = DGEList( data_test[filter,] )\ngeneExpr = calcNormFactors( geneExpr )\n\nmetadata <- read.csv(""MT_core_key.csv"",sep="","",row.names = 1)\n\nparam = SnowParam(4, ""SOCK"", progressbar=TRUE)\n \nregister(param)\n\n\n# The variable to be tested must be a fixed effect\nform <- ~ 0 + CST_MT_red + (1|Subject) \n\n# estimate weights using linear mixed model of dream\nvobjDream = voomWithDreamWeights(geneExpr, form, metadata)\n\n# Fit the dream model on each gene\n# By default, uses the Satterthwaite approximation for the hypothesis test\nfitmm = dream( vobjDream, form, metadata )\nL1 = getContrast( vobjDream, form, metadata, c(""CST_MT_redI"", ""CST_MT_redIV""))\nL2 = getContrast( vobjDream, form, metadata, c(""CST_MT_redI"", ""CST_MT_redIII""))\nL3 = getContrast( vobjDream, form, metadata, c(""CST_MT_redIII"", ""CST_MT_redIV""))\nL = cbind(L1,L2,L3)\n\nfit = dream( vobjDream, form, metadata,L,ddf=""Kenward-Roger"")\n\n# Get results of hypothesis test on coefficients of interest\ncst_I_IV <- topTable(fit, coef=\'L1\',number = 6000,sort.by = ""logFC"")\ncst_I_III <- topTable(fit, coef=\'L2\',number = 6000,sort.by = ""logFC"")\ncst_III_IV <- topTable(fit, coef=\'L3\',number = 6000,sort.by = ""logFC"")\n\n\n#pathway enrichment\ndrops <- c(""AveExpr"",""t"",""P.Value"",""z.std"")\nCST_I_IV_red <- cst_I_IV[ , !(names(cst_I_IV) %in% drops)]\nCST_I_IV_red$GeneID <- rownames(CST_I_IV_red)\nCST_I_IV_red <- CST_I_IV_red[,c(3,1,2)]\n\nform2 = ~ (1|Subject) + (1|CST_MT_red)\nvp = fitExtractVarPartModel(vobjDream, form2, metadata)\n\nplotVarPart(sortCols(vp))\n\nwrite.csv(cst_I_IV,""MT_cstI_IV_kegg_de.csv"",sep="","")\nwrite.csv(cst_I_III,""MT_cstI_III_kegg_de.csv"",sep="","")\nwrite.csv(cst_III_IV,""MT_cstIII_IV_kegg_de.csv"",sep="","")\nwrite.csv(vp,""MT_kegg_variance_partition.csv"")\n\n####MG data analysis\ndata_test <- read.csv(""MG_UMD_KEGGsums.txt"",sep=""\\t"",row.names = 1)\nfilter = rowSums(cpm(data_test)>0.1) >= 5\n\ngeneExpr = DGEList( data_test[filter,] )\ngeneExpr = calcNormFactors( geneExpr )\n\nmetadata <- read.csv(""MT_core_key.csv"",sep="","",row.names = 1)\n\nparam = SnowParam(4, ""SOCK"", progressbar=TRUE)\n\nregister(param)\n\n\n# The variable to be tested must be a fixed effect\nform <- ~ 0 + CST_MG_red + (1|Subject) \n\n# estimate weights using linear mixed model of dream\nvobjDream = voomWithDreamWeights(geneExpr, form, metadata)\n\n# Fit the dream model on each gene\n# By default, uses the Satterthwaite approximation for the hypothesis test\nfitmm = dream( vobjDream, form, metadata )\nL1 = getContrast( vobjDream, form, metadata, c(""CST_MG_redI"", ""CST_MG_redIV""))\nL2 = getContrast( vobjDream, form, metadata, c(""CST_MG_redI"", ""CST_MG_redIII""))\nL3 = getContrast( vobjDream, form, metadata, c(""CST_MG_redIII"", ""CST_MG_redIV""))\nL = cbind(L1,L2,L3)\n\nfit = dream( vobjDream, form, metadata,L)\n\n# Get results of hypothesis test on coefficients of interest\ncst_I_IV <- topTable(fit, coef=\'L1\',number = 6000,sort.by = ""logFC"")\ncst_I_III <- topTable(fit, coef=\'L2\',number = 6000,sort.by = ""logFC"")\ncst_III_IV <- topTable(fit, coef=\'L3\',number = 6000,sort.by = ""logFC"")\n\n\n#pathway enrichment\ndrops <- c(""AveExpr"",""t"",""P.Value"",""z.std"")\nCST_I_IV_red <- cst_I_IV[ , !(names(cst_I_IV) %in% drops)]\nCST_I_IV_red$GeneID <- rownames(CST_I_IV_red)\nCST_I_IV_red <- CST_I_IV_red[,c(3,1,2)]\n\nform2 = ~ (1|Subject) + (1|CST_MT_red)\nvp = fitExtractVarPartModel(vobjDream, form2, metadata)\n\nplotVarPart( sortCols(vp))\n\nwrite.csv(cst_I_IV,""MG_cstI_IV_kegg_de.csv"",sep="","")\nwrite.csv(cst_I_III,""MG_cstI_III_kegg_de.csv"",sep="","")\nwrite.csv(cst_III_IV,""MG_cstIII_IV_kegg_de.csv"",sep="","")\nwrite.csv(vp,""MG_kegg_variance_partition.csv"")', 'library(""nlme"")\nrequire(""multcomp"")\nsetwd(""~/bioinformatics_analysis/PGATES/MT/MG_comparison/"")\nrela_data <- read.csv(""species_relative_exp.csv"",sep="","")\n\nrela_data_trim <- rela_data[rela_data$Read_keep_5mil != 0, ]\n\nAR1<-lme(Ratio ~Species + LogAbund + LogAbund*Species, random=~1|SID/UID,\n       data=rela_data_trim, na.action = (na.omit), method = ""REML"")\nsummary(AR1)\nanova(AR1)\nTukeyHSD(AR1)\n', 'setwd(""~/bioinformatics_analysis/MT_MG_paper/Figures/Predict_updown/"")\npredict_data <- read.csv(""all_species_expr_predict.csv"")\npredict_data$Compar <- as.factor(predict_data$Compar)\npredict_data$Species <- as.factor(predict_data$Species)\n\nAR2<-lme(LogChange_abs ~ LogPriorExpr + Species + LogPriorExpr*Species, random=~1|SID/Compar,\n         data=predict_data, na.action = (na.omit), method = ""REML"")\n\nsummary(AR2)\nanova(AR2)\npost_hoc <- summary(glht(AR2, linfct=mcp(Species=""Tukey"")), test = adjusted(type = ""hochberg""))\npost_hoc_cld <- cld(post_hoc,level=0.05)\npost_hoc_cld', 'library(""PMA"")\nlibrary(""kernlab"")\nlibrary(\'variancePartition\')\nlibrary(\'edgeR\')\nlibrary(\'BiocParallel\')\nlibrary(""ade4"")\n\nsetwd(""~/bioinformatics_analysis/PGATES/MT/MT_species_corr/tpm_analysis/"")\n\n\n#######Lactobacillus crispatus\ny_data <- read.csv(""./input_files/Lactobacillus_crispatus_50cut_TPM.csv"",sep="","")\n\ny_data_trim <- y_data[,-1]\nrownames(y_data_trim) <- y_data[,1]\ny_data_trim <- y_data_trim[ rowSums(y_data_trim)!=0, ]\n#y_data_trim <- y_data_trim[rowMeans(y_data_trim!=0)>0.1,]\ny_data_trim.scale <- data.frame(scale(t(y_data_trim)))\n\nkeeps <- colnames(y_data[-1])\nx_data <- read.csv(""input_files/MG_taxa_104_cut_CLR.csv"",sep="","")\nx_data_trim <- x_data[ , (names(x_data) %in% keeps)]\nrownames(x_data_trim) <- x_data[,1]\nx_data_trim <- x_data_trim[ rowSums(x_data_trim)!=0,]\n#x_data_trim <- x_data_trim[rowMeans(x_data_trim!=0)>0.75,]\n#x_data_trim <- x_data_trim * 1000000\nx_data_trim.scale <- data.frame(scale(t(x_data_trim)))\n\npca1 = dudi.pca(x_data_trim.scale, scal = FALSE,scann=FALSE)\npca2 = dudi.pca(y_data_trim.scale, scal = FALSE, scann = FALSE)\n\nrv1 = RV.rtest(pca1$tab, pca2$tab, 999)\nrv1\n\ncca_output_perm <- CCA.permute(x=t(y_data_trim),z=t(x_data_trim),typex = ""standard"",typez = ""standard"",niter=100,trace=TRUE,standardize=TRUE,penaltyxs =c(0.05,0.05,0.05,0.05,0.05,0.05,0.075,0.075,0.075,0.075,0.075,0.075,0.1,0.1,0.1,0.1,0.1,.1,0.125,0.125,0.125,0.125,0.125,.125,0.15,0.15,0.15,0.15,0.15,.15,.2,.2,.2,.2,.2,.2),penaltyzs = c(0.2,0.3,0.4,0.5,0.6,0.7,0.2,0.3,0.4,0.5,0.6,0.7,0.2,0.3,0.4,0.5,0.6,0.7,0.2,0.3,0.4,0.5,0.6,0.7,0.2,0.3,0.4,0.5,0.6,0.7,0.2,0.3,0.4,0.5,0.6,0.7))\ncca_output_perm\nplot(cca_output_perm)\ncca_output <- CCA(x=t(y_data_trim),z=t(x_data_trim),typex=""standard"",typez=""standard"",penaltyx=0.1,penaltyz=0.2,niter=10000,trace=TRUE,standardize=TRUE,K=3)\ncca_output\n\nscore.x <- scale(t(y_data_trim)) %*% cca_output$u\nscore.y <- scale(t(x_data_trim)) %*% cca_output$v\ndiag(cor(score.x, score.y))\n\nx <- cca_output[""u""] \nx <- data.frame(x[1])\nrow.names(x) <- row.names((y_data_trim))\n\nwrite.csv(x,""./output_files/Lactobacillus_crispatus_CCA_VOG.csv"")\n\ny <- cca_output[""v""] \ny <- data.frame(y[1])\nrow.names(y) <- row.names((x_data_trim))\n\nwrite.csv(y,""./output_files/Lactobacillus_crispatus_CCA_VOGspecies.csv"")\n\nx_data_trim_pred = x_data_trim * y$v.1\nx_pred <- colSums(x_data_trim_pred)\ny_data_trim_pred = y_data_trim * x$u.1\ny_pred <- colSums(y_data_trim_pred)\nplot(y_pred,x_pred,pch=16)\n\npredicted_scores <- data.frame(x_pred,y_pred)\nwrite.csv(predicted_scores,""./output_files/Lactobacillus_crispatus_axesCov.csv"")\n\n\n#plotting?\nrownames(x_data_trim)[which(cca_output$v[,1]>0)]\ncombined = cbind(t(y_data_trim[cca_output$u[,1] != 0, ]),\n                 t(x_data_trim[cca_output$v[,1] != 0, ]))\n\npcaRes = dudi.pca(combined, scannf= TRUE,center=TRUE,scale=TRUE)\n\ngenotype     = substr(rownames(pcaRes$li), 1, 2)\nsampleType  = substr(rownames(pcaRes$l1), 3, 4)\nfeatureType = c(rep(""Gene"",41),rep(""Taxa"",2))\nsampleInfo  = data.frame(pcaRes$li, genotype, diet=sampleType)\nfeatureInfo = data.frame(pcaRes$c1,\n                         feature = substr(colnames(combined), 1, 6))\nlibrary(ggplot2)\nlibrary(ggrepel)\nggplot() +  geom_point(data = sampleInfo,\n                       aes(x = Axis1, y = Axis2), size = 3) +\n  geom_label_repel(data = featureInfo,\n                   aes(x = 5.5 * CS1, y = 5.5 * CS2, label = feature, fill = featureType),\n                   size = 2, segment.size = 0.3,\n                   label.padding = unit(0.1, ""lines""), label.size = 0) +\n  geom_point(data = featureInfo,\n             aes(x = 5.5 * CS1, y = 5.5 * CS2, fill = featureType),\n             size = 1, shape = 23, col = ""#383838"") +\n  scale_color_brewer(palette = ""Set2"") +\n  scale_fill_manual(values = c(""#a6d854"", ""#e78ac3"")) +\n  guides(fill = guide_legend(override.aes = list(shape = 32, size = 0))) +\n  coord_fixed()+\n  labs(x = sprintf(""Axis1 [%s%% Variance]"",\n                   100 * round(pcaRes$eig[1] / sum(pcaRes$eig), 2)),\n       y = sprintf(""Axis2 [%s%% Variance]"",\n                   100 * round(pcaRes$eig[2] / sum(pcaRes$eig), 2)),\n       fill = ""Feature Type"", col = ""Sample Type"")\n\npca_component_coord <- pcaRes$co\npca_components <- pcaRes$li\n\nwrite.csv(pca_components,""Lcrispatus_pcaAxes.csv"")\nwrite.csv(pca_component_coord,""Lcrispatus_pcaCoords.csv"")\n\n\n#########Lactobacillus iners\ny_data <- read.csv(""./input_files/Lactobacillus_iners_50cut_TPM.csv"",sep="","")\n\ny_data_trim <- y_data[,-1]\nrownames(y_data_trim) <- y_data[,1]\ny_data_trim <- y_data_trim[ rowSums(y_data_trim)!=0, ]\n#y_data_trim <- y_data_trim[rowMeans(y_data_trim!=0)>0.1,]\ny_data_trim.scale <- data.frame(scale(t(y_data_trim)))\n\nkeeps <- colnames(y_data[-1])\nx_data <- read.csv(""./input_files/MG_taxa_104_cut_CLR.csv"",sep="","")\nx_data_trim <- x_data[ , (names(x_data) %in% keeps)]\nrownames(x_data_trim) <- x_data[,1]\nx_data_trim <- x_data_trim[ rowSums(x_data_trim)!=0,]\n#x_data_trim <- x_data_trim[rowMeans(x_data_trim!=0)>0.75,]\n#x_data_trim <- x_data_trim * 1000000\nx_data_trim.scale <- data.fram']",0,"    Vaginal microbiome
    Bacterial ecology
    Metagenomics
    Metatranscriptomics
    Integrative analysis
    Datasets
    Code
    Research study
    Microbial diversity
    Gene expression
    Microbial metabolism
    Microbial interactions
    Microbial communities
    DNA sequencing
    RNA sequencing
    Taxonomic profiling
    Functional profiling
    Microbiota-host interactions
    Health and disease."
"Codes in R for spatial statistics analysis, ecological response models and spatial distribution models","In the last decade, a plethora of algorithms have been developed for spatial ecology studies. In our case, we use some of these codes for underwater research work in applied ecology analysis of threatened endemic fishes and their natural habitat. For this, we developed codes in Rstudio script environment to run spatial and statistical analyses for ecological response and spatial distribution models (e.g., Hijmans & Elith, 2017; Den Burg et al., 2020). The employed R packages are as follows: caret (Kuhn et al., 2020), corrplot (Wei & Simko, 2017), devtools (Wickham, 2015), dismo (Hijmans & Elith, 2017), gbm (Freund & Schapire, 1997; Friedman, 2002), ggplot2 (Wickham et al., 2019), lattice (Sarkar, 2008), lattice (Musa & Mansor, 2021), maptools (Hijmans & Elith, 2017), modelmetrics (Hvitfeldt & Silge, 2021), pander (Wickham, 2015), plyr (Wickham & Wickham, 2015), pROC (Robin et al., 2011), raster (Hijmans & Elith, 2017), RColorBrewer (Neuwirth, 2014), Rcpp (Eddelbeuttel & Balamura, 2018), rgdal (Verzani, 2011), sdm (Naimi & Araujo, 2016), sf (e.g., Zainuddin, 2023), sp (Pebesma, 2020) and usethis (Gladstone, 2022).It is important to follow all the codes in order to obtain results from the ecological response and spatial distribution models. In particular, for the ecological scenario, we selected the Generalized Linear Model (GLM) and for the geographic scenario we selected DOMAIN, also known as Gower's metric (Carpenter et al., 1993). We selected this regression method and this distance similarity metric because of its adequacy and robustness for studies with endemic or threatened species (e.g., Naoki et al., 2006). Next, we explain the statistical parameterization for the codes immersed in the GLM and DOMAIN running:In the first instance, we generated the background points and extracted the values of the variables (Code2_Extract_values_DWp_SC.R). Barbet-Massin et al. (2012) recommend the use of 10,000 background points when using regression methods (e.g., Generalized Linear Model) or distance-based models (e.g., DOMAIN). However, we considered important some factors such as the extent of the area and the type of study species for the correct selection of the number of points (Pers. Obs.). Then, we extracted the values of predictor variables (e.g., bioclimatic, topographic, demographic, habitat) in function of presence and background points (e.g., Hijmans and Elith, 2017).Subsequently, we subdivide both the presence and background point groups into 75% training data and 25% test data, each group, following the method of Sobern & Nakamura (2009) and Hijmans & Elith (2017). For a training control, the 10-fold (cross-validation) method is selected, where the response variable presence is assigned as a factor. In case that some other variable would be important for the study species, it should also be assigned as a factor (Kim, 2009).After that, we ran the code for the GBM method (Gradient Boost Machine; Code3_GBM_Relative_contribution.R and Code4_Relative_contribution.R), where we obtained the relative contribution of the variables used in the model. We parameterized the code with a Gaussian distribution and cross iteration of 5,000 repetitions (e.g., Friedman, 2002; kim, 2009; Hijmans and Elith, 2017). In addition, we considered selecting a validation interval of 4 random training points (Personal test). The obtained plots were the partial dependence blocks, in function of each predictor variable.Subsequently, the correlation of the variables is run by Pearson's method (Code5_Pearson_Correlation.R) to evaluate multicollinearity between variables (Guisan & Hofer, 2003). It is recommended to consider a bivariate correlation  0.70 to discard highly correlated variables (e.g., Awan et al., 2021).Once the above codes were run, we uploaded the same subgroups (i.e., presence and background groups with 75% training and 25% testing) (Code6_Presence&backgrounds.R) for the GLM method code (Code7_GLM_model.R). Here, we first ran the GLM models per variable to obtain the p-significance value of each variable (alpha  0.05); we selected the value one (i.e., presence) as the likelihood factor. The generated models are of polynomial degree to obtain linear and quadratic response (e.g., Fielding and Bell, 1997; Allouche et al., 2006). From these results, we ran ecological response curve models, where the resulting plots included the probability of occurrence and values for continuous variables or categories for discrete variables. The points of the presence and background training group are also included.On the other hand, a global GLM was also run, from which the generalized model is evaluated by means of a 2 x 2 contingency matrix, including both observed and predicted records. A representation of this is shown in Table 1 (adapted from Allouche et al., 2006). In this process we select an arbitrary boundary of 0.5 to obtain better modeling performance and avoid high percentage of bias in type I (omission) or II (commission) errors (e.g., Carpenter et al., 1993; Fielding and Bell, 1997; Allouche et al., 2006; Kim, 2009; Hijmans and Elith, 2017).Table 1. Example of 2 x 2 contingency matrix for calculating performance metrics for GLM models. A represents true presence records (true positives), B represents false presence records (false positives - error of commission), C represents true background points (true negatives) and D represents false backgrounds (false negatives - errors of omission). Validation setModelTrueFalsePresenceABBackgroundCDWe then calculated the Overall and True Skill Statistics (TSS) metrics. The first is used to assess the proportion of correctly predicted cases, while the second metric assesses the prevalence of correctly predicted cases (Olden and Jackson, 2002). This metric also gives equal importance to the prevalence of presence prediction as to the random performance correction (Fielding and Bell, 1997; Allouche et al., 2006).The last code (i.e., Code8_DOMAIN_SuitHab_model.R) is for species distribution modelling using the DOMAIN algorithm (Carpenter et al., 1993). Here, we loaded the variable stack and the presence and background group subdivided into 75% training and 25% test, each. We only included the presence training subset and the predictor variables stack in the calculation of the DOMAIN metric, as well as in the evaluation and validation of the model.Regarding the model evaluation and estimation, we selected the following estimators:1) partial ROC, which evaluates the approach between the curves of positive (i.e., correctly predicted presence) and negative (i.e., correctly predicted absence) cases. As farther apart these curves are, the model has a better prediction performance for the correct spatial distribution of the species (Manzanilla-Quiones, 2020).2) ROC/AUC curve for model validation, where an optimal performance threshold is estimated to have an expected confidence of 75% to 99% probability (De Long et al., 1988).","['# FILTRADO DE REGISTROS ATÍPICOS CLIMÁTICOS\r\n\r\nlibrary(raster)\r\nlibrary(sp)\r\nlibrary(RColorBrewer)\r\n\r\n# Cargar archivo con puntos\r\nsetwd(""D:/TDWR_LB_ML/R"")\r\nOcc<-read.csv(""./ARCHIVOS/DATOS-OCURRENCIAS/VER_2019/ADULTOS/Occ_records_DOMAIN_At_Ad_19.csv"", header = TRUE, sep = "","")\r\nnames(Occ)\r\nView(Occ)\r\n\r\n\r\n# variables usadas para detectar atipicos\r\n\r\nWDp <- raster(""./VARIABLES3_ML_VER19/CONTINUOUS VARIABLE_WDp_ML_VER19_0.1M.tif"")\r\n\r\nSC <- raster(""./VARIABLES3_ML_VER19/CATEGORICAL VARIABLE_UC_ML_VER19_0.1M.tif"")\r\n\r\n\r\n\r\n\r\nrast_var<- stack(WDp, SC)\r\nrast_var\r\n\r\nplot(rast_var,1, col= brewer.pal(6, ""Blues""), main=""Digital bathymetric model (DBM)"")\r\nplot(rast_var,2, col = brewer.pal(9, ""RdYlGn""), main=""Subacuatic coverage"")\r\n\r\n# Extracci?n de valores\r\n\r\nnames(Occ)\r\nsummary(Occ)\r\ndatos2<-Occ[,c(2:3)]\r\nhead(datos2)\r\nextent(-11135127, -11133410, 2494679, 2495150)\r\n\r\nclim_Occ<-extract(rast_var, datos2)\r\nclim_Occ\r\npc_Occ<-cbind(Occ, clim_Occ)\r\nView(pc_Occ)\r\nsummary(pc_Occ)\r\npc_Occ=na.omit(pc_Occ)\r\n\r\n# eliminar las filas con NA\r\npc2<- subset(pc_Occ, !is.na())\r\ndim(pc2)\r\nView(pc2)\r\n\r\nsetwd(""D:/TDWR_LB_ML/PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri/ENVIROMENTAL FILTER"")\r\nwrite.csv(pc_Occ, ""At_Ad_EF_2019.csv"")\r\npc2_Occ<-read.csv(""At_Ad_EF_2019.csv"", header = TRUE, sep = "","")\r\nhead(pc2_Occ)\r\n\r\n##Funcion para encontrar atipicos\r\nFindOutliers <- function(data) {\r\n  lowerq = quantile(data)[2]\r\n  upperq = quantile(data)[4]\r\n  iqr = upperq - lowerq #Or use IQR(data)\r\n  # we identify extreme outliers\r\n  extreme.threshold.upper = (iqr * 1.5) + upperq\r\n  extreme.threshold.lower = lowerq - (iqr * 1.5)\r\n  result <- which(data > extreme.threshold.upper | data < extreme.threshold.lower)\r\n}\r\n\r\n(WDp<-FindOutliers (pc2_Occ$INTERPOLACIÓN_PROF_ML_VER19_FINAL_0.1M))\r\n(SC<-FindOutliers (pc2_Occ$CLASIFICACIÓN_VEG_ML_VER19_FINAL_ACT_0.1M))\r\n\r\n\r\n\r\n\r\nWDp<-as.data.frame(WDp)\r\nhead(WDp)\r\n\r\nlibrary(ggplot2)\r\n\r\natipicos<-ggplot()+\r\n  geom_point(data=pc2_Occ, aes(x=INTERPOLACIÓN_PROF_ML_VER19_FINAL_0.1M, y=CLASIFICACIÓN_VEG_ML_VER19_FINAL_ACT_0.1M))+\r\n  geom_label(data=pc2_Occ, aes(x=INTERPOLACIÓN_PROF_ML_VER19_FINAL_0.1M, y=CLASIFICACIÓN_VEG_ML_VER19_FINAL_ACT_0.1M, label = ID))\r\nnames(pc2_Occ)\r\n\r\natipicos\r\n', '##1. Load libraries\r\nlibrary(devtools)\r\nlibrary(usethis)\r\nlibrary(ggplot2)\r\nlibrary(lattice)\r\nlibrary(plyr)\r\n\r\n##2. Set work directory\r\nsetwd(""D:/TDWR_LB_ML/PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri"")\r\n\r\n##3. Load file\r\nPB2<- read.csv(""Occ_records_by_sector&period_At_Ad_Ju.csv"")\r\nPB2\r\nhead(PB2)\r\n\r\nsummary(PB2)\r\n\r\n##4. Load a font from Windows System\r\nwindowsFonts(A = windowsFont(""Times New Roman"")) \r\n\r\n##5. Order the levels and categories\r\nPB2$Life_stage<- ordered(PB2$Life_stage, levels = c(""Adult"", ""Juvenile""))\r\nLS<-PB2$Life_stage\r\n\r\n\r\nPB2$Sector<- ordered(PB2$Sector, levels = c(""S1"", ""S2"", ""S3"", ""S4"", ""S5"", ""S6"", ""S7"", ""S8"", ""S9"", ""S10"",""S11"", ""S12"", ""S13"", ""S14"", ""S15""))\r\nLSS<-PB2$Sector\r\n\r\n\r\n##6. Plot the presence records by life stage, sector and year\r\nb<- ggplot(data = PB2, aes(fill= Period, x=Sector, y=Ocurrences))\r\n\r\nwindows ();b +  geom_bar( position = ""dodge"", stat = ""identity"", width = 0.7, colour=""black"") + \r\n  facet_grid(Life_stage~Period, scales=""fixed"", space = ""free"") + theme_gray() +##+ geom_label(aes(label = Presence), nudge_y = 3)+\r\n  labs(#title = ""Presence records of each sector"", subtitle = ""Per life stages and study period"", #\r\n    x = ""Sector"", y = ""Occurrence records"", col = ""Period"") + \r\n  theme(title = element_text(color = ""grey10"", size = 20, angle = 0, hjust = .5, vjust = -2, face = ""bold""),\r\n        axis.title.x = element_text(family= ""A"", colour = ""grey10"", size = 24, angle = 0, hjust = .5, vjust = -0.5, face = ""bold""),\r\n        axis.text.x = element_text(family= ""A"", colour = ""grey20"", size = 21, angle = -90, hjust = .5, vjust = 0.5, face = ""plain""),\r\n        axis.title.y = element_text(family= ""A"", colour = ""grey10"", size = 24, angle = 90, hjust = .5, vjust = 1.5, face = ""bold""),\r\n        axis.text.y = element_text(family= ""A"", colour = ""grey20"", size = 21, angle = 0, hjust = .5, vjust = 0, face = ""plain""),\r\n        strip.text.x = element_text(family= ""A"", colour=""grey10"", size = 24, angle = 0, hjust = .5, vjust = .7, face = ""plain""),\r\n        strip.text.y = element_text(family= ""A"", colour=""grey10"", size = 24, angle = -90, hjust = .5, vjust = .7, face = ""plain""),\r\n        panel.grid.major.x = element_line(),\r\n        panel.grid.minor.x = element_line()) + theme(legend.title = element_text(family= ""A"", colour= ""grey10"", size = 28, face= ""bold""),\r\n                                                     legend.text = element_text(family= ""A"", colour=""black"", size=20, face=""plain""),\r\n                                                     legend.background = element_rect(fill=""grey90"", linetype=""dashed"", colour =""black"", size=NULL),\r\n                                                     legend.position = ""none"", legend.spacing.y = unit(1.5, ""char""), legend.spacing.x = unit(.5, ""char""),  legend.margin = margin(c(20, 20, 20, 14))) +\r\n  scale_fill_manual("" Period"", labels= c(""1999"", ""2009"", ""2019""), values = c(\'red\',\'green\', \'blue\')) + scale_y_continuous(breaks = c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110), limits = c(0,120), expand = c(0,0))+\r\n  stat_smooth(aes(y= Ocurrences, group = LS, linetype= Life_Stage), \r\n              method = ""lm"", formula = y ~ poly(x,2), \r\n              se= F, orientation = ""x"", span = 6,\r\n              linetype= ""dashed"", color= ""black"", size= 0.7,\r\n              level = 0.95) + geom_segment(aes(x = ""S1"", y = 0, xend = ""S14"", yend = 0)) \r\n\r\n', '##0. Reset environment\r\nrm(list=ls())\r\n\r\n##1. Load libraries\r\nlibrary(sp)\r\nlibrary(raster)\r\nlibrary(dismo)     # package to run the model\r\nlibrary(maptools)\r\nlibrary(rgdal)\r\nlibrary(RColorBrewer)\r\nlibrary(Rcpp)\r\nlibrary(ggplot2)\r\nlibrary(lattice)\r\nlibrary(caret)\r\nlibrary(pROC)\r\nlibrary(sdm)\r\n\r\n##2. Load word directory\r\nsetwd(""D:/TDWR_LB_ML/R/VARIABLES_ML_VER09"")\r\n\r\n\r\n##3. Variables stack\r\ndatafiles = Sys.glob(""*.tif"") #Or whatever identifies your files\r\nstck = stack() #empty stack for raster\r\nfor(i in 1:NROW(datafiles)){\r\n  tempraster = raster(datafiles[i])\r\n  stck = stack(stck,tempraster)\r\n}\r\n\r\nstck #raster predictors as a stackplot\r\n\r\n##4. To show variables\r\nplot(stck,1, col= brewer.pal(6, ""RdYlGn""), main=""Underwater coverage"")\r\nplot(stck,2, col = brewer.pal(9, ""Blues""), main=""Water depth (m)"")\r\n\r\n\r\n##5. Load presence data\r\ndato= read.csv(""D:/TDWR_LB_ML/PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri/DOMAIN/Occ_records_DOMAIN_At_Ad_19.csv"")\r\nhead(dato)\r\ndato1= dato[,-1]#first column not needed\r\nhead(dato1)\r\next = extent(-11135127, -11133410, 2494679, 2495150)\r\n\r\npoints(dato1, col=\'red\', pch=\'+\') \r\n\r\n## Extract values presence-variables \r\nprs1= extract(stck, dato1)\r\n\r\nhead(prs1)\r\n\r\nset.seed(1)\r\n\r\n##6. Generate backgrounds and extract values background-variables \r\nbackgr = randomPoints(stck, 500, ext = ext) #500 random points, 80-20 ratio\r\nabsvals = extract(stck, backgr) #choose absence values from the background\r\npb = c(rep(1, nrow(prs1)), rep(0, nrow(absvals)))\r\nsdmdata = data.frame(cbind(pb, rbind(prs1, absvals)))\r\n\r\nhead(sdmdata)\r\n\r\nsdmdata=na.omit(sdmdata)\r\nsummary(sdmdata)\r\n\r\ntail(sdmdata)\r\n\r\n##7. To save the new data frame with presence + background points\r\nsetwd(""D:/TDWR_LB_ML/PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri/RELATIVE_INFLUENCE"")\r\nwrite.csv(sdmdata,""GBM_rel_contribution_At_Ad_19.csv"")\r\n', '##0. Reset enviroment\r\nrm(list=ls())\r\n\r\n##1. Load libraries\r\nlibrary(sp)\r\nlibrary(raster)\r\nlibrary(dismo)     # package to run the model\r\nlibrary(maptools)\r\nlibrary(rgdal)\r\nlibrary(RColorBrewer)\r\nlibrary(Rcpp)\r\nlibrary(ggplot2)\r\nlibrary(lattice)\r\nlibrary(caret)\r\nlibrary(pROC)\r\nlibrary(sdm)\r\nlibrary(gbm)\r\nlibrary(ModelMetrics)\r\n\r\n##2. Load word directory\r\nsetwd(""D:/TDWR_LB_ML/PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri/RELATIVE_INFLUENCE"")\r\n\r\n##3. Load presence data\r\npa=read.csv(""GBM_rel_contribution_At_Ad_09.csv"")\r\nhead(pa)\r\npa1= pa[,-1]#first column not needed\r\nhead(pa1)#1 --> species occurrence\r\ntail(pa1)\r\next = extent(-11135127, -11133410, 2494679, 2495150)\r\n\r\nlibrary(caret)\r\nset.seed(1) #pseudo-repeatability\r\ntrainIndex = createDataPartition(pa1$pb, p = .8, list = FALSE, times = 1) #y as basis of splitting, p es calculada mediante\r\n## regla de Thumb (Huberty, 1994) para determinar el radio de datos de prueba (test) p= [1+(p-1)^1/2]^-1\r\n## This approximates to a training set consisting of 75% of the cases when p > 10.\r\n\r\ntraining = pa1[ trainIndex,] #75% data for model training\r\ntesting= pa1[-trainIndex,] #25% for model testing\r\n\r\nhead(training)\r\ntail(training)\r\n\r\nset.seed(825)\r\n\r\n#y is pb\r\npb=as.factor(training$pb) #1 stands for presence and 0 for absence\r\nveg=as.factor(training$SC) #Vegetation categories are categorical\r\n\r\n### Partial dependence plots\r\nlibrary(gbm)\r\n\r\nboost= gbm(pb~.,data=training,distribution = \'gaussian\', n.trees = 5000, interaction.depth = 4)\r\n\r\nsummary(boost, xlim= c(0,100), ylab = ""Predictor variables"")\r\n\r\n\r\n##plot(boost, i=\'CATEGORICAL VARIABLE_UC_ML_VER09_0.1M\')\r\n##plot(boost, i= \'CONTINUOUS VARIABLE_WDp_ML_VER09_0.1M\')', '##1. Load libraries\r\nlibrary(devtools)\r\nlibrary(usethis)\r\nlibrary(ggplot2)\r\nlibrary(lattice)\r\nlibrary(plyr)\r\nremotes::install_github(""coolbutuseless/ggpattern"")\r\nlibrary(ggpattern)\r\n\r\n\r\n##2. Set work directory\r\nsetwd(""D:/TDWR_LB_ML/PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri/RELATIVE_INFLUENCE"")\r\n\r\n##3. Load file\r\nDB<- read.csv(""Summary_GBM_At_Ad_Ju.csv"")\r\nDB\r\nhead(DB)\r\n\r\nsummary(DB)\r\n\r\n##4. Load a font from Windows System\r\nwindowsFonts(A = windowsFont(""Times New Roman"")) \r\n\r\n##5. Order the levels and categories\r\nDB$Life_stage<- ordered(DB$Life_stage, levels = c(""Adult"", ""Juvenile""))\r\nLS<-DB$Life_stage\r\n\r\n\r\nDB$Variable<- ordered(DB$Variable, levels = c(""WDp"", ""UC""))\r\nLSS<-DB$Variable\r\n\r\n##6. Plot the contributión of each predictor variables by year and life stage\r\nwindows(); ggplot(data = DB, aes(fill= LSS, x=LSS, y=Rel_inf))+  \r\n  geom_bar_pattern( position = position_dodge(preserve = ""single""), stat = ""identity"", width = 1.0, color=""black"", pattern_fill = ""white"", pattern_angle = 35, pattern_density = 0.1, pattern_spacing = 0.1, pattern_key_scale_factor = 0.6)+\r\n  geom_label(aes(label=Rel_inf), family= ""A"", colour = ""gray"", size = 10, angle = 0, hjust = .5, vjust=2.4)+\r\n  geom_text(aes(label=round(Rel_inf, digits = 3)), position=position_dodge(width=1),family= ""A"", colour = ""black"", size = 8, angle = 0, hjust = .5, vjust=3.5)+\r\n  geom_text(aes(label= round(Rel_inf, digits = 3)), position=position_dodge(width=0.2),family= ""A"", colour = ""black"", size = 8, angle = 0, hjust = .5, vjust=-1.96)+\r\n  facet_grid(Life_stage~Period, scales=""fixed"", space = ""free"") + theme_grey()+\r\n  labs(x = ""Predictor variables"", y = ""Contribution (%)"", col = ""Year"")+\r\n  theme(title = element_text(color = ""black"", size = 20, angle = 0, hjust = .5, vjust = -2, face = ""bold""),\r\n        axis.title.x = element_text(family= ""A"", colour = ""black"", size = 26, angle = 0, hjust = .5, vjust = -0.5, face = ""bold""),\r\n        axis.text.x = element_text(family= ""A"", colour = ""grey20"", size = 21, angle = 0, hjust = .5, vjust = 0.5, face = ""plain""),\r\n        axis.title.y = element_text(family= ""A"", colour = ""grey10"", size = 26, angle = 90, hjust = .5, vjust = 1.5, face = ""bold""),\r\n        axis.text.y = element_text(family= ""A"", colour = ""grey20"", size = 21, angle = 0, hjust = .5, vjust = .1, face = ""plain""),\r\n        strip.text.x = element_text(family= ""A"", colour=""black"", size = 26, angle = 0, hjust = .5, vjust = .7, face = ""plain""),\r\n        strip.text.y = element_text(family= ""A"", colour=""black"", size = 26, angle = -90, hjust = .5, vjust = .7, face = ""plain""),\r\n        panel.grid.major.x = element_line(),\r\n        panel.grid.minor.x = element_line()) + theme(legend.title = element_text(family= ""A"", colour= ""grey10"", size = 28, face= ""bold""),\r\n                                                     legend.text = element_text(family= ""A"", colour=""black"", size=20, face=""plain""),\r\n                                                     legend.background = element_rect(fill=""black"", linetype=""dashed"", colour =""black"", size=NULL),\r\n                                                     legend.position = ""none"", legend.spacing.y = unit(1.5, ""char""), legend.spacing.x = unit(.5, ""char""),  legend.margin = margin(c(20, 20, 20, 14))) +\r\n  scale_fill_manual("" Variable"", labels= c(""WDp"", ""UC""), values = c(\'gray\',\'gray20\')) + scale_y_continuous(breaks = c(0, 20, 45, 70, 95), limits = c(0,100), expand = c(0,0))    \r\n', '##0. Reset environment\r\nrm(list=ls()) \r\n\r\n\r\n##1. Load libraries\r\nlibrary(corrplot)\r\nlibrary(raster)\r\nlibrary(raster)\r\nlibrary(rgdal)\r\nlibrary(sf)\r\n\r\n\r\n##2. Load word directory\r\nsetwd(""D:/TDWR_LB_ML/PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri/RELATIVE_INFLUENCE"")\r\n\r\n\r\n##3. Load database and select the columns with the variables data\r\nVarAm<- read.csv(""GBM_%contribution_At_Ad_09.csv"", h=T)\r\nhead(VarAm)\r\ndatos2<-VarAm[,c(3:4)]\r\nhead(datos2)\r\n\r\n\r\n\r\n##4. Pearson correlation. \r\ncorrel <- cor(datos2,method=\'pearson\')\r\ncorrplot(correl, type=""upper"", order=""hclust"")\r\nCorPer<-corrplot(round(correl,2), type=""upper"", order=""hclust"", method=""number"", tl.cex=0.8, pch.cex = 0.5, number.font =  6, cl.cex = 0.8, number.cex = 1)\r\nwindows (); CorPer\r\n?corrplot\r\nas.data.frame(CorPer)\r\nsetwd(""D:/TDWR_LB_ML/PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri/PEARSON_CORRELATION"")\r\nwrite.csv(CorPer,""Pearson_corr_At_Ad_09.csv"")\r\n\r\n\r\n##5. Correlation significance\r\ncor.test(x = datos2$SC, y = datos2$WDp , alternative = ""two.sided"", conf.level = 0.95, method = ""pearson"")\r\n', '## 0. Reset environment\r\nrm(list=ls()) \r\n\r\n## 1. Load libraries\r\nlibrary(sp)\r\nlibrary(raster)\r\nlibrary(dismo)     # package to run the model\r\nlibrary(maptools)\r\nlibrary(rgdal)\r\nlibrary(RColorBrewer)\r\nlibrary(Rcpp)\r\nlibrary(ggplot2)\r\nlibrary(lattice)\r\nlibrary(caret)\r\nlibrary(pROC)\r\nlibrary(sdm)\r\n\r\n## 2. Set a work directory for the variables or general\r\nsetwd(""D:/TDWR_LB_ML/R/VARIABLES_ML_VER99"")\r\n\r\n\r\n## 3. Load the stack of all variables\r\ndatafiles = Sys.glob(""*.tif"") #Or whatever identifies your files\r\nstck = stack() #empty stack for raster\r\nfor(i in 1:NROW(datafiles)){\r\n  tempraster = raster(datafiles[i])\r\n  stck = stack(stck,tempraster)\r\n}\r\n\r\nstck #raster predictors as a stackplot\r\n\r\nplot(stck,1, col= brewer.pal(6, ""RdYlGn""), main=""Cobertura subacuática"")\r\nplot(stck,2, col = brewer.pal(9, ""Blues""), main=""Profundidad"")\r\n\r\n\r\n##4. To load data\r\ndato= read.csv(""D:/TDWR_LB_ML/PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri/DOMAIN/AtJu99.csv"")\r\nhead(dato)\r\ndato1= dato[,-1]#No se necesita primera y segunda columna\r\nhead(dato1)\r\next = extent(-11135126, -11133410, 2494678, 2495150)\r\n\r\npoints(dato1, col=\'red\', pch=\'+\') \r\n\r\n\r\n##6. To generate random background points (i.e., 500 points)\r\nset.seed(1)\r\n\r\nbackgr = randomPoints(stck, 500, ext = ext) #500 random points, 80-20 ratio\r\ncolnames(backgr) = c(\'X\', \'Y\')\r\nhead(backgr)\r\npb = c(rep(1, nrow(dato1)), rep(0, nrow(backgr)))\r\nsdmdata = data.frame(cbind(pb, rbind(dato1, backgr)))\r\n\r\nhead(sdmdata)\r\n\r\nsdmdata=na.omit(sdmdata)\r\nsummary(sdmdata)\r\n\r\ntail(sdmdata)\r\n\r\n\r\n##7. To save the new data frame with presence + background points\r\nsetwd(""D:/TDWR_LB_ML/PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri/GLM"")\r\nwrite.csv(sdmdata,""Pres_back_GLM_At_Ad_99.csv"")\r\n', '## 0.Reset environment\r\nrm(list=ls())\r\n\r\n## 1. Set working directory\r\nsetwd(""D:/TDWR_LB_ML"")\r\n\r\n## 2. Install packages\r\ninstall.packages(""raster"")\r\ninstall.packages(""sp"")\r\ninstall.packages(""RcolorBrewer"")\r\ninstall.packages(""pander"")\r\ninstall.packages(""rgdal"")\r\n\r\n## 3. Load libraries\r\nlibrary(raster)\r\nlibrary(sp)\r\nlibrary(RColorBrewer)\r\nlibrary(pander)\r\nlibrary(rgdal)\r\nlibrary(dismo)     # package to run the model\r\nlibrary(maptools)\r\nlibrary(Rcpp)\r\nlibrary(ggplot2)\r\nlibrary(lattice)\r\nlibrary(caret)\r\nlibrary(pROC)\r\nlibrary(sdm)\r\nlibrary(gbm)\r\nlibrary(ModelMetrics)\r\n\r\n\r\n## 4. Load occurrence data\r\nOccurrences <- read.csv(""./PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri/GLM/Pres_back_GLM_At_Ad_09.csv"", h=T)\r\n\r\n\r\n## 5. Load the three climatic rasters\r\nWDp <- raster(""./PROYECTO_MDP_TESIS_MEDIA LUNA/RASTER/PROF_CAPA_ML_VER09/INTERPOLACIÓN_PROF_ML_VER09_ACT_0.1M.tif"")\r\nSC <- raster(""./PROYECTO_MDP_TESIS_MEDIA LUNA/RASTER/VEG_CAPA_ML_VER09/CONTINUOUS VARIABLE_WDp_ML_VER09_0.1M.tif"")\r\n\r\n\r\n## 6. Visualize the climatic maps\r\npar(mfrow = c(2, 1), mar = c(0.1, 0.1, 0.1, 0.1))\r\nplot(WDp, col = brewer.pal(9, ""PuBu""), box = F, axes = F, legend = F)\r\nplot(WDp, legend.only = T, horizontal = T, add = T, col = brewer.pal(9,""PuBu""), smallplot = c(0.25, 0.75, 0.14, 0.16))\r\nplot(SC, col = brewer.pal(9, ""RdYlGn""), box = F, axes = F, legend = F)\r\nplot(SC, legend.only= T, horizontal = T, add = T, col = rev(heat.colors(20)), smallplot = c(0.25, 0.75, 0.14, 0.16))\r\n\r\n\r\n## 7. Save the dataframe ""Habitat""\r\nvariables <- cbind(WDp=extract(WDp, Occurrences[, c(""X"", ""Y"")]), SC= extract(SC, Occurrences[, c(""X"", ""Y"")]))\r\n\r\n\r\n## 8. Append climatic data to occurrences, discard incomplete observations\r\nOccurrences <- cbind(Occurrences, variables)\r\nOccurrences <- na.omit(Occurrences)\r\n\r\n\r\n## 9. Plot climate raster and presence / background data (random sample, otherwise we can´t see the map well)\r\ntable(Occurrences$pb)/ nrow(Occurrences)*100\r\n\r\nindx.oc<- sample(which(Occurrences$pb == 1), round(500 * sum(Occurrences$pb == 1)/ nrow(Occurrences)))\r\nindx.ab<- sample(which(Occurrences$pb == 0), round(500 * sum(Occurrences$pb == 0)/ nrow(Occurrences)))\r\n\r\npar(mfrow = c(2, 1), mar = c(0.1, 0.1, 0.1, 0.1))\r\nplot(SC, col= brewer.pal(6, ""RdYlGn""), box = F, axes = F)\r\npoints(Occurrences$X[indx.oc], Occurrences$Y[indx.oc], col = ""green4"", pch = 3, cex = 0.5)\r\nplot(SC, col= brewer.pal(6, ""RdYlGn""), box = F, axes = F)\r\npoints(Occurrences$X[indx.ab], Occurrences$Y[indx.ab], col = ""blue"", pch = 3, cex = 0.5)\r\nlegend(""bottomright"", legend = c(""Present"", ""Background""), pch = c(3, 3), col = c(""green4"",""blue""))\r\n\r\n\r\n## 10. Fit GLM based on depth\r\nglm.uni <- glm(pb ~ poly(WDp, 2), data = Occurrences, family = ""binomial"", maxit = 100)\r\npander::pander(summary(glm.uni)$coefficients, caption = ""Summary of GLM model based on water depth"")\r\n\r\n\r\n## 11. Plot relationship depht and probability of presence\r\npar(mfrow = c(1, 1), mar= c(5,5,5,5))\r\ndata_plot <- data.frame(cbind(Occurrences$WDp, predict(glm.uni, type = ""response"")))\r\nsort1 <- na.omit(data_plot[order(data_plot[, 1], decreasing = FALSE),])\r\ndata_plot <- data.frame(cbind(sort1[, 1], sort1 [, 2]))\r\nplot(data_plot[, 1], data_plot[, 2], xlab = ""Water depth (m)"", ylab = ""Occurrence probability"", frame.plot = F, type = ""l"", lwd= 1, ylim = c(0, 1), family= ""serif"", cex.lab = 2.25, cex.axis = 1.7)\r\n\r\n\r\n## 12. Add observed response variable, presences in red and backgrounds in blue\r\npoints(Occurrences$WDp[Occurrences$pb == 1], Occurrences$pb[Occurrences$pb == 1], col = rgb(1, 0, 0, alpha = 0.2), pch = 16)\r\npoints(Occurrences$WDp[Occurrences$pb == 0], Occurrences$pb[Occurrences$pb == 0], col = rgb(0, 0, 1, alpha = 0.2), pch = 16)     \r\n#legend(-30.80, 0.95, col = c(""red"", ""blue"", ""black""), legend = c(""Presence"", ""Background"", ""Probability""), pch = c(16, 16, NA), lty = c(NA, NA, 1))\r\n\r\n\r\n## zoom to the  depth of activity\r\nplot(data_plot[, 1], data_plot[, 2], xlab = ""Water depth (m)"", ylab = ""Occurrence probability"", frame.plot = F, type = ""l"", lwd= 1, ylim = c(0, 1), xlim= c(-3.5, 0), family= ""serif"", cex.lab = 3, cex.axis = 2)\r\npoints(Occurrences$WDp[Occurrences$pb == 1], Occurrences$pb[Occurrences$pb == 1], col = rgb(1, 0, 0, alpha = 0.2), pch = 16)\r\npoints(Occurrences$WDp[Occurrences$pb == 0], Occurrences$pb[Occurrences$pb == 0], col = rgb(0, 0, 1, alpha = 0.2), pch = 16)     \r\n#legend(-3.50, 0.9, col = c(""red"", ""blue"", ""black""), legend = c(""Presence"", ""Background"", ""Probability""), pch = c(16, 16, NA), lty = c(NA, NA, 1))\r\n\r\n\r\n## 13. Fit GLM based on subaquatic coberture\r\nglm.uni <- glm(pb ~ poly(SC, 2), data = Occurrences, family = ""binomial"", maxit = 100)\r\npander::pander(summary(glm.uni)$coefficients, caption = ""Summary of GLM model based on underwater coverage"")\r\n\r\n\r\n## 14. Plot a curve probability of presence in dependence of subaquatic coberture\r\npar(mfrow = c(1, 1), mar=c(5,5,5,5))\r\ndata_plot <- data.frame(cbind(Occurrences$SC, predict(glm.uni, type = ""response"")))\r\nsort1 <- na.omit(data_plot[order(data_plot[, 1], ', '## 0.Reset environment\r\nrm(list=ls())\r\n\r\n## 1. Install packages\r\ninstall.packages(""caret"", dependencies = TRUE)\r\ninstall.packages(""ggplot2"", dependencies = TRUE)\r\ninstall.packages(""lattice"", dependencies = TRUE)\r\ninstall.packages(""Rcpp"")\r\ninstall.packages(""raster"")\r\ninstall.packages(""sp"")\r\ninstall.packages(""RcolorBrewer"")\r\ninstall.packages(""pander"")\r\ninstall.packages(""rgdal"")\r\ninstall.packages(""dismo"")\r\ninstall.packages(""maptools"")\r\ninstall.packages(""pROC"", dependencies= TRUE)\r\ninstall.packages(""sdm"")\r\ninstall.packages(""randomForest"")\r\ninstall.packages(""gbm"")\r\ninstall.packages(""ModelMetrics"")\r\n\r\n## 2. Load libraries\r\nlibrary(sp)\r\nlibrary(raster)\r\nlibrary(dismo)     # package to run the model\r\nlibrary(maptools)\r\nlibrary(rgdal)\r\nlibrary(RColorBrewer)\r\nlibrary(Rcpp)\r\nlibrary(ggplot2)\r\nlibrary(lattice)\r\nlibrary(caret)\r\nlibrary(pROC)\r\n\r\n## 3. Set working directory\r\nsetwd(""D:/TDWR_LB_ML/R/VARIABLES3_ML_VER19"")\r\n\r\n## 4. Load and visualize the stack of all variables\r\ndatafiles = Sys.glob(""*.tif"") #Or whatever identifies your files\r\nstck = stack() #empty stack for raster\r\nfor(i in 1:NROW(datafiles)){\r\n  tempraster = raster(datafiles[i])\r\n  stck = stack(stck,tempraster)\r\n}\r\n\r\nstck #raster predictors as a stackplot\r\n\r\nplot(stck,1, col = brewer.pal(6, ""RdYlGn""), main=""Underwater coverage"")\r\nplot(stck,2, col = brewer.pal(9, ""Blues""), main=""Water depth (m)"")\r\n\r\n## 5. Load occurrence data\r\ndato= read.csv(""D:/TDWR_LB_ML/PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri/DOMAIN/AtAd19.csv"")\r\nhead(dato)\r\ndato1= dato[,-1]#first column not needed\r\nhead(dato1)\r\next = extent(-11135127, -11133410, 2494679, 2495150)\r\nsummary(dato)\r\npoints(dato1, col=\'red\', pch=\'+\') \r\n\r\n## 6. Select for occurrence data a train group (80%) and a test group (20%)\r\ngroup = kfold(dato1, 5)\r\npres_train = dato1[group != 1, ]\r\npres_test = dato1[group == 1, ]\r\n\r\n## 7. Generate random background points (i.e., 500 points) and order in a train group (80%) and a test group (20%)\r\nbackg = randomPoints(stck, n=500,ext=ext, extf = 1.25)\r\ncolnames(backg) = c(\'X\', \'Y\')\r\ngroup = kfold(backg, 5)\r\n\r\nbackg_train = backg[group != 1, ]\r\nbackg_test = backg[group == 1, ]\r\n\r\n## 8. Plot the train and test group of occurrence/background data\r\nr = raster(stck, 1)\r\nplot(!is.na(r), col=c(\'white\', \'light grey\'), legend=FALSE)\r\nplot(ext, add=TRUE, col=\'red\', lwd=2)\r\npoints(backg_train, pch=\'-\', cex=0.5, col=\'red\')\r\npoints(backg_test, pch=\'-\',  cex=0.5, col=\'black\')\r\npoints(pres_train, pch= \'+\', col=\'green\')\r\npoints(pres_test, pch=\'+\', col=\'blue\')\r\n\r\n\r\n## 9. Run the DOMAIN Model and the evaluation Metricts (Partial ROC and ROC/AUC curve)\r\nrequire(dismo)\r\ndm =domain(stck, pres_train) #domain model- presence data only \r\ne = evaluate(pres_test, backg_test, dm, stck)\r\ne\r\npar(mfrow=c(1,1))\r\ndensity(e, family= ""serif"", cex.lab = 2.25, cex.axis = 2.0)\r\nlegend(""top"", legend= ""Bandwidth= 0.026"", bty = ""n"", text.font = 7, cex = 2.0)\r\nplot(e, \'ROC\', type = ""o"", lwd= 2, family= ""serif"", cex.lab = 2.25, cex.axis = 2.0)\r\nlegend(-0.0, 0.2,legend=c(""AUC= 0.746""), bty= ""n"", text.font = 7, cex = 2.0)\r\n\r\n\r\n\r\n## 10. Project the predictive mapping \r\npd = predict(stck, dm, ext=ext, progress=\'\') #predict for p mapping\r\npar(mfrow=c(1,1)) ## par(mfrow=c(1,2)) Two maps in a screen\r\nplot(pd, main=\'Domain Model\')\r\nplot(pd, add=TRUE, border= \'dark grey\')\r\ntr <- threshold(e, \'spec_sens\')\r\nplot(pd > tr, main=\'Occurrence/Background\')\r\nplot(pd > tr, add=TRUE, border =\'dark grey\')\r\npoints(pres_train, pch=\'+\')\r\n\r\n## 11. Save the final raster\r\nsetwd(""D:/TDWR_LB_ML/PRODUCTOS-ARTÍCULOS/Spatial ecology of A. toweri/RESULTS/DOMAIN_&_SUIT_HAB"")\r\nwriteRaster(pd, ""AtJu99_Domain.tif"")\r\nwriteRaster(pd > tr, ""AtJu99_SuitHab.tif"")\r\n']",0,"- Community assembly
- Spatial scales
- Traits
- Phylogenetic relationships
- Wood-inhabiting fungi
- Joint species distribution modelling
- European-scale dataset
- Local scale
- Regional scale
- Deadwood decay stage"
Data and code from: Traits and phylogenies modulate the environmental responses of wood-inhabiting fungal communities across spatial scales,"Identifying the spatial scales at which community assembly processes operate is fundamental for gaining a mechanistic understanding of the drivers shaping ecological communities. In this study, we examined whether and how traits and phylogenetic relationships structure fungal community assembly across spatial scales.We applied joint species distribution modelling to a European-scale dataset on 215 wood-inhabiting fungal species, which includes data on traits, phylogeny and environmental variables measured at the local (log-level) and regional (site-level) scales.At the local scale, wood-inhabiting fungal communities were mostly structured by deadwood decay stage, and the trait and phylogenetic patterns along this environmental gradient suggested the lack of diversifying selection.At regional scales, fungal communities and their trait distributions were influenced by climatic and connectivity-related variables. The fungal climatic niches were not phylogenetically structured, suggesting that diversifying selection or stabilizing selection for climatic niches has played a strong role in wood-inhabiting communities. In contrast, we found a strong phylogenetic signal in the responses to connectivity-related variables, revealing phylogenetic homogenization in small and isolated forests.Altogether, our results show that species-level traits and phylogenies modulate the responses of wood-inhabiting fungi to environmental processes acting at different scales. This result suggests that the evolutionary histories of fungal traits diverge along different environmental axes.","['###This script builds the HMSC models of Abrego et al. \r\n###The model objects are stored in the "".../models"" folder\r\n\r\n#Set working directory and indicate folders\r\n#setwd(...)\r\n\r\nlocalDir = "".""\r\nModelDir = file.path(localDir, ""models"")\r\nResultDir = file.path(localDir, ""results"")\r\n\r\n#open (and install beforehand if needed) the packages and functions needed to run this script\r\nlibrary(Hmsc)\r\nlibrary(coda)\r\nlibrary(ape)\r\nlibrary(vegan)\r\nlibrary(fastDummies)\r\nlibrary(devtools)\r\nlibrary(ggbiplot)\r\nlibrary(cowplot)\r\nlibrary(ggpubr)\r\n\r\n#Load data\r\nload(""preparedData.RData"") #Y,X,Tr,P,T1,T2,T3\r\n\r\n#Indicate the X formula of environmental covariates to be included in the HMSC models\r\nXFormula = ~log.AREA + CONNECT10 + TEMPR + PRECIP + DBH.CM + poly(AVERDP, degree = 2, raw = TRUE) \r\n\r\n\r\n#Indicate the Trait formula to be included in the HMSC models\r\n#We indicate three different trait formulas, one for each of the alternative HMSC models that include trait data\r\nTrFormula1 = ~X1 + X2 + X3\r\nTrFormula2 = ~X1 + X2 + X3\r\nTrFormula3 = ~FB.type + Sp.log.vol.µ3 + Lifestyle\r\n\r\n\r\n#We indicate which are the random effects and which levels they have\r\n\r\nstudyDesign = data.frame(sample = as.factor(X$Log_ID), region = as.factor(X$REGION), reserve = as.factor(X$RESERVE))\r\nrL.region = HmscRandomLevel(units = levels(studyDesign$region))\r\nrL.reserve = HmscRandomLevel(units = levels(studyDesign$reserve))\r\n\r\n\r\n##And here we set the four alternative HMSC models\r\n\r\n#m1: Overall PCA\r\nm1 = Hmsc(Y=Y, XData = X,  XFormula = XFormula,\r\n          TrData = T1,TrFormula = TrFormula1,\r\n          phyloTree = P,\r\n          distr=""probit"",\r\n          studyDesign=studyDesign,\r\n          ranLevels={list(region=rL.region,reserve=rL.reserve)})\r\n\r\n#m2: Group-specific PCA\r\nm2 = Hmsc(Y=Y, XData = X,  XFormula = XFormula,\r\n          TrData = T2,TrFormula = TrFormula2,\r\n          phyloTree = P,\r\n          distr=""probit"",\r\n          studyDesign=studyDesign,\r\n          ranLevels={list(region=rL.region,reserve=rL.reserve)})\r\n\r\n#m3: Raw traits\r\nm3 = Hmsc(Y=Y, XData = X,  XFormula = XFormula,\r\n          TrData = T3,TrFormula = TrFormula3,\r\n          phyloTree = P,\r\n          distr=""probit"",\r\n          studyDesign=studyDesign,\r\n          ranLevels={list(region=rL.region,reserve=rL.reserve)})\r\n\r\n#m1: No traits\r\nm4 = Hmsc(Y=Y, XData = X,  XFormula = XFormula,\r\n          phyloTree = P,\r\n          distr=""probit"",\r\n          studyDesign=studyDesign,\r\n          ranLevels={list(region=rL.region,reserve=rL.reserve)})\r\n\r\n\r\n\r\n\r\n#And we save all models into an object called ""models""\r\nmodels = list(m1,m2,m3, m4)\r\nmodelnames = c(""overall_PCA"",""group_PCA"",""raw_traits"", ""no traits"")\r\n\r\nsave(models,modelnames,file = file.path(ModelDir, ""unfitted_models.RData""))\r\n\r\n', '###Fits the HMSC models\r\n###The fitted model objects are stored in the "".../models"" folder\r\n###The results generated in this script will be stored in the "".../results"" folder\r\n\r\n\r\n#Set workind directory and indicate folders\r\n#setwd(...)\r\n\r\nsamples = 250\r\nnChains = 4\r\nthin = 100\r\n\r\nload(""models/unfitted_models.RData"")\r\n\r\nprint(paste0(""thin = "",as.character(thin),""; samples = "",as.character(samples)))\r\n\r\nfilename_out = paste(""models/models_thin_"", as.character(thin),\r\n                     ""_samples_"", as.character(samples),\r\n                     ""_chains_"",as.character(nChains),\r\n                     "".Rdata"",sep = """")\r\n\r\nnm = length(models)\r\nfor (model in 1:nm) {\r\n  print(paste0(""model = "",as.character(model)))\r\n  m = models[[model]]\r\n  models[[model]] = sampleMcmc(m, samples = samples, thin=thin,\r\n                               adaptNf=rep(ceiling(0.4*samples*thin),m$nr), \r\n                               transient = ceiling(0.5*samples*thin),\r\n                               nChains = nChains)\r\n}\r\nsave(models,modelnames,file=filename_out)\r\n', '###Evaluates the convergence of the fitted HMSC models\r\n###The fitted model objects are stored in the "".../models"" folder\r\n###The results generated in this script will be stored in the "".../results"" folder\r\n\r\n\r\n#Set working directory and indicate folders\r\n#setwd(...)\r\nlocalDir = "".""\r\nModelDir = file.path(localDir, ""models"")\r\nResultDir = file.path(localDir, ""results"")\r\n\r\n#open (and install beforehand if needed) the packages and functions needed to run this script\r\nlibrary(Hmsc)\r\nlibrary(colorspace)\r\nlibrary(vioplot)\r\n\r\n#read in the fitted model objects\r\nnChains = 4\r\nthin = 100\r\nsamples = 250\r\n\r\n\r\nfilename = paste(""models/models_thin_"", as.character(thin),\r\n                 ""_samples_"", as.character(samples),\r\n                 ""_chains_"",as.character(nChains),\r\n                 "".Rdata"",sep = """")\r\nload(filename)\r\nnm = length(models)\r\n\r\nfor(j in 1:nm){\r\n  print(modelnames[j])\r\n  mpost = convertToCodaObject(models[[j]], spNamesNumbers = c(T,F), covNamesNumbers = c(T,F))\r\n  psrf.beta = gelman.diag(mpost$Beta,multivariate=FALSE)$psrf\r\n  print(""beta:"")\r\n  print(summary(psrf.beta))\r\n  psrf.gamma = gelman.diag(mpost$Gamma,multivariate=FALSE)$psrf\r\n  print(""gamma:"")\r\n  print(summary(psrf.gamma))\r\n  print(""rho:"")\r\n  psrf.rho = gelman.diag(mpost$Rho,multivariate=FALSE)$psrf\r\n  print(psrf.rho)\r\n  print(summary(mpost$Rho))\r\n}\r\n', '###Evaluates the explanatory and predictive powers of the fitted HMSC models\r\n###The fitted model objects are stored in the "".../models"" folder\r\n###The results generated in this script will be stored in the "".../results"" folder\r\n\r\n\r\n#Set workind directory and indicate folders\r\n#setwd(...)\r\nlocalDir = "".""\r\nModelDir = file.path(localDir, ""models"")\r\nResultDir = file.path(localDir, ""results"")\r\n\r\n#open (and install beforehand if needed) the packages and functions needed to run this script\r\nlibrary(Hmsc)\r\n#read in the fitted model objects\r\nnChains = 4\r\nthin = 100\r\nsamples = 250\r\n\r\nfilename = paste(""models/models_thin_"", as.character(thin),\r\n                 ""_samples_"", as.character(samples),\r\n                 ""_chains_"",as.character(nChains),\r\n                 "".Rdata"",sep = """")\r\nload(filename)\r\nnm = length(models)\r\nMF = list()\r\nMFCV = list()\r\nWAIC = list()\r\nfor(j in 1:nm){\r\n  print(modelnames[j])\r\n  m = models[[model]]\r\n  preds = computePredictedValues(m)\r\n  MF[[j]] = evaluateModelFit(hM=m, predY=preds)\r\n  \r\n  partition = createPartition(m, nfolds = 2)\r\n  preds = computePredictedValues(m,partition=partition,nParallel = nChains)\r\n  MFCV[[j]] = evaluateModelFit(hM=m, predY=preds)\r\n}\r\nfilename = paste(""models/MF_thin_"", as.character(thin),\r\n                 ""_samples_"", as.character(samples),\r\n                 ""_chains_"",as.character(nChains),\r\n                 "".Rdata"",sep = """")\r\nsave(MF,MFCV,file = filename)\r\n', '###Evaluates the explanatory and predictive powers of the fitted HMSC models\r\n###The fitted model objects are stored in the "".../models"" folder\r\n###The results generated in this script will be stored in the "".../results"" folder\r\n\r\n\r\n#Set workind directory and indicate folders\r\n#setwd(...)\r\nlocalDir = "".""\r\nModelDir = file.path(localDir, ""models"")\r\nResultDir = file.path(localDir, ""results"")\r\n\r\n#read in the pre-computed model fits\r\nnChains = 4\r\nthin = 10\r\nsamples = 250\r\n\r\nfilename = paste(""models/MF_thin_"", as.character(thin),\r\n                 ""_samples_"", as.character(samples),\r\n                 ""_chains_"",as.character(nChains),\r\n                 "".Rdata"",sep = """")\r\nload(filename)\r\nnm = length(MF)\r\nfor(j in 1:nm){\r\n  print(""model:"")\r\n  print(j)\r\n  cMF = MF[[j]]\r\n  cMFCV = MFCV[[j]]\r\n  print(""AUC (exp), AUC (pred): "")\r\n  print(c(mean(cMF$AUC),mean(cMFCV$AUC)))\r\n  print(""Tjur (exp), Tjur (pred): "")\r\n  print(c(mean(cMF$TjurR2),mean(cMFCV$TjurR2)))\r\n}\r\n\r\n', '###Shows the parameter estimates of the fitted HMSC models\r\n###The fitted model objects are stored in the "".../models"" folder\r\n###The results generated in this script will be stored in the "".../results"" folder\r\n\r\n\r\n#Set workind directory and indicate folders\r\nsetwd(""C:/LocalData/abrego/all stuff/MANUSCRIPTS/in preparation/BB Traits/Statistical analyses"")\r\n#setwd(...)\r\n\r\nlocalDir = "".""\r\nModelDir = file.path(localDir, ""models"")\r\nResultDir = file.path(localDir, ""results"")\r\n\r\n#open (and install beforehand if needed) the packages and functions needed to run this script\r\nlibrary(Hmsc)\r\nlibrary(colorspace)\r\nlibrary(corrplot)\r\n\r\n#read in the fitted model objects\r\nnChains = 4\r\nthin = 100\r\nsamples = 250\r\n\r\n\r\nfilename = paste(""models/models_thin_"", as.character(thin),\r\n                 ""_samples_"", as.character(samples),\r\n                 ""_chains_"",as.character(nChains),\r\n                 "".Rdata"",sep = """")\r\nload(filename)\r\nnm = length(models)\r\nmodelnames\r\n\r\n#Here we compute the variance partitioning for each of the models\r\n# We do the variance partitioning for the variables (Covariates variance partitioning)\r\n# And the traits (Trait variance partitioning)\r\n\r\nfor(j in 1:nm){\r\n  m = models[[j]]\r\n  \r\n  VP = computeVariancePartitioning(m)\r\n  vals = VP$vals\r\n  mycols = rainbow(nrow(VP$vals))\r\n  plotVariancePartitioning(hM=m, VP=VP,\r\n                           main = paste0(""Covariates variance partitioning, "",\r\n                                         modelnames[[j]]), cex.main=0.8, \r\n                           cols = mycols, args.leg=list(bg=""white"",cex=0.7))\r\n  \r\n  filename = paste(""results/Trait variance partitioning"",modelnames[[j]],"".csv"")\r\n  write.csv(VP$R2T$Beta,file=filename)\r\n  \r\n\r\n#Here we plot the estimated beta parameters\r\n  postBeta = getPostEstimate(m, parName=""Beta"")  \r\n\r\n  png(file.path(ResultDir,paste0(""Figure 3B_"",modelnames[[j]],"".png"")), width = 500, height = 1200)\r\n  plotBeta(m, post=postBeta, supportLevel = 0.95,param=""Sign"",\r\n           plotTree = TRUE,\r\n           covNamesNumbers = c(FALSE,FALSE),\r\n           spNamesNumbers=c(FALSE,FALSE),\r\n           cex=c(0.6,0.6,0.8))\r\n  #title(main=paste0(""BetaPlot "",modelnames[[j]]), line=2.5,cex.main=0.8)\r\n  \r\n  dev.off()\r\n  \r\n  ##Here we plot the estimated gamma parameters\r\n  if(j != 4){\r\n    postGamma = getPostEstimate(m, parName=""Gamma"")\r\n    plotGamma(m, post=postGamma, supportLevel = 0.9, param=""Sign"",\r\n              covNamesNumbers = c(TRUE,FALSE),\r\n              trNamesNumbers=c(TRUE,FALSE),\r\n              cex=c(0.6,0.6,0.8))\r\n    title(main=paste0(""GammaPlot "",modelnames[[j]]), line=2.5,cex.main=0.8)\r\n  }  \r\n}\r\n\r\n', '###Makes phylogenetic tests for the b parameters of HMSC\r\n###And traits\r\n###The results generated in this script will be stored in the "".../results"" folder\r\n\r\n\r\n#Set workind directory and indicate folders\r\n#setwd(...)\r\n\r\nlocalDir = "".""\r\nModelDir = file.path(localDir, ""models"")\r\nResultDir = file.path(localDir, ""results"")\r\n\r\n#open (and install beforehand if needed) the packages and functions needed to run this script\r\nlibrary(nlme)\r\nlibrary(ape)\r\nlibrary(phytools)\r\nlibrary(Hmsc)\r\n\r\n#read in the fitted model objects\r\nnChains = 4\r\nthin = 100\r\nsamples = 250\r\n\r\nfilename = paste(""models/models_thin_"", as.character(thin),\r\n                 ""_samples_"", as.character(samples),\r\n                 ""_chains_"",as.character(nChains),\r\n                 "".Rdata"",sep = """")\r\nload(filename)\r\nnm = length(models)\r\nmodelnames\r\n\r\n#Here we test the phylogenetic signal in the beta parameters for each of the 4 alternative HMSC models\r\n\r\nfor(i in 1:length(modelnames)){\r\n  m = models[[i]]\r\n  postBeta = getPostEstimate(m, parName=""Beta"")\r\n  phyloTree = m$phyloTree\r\n  nc = dim(postBeta$mean)[1]\r\n  results = matrix(NA,nrow=nc,ncol = 2)\r\n  for(j in 1:nc){\r\n    y = postBeta$mean[j,]\r\n    m1<-gls(y~1)\r\n    m2<-gls(y~1,correlation=corPagel(1, phyloTree, fixed=FALSE))\r\n    z1=summary(m2)\r\n    z2=anova(m1,m2)\r\n    lambda= z1$modelStruct[1]$corStruct\r\n    pvalue=z2$`p-value`[2]\r\n    results[j,1]=pvalue\r\n    results[j,2]=lambda\r\n  }\r\n  colnames(results) = c(""p-value"",""lambda"")\r\n  row.names(results) = m$covNames\r\n  write.csv(file = paste0(""results/beta_phylogenetic_signal_"",modelnames[i],"".csv""),results)\r\n}\r\n\r\n\r\n##Here we test for the phylogenetic signal for the traits (from overall traits PCA and group-specific PCA)\r\nfor(i in 1:2){\r\n  m = models[[i]]\r\n  phyloTree = m$phyloTree\r\n  traitData = t(m$Tr[,2:4])\r\n  nc = dim(traitData)[1]\r\n  results = matrix(NA,nrow=nc,ncol = 2)\r\n  for(j in 1:nc){\r\n    y = traitData[j,]\r\n    m1<-gls(y~1)\r\n    m2<-gls(y~1,correlation=corPagel(0.9, phyloTree, fixed=FALSE))\r\n    z1=summary(m2)\r\n    z2=anova(m1,m2)\r\n    lambda= z1$modelStruct[1]$corStruct\r\n    pvalue=z2$`p-value`[2]\r\n    results[j,1]=pvalue\r\n    results[j,2]=lambda\r\n  }\r\n  colnames(results) = c(""p-value"",""lambda"")\r\n  row.names(results) = c(""trait_1"",""trait_2"",""trait_3"")\r\n  write.csv(file = paste0(""results/traits_phylogenetic_signal_"",modelnames[i],"".csv""),results)\r\n}\r\n', '###Makes predictions of the weighted mean trait values of those traits that \r\n###showed statistically supported gamma parameter estimates in the HMSC models\r\n###The results generated in this script will be stored in the "".../results"" folder\r\n\r\n#Set workind directory and indicate folders\r\n#setwd(...)\r\n\r\nlocalDir = "".""\r\nModelDir = file.path(localDir, ""models"")\r\nResultDir = file.path(localDir, ""results"")\r\n\r\n#open (and install beforehand if needed) the packages and functions needed to run this script\r\nlibrary(Hmsc)\r\nlibrary(colorspace)\r\nlibrary(corrplot)\r\nsource(""put_fig_letter.r"")\r\n\r\n#read in the fitted model objects\r\nnChains = 4\r\nthin = 100\r\nsamples = 250\r\n\r\n\r\nfilename = paste(""models/models_thin_"", as.character(thin),\r\n                 ""_samples_"", as.character(samples),\r\n                 ""_chains_"",as.character(nChains),\r\n                 "".Rdata"",sep = """")\r\nload(filename)\r\nnm = length(models)\r\nmodelnames\r\n\r\n##Make predictions from the model including raw trait values\r\nm = models[[3]]\r\npdf(file.path(ResultDir,""Figure 2.pdf""), width = 4.5, height =6)\r\n#png(file.path(ResultDir,""Figure 2.png""), width = 1800, height =2000, res=300)\r\npar(mfrow=c(2,3))\r\n\r\nGradient = constructGradient(m,focalVariable = ""AVERDP"")\r\nGradient$XDataNew$AVERDP<-seq(from=1, to=5, by= 5/24)\r\npredY = predict(m, Gradient=Gradient, expected = TRUE)  \r\n\r\nplotGradient(m, Gradient, pred=predY, \r\n             yshow = c(2,7), measure=""T"", index=7, showData = TRUE, cicol=""violet"",\r\n             ylabel = ""Spore volume log(µm^3)"", xlabel = ""Decay stage"")\r\nput.fig.letter(label=""A"", location=""topleft"", font=2)\r\n\r\nplotGradient(m, Gradient, pred=predY, cicol=""violet"",\r\n             yshow = c(0, 0.6), measure=""T"", index=4, showData = TRUE,\r\n             ylabel = ""Polyporoid pileates"", xlabel = ""Decay stage"")\r\nput.fig.letter(label=""B"", location=""topleft"", font=2)\r\n\r\nGradient = constructGradient(m,focalVariable = ""TEMPR"")\r\npredY = predict(m, Gradient=Gradient, expected = TRUE)  \r\nplotGradient(m, Gradient, pred=predY, \r\n             yshow = c(0, 0.6), measure=""T"", index=5, showData = TRUE,\r\n             cicol=""violet"", ylabel = ""Polyporoid resupinates"", xlabel = ""Annual\\n temperature range (°C)"" )\r\nput.fig.letter(label=""C"", location=""topleft"", font=2)\r\n\r\nGradient = constructGradient(m,focalVariable = ""CONNECT10"")\r\npredY = predict(m, Gradient=Gradient, expected = TRUE)  \r\nplotGradient(m, Gradient, pred=predY, \r\n             yshow = c(0.3, 1), measure=""T"", index=8, showData = TRUE,\r\n             cicol=""violet"", ylabel = ""Deciduous specialists"", xlabel = ""Forest connectivity"")\r\nput.fig.letter(label=""D"", location=""topleft"", font=2)\r\n\r\nGradient = constructGradient(m,focalVariable = ""log.AREA"")\r\npredY = predict(m, Gradient=Gradient, expected = TRUE)  \r\nplotGradient(m, Gradient, pred=predY, \r\n             yshow = c(2,7), measure=""T"", index=7, showData = TRUE,\r\n             cicol=""violet"", ylabel = ""Spore volume log(µm^3)"", xlabel = ""Reserve area log(ha)"")\r\nput.fig.letter(label=""E"", location=""topleft"", font=2)\r\ndev.off()\r\n', '###Makes sensitivity analyses concerning the rare species\r\n\r\n#Set working directory and indicate folders\r\n#setwd(...)\r\n\r\nlocalDir = "".""\r\nModelDir = file.path(localDir, ""models"")\r\nResultDir = file.path(localDir, ""results"")\r\n\r\n#open (and install beforehand if needed) the packages and functions needed to run this script\r\nlibrary(nlme)\r\nlibrary(ape)\r\nlibrary(phytools)\r\nlibrary(lme4)\r\n\r\nload(""preparedData_all.RData"") #Y.all,X,Tr.all,P.all,T1.all,T2.all,T3.all,rarespecies\r\n\r\n#TESTING IF TRAITS PREDICT IF THE SPECIES IS RARE\r\nns = dim(Y.all)[2]\r\ny = rep(0,ns)\r\ny[rarespecies] = 1\r\nm.null = glm(y~1,family = ""binomial"")\r\n\r\nm1 = glm(y~T1.all[,1]+T1.all[,2]+T1.all[,3],family = ""binomial"")\r\nsummary(m1)\r\nAIC(m.null,m1)\r\n\r\nm2 = glm(y~T2.all[,1]+T2.all[,2]+T2.all[,3],family = ""binomial"")\r\nsummary(m2)\r\nAIC(m.null,m2)\r\n\r\nm3 = glm(y~T3.all[,1]+T3.all[,2]+T3.all[,3],family = ""binomial"")\r\nsummary(m3)\r\nAIC(m.null,m3)\r\n\r\n#The null model is better supported in all cases, thus traits do not predict rarity, and thus\r\n# rare and common species do not systematically differ in their traits\r\n\r\n\r\n#TESTING IF RARE AND COMMON SPECIES DIFFER IN THEIR ENVIRONMENTAL RESPONSES\r\n\r\nS.rare = rowSums(Y.all[,rarespecies]>0)\r\nS.common = rowSums(Y.all[,-rarespecies]>0)\r\nsum(S.rare)/(sum(S.rare)+sum(S.common))*100\r\n100*length(rarespecies)/ns\r\nhist(S.common)\r\nhist(S.rare)\r\n\r\nm.common = glmer(S.common~X$log.AREA + X$CONNECT10 + X$TEMPR + X$PRECIP + X$DBH.CM + poly(X$AVERDP, degree = 2, raw = TRUE) + (1|X$RESERVE) + (1|X$REGION),family = ""poisson"")\r\nsummary(m.common)\r\n\r\nm.rare = glmer(S.rare~X$log.AREA + X$CONNECT10 + X$TEMPR + X$PRECIP + X$DBH.CM + poly(X$AVERDP, degree = 2, raw = TRUE) + (1|X$RESERVE) + (1|X$REGION),family = ""poisson"")\r\nsummary(m.rare)\r\n\r\nny = dim(X)[1]\r\nXX=rbind(X,X)\r\nXX$Log_ID = as.factor(XX$Log_ID)\r\nS.type = as.factor(c(rep(""rare"",ny),rep(""common"",ny)))\r\nm = glmer(c(S.rare,S.common)~S.type*(XX$log.AREA + XX$CONNECT10 + XX$TEMPR + XX$PRECIP + XX$DBH.CM + poly(XX$AVERDP, degree = 2, raw = TRUE))  + (1|XX$Log_ID) + (1|XX$RESERVE) + (1|XX$REGION),family = ""poisson"")\r\nsummary(m)\r\n\r\n']",0,"transcriptomics, data-driven, gene space, liver cytopathology, drug-induced liver injury, big data, data fusion, adverse outcomes, cellular level, organismal level, predictive toxicogenomics space, PTGS, cytotoxicity"
A transcriptomics data-driven gene space accurately predicts liver cytopathology and drug-induced liver injury,"Predicting unanticipated harmful effects of chemicals and drug molecules is a difficult and costly task. Here we utilize a big data compacting and data fusion- concept to capture diverse adverse outcomes on cellular and organismal levels. The approach generates from transcriptomics data set a predictive toxicogenomics space (PTGS) tool composed of 1331 genes distributed over 14 overlapping cytotoxicity-related gene space components. Involving approximately 2.5 x 108 data points and 1300 compounds to construct and validate the PTGS, the tool serves to: explain dose-dependent cytotoxicity effects, provide a virtual cytotoxicity probability estimate intrinsic to omics data, predict chemically-induced pathological states in liver resulting from repeated dosing of rats, and furthermore, predict human drug-induced liver injury (DILI) from hepatocyte experiments. Analyzing 68 DILI-annotated drugs, the PTGS tool outperforms and complements existing tests, leading to a hereto-unseen level of DILI prediction accuracy. Custom R code and methods to calculate the component-based PTGS scores using gene expression data.","['# PTGS 1.0 example script for analysis using pre-computed GSEA statistics: #\r\n\r\n# Copyright (C) 2012-2017 Juuso Parkkinen, Pekka Kohonen, Samuel Kaski & Roland GrafstrÃ¶m\r\n# All rights reserved.\r\n# \r\n# This program is open source software; you can redistribute it and/or modify it under the terms of the FreeBSD License (keep this notice): http://en.wikipedia.org/wiki/BSD_licenses\r\n# \r\n# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n# Overview\r\n# 1. Get differential expression data\r\n# 2. Compute GSEA\r\n# 3. Compute PTGS scores\r\n\r\n# Define folders to work in\r\nmain.folder <- getwd() # point R to the destination folder\r\nptgs.folder <- file.path(main.folder, ""PTGS_computation"")\r\nptgs.data <- file.path(ptgs.folder, ""data"")\r\n\r\n## 1. Get differential expression data ##############\r\n\r\n# The input into the tool consists of gene-level statistics with sign information (e.g. t-statistics and fold-change). \r\n# These can be obtained e.g., from the R/Bioconductor package ""limma"" using the ""coef"" values (which denote the fold-change) \r\n# or the moderated t-statistics. In the study the fold-change values were used.\r\n\r\n# The example data comes from Carcinogenomics project (http://wwwdev.ebi.ac.uk/fg/dixa/group/DIXA-002)\r\n# The Dixa accession is DIXA-002\r\n# PIPERONYL BUTOXIDE (CHEMBL1201131) was measured in human cultured liver cells (HepaRG at either 1.6 or 3.2 microMolar concentration)\r\n\r\n## 2. Compute GSEA ##############\r\n\r\n# The ""WriteRankedGenes"" function takes a data frame of gene names and directional statistics and outputs a ranked list for GSEA\r\n\r\n# Calculation of the PTGS components uses the ""GSEApreRanked"" method, available from\r\n# The Broad Institute\'s page (http://software.broadinstitute.org/gsea/index.jsp) and described in\r\n# http://software.broadinstitute.org/gsea/doc/GSEAUserGuideFrame.html?_GSEAPreranked_Page.\r\n# For this analysis the version gsea2-2.1.0 was used.\r\n# \r\n# The PTGS component model is built with MSigDB-C2 gene set, version 2.5.\r\n#\r\n# Settings (default settings for the method, please check):\r\n# -Xmx1024m xtools.gsea.GseaPreranked # scoring method (gene-wise permutation)\r\n# -norm meandiv \r\n# -nperm 1000 # number of permutations (may take some time e.g., 5-10 min, to calculate)\r\n# -scoring_scheme weighted # scoring scheme\r\n# -set_max 500 -set_min 15 # minimum and maximum length of gene sets to be tested\r\n# \r\n# These setting can be accessed in the GUI mode from the console\r\n# \r\n# The positive and negative results are utilized to calculate PTGS scores (see below and example files)\r\n# The results will appear in a designated folder\r\n\r\n## 3. Compute PTGS scores #########################\r\n\r\n# Load PTGS data\r\nload(file.path(ptgs.folder, ""PTGS_Data.RData""))\r\n\r\n# Source auxiliary functions\r\nsource(file.path(ptgs.folder, ""PTGS_v.1.0_functions.R""))\r\n\r\n# Load GSEA results: xls files to be found in the results folder (positive=pos and negative = neg) #\r\ngsea.pos1 <- read.delim(file.path(ptgs.data, ""gsea_report_for_PBO_16_pos.xls""), stringsAsFactors=FALSE)\r\ngsea.neg1 <- read.delim(file.path(ptgs.data, ""gsea_report_for_PBO_16_neg.xls""), stringsAsFactors=FALSE)\r\ngsea.pos2 <- read.delim(file.path(ptgs.data, ""gsea_report_for_PBO_32_pos.xls""), stringsAsFactors=FALSE)\r\ngsea.neg2 <- read.delim(file.path(ptgs.data, ""gsea_report_for_PBO_32_neg.xls""), stringsAsFactors=FALSE)\r\ngsea.res <- list(PBO_16=list(pos=gsea.pos1[,c(""NAME"", ""FDR.q.val"")], neg=gsea.neg1[,c(""NAME"", ""FDR.q.val"")]),\r\n                 PBO_32=list(pos=gsea.pos2[,c(""NAME"", ""FDR.q.val"")], neg=gsea.neg2[,c(""NAME"", ""FDR.q.val"")]))\r\n    \r\n# Convert FDR q-values to counts\r\ngenesets <- colnames(model$comp2geneset)[1:1321]\r\ngsea.counts <- lapply(gsea.res, function(x) ConvertFDRtoCounts(fdr.values=x, genesets=genesets, max.count=model$parameters$max.count))\r\n\r\n# Get sparse counts for the component model\r\ngsea.sparse.counts <- lapply(gsea.counts, function(x) GetSparseCounts(counts=x))\r\n\r\n# Estimate LDA component probabilities\r\nset.seed(1)\r\nlda.samples <- lapply(gsea.sparse.counts, function(x) EstimateComponents(sparse.counts=x, alpha=model$parameters$alpha, phi=model$comp2geneset, Niter=100))\r\n\r\n# Compute PTGS scores\r\ntemp.chem2comp <- t(sapply(lda.samples, function(x) x$n/sum(x$n)))\r\nPTGS.scores <- apply(temp.chem2comp[, as.numeric(model$PTGS.components)], 1, sum)\r\n\r\n\r\n\r\n\r\n\r\n', '# Functions for PTGS computations\r\n\r\n# Copyright (C) 2012-2017 Juuso Parkkinen, Pekka Kohonen, Samuel Kaski & Roland GrafstrÃ¶m\r\n# All rights reserved.\r\n# \r\n# This program is open source software; you can redistribute it and/or modify it under the terms of the FreeBSD License (keep this notice): http://en.wikipedia.org/wiki/BSD_licenses\r\n# \r\n# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n# Write ranked gene lists for each experiment\r\nWriteRankedGenes <- function(data, input.folder) {\r\n  \r\n  message(""Writing ranked genes for "", nrow(data),"" experiments to folder "", input.folder, ""..."", appendLF=FALSE)\r\n  if (!is.null(rownames(data)))\r\n    exp.names <- rownames(data)\r\n  else\r\n    exp.names <- paste0(""experiment"", 1:nrow(data))\r\n  for (i in 1:nrow(data)) {\r\n    ranked.dat <- sort(data[i,])\r\n    filename <- file.path(input.folder, paste0(exp.names[i],"".rnk""))\r\n    write(c(""GeneSymbols"", ""LogRatios""), ncol=2, sep=""\\t"", file=filename)\r\n    write.table(ranked.dat, row.names=T, col.names=F, quote=F, sep=""\\t"", file=filename, append=T)  \r\n  }\r\n  message(""DONE"") \r\n}\r\n\r\n# Estimate component probabilities for a new sample\r\nEstimateComponents <- function(sparse.counts, alpha, phi, Niter) {\r\n  \r\n  C <- nrow(phi)\r\n  Ndata <- length(sparse.counts)\r\n  \r\n  z <- sample(1:C, Ndata, replace=TRUE)\r\n  n <- rep(0, C)\r\n  temp <- table(z)\r\n  n[as.numeric(names(temp))] <- temp\r\n  logmeanprob <- rep(NA, Niter)\r\n  for (i in 1:Niter) {\r\n    if (i %% 10 == 0) cat(i,"".."")\r\n    for (d in 1:Ndata) {\r\n      # Get word and subtract count\r\n      n[z[d]] <- n[z[d]] -1\r\n      # Compute probs for components\r\n      probs <- rep(0, C)\r\n      for (c in 1:C) \r\n        probs[c] <- (n[c] + alpha)*phi[c, sparse.counts[d]]\r\n      \r\n      # Sample new component and add count\r\n      z[d] <- multinom.single(probs)\r\n      n[z[d]] <- n[z[d]] +1\r\n      logmeanprob[i] <- log(probs[z[d]]/sum(probs))\r\n    }\r\n    \r\n  }\r\n  cat(""DONE\\n"")\r\n  #  theta <- (n + alpha)/ sum(n + C*alpha)\r\n  theta <- (n + alpha)/ sum(n + alpha)\r\n  return(list(z=z, n=n, theta=theta, logmeanprob=logmeanprob))\r\n}\r\n\r\nmultinom.single <- function(prob) {\r\n  cs <- cumsum(prob)\r\n  which.max(runif(1) <= cs/cs[length(cs)])\r\n}\r\n\r\n\r\n# Get sparse counts from count matrix\r\nGetSparseCounts <- function(counts) {\r\n  \r\n  sparse.counts <- c()\r\n  if (max(counts)>0) {\r\n    for (m in 0:(max(counts)-1)) {\r\n      temp <- which((counts-m)>0)\r\n      sparse.counts <- c(sparse.counts, temp)\r\n    }\r\n  }\r\n  return(sparse.counts)\r\n}\r\n\r\n# Convert FDR q-values to counts\r\nConvertFDRtoCounts <- function(fdr.values, genesets, max.count) {\r\n  \r\n  counts <- c()\r\n  for (direction in c(""pos"", ""neg"")) {\r\n    \r\n    # Convert FDR q-values to counts\r\n    fdr.raw <- fdr.values[[direction]]\r\n    fdr.df <- droplevels(subset(fdr.raw, NAME %in% genesets))\r\n    counts.temp <- round(-log2(fdr.df$FDR.q.val)) -1\r\n    counts.temp[which(counts.temp==-1 | counts.temp==-Inf)] <- 0\r\n    counts.temp[which(counts.temp==Inf)] <- max.count\r\n    \r\n    # Match to genests and add to results\r\n    counts.res <- rep(0, length(genesets))\r\n    counts.res[match(fdr.df$NAME, genesets)] <- counts.temp\r\n    counts <- c(counts, counts.res)\r\n  } \r\n  return(counts)\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n']",0,"Macroevolution, axial morphospace, innovations, marine environments, elapid snakes, Hydrophis-Microcephalophis clade, body shape variation, forebody, hindbody, girths, burrowing prey, sympatric"
Data for: Macroevolution in axial morphospace: Innovations accompanying the transition to marine environments in elapid snakes,"Sea snakes in the Hydrophis-Microcephalophis clade (Elapidae) show exceptional body shape variation along a continuum from similar forebody and hindbody girths, to dramatically reduced girths of the forebody relative to hindbody. The latter is associated with specialisations on burrowing prey. This variation underpins high sympatric diversity and species richness and is not shared by other marine (or terrestrial) snakes. Here, we examined a hypothesis that macroevolutionary changes in axial development contribute to the propensity, at clade level, for body shape change. We quantified variation in the number and size of vertebrae in two body regions (pre- and post-apex of the heart) for ~94 terrestrial and marine elapids. We found Hydrophis-Microcephalophis exhibit increased rates of vertebral evolution in the pre- versus post-apex regions compared to all other Australasian elapids. Unlike other marine and terrestrial elapids, axial elongation in Hydrophis-Microcephalophis occurs via the preferential addition of vertebrae pre heart apex, which is the region that undergoes concomitant shifts in vertebral number and size during transitions along the relative fore- to hindbody girth axis. We suggest that this macroevolutionary developmental change has potentially acted as a key innovation in Hydrophis-Microcephalophis by facilitating novel (especially burrowing) prey specialisations that are not shared with other marine snakes.","['# Analysing rates of phenotypic evolution in vertebral number per- and post-apex (fore and hind body) of Elapid snakes\n\n# Code by Emma Sherratt September 2020\n# emma.sherratt@gmail.com\n# Updated July 2022\n\n# Libraries\nlibrary(ape) # v. 5.6-2\nlibrary(phytools) # 1.0-3\nlibrary(geomorph) # 4.0.4\n# Set WD to source file location\n# read in vertebral count data\nvdata <- read.csv(""All Elapid Vertebra Means.csv"", header = TRUE, row.names = 1)\n# Averaged number of vertebrae for 94 species of elapid snake. Group corresponds to terrestrial or marine (including semi-aquatic species). Group2 corresponds to the four groups, terrestrial species and the three main groups of marine species: semi-aquatic = Ephalophis, Hydrelaps and Parahydrophis, marine2 = Aipysurus + Emydocephalus clade, and royal blue is marine1 = Hydrophis + Microcephalophis clade. Measurer of specimen is initials of author. N is number of specimens. Totalv = total number of vertebrae. Precloacav = number of pre-cloacal vertebrae. Preatrialv = number of pre-apex vertebrate, i.e., in forebody. Postatrialv = number of post-apex vertebrate, i.e., in hindbody. Tailv = number of vertebrae in tail. Finally, species are indicated whether they are in shape dataset (Y) or not. \n\nvdata <- vdata[complete.cases(vdata[,(4:6)]),] # 94 species\n    vdata$group <- factor(vdata$group)\n    vdata$group2 <- factor(vdata$group2)\n    gp.col <- rep(""forest green"", length(vdata$group)) \n    gp.col[which(vdata$group == ""Marine"")] <- ""royal blue""\n    names(gp.col) <- rownames(vdata)\n    gp.col2 <- gp.col\n    gp.col2[which(vdata$group2 == ""Marine2"")] <- ""cyan""\n    gp.col2[which(vdata$group2 == ""Semi-aquatic"")] <- ""brown""\n\n    # General statistics\n    mean(vdata$precloacav[which(vdata$group == ""Marine"")]) # 182.7721\n    sd(vdata$precloacav[which(vdata$group == ""Marine"")]) # 28.4227\n    mean(vdata$precloacav[which(vdata$group == ""Terrestrial"")]) # 181.4451\n    sd(vdata$precloacav[which(vdata$group == ""Terrestrial"")]) # 43.44748\n    \n# Dual plot: A) Number of precloacal vertebrae in Marine vs Terrestrial\n# B) plot pre vs post atrial number of vertebrae by ecology group\nlayout(matrix(c(1:2), ncol=2))\n# A\nboxplot(vdata$precloacav ~ vdata$group, ylab = ""Number of precloacal vertebrae"",\n     xlab= ""Habitat"", col=c(""blue"", ""forest green""))\n# B\nplot(vdata$preatrialv, vdata$postatrialv, pch=21, bg=gp.col2,\n     xlab = ""Number of preatrial vertebrae"",\n     ylab = ""Number of postatrial vertebrae"", asp=1, ylim=c(50,250), xlim=c(10,140))\nlegend(80,200, legend = levels(vdata$group2), pch=21,pt.bg=c(""royal blue"", ""cyan"", ""brown"", ""forest green""), bty=""n"")\n    # text(vdata$preatrialv, vdata$postatrialv, c(1:nrow(vdata)))\n\n\n# FIGURE 1\n# A) Number of forebody vertebrae in Marine1, Marine2, SemiAq vs Terrestrial\n# B) Number of hindbody vertebrae in Marine1, Marine2, SemiAq vs Terrestrial\nlayout(matrix(c(1,1,2,3), ncol=2, byrow = TRUE))\npar(mar=c(5, 0, 0, 0))\nboxplot(vdata$precloacav ~ vdata$group, xlab = ""Precloacal vertebrae"", ylab="""",\n        col=c(""blue"", ""forest green""), yaxt=\'n\', horizontal = TRUE, frame=FALSE, ylim=c(100, 350), cex.axis=1.5, cex.lab=1.5)\npar(mar=c(5, 0, 0, 0)) #c(bottom, left, top, right)\nboxplot(vdata$preatrialv~ vdata$group2, xlab = ""Forebody vertebrae"", \n        col=c(""royal blue"", ""cyan"", ""brown"", ""forest green""), horizontal = TRUE, \n        frame=FALSE, yaxt=\'n\', ylim=c(150,0), cex.axis=1.5, cex.lab=1.5)\npar(mar=c(5, 0, 0, 0))\nboxplot(vdata$postatrialv ~ vdata$group2, xlab = ""Hindbody vertebrae"",\n        col=c(""royal blue"", ""cyan"", ""brown"", ""forest green""), horizontal = TRUE, \n        frame=FALSE, yaxt=\'n\',ylim=c(50,250), cex.axis=1.5, cex.lab=1.5)\n# Phylogenetic analyses\n# Read in Elapid tree (Lee et al 2016 tree + new taxa from James N July 2022)\ntree <- read.nexus(""Elapid_tree_July2022.nex"") # 183 taxa\n\nrow.names(vdata)[which(is.na(match(row.names(vdata), tree$tip.label)))] # species not in tree, no seq info available\n# [1] ""Brachyurophis_fasciolatus"" ""Denisonia_maculata""        ""Hoplocephalus_stephensii"" \n# [4] ""Salmonelaps_par""           ""Hydrophis_melanosoma""   \n\n# row.names(vdata)[which(row.names(vdata)==""Brachyurophis_fasciolatus"")] <- ""Brchyurophis"" # replace with closely related species?\n\nrow.names(vdata)[which(row.names(vdata)==""Denisonia_maculata"")] <- ""Denisonia_devisi"" # replace with closely related species\n\ntaxaintree <- row.names(vdata)[which(!is.na(match(row.names(vdata), tree$tip.label)))]\n\ntaxanotintree <- tree$tip.label[which(is.na(match(tree$tip.label, row.names(vdata))))]\ntree <- drop.tip(tree, tip=taxanotintree) # 90 taxa of 94\nlayout(1); plot(tree) \n\nvdata.phylo <- vdata[!is.na(match(row.names(vdata), tree$tip.label)),]\nvdata.phylo <- vdata.phylo[tree$tip.label,]\nmatch(row.names(vdata.phylo), tree$tip.label) == 1:90 # perfect!\n\n# Rates of evolution - using geomorph functions\necology <- vdata.phylo$group\n    names(ecology) <- row.names(vdata.phylo)\necology2 <- vdata.phylo$group2\n    names(ecology2) <- row.names(vdata.phylo)\nprecloacav <- vdata.phylo$pr', '# Analysis of intracolumnar vertebra length across species of Elapid snakes\n\n# Code by Emma Sherratt April 2021 - August 2021\n# emma.sherratt@gmail.com\n# Updated July 2022\n\n# Libraries\nlibrary(ape) # v. 5.6-2\nlibrary(geomorph) # 4.0.4\n\n# Set WD to source file location\n# read in classifier\nclassifier <- read.csv(""Classifier.csv"", header = TRUE)\n\n# read in averaged interlmk distances\nsetwd(""Macrovert raw lindist"")\nfilelist <- list.files(pattern = ""_lindist.csv"", recursive=TRUE)\n# 69 specimens\nfile.labels <- matrix(unlist(strsplit(filelist, ""_"")), ncol=7, byrow = TRUE)\ncolnames(file.labels) <- c(""Genus"", ""species"", ""Museum"", ""ID"", ""rep"", ""digitiser"", ""suffix"")\n\n# Extract new simpler label\nlabel <- paste(file.labels[,1],file.labels[,2],file.labels[,3],file.labels[,4], sep=""_"")\n\n# Read in each and save as element of a list\ncurves <- vector(""list"", length(filelist)) # empty list object\nfor(i in 1: length(filelist)){\n  tmp <- as.matrix(read.csv(filelist[i], header = TRUE, row.names = 1))\n  tmp <- data.frame(tmp, (tmp / sum(tmp)) * 100) # original data in x, scaled data in x.1\n  curves[[i]] <- tmp\n}\nnames(curves) <- label\nremove(tmp, i)\n\nclassifier <- classifier[match(filelist, classifier$Filename),]\nclassifier <- classifier[complete.cases(classifier),] # 61 specimens\n\n# Make short label vector\nlabel.small <- paste(substr(classifier$Genus, 1,3), substr(classifier$Species, 1,3), sep=""_"")\n\n# Make species label\nspp.label <- paste(classifier$Genus, classifier$Species, sep = ""_"")\n\n# Make colour vector\nspp.col <- rainbow(length(spp.label)) ; names(spp.col)<- label.small\n\n# Make colour vector for habitat\ncolours <- c(""blue"", ""cyan"", ""brown"", ""green"")\nnames(colours) <- levels(as.factor(classifier$Habitats))\nhab.col <- colours[match(as.factor(classifier$Habitats), names(colours))]\nnames(hab.col) <- spp.label\n\n# Snake sizes\nbody.lengths <- NULL\nfor(i in 1:length(label)){ body.lengths <- c(body.lengths, sum(curves[[i]]$x)) }\nnames(body.lengths) <- label\n\n# Plot each specimen separately and save as a PDF\npdf(""All.pdf"",width=6,height=4,paper=\'special\')\nfor(i in 1:length(label)){\n  tmp <- curves[[i]]\n  plot(x=c(1:length(tmp$x.1)), y=tmp$x.1, pch=21, bg=spp.col[i], ylab=""vertebra length"", xlab=""vertebra number"")\n  title(label[i]) }\ndev.off()\n\n# Plot each specimen separately in a multipanel plot\n# Needs plot window to be full screen height\nlayout(mat=matrix(c(1:30), ncol=5))\npar(mar = c(4, 3, 3, 1))\nfor(i in 1:length(label)){\n  tmp <- curves[[i]]\n  plot(x=c(1:length(tmp$x.1)), y=tmp$x.1, pch=21, bg=spp.col[i], ylab=""vertebra length"", xlab=""vertebra number"")\n  which\n  title(label[i], cex.main=0.7)\n  arrows(x0= classifier$Heart[i], y0=0, x1=classifier$Heart[i], y1=tmp$x.1[classifier$Heart[i]],\n         length = 0.1, angle = 15,lwd=2)}\n\n# Plot all profiles in single space\nlayout(1)\npar(mar = c(5, 4, 4, 1))\nplot(curves[[50]]$x, xlim = c(1,240), ylim=c(0,1), type=""n"", ylab=""relative vertebra length"", xlab=""vertebra number"")\nfor(i in 1:length(label)){\n  tmp <- curves[[i]]\n  points(x=c(1:length(tmp$x.1)), y=tmp$x.1, pch=21, bg=spp.col[i], cex=0.8)}\n\n###################################################################################\n# Fit 4th polynomial curves and plot in single space\nplot(curves[[50]]$x, xlim = c(1,240), ylim=c(0,1), type=""n"", ylab=""relative vertebra length"", xlab=""vertebra number"")\npoly.curves <- vector(""list"", length(filelist)) # empty list object\norig.curve.vertno <- NULL\nfor(i in 1:length(label)){\n  tmp <- curves[[i]]\n  # Curve fitting \n  x=c(1:length(tmp$x.1))\n  y=tmp$x.1\n  #fourth degree\n  fit4 <- lm(y~ poly(x,4,raw=TRUE))\n  # generate range of numbers length of curve\n  xx <- seq(1,length(x), length=length(x))\n  # Predict values of y\n  yy <- predict(fit4, data.frame(x=xx))\n  # plot\n  lines(xx, yy, col=spp.col[i])\n  text(x=-0.1, y=yy[1], labels=label.small[i], \n       col=spp.col[i], cex=0.7, pos=2)\n  poly.curves[[i]] <- yy # save new y values\n  # save original length of profile\n  orig.curve.vertno <- c(orig.curve.vertno, length(x))\n}\nnames(poly.curves) <- spp.label\nnames(orig.curve.vertno) <- spp.label\n\n# Plot each specimen separately, with poly line and save as a PDF\npdf(""All w poly.pdf"",width=6,height=4,paper=\'special\')\nfor(i in 1:length(spp.label)){\n  tmp <- curves[[i]]\n  x=c(1:length(tmp$x.1))\n  plot(x=x, y=tmp$x.1, pch=21, bg=spp.col[i], ylab=""vertebra length"", xlab=""vertebra number"")\n  lines(x=x, y=poly.curves[[i]], col=spp.col[i])\n  title(label[i]) }\ndev.off()\n\n############################################################################# \n# Need to standardise profiles to same number of vertebrae for comparison \nN=100\n  scaled.standard.curves <- array(NA, dim=c(N,2,length(label)))\n  dimnames(scaled.standard.curves)[[3]] <- spp.label\n  for(i in 1:length(label)){\n    tmp <- curves[[i]]\n    # Curve fitting \n    x=c(1:length(tmp$x.1))\n    y=tmp$x.1\n    #fourth degree\n    fit4 <- lm(y~ poly(x,4,raw=TRUE))\n    # generate range of numbers length of curve\n    xx <- seq(1,length(x), length=N)\n    # Predict values of ']",0,"Fieldwork
- Money spiders
- Wolf spiders
- Barley fields
- South Wales
- Araneae
- Linyphiidae
- Lycosidae
- Transects
- Crop tramline"
Evidence for nutrient-specific foraging of predators under field conditions,"FieldworkMoney spiders (Araneae: Linyphiidae) and wolf spiders (Araneae: Lycosidae) were visually located along transects in two adjacent barley fields at Burdons Farm, Wenvoe in South Wales (5126'24.8""N, 316'17.9""W) and collected from occupied webs and the ground between April and September 2018. Each belt transect was adjacent to a randomly selected crop tramline and were distributed across the entire field and ran its length. The areas searched were 4 m2 quadrats at least 10 m apart and all observed linyphiids and lycosids were collected. Spiders were taken from 64 randomly selected locations along the aforementioned transects. Following collection of spiders, 4 m2 of ground and crop stems was suction sampled for approximately 30 seconds, with the collected material emptied into a bag and any organisms immediately killed with ethyl-acetate. Suction sampling used a G-vac modified garden leaf-blower. All material was later frozen at -20 C for storage before sorting in the lab. These invertebrates were collected for background population densities and not for any molecular work.All invertebrates were identified to family level. Further identifications were not carried out due to the inability to identify some of the invertebrate groups further via the associated metabarcoding-derived dietary data (e.g., Sciaridae), and the difficulty associated with finer taxonomic resolution of many damaged or immature specimens. The only taxa not identified to family level were springtails of the superfamily Sminthuroidea (Sminthuridae and Bourletiellidae, which were often indistinguishable following suction sampling and preservation due to the fine features necessary to differentiate them) which were left at super-family, mites (many of which were immature or in poor condition, or lacked appropriate taxonomic keys) which were identified to order level and wasps of the superfamily Ichneumonoidea (which were identified no further due to obscurity of wing venation due to damage); in these cases, these taxonomic assignments were pooled to family-level for later analyses. Extraction and high-throughput sequencing of spider gut content DNAGiven their prevalence in field collections, dietary analysis was carried out for the linyphiid spider genera Erigone, Tenuiphantes, Bathyphantes and Microlinyphia (Araneae: Linyphiidae), and Pardosa (Araneae: Lycosidae). Spiders were transferred to and washed in fresh 100% ethanol to reduce external contaminants prior to identification via morphological keys(1). Abdomens were removed from spiders and again transferred to and washed in fresh 100% ethanol. DNA was extracted from the abdomens via Qiagen TissueLyser II and DNeasy Blood & Tissue Kit (Qiagen) as per the manufacturer protocol, but with an extended lysis time of 12 hours to account for the complex and branched gut system in spider abdomens(2).For amplification of DNA, two primer pairs were used. BerenF-LuthienR(3) amplified a broad range of invertebrates including spiders, and TelperionF-LaureR, amplified a range of invertebrates with the exception of some spiders (modified from TelperionF-LaurelinR(3) (via one base-pair change to decrease host DNA amplification; 5-ggrtawacwgttcawccagt-3). These two primer pairs amplified 314 bp (BerenF-LuthienR) and 302 bp (TelperionF-LaureR) regions of COI. Primers were labelled with unique 10 bp molecular identifier tags (MID-tags) so that each individual had a unique pairing of forward and reverse for identification of each spider post-sequencing. PCR reactions of 25 l volumes contained 12.5 l Qiagen PCR Multiplex kit, 0.2 mol (2.5 l of 2 M) of each primer and 5 l template DNA. Reactions were carried out in the same thermocycler, optimized via temperature gradient, with an initial 15 minutes at 95 C, 35 cycles of 95 C for 30 seconds, the primer-specific annealing temperature for 90 seconds and 72 C for 90 seconds, respectively, followed by a final extension at 72 C for 10 minutes. BerenF-LuthienR and TelperionF-LaureR used annealing temperatures of 52 C and 42 C, respectively.Within each PCR 96-well plate, 12 negative controls (extraction and PCR), 2 blank controls and 2 positive controls were included (i.e. 80 samples per plate), based on Taberlet et al.(4). Positive controls were mixtures of invertebrate DNA comprised of non-native Asiatic species in four different proportions (Table S1) and blanks were empty wells within each plate to identify tag-jumping into unused MID-tag combinations. PCR negative controls were DNase-free water treated identically to DNA samples. A negative control was present for each MID-tag to identify any contamination of primers. All PCR products were visualized in a 2 % agarose gel with SYBRSafe (Thermo Fisher Scientific, Paisley, UK) and placed in categories based on their relative brightness. The concentration of these brightness categories was quantified via Qubit dsDNA High-sensitivity Assay Kits (Thermo Fisher Scientific, Waltham, MA, USA) with at least three representatives of each category per plate. The PCR products were then proportionally pooled according to these concentrations. Each pool was cleaned via SPRIselect beads (Beckman Coulter, Brea, USA), with a left-side size selection using a 1:1 ratio (retaining ~300-1000 bp fragments). The concentration of the pooled DNA was then determined via Qubit dsDNA High-sensitivity Assay Kits and pooled together into one library per primer pair. Library preparation for Illumina sequencing was carried out on the cleaned libraries via NEXTflex Rapid DNA-Seq Kit (Bioo Scientific, Austin, USA) and samples were sequenced on an Illumina MiSeq via a V3 chip with 300-bp paired-end reads (expected capacity 25,000,000 reads). Bioinformatic analysisThe Illumina run generated 11,165,405 and 10,959,010 reads for BerenF-LuthienR and TelperionF-LaureR, respectively, which were quality-checked and paired via FastP(5) to retain only sequences of at least 200 bp with a quality threshold of 33, resulting in 10,561,874 and 9,355,112 paired reads. The paired reads were demultiplexed and assigned to their respective spider sample according to their MID-tags via the trim.seqs command in Mothur v1.39.5(6), leaving 7,854,610 and 7,437,929 reads with exact matches to the primer and MID-tags.Replicates were removed, and denoising and clustering to amplicon sequence variants (ASVs; clustered without % identity to avoid multiple species represented within a single operational taxonomic unit (OTU)) completed via Unoise3 in Usearch11(7). The resultant sequences were assigned a taxonomic identity from GenBank via BLASTn v2.7.1(8) using a 97% identity threshold(9). The BLAST output was analyzed in MEGAN v6.15.2(10). Where the top BLAST hit, determined by lowest e-value, was resolved at a higher taxonomic level than species-level, the results were checked; where possibly erroneous entries were preventing species-level assignment (e.g., poorly resolved identifications on GenBank), finer resolution was assigned based on the next-closest match. Where ASVs were assigned the same taxon, these were aggregated.Data clean-up followed the protocol described as optimal by Drake et al.(11). The maximum value for an ASV present in blank or negative controls was identified and subtracted from all read counts for that ASV to remove background contaminants. Simultaneously, known lab contaminants (e.g., German cockroach Blattella germanica), artefacts and errors of the sequencing process, unexpected reads in positive controls and positive control taxon reads in dietary samples were identified. These were calculated as a percentage of their respective samples read count and any read counts lower than the highest of these percentages for their respective sample were removed to eliminate additional instances of contamination. These thresholds were defined as 0.38% and 0.39% for BerenF-LuthienR and TelperionF-LaureR, respectively. The data from the two libraries (i.e., from each primer pair) were then aggregated together by sample and aggregated again by taxon. Non-target taxa (e.g., fungi) and instances in which predator DNA was amplified (i.e. ASVs with high read counts matching the individuals morphological identity) were removed. All remaining read counts were converted to presence-absence. Macronutrient determinationSpecimens were taken for macronutrient analysis from the same suction samples collected for invertebrate community identification. Representatives were taken from each family found in the community samples for which specimens were intact, in visually good condition and relatively clean of soil and other contaminants. If specimens were from a relatively uncommon family but unclean, soil and other surface contaminants were physically removed, and the specimen then momentarily dipped in water to remove remaining surface contaminants without greatly dislodging surface lipids. Macronutrient contents were determined following the MEDI protocol(12, 13) with minor alterations to account for the small size of most of the invertebrates processed(14). During extraction, half volumes (i.e. 500 l) of solvents were used. For the lipid assays, 15 l of sulfuric acid was added for a 15 min incubation, followed by only 200 l of vanillin reagent to increase the concentration and development of analyte for more accurate readings from smaller invertebrates. Lipid and protein standard series were diluted to 50% of the concentration specified in the original protocol (i.e. 0-1 mg ml-1). Carbohydrate assays used 140 l of reagent with 30 min incubation at 92 C followed by a further 30 min at room temperature. Carbohydrate standard series were diluted to 1% of the concentrations specified in the original protocol (i.e. 0-0.02 mg ml-1) to ensure signals overcame the higher limit of detection relative to typical invertebrate carbohydrate content. Statistical analysisAll analyses were conducted in R v.4.0.3(15). In situ spider prey choice was analyzed using network-based null models in econullnetr(16) with the generate_null_net command. A bespoke set of functions was used alongside econullnetr to randomly generate an expected diet for each individual spider based on local prey communities determined via suction sampling. Macronutrient data were allocated to each dietary taxon and the mean macronutrient proportions calculated. The mean macronutrient contents were compared between expected and observed diets using a multivariate linear model (MLM) via mvabund(17) and significant differences visually represented through ternary plots using ggtern(18) and ggplot2(19). The observed mean nutrient proportions of spider diets were compared between spider genera, life stages and sexes using a MLM. To ascertain how prey choice factors into these dietary differences, the difference in macronutrient proportions between expected and observed spider diets were also compared between spider genera, life stages and sexes in a MLM.To group taxa into tropho-species, mean macronutrient values for each taxon were first determined to prevent splitting of taxa across clusters; these were represented at the family, order and class levels to allow tropho-species assignment for families for which macronutrient content was not determined, but was at a higher level. Macronutrient values were scaled by subtracting the mean of each column from each contained value and dividing it by the column standard deviation using the scale function. A Euclidean distance matrix was calculated using the dist function. Hierarchical clustering of scaled macronutrient distance matrix used the hclust function. Optimal clustering solutions were determined by comparison of Dunns index between methods and k values; this was calculated using the dunn function in the clValid package(20) for each cluster k value above five until the Dunn index decreased, the first instance of the value preceding the decrease deemed the maximum value, thus optimal solution. Clustering solutions based on average, complete, single, median, centroid and mcquitty linkages were compared, and the complete method selected for subsequent analysis as it resulted in the smallest number of clusters (20; thus, the most efficient simplification of the taxa analyzed). Three uncommon families (present in small numbers in one community sample each, but no dietary detections) were removed from further tropho-species analyses due to the lack of class-level macronutrient data (Arionidae, Lithobiidae and Polydesmidae).To name the tropho-species, a second clustering stage was used in which the tropho-species were grouped according to their mean macronutrient content for each of the three nutrients separately. Single linkage clustering was found to be the optimal method for this step and created ten, seven and six groups for carbohydrate, lipid and protein, respectively. These clusters were labelled from one to the total number of clusters for each macronutrient to represent low-to-high content of that nutrient relative to other tropho-species. Names used the structure CxLyPz to denote the relative content of each tropho-species (x, y and z replaced with the cluster number for carbohydrate, lipid and protein, respectively).Clusters were henceforth termed tropho-species, with all taxa within a single cluster representing a single aggregated tropho-species. Heatmap dendrograms were produced using the heatmap.2 function in the gplots package(21), with cluster colors assigned with the Accent palette of RColorBrewer(22) and relative macronutrient content color scaling produced using the viridis package(23). Ternary plots were produced to visualize the macronutrient content of taxa within each cluster, and differences in mean macronutrient contents between tropho-species.Tropho-species were assigned to each taxon present in dietary and prey community samples. Where family-level macronutrient data were not obtained (usually low abundance and poor condition invertebrates or families identified in the diet that were not subsequently observed in community samples), order-level tropho-species assignment was used, or class where order-level data were not available (12 and 2 instances of uncommon taxa, respectively).In situ spider prey choice with respect to tropho-species was analyzed using network-based null models in econullnetr(16) with the generate_null_net command, visually represented with the plot_preferences command. Standardized effect sizes of prey choice for each combination of spider genus, sex and life stage, indicative of the extent of deviation from random, were extracted from the null models and compared between genera, sexes and life stages using permutational multivariate analysis of variance (PerMANOVA) via the adonis function in vegan(24). To determine any tropho-species-specific differences, these data were further analyzed via similarity percentages analysis (SIMPER), also in vegan. 1. M. J. Roberts, The Spiders of Great Britain and Ireland (Compact Edition) (Harley Books, Colchester, UK, ed. 3rd, 1993).2. H. Krehenwinkel, S. Kennedy, S. Pekr, R. G. Gillespie, A cost-efficient and simple protocol to enrich prey DNA from extractions of predatory arthropods for large-scale gut content analysis by Illumina sequencing. Methods Ecol. Evol. 8, 126134 (2017).3. J. P. Cuff, L. E. Drake, M. P. T. G. Tercel, J. E. Stockdale, P. Orozco-terWengel, J. R. Bell, I. P. Vaughan, C. T. Mller, W. O. C. Symondson, Money spider dietary choice in pre- and post-harvest cereal crops using metabarcoding. Ecol. Entomol. 46, 249261 (2021).4. P. Taberlet, A. Bonin, L. Zinger, E. Coissac, Environmental DNA (Oxford University Press, Oxford, 2018).5. S. Chen, Y. Zhou, Y. Chen, J. Gu, Fastp: An ultra-fast all-in-one FASTQ preprocessor. Bioinformatics. 34, i884i890 (2018).6. P. D. Schloss, S. L. Westcott, T. Ryabin, J. R. Hall, M. Hartmann, E. B. Hollister, R. A. Lesniewski, B. B. Oakley, D. H. Parks, C. J. Robinson, J. W. Sahl, B. Stres, G. G. Thallinger, D. J. Van Horn, C. F. Weber, Introducing mothur: open-source, platform-independent, community-supported software for describing and comparing microbial communities. Appl. Environ. Microbiol. 75, 75377541 (2009).7. R. C. Edgar, Search and clustering orders of magnitude faster than BLAST. Bioinformatics. 26, 24602461 (2010).8. C. Camacho, G. Coulouris, V. Avagyan, N. Ma, J. Papadopoulos, K. Bealer, T. L. Madden, BLAST+: architecture and applications. BMC Bioinformatics. 10, 19 (2009).9. A. Alberdi, O. Aizpurua, M. T. P. Gilbert, K. Bohmann, Scrutinizing key steps for reliable metabarcoding of environmental samples. Methods Ecol. Evol. 9, 114 (2017).10. D. H. Huson, S. Beier, I. Flade, A. Grska, M. El-Hadidi, S. Mitra, H. J. Ruscheweyh, R. Tappu, MEGAN Community Edition - interactive exploration and analysis of large-scale microbiome sequencing data. PLoS Comput. Biol. 12, 112 (2016).11. L. E. Drake, J. P. Cuff, R. E. Young, A. Marchbank, E. A. Chadwick, W. O. C. Symondson, Post-bioinformatic methods to identify and reduce the prevalence of artefacts in metabarcoding data. Authorea. April 13 (2021), doi:https://doi.org/10.22541/au.161830201.18684167/v1.12. J. P. Cuff, S. M. Wilder, M. P. T. G. Tercel, R. Hunt, S. Oluwaseun, P. S. Morley, R. A. Badell-Grau, I. P. Vaughan, J. R. Bell, P. Orozco-terWengel, W. O. C. Symondson, C. T. Mller, MEDI: Macronutrient Extraction and Determination from invertebrates, a rapid, cheap and streamlined protocol. Methods Ecol. Evol. 2021, 19 (2021).13. J. P. Cuff, S. M. Wilder, MEDI: Macronutrient Extraction and Determination from Invertebrates. Protocols.io (2021), p. 49505.14. J. P. Cuff, Further micro-scaled MEDI (macronutrient extraction and determination from invertebrates). Protocols.io (2021), , doi:https://dx.doi.org/10.17504/protocols.io.bw5hpg36.15. R Core Team, R: A language and environment for statistical computing (2020).16. I. P. Vaughan, N. J. Gotelli, J. Memmott, C. E. Pearson, G. Woodward, W. O. C. Symondson, econullnetr: an r package using null models to analyse the structure of ecological networks and identify resource selection. Methods Ecol. Evol. 9, 728733 (2018).17. Y. Wang, U. Naumann, S. T. Wright, D. I. Warton, mvabund  an R package for model-based analysis of multivariate abundance data. Methods Ecol. Evol. 3, 471474 (2012).18. N. E. Hamilton, M. Ferry, ggtern: ternary diagrams using ggplot2. J. Stat. Softw. 87, 3 (2018).19. H. Wickham, ggplot2: Elegant Graphics for Data Analysis (2016).20. G. Brock, V. Pihur, S. Datta, S. Datta, clValid: an R package for cluster validation. J. Stat. Softw. 25, 122 (2008).21. G. R. Warnes, B. Bolker, L. Bonebakker, R. Gentleman, W. Huber, A. Liaw, T. Lumley, M. Maechler, A. Magnusson, S. Moeller, M. Schwartz, B. Venables, gplots: Various R programming tools for plotting data (2020).22. E. Neuwirth, RColorBrewer: ColorBrewer palettes (2014).23. S. Garnier, viridis: default color maps from matplotlib (2018).24. J. Oksanen, F. G. Blanchet, R. Kindt, P. Legendre, P. R. Minchin, R. B. OHara, G. L. Simpson, P. Solymos, M. H. H. Stevens, E. Szoecs, H. Wagner, vegan: Community Ecology Package (2016).","['#### Modified null modelling function, April 2021 ####\r\n\r\ngenerate_null_net <- function(consumers, resources, sims = 100,\r\n                              data.type = ""names"", maintain.d = NULL,\r\n                              summary.type = ""sum"", c.samples = NULL,\r\n                              r.samples = NULL, r.weights = NULL,\r\n                              prog.count = TRUE){\r\n  \r\n  # Ensure input data are in data frame format (important when data are stored\r\n  #   in tidyverse \'tibble\' format)\r\n  consumers <- as.data.frame(consumers)\r\n  consumers[, 1] <- as.factor(consumers[, 1])\r\n  resources <- as.data.frame(resources)\r\n  if(!is.null(c.samples)) c.samples <- as.data.frame(c.samples)[, 1]\r\n  if(!is.null(r.samples)) r.samples <- as.data.frame(r.samples)[, 1]\r\n  if(!is.null(r.weights)) r.weights <- as.data.frame(r.weights)\r\n  \r\n  # --------------------------------------\r\n  # Initial error handling:\r\n  # --------------------------------------\r\n  # 1. Column names are identical (names and order):\r\n  if (!identical(colnames(consumers)[-1], colnames(resources))) {\r\n    stop(""Resource names do not match in \'consumers\' and \'resources\'"")}\r\n  if(!is.null(r.weights) && (!identical(colnames(r.weights)[-1],\r\n                                        colnames(resources)))) {\r\n    stop(""Names of \'r.weights\' do not match names of \'resources\'"")}\r\n  \r\n  # 2. Either both or neither of r.samples and c.samples are present\r\n  if (sum(is.null(r.samples), is.null(c.samples)) == 1) {\r\n    stop(""Only one of \'r.samples\' and \'c.samples\' supplied"")}\r\n  \r\n  # 3. Check resource abundance data: length and sample codes\r\n  # When sample codes are not present, \'resources\' should be a single row\r\n  if (is.null(r.samples) && (nrow(resources) != 1)) {\r\n    stop(""Resource abundances should have one row"")}\r\n  \r\n  # When sample codes are present, the the length of \'c.samples\' and \'r.samples\'\r\n  #   should match the number of rows in \'consumers\' and \'resources\' respectively,\r\n  #   and the factor levels should be the same\r\n  if(!is.null(r.samples) && {\r\n    (nrow(resources) != length(r.samples)) ||\r\n      (nrow(consumers) != length(c.samples)) ||\r\n      (!identical(sort(unique(r.samples)), sort(unique(c.samples))))}) {\r\n    stop(""There is a problem with the sample codes:\r\n\'c.samples\' and/or \'r.samples\'\r\n       are of incorrect length or their sample codes do not match"")}\r\n  \r\n  # 4. Abundance weights (typically forbidden link values) are in the range 0 - 1\r\n  if(!is.null(r.weights)) {\r\n    if(max(r.weights[, -1]) > 1 || min(r.weights[, -1]) < 0) {\r\n      stop(""Abundance weights (\'r.weights\') must be between 0 and 1"")}}\r\n  \r\n  # 5. Issue a warning (cf. error) if a resource is consumed where the recorded\r\n  #    abundance of the resource = 0 i.e. a resource that cannot be consumed in\r\n  #    the null model. This may or may not require action by the user.\r\n  if (!is.null(c.samples)) {\r\n    spp.consumed <- stats::aggregate(consumers[, -1], by = list(c.samples), sum)\r\n    spp.consumed <- spp.consumed[order(spp.consumed$Group.1, decreasing = FALSE), ]\r\n    all.res <- cbind(r.samples, resources)\r\n    all.res <- all.res[order(all.res$r.samples, decreasing = FALSE), ]\r\n    if(sum(spp.consumed[, -1] > 0 & all.res[, -1] == 0) > 0) {warning(\r\n      ""One or more instances detected where a consumer interacted with a\r\n       resource that has zero abundance in \'resources\'"")}\r\n  } else {\r\n    spp.consumed <- colSums(consumers[, -1])\r\n    all.res <- resources\r\n    if(sum(spp.consumed >0 & all.res == 0) > 0) {warning(\r\n      ""There is at least one instance where a resource was consumed but\r\n      was zero in the abundance data (i.e. \'resources\')"")}\r\n  }\r\n  \r\n  # 6. Correct types of data for data.type = ""names"" and ""counts""\r\n  if(data.type == ""names"" & {sum(consumers[, -1] == 1 | consumers[, -1] == 0) !=\r\n      (nrow(consumers) * ncol(consumers[, -1]))}) {\r\n    stop(""Entries in the consumer data should equal 0 or 1"")}\r\n  if(data.type == ""counts"" & {sum(consumers[, -1] - round(consumers[, -1], 0)) > 0}) {\r\n    stop(""Entries in the consumer data should be integers"")}\r\n  # --------------------------------------\r\n  \r\n  # --------------------------------------\r\n  # Set up a data frame that summarises the consumer data, containing:\r\n  #   1. The original order of the data; 2. Consumer species; 3. Number of\r\n  #      interactions per individual and total interactions (sum); 4. Sample\r\n  #     codes (or ""A"" if samples not specified)\r\n  if (!is.null(c.samples)){\r\n    c.summary <- data.frame(ord = seq(1, nrow(consumers)), c.sample = c.samples,\r\n                            species = consumers[, 1],\r\n                            links = rowSums(consumers[, 2:ncol(consumers)] != 0),\r\n                            total = rowSums(consumers[, 2:ncol(consumers)]))\r\n  } else {\r\n    c.summary <- data.frame(ord = seq(1, nrow(consumers)),\r\n                            c.sample = rep(""A"", nrow(consumers)),\r\n                            species = consumers[, 1],\r\n                            ', '##### Libraries #####\r\n\r\nlibrary(\'mvabund\')\r\nlibrary(\'ggtern\')\r\nlibrary(\'vegan\')\r\nlibrary(""flashClust"")\r\nlibrary(""dendextend"")\r\nlibrary(""plyr"")\r\nlibrary(""dplyr"")\r\nlibrary(""ggplot2"")\r\nlibrary(""viridis"")\r\nlibrary(""RColorBrewer"")\r\nlibrary(""cluster"")\r\nlibrary(""cooccur"")\r\nlibrary(""ggrepel"")\r\nlibrary(""clValid"")\r\nlibrary(""econullnetr"")\r\nlibrary(""gplots"")\r\nlibrary(""ggalt"")\r\nlibrary(""car"")\r\nlibrary(""scales"")\r\nlibrary(""Rcpp"")\r\n\r\n##### Tropho-species cluster method determination #####\r\n\r\ntropho <- read.csv(""Mean macros per taxon.csv"")\r\nrownames(tropho) <- tropho[,1]\r\ntropho_taxon <- tropho$Taxon\r\ntropho <- tropho[2:4]\r\nsummary(tropho)\r\n\r\n\r\ntropho_sc <- as.data.frame(scale(tropho))\r\nsummary(tropho_sc)\r\n\r\ntrophodist<- dist(tropho_sc, method = ""euclidean"")\r\n\r\n# average\r\n\r\ntrophotreeAVG <- hclust(trophodist, method = ""average"")\r\nplot(trophotreeAVG, main="""")\r\n\r\nx <- c(3:43)\r\nfor (i in x) {\r\n  trophocut_avg <- cutree(trophotreeAVG, k = i )\r\n  trophodunn <- dunn(distance= trophodist, clusters = trophocut_avg, method= \'euclidean\') \r\n  print(trophodunn)\r\n}\r\n\r\nplot(trophotreeAVG, main="""")\r\nrect.hclust(trophotreeAVG, k = 41, border = 2:28)\r\n\r\ntrophocut_avg41 <- cutree(trophotreeAVG, k = 41)\r\ntrophodunn_avg41 <- dunn(distance= trophodist, clusters = trophocut_avg41, method= \'euclidean\') \r\ntrophodunn_avg41\r\n\r\n\r\n# single\r\n\r\ntrophotreeSIN <- hclust(trophodist, method = ""single"")\r\nplot(trophotreeSIN, main="""")\r\n\r\nx <- c(5:50)\r\nfor (i in x) {\r\n  trophocut_sin <- cutree(trophotreeSIN, k = i )\r\n  trophodunn <- dunn(distance= trophodist, clusters = trophocut_sin, method= \'euclidean\') \r\n  print(trophodunn)\r\n}\r\n\r\nrect.hclust(trophotreeSIN, k = 5, border = 2:28)\r\n\r\n\r\ntrophocut_sin5 <- cutree(trophotreeSIN, k = 5)\r\ntrophodunn_sin5 <- dunn(distance= trophodist, clusters = trophocut_sin5, method= \'euclidean\') \r\ntrophodunn_sin5\r\n\r\n\r\n# complete\r\n\r\ntrophotreeCOM <- hclust(trophodist, method = ""complete"")\r\nplot(trophotreeCOM, main="""")\r\n\r\nx <- c(5:21)\r\nfor (i in x) {\r\n  trophocut_com <- cutree(trophotreeCOM, k = i )\r\n  trophodunn <- dunn(distance= trophodist, clusters = trophocut_com, method= \'euclidean\') \r\n  print(trophodunn)\r\n}\r\n\r\nrect.hclust(trophotreeCOM, k = 20, border = 2:28)\r\n\r\ntrophocut_com20 <- cutree(trophotreeCOM, k = 20)\r\ntrophodunn_com20 <- dunn(distance= trophodist, clusters = trophocut_com20, method= \'euclidean\') \r\ntrophodunn_com20\r\n\r\n\r\n# mcquitty\r\n\r\ntrophotreeMCQ <- hclust(trophodist, ""mcquitty"")\r\nplot(trophotreeMCQ, main="""")\r\n\r\nx <- c(5:50)\r\nfor (i in x) {\r\n  trophocut_mcq <- cutree(trophotreeMCQ, k = i )\r\n  trophodunn <- dunn(distance= trophodist, clusters = trophocut_mcq, method= \'euclidean\') \r\n  print(trophodunn)\r\n}\r\n\r\nrect.hclust(trophotreeMCQ, k = 29, border = 2:28)\r\n\r\ntrophocut_mcq29 <- cutree(trophotreeMCQ, k = 29)\r\ntrophodunn_mcq29 <- dunn(distance= trophodist, clusters = trophocut_mcq29, method= \'euclidean\') \r\ntrophodunn_mcq29\r\n\r\n\r\n# median\r\n\r\ntrophotreeMED <- hclust(trophodist, ""median"")\r\nplot(trophotreeMED, main="""")\r\n\r\nx <- c(5:40)\r\nfor (i in x) {\r\n  trophocut_med <- cutree(trophotreeMED, k = i )\r\n  trophodunn <- dunn(distance= trophodist, clusters = trophocut_med, method= \'euclidean\') \r\n  print(trophodunn)\r\n}\r\n\r\nrect.hclust(trophotreeMED, k = 31, border = 2:28)\r\n\r\ntrophocut_med31 <- cutree(trophotreeMED, k = 31)\r\ntrophodunn_med31 <- dunn(distance= trophodist, clusters = trophocut_med31, method= \'euclidean\') \r\ntrophodunn_med31\r\n\r\n\r\n# centroid\r\n\r\ntrophotreeCEN <- hclust(trophodist, ""centroid"")\r\nplot(trophotreeCEN, main="""")\r\n\r\nx <- c(5:50)\r\nfor (i in x) {\r\n  trophocut_cen <- cutree(trophotreeCEN, k = i )\r\n  trophodunn <- dunn(distance= trophodist, clusters = trophocut_cen, method= \'euclidean\') \r\n  print(trophodunn)\r\n}\r\n\r\nrect.hclust(trophotreeCEN, k = 43, border = 2:28)\r\n\r\ntrophocut_cen43 <- cutree(trophotreeAVG, k = 43)\r\ntrophodunn_cen43 <- dunn(distance= trophodist, clusters = trophocut_cen43, method= \'euclidean\') \r\ntrophodunn_cen43\r\n\r\n\r\n##### Tropho-species clustering #####\r\n\r\ntrophotreeCOM <- hclust(trophodist, method = ""complete"")\r\nplot(trophotreeCOM, main="""")\r\n\r\nx <- c(5:21)\r\nfor (i in x) {\r\n  trophocut_com <- cutree(trophotreeCOM, k = i )\r\n  trophodunn <- dunn(distance= trophodist, clusters = trophocut_com, method= \'euclidean\') \r\n  print(trophodunn)\r\n}\r\n\r\nplot(trophotreeCOM, main="""")\r\nrect.hclust(trophotreeCOM, k = 20, border = 2:28)\r\n\r\ntrophocut_com20 <- cutree(trophotreeCOM, k = 20)\r\ntrophodunn_com20 <- dunn(distance= trophodist, clusters = trophocut_com20, method= \'euclidean\') \r\ntrophodunn_com20\r\n\r\ntropho_cl <- mutate(tropho, cluster = trophocut_com20)\r\ncount(tropho_cl,cluster)\r\n\r\nggplot(tropho_cl, aes(x=Lipid, y = Protein, color = factor(cluster))) + geom_point()\r\n\r\nTrophoClusters <- table(tropho_cl$cluster,tropho_taxon)\r\n\r\nwrite.csv(TrophoClusters, ""TrophoClusters.csv"")\r\n\r\nplot(trophotreeCOM, main="""")\r\nrect.hclust(trophotreeCOM, k = 20, border = 2:28)\r\nabline(h = 0.74, col = \'red\')\r\n\r\npdf(""TS dendrogram.pdf"", width = 12, height = 8) \r\nplot(trophotreeCOM, main="""")\r\nrect.hclust(trophotreeCOM, k = 20, border = 2:28']",0,"16S rRNA, in vitro model, human dental plaque, bacterial community, oral microcosms, microbiome, variability, storage, propagation, 24-well culture plates, anaerobic atmosphere, undefined medium, nutrients, fastidious"
16S rRNA data for an in vitro model of the human dental plaque bacterial community (3 hosts),"The creation of oral microcosms with reproducible composition is important for developing model systems of the oral microbiome. Here, we report on the outcome of a methodologically simple but scientifically informative approach, in which we sample the dental plaque microbiome from 3 individuals and characterize the variability among the microbiomes after storage and subsequent propagation. We use 24-well culture plates with artificially generated pellicle under a defined anaerobic atmosphere and an undefined medium supplemented by nutrients for fastidious organisms to generate the cultures, including the initial, preserved, and propagated cultures. Harvested cultures are extracted with the Qiagen PowerSoil kit. Culture composition is determined by 16S rRNA sequencing on the Illumina MiSeq platform and the mothur pipeline. Data analysis is performed in R with the phyloseq, and vegan. Our results show that cultures from 2 out of 3 individuals cluster into an 'attractor' compositional type, and the samples from the remaining individual can adopt this compositional type after in vitro propagation, even though the original composition did not display this type. The results suggest that simple selective environments could help create reproducible microcosms from different individuals, in this case, reproducible microcosms composed of early colonizers of the dental plaque bacterial community. The attractor composition also has potential implications for synergistic interactions between members of the Streptococcus and Veillonella genera, and for antagonism between members of the Streptococcus and Prevotella genera. Together, these findings show that this dental microbiome model may be a promising start of a reproducible in vitro microbiome model that captures common ""baseline"" members of the human oral bacterial community.","['library(dplyr)\r\nlibrary(reshape)\r\nlibrary(ggplot2)\r\nlibrary(phyloseq)\r\nlibrary(broom)\r\nlibrary(data.table)\r\nlibrary(RColorBrewer)\r\nlibrary(devtools)\r\nlibrary(ggsci)\r\nlibrary(vegan)\r\nlibrary(plyr)\r\nlibrary(DT)\r\nlibrary(microbiome)\r\nlibrary(microbiomeutilities)\r\nlibrary(ggpubr)\r\nlibrary(stringr)\r\n\r\nsetwd(""C:/Users/Baoqing/Documents/Rdissert/mothur_phyloseq_analysis_BZ"")\r\n\r\n###LOADING DATA########\r\n#Silva biom classified\r\nmothur.silva.biom <- import_biom(""oral_preserve.v2.mothur.all.biom"",parseFunction=parse_taxonomy_default)\r\ncolnames(tax_table(mothur.silva.biom)) <- c(Rank1=""Domain"",Rank2=""Phylum"",Rank3=""Class"",Rank4=""Order"",Rank5=""Family"",Rank6=""Genus"")\r\noral.biom <- mothur.silva.biom\r\n\r\n#Add an updated metadata table and replace in phyloseq\r\nmetap <-read.table(""op.mothur.revised.txt"", row.names=""Sample"", check.names=FALSE, header=TRUE)\r\nsample_data(oral.biom) <- metap\r\n\r\n####HOUSEKEEPING FOR CONSISTENCY########\r\n\r\n#Issues with underscore in SILVA taxonomy, remove anything after the first underscore\r\ntax_table(oral.biom) <- gsub(""_.*"","""", tax_table(oral.biom))\r\n\r\n#Reorder factors and substitute NAs with None for Pres_sample\r\nsample_data(oral.biom)$Pres_samp <- factor(sample_data(oral.biom)$Pres_samp, levels =c(""Plaque"",""Culture"", ""ref_1"",""ref_3"",""frz_10"",""frz_39"", ""None""))\r\nsample_data(oral.biom)$Pres_samp[is.na(sample_data(oral.biom)$Pres_samp)] <- ""None""\r\n\r\n#Reorder factors for Type, for consistent symbol and color assignment\r\nsample_data(oral.biom)$Type <- factor(sample_data(oral.biom)$Type, levels =c(""Plaque"",""Culture"", ""Preserved"",""Prop"",""Prop_Control"",""Control"", ""Pellicle"",""Spiked"", ""Control_PCR"",""PBS_Gly"",""media"",""Control_ext"",""Mock_ext"",""Mock_PCR""))\r\n\r\n#Do a lot of analysis of Pres_samples, so set it up\r\n#Replace low DNA quant with NA and detected with \r\nsample_data(oral.biom)$PG <- as.numeric(sample_data(oral.biom)$PG)\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG < 0.0199] <- NA\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG > 0.0199] <- ""PG""\r\nsample_data(oral.biom)$PG <- as.factor(sample_data(oral.biom)$PG)\r\n\r\n##Set-up for consistent Genus and OTU naming\r\n#Append on an extra level to the OTU table to get Taxon_OTU#\r\ntax_table(oral.biom)<- cbind(tax_table(oral.biom), Strain=taxa_names(oral.biom))\r\nmyranks = c(""Genus"",""Strain"")\r\nmylabels = apply(tax_table(oral.biom)[,myranks],1,paste,sep="""",collapse=""_"")\r\ntax_table(oral.biom) <- cbind(tax_table(oral.biom), genusOTU=mylabels)\r\n\r\n#assign specific colors to specific Genus \r\nset.seed(132)\r\ngetPalette <- colorRampPalette(brewer.pal(9, ""Set1""))\r\nGenusList <- unique(tax_table(oral.biom)[,""Genus""])\r\nGenusList <- sample(GenusList)\r\nGenusPalette = getPalette(length(GenusList))\r\nnames(GenusPalette) = GenusList\r\n\r\n#assign specific colors to GenusOTUs\r\nOTUList <- unique(tax_table(oral.biom)[,""genusOTU""])\r\nOTUList <- sample(OTUList)\r\nGenusPalette2 = getPalette(length(OTUList))\r\nnames(GenusPalette2) = OTUList\r\n\r\n#Rarefy; 95 samples removed because they contain fewer reads than sample.size, and 791 OTUs removed\r\nrarefy <-rarefy_even_depth(oral.biom, sample.size=240, rngseed=23167, replace=FALSE, trimOTUs=TRUE)\r\n\r\n#Convert to abundances for further analyses\r\nall.abund <- transform_sample_counts(oral.biom, function(x) x/sum(x))\r\nrare.abund <- transform_sample_counts(rarefy, function(x) x/sum(x))\r\nrare.abund <- subset_samples(rare.abund, !sample_names(rare.abund) %in% c(43.1))\r\n\r\n\r\n####GENERATE FIGURE 2########\r\n\r\n\r\n#For sequencing/control/mock analyses, etc., see oral_preserve_V3_code.R\r\n\r\n#Assign a colorblind-safe palette\r\ncolorsfig2 <- c(""#CC79A7"", ""#999999"", ""#E69F00"", ""#56B4E9"", ""#009E73"", ""#F0E442"", ""#0072B2"", ""#D55E00"", ""firebrick4"")\r\ncolorsfig2test <- c(""darkorchid3"", ""magenta"", ""#CC79A7"", ""#999999"", ""#E69F00"", ""#56B4E9"", ""#F0E442"",""#009E73"", ""#0072B2"", ""#D55E00"", ""firebrick4"")\r\n\r\n#Select abundances to be included in graph, > 0.1% only\r\nfig2.rareabund <- subset_samples(rare.abund, Type%in%c(\'Plaque\',\'Culture\',\'Preserved\',\'Prop\'))\r\nfig2.rareabundh <- filter_taxa(fig2.rareabund, function (x) mean (x) > 1e-3, TRUE)\r\nfig2.rareh <- plot_bar(fig2.rareabundh, x=\'Sample\',fill=\'genusOTU\') + scale_fill_manual(values = GenusPalette2) + theme_bw()\r\n#Reorder samples by sample number\r\nfig2.rareh$data$Sample <- str_pad(fig2.rareh$data$Sample, width = 5, side = \'left\', pad = \'0\')\r\n#Plot\r\nfig2.rareh.plot <- fig2.rareh + labs(x=\'Samples\', y=\'Abundances\', size = 12) + theme(axis.text.x=element_text(angle=70, hjust=1, size=12), axis.text.y=element_text (size=12),legend.position=\'bottom\',legend.background=element_rect(size=1, linetype=\'solid\',color=\'black\'),legend.title=element_blank())\r\nfig2.rareh.hist <- fig2.rareh.plot + facet_wrap(~Host, scales=\'free_x\')\r\n\r\n#Average duplicate abundances from two different PCR plates (same original culture)\r\nfig2.rab.notest <- subset_samples(fig2.rareabund, !Name%in%c(\'H1\',\'H3\'))\r\nfig2.rabh.notest <- filter_taxa(fig2.rab.notest, function (x) mean (x) > 1e-3, TRUE)\r\nfig2.rareh.notest <- plot_bar(fig2.rabh.notest, x=\'Sample\',fill=\'g', 'library(dplyr)\r\nlibrary(reshape)\r\nlibrary(ggplot2)\r\nlibrary(phyloseq)\r\nlibrary(broom)\r\nlibrary(data.table)\r\nlibrary(RColorBrewer)\r\nlibrary(devtools)\r\nlibrary(ggsci)\r\nlibrary(vegan)\r\nlibrary(plyr)\r\nlibrary(DT)\r\nlibrary(microbiome)\r\nlibrary(microbiomeutilities)\r\nlibrary(ggpubr)\r\nlibrary(stringr)\r\n\r\nsetwd(""C:/Users/Baoqing/Documents/Rdissert/mothur_phyloseq_analysis_BZ"")\r\n\r\n###LOADING DATA########\r\n#Silva biom classified\r\nmothur.silva.biom <- import_biom(""oral_preserve.v2.mothur.all.biom"",parseFunction=parse_taxonomy_default)\r\ncolnames(tax_table(mothur.silva.biom)) <- c(Rank1=""Domain"",Rank2=""Phylum"",Rank3=""Class"",Rank4=""Order"",Rank5=""Family"",Rank6=""Genus"")\r\noral.biom <- mothur.silva.biom\r\n\r\n#Add an updated metadata table and replace in phyloseq\r\nmetap <-read.table(""op.mothur.revised.txt"", row.names=""Sample"", check.names=FALSE, header=TRUE)\r\nsample_data(oral.biom) <- metap\r\n\r\n####HOUSEKEEPING FOR CONSISTENCY########\r\n\r\n#Issues with underscore in SILVA taxonomy, remove anything after the first underscore\r\ntax_table(oral.biom) <- gsub(""_.*"","""", tax_table(oral.biom))\r\n\r\n#Reorder factors and substitute NAs with None for Pres_sample\r\nsample_data(oral.biom)$Pres_samp <- factor(sample_data(oral.biom)$Pres_samp, levels =c(""Plaque"",""Culture"", ""ref_1"",""ref_3"",""frz_10"",""frz_39"", ""None""))\r\nsample_data(oral.biom)$Pres_samp[is.na(sample_data(oral.biom)$Pres_samp)] <- ""None""\r\n\r\n#Reorder factors for Type, for consistent symbol and color assignment\r\nsample_data(oral.biom)$Type <- factor(sample_data(oral.biom)$Type, levels =c(""Plaque"",""Culture"", ""Preserved"",""Prop"",""Prop_Control"",""Control"", ""Pellicle"",""Spiked"", ""Control_PCR"",""PBS_Gly"",""media"",""Control_ext"",""Mock_ext"",""Mock_PCR""))\r\n\r\n#Do a lot of analysis of Pres_samples, so set it up\r\n#Replace low DNA quant with NA and detected with \r\nsample_data(oral.biom)$PG <- as.numeric(sample_data(oral.biom)$PG)\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG < 0.0199] <- NA\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG > 0.0199] <- ""PG""\r\nsample_data(oral.biom)$PG <- as.factor(sample_data(oral.biom)$PG)\r\n\r\n##Set-up for consistent Genus and OTU naming\r\n#Append on an extra level to the OTU table to get Taxon_OTU#\r\ntax_table(oral.biom)<- cbind(tax_table(oral.biom), Strain=taxa_names(oral.biom))\r\nmyranks = c(""Genus"",""Strain"")\r\nmylabels = apply(tax_table(oral.biom)[,myranks],1,paste,sep="""",collapse=""_"")\r\ntax_table(oral.biom) <- cbind(tax_table(oral.biom), genusOTU=mylabels)\r\n\r\n#assign specific colors to specific Genus \r\nset.seed(132)\r\ngetPalette <- colorRampPalette(brewer.pal(9, ""Set1""))\r\nGenusList <- unique(tax_table(oral.biom)[,""Genus""])\r\nGenusList <- sample(GenusList)\r\nGenusPalette = getPalette(length(GenusList))\r\nnames(GenusPalette) = GenusList\r\n\r\n#assign specific colors to GenusOTUs\r\nOTUList <- unique(tax_table(oral.biom)[,""genusOTU""])\r\nOTUList <- sample(OTUList)\r\nGenusPalette2 = getPalette(length(OTUList))\r\nnames(GenusPalette2) = OTUList\r\n\r\n#Try using a pruned, rarefied dataset\r\noral.biom.prune <- subset_samples(oral.biom, !sample_names(oral.biom) %in% c(43.1))\r\noral.biom.prune <- prune_samples(sample_sums(oral.biom.prune)>=1000, oral.biom.prune)\r\n\r\n#Rarefy\r\nrarefy <-rarefy_even_depth(oral.biom.prune, sample.size=1000, rngseed=23167, replace=FALSE, trimOTUs=TRUE)\r\n\r\n#Convert to abundances for further analyses\r\nall.abund <- transform_sample_counts(oral.biom, function(x) x/sum(x))\r\nrare.abund <- transform_sample_counts(rarefy, function(x) x/sum(x))\r\n\r\nhostlabels <- c(\'1\' = ""Host 1"", \'2\' = ""Host 2"", \'3\' = ""Host 3"")\r\n\r\n### Culturing Analysis ####\r\n#Include cultures and culture negative controls (155 samples)\r\nculture <- subset_samples(rare.abund, Type%in%c(""Culture"",""Preserved"", ""Prop""))\r\nculture <- prune_taxa(taxa_sums(culture) > 0, culture)\r\n\r\n##Color for the 5 samples\r\ncolors3 <-c(""goldenrod2"",""#762a83"", ""magenta"", ""#91bfdb"",""#4575b4"", ""black"", ""deeppink4"", ""green"",""grey30"",""lightpink2"", ""brown"", ""slategrey"",""beige"")\r\n\r\n##Ordination\r\nculture.rare.mds <- ordinate(culture, method=""MDS"", distance=""bray"")\r\nculture.rare.nmds <- ordinate(culture, method=""NMDS"", distance=""bray"", try=200)\r\n\r\nculture.rare.pcoa <- ordinate(culture, method=""PCoA"", distance=""bray"")\r\nculture.rare.pcoa.plot <- plot_ordination(culture, culture.rare.pcoa, type=""sample"",color=""Pres_samp"", shape=""Type"")\r\n##This code gives odd ""double-layered"" shapes\r\n#culture.rare.pcoa.plot.main <- culture.rare.pcoa.plot + facet_wrap(~Host, labeller = labeller(Host = hostlabels)) + scale_color_manual(values=colors3) + theme_bw() + geom_point(size = 4, stroke = 0.8) + scale_shape_discrete(solid = F) + theme(axis.title = element_text(size = 11), axis.text.y=element_text(size=10), legend.position=""right"", legend.title = element_blank(), panel.border = element_rect(size=0.5), panel.spacing.x = unit(1.5, ""lines""), strip.text = element_text(size = 10, face = ""bold""))\r\n\r\n##Plot using the following code to avoid double-layered shapes\r\n##Note: This methods does NOT give the ""percent variation accounted for"" in the axis titles, so those mus', 'library(dplyr)\r\nlibrary(reshape)\r\nlibrary(ggplot2)\r\nlibrary(phyloseq)\r\nlibrary(broom)\r\nlibrary(data.table)\r\nlibrary(RColorBrewer)\r\nlibrary(devtools)\r\nlibrary(ggsci)\r\nlibrary(vegan)\r\nlibrary(plyr)\r\nlibrary(DT)\r\nlibrary(microbiome)\r\nlibrary(microbiomeutilities)\r\nlibrary(ggpubr)\r\nlibrary(stringr)\r\n\r\nsetwd(""C:/Users/Baoqing/Documents/Rdissert/mothur_phyloseq_analysis_BZ"")\r\n\r\n###LOADING DATA########\r\n#Silva biom classified\r\nmothur.silva.biom <- import_biom(""oral_preserve.v2.mothur.all.biom"",parseFunction=parse_taxonomy_default)\r\ncolnames(tax_table(mothur.silva.biom)) <- c(Rank1=""Domain"",Rank2=""Phylum"",Rank3=""Class"",Rank4=""Order"",Rank5=""Family"",Rank6=""Genus"")\r\noral.biom <- mothur.silva.biom\r\n\r\n#Add an updated metadata table and replace in phyloseq\r\nmetap <-read.table(""op.mothur.revised.txt"", row.names=""Sample"", check.names=FALSE, header=TRUE)\r\nsample_data(oral.biom) <- metap\r\n\r\n####HOUSEKEEPING FOR CONSISTENCY########\r\n\r\n#Issues with underscore in SILVA taxonomy, remove anything after the first underscore\r\ntax_table(oral.biom) <- gsub(""_.*"","""", tax_table(oral.biom))\r\n\r\n#Reorder factors and substitute NAs with None for Pres_sample\r\nsample_data(oral.biom)$Pres_samp <- factor(sample_data(oral.biom)$Pres_samp, levels =c(""Plaque"",""Culture"", ""ref_1"",""ref_3"",""frz_10"",""frz_39"", ""None""))\r\nsample_data(oral.biom)$Pres_samp[is.na(sample_data(oral.biom)$Pres_samp)] <- ""None""\r\n\r\n#Reorder factors for Type, for consistent symbol and color assignment\r\nsample_data(oral.biom)$Type <- factor(sample_data(oral.biom)$Type, levels =c(""Plaque"",""Culture"", ""Preserved"",""Prop"",""Prop_Control"",""Control"", ""Pellicle"",""Spiked"", ""Control_PCR"",""PBS_Gly"",""media"",""Control_ext"",""Mock_ext"",""Mock_PCR""))\r\n\r\n#Do a lot of analysis of Pres_samples, so set it up\r\n#Replace low DNA quant with NA and detected with \r\nsample_data(oral.biom)$PG <- as.numeric(sample_data(oral.biom)$PG)\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG < 0.0199] <- NA\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG > 0.0199] <- ""PG""\r\nsample_data(oral.biom)$PG <- as.factor(sample_data(oral.biom)$PG)\r\n\r\n##Set-up for consistent Genus and OTU naming\r\n#Append on an extra level to the OTU table to get Taxon_OTU#\r\ntax_table(oral.biom)<- cbind(tax_table(oral.biom), Strain=taxa_names(oral.biom))\r\nmyranks = c(""Genus"",""Strain"")\r\nmylabels = apply(tax_table(oral.biom)[,myranks],1,paste,sep="""",collapse=""_"")\r\ntax_table(oral.biom) <- cbind(tax_table(oral.biom), genusOTU=mylabels)\r\n\r\n#assign specific colors to specific Genus \r\nset.seed(132)\r\ngetPalette <- colorRampPalette(brewer.pal(9, ""Set1""))\r\nGenusList <- unique(tax_table(oral.biom)[,""Genus""])\r\nGenusList <- sample(GenusList)\r\nGenusPalette = getPalette(length(GenusList))\r\nnames(GenusPalette) = GenusList\r\n\r\n#assign specific colors to GenusOTUs\r\nOTUList <- unique(tax_table(oral.biom)[,""genusOTU""])\r\nOTUList <- sample(OTUList)\r\nGenusPalette2 = getPalette(length(OTUList))\r\nnames(GenusPalette2) = OTUList\r\n\r\n#Try using a pruned, rarefied dataset\r\noral.biom.prune <- subset_samples(oral.biom, !sample_names(oral.biom) %in% c(43.1))\r\noral.biom.prune <- prune_samples(sample_sums(oral.biom.prune)>=1000, oral.biom.prune)\r\n\r\n#Rarefy\r\nrarefy <-rarefy_even_depth(oral.biom, sample.size=1000, rngseed=23167, replace=FALSE, trimOTUs=TRUE)\r\n\r\n#Convert to abundances for further analyses\r\nall.abund <- transform_sample_counts(oral.biom, function(x) x/sum(x))\r\nrare.abund <- transform_sample_counts(rarefy, function(x) x/sum(x))\r\n\r\n### Preserve and Propagate Analysis######\r\n#145 samples, 43 taxa in rarefied, remove OTUs found in mocks \r\npresprop <- subset_samples(rare.abund, Type%in%c(""Preserved"",""Culture"",""Prop""))\r\n#Remove OTUs from mocks/spike ins\r\npresprop <- subset_taxa(presprop, !Strain%in%c(""Otu003"",""Otu004""))\r\npresprop <- prune_taxa(taxa_sums(presprop) > 0, presprop)\r\n\r\n#Abundance select for those that are at least 0.1%  18 taxa\r\npresprop.h <- filter_taxa(presprop, function (x) mean (x) > 1e-3, TRUE)\r\npresprop.h4 <- filter_taxa(presprop, function (x) mean (x) > 1e-4, TRUE)\r\n\r\n\r\n#Histograms for the heck of it, reorder samples by zero-fill data to sort numerically\r\npp <- plot_bar(presprop, x=""Sample"", fill=""genusOTU"") + scale_fill_manual(values=GenusPalette2) +theme_bw()\r\npp$data$Sample <- str_pad(pp$data$Sample, width=5, side=""left"", pad=""0"")\r\npp <- pp + labs(x=""Samples"", y=""genusOTU"", size=14) +  theme(axis.text.x=element_text(angle=70, hjust=1, size=14), axis.text.y=element_text (size=14),legend.position=""bottom"", legend.background=element_rect(size=1, linetype=""solid"", color=""black""), legend.title=element_blank())\r\npp.his <-pp + facet_wrap(Host~Orig_well~Pres_samp, scales=""free_x"")\r\n\r\n\r\n##Dot-plot of OTUs that make up at least 0.1%\r\n#Grab data from phyloseq histogram to put into R for ggplot2 \r\npp.high <- plot_bar(presprop.h, x=""Sample"", fill=""genusOTU"") + scale_fill_manual(values=GenusPalette2) +theme_bw()\r\npp.high.data <- pp.high$data\r\n\r\n##Reproduce initial plot\r\n#melt data on abundance and then get mean and SD for each sample\r\nmelt.pp<-melt(pp.high.data, ', 'library(dplyr)\r\nlibrary(reshape)\r\nlibrary(ggplot2)\r\nlibrary(phyloseq)\r\nlibrary(broom)\r\nlibrary(data.table)\r\nlibrary(RColorBrewer)\r\nlibrary(devtools)\r\nlibrary(ggsci)\r\nlibrary(vegan)\r\nlibrary(plyr)\r\nlibrary(DT)\r\nlibrary(microbiome)\r\nlibrary(microbiomeutilities)\r\nlibrary(ggpubr)\r\nlibrary(stringr)\r\nlibrary(tidyr)\r\n\r\n###LOADING DATA########\r\n#Silva biom classified\r\nsetwd(""C:/Users/Baoqing/Documents/Rdissert/mothur_phyloseq_analysis_BZ"")\r\nmothur.silva.biom <- import_biom(""oral_preserve.v2.mothur.all.biom"",parseFunction=parse_taxonomy_default)\r\ncolnames(tax_table(mothur.silva.biom)) <- c(Rank1=""Domain"",Rank2=""Phylum"",Rank3=""Class"",Rank4=""Order"",Rank5=""Family"",Rank6=""Genus"")\r\noral.biom <- mothur.silva.biom\r\n\r\n#HOMD biom classified\r\n#mothur.homd.biom <- import_biom(""oral_preserve.opti_mcc.0.03.all.homd.biom"", parseFunction=parse_taxonomy_default)\r\n#colnames(tax_table(mothur.homd.biom)) <- c(Rank1=""Domain"",Rank2=""Phylum"",Rank3=""Class"",Rank4=""Order"",Rank5=""Family"",Rank6=""Genus"", Rank7=""Species"")\r\n\r\n#Add an updated metadata table and replace in phyloseq\r\nmetap <- read.table(""op.mothur.revised.txt"", row.names=""Sample"", check.names=FALSE, header=TRUE)\r\nsample_data(oral.biom) <- metap\r\n\r\n####HOUSEKEEPING FOR CONSISTENCY########\r\n\r\n#Issues with underscore in SILVA taxonomy, remove anything after the first underscore\r\ntax_table(oral.biom) <- gsub(""_.*"","""", tax_table(oral.biom))\r\n\r\n#Reorder factors and substitute NAs with None for Pres_sample\r\nsample_data(oral.biom)$Pres_samp <- factor(sample_data(oral.biom)$Pres_samp, levels =c(""Plaque"",""Culture"", ""ref_1"",""ref_3"",""frz_10"",""frz_39"", ""None""))\r\nsample_data(oral.biom)$Pres_samp[is.na(sample_data(oral.biom)$Pres_samp)] <- ""None""\r\n\r\n#Reorder factors for Type, for consistent symbol and color assignment\r\nsample_data(oral.biom)$Type <- factor(sample_data(oral.biom)$Type, levels =c(""Plaque"",""Culture"", ""Preserved"",""Prop"",""Prop_Control"",""Control"", ""Pellicle"",""Spiked"", ""Control_PCR"",""PBS_Gly"",""media"",""Control_ext"",""Mock_ext"",""Mock_PCR""))\r\n\r\n#Do a lot of analysis of Pres_samples, so set it up\r\n#colorblind safe palette\r\ncolors <- c(""#d73027"", ""goldenrod2"",""#762a83"",""magenta"",""#91bfdb"",""#4575b4"")\r\ncolors2 <- c(""red"", ""goldenrod2"",""#762a83"", ""magenta"", ""#91bfdb"",""#4575b4"", ""black"", ""deeppink4"", ""green"",""grey30"",""lightpink2"", ""brown"", ""slategrey"",""beige"")\r\n\r\n#Replace low DNA quant with NA and detected with \r\nsample_data(oral.biom)$PG <- as.numeric(sample_data(oral.biom)$PG)\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG < 0.0199] <- NA\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG > 0.0199] <- ""PG""\r\nsample_data(oral.biom)$PG <- as.factor(sample_data(oral.biom)$PG)\r\n\r\n##Set-up for consistent Genus and OTU naming\r\n#Append on an extra level to the OTU table to get Taxon_OTU#\r\ntax_table(oral.biom)<- cbind(tax_table(oral.biom), Strain=taxa_names(oral.biom))\r\nmyranks = c(""Genus"",""Strain"")\r\nmylabels = apply(tax_table(oral.biom)[,myranks],1,paste,sep="""",collapse=""_"")\r\ntax_table(oral.biom) <- cbind(tax_table(oral.biom), genusOTU=mylabels)\r\n\r\n#assign specific colors to specific Genus \r\nset.seed(132)\r\ngetPalette <- colorRampPalette(brewer.pal(9, ""Set1""))\r\nGenusList <- unique(tax_table(oral.biom)[,""Genus""])\r\nGenusList <- sample(GenusList)\r\nGenusPalette = getPalette(length(GenusList))\r\nnames(GenusPalette) = GenusList\r\n\r\n#assign specific colors to GenusOTUs\r\nOTUList <- unique(tax_table(oral.biom)[,""genusOTU""])\r\nOTUList <- sample(OTUList)\r\nGenusPalette2 = getPalette(length(OTUList))\r\nnames(GenusPalette2) = OTUList\r\n\r\n#Try using a pruned, rarefied dataset\r\noral.biom.prune <- subset_samples(oral.biom, !sample_names(oral.biom) %in% c(43.1))\r\noral.biom.prune <- prune_samples(sample_sums(oral.biom.prune)>=1000, oral.biom.prune)\r\n\r\n#Rarefy\r\nrarefy <-rarefy_even_depth(oral.biom, sample.size=1000, rngseed=23167, replace=FALSE, trimOTUs=TRUE)\r\n\r\n#Convert to abundances for further analyses\r\nall.abund <- transform_sample_counts(oral.biom, function(x) x/sum(x))\r\nrare.abund <- transform_sample_counts(rarefy, function(x) x/sum(x))\r\n\r\n#Remove all controls, all ""spiked"" samples, mocks, an outlier (43.1 with >700,000 reads), and samples with less than 1000 reads (gets rid of plaque)\r\noral.biom.prune <- subset_samples(oral.biom, !sample_names(oral.biom) %in% c(43.1))\r\noral.biom.prune <- subset_samples(oral.biom.prune, !Name %in% c(""JMoS12P"",""HPS12P"",""BZS12P"",""H2F12S1"",""H3F22S11"",""SHI4"",""SHI2""))\r\noral.biom.prune <- subset_samples(oral.biom.prune, !Sample_or_Control %in% c(""Control""))\r\noral.biom.prune <- subset_samples(oral.biom.prune, !Type %in% c(""Mock_ext"",""Mock_PCR""))\r\noral.biom.prune <- prune_samples(sample_sums(oral.biom.prune)>=1000, oral.biom.prune)\r\n\r\n\r\n\r\n###### PRINCIPAL COMPONENT ANALYSIS ######\r\nlibrary(rela)\r\nlibrary(FactoMineR)\r\nlibrary(factoextra)\r\nlibrary(ggrepel)\r\n\r\n#Construct and apply a function that simplifies the matrix of major components\r\nhalfColMax <- function(df1) sapply(abs(df1)/2, max, na.rm = TRUE)\r\n\r\n### Try analysis with reads (not relative abundances)\r\n#This will not really be used ', 'library(devtools)\r\nlibrary(phyloseq)\r\nlibrary(nlme)\r\nlibrary(plyr)\r\nlibrary(dplyr)\r\nlibrary(tidyverse)\r\nlibrary(composition)\r\nlibrary(reshape)\r\nlibrary(ggplot2)\r\nlibrary(broom)\r\nlibrary(data.table)\r\nlibrary(RColorBrewer)\r\nlibrary(ggsci)\r\nlibrary(vegan)\r\nlibrary(DT)\r\nlibrary(microbiome)\r\nlibrary(microbiomeutilities)\r\nlibrary(ggpubr)\r\nlibrary(stringr)\r\n\r\n###LOADING DATA########\r\n#Silva biom classified\r\nsetwd(""C:/Users/Baoqing/Documents/Rdissert/mothur_phyloseq_redo"")\r\nmothur.silva.biom <- import_biom(""oral_preserve.v2.mothur.all.biom"",parseFunction=parse_taxonomy_default)\r\ncolnames(tax_table(mothur.silva.biom)) <- c(Rank1=""Domain"",Rank2=""Phylum"",Rank3=""Class"",Rank4=""Order"",Rank5=""Family"",Rank6=""Genus"")\r\noral.biom <- mothur.silva.biom\r\n\r\n#HOMD biom classified\r\n#mothur.homd.biom <- import_biom(""oral_preserve.opti_mcc.0.03.all.homd.biom"", parseFunction=parse_taxonomy_default)\r\n#colnames(tax_table(mothur.homd.biom)) <- c(Rank1=""Domain"",Rank2=""Phylum"",Rank3=""Class"",Rank4=""Order"",Rank5=""Family"",Rank6=""Genus"", Rank7=""Species"")\r\n\r\n#Add an updated metadata table and replace in phyloseq\r\nmetap <- read.table(""op.mothur.revised.txt"", row.names=""Sample"", check.names=FALSE, header=TRUE)\r\n#Add a column to the metadata that helps with plotting colors later (rarefaction curve)\r\nmetap <- metap %>% \r\n  mutate(Samp_color = case_when(\r\n    startsWith(Pres_samp, ""Plaque"") ~ ""1"",\r\n    startsWith(Pres_samp, ""Culture"") ~ ""2"",\r\n    startsWith(Pres_samp, ""ref_1"") ~ ""3"",\r\n    startsWith(Pres_samp, ""ref_3"") ~ ""4"",\r\n    startsWith(Pres_samp, ""frz_10"") ~ ""5"",\r\n    startsWith(Pres_samp, ""frz_39"") ~ ""6"",\r\n    startsWith(Pres_samp, ""none"") ~ ""7"",\r\n    startsWith(Pres_samp, ""Control"") ~ ""8"",\r\n    startsWith(Pres_samp, ""1A"") ~ ""9"",\r\n    startsWith(Pres_samp, ""2A"") ~ ""10"",\r\n    startsWith(Pres_samp, ""neg_80C"") ~ ""11""\r\n  )) \r\nsample_data(oral.biom) <- metap\r\n\r\n####HOUSEKEEPING FOR CONSISTENCY########\r\n\r\n#Issues with underscore in SILVA taxonomy, remove anything after the first underscore\r\ntax_table(oral.biom) <- gsub(""_.*"","""", tax_table(oral.biom))\r\n\r\n#Reorder factors and substitute NAs with None for Pres_sample\r\nsample_data(oral.biom)$Pres_samp <- factor(sample_data(oral.biom)$Pres_samp, levels =c(""Plaque"",""Culture"", ""ref_1"",""ref_3"",""frz_10"",""frz_39"", ""None""))\r\nsample_data(oral.biom)$Pres_samp[is.na(sample_data(oral.biom)$Pres_samp)] <- ""None""\r\n\r\n#Reorder factors for Type, for consistent symbol and color assignment\r\nsample_data(oral.biom)$Type <- factor(sample_data(oral.biom)$Type, levels =c(""Plaque"",""Culture"", ""Preserved"",""Prop"",""Prop_Control"",""Control"", ""Pellicle"",""Spiked"", ""Control_PCR"",""PBS_Gly"",""media"",""Control_ext"",""Mock_ext"",""Mock_PCR""))\r\n\r\n#Do a lot of analysis of Pres_samples, so set it up\r\n#Replace low DNA quant with NA\r\nsample_data(oral.biom)$PG <- as.numeric(sample_data(oral.biom)$PG)\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG < 0.0199] <- NA\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG > 0.0199] <- ""PG""\r\nsample_data(oral.biom)$PG <- as.factor(sample_data(oral.biom)$PG)\r\n\r\n##Set-up for consistent Genus and OTU naming\r\n#Append on an extra level to the OTU table to get Taxon_OTU#\r\ntax_table(oral.biom)<- cbind(tax_table(oral.biom), Strain=taxa_names(oral.biom))\r\nmyranks = c(""Genus"",""Strain"")\r\nmylabels = apply(tax_table(oral.biom)[,myranks],1,paste,sep="""",collapse=""_"")\r\ntax_table(oral.biom) <- cbind(tax_table(oral.biom), genusOTU=mylabels)\r\n\r\n##Color & label assignments\r\n#colorblind safe palette\r\ncolors <- c(""#d73027"", ""goldenrod2"",""#762a83"",""magenta"",""#91bfdb"",""#4575b4"")\r\ncolors2 <- c(""red"", ""goldenrod2"",""#762a83"", ""magenta"", ""#91bfdb"",""#4575b4"", ""black"", ""deeppink4"", ""green"",""grey30"",""lightpink2"", ""brown"", ""slategrey"",""beige"")\r\ncolors3 <- c(""goldenrod2"", ""#762a83"", ""magenta"", ""#91bfdb"", ""#4575b4"", ""black"", ""deeppink4"", ""green"",""grey30"",""lightpink2"", ""brown"", ""slategrey"", ""beige"")\r\n#Labels\r\ntypelaball <- c(""Plaque"",""Cx"",""4°C, 1 Day"",""4°C, 3 Days"",""-80°C, 1.5 Wks"",""-80°C, 5.5 Wks"",""NA"")\r\ntypelabsamp <- c(""Cx"",""4°C, 1 Day"",""4°C, 3 Days"",""-80°C, 1.5 Wks"",""-80°C, 5.5 Wks"")\r\nlabel1 <- c(\'1\' = ""Host 1"", \'2\' = ""Host 2"", \'3\' = ""Host 3"", \'Control\' = ""Negative Controls"", \'mock\' = ""Mock"", \'Pellicle\' = ""Pellicle"")\r\n\r\n#assign specific colors to specific Genus \r\nset.seed(132)\r\ngetPalette <- colorRampPalette(brewer.pal(9, ""Set1""))\r\nGenusList <- unique(tax_table(oral.biom)[,""Genus""])\r\nGenusList <- sample(GenusList)\r\nGenusPalette = getPalette(length(GenusList))\r\nnames(GenusPalette) = GenusList\r\n\r\n#assign specific colors to GenusOTUs\r\nOTUList <- unique(tax_table(oral.biom)[,""genusOTU""])\r\nOTUList <- sample(OTUList)\r\nGenusPalette2 = getPalette(length(OTUList))\r\nnames(GenusPalette2) = OTUList\r\n\r\n\r\n##########SEQUENCING ASSESSEMENT ##########\r\n\r\n#Plot by sequence depth\r\ndf <- as.data.frame(sample_data(oral.biom)) \r\ndf$LibrarySize <- sample_sums(oral.biom)\r\ndf <- df[order(df$LibrarySize),]\r\n#Order each sample by size\r\ndf$Index <- seq(nrow(df))\r\n#Assign samples_type to colors and type to shape\r\nopen.index <- ggplot(data=df, aes(x=Index, y=Libr', 'library(dplyr)\r\nlibrary(reshape)\r\nlibrary(ggplot2)\r\nlibrary(phyloseq)\r\nlibrary(broom)\r\nlibrary(data.table)\r\nlibrary(RColorBrewer)\r\nlibrary(devtools)\r\nlibrary(ggsci)\r\nlibrary(vegan)\r\nlibrary(plyr)\r\nlibrary(DT)\r\nlibrary(microbiome)\r\nlibrary(microbiomeutilities)\r\nlibrary(ggpubr)\r\nlibrary(stringr)\r\nlibrary(tidyr)\r\nlibrary(microViz) #need devtools\r\n#for clr analysis\r\nlibrary(compositions)\r\nlibrary(mixOmics) \r\n\r\n#library(clr) #might not use\r\n#library(propr) #may not use\r\n#library(ggdendro)\r\n#library(plotly) #might not use\r\n#library(rela)\r\n#library(FactoMineR)\r\n#library(factoextra)\r\n#library(ggrepel)\r\n\r\n###LOADING DATA########\r\n#Silva biom classified\r\nsetwd(""C:/Users/Baoqing/Documents/Rdissert/mothur_phyloseq_analysis_BZ"")\r\nmothur.silva.biom <- import_biom(""oral_preserve.v2.mothur.all.biom"",parseFunction=parse_taxonomy_default)\r\ncolnames(tax_table(mothur.silva.biom)) <- c(Rank1=""Domain"",Rank2=""Phylum"",Rank3=""Class"",Rank4=""Order"",Rank5=""Family"",Rank6=""Genus"")\r\noral.biom <- mothur.silva.biom\r\n\r\n#HOMD biom classified\r\n#mothur.homd.biom <- import_biom(""oral_preserve.opti_mcc.0.03.all.homd.biom"", parseFunction=parse_taxonomy_default)\r\n#colnames(tax_table(mothur.homd.biom)) <- c(Rank1=""Domain"",Rank2=""Phylum"",Rank3=""Class"",Rank4=""Order"",Rank5=""Family"",Rank6=""Genus"", Rank7=""Species"")\r\n\r\n#Add an updated metadata table and replace in phyloseq\r\nmetap <- read.table(""op.mothur.revised.txt"", row.names=""Sample"", check.names=FALSE, header=TRUE)\r\nsample_data(oral.biom) <- metap\r\n\r\n####HOUSEKEEPING FOR CONSISTENCY########\r\n\r\n#Issues with underscore in SILVA taxonomy, remove anything after the first underscore\r\ntax_table(oral.biom) <- gsub(""_.*"","""", tax_table(oral.biom))\r\n\r\n#Reorder factors and substitute NAs with None for Pres_sample\r\nsample_data(oral.biom)$Pres_samp <- factor(sample_data(oral.biom)$Pres_samp, levels =c(""Plaque"",""Culture"", ""ref_1"",""ref_3"",""frz_10"",""frz_39"", ""None""))\r\nsample_data(oral.biom)$Pres_samp[is.na(sample_data(oral.biom)$Pres_samp)] <- ""None""\r\n\r\n#Reorder factors for Type, for consistent symbol and color assignment\r\nsample_data(oral.biom)$Type <- factor(sample_data(oral.biom)$Type, levels =c(""Plaque"",""Culture"", ""Preserved"",""Prop"",""Prop_Control"",""Control"", ""Pellicle"",""Spiked"", ""Control_PCR"",""PBS_Gly"",""media"",""Control_ext"",""Mock_ext"",""Mock_PCR""))\r\n\r\n#Do a lot of analysis of Pres_samples, so set it up\r\n#colorblind safe palette\r\ncolors <- c(""#d73027"", ""goldenrod2"",""#762a83"",""magenta"",""#91bfdb"",""#4575b4"")\r\ncolors2 <- c(""red"", ""goldenrod2"",""#762a83"", ""magenta"", ""#91bfdb"",""#4575b4"", ""black"", ""deeppink4"", ""green"",""grey30"",""lightpink2"", ""brown"", ""slategrey"",""beige"")\r\npreslabs <- c(""Cx"",""4°C, 1 Day"",""4°C, 3 Days"",""-80°C, 1.5 Wks"",""-80°C, 5.5 Wks"")\r\n\r\n#Replace low DNA quant with NA and detected with \r\nsample_data(oral.biom)$PG <- as.numeric(sample_data(oral.biom)$PG)\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG < 0.0199] <- NA\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG > 0.0199] <- ""PG""\r\nsample_data(oral.biom)$PG <- as.factor(sample_data(oral.biom)$PG)\r\n\r\n##Set-up for consistent Genus and OTU naming\r\n#Append on an extra level to the OTU table to get Taxon_OTU#\r\ntax_table(oral.biom)<- cbind(tax_table(oral.biom), Strain=taxa_names(oral.biom))\r\nmyranks = c(""Genus"",""Strain"")\r\nmylabels = apply(tax_table(oral.biom)[,myranks],1,paste,sep="""",collapse=""_"")\r\ntax_table(oral.biom) <- cbind(tax_table(oral.biom), genusOTU=mylabels)\r\n\r\n#assign specific colors to specific Genus \r\nset.seed(132)\r\ngetPalette <- colorRampPalette(brewer.pal(9, ""Set1""))\r\nGenusList <- unique(tax_table(oral.biom)[,""Genus""])\r\nGenusList <- sample(GenusList)\r\nGenusPalette = getPalette(length(GenusList))\r\nnames(GenusPalette) = GenusList\r\n\r\n#assign specific colors to GenusOTUs\r\nOTUList <- unique(tax_table(oral.biom)[,""genusOTU""])\r\nOTUList <- sample(OTUList)\r\nGenusPalette2 = getPalette(length(OTUList))\r\nnames(GenusPalette2) = OTUList\r\n\r\n#Rarefy\r\nrarefy <-rarefy_even_depth(oral.biom, sample.size=240, rngseed=23167, replace=FALSE, trimOTUs=TRUE)\r\n\r\n#Convert to abundances for further analyses\r\nall.abund <- transform_sample_counts(oral.biom, function(x) x/sum(x))\r\nrare.abund <- transform_sample_counts(rarefy, function(x) x/sum(x))\r\n\r\n#Counts data\r\n#Remove all controls, all ""spiked"" samples, mocks, an outlier (43.1 with >700,000 reads) from counts\r\noral.biom.prune <- subset_samples(oral.biom, !sample_names(oral.biom) %in% c(43.1))\r\noral.biom.prune <- subset_samples(oral.biom.prune, !Name %in% c(""JMoS12P"",""HPS12P"",""BZS12P"",""H2F12S1"",""H3F22S11"",""SHI4"",""SHI2""))\r\noral.biom.prune <- subset_samples(oral.biom.prune, !Sample_or_Control %in% c(""Control""))\r\noral.biom.prune <- subset_samples(oral.biom.prune, !Type %in% c(""Mock_ext"",""Mock_PCR""))\r\n\r\n#Counts data, rarefied\r\nrare.prune <- subset_samples(rarefy, !sample_names(rarefy) %in% c(43.1))\r\nrare.prune <- subset_samples(rare.prune, !Name %in% c(""JMoS12P"",""HPS12P"",""BZS12P"",""H2F12S1"",""H3F22S11"",""SHI4"",""SHI2""))\r\nrare.prune <- subset_samples(rare.prune, !Sample_or_Control %in% c(""Control""))\r\nrare.prune <- subset_sam', 'library(dplyr)\r\nlibrary(reshape)\r\nlibrary(ggplot2)\r\nlibrary(phyloseq)\r\nlibrary(broom)\r\nlibrary(data.table)\r\nlibrary(RColorBrewer)\r\nlibrary(devtools)\r\nlibrary(ggsci)\r\nlibrary(vegan)\r\nlibrary(plyr)\r\nlibrary(DT)\r\nlibrary(microbiome)\r\nlibrary(microbiomeutilities)\r\nlibrary(ggpubr)\r\nlibrary(stringr)\r\n\r\nsetwd(""C:/Users/Baoqing/Documents/Rdissert/mothur_phyloseq_analysis_BZ"")\r\n\r\n###LOADING DATA########\r\n#Silva biom classified\r\nmothur.silva.biom <- import_biom(""oral_preserve.v2.mothur.all.biom"",parseFunction=parse_taxonomy_default)\r\ncolnames(tax_table(mothur.silva.biom)) <- c(Rank1=""Domain"",Rank2=""Phylum"",Rank3=""Class"",Rank4=""Order"",Rank5=""Family"",Rank6=""Genus"")\r\noral.biom <- mothur.silva.biom\r\n\r\n#Add an updated metadata table and replace in phyloseq\r\nmetap <- read.table(""op.mothur.revised.txt"", row.names=""Sample"", check.names=FALSE, header=TRUE)\r\nsample_data(oral.biom) <- metap\r\n\r\n####HOUSEKEEPING FOR CONSISTENCY########\r\n\r\n#Issues with underscore in SILVA taxonomy, remove anything after the first underscore\r\ntax_table(oral.biom) <- gsub(""_.*"","""", tax_table(oral.biom))\r\n\r\n#Reorder factors and substitute NAs with None for Pres_sample\r\nsample_data(oral.biom)$Pres_samp <- factor(sample_data(oral.biom)$Pres_samp, levels =c(""Plaque"",""Culture"", ""ref_1"",""ref_3"",""frz_10"",""frz_39"", ""None""))\r\nsample_data(oral.biom)$Pres_samp[is.na(sample_data(oral.biom)$Pres_samp)] <- ""None""\r\n\r\n#Reorder factors for Type, for consistent symbol and color assignment\r\nsample_data(oral.biom)$Type <- factor(sample_data(oral.biom)$Type, levels =c(""Plaque"",""Culture"", ""Preserved"",""Prop"",""Prop_Control"",""Control"", ""Pellicle"",""Spiked"", ""Control_PCR"",""PBS_Gly"",""media"",""Control_ext"",""Mock_ext"",""Mock_PCR""))\r\n\r\n#Do a lot of analysis of Pres_samples, so set it up\r\n#Replace low DNA quant with NA and detected with \r\nsample_data(oral.biom)$PG <- as.numeric(sample_data(oral.biom)$PG)\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG < 0.0199] <- NA\r\nsample_data(oral.biom)$PG[sample_data(oral.biom)$PG > 0.0199] <- ""PG""\r\nsample_data(oral.biom)$PG <- as.factor(sample_data(oral.biom)$PG)\r\n\r\n##Set-up for consistent Genus and OTU naming\r\n#Append on an extra level to the OTU table to get Taxon_OTU#\r\ntax_table(oral.biom)<- cbind(tax_table(oral.biom), Strain=taxa_names(oral.biom))\r\nmyranks = c(""Genus"",""Strain"")\r\nmylabels = apply(tax_table(oral.biom)[,myranks],1,paste,sep="""",collapse=""_"")\r\ntax_table(oral.biom) <- cbind(tax_table(oral.biom), genusOTU=mylabels)\r\n\r\n#assign specific colors to specific Genus \r\nset.seed(132)\r\ngetPalette <- colorRampPalette(brewer.pal(9, ""Set1""))\r\nGenusList <- unique(tax_table(oral.biom)[,""Genus""])\r\nGenusList <- sample(GenusList)\r\nGenusPalette = getPalette(length(GenusList))\r\nnames(GenusPalette) = GenusList\r\n\r\n#assign specific colors to GenusOTUs\r\nOTUList <- unique(tax_table(oral.biom)[,""genusOTU""])\r\nOTUList <- sample(OTUList)\r\nGenusPalette2 = getPalette(length(OTUList))\r\nnames(GenusPalette2) = OTUList\r\n\r\n#Assign a name to the data set that removes all controls, all ""spiked"" samples, mocks, an outlier (43.1 with >700,000 reads), and samples with less than 1000 reads (gets rid of plaque)\r\noral.biom.prune <- subset_samples(oral.biom, !sample_names(oral.biom) %in% c(43.1))\r\noral.biom.prune <- subset_samples(oral.biom.prune, !Name %in% c(""JMoS12P"",""HPS12P"",""BZS12P"",""H2F12S1"",""H3F22S11"",""SHI4"",""SHI2"",""H1"",""H3""))\r\noral.biom.prune <- subset_samples(oral.biom.prune, !Sample_or_Control %in% c(""Control""))\r\noral.biom.prune <- subset_samples(oral.biom.prune, !Type %in% c(""Mock_ext"",""Mock_PCR""))\r\noral.biom.prune <- prune_samples(sample_sums(oral.biom.prune)>=1000, oral.biom.prune)\r\n\r\n#Rarefy\r\nrarefy <- rarefy_even_depth(oral.biom, sample.size=240, rngseed=23167, replace=FALSE, trimOTUs=TRUE)\r\n\r\n#Convert to abundances for further analyses\r\nall.abund <- transform_sample_counts(oral.biom, function(x) x/sum(x))\r\nrare.abund <- transform_sample_counts(rarefy, function(x) x/sum(x))\r\n\r\n#Assign a colorblind-safe palette\r\ncolors <- c(""#CC79A7"", ""#999999"", ""#E69F00"", ""#56B4E9"", ""#009E73"", ""#F0E442"", ""#0072B2"", ""#D55E00"", ""firebrick4"")\r\ncolors2 <- c(""red"", ""goldenrod2"", ""#762a83"", ""magenta"", ""#91bfdb"", ""#4575b4"", ""black"", ""deeppink4"", ""green"",""grey30"",""lightpink2"", ""brown"", ""slategrey"", ""beige"")\r\ncolors3 <- c(""goldenrod2"", ""#762a83"", ""magenta"", ""#91bfdb"", ""#4575b4"", ""black"", ""deeppink4"", ""green"",""grey30"",""lightpink2"", ""brown"", ""slategrey"", ""beige"")\r\n\r\ncolorsfig2test <- c(""darkorchid3"", ""magenta"", ""#CC79A7"", ""#999999"", ""#E69F00"", ""#56B4E9"", ""#F0E442"",""#009E73"", ""#0072B2"", ""#D55E00"", ""firebrick4"")\r\n\r\n#Some stats to put in the paper/dissertation\r\n#Total reads per sample - then look at low-read samples by sorting\r\nreads.sum <- sample_sums(oral.biom.prune)\r\nsort(reads.sum)\r\n#Info on number of OTUs at different taxonomic levels ([5] for phyla, [6] for genus, [7] for species)\r\n#get_taxa_unique(oral.biom, taxonomic.rank = rank_names(oral.biom)[6]) #207 - 1 ""NA""\r\n#get_taxa_unique(oral.biom, taxonomic.rank = rank_names(oral.biom)[7]) #983\r\n#get_taxa_unique(oral.biom, taxonomic']",0,"SARS-CoV-2, variants of concern, viral dynamics, non-human primates, dataset, structural models, mlxtran files, R script files, simulations, functions, figures, results."
Impact of variants of concern on SARS-CoV-2 viral dynamics in non-human primates,Contents1. Dataset- Full dataset of all 78 animals infected with different strains of SARS-CoV-22. Monolix files- Structural models of the 5 models used- mlxtran files of the 6 models used3. R script files- Main script used to reproduce all figures and results in the article- File used to store functions used in the main script- Result file of the simulations,"['\r\ninvlogit = function(x){\r\n  return(exp(x) / (1 + exp(x)))\r\n}\r\n\r\nlogit = function(x){\r\n  return(log(x / (1 - x)))\r\n}\r\n\r\n\r\n\r\n\r\n# Get individuals parameters AVEC DECROISSANCE EXP\r\nGetParamsIndivs_OnlyNasal_10Bet_Model_1.3 = function(nPop, nIndiv, project.file, CDC = TRUE, popParams, covarMatrix ){\r\n  \r\n  set.seed(1996)\r\n  # nPop = Number of population parameters drawn from the covariance matrix of the estimates\r\n  # nIndiv = Number of individual parameters drawn form each population parameters \r\n  \r\n  # pops = simpopmlx(n = nPop, project = project.file, fim = ""sa"")\r\n  \r\n  \r\n  # if (CDC){\r\n  #   props.proba = simpopmlx(n = nPop, project = ""/home/aurelien.marc/Variants/FinalModel/FinalModel/Lancet_Incubation_VA_seed3.mlxtran"", fim = ""sa"")\r\n  #   \r\n  # } else {\r\n  #   pops.proba = simpopmlx(n = nPop, project = ""AnalyseSensi/Lancet_Incubation_VA_seed3.mlxtran"", fim = ""sa"")\r\n  # \r\n  # }\r\n  \r\n  # Get my covariance matrix\r\n  covarMatrix = head(covarMatrix, -3) # We remove the last 3 i.e the error terms\r\n  mypars = covarMatrix$V1\r\n  covarMatrix = covarMatrix[2:(length(covarMatrix)-3)] # We remove the last 3 columns as well\r\n  \r\n  varcov = as.matrix(covarMatrix)\r\n  \r\n  # Get the vector of estimated populational parameters\r\n  popParams = popParams[,c(1,2)]\r\n  \r\n  \r\n  names = popParams$parameter\r\n  mu = popParams[popParams$parameter %in% mypars,]$value\r\n  \r\n  pops = as.data.frame(mvrnorm(n = 10000, mu, varcov))\r\n  setnames(pops, old = names(pops), new = as.character(mypars))\r\n  \r\n  # Only betas can be < all other parameters cannot to we remove every one of them\r\n  myInf = pops[, grep(pattern = \'^beta_\', x = colnames(pops), invert = T, value = T)] <0\r\n  pops[, grep(pattern = \'^beta_\', x = colnames(pops), invert = T, value = T)][myInf] = NA\r\n  \r\n  pops = data.table(pops)\r\n  pops[, R0 := (10^(-beta_pow_pop)*pN_pop*12500*mu_pop) / (10*delta_pop)]\r\n  \r\n  pops$R0[pops$R0<1] = NA\r\n  \r\n  pops = pops[complete.cases(pops),][1:nPop,]\r\n  \r\n  \r\n  params_Historical = data.frame()\r\n  params_Beta = data.frame()\r\n  params_Gamma = data.frame()\r\n  params_Delta = data.frame()\r\n  params_Omicron = data.frame()\r\n  \r\n  # Use popParams for fixed parameters \r\n  # Use pops for parameters withtout inter-indiv variability as they still need to account for se\r\n  \r\n  for (i in c(1:nrow(pops))){\r\n    # Create the parameters for each strains\r\n    # // HISTORICAL\r\n    dat1 = data.frame(delta = exp(rnorm(500, log(pops$delta_pop[i]), pops$omega_delta[i])),\r\n                      pN = exp(rnorm(500, log(pops$pN_pop[i]), pops$omega_pN[i])),\r\n                      theta = exp(rnorm(500, log(pops$theta_pop[i]), pops$omega_theta[i])),\r\n                      \r\n                      mu = pops$mu_pop[i],\r\n                      beta_pow = pops$beta_pow_pop[i],\r\n                      f = pops$f_pop[i],\r\n                      k = 4,\r\n                      T0 = 12500,\r\n                      df = 0.4,\r\n                      n = 1,\r\n                      g = 6.67,\r\n                      h = invlogit(rnorm(500, logit(0.2), 2))\r\n\r\n    )\r\n    \r\n    dat1 = data.table(dat1)\r\n    dat1[, R0 := (10^(-beta_pow)*pN*12500*mu) / (10*delta)]\r\n    \r\n    dat1$R0[dat1$R0<1] = NA\r\n    dat1 = dat1[complete.cases(dat1),][1:nIndiv,]\r\n    \r\n    \r\n    \r\n    # beta\r\n    dat2 = data.frame(delta = exp(rnorm(500, log(pops$delta_pop[i]) + pops$beta_delta_Beta_G_Beta[i], pops$omega_delta[i])),\r\n                      pN = exp(rnorm(500, log(pops$pN_pop[i]), pops$omega_pN[i])),\r\n                      theta = exp(rnorm(500, log(pops$theta_pop[i]), pops$omega_theta[i])),\r\n                      \r\n                      mu = pops$mu_pop[i],\r\n                      beta_pow = pops$beta_pow_pop[i],\r\n                      f = pops$f_pop[i],\r\n                      k = 4,\r\n                      T0 = 12500,\r\n                      df = 0.4,\r\n                      n = 1,\r\n                      g = 6.67,\r\n                      h = invlogit(rnorm(500, logit(0.2), 2))\r\n\r\n    )\r\n    \r\n    dat2 = data.table(dat2)\r\n    dat2[, R0 := (10^(-beta_pow)*pN*12500*mu) / (10*delta)]\r\n    \r\n    dat1$R0[dat1$R0<1] = NA\r\n    dat2 = dat2[complete.cases(dat2),][1:nIndiv,]\r\n    \r\n    \r\n    \r\n    \r\n    # Gamma\r\n    dat3 = data.frame(delta = exp(rnorm(500, log(pops$delta_pop[i]), pops$omega_delta[i])),\r\n                      pN = exp(rnorm(500, log(pops$pN_pop[i]), pops$omega_pN[i])),\r\n                      theta = exp(rnorm(500, log(pops$theta_pop[i]) + pops$beta_theta_Gamma_G_Gamma[i], pops$omega_theta[i])),\r\n                      \r\n                      mu = pops$mu_pop[i],\r\n                      beta_pow = pops$beta_pow_pop[i],\r\n                      f = pops$f_pop[i],\r\n                      k = 4,\r\n                      T0 = 12500,\r\n                      df = 0.4,\r\n                      n = 1,\r\n                      g = 6.67,\r\n                      h = invlogit(rnorm(500, logit(0.2), 2))\r\n\r\n    )\r\n    \r\n    dat3 = data.table(dat3)\r\n    dat3[, R0 := (10^(-beta_pow)*pN*12500*mu) / (10*delta)]\r\n    \r\n    dat3$R0[dat3$R0<1] = NA\r\n   ', 'library(rstan)\r\nlibrary(ggplot2)\r\nlibrary(plyr)\r\nlibrary(bayesplot)\r\nlibrary(magrittr)\r\nlibrary(tidyverse)\r\nlibrary(parallel)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(RColorBrewer) \r\nlibrary(svglite)\r\nlibrary(ggthemes)\r\nlibrary(deSolve)\r\nlibrary(LaplacesDemon)\r\nlibrary(lemon)\r\nlibrary(fs)\r\nlibrary(reshape2)\r\nlibrary(MASS, include.only = \'mvrnorm\') \r\nlibrary(writexl)\r\nlibrary(ggpubr)\r\nlibrary(rstudioapi)\r\nlibrary(pracma)\r\nlibrary(data.table)\r\nlibrary(LaplacesDemon)\r\nlibrary(gridExtra)\r\nlibrary(cmdstanr)\r\nlibrary(xlsx)\r\nlibrary(car)\r\n\r\nlibrary(RsSimulx)\r\n\r\n# LixoftConnectorsPath =  ""C:/ProgramData/Lixoft2020/MonolixSuite2019R2""\r\n# library(lixoftConnectors, lib.loc = LixoftConnectorsPath )\r\n\r\n# LixoftConnectorsPath =  ""C:/ProgramData/Lixoft2021/MonolixSuite2021R1""\r\n# library(lixoftConnectors, lib.loc = LixoftConnectorsPath )\r\n\r\nmonolix2019R2.path =  ""C:/ProgramData/Lixoft2020/MonolixSuite2019R2""\r\nlibrary(lixoftConnectors, lib.loc = monolix2019R2.path )\r\n\r\n# library(mlxR)\r\n# initMlxR(path = monolix2019R2.path)   #(adapt the path if necessary).\r\n# library(Rsmlx, lib.loc = ""C:/ProgramData/Lixoft2021/MonolixSuite2021R1/"")\r\nlibrary(truncnorm)\r\n\r\nlw = 1.2\r\nps = 2.75\r\nst = 1.1\r\n\r\nfz = 19\r\nts = 17\r\n\r\nsetwd(""C:/Users/Aurelien/Desktop/PhD/PROJETS/Main publications/Plos-Variants NHPs/"")\r\n\r\n# COLOR SCHEME WITH OMICRON\r\n\r\nFILL_GGPLOT = scale_fill_manual(breaks = c(""Historical"",""Beta"",""Gamma"",""Delta"",""Omicron""),\r\n                                values = c(""#A993C8"",""#8AC926"",""#FFCA3A"",""#38A3E5"",""#FF5C61""))\r\n\r\n\r\nCOLOR_GGPLOT = scale_color_manual(breaks = c(""Historical"",""Beta"",""Gamma"",""Delta"",""Omicron""),\r\n                                  values = c(""#A993C8"",""#8AC926"",""#FFCA3A"",""#38A3E5"",""#FF5C61""))\r\n\r\n\r\nxdata = read.csv(file= ""FinalModel/CompleteDataset_NasalOnly.csv"", header = TRUE, sep = "";"") # data\r\nxdata$GRP5 = factor(xdata$GRP5, levels = c(""Historical"",""Beta"",""Gamma"",""Delta"",""Omicron""))\r\nnames(xdata)[1] = ""id""\r\ngrps = xdata[!duplicated(xdata$id),]\r\n\r\nxdata = data.table(xdata)\r\n\r\n\r\n\r\n\r\n# DESCRIPTIVE STATISTICS----------------- --------------------------------------------------\r\n\r\n\r\n\r\n# Metrics of animals  -----------------------------------------------------\r\n\r\n\r\nanim = xdata[NTYPE %in% c(1,5), c(""id"", ""TIME"", ""LVL"", ""CENS"", ""Weight"", ""INOCULUM"", ""GRP5"", ""NTYPE"")]\r\nanim$GRP5 = factor(anim$GRP5, levels = c(""Historical"",""Beta"",""Gamma"",""Delta"",""Omicron""))\r\n\r\n\r\nanim = data.table(anim)\r\n\r\n\r\nanim[, peakLVL := max(LVL), by=c(""id"",""GRP5"",""NTYPE"")]\r\nanim[, TTP := TIME[LVL==peakLVL], by = c(""id"",""GRP5"",""NTYPE"")]\r\n\r\nanim[, meanPek := round(mean(peakLVL),1), by = c(""GRP5"",""NTYPE"")]\r\nanim[, meanInoculum := mean(INOCULUM), by = c(""GRP5"")]\r\nanim[, meanWgt := mean(Weight), by = c(""GRP5"")]\r\n\r\nanim[TIME >= TTP, TTVC := TIME[CENS == 1][1], by = c(""id"",""GRP5"",""NTYPE"")]\r\nanim[, meanTTVC := mean(TTVC, na.rm = T), by = c(""GRP5"",""NTYPE"")]\r\n\r\n\r\nanim[id == 2]\r\n\r\nanim2 = anim[, list(meanPek = meanPek[1], meanInoculum = meanInoculum[1], meanWgt = meanWgt[1], meanTTVC = meanTTVC[1]),\r\n             by = c(""NTYPE"",""GRP5"")]\r\n\r\n\r\n\r\n# Longitudinal data -------------------------------------------------------\r\n\r\n\r\n\r\n# Nombre de swabs par biomarkeur\r\ntest2 = xdata\r\n\r\ntest2[, AllSwabs := sum(.N), by = c(""GRP5"",""NTYPE"")]\r\ntest2[, AllSwabsID := sum(.N), by = c(""NTYPE"",""id"")]\r\ntest2[, MeanSwabID := round(mean(AllSwabsID),1), by = c(""GRP5"",""NTYPE"")]\r\nMeanSwabs = test2[, list(MeanSwabID = MeanSwabID[1]), by = c(""GRP5"",""NTYPE"")]\r\nallswabs = test2[, list(AllSwabs = AllSwabs[1]), by = c(""GRP5"",""NTYPE"")]\r\nMeanSwabs[order(GRP5, NTYPE)]\r\ntest = xdata[xdata$GRP5 == ""Historical"" & xdata$NTYPE == 1,]\r\nsum(test[, .N, by = c(""id"")]$N)\r\n\r\ngrps = data.table(grps)\r\ngrps[, nbAnimals := .N, by = c(""GRP5"")]\r\n\r\nnbAnimals = grps[, list(nbAnimals = nbAnimals[1]), by = c(""GRP5"")]\r\n\r\n\r\n\r\nlw = 1.2\r\nps = 2.75\r\nst = 1.1\r\n\r\nfz = 19\r\nts = 17\r\n\r\nFILL_GGPLOT = scale_fill_manual(breaks = c(""Historical"",""Beta"",""Gamma"",""Delta"",""Omicron""),\r\n                                values = c(""#A993C8"",""#8AC926"",""#FFCA3A"",""#38A3E5"",""#FF5C61""))\r\n\r\n\r\nCOLOR_GGPLOT = scale_color_manual(breaks = c(""Historical"",""Beta"",""Gamma"",""Delta"",""Omicron""),\r\n                                  values = c(""#A993C8"",""#8AC926"",""#FFCA3A"",""#38A3E5"",""#FF5C61""))\r\n\r\n\r\n\r\nNTYPE.labs = c(""Genomic RNA\\n(log10 copies/mL)"",""Subgenomic RNA\\n(log10 copies/mL)"",""Infectious titers\\n(log10 PFU/mL)"")\r\nnames(NTYPE.labs) = c(1,3,5)\r\n\r\n\r\nGRP5.labs = c(""Historical\\nn = 44"",""Beta\\nn = 9"",""Gamma\\nn = 5"",""Delta\\nn = 11"",""Omicron\\nn = 9"")\r\nnames(GRP5.labs) = c(""Historical"",""Beta"",""Gamma"",""Delta"",""Omicron"")\r\n\r\n\r\n\r\n# dev.new()\r\n\r\nxdata_nasal = xdata[TYPE == ""Nasal""]\r\n\r\n\r\nggplot()+\r\n  # Lines\r\n  geom_line(data = xdata_nasal, aes(x = TIME, y = LVL, color = as.factor(GRP5) ,group =as.factor(id))\r\n            ,size = lw, alpha = 0.80)+\r\n  \r\n  # Non-Censored & right censored points\r\n  geom_point(data = xdata_nasal[CENS %in% c(0,-1),], aes(x = TIME, y = LVL, fill = as.factor(GRP5), group =as.factor(id), shape = as.factor(CENS)),\r\n             color = ""black"",']",0,"ocean acidification, gene expression, DNA methylation, mantle tissue, Eastern oyster, Crassostrea virginica, phenotypic responses, marine calcifying species, extrapallial fluid, calcification site, pCO2"
Data from: Ocean acidification induces subtle shifts in gene expression and DNA methylation in mantle tissue of the Eastern oyster (Crassostrea virginica),"Early evidence suggests that DNA methylation can mediate phenotypic responses of marine calcifying species to ocean acidification (OA). Few studies, however, have explicitly studied DNA methylation in calcifying tissues through time. Here, we examined the phenotypic and molecular responses in the extrapallial fluid and mantle (fluid and tissue at the calcification site) in adult eastern oyster (Crassostrea virginica) exposed to experimental OA over 80 days. Oysters were reared under three experimental pCO2 treatments ('control', 580 atm; 'moderate OA', 1000 atm; 'high OA', 2800 atm) and sampled at 6 time points (24 hours - 80 days). We found that high OA initially induced an increase in the pH of the extrapallial fluid (pHEPF) relative to the external seawater that peaked at day 9, but then diminished over time. Calcification rates were significantly lower in the high OA treatment compared to the other treatments. To explore how oysters regulate their extrapallial fluid, gene expression and DNA methylation were examined in the mantle-edge tissue of oysters from days 9 and 80 in the control and high OA treatments. Mantle tissue mounted a significant global molecular response (both in the transcriptome and methylome) to OA that shifted through time. Although we did not find individual genes that were significantly differentially expressed under OA, the pHEPF was significantly correlated with the eigengene expression of several co-expressed gene clusters. A small number of OA-induced differentially methylated loci were discovered, which corresponded with a weak association between OA-induced changes in genome-wide gene body DNA methylation and gene expression. Gene body methylation, however, was not significantly correlated with the eigengene expression of pHEPF-correlated gene clusters. These results suggest that OA induces a subtle response in a large number of genes in C. virginica, but also indicate that plasticity at the molecular level may be limited. Our study highlights the need to reassess our understanding of tissue-specific molecular responses in marine calcifiers, as well as the role of DNA methylation and gene expression in mediating physiological and biomineralization responses to OA.","#### Filtering and Storing Expression Matrices ####

## Packages 
library(edgeR,quietly = TRUE)
library(limma,quietly = TRUE)
#library(devtools)
#install_github(""vqv/ggbiplot"")
library(ggbiplot,quietly = TRUE)

#### Data#####
## Set working directory
wd <- ""~/Github/AE17_Cvirginica_MolecularResponse/data""
outputDir <- ""~/Github/AE17_Cvirginica_MolecularResponse/data/Analysis""
# This should be set to the path for the local version of the `2017OAExp_Oysters` github repo.
## RSEM counts 
# Gene matrix
RSEM <-  readRDS(paste0(wd,""/RNAseq/RSEM_gene_Summary.Rdata""))
# Separate out RSEM counts and rename rows with LOC ID
rsem_c <- RSEM$Count # Stick with raw gene counts  (not TPM or some other normalizaed count)
rm(RSEM)
## Transcript File  
# Transcript file
tran <- readRDS(paste0(wd,""/references/STAR_gnomon_tximportGeneFile.RData""))
# Genes in File
gene <- tran[!duplicated(tran$GENEID),]
## Meta Data  
# metaTemp <- read.csv(""~/Github/AE17_Cvirginica_MolecularResponse/data/meta/AE17_RNAmetaData.csv"",stringsAsFactors = FALSE)
# metaTemp$Treatment <- as.factor(metaTemp$Treatment)
# metaTemp$Time <- as.factor(metaTemp$Time)
# metaTemp$SFV <- as.factor(metaTemp$SFV)
# metaTemp$Pop <- as.factor(metaTemp$Pop)
#saveRDS(metaTemp,""~/Github/AE17_Cvirginica_MolecularResponse/data/meta/AE17_RNAmetaData.RData"")
meta <- readRDS(paste0(wd,""/meta/AE17_RNAmetaData.RData""))
#Create new factor levels (one for each level combination)
meta$SFVrn <- as.factor(paste0(""D"",meta$SFV))
meta$Sample_Index <- as.factor(meta$sample_index)
meta$TankID <- as.factor(meta$tankID)

#### Data Manipulation ####
# Order genes from annotation list to match the order in the count matrix
gene_order <- gene[order(gene$gene_id),]
identical(rownames(rsem_c),gene_order$gene_id) # TRUE confirms the order matches
# Relabel the rows of the count matrix with LOC ID
rownames(rsem_c) <- gene_order$GENEID

# Reorder by LOCID 
geneC_full <- rsem_c

rm(rsem_c)

#### Filtering counts ####
## Round to whole counts
geneC_all <- round(geneC_full)

## Genes 
# Breaking down expression coverage by treatment*time combination
#Day 9 Trt 2800
keep_D9.2800 <- rowSums(cpm(geneC_all[,meta$SFVrn==""D09.2800""])>=1) >= 5
sum(keep_D9.2800)
#Day 9 Trt 400
keep_D9.400 <- rowSums(cpm(geneC_all[,meta$SFVrn==""D09.400""])>=1) >= 5
sum(keep_D9.400)
#Day 80 Trt 2800
keep_D80.2800 <- rowSums(cpm(geneC_all[,meta$SFVrn==""D80.2800""])>=1) >= 5
sum(keep_D80.2800)
#Day 80 Trt 400
keep_D80.400 <- rowSums(cpm(geneC_all[,meta$SFVrn==""D80.400""])>=1) >= 5
sum(keep_D80.400)

keep_gene_a2 <- rowSums(cbind(keep_D9.2800,keep_D9.400,
                              keep_D80.2800,keep_D80.400)) >= 1

# Filter 
geneC_a2 <- geneC_all[keep_gene_a2, ]
gene_final_a2 <- gene_order[keep_gene_a2,]

## Create DGEList
dge_gene_a2 <- DGEList(geneC_a2,genes = gene_final_a2) # counts - rsem

## Count summary tables
# All genes captures before filter
perSampleMeanCount <- mean(colSums(geneC_all)) #mean
perSampleSDCount <- sd(colSums(geneC_all)) #sd
perSampleMinCount <- min(colSums(geneC_all)) #min
perSampleMaxCount <- max(colSums(geneC_all)) #max
numberOfGenes <- sum(rowSums(geneC_all)>0) # Number of genes in dataset (rows)
table <- data.frame(Data=""All counts"",mean=perSampleMeanCount,sd=perSampleSDCount,min=perSampleMinCount,max=perSampleMaxCount,genes=numberOfGenes)
# Genes counts after filtering
perSampleMeanCountF <- mean(colSums(geneC_a2)) #mean
perSampleSDCountF <- sd(colSums(geneC_a2)) #sd
perSampleMinCountF <- min(colSums(geneC_a2)) #min
perSampleMaxCountF <- max(colSums(geneC_a2)) #max
numberOfGenesF <- sum(rowSums(geneC_a2)>0) # Number of genes in dataset (rows)
table <- rbind(table,
               data.frame(Data=""Filtered counts"",
                          mean=perSampleMeanCountF,sd=perSampleSDCountF,
                          min=perSampleMinCountF,max=perSampleMaxCountF,
                          genes=numberOfGenesF))
print(table)

#### Save initial DGEList objects####
#saveRDS(dge_gene_a2,paste0(outputDir,""/RNA_gene_preNormalization_DGEListObj.RData""))

#### Normalization with edgeR ####
# Calculate normalization factors for scaling raw lib. size
dge_gene_a2_norm <- calcNormFactors(dge_gene_a2,method = ""TMMwsp"") # gene - approach 2
# Bar plot of normalization factors
barplot(dge_gene_a2_norm$samples$norm.factors~rownames(dge_gene_a2_norm$samples),
        las=2,ylab=""Normalization factor"",xlab=""Samples"")
plotMDS(dge_gene_a2_norm, col = as.numeric(meta$SFVrn))

## Create design matrix  
design <- model.matrix(~0+SFVrn,data=meta) # 0+ is needed here otherwise the first level defaults to 1.
#Rename columns
colnames(design) <- levels(meta$SFVrn)

#### Transform and create observational level weights ####
## Gene Features 
dge_gene_a2_o1_voom <- voomWithQualityWeights(dge_gene_a2_norm,design,plot = TRUE)
## Plots
barplot(dge_gene_a2_o1_voom$targets$sample.weights~rownames(dge_gene_a2_o1_voom$targets),
        las=2,ylab=""Sample Specific Weights"",xlab=""Samples"")
ge.pca <- prcomp(t(dge_gene_a2_o1_voom$E), center = TRUE,scale = TRUE)
ggbiplot(ge.pca,
         ellipse=FALSE,
         obs.scale = 1,
         var.scale = 1,
         var.axes=FALSE,
         labels= colnames(dge_gene_a2_o1_voom$E),
         groups=meta$SFVrn)
plotMDS(dge_gene_a2_o1_voom, col = as.numeric(meta$SFVrn))

# Saving the final transformation of the data (after individual weights)
# saveRDS(dge_gene_a2_o1_voom,paste0(outputDir,""/RNA_gene_postVoomAndNormalization_DGEListObj.RData""))

# Dendrogram based on co-expression clustering. Further illustrates 17005 as an outlier.
out <- dist(t(scale(dge_gene_a2_o1_voom$E)))
out2 <- hclust(out,method = ""complete"")
plot(out2)",0,"diamondback moth, Plutella xylostella, migration trajectories, population genomic variation, single nucleotide polymorphisms, double-digest RAD sequencing, genetic structure, discriminant analysis of principal components, assignment tests, spatial kinship patterns,"
Migration trajectories of the diamondback moth Plutella xylostella in China inferred from population genomic variation,"BACKGROUND:The diamondback moth (DBM), Plutella xylostella (Lepidoptera: Plutellidae), is a notorious pest of cruciferous plants. In temperate areas, annual populations of DBM originate from adult migrants. However, the source populations and migration trajectories of immigrants remain unclear. Here, we investigated migration trajectories of DBM in China with genome-wide single nucleotide polymorphisms (SNPs) genotyped using double-digest RAD (ddRAD) sequencing. We first analyzed patterns of spatial and temporal genetic structure among southern source and northern recipient populations, then inferred migration trajectories into northern regions using discriminant analysis of principal components (DAPC), assignment tests and spatial kinship patterns.RESULTS:Temporal genetic differentiation among populations was low, indicating sources of recipient populations and migration trajectories are stable. Spatial genetic structure indicated three genetic clusters in the southern source populations. Assignment tests linked northern populations to the Sichuan cluster, and central-eastern populations to the South and Yunnan clusters, indicating that Sichuan populations are sources of northern immigrants and South and Yunnan populations are sources of central-eastern populations. First-order (full-sib) and second-order (half-sib) kin pairs were always found within populations, but about 35-40% of third-order (cousin) pairs were found in different populations. Closely related individuals in different populations were in about 35-40% of cases found at distances of 900 to 1500 km, while some were separated by over 2000 km.CONCLUSION:This study unravels seasonal migration patterns in the DBM. We demonstrate how careful sampling and population genomic analyses can be combined to help understand cryptic migration patterns in insects.","['library(assignPOP)\nlibrary(klaR)\nlibrary(mime)\nlibrary(genepopedit)\n\nwork_dir <- ""~/Desktop/CMZ/assignpop/"" \nsetwd(work_dir)\nall.gen  = ""2017_assignpop_source_pop_test5.txt""\nall.renamed.gen=""all.renamed.gen""\nspecies = ""px_test_5""\ngenepop_ID(genepop=all.gen, path=paste0(work_dir,""/"", all.renamed.gen)) \nPopNames.all <- genepop_detective(all.renamed.gen, variable=""Pops"")\nPopCounts.all <- genepop_detective(all.renamed.gen, variable=""PopNum"")\nPopCounts.all\n\npop.sublist <- c(""ST"",""SW"")\npop.sublistname <- paste(pop.sublist, collapse =""_"", sep=""."")\n\npop.sub.gen <- paste(pop.sublistname, "".gen"", sep="""")\nsubset_genepop(genepop= all.renamed.gen, keep = TRUE, \n               spop = pop.sublist, \n               path = paste0(work_dir,""/"", pop.sub.gen))\nused.PopNames <- genepop_detective(pop.sub.gen, variable=""Pops"")\nall_ref <- read.Genepop(pop.sub.gen, pop.names=used.PopNames)\n#all_inc <- read.Genepop( ""nokin_allq9_inc.txt"")\n#all_ref_rd <- reduce.allele(all_ref, p = 0.95)\nassign_trial = assign.MC(all_ref, train.inds=c(25,28,32), train.loci=c(0.6,0.8,1), \n                         loci.sample=""fst"", iterations=1000, dir = paste(species, pop.sublistname, ""mc_svm/"", sep="".""), model=""svm"" )   \naccuMC <- accuracy.MC(dir = paste(species, pop.sublistname, ""mc_svm/"", sep="".""))\npdf(file=paste(species, pop.sublistname, ""assignpop.mc.plot.pdf"", sep="".""))\nplot.result <- accuracy.plot(accuMC, pop = c(""all"", used.PopNames))\nprint(plot.result)\ndev.off()\n\ncheck.loci(dir = paste(species, pop.sublistname, ""mc_svm/"", sep="".""),top.loci = 20)\n      \nassign.kfold( all_ref, k.fold=c(2,3), train.loci=c(1), \n              loci.sample=""random"", model=""svm"", \n              dir = paste(species, pop.sublistname, ""kfold_result/"", sep="".""))\naccuKF <- accuracy.kfold(dir = paste(species, pop.sublistname, ""kfold_result/"", sep=""."")) #Use this function for K-fold cross-validation results\n      \npdf(file=paste(species, pop.sublistname, ""kfold.membership.plot.pdf"", sep="".""), width=30, height=8)\nmembership.plot(dir = paste(species, pop.sublistname, ""kfold_result/"", sep=""."")) \ndev.off()\n\nfile.remove(pop.sub.gen)\n\nfile.remove(""all.renamed.gen"")\n\ngenin_known <- read.Genepop(""2017_assignpop_source_pop_test5.txt"",pop.names = c(""ST"",""SW""))\ngenin_unknown <- read.Genepop(""2017_assignpop_test_CE_NT.txt"")\nassign.X(genin_known, genin_unknown, dir = ""Result1/"",model = ""svm"")\n\ncheck.loci(dir = ""test5/px_test_5.ST_SW.mc_svm/"", top.loci = 20)\n\ndev.off()\n', '#Setup. Load libraries, directory, and original data\r\n#devtools::install_github(\'cran/ggplot2\') \r\nlibrary(""ggplot2"")\r\n#library(ggforce)\r\n#library(ggrepel)\r\nlibrary(dplyr) #For sampling\r\nrels <- read.delim(""test_18.txt"")\r\n\r\n#Creat subcategories through filtering the data\r\n\r\nrelsedited <- rels[ which(rels$type!=\'same\'),]\r\n\r\n### The most important subcategories ###\r\n\r\nfullsibs <- relsedited[ which(relsedited$R == ""FS""),]\r\nhalfsibs <- relsedited[ which(relsedited$R == ""HS""),]\r\nsibs <- relsedited[ which(relsedited$R  == ""FS"" | relsedited$R == ""HS""),]\r\next <- relsedited[ which(relsedited$kinship >= 0.063 & relsedited$R == ""U""),]\r\nlot <- relsedited[ which(relsedited$kinship >=0.063),]\r\n\r\n#Functions to create 95% interval for dispersal distance (based on 2x standard deviation)... not sure if legit\r\n#edit: the following unimportant code... \r\n\r\n2*sd(sibs$km) #NS = 4*pi*sigma^2*density\r\n2*(mean(sibs$km^2))\r\n2*sd(ext$km)\r\n2*sd(fullsibs$km)\r\n2*sd(relsedited$km)\r\n2*sd(halfsibs$km)\r\n\r\n#Distribution\r\n\r\nmean_se <- function(x, mult = 1) {  \r\n  x <- na.omit(x)\r\n  se <- mult * sqrt(var(x) / length(x))\r\n  mean <- mean(x)\r\n  data.frame(y = mean, ymin = mean - se, ymax = mean + se)\r\n}\r\n\r\n# Mean and confidence interval for mean... \r\n\r\nquantile(replicate(1000, mean(sample(ext$km, rep=TRUE, size = 10))), c(0.025,0.5, 0.975))\r\n\r\n#Population parameter determination\r\n\r\nksd <- sd(relsedited[which(relsedited$R == ""U""),]$kinship)\r\n\r\nsd(relsedited[which(relsedited$R == ""HS""),]$kinship)\r\nsd(relsedited[which(relsedited$R == ""FS""),]$kinship)\r\n\r\nmean(relsedited[which(relsedited$R == ""U""),]$kinship)\r\n\r\nkmed <- median(relsedited[which(relsedited$R == ""U""),]$kinship)\r\n#kmed <- median(relsedited$kinship)\r\n\r\nkmed + 3*ksd\r\n\r\n#Simulations for determining cutoff\r\n\r\nrnorm2 <- function(n,mean,sd) { mean+sd*scale(rnorm(n))}\r\ntester <- data.frame(1:100000)\r\n\r\ntester$nonrels_sample <- rnorm2(100000, kmed, ksd)\r\ntester$seccous_sample <- rnorm2(100000, 0.0156, ksd)\r\ntester$cous_sample <- rnorm2(100000, 0.0625, ksd)\r\ntester$cous2_sample <- rnorm2(100000, 0.03125, ksd)\r\ntester$cousb_sample <- sample_n(data.frame(c(tester$cous_sample, tester$cous2_sample)), 100000)[,1]\r\ntester$halfs_sample <- rnorm2(100000, 0.125, 0.044)\r\ntester$fulls_normal <- rnorm2(100000, 0.25, 0.039)\r\ntester$totals_sample <- sample_n(data.frame(c(tester$nonrels_sample[1:100000], tester$seccous_sample[1:160], tester$cousb_sample[1:80], tester$halfs_sample[1:20], tester$fulls_normal[1:20])), 100000)\r\ntester$totals_sample\r\np=ggplot(tester)+\r\n  #geom_histogram(data = tester[1:100000,], mapping = aes(x=totals_sample), binwidth = 0.0025, colour = ""grey"", fill=""lightgrey"")+\r\n  geom_freqpoly(data = tester[1:20,], mapping = aes(x=fulls_normal), binwidth=0.0025, colour = ""blue"", fill = ""lightblue"", size=2) +\r\n  geom_freqpoly(data = tester[1:20,], mapping = aes(x=halfs_sample), binwidth=0.0025, colour = ""darkgreen"", fill=""lightgreen"", size=1.5)+\r\n  #geom_freqpoly(data = tester[1:500,], mapping = aes(x=cous_sample), binwidth=0.0025, colour = ""orange"", fill=""yellow"", size=1.5)+\r\n  geom_freqpoly(data = tester[1:80,], mapping = aes(x=cousb_sample), binwidth=0.0025, colour = ""orange"", fill=""yellow"", size=1.5)+\r\n  geom_freqpoly(data = tester[1:160,], mapping = aes(x=seccous_sample), binwidth=0.0025, colour = ""red"", fill=""salmon"", size=1.5)+\r\n  geom_freqpoly(data = tester[1:100000,], mapping = aes(x=nonrels_sample), binwidth=0.0025, fill=""white"", colour=""black"", alpha=1, size=1.5)+\r\n  geom_segment(x=0.063, xend=0.063, y=0, yend=500, linetype = 2, size=1, colour=""black"")+\r\n  labs(x=""kinship value"",y=""count"")+\r\n  theme_bw()+\r\n  #scale_y_log10()\r\n  coord_cartesian(ylim = c(0,35), xlim = c(0, 0.26))\r\n  p=p + annotate(""text"", x=0.22, y=32,label=""full siblings"",size=5)+\r\n  annotate(""text"", x=0.22, y=30, label=""half siblings"",size=5)+\r\n  annotate(""text"",x=0.22,y=28,label=""1st cousins"",size=5)+\r\n  annotate(""text"",x=0.22,y=26,label=""2nd cousins"",size=5)+\r\n  annotate(""text"",x=0.22,y=24,label=""unrelated"",size=5)+ \r\n  annotate(""text"",x=0.23,y=22,label=""0.063 kinship threshold"",size=5)+\r\n  annotate(""segment"",x=0.175,xend=0.195,y=32,yend=32,col=""blue"",size=1)+\r\n  annotate(""segment"",x=0.175,xend=0.195,y=30,yend=30,col=""darkgreen"",size=1)+\r\n  annotate(""segment"",x=0.175,xend=0.195,y=28,yend=28,col=""orange"",size=1)+\r\n  annotate(""segment"",x=0.175,xend=0.195,y=26,yend=26,col=""red"",size=1)+\r\n  annotate(""segment"",x=0.175,xend=0.195,y=24,yend=24,col=""black"",size=1)+\r\n  annotate(""segment"",x=0.175,xend=0.195,y=22,yend=22,col=""black"",size=1,lty=2)\r\n  p\r\n\r\n#Compare fit of this distribution with that of field data (histogram)\r\n  \r\nggplot(relsedited[1:80000,])+\r\n  geom_histogram(data = tester[1:80000,], mapping = aes(x=totals_sample), binwidth = 0.005, colour = ""grey95"", fill = ""indianred1"")+\r\n  #geom_histogram(data = tester[1:80000,], mapping = aes(x=nonrels_sample), binwidth = 0.005, colour = ""grey95"", fill = ""cornflowerblue"")+\r\n  geom_freqpoly(data = relsedited[1:80000,], mapping = aes(x=kinship), binwidth = 0.005, colour = ""black"",', '\n##install.packages(c(""poppr"", ""mmod"", ""magrittr"", ""treemap""), repos = ""http://cran.rstudio.com"", dependencies = TRUE)\nlibrary(vcfR)\nlibrary(pinfsc50)\nlibrary(ape)\nlibrary(reshape2)\nlibrary(RColorBrewer)\n#setwd(""~/Desktop/ML/ddrad/PS/filter"")\ngetwd()\n\n########################Part 1: import and read vcf file into R, check data#################\npx.vcf.raw <- vcfR::read.vcfR(""px_2017_mac30_289_1363.recode.vcf"", convertNA = TRUE,verbose = FALSE) #time-costing\n\nsave.image(""/ipm1/chenmingzhu/px.rad/px.filter"")\n\npx.vcf <- px.vcf.raw\n#strwrap(px.vcf@meta[1:10])\n#queryMETA(px.vcf)\n#queryMETA(px.vcf, element = \'DP\')\n#queryMETA(px.vcf, element = \'FORMAT=<ID=DP\')\n#head(getFIX(px.vcf))\npx.vcf@gt[1:10, 1:10]\n\n#########################################Part 2: Quality Control############################\n##check summary data, including missing data\npx.vcf\n\n##extract depth of genotype#\ndp <- extract.gt(px.vcf, element = ""DP"", as.numeric=TRUE) #time-costing\n\n##barplot of missingness for every sample#\nmyMiss <- apply(dp, MARGIN = 2, function(x){ sum(is.na(x)) }) ###MARGIN = 2, missingness for every sample.data from bwa and bowtie2, miss data will represent with NA, while data from bowtie, miss data will be indecated by 0.\n#myMiss <- apply(dp, MARGIN = 2, function(x){ sum(x==0) })   ####data from bowtie?\n\nmyMiss <- myMiss/nrow(px.vcf)\npdf(file=""gm_ddRAD_missing_barplot11.pdf"", width=30, height=8)  ##histogram of missingness for every sample#\npalette(brewer.pal(n=12, name = \'Set3\'))\npar(mar = c(16,4,4,2))\nbarplot(myMiss, las = 2, col = 1:16)\ntitle(ylab = ""Missingness (%)"")\ndev.off()\n\nmyMiss <- apply(dp, MARGIN = 1, function(x){ sum(is.na(x)) }) ###MARGIN = 1, missingness for snps. data from bwa, miss data will represent with NA, while data from bowtie, miss data will be indecated by 0.\n#myMiss <- apply(dp, MARGIN = 1, function(x){ sum(x==0) })   ####data from bowtie\nmyMiss <- myMiss/ncol(px.vcf@gt[,-1])\npdf(file=""gm_ddRAD_missing_histogram.pdf"", width=10, height=8)  ####histogram of missingness for snps#\nhist(myMiss, col = ""#8DD3C7"", xlab = ""Missingness (%)"", main = """")\ndev.off()\n\n##draw barplot of depth for every sample#\npdf(file=""gm_ddRAD_depth_boxplot.pdf"", width=30, height=8)\npar(mar=c(12,4,4,2))\nboxplot(dp, col=2:8, las=3)\ntitle(ylab = ""Depth (DP)"")\ndev.off()\n\n##draw violin plot of depth#\nlibrary(reshape2)\nlibrary(ggplot2) \nlibrary(cowplot)\n\n##Melt our matrix into a long form data.frame.\ndpf <- melt(dp, varnames=c(\'Index\', \'Sample\'), value.name = \'Depth\', na.rm=TRUE)\n#dpf <- dpf[ dpf$Depth > 5,]\n#str(dpf)\n##Create a row designator, You may want to adjust this\nsamps_per_row <- 25                               ####it is better to be divided evenly by numbel of individuals\nmyRows <- ceiling(length(levels(dpf$Sample))/samps_per_row)\nmyList <- vector(mode = ""list"", length = myRows)\n\nfor(i in 1:myRows){\n  myIndex <- c(i*samps_per_row - samps_per_row + 1):c(i*samps_per_row)\n  myIndex <- myIndex[myIndex <= length(levels(dpf$Sample))]\n  myLevels <- levels(dpf$Sample)[myIndex]\n  myRegex <- paste(myLevels, collapse = ""$|^"")\n  myRegex <- paste(""^"", myRegex, ""$"", sep = """")\n  myList[[i]] <- dpf[grep(myRegex, dpf$Sample),]\n  myList[[i]]$Sample <- factor(myList[[i]]$Sample)\n}\n\n# Create the plot.\n\tpdf(""gm_ddRAD_depth_violin.pdf"", width = 10, height=8)\n\tmyPlots <- vector(mode = ""list"", length = myRows)\n\tfor(i in 1:myRows){\n\t  myPlots[[i]] <- ggplot(myList[[i]], aes(x=Sample, y=Depth)) + \n\t    geom_violin(fill=""#8dd3c7"", adjust=1.0, scale = ""count"", trim=TRUE)\n\t  \n\t  myPlots[[i]] <- myPlots[[i]] + theme_bw()\n\t  myPlots[[i]] <- myPlots[[i]] + theme(axis.title.x = element_blank(), \n\t                                       axis.text.x = element_text(angle = 60, hjust = 1))\n\t  myPlots[[i]] <- myPlots[[i]] + scale_y_continuous(trans=scales::log2_trans(), \n\t                                                    breaks=c(1, 10, 100, 800),\n\t                                                    minor_breaks=c(1:10, 2:10*10, 2:8*100))\n\t  myPlots[[i]] <- myPlots[[i]] + theme( panel.grid.major.y=element_line(color = ""#A9A9A9"", size=0.6) )\n\t  myPlots[[i]] <- myPlots[[i]] + theme( panel.grid.minor.y=element_line(color = ""#C0C0C0"", size=0.2) )\n\t}\n\tmyPlots\n\tdev.off()\n\n##heatmap of depth#\npdf(""gm_ddRAD_depth_raw_variant_heatmap.pdf"", width = 40, height=40)\nheatmap.bp((dp[1:1000,])^(1/4),rlabels = F)           #########some sequenced samples are so deep that it was hard to read by heatmap, so the depth was divided by the square root---Lijun\ndev.off()\n\n##ommit data, replace low and high coverage SNPs (DP value) with NA#. MARGIN=2, missingness for every samples.\ntotal.dep <- apply(dp, MARGIN=2, sum, na.rm=TRUE)  ###total depth of every sample\nrel.dep <- t(apply(dp,MARGIN=1,FUN = ""/"",total.dep[1:length(total.dep)]))  ###relative depth of each SNP on each samle, need transposition t()\n\nrel.dep.snp <- apply(rel.dep, MARGIN = 1, sum,na.rm=TRUE)    ##accumulated relative depth of each SNP on each samle, need transposition t()\nquants <- quantile(rel.dep.snp, probs = c(0.01, ']",0,"Molecular analysis, phenotypic analysis, rodent models, conserved modulators, species-specific modulators, human sarcopenia, skeletal muscle mass, skeletal muscle function, mice, rats, model organisms, predisposing factors, disease aspects"
Molecular and phenotypic analysis of rodent models reveals conserved and species-specific modulators of human sarcopenia,"Loss of skeletal muscle mass and function (sarcopenia), affects 5-13% of individuals aged 60-70 years. Mice and rats are widely used as model organisms to identify predisposing factors, yet what aspects of the disease are recapitulated in different animal models is not known. Here we generated a time series of phenotypic measurements and RNA sequencing data from the gastrocnemius muscle of mice, and analyzed them along analogous data from rats and humans. We found that rodents recapitulate especially the mitochondrial function changes observed in human sarcopenia, while inflammatory responses are more conserved at the pathway and not the gene level. Rats recapitulate human sarcopenia-associated perturbations in extracellular matrix, while mice those in RNA processing and autophagy. We further inferred transcription regulators of early and late molecular changes, which could be targeted by therapeutic interventions. Finally, our study demonstrates that phenotypic measurements should be considered in analyzing aging-related molecular data.","['#!/usr/bin/env Rscript\n\n### Created: Apr 14, 2020\n### Author: Anastasiya Boersch\n### Company: Zavolan Group, Biozentrum, University of Basel\n\n##### Load/install dependencies\nlibrary(KEGGREST)\nlibrary(biomaRt)\n\n# Mouse\npath.list <- keggLink(""mmu"",""pathway"")\npath.list.m <- as.matrix(path.list)\npath.id <- unique(rownames(path.list.m))\npath.gene <- data.frame(rownames(path.list.m),path.list.m)\ncolnames(path.gene) <- c(""path_id"", ""entrez_id"")\nent.id <- substring(path.gene$entrez_id,5)\npath.gene$entrez_id <- as.numeric(ent.id)\n\n# Convert Entrez Ids to Ensembl gene Ids\nmart = useMart(\'ensembl\')\nlistDatasets(mart)\nmouse = useMart(""ensembl"", dataset = ""mmusculus_gene_ensembl"")\nx <- listAttributes(mouse)\nmapping <- getBM(attributes = c(""entrezgene_id"", ""ensembl_gene_id""), mart = mouse, uniqueRows = TRUE)\ncolnames(mapping) <- c(""entrez_id"",""ensembl_id"")\nmap_ens_uniq <- mapping[!duplicated(mapping$ensembl_id), ]\n\n# Merge pathways, entrez ids and pathway ids\npath.gene <- merge(path.gene,map_ens_uniq,by=""entrez_id"")\n\n# Add gene names\nens2name_mouse <- read.table(""GeneIdTOGeneName_mouse.txt"",header=F, sep=""\\t"")\ncolnames(ens2name_mouse) <- c(""ensembl_id"",""gene_symbol"")\n\npath.gene <- merge(path.gene,ens2name_mouse,by=""ensembl_id"")\n\n# Add pathway names\npathid <- as.character(unique(path.gene$path_id))\ntmp1 <- rep(NA,length(pathid))\ntmp2 <- rep(NA,length(pathid))\nfor (i in seq(length(pathid))) {\n  tmp2[i] <- pathid[i]\n  res <- try(keggGet(pathid[i])[[1]]$NAME)\n  if(!inherits(res, ""try-error""))\n  { tmp1[i] <- keggGet(pathid[i])[[1]]$NAME\n  tmp1[i] <- substr(tmp1[i],1,nchar(tmp1[i])-23)\n  }\n}\npath_id_name <- data.frame(path_id=tmp2,path_name=tmp1)\n\n# Write a table containing pathway IDs and pathway names\nwrite.table(path_id_name,""MousePathIdName.txt"",sep=""\\t"",quote = T)\n\n# Write a table containing pathways and genes annotated to these pathways\npath.gene <- merge(path.gene,path_id_name,by=""path_id"")\nwrite.table(path.gene,""MouseGenesPathways.txt"",sep=""\\t"",quote = T)\n\n# Rat\npath.list <- keggLink(""rno"",""pathway"")\npath.list.m <- as.matrix(path.list)\npath.id <- unique(rownames(path.list.m))\npath.gene <- data.frame(rownames(path.list.m),path.list.m)\ncolnames(path.gene) <- c(""path_id"", ""entrez_id"")\nent.id <- substring(path.gene$entrez_id,5)\npath.gene$entrez_id <- as.numeric(ent.id)\n\n# Convert Entrez Ids to Ensembl gene Ids\nmart = useMart(\'ensembl\')\nlistDatasets(mart)\nrat = useMart(""ensembl"", dataset = ""rnorvegicus_gene_ensembl"")\nx <- listAttributes(rat)\nmapping <- getBM(attributes = c(""entrezgene_id"", ""ensembl_gene_id""), mart = rat, uniqueRows = TRUE)\ncolnames(mapping) <- c(""entrez_id"",""ensembl_id"")\nmap_ens_uniq <- mapping[!duplicated(mapping$ensembl_id), ]\n\n# Merge pathways, entrez ids and pathway ids\npath.gene <- merge(path.gene,map_ens_uniq,by=""entrez_id"")\n\n# Add gene names\nens2name_rat <- read.table(""GeneIdTOGeneName_rat.txt"",header=F, sep=""\\t"")\ncolnames(ens2name_rat) <- c(""ensembl_id"",""gene_symbol"")\n\npath.gene <- merge(path.gene,ens2name_rat,by=""ensembl_id"")\n\n# Add pathway names\npathid <- as.character(unique(path.gene$path_id))\ntmp1 <- rep(NA,length(pathid))\ntmp2 <- rep(NA,length(pathid))\nfor (i in seq(length(pathid))) {\n  tmp2[i] <- pathid[i]\n  res <- try(keggGet(pathid[i])[[1]]$NAME)\n  if(!inherits(res, ""try-error""))\n  { tmp1[i] <- keggGet(pathid[i])[[1]]$NAME\n  tmp1[i] <- substr(tmp1[i],1,nchar(tmp1[i])-25)\n  }\n}\npath_id_name <- data.frame(path_id=tmp2,path_name=tmp1)\n\n# Write a table containing pathway IDs and pathway names\nwrite.table(path_id_name,""RatPathIdName.txt"",sep=""\\t"",quote = T)\n\n# Write a table containing pathways and genes annotated to these pathways\npath.gene <- merge(path.gene,path_id_name,by=""path_id"")\nwrite.table(path.gene,""RatGenesPathways.txt"",sep=""\\t"",quote = T)\n\n# Human\npath.list <- keggLink(""hsa"",""pathway"")\npath.list.m <- as.matrix(path.list)\npath.id <- unique(rownames(path.list.m))\npath.gene <- data.frame(rownames(path.list.m),path.list.m)\ncolnames(path.gene) <- c(""path_id"", ""entrez_id"")\nent.id <- substring(path.gene$entrez_id,5)\npath.gene$entrez_id <- as.numeric(ent.id)\n\n# Convert Entrez Ids to Ensembl gene Ids\nmart = useMart(\'ensembl\')\nlistDatasets(mart)\nhuman = useMart(""ensembl"", dataset = ""hsapiens_gene_ensembl"")\nx <- listAttributes(human)\nmapping <- getBM(attributes = c(""entrezgene_id"", ""ensembl_gene_id""), mart = human, uniqueRows = TRUE)\ncolnames(mapping) <- c(""entrez_id"",""ensembl_id"")\nmap_ens_uniq <- mapping[!duplicated(mapping$ensembl_id), ]\n\n# Merge pathways, entrez ids and pathway ids\npath.gene <- merge(path.gene,map_ens_uniq,by=""entrez_id"")\n\n# Add gene names\nens2name_human <- read.table(""GeneIdTOGeneName_human.txt"",header=F, sep=""\\t"")\ncolnames(ens2name_human) <- c(""ensembl_id"",""gene_symbol"")\n\npath.gene <- merge(path.gene,ens2name_human,by=""ensembl_id"")\n\n# Add pathway names\npathid <- as.character(unique(path.gene$path_id))\ntmp1 <- rep(NA,length(pathid))\ntmp2 <- rep(NA,length(pathid))\nfor (i in seq(length(pathid))) {\n  tmp2[i] <- pathid[i]\n  res <- try(keggGet(pathid[i])[[1]]$NAME)\n  if(!inherits(', 'library(biomaRt)\n\n# Load gene annotation for mouse\nens2name_mouse <- read.table(""GeneIdTOGeneName_mouse.txt"",header=F, sep=""\\t"")\ncolnames(ens2name_mouse) <- c(""gene_id"",""gene_name"")\n# Load gene annotation for rat\nens2name_rat <- read.table(""GeneIdTOGeneName_rat.txt"",header=F, sep=""\\t"")\ncolnames(ens2name_rat) <- c(""gene_id"",""gene_name"")\n# Load gene_annotation for human\nens2name_human <- read.table(""GeneIdTOGeneName_human.txt"",header=F, sep=""\\t"")\ncolnames(ens2name_human) <- c(""gene_id"",""gene_name"")\n\n# Get orthologues for mouse, rat and human\n# Connect to the bioMart\nmouse = useMart(""ensembl"", dataset = ""mmusculus_gene_ensembl"")\nrat = useMart(""ensembl"", dataset = ""rnorvegicus_gene_ensembl"")\nhuman = useMart(""ensembl"", dataset = ""hsapiens_gene_ensembl"")\n\n# Convert mouse ensembl ids to rat ensembl ids\nmouse_rat <- getLDS(attributes = c(""ensembl_gene_id""),\n                    filters = ""ensembl_gene_id"", values = ens2name_mouse$gene_id,mart = mouse,\n                    attributesL = c(""ensembl_gene_id""), martL = rat)\ncolnames(mouse_rat) <- c(""gene_id_mouse"",""gene_id_rat"")\nmouse_rat <- mouse_rat[!duplicated(mouse_rat$gene_id_mouse),]\nmouse_rat <- mouse_rat[!duplicated(mouse_rat$gene_id_rat),]\n\n# Convert mouse ensembl ids to human ensembl ids\nmouse_human <- getLDS(attributes = c(""ensembl_gene_id""),\n                      filters = ""ensembl_gene_id"", values = ens2name_mouse$gene_id,mart = mouse,\n                      attributesL = c(""ensembl_gene_id""), martL = human)\ncolnames(mouse_human) <- c(""gene_id_mouse"",""gene_id_human"")\nmouse_human <- mouse_human[!duplicated(mouse_human$gene_id_mouse),]\nmouse_human <- mouse_human[!duplicated(mouse_human$gene_id_human),]\n\n# Merge tables relating the ensembl annotation of genes in mouse, rat and human\nmouse_rat_human <- merge(mouse_rat,mouse_human,by=""gene_id_mouse"")\nwrite.table(mouse_rat_human, ""MouseRatHuman_orthologues.txt"",sep=""\\t"",col.names = T, row.names = F, quote = F)\n', '#!/usr/bin/env Rscript\n\n### Created: Apr 14, 2020\n### Author: Anastasiya Boersch\n### Company: Zavolan Group, Biozentrum, University of Basel\n\nlibrary(venn)\nlibrary(reshape2)\nlibrary(ggdendro)\nlibrary(matrixStats)\nlibrary(edgeR)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(ggplot2)\nlibrary(biomaRt)\nlibrary(""RDAVIDWebService"")\noptions(java.parameters = ""-Xmx10000m"")\n\n# The function to infer names of GO terms\nsplitfun <- function(a) substr(a,12,1000000L)\n\n# Create directories for storing figures\nsystem(""mkdir -p Figures_raw"")\nsystem(""mkdir -p Figures_raw/01_VarianceIncrAging"")\nsystem(""mkdir -p Figures_raw/02_CorrelationPCAPhenotypes"")\nsystem(""mkdir -p Figures_raw/03_ProjGenePCAGSEA"")\nsystem(""mkdir -p Figures_raw/04_PathSlopesOverlap"")\nsystem(""mkdir -p Figures_raw/05_ISMARA"")\n\n# Load gene annotation for mouse\nens2name_mouse <- read.table(""GeneIdTOGeneName_mouse.txt"",header=F, sep=""\\t"")\ncolnames(ens2name_mouse) <- c(""gene_id"",""gene_name"")\n# Load gene annotation for rat\nens2name_rat <- read.table(""GeneIdTOGeneName_rat.txt"",header=F, sep=""\\t"")\ncolnames(ens2name_rat) <- c(""gene_id"",""gene_name"")\n\n# Load pre-processed data for mouse time course data set\ntpms_gene_mouse_ts <- read.table(""GSEXXXXXX_tpms_mouse_aging_timecourse.txt"",header = T,sep=""\\t"")\nrownames(tpms_gene_mouse_ts) <- tpms_gene_mouse_ts$ensembl_id\ntpms_gene_mouse_ts <- tpms_gene_mouse_ts[,c(-1,-2)]\n\ncounts_gene_mouse_ts <- read.table(""GSEXXXXXX_counts_mouse_aging_timecourse.txt"",header = T,sep=""\\t"")\nrownames(counts_gene_mouse_ts) <- counts_gene_mouse_ts$ensembl_id\ncounts_gene_mouse_ts <- counts_gene_mouse_ts[,c(-1,-2)]\n\n# Annotation of replicates for the mouse time series\ndes_mouse_ts=c(""8m"",""18m"",""22m"",""24m"",""26m"",""28m"")\nn_rep_mouse_ts <- c(8,8,8,8,9,9)\n\n# Get pathway annotation of mouse\npath_id_name_mouse <- read.table(""MousePathIdName.txt"",sep=""\\t"")\npath_gene_mouse <- read.table(""MouseGenesPathways.txt"",sep=""\\t"")\n# Remove disease pathways\npath_id_name_mouse <- path_id_name_mouse[which(substr(path_id_name_mouse$path_id,10,10)!=""5""),]\npath_gene_mouse <- path_gene_mouse[which(substr(path_gene_mouse$path_id,10,10)!=""5""),]\npath_id_name_mouse <- path_id_name_mouse[-which(path_id_name_mouse$path_id  %in% c(""path:mmu04930"",""path:mmu04940"",""path:mmu04950"",""path:mmu04932"",""path:mmu04931"",""path:mmu04933"",""path:mmu04934"")),]\npath_gene_mouse <- path_gene_mouse[-which(path_gene_mouse$path_id  %in% c(""path:mmu04930"",""path:mmu04940"",""path:mmu04950"",""path:mmu04932"",""path:mmu04931"",""path:mmu04933"",""path:mmu04934"")),]\n# Remove the broad category called ""Metabolic pathways""\npath_id_name_mouse <- path_id_name_mouse[-which(path_id_name_mouse$path_id ==""path:mmu01100""),]\npath_gene_mouse <- path_gene_mouse[-which(path_gene_mouse$path_id ==""path:mmu01100""),]\n\n# Load pre-processed data for mouse time course data set\ntpms_gene_rat_ts <- read.table(""GSEYYYYYY_tpms_kallisto_gastrocnemius.txt"",header = T,sep=""\\t"")\nrownames(tpms_gene_rat_ts) <- tpms_gene_rat_ts$ensembl_id\ntpms_gene_rat_ts <- tpms_gene_rat_ts[,c(-1,-2)]\n\ncounts_gene_rat_ts <- read.table(""GSEYYYYYY_counts_kallisto_gastrocnemius.txt"",header = T,sep=""\\t"")\nrownames(counts_gene_rat_ts) <- counts_gene_rat_ts$ensembl_id\ncounts_gene_rat_ts <- counts_gene_rat_ts[,c(-1,-2)]\n\n# Annotation of replicates for the rat time series\nn_rep_rat_ts=c(10,10,10,10,9)\ndes_rat_ts=c(""8m"",""18m"",""20m"",""22m"",""24m"")\n\n# Get pathway annotation of rat\npath_id_name_rat <- read.table(""RatPathIdName.txt"",sep=""\\t"")\npath_gene_rat <- read.table(""RatGenesPathways.txt"",sep=""\\t"")\n# Remove disease pathways\npath_id_name_rat <- path_id_name_rat[which(substr(path_id_name_rat$path_id,10,10)!=""5""),]\npath_gene_rat <- path_gene_rat[which(substr(path_gene_rat$path_id,10,10)!=""5""),]\npath_id_name_rat <- path_id_name_rat[-which(path_id_name_rat$path_id  %in% c(""path:rno04930"",""path:rno04940"",""path:rno04950"",""path:rno04932"",""path:rno04931"",""path:rno04933"",""path:rno04934"")),]\npath_gene_rat <- path_gene_rat[-which(path_gene_rat$path_id  %in% c(""path:rno04930"",""path:rno04940"",""path:rno04950"",""path:rno04932"",""path:rno04931"",""path:rno04933"",""path:rno04934"")),]\n# Remove the broad category called ""Metabolic pathways""\npath_id_name_rat <- path_id_name_rat[-which(path_id_name_rat$path_id ==""path:rno01100""),]\npath_gene_rat <- path_gene_rat[-which(path_gene_rat$path_id ==""path:rno01100""),]\n\n# Download the table relating the ensembl annotation of genes in mouse, rat and human\nmouse_rat_human <- read.table(""MouseRatHuman_orthologues.txt"",sep=""\\t"",header = T)\n\n# Make PC1-PC2 plots for all species\n# Mouse\nkeep <- rowSums(tpms_gene_mouse_ts>1)>=min(n_rep_mouse_ts)\ntpms_gastro_mouse_k <- tpms_gene_mouse_ts[keep,]\ntpms_gastro_mouse.log <- log2(tpms_gastro_mouse_k+1)\ntpms_gene_s1 <- apply(tpms_gastro_mouse.log,2,scale,scale=F,center=T)\ntpms_gene_s2 <- apply(tpms_gene_s1,1,scale,scale=F,center=T)\nsvd_mat_mouse_ts <- svd(t(tpms_gene_s2))\nev_mouse_ts <- svd_mat_mouse_ts$v\nd_mouse_ts <- svd_mat_mouse_ts$d\n\npc1_mouse_ts <- -ev_mouse_ts[,1] # Increasing profile\npc1_var_mouse_ts <- round(d_mouse_ts[1']",0,"Histone H4, Histone H2A.Z, Acetylation, RNA transcription, African trypanosomes, Genomic loci, Histone modifications, Transcription start sites, Histone acetyltransferases, HAT2,"
Distinct roles for H4 and H2A.Z acetylation in RNA transcription in African trypanosomes,"This repository contains the run scripts for this publication.Complete data downloadhttp://resources-molpara.vetmed.lmu.de/Kraus_et_al_2019_sequencing_analysis.tar.gzContents of the tar.gz archive:upload_sequencing/ bin/  scripts necessary to run the pipeline input/  input files output/  output files for ChIP and RNA-seq output2/  output files for ATAC-seqAbstractDespite histone H2A variants and acetylation of histones occurring in almost every eukaryotic organism, it has been difficult to establish direct functional links between canonical histone or H2A variant acetylation, deposition of H2A variants and transcription. To disentangle these complex interdependent processes, we devised a highly sensitive strategy for quantifying histone acetylation levels at specific genomic loci. Taking advantage of the unusual genome organization in Trypanosoma brucei, we identify 58 histone modifications enriched at transcription start sites (TSSs). Furthermore, we find TSS-associated H4 and H2A.Z acetylation to be mediated by two different histone acetyltransferases, HAT2 and HAT1, respectively. Whereas depletion of HAT2 decreases H2A.Z deposition and shifts the site of transcription initiation, depletion of HAT1 does not affect H2A.Z deposition but reduces total mRNA levels by 50%. Thus, specifically reducing H4 or H2A.Z acetylation levels enabled us to reveal distinct roles for these modifications in H2A.Z deposition and RNA transcription.","['library(DESeq2)\nlibrary(RColorBrewer)\nlibrary(gplots)\nlibrary(GenomicFeatures)\n\nargs <- commandArgs(trailingOnly = TRUE)\nsample_table <- read.table(args[1], stringsAsFactors = F)\ngff_file <- args[2]\nercc_gff_file <- args[3]\noutputFolder <- args[4]\n\nsamples <- data.frame(row.names=basename(as.character(sample_table[1,])), condition=as.character(sample_table[3,]), lib=as.character(sample_table[2,]))\nprint(samples)\n\nbamfiles <- as.character(sample_table[1,])\n\n\ntxdb <- GenomicFeatures::makeTxDbFromGFF(gff_file)\nebg <- transcriptsBy(txdb, by = ""cds"", use.names=T)\nse <- GenomicAlignments::summarizeOverlaps(features=ebg, reads=bamfiles,\n                        mode=""Union"",\n                        singleEnd=FALSE,\n                        ignore.strand=FALSE,\n                        fragments=TRUE )\ncountTable <- assay(se)\n# Adding up downstream and upstram counts of target region for RNAi for proper quantification\ninsertedPosition <- which(rownames(countTable) == ""Tb427v9_001141300.1:DS"")\ncountTable[insertedPosition,] <- countTable[insertedPosition,] + countTable[(insertedPosition+2),]\nrownames(countTable)[insertedPosition] <- ""Tb427v9_001141300.1""\ncountTable <- countTable[-(insertedPosition+2),]\n\ninsertedPosition <- which(rownames(countTable) == ""Tb427v9_000468200.1:US"")\ncountTable[insertedPosition,] <- countTable[insertedPosition,] + countTable[(insertedPosition+2),]\nrownames(countTable)[insertedPosition] <- ""Tb427v9_000468200.1""\ncountTable <- countTable[-(insertedPosition+2),]\n\n\n# Treat ERCC spike ins as separate data set and estimate size factore for it.\nercc_txdb <- GenomicFeatures::makeTxDbFromGFF(ercc_gff_file)\nercc_ebg <- transcriptsBy(ercc_txdb)\nercc_se <- GenomicAlignments::summarizeOverlaps(features=ercc_ebg, reads=bamfiles,\n                                           mode=""Union"",\n                                           singleEnd=FALSE,\n                                           ignore.strand=FALSE,\n                                           fragments=TRUE )\n\ncolData(ercc_se) <- DataFrame(samples)\n\nerccDds <- DESeqDataSet(ercc_se, design=~condition)\nerccDds <- estimateSizeFactors(erccDds)\n\nerccSizeFactors <- sizeFactors(erccDds)\n\nprint(erccSizeFactors)\n\nwrite.table(erccSizeFactors, sep=\'\\t\', sprintf(""%s/ERCC_size_Factors.txt"", outputFolder))\n\n\ndds <- DESeqDataSetFromMatrix(countData=countTable, colData=DataFrame(samples), design=~condition)\ndds <- DESeq(dds)\n\n## Use the ERCC size factor for this data set\nsizeFactors(dds) <- erccSizeFactors\ndds <- estimateDispersions(dds)\ndds <- nbinomWaldTest(dds)\n\n#\n## PCA plot\npdf(sprintf(""%s/sample_comparison_pca_heatmap_cleaned.pdf"", outputFolder))\nrld <- rlog(dds)\nprint(plotPCA(rld, intgroup=c(\'condition\')))\n\n## Heatmap\ndistsRL <- dist(t(assay(rld)))\nmat <- as.matrix(distsRL)\nrownames(mat) <- with(colData(dds), paste(lib, sep=\' : \'))\nhmcol <- colorRampPalette(brewer.pal(9, \'GnBu\'))(100)\nheatmap.2(mat, trace=\'none\', col = rev(hmcol), margin=c(13, 13))\ndev.off()\n\ncomp <- results(dds, contrast=c(\'condition\', ""RNA_HAT2KD2T1_48h"", ""RNA_WT""))\ninputTableAndComp <- cbind(assay(dds), comp)\nwrite.csv(inputTableAndComp, file=sprintf(""%s/deseq_comp_RNA_HAT2KD2T1_48h_vs_RNA_WT_cleaned.csv"", outputFolder),\n            quote=FALSE, row.names=TRUE)\npdf(sprintf(""%s/deseq_comp_RNA_HAT2KD2T1_48h_vs_RNA_WT_cleaned.pdf"", outputFolder))\nplotMA(comp)\ndev.off()\n\ncomp <- results(dds, contrast=c(\'condition\', ""RNA_HAT1KD2T1_48h"", ""RNA_WT""))\ninputTableAndComp <- cbind(assay(dds), comp)\nwrite.csv(inputTableAndComp, file=sprintf(""%s/deseq_comp_RNA_HAT1KD2T1_48h_vs_RNA_WT_cleaned.csv"", outputFolder),\n          quote=FALSE, row.names=TRUE)\npdf(sprintf(""%s/deseq_comp_RNA_HAT1KD2T1_48h_vs_RNA_WT_cleaned.pdf"", outputFolder))\nplotMA(comp)\ndev.off()\n\ncomp <- results(dds, contrast=c(\'condition\', ""RNA_HAT2KD2T1_48h"", ""RNA_HAT1KD2T1_48h""))\ninputTableAndComp <- cbind(assay(dds), comp)\nwrite.csv(inputTableAndComp, file=sprintf(""%s/deseq_comp_RNA_HAT2KD2T1_48h_vs_RNA_HAT1KD2T1_48h_cleaned.csv"", outputFolder),\n           quote=FALSE, row.names=TRUE)\npdf(sprintf(""%s/deseq_comp_RNA_HAT2KD2T1_48h_vs_RNA_HAT1KD2T1_48h_cleaned.pdf"", outputFolder))\nplotMA(comp)\ndev.off()\n\n']",0,"ATAC-Seq, 4sU-Seq, PASsUS, R, log2-fold-change, RPKM, BAM"
Pipeline for ATAC-Seq and 4sU-Seq plotting (PASsUS),"Pipeline for ATAC-Seq and 4sU-Seq plotting (PASsUS)Autor: Tobias Eberhard HaasE-Mail: TobiasEberhardHaas@googlemail.comDatum: 22.04.2021Diese Skripte sind Teil der Dissertation ""Analyse der RNA-Landschaft und Chromatinorganisation in lytischer HSV-1 Infektion und Stress""zur Erlangung der Doktorwrde der Medizinischen Fakultt der Julius-Maximilians-Universitt Wrzburg, vorgelegt von Tobias Eberhard Haas, geboren am 20.11.1992 in Wiesbaden.Um die ATAC-Seq- und 4sU-Seq-Daten zu verschiedenen Zeitpunkten post interventionem(p.i.) miteinander zu vergleichen, wurde im Rahmen der Dissertation ""Analyse der RNA-Landschaft und Chromatinorganisation in lytischer HSV-1 Infektion und Stress"" die Pipeline for ATAC-Seq and 4sU-Seq plotting (PASsUS) in der Programmiersprache R [38] geschrieben.Diese kann den log2-fold-change von ATAC-Seq-Reads bzw. von 4sU-Seq-Reads Infizierter Zellen im Bezug zu den uninfizierten Zellen in verschiedenen Downstreambereichen errechnen und erstellt Grafiken zur Veranschaulichung. Alternativ knnen anstatt der log2-fold-changes auch die absoluten Reads Per Kilobase Milli-on (RPKM) oder die Differenzen der RPKM zwischen den Interventions- und Kontrolldaten berechnet werden.Technisch gesehen knnen mit PASsUS auch BAM-Dateien miteinander verglichen werden, die nicht aus ATAC-Seq oder 4sU-Seq abgeleitet sind.Bedienung und Funktionen von PASsUSUm PASsUS ber R aufrufen zu knnen, mssen folgende flags gesetzt sein:starts und endsMit ""starts"" (vector) werden die Startpunkte der Downstreambereiche angegeben, die untersucht werden sollen und mit ""ends"" (integer) die Lnge ebendieser.inputGTFBei ""inputGTF"" (string) muss der Dateipfad zu einer General Transfer Format(GTF) Datei angeben werden. Um zu bercksichtigen, dass die ATAC-Seq- und 4sU-Seq-Daten auf unterschiedliche Genome Reference Consortium (GRC) Versionen gemapped werden knnen, wird in ""inputGTF_ATAC"" und ""inputGTF_4sU"" unterschieden. In einem ersten Schritt werden die Downstreambereiche anhand der in der GTF-Datei annotierten Genendpunkte berechnet und in eine neue GTF-Datei gespeichert. Dabei werden alle Eintrge entfernt, die nicht dem Feature-Type Gene entsprechen. Unter Bercksichtigung des Strangs wird aus dem Endpunkt eines Genes und der Summe aus ""starts"" und 1 der Anfang des jeweigen Downstreambereiches berechnet. Aus dem Endpunkt eines Genes und der Summe aus ""starts"", 1 und ""ends"" wird das Ende des jeweiligen Downstreambereiches berechnet. Das addieren von 1 soll be-wirken, dass der Downstreambereich nicht mit dem Gen berlappt. Dadurch kann es bei Genen auf dem Minusstrang vorkommen, dass der Downstreambereich miteiner negativen Zahl annotiert ist, was einer spteren Weiterverarbeitung mit featureCounts im Wege steht. Deshalb werden diese Eintrge entfernt.subtract_regions und intersect_regionsIst bei ""subtract_regions"" (string) der Dateipfad zu einer BED-Datei angegeben, so werden die Intervalle aus der Datei mit Hilfe von bedtools subtract [39] unter Verwendung der Standarteinstellungen entfernt. Dies wird hier genutzt, um die Peaks der uninfizierten Replikate in p.i.-Replikaten auszuschlieen, was wiederum z.B. Transcription Factor Binding Sites (TFBS) aus der Auswertung entfernt. Da das Augenmerk auf greren Bereichen mit offenem Chromatin liegt, knnten kleine Bereiche mit vielen Reads das Bild verzerren. Analog dazu ruft ""intersect_regions"" (string) bedtools intersect [39] unter Verwendung der Standardeinstellungen auf. Dies erlaubt z.B. nur Reads zu bercksichtigen, die in p.i. Peaks enthalten sind.pre_normalizeJedes Replikat unterliegt Verzerrungen aufgrund unterschiedlicher Bedingungen whrend der Durchfhrung der Experimente. Deshalb mssen die Replikate transformiert werden, um sie direkt vergleichbar zu machen. Dies geschieht durch Skalierung anhand von Merkmalen, die zwischen den Replikaten die gleiche Ausprgung aufweisen sollten. Da unterschiedliche Downstreambereiche eines Replikats auf dieselbe Art und Weise normalisiert werden knnen und dies damit nur einmalig notwendig ist, kann man mit ""pre_normalize"" (logical) whlen, ob die fr die Normalisation notwendige Datei erstellt werden soll. Die 4sU-Seq Daten werden nach rRNA normalisiert, deren Menge entweder aus Report Dateien importiert wird oder selbst berechnet werden kann. Die ATAC-Seq Daten werden nach mtDNA normalisiert. Hierfr wird die Anzahl der mtDNA-Reads mit Hilfe von SAMtools [40] ausgelesen (samtools view -c -F260 file MT). Bei diesem Schritt braucht das Programm die Angabe einer gtf.genes.tab Datei. Die Tabelle enthlt in der ersten Spalte die ermittelten Reads und in der zweitenSpalte die daraus berechneten Normalisierungsfaktoren, wobei zuerst die Faktoren fr ATAC-Seq nach Zeit sortiert aufgelistet werden und darunter die Faktoren fr 4sU-Seq. Die Zuordnung der einzelnen Zeilen zu den Versuchsbedingungen erfolgt also anhand der Anzahl von als Input gegebenen BAM-Dateien.count_readsAnschlieend wird im ""count_reads"" (logical) Schritt featureCounts [41] genutzt, um zu zhlen, wie viele ATAC-Seq Reads bzw. 4sU-Seq Reads im entsprechenden Downstreambereich vorhanden sind (featureCounts -t gene -s 2 -p -a files). Dabei werden Reads, welche nicht eindeutig einem Bereich zugeordnet werden knnen, nicht gezhlt.no_overlapDa sich der Readthrough teilweise ber 100kbp erstreckt, reicht er auch ber andere Gene hinaus. Ist no_overlap(logical) aktiviert, so werden alle Gene ausgeschlossen, bei denen sich zwischen Beginn des Gens und Ende des zu betrachtenden Downstreambereiches noch ein anderes Gen oder ein anderer Downstreambereich befindet. Da dies in Abhngigkeit zum Betrachtungsbereich steht, fhren groe Werte fr ""starts"" oder ""ends"" zum Ausschluss von vielen Genen. Betrachtet man z.B. den Downstreambereich 200kbp-205kbp, sind bis auf 29 Gene alle ausgeschlossen.exp_RPKM_cut-offsDie Ergebnisse knnen fr jedes Gen in Abhngikeit von dem RPKM Wert 8h p.i.stratifiziert werden. Mit ""exp_RPKM_cut-offs""(vector) werden die cut-off-Werte zwischen den einzelnen Strata angegeben.draw_plotsNachdem alle Vorbereitungen getroffen sind, werden die Daten nun im Schritt ""draw_plots""(logical) geplottet. Zunchst werden die Gene auf alle Gene in einer xlsx-Datei, die mit ""input_xlsx""(string) angegeben wird, reduziert. Dies hat den Hintergrund, dass Gene, welche zu keinem Zeitpunkt exprimiert sind, in der Auswertung nicht hilfreich sind und daher ausgeschlossen werden knnen. Auerdem werden aus dieser Datei weitere Informationen zum Filtern der Gene gewonnen. So kann z.B. nach Read-through und RPKM-Werten gefiltert werden. Dann wird durch Division der vorher erstellten Faktoren, welche in pre_normalize erstellt wurden, normalisiert. Der log2 fold change wird unter Nutzung von LFC[16] berechnet. LFC errechnet eine sinnvolle Anzahl an Pseudocounts. Dies hat den Vorteil, dass weniger Verzerrung durch eine willkrliche Wahl der Anzahl an Pseudocounts entsteht. Der log2 fold change wird fr jeweils aus den Reads eines bestimmten Zeitraums p.i. (Divident) im Verhltnis zu den Reads 0 h p.i. (Divisor) berechnet. Die Datentabellen werden dann fr das Plotten mit ggplot [42] umgeformt und zur Dokumentation in einem Ordner fr temporre Dateien abgelegt.Bentigte Dateien:Zwei Reihen BAM-Dateien, die miteinander verglichen werden sollen.Eine Homo_Sapiens.gene.gtf, die die Positionen von Genen enthlt.Eine bed-File, die Positionen enthlt, die irgnoriert werden sollenEine xlsx-Datei die die Expression von Genen und deren Read-Through enthlt (z.B. https://static-content.springer.com/esm/art%3A10.1038%2Fncomms8126/MediaObjects/41467_2015_BFncomms8126_MOESM769_ESM.xlsx)[6]Beispielaufruf:RTE_master(starts=c(0,5000,10000,30000,50000,70000,90000,200000),ends=5000,inputGTF_ATAC=""path/to/Homo_sapiens.Subset.gene.gtf"",inputGTF_4sU=""path/to/Homo_sapiens.Subset.gene.gtf"",subtract_regions=""path/to/WT-mock.fseq.bed"",intersect_regions=NULL,input_xlsx=""path/to/journal.ppat.s001.xlsx"",make_gtf=TRUE,pre_normalize=TRUE,count_reads=TRUE,draw_plots=TRUE,comparison=""\hspace{0cm}absolut"",preselection=TRUE,no_overlap=TRUE,sum_plots=TRUE,output1=""HSV1"",exp_RPKM_brakes=c(1,3),scale_x_mosaic=c(-9,3),scale_y_mosaic=c(-7,1),scale_x_means=c(0,0.3),scale_y_means=c(0,0.15),atac_RepI_mock=""path/to/WT-mock_Aligned.out.sort.bam"",atac_RepI_1hpi=""path/to/WT-1-hpi_Aligned.out.sort.bam"",atac_RepI_2hpi=""path/to/WT-2-hpi_Aligned.out.sort.bam"",atac_RepI_4hpi=""path/to/WT-4-hpi_Aligned.out.sort.bam"",atac_RepI_6hpi=""path/to/WT-6-hpi_Aligned.out.sort.bam"",atac_RepI_8hpi=""path/to/WT-8-hpi_Aligned.out.sort.bam"",newly_RepI_mock=""path/to/mock_newly_Rep1.bam"",newly_RepI_1hpi=""path/to/1hpi_newly_Rep1.bam"",newly_RepI_2hpi=""path/to/2hpi_newly_Rep1.bam"",newly_RepI_4hpi=""path/to/4hpi_newly_Rep1.bam"",newly_RepI_6hpi=""path/to/6hpi_newly_Rep1.bam"",newly_RepI_8hpi=""path/to/8hpi_newly_Rep1.bam"",newly_RepII_mock=""path/to/mock_newly_Rep2.bam"",newly_RepII_1hpi=""path/to/1hpi_newly_Rep2.bam"",newly_RepII_2hpi=""path/to/2hpi_newly_Rep2.bam"",newly_RepII_4hpi=""path/to/4hpi_newly_Rep2.bam"",newly_RepII_6hpi=""path/to/6hpi_newly_Rep2.bam"",newly_RepII_8hpi=""path/to/8hpi_newly_Rep2.bam"",newly_RepI_mock_report=""path/to/mock_newly_Rep1.reads.tsv"",newly_RepI_1hpi_report=""path/to/1hpi_newly_Rep1.reads.tsv"",newly_RepI_2hpi_report=""path/to/2hpi_newly_Rep1.reads.tsv"",newly_RepI_4hpi_report=""path/to/4hpi_newly_Rep1.reads.tsv"",newly_RepI_6hpi_report=""path/to/6hpi_newly_Rep1.reads.tsv"",newly_RepI_8hpi_report=""path/to/8hpi_newly_Rep1.reads.tsv"",newly_RepII_mock_report=""path/to/mock_newly_Rep2.reads.tsv"",newly_RepII_1hpi_report=""path/to/1hpi_newly_Rep2.reads.tsv"",newly_RepII_2hpi_report=""path/to/2hpi_newly_Rep2.reads.tsv"",newly_RepII_4hpi_report=""path/to/4hpi_newly_Rep2.reads.tsv"",newly_RepII_6hpi_report=""path/to/6hpi_newly_Rep2.reads.tsv"",newly_RepII_8hpi_report=""path/to/8hpi_newly_Rep2.reads.tsv"")[6] A. J. Rutkowski, F. Erhard, A. LHernault, T. Bonfert, M. Schilhabel,C. Crump, P. Rosenstiel, S. Efstathiou, R. Zimmer, C. C. Friedel, and L. Dolken,Widespread disruption of host transcription termination in hsv-1 infection,Nature Communications, vol. 6, p. 7126, 2015.[16] F. Erhard, Estimating pseudocounts and fold changes for digital expressionmeasurements, Bioinformatics, 2018.[38] R Development Core Team,R: A Language and Environment for StatisticalComputing. R Foundation for Statistical Computing, Vienna, Austria, 2008.ISBN 3-900051-07-0.[39] A. R. Quinlan and I. M. Hall, Bedtools: a flexible suite of utilities for comparinggenomic features,Bioinformatics, vol. 26, no. 6, pp. 8412, 2010.[40] H. Li, B. Handsaker, A. Wysoker, T. Fennell, J. Ruan, N. Homer, G. Marth,G. Abecasis, and R. Durbin, The sequence alignment/map format and sam-tools,Bioinformatics, vol. 25, no. 16, pp. 20789, 2009.[41] Y. Liao, G. K. Smyth, and W. Shi, featurecounts: an efficient general purposeprogram for assigning sequence reads to genomic features,Bioinformatics,vol. 30, no. 7, pp. 92330, 2014.[42] H. Wickham,ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag NewYork, 2016.","['# GTF_Setup ändert das 3\' Ende um xbp und filtert nach ""gene"", um hinterher mit bedtools sicher stellen zu können, dass es keine Überschneidungen zwischen dem Downstreambereich und anderen Genen gibt.\n\n# start = Start des Downstreambereiches\n# end = Ende des Downstreambereiches\n# Z.B. Für ein 5kbp Downstreamfester 0,5000\n#inputGTF=Homo_sapiens.GRCh38.86.gtf=$2\n#output1=regions_of_interest.gtf=$4\n\nGTF_Setup <- function(start1=0,end1=5000,inputGTF=""Homo_sapiens.GRCh38.86.gtf"",output1)\n  {\n  \n# Argumente in den richtigen Charaktertyp überführen.\n  inputGTF<-as.character(inputGTF)\n  output1<-as.character(output1)\n  outputGTF = paste(output1,""/"",start1,""-"",end1,""bp-shift.gtf"",sep="""")\n  outputGTF<-as.character(outputGTF)  \n\n# Um mit bedtools festzustellen, ob der Downstream-Bereich mit einem anderen Gen überlappt, darf er nicht mit dem ""eigenen"" Gen überlappen.\n  start1=as.numeric(start1)\n  end1=as.numeric(end1)\n  \n# lesen von xlsx Format ermöglichen.\n  library(readxl)\n  \n# Gene Annotations laden.\n  annotations <- read.delim(inputGTF, header=F, sep=""\\t"", quote ="""", strip.white=TRUE)\n  \n\n# Zeilen die in V7(strang) . oder leer enthalten streichen, nur Zeilen mit + und - behalten.\n  annotations2 <- annotations\n  annotations2 <- annotations2[!(annotations$V7!=""+"" & annotations$V7!=""-""), ]\n  annotations2 <- annotations2[annotations2[,3]==""gene"",]\n\n#Nächster Schritt: Gen Bereich auf Downstream Bereich ändern\n\n#Schleife um V4 und V5 auf den Endpunkt eines Gens festzulegen und um von dem Endpunkt xBPs abzuziehen bzw hinzuzufügen. die start Variable wird erst später genutzt, da mit Bedtools-intersect ein überlappen der Bereiche mit anderen Genen zuerst ausgeschlossen werden muss.\n  for (i in 1:nrow(annotations2)) {\n    if (annotations2[i,7] == ""+""){\n      annotations2[i,4] = annotations2[i,5]\n      annotations2[i,5] = (as.numeric(annotations2[i,5])+end1)\n      annotations2[i,4] = (as.numeric(annotations2[i,4])+1)\n    } else {\n      annotations2[i,5] = annotations2[i,4]\n      annotations2[i,4] = (as.numeric(annotations2[i,4])-end1)\n      annotations2[i,5] = (as.numeric(annotations2[i,5])-1)\n      if(as.numeric(annotations2[i,4]) <= 0 ) {\n        annotations2[i,4] = 1\n        }\n\n\n    } \n  }\n  \n#Es darf kein Genlocus einen negativen Startpunkt haben  \n#annotations2 <- annotations2[annotations2[5,] >= 1,]\nannotations2 <- subset(annotations2, V4 > 0)\nannotations2 <- subset(annotations2, V5 > 0)\n\n \n  write.table(annotations2, outputGTF, quote=F, sep=""\\t"", row.names=F, col.names=F)\n\n}\n\n', '#Normalisiert eine Reihe von bam-Dateien anhand einer vorher erstellten Tabelle.\n\nNormalize<-function(\noutput2,all_Rep)\n{\n\n# Ermitteln der reads\n\nwrite.table(all_Rep, paste(output2,""_files"",sep=""""),quote=F, sep=""\\t"", row.names=F, col.names=F)\n\nsystem(paste(""rm "",output2,""_reads_per_file"",sep=""""))\nsystem(paste(""filename=\'"",output2,""_files\';filelines=`cat $filename`;for line in $filelines ; do samtools view -c $line >> "",output2,""_reads_per_file; done"",sep=""""))\n\nfiles<-read.delim(header=F,paste(output2,""_files"",sep=""""),sep=""\\t"", quote ="""")\nreads<-read.delim(header=F,paste(output2,""_reads_per_file"",sep=""""),sep=""\\t"", quote ="""")\n\n# Normalisieren\n\nx<-nrow(reads)\n\t  for (i in 1:x) {\n\t      reads[i,2] = reads[i,1]/max(reads[,1])\n\t    }\n\nwrite.table(reads, paste(output2,""_reads_per_file"",sep=""""),quote=F, sep=""\\t"", row.names=F, col.names=F)\n\n\n\n}\n', '#Erstellt eine Tabelle mit Normalisierungsfaktoren aus einer Reihe von BAM-Dateien.\n\nPre_normalize<-function(\noutput1,\nall_Rep,\natac_Rep,\nnewly_Rep,\ninput_genes_ATAC.tab,\ninput_genes_4sU.tab,\ninputGTF_ATAC=""Homo_sapiens.GRCh38.86.gtf"",\ninputGTF_4sU=""Homo_sapiens.GRCh38.86.gtf"",\natac_RepI_mock=NULL, atac_RepI_1hpi=NULL, atac_RepI_2hpi=NULL, atac_RepI_3hpi=NULL, atac_RepI_4hpi=NULL, atac_RepI_5hpi=NULL, atac_RepI_6hpi=NULL, atac_RepI_7hpi=NULL, atac_RepI_8hpi=NULL,\natac_RepII_mock=NULL, atac_RepII_1hpi=NULL, atac_RepII_2hpi=NULL, atac_RepII_3hpi=NULL, atac_RepII_4hpi=NULL, atac_RepII_5hpi=NULL, atac_RepII_6hpi=NULL, atac_RepII_7hpi=NULL, atac_RepII_8hpi=NULL,\natac_RepIII_mock=NULL, atac_RepIII_1hpi=NULL, atac_RepIII_2hpi=NULL, atac_RepIII_3hpi=NULL, atac_RepIII_4hpi=NULL, atac_RepIII_5hpi=NULL, atac_RepIII_6hpi=NULL, atac_RepIII_7hpi=NULL, atac_RepIII_8hpi=NULL,\n\nnewly_RepI_mock=NULL, newly_RepI_1hpi=NULL, newly_RepI_2hpi=NULL, newly_RepI_3hpi=NULL, newly_RepI_4hpi=NULL, newly_RepI_5hpi=NULL, newly_RepI_6hpi=NULL, newly_RepI_7hpi=NULL, newly_RepI_8hpi=NULL,\nnewly_RepII_mock=NULL, newly_RepII_1hpi=NULL, newly_RepII_2hpi=NULL, newly_RepII_3hpi=NULL, newly_RepII_4hpi=NULL, newly_RepII_5hpi=NULL, newly_RepII_6hpi=NULL, newly_RepII_7hpi=NULL, newly_RepII_8hpi=NULL,\nnewly_RepIII_mock=NULL, newly_RepIII_1hpi=NULL, newly_RepIII_2hpi=NULL, newly_RepIII_3hpi=NULL, newly_RepIII_4hpi=NULL, newly_RepIII_5hpi=NULL, newly_RepIII_6hpi=NULL, newly_RepIII_7hpi=NULL, newly_RepIII_8hpi=NULL,\n\nnewly_RepI_mock_report=NULL, newly_RepI_1hpi_report=NULL, newly_RepI_2hpi_report=NULL, newly_RepI_3hpi_report=NULL, newly_RepI_4hpi_report=NULL, newly_RepI_5hpi_report=NULL, newly_RepI_6hpi_report=NULL, newly_RepI_7hpi_report=NULL, newly_RepI_8hpi_report=NULL,\nnewly_RepII_mock_report=NULL, newly_RepII_1hpi_report=NULL, newly_RepII_2hpi_report=NULL, newly_RepII_3hpi_report=NULL, newly_RepII_4hpi_report=NULL, newly_RepII_5hpi_report=NULL, newly_RepII_6hpi_report=NULL, newly_RepII_7hpi_report=NULL, newly_RepII_8hpi_report=NULL,\nnewly_RepIII_mock_report=NULL, newly_RepIII_1hpi_report=NULL, newly_RepIII_2hpi_report=NULL, newly_RepIII_3hpi_report=NULL, newly_RepIII_4hpi_report=NULL, newly_RepIII_5hpi_report=NULL, newly_RepIII_6hpi_report=NULL, newly_RepIII_7hpi_report=NULL, newly_RepIII_8hpi_report=NULL,\n...)\n{\n\n# Hilfsvariable für 4sU.\n\nnewly_RepI=c(newly_RepI_mock, newly_RepI_1hpi, newly_RepI_2hpi, newly_RepI_3hpi, newly_RepI_4hpi, newly_RepI_5hpi, newly_RepI_6hpi, newly_RepI_7hpi, newly_RepI_8hpi)\nnewly_RepII=c(newly_RepII_mock, newly_RepII_1hpi, newly_RepII_2hpi, newly_RepII_3hpi, newly_RepII_4hpi, newly_RepII_5hpi, newly_RepII_6hpi, newly_RepII_7hpi, newly_RepII_8hpi)\nnewly_RepIII=c(newly_RepIII_mock, newly_RepIII_1hpi, newly_RepIII_2hpi, newly_RepIII_3hpi, newly_RepIII_4hpi, newly_RepIII_5hpi, newly_RepIII_6hpi, newly_RepIII_7hpi, newly_RepIII_8hpi)\n\n\nnewly_Rep_report=c(\nnewly_RepI_mock_report, newly_RepI_1hpi_report, newly_RepI_2hpi_report, newly_RepI_3hpi_report, newly_RepI_4hpi_report, newly_RepI_5hpi_report, newly_RepI_6hpi_report, newly_RepI_7hpi_report, newly_RepI_8hpi_report,\nnewly_RepII_mock_report, newly_RepII_1hpi_report, newly_RepII_2hpi_report, newly_RepII_3hpi_report, newly_RepII_4hpi_report, newly_RepII_5hpi_report, newly_RepII_6hpi_report, newly_RepII_7hpi_report, newly_RepII_8hpi_report,\nnewly_RepIII_mock_report, newly_RepIII_1hpi_report, newly_RepIII_2hpi_report, newly_RepIII_3hpi_report, newly_RepIII_4hpi_report, newly_RepIII_5hpi_report, newly_RepIII_6hpi_report, newly_RepIII_7hpi_report, newly_RepIII_8hpi_report\n)\n\n\n# Da 4sU nach rRNA und ATAC-Seq nach total RNA normalisiert werden, werden sie getrennt voneinander normalisiert.\n\n### Part 4sU Normalisierung.\n\n      if (is.null(newly_Rep_report)) {\n        print(""newly_Rep_report is NULL, Normalisierung nach rRNA aus genes.tab"")\n\n# Aus der genes.tab Datei müssen die Eintrage für rRNA gefiltert werden, da nach rRNA normalisiert wird.\n  system(paste(""grep rRNA "",input_genes_4sU.tab,""|grep -v Mt_rRNA > "",output1,""/rRNA_locations"",sep=""""))\n\n# Das Ergebnis des letzten Schrittes und die inputGTF werden geladen.\n  locations <- read.delim(header=F,paste(output1,""/rRNA_locations"",sep=""""),sep=""\\t"", quote ="""")\n  annotations <- read.delim(header=F,inputGTF_4sU,sep=""\\t"", quote ="""")\n\n  colnames(locations)[1] <- ""Geneid""\n\n\n# Zeilen die in V7(strang) . oder leer enthalten streichen, nur Zeilen mit + und - behalten.\n  annotations2 <- annotations\n\n  annotations2 <- annotations2[!(annotations$V7!=""+"" & annotations$V7!=""-""), ]\n  annotations2 <- annotations2[annotations2[,3]==""gene"",]\n  annotations2 <- as.data.frame(annotations2)\n\n# Neue Spalte mit gene_id erstellen, um danach mergen zu können.\nannotations2[ , ""Geneid""] <- substr(annotations2[,9], 10, 24)\n\n# Dateien mergen und in ein Format bringen, in dem es von feature Counts als gtf Datei erkannt wird.\nrRNA.gtf<-merge(annotations2,locations, by =""Geneid"", all = FALSE, incomparables = NULL)\nrRNA.gtf<-as.data.frame(rRNA.gtf[2:10])\nrRNA.gtf[,5', '#Read_through_examiner ruft verschiedene Skripte auf, um letzten Endes die Reads zwischen zwei unterschiedlichen Verfahren (hier ATAC-Seq und 4sU-Seq) in einem definierten Downstreambereich zu verschiedenen definierten post-interventionellen Bedingungen (z.B. Zeitpunkte) zu vergleichen.\n\nRead_through_examiner<-function(\nstart1=0,end1=50000,\ninputGTF_ATAC=""Homo_sapiens.GRCh38.86.gtf"",\ninputGTF_4sU=""Homo_sapiens.GRCh38.86.gtf"",\ninput_xlsx=""ncomms8126-s2.xlsx"",\nsubtract_regions=NULL,\nintersect_regions=NULL,\nmake_gtf=TRUE,\npre_normalize=TRUE,\ncount_reads=TRUE,\ndraw_plots=TRUE,\ncomparison=""quotient"",\npreselection=TRUE,\nno_overlap=TRUE,\ncalc_stats=TRUE,\nstarts=NULL,\ncounter=NULL,\noutput1=""HSV1"",\nexp_RPKM_brakes=c(1,3,5,9),\n\natac_RepI_mock=NULL, atac_RepI_1hpi=NULL, atac_RepI_2hpi=NULL, atac_RepI_3hpi=NULL, atac_RepI_4hpi=NULL, atac_RepI_5hpi=NULL, atac_RepI_6hpi=NULL, atac_RepI_7hpi=NULL, atac_RepI_8hpi=NULL,\natac_RepII_mock=NULL, atac_RepII_1hpi=NULL, atac_RepII_2hpi=NULL, atac_RepII_3hpi=NULL, atac_RepII_4hpi=NULL, atac_RepII_5hpi=NULL, atac_RepII_6hpi=NULL, atac_RepII_7hpi=NULL, atac_RepII_8hpi=NULL,\natac_RepIII_mock=NULL, atac_RepIII_1hpi=NULL, atac_RepIII_2hpi=NULL, atac_RepIII_3hpi=NULL, atac_RepIII_4hpi=NULL, atac_RepIII_5hpi=NULL, atac_RepIII_6hpi=NULL, atac_RepIII_7hpi=NULL, atac_RepIII_8hpi=NULL,\n\nnewly_RepI_mock=NULL, newly_RepI_1hpi=NULL, newly_RepI_2hpi=NULL, newly_RepI_3hpi=NULL, newly_RepI_4hpi=NULL, newly_RepI_5hpi=NULL, newly_RepI_6hpi=NULL, newly_RepI_7hpi=NULL, newly_RepI_8hpi=NULL,\nnewly_RepII_mock=NULL, newly_RepII_1hpi=NULL, newly_RepII_2hpi=NULL, newly_RepII_3hpi=NULL, newly_RepII_4hpi=NULL, newly_RepII_5hpi=NULL, newly_RepII_6hpi=NULL, newly_RepII_7hpi=NULL, newly_RepII_8hpi=NULL,\nnewly_RepIII_mock=NULL, newly_RepIII_1hpi=NULL, newly_RepIII_2hpi=NULL, newly_RepIII_3hpi=NULL, newly_RepIII_4hpi=NULL, newly_RepIII_5hpi=NULL, newly_RepIII_6hpi=NULL, newly_RepIII_7hpi=NULL, newly_RepIII_8hpi=NULL,\n\nnewly_RepI_mock_report=NULL, newly_RepI_1hpi_report=NULL, newly_RepI_2hpi_report=NULL, newly_RepI_3hpi_report=NULL, newly_RepI_4hpi_report=NULL, newly_RepI_5hpi_report=NULL, newly_RepI_6hpi_report=NULL, newly_RepI_7hpi_report=NULL, newly_RepI_8hpi_report=NULL,\nnewly_RepII_mock_report=NULL, newly_RepII_1hpi_report=NULL, newly_RepII_2hpi_report=NULL, newly_RepII_3hpi_report=NULL, newly_RepII_4hpi_report=NULL, newly_RepII_5hpi_report=NULL, newly_RepII_6hpi_report=NULL, newly_RepII_7hpi_report=NULL, newly_RepII_8hpi_report=NULL,\nnewly_RepIII_mock_report=NULL, newly_RepIII_1hpi_report=NULL, newly_RepIII_2hpi_report=NULL, newly_RepIII_3hpi_report=NULL, newly_RepIII_4hpi_report=NULL, newly_RepIII_5hpi_report=NULL, newly_RepIII_6hpi_report=NULL, newly_RepIII_7hpi_report=NULL, newly_RepIII_8hpi_report=NULL,\n...)\n{\n\n\n\n# Vorarbeit: Variablen setzen.\n\natac_RepI=c(atac_RepI_mock, atac_RepI_1hpi, atac_RepI_2hpi, atac_RepI_3hpi, atac_RepI_4hpi, atac_RepI_5hpi, atac_RepI_6hpi, atac_RepI_7hpi, atac_RepI_8hpi)\natac_RepII=c(atac_RepII_mock, atac_RepII_1hpi, atac_RepII_2hpi, atac_RepII_3hpi, atac_RepII_4hpi, atac_RepII_5hpi, atac_RepII_6hpi, atac_RepII_7hpi, atac_RepII_8hpi)\natac_RepIII=c(atac_RepIII_mock, atac_RepIII_1hpi, atac_RepIII_2hpi, atac_RepIII_3hpi, atac_RepIII_4hpi, atac_RepIII_5hpi, atac_RepIII_6hpi, atac_RepIII_7hpi, atac_RepIII_8hpi)\n\nnewly_RepI=c(newly_RepI_mock, newly_RepI_1hpi, newly_RepI_2hpi, newly_RepI_3hpi, newly_RepI_4hpi, newly_RepI_5hpi, newly_RepI_6hpi, newly_RepI_7hpi, newly_RepI_8hpi)\nnewly_RepII=c(newly_RepII_mock, newly_RepII_1hpi, newly_RepII_2hpi, newly_RepII_3hpi, newly_RepII_4hpi, newly_RepII_5hpi, newly_RepII_6hpi, newly_RepII_7hpi, newly_RepII_8hpi)\nnewly_RepIII=c(newly_RepIII_mock, newly_RepIII_1hpi, newly_RepIII_2hpi, newly_RepIII_3hpi, newly_RepIII_4hpi, newly_RepIII_5hpi, newly_RepIII_6hpi, newly_RepIII_7hpi, newly_RepIII_8hpi)\n\natac_Rep=c(atac_RepI,atac_RepII,atac_RepIII)\nnewly_Rep=c(newly_RepI,newly_RepII,newly_RepIII)\nall_Rep=c(atac_RepI,atac_RepII,atac_RepIII,newly_RepI,newly_RepII,newly_RepIII)\n\n\n\nstart1=as.numeric(start1)\nend1=as.numeric(end1)\noutput2=(paste(output1,""/"",output1,""_"",start1,""-"",end1,""bp-shift"",sep=""""))\n#outputGTF = paste(output1,""/"",start1,""-"",end1,""bp-shift.gtf"",sep="""")\noutputGTF_ATAC = paste(output1,""/"",start1,""-"",end1,""bp-shift_ATAC.gtf"",sep="""")\noutputGTF_4sU = paste(output1,""/"",start1,""-"",end1,""bp-shift_4sU.gtf"",sep="""")\n\n#für no_overlap\n#outputGTF2 = paste(output1,""/"",start1,""-"",end1,""bp-shift_no-overlap.gtf"",sep="""")\noutputGTF_ATAC2 = paste(output1,""/"",start1,""-"",end1,""bp-shift_no-overlap_ATAC.gtf"",sep="""")\noutputGTF_4sU2 = paste(output1,""/"",start1,""-"",end1,""bp-shift_no-overlap_4sU.gtf"",sep="""")\n\n#für Pre_normalize\ninput_genes_ATAC.tab = paste(inputGTF_ATAC,"".genes.tab"",sep="""")\ninput_genes_4sU.tab = paste(inputGTF_4sU,"".genes.tab"",sep="""")\n\n# Argumente in den richtigen Charaktertyp überführen.\n\n#  inputGTF<-as.character(inputGTF)\n\n  inputGTF_ATAC<-as.character(inputGTF_ATAC)\n  inputGTF_4sU<-as.character(inputGTF_4sU)\n\n#outputGTF2<-as.character(outputGTF2)\n#out', '# Masterdatei\n# RTE_master ruft das Skript Read_through_examiner.R für verschiedene Downstreambereiche auf. Anschließend werden die Ergebnisse in Plots zusammengefasst. \n\nRTE_master<-function(\nstarts=c(0,5000,10000,30000,50000,70000,90000,200000),\nends=5000,\ninputGTF_ATAC=""Homo_sapiens.GRCh38.86.gtf"",\ninputGTF_4sU=""Homo_sapiens.GRCh38.86.gtf"",\ninputGTF_norm_ATAC=""Homo_sapiens.GRCh38.86.gtf"",\ninputGTF_norm_4sU=""Homo_sapiens.GRCh38.86.gtf"",\ninput_xlsx=""ncomms8126-s2.xlsx"",\nsubtract_regions=NULL,\nintersect_regions=NULL,\nmake_gtf=TRUE,\npre_normalize=TRUE,\ncount_reads=TRUE,\ndraw_plots=TRUE,\ncomparison=""quotient"",\npreselection=TRUE,\nno_overlap=TRUE,\ncalc_stats=TRUE,\nsum_plots=TRUE,\noutput1=""HSV1"",\nexp_RPKM_brakes=c(1,3,5,9),\n\n# Achsenabschnitte\nscale_x_mosaic=NULL,\nscale_y_mosaic=NULL,\nscale_x_means=NULL,\nscale_y_means=NULL,\n\n# Rohdaten\natac_RepI_mock=NULL, atac_RepI_1hpi=NULL, atac_RepI_2hpi=NULL, atac_RepI_3hpi=NULL, atac_RepI_4hpi=NULL, atac_RepI_5hpi=NULL, atac_RepI_6hpi=NULL, atac_RepI_7hpi=NULL, atac_RepI_8hpi=NULL,\natac_RepII_mock=NULL, atac_RepII_1hpi=NULL, atac_RepII_2hpi=NULL, atac_RepII_3hpi=NULL, atac_RepII_4hpi=NULL, atac_RepII_5hpi=NULL, atac_RepII_6hpi=NULL, atac_RepII_7hpi=NULL, atac_RepII_8hpi=NULL,\natac_RepIII_mock=NULL, atac_RepIII_1hpi=NULL, atac_RepIII_2hpi=NULL, atac_RepIII_3hpi=NULL, atac_RepIII_4hpi=NULL, atac_RepIII_5hpi=NULL, atac_RepIII_6hpi=NULL, atac_RepIII_7hpi=NULL, atac_RepIII_8hpi=NULL,\n\nnewly_RepI_mock=NULL, newly_RepI_1hpi=NULL, newly_RepI_2hpi=NULL, newly_RepI_3hpi=NULL, newly_RepI_4hpi=NULL, newly_RepI_5hpi=NULL, newly_RepI_6hpi=NULL, newly_RepI_7hpi=NULL, newly_RepI_8hpi=NULL,\nnewly_RepII_mock=NULL, newly_RepII_1hpi=NULL, newly_RepII_2hpi=NULL, newly_RepII_3hpi=NULL, newly_RepII_4hpi=NULL, newly_RepII_5hpi=NULL, newly_RepII_6hpi=NULL, newly_RepII_7hpi=NULL, newly_RepII_8hpi=NULL,\nnewly_RepIII_mock=NULL, newly_RepIII_1hpi=NULL, newly_RepIII_2hpi=NULL, newly_RepIII_3hpi=NULL, newly_RepIII_4hpi=NULL, newly_RepIII_5hpi=NULL, newly_RepIII_6hpi=NULL, newly_RepIII_7hpi=NULL, newly_RepIII_8hpi=NULL,\n\nnewly_RepI_mock_report=NULL, newly_RepI_1hpi_report=NULL, newly_RepI_2hpi_report=NULL, newly_RepI_3hpi_report=NULL, newly_RepI_4hpi_report=NULL, newly_RepI_5hpi_report=NULL, newly_RepI_6hpi_report=NULL, newly_RepI_7hpi_report=NULL, newly_RepI_8hpi_report=NULL,\nnewly_RepII_mock_report=NULL, newly_RepII_1hpi_report=NULL, newly_RepII_2hpi_report=NULL, newly_RepII_3hpi_report=NULL, newly_RepII_4hpi_report=NULL, newly_RepII_5hpi_report=NULL, newly_RepII_6hpi_report=NULL, newly_RepII_7hpi_report=NULL, newly_RepII_8hpi_report=NULL,\nnewly_RepIII_mock_report=NULL, newly_RepIII_1hpi_report=NULL, newly_RepIII_2hpi_report=NULL, newly_RepIII_3hpi_report=NULL, newly_RepIII_4hpi_report=NULL, newly_RepIII_5hpi_report=NULL, newly_RepIII_6hpi_report=NULL, newly_RepIII_7hpi_report=NULL, newly_RepIII_8hpi_report=NULL,\n...)\n{\n\n# Variablen definieren\n\natac_RepI=c(atac_RepI_mock, atac_RepI_1hpi, atac_RepI_2hpi, atac_RepI_3hpi, atac_RepI_4hpi, atac_RepI_5hpi, atac_RepI_6hpi, atac_RepI_7hpi, atac_RepI_8hpi)\natac_RepII=c(atac_RepII_mock, atac_RepII_1hpi, atac_RepII_2hpi, atac_RepII_3hpi, atac_RepII_4hpi, atac_RepII_5hpi, atac_RepII_6hpi, atac_RepII_7hpi, atac_RepII_8hpi)\natac_RepIII=c(atac_RepIII_mock, atac_RepIII_1hpi, atac_RepIII_2hpi, atac_RepIII_3hpi, atac_RepIII_4hpi, atac_RepIII_5hpi, atac_RepIII_6hpi, atac_RepIII_7hpi, atac_RepIII_8hpi)\n\nnewly_RepI=c(newly_RepI_mock, newly_RepI_1hpi, newly_RepI_2hpi, newly_RepI_3hpi, newly_RepI_4hpi, newly_RepI_5hpi, newly_RepI_6hpi, newly_RepI_7hpi, newly_RepI_8hpi)\nnewly_RepII=c(newly_RepII_mock, newly_RepII_1hpi, newly_RepII_2hpi, newly_RepII_3hpi, newly_RepII_4hpi, newly_RepII_5hpi, newly_RepII_6hpi, newly_RepII_7hpi, newly_RepII_8hpi)\nnewly_RepIII=c(newly_RepIII_mock, newly_RepIII_1hpi, newly_RepIII_2hpi, newly_RepIII_3hpi, newly_RepIII_4hpi, newly_RepIII_5hpi, newly_RepIII_6hpi, newly_RepIII_7hpi, newly_RepIII_8hpi)\n\natac_Rep=c(atac_RepI,atac_RepII,atac_RepIII)\nnewly_Rep=c(newly_RepI,newly_RepII,newly_RepIII)\nall_Rep=c(atac_RepI,atac_RepII,atac_RepIII,newly_RepI,newly_RepII,newly_RepIII)\n\n# Achsenbeschriftung\nif(comparison==""quotient""){\n \n  scale_x_title <- ""4sU-seq, log2( hpi / mock)""\n  scale_y_title <- ""ATAC-seq, log2( hpi / mock)""\n  scale_x_title_means <- ""4sU-seq, log2( hpi / mock)""\n  scale_y_title_means <- ""ATAC-seq, log2( hpi / mock)""\n\n}else if (comparison==""absolut""){\n\n  scale_x_title <- ""4sU-seq, log2(RPKM)""\n  scale_y_title <- ""ATAC-seq, log2(RPKM)""\n  scale_x_title_means <- ""4sU-seq (RPKM)""\n  scale_y_title_means <- ""ATAC-seq (RPKM)""\n\n}else if (comparison==""diff""){\n\n  scale_x_title <- ""4sU-seq, log2( hpi_RPKM - mock_RPKM)""\n  scale_y_title <- ""ATAC-seq, log2( hpi_RPKM - mock_RPKM)""\n  scale_x_title_means <- ""4sU-seq, hpi_RPKM - mock_RPKM""\n  scale_y_title_means <- ""ATAC-seq, hpi_RPKM - mock_RPKM""\n} else {\nprint(""comparison method missing"")\n}\n\n# cont.\n\nsystem(paste(""mkdir -p "", output1,sep=""""))\nsystem(paste(""mkdir -p "", output1,""/temp"",sep=""""))\n\n# Damit Zahlen in ', '# Seq_Auswerter erstellt aus mit featureCounts gezählten Reads Tabellen, aus denen es wiederum Plots bildet. In den Plots werden Reads aus unterschiedlichen Experimenten gegenüber gestellt.\n\nSeq_Auswerter<-function(\noutput1,\noutput2,\ninput_xlsx=""journal.ppat.1006954.s001.xlsx"",\ninput_xlsx_RPKMs=""ncomms8126-s2.xlsx"",\ninput_atac=paste(output2,""_atac.seq"",sep=""""),\ninput_4sU=paste(output2,""_4sU.seq"",sep=""""),\ninput_norm=paste(output1,""/"",output1,""_norm"",sep=""""),\ninput_reads=paste(output1,""/"",output1,""_reads_per_file"",sep=""""),\nstart1=0,end1=5000,\npreselection=TRUE,\ndraw_plots=TRUE,\ncalc_stats=TRUE,\nno_overlap=TRUE,\nstarts=NULL,\ncounter=NULL,\ncomparison=""quotient"", #""absolut"" ""diff""\natac_RepI_mock=NULL, atac_RepI_1hpi=NULL, atac_RepI_2hpi=NULL, atac_RepI_3hpi=NULL, atac_RepI_4hpi=NULL, atac_RepI_5hpi=NULL, atac_RepI_6hpi=NULL, atac_RepI_7hpi=NULL, atac_RepI_8hpi=NULL,\natac_RepII_mock=NULL, atac_RepII_1hpi=NULL, atac_RepII_2hpi=NULL, atac_RepII_3hpi=NULL, atac_RepII_4hpi=NULL, atac_RepII_5hpi=NULL, atac_RepII_6hpi=NULL, atac_RepII_7hpi=NULL, atac_RepII_8hpi=NULL,\natac_RepIII_mock=NULL, atac_RepIII_1hpi=NULL, atac_RepIII_2hpi=NULL, atac_RepIII_3hpi=NULL, atac_RepIII_4hpi=NULL, atac_RepIII_5hpi=NULL, atac_RepIII_6hpi=NULL, atac_RepIII_7hpi=NULL, atac_RepIII_8hpi=NULL,\nnewly_RepI_mock=NULL, newly_RepI_1hpi=NULL, newly_RepI_2hpi=NULL, newly_RepI_3hpi=NULL, newly_RepI_4hpi=NULL, newly_RepI_5hpi=NULL, newly_RepI_6hpi=NULL, newly_RepI_7hpi=NULL, newly_RepI_8hpi=NULL,\nnewly_RepII_mock=NULL, newly_RepII_1hpi=NULL, newly_RepII_2hpi=NULL, newly_RepII_3hpi=NULL, newly_RepII_4hpi=NULL, newly_RepII_5hpi=NULL, newly_RepII_6hpi=NULL, newly_RepII_7hpi=NULL, newly_RepII_8hpi=NULL,\nnewly_RepIII_mock=NULL, newly_RepIII_1hpi=NULL, newly_RepIII_2hpi=NULL, newly_RepIII_3hpi=NULL, newly_RepIII_4hpi=NULL, newly_RepIII_5hpi=NULL, newly_RepIII_6hpi=NULL, newly_RepIII_7hpi=NULL, newly_RepIII_8hpi=NULL,\nexp_RPKM_brakes=c(1,3,5,7,9),...)\n{\n\noptions(scipen=999)\n\n#Variablen anlegen\n\natac_RepI=c(atac_RepI_mock, atac_RepI_1hpi, atac_RepI_2hpi, atac_RepI_3hpi, atac_RepI_4hpi, atac_RepI_5hpi, atac_RepI_6hpi, atac_RepI_7hpi, atac_RepI_8hpi)\natac_RepII=c(atac_RepII_mock, atac_RepII_1hpi, atac_RepII_2hpi, atac_RepII_3hpi, atac_RepII_4hpi, atac_RepII_5hpi, atac_RepII_6hpi, atac_RepII_7hpi, atac_RepII_8hpi)\natac_RepIII=c(atac_RepIII_mock, atac_RepIII_1hpi, atac_RepIII_2hpi, atac_RepIII_3hpi, atac_RepIII_4hpi, atac_RepIII_5hpi, atac_RepIII_6hpi, atac_RepIII_7hpi, atac_RepIII_8hpi)\n\nnewly_RepI=c( newly_RepI_mock, newly_RepI_1hpi, newly_RepI_2hpi, newly_RepI_3hpi, newly_RepI_4hpi, newly_RepI_5hpi, newly_RepI_6hpi, newly_RepI_7hpi, newly_RepI_8hpi)\nnewly_RepII=c( newly_RepII_mock, newly_RepII_1hpi, newly_RepII_2hpi, newly_RepII_3hpi, newly_RepII_4hpi, newly_RepII_5hpi, newly_RepII_6hpi, newly_RepII_7hpi, newly_RepII_8hpi)\nnewly_RepIII=c( newly_RepIII_mock, newly_RepIII_1hpi, newly_RepIII_2hpi, newly_RepIII_3hpi, newly_RepIII_4hpi, newly_RepIII_5hpi, newly_RepIII_6hpi, newly_RepIII_7hpi, newly_RepIII_8hpi)\n\natac_Rep=c(atac_RepI,atac_RepII,atac_RepIII)\nnewly_Rep=c(newly_RepI,newly_RepII,newly_RepIII)\n\n# Datentabelle aus dem Paper einlesen\n  library(readxl)\n  ncomms <- read_excel(input_xlsx)\n \n# RPKM-Werte aus anderer Exel-Datei importieren und in die letzte/vorletzte Spalte der Ursprünglichen Exel-Datei schreiben, um später nach RPKM trennen zu können\n  ncomms_RPKMs <- read_excel(input_xlsx_RPKMs)\n  colnames(ncomms)[1] <- ""Geneid""\n  colnames(ncomms_RPKMs)[1] <- ""Geneid""\n  ncomms_m<-merge(ncomms,ncomms_RPKMs, by =""Geneid"", all = FALSE, incomparables = NULL, sort= FALSE)\n  ncomms_m$""%Read-through 7-8h p.i."" <- ncomms_m$""7-8h p.i. 4sU-RNA""\n  \n  ncomms <- ncomms_m[,1:15]\n\n\n# Rest einlesen\n  df_atac <- read.delim(header=T,input_atac,sep=""\\t"", quote ="""",skip = 1)\n  df_4sU <- read.delim(header=T,input_4sU,sep=""\\t"", quote ="""",skip = 1)\n  df_normalizer <- read.delim(header=F,input_norm,sep=""\\t"", quote ="""")\n\n\n# Normalisation anwenden\n\n  if (comparison==""quotient""){\n\n  for (i in 1:length(atac_Rep)) {\n    df_atac[i+6] <- df_atac[i+6] / df_normalizer[i,2]\n  }\n  \n  for (i in 1:length(newly_Rep)) {\n    df_4sU[i+6] <- df_4sU[i+6] / df_normalizer[i+length(atac_Rep),2]\n  }\n  }\n\n# Reads Rep-abhängig zusammenzählen und durch Anzahl der Reps teilen.\n\n  if (is.null(atac_RepIII)!=TRUE) {\n\n      for (i in 1:length(atac_RepIII)) {\n      df_atac[i+6] <- (df_atac[i+6]+df_atac[i+6+length(atac_RepIII)]+df_atac[i+6+2*length(atac_RepIII)])/3\n      }\n\n  } else if (is.null(atac_RepII)!=TRUE) {\n\n      for (i in 1:length(atac_RepII)) {\n      df_atac[i+6] <- (df_atac[i+6]+df_atac[i+6+length(atac_RepII)])/2\n      }\n\n  } else {\n\n      for (i in 1:length(atac_RepI)) {\n      df_atac[i+6] <- df_atac[i+6]\n      }\n  }\n\n  if (is.null(newly_RepIII)!=TRUE) {\n\n      for (i in 1:length(newly_RepIII)) {\n      df_4sU[i+6] <- (df_4sU[i+6]+df_4sU[i+6+length(newly_RepIII)]+df_4sU[i+6+2*length(newly_RepIII)])/3\n      }\n\n  } else if (is.null(newly_RepII)!=TRUE) {\n\n      for (i in 1:length(newly_RepII)) {\n      df_4']",0,"epileptogenesis, proteome alterations, R-code, systems level analysis, biomarkers, target candidates, molecular pathophysiology, hippocampus, parahippocampal cortex, Weighted Gene Co-expression Network analysis, molecular processes,"
R-code for: A systems level analysis of epileptogenesis-associated proteome alterations,"Despite intense research efforts, the knowledge about the mechanisms of epileptogenesis and epilepsy is still considered incomplete and limited. However, an in-depth understanding of molecular pathophysiological processes is crucial for the rational selection of innovative biomarkers and target candidates.Here, we subjected proteomic data from different phases of a chronic rat epileptogenesis model to a comprehensive systems level analysis. Weighted Gene Co-expression Network analysis identified several modules of interconnected protein groups reflecting distinct molecular aspects of epileptogenesis in the hippocampus and the parahippocampal cortex. Characterization of these modules did not only further validate the data but also revealed regulation of molecular processes not described previously in the context of epilepsy development. The data sets also provide valuable information about temporal patterns, which should be taken into account for development of preventive strategies in particular when it comes to multi-targeting network pharmacology approaches.In addition, principal component analysis suggests candidate biomarkers, which might inform the design of novel molecular imaging approaches aiming to predict epileptogenesis during different phases or confirm epilepsy manifestation. Further studies are necessary to distinguish between molecular alterations, which correlate with epileptogenesis versus those reflecting a mere consequence of the status epilepticus.","['########################################################################################\n## WORKFLOW TO ANALYSE BITSIKA DATA\n##\n## These script contains a workflow used to reproduce results generated in paper\n## ""A systems level analysis of epileptogenesis-associated proteome alterations"" by\n## M. Keck and G. Androsova et al. (2016).\n## \n## Script author: Ganna Androsova, ganna.androsova@uni.lu\n## \n########################################################################################\nsource(""Data_analysis_and_network_construction.R"")\n\n#General settings\noptions(stringsAsFactors = FALSE)\n\n#Indicate the directory with original data\noriginal_directory = ""/Original_data""\n\n#Indicate the directory for input files\ninput_directory = ""/Preprocessed_data""\n\n#Indicate the output directory\noutput_directory = ""/Generated_results""\n\n#Indicate time points of the data\ntime_label = c(""1 day"", ""3 days"", ""30 days"")\n\n#Indicate the names of sheets to read from the original Bitsika\'s excel files\nsheets_to_read = c(""1dpi-all proteins"", ""3dpi-all proteins"", ""30dpi-all proteins"")\n\n################################### Data preprocessing ################################\n#Read-in original data\nBitsika_original_data = lapply(sheets_to_read, function(sheet_name){\n  input_file = read.xlsx(file.path(original_directory, ""pr6b00003_si_005.xlsx""), \n                         sheet = sheet_name, colNames = TRUE, startRow = 2)\n  #Transform excel output into data frame and keep only expression values\n  df = data.frame(input_file[which(unique(input_file[,2]) == input_file[,2]),c(12:16,7:11)])\n  rownames(df) = input_file[which(unique(input_file[,2]) == input_file[,2]),2]\n  return(df)\n})\n\n#Merge the time points\nBitsika_merged_data = Bitsika_original_data[[1]]\nfor(i in c(2:length(Bitsika_original_data))){\n  merge1 = merge(Bitsika_merged_data, Bitsika_original_data[[i]], \n                 by=""row.names"", all = FALSE)\n  Bitsika_merged_data = merge1[,2:ncol(merge1)]\n  rownames(Bitsika_merged_data) = merge1[,1]\n}\ncolnames(Bitsika_merged_data) = c(rep(""NaCl_1d"", 5), rep(""KA_1d"",5), \n                                  rep(""NaCl_3d"", 5), rep(""KA_3d"",5), \n                                  rep(""NaCl_30d"", 5), rep(""KA_30d"",5))\n\npng(file = paste(output_directory, ""/Bitsika_expression_before_norm.png"", sep = """"), \n    height = 5, width = 7, units=""in"", res=300)\nboxplot(Bitsika_merged_data, xlab=""Samples"", ylab=""Protein abundances"", \n        main=""Protein expression values before normalization"")\ndev.off()\n\n#PCA for each seperate timepoint with all detected proteins\nplot_separate_PCA(Bitsika_original_data, output_directory, ""Bitsika"", time_label)\n\n#Normalize the data\nBitsika_preprocessed = normalization(Bitsika_merged_data, input_directory, \n                                   output_directory, ""Bitsika"")\n\n#Get protein expression values\nBitsika_post_stat = DE_prots(Bitsika_preprocessed, output_directory, time_label, ""Bitsika"")\n\n#Gen names of differentially expressed proteins\nBitsika_list_of_DE_prots = lapply(Bitsika_post_stat, function(x){\n  prot=subset(x, q_values<.05 & (x$FC<=0.67|x$FC>=1.5))\n  return(rownames(prot))\n})\n\n#Calculate the overlap of DE proteins between Bitsika data and HC/PHC\nDE_overlap = matrix(0, ncol = 6, nrow = 3, dimnames = list(\n  c(paste(""Bitsika"", time_label)), \n  c(""HC 2 days"", ""HC 10 days"", ""HC 8 weeks"", ""PHC 2 days"", ""PHC 10 days"", ""PHC 8 weeks"")))\nfor(row in c(1:3)){\n  for(col in c(1:3)){\n    DE_overlap[row,col] = length(Reduce(intersect, list(Bitsika_list_of_DE_prots[[row]], HC_list_of_DE_prots[[col]])))\n    DE_overlap[row,col+3] = length(Reduce(intersect, list(Bitsika_list_of_DE_prots[[row]], PHC_list_of_DE_prots[[col]])))\n  }\n}\n\n#Plot the overlap of DE proteins between Bitsika data and HC/PHC\npng(file = paste(output_directory, ""/"", ""DE_protein_overlap.png"", sep = """"), height = 3.5, width = 7, units=""in"", res=300)\nlabeledHeatmap(Matrix = DE_overlap, \n               xLabels = colnames(DE_overlap),\n               yLabels = rownames(DE_overlap), \n               colorLabels = F, colors = blueWhiteRed(100)[50:100],\n               cex.text = 0.8, textMatrix = DE_overlap,\n               cex.lab = 0.8, xLabelsAngle=25)\ndev.off()\n\n#Plot the differentially expressed proteins\nbarplot_DE(Bitsika_post_stat, output_directory, ""Bitsika"")\n\n#Plot the correlation matrix\nBitsika_correlation_matrix = cor(t(Bitsika_preprocessed), \n                                 method=""spearman"", use=""complete.obs"")\nvisualize_correlation(Bitsika_correlation_matrix, output_directory, ""Bitsika"")\n\n#Create adjacency matrix\nBitsika_adjacency_matrix = estimate_correlation(Bitsika_preprocessed)\n\n#Calculate network topology\nBitsika_topological_table = estimate_network_topology(Bitsika_adjacency_matrix, \n                                                      output_directory, ""Bitsika"")\n\n#Module detection\nBitsika_modules = detect_modules(Bitsika_adjacency_matrix, output_directory, ""Bitsika"")\n\n#Export the network to Cytoscape\nexportNetworkToCytoscape(Bitsika_adjacency_matrix, weighted', '############################## Config Settings ##############################\n\n#General settings\noptions(stringsAsFactors = FALSE)\n\n#Indicate the directory with original data\noriginal_directory = ""/Original_data""\n\n#Indicate the directory for input files\ninput_directory = ""/Preprocessed_data""\n\n#Indicate the output directory\noutput_directory = ""/Generated_results""\n\n#List excel files with original data\nHC_excel_files = list(""HC_combinedfiles_network_2days.xlsx"", \n                      ""HC_combinedfiles_network_10days.xlsx"", \n                      ""HC_combinedfiles_network_8weeks.xlsx"")\nPHC_excel_files = list(""PHC_combinedfiles_network_2days.xlsx"", \n                       ""PHC_combinedfiles_network_10days.xlsx"", \n                       ""PHC_combinedfiles_network_8weeks.xlsx"")\n\ntime_label = c(""2 days"", ""10 days"", ""8 weeks"")\n', '########################################################################################\n## FUNCTIONS FOR DATA ANALYSIS AND NETWORK CONSTRUCTION\n##\n## These script contains the functions used to reproduce results generated in paper\n## ""A systems level analysis of epileptogenesis-associated proteome alterations"" by\n## M. Keck and G. Androsova et al. (2016).\n## \n## Script author: Ganna Androsova, ganna.androsova@uni.lu\n## \n########################################################################################\n\n# ipak function was developed by Steven Worthington and deposited at \n# https://gist.github.com/stevenworthington/3178163\nipak = function(pkg){\n  new.pkg = pkg[!(pkg %in% installed.packages()[, ""Package""])]\n  if (length(new.pkg))\n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\nipak(c(""openxlsx"", ""FactoMineR"", ""qvalue"", ""WGCNA"", ""flashClust"", ""made4"", ""limma""))\n\nif (!require(""factoextra"")) {\n  install_github(""kassambara/factoextra"")\n  library(""factoextra"")\n}\n\nmerge_original_tables = function(original_directory, excel_files, brain_region){\n  \n  original_data = lapply(excel_files, function(file){\n    input_file = read.xlsx(file.path(original_directory, file), \n                           sheet = 1, colNames = TRUE, startRow = 3)\n    \n    #Transform excel output into data frame and keep only expression values\n    df = data.frame(input_file[which(input_file[,7]!=""N/A""),18:27])\n    rownames(df) = input_file[which(input_file[,7]!=""N/A""),7]\n    colnames(df) = c(rep(""Ctrl"",5), rep(""SE"", 5))\n    \n    return(df)\n  })\n  \n  merged_full_data = original_data[[1]]\n  for(i in c(2:length(original_data))){\n    merge2 = merge(merged_full_data, original_data[[i]], by=""row.names"", all = TRUE)\n    merged_full_data = merge2[,2:ncol(merge2)]\n    rownames(merged_full_data) = merge2[,1]\n  }\n  colnames(merged_full_data) <- c(rep(""Ctrl_2d"",5), rep(""SE_2d"", 5), \n                                  rep(""Ctrl_10d"",5), rep(""SE_10d"", 5), \n                                  rep(""Ctrl_8w"",5), rep(""SE_8w"", 5))\n  \n  png(file = paste(output_directory, ""/"", brain_region, ""_expression_before_norm.png"", sep = """"), \n      height = 5, width = 7, units=""in"", res=300)\n  boxplot(merged_full_data, xlab=""Samples"", ylab=""Protein abundances"", \n          main=""Protein expression values before normalization"")\n  dev.off()\n  \n  return(list(merged_full_data, original_data))\n}\n\n\nplot_PCA = function(filtered_measurements, output_directory, brain_region, type){\n  #The proteins with highest contribution to the variability of data tend to be on horizontal axis (of principle component 1) thus it is difficult to have a clear separation\n  res.pca = PCA(filtered_measurements, quali.sup=ncol(filtered_measurements), graph=FALSE)\n  \n  png(file = paste(output_directory, ""/"", brain_region, ""_"", type, ""_PCA.png"", sep = """"), height = 5, width = 5, units=""in"", res=300)\n  print(fviz_pca_ind(res.pca, pointsize=4, invisible=""quali"", label=""none"", habillage=ncol(filtered_measurements)) + labs(title = brain_region) + scale_color_brewer(palette=""Set1"") + theme_minimal())\n  dev.off()\n  print(fviz_pca_ind(res.pca, pointsize=4, invisible=""quali"", label=""none"", habillage=ncol(filtered_measurements)) + labs(title = brain_region) + scale_color_brewer(palette=""Set1"") + theme_minimal())\n  \n  top_10_contributors = sapply(1:ncol(res.pca$var$contrib), function(x){names(sort(res.pca$var$contrib[,x], decreasing = T)[1:10])})\n  colnames(top_10_contributors) = colnames(res.pca$var$contrib)\n  write.table(top_10_contributors, file=paste(output_directory, ""/"", brain_region, ""_"", type, ""_top10_PCA_proteins.txt"", sep = """"), sep=""\\t"", row.names=F, quote=FALSE)\n}\n\n\nplot_separate_PCA = function(separate_data, output_directory, brain_region, time_label){\n  sapply(c(1:length(separate_data)), function(i){\n    time_point = t(asinh(separate_data[[i]]))\n    rownames(time_point) = NULL\n    time_point = as.data.frame(time_point)\n    filtered_measurements = time_point[, colSums(is.na(time_point)) != nrow(time_point)]\n    filtered_measurements$group = c(rep(""Ctrl"", 5), rep(""SE"", 5))\n    plot_PCA(filtered_measurements, output_directory, brain_region, time_label[i])\n  })\n}\n\n\nnormalization = function(merged_data, input_directory, output_directory, brain_region, data.type){\n  \n  #Arcsin transformation and median centering\n  asin_df = as.data.frame(asinh(merged_data))\n  preprocessed_data = t(apply(asin_df,2,function(x){x-median(x[!is.na(x)])}))\n  \n  png(file = paste(output_directory, ""/"", brain_region, ""_expression_after_norm.png"", sep = """"), height = 6, width = 8, units=""in"", res=300)\n  boxplot(t(preprocessed_data), xlab=""Samples"", ylab=""Protein abundances"", main=""Protein expression values after normalization"")\n  dev.off()\n  outliers = NULL\n  for (i in 1:nrow(preprocessed_data)){\n    outliers = c(outliers, names(which(preprocessed_data[i,]<(-10))))\n  }\n  outliers = unique(outliers)\n  \n  write.csv(preprocessed_data, file = file.path(input_directory, paste0(brain_region,', '########################################################################################\n## WORKFLOW TO REPRODUCE MANUSCRIPT RESULTS\n##\n## These script contains a workflow used to reproduce results generated in paper\n## ""A systems level analysis of epileptogenesis-associated proteome alterations"" by\n## M. Keck and G. Androsova et al. (2016).\n## \n## Script author: Ganna Androsova, ganna.androsova@uni.lu\n## \n########################################################################################\n\n### Call scripts with functions and configurations\nsource(""Data_analysis_and_network_construction.R"")\nsource(""Configurations.R"")\n\n################################### Data preprocessing #################################\n#Read-in original data\nHC_data = merge_original_tables(original_directory, HC_excel_files, ""HC"")\nPHC_data = merge_original_tables(original_directory, PHC_excel_files, ""PHC"")\n\n#PCA for each seperate timepoint with all detected proteins\nplot_separate_PCA(HC_data[[2]], output_directory, ""HC"", time_label)\nplot_separate_PCA(PHC_data[[2]], output_directory, ""PHC"", time_label)\n\n#Arcsinh transformation of median-centering\nHC_preprocessed_all = normalization(HC_data[[1]], input_directory, \n                                    output_directory, ""HC"")\nPHC_preprocessed_all = normalization(PHC_data[[1]], input_directory, \n                                     output_directory, ""PHC"")\n\n#Identify differentially expressed proteins\nHC_DEGs = DE_prots(HC_preprocessed_all, output_directory, time_label, ""HC"")\nPHC_DEGs = DE_prots(PHC_preprocessed_all, output_directory, time_label, ""PHC"")\n\nHC_list_of_DE_prots = lapply(HC_DEGs, function(x){\n  prot=subset(x, p_values<.05 & (x$FC<=0.67|x$FC>=1.5))\n  return(rownames(prot))\n})\nPHC_list_of_DE_prots = lapply(PHC_DEGs, function(x){\n  prot=subset(x, p_values<.05 & (x$FC<=0.67|x$FC>=1.5))\n  return(rownames(prot))\n})\n\nhist_FC(HC_DEGs, output_directory, ""HC"")\nhist_FC(PHC_DEGs, output_directory, ""PHC"")\n\nvolcano_DE(HC_DEGs, output_directory, ""HC"")\nvolcano_DE(PHC_DEGs, output_directory, ""PHC"")\n\nbarplot_DE(HC_DEGs, output_directory, ""HC"")\nbarplot_DE(PHC_DEGs, output_directory, ""PHC"")\n\n#Get the merged datasets with proteins present at all timepoints\nHC_merged_data = get_common_prots(HC_data[[2]], input_directory, output_directory, ""HC_common"")\nPHC_merged_data = get_common_prots(PHC_data[[2]], input_directory, output_directory, ""PHC_common"")\n\n#Identify differential expression among common proteins\nDE_prots(HC_merged_data, output_directory, time_label, ""HC_common"")\nDE_prots(PHC_merged_data, output_directory, time_label, ""PHC_common"")\n\nHC_correlation_matrix = cor(t(HC_merged_data), method=""spearman"", use=""complete.obs"")\nPHC_correlation_matrix = cor(t(HC_merged_data), method=""spearman"", use=""complete.obs"")\n\nvisualize_correlation(HC_correlation_matrix, output_directory, ""HC"")\nvisualize_correlation(PHC_correlation_matrix, output_directory, ""PHC"")\n\n############################## Network construction ####################################\n\n#Create adjacency matrix\nHC_adjacency_matrix = estimate_correlation(HC_merged_data)\nPHC_adjacency_matrix = estimate_correlation(PHC_merged_data)\n\n#Calculate network topology\nHC_topological_table = estimate_network_topology(HC_adjacency_matrix, output_directory, ""HC"")\nPHC_topological_table = estimate_network_topology(PHC_adjacency_matrix, output_directory, ""PHC"")\ntopology = rbind(HC_topological_table, PHC_topological_table)\nrownames(topology) = c(""Hippocampus"", ""Parahippocampus"")\n\n#Module detection\nHC_modules = detect_modules(HC_adjacency_matrix, output_directory, ""HC"")\nPHC_modules = detect_modules(PHC_adjacency_matrix, output_directory, ""PHC"")\n\n#Export network to Cytoscape\nexportNetworkToCytoscape(HC_adjacency_matrix, weighted = TRUE, threshold = 0.26, \n                         nodeNames = colnames(HC_adjacency_matrix), nodeAttr = HC_modules,\n                         edgeFile = file.path(output_directory, ""HC_network_edges.txt""),\n                         nodeFile = file.path(output_directory, ""HC_node_color_attribute.txt""))\nexportNetworkToCytoscape(PHC_adjacency_matrix, weighted = TRUE, threshold = 0.26, \n                         nodeNames = colnames(PHC_adjacency_matrix), nodeAttr = PHC_modules,\n                         edgeFile = file.path(output_directory, ""PHC_network_edges.txt""),\n                         nodeFile = file.path(output_directory, ""PHC_node_color_attribute.txt""))\n\n#Plot module eigengenes\nplot_eigengenes(HC_merged_data, HC_modules, output_directory, ""HC"")\nplot_eigengenes(PHC_merged_data, PHC_modules, output_directory, ""PHC"")\n\n#Module overlap\nplot_overlap_between_modules(HC_merged_data, HC_modules, PHC_merged_data, PHC_modules, output_directory)\n\n############################## Module-trait relationship ##############################\n#Create the trait matrix\ntrait = matrix(rep(c(rep(0,5), rep(1,5)),3))\nrownames(trait) = rownames(HC_merged_data)\nsource(""Data_analysis_and_network_construction.R"")\n#Get protein significance\nHC_protein_significance = calcula']",0,"Human Th17 cells, gasdermin E, pore-forming molecule, pyroptotic cell death, cancer checkpoint, durable viability, alarmin, IL-1a, subset, C. albicans, T cell-intrinsic N"
Human Th17 cells engage gasdermin E pores to release IL-1a upon NLRP3 inflammasome activation,"There is evidence that innate immune responses coopt adaptive properties such as memory. Whether T cells harness innate immune signaling pathways to diversify their repertoire of effector functions remains unknown. Here, we found that human T cells expressed gasdermin E (GSDME), a membrane pore-forming molecule that has recently been shown to execute pyroptotic cell death and thus to serve as a potential cancer checkpoint. In T cells, GSDME expression was, in contrast, associated with durable viability and was repurposed for the tunneled release of the alarmin IL-1a. This property was restricted to a subset of human Th17 cells with specificity for C. albicans and was regulated by a T cell-intrinsic NLRP3 inflammasome and its engagement of a proteolytic cascade of successive caspase-8, caspase-3 and GSDME cleavage following T-cell receptor stimulation and calcium-licensed calpain maturation of the pro-IL1a form. Our results propose GSDME pore formation in T cells as a mechanism of unconventional cytokine release through harnessing of innate signaling platforms in response to adaptive stimuli. This finding diversifies the functional repertoire and mechanistic equipment of T cells with implications for anti-fungal host defense.","['rm(list=ls());\ngraphics.off();\nif (1){\nargs <- commandArgs ( TRUE );\nif ( length ( args ) < 2 ){\n  stop ( ""Error. Check usage!"" );\n}\n    top <- args;\n} else {\n#    top <- c(""top50_david.txt"", ""top100_david.txt"", ""top500_david.txt"", ""top1000_david.txt"");\n    top <- list.files(path=""./gene_sets/"", pattern=""_DAVID.txt"");\n\ttop <- paste(""./gene_sets/"", top, sep="""");\n}\n\n\nfor (file in top){\n    if( ! file.exists(file) ){\n        stop( paste(file, ""does not exist!"" ) );\n    }\n}\n\n\n#library(""DESeq2"");\nlibrary(""tidyr"");\nlibrary(""dplyr"");\nlibrary(""stringr"");\n#library(""gplots"")\n#library(""RColorBrewer"");\nlibrary(""ggplot2"");\nlibrary(""pheatmap"");\n#library(""ggsci"");\n#library(""ggrepel"");\n#library(""ggpubr"");\n#library(""rospca"");\n#library(""data.table"");\n#library(""tidyverse"");\n\noutput <- paste(basename(top[1]), ""DAVIDHeatmap.r"", sep=""_"");\n\n###################################################\n### READ DATA\n###################################################\n\ndf_top <- tibble();\nfor (file in top){\n    if( ! file.exists(file) ){\n        stop( paste(file, ""does not exist!"" ) );\n    }\n    aux <- read.table(file, header = T, sep=""\\t"", comment.char = \'#\', quote="""");\n\n\taux <- aux %>% mutate(geneset=basename(file));\n    aux <- aux %>% mutate(geneset=gsub(""FCSelect.r_Foreground"", """", geneset)) %>% mutate(geneset=gsub("".txt_DAVID.txt"", """", geneset));\n\n    df_top <- bind_rows(df_top, aux);\n}\n\ndf <- df_top %>% \nselect(Category, Term, Count, Pop_Hits, FDR, geneset) %>%\nunite(""id"", Category:Term, sep=""_""); \n\n### FILTER OUT ""UNINTERESTING TERMS""\ndf_filtered <- df %>% filter(!grepl(""GOTERM_CC|UP_TISSUE|_ALL_GO|_DIRECT_GO|GOTERM_MF_"", id));\ndf_filtered <- df_filtered %>% mutate(id=gsub(""GOTERM_.._FAT_|KEGG_PATHWAY_"", """", id));\nprint(nrow(df_filtered));\n\ndf_filtered <- df_filtered %>% filter(Count>=5);\ndf_filtered <- df_filtered %>% select(-Count, -Pop_Hits); \nprint(head(df_filtered));\n\n\n### ALWAYS SIGNIFICANT\nfdr_threshold <- 5;\nNAnalyses_threshold <- floor(length(levels(factor(df_filtered$geneset))) * 0.5);\nNAnalyses_threshold <- 3;\ndf_sig <- df_filtered %>% filter(FDR <= fdr_threshold) %>% group_by(id) %>% summarise(NAnalyses=n_distinct(geneset)) %>% filter(NAnalyses>=NAnalyses_threshold);\nprint(dim(df_sig));\n\n\n### INTERESTING TERMS\ndeath <- df_filtered %>% filter(grepl(\'death|apoptosis|pyroptosis|necroptosis|necrosis|senescence|ferroptosis|parthanatos|entotic\',id));\ndeath_sig <- death %>% filter(FDR <= fdr_threshold) %>% group_by(id) %>% summarise(NAnalyses=n_distinct(geneset)) %>% filter(NAnalyses>=NAnalyses_threshold);\nprint(dim(death_sig));\n\nterms_to_include <- unique(c(as.character(df_sig$id), as.character(death$id)));\n\n### FILTER TERMS FOR HEATMAP\ndf_heatmap <- df_filtered %>% filter(id %in% terms_to_include);\n\n### COMPUTE -LOG10FDR; FDR IS A PERCENTAGE!\ndf_heatmap <- df_heatmap %>% mutate(mlogFDR=-log10(FDR/100)) %>% select(-FDR);\n\n### CAP THE -LOG10FDR VALUES\ncap <- -log10(1e-10)\ndf_heatmap <- df_heatmap %>% mutate(mlogFDR=ifelse(mlogFDR>cap, cap, mlogFDR));\n\n###################################################\n### GET WIDE FORMAT\n###################################################\n\ndf_heatmap_wide <- df_heatmap %>% pivot_wider(names_from = geneset, values_from = mlogFDR, values_fill=0)# %>% mutate(id=gsub(""GOTERM_BP_FAT_"", """", id));\nprint(dim(df_heatmap_wide));\n\n\nz <- data.frame(df_heatmap_wide, check.names=FALSE);\nrownames(z) <- as.character(df_heatmap_wide$id);\nz$id <- NULL;\nz <- data.matrix(z);\n\n\n\n###################################################\n### HEATMAP\n###################################################\nmy_max <- min(ceiling(max(z)), 10);\nmy_breaks <- seq(2, my_max, by=1);\nmy_breaks <- c(0, -log10(0.05), my_breaks);\nmy_colors <- colorRampPalette(c(""white"", ""red""))(length(my_breaks));\n\n#### ANNOTATION\nannotation_rows <- data.frame(id=rownames(z));\nannotation_rows <- annotation_rows %>% mutate(death=ifelse(id %in% death$id, \'yes\', \'no\'));\nrownames(annotation_rows) <- annotation_rows$id;\nannotation_rows <- annotation_rows %>% select(-id);\n\naux <- c(\'black\', \'skyblue\');\nnames(aux) <- c(\'yes\', \'no\');\nannotation_rows_col <- list(death=aux);\n\norder_cols <- as.numeric(str_extract(colnames(z), ""\\\\d+""));\norder_cols <- order(order_cols);\nz <- z[, order_cols];\n\npdf(paste(output, "".pdf"", sep=""""), width=9, height=6);\npHeatmap <- pheatmap(z, \n cluster_cols=FALSE,\n angle_col=90, \n fontsize_col=7,\n fontsize_row=7,\n cellwidth=7,\n border_color=NA,\n col=my_colors,\n breaks=my_breaks,\n annotation_row=annotation_rows,\n annotation_colors=annotation_rows_col,\n #cellheight=5,\n #cluster_cols=FALSE\n)\ndev.off()\nsave(pHeatmap, z, file=paste(output, "".RData"", sep=""""));\n', 'rm(list=ls());\nif (1){\nargs <- commandArgs ( TRUE );\nif ( length ( args ) != 2 ){\n\tstop ( ""Error. Check usage!"" );\n}\nfile <- args[1];\noutput_dir <- args[2];\n} else {\nfile <- ""samples_simple.txt_counts.txt_AggregatedByPatient.txt_biomart.txt_protein_coding.txt_refined.txt_RunDESeq2.r_treatment_KOvsNTC.txt"";\noutput_dir <- ""output_dir"";\n}\n\nif ( !file.exists( file ) ){\n    stop ( paste ( file, ""does not exist!"" ) );\n}\nif (!file.exists(output_dir)){\n    stop (paste(output_dir, ""does not exist!""));\n}\n\n\nlibrary(""dplyr"");\nlibrary(""tidyr"");\n\nx <- read.table(file, sep=""\\t"", header=TRUE);\nprint(length(which(!is.na(x$padj))));\n\n### SELECT GENES\ngene_list_size <- c(50, 100, 500, 1000, 2000, 2500);\n\noutput <- ""FCSelect.r_"";\n\nfor (i in gene_list_size){\n    z <- x %>% arrange(desc(abs(log2FoldChange)));\n    z <- z %>% head(n=i);\n    write(z$external_gene_name, file=paste(output_dir, ""/"", output, ""ForegroundN"", i, "".txt"", sep=""""));\n}\n\nwrite(x$external_gene_name, file=paste(output_dir, ""/"", output, ""Background.txt"", sep=""""));\n', 'rm(list=ls());\r\nif (1){\r\nargs <- commandArgs ( TRUE );\r\nif ( length ( args ) != 2 ){\r\n  stop ( ""Error. Check usage!"" );\r\n}\r\ncounts <- args[1];\r\nsamples <- args[2];\r\n} else {\r\n\t#counts <- ""/data2/projects/gasderminE/20210906_analysis_up_to_counttable/20220607_nottrimmed_Tn_KOvsNTC_forGSEA/counts.txt_AggregatedByPatient.txt_biomart.txt_protein_coding.txt"";\r\n\t#samples <- ""/data2/projects/gasderminE/20210906_analysis_up_to_counttable/20220607_nottrimmed_Tn_KOvsNTC_forGSEA/samples_simple.txt"";\r\n\tcounts <- ""C:/Users/Silvia!/Downloads/Uni/deseq/counts.txt_AggregatedByPatient.txt_biomart.txt_protein_coding.txt"";\r\n\tsamples <- ""C:/Users/Silvia!/Downloads/Uni/deseq/samples_simple.txt"";\r\n\t\r\n}\r\n\r\n\r\nif( ! file.exists(counts) ){\r\n    stop( paste(counts, ""does not exist!"" ) );\r\n}\r\nif( ! file.exists(samples) ){\r\n    stop( paste(samples, ""does not exist!"" ) );\r\n}\r\n\r\nlibrary(""DESeq2"");\r\nlibrary(""tidyr"");\r\nlibrary(""dplyr"");\r\n#library(""RColorBrewer"");\r\n#library(""ggplot2"");\r\n#library(""ggsci"");\r\n#library(""ggrepel"");\r\n#library(""ggpubr"");\r\n#library(""rospca"");\r\n#library(""data.table"");\r\n#library(""tidyverse"");\r\n\r\noutput <- paste(basename(samples), basename(counts), ""RunDESeq2.r"", sep=""_"");\r\n\r\n###################################################\r\n### READ DATA\r\n###################################################\r\n\r\nwrite( paste( ""# data ="", counts ), file="""" );\r\nwrite( paste( ""# samples ="", samples ), file="""" );\r\n\r\n## COUNT TABLE\r\nwrite(paste(""# reading count table...""), file="""");\r\nmy_counts1 <- read.delim(counts, header=TRUE, check.names=FALSE, sep=""\\t""); \r\n#print(head(my_counts1));\r\nprint(dim(my_counts1));\r\n#my_counts2 <- my_counts1 %>% separate(external_gene_name, c(""A"",""B""), sep = \',\')\r\n#names(my_counts1)[8] <- ""ensembl_gene_id""\r\nmy_counts <- my_counts1 %>% select(ensembl_gene_id, Teff_KO1, Teff_KO2, Teff_KO3, Teff_NTC1, Teff_NTC2, Teff_NTC3); #single patient IDs need to be generalized as it wont work for other samples automatically\r\nprint(head(my_counts));\r\nprint(dim(my_counts));\r\n\r\n## METADATA\r\nwrite(paste(""# reading metadata table...""), file="""");\r\nmy_samples <- read.table(samples, header=TRUE, check.names=FALSE, sep=""\\t"");\r\nprint(dim(my_samples));\r\n\r\npdf(paste(output, "".pdf"", sep=""""));\r\n\r\n###################################################\r\n### PREPARE COUNT MATRIX - SORT\r\n###################################################\r\n\r\nmy_cols <- as.character(my_samples$SampleID);\r\nmy_cols <- c(""ensembl_gene_id"", my_cols);\r\nif( length(intersect(my_cols, colnames(my_counts))) != length(my_cols) ){\r\n\terror(""Count matrix does not contain all samples!"");\r\n}\r\n\r\n\r\nx1 <- my_counts[, my_cols];\r\nx2 <- x1[order(x1$ensembl_gene_id),]\r\nx <- x2[!duplicated(x2$ensembl_gene_id),]\r\nrownames(x) <- x$ensembl_gene_id;\r\nx$ensembl_gene_id <- NULL;\r\nx <- data.matrix(x);\r\nprint(dim(x));\r\n\r\n\r\n###################################################\r\n### CREATE DESEQ OBJECT\r\n###################################################\r\n\r\ndds <- DESeqDataSetFromMatrix(x, colData=my_samples, design= ~ BR + Symptom);\r\n\r\n#Filter lines with no counts for all the samples\r\nkeep <- rowSums(counts(dds)) >= 1\r\n#keep only the lines which passed the filter\r\ndds <- dds[keep,]\r\nprint(dim(dds));\r\n\r\n##################################################\r\n### Quick excursus to get the normalized counts for each sample for later GSEA analysis\r\ndds <- DESeqDataSetFromMatrix(x, colData=my_samples, design= ~ BR + Symptom);\r\n#dds <- collapseReplicates(dds, dds$Descr_Patient, dds$TechnicalRep)\r\ndds <- DESeq(dds);\r\nmy_norm_counts_persample <- data.frame(counts(dds, normalize=TRUE), check.names=FALSE);\r\nwrite.table(my_norm_counts_persample,file=""norm_counts_persample.txt"")\r\n#norm_counts_persample <- read.csv(""norm_counts_persample.csv"")\r\n#head(norm_counts_persample)\r\n#dim(norm_counts_persample)\r\n###################################################\r\n\r\n#### NORMALIZED COUNTS\r\n# Regularized log transformation for clustering/heatmaps, etc\r\n\r\nrld <- rlogTransformation(dds);\r\naux <- data.frame(assay(rld), check.names=FALSE);\r\naux <- cbind(ensembl_gene_id=rownames(aux), aux);\r\nrownames(aux) <- NULL;\r\n\r\nmy_counts2 <- my_counts1 %>%\r\n  select(ensembl_gene_id,external_gene_name)\r\naux$external_gene_name <- my_counts2$external_gene_name[match(aux$ensembl_gene_id,my_counts2$ensembl_gene_id)]\r\naux <- aux %>%\r\n  select(external_gene_name, ensembl_gene_id, Teff_KO1, Teff_KO2, Teff_KO3, Teff_NTC1, Teff_NTC2, Teff_NTC3)\r\n\r\nwrite.table(aux, file=paste(output, ""_rld.txt"", sep=""""), quote=FALSE, sep=""\\t"", row.names=FALSE);\r\nprint(head(aux));\r\n\r\n###################################################\r\n### RUN DESEQ PIPELINE\r\n###################################################\r\n\r\ndds <- DESeq(dds);\r\n\r\nplotDispEsts(dds);\r\n\r\nmy_results <- results(dds, contrast=c(""Symptom"", ""KO"", ""NTC""));\r\nmy_results <- data.frame(my_results, check.names=FALSE) %>% arrange(padj);\r\nmy_results <- cbind(external_gene_name=rownames(my_results), my_results);\r\nrownames(my_results) <- NULL;\r\n\r\n#my_results$external_gene_name <- my_counts2$external_gene_name[mat']",0,"ecology, social constraints, evolution, non-breeding strategies, clownfish, cooperative breeding theory, natural selection, genes, benefits, relatives, breeding positions, dispersal, mortality, contest, eviction, exchange, economic bargaining theory, reproduction,"
Ecological and social constraints combine to promote evolution of non-breeding strategies in clownfish,"Individuals that forgo their own reproduction in animal societies represent an evolutionary paradox because it is not immediately apparent how natural selection can preserve the genes that underlie non-breeding strategies. Cooperative breeding theory provides a solution to the paradox: non-breeders benefit by helping relatives and/or inheriting breeding positions; non-breeders do not disperse to breed elsewhere because of ecological constraints. However, the question of why non-breeders do not contest to breed within their group has rarely been addressed. Here, we use a wild population of clownfish (Amphiprion percula), where non-breeders wait peacefully for years to inherit breeding positions, to show non-breeders will disperse when ecological constraints (risk of mortality during dispersal) are experimentally weakened. In addition, we show non-breeders will contest when social constraints (risk of eviction during contest) are experimentally relaxed. Our results show it is the combination of ecological and social constraints that promote the evolution of non-breeding strategies. The findings highlight parallels between, and potential for fruitful exchange between, cooperative breeding theory and economic bargaining theory: individuals will forgo their own reproduction and wait peacefully to inherit breeding positions (engage in cooperative options) when there are harsh ecological constraints (poor outside options) and harsh social constraints (poor inside options).","['setwd(""~/Desktop/PNG"")\nlibrary(readxl)\n\ndata <- read_excel(""ratio_R2-R3.xlsx"")\nView(data)\n \nhist(data$ratio,main= ""R2-R3 ratio"", xlab=""frequency"") \n\nhist(data$ratio,xlim=c(0.4,0.9),ylim=c(0, 30), main= ""R2-R3 ratio"", xlab=""R2-R3 RATIO"") \n\n\n# ECOLOGICAL CONSTRAINTS FIRST EXPERIMENT: \n#TEST 1 (MALE VS EMPTY) -EFFECT OF VARIATION IN THE ALTERNATIVE OPTION   - DO NON BREEDRS/R3 DISPERSE TO AN OUTSIDE OPTION PLACED AT 0.5 M? \nTEST1 <- matrix(c(16, 15, 0,1), ncol = 2)\ncolnames(TEST1) <- c(""NOT moved TEST 1"",""moved TEST 1"") # <- gives column names\nrownames(TEST1) <- c(""EMPTY"", ""MALE"") # <- gives row names\nfisher.test(TEST1, conf.int = TRUE, conf.level = 0.99)\nbarplot(TEST1, beside=TRUE)\n\n\n# ECOLOGICAL CONSTRAINTS SECOND EXPERIMENT: \n#TEST 2 (MALE VS EMPTY) -EFFECT OF VARIATION IN THE ALTERNATIVE OPTION   - DO NON BREEDRS/R3 RETURN TO THEIR HOME ANEMONE WHEN DISPLACED INTO OUTSIDE OPTIONS PLACED AT 0.5 M?\nTEST2 <- matrix(c(9, 13, 7,3), ncol = 2)\ncolnames(TEST2) <- c(""moved TEST 2"",""NOT moved TEST 2"") # <- gives column names\nrownames(TEST2) <- c(""MALE"", ""EMPTY"") # <- gives row names\nfisher.test(TEST2, conf.int = TRUE, conf.level = 0.99)\nbarplot(TEST2, beside=TRUE, legend=TRUE)\n\n\n# TEST 3 (MALE VS EMPTY) -EFFECT OF VARIATION IN THE ALTERNATIVE OPTION   - DO NON BREEDRS/R3 RETURN TO THEIR HOME ANEMONE WHEN DISPLACED INTO OUTSIDE OPTIONS PLACED AT 5 M?\nTEST3<- matrix(c(0, 0, 16,16), ncol = 2)\ncolnames(TEST3) <- c(""moved TEST 2"",""NOT moved TEST 2"") # <- gives column names\nrownames(TEST3) <- c(""MALE"", ""EMPTY"") # <- gives row names\nfisher.test(TEST3, conf.int = TRUE, conf.level = 0.99)\nbarplot(TEST3, beside=TRUE, legend=TRUE)\n\n\n# TEST 4 EMPTY OPTION ONLY: EFFECT OF DISTANCE 0.5 M VS 5 M DISTANCE -DO NON BREEDRS/R3 RETURN TO THEIR HOME ANEMONE WHEN DISPLACED INTO EMPTY OUTSIDE OPTIONS PLACED AT 5 M VS O.5 M?\nTEST4<- matrix(c(13, 0, 3,16), ncol = 2)\ncolnames(TEST4) <- c(""EMPTY moved"","" EMPTY NOT moved"") # <- gives column names\nrownames(TEST4) <- c(""TEST2"", ""TEST3"") # <- gives row names\nfisher.test(TEST4, conf.int = TRUE, conf.level = 0.99)\nbarplot(TEST4, beside=TRUE, legend=TRUE)\n\n# TEST 5 BREEDING MALE OPTION ONLY: EFFECT OF DISTANCE 0.5 M VS 5 M DISTANCE -DO NON BREEDRS/R3 RETURN TO THEIR HOME ANEMONE WHEN DISPLACED INTO OUTSIDE OPTIONS WITH A BREEDING MALE PLACED AT 5 M VS O.5 M?\nTEST5<- matrix(c(9, 0, 7,16), ncol = 2)\ncolnames(TEST5) <- c(""MALE moved"","" MALE NOT moved"") # <- gives column names\nrownames(TEST5) <- c(""TEST2"", ""TEST3"") # <- gives row names\nfisher.test(TEST5, conf.int = TRUE, conf.level = 0.99)\nbarplot(TEST5, beside=TRUE, legend=TRUE)\n\n\n\n# ECOLOGICAL CONSTRAINTS: EXPERIMENT 1 (MALE+EMPTY) VS EXPERIMENT 2   -MOVEMENTS IN THE 1ST EXP VS MOVEMENTS IN THE SECOND EXP A5 O.5. M   \nTESTE<- matrix(c(1, 22, 31,10), ncol = 2)\ncolnames(TESTE) <- c("" moved"","" NOT moved"") # <- gives column names\nrownames(TESTE) <- c(""TEST1"", ""TEST2"") # <- gives row names\nfisher.test(TESTE, conf.int = TRUE, conf.level = 0.99)\nbarplot(TESTE, beside=TRUE, legend=TRUE)\n\n\n# SOCIAL CONTRAINTS: EVICTIONS/SMALL VS BIG\nx <- matrix(c(12, 3, 4,13), ncol = 2)\ncolnames(x) <- c(""EVICTED"",""TOLLERATED"") # <- gives column names\nrownames(x) <- c(""Bigger R3"", ""Smaller R3"") # <- gives row names\nfisher.test(x, conf.int = TRUE, conf.level = 0.99)\nbarplot(x, legend=TRUE,beside=TRUE, main= ""Eviction status by size"", ylab = ""Frequency of outocmes"", ylim=c(0,16))\n                                                                                         ']",1,"Cattle, nutritional quality, North America, protein supply, greenhouse gases, global nutrient fluxes, grazers, drought, atmospheric CO2 concentrations, forage, cattle performance, supplementation, protein deficiency, trajectory, dietary quality, US cattle,"
Data from: Long-term declines in dietary nutritional quality for North American cattle,"With over 1 billion cattle in the world as well as over 2 billion sheep, goats and buffalo, these animals contribute approximately 15% of the global human protein supply while producing a significant proportion of anthropogenic emissions of greenhouse gases and global nutrient fluxes. Despite increasing reliance on grazers for protein production globally, the future of grazers in a changing world is uncertain. Factors such as increased prevalence of drought, rising atmospheric CO2 concentrations, and sustained nutrient export all have the potential to reduce cattle performance by reducing the nutritional quality of forage. However, there are no analyses to quantify changes in diet quality, subsequent impact on cattle performance and cost of supplementation necessary to mitigate any predicted protein deficiency. To quantify the trajectory of nutritional stress in cattle, we examined more than 36 000 measurements of dietary quality taken over 22 yr for US cattle. Here, we show that standardizing for spatial and temporal variation in drought and its effects on forage quality, cattle have been becoming increasingly stressed for protein over the past two decades, likely reducing cattle weight gain. In economic terms, the replacement costs of reduced protein provision to US cattle are estimated to be the equivalent of $1.9 B annually. Given these trends, nitrogen enrichment of grasslands might be necessary if further reduction in protein content of forages is to be prevented.","['library(car)\nlibrary(lme4)\nlibrary(Hmisc)\nlibrary(smatr)\n\n\n##includes random effects\n##this R script will run models and plots for Craine et al. 2017 ERL paper\n\n\nmodel.GANLABDietQuality2<-function()\n\n\n{\n  sink(file = ""Output.GANLABDietQuality2Output.txt"", append = TRUE, type = c(""output""),split = FALSE)  \n  print(""-----------------------------------"")\n  print(date())  \n  \n  \n  ##This is the master data file###\n    datatemp<-data.frame(read.csv(""InputGanlabquality.csv""))\n    \n  ##This is the data file for the Nutbal model results###\n    datanutbal<-data.frame(read.csv(""Nutbal.csv""))\n  \n  ##domains are for 10 ecoregions used here. Also included are the domain names##\n  ##3=Southeast, 6=Prairie Peninsula,8=Ozark Complex,9=Northern Plains, 10=Central Plains,11=Southern Plains, 12=Northern Rocky Mountains,13=Southern Rocky Mountains,14=Desert Southwest,15=Great Basin\n  domain=c(3, 6, 8, 9, 10, 11, 12, 13, 14, 15)\n  domainnames=c(""Southeast"",""Prairie Peninsula"",""Ozark Complex"",""Northern Plains"", ""Central Plains"",""Southern Plains"", ""Northern Rocky Mountains"",""Southern Rocky Mountains"",""Desert Southwest"",""Great Basin"")\n\n  ##predictors are the 5 variables included in the regression model\n  predictors<-c(""intercept"", ""PDSI"", ""Year"", ""latitude"", ""longitude"")\n\n  ##season are the 3 seasons in the data file. Winter not included here\n  season=c(""Spring"",""Summer"",""Fall"")\n  \n  ##qualmetric are the 3 metrics for dietary quality\n  qualmetric=c(18, 19, 20)\n  qualmetricname=c(""cp"",""dom"",""domcp"")\n\n  #######DOY and PDSI#########\n  ##make a matrix 6 x 10\n  DOYmax <- data.frame(matrix(ncol = 11, nrow = 10))\n  \n  ##subset the matrix for only those samples that are to be included and taken after 1993.\n  data2<-(subset(datatemp,datatemp[,2]==""Keep"" & datatemp[,16]>1993))\n  \n \n    \n  ##loop 10 times, subset only data that says keep, for each domain, fit a spline, save predicteds, and then store max and quantile values##\n  for(i in 1:10){\n  \n  data3<-(subset(data2,data2$DomainID==domain[i]))\n  data3SSA<-subset(data3, data3$DOY>59 & data3$DOY<330)\n  mod<-smooth.spline(data3$DOY, data3$cp, df=10)\n  df<-predict(mod, data3$DOY)\n  df2<-cbind(df$x, df$y)\n  df3<-subset(df2, df2[,2]==max(df2[,2]))\n  \n  ##DOYmax (for each domain) saves the day of year at maximum quality, the maximum quality value as well as quantiles\n  DOYmax[i,]=c(domain[i], df3[1,1],df3[1,2],quantile(resid(mod),probs=0.025), quantile(resid(mod),probs=0.975), quantile(resid(mod),probs=0.975)-quantile(resid(mod),probs=0.025), quantile(data3$m0,probs=0.025), quantile(data3$m0,probs=0.975), quantile(data3$m0,probs=0.975)-quantile(data3$m0,probs=0.025),length(data3$cp),length(data3SSA$cp))\n  }\n  \n  ##add the column names, print out DOYmax and then write the file\n  colnames(DOYmax)<-c(""domain"",""DOYmax"",""CPmax"",""q2.5cpresid"",""q97.5cpresid"",""95.cpresid"",""q2.5PDSI"",""q97.5PDSI"",""95PDSI"",""n.SSAW"", ""n.SSA"")\n  print(DOYmax)\n  \n  ###total n\n  print("""")\n  print(paste(""total n = "", sum(DOYmax$n.SSAW)))\n  \n  print(paste(""total n in spring+summer+autumn = "" , sum(DOYmax$n.SSA)))\n  \n  ##write file DOYmax.csv\n  write.csv(DOYmax,""DOYmax.csv"")\n  \n  ##calculate average quantiles and difference in quantiles for all 10 domains\n  DOYquantiles<-colMeans(DOYmax[4:9], na.rm=FALSE)\n  \n  print(""DOY quantiles"")\n  print(DOYquantiles)\n    \n  #################Quality metrics regression models##########\n  ##this creates the dataframe for the model results to go into  \n  output <- data.frame(matrix(ncol = 8, nrow = 450))\n  \n  \n  ##l loop for the 3 quality metrics\n  for (l in 1:3){\n  \n  ##j loop is for the 3 seasons\n  for(j in 1:3)\n  {  \n    ##subset datatemp for keep, >1993, and for each season\n    data2<-(subset(datatemp,datatemp[,2]==""Keep"" & datatemp[,16]>1993 & datatemp[15]==season[j]))\n  \n  ##i loop is for the 10 domains\n  for(i in 1:10){\n    data3<-(subset(data2,data2$DomainID==domain[i]))\n    data3$splineresid <- resid(smooth.spline(data3$DOY, data3[,qualmetric[l]], df=10))\n    fit<-lmer(splineresid~m0 + Year + latitude + longitude + (1 | Location),  data=data3)\n   # print(summary(fit)$coefficients[1:5,])\n    coefs <- data.frame(coef(summary(fit)))\n    coefs$p.value<-(2 * (1 - pnorm(abs(coefs$t.value))))\n    #print(Anova(fit,type = ""III"")) ##if SS are desired use this.\n    \n    ##subset for just the stuff we want\n    data4<-subset(data3,select=c(ranchid, latitude, longitude, DomainID, DateCollected, Year, DOY, Season, m0, splineresid))\n    \n    \n      ##add the quality metric name here\n    data4$qual<-qualmetric[l]\n    \n    \n    ###table for model results\n    ##write the table. the first time through, initiate the table, an then append it every other time\n    if(l==1 & j==1 & i==1) write.table(data4,file=""test.csv"", sep="","", append=FALSE, col.names=FALSE)\n    else write.table(data4,file=""test.csv"", sep="","", append=TRUE, col.names=FALSE)\n      \n    output[(l-1)*150+(j-1)*50+i+10*0:4,5:7]<-rbind(summary(fit)$coefficients[1:5,])\n    output[(l-1)*150+(j-1)*50+i+10*0:4,8]<-coefs$p.value[3]\n    output[(l-1)*150+']",1,"Genomic analyses, phenotypic differences, native populations, invasive populations, diffuse knapweed, Centaurea diffusa, invasive species, evolutionary potential, ecologically-important traits, genetic mechanisms, genome-wide association (GWAS), draft"
Genomic analyses of phenotypic differences between native and invasive populations of diffuse knapweed (Centaurea diffusa),"Invasive species represent excellent opportunities to study the evolutionary potential of traits important to success in novel environments. Although some ecologically-important traits have been identified in invasive species, little is typically known about the genetic mechanisms that underlie invasion success in non-model species. Here, we use a genome-wide association (GWAS) approach to identify the genetic basis of trait variation in the non-model, invasive, diffuse knapweed (Centaurea diffusa Lam. [Asteraceae]). To assist with this analysis, we have assembled the first draft genome reference and fully annotated plastome assembly for this species, and the one of the first from this large, weedy, genus, which is of major ecological and economic importance. We collected phenotype data from 372 individuals from four native and four invasive populations of C. diffusa grown in a common environment. Using these individuals, we produced reduced-representation genotype-by-sequencing (GBS) libraries and identified 7058 SNPs. We identify two SNPs associated with leaf width in these populations, a trait which significantly varies between native and invasive populations. In this rosette forming species, increased leaf width is a major component of increased biomass, a common trait in invasive plants correlated with increased fitness. Finally, we use annotations from Arabidopsis thaliana to identify 98 candidate genes that are near the associated SNPs and highlight several good candidates for leaf width variation.","['library(""tidyverse"")\nlibrary(""vcfR"")\nlibrary(""multtest"")\nlibrary(""gplots"")\nlibrary(""LDheatmap"")\nlibrary(""genetics"")\nlibrary(""ape"")\nlibrary(""EMMREML"")\nlibrary(""scatterplot3d"")\nlibrary(""compiler"")\nsource(""http://zzlab.net/GAPIT/gapit_functions.txt"")\nsource(""http://zzlab.net/GAPIT/emma.txt"")\nlibrary(""stringr"")\nlibrary(""lme4"")\nlibrary(""car"")\nlibrary(""emmeans"")\nlibrary(""qqman"")\n\n\n\n######################################\n### --- Change vcf into hapmap --- ###\n######################################\n\nvcf <- read.vcfR(""data/CdiffSNPs_GWAS_10_filt.recode.vcf.gz"", verbose = TRUE)\n\ninfo <- getFIX(vcf)\nGT <- extract.gt(vcf, return.alleles=TRUE, IDtoRowNames = FALSE, convertNA = FALSE)\n\nGT <- gsub(""A.A"", ""A"", GT)\nGT <- gsub(""T.T"", ""T"", GT)\nGT <- gsub(""C.C"", ""C"", GT)\nGT <- gsub(""G.G"", ""G"", GT)\nGT <- gsub(""A.G"", ""R"", GT)\nGT <- gsub(""G.A"", ""R"", GT)\nGT <- gsub(""C.T"", ""Y"", GT)\nGT <- gsub(""T.C"", ""Y"", GT)\nGT <- gsub(""G.C"", ""S"", GT)\nGT <- gsub(""C.G"", ""S"", GT)\nGT <- gsub(""A.T"", ""W"", GT)\nGT <- gsub(""T.A"", ""W"", GT)\nGT <- gsub(""T.G"", ""K"", GT)\nGT <- gsub(""G.T"", ""K"", GT)\nGT <- gsub(""C.A"", ""M"", GT)\nGT <- gsub(""A.C"", ""M"", GT)\nGT <- gsub(""\\\\W+"", ""N"", GT)\n\ncolnames(GT) <- gsub(""\\\\."", ""\\\\_"", colnames(GT))\n\nrs <- paste(info[,1], info[,2], sep = ""_"")\nalleles <- paste(info[,4], info[,5], sep = ""/"")\nchrom <- info[,1]\npos <- info[,2]\nstrand <- ""+""\nassembly <- NA\ncenter <- NA\nprotLSID <- NA\nassayLSID <- NA\npanelLSID <- NA\nQcode <- NA\n\nhapmap <- cbind(rs, alleles, chrom, pos, strand, assembly, center, protLSID, assayLSID, panelLSID, Qcode, GT)\nwrite.table(hapmap, file=""data/Cdiff_hapmap.txt"", sep=""\\t"", row.names = FALSE, quote=FALSE)\n\n\n\n################################################\n### --- Filter hapmap for use with GAPIT --- ###\n################################################\n\nmyG <- read.table(""data/Cdiff_hapmap.txt"", head = TRUE)\n\n# Remove triallelic SNPs\nmyG <- myG[grep("","", myG$alleles, invert=TRUE),]\n\n# Make chromosome names numeric and sequential for GAPIT\n\n# Pull out chromosomes\nlist <- c(""TR001.Ccrd1"", ""TR001.Ccrd2"", ""TR001.Ccrd3"", ""TR001.Ccrd4"", ""TR001.Ccrd5"", ""TR001.Ccrd6"", ""TR001.Ccrd7"", ""TR001.Ccrd8"", ""TR001.Ccrd9"", ""TR001.Ccrd10"", ""TR001.Ccrd11"", ""TR001.Ccrd12"", ""TR001.Ccrd13"",  ""TR001.Ccrd14"", ""TR001.Ccrd15"", ""TR001.Ccrd16"", ""TR001.Ccrd17"")\nmyG1 <- myG[myG$chrom %in% list,]\nmyG1$chrom <- gsub(""TR001.Ccrd"", """", myG1$chrom)\n\n# Collapse other contigs onto another chromosome\nmyG2 <- myG[!myG$chrom %in% list,]\nmyG2$chrom <- rep(18, length(nrow(myG2)))\nmyG2$pos <- seq(1, nrow(myG2)*10000, by=10000)\n\n# Merge tables \nmyG3 <- rbind(myG1, myG2)\nwrite.table(myG3, file=""data/Cdiff_hapmap.txt"", sep=""\\t"", row.names = FALSE, quote=FALSE)\n\n\n\n###################################\n### --- Do GWAS using GAPIT --- ###\n###################################\n\n# load phenotypic data\nmyY <- read.table(""data/Cdiff_pheno.txt"", head = TRUE)\nmyY <- myY[,c(1,3:8)]\n\n# load genotypic data\nmyG <- read.table(""data/Cdiff_hapmap.txt"", head = FALSE)\nmyG[1:10,1:15]\n# calculate missing data\nmissing <- apply(myG, 1, function(x){ sum(as.numeric(x[12:ncol(myG)]==""N""), na.rm = TRUE) })\nhist(missing)\n# calculate heterozygosity\nn.homozygous <- apply(myG, 1, function(x){ sum(as.numeric(x[12:ncol(myG)]==""A"" | x[12:ncol(myG)]==""T"" | x[12:ncol(myG)]==""C"" | x[12:ncol(myG)]==""G""), na.rm = TRUE) })\nheterozygosity <- 1 - (n.homozygous/(ncol(myG)-11-missing))\nhist(heterozygosity)\n\n# count number of individuals in each population\ntable(gsub(""_.*"", """", intersect(unlist(myG[1,12:383]), myY[,1])))\n\n# Run GAPIT\nsetwd(paste0(getwd(), ""/GAPIT_output_final""))\n# use Major.allele.zero = TRUE to make sign of allelic effect will be relative to the minor allele\nmyGAPIT <- GAPIT(Y = myY, G = myG, PCA.total = 30, Model.selection = TRUE, kinship.algorithm = ""VanRaden"")\n# according to the model selection, we should not include PCs for any trait\n\n# test other ways methods to make sure the overall patterns do not change\nsetwd(paste0(getwd(), ""/GAPIT_output_temp""))\n\n# missing SNPs imputed with major allele\nmyGAPIT <- GAPIT(Y = myY, G = myG, PCA.total = 0, kinship.algorithm = ""VanRaden"", SNP.impute = ""Major"") # no difference\n\n# try with no missing data\nmyG_nomissing <- myG[missing < 1,]\nmyGAPIT <- GAPIT(Y = myY, G = myG_nomissing, PCA.total = 0, kinship.algorithm = ""VanRaden"") # no difference\n\n# try without CHR 18\nmyG_CHRonly <- myG[myG$V3!=18,]\nmyGAPIT <- GAPIT(Y = myY, G = myG_CHRonly, PCA.total = 0, kinship.algorithm = ""VanRaden"") # no difference\n\n# force GAPIT to include some PCs\nmyGAPIT <- GAPIT(Y = myY, G = myG, PCA.total = 3, kinship.algorithm = ""VanRaden"")  # no difference\nmyGAPIT <- GAPIT(Y = myY, G = myG, PCA.total = 6, kinship.algorithm = ""VanRaden"") # just barely not significant\n\n# try different kinship matrices\nmyGAPIT <- GAPIT(Y = myY, G = myG, PCA.total = 0, kinship.algorithm = ""Loiselle"") # no difference\nmyGAPIT <- GAPIT(Y = myY, G = myG, PCA.total = 0, kinship.algorithm = ""EMMA"") # no difference\n\n# try the old version of GAPIT\nmyGAPIT <- GAPIT2(Y = myY, G = myG, PCA.total = 0, kinship.algo']",1,"capuchin monkeys, Sapajus spp., opportunity costs, intertemporal choice, economics, rewards, delay tolerance, intake rate, choices, decision making, non-human primates, theoretical frameworks, human economic behaviour."
Data from: Are capuchin monkeys (Sapajus spp.) sensitive to lost opportunities? The role of opportunity costs in intertemporal choice,"Principles of economics predict that the costs associated with obtaining rewards can influence choice. When individuals face choices between a smaller, immediate option and a larger, later option, they often experience opportunity costs associated with waiting for delayed rewards because they must forego the opportunity to make other choices. We evaluated how reducing opportunity costs affects delay tolerance in capuchin monkeys. After choosing the larger option, in the High cost condition subjects had to wait for the delay to expire, whereas in the Low cost different condition they could perform a new choice during the delay. To assess the effect of intake rate on choices, the Low cost same condition had the same intake rate ratio as the High cost condition. We found that capuchins attended both to intake rates and to opportunity costs. They chose the larger option more often in the Low cost different and Low cost same conditions than in the High cost condition, and more often in the Low cost different condition than in the Low cost same condition. Understanding how non-human primates represent and use costs in making decisions not only helps to develop theoretical frameworks to explain their choices but also addresses similarities with and differences from human decision making. These outcomes provide insights into the origins of human economic behaviour.","['####\n### addessi_etal_rcode.R\n### Created by Jeffrey R. Stevens on 28 Sept 2013 (jeffrey.r.stevens@gmail.com)\n### Finalized on: 2020-07-16\n### Summary: This script calculates descriptive and inferential statistics, \n###     and generates figures for the analysis of capuchin intertemporal choice data.\n### Instructions: Place this file and the data files (addessi_etal_data.csv)\n### \tin the same directory.  Create a folder called ""figures"". Set the R\n### \tworking directory to this directory.  At the R command prompt, type \n### \t> source(""addessi_etal_rcode.R"")\n### \tThis will run the script, adding all of the calculated variables to the\n### \tworkspace and saving PNG versions of the figures in the figures directory.\n### Uses: This script can be reproduced and modified for personal and scientific use.\n### Data files: Description of the data columns:\n###  addessi_etal_data.csv--intertemporal choice data\n###   subject - subject name\n###   session - session number\n###   date - date\n###   condition - experimental condition (High cost, Low cost same, Low cost different--see manuscript for explanation)\n###   trial_num - trial number\n###   ll_side - side on which the larger, later option was placed\n###   choice - choice between larger, later (1) and smaller, sooner (0) option\n###   last_choice - choice in previous trial (0 = smaller, sooner; 1 = larger, later)\n####\n\n####\n# Load libraries and define functions -------------------------------------\n####\n\nlibrary(BayesFactor)  # needed to calculate Bayes factors\nlibrary(bayestestR)  # needed for estimating Bayes factors for models using BICs\nlibrary(brms)  # needed to fit Bayesian models\nlibrary(emmeans)  # needed to calculate estimated marginal means\nlibrary(foreach)  # needed for iterations\nlibrary(here)\t\t# needed for here\nlibrary(lme4)\t# needed for GLMMs\nlibrary(papaja)    # needed for within-subjects confidence intervals\nlibrary(tidyverse)\t# needed for tidyverse\n\n  \n###\n## Create themes for plots\n###\t\t\ntheme_plots <- function () { \n  theme_bw(base_size=20) %+replace% \n    theme(\n      panel.grid = element_blank(),\n      legend.position = ""none"",\n      \n    )\n}\n\ntheme_legend <- function () { \n  theme_bw(base_size=30) %+replace% \n    theme(\n      panel.grid = element_blank(),\n      legend.title = element_blank()  # remove legend title\n    )\n}\n\n\n####\n# Input and prepare data -------------------------------------\n####\n\n## Input data\ndata <- read_csv(here(""data/addessi_etal_data.csv"")) %>% \t# input data file\n  mutate(condition = factor(condition, levels = c(""High cost"", ""Low cost different"", ""Low cost same""))) %>% \t# assign levels of condition\n  arrange(subject, session)  # sort rows\n\n## Summarize data over sessions\ncondition_subject <- data %>% \n  group_by(subject, condition) %>%  # for each subject and session\n  summarize(num_sessions = max(session), # calculate the number of sessions\n            min_session = num_sessions - 5, # find the minmum number of sessions for stable data\n            mean_choice = mean(choice, na.rm = TRUE))  # calculate the mean choice proportions\n\n## Extract only the stable data\n# Initiate data frame\nstable_data <- data[1, ]  # get first row of data\nstable_data$end_session <- NA  # add end_session column\nstable_data <- stable_data[-1, ]  # remove data\n\n# Extract stable data for each subject and condition\nforeach(current_subject = unique(data$subject)) %do% {  # for each subject\n  foreach(current_condition = unique(data$condition)) %do% {  # for each condition\n    current_data <- filter(data, subject == current_subject & condition == current_condition & session > condition_subject$min_session[which(condition_subject$subject == current_subject & condition_subject$condition == current_condition)])  # filter the last five sessions for current subject and condition\n    current_data$end_session <- current_data$session - max(current_data$session) + 5  # renumber sessions to be 1-5\n    stable_data <- bind_rows(stable_data, current_data)  # append data\n  }\n}\n\n## Summarize data over sessions for stable data\ncondition_subject_stable <- stable_data %>% \n  group_by(subject, condition) %>%  # for each subject and condition\n  summarize(num_sessions = max(session), # calculate the total number of sessions\n            mean_choice = mean(choice, na.rm = TRUE))  # calculate the mean choice proportions\n\nsubject_means <- condition_subject_stable %>% \n  group_by(subject) %>%  # for each subject\n  summarize(mean_choice = mean(mean_choice))  # calculate the mean choice proportions\n  \n# Calculate number of standard deviations from the overall mean for Robot\noverall_mean <- mean(subject_means$mean_choice)  # calculate overall mean\noverall_sd <- sd(subject_means$mean_choice)  # calculate overall SD\nrobot_sds <- (filter(condition_subject_stable, subject == ""Robot"" & condition == ""Low cost same"")$mean_choice - overall_mean) / overall_sd  # calculate departure from overall mean for Robot\'s Low cost same response\n\n\n####\n# Descriptive statistics --------------------------------']",1,"Large carnivores, Europe, human population density, land cover changes, grey wolf, Eurasian lynx, brown bear, species distribution models, habitat suitability, forest cover, mosaics of cropland, protection level, co-existence"
Large carnivore expansion in Europe is associated with human population density and land cover changes,"Aim: The recent recovery of large carnivores in Europe has been explained as resulting from a decrease in human persecution driven by widespread rural land abandonment, paralleled by forest cover increase and the consequent increase in availability of shelter and prey. We investigated whether land cover and human population density changes are related to the relative probability of occurrence of three European large carnivores: the grey wolf (Canis lupus), the Eurasian lynx (Lynx lynx) and the brown bear (Ursus arctos).Location: Europe, west of 64 longitude.Methods: We fitted multi-temporal species distribution models using >50,000 occurrence points with time series of land cover, landscape configuration, protected areas, hunting regulations, and human population density covering a 24-year period (1992-2015). Within the temporal window considered, we then predicted changes in habitat suitability for large carnivores throughout Europe.Results: Between 1992 and 2015, the habitat suitability for the three species increased in Eastern Europe, the Balkans, North-West Iberian Peninsula and Northern Scandinavia, but showed mixed trends in Western and Southern Europe. These trends were primarily associated with increases in forest cover and decreases in human population density, and, additionally, with decreases in the cover of mosaics of cropland and natural vegetation.Main Conclusions: Recent land cover and human population changes appear to have altered the habitat suitability pattern for large carnivores in Europe, whereas protection level did not play a role. While projected changes largely match the observed recovery of large carnivore populations, we found mismatches with the recent expansion of wolves in Central and Southern Europe, where factors not included in our models may have played a dominant role. This suggests that large carnivores' co-existence with humans in European landscapes is not limited by habitat availability, but other factors such as favorable human tolerance and policy.","['##########################################################################################################################################\r\n##########################################################################################################################################\r\n#                    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \r\n# Author: Marta Cimatti                    \t    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \r\n# e-mail: cimatti.marta@gmail.com\t\t            \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \r\n# Reference: \r\n#Manuscript title: ""Large carnivore expansion in Europe is associated \r\n#with human population density and land cover changes""\r\n\r\n#                                      \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \r\n##########################################################################################################################################\r\n##########################################################################################################################################\r\n\r\n\r\n#This codes built the original multi-temporal SDMs for the wolf.\r\n\r\n#Change the name of the species when needed.\r\n\r\n#This codes use a generalized linear mixed models (GLMM) \r\n#with a LASSO (Least Absolute Shrinkage and Selection Operator) penalizing algorithm through\r\n#different steps\r\n\r\n#1)calculate the best lambda, \r\n#2)model the probability of occurrence, \r\n#3)predicts it at different time steps, \r\n\r\n#then calculate the most important variable \r\n#and finally evaluate the model using the Boyce Index\r\n\r\n#it requires:\r\n#1) the dataset of occurrence points and pseudo absences matched with the value of the differents predictors for each species\r\n#2) a dataset with all the predictors value for each year\r\n\r\n\r\n\r\n# Load packages -----------------------------------------------------------\r\n\r\npacman::p_load(rgdal, proj4,tidyverse , raster, sp,sf, dplyr,ggplot2,glmmLasso)\r\npacman::p_load(ecospat, plotmo, dismo, PresenceAbsence, caret,broom,SDMTools,caret)\r\n\r\n# Load data ---------------------------------------------------------------\r\n\r\n#set directory\r\nDR<-\'\'\r\n\r\ndata<-read.csv(""DR/wolf_lasso_points.csv"")\r\n#check your data\r\ndata<-data[complete.cases(data),]\r\nnrow(data)\r\nhead(data)\r\n\r\n\r\n#########################\r\n########rename variables\r\nTotWolfDf<-data.frame(crop=as.numeric(data$cropland_10Km),\r\n                      distU=as.numeric(data$eudist_urban),\r\n                      For=as.numeric(data$forest_10Km),\r\n                      Flooded=as.numeric(data$floodedvegetation_10Km),\r\n                      Patch=as.numeric(data$FC_patch.cohesion.index),\r\n                      Gras=as.numeric(data$grassland_10Km),\r\n                      HP=as.numeric(data$H_popdens),\r\n                      HPcv=as.numeric(data$HPD_coeffvar),\r\n                      Cont=as.numeric(data$INDEX_contag),\r\n                      Sh=as.numeric(data$INDEX_land_shan),\r\n                      disRail=as.numeric(data$lea_ed_rail),\r\n                      disRd1=as.numeric(data$lea_ed_rd1),\r\n                      disRd2=as.numeric(data$lea_ed_rd2),\r\n                      disRd3=as.numeric(data$lea_ed_rd3),\r\n                      ForShr=as.numeric(data$mosaicforestshrub_10Km),\r\n                      MosNatCrop=as.numeric(data$mosaicnaturalcrop_10Km),\r\n                      Shr=as.numeric(data$shrubland_10Km),\r\n                      SpVeg=as.numeric(data$sparsevegetation_10Km),\r\n                      Slope=as.numeric(data$slope_eu),\r\n                      TRI=as.numeric(data$TRI_eu),\r\n                      LawBin=as.factor(data$law_wolf_impl_bin),\r\n                      Hunt=as.factor(data$hunting_wolf),\r\n                      PAc=as.numeric(data$PA_coverage),\r\n                      NUTS2=(data$NUTS2),\r\n                      presencepseudoabs=(data$presencepseudoabs)\r\n                      \r\n)\r\nhead(TotWolfDf)\r\n###squared predictors\r\nTotWolfDf$cropS2=(TotWolfDf$crop)^2\r\nTotWolfDf$distUS2=(TotWolfDf$distU)^2\r\nTotWolfDf$ForS2=(TotWolfDf$For)^2\r\nTotWolfDf$FloodedS2=(TotWolfDf$Flooded)^2\r\nTotWolfDf$PatchS2=(TotWolfDf$Patch)^2\r\nTotWolfDf$GrasS2=(TotWolfDf$Gras)^2\r\nTotWolfDf$HPS2=(TotWolfDf$HP)^2\r\nTotWolfDf$HPcvS2=(TotWolfDf$HPcv)^2\r\nTotWolfDf$ContS2=(TotWolfDf$Cont)^2\r\nTotWolfDf$ShS2=(TotWolfDf$Sh)^2\r\nTotWolfDf$ForShrS2=(TotWolfDf$ForShr)^2\r\nTotWolfDf$MosNatCropS2=(TotWolfDf$MosNatCrop)^2\r\nTotWolfDf$ShrS2=(TotWolfDf$Shr)^2\r\nTotWolfDf$SpVegS2=(TotWolfDf$SpVeg)^2\r\nTotWolfDf$SlopeS2=(TotWolfDf$Slope)^2\r\nTotWolfDf$TRIS2=(TotWolfDf$TRI)^2\r\nTotWolfDf$PAcS2=(TotWolfDf$PAc)^2\r\n\r\nhead(TotWolfDf)\r\nlength(TotWolfDf$crop)\r\nlength(TotWolfDf$For)\r\nnrow(TotWolfDf)\r\n\r\n\r\n#lambda selection-----------------------------------------------------------\r\n###\r\n# GLM with lasso - selection on lambda parameter using cross-validation (80-20)\r\n###\r\nlibrary(caret)\r\nlibrary(PresenceAbsence)\r\nlibrary(dismo)\r\nlibrary(plotmo)\r\nlibrary(ecospat)\r\n\r\n# randomly split the dataset into training (80%) and test (20%) sets 10 times for every \r\n# lambda value between 0 and 15 by 0.5-unit intervals, \r\n# and selected the lambda corresponding to the hig']",1,"Dataset, R script, analysis, food waste, environmental education, peers, family influence, primary school students, Northern Italy, Journal of Cleaner Production, metadata, csv format, semicolons, missing data, variables, models, main text,"
"Dataset and R script for the analysis in the article ""Food waste between environmental education, peers, and family influence. Insights from primary school students in Northern Italy"", Journal of Cleaner Production","We hereby publish the dataset (with metadata) and the R script (R Core team, 2018) used for implementing the analysis presented in the paper ""Food waste between environmental education, peers, and family influence. Insights from primary school students in Northern Italy"", Journal of Cleaner Production (Piras et al., 2023). The dataset is provided in csv format with semicolons as separators and ""NA"" for missing data. The dataset includes all the variables used in at least one of the models presented in the paper, either in the main text or in the Supplementary Material. Other variables gathered by means of the questionnaires included as Supplementary Material of the paper have been removed. The dataset includes inputted values for missing data on independent variables. These were inputted using two approaches: last observation carried forward (LOCF) - preferred when possible - and last observation carried backward (LOCB). The metadata are presented as a PDF file.","['#Script used to produce the analysis in the article ""Food waste between environmental education, peers, and family influence. Insights from primary school students in Northern Italy,"" Journal of Cleaner Production (Piras et al., 2023)\r\n\r\n# load dataset\r\ndb <- read.table(""Dataset_final.csv"",header=T, sep="";"", dec=""."")\r\n\r\n# load libraries\r\nlibrary(lme4)\r\nlibrary(ordinal)\r\n\r\n# DATA MANAGEMENT ####\r\n\r\n# characteristics of dataset\r\ndim(db)\r\nnames(db)\r\n\r\n# declaration of ordinal variables\r\n\r\ndb$FREQ_FW_CANT <- factor(db$FREQ_FW_CANT, ordered=T, levels=6:1)\r\ndb$FREQ_FW_HOME <- factor(db$FREQ_FW_HOME, ordered=T, levels=6:1)\r\ndb$SELF_FW_CANT <- factor(db$SELF_FW_CANT, ordered=T, levels=5:1)\r\n\r\n# DESCRIPTIVE STATISTICS ####\r\n\r\n# list of descriptive variables\r\nvar <- c(""SEX"",""N_FAM_CL"",""FOREIGN"",""CARS"",""PGG"",""DICTATOR"",""TRUST"",""SCOLD"",""STRICTNESS"",""OP_PAR_ENV"",""OP_PAR_MAT"",""OP_PAR_ETH"",""FREQ_FW_CANT"",""FREQ_FW_CANT_NEARBY_CL"",""FREQ_FW_CANT_FRIENDS_CL"",""FREQ_FW_CANT_NONFRIENDS_CL"",""FREQ_FW_CANT_BREAK_CL"",""FREQ_FW_CANT_NICE_CL"",""FREQ_FW_CANT_POPULAR_CL"",""FREQ_FW_CANT_DESK_CL"",""FREQ_FW_CANT_DESIRED_CL"",""OP_CH_ENV"",""OP_CH_MAT"",""OP_CH_ETH"")\r\n\r\n# type of descriptive variables (N=Numeric, C=Categorical)\r\ntype_var <- c(""C"",""C"",""C"",""C"",""N"",""N"",""N"",""C"",""C"",""C"",""C"",""C"",""C"",""C"",""C"",""C"",""C"",""C"",""C"",""C"",""C"",""C"",""C"",""C"",""C"")  \r\n\r\n# stratification variable \r\nstrat_var <- ""WAVE""\r\n\r\n# row selection variable \r\nsel_var <- rep(T,1260) # all rows\r\n\r\n# loop for calculating descriptive statistics\r\n\r\ntab <- data.frame(matrix(NA,0,5+2*length(levels(droplevels(as.factor(db[sel_var,strat_var]))))))\r\n\r\nfor (i in 1:length(var)) {\r\n  if (type_var[i]==""C"") {\r\n    row <- data.frame(matrix(NA,length(levels(droplevels(as.factor(db[sel_var,var[i]])))),4+2*length(levels(droplevels(as.factor(db[sel_var,strat_var]))))))\r\n    row[,1] <- levels(droplevels(as.factor(db[sel_var,var[i]])))\r\n    cols_index <- seq(2,length(levels(droplevels(as.factor(db[sel_var,strat_var]))))*2,2)\r\n    row[,cols_index] <- table(droplevels(as.factor(db[sel_var,var[i]])), db[sel_var,strat_var])\r\n    for (k in cols_index) { row[,k+1] <- row[,k] / sum(row[,k]) }\r\n    row[,length(cols_index)*2+2] <- table(droplevels(as.factor(db[sel_var,var[i]])))\r\n    row[,length(cols_index)*2+3] <- row[,length(cols_index)*2+2] / sum(row[,length(cols_index)*2+2])\r\n    row[,length(cols_index)*2+4] <- fisher.test(table(droplevels(as.factor(db[sel_var,var[i]])), droplevels(as.factor(db[sel_var,strat_var]))), simulate.p.value=TRUE)$p.value\r\n    row[,length(cols_index)*2+5] <- sum(is.na(db[sel_var,var[i]]))\r\n    tab <- rbind(tab,row)\r\n  }\r\n  if (type_var[i]==""N"") {\r\n    row <- data.frame(matrix(NA,1,4+2*length(levels(droplevels(as.factor(db[sel_var,strat_var]))))))\r\n    row[,1] <- var[i]\r\n    cols_index <- seq(2,length(levels(droplevels(as.factor(db[sel_var,strat_var]))))*2,2)\r\n    row[,cols_index] <- by(db[sel_var,var[i]], db[sel_var,strat_var], FUN=function(x) mean(x, na.rm=T))\r\n    row[,cols_index+1] <- by(db[sel_var,var[i]], db[sel_var,strat_var], FUN=function(x) sd(x, na.rm=T))\r\n    row[,length(cols_index)*2+2] <- mean(db[sel_var,var[i]], na.rm=T)\r\n    row[,length(cols_index)*2+3] <- sd(db[sel_var,var[i]], na.rm=T)\r\n    row[,length(cols_index)*2+4] <- kruskal.test(db[sel_var,var[i]] ~ db[sel_var,strat_var])$p.value\r\n    row[,length(cols_index)*2+5] <- sum(is.na(db[sel_var,var[i]]))\r\n    tab <- rbind(tab,row)\r\n  }\r\n}\r\n\r\n# save results\r\nwrite.table(tab,""DESCRIPTIVE.csv"", sep="";"", dec=""."", row.names = F)\r\n\r\n# REGRESSION MODELS ####\r\n\r\n# FOOD WASTE AT SCHOOL CANTEEN\r\n\r\n# Proportional Odds Model - POM\r\nmod_FREQ_FW_CANT <- clmm(FREQ_FW_CANT~ SEX + CARS + N_FAM_CL + FOREIGN + PGG + OP_PAR_ENV + OP_PAR_MAT + OP_PAR_ETH + SCOLD + STRICTNESS + FREQ_FW_CANT_NEARBY_CL + as.factor(WAVE) * TREATED + (1|UNIQUEID) + (1|SCHOOL_CLASS), data=db)\r\nsummary(mod_FREQ_FW_CANT)\r\n\r\n# Export results\r\ntab_s <- matrix(NA,16,4)\r\ntab_s[,1] <- exp(summary(mod_FREQ_FW_CANT)$coef[-1:-5,1]) # cOR\r\ntab_s[,2:3] <- exp(confint.default(mod_FREQ_FW_CANT)[-1:-5,]) # 95% CI\r\ntab_s[,4] <- summary(mod_FREQ_FW_CANT)$coef[-1:-5,4] # p-value\r\nwrite.table(tab_s,""FREQ_FW_CANT_MULTI.csv"", sep="";"", row.names=F, dec=""."")\r\n\r\n# FOOD WASTE AT HOME\r\n\r\n# Proportional Odds Model - POM\r\nmod_FREQ_FW_HOME <- clmm(FREQ_FW_HOME ~ SEX + CARS + N_FAM_CL + FOREIGN + PGG + OP_PAR_ENV + OP_PAR_MAT + OP_PAR_ETH + SCOLD + STRICTNESS + FREQ_FW_CANT_NEARBY_CL + as.factor(WAVE) * TREATED + (1|UNIQUEID) + (1|SCHOOL_CLASS), data=db)\r\nsummary(mod_FREQ_FW_HOME)\r\n\r\n# Export results\r\ntab_s <- matrix(NA,16,4)\r\ntab_s[,1] <- exp(summary(mod_FREQ_FW_HOME)$coef[-1:-5,1]) # cOR\r\ntab_s[,2:3] <- exp(confint.default(mod_FREQ_FW_HOME)[-1:-5,]) # 95% CI\r\ntab_s[,4] <- summary(mod_FREQ_FW_HOME)$coef[-1:-5,4] # p-value\r\nwrite.table(tab_s,""FREQ_FW_HOME_MULTI.csv"", sep="";"", row.names=F, dec=""."")\r\n\r\n# SELF-ASSESSMENT OF FOOD WASTE AT SCHOOL CANTEEN\r\n\r\n# Proportional Odds Model - POM\r\nmod_SELF_FW_CANT <- clmm(SELF_FW_CANT ~ SEX + CARS + N_FAM_CL + FOREIGN + PGG + OP_PAR_ENV + OP_PAR_MAT + OP_PAR_']",1,"-European spruce bark beetle 
-Monitoring system 
-Pheromone 
-Sanitary felling 
-Outbreaks 
-Ecological consequences 
-Effective pheromone 
-Side effects 
-Disturbance conditions"
Difference in effect of pheromone for monitoring the European spruce bark beetle,"In recent decades there have been an increasing number of outbreaks of the European spruce bark beetle (Ips typographus) in Europe. A large amount of sanitary felling has taken place, with significant economic and ecological consequences. In order to anticipate such large-scale outbreaks, an effective monitoring system should be set up. One important aspect of monitoring is the decision on which pheromone to use. We suggest a framework for selecting an effective pheromone with few side effects and implemented it on five different pheromones under different disturbance conditions: Pheroprax, IT Ecolure, Ipstyp, Ipsowit and Typosan. We set 50 traps in two areas with sites that were disturbed and undisturbed by wind storms. We collected bark beetles from traps every one to two weeks from the end of March until the end of September in 2019. We investigated the number of bark beetles caught, bark beetle dynamics, amount of bycatch and predators, the taxonomic groups of the bycatch and the overall costs of the monitoring system. We found that Pheroprax, IT Ecolure and Ipsowit caught the most bark beetles and best showed the population dynamics. There was a low amount of bycatch (less than 6% of the total catch) and predators (a few individuals), but some groups seem to prefer certain pheromones. The cost of the pheromones increased with their effectiveness. However, pheromone costs are low relative to the personnel costs involved in setting traps and collecting bark beetles. The framework and the results will help professionals to decide which pheromones to purchase for their bark beetle monitoring system.","['pherbark<-read.delim("".../Copy of KoncnaCRPtabela.txt"",dec="","")\r\nsummary(pherbark)\r\nattach(pherbark)\r\n\r\npherbark$abbreviationID[pherbark$abbreviationID == ""T""] <- ""J""\r\npherbark$abbreviationID[pherbark$abbreviationID == ""M""] <- ""SG""\r\nsummary(pherbark)\r\n\r\nplot(Sum.of.MlOsebkovTyphographus~abbreviationID)\r\nplot(Sum.of.MlOsebkovTyphographus~Pheromone)\r\nplot(Sum.of.MlOsebkovTyphographus~disturbance)\r\n\r\nplot(Sum.of.NIpsTyphographus~abbreviationID)\r\nplot(Sum.of.NIpsTyphographus~Pheromone)\r\nplot(Sum.of.NIpsTyphographus~disturbance)\r\n\r\nlibrary(lme4)\r\nlibrary(MASS)\r\nmod<-glm.nb(Sum.of.NIpsTyphographus~Pheromone*disturbance+abbreviationID,link=""log"", data = pherbark)\r\nmod1<-glm.nb(Sum.of.NIpsTyphographus~Pheromone+disturbance+abbreviationID,link=""log"", data = pherbark)\r\nmod2<-glm.nb(Sum.of.NIpsTyphographus~Pheromone+abbreviationID,link=""log"", data = pherbark)\r\nmod3<-glm.nb(Sum.of.NIpsTyphographus~disturbance+abbreviationID,link=""log"", data = pherbark)\r\nmod4<-glm.nb(Sum.of.NIpsTyphographus~1+abbreviationID,link=""log"", data = pherbark)\r\nAIC(mod,mod1,mod2,mod3,mod4)\r\nsummary(mod1)\r\nlibrary(car)\r\nAnova(mod)\r\n\r\nmod<-lme(Sum.of.MlOsebkovTyphographus~Pheromone*disturbance,random = ~1|abbreviationID,data = pherbark)\r\nmod1<-lme(Sum.of.MlOsebkovTyphographus~Pheromone+disturbance,random = ~1|abbreviationID,data = pherbark)\r\nmod2<-lme(Sum.of.MlOsebkovTyphographus~Pheromone,random = ~1|abbreviationID,data = pherbark)\r\n\r\nAIC(mod,mod1,mod2)\r\n\r\nsummary(mod)\r\n\r\nlibrary(ggplot2)\r\nggplot(pherbark, aes(x=Pheromone, y=Sum.of.NIpsTyphographus, fill=disturbance)) +\r\n  geom_boxplot()+  theme_classic()+xlab("""")+ylab(""number of individuals"")+ \r\n  theme(legend.position=""none"")+scale_fill_manual(values=c(""grey"", ""white""))\r\n\r\nggplot(pherbark, aes(x=abbreviationID, y=Sum.of.NIpsTyphographus)) +\r\n  geom_boxplot()+  theme_classic()+xlab("""")+ylab(""number of individuals"")+ \r\n  theme(legend.position=""none"")+scale_fill_manual(values=c(""grey"", ""white""))\r\n\r\nggplot(pherbark, aes(x=Pheromone, y=Sum.of.MlOsebkovTyphographus, fill=disturbance)) +\r\n  geom_boxplot()+ scale_color_grey() + theme_classic()\r\n#----------------------------------------------------\r\n#bycatch\r\nmod<-glm.nb(Sum.of.AllByCatch~Pheromone*disturbance+abbreviationID,link=""log"", data = pherbark)\r\nmod1<-glm.nb(Sum.of.AllByCatch~Pheromone+disturbance+abbreviationID,link=""log"", data = pherbark)\r\nmod2<-glm.nb(Sum.of.AllByCatch~Pheromone+abbreviationID,link=""log"", data = pherbark)\r\nmod3<-glm.nb(Sum.of.AllByCatch~disturbance+abbreviationID,link=""log"", data = pherbark)\r\nmod4<-glm.nb(Sum.of.AllByCatch~1+abbreviationID,link=""log"", data = pherbark)\r\n\r\nAIC(mod,mod1,mod2,mod3,mod4)\r\nsummary(mod)\r\n\r\nAnova(mod)\r\n\r\nggplot(pherbark, aes(x=Pheromone, y=Sum.of.AllByCatch, fill=disturbance)) +\r\n  geom_boxplot()+  theme_classic()+xlab("""")+ylab(""number of individuals"")+ \r\n  theme(legend.position=""none"")+scale_fill_manual(values=c(""grey"", ""white""))\r\n\r\nggplot(pherbark, aes(x=abbreviationID, y=Sum.of.AllByCatch, fill=disturbance)) +\r\n  geom_boxplot()+ theme_classic()+xlab("""")+ylab(""number of individuals"")+ \r\n  theme(legend.position=""none"")+scale_fill_manual(values=c(""grey"", ""white""))\r\n\r\n#----------------------------------------------------\r\n#bycatch predators Ips typographus\r\nlibrary(pscl)\r\nmodpred<-hurdle(Sum.of.PredatorsTyphographus~Pheromone*disturbance+abbreviationID,dist=""poisson"", link=""logit"", data = pherbark)\r\nmod1<-hurdle(Sum.of.PredatorsTyphographus~Pheromone+disturbance+abbreviationID,dist=""poisson"", link=""logit"", data = pherbark)\r\nmod2<-hurdle(Sum.of.PredatorsTyphographus~Pheromone+abbreviationID,dist=""poisson"", link=""logit"", data = pherbark)\r\nmod3<-hurdle(Sum.of.PredatorsTyphographus~disturbance+abbreviationID,dist=""poisson"", link=""logit"", data = pherbark)\r\nmod4<-hurdle(Sum.of.PredatorsTyphographus~1+abbreviationID,dist=""poisson"", link=""logit"", data = pherbark)\r\n\r\nAIC(mod1,mod2,mod3,mod4)\r\nsummary(mod2)\r\n\r\nAnova(mod)\r\n\r\nlibrary(ggplot2)\r\nggplot(pherbark, aes(x=Pheromone, y=Sum.of.PredatorsTyphographus)) +\r\n  geom_boxplot()+ scale_color_grey() + theme_classic()\r\n\r\nggplot(pherbark, aes(x=Sum.of.AllByCatch, y=Sum.of.NIpsTyphographus, shape=Pheromone, color=Pheromone)) +\r\n  geom_point()+ stat_ellipse(type = ""norm"")\r\n\r\n#--------------------------------------------------------\r\n#difference in composition of bycatch\r\nlibrary(vegan)\r\n\r\nbycatch<-with(pherbark,cbind(Sum.of.Anobiidae, Sum.of.Anthribidae, \r\n               Sum.of.Apionidae, Sum.of.Attelabidae, Sum.of.Biphyllidae, Sum.of.Buprestidae, Sum.of.Byrrhidae, Sum.of.Byturidae,\r\n               Sum.of.Cantharidae, Sum.of.Carabidae, Sum.of.Cerambycidae, Sum.of.Cerylonidae, Sum.of.Chrysomelidae, Sum.of.Ciidae,\r\n               Sum.of.Cleridae, Sum.of.Coccinellidae, Sum.of.Cryptophagidae, Sum.of.Curculionidae, Sum.of.Dermestidae,\r\n               Sum.of.Dytiscidae, Sum.of.Elateridae, Sum.of.Endomychidae, Sum.of.Erotylidae, Sum.of.Eucnemidae, Sum.of.Geotrupidae,\r\n               Sum.of.Heteroceridae, Sum.of.Histeridae, Sum.of.Hydraenidae, Sum.of.Hydrophilidae, Sum.of.K', 'phenpherbark<-read.delim(""C:/Users/maartendeg/Documents/projects/CRP bark beetle monitoring 2018-2021/wp 3/analysis/phenology/KoncnaCRPtabela 16032020.txt"",dec="","")\r\nsummary(phenpherbark)\r\nattach(phenpherbark)\r\ntail(phenpherbark)\r\n\r\nplot(AllByCatch~DatumEmptying)\r\nplot(NIpsTyphographus~DatumEmptying)\r\nplot(MlOsebkovTyphographus~DatumEmptying)\r\n\r\n\r\n\r\n#date conversion to day of the year\r\nphenpherbark$doy <- strptime(phenpherbark$DatumEmptying, ""%d.%m.%Y"")$yday+1\r\nphenpherbark$doy\r\nphenpherbark$doy<-as.numeric(phenpherbark$doy)\r\nmax(phenpherbark$doy)\r\n\r\n\r\nphenpherbark$rel.abun<-NIpsTyphographus/NoDaysAfterEmptying\r\nphenpherbark$rel.abun<-round(phenpherbark$rel.abun)\r\nsummary(phenpherbark$rel.abun)\r\n\r\nSG<-subset(phenpherbark,GGO==""SG"",lty=Pheromone)\r\nKR<-subset(phenpherbark,GGO==""KR"")\r\n\r\ndis.yes<-subset(phenpherbark,disturbance==""yes"",lty=Pheromone)\r\ndis.no<-subset(phenpherbark,disturbance==""no"")\r\n\r\n\r\n\r\nplot(SG$rel.abun~SG$DatumEmptying)\r\n\r\nlibrary(ggplot2)\r\nsubphenbark<-with(phenpherbark,data.frame(rel.abun,doy,GGO,Pheromone,disturbance))\r\nhead(subphenbark)\r\nlibrary(mgcv)\r\ngammod<-gam(rel.abun~\r\n            s(doy,by=factor(GGO))+\r\n            +factor(GGO),data=phenpherbark,family=poisson,na.action=na.fail)\r\n\r\nsummary(gammod)\r\n\r\n#library(MuMIn)\r\n#dd<-dredge(gammod)\r\n#dd\r\n\r\n#op<-par(mfrow=c(2,2))\r\n#gam.check(gammod)\r\n#par(op)\r\n\r\n#op<-par(mfrow=c(2,2))\r\n#plot(gammod)\r\n#par(op)\r\n\r\n\r\npredabun<-predict.gam(gammod,type=""response"")\r\nplot(predabun~doy,data=phenpherbark)\r\nlibrary(ggplot2)\r\nphenpherbark$log.abun<-log(1+phenpherbark$rel.abun)\r\nggplot(phenpherbark,aes(doy, rel.abun, lty=GGO)) + geom_point(col=""gray"") + \r\n  geom_line(aes(y=predabun), size=0.8) +\r\n  theme_bw()+ labs(x=""day of the year"",y=""number of bark beetles"", fill = ""period"")\r\n+ theme(legend.position=\'none\')\r\n\r\n#SG\r\ngammod<-gam(rel.abun~\r\n              s(doy,by=factor(Pheromone))+\r\n              factor(Pheromone),data=SG,na.action=na.fail,family=poisson)\r\n\r\nsummary(gammod)\r\n\r\n#library(MuMIn)\r\n#dd<-dredge(gammod)\r\n#dd\r\n\r\nop<-par(mfrow=c(2,2))\r\ngam.check(gammod)\r\npar(op)\r\n\r\nop<-par(mfrow=c(3,2))\r\nplot(gammod)\r\npar(op)\r\n\r\npredabun<-predict.gam(gammod,type=""response"")\r\nplot(predabun~doy,data=SG)\r\nlibrary(ggplot2)\r\nSG$log.abun<-log(1+SG$rel.abun)\r\nggplot(SG,aes(doy, rel.abun, lty=Pheromone)) + geom_point(col=""gray"") + \r\n  geom_line(aes(y=predabun), size=0.8) +\r\n  theme_bw()+ labs(x=""day of the year"",y=""number of bark beetles"", fill = ""period"")\r\n+ theme(legend.position=\'none\')\r\n\r\n#KR\r\ngammod<-gam(rel.abun~\r\n              s(doy,by=factor(Pheromone))+\r\n              Pheromone,data=KR,na.action=na.fail,family=poisson)\r\n\r\nsummary(gammod)\r\n\r\n#library(MuMIn)\r\n#dd<-dredge(gammod)\r\n#dd\r\n\r\nop<-par(mfrow=c(2,2))\r\ngam.check(gammod)\r\npar(op)\r\n\r\nop<-par(mfrow=c(3,2))\r\nplot(gammod)\r\npar(op)\r\n\r\nKR$predabun<-predict.gam(gammod,type=""response"")\r\nplot(predabun~doy,data=KR)\r\nlibrary(ggplot2)\r\nKR$log.abun<-log(1+KR$rel.abun)\r\nggplot(KR,aes(doy, rel.abun, lty=Pheromone)) + geom_point(col=""gray"") + \r\n  geom_line(aes(y=predabun), size=0.8) +\r\n  theme_bw()+ labs(x=""day of the year"",y=""number of bark beetles"")\r\n+ theme(legend.position=\'none\')\r\n\r\n\r\n']",1,"Mexican free-tailed bats, age, sex, reproductive status, North America, guano mining, roost destruction, organochlorine pesticides, long-distance migrations, roosts, wind energy development, bat communities, bat conservation,"
"Seasonal variation in age, sex, and reproductive status of Mexican free-tailed bats","In North America, Mexican free-tailed bats (Tadarida brasiliensis mexicana) consume vast numbers of insects contributing to the economic well-being of society. Mexican free-tailed bats have declined due to historic guano mining, roost destruction, and bioaccumulation of organochlorine pesticides. Long-distance migrations and dense congregations at roosts exacerbate these declines. Wind energy development further threatens bat communities worldwide and presents emerging challenges to bat conservation. Effective mitigation of bat mortality at wind energy facilities requires baseline data on the biology of affected populations. We collected data on age, sex, and reproductive condition of Mexican free-tailed bats at a cave roost in eastern Nevada located six km from a 152-megawatt industrial wind energy facility. Over five years, we captured 46,353 Mexican free-tailed bats. Although just over half of the caught individuals were non-reproductive adult males (53.6%), 826 pregnant, 892 lactating, 10,101 post-lactating, and 4,327 non-reproductive adult females were captured. Juveniles comprised 11.5% of captures. Female reproductive phenology was delayed relative to conspecific roosts at lower latitudes, likely due to cooler temperatures. Roost use by reproductive females and juvenile bats demonstrates this site is a maternity roost, with significant ecological and conservation value. To our knowledge, no other industrial scale wind energy facilities exist in such close proximity to a heavily used bat roost in North America. Given the susceptibility of Mexican free-tailed bats to wind turbine mortality and the proximity of this roost to a wind energy facility, these data provide a foundation from which differential impacts on demographic groups can be assessed.","['######################/|\\ ^._.^ /|\\######################################\r\n#Seasonal variation in age, sex, and reproductive condition of \r\n#Mexican free-tailed bats at a cave roost in eastern Nevada\r\n#Authors: Joseph R. Danielson, Jason A. Williams, Richard E. Sherwin, \r\n#Kelsey L. Ekholm, Bryan T. Hamilton\r\n#######################/|\\ ^._.^ /|\\###########################################\r\n#Load packages\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(lubridate)\r\nlibrary(ggplot2)\r\nlibrary(\'lattice\')\r\nlibrary(lme4)\r\nlibrary(MuMIn)#This package calculates AIC weights for model set\r\n#set working directory \r\nsetwd(""F:/Publications/BatDemographics"")\r\n#Read in the Tadarida data\r\nbats=read.csv(""F:/Publications/BatDemographics/TadaridaData.csv"")\r\n#Model juveniles by julian day\r\nbats$count=1 #add count colummn to make summary statistics easier to calculate\r\nAgeData=  bats %>%\r\n  complete(year,jday,age, fill = list(count = 0)) %>% \r\n  group_by(year,jday,age, .drop=FALSE) %>% \r\n  summarise(count = sum(count)) %>%  \r\n  spread(key=age,value=count) %>% \r\n  filter(A!=0|SA!=0) %>% \r\n  print(n=Inf) \r\nAgeData$year=as.numeric(AgeData$year)\r\nAgeData$jday=as.numeric(AgeData$jday)\r\nAgeData=as.data.frame(AgeData)#convert to data frame, some strange errors with tibble\r\n#rescale and center the day and year\r\n#this helps the glmer mixedx models run\r\nAgeData=AgeData %>%\r\n  mutate(year2=scale(year, center = TRUE, scale = TRUE))\r\nAgeData$jday2=scale(AgeData$jday, center = TRUE, scale = TRUE)\r\nhead(AgeData)\r\nnames(AgeData)\r\n################################\r\n#Build Models for Juveniles\r\n##################################\r\nmodel1=glmer(cbind(AgeData$SA,AgeData$A)~1+\r\n               (1|year2),data=AgeData,family=binomial)\r\nmodel2=glmer(cbind(AgeData$SA,AgeData$A)~jday2+\r\n               (1|year2),data=AgeData,family=binomial)\r\nmodel3=glmer(cbind(AgeData$SA,AgeData$A)~jday2+I(jday2^2)+\r\n               (1|year2),data=AgeData,family=binomial)\r\nmodel4=glmer(cbind(AgeData$SA,AgeData$A)~jday2+I(jday2^2)+I(jday2^3)+\r\n               (1|year2),data=AgeData,family=binomial)\r\nw=Weights(AICc(model1,model2,model3,model4))\r\nww=cbind(AICc(model1,model2,model3,model4),w)\r\nww[order(-w),]\r\nww[order(ww[,2]),]\r\n#model 4 is highest weighted with 100%\r\nsummary(model4)\r\ncoef(summary(model4)) \r\n##########################################\r\n#Model Adult females by julian day\r\n##############################################\r\nbats=read.csv(""F:/Publications/BatDemographics/TadaridaData.csv"")\r\nhead(bats)\r\nnames(bats)\r\nbats=subset(bats, bats$age==""A"")#Subset to only include adults\r\nbats=droplevels(bats)\r\nbats$count=1 #add count colummn to make summary statistics easier to calculate\r\nbats=  bats %>%\r\n  group_by(year,jday,sex, .drop=FALSE) %>% \r\n  summarise(count = sum(count)) %>%  \r\n  spread(key=sex,value=count) %>% \r\n  filter(M!=0|F!=0) %>% \r\n  print(n=Inf) \r\nbats$year=as.numeric(bats$year)\r\nbats$jday=as.numeric(bats$jday)\r\nbats=as.data.frame(bats)#convert to data frame, some strange errors with tibble\r\nbats$total=with(bats,(M+F))\r\n#rescale and center the day and year\r\n#this helps the glmer mixedx models run\r\nbats=bats %>%\r\n  mutate(year2=scale(year, center = TRUE, scale = TRUE))\r\nbats$jday2=scale(bats$jday, center = TRUE, scale = TRUE)\r\n#############################################\r\n#Build Models for adult female bats\r\n###############################################\r\nmodel1=glmer(cbind(bats$F,bats$total-bats$F)~1+(1|year2),\r\n             data=bats,family=binomial)\r\nmodel2=glmer(cbind(bats$F,bats$total-bats$F)~jday2+(1|year2),\r\n             data=bats,family=binomial)\r\nmodel3=glmer(cbind(bats$F,bats$total-bats$F)~jday2+I(jday2^2)+\r\n               (1|year2),data=bats,family=binomial)\r\nmodel4=glmer(cbind(bats$F,bats$total-bats$F)~jday2+(I(jday2^2))+(I(jday2^3))+\r\n               (1|year2), data=bats,family=binomial)\r\nw=Weights(AICc(model1,model2,model3,model4))\r\nww=cbind(AICc(model1,model2,model3,model4),w)\r\nww[order(-w),]\r\nww[order(ww[,2]),]\r\n#Model 4 is by far the highest weighted model\r\nsummary(model4)\r\ncoef(summary(model4)) \r\n\r\n#Model Pregnant Females\r\nbats=read.csv(""F:/Publications/BatDemographics/TadaridaData.csv"")\r\nfemales=subset(bats, bats$sex==""F"")\r\nfemales=subset(females, females$age==""A"")\r\nfemales=droplevels(females)\r\nfemales$count=1 #add count colummn to make summary statistics easier to calculate\r\nfemales$reproduct_stat=factor(females$reproduct_stat, \r\n                              levels = c(""PG"",""LAC"",""PL"",""NRF""), ordered = TRUE)\r\nfemales=  females %>%\r\n  complete(year,jday,reproduct_stat, fill = list(count = 0)) %>% \r\n  group_by(year,jday,reproduct_stat, .drop=FALSE) %>% \r\n  summarise(count = sum(count)) %>%  \r\n  spread(key=reproduct_stat,value=count) %>% \r\n  filter(PG!=0|LAC!=0|PL!=0|NRF!=0) %>% \r\n  print(n=Inf) \r\nfemales$year=as.numeric(females$year)\r\nfemales$jday=as.numeric(females$jday)\r\nfemales=as.data.frame(females)#convert to data frame, some strange errors with tibble\r\nfemales$total=with(females,(PG+LAC+PL+NRF))\r\n#rescale and center the day and year\r\n#this helps']",1,"gypsy moth, Lymantria dispar, invasive species, thermal performance, adaptability, climate change, range limits, temperature sensitivity, mortality, fitness, pupal mass, development time, trade-offs, conservation, invasion ecology"
"Data from: Variation in growth and developmental responses to supraoptimal temperatures near latitudinal range limits of gypsy moth Lymantria dispar (L.), an expanding invasive species","Variation in thermal performance within and between populations provides the potential for adaptive responses to increasing temperatures associated with climate change. Organisms experiencing temperatures above their optimum on a thermal performance curve exhibit rapid declines in function and these supraoptimal temperatures can be a critical physiological component of range limits. The gypsy moth, Lymantria dispar (L.) (Lepidoptera: Erebidae), is one of the best-documented biological invasions and factors driving its spatial spread are of significant ecological and economic interest. The present study examines gypsy moth sourced from different latitudes across its North American range for sensitivity to high temperature in constant temperature growth chamber experiments. Supraoptimal temperatures result in higher mortality in northern populations compared with populations from the southern range extent (West Virginia and coastal plain of Virginia, U.S.A.). Sublethal effects of high temperature on traits associated with fitness, such as smaller pupal mass, are apparent in northern and West Virginia populations. Overall, the results indicate that populations near the southern limits of the range are less sensitive to high temperatures than northern populations from the established range. However, southern populations are lower performing overall, based on pupal mass and development time, relative to northern populations. This suggests that there may be a trade-off associated with decreased heat sensitivity in gypsy moth. Understanding how species adapt to thermal limits and possible fitness trade-offs of heat tolerance represents an important step toward predicting climatically driven changes in species ranges, which is a particularly critical consideration in conservation and invasion ecology.","['# Data Analysis used for publication in Thompson et al. 2017 for Physiological Entomology\r\n    # Lily Thompson & Trevor Faske\r\n\r\n# VCU Data 2014 = latitudinal range\r\n\r\n# import ""Dryad Latitudinal Range Data.csv""\r\n\r\nVCU_Data_2014 <- Dryad_Latitudinal_Range_Data\r\n\r\n#### Dataset for analysis of 4th instar survival ####\r\nVCUDatasurv <- VCU_Data_2014[which(VCU_Data_2014$SurvAnalyzed == ""Y""),]\r\n  class(VCUDatasurv$Trmt) # needs to be factor\r\n  VCUDatasurv$Trmt <- as.factor(VCUDatasurv$Trmt)\r\n  class(VCUDatasurv$Surv4th) # needs to be factor\r\n  VCUDatasurv$Surv4th <- as.factor(VCUDatasurv$Surv4th)\r\n  summary(VCUDatasurv$Surv4th)\r\n\r\n#### 4th instar Survival ####\r\n\r\n# logistic regression\r\n  require(lme4); require(lmerTest)\r\n  modelSurv4th <- glm(Surv4th ~ Trmt * Pop, data = VCUDatasurv, family = binomial)\r\n  require(car)\r\n  Anova(modelSurv4th, type = ""II"", test.statistic = ""Wald"")\r\n\r\n#### Dataset for analysis of 4th instar mass & development time#### \r\nGMVCU <- VCU_Data_2014[which(VCU_Data_2014$Analyzed == ""Y""),]\r\n  GMVCU$Trmt <- as.factor(GMVCU$Trmt)\r\n  GMVCU$Pop <- as.factor(GMVCU$Pop)\r\n\r\n#### 4th instar mass ####\r\n\r\n  # checking for normality\r\n    hist(GMVCU$Mass4th) # right skewed\r\n    qqnorm(GMVCU$Mass4th); qqline(GMVCU$Mass4th)\r\n    hist(log(GMVCU$Mass4th)) # this fixes it\r\n    qqnorm(log(GMVCU$Mass4th)); qqline(log(GMVCU$Mass4th))\r\n\r\n  # two way ANOVA using natural log of 4th instar mass\r\n    modelmass4th <- lm(log(Mass4th) ~ Trmt * Pop, data = GMVCU)\r\n    anova(modelmass4th)\r\n    require(lme4); require(lmerTest)\r\n    modelmass4tha <- lmer(log(Mass4th) ~ Trmt * Pop + (1|Rep), data = GMVCU) # to get denom df\r\n    anova(modelmass4tha)\r\n\r\n#### 4th instar development time ####\r\n\r\n  # checking normality\r\n    hist(GMVCU$DT4th) #right skewed\r\n    qqnorm(GMVCU$DT4th); qqline(GMVCU$DT4th)\r\n\r\n  # two way ANOVA for development time to 4th instar with Satterthwaite df\r\n    require(lme4);require(lmerTest)\r\n    modeldt4th <- lmer(DT4th ~ Trmt * Pop + (1|Rep), data = GMVCU)\r\n    anova(modeldt4th)\r\n', '# Data Analysis used for publication in Thompson et al. 2017 for Physiological Entomology\r\n    # Lily Thompson & Trevor Faske\r\n\r\n# UR Data from 2015 = Southern invasion front\r\n\r\n# import Dryad_Southern_Range_Survival_Data.csv\r\n# import Dryad_Southern_Range_Data.csv\r\n\r\nUR_Survival_Data_2015 <-Dryad_Southern_Range_Survival_Data\r\nUR_Data_2015 <- Dryad_Southern_Range_Data\r\n\r\n#### Dataset for analysis of survival ####\r\nURsurv <- UR_Survival_Data_2015\r\n  URsurv$Trmt <- as.factor(URsurv$Trmt) \r\n  URsurv$survival2pupa <- URsurv$AlivePupae/(URsurv$AlivePupae+URsurv$DeadBodies)\r\n  head(URsurv)\r\n\r\n#### Survival to pupation ####\r\n  # checking for normality\r\n    asinsqrtTransform <- function(p) (asin(sqrt(p))) #function to get the arcsin squareroot transformaion\r\n    hist(asinsqrtTransform(URsurv$survival2pupa))\r\n  \r\n  # two-way ANOVA with cup as a random variable\r\n    require(lme4)\r\n    modelURsurv <- lm(asinsqrtTransform(survival2pupa) ~ Pop * Trmt + (1|Cup), data = URsurv)\r\n    anova(modelURsurv)\r\n\r\n#### Dataset for analysis of larval development time & pupal mass ####\r\nURData <- UR_Data_2015\r\n\r\n# Split into male and female datasets\r\n  FemaleUR <- subset(URData, Sex == ""F""); FemaleUR <- droplevels(FemaleUR)\r\n  MaleUR <- subset(URData, Sex == ""M""); MaleUR <- droplevels(MaleUR); MaleUR <- na.omit(MaleUR) #there was one male that emerged, but no pupal mass\r\n\r\n# calculating cup means for females\r\n  attach(FemaleUR)\r\n  # number of individuals per cup \r\n    require(plyr) # for count function  \r\n    CupFemaleN <- count(MassPupa,c(\'Trmt\',\'Pop\',\'Cup\'));names(CupFemaleN) <- c(\'Trmt\',\'Pop\',\'Cup\',\'N_ind\')\r\n  # means of each performance metric by cup\r\n    CupFemaleMassPupa <- aggregate(MassPupa, by = list(Trmt,Pop,Cup), FUN = ""mean"", na.rm=TRUE); names(CupFemaleMassPupa) <- c(\'Trmt\',\'Pop\',\'Cup\',\'MassPupa\')\r\n    CupFemaleDTlarva <- aggregate(DTlarva, by = list(Trmt,Pop,Cup), FUN = ""mean"", na.rm=TRUE); names(CupFemaleDTlarva) <- c(\'Trmt\',\'Pop\',\'Cup\',\'DTlarva\')\r\n    CupFemaleDTpupa <- aggregate(DTpupa, by = list(Trmt,Pop,Cup), FUN = ""mean"", na.rm=TRUE); names(CupFemaleDTpupa) <- c(\'Trmt\',\'Pop\',\'Cup\',\'DTpupa\')\r\n    CupFemaleDTtotal <- aggregate(DTtotal, by = list(Trmt,Pop,Cup), FUN = ""mean"", na.rm=TRUE); names(CupFemaleDTtotal) <-c(\'Trmt\',\'Pop\',\'Cup\',\'DTtotal\')\r\n  # merge performance metrics into one data frame\r\n    CupFemaleData <- merge(CupFemaleMassPupa, CupFemaleN, by = c(""Trmt"",""Pop"",""Cup""))\r\n    CupFemaleData <- merge(CupFemaleData, CupFemaleDTlarva, by = c(""Trmt"",""Pop"",""Cup""))\r\n    CupFemaleData <- merge(CupFemaleData, CupFemaleDTpupa, by = c(""Trmt"",""Pop"",""Cup""))\r\n    CupFemaleData <- merge(CupFemaleData, CupFemaleDTtotal, by = c(""Trmt"",""Pop"",""Cup""))\r\n  # makes cup a factor\r\n    CupFemaleData$Cup <- as.factor(CupFemaleData$Cup)\r\n  detach(FemaleUR)\r\n\r\n# calculating cup means for males\r\n  attach(MaleUR)\r\n  # number of individuals per cup \r\n    require(plyr) # for count function\r\n    CupMaleN <- count(MassPupa,c(\'Trmt\',\'Pop\',\'Cup\'));names(CupMaleN) <- c(\'Trmt\',\'Pop\',\'Cup\',\'N_ind\')\r\n  # means of each performance metric by cup\r\n    CupMaleMassPupa <- aggregate(MassPupa, by = list(Trmt,Pop,Cup), FUN = ""mean"", na.rm=TRUE); names(CupMaleMassPupa) <- c(\'Trmt\',\'Pop\',\'Cup\',\'MassPupa\')\r\n    CupMaleDTlarva <- aggregate(DTlarva, by = list(Trmt,Pop,Cup), FUN = ""mean"", na.rm=TRUE); names(CupMaleDTlarva) <- c(\'Trmt\',\'Pop\',\'Cup\',\'DTlarva\')\r\n    CupMaleDTpupa <- aggregate(DTpupa, by = list(Trmt,Pop,Cup), FUN = ""mean"", na.rm=TRUE); names(CupMaleDTpupa) <- c(\'Trmt\',\'Pop\',\'Cup\',\'DTpupa\')\r\n    CupMaleDTtotal <- aggregate(DTtotal, by = list(Trmt,Pop,Cup), FUN = ""mean"", na.rm=TRUE); names(CupMaleDTtotal) <-c(\'Trmt\',\'Pop\',\'Cup\',\'DTtotal\')\r\n  # merge performance metrics into one data frame\r\n    CupMaleData <- merge(CupMaleMassPupa, CupMaleN, by = c(""Trmt"",""Pop"",""Cup""))\r\n    CupMaleData <- merge(CupMaleData, CupMaleDTlarva, by = c(""Trmt"",""Pop"",""Cup""))\r\n    CupMaleData <- merge(CupMaleData, CupMaleDTpupa, by = c(""Trmt"",""Pop"",""Cup""))\r\n    CupMaleData <- merge(CupMaleData, CupMaleDTtotal, by = c(""Trmt"",""Pop"",""Cup""))\r\n  # makes cup a factor\r\n    CupMaleData$Cup <- as.factor(CupMaleData$Cup)\r\n  detach(MaleUR)\r\n\r\n#### Larval development time analysis ####\r\n\r\n# female larval duration using cup means\r\n  # checking for normality\r\n    hist(CupFemaleData$DTlarva) # somewhat right skewed\r\n    qqnorm(CupFemaleData$DTlarva); qqline(CupFemaleData$DTlarva)\r\n\r\n  # two-way ANOVA accounting for the number of individuals per cup\r\n    require(lme4);require(lmerTest)\r\n    CupFemaleData$Trmt<-as.factor(CupFemaleData$Trmt)\r\n    modelDTlarva.F <- lmer(DTlarva ~ Trmt * Pop + (1|N_ind), data= CupFemaleData, REML=FALSE)\r\n    anova(modelDTlarva.F)\r\n    qqnorm(resid(modelDTlarva.F)); qqline(resid(modelDTlarva.F),col = 2) \r\n    \r\n# male larval duration using cup means\r\n  # checking for normality\r\n    hist(CupMaleData$DTlarva) # somewhat right skewed\r\n    qqnorm(CupMaleData$DTlarva); qqline(CupMaleData$DTlarva) \r\n    \r\n  # two-way ANOVA accounting for the number of individuals per cup\r']",1,"emerald ash borer, mortality factors, host resistance, population dynamics, urban forests, natural enemies, disease, stage-specific survivorship, density, split-rearing manipulative experiment, Agrilus planipennis, outbreak dynamics, North America"
Data from: Influence of mortality factors and host resistance on the population dynamics of emerald ash borer (Coleoptera: Buprestidae) in urban forests,"The success of emerald ash borer (Agrilus planipennis Fairmaire) in North America is hypothesized to be due to both the lack of significant natural enemies permitting easy establishment and a population of trees that lack the ability to defend themselves, which allows populations to grow unchecked. Since its discovery in 2002, a number of studies have examined mortality factors of the insect in forests, but none have examined the role of natural enemies and other mortality agents in the urban forest. This is significant because it is in the urban forest where the emerald ash borer has had the most significant economic impacts. We studied populations in urban forests in three municipalities in Ontario, Canada, between 2010 and 2012 using life tables and stage-specific survivorship to analyze data from a split-rearing manipulative experiment. We found that there was little overall mortality caused by natural enemies; most mortality we did observe was caused by disease. Stage-specific survivorship was lowest in small and large larvae, supporting previous observations of high mortality in these two stages. We also used our data to test the hypothesis that mortality and density in emerald ash borer are linked. Our results support the prediction of a negative relationship between mortality and density. However, the relationship varies between insects developing in the crown and those in the trunk of the tree. This relationship was significant because when incorporated with previous findings, it suggests a mechanism and hypothesis to explain the outbreak dynamics of the emerald ash borer.","['# This script analyses the relationship between density and mortality for\r\n# EAB larvae inhabiting logs in Oakville, Toronto and S. S. Marie.\r\n#\r\n# CJK MacQuarrie \r\n# Natural Resources Canada Canadian Forest Service\r\n# Great Lakes Forestry Centre, Sault Ste Marie, Ontario, Canada\r\n#\r\n# R version: \r\n#\r\n# Date July 2013 Revised November 2013 to include rearing treatment\r\n\r\n# Set environment ==============================================================\r\n\r\n# Working directory ------------------------------------------------------------\r\nsetwd(""P://My Documents//Project - EAB lifetable"")\r\n\r\n# Libraries --------------------------------------------------------------------\r\nlibrary(plyr)\r\nlibrary(ggplot2)\r\nlibrary(scales)\r\nlibrary(reshape2)\r\nlibrary(gridExtra)\r\nlibrary(lme4)\r\n\r\n\r\n# Functions --------------------------------------------------------------------\r\nsource( ""./analysis/survivalAnalysisFunctions.R"")\r\nsource( ""./analysis/figureFunctions.R"")\r\n\r\n# Datasets ---------------------------------------------------------------------\r\n\r\neab  <- read.csv( ""./data/mergedLiveEabData.csv"")\r\n# full <- read.csv( ""./data/allDataMerged.csv"")\r\n# wood <- read.csv( ""./data/masterBoltBranchData.csv"")\r\n\r\n# 0.1 Housekeeping =============================================================\r\n\r\nstr(eab)\r\n\r\n# replace NA values in each nSamples and nDeadSamples with 0s\r\n\r\neab$nSamples[ is.na(eab$nSamples)] <- 0\r\n\r\neab$nDeadSamples[ is.na(eab$nDeadSamples)] <- 0\r\n\r\n# compute the total samples\r\neab$totalSamples <- eab$nSamples + eab$nDeadSamples\r\n  \r\n# set a factor for year:\r\neab$fYear <- factor(eab$year.x)\r\n\r\n# set size as a factor:\r\neab$size <- factor(eab$size, \r\n                   levels = c(""pupae"", ""small"", ""medium"", ""large"", \r\n                              ""prepupae"", ""adult""))\r\n\r\n\r\neab$fCollection_number <- factor(eab$collection_number,\r\n                                 levels = c(1,2,3,4))\r\n\r\n# compute densities (insects per m^2)\r\neab$densityTotal <- (eab$totalSamples/eab$sampledArea) * 10000 \r\n\r\neab$densityLive <- (eab$nSamples/eab$sampledArea) * 10000 \r\n\r\neab$densityDead <- (eab$nDeadSamples/eab$sampledArea) * 10000 \r\n\r\n# 1.0 Plot data ================================================================\r\n\r\n# detection of density dependant mortality is visualized as size of population \r\n# vs. % dead individuals. \r\n\r\neab$percentMort <- with(eab,\r\n                        (nDeadSamples/totalSamples) * 100 )\r\n\r\nhist(eab$percentMort)\r\n\r\nggplot( subset( eab,\r\n                percentMort > 0 &\r\n                size %in% c(""small"", ""medium"", ""large"") ),\r\n        aes( x = densityTotal, y = percentMort ) ) +\r\n  \r\n  geom_point( aes( color = city, shape = crown_level) ) +\r\n  \r\n  scale_x_log10() +\r\n \r\n  facet_grid(size~., scales = ""free_x"") +\r\n  \r\n  labs( y = ""percent mortality"", \r\n        x = expression( paste( ""density (larvae per "", m^2, "")"" ) ) )  +\r\n  \r\n  ggCJKMTheme\r\n\r\n# appear to be almost no observations from the upper portion of the tree:\r\n\r\nboxplot(percentMort ~ crown_level,\r\n        subset( eab,\r\n                percentMort > 0 &\r\n                  size %in% c(""small"", ""medium"", ""large"") ) ) # no variability\r\n\r\n  \r\nlog(eab$densityLive)\r\n\r\n\r\n# 2.0 Analysis =================================================================\r\n\r\n# Interested in the effect of density on percent mortality, and if effect is \r\n# influenced by the size of larvae (small, medium, large) and the place where it\r\n# was collected (bolt, or crown level). Are only enough data to test the effect\r\n# of crown level on bolts, lower and middle crown so will restrict analysis to\r\n# this portion of the tree.Random factors are the city the larvae was collected\r\n# in (Oakville, Toronto, SSM) and the year (2010, 2011, 2013), however there are\r\n# too few levels of these observations, so we will treat them as fixed effects.\r\n# Trees will still be treated as a random effect. Graphing the data shows a\r\n# negative relationship between mortality and the log of density. Therefore\r\n# will regress log(density) on mortality. Method follows that of Zurr et al 2010\r\n# for a GLMM analysis (i.e., comments referring to \'pathway\').\r\n\r\n# data lend themselves to fitting a binomial distribution since the data are \r\n# proportions (expressed as percentages). Will express the response as \r\n# a proportion, and use the number of samples as a weighting factor.\r\n\r\n# 2.0.1 The model --------------------------------------------------------------\r\n\r\n#The binomial model with a random effect is equall to:\r\n#numberDeadLarvae_ij ~ Bernoulli(totalSamples, pi_ij)\r\n#logit(pi_ij) = Fixed stuff  + a_i\r\n#a_i ~ N(0, sigma^2_indexTree)\r\n\r\n\r\n\r\n# 2.1 Set up the data ----------------------------------------------------------\r\n\r\neab$propMort <- eab$nDeadSamples /eab$totalSamples\r\n\r\neabMort <- subset(eab, propMort > 0 & \r\n                        size  %in% c(""small"", ""medium"", ""large"") ) \r\n\r\neabMort <- droplevels(eabMort)\r\n\r\nhist(eabMort$propMort)# 286 observations or ~ 9% of dataset\r\n\r\n# create an individual label for each tre', '# custom plotting functions, mostly using ggplot.\r\n# also some themes I find useful.\r\n\r\n# Themes =======================================================================\r\n\r\n\r\n# ggCJKMTheme ------------------------------------------------------------------\r\n\r\n# futher stripped down version expandingon theme_bw()\r\n\r\nggCJKMTheme <- theme_bw() +\r\n  \r\n  theme(legend.position=""bottom"",\r\n        legend.key = element_rect(color = ""white""),\r\n        strip.background = element_rect( fill = ""white"") ) \r\n\r\n\r\n# Functions ====================================================================\r\n\r\n# ggFluctuationCustom ----------------------------------------------------------\r\n\r\n# a customized version of the ggFluctuation plot\r\n# code ripped from now depricated ggFluctuation plot and modified.\r\n\r\nggfluctuationCustom <-function(table, type = ""size"", boxFill = ""grey90"",\r\n                               floor = 0, ceiling = max(table$freq, \r\n                                                        na.rm = TRUE)) \r\n{\r\n  require(ggplot2)\r\n  \r\n  # base code from ggplot2:ggfluctionation\r\n  \r\n  if (is.table(table)) \r\n    table <- as.data.frame(t(table))\r\n  oldnames <- names(table)\r\n  names(table) <- c(""x"", ""y"", ""result"")\r\n  table <- transform(table, x = as.factor(x), y = as.factor(y), \r\n                     freq = result)\r\n  if (type == ""size"") {\r\n    table <- transform(table, freq = sqrt(pmin(freq, ceiling)/ceiling), \r\n                       border = boxFill)\r\n    table[is.na(table$freq), ""freq""] <- 1\r\n    table <- subset(table, freq * ceiling >= floor)\r\n  }\r\n  if (type == ""size"") {\r\n    nx <- length(levels(table$x))\r\n    ny <- length(levels(table$y))\r\n    p <- ggplot(table, aes_string(x = ""x"", y = ""y"", height = ""freq"", \r\n                                  width = ""freq"", fill = ""border"")) + \r\n      \r\n      geom_tile(colour = ""black"") + \r\n      \r\n      scale_fill_identity() + theme(aspect.ratio = ny/nx)\r\n  }\r\n  else {\r\n    p <- ggplot(table, aes_string(x = ""x"", y = ""y"", fill = ""freq"")) + \r\n      geom_tile(colour = ""grey50"") + scale_fill_gradient2(low = ""white"", \r\n                                                          high = ""darkgreen"")\r\n  }\r\n  p$xlabel <- oldnames[1]\r\n  p$ylabel <- oldnames[2]\r\n  p\r\n}\r\n\r\n\r\n# ggCoefPlot -------------------------------------------------------------------\r\n\r\n# make a pretty coefecient plot using ggplot. Based on code in coefplot2\r\n# vignette\r\n\r\n# fortify.coeftab takes a coefplot2 coefficinet table and modifies it for \r\n# use in the plotting function. Can be called seperatly, or done internally\r\n# in ggCoefplot\r\n\r\n\r\n# to create a plotting object for ggCoefplot\r\nfortify.coeftab <- function(object) {\r\n  \r\n  object$pnames <- rownames(object)\r\n  \r\n  vlocs <- match(c(""Std. Error"", ""2.5%"", ""25%"", ""75%"", ""97.5%""), names(object))\r\n  \r\n  names(object)[vlocs] <- c(""std_error"", ""lwr"", ""lwr2"", ""upr2"", ""upr"")\r\n  \r\n  as.data.frame(object)\r\n}\r\n\r\n# workhorse function, actually does the plotting. \r\n# requires coefplot2\r\n\r\nggCoefPlot <- function( model, varNames = NULL, ciAxisLimits = NULL) {\r\n  \r\n  require(ggplot2)\r\n  \r\n  require(coefplot2)\r\n  \r\n  ggObject <- ggplot( data = fortify(coefplot2::coeftab(model) ) ) +\r\n    \r\n    # outer estimates\r\n    geom_pointrange( aes( x = pnames, y = Estimate, \r\n                          ymin = lwr, ymax = upr) ) +\r\n    # inner estimates\r\n    geom_pointrange( aes(pnames, Estimate, \r\n                         ymin = lwr2, ymax = upr2), \r\n                     col = ""black"", \r\n                     size = 1) \r\n  \r\n  # annotate the plot with custom labels if provided\r\n  if( is.null(varNames) == FALSE) {\r\n    \r\n    ggObject <- ggObject + scale_x_discrete( labels = varNames) \r\n  }\r\n  \r\n  # add a vertical line, label the axis \r\n  ggObject <- ggObject +\r\n    \r\n    geom_hline( yintercept = 0) +\r\n    \r\n    labs( y = expression( ""regression estimate "" %+-% "" 95% CI"" ),\r\n          x = """")  \r\n  \r\n  # set to a custom axis range if provided. \r\n  if( is.null(ciAxisLimits) == FALSE) {\r\n\r\n    # zoom into range set here\r\n    ggObject <- ggObject + coord_flip( ylim = ciAxisLimits) \r\n  \r\n  } else {\r\n    \r\n    # else just flip the plot\r\n    ggObject <- ggObject + coord_flip()\r\n    \r\n  }\r\n  \r\n  return(ggObject)\r\n}\r\n\r\n\r\n\r\n\r\n', '# This script performs the life table analysis of the S. Ontario EAB data\r\n#\r\n# CJK MacQuarrie \r\n# Natural Resources Canada Canadian Forest Service\r\n# Great Lakes Forestry Centre, Sault Ste Marie, Ontario, Canada\r\n#\r\n# R version: \r\n#\r\n# Date May 2013\r\n\r\n# Set environment ===============================================================\r\n\r\n# Working directory ------------------------------------------------------------\r\nsetwd(""P://My Documents//Project - EAB lifetable"")\r\n\r\n# Libraries --------------------------------------------------------------------\r\nlibrary(plyr)\r\nlibrary(ggplot2)\r\nlibrary(reshape2)\r\nlibrary(gridExtra)\r\n\r\n# Functions --------------------------------------------------------------------\r\nsource( ""./analysis/survivalAnalysisFunctions.R"")\r\nsource( ""./analysis/figureFunctions.R"")\r\n\r\n# Datasets ---------------------------------------------------------------------\r\n\r\neab  <- read.csv( ""./data/mergedLiveEabData.csv"")\r\nfull <- read.csv( ""./data/allDataMerged.csv"")\r\nwood <- read.csv( ""./data/masterBoltBranchData.csv"")\r\n\r\n# 0.1 Housekeeping =============================================================\r\n\r\n# live EAB is prepared in the prepareData.R script. Inspect its structure:\r\n\r\nstr(eab)\r\n\r\n# replace NA values in each nSamples and nDeadSamples with 0s\r\n\r\neab$nSamples[ is.na(eab$nSamples)] <- 0\r\neab$nDeadSamples[ is.na(eab$nDeadSamples)] <- 0\r\n\r\n# compute the total samples\r\neab$totalSamples <- eab$nSamples + eab$nDeadSamples\r\n  \r\n# set a factor for year:\r\neab$fYear <- factor(eab$year.x)\r\n\r\n\r\n# specify custom ggPlot theme element (for plotting)\r\n\r\nggCJKMTheme <- theme_bw() +\r\n  \r\n  theme(legend.position=""bottom"",\r\n        legend.key = element_rect(color = ""white""),\r\n        strip.background = element_rect( fill = ""white"") ) \r\n  \r\n\r\n# 1.0 Timing of sampling =======================================================\r\n\r\n# 1.1 Plot collection timing for all stages ------------------------------------\r\n\r\n\r\n# life tables will organize lifestages aphabetically within year. Problem is \r\n# prepupae and pupae can occur at the beginning and end of year. Which can\r\n# affect where they are placed in the table. need to see when most samples were\r\n# collected to make sure things are placed right.\r\n\r\n# the original version of this plot arranged orders alphabetically, this version\r\n# plots them according to when stages were collected\r\n\r\neab$size <- factor(eab$size, \r\n                   levels = c(""pupae"", ""small"", ""medium"", ""large"", \r\n                              ""prepupae"", ""adult""))\r\n\r\neab$fCollection_number <- factor(eab$collection_number,\r\n                                 levels = c(1,2,3,4))\r\n\r\n\r\n# original code, show for comparison\r\n# ggplot( subset(eab, \r\n#                size %in%  c(""large"", ""medium"", ""prepupae"", ""pupae"", ""small"") ),\r\n#         aes( x = collection_number, y = totalSamples) ) +\r\n\r\nggplot( subset(eab, is.na(size) == FALSE), \r\n        aes( x = collection_number, y = totalSamples) ) +\r\n    \r\n  geom_jitter( aes(  color = city),\r\n               position = position_jitter(width = .3) )+\r\n  \r\n  facet_grid(fYear~size, scales = ""free_y"") +\r\n  \r\n  ggCJKMTheme +\r\n  \r\n  labs( y = ""number collected"", x = ""collection"" )\r\n\r\n# most pupae collected in the spring, most prepupae in late summer, fall.\r\n# adults scattered throughout the year.\r\n\r\n# 2.0 Construct lifetables ====================================================\r\n\r\n# compute the number of live and dead insects in each stage by year, city\r\n# rearing status, and area sampled to find. Subsetting the data frame\r\n# removes the samples where no eab were found\r\n\r\n# 2.1 Numerical lifetable -----------------------------------------------------\r\n\r\n# a lifetable with lx and dx as counts summarized from the raw daata\r\n# (this lifetable is not shown in the paper, here only for comparison)\r\n\r\n\r\nlifetable1 <- ddply( eab[ is.na(eab$size) == FALSE,],\r\n                    c(""fYear"",""city"",""reared"",""size""),\r\n                    summarize,\r\n\r\n                    totalSampledArea = round( sum( sampledArea / 10000, \r\n                                                   na.rm = T),\r\n                                              2),\r\n                            meanArea = round( mean( sampledArea / 10000, \r\n                                                   na.rm = T),\r\n                                              2),\r\n                              sdArea = round( sd( sampledArea / 10000,\r\n                                                  na.rm = T),\r\n                                              2),\r\n                          numSamples = length( sampledArea),\r\n#                                 lx = total number of individuals entering x\r\n                                  lx = sum(totalSamples), \r\n#                                 dx = total number of inds. dying during x\r\n                                  dx = sum(nDeadSamples),\r\n#                              100qx = % dying during x        \r\n                              x100qx = round( (dx/lx)*100,2 ) )\r\n\r\n# uncomment belo', '#  Helper and supporting functions for the EAB survival analysis script\r\n\r\n# computes the lateral area of a truncated cone. Used to determine the surface \r\n# area of the sampled bolts\r\n\r\ncomputeLateralAreaTruncatedCone <- function( r1 = 0.0, r2 = 0.0, h = 0.0) {\r\n\r\nifelse( r1 > r2, rBig <- r1, rBig <- r2) # the larger value to R, \r\nifelse( r1 > r2, rSmall <- r2, rSmall <- r1) # and the smaller value to r\r\n\r\n# compute the slant height and the lateral area\r\n      s <- sqrt( h^2 + (rBig - rSmall)^2) # slant height\r\n      lateralArea <- pi * (rBig + rSmall) * s\r\n\r\nreturn(lateralArea)\r\n\r\n}\r\n\r\n# checks if the one diamater is twice the size of the other diameter\r\n# indication of a data entry error.\r\ndiamCheck <- function( diam1, diam2) {\r\n  \r\n  ifelse( diam1 > diam2, \r\n          dBig <- diam1, \r\n          dBig <- diam2) # the larger value to dBig, \r\n  \r\n  ifelse( diam1 > diam2, \r\n          dSmall <- diam2, \r\n          dSmall <- diam1) # and the smaller value to dSmall\r\n  \r\n  # if the difference is twice the small diameter, flag the error for inspection\r\n  ifelse( (dBig - dSmall) > (0.5*dSmall), \r\n          response <- ""TRUE"",\r\n          response <- ""FALSE"")\r\n  \r\n  return(response)  \r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\n# computes the stage specific survivial when passed two vectors of stages\r\n# and densitiess\r\n\r\nsurvivalFunction <- function( stages, densities ) { \r\n                         \r\n                    if ( length(stages) == 1) {\r\n                    # if only one stage passed return NA   \r\n                    out <- NA \r\n                    \r\n                    } else { \r\n                    # create a data frame of stages, densities and lagged\r\n                    # densities\r\n                    x <- data.frame( j = stages ,\r\n                                     nj = densities, \r\n                                     nj1 = c( densities[2:length(densities)],\r\n                                               NA) \r\n                                    )\r\n                    # compute the numerical difference between each stage number\r\n                    # and the log survival rate (Hj) following method of\r\n                    # Royama (1984)\r\n                    x <- transform( x,\r\n                                    diffj = c( diff(j), NA) ,\r\n                                    Hj = log(nj1) - log(nj) \r\n                                    )\r\n                    # Hj values calculated between stages that are >1 step\r\n                    # apart are non-sensical. Set these values to NA\r\n                    x$Hj[ x$diffj > 1] <- NA \r\n\r\n                    out <- x$Hj\r\n                    }\r\n\r\n                    return(out)\r\n}\r\n\r\n\r\n# function for computing some mean diameters.\r\n\r\nconditionalMean <- function( obs = c(0, 1,2,3,4,5) ) {\r\n  # computes the mean of all vector using all values > 0\r\n  \r\n  #check for NA\'s and returns NA if any values in the vector are NA   \r\n  if( is.na(sum(obs)) == TRUE ) {\r\n    values <- NA\r\n  } else {\r\n    \r\n    # check if the vector is all 0s, if so report 0\r\n    if(sum(obs) == 0 ) {\r\n      values <- 0\r\n    } else {\r\n      \r\n      \r\n      values <- mean( obs[ obs > 0])\r\n    }\r\n  }\r\n  return (values)\r\n}                          \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# old broken functions\r\n# computeLateralAreaTruncatedCone <- function( r1 = 0.0, r2 = 0.0, h = 0.0) {\r\n\r\n# if NA values are passed sets the radius and height values to 0 which forces\r\n# the function to return NA\r\n\r\n# if ( is.na( r1 & r2 & h) == TRUE ) {\r\n\r\n#     lateralArea <- NA\r\n\r\n# } else {\r\n\r\n# check if radii and height values are all sensical i.e., > 0 \r\n # if ( r1 > 0.0 & r2 > 0.0 & h > 0.0 ) {\r\n #   \r\n #     # set the radii so that the larger value is set as R \r\n #     if (r1 > r2) {\r\n #          R <- r1\r\n #          r <- r2\r\n #     } else {\r\n #          R <- r2\r\n #          r <- r1\r\n #     }\r\n #     # compute the slant height and the lateral area\r\n #     s <- sqrt( h^2 + (R - r)^2) # slant height\r\n #    lateralArea <- pi * (R + r) * s\r\n   \r\n # } else {\r\n #    lateralArea <- 0\r\n # }\r\n\r\n#}\r\n\r\n# return(lateralArea)\r\n\r\n#}\r\n\r\n#within( test.frame <- data.frame( height = c(1, 1:5, 0, 0, NA, 1), \r\n#                    radius1 = c(0, 2:6, NA, 0, NA, 0), \r\n#                    radius2 = c(0, 3:7, NA, 0, NA, 0) ),\r\n#      lateralareas <- computeLateralAreaTruncatedCone (h = height,\r\n#                                                       r1 = radius1,\r\n#                                                       r2 = radius2) )\r\n\r\n#test.frame$lateralAreas <- computeLateralAreaTruncatedCone(\r\n#                               h = test.frame$height,\r\n#                               r1 = test.frame$radius1,\r\n#                               r2 = test.frame$radius2)\r\n\r\n', '# EAB density, stage survival analysis\r\n\r\n# This script analyzes the EAB sampling data from Oakville, Toronto and Sault\r\n# Ste Marie, Ontario collected in 2010 and 2011. \r\n\r\n# CJK MacQuarrie \r\n# Natural Resources Canada Canadian Forest Service\r\n# Great Lakes Forestry Centre, Sault Ste Marie, Ontario, Canada\r\n#\r\n# R version: \r\n#\r\n# Date Spetember 2013\r\n# ------------------------------------------------------------------------------\r\n\r\n# TO DO ========================================================================\r\n\r\n# Set environment ==============================================================\r\n\r\n# Working directory ------------------------------------------------------------\r\n\r\nsetwd(""P://My Documents//Project - EAB lifetable"")\r\n\r\n# Libraries --------------------------------------------------------------------\r\n\r\nlibrary(plyr)\r\nlibrary(ggplot2)\r\nlibrary(nlme)\r\nlibrary(gridExtra)\r\nlibrary(scales)\r\n\r\n\r\n# Functions --------------------------------------------------------------------\r\nsource( ""./analysis/survivalAnalysisFunctions.R"")\r\nsource( ""./analysis/figureFunctions.R"")\r\n\r\n\r\n# Datasets ---------------------------------------------------------------------\r\n\r\nbranchBoltData <- read.csv( ""./data/masterBoltBranchData.csv"")\r\ninsectData <- read.csv( ""./data/masterInsectData.csv"")\r\n\r\n# 0.1 Housekeeping =============================================================\r\n\r\n\r\napply( branchBoltData[12:23], 2, range) \r\n# fails because is one bolt where data sheet was lost. Data entered as ""NA"". \r\n\r\n# Dropping this record (2010K3L3)\r\nbranchBoltData <- branchBoltData[ branchBoltData$index != ""2010K3L3"", ]\r\n\r\napply( branchBoltData[12:23], 2, range) # works. \r\n\r\n# All values within reasonable ranges. Some error correction was required\r\n# see associated script to view detection + correction methods.\r\n\r\n# reformat the date_collected column into dates acceptable to R\r\n\r\nbranchBoltData$rDateColl <- as.Date( branchBoltData$date_collected ,\r\n                                     format = ""%d-%b-%y"")\r\n\r\n\r\n# 1.0 Prepare analysis datasets ================================================\r\n\r\n# 1.1 Create brancBoltData dataset -----------------------------------------------\r\n\r\n# compute the area of all sampled peices\r\nbranchBoltData <- transform( branchBoltData,\r\n                             p1_area = computeLateralAreaTruncatedCone(\r\n                                           h = p1_length,\r\n                                           r1 = p1_diam1,\r\n                                           r2 = p1_diam2) ,\r\n\t\t\t\t\t\t\t p2_area = computeLateralAreaTruncatedCone(\r\n                                           h = p2_length,\r\n                                           r1 = p2_diam1,\r\n                                           r2 = p2_diam2),\r\n                            p3_area = computeLateralAreaTruncatedCone(\r\n                                           h = p3_length,\r\n                                           r1 = p3_diam1,\r\n                                           r2 = p3_diam2),\r\n                            p4_area = computeLateralAreaTruncatedCone(\r\n                                           h = p4_length,\r\n                                           r1 = p4_diam1,\r\n                                           r2 = p4_diam2) )\r\n# sum the areas\r\nbranchBoltData <- transform( branchBoltData,\r\n                             totalArea = p1_area + p2_area + p3_area + p4_area)\r\n\r\n# look at the distribution of sampled areas in the data\r\nboxplot( totalArea ~ crown_level, data = branchBoltData, varwidth = T,\r\n         xlab = ""position in crown"", ylab = ""area"") \r\n\r\n# Determine the area peeled to find all larvae \r\n\r\n# multiplies total area * percent area peeled (converted to a proportion)\r\n\r\nbranchBoltData <- transform( branchBoltData, \r\n                             sampledArea = totalArea * (area_peeled / 100) )\r\n\r\n\r\n# 1.2 Create the liveEab dataset -----------------------------------------------\r\n\r\n# Compute the number of live eab in each stage  done for each sample\r\n\r\nliveEab <- ddply( insectData[ insectData$status == ""live"" &\r\n                              insectData$stageNumber > 0, ],\r\n                  c(""index"",""year"",""record_num"",""size"",""stageNumber""),\r\n                  summarise, \r\n                  nSamples = length(stageNumber) )\r\n\r\n# 1.3 Create the mergedData dataset --------------------------------------------\r\n\r\n# Merge live EAB data and data from bolts \r\n\r\nmergedData <- merge( branchBoltData[ c(""year"", ""city"", ""record_num"", \r\n                                       ""tree_number"", ""rDateColl"",\r\n                                       ""crown_level"", ""reared"",""sampledArea"") ],\r\n                     liveEab,\r\n                     by = c(""year"", ""record_num"") )\r\n\r\n# 1.4 Create the eabSummarized dataset -----------------------------------------\r\n\r\n# Compute the number of live EAB in each stage in each tree by year, city and \r\n# crown level and rearing status.\r\n\r\neabSummarized <- ddply( mergedData, \r\n                        c( ""year"", ""city"", ""']",1,"invasive tephritid fruit flies, economic challenge, agricultural pest management, insecticide resistance, novel class, peptide-based biorational insecticides, active ingredient, GS-omega/kappa-Hxtx-Hv1a (Hv"
Comparative efficacy of GS-Hv1a against invasive tropical tephritids,"Invasive tephritid fruit flies collectively pose a significant economic challenge to agricultural pest management globally. Insecticide resistance to a number of previously effective chemical classes has increased the risk these pests pose as management options become limited. We evaluated the efficacy of a novel class of peptide-based biorational insecticides containing the active ingredient, GS-omega/kappa-Hxtx-Hv1a (Hv1a), against four of tephritid species, oriental fruit fly (Bactrocera dorsalis), melon fly (Zeugodacus cucurbitae), Mediterranean fruit fly (Ceratitis capitata, ""Medfly""), and Malaysian fruit fly (B. latrifrons). This assessment aimed to determine whether Hv1a has the potential for effective management of tropical tephritids, particularly in regions where insecticide resistance has developed to other products. We conducted two sets of screening assays with different methods of exposure, oral and topical, and compared survival over time in response to increasing concentrations of Hv1a up to 3.86 mg/ml. We found that efficacy was species dependent: Ceratitis capitata was susceptible following topical exposure, while the other species of fruit fly were not. While Medfly was susceptible following oral ingestions as well, the dose required to achieve mortality was greater than topical. We observed that mortality continued to decline up to 72 hours after exposure, indicating a delayed effect similar to other reduced-risk insecticidal products such as fipronil. For this reason, Hv1a may be useful as a horizontal transfer tool in conjunction with C. capitata male pheromone lures.","['###load packages\r\nlibrary(ggplot2)\r\nlibrary(reshape2)\r\nlibrary(dplyr)\r\nlibrary(lme4)\r\nlibrary(lmerTest)\r\nlibrary(emmeans)\r\nlibrary(multcomp)\r\nlibrary(RVAideMemoire)\r\nlibrary(survival)\r\nlibrary(coxme)\r\nlibrary(survminer)\r\nlibrary(MASS)\r\nlibrary(ecotox)\r\nlibrary(car)\r\n\r\n###using ecotox - find LC50 at different time points (DOSE RESPONSE)\r\ndata<-read.csv(""Spear lep INGESTION Malaysian fruit fly 2021 -dose response format.csv"", h=T)\r\nView(data)\r\nhead(data)\r\nattach(data)\r\n###using ecotox - find LC50 at different time points\r\nLC_probit(cbind(dead72, alive72) ~ log10(Concentration), p = c(50),\r\n          data = data[data$nominaldose != 0, ])\r\n\r\n###cox model SURVIVAL ANALYSIS\r\ndata2<-read.csv(""Spear lep INGESTION Malaysian fruit fly 2021 - survival analysis format.csv"", h=T)\r\nView(data2)\r\nhead(data2)\r\nattach(data2)\r\n\r\n###assign factors\r\ndata2$replicate<-as.factor(data2$replicate)\r\ndata2$dose<-as.factor(data2$dose)\r\nstr(data2)\r\n\r\n###run model\r\nmodel <- coxph(Surv(time, status) ~ concentration, data=data2)\r\nsummary(model)\r\nAnova(model)\r\nposthoc <- emmeans(model, ~dose)\r\ncld(posthoc)\r\n\r\n# survival plots\r\nfit1 <- survival::survfit(Surv(time, status) ~ dose, data = data2)\r\nggsurvplot(fit1, data = data2, pval=FALSE, xlab=""Time"", ylab=""Survival Probability"",\r\n           tables.y.text = FALSE, tables.col=""dose"", tables.theme = theme_cleantable(),\r\n           ggtheme=theme(axis.title.x = element_text(size=10), axis.title.y = element_text(size=10)),\r\n           legend.title=element_blank())  ', '###load packages\r\nlibrary(ggplot2)\r\nlibrary(reshape2)\r\nlibrary(dplyr)\r\nlibrary(lme4)\r\nlibrary(lmerTest)\r\nlibrary(emmeans)\r\nlibrary(multcomp)\r\nlibrary(RVAideMemoire)\r\nlibrary(survival)\r\nlibrary(coxme)\r\nlibrary(survminer)\r\nlibrary(MASS)\r\nlibrary(ecotox)\r\n\r\n###using ecotox - find LC50 at different time points (DOSE RESPONSE)\r\ndata<-read.csv(""Spear lep INGESTION Med fly 2021 - dose response format.csv"", h=T)\r\nhead(data)\r\nattach(data)\r\nLC_probit(cbind(dead72, alive72) ~ log10(Concentration), p = c(50),\r\n          data = data[data$nominaldose != 0, ])\r\n\r\n\r\n###cox model SURVIVAL ANALYSIS\r\ndata2<-read.csv(""Spear lep INGESTION Med fly 2021 - survival analysis format.csv"", h=T)\r\nView(data2)\r\nhead(data2)\r\nattach(data2)\r\n\r\n###assign factors\r\ndata2$replicate<-as.factor(data2$replicate)\r\ndata2$dose<-as.factor(data2$dose)\r\nstr(data2)\r\n\r\n###run model\r\nmodel <- coxph(Surv(time, status) ~ concentration, data=data2)\r\nsummary(model)\r\nanova(model)\r\nposthoc <- emmeans(model, ~dose)\r\ncld(posthoc)\r\n\r\n# survival plots\r\nfit1 <- survival::survfit(Surv(time, status) ~ dose, data = data2)\r\nggsurvplot(fit1, data = data2, pval=FALSE, xlab=""Time"", ylab=""Survival Probability"",\r\n           tables.y.text = FALSE, tables.col=""dose"", tables.theme = theme_cleantable(),\r\n           ggtheme=theme(axis.title.x = element_text(size=10), axis.title.y = element_text(size=10)),\r\n           legend.title=element_blank())  ', '###load packages\r\nlibrary(ggplot2)\r\nlibrary(reshape2)\r\nlibrary(dplyr)\r\nlibrary(lme4)\r\nlibrary(lmerTest)\r\nlibrary(emmeans)\r\nlibrary(multcomp)\r\nlibrary(RVAideMemoire)\r\nlibrary(survival)\r\nlibrary(coxme)\r\nlibrary(survminer)\r\nlibrary(MASS)\r\nlibrary(ecotox)\r\n\r\n###using ecotox - find LC50 at different time points (DOSE RESPONSE)\r\ndata<-read.csv(""Spear lep INGESTION Melon fly 2021 - dose response format.csv"", h=T)\r\nView(data)\r\nhead(data)\r\nattach(data)\r\nLC_probit(cbind(dead72, alive72) ~ log10(Concentration), p = c(50),\r\n          data = data[data$nominaldose != 0, ])\r\n\r\n###cox model SURVIVAL ANALYSIS\r\ndata2<-read.csv(""Spear lep INGESTION Melon fly 2021 - survival analysis format.csv"", h=T)\r\nView(data2)\r\nhead(data2)\r\nattach(data2)\r\n\r\n###assign factors\r\ndata2$replicate<-as.factor(data2$replicate)\r\ndata2$dose<-as.factor(data2$dose)\r\nstr(data2)\r\n\r\n###run model\r\nmodel <- coxph(Surv(time, status) ~ concentration, data=data2)\r\nsummary(model)\r\nanova(model)\r\nposthoc <- emmeans(model, ~dose)\r\ncld(posthoc)\r\n\r\n# survival plots\r\nfit1 <- survival::survfit(Surv(time, status) ~ dose, data = data2)\r\nggsurvplot(fit1, data = data2, pval=FALSE, xlab=""Time"", ylab=""Survival Probability"",\r\n           tables.y.text = FALSE, tables.col=""dose"", tables.theme = theme_cleantable(),\r\n           ggtheme=theme(axis.title.x = element_text(size=10), axis.title.y = element_text(size=10)),\r\n           legend.title=element_blank())  ', '###load packages\r\nlibrary(ggplot2)\r\nlibrary(reshape2)\r\nlibrary(dplyr)\r\nlibrary(lme4)\r\nlibrary(lmerTest)\r\nlibrary(emmeans)\r\nlibrary(multcomp)\r\nlibrary(RVAideMemoire)\r\nlibrary(survival)\r\nlibrary(coxme)\r\nlibrary(survminer)\r\nlibrary(MASS)\r\nlibrary(ecotox)\r\n\r\n###using ecotox - find LC50 at different time points (DOSE RESPONSE)\r\ndata<-read.csv(""Spear lep INGESTION Oriental fruit fly 2021 - dose response format.csv"", h=T)\r\nhead(data)\r\nattach(data)\r\nLC_probit(cbind(dead72, alive72) ~ log10(Concentration), p = c(50),\r\n          data = data[data$nominaldose != 0, ])\r\n\r\n###cox model SURVIVAL ANALYSIS\r\ndata2<-read.csv(""Spear lep INGESTION Oriental fruit fly 2021 - survival analysis format.csv"", h=T)\r\nView(data2)\r\nhead(data2)\r\nattach(data2)\r\n\r\n###assign factors\r\ndata2$replicate<-as.factor(data2$replicate)\r\ndata2$dose<-as.factor(data2$dose)\r\nstr(data2)\r\n\r\n###run model\r\nmodel <- coxph(Surv(time, status) ~ concentration, data=data2)\r\nsummary(model)\r\nanova(model)\r\nposthoc <- emmeans(model, ~dose)\r\ncld(posthoc)\r\n\r\n# survival plots\r\nfit1 <- survival::survfit(Surv(time, status) ~ dose, data = data2)\r\nggsurvplot(fit1, data = data2, pval=FALSE, xlab=""Time"", ylab=""Survival Probability"",\r\n           tables.y.text = FALSE, tables.col=""dose"", tables.theme = theme_cleantable(),\r\n           ggtheme=theme(axis.title.x = element_text(size=10), axis.title.y = element_text(size=10)),\r\n           legend.title=element_blank())  ', '###load packages\r\nlibrary(ggplot2)\r\nlibrary(reshape2)\r\nlibrary(dplyr)\r\nlibrary(lme4)\r\nlibrary(lmerTest)\r\nlibrary(emmeans)\r\nlibrary(multcomp)\r\nlibrary(RVAideMemoire)\r\nlibrary(survival)\r\nlibrary(coxme)\r\nlibrary(survminer)\r\nlibrary(MASS)\r\nlibrary(ecotox)\r\n\r\n###using ecotox - find LC50 at different time points (DOSE RESPONSE)\r\ndata<-read.csv(""Spear lep TOPICAL Malaysian fruit fly 2022 - dose response format.csv"", h=T)\r\nhead(data)\r\nattach(data)\r\nLC_probit(cbind(dead72, alive72) ~ log10(Concentration), p = c(50),\r\n          data = data[data$nominaldose != 0, ])\r\n\r\n\r\n###cox model SURVIVAL ANALYSIS\r\ndata2<-read.csv(""Spear lep TOPICAL Malaysian fruit fly 2022 - survival analysis format.csv"", h=T)\r\nView(data2)\r\nhead(data2)\r\nattach(data2)\r\n\r\n###assign factors\r\ndata$replicate<-as.factor(data$replicate)\r\ndata$dose<-as.factor(data$dose)\r\nstr(data)\r\n\r\n###run model \r\nmodel <- coxph(Surv(time, status) ~ concentration, data=data2)\r\nsummary(model)\r\nanova(model)\r\nposthoc <- emmeans(model, ~dose)\r\ncld(posthoc)\r\n\r\n# survival plots\r\nfit1 <- survival::survfit(Surv(time, status) ~ dose, data = data2)\r\nggsurvplot(fit1, data = data2, pval=FALSE, xlab=""Time"", ylab=""Survival Probability"",\r\n           tables.y.text = FALSE, tables.col=""dose"", tables.theme = theme_cleantable(),\r\n           ggtheme=theme(axis.title.x = element_text(size=10), axis.title.y = element_text(size=10)),\r\n           legend.title=element_blank())  \r\n', '###load packages\r\nlibrary(ggplot2)\r\nlibrary(reshape2)\r\nlibrary(dplyr)\r\nlibrary(lme4)\r\nlibrary(lmerTest)\r\nlibrary(emmeans)\r\nlibrary(multcomp)\r\nlibrary(RVAideMemoire)\r\nlibrary(survival)\r\nlibrary(coxme)\r\nlibrary(survminer)\r\nlibrary(MASS)\r\nlibrary(ecotox)\r\n\r\n###using ecotox - find LC50 at different time points (DOSE RESPONSE)\r\ndata<-read.csv(""Spear lep TOPICAL Medfly 2022 - dose response format.csv"", h=T)\r\nhead(data)\r\nattach(data)\r\nLC_probit(cbind(dead72, alive72) ~ log10(Concentration), p = c(50),\r\n          data = data[data$nominaldose != 0, ])\r\n\r\n###cox model SURVIVAL ANALYSIS\r\ndata2<-read.csv(""Spear lep TOPICAL Medfly 2022 - survival analysis format.csv"", h=T)\r\nView(data2)\r\nhead(data2)\r\nattach(data2)\r\n\r\n###assign factors\r\ndata2$replicate<-as.factor(data2$replicate)\r\ndata2$dose<-as.factor(data2$dose)\r\nstr(data2)\r\n\r\n###run model\r\nmodel <- coxph(Surv(time, status) ~ concentration, data=data2)\r\nsummary(model)\r\nanova(model)\r\nposthoc <- emmeans(model, ~dose)\r\ncld(posthoc)\r\n\r\n# survival plots\r\nfit1 <- survival::survfit(Surv(time, status) ~ dose, data = data2)\r\nggsurvplot(fit1, data = data2, pval=FALSE, xlab=""Time"", ylab=""Survival Probability"",\r\n           tables.y.text = FALSE, tables.col=""dose"", tables.theme = theme_cleantable(),\r\n           ggtheme=theme(axis.title.x = element_text(size=10), axis.title.y = element_text(size=10)),\r\n           legend.title=element_blank())  \r\n', '###load packages\r\nlibrary(ggplot2)\r\nlibrary(reshape2)\r\nlibrary(dplyr)\r\nlibrary(lme4)\r\nlibrary(lmerTest)\r\nlibrary(emmeans)\r\nlibrary(multcomp)\r\nlibrary(RVAideMemoire)\r\nlibrary(survival)\r\nlibrary(coxme)\r\nlibrary(survminer)\r\nlibrary(MASS)\r\nlibrary(ecotox)\r\n\r\n###using ecotox - find LC50 at different time points (DOSE RESPONSE)\r\ndata<-read.csv(""Spear lep TOPICAL Melonfly 2022 - dose response format.csv"", h=T)\r\nhead(data)\r\nattach(data)\r\nLC_probit(cbind(dead72, alive72) ~ log10(Concentration), p = c(50),\r\n          data = data[data$nominaldose != 0, ])\r\n\r\n###cox model SURVIVAL ANALYSIS\r\ndata2<-read.csv(""Spear lep TOPICAL Melonfly 2022 - survival analysis format.csv"", h=T)\r\nView(data2)\r\nhead(data2)\r\nattach(data2)\r\n\r\n###assign factors\r\ndata2$replicate<-as.factor(data2$replicate)\r\ndata2$dose<-as.factor(data2$dose)\r\nstr(data2)\r\n\r\n###run model\r\nmodel <- coxph(Surv(time, status) ~ concentration, data=data2)\r\nsummary(model)\r\nanova(model)\r\nposthoc <- emmeans(model, ~concentration)\r\ncld(posthoc)\r\n\r\n# survival plots\r\nfit1 <- survival::survfit(Surv(time, status) ~ dose, data = data2)\r\nggsurvplot(fit1, data = data2, pval=FALSE, xlab=""Time"", ylab=""Survival Probability"",\r\n           tables.y.text = FALSE, tables.col=""dose"", tables.theme = theme_cleantable(),\r\n           ggtheme=theme(axis.title.x = element_text(size=10), axis.title.y = element_text(size=10)),\r\n           legend.title=element_blank())  ', '###load packages\r\nlibrary(ggplot2)\r\nlibrary(reshape2)\r\nlibrary(dplyr)\r\nlibrary(lme4)\r\nlibrary(lmerTest)\r\nlibrary(emmeans)\r\nlibrary(multcomp)\r\nlibrary(RVAideMemoire)\r\nlibrary(survival)\r\nlibrary(coxme)\r\nlibrary(survminer)\r\nlibrary(MASS)\r\nlibrary(ecotox)\r\n\r\n###using ecotox - find LC50 at different time points (DOSE RESPONSE)\r\ndata<-read.csv(""Spear lep TOPICAL Oriental fruit fly 2022 - dose response format.csv"", h=T)\r\nhead(data)\r\nattach(data)\r\nLC_probit(cbind(dead72, alive72) ~ log10(Concentration), p = c(50),\r\n          data = data[data$nominaldose != 0, ])\r\n\r\n###cox model\r\ndata2<-read.csv(""Spear lep TOPICAL Oriental fruit fly 2022 - survival analysis format.csv"", h=T)\r\nView(data2)\r\nhead(data2)\r\nattach(data2)\r\n\r\n###assign factors\r\ndata2$replicate<-as.factor(data2$replicate)\r\ndata2$dose<-as.factor(data2$dose)\r\nstr(data2)\r\n\r\n###run model\r\nmodel <- coxph(Surv(time, status) ~ concentration, data=data2)\r\nsummary(model)\r\nanova(model)\r\nposthoc <- emmeans(model, ~concentration)\r\ncld(posthoc)\r\n\r\n# survival plots\r\nfit1 <- survival::survfit(Surv(time, status) ~ dose, data = data2)\r\nggsurvplot(fit1, data = data2, pval=FALSE, xlab=""Time"", ylab=""Survival Probability"",\r\n           tables.y.text = FALSE, tables.col=""dose"", tables.theme = theme_cleantable(),\r\n           ggtheme=theme(axis.title.x = element_text(size=10), axis.title.y = element_text(size=10)),\r\n           legend.title=element_blank())  \r\n']",1,"genomic selection, animal breeding, plant breeding, reference population, young individuals, performance prediction, genetic diversity, breeding individuals, phenotyping, economic interest, genetic gain, French Montbliarde dairy cattle, SNP chip genotypes, simulations,"
Which individuals to choose to update the reference population? Minimizing the loss of genetic diversity in animal Genomic Selection programs,"Genomic selection is commonly used in livestock and increasingly in plant breeding. Relying on phenotypes and genotypes of a reference population, genomic selection allows performance prediction for young individuals having only genotypes. This is expected to achieve fast high genetic gain but with a potential loss of genetic diversity. Existing methods to conserve genetic diversity depend mostly on the choice of the breeding individuals. In this study we propose a modification of the reference population composition to mitigate diversity loss. Since the high cost of phenotyping is the limiting factor for genomic selection our findings are of major economic interest. This study aims to answer the following questions: How would decisions on the reference population affect the breeding population? How to best select individuals to update the reference population and balance maximizing genetic gain and minimizing loss of genetic diversity? We investigated three updating strategies for the reference population: random, truncation and optimal contribution strategies. Optimal contribution maximizes genetic merit for a fixed loss of genetic diversity. A French Montbliarde dairy cattle population with 50K SNP chip genotypes and simulations over ten generations were used to compare these different strategies using milk production as the trait of interest. Candidates were selected to update the reference population. Prediction bias and both genetic merit and diversity were measured. Changes in the reference population composition slightly affected the breeding population. Optimal contribution strategy appeared to be an acceptable compromise to maintain both genetic gain and diversity in the reference and the breeding populations.","['setwd(\'/travail/seynard/\')\ngen<-3\ng<-\nmethod<-\nbv<-read.table(paste(\'r_method/V_ebvall\',gen,g,sep=\'_\'),h=T)\nif (method==""Truncation""){\nbv<-bv[order(-bv$EBV),]\nbvkept<-bv[1:nadd,]\n} else if (method==""Random""){\nbvkept<-sample(bv[,1],nadd,replace=F)\nbvkept<-subset(bv,bv[,1] %in% bvkept)\n} else if (method==""OC""){\ngoc<-read.table(\'r_method/GenCont/Gencont.001\',fill=T)\nn<-which(goc[,1]==\'Name\')\nindoc<-goc[(n[1]+1):nrow(goc),1]\nbvkept<-subset(bv,bv[,1] %in% indoc)\n}\nwrite.table(bvkept,paste(\'r_method/V_ebv\',gen,g,sep=\'_\'),row.names=F,quote=F)\nwrite.table(bvkept,paste(\'res_saved/V_ebv\',gen,\'r_method\',sep=\'_\'),row.names=F,quote=F)\n\n', ""setwd('/travail/seynard/')\ngen<-\ng<-\nmethod<-\nres<-read.table('r_method/GS3/RES_GS3_prod.txt',h=T)\nmax<-res[which.max(res$blup),2]\nnbv<-read.table(paste('r_method/GS3/RES/BLUP_predictions_lait_poly',max,'.txt',sep=''),h=T)\nnbv<-nbv[,c('id','prediction')]\ndtmp<-read.table('data.tmp',h=T)\nif (g==1){\ndtmp<-subset(dtmp,dtmp$G==0)\n} else if (g==3){\ndtmp<-subset(dtmp,dtmp$G==1)\n}\ncolnames(dtmp)[1]<-'id'\nbv<-merge(dtmp,nbv,by='id')\nbv<-data.frame(ID=bv[,1],EBV=bv[,16])\ncolnames(bv)<-c('ID','EBV')\nwrite.table(bv,paste('r_method/V_ebvall',gen,g,sep='_'),row.names=F,quote=F)\nif (g==3){\nwrite.table(bv,paste('res_saved/V_ebvall',gen,'r_method',sep='_'),row.names=F,quote=F)}\n\n"", ""setwd('/travail/seynard/')\ngen<-\ng<-\nmethod<-\nres<-read.table('r_method/GS3/RES_GS3_prod.txt',h=T)\nmax<-res[which.max(res$blup),2]\nnbv<-read.table(paste('r_method/GS3/RES/BLUP_predictions_lait_poly',max,'.txt',sep=''),h=T)\nnbv<-nbv[,c('id','prediction')]\ndtmp<-read.table('data.tmp',h=T)\nif (g==1){\ndtmp<-subset(dtmp,dtmp$G==0)\n} else if (g==3){\ndtmp<-subset(dtmp,dtmp$G==1)\n}\ncolnames(dtmp)[1]<-'id'\nbv<-merge(dtmp,nbv,by='id')\nbv<-data.frame(ID=bv[,1],EBV=bv[,16])\ncolnames(bv)<-c('ID','EBV')\nwrite.table(bv,paste('r_method/V_ebvall',gen,g,'b',sep='_'),row.names=F,quote=F)\nwrite.table(bv,paste('res_saved/V_ebvall',gen,'b_r_method',sep='_'),row.names=F,quote=F)\n\n"", ""setwd('/travail/seynard/')\ngen<-3\ng<-\nAall<-read.table(paste('r_method/A',gen,g,sep='_'),h=T)\nVe<-read.table(paste('r_method/V_ebvall',gen,g,sep='_'),h=T)\ncolnames(Ve)<-c('Progeny','EBV')\nVt<-read.table(paste('r_method/V_tbv',gen,g,sep='_'),h=T)\nV<-merge(Vt,Ve,by='Progeny')\n\nid_inc<-read.table('r_method/GenCont/id_inc')\nid<-unique(id_inc[,1])\ninc<-unique(id_inc[,2])\nid_inc<-data.frame(id,inc)\n\nincA<-subset(Aall,Aall$Progeny %in% id_inc[,2])\nid_incA<-subset(id_inc,id_inc[,2] %in% Aall$Progeny)\nincAn<-data.frame(id_incA[,1],incA$Progeny,rep(2,nrow(incA)),rep(1,nrow(incA)),incA$Final_EBV)\ncolnames(incAn)<-c('id','name','sex','avail','ebv')\nincV<-subset(V,V$Progeny %in% id_inc[,2])\nid_incV<-subset(id_inc,id_inc[,2] %in% V$Progeny)\nincVn<-data.frame(id_incV[,1],incV$Progeny,rep(1,nrow(incV)),rep(1,nrow(incV)),incV$EBV)\ncolnames(incVn)<-c('id','name','sex','avail','ebv')\ninc<-rbind(incAn,incVn)\nwrite.table(inc,'r_method/GenCont/inc.txt',row.names=F,col.names=F,quote=F)\n\n"", ""setwd('/travail/seynard')\ngen<-3\ng<-2\nmethod<-random\n# pedigree\nif (gen==1){\ng0<-read.table('r_method/G0_data_001.txt',h=T)\npedg0<-g0[,c('Progeny','Sire','Dam')]\ns0<-read.table('r_method/Simul0_data_001.txt',h=T)\npeds0<-subset(s0[,c('Progeny','Sire','Dam')],s0$G==1)\npedigree<-rbind(pedg0,peds0)\nif (g > 1){\nga<-read.table('data.tmp',h=T)\npeda<-subset(ga[,c('Progeny','Sire','Dam')],ga$G!=0)\npedigree<-rbind(pedigree,peda)}\n} else {\nped_before<-read.table('r_method/input/ped')\ncolnames(ped_before)<-c('Progeny','Sire','Dam')\nped_add<-read.table('data.tmp',h=T)\nped_add2<-subset(ped_add[,c('Progeny','Sire','Dam')],ped_add$G!=0)\npedigree<-rbind(ped_before,ped_add2)}\nwrite.table(pedigree,'r_method/input/ped',col.names=F,row.names=F,quote=F)\nwrite.table(pedigree,'res_saved/ped',col.names=F,row.names=F,quote=F)\n\n# A, V and perf\nif (gen==1){\nsimul<-read.table('r_method/Simul0_data_001.txt',h=T)\nA<-subset(simul,simul$G==0 &simul$Sex=='M')\n} else {\nA<-read.table(paste('r_method/A',(gen-1),'b',sep='_'),h=T)\n}\nsim<-read.table('data.tmp',h=T)\nif (g==1){V<-subset(sim,sim$G==0)\n} else if (g==3){V<-subset(sim,sim$G==1)}\nwrite.table(A,paste('r_method/A',gen,g,sep='_'),row.names=F,quote=F)\nwrite.table(V,paste('r_method/V_tbv',gen,g,sep='_'),row.names=F,quote=F)\nperfA<-data.frame(A$Progeny,rep(1,nrow(A)),rep(1,nrow(A)),A$Phen,rep('A',nrow(A)))\ncolnames(perfA)<-c('id','breed','w','perf','group')\nperfV<-data.frame(V$Progeny,rep(1,nrow(V)),rep(1,nrow(V)),V$Phen,rep('V',nrow(V)))\ncolnames(perfV)<-c('id','breed','w','perf','group')\nperf<-rbind(perfA,perfV)\nwrite.table(perf,'r_method/input/perf',col.names=F,row.names=F,quote=F)\nif(g==3){\nwrite.table(perf,paste('res_saved/perf',gen,'r_method',sep='_'),col.names=F,row.names=F,quote=F)\nwrite.table(A,paste('res_saved/A',gen,'r_method',sep='_'),row.names=F,quote=F)\nwrite.table(V,paste('res_saved/V_tbv',gen,'r_method',sep='_'),row.names=F,quote=F)}\n\n# ParGlob\nin_script<-read.table('r_method/report.txt',fill=T)\na<-which(in_script[,1]=='h2')\nb<-which(in_script[,1]=='QTL')\nc<-which(in_script[,1]=='Residual')\nh2<-as.character(in_script[a,3])\nqtl_var<-as.character(in_script[b[1],4])\nr_var<-as.character(in_script[c,4])\nparglob<-paste('1 lait',r_var,qtl_var,h2,'0.5',sep=' ')\nwrite.table(parglob,'r_method/input/ParGlob',col.names=F,row.names=F,quote=F)\n\n# Freq\nif (gen ==1){\nfreqg0<-read.table('r_method/G0_freq_mrk_001.txt',h=T,fill=T)\nf<-matrix(ncol=2,nrow=nrow(freqg0))\nfor (j in 1:nrow(freqg0)){\nif (substr(freqg0[j,4],1,1)==1){ f[[j,1]]<-substr(freqg0[j,4],3,100)} \nif (substr(freqg0[j,4],1,1)==2){ f[[j,2]]<-substr(freqg0[j,4],3,100)} \nif (substr(freqg0[j,5],1,1)==1){ f[[j,1]]<-substr(freqg0[j,5],3,100)} \nif (substr(freqg0[j,5],1,1)==2){ f[[j,2]]<-substr(freqg0[j,5],3,100)} \n}\nf[is.na(f)]<-0\nf<-data.frame(f[,1],f[,2])\nwrite.table(f,paste('res_saved/freq_G0',gen,'r_method',sep='_'),row.names=F,col.names=F,quote=F)\nfreqs0<-read.table('r_method/Simul0_freq_mrk_001.txt',h=T,fill=T)\nfreqs0<-subset(freqs0,freqs0$Gen ==0)\nf<-matrix(ncol=2,nrow=nrow(freqs0))\nfor (j in 1:nrow(freqs0)){\nif (substr(freqs0[j,4],1,1)==1){ f[[j,1]]<-substr(freqs0[j,4],3,100)} \nif (substr(freqs0[j,4],1,1)==2){ f[[j,2]]<-substr(freqs0[j,4],3,100)} \nif (substr(freqs0[j,5],1,1)==1){ f[[j,1]]<-substr(freqs0[j,5],3,100)} \nif (substr(freqs0[j,5],1,1)==2){ f[[j,2]]<-substr(freqs0[j,5],3,100)} \n}\nf[is.na(f)]<-0\nf<-data.frame(f[,1],f[,2])\nwrite.table(f,paste('res_saved/freq_S0',gen,'r_method',sep='_'),row.names=F,col.names=F,quote=F)}\n\nif (gen == 1) {\nfreq<-read.table('r_method/Simul0_freq_mrk_001.txt',h=T,fill=T)\n} else { \nfreq<-read.table(paste('r_method/Simul',(gen-1),'a_freq_mrk_001.txt',sep=''),h=T,fill=T)\n}\nif (g==1){freq<-subset(freq,freq$G==0)\n} else if (g==3){freq<-subset(freq,freq$G==1)}\nfreqm<-list()\nfor (i in 1:max(freq$Chr)){\nfreqn<-subset(freq,freq$Chr==i)\nid<-substr(freqn[,1],2,100)\nf<-matrix(ncol=2,nrow=nrow(freqn))\nfor (j in 1:nrow(freqn)){\nif (substr(freqn[j,4],1,1)==1){ f[[j,1]]<-substr(freqn[j,4],3,100)} \nif (substr(freqn[j,4],1,1)==2){ f[[j,2]]<-substr(freqn[j,4],3,100)} \nif (substr(freqn[j,5],1,1)==1){ f[[j,1]]<-substr(freqn[j,5],3,100)} \nif (substr(freqn[j,5],1,1)==2){ f[[j,2]]<-substr(freqn[j,5],3,100)} \n}\nf[is.na(f)]<-0\nfreqm[[i]]<-data.frame(id,rep(2,nrow(freqn)),rep(1,nrow(freqn)),f[,1],rep(2,nrow(freqn)),f[,2])\nwrite.table(freqm[[i]],paste('r_method/input/Freq',i,sep=''),row.names=F,col.names=F,quote=F)}\nf<-matrix(ncol=2,nrow=nrow(freq))\nfor (j in 1:nrow(freq)){\nif (substr(freq[j,4],1,1)==1){ f[[j,1]]<-substr(freq[j,4],3,100)} \nif (substr(freq[j,4],1,1)==2){ f[[j,2]]<-substr(freq[j,4],3,100)} \nif (substr(freq[j,5],1,1)==1){ f[[j,1]]<-substr(freq[j,5],3,100)} \nif (substr(freq[j,5],1,1)==2){ f[[j,2]]<-substr(freq[j,5],3,100)} \n}\nf[is.na(f)]<-0\nf<-data.frame(f[,1],f[,2])\nif(g==3){\nwrite.table(f,paste('res_saved/freq_S',gen,'r_method',sep='_'),row.names=F,col.names=F,quote=F)}\n\n"", ""setwd('/travail/seynard')\ngen<-3\ng<-2\nmethod<-random\n# pedigree\nped_before<-read.table('r_method/input/ped')\ncolnames(ped_before)<-c('Progeny','Sire','Dam')\nped_add<-read.table('data.tmp',h=T)\nped_add2<-subset(ped_add[,c('Progeny','Sire','Dam')],ped_add$G!=0)\npedigree<-rbind(ped_before,ped_add2)\nwrite.table(pedigree,'r_method/input/ped',col.names=F,row.names=F,quote=F)\nwrite.table(pedigree,'res_saved/ped',col.names=F,row.names=F,quote=F)\n\n# A, V and perf\nA<-read.table(paste('r_method/A',gen,'b',sep='_'),h=T)\ns<-read.table(paste('r_method/Simul',gen,'a_data_001.txt',sep=''),h=T)\nAs<-subset(s,s$Progeny %in% A$Progeny)\nAkept<-subset(A,!A$Progeny %in% As$Progeny)\nA<-rbind(Akept,As)\nA$NMPrg[(A$NMPrg+A$NFPrg)==0]<-1\nsim<-read.table('data.tmp',h=T)\nV<-subset(sim,sim$G==1)\nwrite.table(A,paste('r_method/A',gen,g,'b',sep='_'),row.names=F,quote=F)\nwrite.table(V,paste('r_method/V_tbv',gen,g,'b',sep='_'),row.names=F,quote=F)\nperfA<-data.frame(A$Progeny,rep(1,nrow(A)),rep(1,nrow(A)),A$Phen,rep('A',nrow(A)))\ncolnames(perfA)<-c('id','breed','w','perf','group')\nperfV<-data.frame(V$Progeny,rep(1,nrow(V)),rep(1,nrow(V)),V$Phen,rep('V',nrow(V)))\ncolnames(perfV)<-c('id','breed','w','perf','group')\nperf<-rbind(perfA,perfV)\nwrite.table(perf,'r_method/input/perf',col.names=F,row.names=F,quote=F)\nwrite.table(perf,paste('res_saved/perf_b',gen,'r_method',sep='_'),col.names=F,row.names=F,quote=F)\nwrite.table(A,paste('res_saved/A',gen,'b_r_method',sep='_'),row.names=F,quote=F)\nwrite.table(V,paste('res_saved/V_tbv',gen,'b_r_method',sep='_'),row.names=F,quote=F)\n\n# ParGlob\nin_script<-read.table('r_method/report.txt',fill=T)\na<-which(in_script[,1]=='h2')\nb<-which(in_script[,1]=='QTL')\nc<-which(in_script[,1]=='Residual')\nh2<-as.character(in_script[a,3])\nqtl_var<-as.character(in_script[b[1],4])\nr_var<-as.character(in_script[c,4])\nparglob<-paste('1 lait',r_var,qtl_var,h2,'0.5',sep=' ')\nwrite.table(parglob,'r_method/input/ParGlob',col.names=F,row.names=F,quote=F)\n\n# Freq\nfreq<-read.table(paste('r_method/Simul',gen,'a_freq_mrk_001.txt',sep=''),h=T,fill=T)\nfreq<-subset(freq,freq$G==1)\nfreqm<-list()\nfor (i in 1:max(freq$Chr)){\nfreqn<-subset(freq,freq$Chr==i)\nid<-substr(freqn[,1],2,100)\nf<-matrix(ncol=2,nrow=nrow(freqn))\nfor (j in 1:nrow(freqn)){\nif (substr(freqn[j,4],1,1)==1){ f[[j,1]]<-substr(freqn[j,4],3,100)} \nif (substr(freqn[j,4],1,1)==2){ f[[j,2]]<-substr(freqn[j,4],3,100)} \nif (substr(freqn[j,5],1,1)==1){ f[[j,1]]<-substr(freqn[j,5],3,100)} \nif (substr(freqn[j,5],1,1)==2){ f[[j,2]]<-substr(freqn[j,5],3,100)} \n}\nf[is.na(f)]<-0\nfreqm[[i]]<-data.frame(id,rep(2,nrow(freqn)),rep(1,nrow(freqn)),f[,1],rep(2,nrow(freqn)),f[,2])\nwrite.table(freqm[[i]],paste('r_method/input/Freq',i,sep=''),row.names=F,col.names=F,quote=F)}\nf<-matrix(ncol=2,nrow=nrow(freq))\nfor (j in 1:nrow(freq)){\nif (substr(freq[j,4],1,1)==1){ f[[j,1]]<-substr(freq[j,4],3,100)} \nif (substr(freq[j,4],1,1)==2){ f[[j,2]]<-substr(freq[j,4],3,100)} \nif (substr(freq[j,5],1,1)==1){ f[[j,1]]<-substr(freq[j,5],3,100)} \nif (substr(freq[j,5],1,1)==2){ f[[j,2]]<-substr(freq[j,5],3,100)} \n}\nf[is.na(f)]<-0\nf<-data.frame(f[,1],f[,2])\nwrite.table(f,paste('res_saved/freq_S',gen,'b_r_method',sep='_'),row.names=F,col.names=F,quote=F)\n\n"", '####### Script data description for real data set #######\n###multiple script together###\nlibrary(lsmeans)\nlibrary(fmsb)\nlibrary(car)\nlibrary(MASS)\nlibrary(lme4)\nn_43801=43801\ncar=paste(\'lait\')\n\n###1###\nped<-read.table(\'pedigree\')\ncolnames(ped)<-c(\'id\',\'ids\',\'idd\',\'name\',\'names\',\'named\',\'sex\',\'YoB\')\nhetind<-read.table(\'Hobs_initial.txt\')\ncolnames(hetind)<-c(\'id\',\'nb_het\')\nindAinb<-read.table(\'pedigree_inbr_coef.dat\')\ncolnames(indAinb)<-c(\'id\',\'inbA\')\nlist<-read.table(\'parameters_groups\',sep=\'/\')\nbv_ini<-read.table(\'perfinitial\',sep=\' \')\nbvi<-bv_ini[,c(1,4)]\ncolnames(bvi)<-c(\'id\',\'tbv\')\ngroup<-list()\ndata<-list()\nn_male<-0\nn_female<-0\nebv<-0\ntbv<-0\ninb<-0\nHo<-0\nfor (i in 1:nrow(list)){\ngroup[[i]]<-read.table(paste(list[i,1]))\ncolnames(group[[i]])<-c(\'id\',\'ebv\')\nd0<-merge(group[[i]],bvi,by=\'id\')\nd1<-merge(d0,ped,by=\'id\')\nd2<-merge(d1,hetind,by=\'id\')\ndata[[i]]<-merge(d2,indAinb,by=\'id\')\n#nmales/nfemale\nn_male[[i]]<-nrow(subset(data[[i]],data[[i]]$sex==1))\nn_female[[i]]<-nrow(subset(data[[i]],data[[i]]$sex==2))\n#ebv\nebv[[i]]<-mean(data[[i]]$ebv)\n#tbv\ntbv[[i]]<-mean(data[[i]]$tbv)\n#inb\ninb[[i]]<-mean(data[[i]]$inbA)\n#het_obs\nHo[[i]]<-mean(data[[i]]$nb_het/n_43801)\nwrite.table(data[[i]][,c(\'id\',\'ebv\',\'tbv\',\'inbA\',\'nb_het\')],paste(\'test_g\',i,sep=\'\'),row.names=F,col.names=F,quote=F)}\nNe<-0\nfor (i in 1:nrow(list)){\nNe[[i]]<-1/(2*(inb[[i]]))}\nres<-data.frame(list,n_male,n_female,ebv,tbv,inb,Ho,Ne,Ne/(n_male+n_female))\nwrite.table(res,\'res\',row.names=F,col.names=T,quote=F)\n\n###2###\ntbv<-read.table(""perf"")\ncolnames(tbv)<-c(\'id\',\'mean\',\'weight\',\'tbv\',\'group\')\nlist<-read.table(""parameters_groups"")\nne<-read.table(\'res\',h=T)\nlistA12<-read.table(\'grouplistA12\',sep=""/"",quote="""")\nlistA12<-data.frame(method=paste(listA12[,6],listA12[,5],sep=\'_\'),group=substr(listA12[,7],1,11))\ng<-0\ngroupA12<-0\nmethodA12<-0\nfor (i in 1:nrow(listA12)){\nif (listA12$group[[i]] == \'Gencont.001\'){g[[i]]<-100\n} else if (listA12$group[[i]] == \'Gencont.002\'){g[[i]]<-200\n} else if (listA12$group[[i]] == \'Gencont.003\'){g[[i]]<-500\n} else if (listA12$group[[i]] == \'Gencont.004\'){g[[i]]<-1000\n} else if (listA12$group[[i]] == \'Gencont.005\'){g[[i]]<-2000}\ngroupA12[[i]]<-g[[i]]\nmethodA12[[i]]<-paste(listA12$method[[i]])} \nlistA2<-read.table(\'grouplistA2\',sep=""/"",quote="""")\nlistA2<-data.frame(method=paste(listA2[,6],listA2[,5],sep=\'_\'),group=substr(listA2[,7],1,11))\ng<-0\ngroupA2<-0\nmethodA2<-0\nfor (i in 1:nrow(listA2)){\nif (listA2$group[[i]] == \'Gencont.001\'){g[[i]]<-100\n} else if (listA2$group[[i]] == \'Gencont.002\'){g[[i]]<-200\n} else if (listA2$group[[i]] == \'Gencont.003\'){g[[i]]<-500\n} else if (listA2$group[[i]] == \'Gencont.004\'){g[[i]]<-1000\n} else if (listA2$group[[i]] == \'Gencont.005\'){g[[i]]<-2000}\ngroupA2[[i]]<-g[[i]]\nmethodA2[[i]]<-paste(listA2$method[[i]])} \nn<-c(100,200,500,1000,2000)\ng<-list()\nfor (j in 1:length(n)){\ngrouprandom<-0\nfor (i in 1:100){\ngrouprandom[[i]]<-paste(\'grouprandom\',n[[j]],i,sep=\'_\')}\ng[[j]]<-grouprandom}\ngr<-c(unlist(g))\ngroup<-c(5969,groupA12,groupA2,100,200,500,1000,2000,rep(100,100),rep(200,100),rep(500,100),rep(1000,100),rep(2000,100))\nmethod<-c(\'A1\',methodA12,methodA2,rep(\'max\',5),rep(\'random\',500))\nNe<-ne[,15]\nnen<-ne[,16]\n\ng<-list()\ndataA12<-list()\nfor (i in 1:nrow(list)){\ng[[i]]<-read.table(paste(\'test_g\',i,sep=\'\'))\ncolnames(g[[i]])<-c(\'id\',\'ebv\',\'tbv\',\'inbA\',\'nb_het\')\ndataA12[[i]]<-cbind(method=rep(method[[i]],nrow(g[[i]])),group_size=rep(group[[i]],nrow(g[[i]])),g[[i]],Ne=Ne[[i]],NeN=nen[[i]])}\nd<-do.call(rbind,dataA12)\nd100<-subset(d,d$group_size==100)\nd100<-data.frame(d100$method,d100$group_size,d100$id,d100$ebv,d100$tbv,d100$inbA,d100$nb_het,d100$Ne,d100$NeN)\ncolnames(d100)<-c(\'method\',\'group_size\',\'id\',\'ebv\',\'tbv\',\'inba\',\'nb_het\',\'ne\',\'nen\')\nd200<-subset(d,d$group_size==200)\nd200<-data.frame(d200$method,d200$group_size,d200$id,d200$ebv,d200$tbv,d200$inbA,d200$nb_het,d200$Ne,d200$NeN)\ncolnames(d200)<-c(\'method\',\'group_size\',\'id\',\'ebv\',\'tbv\',\'inba\',\'nb_het\',\'ne\',\'nen\')\nd500<-subset(d,d$group_size==500)\nd500<-data.frame(d500$method,d500$group_size,d500$id,d500$ebv,d500$tbv,d500$inbA,d500$nb_het,d500$Ne,d500$NeN)\ncolnames(d500)<-c(\'method\',\'group_size\',\'id\',\'ebv\',\'tbv\',\'inba\',\'nb_het\',\'ne\',\'nen\')\nd1000<-subset(d,d$group_size==1000)\nd1000<-data.frame(d1000$method,d1000$group_size,d1000$id,d1000$ebv,d1000$tbv,d1000$inbA,d1000$nb_het,d1000$Ne,d1000$NeN)\ncolnames(d1000)<-c(\'method\',\'group_size\',\'id\',\'ebv\',\'tbv\',\'inba\',\'nb_het\',\'ne\',\'nen\')\nd2000<-subset(d,d$group_size==2000)\nd2000<-data.frame(d2000$method,d2000$group_size,d2000$id,d2000$ebv,d2000$tbv,d2000$inbA,d2000$nb_het,d2000$Ne,d2000$NeN)\ncolnames(d2000)<-c(\'method\',\'group_size\',\'id\',\'ebv\',\'tbv\',\'inba\',\'nb_het\',\'ne\',\'nen\')\ndt<-rbind(d100,d200,d500,d1000,d2000)\ncor<-read.table(\'RES_GS3_prod.txt\',header=T)\nparvce<-read.table(\'ParGlob_vce_prod.txt\')\nc<-list()\nrow<-list()\nmax<-0\ndataV<-list()\nprecis<-list()\ndataVn<-list()\nfor (i in 1:nrow(list)){\nrow[[i]]<-which(cor$car==paste(car,i,sep=\'\'))\nc[[i]]<-cor[row[[i]],]\nmax[[i]]<-which.max(c[[i]][,3])\ndataV[[i]]<-read.table(past', ""####### Script data description for simulated data set #######\nnbsimul<-50\n##### Random #####\nprecisVb_rand<-matrix(nrow=nbsimul,ncol=10)\nprecisVbm_rand<-0\ntbvb0_rand<-matrix(nrow=nbsimul,ncol=10)\ntbvb0m_rand<-0\nhetb0_rand<-matrix(nrow=nbsimul,ncol=10)\nhetb0m_rand<-0\ninbb0_rand<-matrix(nrow=nbsimul,ncol=10)\ninbb0m_rand<-0\nfor (j in 1:nbsimul){\n\tfor (i in 1:10){\n\tinb<-read.table(paste('Random/',j,'/res_saved/pedigree_inbr_coef_11_r_Random',j,'.dat',sep=''))\n\tcolnames(inb)<-c('Progeny','Inb')\n\tVb_tbv<-read.table(paste('Random/',j,'/res_saved/V_tbv_',i,'_b_r_Random',j,sep=''),h=T)\n\tVb_tbv<-Vb_tbv[-c(4,15)]\n\tVb_ebv<-read.table(paste('Random/',j,'/res_saved/V_ebvall_',i,'_b_r_Random',j,sep=''),h=T)\n\tcolnames(Vb_ebv)<-c('Progeny','Final_EBV')\n\tVb<-merge(Vb_tbv,Vb_ebv,by='Progeny')\n\tVb<-merge(Vb,inb,by='Progeny')\n\tb<-read.table(paste('Random/',j,'/Simul',i,'b_data_001.txt',sep=''),h=T)\n\tb<-merge(b,inb,by='Progeny')\n\tb0<-subset(b,b$G==0)\n\t#precision\n\tprecisVb_rand[j,i]<-mean(abs((Vb$Final_EBV-Vb$QTL)/sd(Vb$QTL)))\n\tprecisVbm_rand[[i]]<-mean(abs(precisVb_rand[,i]))\n\t#genetic gain\n\ttbvb0_rand[j,i]<-mean(b0$QTL)\n\ttbvb0m_rand[[i]]<-mean(tbvb0_rand[,i])\n\t#genetic diversity heterozygosity\n\thetb0_rand[j,i]<-mean(1-b0$Homo)\n\thetb0m_rand[[i]]<-mean(hetb0_rand[,i])\n\t#genetic diversity inbreeding\n\tinbb0_rand[j,i]<-mean(b0$Inb)\n\tinbb0m_rand[[i]]<-mean(inbb0_rand[,i])\n\t}\n}\n##### Truncation #####\nprecisVb_trunc<-matrix(nrow=nbsimul,ncol=10)\nprecisVbm_trunc<-0\ntbvb0_trunc<-matrix(nrow=nbsimul,ncol=10)\ntbvb0m_trunc<-0\nhetb0_trunc<-matrix(nrow=nbsimul,ncol=10)\nhetb0m_trunc<-0\ninbb0_trunc<-matrix(nrow=nbsimul,ncol=10)\ninbb0m_trunc<-0\nfor (j in 1:nbsimul){\n\tfor (i in 1:10){\n\tinb<-read.table(paste('Truncation/',j,'/res_saved/pedigree_inbr_coef_11_r_Truncation',j,'.dat',sep=''))\n\tcolnames(inb)<-c('Progeny','Inb')\n\tVb_tbv<-read.table(paste('Truncation/',j,'/res_saved/V_tbv_',i,'_b_r_Truncation',j,sep=''),h=T)\n\tVb_tbv<-Vb_tbv[-c(4,15)]\n\tVb_ebv<-read.table(paste('Truncation/',j,'/res_saved/V_ebvall_',i,'_b_r_Truncation',j,sep=''),h=T)\n\tcolnames(Vb_ebv)<-c('Progeny','Final_EBV')\n\tVb<-merge(Vb_tbv,Vb_ebv,by='Progeny')\n\tVb<-merge(Vb,inb,by='Progeny')\n\tb<-read.table(paste('Truncation/',j,'/Simul',i,'b_data_001.txt',sep=''),h=T)\n\tb<-merge(b,inb,by='Progeny')\n\tb0<-subset(b,b$G==0)\n\t#precision\n\tprecisVb_trunc[j,i]<-mean(abs((Vb$Final_EBV-Vb$QTL)/sd(Vb$QTL)))\n\tprecisVbm_trunc[[i]]<-mean(abs(precisVb_trunc[,i]))\n\t#genetic gain\n\ttbvb0_trunc[j,i]<-mean(b0$QTL)\n\ttbvb0m_trunc[[i]]<-mean(tbvb0_trunc[,i])\n\t#genetic diversity heterozygosity\n\thetb0_trunc[j,i]<-mean(1-b0$Homo)\n\thetb0m_trunc[[i]]<-mean(hetb0_trunc[,i])\n\t#genetic diversity inbreeding\n\tinbb0_trunc[j,i]<-mean(b0$Inb)\n\tinbb0m_trunc[[i]]<-mean(inbb0_trunc[,i])\n\t}\n}\n##### OC #####\nprecisVb_oc<-matrix(nrow=nbsimul,ncol=10)\nprecisVbm_oc<-0\ntbvb0_oc<-matrix(nrow=nbsimul,ncol=10)\ntbvb0m_oc<-0\nhetb0_oc<-matrix(nrow=nbsimul,ncol=10)\nhetb0m_oc<-0\ninbb0_oc<-matrix(nrow=nbsimul,ncol=10)\ninbb0m_oc<-0\nfor (j in 1:nbsimul){\n\tfor (i in 1:10){\n\tinb<-read.table(paste('OC/',j,'/res_saved/pedigree_inbr_coef_11_r_OC',j,'.dat',sep=''))\n\tcolnames(inb)<-c('Progeny','Inb')\n\tVb_tbv<-read.table(paste('OC/',j,'/res_saved/V_tbv_',i,'_b_r_OC',j,sep=''),h=T)\n\tVb_tbv<-Vb_tbv[-c(4,15)]\n\tVb_ebv<-read.table(paste('OC/',j,'/res_saved/V_ebvall_',i,'_b_r_OC',j,sep=''),h=T)\n\tcolnames(Vb_ebv)<-c('Progeny','Final_EBV')\n\tVb<-merge(Vb_tbv,Vb_ebv,by='Progeny')\n\tVb<-merge(Vb,inb,by='Progeny')\n\tb<-read.table(paste('OC/',j,'/Simul',i,'b_data_001.txt',sep=''),h=T)\n\tb<-merge(b,inb,by='Progeny')\n\tb0<-subset(b,b$G==0)\n\tb1<-subset(b,b$G==1)\n\t#precision\n\tprecisVb_oc[j,i]<-mean(abs((Vb$Final_EBV-Vb$QTL)/sd(Vb$QTL)))\n\tprecisVbm_oc[[i]]<-mean(abs(precisVb_oc[,i]))\n\t#genetic gain\n\ttbvb0_oc[j,i]<-mean(b0$QTL)\n\ttbvb0m_oc[[i]]<-mean(tbvb0_oc[,i])\n\t#genetic diversity heterozygosity\n\thetb0_oc[j,i]<-mean(1-b0$Homo)\n\thetb0m_oc[[i]]<-mean(hetb0_oc[,i])\n\t#genetic diversity inbreeding\n\tinbb0_oc[j,i]<-mean(b0$Inb)\n\tinbb0m_oc[[i]]<-mean(inbb0_oc[,i])\n\t}\n}\n\nsdtbv_rand<-0\nsdtbv_trunc<-0\nsdtbv_oc<-0\nfor (i in 1:10){\nsdtbv_rand[[i]]<-sd(tbvb0_rand[,i])/sqrt(length(tbvb0_rand[,i]))\nsdtbv_trunc[[i]]<-sd(tbvb0_trunc[,i])/sqrt(length(tbvb0_trunc[,i]))\nsdtbv_oc[[i]]<-sd(tbvb0_oc[,i])/sqrt(length(tbvb0_oc[,i]))}\nmintbvb<-min(tbvb0m_rand-sdtbv_rand,tbvb0m_trunc-sdtbv_trunc,tbvb0m_oc-sdtbv_oc)\nmaxtbvb<-max(tbvb0m_rand+sdtbv_rand,tbvb0m_trunc+sdtbv_trunc,tbvb0m_oc+sdtbv_oc)\nsdprecis_rand<-0\nsdprecis_trunc<-0\nsdprecis_oc<-0\nfor (i in 1:10){\nsdprecis_rand[[i]]<-sd(abs(precisVb_rand[,i]))/sqrt(length(precisVb_rand[,i]))\nsdprecis_trunc[[i]]<-sd(abs(precisVb_trunc[,i]))/sqrt(length(precisVb_trunc[,i]))\nsdprecis_oc[[i]]<-sd(abs(precisVb_oc[,i]))/sqrt(length(precisVb_oc[,i]))}\nminprecisb<-min(abs(precisVbm_rand)-sdprecis_rand,abs(precisVbm_trunc)-sdprecis_trunc,abs(precisVbm_oc)-sdprecis_oc)\nmaxprecisb<-max(abs(precisVbm_rand)+sdprecis_rand,abs(precisVbm_trunc)+sdprecis_trunc,abs(precisVbm_oc)+sdprecis_oc)\nsdhet_rand<-0\nsdhet_trunc<-0\nsdhet_oc<-0\nfor (i in 1:10){\nsdhet_rand[[i]]<-s"", ""##### This script will create the seed.txt file that will be used in the Simul.prm script. The advantage of writing your own seed instead of letting QMSim write a seed itself is that you can control that replicates of your experiment start with the same seed and therefore can be comparible #####\nRND<-matrix(round(runif(8*78,0,1000000000),digits=0),nrow=78,ncol=8)\nRND<-as.data.frame(RND)\ncolnames(RND)<-c('State vector for MT19937:','','','','','','','')\nwrite.table(RND,'seed.txt',row.names=F,quote=F)\n"", ""####### Post-hoc analysis real data set #######\nlibrary(lsmeans)\nlibrary(fmsb)\nlibrary(car)\nlibrary(MASS)\ndt_real<-read.table('dt_real',h=T)\ndvt_real<-read.table('dvt_real',h=T)\n##### tests\n#tbv\nmtbv<-lm(tbv~(method+as.factor(group_size))^2+nen,data=dt_real,contrasts=list(method=contr.sum))\nqqnorm(mtbv$residuals)\nhist(mtbv$residuals)\nsummary(mtbv)\nAnova(mtbv,type=2)\nlstbv<-lsmeans(mtbv,list(pairwise~method|as.factor(group_size)))\nlstbv\ncld(lstbv)\n#het\nmhet<-lm(asin(sqrt(het/43801))~(method+as.factor(group_size))^2+nen,data=dt_real,contrasts=list(method=contr.sum))\nqqnorm(mhet$residuals)\nhist(mhet$residuals)\nsummary(mhet)\nAnova(mhet,type=2)\nlshet<-lsmeans(mhet,list(pairwise~method|as.factor(group_size)))\nlshet\ncld(lshet)\n#inb\nminb<-lm(asin(sqrt(inba))~(method+as.factor(group_size))^2+nen,data=dt_real,contrasts=list(method=contr.sum))\nqqnorm(minb$residuals)\nhist(minb$residuals)\nsummary(minb)\nAnova(minb,type=2)\nlsinb<-lsmeans(minb,list(pairwise~method|as.factor(group_size)))\nlsinb\ncld(lsinb)\n#precision\nmprecis<-lm(precision~(method+as.factor(group_size))^2+nen,data=dvt_real,contrasts=list(method=contr.sum))\nqqnorm(mprecis$residuals)\nhist(mprecis$residuals)\nsummary(mprecis)\nAnova(mprecis,type=2)\nlsprecis3<-lsmeans(mprecis,list(pairwise~method|as.factor(group_size)))\nlsprecis3\ncld(lsprecis3)\n\n"", '####### Post-hoc analysis simulated data set #######\nlibrary(car)\nlibrary(lsmeans)\nlibrary(MASS)\nlibrary(lme4)\nlibrary(MuMIn)\ndt_simul<-read.table(\'dt_simul\',h=T)\ndt_simul$NeN<-scale(dt_simul$NeN)\ndvt_simul<-read.table(\'dvt_simul\',h=T)\ndvt_simul$NeN<-scale(dvt_simul$NeN)\n##### tests \n#tbv\nmtbv<-lmer(tbv~(Method+Generation)^2+NeN+(1|Sim_id),data=dt_simul,contrasts=list(Method=contr.sum))\nqqnorm(resid(mtbv))\nhist(resid(mtbv))\nsummary(mtbv)\nAnova(mtbv,type=2)\nlstbv1<-lsmeans(mtbv,pairwise~Method)\nlstbv1\ncld(lstbv1)\nlstbv2<-lsmeans(mtbv,pairwise~Method|Generation,at=list(Generation=c(1,2,3,4,5,6,7,8,9,10)))\nlstbv2\ncld(lstbv2)\nlttbv<-lstrends(mtbv,~Method,var=""Generation"")\nlttbv\ncld(lttbv)\n#het\nmhet<-lmer(asin(sqrt(het))~(Method+Generation)^2+NeN+(1|Sim_id),data=dt_simul,contrasts=list(Method=contr.sum))\nqqnorm(resid(mhet))\nhist(resid(mhet))\nsummary(mhet)\nAnova(mhet,type=2)\nlshet1<-lsmeans(mhet,~Method)\nlshet1\ncld(lshet1)\nlshet2<-lsmeans(mhet,pairwise~Method|Generation,at=list(Generation=c(1,2,3,4,5,6,7,8,9,10)))\nlshet2\ncld(lshet2)\nlthet<-lstrends(mhet,~Method,var=""Generation"")\nlthet\ncld(lthet)\n#inb\nminb<-lmer(asin(sqrt(inb))~(Method+Generation)^2+NeN+(1|Sim_id),data=dt_simul,contrasts=list(Method=contr.sum))\nqqnorm(resid(minb))\nhist(resid(minb))\nsummary(minb)\nAnova(minb,type=2)\nlsinb11<-lsmeans(minb,pairwise~Method,at=(Generation=1:10))\nlsinb11\ncld(lsinb11)\nlsinb2<-lsmeans(minb,pairwise~Method|Generation,at=list(Generation=c(1,2,3,4,5,6,7,8,9,10)))\nlsinb2\ncld(lsinb2)\nltinb<-lstrends(minb,~Method,var=""Generation"")\nltinb\ncld(ltinb)\n#precision\nmprecis<-lmer(perror~(Method+Generation)^2+NeN+(1|Sim_id),data=dvt_simul,contrasts=list(Method=contr.sum))\nqqnorm(resid(mprecis))\nhist(resid(mprecis))\nsummary(mprecis)\nAnova(mprecis,type=2)\nlsprecis1<-lsmeans(mprecis,pairwise~Method)\nlsprecis1\ncld(lsprecis1)\nlsprecis2<-lsmeans(mprecis,pairwise~Method|Generation,at=list(Generation=c(1,2,3,4,5,6,7,8,9,10)))\nlsprecis2\ncld(lsprecis2)\nltprecis<-lstrends(mprecis,~Method,var=""Generation"")\nltprecis\ncld(ltprecis)\n\n\n\n']",1,"pollinator declines, native bee ecology, Midwestern US agriculture, crop rotation, fallow growing season, cover crop treatments, crimson clover, red clover, ladino clover, Bob oats, season-long floral resources, bee diversity,"
Efficacy of cover crops for pollinator habitat provision and weed suppression,"Pollinator declines have been documented globally, but little information is available about native bee ecology in Midwestern US agriculture. This project seeks to optimize pollinator support and weed suppression in a 3-year crop rotation with a fallow growing season. During fallow, one of five cover crop treatments (T1: crimson, red, and ladino clover and Bob oats [Trifolium incarnatum, T. pratense, T. repens, Avena sativa]; T2: crimson clover and oats; T3: red clover and oats; T4: ladino clover and oats; T5: no cover crop; T6/control: winter wheat [Triticum aestivum]) was seeded in one-half of 25 agricultural fields, while wheat was left unharvested in the other half as a comparison. Treatments that provide season-long floral resources support the greatest bee diversity and abundance (T1), and treatments with red clover support declining Bombus species (T1 and T3). Late-season floral resources may be important, yet limited (T1 and T4), and some species of agricultural weeds provide floral resources. Floral diversity may be less important than flower abundance or timing for pollinator diversity (T1  T4). Weed diversity was greatest in the no cover crop treatment (T5), least in winter wheat (T6), and intermediate in cover crop treatments (T1  T4) with no differences in weeds of economic concern. Wheat suppresses weeds but does not provide floral resources for pollinators. These results may also be applicable to marginal lands taken out of cultivation or field margin pollinator plantings in a typical corn-soybean rotation. Floral resource availability across the landscape is critical to maintain pollinator diversity.","['library(vegan)\n\nBenv.univ<-read.csv(""Bee ENV.csv"", header=TRUE)\nhead(Benv.univ)\n\nBenv.univ.net<-subset(Benv.univ, Method == ""Net"")\nhead(Benv.univ.net)\nBenv.univ.trap<-subset(Benv.univ, Method == ""Trap"")\nhead(Benv.univ.trap)\n\nBenv<-read.csv(""Bee ENV remove0_1sremoved.csv"", header=TRUE)\nBcomm<-read.table(""Bee comm_1sremoved.txt"", header=TRUE, row.names=1, sep=""\t"")\n\nhead(Benv)\n\nBenv$Treatment<-factor(Benv$Treatment, levels=c(""1"", ""2"", ""3"",""4"",""5"",""6""))\nBenv.univ$Treatment<-factor(Benv.univ$Treatment, levels=c(""1"", ""2"", ""3"",""4"",""5"",""6""))\n\nhead(Benv.univ)\n\n#True diversity\nspecpool(Bcomm)\n106/131.9*100\n\n\n#blocking by round:Field\nBblock<-interaction(Benv$Field,Benv$Round)\n\nadonis(Bcomm~Benv$Treatment,  permutation=999, method=""bray"", strata= Bblock,parallel\xa0=\xa0getOption(""mc.cores""), na.rm=TRUE, by=""margin"")\n\nadonis(Bcomm~Benv$CorT,  permutation=999, method=""bray"", strata= Bblock,parallel\xa0=\xa0getOption(""mc.cores""), na.rm=TRUE, by=""margin"")\n\nBdis<-vegdist(Bcomm, method=""bray"")\nanova(betadisper(Bdis, Benv$Treatment, type=""centroid""))\n\n\nd1<-metaMDS(Bcomm, distance=""bray"", k=1, parallel\xa0=\xa0getOption(""mc.cores""),trymax=500, trace=1, noshare=0.5)\nd2<-metaMDS(Bcomm, distance=""bray"", k=2, parallel\xa0=\xa0getOption(""mc.cores""),trymax=200, trace=1, noshare=0.5)\nd3<-metaMDS(Bcomm, distance=""bray"", k=3, parallel\xa0=\xa0getOption(""mc.cores""),trymax=200, trace=1, noshare=0.5)\nd4<-metaMDS(Bcomm, distance=""bray"", k=4, parallel\xa0=\xa0getOption(""mc.cores""),trymax=500, trace=1, noshare=0.5)\n#all abundances of 1 dropped\n\n\nd3$stress\n#3d plot converged, noshare=0.5, stress=0.2177379\nOneAndThree<-scores(d3)\n\n\nhead(Benv)\nbeevec<-envfit(d3,Benv[,c(14,15,18)], p.max=0.05)\nbeevec\n\n\ntiff(file = ""BEE FA vector.tif"",width = 3000, height = 9000, units = ""px"", res = 600) \npar(mfrow=c(3,1))\nordiplot(d3,choices=c(1,2), type=""none"", display=""sites"")\npoints(OneAndThree[c(18:22,162:166,208:213,240:245,252:257),c(1,2)],cex=1.0, col=""gold"", pch=16)\npoints(OneAndThree[c(6:10,29:34,41:46,174:179,185:189),c(1,2)],cex=1.0, col=""purple"", pch=16)\npoints(OneAndThree[c(53:58,74:78,86:90,151:155,276:281),c(1,2)],cex=1.0, col=""red"", pch=16)\npoints(OneAndThree[c(64:68,107:112,119:124,196:201,219:224),c(1,2)],cex=1.0, col=""darkorange"", pch=16)\npoints(OneAndThree[c(96:101,128:132,139:144,230:234,264:269),c(1,2)],cex=1.0, col=""green4"", pch=16)\npoints(OneAndThree[c(1:5,11:16,23:28,35:40,47:52,59:63,69:73,79:84,91:95,102:106,113:118,125:127,132:138,145:150,156:161,168:173,180:184,190:195,202:207,214:218,225:229,235:239,246:252,258:263,270:275),c(1,2)],cex=1.0, col=""deepskyblue"", pch=16)\nplot(beevec,asp=1,col=""black"",p.max=0.05)\nlegend(""topleft"", legend=c(""T1"", ""T2"", ""T3"", ""T4"", ""T5"", ""T6""), pch=c(16,16,16,16,16,16), col=c(""gold"", ""purple"", ""red"", ""darkorange"",""green4"", ""deepskyblue""), ncol=2,bty=\'n\')\nlegend(""topright"",legend=""stress = 0.22"", bty=\'n\')\n\nordiplot(d3,choices=c(1,3), type=""none"", display=""sites"")\npoints(OneAndThree[c(18:22,162:166,208:213,240:245,252:257),c(1,3)],cex=1.0, col=""gold"", pch=16)\npoints(OneAndThree[c(6:10,29:34,41:46,174:179,185:189),c(1,3)],cex=1.0, col=""purple"", pch=16)\npoints(OneAndThree[c(53:58,74:78,86:90,151:155,276:281),c(1,3)],cex=1.0, col=""red"", pch=16)\npoints(OneAndThree[c(64:68,107:112,119:124,196:201,219:224),c(1,3)],cex=1.0, col=""darkorange"", pch=16)\npoints(OneAndThree[c(96:101,128:132,139:144,230:234,264:269),c(1,3)],cex=1.0, col=""green4"", pch=16)\npoints(OneAndThree[c(1:5,11:16,23:28,35:40,47:52,59:63,69:73,79:84,91:95,102:106,113:118,125:127,132:138,145:150,156:161,168:173,180:184,190:195,202:207,214:218,225:229,235:239,246:252,258:263,270:275),c(1,2)],cex=1.0, col=""deepskyblue"", pch=16)\nplot(beevec,asp=1,col=""black"",p.max=0.05)\nlegend(""topleft"", legend=c(""T1"", ""T2"", ""T3"", ""T4"", ""T5"", ""T6""), pch=c(16,16,16,16,16,16), col=c(""gold"", ""purple"", ""red"", ""darkorange"",""green4"", ""deepskyblue""), ncol=2,bty=\'n\')\nlegend(""topright"",legend=""stress = 0.22"", bty=\'n\')\n\nordiplot(d3,choices=c(2,3), type=""none"", display=""sites"")\npoints(OneAndThree[c(18:22,162:166,208:213,240:245,252:257),c(2,3)],cex=1.0, col=""gold"", pch=16)\npoints(OneAndThree[c(6:10,29:34,41:46,174:179,185:189),c(2,3)],cex=1.0, col=""purple"", pch=16)\npoints(OneAndThree[c(53:58,74:78,86:90,151:155,276:281),c(2,3)],cex=1.0, col=""red"", pch=16)\npoints(OneAndThree[c(64:68,107:112,119:124,196:201,219:224),c(2,3)],cex=1.0, col=""darkorange"", pch=16)\npoints(OneAndThree[c(96:101,128:132,139:144,230:234,264:269),c(2,3)],cex=1.0, col=""green4"", pch=16)\npoints(OneAndThree[c(1:5,11:16,23:28,35:40,47:52,59:63,69:73,79:84,91:95,102:106,113:118,125:127,132:138,145:150,156:161,168:173,180:184,190:195,202:207,214:218,225:229,235:239,246:252,258:263,270:275),c(1,2)],cex=1.0, col=""deepskyblue"", pch=16)\nplot(beevec,asp=1,col=""black"",p.max=0.05)\nlegend(""topleft"", legend=c(""T1"", ""T2"", ""T3"", ""T4"", ""T5"", ""T6""), pch=c(16,16,16,16,16,16), col=c(""gold"", ""purple"", ""red"", ""darkorange"",""green4"", ""deepskyblue""), ncol=2,bty=\'n\')\nlegend(""topright"",legend=""stress = 0.22"", bty=\'n\')\n\ndev.off()\n\n\n#pairwise.adonis from Pedro Martinez Arbizu; ', 'FA<-read.csv(""Floral Abund.csv"")\nhead(FA)\n\nlibrary(emmeans)\nlibrary(multcompView)\nlibrary(nlme)\n\nFA$Treatment<-factor(FA$Treatment)\nFA$Round<-factor(FA$Round)\n\n\n\nabund<-lme(log(N+1)~Treatment*Round,random = ~1|Field,cor= corCompSymm(form = ~1|Field/Round), data= FA, method=""REML"")\nplot(abund)\nanova(abund, type=""marginal"")\n\nabundem<-emmeans(abund, ~Treatment|Round)\nabund.contrasts<-contrast(abundem, type=""response"", adjust=""tukey"")\nmultcomp::cld(abund.contrasts)\nabundem\n\nFA1<-subset(FA, Round==""1"")\nFA2<-subset(FA, Round==""2"")\nFA3<-subset(FA, Round==""3"")\nFA4<-subset(FA, Round==""4"")\nFA5<-subset(FA, Round==""5"")\nFA6<-subset(FA, Round==""6"")\n\n\n\n#round 3: ab, ab, ab, b, ab, a\ntiff(file = ""Fig 2 EnvEnt.tif"",width = 3500*3, height = 3500*2, units = ""px"", res = 1200) \npar(mfrow=c(2,3))\nboxplot(log(N+1)~Treatment, data= FA1, main=""Round 1"", ylim=c(0,10.9), ylab= ""Floral Abundance (Log Scale)"", xlab=""Treatment"", cex.lab=1.4, cex.main=1.5)\ntext(1:6, 10, ""a"", cex=1.2)\nmtext(side=1, line= 5.5, expression(""Round*Treatment ""*italic(F)[""25, 240""]*"" = 1.56, ""*italic(P)*"" = 0.049""), cex=1.2, adj=0)\npoints(1:6, c(7.141519, 7.087142, 6.503991, 6.561617, 6.339931, 5.946210), pch=16, cex=1.2)\n\nboxplot(log(N+1)~Treatment, data= FA2, main=""Round 2"", ylim=c(0,10.9), ylab= ""Floral Abundance (Log Scale)"", xlab=""Treatment"", cex.lab=1.4, cex.main=1.5)\ntext(1:6, 10, ""a"", cex=1.2)\npoints(1:6, c(6.554827, 6.061587, 6.005516, 5.789004, 6.180540, 5.711410), pch=16, cex=1.2)\n\nboxplot(log(N+1)~Treatment, data= FA3, main=""Round 3"", ylim=c(-2,10.9), ylab= ""Floral Abundance (Log Scale)"", xlab=""Treatment"", cex.lab=1.4, cex.main=1.5)\ntext(1:6, 10, c(""ab"", ""ab"", ""ab"", ""b"", ""ab"", ""a""), cex=1.2)\npoints(1:6, c(2.534305, 1.360194, 1.456189, 4.140508, 2.069852, 1.634280), pch=16, cex=1.2)\n\nboxplot(log(N+1)~Treatment, data= FA4, main=""Round 4"", ylim=c(0,10.9), ylab= ""Floral Abundance (Log Scale)"", xlab=""Treatment"", cex.lab=1.4, cex.main=1.5)\ntext(1:6, 10, ""a"", cex=1.2)\npoints(1:6, c(6.014199, 3.597484, 5.937193, 5.746428, 4.585431, 4.217498), pch=16, cex=1.2)\n\nboxplot(log(N+1)~Treatment, data= FA5, main=""Round 5"", ylim=c(0,10.9), ylab= ""Floral Abundance (Log Scale)"", xlab=""Treatment"", cex.lab=1.4, cex.main=1.5)\ntext(1:6, 10, ""a"", cex=1.2)\npoints(1:6, c(4.424330, 4.087755, 6.917743, 6.165310, 4.545273, 4.840448), pch=16, cex=1.2)\n\nboxplot(log(N+1)~Treatment, data= FA6, main=""Round 6"", ylim=c(0,10.9), ylab= ""Floral Abundance (Log Scale)"", xlab=""Treatment"", cex.lab=1.4, cex.main=1.5)\ntext(1:6, 10, ""a"", cex=1.2)\npoints(1:6, c(5.417823, 4.018722, 5.448540, 4.020602, 5.320057, 5.433115), pch=16, cex=1.2)\n\ndev.off()\n\n\n\n\nabundS<-lme(S~Treatment*Round,random = ~1|Field,cor= corCompSymm(form = ~1|Field/Round), data= FA, method=""REML"")\nanova(abundS, type=""marginal"")\n\n\n\nabundSR.lsm<-emmeans(abundS, specs=""Round"")\nabundSR.contrasts<-contrast(abundSR.lsm, type=""response"", adjust=""Tukey"")\nmultcomp::cld(abundSR.contrasts)\nabundSR.lsm\n\n#c, c, a, b, b, b\ntiff(file = ""FA Floral Richness Round int.tif"",width = 3500, height = 3500, units = ""px"", res = 600) \nboxplot(S~Treatment, data=FA, xlab=""Round"", ylab=""Floral Richness"", ylim=c(0,13))\ntext(1:6, 11.5, c(""c"", ""c"", ""a"", ""b"", ""b"", ""b""))\ntext(2, 12.5, expression(italic(F)[""5, 240""]*"" = 9.47, ""*italic(P)*"" < 0.001""))\npoints(c(1,2,3,4,5,6), c(6.693333,6.593333,1.780000,3.460000,3.186667,3.726667), pch=16)\n\ndev.off()\n\n\n\n\nabundH<-lme(e.H.~Treatment*Round,random = ~1|Field,cor= corCompSymm(form = ~1|Field/Round), data= FA, method=""REML"")\nanova(abundH, type=""marginal"")\n\n\n\nabundH.lsm<-emmeans(abundH, specs=""Treatment"")\nabundH.contrasts<-contrast(abundH.lsm, type=""response"", adjust=""Tukey"")\nmultcomp::cld(abundH.contrasts)\nabundH.lsm\n\nfloralS.table<-as.matrix(summary(abundSR.lsm))\nfloral.eH.table<-as.matrix(summary(abundH.lsm))\n\nwrite.csv(floralS.table, file=""Floral Richness Table.csv"")\nwrite.csv(floral.eH.table, file=""Floral Diversity Table.csv"")\n\n\n#ab, ab, a, ab, b, ab\ntiff(file = ""/Users/Drew/FA Floral Diversity ttt int.tif"",width = 3500, height = 3500, units = ""px"", res = 600) \nboxplot(e.H.~Treatment, data=FA, xlab=""Treatment"", ylab=""Floral Effective Number of Species"", ylim=c(0,6.5), yaxs=""i"")\ntext(1:6, 5.5, c(""a"", ""ab"", ""a"", ""ab"", ""b"", ""ab""))\ntext(2, 6, expression(italic(F)[""5, 240""]*"" = 1.22, ""*italic(P)*"" = 0.020""))\npoints(c(1,2,3,4,5,6), c(1.993671,1.818111, 1.465825, 1.941493, 2.112274, 1.860864), pch=16)\n\ndev.off()\n\n\n\n\nFA$FloralJ<-FA$H..loge./log(FA$S)\nFA$FloralJ[is.na(FA$FloralJ)] <- 0\n\n\nfloraJ.mod<-lme(FloralJ~Treatment*Round,random = ~1|Field,cor= corCompSymm(form = ~1|Field/Round), data= FA, method=""REML"")\nsummary(floraJ.mod)\n\nplot(floraJ.mod)\nanova(floraJ.mod, type=""marginal"")\n\n\n\n\ntiff(file = ""Fig 3 EnvEnt.tif"",width = 4000*2, height = 4000*2, units = ""px"", res = 1200) \npar(mfrow=c(2,2))\n\nboxplot(S~Treatment, data=FA, xlab=""Round"", ylab=""Floral Richness"", ylim=c(0,14))\ntext(1:6, 11.5, c(""c"", ""c"", ""a"", ""b"", ""b"", ""b""))\nlegend(""top"", expression(italic(F)[""5, 240""]*"" = 9.47, ""*italic(P)*"" < 0.001""),bty=\'n\')\npoints(c(1,2,3,4,5,', 'library(vegan)\n\n\nenv<-read.csv(""Weeds 2017 ENV.csv"", header=TRUE)\ncomm<-read.table(""Weeds 2017.txt"", header=TRUE, row.names=1)\n\n\n#treatment species % cover removed; average\n\n#coverts treatment to a factor\nenv$Treatment<-factor(env$Treatment, levels=c(""1"", ""2"", ""3"",""4"",""5"",""6""))\n\n\n#blocking by round\n\nblock<-interaction(env$Field,env$Round)\n\nadonis(comm~env$Treatment,  permutation=999, method=""bray"", strata= block,parallel\xa0=\xa0getOption(""mc.cores""), na.rm=TRUE, by=""margin"")\n\nadonis(comm~env$CorT,  permutation=999, method=""bray"", strata= block,parallel\xa0=\xa0getOption(""mc.cores""), na.rm=TRUE, by=""margin"")\n\n\ndis<-vegdist(comm, method=""bray"")\nanova(betadisper(dis, env$Treatment, type=""centroid""))\n\n\n\n\n#pairwise.adonis from Pedro Martinez Arbizu; I added na.rm=TRUE \npairwise.adonis <- function(x,factors, sim.function = \'vegdist\', sim.method = \'bray\', p.adjust.m =\'bonferroni\')\n{\nlibrary(vegan)\n\nco = combn(unique(as.character(factors)),2)\npairs = c()\nF.Model =c()\nR2 = c()\np.value = c()\n\n\nfor(elem in 1:ncol(co)){\nif(sim.function == \'daisy\'){\nlibrary(cluster); x1 = daisy(x[factors %in% c(co[1,elem],co[2,elem]),],metric=sim.method)\n} else{x1 = vegdist(x[factors %in% c(co[1,elem],co[2,elem]),],method=sim.method)}\n\nad = adonis(x1 ~ factors[factors %in% c(co[1,elem],co[2,elem])] );\npairs = c(pairs,paste(co[1,elem],\'vs\',co[2,elem]));\nF.Model =c(F.Model,ad$aov.tab[1,4]);\nR2 = c(R2,ad$aov.tab[1,5]);\np.value = c(p.value,ad$aov.tab[1,6])\n}\np.adjusted = p.adjust(p.value,method=p.adjust.m)\nsig = c(rep(\'\',length(p.adjusted)))\nsig[p.adjusted <= 0.05] <-\'.\'\nsig[p.adjusted <= 0.01] <-\'*\'\nsig[p.adjusted <= 0.001] <-\'**\'\nsig[p.adjusted <= 0.0001] <-\'***\'\n\npairw.res = data.frame(pairs,F.Model,R2,p.value,p.adjusted,sig)\nprint(""Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"")\nreturn(pairw.res)\n\n} \n\n\n#default adjustment is bonferroni. Less conservative corrections are also included by Holm (1979) (""holm""), Hochberg (1988) (""hochberg""), Hommel (1988) (""hommel""), Benjamini & Hochberg (1995) (""BH""\xa0or its alias\xa0""fdr""), and Benjamini & Yekutieli (2001) (""BY""), respectively. A pass-through option (""none"") is also included.\xa0 \n\n#no multiple comparison corection\npairwise.adonis(comm, env$Treatment, p.adjust.m=""none"")\n\n#still a few significant differences with multiple comparison correction\npairwise.adonis(comm, env$Treatment, p.adjust.m=""hommel"")\n\n\n#Ordination\n\nd1<-metaMDS(comm, distance=""bray"", k=1, parallel\xa0=\xa0getOption(""mc.cores""),trymax=3000)\nd2<-metaMDS(comm, distance=""bray"", k=2, parallel\xa0=\xa0getOption(""mc.cores""),trymax=3000)\nd3<-metaMDS(comm, distance=""bray"", k=3, parallel\xa0=\xa0getOption(""mc.cores""),trymax=3000)\nd4<-metaMDS(comm, distance=""bray"", k=4, parallel\xa0=\xa0getOption(""mc.cores""),trymax=3000)\n\n\nplot(c(1,2,3,4),c(d1$stress, d2$stress, d3$stress, d4$stress))\nd2$stress\n\n#3d didnt show any more seperation than 2d\n#might want to use rgl package to make 3d plots in future bc it has an ellipsoid function\n\npoint<-scores(d2)\n\n#add ordicenter function; like ordispider but only centroids displayed\nsource (\'http://www.davidzeleny.net/anadat-r/doku.php/en:customized_functions:ordicenter?do=export_code&codeblock=0\')\n\n#vector fitting\nhead(env)\n\nweeddiv<-envfit(d2,env[,c(8,9,11)], p.max=0.05)\nprint(weeddiv)\n\nflorassdiv<-envfit(d2,env[,c(12,13,15)], p.max=0.05)\nprint(florassdiv)\n\n#NMDS Ordinations with Weed Div vectors\n\ntiff(file = ""Weed Div vector.tif"",width = 3500, height = 3500, units = ""px"", res = 600) \nordiplot(d2, choices=c(1,2), type=""none"", yaxt=\'n\', xaxt=\'n\')\npoints(point[,1],point[,2],pch=16, col=c(""deepskyblue"",""deepskyblue"",""deepskyblue"",""purple"",""purple"",""purple"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""gold"",""gold"",""gold"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""purple"",""purple"",""purple"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""purple"",""purple"",""purple"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""red"",""red"",""red"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""darkorange"",""darkorange"",""darkorange"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""red"",""red"",""red"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""red"",""red"",""red"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""green4"",""green4"",""green4"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""darkorange"",""darkorange"",""darkorange"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""darkorange"",""darkorange"",""darkorange"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""green4"",""green4"",""green4"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""green4"",""green4"",""green4"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""red"",""red"",""red"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""gold"",""gold"",""gold"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""purple"",""purple"",""purple"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""purple"",""purple"",""purple"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""darkorange"",""darkorange"",""darkorange"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""gold"",""gold"",""gold"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""darkorange"",""darkorange"",""darkorange"",""deepskyblue"",""deepskyblue"",""deepskyblue"",""green4"",""green4"",""green4"",""deepskyblue""']",1,"Alien insect dispersal, globalization, economic growth, biological invasions, new introductions, movement of people, movement of goods, imported cargo, arriving passengers, interceptions, air ports, land ports, maritime ports, Australia, New Zealand,"
Alien insect dispersal mediated by the global movement of commodities,"Globalization and economic growth are recognized as key drivers of biological invasions. Alien species have become a feature of almost every biological community worldwide, and rates of new introductions continue to rise as the movement of people and goods accelerates. Insects are among the most numerous and problematic alien organisms, and are mainly introduced unintentionally with imported cargo or arriving passengers. However, the processes occurring prior to insect introductions remain poorly understood. We used a unique dataset of 1,902,392 border interception records from inspections at air, land and maritime ports in Australia, New Zealand, Europe, Japan, the United States of America and Canada to identify key commodities associated with insect movement through trade and travel. A total of 8,939 species were intercepted, and commodity association data were available for 1,242 species recorded between 1960 and 2019. We used rarefaction and extrapolation methods to estimate the total species richness and diversity associated with different commodity types. Plant and wood products were the main commodities associated with insect movement across cargo, passenger baggage and international mail. Furthermore, certain species were mainly associated with specific commodities within these, and other broad categories. More closely related species tended to share similar commodity associations, but this occurred largely at the genus level rather than within orders or families. These similarities within genera can potentially inform pathway management of new alien species. Combining interception records across regions provides a unique window into the unintentional movement of insects, and provides valuable information on establishment risks associated with different commodity types and pathways.","['# ----- Load libraries ----- \n# install.packages(""readr"")\nlibrary(readr) # Loading csv files\n# install.packages(""vegan"")\nlibrary(vegan) # PERMANOVA and CCA analyses\n# install.packages(""iNEXT"")\nlibrary(iNEXT) # Rarefaction and Extrapolation methods\n# install.packages(""ggplot2"")\nlibrary(ggplot2) # Create figures\n# install.packages(""ggpubr"")\nlibrary(ggpubr) # Arrange multiple figures\n# install.packages(""ade4"")\nlibrary(ade4) # Correspondence analysis\n# install.packages(""factoextra"")\nlibrary(factoextra) # Plot ordination output\n# install.packages(""cluster"")\nlibrary(cluster) # Hierarchical agglomerative clustering\n# install.packages(""ape"")\nlibrary(ape) # Load and edit Newick trees\n# install.packages(""phylobase"")\nlibrary(phylobase) # Create phylo4d objects\n# install.packages(""phylosignal"")\nlibrary(phylosignal) # Calculate phylogenetic signal\n\n\n####################################################################################################\n######    Pooling interception records across regions: partial Constrained Correspondence    #######\n######    Analysis (CCA) of species\' commodity profiles in each region                       #######\n####################################################################################################\n\n# ------ Import data ------\n# The proportion of interceptions per HS-2 commodity in each region, for the 59 species intercepted more than 20 times in two or more regions\nspe_profiles_allregions <- read.csv(""spe_profiles_allregions.csv"", row.names = ""species"")\n\n# The species and region each species-by-region commodity profile belongs to\nspecies_region<- read.csv(""species_region.csv"")\n\n\n# ------ Format species and interception region data ------\n### Table of the species-by-region combinations per species\nspecies_profiles <- xtabs(formula = intercepted ~ species_region + species, data = species_region)\n\n### Table of the species-by-region combinations per interception region \nregion_profiles <- xtabs(formula = intercepted ~ species_region + region, data = species_region)\n\n\n# ------ Partial CCA ------ \n# Estimate the variance in commodity associations explained by species, once the effect of interception region is removed \n\ncca<- cca(spe_profiles_allregions ~ species_profiles + Condition(region_profiles)) # The interception region of each commodity profile as a conditioning variable\n\n# ANOVA like permutation test for CCA\nanova.cca(cca) \n\n\n\n####################################################################################################\n###### Estimate species richness and Shannon diversity transported by each commodity class  ########\n####################################################################################################\n\n# ------ Import data ------\n# The number of interceptions per species on each commodity class\nspe_comclass <- read.csv(""spe_comclass.csv"", row.names = ""species"")\n\n\n# ------ Calculate richness and diversity estimates ------\n# Estimate species richness per commodity class\nchao_rich<- ChaoRichness(spe_comclass, datatype = ""abundance"", conf = 0.95)\nchao_rich$class<- as.character(rownames(chao_rich))\nnames(chao_rich)[4:5] <- c(""lower"", ""upper"") # Rename columns for plotting\nchao_rich[order(chao_rich$Estimator),] # Order by richness\n\n# Estimate Shannon diversity per commodity class\nchao_SH<- ChaoShannon(spe_comclass, datatype = ""abundance"", conf = 0.95)\nchao_SH$class<- as.character(rownames(chao_SH))\nnames(chao_SH)[4:5] <- c(""lower"", ""upper"") # Rename columns for plotting\nchao_SH[order(chao_SH$Estimator),] # Order by diversity\n\n\n# ------ Plot data ------\n# Species richness per commodity class: separate columns into observed and estimated richness\nchao_rich$obs_est<- chao_rich$Estimator - chao_rich$Observed \nobs<- as.data.frame(chao_rich[c(6,1,4,5)]) \nobs$species<- rep(""Observed"", length(obs$class)) \nobs_est<- as.data.frame(chao_rich[c(6,7,4,5)])\nobs_est$species<- rep(""Estimated"", length(obs_est$class)) \nobs_est[c(3,4)] <- NA\nnames(obs_est) <- c(""class"",""Observed"",""lower"",""upper"",""species"")\nrichness<- rbind(obs,obs_est) \n\n# Create barplot\nRichness_plot<- ggplot(data = richness, aes(x = reorder(class, -Observed), y = Observed, fill = species)) + geom_bar(stat = ""identity"") + theme_classic() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(y =""Number of species"", x = """") + scale_y_continuous(breaks = seq(0, 13000, by = 1500)) + ggtitle(""a)"") + geom_errorbar(aes(ymin= lower, ymax= upper), width=.2) + theme(legend.position = ""none"", legend.title = element_blank()) + theme(text=element_text(size=14))\n\n# Shannon diversity per commodity class: separate columns into observed and estimated diversity\nchao_SH$obs_est<- chao_SH$Estimator - chao_SH$Observed\nobs<- as.data.frame(chao_SH[c(6,1,4,5)]) \nobs$diversity<- rep(""Observed"", length(obs$class)) \nobs_est<- as.data.frame(chao_SH[c(6,7,4,5)])\nobs_est$diversity<- rep(""Estimated"", length(obs_est$class)) \nobs_est[c(3,4)] <- NA\nnames(obs_est) <- c(""class"",""Observed"",""lower"",""upper"",""diversity"")\ndiversity<']",1,"Townsend's big-eared bats, Corynorhinus townsendii, species distribution models, habitat suitability, climate change, species occurrence probability, population level differences, seasonal and spatial differences, phenological stages, Environmental Protection Agency Level III E"
Data for: Predicting habitat suitability for Townsend's big-eared bats across California in relation to climate change,"Aim: Effective management decisions depend on knowledge of species distribution and habitat use. Maps generated from species distribution models are important in predicting previously unknown occurrences of protected species. However, if populations are seasonally dynamic or locally adapted, failing to consider population level differences could lead to erroneous determinations of occurrence probability and ineffective management. The study goal was to model the distribution of a species of special concern, Townsend's big-eared bats (Corynorhinus townsendii), in California. We incorporate seasonal and spatial differences to estimate the distribution under current and future climate conditions.Methods: We built species distribution models using all records from statewide roost surveys and by subsetting data to seasonal colonies, representing different phenological stages, and to Environmental Protection Agency Level III Ecoregions to understand how environmental needs vary based on these factors. We projected species' distribution for 2061-2080 in response to low and high emissions scenarios and calculated the expected range shifts.Results: The estimated distribution differed between the combined (full dataset) and phenologically-explicit models, while ecoregion-specific models were largely congruent with the combined model. Across the majority of models, precipitation was the most important variable predicting the presence of C. townsendii roosts. Under future climate scnearios, distribution of C. townsendii is expected to contract throughout the state, however suitable areas will expand within some ecoregions. Main conclusion: Comparison of phenologically-explicit models with combined models indicate the combined models better predict the extent of the known range of C. townsendii in California. However, life history-explicit models aid in understanding of different environmental needs and distribution of their major phenological stages. Differences between ecoregion-specific and statewide predictions of habitat contractions highlight the need to consider regional variation when forecasting species' responses to climate change. These models can aid in directing seasonally explicit surveys and predicting regions most vulnerable under future climate conditions.","['model.eval <- function(me) {\n  min.pres <- min(me@presence)\n  pres10 <- quantile(me@presence, 0.10)\n  max.TNPR <- threshold(me, ""spec_sens"")\n  RES <- list(me@auc, abs(sum(me@presence > min.pres)/length(me@presence) - 1), me@TPR[which(me@t > min.pres)[1]] + me@TNR[which(me@t > min.pres)[1]] - 1, abs(sum(me@presence > pres10)/length(me@presence) - 1), me@TPR[which(me@t > pres10)[1]] + me@TNR[which(me@t > pres10)[1]] - 1, abs(sum(me@presence > max.TNPR)/length(me@presence) - 1), me@TPR[which(me@t > max.TNPR)[1]] + me@TNR[which(me@t > max.TNPR)[1]] - 1)\n  names(RES) <- c(""AUC"", ""ORate.MPT"", ""TSS.MPT"", ""ORate.10PT"", ""TSS.10PT"", ""ORate.Max"", ""TSS.Max"")\n  RES\n}', '###########Cropping Raster Files################\nlibrary(raster)\n\n#Bring in the raster data layers#\nsetwd()#set path to your working directory where the raster files are\nDisturb<-raster(""Dist2urb_California.tif"") #bring in the distance to urban centers raster file, the extent is already set to California, so you can use this to crop all the larger data files\nfiles2=list.files(pattern = \'.tif$\', full.names = TRUE)\n\nfor (i in 1:length(files2)) {\n  Bio <- raster(files2[i])\n  resample<-resample(Bio, Disturb)\n  fn=paste(files2[i], ""cropped"")\n  writeRaster(resample, filename = fn)\n}\n\n', '                    ##### Species Distribution Model Script ###\n\n##============================================================================##\n## Load the libraries ----\n##============================================================================##\nlibrary(raster)\nlibrary(rgdal)\nlibrary(dismo)\nlibrary(dplyr)\nlibrary(kernlab)\nlibrary(ecospat)\nlibrary(sdm)\nlibrary(randomForest)\n##============================================================================##\n\n##============================================================================##\n## Load the model.eval function ----\n##============================================================================##\n\nmodel.eval <- function(me) {\n  min.pres <- min(me@presence)\n  pres10 <- quantile(me@presence, 0.10)\n  max.TNPR <- threshold(me, ""spec_sens"")\n  RES <- list(me@auc, abs(sum(me@presence > min.pres)/length(me@presence) - 1), me@TPR[which(me@t > min.pres)[1]] + me@TNR[which(me@t > min.pres)[1]] - 1, abs(sum(me@presence > pres10)/length(me@presence) - 1), me@TPR[which(me@t > pres10)[1]] + me@TNR[which(me@t > pres10)[1]] - 1, abs(sum(me@presence > max.TNPR)/length(me@presence) - 1), me@TPR[which(me@t > max.TNPR)[1]] + me@TNR[which(me@t > max.TNPR)[1]] - 1)\n  names(RES) <- c(""AUC"", ""ORate.MPT"", ""TSS.MPT"", ""ORate.10PT"", ""TSS.10PT"", ""ORate.Max"", ""TSS.Max"")\n  RES\n}\n##============================================================================##\n\n\n##============================================================================##\n## Load the Population Data ----\n##============================================================================##\nsetwd(""Path"")\n\nloc<-read.csv(""path"") #path to CSV file containing occurrence data\nloc<-loc[c(3,4)] # only retain columns that are lat/long\nloc<-loc[c(2,1)]##rearrange the order, has to be long/lat\ncolnames(loc)=c(""longitude"",""latitude"") ## standardize name of columns\n\n#set geographic extent\ngeographic.extent<-loc #make a new object out of the long/lat values\nmax.lat <- ceiling(max(geographic.extent$latitude+10))\nmin.lat<- floor(min(geographic.extent$latitude-10))\nmax.lon <-ceiling(max(geographic.extent$longitude+10))\nmin.lon <- floor(min(geographic.extent$longitude-10))\ngeographic.extent <- extent(x=c(min.lon,max.lon, min.lat, max.lat))\n##============================================================================##\n\n\n##============================================================================##\n## Load the study area ----\n##============================================================================##\n### Present ###\nsetwd(""Path"")## path to environmental variables\nfiles2=list.files(pattern = \'.tif$\', full.names = TRUE) #list all .tif files (replace with extension that matches your rasters)\npred = stack(files2[3], files2[6], files2[7], files2[8], files2[9], files2[12])\nnames(pred)=c(""bio03"", ""bio12"", ""bio15"", ""bio18"", ""DEM"", ""slope"" ) #create raster stack\nback=randomPoints(pred, n=1000, ext=geographic.extent) ## background points\ncolnames(back) = c(\'longitude\', \'latitude\') #set names for columns\n\n##============================================================================##\n\n##============================================================================##\n## Load future climate data area Example ----\n##============================================================================##\n\nsetwd(""Paths"")\nfiles3=list.files(pattern = \'.tif$\', full.names = TRUE)\nhe26_70=stack(files3[1], files3[2],files3[3], files3[4], files3[5], files3[6])\nnames(he26_70)=c(""bio03"", ""bio12"", ""bio15"", ""bio18"", ""DEM"", ""slope"" )\n\n##============================================================================##\n## Model Evaluation ----\n##============================================================================##\n\n### setting aside testing and training data ###\nk<-5\ngrouppres<-kfold(loc,k) #splitting presence data into 5 groups\ngroupback<-kfold(back,k) # splitting background data into 5 groups\nemaxent <- list() #naming a list where we will place evaluate() data\nxm.plot<-list() #naming a list wehre will will place maxent plots\nmodelevalmaxent<-list() #naming a list where we will place model.eval() data\n\n\n## loop for training and testing data ##\nfor (i in 1:k) {\n  pres_train=loc[grouppres != i, ]\n  pres_test=loc[grouppres == i, ]\n  backg_train=back[groupback != i, ]\n  backg_test=back[groupback == i, ]\n  #### maxent ###\n  xm=maxent(pred, p=pres_train, a=backg_train)\n  emaxent[[i]] <- evaluate(p=pres_test, a=backg_test, xm, pred)\n  e2maxent<-evaluate(p=pres_test, a=backg_test, xm, pred)\n  xm.plot[[i]] <-plot(xm)\n  modelevalmaxent[[i]]<-model.eval(e2maxent)\n  pdf(paste(""plot"", i, "".pdf"", sep = """"))\n  response(xm)\n  dev.off()\n  plot<-plot(xm)\n  plot\n\n  \n}\n\n\n### Model AUC and Spec Sens #### \nauc.maxent <- sapply(emaxent, function(x){x@auc})\nmean(auc.maxent)##mean auc data from the 5 model evaluation runs\nmean.auc.maxent<-mean(auc.maxent)\nspec_sens.maxent<-sapply( emaxent, function(x){ threshold(x)[\'spec_sens\'] } )## get the threshold data that will be used in creating', '\n##============================================================================##\n## Load the libraries ----\n##============================================================================##\nlibrary(raster)\nlibrary(biomod2)\nlibrary(biogeo)\nlibrary(dismo)\nlibrary(ecospat)\n##============================================================================##\n\n##============================================================================##\n## Load the study area ----\n##============================================================================##\n\nsetwd(""/scratch/user/nhamilton/COTO_SDM/Environmental_Variables/Present_2/ER1"")\nfiles=list.files(pattern = \'.tif$\', full.names = TRUE)\npred = stack(files[1], files[2], files[3], files[4], files[5], files[6])\nnames(pred)=c(""bio03"", ""bio12"", ""bio15"", ""bio18"", ""DEM"", ""slope"")\nDEM=raster(""DEM_1.tif"")\nSlope=raster(""Slope_ER1_Present.tif"")\n\n##============================================================================##\n## Load future data ----\n##============================================================================##\nsetwd(""Path"")\nfiles3=list.files(pattern = \'.tif$\', full.names = TRUE)\nhe26_70=stack(files3[1], files3[2],files3[3], files3[4], DEM, Slope)\nnames(he26_70)=c(""bio03"", ""bio12"", ""bio15"", ""bio18"", ""DEM"", ""slope"" )\n\n\n##============================================================================##\n## Load the occurrences ----\n##============================================================================##\nsetwd(""/scratch/user/nhamilton/COTO_SDM/Population_data"")\nloc=read.csv(""ER1.csv"")\nloc=loc[c(10,9)]\ncolnames(loc)=c(""longitude"",""latitude"")\nloc$species<-""COTO""\nloc$presence<-1\nback=randomPoints(pred, n=1000)\ncolnames(back) = c(\'longitude\', \'latitude\')\ndata=c(rep(1,nrow(loc)), rep(0, nrow(back)))\nbind=rbind(loc[,1:2], back)\n##============================================================================##\n\nsetwd(""/scratch/user/nhamilton/COTO_SDM/Results/Reviews/ER1"")\n##============================================================================##\n## Format the data in the way biomod2 and ecospat want them\n##============================================================================##\nbmData <- BIOMOD_FormatingData(resp.var = data,\n                               resp.xy = bind, \n                               resp.name =""COTO"",\n                               expl.var = pred,\n                               PA.nb.rep=0,\n                               PA.nb.absences=1000,\n                               PA.strategy=""random"",\n                               na.rm=T)\n\n##============================================================================##\n## Calibrate the simple bivariate models\n##============================================================================##\nmy_ESM_COTO <- ecospat.ESM.Modeling( data = bmData,\n                                         models = c(\'MAXENT.Phillips.2\'),\n                                         NbRunEval = 10,\n                                         DataSplit = 80,\n                                         weighting.score = c(\'AUC\'),\n                                         parallel = T)\n\n##============================================================================##\n## Evaluate and average the simple bivariate models to ESMs\n##============================================================================##\nmy_ESM_COTO_EF <- ecospat.ESM.EnsembleModeling(my_ESM_COTO, \n                                                   weighting.score = c(""AUC""), \n                                                   models = \'all\', \n                                                   threshold = 0.5)\n\n\n\nmy_ESM_COTO_EF$weights.EF\nmy_ESM_COTO_EF$weights\nmy_ESM_COTO_EF$ESM.evaluations\n\nEnsemble_evaluations <- tibble::as_tibble(my_ESM_COTO_EF$ESM.evaluations)\nEnsemble_evaluations$model <- factor(Ensemble_evaluations$model)\nlevels(Ensemble_evaluations$model)\n\nmean(Ensemble_evaluations$TSS)\nmean(Ensemble_evaluations$SomersD)\nmean(Ensemble_evaluations$Kappa)\nmean(Ensemble_evaluations$AUC)\nmean(Ensemble_evaluations$MPA)\n\necospat.ESM.VarContrib(my_COTO_ESM, my_COTO_ESM_EF)\necospat.ESM.VarContrib(my_COTO_ESM_EF)\n\n##============================================================================##\n## Projection of the simple bivariate models into new space\n##============================================================================##\nmy_ESM_COTO_proj_current <- ecospat.ESM.Projection(ESM.modeling.output = my_ESM_COTO,new.env = pred,parallel = T)\n\n##============================================================================##\n## Projection of the calibrated ESMs into new space\n##============================================================================##\nmy_ESM_COTO_EFproj_current <- ecospat.ESM.EnsembleProjection(ESM.prediction.output = my_ESM_COTO_proj_current, ESM.EnsembleModeling.output = my_ESM_COTO_EF)\n\nwriteRaster(my_ESM_COTO_EFproj_current, ""Present_ER1_ESM"", format=\'GTiff\', overwrite=T)\n\n##============================================================================']",1,"invasive species, population genomics, Halyomorpha halys, agriculture, pest, genome-wide polymorphisms, native, invasive, intercepted samples, spatial structure, admixture rates, genomic diversity, bridgehead events, reduced-re"
"Data for: Population genomic insights into invasion success in the polyphagous agricultural pest, Halyomorpha halys","Invasive species are increasingly threatening ecosystems and agriculture by rapidly expanding their range and adapting to environmental and human-imposed selective pressures. The genomic mechanisms that underlie such rapid changes remain unclear, especially for agriculturally important pests. Here, we use genome-wide polymorphisms derived from native, invasive, and intercepted samples and populations of the brown marmorated stink bug (BMSB), Halyomorpha halys, to gain insights into population genomics processes that have promoted the successful global invasion of this polyphagous pest. Our analysis demonstrated that BMSB exhibits spatial structure but admixture rates are high among introduced populations, resulting in similar levels of genomic diversity across native and introduced populations. These spatial genomic patterns suggest a complex invasion scenario, potentially with multiple bridgehead events, posing a challenge for accurately assigning BMSB incursions to their source using reduced-representation genomic data. By associating allele frequencies with the invasion status of BMSB populations, we found significantly differentiated SNPs located in close proximity to genes for insecticide resistance and olfaction. Comparing variations in allele frequencies among populations for outlier SNPs suggests that BMSB invasion success has likely evolved from standing genetic variation. In addition to being a major nuisance of households, BMSB has caused significant economic losses to agriculture in recent years and continues to expand its range. Despite no record of BMSB insecticide resistance to date, our results show high capacity for potential evolution of such characters, highlighting the need for future sustainable and targeted management strategies.","['library(assignPOP)\r\nlibrary(ggplot2)\r\nlibrary(ggpubr)\r\n\r\nYourGenepop <- read.Genepop( ""allwithNZ_neutral_noNZ_assignPOP.gen"", pop.names=c(""Austria"",""Chile"",""China"",""Georgia"",""Hungary"",""Italy"",""Japan"",""Romania"",""Serbia"",""Slovenia"",""Turkey"",""USA""), haploid = FALSE)\r\n\r\n  \r\nassign.MC(YourGenepop, train.inds=c(0.5, 0.9), train.loci=c(0.5, 0.9, 1),\r\n           loci.sample=""fst"", iterations=50, model=""svm"", dir=""svm/"", processors=10)\r\n\r\n\r\naccuMC_svm <- accuracy.MC(dir = ""svm/"")\r\n\r\n##plotting\r\npdf(""./accuMC_plot_allsvm.pdf"")\r\naccuracy.plot(accuMC_svm, pop = ""all"")\r\ndev.off()\r\n\r\n##per population plots\r\n\r\npdf(""./accuMC_plot_perpopsvm.pdf"")\r\nrow1 <- accuracy.plot(accuMC_svm, pop=c(""USA"", ""Austria"",""Chile"",""China"")) + ylim(0, 1) + #Set y limit between 0 and 1\r\n  ggtitle(""Monte-Carlo cross-validation using genetic loci"")+ #Add a plot title\r\n  theme(plot.title = element_text(size=10)) #Edit plot title text size\r\n\r\nrow2 <- accuracy.plot(accuMC_svm, pop=c(""Georgia"",""Hungary"",""Italy"",""Japan"")) + ylim(0, 1) + theme(plot.title = element_text(size=10))\r\n\r\nrow3 <- accuracy.plot(accuMC_svm, pop=c(""Romania"",""Serbia"",""Slovenia"",""Turkey"")) + ylim(0, 1) + theme(plot.title = element_text(size=10))\r\n\r\nrow4 <- accuracy.plot(accuMC_svm, pop=c(""all"")) + ylim(0, 1) + theme(plot.title = element_text(size=10))\r\n\r\nggarrange(row1,row2,row3,row4,ncol = 1, nrow = 4)\r\n\r\ndev.off()\r\n', 'setwd(""C:/Users/eparvizi/project/BMSB_final/MAF05LD50502/1_FINAL analysis with NZ included vcf/neutral snps/assignPOP/"")\r\n\r\nNZItaly <- read.table(""./NZItaly_svm/AssignmentResult.txt"", header = TRUE)\r\nNZItaly <- NZItaly[,-2]\r\n\r\nNZUSA <- read.table(""./NZUSA_svm/AssignmentResult.txt"", header = TRUE)\r\nNZUSA <- NZUSA[,-2]\r\n\r\nNZJP <- read.table(""./NZJP_svm/AssignmentResult.txt"", header = TRUE)\r\nNZJP <- NZJP[,-2]\r\n\r\n\r\nlibrary(reshape2)\r\nlibrary(ggplot2)\r\n\r\n\r\n##create input for bar charts\r\nNZItaly_melted <- melt(NZItaly)\r\nNZItaly_melted$value <- NZItaly_melted$value*100\r\nggplot(NZItaly_melted, aes(x=1, y=value, fill=variable)) +\r\n  geom_bar(width = 1, stat = ""identity"") + facet_grid(~Ind.ID) +\r\n  scale_fill_manual(values=c(""#6f8050"", ""#f55204"", ""#432ab7"", ""#fac7d5"", ""#b6bd7b"",\r\n                             ""#9e5e3f"",""#abc8f5"",""#f8d648"",""#aaba0d"",""#5b6405"",""#9c5468"",\r\n                             ""#631559"")) +\r\n  theme_classic()\r\n\r\nNZUSA_melted <- melt(NZUSA)\r\nNZUSA_melted$value <- NZUSA_melted$value*100\r\nggplot(NZUSA_melted, aes(x=1, y=value, fill=variable)) +\r\n  geom_bar(width = 1, stat = ""identity"") + facet_grid(~Ind.ID) +\r\n  scale_fill_manual(values=c(""#6f8050"", ""#f55204"", ""#432ab7"", ""#fac7d5"", ""#b6bd7b"",\r\n                             ""#9e5e3f"",""#abc8f5"",""#f8d648"",""#aaba0d"",""#5b6405"",""#9c5468"",\r\n                             ""#631559"")) +\r\n  theme_classic()\r\n\r\n\r\nNZJP_melted <- melt(NZJP)\r\nNZJP_melted$value <- NZJP_melted$value*100\r\nggplot(NZJP_melted, aes(x=1, y=value, fill=variable)) +\r\n  geom_bar(width = 1, stat = ""identity"") + facet_grid(~Ind.ID) +\r\n  scale_fill_manual(values=c(""#6f8050"", ""#f55204"", ""#432ab7"", ""#fac7d5"", ""#b6bd7b"",\r\n                             ""#9e5e3f"",""#abc8f5"",""#f8d648"",""#aaba0d"",""#5b6405"",""#9c5468"",\r\n                             ""#631559"")) +\r\n  theme_classic()\r\n\r\n', '\r\nlibrary(corrplot)\r\n\r\n\r\n\r\nsetwd(""C:/Users/eparvizi/project/BMSB_final/MAF05LD50502/1_FINAL analysis with NZ included vcf/neutral snps/"")\r\n\r\n#BayesAss\r\ndata <- as.matrix(read.table(""./BayesAss/final 3 million/average over 3 runs2.txt"", header = TRUE))\r\n\r\n\r\n# Heatmap\r\ncorrplot(data, method=""color"", order = ""alphabet"", is.corr = FALSE, col=colorRampPalette(c(""white"",""#00b4d8"",""#03045e""))(200))\r\n\r\n\r\n\r\n#Gst\r\ndata2 <- as.matrix(read.table(""./divmigrate/63 outliers_final_excluding NZ/Gst matrix.txt"", header = TRUE))\r\n\r\ncorrplot(data2, method=""color"", order = ""alphabet"", is.corr = FALSE, col=colorRampPalette(c(""white"",""#00b4d8"",""#03045e""))(200))\r\n']",1,"immune gene silencing, dsRNA feeding, pathogenic viruses, Argentine ant, pest control, RNA interference, biological invader, immunity-associated genes, Spaetzle, Dicer-1, target gene silencing, gene knockdown, gene"
Can immune gene silencing via dsRNA feeding promote pathogenic viruses to control the globally invasive Argentine ant?,"Pest control methods that can target pest species with limited environmental impacts are a conservation and economic priority. Species-specific pest control using RNA interference is a challenging but promising avenue in developing the next generation of pest management. We investigate the feasibility of manipulating a biological invader's immune system using double-stranded RNA (dsRNA) in order to increase susceptibility to naturally occurring pathogens. We used the invasive Argentine ant as a model, targeting the immunity-associated genes Spaetzle and Dicer-1 with dsRNA. We show that feeding of Spaetzle dsRNA can result in partial target gene silencing for up to 28 days in the laboratory and five days in the field. Dicer-1 dsRNA only resulted in partial gene knockdown after two days in the laboratory. Double-stranded RNA treatments were associated with significant gene expression disruptions across immune pathways in the laboratory and to a lower extent in the field. We observed occasional changes in viral loads in dsRNA-treated groups. However, immune pathways disruption did not result in consistent increase in microbial infections, nor did they alter ant abundance in the field. Our study explores the feasibility of lowering a pest's immunity as a control tool. We demonstate that it is possible to alter immune gene expression of pest species and pathogen loads, though in our system the affected pathogens did not appear to influence pest abundance. We provide advice on future directions for dsRNA-mediated immune disruption in pest species, including potential avenues to improve dsRNA delivery as well as the importance of the biology of the pest system and its pathogens.","['##### RNAi analysis\n##################################################################################################################################\n\n##### Working directories ########################################################################################################\n\nrm(list = ls())\n\n# working directory\ndirectory = ""/Users/antoinefelden/Dropbox/RNAi/""\n\nDataDir = paste0(directory,""01_data/"")\nFigDir = ""~/Documents/Research/Manuscripts/X-ImmunitySilencing/Analysis/03_figures/Final_figs/""\n\n##### Libraries and functions ####################################################################################################\n\nlibrary(""ggplot2"")\nlibrary(""gplots"")\nlibrary(""gridExtra"")\nlibrary(""pgirmess"")\nlibrary(""RVAideMemoire"")\nlibrary(""car"")\nlibrary(""ade4"")\nlibrary(""factoextra"")\nlibrary(""nlme"")\nlibrary(""vegan"")\nlibrary(""multcomp"")\nlibrary(""digest"")\nlibrary(""ggpubr"")\nlibrary(""egg"")\nlibrary(""cowplot"")\nlibrary(""viridis"")\nlibrary(""PerformanceAnalytics"")\nlibrary(""reshape2"")\nlibrary(""lme4"")\nlibrary(""lmerTest"")\nlibrary(""MuMIn"")\nlibrary(""FSA"")\nlibrary(""dplyr"")\nlibrary(""pheatmap"")\nlibrary(""RColorBrewer"")\nlibrary(""reshape2"")\nlibrary(""reshape"")\nlibrary(""vegan"")\nlibrary(""cowplot"")\nlibrary(""BiodiversityR"")\n\n##### Analysis ###################################################################################################################\n\n### Load and arrange data\n\n#subset_points_ddCt_cards_all <- read.csv(paste0(DataDir,""all_Taqman_data.csv""))\nsubset_points_ddCt_cards_all <- read.csv(paste0(DataDir,""all_Taqman_data_no_0.csv""))\n\nsubset_points_ddCt_cards_all$post_treat <- ifelse(subset_points_ddCt_cards_all$Time == 35, ""_pt"", """")\nsubset_points_ddCt_cards_all$dsRNA_pt <- paste0(subset_points_ddCt_cards_all$dsRNA, subset_points_ddCt_cards_all$post_treat)\n\nsamples_E5 <- read.csv(paste0(DataDir,""samples_FEXP5.csv""))[c(""New_Name"",""Comment"")]; names(samples_E5) <- c(""Sample"",""Comment""); samples_E5$Experiment <- ""E5""\nsamples_E6 <- read.csv(paste0(DataDir,""samples_FEXP6.csv""))[c(""Sample"",""Comment"")]; samples_E6$Experiment <- ""E6""\nsamples_E7 <- read.csv(paste0(DataDir,""samples_FEXP7.csv""))[c(""Sample"",""Comment"")]; samples_E7$Experiment <- ""E7""\n\nsample_data <- rbind(samples_E5, samples_E6,samples_E7); sample_data$Sample <- paste(sample_data$Sample,sample_data$Experiment,sep=""_"")\nsample_data$Colony <- paste0(sample_data$Comment,sample_data$Experiment)\n\nsubset_points_ddCt_cards_all$Colony <- merge(subset_points_ddCt_cards_all,sample_data, by = ""Sample"", all.x = TRUE)[,""Colony""]\n\n#all_data_PCA <- subset_points_ddCt_cards_all[,c(""Sample"",""TargetName"",""two_dCt"",""RelEx"",""Time"",""dsRNA"",""dsRNA_pt"", ""Experiment"", ""Colony"",""lipofectamine"")]\nall_data_PCA <- subset_points_ddCt_cards_all[,c(""Sample"",""TargetName"",""two_dCt"",""RelEx"",""RelEx_no_0"",""Time"",""dsRNA"",""dsRNA_pt"", ""Experiment"", ""Colony"",""lipofectamine"")]\n\n\n# Fix gene names\nall_data_PCA[all_data_PCA == ""KBVv2""] <- ""KBV""\nall_data_PCA[all_data_PCA == ""Hymenaoptaecin""] <- ""Hymenoptaecin""\nall_data_PCA[all_data_PCA == ""Beta-1-3-GPB-like-A""] <- ""Beta-1-3-GBP-like-A""\nall_data_PCA[all_data_PCA == ""Beta-1-3-BGP-like-B""] <- ""Beta-1-3-GBP-like-B""\n\n# Absolute expression\nall_twodCt_casted <- data.frame(""Sample"" = unique(all_data_PCA$Sample),\n                              ""Experiment"" = all_data_PCA[match(unique(all_data_PCA$Sample),all_data_PCA$Sample),""Experiment""],\n                              ""Time"" = all_data_PCA[match(unique(all_data_PCA$Sample),all_data_PCA$Sample),""Time""],\n                              ""dsRNA"" = all_data_PCA[match(unique(all_data_PCA$Sample),all_data_PCA$Sample),""dsRNA""],\n                              ""dsRNA_pt"" = all_data_PCA[match(unique(all_data_PCA$Sample),all_data_PCA$Sample),""dsRNA_pt""],\n                              ""Colony"" = all_data_PCA[match(unique(all_data_PCA$Sample),all_data_PCA$Sample),""Colony""],\n                              ""Lipofectamine"" = all_data_PCA[match(unique(all_data_PCA$Sample),all_data_PCA$Sample),""lipofectamine""])\ncolnames_twodCt <- NULL\nfor (target in unique(all_data_PCA$TargetName)) {\n  subset_target <- subset(all_data_PCA,all_data_PCA$TargetName == target, select=c(""Sample"",""two_dCt"",""Time"",""dsRNA"",""dsRNA_pt"",""Experiment"",""Colony""))\n  colnames_twodCt <- c(colnames_twodCt,target)\n  all_twodCt_casted <- as.data.frame(cbind(all_twodCt_casted, subset_target[match(all_twodCt_casted$Sample,subset_target$Sample, nomatch = NA),2]))}\ncolnames(all_twodCt_casted) <- c(""Sample"",""Experiment"",""Time"",""dsRNA"",""dsRNA_pt"",""Colony"", ""lipofectamine"", colnames_twodCt)\n\nall_twodCt_casted[is.na(all_twodCt_casted)] <- 0 # Remove offending NAs (because Pseudomonas phage not present in Exp 7 and 5)\nnames(all_twodCt_casted) <- gsub(""-"", """", names(all_twodCt_casted))\nall_twodCt_casted$dsRNA <- gsub(""-"", """", all_twodCt_casted$dsRNA)\nall_twodCt_casted$dsRNA_pt <- gsub(""-"", """", all_twodCt_casted$dsRNA_pt)\nrownames(all_twodCt_casted) <- all_twodCt_casted$Sample\nall_twodCt_casted$IntFac <- interaction(all_twodCt_casted$dsRNA, all_twodCt_casted$Time, drop=T)\n\n# Add 1 to the dat']",1,"freshwater fisheries, species interactions, size spectrum models, ecosystem-based fisheries management, Lake Nipissing, Ontario, Canada, fish community, management scenarios, fishing mortality, biomass, community size structure, stock recovery times, top predators, competition,"
Data for: Size spectrum model reveals importance of considering species interactions in a freshwater fisheries management context,"Inland fisheries have significant cultural and economic value around the globe, providing dietary protein, income, and recreation. Consequently, methods for monitoring and managing these important fisheries are continually being refined. In marine systems, multi-species size spectrum models have been increasingly used to explore management scenarios of important fish stocks within an ecosystem-based fisheries management framework; however, these models have not been applied in freshwater systems. In this study, we developed a multi-species size spectrum model for the fish community of Lake Nipissing, a large, productive lake in Ontario, Canada. To the best of our knowledge, this is the first fully calibrated multi-species size spectrum model for an inland fishery. Using this model, we explored the impacts of different management scenarios on fish community dynamics while taking species interactions into account. Specifically, we examined how changes in fishing mortality affect: (1) species biomass; (2) community size structure; and (3) stock recovery times. We found that community dynamics following changes in fishing mortality were driven by complex interactions among species, including competition and predation. The greatest changes in biomass and community size structure were observed following changes in fishing mortality to top predators, with community size structure most strongly influenced by changes in mortality to the largest species in the community. Counter to predictions based on generation time, the smallest species in our model exhibited the longest time to recovery due to strong competition and predation. Our results demonstrate the importance of taking an ecosystem-based approach and considering species interactions in the management of inland fisheries and highlight the potential of size spectrum model use in freshwater systems.","['#Code for: Evaluating impacts of fisheries management practices on a freshwater fish community through size spectrum modelling\r\n#Author: David Benoit\r\n#Institution: University of Toronto \r\n\r\n#load empirical growth curve data\r\ng_curves <- read.csv(""weight_by_age.csv"", sep="","", header=TRUE)\r\n\r\n#function to calcuate error between modelled biomass & growth with empirical data\r\nBio_growth_ERROR3 <- function(Newcomm, meantsteps= 50){\r\n  \r\n  #create vector of empirical biomass\r\n  #values taken from data frame\r\n  bio_obs<- Newcomm@params@species_params$Biomass_relative_perch\r\n  bio_obs <- bio_obs[!is.na(bio_obs)]\r\n  \r\n  #Get Total Biomass of each species from model\r\n  #takes average of last ten years\r\n  bset <- getBiomass(Newcomm)[c(I(dim(Newcomm@n)[1]-meantsteps): dim(Newcomm@n)[1]), ]\r\n  Bhat <- colMeans(bset)\r\n  Bhat <- Bhat[c(1,3,4,5)]\r\n  Bhat <- Bhat/Bhat[4]\r\n  \r\n  #Get difference between observed and predicted biomass\r\n  Bobs <- bio_obs\r\n  \r\n  bdif<- log10(Bhat)-log10(Bobs)\r\n  \r\n  #this is biomass portion of error\r\n  Bio_ERROR <- sum(bdif^2)\r\n  \r\n  \r\n  #error from growth curves below\r\n  #get growth curve for Walleye\r\n  growth <- getGrowthCurves2(Newcomm, ""Walleye"")\r\n  df <- as.data.frame.table(growth, stringsAsFactors = FALSE)\r\n  df$Age <- as.numeric(df$Age)\r\n  #extract ages 1 through 5\r\n  df <- df[2:6,]\r\n  \r\n  #difference between model and empirical growth (weight at age)\r\n  g_diff <- colSums(log10(df$Freq) - log10(g_curves[2]))\r\n  \r\n  Bio_walleye_growth <- (g_diff^2)\r\n  \r\n  #get growth curve for Cisco\r\n  growth2 <- getGrowthCurves2(Newcomm, ""Cisco"")\r\n  df2 <- as.data.frame.table(growth2, stringsAsFactors = FALSE)\r\n  df2$Age <- as.numeric(df2$Age)\r\n  #extract ages 1 through 5\r\n  df2 <- df2[2:6,]\r\n  \r\n  #difference between model and empirical growth (weight at age)\r\n  g_diff2 <- colSums(log10(df2$Freq) - log10(g_curves[3]))\r\n  \r\n  Bio_cisco_growth <- (g_diff2^2)\r\n  \r\n  #get growth curve for Northern Pike\r\n  growth3 <- getGrowthCurves2(Newcomm, ""Northern Pike"")\r\n  df3 <- as.data.frame.table(growth3, stringsAsFactors = FALSE)\r\n  df3$Age <- as.numeric(df3$Age)\r\n  #extract ages 1 through 5\r\n  df3 <- df3[2:6,]\r\n  \r\n  #difference between model and empirical growth (weight at age)\r\n  g_diff3 <- colSums(log10(df3$Freq) - log10(g_curves[4]))\r\n  \r\n  Bio_pike_growth <- (g_diff3^2)\r\n  \r\n  #get growth curve for Perch\r\n  growth4 <- getGrowthCurves2(Newcomm, ""Yellow Perch"")\r\n  df4 <- as.data.frame.table(growth4, stringsAsFactors = FALSE)\r\n  df4$Age <- as.numeric(df4$Age)\r\n  #extract ages 1 through 5\r\n  df4 <- df4[2:6,]\r\n  \r\n  #difference between model and empirical growth (weight at age)\r\n  g_diff4 <- colSums(log10(df4$Freq) - log10(g_curves[5]))\r\n  \r\n  Bio_perch_growth <- (g_diff4^2)\r\n  \r\n  #sum both components of the error\r\n  G_Error <- (sum(Bio_walleye_growth, Bio_cisco_growth, Bio_pike_growth, Bio_perch_growth))/5\r\n  BG_Error <- sum(Bio_ERROR*50, G_Error)\r\n  # BG_Error <- sum(Bio_walleye_growth, Bio_cisco_growth, Bio_pike_growth, Bio_perch_growth)\r\n  return(BG_Error)\r\n  #  return(G_Error)\r\n  \r\n}\r\n\r\n################################################################################################\r\n#Get growth curves function\r\ngetGrowthCurves2 <- function(object, \r\n                             species,\r\n                             max_age = 20,\r\n                             percentage = FALSE) {\r\n  if (is(object, ""MizerSim"")) {\r\n    params <- object@params\r\n    t <- dim(object@n)[1]\r\n    n <- object@n[t, , ]\r\n    n_pp <- object@n_pp[t, ]\r\n  } else if (is(object, ""MizerParams"")) {\r\n    params <- validParams(object)\r\n    n <- object@initial_n\r\n    n_pp <- object@initial_n_pp\r\n  }\r\n  if (missing(species)) {\r\n    species <- dimnames(n)$sp\r\n  }\r\n  # reorder list of species to coincide with order in params\r\n  idx <- which(dimnames(n)$sp %in% species)\r\n  species <- dimnames(n)$sp[idx]\r\n  age <- seq(0, max_age, by=1)\r\n  ws <- array(dim = c(length(species), length(age)),\r\n              dimnames = list(Species = species, Age = age))\r\n  g <- getEGrowth(params, n, n_pp)\r\n  for (j in seq_along(species)) {\r\n    i <- idx[j]\r\n    g_fn <- stats::approxfun(params@w, g[i, ])\r\n    myodefun <- function(t, state, parameters) {\r\n      return(list(g_fn(state)))\r\n    }\r\n    ws[j, ] <- deSolve::ode(y = params@w[params@w_min_idx[i]], \r\n                            times = age, func = myodefun)[, 2]\r\n    if (percentage) {\r\n      ws[j, ] <- ws[j, ] / params@species_params$w_inf[i] * 100\r\n    }\r\n  }\r\n  return(ws)\r\n}\r\n\r\n#######################################################################################################################\r\n#Code for calibration \r\n#function to estimate recruitment \r\n#Calibrate recruitment function \r\ncalibrateRmax_with_OPTIM <- function(pars, Initialcomm, meantsteps=10) {\r\n  \r\n  #Keep runing count of function evaluations by optim()  \r\n  optimizer_count <- optimizer_count + 1                                                \r\n  assign(""optimizer_count"", optimizer_count, pos = .GlobalEnv)\r\n  \r\n  #Take antilog of par values (opt', '#Code for updated Lake Nipissing Size Spectrum Model\r\n#Including four fishing scenarios\r\n\r\nlibrary(mizer)\r\n\r\n#Lake Nipissing Model\r\n#Based on avegare FWIN values across decades\r\n#interaciton matrix from BsM\r\n\r\n#Load mizer details\r\nspecies_params2 <- read.csv(""Nipissing_FWIN_paramslowdt2.csv"", header=TRUE, sep ="","")\r\n\r\n\r\n#interaction matrix\r\ninter <- read.csv(""Nip_interaction_dryad.csv"", header = TRUE, sep = "","", row.names = NULL, stringsAsFactors = TRUE )\r\ninter <- inter[-c(1)]\r\ncolnames(inter) <- c(""Cisco"", ""Forage Fish"", ""Northern Pike"", ""Walleye"", ""Yellow Perch"")\r\nrownames(inter) <- colnames(inter)\r\ninter <- as.matrix(inter)\r\n\r\neffort <- c(0.154425,0,0.52554398, 0.3163, 0.154425)\r\n\r\nparams <- MizerParams(species_params = species_params2, interaction = inter, kappa = 150, w_pp_cutoff = 2)\r\n#params@species_params$erepro <- 0.1\r\n\r\nsim <- project(params, effort = effort, t_max=200, dt=0.1, t_save=1)\r\nplot(sim)\r\n\r\n##############################################################################################################################\r\n#Scenario 1: Increase fishing effort of Walleye\r\n\r\ngear_params <- data.frame(species = species_params2$species,\r\n                          gear = species_params2$species,\r\n                          sel_func = ""sigmoid_length"",\r\n                          l25 = species_params2$l25,\r\n                          l50= species_params2$l50)\r\n\r\nf_history <- read.csv(""f_history_scenario1.csv"", header= TRUE, sep ="","", row.names=1)\r\ncolnames(f_history) <- c(""Cisco"", ""Forage Fish"", ""Northern Pike"", ""Walleye"", ""Yellow Perch"")\r\nf_history <- as.matrix(f_history)\r\n\r\nparams <- MizerParams(species_params2,\r\n                      interaction = inter,\r\n                      kappa = 150,\r\n                      gear_params = gear_params,\r\n                      w_pp_cutoff = 2)\r\n\r\nnames(dimnames(f_history)) <- c(""time"", ""gear"")\r\n\r\nsim <- project(params, effort=f_history, dt=0.1)\r\nplot(sim)\r\nplotBiomass(sim, start_time=2010, end_time = 2100)\r\n\r\n#get mean slope for years preceding change (2011-2021)\r\nslope <- as.data.frame(getCommunitySlope(sim)[91:101,,])\r\ncolMeans(slope)\r\ncolSds(as.matrix(slope))\r\n\r\n#get mean slope for final years (2090-2100)\r\nslope <- as.data.frame(getCommunitySlope(sim)[170:180,,])\r\ncolMeans(slope)\r\ncolSds(as.matrix(slope))\r\n\r\n#get mean biomass preceding change\r\nbio1 <- as.data.frame(getBiomass(sim))\r\nbio1[,6] <- rowSums(bio1)\r\nbio2 <- bio1[91:101,]\r\ncolMeans(bio2)\r\n\r\n#mean biomass in final ten years\r\nbio3 <- bio1[170:180,]\r\ncolMeans(bio3)\r\n\r\n\r\n\r\np1 <- plotBiomass(sim, start_time=2010, end_time = 2100) + theme(axis.title.x = element_blank()) \r\n\r\n\r\n###############################################################################################################################\r\n#Scenario 2: Decrease fishing effort of Walleye\r\ngear_params <- data.frame(species = species_params2$species,\r\n                          gear = species_params2$species,\r\n                          sel_func = ""sigmoid_length"",\r\n                          l25 = species_params2$l25,\r\n                          l50= species_params2$l50)\r\n\r\nf_history <- read.csv(""f_history_scenario2.csv"", header= TRUE, sep ="","", row.names=1)\r\ncolnames(f_history) <- c(""Cisco"", ""Forage Fish"", ""Northern Pike"", ""Walleye"", ""Yellow Perch"")\r\nf_history <- as.matrix(f_history)\r\n\r\nparams <- MizerParams(species_params2,\r\n                      interaction = inter,\r\n                      kappa = 150,\r\n                      gear_params = gear_params,\r\n                      w_pp_cutoff = 2)\r\n\r\nparams@species_params$erepro <- 0.1\r\n\r\nnames(dimnames(f_history)) <- c(""time"", ""gear"")\r\n\r\nsim2 <- project(params, effort=f_history, dt=0.1, t_max=2100)\r\nplot(sim2)\r\nplotBiomass(sim2, start_time=2015, end_time = 2100)\r\n\r\n#get mean slope for years preceding change (2011-2021)\r\nslope <- as.data.frame(getCommunitySlope(sim2)[91:101,,])\r\ncolMeans(slope)\r\ncolSds(as.matrix(slope))\r\n\r\n#get mean slope for final years (2090-2100)\r\nslope <- as.data.frame(getCommunitySlope(sim2)[170:180,,])\r\ncolMeans(slope)\r\ncolSds(as.matrix(slope))\r\n\r\n#get mean biomass preceding change\r\nbio1 <- as.data.frame(getBiomass(sim2))\r\nbio1[,6] <- rowSums(bio1)\r\nbio2 <- bio1[91:101,]\r\ncolMeans(bio2)\r\n\r\n#mean biomass in final ten years\r\nbio3 <- bio1[170:180,]\r\ncolMeans(bio3)\r\n\r\np2 <- plotBiomass(sim2, start_time=2010, end_time = 2100) + theme(legend.position = ""none"")\r\n\r\n#############################################################################################################################\r\n#Scenario 3: Increase fishing effort of Yellow Perch\r\ngear_params <- data.frame(species = species_params2$species,\r\n                          gear = species_params2$species,\r\n                          sel_func = ""sigmoid_length"",\r\n                          l25 = species_params2$l25,\r\n                          l50= species_params2$l50)\r\n\r\nf_history <- read.csv(""f_history_scenario3.csv"", header= TRUE, sep ="","", row.names=1)\r\ncolnames(f_history) <- c(""Cisco"", ""Forage Fish"", ""Northern Pike"", ""Walleye"", ""Yellow']",1,"Data, freshwater fisheries, species interactions, size spectrum model, ecosystem-based fisheries management, multi-species, fish stocks, Lake Nipissing, Ontario, Canada, biomass, community size structure, stock recovery times, fishing mortality, competition, predation, top predators, management scenarios, inland fisheries, cultural value, economic value, dietary protein, income, recreation, monitoring, managing."
Data from: Flexible drought deciduousness in a neotropical understory herb,"PREMISE OF THE STUDY: Adaptive divergence across environmental gradients is a key driver of speciation. Precipitation seasonality gradients are common in the tropics, yet drought adaptation is almost entirely unexplored in neotropical understory herbs. We examine two recently diverged neotropical spiral ginger species, one adapted to seasonal drought and one reliant on perennial water, to uncover physiological and life history traits involved in drought adaptation.METHODS: We combine ecophysiological trait measurements in the field and greenhouse with experimental and observational assessment of real-time drought response to determine how Costus villosissimus (Costaceae) differs from C. allenii to achieve drought adaptation.KEY RESULTS: We find that drought-adapted C. villosissimus has several characteristics indicating flexible drought avoidance via semi-drought-deciduousness and a fast economic strategy. Although the two species do not differ in water use efficiency, C. villosissimus has a more rapid growth rate, higher light-saturated photosynthetic rate, higher leaf nitrogen, higher stomatal pore density, lower stem density, and lower leaf mass per area. These fast economic strategy traits align with both field-based observations and experimental dry-down results. When faced with drought, C. villosissimus displays facultative drought-deciduousness, losing lower leaves during the dry season and rapidly growing new leaves in the wet season.CONCLUSIONS: Our research reveals a drought avoidance strategy previously undocumented in neotropical herbs. This divergent drought adaptation evolved in the last ~1my and is critical to reproductive isolation between C. villosissimus and C. allenii, indicating that adaptive shifts to drought adaptation may be an underappreciated axis of neotropical understory plant diversification.--PREMISA DEL ESTUDIO: La divergencia adaptativa a lo largo de gradientes ambientales es un factor clave de la especiacin. Los gradientes de estacionalidad de la precipitacin son comunes en los trpicos, sin embargo, la adaptacin a la sequa es casi inexplorada en las hierbas neotropicales del sotobosque. Examinamos dos especies de caa agria neotropicales que divergieron recientemente , uno adaptado a la sequa estacional y otro que depende del agua perenne, para descubrir la base de la adaptacin a la sequa.MTODOS: Combinamos mediciones ecofisiolgicas en el campo y el invernadero con una evaluacin experimental y observacional de la respuesta a la sequa en tiempo real para determinar cmo Costus villosissimus (Costaceae) difiere de C. allenii para lograr la adaptacin a la sequa.RESULTADOS CLAVE: Encontramos que C. villosissimus, que est adaptado a la sequa, tiene varias caractersticas que indican que evita la deshidratacin a travs de la caducididad y una estrategia de vida rpida. Aunque las dos especies no difieren en la eficiencia del uso del agua, C. villosissimus tiene una tasa de crecimiento ms rpida, menor masa foliar por rea, menor densidad del tallo, mayor nitrgeno foliar y una fuerte tendencia de mayores tasas fotosintticas saturadas de luz. Estos atributos de la estrategia de vida rpida se alinean tanto con las observaciones basadas en el campo como con los resultados experimentales de sequa. Para sobrevivir a la sequa, C. villosissimus es caducifolia facultativa, perdiendo hojas inferiores durante la estacin seca y creciendo rpidamente hojas nuevas en la estacin hmeda.CONCLUSIONES: Revelamos una estrategia de adaptacin a la sequa que, hasta donde sabemos, no ha sido documentada previamente en hierbas tropicales. Esta adaptacin divergente a la sequa evolucion recientemente y es un componente importante del aislamiento reproductivo entre C. villosissimus y C. allenii, lo que indica que los cambios adaptativos para sobrevivir a la sequa estacional pueden ser un eje subestimado de la diversificacin de las plantas del sotobosque neotropical.","['## RScript to fit light response curves\n## Code written by Eleinis Avila-Lovera\n## Last modified on 5/5/21\n\n#Set working directory\nsetwd()\n\n#Read in data\ndata_10_500<-read.csv(""Chen_LICOR_10-500.csv"", header=T)\n\n## Using package ""photosynthesis"" (https://github.com/cdmuir/photosynthesis)\n## Non-rectangular hyperbolic model of light response from  Marshall B, Biscoe P. 1980. A model for C3 leaves \n## describing the dependence of net photosynthesis on irradiance. J Ex Bot 31:29-39\nlibrary(photosynthesis)\n\n#Fit curves using 10-500 PAR data\nfits2<-fit_many(\n  data=data_10_500, \n  varnames=list(A_net=""photo"", PPFD=""par"", group=""id""\n  ), \n  funct=fit_aq_response, group=""id""\n)\n\n#Explore results\nsummary(fits2[[3]][[1]]) #print model summary for list 3\nfits2[[3]][[2]] #print fitted parameters (second sublist) of list 3\nfits2[[3]][[3]] #print graph (third sublist) of list 3\n\n#Compile parameters into dataframe for analysis\nfits2_pars <- compile_data(fits2,\n                          output_type = ""dataframe"",\n                          list_element = 2\n)\n\nwrite.csv(fits2_pars, file=""A_Q_results2_10-500.csv"", row.names=FALSE)\n', '# Analyses for, ""Avoidance of seasonal drought in a neotropical understory herb""\n# By: Julia Harenčár \n# February 23, 2022\n\nsetwd(""/Users/Julia/Library/CloudStorage/GoogleDrive-jharenca@ucsc.edu/My Drive/GitHub/vill_drought_adaptation"")\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(Hmisc)\n\n#### Field Observations ####\n# importing data and removing empty rows\ntrait <- read.csv(""vill_vs_alle_210505.csv"", header = T) %>% filter(!is.na(Date))\nvill.trait <-trait %>% filter(Spp==\'vill\')\nalle.trait <-trait %>% filter(Spp==\'alle\')\n\n### stomatal density, pore length, and SPI\nshapiro.test(vill.trait$stomatal.density) # not normal; W = 0.72726, p-value = 0.01183\nshapiro.test(alle.trait$stomatal.density) # W = 0.85093, p-value = 0.1975\nqqnorm(vill.trait$stomatal.density)\nqqnorm(alle.trait$stomatal.density)\n# wilcox.test(trait$stomatal.density~trait$Spp) # W = 20, p-value = 0.4102\nt.test(trait$stomatal.density~trait$Spp) # t = -0.057385, df = 7.2793, p-value = 0.9558\n\n#shapiro.test(trait$stomatal.pore.length- mean(trait$stomatal.pore.length, na.rm=T)) # normal; W = 0.97349, p-value = 0.9194\nshapiro.test(vill.trait$stomatal.pore.length) # W = 0.90601, p-value = 0.4107\nshapiro.test(alle.trait$stomatal.pore.length) # W = 0.84841, p-value = 0.1896\nqqnorm(vill.trait$stomatal.pore.length)\nqqnorm(alle.trait$stomatal.pore.length)\nt.test(trait$stomatal.pore.length~trait$Spp) # t = -2.1398, df = 8.9529, p-value = 0.06119; means: A-0.0216 V-0.0262\n\nshapiro.test(vill.trait$SPI) # W = 0.8632, p-value = 0.2004\nshapiro.test(alle.trait$SPI) # W = 0.87428, p-value = 0.2842\nqqnorm(vill.trait$SPI)\nqqnorm(alle.trait$SPI)\nt.test(trait$SPI~trait$Spp) # t = -2.2678, df = 7.0147, p-value = 0.05759 ; means A-0.039  V-0.056\n\nshapiro.test(vill.trait$gmax) # W = 0.84967, p-value = 0.1564\nshapiro.test(alle.trait$gmax) # W = 0.83326, p-value = 0.1471\nqqnorm(vill.trait$gmax)\nqqnorm(alle.trait$gmax)\n# wilcox.test(trait$gmax~trait$Spp) # W = 14, p-value = 0.9307\nt.test(trait$gmax~trait$Spp) # t = -0.68085, df = 7.2512, p-value = 0.5171\n\n# #scatterplot of gmax against spi\n# ggplot(trait, aes(x=gmax, y=SPI, color=Spp)) +\n#   geom_point() +\n#   theme_bw() +\n#   scale_colour_manual(\'Species\', labels= c(expression(italic(""C. allenii"")), \n#                                 expression(italic(""C. villosissimus""))), \n#                       values = c(""#009E73"", ""#E69F00"")) +\n#   theme(legend.text.align = 0) +\n#   labs(x= bquote(~g[max]))\n\n### delta 13 C\nshapiro.test(vill.trait$d13C) # W = W = 0.8475, p-value = 0.1502\nshapiro.test(alle.trait$d13C) # W = 0.89814, p-value = 0.363\nqqnorm(vill.trait$d13C)\nqqnorm(alle.trait$d13C)\n# wilcox.test(trait$d13C~ trait$Spp) # W = 6, p-value = 0.06494\nt.test(trait$d13C ~ trait$Spp) # t = -1.8714, df = 8.2019, p-value = 0.09729\n\n### Leaf N\nshapiro.test(vill.trait$leaf.N) # W = 0.95847, p-value = 0.8079\nshapiro.test(alle.trait$leaf.N) # W = 0.92478, p-value = 0.5404\nqqnorm(vill.trait$leaf.N)\nqqnorm(alle.trait$leaf.N)\nt.test(trait$leaf.N ~ trait$Spp) # t = -2.5547, df = 8.7165, p-value = 0.03172\n\n### Stem density (sig)\nshapiro.test(vill.trait$Stem.density) # W = 0.91364, p-value = 0.4608\nshapiro.test(alle.trait$Stem.density) # W = 0.83586, p-value = 0.1204\nqqnorm(vill.trait$Stem.density)\nqqnorm(alle.trait$Stem.density)\n# wilcox.test(trait$Stem.density ~ trait$Spp) # W = 31, p-value = 0.04113\nt.test(trait$Stem.density ~ trait$Spp) # t = 2.7607, df = 9.921, p-value = 0.02025; means: alle = 0.13048169 vill = 0.09252183 \n\n### Hydraulic Conductivity\n# removing empty rows and dividing by area to get conductivity (rather than conductance)\nhyd.cond <- trait %>% filter(!is.na(theo.kh)) %>% mutate(theo.kh = theo.kh/0.000002414)\nvill.hyd.cond <-hyd.cond %>% filter(Spp==\'vill\')\nalle.hyd.cond <-hyd.cond %>% filter(Spp==\'alle\')\n\nshapiro.test(vill.hyd.cond$theo.kh) # W = 0.87205, p-value = 0.2345\nshapiro.test(alle.hyd.cond$theo.kh) # W = 0.90849, p-value = 0.4744\nqqnorm(vill.hyd.cond$theo.kh)\nqqnorm(alle.hyd.cond$theo.kh)\n# wilcox.test(hyd.cond$theo.kh ~ hyd.cond$Spp) # W = 9, p-value = 0.6095\nt.test(hyd.cond$theo.kh ~ hyd.cond$Spp) # t = -1.1073, df = 7.3146, p-value = 0.3032; means: alle = 10.45360 vill = 16.94283 \n\n### Rhizome water content\nshapiro.test(vill.trait$Rhizome.water.content) # W = 0.9746, p-value = 0.9218\nshapiro.test(alle.trait$Rhizome.water.content) # W = 0.91917, p-value = 0.4994\nqqnorm(vill.trait$Rhizome.water.content) \nqqnorm(alle.trait$Rhizome.water.content)  \nt.test(trait$Rhizome.water.content ~ trait$Spp) # t = 0.26028, df = 9.2048, p-value = 0.8004; means: alle = 86.68057 vill = 87.56916  \n\n### visually checking for correlation\npairs(~SPI+d13C+leaf.N+Stem.density+theo.kh+Rhizome.water.content, data = trait)\n\n# checking for correlations between SPI and other traits\n\ncor(na.omit(trait$SPI),trait[c(1:5,7:12),c( \'d13C\',\'leaf.N\',\'Stem.density\',\'Rhizome.water.content\')], method = ""pearson"")\n# highest Pearson CC = 0.608 (leaf.N and SPI)\n# highest Spearman CC = 0.573 (leaf.N and SPI)\n\n# (((gmax and d13C sig correla']",1,"thermal impacts, freshwater, life stages, temperature management, anadromous salmonids, Chinook salmon, physiology, behavior, survival, ectotherms, thermal performance models, spatial distribution, phenology data, maps, physiological performance, energy"
Quantification of thermal impacts across freshwater life stages to improve temperature management for anadromous salmonids,"Water temperature is the major controlling factor that shapes the physiology, behavior, and ultimately, survival of aquatic ectotherms. Here we examine temperature effects on the survival of Chinook salmon (Oncorhynchus tshawytscha), a species of high economic and conservation importance. We implement a framework to assess how incremental changes in temperature impact survival across populations that is based on thermal performance models for three freshwater life stages of Chinook salmon. These temperature-dependent models were combined with local spatial distribution and phenology data to translate spatial-temporal stream temperature data into maps of life stage-specific physiological performance in space and time. Specifically, we converted temperature-dependent performance (i.e., energy used by pre-spawned adults, mortality of incubating embryos, and juvenile growth rate) into a common currency that measures survival in order to compare thermal effects across life stages. Based on temperature data from two abnormally warm and dry years for three managed rivers in the Central Valley, California, temperature-dependent mortality during pre-spawning holding was higher than embryonic mortality or juvenile mortality prior to smolting. However, we found that local phenology and spatial distribution helped to mitigate negative thermal impacts. In a theoretical application, we showed that high temperatures may inhibit successful reintroduction of threatened Central Valley spring-run Chinook salmon to two rivers where they have been extirpated. To increase Chinook salmon population sizes, especially for the threatened and declining spring-run, our results indicate that adults may need more cold-water holding habitat than currently available in order to reduce pre-spawning mortality stemming from high temperatures. To conclude, our framework is an effective way to calculate thermal impacts on multiple salmonid populations and life stages within a river over time, providing local managers the information to minimize negative thermal impacts on salmonid populations, particularly important during years when cold-water resources are scarce.","['###FitzGerald, A.M., & Martin, B.M. (2022). Conservation Physiology\r\n\r\n####packages required####\r\nlibrary(gplots)\r\nlibrary(colorRamps)\r\nlibrary(ggplot2)\r\nlibrary(reshape2)\r\nlibrary(cowplot)\r\ntheme_set(theme_cowplot())\r\n#####\r\n\r\n#####TEMPERATURE DATA#####\r\n#read temp data and convert date\r\n#Here, we are applying Clear Creek stream temperature from 2013-2014\r\ntemps <- read.csv(""ClearCreek_streamtemp.csv"", TRUE, "","") #whatever csv name is\r\ndata <- as.matrix(temps[,4020:4384]) #columns specify time period of interest\r\nDATE <- read.csv(""ClearCreek_streamtemp.csv"", FALSE, "","") #puts date as first row instead of header\r\nDATE <- DATE[,4020:4384]\r\ntempsDate<-format(as.Date(as.vector(as.matrix(DATE[1,1:ncol(DATE)])), format = ""%m/%d/%Y""),format=""%d-%b-%Y"")\r\nJ<-as.numeric(format(as.Date(as.vector(as.matrix(DATE[1,1:ncol(DATE)])), format = ""%m/%d/%Y""),format=""%j""))\r\n\r\n#stream temp fig\r\n#replicates Fig. 5 left panel\r\nh<-heatmap.2(as.matrix(data), Rowv=NA, dendrogram=""none"", Colv=NA, col=(matlab.like2(60)), trace=""none"", \r\n             main=""Daily temperature (C) \\n along the X River"", \r\n             xlab=""Date"", ylab=""River km"", labRow=temps[,1], labCol = tempsDate, key.title=NA, \r\n             key.xlab=""Temperature (C)"", key.ylab=NA, density.info = ""none"", srtCol = 90, na.color=""gray"") \r\nhh<-rep(\'\',ncol(data)) #empty vector of length ncol(temps)\r\nii<-rep(\'\',nrow(data))\r\nhh[h$colInd[seq(1,ncol(data),50)]]<-tempsDate[seq(1,(length(tempsDate)),50)]\r\nii[h$rowInd[seq(1,nrow(data),5)]] <- seq(0,28,5)\r\nheatmap.2(as.matrix(data), Rowv=NA, dendrogram=""none"", Colv=NA, col=(matlab.like2(60)), trace=""none"", \r\n    main=""Daily temperature (C) \\n along Clear Creek"", \r\n    xlab=""Date"", ylab=""River km"", labRow=temps[,1], labCol = hh, key.title=NA, \r\n    key.xlab=""Temperature (C)"", key.ylab=NA, density.info = ""none"", srtCol = 90, \r\n    breaks=seq(0,30,0.5),\r\n    cex.main = 0.5, cexCol = 0.7, na.color=""gray"") \r\n#####\r\n\r\n#####Salmon distributions#####\r\n#Here, we are applying Clear Creek spring-run Chinook\r\n#see Table S3.1 and S3.2 in supplementary materials from this article\r\n#phenology, based on Julian day\r\narrival  <- rnorm(1000, mean = 164, sd = 26.4) #normal\r\nSpawning <- rnorm(1000, mean = 276, sd = 10.9) #normal\r\n#spatial, based on RKM\r\nreardistrib         <- runif(1000, min=0,    max=29.5) #uniform\r\nspatialreddsdistrib <- runif(1000, min=12.5, max=29.5) #uniform\r\n#####\r\n\r\n#####ADULT HOLDING ENERGY EXPENDITURE#####\r\nadholdreps <- replicate(1000, {\r\n#parameters\r\nj = 1.358442*(10^-5) #[MJ/mgO2] to convert from mgO2 kg-1 to MJ kg-1: 1 mgO2 = 1.358442*10-5 MJ\r\nc = 65.21*24 #mgO2/d/kg #Intercept for mass-specific maintenance rate; Martin et al. 2015\r\nb = -0.217 #Mass exponent for mass-specific maintenance rate #unitless #Martin et al. 2015\r\nd = 0.068 #C^-1 #Temperature exponent for maintenance rate; Martin et al. 2015\r\nM = 7.37 #kg #avg female mass from Bowerman et al. 2017\r\n\r\n#length of holding period, the recursive index\r\n#replaces days prior to 30d before arrival and 30d after last spawning day as NA\r\n#in other words, assumes that salmon do not arrive much earlier than data shows, or after last spawning date\r\nholdper <- J\r\nfor (i in 1:length(holdper)){\r\n  if (holdper[i] < (min(arrival)-30)){\r\n    holdper[i] <- NA\r\n  }else if(holdper[i] > (max(Spawning)+30)){\r\n    holdper[i] <- NA\r\n  }else{\r\n    holdper[i] <- holdper[i]\r\n  }\r\n}\r\n\r\n#assumes that arrivals from day 1 are holding until randomly picked spawning date, and arrivals after pk spawning do not hold through day 366\r\n#Spawning is spawning phenology distribution\r\nrandomSpawningDate <- sample(Spawning,1,replace=FALSE)\r\nprint(randomSpawningDate)\r\nholdper = randomSpawningDate - holdper\r\nfor (i in 1:length(holdper)){\r\n  if(is.na(holdper[i]) == TRUE){\r\n    holdper[i] <- NA\r\n  }else if(holdper[i] < 0){\r\n    holdper[i] <- 0\r\n  }else{\r\n    holdper[i] <- holdper[i]\r\n  }\r\n}\r\nholdper.func <- function(coln) {\r\n  holdper[coln]\r\n}\r\n\r\n#function and recursion\r\nP<- function(temp){return (j*c*(M^b)*exp(d*temp))} #temp is value from the matrix\r\n\r\nrecurRow <- function(rown, coln, netSum, RIn){\r\n  if(RIn==0){\r\n    return (netSum)\r\n  }else if(coln==length(holdper)){\r\n    return(netSum)\r\n  }else{\r\n    recurRow(rown, coln+1, P(temp = data[rown,coln])+netSum, RIn-1)\r\n  }\r\n}\r\n#rown is the row number we are adding first, \r\n  #coln is the col number, \r\n  #netSum is the added value across rows, or net sum for number of reps so far, \r\n  #and RIn is how many times we\'ve gone through so far (start with RIn=recurIndex, subtract 1 each time, end with 0)\r\n#may need to add a wrap-around part depending on your needs\r\n\r\nrecurWholeRowHelper <- function(rown, coln, vect){\r\n  if(coln > ncol(data)){\r\n    return (vect)\r\n  }else{\r\n    Holdingper<-as.integer(holdper.func(coln))\r\n    if(is.na(Holdingper) == TRUE){\r\n      val <- NA\r\n    }else{\r\n      val <- recurRow(rown,coln, 0, Holdingper)\r\n    }\r\n    vect[coln] <- val\r\n    recurWholeRowHelper(rown, coln+1,vect)\r\n    \r\n  }\r\n}\r\n\r\nrecurWholeRow <- function(rown){ recurWholeRowHel']",1,"1. Forest certification 
2. Forest Stewardship Council (FSC) 
3. Sustainable forest management 
4. Environmental standards 
5. Economic standards 
6. Social standards 
7. Forest conservation 
8. Certified forests"
Data from: Effects of forest certification on the ecological condition of Mediterranean streams,"1. Forest certification, a proxy for sustainable forest management, covers more than 10% of the world's forests. Under forest certification, forest managers and landowners must comply with environmental, economic and social management standards aiming to promote forest conservation. Despite an increasing area of certified forests, there is a dearth of data on how forest certification is affecting the conservation of forest ecosystems and associated habitats. 2. Here we assess the effects of Forest Stewardship Council (FSC) certification, one of the largest certification schemes in the world, on the ecological condition of streams crossing Mediterranean evergreen oak woodlands. 3. We used the Stream Visual Assessment Protocol (SVAP) to compare the ecological condition of streams located in areas with three and five years of certification, in non-certified areas and in least-disturbed streams. 4. Forest certification positively affected the ecological condition of the surveyed streams but its effects were only measurable after five years of certification. Streams with five years of certification had more continuous, dense and diverse riparian vegetation when compared to streams located in non-certified areas. Moreover, the condition of streams located in areas with five years of forest certification was similar to the condition of least- disturbed streams. 5. Synthesis and applications. Forest certification promotes the ecological condition of streams occurring within Mediterranean evergreen oak woodlands. This mainly happens because in areas under forest certification managers and landowners have to comply with management practices that require them to remove or reduce the main causes for stream degradation, allowing riparian habitats to recover. Within landscapes with large and increasing areas under forest certification, such as the Mediterranean cork oak woodlands, the positive effects of certification on the ecological condition of streams may spread across the hydrographic network in the medium to long term.","['###Description of the columns in the data.frame() ""data""\n\n#Reach - unique identifier for each reach\n#Estate - unique identifier for each estate\n#CCO - Channel condition score\n#HAL - Hydrologic alteration score\n#BCO - Bank condition score\n#RQT - Riparian quantity score\n#RQL - Riparian quality score\n#CAN - Canopy cover score\n#WAP - Water apperance score\n#MAN - Manure and human waste score\n#POO - Pools score\n#BFM - Barriers to fish movement score\n#FHA - Fish habitat complexity score\n#IHA - Aquatic invertebrate habitat score\n#x - x coordinate (longitude)\n#y - y coordinate (latitude)\n#Sites - four-level categorical variable: ""C3"" - three years of certification, ""C5"" - five years of certification\n#""LD"" - least-disturbed sites and ""NC"" - non-certified sites\n\n#Required packages\n\nlibrary(nlme)\nlibrary(multcomp)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(plyr)\n\n\n#Set ""NC"" as the baseline level for the variable ""Sites""\ndata$Sites<-relevel(data$Sites, ref=""NC"")\n\n#Calculate Variance Inflation Factor (VIF). \n\n#corvif() and myvif() were written by:\n#Mixed effects models and extensions in ecology with R. (2009).\n#Zuur, AF, Ieno, EN, Walker, N, Saveliev, AA, and Smith, GM. Springer.\n#Available at: http://www.highstat.com/BGS/GAM/HighstatLibV4.R\n\n\ncorvif(data[,4:15])\ncorvif(data[,4:14]) #Note that VIF scores lower when FHA is not considered. Please read the Methods section for an explanation on why IHA was kept in the analysis.\n\n\n\n\n###Analysis\n\n\n##Linear Mixed Effects Models\n\ncontrol_new<-lmeControl(maxIter=5000,niterEM = 1000, msMaxIter = 10000,opt=""optim"") #Alternative control values for lme fit\n\nSVAPm<-lme(SVAP~Sites,random = ~ 1 | Estate,method=""REML"",correlation = corLin(form =~ x + y, nugget = TRUE),data=data)\nCCOm<-lme(CCO~Sites,random = ~ 1 | Estate,weights=varIdent(form= ~1|Sites),method=""REML"",correlation = corGaus(form =~ x + y, nugget = TRUE),data=data)\nHALm<-lme(HAL~Sites,random = ~ 1 | Estate,method=""REML"",data=data)\nBCOm<-lme(BCO~Sites,random = ~ 1 | Estate,weights=varIdent(form= ~1|Sites), method=""REML"",correlation = corRatio(form =~ x + y, nugget = TRUE),data=data)\nRQTm<-lme(RQT~Sites,random = ~ 1 | Estate,weights=varIdent(form= ~1|Sites),method=""REML"",correlation = corRatio(form =~ x + y, nugget = TRUE),data=data)\nRQLm<-lme(RQL~Sites,random = ~ 1 | Estate,method=""REML"",control=control_new,correlation = corLin(form =~ x + y, nugget = TRUE),data=data)\nCANm<-lme(CAN~Sites,random = ~ 1 | Estate,method=""REML"",correlation = corRatio(form =~ x + y, nugget = TRUE),data=data)\nWAPm<-lme(WAP~Sites,random = ~ 1 | Estate,method=""REML"",correlation = corGaus(form =~ x + y, nugget = TRUE),data=data)\nNENm<-lme(NEN~Sites,random = ~ 1 | Estate,method=""REML"",correlation = corExp(form =~ x + y, nugget = TRUE),data=data)\nMANm<-lme(MAN~Sites,random = ~ 1 | Estate,weights=varIdent(form= ~1|Sites),method=""REML"",correlation = corExp(form =~ x + y, nugget = TRUE),data=data)\nPOOm<-lme(POO~Sites,random = ~ 1 | Estate,method=""REML"",correlation = corGaus(form =~ x + y, nugget = TRUE),data=data)\nBFMm<-lme(BFM~Sites,random = ~ 1 | Estate,method=""REML"",data=data)\nFHAm<-lme(FHA~Sites,random = ~ 1 | Estate,method=""REML"",control=control_new,correlation = corLin(form =~ x + y, nugget = TRUE),data=data)\nIHAm<-lme(IHA~Sites,random = ~ 1 | Estate,method=""REML"",correlation = corGaus(form =~ x + y, nugget = TRUE),data=data)\n\n\n\n##Model validation as described in the Methods section - Replace ""INSERT"" with one of the above models\n\nmodel<-INSERT\nEX<-resid(model,type=""normalized"")\nFX<-fitted(model)\n\n\nop<-par(mfrow=c(2,3),mar=c(3,3,3,3))\n\nplot(y=EX,x=FX,ylab=""Normalized Residuals"",xlab=""Fitted values"",main=""Residuals vs Fitted values"")\nabline(h=0)\nlines(lowess(y=EX,x=FX),col=2)\n\nplot(y=EX,x=data$Sites,ylab=""Normalized residuals"",xlab=""Sites"",main=""Residuals vs Sites"")\nlines(lowess(y=EX,x=data$Sites),col=2)\n\nplot(y=EX,x=data$Estate,ylab=""Normalized residuals"",xlab=""Local"",main=""Residuals vs Estate"")\nlines(lowess(y=EX,x=data$Estate),col=2)\n\nqqnorm(EX,main=""QQplot Residuals"")\n\ndata.frame(ranef(model))->ri\nqqnorm(ri$X.Intercept.,main=""QQplot Random Intercepts 99% CI"")\n\n\npar(mfrow=c(1,1))\n\n#Spatial autocorrelation\nplot(Variogram(model, form = ~ x +y, robust = TRUE))\n\n\n\n\n##Multiple comparisons -  Replace ""INSERT"" with one of the above models\n\nmodel<-SVAPm\ncomp <- glht(model,linfct = mcp(Sites=""Tukey""))\nsummary(comp)\n\n\n#Plot boxplots with compact letter-based display\n\ntuk.cld<-cld(comp)   \nopar <- par(mar=c(1,1,1.5,1))\nplot(tuk.cld)\npar(opar)\n\n\n#Code for Figure 2\n\n#SVAP\ndf<-data.frame(SVAP=data$SVAP,Sites=factor(data$Sites, levels=unique(data$Sites)))\np_meds <- ddply(df, .(Sites), summarise, med = median(SVAP))\na<-ggplot(df,aes(x=Sites,y=SVAP))+geom_boxplot()+stat_boxplot(geom =\'errorbar\') +guides(fill=FALSE)+\n  geom_point(data=p_meds, mapping=aes(x=Sites, y=med),size=4,shape=8) +\n  scale_y_continuous(limits=c(1,13))+labs(list(title=""SVAP"",x="""",y=""Score""))+theme(text = element_text(size=15))+\n  theme(axis.text.x = element_text(colour=""black"",size=15))+ \n  anno']",1,"Data 
- Code 
- Climate change 
- Salmonids 
- Productivity 
- Freshwater ecosystems 
- Global meta-analysis 
- Systematic review 
- Quantitative synthesis 
- Standardized effect size 
- Weight"
Data and code  Effects of climate on salmonid productivity: A global meta-analysis across freshwater ecosystems,"Salmonids are of immense socio-economic importance in much of the world but are threatened by climate change. This has generated a substantial literature documenting effects of climate variation on salmonid productivity in freshwater ecosystems, but there has been no global quantitative synthesis across studies. We conducted a systematic review and meta-analysis to gain quantitative insight into key factors shaping the effects of climate on salmonid productivity, ultimately collecting 1,321 correlations from 156 studies, representing 23 species across 24 countries. Fisher's Z was used as the standardized effect size, and a series of weighted mixed-effects models were compared to identify covariates that best explained variation in effects. Patterns in climate effects were complex, and were driven by spatial (latitude, elevation), temporal (time-period, age-class), and biological (range, habitat type, anadromy) variation within and among study populations. These trends were often consistent with predictions based on salmonid thermal tolerances. Namely, warming and decreased precipitation tended to reduce productivity when high temperatures challenged upper thermal limits, while opposite patterns were common when cold temperatures limited productivity. Overall, variable climate impacts on salmonids suggest that future declines in some locations may be counterbalanced by gains in others. In particular, we suggest that future warming should (1) increase salmonid productivity at high latitudes and elevations (especially >60 and >1,500m), (2) reduce productivity in populations experiencing hotter and dryer growing season conditions, (3) favor non-native over native salmonids, and (4) impact lentic populations less negatively than lotic ones. These patterns should help conservation and management organizations identify populations most vulnerable to climate change, which can then be prioritized for protective measures. Our framework enables broad inferences about future productivity that can inform decision-making under climate change for salmonids and other taxa, but more widespread, standardized, and hypothesis-driven research is needed to expand current knowledge.","['# Created by Brian Gallagher, June 3, 2022\r\n# Edited by Brian Gallagher, August 9, 2022\r\n# Gallagher et al_Meta-Analysis Results.R\r\n\r\n# The purpose of this script is to reproduce all of the main results shown in the manuscript\r\n# entitled \'Effects of climate on salmonid productivity: a global meta-analysis across freshwater ecosystems\', \r\n# submitted to Global Change Biology in June 2022.\r\n\r\n# This code is broken up into sections, which are separated by ### headers ### at the top of each section.\r\n# The first sections provide descriptions of the database and main variables of interest, and best-fit\r\n# models for the four datasets with Abundance-Precipitation (A-P), Abundance-Temperature (A-T),\r\n# Growth-Precipitation (G-P), and Growth-Temperature effects created in the Metafor package. Subsequently, \r\n# performance of best-fit models is assessed through AICc scores, likelihood ratio tests and pseudo-R2 \r\n# values, and mean effect sizes and heterogeneity are examined. Best-fit model coefficients are then displayed\r\n# and used to create plots for each data set. The last sections focus on various tests of model robustness to \r\n# collinearity, publication bias, taxonomic differences, methodological covariates, and influential studies.\r\n# When relevant, I add links to websites that aided programming within the Metafor package.\r\n \r\n# Metadata describing all variables in the database will be shared along with data and code. \r\n# This code is not exhaustive, and does not include model selection results for the sake of brevity (although\r\n# these can be obtained by running additional models with the data provided). Code used to identify studies\r\n# associated with publication bias in the A-P data set is also not included. For questions, concerns, or\r\n# requests for additional code, please contact Brian Gallagher at brian.kenneth.gallagher@gmail.com.\r\n\r\n\r\n\r\n##### Housekeeping #####\r\n\r\n# Specify location where meta-analysis data file was saved\r\nmeta_directory <- (""C:/..."")\r\n# Note: Data will be made available on Dryad prior to publication (see Data Availability Statement)\r\n\r\n# Set work directory\r\nsetwd(meta_directory)\r\n\r\n# Install packages - can skip if already installed\r\ninstall.packages(""ggplot2"")\r\ninstall.packages(""metafor"")\r\ninstall.packages(""data.table"")\r\n\r\n# Load libraries for installed packages\r\nlibrary(""ggplot2"")\r\nlibrary(""metafor"")\r\nlibrary(""data.table"")\r\n\r\n\r\n\r\n##### Input and summarize data #####\r\n\r\n# Load and summarize data - include stringsAsFactors argument to read categorical data as factors\r\nmeta <- read.csv(""Gallagher et al_Meta-Analysis Database_Updated May 2022.csv"", stringsAsFactors=TRUE)\r\nsummary(meta)\r\n\r\n# Make Study_Num a factor\r\nmeta$Study_Num <- as.factor(meta$Study_Num)\r\nsummary(meta)\r\n\r\n# Check levels of Study_Num and Study_Name, which should be identical\r\nnlevels(meta$Study_Num) #n=156 as expected\r\nnlevels(meta$Study_Name) #n=156 as expected\r\n\r\n# Summarize Abs_Lat and Elevation used in Stage 1 of model selection (continuous)\r\nsummary(meta$Abs_Lat) # Range from 35-80 as expected (see L in Table 1)\r\nsummary(meta$Altitude) # Range from 0-3000 as expected (see E in Table 1)\r\n\r\n# Check levels of Age_Class, Season, Life_Stage, and Life_Age_Int used in Stage 2\r\nlevels(meta$Age_Class) #n=4 as expected (see AC in Table 1)\r\nlevels(meta$Season) #n=5 as expected (see SE in Table 1)\r\nlevels(meta$Life_Stage) #n=7 as expected (see LS in Table 1)\r\nlevels(meta$Life_Age_Int) #n=10 as expected (see LSA in Table 1)\r\n\r\n# Check levels of binary study context variables used in Stage 3\r\nlevels(meta$Range) #n=2 as expected (see N in Table 1)\r\nlevels(meta$Anadromy) #n=2 as expected (see A in Table 1)\r\nlevels(meta$Study_Type) #n=2 as expected (see S in Table 1)\r\nlevels(meta$System_Type) #n=2 as expected (see H in Table 1)\r\n\r\n# Check levels of methodological covariates used in robustness tests\r\nlevels(meta$Response_Subtype) #n=7 as expected (see RT in Table 1)\r\nlevels(meta$Predictor_Subtype) #n=6 as expected (see PT in Table 1)\r\nlevels(meta$Data_Transformation) #n=2 as expected (see DT in Table 1)\r\nlevels(meta$Data_Source) #n=2 as expected (see DM in Table 1)\r\n\r\n\r\n\r\n##### Calculate effect sizes and create data sets #####\r\n\r\n# Use escalc function in Metafor to estimate Fisher\'s Z and its variance\r\nmeta <- escalc(measure=""ZCOR"", ri=Correlation, ni=Sample_Size, data=meta)\r\nmeta <- data.frame(meta)\r\nsummary(meta)\r\n# All estimates with a sample size < 5 have already been removed (see Supplementary Material)\r\n\r\n# Create Abundance-Precipitation data set\r\nmeta_abund_precip <- subset(meta, Subset==""Abundance-Precipitation"")\r\nsummary(meta_abund_precip)\r\n# n = 362\r\n\r\n# Create Abundance-Temperature data set\r\nmeta_abund_temp <- subset(meta, Subset==""Abundance-Temperature"")\r\nsummary(meta_abund_temp)\r\n# n = 610\r\n\r\n# Create Growth-Precipitation data set\r\nmeta_growth_precip <- subset(meta, Subset==""Growth-Precipitation"")\r\nsummary(meta_growth_precip)\r\n# n = 66\r\n\r\n# Create Growth-Temperature data set\r\nmeta_growth_temp <- subset(meta, Subset']",1,"Deforestation, unemployment, low-income, rural communities, Kebbi, Nigeria, environmental issue, advocacy, fuel wood, firewood, charcoal, domestic activities, commercial activities, secondary dataset, R-Programming, World Bank Microdata Library,"
"Deforestation: An implication of high rates of unemployed and low-Income household members in rural communities in Kebbi, Nigeria","Deforestation has been a major environmental issue in Northern Nigeria and despite several advocacy, yet such practice has continued to endure. A root course factors are unemployment and low-income among households in Northern Nigeria. As a result of low social/financial status of the household, they depend highly on cutting down trees to make fuel wood (firewood) and charcoal (gotten from burnt wood) for the source of energy for their domestic and commercial activities. This was proven by the analysis of a secondary dataset using R-Programming, sourced from the World Bank Microdata Library, a survey carried out in 2015 in Kebbi, Nigeria titled Nigeria-Feed the Future Livelihood Project 2015, Baseline Survey worldbank.org. The dataset contains 3976 households observed and 296 values were found missing which resulted to analyzing 3758 household after the missing values were omitted. Variables describing the employed members of the household, energy sources for lighting fuel and energy sources for cooking fuel for the households were extracted from the secondary dataset. Results revealed that there are over 3000 households dependent on fuel wood as an energy source. Also, results (ANOVA) revealed that annual income of the households has significant impact on cooking fuel sources (0.0104<0.05) and has a very strong significance on lighting fuel sources (0.000<0.05). The low-income rate of households was evident showing the relationship between employed members of the households with a very strong significance on annual income of the households (0.000375<0.05). This revealed that unemployed members of the households results to low-income of the households in Kebbi make them to highly depend on the use of fuel wood (firewood) and charcoal (gotten from burnt wood) on a dialy bases as a source of energy for cooking and lighting fuel. They also export and sell fuel wood and charcoal for financial gain to neighboring countries.","['# This is the code for the analysis of the data set ""Kebbi.csv"" for the \r\n# Environmental Data Analysis project (ENVDA -MOD 1) titled Deforestation:\r\n# An Implication of High Rate of Unemployed Household in Rural Communities \r\n# in Kebbi State, Nigeria.\r\n\r\n# I first set the working directory: Click on session tab, select working directory,\r\n# choose working directory and click on the folder where I save the data set \r\n# ""Kebbi.csv"".\r\n\r\n# Reading the data set ""Kebbi.csv""\r\nKebbitable<-read.csv(""Kebbi.csv"")\r\nhead(Kebbitable)\r\n\r\n# Check is there are missing value in the data set ""Kebbi.csv"".\r\nsummary(Kebbitable)\r\n\r\n# Omitting missing values from the data set ""Kebbi.csv"".\r\nKebbitable.1<-na.omit(Kebbitable)\r\nsummary(Kebbitable.1)\r\nView(Kebbitable.1)\r\n\r\nattach(Kebbitable.1)\r\n\r\n\r\npar(mfcol=c(1,2))\r\n\r\n# Get the frequency of employed members of the households in Kebbi\r\nhist(Employed_members,freq = TRUE,col = ""red"",main = paste(""Employed Members""))\r\nabline(v=mEmploy<-mean(Employed_members),lwd=2,lty=""dashed"")\r\n\r\n# Get the frequency of the annual income of households in Kebbi\r\nhist(Annual_income,freq = TRUE,col = ""red"",main = paste(""Annual Income""))\r\nabline(v=mIncome<-mean(Annual_income),lwd=2,lty=""dashed"")\r\n\r\npar(mfcol=c(1,2))\r\n\r\n# Get the frequency of the different sources of lighting fuel in households in Kebbi\r\nhist(Lighting_fuel_source,freq = TRUE,col = ""red"",main = paste(""Sources of Lighting Fuel""))\r\nabline(v=mLight<-mean(Lighting_fuel_source),lwd=2,lty=""dashed"")\r\n\r\n\r\n# Get the frequency of the different sources of cooking fuel in households in Kebbi\r\nhist(Cooking_fuel_source,freq = TRUE,col = ""red"",main = paste(""Sources of Cooking Fuel""))\r\nabline(v=mCook<-mean(Cooking_fuel_source),lwd=2,lty=""dashed"")\r\n\r\npar(mfcol=c(1,2))\r\n\r\n# Calculating the mean of the employed members of the households in Kebbi.\r\nboxplot(Employed_members, main = ""Mean of Employed Members"", horizontal = FALSE,col = ""red"")\r\nmEmploy\r\n\r\n# Calculating the mean of annual income of the households in Kebbi.\r\nboxplot(Annual_income, main = ""Mean of Annual Income"", horizontal = FALSE,col = ""red"")\r\nmIncome\r\n\r\npar(mfcol=c(1,2))\r\n\r\n# Calculating the mean of sources of lighting fuel of the households in Kebbi.\r\nboxplot(Lighting_fuel_source, main = ""Mean of Sources of Lighting Fuel"", horizontal = FALSE,col = ""red"")\r\nmLight\r\n\r\n# Calculating the mean of sources of cooking fuel of the households in Kebbi.\r\nboxplot(Cooking_fuel_source, main = ""Mean of Sources of Cooking Fuel"", horizontal = FALSE,col = ""red"")\r\nmCook\r\n\r\n\r\n# Hypothesis Testing\r\nlevels(Lighting_fuel_source)<-c(""collected firewood"",""purchased firewood"",""grass"",""kerosene"",""phcn electricity"",""generator"",""gas"",""battery"",""candle"")\r\nlevels(Cooking_fuel_source)<-c(""collected firewood"",""purchased firewood"",""coal"",""grass"",""kerosene"",""phcn electricity"",""generator"",""gas"",""others"")\r\nlevels(Annual_income)<-c(""N1"",""N2"",""N3"",""N4"",""N5"",""N6"",""N7"",""N8"",""N9"",""N10"",""N11"",""N12"",""N13"",""N14"",""N15"",""N16"",""N17"",""N18"",""N19"",""20"")\r\n\r\n\r\n## Testing for Null Hypothesis H0 (1a) The member of household that has worked in a business or self-employment in the past 12 months has no effect on the source of lighting fuel \r\nboxplot(Employed_members~Lighting_fuel_source,main=""emplyed members vs lighting fuel"",cex.lab=1.5,col=""lightblue"")\r\nsummary(aov(Employed_members~Lighting_fuel_source))\r\n\r\n## Testing for Null Hypothesis H0 (1b) The member of household that has worked in a business or self-employment in the past 12 months has no effect on the source of cooking fuel \r\nboxplot(Employed_members~Cooking_fuel_source,main=""emplyed members vs cooking fuel"",cex.lab=1.5,col=""lightblue"")\r\nsummary(aov(Employed_members~Cooking_fuel_source))\r\n\r\n# Testing for Null Hypothesis Ho (2) The member of household that has worked in a business or self-employment in the past 12 months has no effect on the number of naira for annual income of the member of the household \r\nboxplot(Employed_members~Annual_income,main=""employed members vs annual income"",cex.lab=1.5,col=""lightblue"")\r\nsummary(aov(Employed_members~Annual_income))\r\n\r\n## Testing for Null Hypothesis H0 (3a) The number of naira for annual income of the member of the household has no effect on the source of lightning and cooking fuel \r\nboxplot(Annual_income~Lighting_fuel_source,main=""annual income vs lighting fuel"",cex.lab=1.5,col=""lightblue"")\r\nsummary(aov(Annual_income~Lighting_fuel_source))\r\n\r\n## Testing for Null Hypothesis H0 (3b) The number of naira for annual income of the member of the household has no effect on the source of lightning and cooking fuel \r\nboxplot(Annual_income~Cooking_fuel_source,main=""annual income vs cooking fuel"",cex.lab=1.5,col=""lightblue"")\r\nsummary(aov(Annual_income~Cooking_fuel_source))\r\n']",1,"Data, analysis code, global protected areas, safeguard, mammals, human-induced extinction, conservation, ecosystem destruction, fragmentation, human population, economic activity, long-term survival, potential population size, criteria, robust protection, threatened, underprotected species,"
Data and analysis code for: Global protected areas seem insufficient to safeguard half of the world's mammals from human-induced extinction,"Protected areas (PAs) are a cornerstone of global conservation and central to international plans to minimize global extinctions. During the coming century, global ecosystem destruction and fragmentation associated with increased human population and economic activity could make the long-term survival of most terrestrial vertebrates even more dependent on PAs. However, the capacity of the current global PA network to sustain species for the long term is unknown. Here, we explore this question for all nonvolant terrestrial mammals for which we found sufficient data, 4,000 species. We first estimate the potential population size of each such mammal species in each PA and then use three different criteria to estimate if solely the current global network of PAs might be sufficient for their long-term survival. Our analyses suggest that current PAs may fail to provide robust protection for about half the species analyzed, including most species currently listed as threatened with extinction and a third of species not currently listed as threatened. Hundreds of mammal species appear to have no viable protected populations. Underprotected species were found across all body sizes, taxonomic groups, and geographic regions. Large-bodied mammals, endemic species, and those in high-biodiversity tropical regions were particularly poorly protected by existing PAs. As new international biodiversity targets are formulated, our results suggest that the global network of PAs must be greatly expanded and most importantly that PAs must be located in diverse regions that encompass species not currently protected and must be large enough to ensure that protected species can persist for the long term.","['# Read me -----\n# Author: David Williams\n# Date: 21st June 2018\n# Overview: Set of functions for PA project\n\n# Packages ----\nlibrary(raster)\nlibrary(dplyr)\n# library(sf)\nlibrary(purrr)\n# library(pryr)\nlibrary(foreign)\nlibrary(countrycode)\nlibrary(data.table)\n\n# 0) Helper functions -----\n# Split a list for parallelising -------\nchunk2 <- function(x,n) split(x, cut(seq_along(x), n, labels = FALSE)) \n\n# 1) Spatial analyses -----\n# 1.1) Set CRS ----\ncrs_fun <- function(rast, template){\n  crs(rast) <- crs(template)\n  return(rast)\n}\n\n# Masking ESH with PAs -----\npa_raster_fun <- function(esh_raster, pa_raster, directory){\n  rast_mod <- pa_raster * esh_raster\n  species <- names(esh_raster)\n  writeRaster(rast_mod,\n              filename = paste(directory, ""/"", species, "".tif"", sep = """"),\n              format = ""GTiff"", overwrite = TRUE)\n  rm(rast_mod)\n}\n\nmask_fun <- function(rast, mask_sf, directory, taxon_lookup){\n  rast_mod <- mask(rast, mask_sf)\n  species <- names(rast)\n  taxon <- taxon_lookup[taxon_lookup$species == species, ""taxon""]\n  writeRaster(rast_mod,\n              filename = paste(directory, ""/"", taxon, ""/"", species, "".tif"", sep = """"),\n              format = ""GTiff"", overwrite = TRUE)\n  rm(rast_mod)\n}\n\n# Wrapper for mask function ----\nmask_wrap_fun <- function(rast_list, mask_sf, directory, taxon_lookup){\n  out <- l_ply(rast_list, function(x) mask_fun(rast = x, \n                                               mask_sf = mask_sf,\n                                               directory = directory,\n                                               taxon_lookup = taxon_lookup))\n  return(out)               \n}\n\n# Grouping PA cells ----\npa_grouping_fun <- function(r, n1, n2){\n  name <- sub(n1, n2, r)\n  rast <- raster(r)\n  rast <- clump(rast, directions = 8)\n  writeRaster(rast, \n              filename = name,\n              overwrite = TRUE)\n}\n\n# wrap it\ngroup_wrap_fun <- function(rast_list, n1, n2){\n  out <- l_ply(rast_list, function(x) pa_grouping_fun(r = x, \n                                                      n1 = n1,\n                                                      n2 = n2))\n  return(out)               \n}\n\n\npa_grouping_fun2 <- function(r){\n  rast <- clump(r, directions = 8)\n  tmp <- stack(r, rast)\n}\n\n# Counting cells in PAs ------\npa_size_fun <- function(pa_id_raster, pa_group_raster, pa_buffered_raster = NULL, \n                         pa_buffered_raster_max = NULL){\n  if(!is.null(pa_buffered_raster) & \n     names(pa_id_raster) %in% names(pa_buffered_raster) &\n     names(pa_id_raster) %in% names(pa_buffered_raster_max)){\n    # Option 1: All rasters are present\n    pa_group_raster <- pa_group_raster[[names(pa_id_raster)]]\n    pa_buffered_raster <- pa_buffered_raster[[names(pa_id_raster)]]\n    pa_buffered_raster_max <- pa_buffered_raster_max[[names(pa_id_raster)]]\n    \n    dt <- data.table(taxon = ""Mammals"",\n                     species = names(pa_id_raster),\n                     WDPAID = getValues(pa_id_raster),\n                     PA_group = getValues(pa_group_raster),\n                     PA_group_buffered = getValues(pa_buffered_raster),\n                     PA_group_buffered_max = getValues(pa_buffered_raster_max))\n    dt <- dt[!is.na(WDPAID), .(no_cells = .N), by = .(taxon, species, WDPAID, \n                                                      PA_group, PA_group_buffered, PA_group_buffered_max)]\n    if(nrow(dt) == 0){\n      dt <- data.table(taxon = ""Mammals"",\n                       species = names(pa_id_raster),\n                       WDPAID = NA,\n                       PA_group = NA,\n                       PA_group_buffered = NA,\n                       PA_group_buffered_max = NA,\n                       no_cells = 0)\n    }\n  } else {\n    if(!is.null(pa_buffered_raster) & \n       !(names(pa_id_raster) %in% names(pa_buffered_raster)) &\n       names(pa_id_raster) %in% names(pa_buffered_raster_max)){\n      # Option 2: Buffered is absent, but max is present\n      # Use normal PA_group for buffered, use max for max\n      pa_group_raster <- pa_group_raster[[names(pa_id_raster)]]\n      pa_buffered_raster_max <- pa_buffered_raster_max[[names(pa_id_raster)]]\n      \n      dt <- data.table(taxon = ""Mammals"",\n                       species = names(pa_id_raster),\n                       WDPAID = getValues(pa_id_raster),\n                       PA_group = getValues(pa_group_raster),\n                       PA_group_buffered = getValues(pa_group_raster),\n                       PA_group_buffered_max = getValues(pa_buffered_raster_max))\n      dt <- dt[!is.na(WDPAID), .(no_cells = .N), by = .(taxon, species, WDPAID, \n                                                        PA_group, PA_group_buffered, PA_group_buffered_max)]\n      if(nrow(dt) == 0){\n        dt <- data.table(taxon = ""Mammals"",\n                         species = names(pa_id_raster),\n                         WDPAID = NA,\n                         PA_group = NA,\n                         PA_group_buffered = NA,\n                         PA_group_buffered_max = ', '# READ ME -----\n# This script is preparation for analysis. It \n# 1) Creates rasters of climate variables needed to estimate population densities. Data from https://www.worldclim.org/data/worldclim21.html\n# 2) Uses data from Santini et al 2018a and those climate variables to refit models from Santini et al 2018b for population density\n#    Data from https://figshare.com/articles/TetraDENSITY_Population_Density_dataset/5371633 \n# 3) Estimates dispersal distances based on body mass from https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/08-1494.1\n# 4) Estimates MVPs based on Hilbers et al (2016) https://doi.org/10.1111/cobi.12846\n\nlibrary(raster)\nlibrary(data.table)\nlibrary(lme4)\nlibrary(MuMIn)\nlibrary(blmeco)\nlibrary(tidyverse)\n\n# 1) Climate rasters -----\n# Use NPP and coefficient of variability of precipitation. Data from https://www.worldclim.org/data/worldclim21.html\nnpp_raster <- raster(""Data/npp_geotiff.tif"")\n#Need to replace negative values with NAs:\nnpp_raster[npp_raster < 0] <- NA\n# Follow Santini and take log10, aggregate to 1 degree\nnpp_raster_log10 <- log10(npp_raster)\nnpp_raster_log10_upscaled <- aggregate(npp_raster_log10, \n                                       fact = 4, \n                                       fun = mean)\n# Repeat for precipitation. \nprecip_stack <- list.files(""Data/ClimateData_Fick_Hijams2017/wc2.0_10m_prec/"",\n                           pattern = ""tif"",\n                           full.names = TRUE)\nprecip_stack <- stack(precip_stack)\n\ncv <- cv(precip_stack)\ncv_upscaled <- raster::aggregate(x = cv, fact = 6)\n\n# Save rasters\nwriteRaster(npp_raster_log10_upscaled,\n            filename = ""GeneratedData/NPPForSantiniModels.tif"",\n            overwrite = TRUE)\nwriteRaster(cv_upscaled,\n            filename = ""GeneratedData/PcvForSantiniModels.tif"",\n            overwrite = TRUE)\nrm(list)\n# 2) Refit Santini models ------\n# 2a) Load density data from Santini et al 2018a ----\n# Available from https://figshare.com/articles/TetraDENSITY_Population_Density_dataset/5371633\ntetra_data <- read_csv(""Data/TetraDENSITY/TetraDENSITY.csv"") %>%\n  filter(Class == ""Mammalia"") %>%\n  select(Class:Species, Longitude, Latitude, Density) %>%\n  filter(complete.cases(.))\n\nnames(tetra_data) <- tolower(names(tetra_data))\n\n# Get coordinates for spatial data\ncoords <- tetra_data %>%\n  select(x = longitude,\n         y = latitude)\n\n# 2b) Load explanatory variables and bind to density data ----\nbody_mass_data <- read_csv(""Data/BodyMassEstimates_AllSpecies.csv"")\nname_conversion <- read_csv(""GeneratedData/SantiniNameConversion.csv"") # A file to convert Santini names to IUCN standard\nnpp <- raster(""GeneratedData/NPPForSantiniModels.tif"")\npcv <- raster(""GeneratedData/PcvForSantiniModels.tif"")\nrichness <- raster(""Data/Mammals_Richness.tif"")\n\n# Reproject species richness to the same low resolution and WGS84\nrichness_wg84 <- projectRaster(richness, npp)\n\ntetra_data <- tetra_data %>%\n  mutate_at(.vars = c(""class"", ""order"", ""family""), toupper) %>%\n  mutate(santini_binomial = paste(genus, species, sep = ""_"")) %>%\n  left_join(., name_conversion) %>%\n  mutate(binomial = ifelse(is.na(iucn_binomial),\n                           yes = santini_binomial,\n                           no = iucn_binomial)) %>%\n  left_join(., body_mass_data %>%\n              select(binomial, combined_body_mass_g)) %>%\n# Need to manually add BM estimates for a couple of species\n  mutate(combined_body_mass_g = case_when(binomial == ""Camelus_dromedarius"" ~ 488000,\n                                 binomial == ""Alouatta_seniculus"" ~ 6398.31,\n                                 binomial == ""Cebus_capucinus"" ~ 3010,\n                                 binomial == ""Pithecia_monachus"" ~ 2110,\n                                 TRUE ~ combined_body_mass_g)) %>%\n  mutate(log10_density = log10(density),\n         log10_body_mass_g = log10(combined_body_mass_g),\n         npp = raster::extract(npp, coords),\n         pcv = raster::extract(pcv, coords),\n         richness = raster::extract(richness_wg84, coords)) %>%\n  select(class:species, binomial,\n         density, log10_density,\n         combined_body_mass_g, log10_body_mass_g,\n         longitude, latitude, npp, pcv, richness) %>%\n  filter(complete.cases(.))\n\nwrite_csv(tetra_data,\n          ""GeneratedData/SantiniModellingData.csv"")\n\n# 2c) Refit the models from Santini et al 2018 ----\n# First rescale the data for multi-model inference\nmean_sd_data <- data.frame(var = c(""log10_density"", ""log10_body_mass_g"", ""npp"", ""pcv"", ""richness""),\n                           mean = NA,\n                           sd = NA,\n                           stringsAsFactors = FALSE)\n\nfor(i in 1:nrow(mean_sd_data)){\n  mean_sd_data$mean[i] <- mean(tetra_data[[mean_sd_data[i,""var""]]], na.rm = TRUE)\n  mean_sd_data$sd[i] <- sd(tetra_data[[mean_sd_data[i,""var""]]], na.rm = TRUE)\n}\n\nfor(i in 1:nrow(mean_sd_data)){\n  n <- paste(mean_sd_data[i,""var""], ""rescaled"", sep = ""_"")\n  tetra_data[[n]] <- (tetra_data[[mean_sd_data[i,""var""]]] - mean_sd_data$mean[i]) /', '#!/usr/bin/env Rscript\n\n# Read me -----\n# Author: David Williams\n# Overview: Calculates the overlap between each species\' ESH and protected areas\n# PA data downloaded from WDPA (https://www.protectedplanet.net/) and rasterised at 1.5 km resolution, \n# following WDPA suggestions on data processing\n# Mammal habitat maps from Carlo Rondinini\n#\n# NOTE: This script is very memory hungry and slow to run! \n# Expect to spend days on a cluster with hundreds of GB of memory\n# If anyone wants to make it more efficient, I would be indebted (Python would be the way forward)\n#\n# Specifics\n# 1) Loads and cleans data\n# 2) Multiplies each habitat raster by the WDPA and saves the overlap\n# 3) Groups adjacent cells and gives each group a unique ID\n# 4) Repeats (2-3) for PAs buffered to account for median dispersal distances\n# 5) Repeats (2-3) for PAs buffered to account for maximum dispersal distances\n# Packages ----\nlibrary(raster)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(sf)\nlibrary(purrr)\nlibrary(foreign)\nlibrary(countrycode)\nlibrary(data.table)\nlibrary(doMC)\nrasterOptions(maxmemory = 1e+11)\noptions(tibble.width = Inf)\n\n# 1) Load data and functions ----\nrm(list=ls())\n# Set the number of cores to run across.\nn_cores <- 5\nregisterDoMC(cores = n_cores)\n\n# Loads functions\nsource(""Scripts/0_Functions.R"")\n\n# Load WDPA data\nwdpa_raster <- raster(""Data/WDPA_Feb2019_Mollweide1.5km.tif"")\n\n# Habitat data \n# Data are individual rasters, one for each species\nesh_files <- list.files(path = ""Data/ESH_Mollweide_1.5km/Mammals"",\n                          pattern = "".tif"",\n                          full.names = TRUE)\n# Clean up in case there are rogue xml files...\nif(sum(grep("".xml"", esh_files)) > 0){\n  esh_files <- esh_files[-grep("".xml"", esh_files)]\n}\n# Name files with species name\nnames(esh_files) <- gsub(""(^.*/)(.*)(\\\\_HabElev.tif)"", ""\\\\2"", esh_files)\n\n# 1)b) Exclude bats ----\ntaxonomy <- fread(""Submission/GeneratedData/DispersalDistances_Estimated.csv"")\nbats <- taxonomy %>%\n  filter(order == ""CHIROPTERA"") %>%\n  select(binomial)\n\nesh_files <- esh_files[!(names(esh_files) %in% bats)]\n\n# 2) Multiply habitat rasters by WDPA ----\n# This saves new rasters into the directory ""GeneratedData/PARasters/""\n# Subset to species that haven\'t been run yet (you can skip this if you back yourself to do it in one...)\nrun <- list.files(""GeneratedData/PARasters"")\nif(sum(grep("".xml"", run)) > 0){\n  run <- run[-grep("".xml"", run)]\n}\nrun <- gsub(""\\\\.tif"", """", run)\n\nesh_files <- esh_files[!(names(esh_files) %in% run)]\nif(length(esh_files) > 0){\n  splits <- c()\n  for(i in 1:(n_cores - 1)){\n    if(i == 1){\n      splits[i] <- ceiling(length(esh_files) / n_cores)\n    } else {\n      splits[i] <- ceiling(length(esh_files) / n_cores) + splits[i - 1]\n    }\n  }\n  split_list <- split(x = esh_files, f = cumsum(1:length(esh_files) %in% splits))\n  \n  # Load the rasters all at once\n  # t1 <- Sys.time()\n  esh_files <- foreach(i = 1:length(split_list)) %dopar% map(split_list[[i]], raster)\n  esh_files <- unlist(esh_files, recursive = FALSE)\n  rm(split_list)\n  # Sys.time() - t1 #\n  \n  # Set CRS\n  esh_files <- map(.x = esh_files, .f = crs_fun, template = wdpa_raster)\n  \n  # Get the names better\n  for(i in 1:length(esh_files)){\n    names(esh_files[[i]]) <- names(esh_files)[i]\n  }\n  \n  # 3) Multiply ESH files by WDPA -----\n  # Split the ESH files across cores\n  split_esh <- chunk2(x = esh_files, \n                      n = n_cores)\n  \n  # t1 <- Sys.time()\n  foreach(i = 1:length(split_esh)) %dopar% map(split_esh[[i]], \n                                               pa_raster_fun, \n                                               pa_raster = wdpa_raster, \n                                               directory = ""GeneratedData/PARasters"")\n  # Sys.time() - t1 \n} else {\n  print(""No PAs needed"")\n}\n\n# 3) Group adjacent cells ----\n# I need to group PAs, because some will be adjacent, others will have multiple habitat\n#     patches within one PA\n# This groups adjacent cells, with each group having its own ID\n# It then stores the grouped rasters in ""GeneratedData/PARasters_Grouped/""\nrm(list=ls()[ls() != ""n_cores""])\n# Reload the functions\nsource(""Scripts/0_Functions.R"")\n# List the PA-habitat overlap rasters\nraster_files <- list.files(""GeneratedData/PARasters"", \n                           recursive = TRUE, \n                           full.names = TRUE)\nif(length(grep(""xml"", raster_files)) > 0) raster_files <- raster_files[-grep(""xml"", raster_files)]\n\n# Cut run files (again, you can skip)\nrun <- list.files(""PARasters_Grouped"", \n                  recursive = TRUE,\n                  full.names = TRUE)\nif(sum(grep("".xml"", run)) > 0){\n  run <- run[-grep("".xml"", run)]\n}\n\nrun <- gsub(""_Grouped"", """", run)\nraster_files <- raster_files[!(raster_files %in% run)]\n\n# Do the grouping function\nif(length(raster_files) > 0){\n  split_raster_files <- chunk2(x = raster_files, \n                               n = n_cores)\n  \n  # t1 <- Sys.time()\n  foreach(i = 1:length(split_raster_files)) %dopar% group_wrap_fun(rast_', '#!/usr/bin/env Rscript\n\n# Read me -----\n# Author: David Williams\n# Overview: Extracts the mean NPP and Pcv, and species richness, for each species\' PA groups (from 2_Habitat_PA_overlap.R)\n# These covariates are needed to estimate population densities\n# Packages ----\nlibrary(raster)\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(purrr)\nlibrary(foreign)\nlibrary(countrycode)\nlibrary(data.table)\nlibrary(doMC)\nrasterOptions(maxmemory = 1e+11)\n# Clean up -----\nrm(list=ls())\nn_cores <- 20\nregisterDoMC(cores = n_cores)\nsource(""Scripts/0_Functions.R"")\n\n# Load the rasters of PAs, and climate (produced in ""1_DataPrep_ClimateDensitiesDispersal.R"")\ngroup_rasters <- list.files(""GeneratedData/PARasters_Grouped"", \n                            recursive = TRUE, \n                            full.names = TRUE)\nif(length(grep(""xml"", group_rasters)) > 0) group_rasters <- group_rasters[-grep(""xml"", group_rasters)]\n\ngroup_rasters <- map(group_rasters, raster)\nnames(group_rasters) <- map_chr(group_rasters, names)\n\nnpp <- raster(""GeneratedData/NPPForSantiniModels.tif"")\npcv <- raster(""GeneratedData/PcvForSantiniModels.tif"")\nrichness <- raster(""Data/Mammals_Richness.tif"")\n\nsplit_group_rasters <- chunk2(x = group_rasters, \n                              n = n_cores)\n\npa_climate_list <- foreach(i = 1:length(split_group_rasters)) %dopar% map_df(.x = split_group_rasters[[i]], \n                                                                             climate_fun, \n                                                                             npp, \n                                                                             pcv, \n                                                                             richness,\n                                                                             .id = ""species"")\npa_climate_list <- rbindlist(pa_climate_list)\n\n# Save this ----\nfwrite(pa_climate_list, \n       file = ""GeneratedData/PAClimates_AllMammals.csv"")\n\n# Then bind to PA sizes ----\npa_size_list <- fread(""GeneratedData/PASizes_AllMammals.csv"")\n\n# Merge seemed to be deleting PAs without IDs (i.e. for those species with no coverage at all),\n# so shifting to left_join, which is slower but should work\npa_size_climate_list <- left_join(pa_size_list, pa_climate_list)\n\n# Save final DT -----\nfwrite(pa_size_climate_list,\n       file = ""GeneratedData/PASizesWithClimates_AllMammals.csv"")\n\n\n\n', '# Read me -----\n# Takes the areas of habitats (from 2_Habitat_PA_overlap.R) and combines them with population\n# density estimates (from 1_DataPrep_ClimateDensitiesDispersalMVPs.R) to estimate population sizes in \n# each PA-cluster.\n# \n# Finally it compares these to the population targets (also from 1_DataPrep_ClimateDensitiesDispersalMVPs.R)\n#\n# Packages ----\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(igraph) \noptions(tibble.width = Inf) \n\n# Data -----\nrm(list=ls())\npa_size_list <- fread(""GeneratedData/PASizes_AllMammals.csv"")\nsetnames(pa_size_list, ""species"", ""binomial"")\nbody_mass_data <- read_csv(""Data/BodyMassEstimates_AllSpecies.csv"")\n\nfixed_effects <- read_csv(""GeneratedData/RefittedSantiniModelCoefficients_fixed.csv"") %>%\n  mutate(taxon = ""Mammals"") %>%\n  select(-class)\nbinomial_effects <- read_csv(""GeneratedData/RefittedSantiniModelCoefficients_random_binomial.csv"")\nfamily_effects <- read_csv(""GeneratedData/RefittedSantiniModelCoefficients_random_family.csv"")\norder_effects <- read_csv(""GeneratedData/RefittedSantiniModelCoefficients_random_order.csv"")\n\n# Load country-of-occurrence data (downloaded from IUCN, https://www.iucnredlist.org/)\nc_occurrence <- data.frame(fread(""Data/IUCN_CountryPresence_mammals.csv""))\n\nmvps <- read_csv(""GeneratedData/MVPEstimates_All.csv"")\n\nspecies_list <- c_occurrence %>%\n  mutate(species_name = gsub(""^.* "", """", Species),\n         binomial = gsub("" "", ""_"", Species)) %>%\n  select(order = Order,\n         family = Family,\n         genus = Genus,\n         species_name, \n         binomial, \n         Category) %>%\n  filter(!duplicated(.),\n         order != ""CHIROPTERA"",\n         binomial %in% body_mass_data$binomial) %>%\n  rename(iucn_category = Category)\n\nregion_lookup <- c_occurrence %>%\n  mutate(species_name = gsub(""^.* "", """", Species),\n         binomial = gsub("" "", ""_"", Species)) %>%\n  rename(region = Region) %>%\n  select(binomial, region) %>%\n  filter(!duplicated(.))\n\n# Quickly look at how many families/orders don\'t have estimates -----\ndata.frame(family = species_list$family) %>%\n  mutate(included = case_when(family %in% family_effects$family ~ TRUE,\n                              TRUE ~ FALSE)) %>%\n  group_by(included) %>%\n  summarise(no_fam = length(unique(family))) %>%\n  ungroup() %>%\n  mutate(total = sum(no_fam),\n         prop = no_fam / total)\n# 60% of families have estimates\ndata.frame(order = species_list$order) %>%\n  mutate(included = case_when(order %in% order_effects$order ~ TRUE,\n                              TRUE ~ FALSE)) %>%\n  group_by(included) %>%\n  summarise(no_order = length(unique(order))) %>%\n  ungroup() %>%\n  mutate(total = sum(no_order),\n         prop = no_order / total)\n# All orders have estimates\n\n# 1) Estimate populations in clusters ----\npopulation_estimates <- pa_size_list %>%\n  # First join to body mass data\n  filter(binomial %in% body_mass_data$binomial) %>%\n  left_join(., body_mass_data %>%\n              select(order:binomial,\n                     body_mass_g = combined_body_mass_g,\n                     log10_bodymass_g)) %>%\n  # Next join to the Santini coefficients\n  left_join(., fixed_effects) %>%\n  left_join(., binomial_effects) %>%\n  left_join(., family_effects) %>%\n  left_join(., order_effects)\n\n# Exclude areas with an NA for NPP or Pcv\n# Need to replace NAs in the intercepts with zeros (except for order: I\'m excluding bats)\npopulation_estimates <- population_estimates %>%\n  mutate(binomial_intercept_adj_weighted  = ifelse(is.na(binomial_intercept_adj_weighted),\n                                                   yes = 0, \n                                                   no = binomial_intercept_adj_weighted),\n         family_intercept_adj_weighted  = ifelse(is.na(family_intercept_adj_weighted),\n                                                 yes = 0, \n                                                 no = family_intercept_adj_weighted))\n\n# Estimate the population density from Santini\npopulation_estimates <- population_estimates %>%\n  mutate(\n    log10_santini_pop_dens_km = intercept_weighted_coef + \n      binomial_intercept_adj_weighted + family_intercept_adj_weighted + order_intercept_adj_weighted + \n      log10_bodymass_g * log10_body_mass_g_weighted_coef +\n      log10_bodymass_g^2 * log10_body_mass_g2_weighted_coef + \n      log10_bodymass_g^3 * log10_body_mass_g3_weighted_coef + \n      mean_npp * npp_weighted_coef + \n      mean_npp^2 * npp2_weighted_coef + \n      mean_pcv * pcv_weighted_coef + \n      mean_pcv^2 * pcv2_weighted_coef + \n      mean_richness * richness_weighted_coef,\n    santini_pop_dens_km = 10^log10_santini_pop_dens_km,\n    population_santini = santini_pop_dens_km * area)\n\n# Need to put 0s where there are NAs when area equals 0\npopulation_estimates <- population_estimates %>%\n  mutate(population_santini = ifelse(area == 0,\n                                     yes = 0,\n                                     no = population_santini))\n\n# 2) Sum up populations in the same PA group and buffered-PA-group -----\n# To do the gr', '# Read me ----\n# Overlapping PA maps with ecoregion and country maps to get an estimate of how many countries/ecoregions have viable populations\nlibrary(raster)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(data.table)\nlibrary(doMC)\nlibrary(ggplot2)\n\n# Clean up ----\nrm(list=ls())\noptions(tibble.width = Inf) \n\nsource(""Scripts/0_Functions.R"")\nn_cores <- 20\nregisterDoMC(cores = n_cores)\n\n# 1) Load PA viability data -----\npa_size_list <- fread(""GeneratedData/PopulationEstimates_AllSpecies_SantiniEstimates_WithMVPs_GroupedPAs.csv"")\npa_size_list <- pa_size_list[, .(binomial, cluster, area, `Threshold Hilbers`, `Threshold Reed`, `Threshold Lande`)]\n\n# Get conversion between PA-grouping and cluster number \nconversion <- fread(""GeneratedData/LinksBetweenClustersAndWDPAIDs.csv"")\nconversion[, WDPAID := NULL]\n\n# 2) Load ecoregion and country rasters ----\necoregion_raster <- raster(""Data/WWF_Ecoregions_Mollweide_1500m.tif"")\ncountry_raster <- raster(""Data/MollweideCountryID_1.5km.tif"")\n\n# Load and split pa rasters into lists -----\nspecies_rasters <- list.files(""GeneratedData/PARasters_Grouped"", \n                              recursive = TRUE, \n                              full.names = TRUE)\nif(length(grep("".xml"", species_rasters)) > 0){\n  species_rasters <- species_rasters[-grep("".xml"", species_rasters)]\n}\nnames(species_rasters) <- gsub(""(^.*/)(.*)(\\\\.tif)"", ""\\\\2"", species_rasters)\nspecies_rasters <- species_rasters[names(species_rasters) %in% pa_size_list$binomial]\n\nsplits <- c()\nfor(i in 1:(n_cores - 1)){\n  if(i == 1){\n    splits[i] <- ceiling(length(species_rasters) / n_cores)\n  } else {\n    splits[i] <- ceiling(length(species_rasters) / n_cores) + splits[i - 1]\n  }\n}\nspecies_rasters <- split(x = species_rasters, f = cumsum(1:length(species_rasters) %in% splits))\n\n# Extract ecoregion and country data  -----\n# Dummy df to match the function\ntaxon_lookup = data.frame(species = unique(pa_size_list$binomial),\n                          taxon = ""Mammals"")\n\n# Ecoregions\npa_ecoregion_list <- foreach(i = 1:length(species_rasters)) %dopar% map_df(.x = species_rasters[[i]],\n                                                                            .f = eco_c_fun,\n                                                                            variable_raster = ecoregion_raster)\npa_ecoregion_list <- rbindlist(pa_ecoregion_list, use.names = TRUE)\npa_ecoregion_list[, no_cells := NULL]\npa_ecoregion_list <- merge(pa_ecoregion_list, conversion)\npa_ecoregion_list[, PA_group := NULL]\npa_ecoregion_list <- unique(pa_ecoregion_list)\npa_ecoregion_list <- merge(pa_size_list, pa_ecoregion_list, \n                           by = c( ""binomial"", ""cluster""),\n                           allow.cartesian = TRUE)\n\nfwrite(pa_ecoregion_list, \n       file = ""GeneratedData/PAEcoregionOverlap.csv"")\nrm(pa_ecoregion_list)\n# Countries\npa_country_list <- foreach(i = 1:length(species_rasters)) %dopar% map_df(.x = species_rasters[[i]],\n                                                                            .f = eco_c_fun,\n                                                                            variable_raster = country_raster)\npa_country_list <- rbindlist(pa_country_list)\npa_country_list[, no_cells := NULL]\npa_country_list <- merge(pa_country_list, conversion)\npa_country_list[, PA_group := NULL]\npa_country_list <- unique(pa_country_list)\npa_country_list <- merge(pa_size_list, pa_country_list, \n                           by = c( ""binomial"", ""cluster""),\n                           allow.cartesian = TRUE)\n\nfwrite(pa_country_list, \n       file = ""GeneratedData/PACountryOverlap.csv"")\n\n# Repeat for full ESH maps -----\nrm(list=ls())\nsource(""Scripts/0_Functions.R"")\nn_cores <- 20\nregisterDoMC(cores = n_cores)\n\ncountry_raster <- raster(""Data/MollweideCountryID_1.5km.tif"")\necoregion_raster <- raster(""Data/WWF_Ecoregions_Mollweide_1500m.tif"")\n\npa_size_list <- fread(""GeneratedData/PopulationEstimates_AllSpecies_SantiniEstimates_WithMVPs.csv"")\npa_size_list <- pa_size_list[, .(binomial)]\nspecies_rasters <- list.files(""/scratch/williams/LandExpansionBiodiversity/ESHFiles/Mammals"",\n                              recursive = TRUE, \n                              full.names = TRUE)\n\nif(length(grep("".xml"", species_rasters)) > 0){\n  species_rasters <- species_rasters[-grep("".xml"", species_rasters)]\n}\nnames(species_rasters) <- gsub(""(^.*/)(.*)(\\\\_HabElev.tif)"", ""\\\\2"", species_rasters)\nspecies_rasters <- species_rasters[names(species_rasters) %in% pa_size_list$binomial]\n\nsplits <- c()\nfor(i in 1:(n_cores - 1)){\n  if(i == 1){\n    splits[i] <- ceiling(length(species_rasters) / n_cores)\n  } else {\n    splits[i] <- ceiling(length(species_rasters) / n_cores) + splits[i - 1]\n  }\n}\nspecies_rasters <- split(x = species_rasters, f = cumsum(1:length(species_rasters) %in% splits))\n\nesh_country_list <- foreach(i = 1:length(species_rasters)) %dopar% map_df(.x = species_rasters[[i]],\n                                                                         .f = esh_c_fun,\n                    ']",1,"- Plant life strategies
- Tropical forests
- Evolutionary processes
- Ecological processes
- Leaf traits
- Stem traits
- Root traits
- Resource optimization
- Environmental gradients
- Trait syndromes
- Phylogenetic"
"Data for: Resolving whole-plant economics from leaf, stem and root traits of 1467 Amazonian tree species","It remains unclear how evolutionary and ecological processes have shaped the wide variety of plant life strategies, especially in highly diverse ecosystems like tropical forests. Some evidence suggests that species have diversified across a gradient of ecological strategies, with different plant tissues converging to optimize resource use across environmental gradients. Alternative hypotheses propose that species have diversified following independent selection on different tissues, resulting in a decoupling of trait syndromes across organs.To shed light on the subject, we assembled an unprecedented dataset combining 19 leaf, stem and root traits for 1467 tropical tree species inventoried across 71 0.1-ha plots spanning broad environmental gradients in French Guiana.Nearly 50% of the overall functional heterogeneity was expressed along four orthogonal dimensions, after accounting for phylogenetic dependences among species. The first dimension related to fine root functioning, while the second and third dimensions depicted two decoupled leaf economics spectra, and the fourth dimension encompassed a wood economics spectrum. Traits involved in orthogonal plant functional strategies, five leaf traits in particular but also trunk bark thickness, were consistently associated with a same gradient of soil texture and nutrient availability. Root traits did not show any significant association with edaphic variation, possibly because of the prevailing influence of other factors (mycorrhizal symbiosis, phylogenetic constraints).Our study emphasises the existence of multiple functional dimensions that allow tropical tree species to optimize their performance in a given environment, bringing new insights into the debate around the presence of a whole plant economic spectrum in tropical forest tree communities. It also emphasizes the key role that soil heterogeneity plays in shaping tree species assembly. The extent to which different organs are decoupled and respond to environmental gradients may help to improve our predictions of species distribution changes in responses to habitat modification and environmental changes.","['###################################################################################\r\n#**********************************************************************************\r\n## APPENDIX S7: R code to perform data analyses described in the methods section ##\r\n#**********************************************************************************\r\n###################################################################################\r\nsetwd(""C:/Users/Jason/Dropbox/DIADEMA Analyses/Data/Arbres/2020(composition+traits)/TRAITS/IMPUTATION/Imputations2/ms5(data_paper)/Jesus_Chris"")\r\nrm(list=ls())\r\n#knitr::stitch(\'AppendixS7_v3.R\')\r\n\r\n\r\n#######################\r\n## Download packages ##\r\n#######################\r\n\r\nlibrary(vegan)        ## many functions to calculate dissimilarities, perform PCA...\r\nlibrary(adespatial)   ## to perform MSR tests\r\nlibrary(spdep)        ## to work with adespatial\r\nlibrary(BiodiversityR)## broken stick models for PCA\r\nlibrary(ggplot2)      ## plot packages\r\nlibrary(FactoMineR)   ## PCA plot\r\nlibrary(""factoextra"") ## PCA plot\r\nlibrary(ade4)         ## plot functions\r\nlibrary(forecast)     ## for data normalisation\r\nlibrary(ecodist)      ## for MRM models\r\nlibrary(relaimpo)     ## for LMG calculations\r\nlibrary(ade4)         ## for plots and PCA \r\nlibrary(hypervolume)  ## to compute multi-dimensional trait space\r\nlibrary(labdsv)       ## Ordination and Multivariate Analysis\r\nlibrary(RColorBrewer) ## color design\r\nlibrary(BHPMF)        ## for data imputations\r\nlibrary(phytools)     ## phylogenetic analyses\r\nlibrary(ape)\r\nlibrary(brranching)\r\nlibrary(phylotate)\r\nlibrary(stringr)\r\nlibrary(motmot)\r\nlibrary(rethinking)   ## Bayesian confidence interval\r\n#citation(""vegan"")    ## package citation\r\n\r\n\r\n##############################\r\n##      Open data file      ##\r\n##############################\r\n\r\n## Environmental data (E1 = untransformed; E2 = normalised-standardised):\r\n#------------------------------------------------------------------------\r\nE   <- as.data.frame(read.table(""AppendixS4.txt"",h =T,row.names = 1))\r\ncolnames(E)\r\nE1  <- E[,-c(1:4)]\r\n# Normalising soil variables:\r\nE2  <- E1\r\nlambda <- as.data.frame(matrix(ncol=ncol(E2),nrow=1)) ; colnames(lambda) <- colnames(E2)\r\nfor(i in 1:ncol(E2)){\r\n  lambda[1,i] <- BoxCox.lambda(E2[,i]+abs(min(E2[,i],na.rm=TRUE))+0.00000000000000000000001, \r\n                                method = ""loglik"", lower = -2,  upper = 2)\r\n  E2[,i]    <- BoxCox(E2[,i], lambda=lambda[1,i])\r\n}\r\n#par(mfrow=c(5,5),mex=0.3)\r\n#for(j in 1:ncol(E2)){hist(E2[,j],col=""red"",main = colnames(E2)[j])}\r\n#par(mfrow=c(1,1))\r\n## Standardising variables:\r\nE2 <- decostand(E2,""standardize"",na.rm=TRUE)\r\n\r\n## Sites:\r\n#--------\r\nsites <- E[,1]\r\n\r\n## Spatial coordinates:\r\n#----------------------\r\nC <- E[,2:3]\r\n\r\n## Habitats:\r\n#-----------\r\nHab <- as.matrix(E[,4])\r\n\r\n## Species abundance data:\r\n#-------------------------\r\nTS  <- as.data.frame(read.table(""AppendixS5.txt"",h =T,row.names = 1))\r\nTS2 <- decostand(TS,""hel"") ## Hellinger-transformed species abundances\r\ndim(TS2)\r\nrownames(TS2)\r\n\r\n#****************************************\r\n###          FUNCTIONAL DATA          ###\r\n#****************************************\r\n#---------------------------------------------------------------------------------\r\n## Imputed values for the 2318 species of the NBA database and the FG inventories:\r\n#---------------------------------------------------------------------------------\r\n## [ NBA = 1630 species ]\r\n##           [ Species in common = 779 ]\r\n##                      [ FG floristic inventories = 1467 species]\r\n## [ 851 sp. ]                         [ 688 sp. ]\r\nZ1 <- as.data.frame(read.table(""AppendixS6.xls"",h=T)) \r\nhead(Z1);dim(Z1)\r\n\r\nj=1\r\ngens       <- as.character(Z1[,2])\r\ngens.names <- names(table(Z1[,2]))\r\n\r\nfor(j in 1:length(gens.names)){\r\n  w       <- as.numeric(which(gens%in%gens.names[j]))\r\n  gens[w] <- as.character(paste0(gens[w],""_"",paste0(rep(""sp"",length(w)),c(1:length(w)))))\r\n}\r\n\r\n## Original matrix of the 8345 individuals with missing trait values:\r\n## (All species [ n = 1630] with observed trait values are in this matrix)\r\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ------\r\ntraits <- as.data.frame(read.table(""AppendixS3.txt"",h=T))\r\nhead(traits);dim(traits)\r\nnamesp.traits <- names(table(traits$Species)) ; namesp.traits\r\n\r\n## Imputed (NBA1) and observed (NBA2) trait values for the NBA database (1630 species):\r\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\r\nw1 <- which(Z1[,1]%in%namesp.traits) ; w1\r\nNBA1           <- Z1[w1,-c(1:6)]\r\nrownames(NBA1) <- Z1[w1,1]\r\nhead(NBA1);dim(NBA1)\r\nsp_list.NBA1  <- Z1[w1,1] ; ge_list.NBA1 <- Z1[w1,2] ; fa_list.NBA1 <- Z1[w1,3]\r\nsp_list.NBA2  <- Z1[w1,1] ; ge_list.NBA2 <- Z1[w1,2] ; fa_list.NBA2 <- Z1[w1,3]\r\n\r\nhead(traits);dim(traits)\r\nsel1 <- c(20:22,24,26,28:30,34,40:41,45:46,58,72:76)\r\ncolnames(traits)[sel1]\r\nNBA2 <- NBA1 ; NBA2[,] <- 0\r\nfor(i in 1:nrow(NBA2)){\r\n  w        <- which   (traits$Species%in%rownames']",1,"- Plant life strategies
- Tropical forests
- Evolutionary processes
- Ecological processes
- Leaf traits
- Stem traits
- Root traits
- Resource optimization
- Environmental gradients
- Trait syndromes
- Phylogenetic"
Data and code from: Geomorphological processes shape plant community traits in the Arctic,"AimGeomorphological processes profoundly affect plant establishment and distributions, but their influence on functional traits is insufficiently understood. Here, we unveil trait-geomorphology relationships in Arctic plant communities. LocationHigh-Arctic Svalbard, low-Arctic Greenland, and sub-Arctic Fennoscandia. Time period2011-2018 Major taxa studiedVascular plants MethodsWe collected field-quantified data on vegetation, geomorphological processes, microclimate, and soil properties from 5280 plots and 200 species across the three Arctic regions. We combined these data with database trait records to relate local plant community trait composition to dominant geomorphological processes of the Arctic, namely cryoturbation, deflation, fluvial processes, and solifluction. We investigated the relationship between plant functional traits and geomorphological processes using hierarchical generalised additive modelling. ResultsOur results demonstrate that community-level traits are related to geomorphological processes, with cryoturbation most strongly influencing both structural and leaf economic traits. These results were consistent across regions, suggesting a coherent biome-level trait response to geomorphological processes. Main conclusionsThe results indicate that geomorphological processes shape plant community traits in the Arctic. We provide empirical evidence for the existence of generalisable relationships between plant functional traits and geomorphological processes. The results indicate that the relationships are consistent across these three distinct tundra regions and that geomorphological processes should be considered in future investigations of functional traits. Kemppinen, Niittynen, Happonen, le Roux, Aalto, Hjort, Maliniemi, Karjalainen, Rautakoski & Luoto. Provisionally accepted. Geomorphological processes shape plant community traits in the Arctic. Global Ecology and Biogeography.These are the data and code from Kemppinen et al. (Provisionally accepted).","['################################################################################################################\r\n################################################################################################################\r\n#### Calculate response curves with HGAM #######################################################################\r\n################################################################################################################\r\n################################################################################################################\r\n\r\nlibrary(tidyverse) # for everything\r\nlibrary(colormap) #generating color codes\r\nlibrary(mgcv) # model\r\nlibrary(gridExtra) #for printing plots on one page\r\nlibrary(gratia) # extended plotting and analysis of mgcv models, uses ggplot\r\nlibrary(itsadug) #plot_smooth function for gam\r\nlibrary(lattice) # print pdf\r\nlibrary(reshape)\r\nlibrary(stringr)\r\n\r\n# start with a clean slate\r\nrm(list=ls(all=TRUE)) \r\n\r\n# read data\r\ntrait <- read.csv(""D:/ARTIKKELIT/ARTIKKELI6/preparation/analysis/data/analysis_data2.csv"", sep="","")\r\n\r\n# output path\r\nfilepath = file.path(""D:/ARTIKKELIT/ARTIKKELI6/preparation/analysis/results/hgam/"")\r\n\r\ntrait$site <- as.factor(trait$site)\r\ntrait$grid <- as.factor(trait$grid)\r\n\r\n############################################################################################\r\n### model and predict ######################################################################\r\n############################################################################################\r\n\r\nnames (trait)\r\n\r\n# preparation for the loop\r\nnmrInd <- 7 # number of responses\r\n\r\nvariables <- c(""area"", ""heig"", ""seed"", ""ldmc"", ""nitr"", ""phos"", ""sla"",\r\n  ""mois"", ""ph"", ""fdd"", ""gdd3"", ""peat"", ""radi"",\r\n  ""cryo"", ""eoli"", ""fluv"", ""soli"") # all variables, first the response(s)\r\n\r\nrespvars <- variables[1:nmrInd] # determine the responses\r\nexpvars <- variables[!(variables %in% respvars)] # determine the predictors\r\n\r\nlen_respvars <- 1:length(respvars) # the number of responses\r\nlen_expvars <- 1:length(expvars) # the number of predictors\r\n\r\npred <- trait # create new data set\r\n\r\n# loop through all responses to model and predict\r\nfor(ii in len_respvars){\r\n  \r\n  #ii <- 1\r\n  \r\n  # print which model you\'re working on\r\n  print(paste0(""modelling "", ii, "". trait: "", respvars[ii]))\r\n  print(Sys.time())\r\n  \r\n  ### model and predict ###################################################################################\r\n  \r\n  # rename the response ii in new data set\r\n  temptrait <- pred\r\n  colnames(temptrait)[colnames(temptrait)==paste0(respvars[ii])] <- ""response""\r\n  \r\n  # use the response ii in the full model\r\n  model_hgam = gam (response ~\r\n                      s(mois, k=20, m=2) + s(mois, site, k=20, m=2, bs=""fs"") +\r\n                      s(ph  , k=20, m=2) + s(ph  , site, k=20, m=2, bs=""fs"") +\r\n                      s(fdd , k=20, m=2) + s(fdd , site, k=20, m=2, bs=""fs"") +\r\n                      s(gdd3, k=20, m=2) + s(gdd3, site, k=20, m=2, bs=""fs"") +\r\n                      s(peat, k=20, m=2) + s(peat, site, k=20, m=2, bs=""fs"") +\r\n                      s(radi, k=20, m=2) + s(radi, site, k=20, m=2, bs=""fs"") +\r\n                      s(cryo, k=20, m=2) + s(cryo, site, k=20, m=2, bs=""fs"") +\r\n                      s(eoli, k=20, m=2) + s(eoli, site, k=20, m=2, bs=""fs"") +\r\n                      s(fluv, k=20, m=2) + s(fluv, site, k=20, m=2, bs=""fs"") +\r\n                      s(soli, k=20, m=2) + s(soli, site, k=20, m=2, bs=""fs"") +\r\n                      s(site, grid, bs = ""re"") +\r\n                      site,\r\n                    method=""REML"", data=temptrait,\r\n                    min.sp = c(rep(1,times=40),0)) # yksi spilini kokonaisvaikutukselle ja kolme poikkeamaa. eli neljä kertaa X (X = muuttujien määrä)\r\n  \r\n  print(Sys.time())\r\n  \r\n  # save the model\r\n  save(model_hgam, file = paste(""D:/ARTIKKELIT/ARTIKKELI6/preparation/analysis/results/hgam/"", respvars[ii],""_model.RData""))\r\n  \r\n  # predict with the model\r\n  terms <- predict.gam(model_hgam, type = ""terms"", se.fit = TRUE)\r\n  \r\n  ### model and predict ###################################################################################\r\n  \r\n  # loop through predictors\r\n  for(iii in len_expvars){\r\n    \r\n    #iii <- 1\r\n    \r\n    # print which model you\'re working on\r\n    print(paste0(""modelling "", ii, "". trait: "", respvars[ii], "" and its "", iii, "". predictor: "", expvars[iii]))\r\n    \r\n    ### model without expvars iii ###################################################################################\r\n    \r\n    # rename the predictor iii in the new data set\r\n    vartrait <- temptrait\r\n    colnames(vartrait)[colnames(vartrait)==paste0(expvars[iii])] <- ""var1""\r\n    \r\n    # rename the rest of the responses\r\n    names(vartrait)[which(names(vartrait) %in% expvars[-iii])] <- paste0(""var"",2:10)\r\n    \r\n    # use the response ii in the full model\r\n    model_hgam_global_var1 = gam (response ~\r\n                                    s(var1, k=20, m=2) +\r\n                   ']",1,"- Geomorphological processes
- Arctic plant communities
- Vascular plants
- Cryoturbation
- Deflation
- Fluvial processes
- Solifluction
- Plant functional traits
- Tundra regions"
Data from: Complex long-term dynamics of pollinator abundance in undisturbed Mediterranean montane habitats over two decades,"Current notions of ""pollinator decline"" and ""pollination crisis"" mainly arose from studies on pollinators of economic value in anthropogenic ecosystems of mid-latitude temperate regions. Comprehensive long-term pollinator data from biologically diverse, undisturbed communities are needed to evaluate the actual extent of the so-called ""global pollination crisis"". This paper analyzes the long-term dynamics of pollinator abundance in undisturbed Mediterranean montane habitats using pollinator visitation data for 65 plant species collected over two decades. Objectives are (1) to elucidate patterns of long-term changes in pollinator abundance from the perspectives of individual plant species, major pollinator groups, and the whole plant community; and (2) to propose a novel methodological implementation based on combining a planned missing data design with the analytical strength of mixed effects models, which allows one to draw community-wide inferences on long-term pollinator trends in species-rich natural habitats. Probabilistic measurements (""patch visitation probability"" and ""flower visitation probability"" per time unit) were used to assess pollinator functional abundance for each plant species on two separate, randomly-chosen years. A total of 13,054 pollinator censuses accounting for a total watching effort of 2,877,039 flowermin were carried out on 299 different dates. Supra-annual unstability in pollinator functional abundance was the rule, with visitation probability to flowering patches and/or individual flowers exhibiting significant heterogeneity between years in the majority of plant species (83%). At the plant-community level, there was a significant linear increase in pollinator functional abundance over the study period. Probability of pollinator visitation to flowering patches and individual flowers increased due to increasing visitation by small solitary bees and, to a lesser extent, small beetles. Visitation to different plant species exhibited contrasting changes, and insect orders and genera differed widely in sign and magnitude of linear abundance trends, thus exemplifying the complex dynamics of community-wide changes in pollinator functional abundance. Results of this investigation indicate that pollinator declines are not universal beyond anthropogenic ecosystems; stress the need for considering broader ecological scenarios and comprehensive samples of plants and pollinators; and illustrate the crucial importance of combining ambitious sampling designs with powerful analytical schemes to draw reliable inferences on pollinator trends at the plant community level.","['# -------------------------------------------------------\r\n# Script 1\r\n# TESTS #1 AND #2 IN TABLE 2\r\n# Section ""Individual plant species: patch visitation""\r\n# -------------------------------------------------------\r\n\r\n# This script runs with these versions of\r\n# required packages. May not work with later versions. \r\n\r\nrequire(car);       # ver. 3.0-0\r\nrequire(broom);     # ver. 0.4.5\r\nrequire(purrr);     # ver. 0.2.5\r\nrequire(ggplot2);   # ver. 3.0.0\r\nrequire(ggthemes);  # ver. 4.0.0\r\nrequire(dplyr) ;    # ver. 0.7.6\r\n\r\nmy_se <- function(x) sqrt(var(x,na.rm=TRUE)/length(which(!is.na(x))))\r\n\r\nworking.dataset <- read.csv(""working.dataset.csv"", stringsAsFactors = FALSE)\r\n\r\n#-------------------------------\r\n# PATCH VISITATION PROBABILITY, \r\n# BY INDIVIDUAL PLANT SPECIES,\r\n# ALL INSECT ORDERS COMBINED\r\n# \r\n# TEST #1 IN TABLE 2\r\n#-------------------------------\r\n\r\nevent.all <- working.dataset %>% group_by(especie, sitio, ano.as.factor, ano, mes, fecha, censo.num) %>% summarize(flores=mean(flores), visitas =sum(visitas)) %>% mutate(event = ifelse(visitas> 0, 1, 0))\r\n\r\nresumen.modelos.patch <- event.all %>% split(.$especie) %>% map_df(~ tidy(car::Anova(glm(event ~ ano.as.factor + scale(flores), family=""binomial"", data=., maxit = 500), type=2, test.statistic=""LR"")[1,3]))\r\n\r\nslopes <- event.all %>% split(.$especie) %>% map_df(~ tidy(coefficients (glm(event ~ ano + flores, family=""binomial"", data=., maxit = 500))[2])) %>% dplyr::select(-names)\r\n\r\nnames(slopes) <- ""slope""\r\n\r\nresumen.modelos.patch$especie <- sort(unique(event.all$especie))\r\n\r\nnames(resumen.modelos.patch) <- c(""pvalue"", ""especie"")\r\n\r\nresumen.modelos.patch$slope <- slopes$slope ; rm(slopes)\r\n\r\n# Benjamini Hochberg correction p values\r\n\r\nresumen.modelos.patch$pvalueBH <- p.adjust(resumen.modelos.patch$pvalue, method=\'BH\')\r\n\r\nlabel.p <- data.frame(especie=sort(unique(event.all$especie)), pvalue=resumen.modelos.patch$pvalueBH, ano = 1999.5, patchvisit=0.82)\r\n\r\nlabel.p$signif.code <- ifelse(label.p$pvalue < 0.05, ""Sign"", ""Nonsign"")\r\n\r\n# freqs slope signs in species with significant heterogeneity\r\n\r\nresumen.modelos.patch$signif <- label.p$signif.code\r\nresumen.modelos.patch$sign <- ifelse(resumen.modelos.patch$slope>=0, \'Positive\', \'Negative\')\r\nwith(resumen.modelos.patch, table(sign, signif, dnn=c(""Sign of slope"", ""Stat. significance"")))\r\n\r\n# Significance of heterogeneity vs number sampling dates\r\n\r\nnumdates <- working.dataset %>% group_by(especie) %>% summarize (dates = length(unique(fecha)))\r\n\r\nnumdates <- full_join(label.p, numdates, by=""especie"") %>% group_by(signif.code) \r\n\r\nnumdates %>% summarize(meandates=mean(dates), sedates=my_se(dates))\r\n\r\nwith(numdates, kruskal.test(dates~factor(signif.code))) ; rm(numdates)\r\n\r\n# FIG. 3 ---------\r\n\r\npatchvisit <- event.all %>% group_by(especie,ano.as.factor, ano) %>% summarise(patchvisit=mean(event), sepatchvisit=my_se(event))\r\n\r\npatchvisit$order <- \'All\'\r\n\r\nfi <- ggplot(patchvisit, aes(x=ano, y=patchvisit, group=especie)) + geom_point() + geom_line() + facet_wrap(~especie, nrow = 13) \r\n\r\nfi <- fi + geom_rect(data = label.p, aes(fill =signif.code),xmin = -Inf,xmax = Inf, ymin = -Inf,ymax = Inf,alpha = 0.3, inherit.aes = FALSE) + scale_fill_manual(values=c(NA, ""grey40""))\r\n\r\nfi <- fi + geom_errorbar(aes(ymin = patchvisit - 2*sepatchvisit, ymax = patchvisit + 2*sepatchvisit), width = 0.5) + geom_point(size=1.5)+ theme_bw() \r\n\r\nfi <- fi + theme(strip.text = element_text(face = ""italic""), strip.background = element_blank()) + labs(x=\'Year\', y =\'Patch visitation probability\') + theme(panel.spacing.y = unit(0.10, ""lines"")) + scale_y_continuous(breaks=c(0, 0.5, 1.0)) + theme(legend.position = ""none"", panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.spacing.y=unit(-0.2, ""lines""), panel.spacing.x=unit(0.2, ""lines""))\r\n\r\nfi <- fi + geom_text(data=label.p, aes(x=ano, y=patchvisit, label = paste0(format.pval(pvalue, digits=1))), inherit.aes = FALSE, size=3)\r\n\r\nprint(fi)\r\n\r\n# uncomment to save figure in pdf file\r\n# ggsave(file=\'Fig.3.Patch.Visitation.by.species.pdf\', width = 8.5, height=10.5, unit =\'in\')\r\n\r\nrm(fi, label.p)\r\n\r\n#-------------------------------\r\n# PATCH VISITATION PROBABILITY,\r\n# BY INDIVIDUAL PLANT SPECIES,\r\n# BY INSECT ORDERS \r\n# \r\n# TEST #2 IN TABLE 2\r\n#-------------------------------\r\n\r\ntmp2 <- working.dataset %>% group_by (especie, sitio, ano.as.factor, ano, mes, fecha, censo.num, flores, order) %>% summarize(visitas = sum(visitas)) %>% mutate(event = ifelse(visitas>0, 1, 0))\r\n\r\n#  COLEOPTERA\r\n\r\nevent.col <- tmp2 %>% mutate(event=ifelse(!order==\'Coleoptera\' | is.na(order), 0, 1)) %>% summarize(event=sum(event))\r\n\r\nevent.col$order=\'Coleoptera\'\r\n\r\nresumen.col <- event.col %>% split(.$especie) %>% map_df(~ tidy(car::Anova(glm(event ~ ano.as.factor + flores, family=""binomial"", data=., maxit = 500))[1,3])) %>% mutate(x = ifelse(x==1, NA, x))\r\n\r\nnames(resumen.col) <- ""p_col""\r\n\r\nresumen.col$especie <- sort(unique(event.col$especie))\r\nresumen.col$order <- \'Coleoptera\'\r\nresumen.col$p_colBH <- ', '# -------------------------------------------------------\r\n# Script 2\r\n# TESTS #3 AND #4 IN TABLE 2\r\n# Section ""Individual plant species: flower visitation""\r\n# -------------------------------------------------------\r\n\r\n# This script runs with these versions of\r\n# required packages. May not work with later versions. \r\n\r\nrequire(car);       # ver. 3.0-0\r\nrequire(broom);     # ver. 0.4.5\r\nrequire(purrr);     # ver. 0.2.5\r\nrequire(ggplot2);   # ver. 3.0.0\r\nrequire(ggthemes);  # ver. 4.0.0\r\nrequire(dplyr) ;    # ver. 0.7.6\r\n\r\nmy_se <- function(x) sqrt(var(x,na.rm=TRUE)/length(which(!is.na(x))))\r\n\r\nworking.dataset <- read.csv(""working.dataset.csv"", stringsAsFactors = FALSE)\r\n\r\n#-------------------------------\r\n# FLOWER VISITATION PROBABILITY, \r\n# BY INDIVIDUAL PLANT SPECIES,\r\n# ALL INSECT ORDERS COMBINED\r\n# \r\n# TEST #3 IN TABLE 2\r\n#-------------------------------\r\n\r\nflower.visit.all <- working.dataset %>% group_by(especie, sitio, ano.as.factor, ano, mes, fecha, censo.num) %>% summarize(flores=mean(flores), visitas =sum(visitas)) %>% mutate(visitas = ifelse(visitas>flores, flores, visitas))\r\n\r\nresumen.modelos <- flower.visit.all %>% split(.$especie) %>% map_df(~ tidy(car::Anova(glm(visitas/flores ~ ano.as.factor + scale(flores), weights=flores, family=""binomial"", data=., maxit = 500), type=2, test.statistic=""LR"")[1,3]))\r\n\r\nnames(resumen.modelos) <- \'p.value\'\r\nresumen.modelos$especie <- sort(unique(flower.visit.all$especie))\r\n\r\nslopes.all <- flower.visit.all %>% split(.$especie) %>% map_df(~ tidy(coefficients (lm(visitas/flores ~ ano, data=.))[2])) %>% dplyr::select(-names)\r\n\r\n# Benjamini Hochberg correction p values\r\n\r\nresumen.modelos$p.valueBH <- p.adjust(resumen.modelos$p.value, method=\'BH\')\r\n\r\ntable(resumen.modelos$p.valueBH<0.05, dnn=""Significant heterogeneity"")\r\n\r\n# FIG. 4 ---------\r\n\r\nflowervisit <- flower.visit.all %>% group_by(especie,ano.as.factor, ano) %>% summarise(flowervisit=mean(visitas/flores), seflowervisit=my_se(visitas/flores))\r\n\r\nflowervisit$order <- \'All\'\r\n\r\nspp.inflors <- c(""Achillea odorata"", ""Allium scorodoprasum"", ""Armeria filicaulis"", ""Carlina hispanica"", ""Catananche caerulea"", ""Centaurea calcitrapa"", ""Chondrilla juncea"", ""Cirsium pyrenaicum"", ""Eryngium campestre"", ""Eryngium dilatatum"", ""Inula montana"", ""Klasea pinnatifida"", ""Knautia subscaposa"", ""Mantisalca salmantica"", ""Santolina rosmarinifolia"", ""Sedum album"", ""Valeriana tuberosa"")\r\n\r\nflowervisit$especie <- as.character(flowervisit$especie)\r\nflowervisit$especie <- ifelse(flowervisit$especie %in% spp.inflors, paste0(flowervisit$especie,""*""),flowervisit$especie )\r\n\r\nlabel.p <- data.frame(especie=sort(unique(flowervisit$especie)), pvalue=resumen.modelos$p.valueBH, ano = 1999.5, flowervisit=0.65)\r\n\r\nlabel.p$signif.code <- ifelse(label.p$pvalue < 0.05, ""Sign"", ""Nonsign"")\r\n\r\nfi <- ggplot(flowervisit, aes(x=ano, y=flowervisit, group=especie)) + geom_point() + geom_line() + facet_wrap(~especie, nrow = 13) \r\n\r\nfi <- fi + geom_rect(data = label.p, aes(fill =signif.code),xmin = -Inf,xmax = Inf, ymin = -Inf,ymax = Inf,alpha = 0.3, inherit.aes = FALSE) + scale_fill_manual(values=c(NA, ""grey40""))\r\n\r\nfi <- fi + geom_errorbar(aes(ymin = flowervisit - 2*seflowervisit, ymax = flowervisit + 2*seflowervisit), width = 0.5) + geom_point(size=1.5)+ theme_bw() \r\n\r\nfi <- fi + theme(strip.text = element_text(face = ""italic""), strip.background = element_blank()) + labs(x=\'Year\', y =\'Flower visitation probability\') + theme(panel.spacing.y = unit(0.10, ""lines"")) + scale_y_continuous(breaks=c(0, 0.5, 1.0)) + theme(legend.position = ""none"", panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.spacing.y=unit(-0.2, ""lines""), panel.spacing.x=unit(0.2, ""lines""))\r\n\r\nfi <- fi + geom_text(data=label.p, aes(x=ano, y=flowervisit, label = paste0(format.pval(pvalue, digits=1))), inherit.aes = FALSE, size=3)\r\n\r\nprint(fi)\r\n\r\n# uncomment to save figure in pdf file\r\n# ggsave(file=\'Fig.4.Flower.Visitation.by.species.pdf\', width = 8.5, height=10.5, unit =\'in\')\r\n\r\nrm(fi)\r\n\r\n# Species means\r\nspecies.means <- flowervisit %>% group_by(especie) %>% summarize(mean.by.species = mean(flowervisit), se.mean.by.species = my_se(flowervisit))\r\n\r\nsummary(species.means$mean.by.species)\r\nspecies.means[which.min(species.means$mean.by.species),]\r\nspecies.means[which.max(species.means$mean.by.species),]\r\n\r\n# Significance of heterogeneity vs number sampling dates\r\n\r\nnumdates <- working.dataset %>% group_by(especie) %>% summarize (dates = length(unique(fecha)))\r\n\r\nnumdates <- full_join(mutate(label.p, especie=resumen.modelos$especie), numdates, by=""especie"") \r\n\r\nnumdates %>% dplyr::group_by(signif.code) %>% summarize(meandates=mean(dates), sedates=my_se(dates))\r\n\r\nwith(numdates, kruskal.test(dates~factor(signif.code))) ; rm(numdates)\r\n\r\n#  slopes\r\nresumen.modelos$slope <- slopes.all$x\r\nresumen.modelos$signif <-resumen.modelos$p.valueBH < 0.05\r\nresumen.modelos$sign <- ifelse(resumen.modelos$slope>=0, \'Positive\', \'Negative\')\r\nwith(resumen.modelos, table(sign, signif, dnn=c(""Sign of', '# -------------------------------------------------------\r\n# Script 3\r\n# \r\n# Section ""Visitation probabilities and flowers per patch""\r\n# -------------------------------------------------------\r\n\r\n# This script runs with the following versions of\r\n# required packages. May not work with later versions. \r\n\r\nrequire(broom);     # ver. 0.4.5\r\nrequire(purrr);     # ver. 0.2.5\r\nrequire(ggplot2);   # ver. 3.0.0\r\nrequire(dplyr) ;    # ver. 0.7.6\r\nrequire(cowplot);   # ver. 0.9.3\r\n\r\nmytheme <- theme_bw(22) + theme(legend.position=""none"", legend.key = element_blank(), strip.background = element_blank(), panel.grid = element_blank(), panel.border= element_rect(size=rel(2.5),color=\'black\'), strip.text.x=element_text(vjust = 1))\r\n\r\nworking.dataset <- read.csv(""working.dataset.csv"", stringsAsFactors = FALSE)\r\n\r\n# Patch visitation probability ----------------\r\n\r\nevent.all <- working.dataset %>% group_by(especie, sitio, ano.as.factor, ano, mes, fecha, censo.num) %>% summarize(flores=mean(flores), visitas =sum(visitas)) %>% mutate(event = ifelse(visitas> 0, 1, 0))\r\n\r\nresumen.modelos.patchvisit <- event.all %>% split(.$especie) %>% map_df(~ tidy(glm(event ~ ano.as.factor + scale(flores), family=""binomial"", data=.)))\r\n\r\ntmp1 <- resumen.modelos.patchvisit %>% filter(term==""scale(flores)"") %>% mutate(slope.flores = estimate, p.flores=p.adjust(p.value,method=\'BH\'), stat.flores=statistic) %>% select(slope.flores, stat.flores, p.flores)\r\n\r\ntmp2 <- resumen.modelos.patchvisit %>% filter(term==""ano.as.factor"") %>% mutate(slope.ano = estimate, p.ano = p.adjust(p.value,method=\'BH\'), stat.ano=statistic) %>% select(slope.ano, stat.ano, p.ano)\r\n\r\nall.slopes1 <- bind_cols(tmp1, tmp2) %>% mutate(especie = sort(unique(event.all$especie))) %>% select(especie, slope.flores:p.ano)\r\n\r\n# Two species excluded because patch visitation probabilities\r\n# were so close to unity in both years (see Fig. 3)\r\n# that parameter estimates for the patch visitation\r\n# probability term in the models were numerically unreliable\r\n# 4 species excluded because had >2 temporal data points\r\n# => >1 parameter estimate for factor(year)\r\n\r\nall.slopes1 <- all.slopes1 %>% filter (!especie %in% c(""Centaurea calcitrapa"", ""Cistus monspeliensis"", ""Helleborus foetidus"", ""Lavandula latifolia"", ""Viola cazorlensis"", ""Gladiolus illyricus""))\r\n\r\nwith(all.slopes1, cor.test(slope.flores, slope.ano, method=""spearman""))\r\n\r\nfi1 <- ggplot(all.slopes1, aes(x=slope.flores, y=slope.ano)) + geom_vline(xintercept = 0, size=1.5, linetype =""dashed"") + geom_point(size=4, shape=21, fill=""white"") + mytheme \r\n\r\nfi1 <- fi1 + labs(x=""Model parameter estimate:\\nFlowers per patch (logits/SD)"", y=""Model parameter estimate:\\nYear (logits)"", title = ""Patch visitation"") + theme(plot.title = element_text(hjust = 0.5))\r\n\r\n# Flower visitation probability ---------------------\r\n\r\nflower.visit.all <- working.dataset %>% group_by(especie, sitio, ano.as.factor, ano, mes, fecha, censo.num) %>% summarize(flores=mean(flores), visitas =sum(visitas)) %>% mutate(visitas = ifelse(visitas>flores, flores, visitas))\r\n\r\nresumen.modelos.flowervisit <- flower.visit.all %>% split(.$especie) %>% map_df(~ tidy(glm(visitas/flores ~ ano.as.factor + scale(flores), weights=flores, family=""binomial"", data=., maxit = 500)))\r\n\r\ntmp3 <- resumen.modelos.flowervisit %>% filter(term==""scale(flores)"") %>% mutate(slope.flores = estimate, p.flores=p.adjust(p.value,method=\'BH\'), stat.flores=statistic) %>% select(slope.flores, stat.flores, p.flores)\r\n\r\ntmp4 <- resumen.modelos.flowervisit %>% filter(term==""ano.as.factor"") %>% mutate(slope.ano =estimate, p.ano = p.adjust(p.value,method=\'BH\'), stat.ano=statistic) %>% select(slope.ano, stat.ano, p.ano)\r\n\r\nall.slopes2 <- bind_cols(tmp3, tmp4) %>% mutate(especie = sort(unique(flower.visit.all$especie))) %>% select(especie, slope.flores:p.ano)\r\n\r\n# 4 species excluded because had >2 temporal data points\r\n# => >1 parameter estimate for factor(year)\r\n\r\nall.slopes2 <- all.slopes2 %>% filter (!especie %in% c(""Helleborus foetidus"", ""Lavandula latifolia"", ""Viola cazorlensis"", ""Gladiolus illyricus""))\r\n\r\nwith(all.slopes2, cor.test(slope.flores, slope.ano, method=""spearman""))\r\n\r\nfi2 <- ggplot(all.slopes2, aes(x=slope.flores, y=slope.ano)) + geom_vline(xintercept = 0, size=1.5, linetype =""dashed"") + geom_point(size=4, shape=21, fill=""white"") + mytheme\r\n\r\nfi2 <- fi2 + labs(x=""Model parameter estimate:\\nFlowers per patch (logits/SD)"", y=""Model parameter estimate:\\nYear (logits)"", title = ""Flower visitation"") + theme(plot.title = element_text(hjust = 0.5))\r\n\r\n# FIGURE 5 -----------------\r\n\r\nplot_grid(fi1, fi2, ncol=2)\r\n\r\n# uncomment to save the figure in pdf file\r\n# ggsave(""Fig.5.Ano.vs.flores.model.parameters.pdf"", width=14, height=7)\r\n\r\n# uncomment to clean up working environment\r\n# rm(list=(ls(pattern=""^resumen"")))\r\n# rm(list=(ls(pattern=""^slopes"")))\r\n# rm(list=(ls(pattern=""^tmp"")))\r\n# rm(list=(ls(pattern=""^fi"")))\r\n# rm(list=(ls(pattern=""^all"")))\r\n# rm(list=(ls(pattern=""all$"")))', '# -------------------------------------------------------\r\n# Script 4\r\n# \r\n# Section ""Supra-annual fluctuations in pollinator composition""\r\n# -------------------------------------------------------\r\n\r\n# This script runs with the following versions of\r\n# required packages. May not work with later versions. \r\n\r\nrequire(ggplot2);   # ver. 3.0.0\r\nrequire(dplyr);     # ver. 0.7.6\r\nrequire(lubridate); # ver. 1.7.4\r\n\r\nmytheme <- theme_bw(22) + theme(legend.position=""none"", legend.key = element_blank(), strip.background = element_blank(), panel.grid = element_blank(), panel.border= element_rect(size=rel(2.5),color=\'black\'), strip.text.x=element_text(vjust = 1))\r\n\r\n# FIG. 6 ---------------\r\n# Frequency distribution number of fluctuating orders\r\n\r\n(read.table(""Table.S1.Appendix.S5.txt"", header=TRUE, sep=\'\\t\') %>% select(signif.orders) %>% table()) ; # output from Script 1\r\n\r\n(read.table(""Table.S2.Appendix.S5.txt"", header=TRUE, sep=\'\\t\') %>% select(signif.orders) %>% table()) ; # output from Script 2\r\n\r\npatch <- c(21,26,14,4,0)\r\nflowers <- c(3,16,22,20,4)\r\n\r\ndatos <- data.frame(speciesnum=c(patch,flowers), orders=rep(0:4, times=2), measurement=c(rep(\'Patch visitation\',5), rep(\'Flower visitation\',5))) %>% mutate(measurement = factor(measurement, levels = rev(levels(measurement))))\r\n\r\nf <- ggplot(datos, aes(x=orders, y=speciesnum, fill=measurement))+ geom_col(position=""dodge"", width=0.6) + scale_fill_manual(values=c(""grey70"", ""black"")) + theme_bw(18) + mytheme + theme(legend.position = \'top\')\r\n\r\nf <- f + labs(x=\'Number of fluctuating orders\', y=\'Number of plant species\', fill=NULL) + theme(panel.border = element_rect(fill=NA, colour = ""black"", size=2)) + theme(legend.key.height=unit(2,""line""))\r\n\r\nprint(f)\r\n\r\n# uncomment to save figure in pdf file\r\n# ggsave(""Fig.6.Freq.Distrib.Fluctuating.Orders.pdf"", width=8, height=8)\r\n\r\nrm(f,datos, patch, flowers)\r\n\r\n# EUCLIDEAN DISTANCES ANALYSES ------------------\r\n\r\n# patch visitation ----\r\n\r\nload(""patch.visitation.by.order.RObj"") ; # output from Script 1\r\n\r\ncc <- ungroup(patchvisit.col) %>% select(especie, patchvisit) %>% rename(col_flower = patchvisit)\r\ndd <- ungroup(patchvisit.dip) %>% select(especie, patchvisit) %>% rename(dip_flower = patchvisit)\r\nhh <- ungroup(patchvisit.hym) %>% select(especie, patchvisit) %>% rename(hym_flower = patchvisit)\r\nll <- ungroup(patchvisit.lep) %>% select(especie, patchvisit) %>% rename(lep_flower = patchvisit)\r\n\r\neuclid.patch <- bind_cols(cc,dd,hh,ll) %>% select(especie, col_flower, dip_flower, hym_flower, lep_flower) %>% group_by(especie)\r\nrm(cc,dd,hh,ll)\r\n\r\neuclid.patch <- summarize(euclid.patch, mean.euclid.patch = mean(dist(c(col_flower,dip_flower, hym_flower, lep_flower))))\r\n\r\n# flower visitation ----\r\n\r\nload(""flower.visitation.by.order.RObj""); # output from Script 2\r\n\r\ncc <- ungroup(flowervisit.col) %>% select(especie, flowervisit) %>% rename(col_flower = flowervisit)\r\ndd <- ungroup(flowervisit.dip) %>% select(especie, flowervisit) %>% rename(dip_flower = flowervisit)\r\nhh <- ungroup(flowervisit.hym) %>% select(especie, flowervisit) %>% rename(hym_flower = flowervisit)\r\nll <- ungroup(flowervisit.lep) %>% select(especie, flowervisit) %>% rename(lep_flower = flowervisit)\r\n\r\neuclid.flower <- bind_cols(cc,dd,hh,ll) %>% select(especie, col_flower, dip_flower, hym_flower, lep_flower) %>% group_by(especie)\r\nrm(cc,dd,hh,ll)\r\n\r\neuclid.flower <- summarize(euclid.flower, mean.euclid.flower = mean(dist(c(col_flower,dip_flower, hym_flower, lep_flower))))\r\n\r\n# merge patch & flower euclidean distance data\r\neuclid.all <- data.frame(full_join(euclid.patch, euclid.flower, by = ""especie""))\r\n\r\n# write.table(format(euclid.all, digits=2), file=""Table.S3.Appendix.S5.txt"",quote = FALSE, sep=\'\\t\',row.names = FALSE)\r\n\r\n# -------------\r\n# Info especies\r\n\r\nespecies.data <- read.csv(""Species.data.csv"") %>% select(-X)\r\n\r\n# Days from 1 Jan\r\n\r\ndiamedio <- read.csv(""working.dataset.csv"", stringsAsFactors = FALSE) %>% mutate(dayofyear = yday(fecha)) %>% group_by(especie,sitio,fecha,censo.num) %>% summarize(diamedio = mean(dayofyear)) %>% group_by(especie) %>% summarize(diamedio = mean(diamedio)) %>% select(diamedio)\r\n\r\n# Supra-annual span\r\n\r\ninterval <- read.csv(""working.dataset.csv"", stringsAsFactors = FALSE) %>% group_by(especie) %>% summarize(interval=max(ano)-min(ano)) %>% select(interval)\r\n\r\n# Number sampling dates\r\n\r\nnumdates <- read.csv(""working.dataset.csv"", stringsAsFactors = FALSE) %>% group_by(especie) %>% summarize (dates = length(unique(fecha))) %>% select(dates) \r\n\r\nall.variation <- bind_cols(especies.data, diamedio, interval, numdates, select(euclid.all, -especie))\r\n\r\n# tests -------\r\n\r\nwith(all.variation, cor.test(mean.euclid.patch, interval, method=\'spearman\'))\r\nwith(all.variation, cor.test(mean.euclid.flower, interval, method=\'spearman\'))\r\n\r\nwith(all.variation, cor.test(mean.euclid.patch, dates, method=\'spearman\'))\r\nwith(all.variation, cor.test(mean.euclid.flower, dates, method=\'spearman\'))\r\n\r\nwith(all.variation, cor.test(mean.euclid.patch, elevation, metho', '# -------------------------------------------------------\r\n# Script 5\r\n# TESTS #5 AND #8 IN TABLE 2\r\n# Section ""Community-level trends: all pollinators""\r\n# -------------------------------------------------------\r\n\r\n# This script runs with the following versions\r\n# of required packages. May not work with later versions. \r\n\r\nrequire(blmeco);    # ver. 1.1\r\nrequire(lme4);      # ver. 1.1-17\r\nrequire(ggplot2);   # ver. 3.0.0\r\nrequire(ggeffects); # ver. 0.4.0\r\nrequire(dplyr) ;    # ver. 0.7.6\r\n\r\nmytheme <- theme_bw(22) + theme(legend.position=""none"", legend.key = element_blank(), strip.background = element_blank(), panel.grid = element_blank(), panel.border= element_rect(size=rel(2.5),color=\'black\'), strip.text.x=element_text(vjust = 1))\r\n\r\nworking.dataset <- read.csv(""working.dataset.csv"", stringsAsFactors = FALSE)\r\n\r\nevent.data <- working.dataset %>% group_by(especie, sitio, ano.as.factor, ano, mes, fecha, censo.num) %>% summarize(flores=mean(flores), visitas=sum(visitas)) %>% ungroup(.) %>% mutate(event = ifelse(visitas>0, 1,0), visitas=ifelse(visitas>flores, flores, visitas), ano.scaled=scale(ano), flores.scaled=scale(flores), visitrate = visitas/flores)\r\n\r\n#-------------------------------\r\n# PATCH VISITATION PROBABILITY, \r\n# PLANT COMMUNITY LEVEL,\r\n# ALL INSECT ORDERS COMBINED\r\n# \r\n# TEST #5 IN TABLE 2\r\n#-------------------------------\r\n\r\nfit0 <- glmer(event ~ 1 + flores.scaled + (1|especie) + (1|sitio), data=event.data, family=\'binomial\')\r\n\r\nfit1 <- glmer(event ~ ano.scaled + flores.scaled + (1|especie) + (1|sitio), data=event.data, family=\'binomial\')\r\n\r\n#  check for overdispersion - passes\r\n  dispersion_glmer(fit0)\r\n  dispersion_glmer(fit1)\r\n\r\nanova(fit0, fit1)\r\nsummary(fit1)\r\n(fit1.confint.profile <- confint.merMod(fit1, method=""profile""))\r\n\r\nfit1.predicted <- ggpredict(fit1, terms=\'ano.scaled\') %>% mutate(ano = sort(unique(event.data$ano)), class = ""Patch visitation"")\r\n\r\n#-------------------------------\r\n# FLOWER VISITATION PROBABILITY, \r\n# PLANT COMMUNITY LEVEL,\r\n# ALL INSECT ORDERS COMBINED\r\n# \r\n# TEST #8 IN TABLE 2\r\n#-------------------------------\r\n\r\nfit2 <- glmer(visitrate ~ 1 + flores.scaled + (1|especie) + (1|sitio), weights = flores, data=event.data, family=\'binomial\')\r\n\r\nfit3 <- glmer(visitrate ~ 1 + ano.scaled + flores.scaled + (1|especie) + (1|sitio), weights = flores, data=event.data, family=\'binomial\')\r\n\r\n#  check for overdispersion - fails\r\n  dispersion_glmer(fit2)\r\n  dispersion_glmer(fit3)\r\n\r\n# add observation-level random effect (olre) and recompute\r\nevent.data$olre = 1:nrow(event.data)\r\n\r\nfit4 <- glmer(visitrate ~ 1 + flores.scaled + (1|especie) + (1|sitio) + (1|olre), weights = flores, data=event.data, family=\'binomial\')\r\n\r\nfit5 <- glmer(visitrate ~ 1 + flores.scaled + ano.scaled + (1|especie) + (1|sitio) + (1|olre), weights = flores, data=event.data, family=\'binomial\')\r\n\r\n# check again for overdispersion - passes\r\n  dispersion_glmer(fit4)\r\n  dispersion_glmer(fit5)\r\n\r\nanova(fit4, fit5)\r\nsummary(fit5)\r\n(fit5.confint.profile <- confint.merMod(fit5, method=""profile"", devtol = 1e-08))\r\n\r\nfit5.predicted <- ggpredict(fit5, terms=\'ano.scaled\') %>% mutate(ano = sort(unique(event.data$ano)), class = ""Flower visitation"")\r\n\r\n# FIG. 8 ---------\r\n\r\nfit.predicted.patch.and.flower <- bind_rows(fit1.predicted, fit5.predicted) %>% mutate(class=factor(class)) %>% mutate(class=factor(class, levels(class)[c(2,1)]))\r\n\r\nfi <- ggplot(fit.predicted.patch.and.flower, aes(x=ano, y=predicted, group=class)) + facet_wrap(~class, scales = ""free"") + geom_ribbon(aes(ymin=conf.low, ymax=conf.high), fill = ""grey80"") + geom_line(size=2) + geom_point(size=3, shape=21, fill=\'white\') + theme_bw() + mytheme + geom_rug(sides=""b"")\r\n\r\nfi <- fi + labs(x=""Year"", y=""Visitation probability"")\r\n\r\nprint(fi)\r\n\r\n# uncomment to produce a pdf file\r\n# ggsave(""Fig.8.Predicted.GLMM,Patch.Flower.pdf"", width=9.5, height=7)\r\n\r\n# uncomment to clean up working environment\r\n# rm(list=(ls(pattern=""^fi"")))\r\n\r\n', '# -------------------------------------------------------\r\n# Script 6\r\n# TESTS #6 AND #9 IN TABLE 2\r\n# Section ""Community-level trends: insect orders""\r\n# -------------------------------------------------------\r\n\r\n# This script runs with the following versions\r\n# of required packages. May not work with later versions. \r\n\r\nrequire(blmeco);    # ver. 1.1\r\nrequire(lme4);      # ver. 1.1-17\r\nrequire(ggplot2);   # ver. 3.0.0\r\nrequire(ggeffects); # ver. 0.4.0\r\nrequire(ggthemes);  # ver. 4.0.0\r\nrequire(dplyr) ;    # ver. 0.7.6\r\n\r\nmytheme <- theme_bw(22) + theme(legend.position=""none"", legend.key = element_blank(), strip.background = element_blank(), panel.grid = element_blank(), panel.border= element_rect(size=rel(2.5),color=\'black\'), strip.text.x=element_text(vjust = 1))\r\n\r\nworking.dataset <- read.csv(""working.dataset.csv"", stringsAsFactors = FALSE)\r\n\r\n#-------------------------------\r\n# PATCH VISITATION PROBABILITY, \r\n# PLANT COMMUNITY LEVEL,\r\n# BY INSECT ORDERS\r\n# \r\n# TEST #6 IN TABLE 2; Results shown in Table 5\r\n#-------------------------------\r\n\r\ntmp1 <- working.dataset %>% group_by (especie, sitio, ano.as.factor, ano, mes, fecha, censo.num, flores, order) %>% summarize(visitas = sum(visitas)) %>% mutate(event = ifelse(visitas>0, 1, 0))\r\n\r\n# COLEOPTERA --------------\r\n\r\nevent.col <- tmp1 %>% mutate(event=ifelse(!order==\'Coleoptera\' | is.na(order), 0, 1)) %>% summarize(event=sum(event)) %>% ungroup() %>% mutate(order=\'Coleoptera\', ano.scaled=scale(ano), flores.scaled=scale(flores)) \r\n\r\nfit0col <- glmer(event ~ 1 + flores.scaled + (1|especie) + (1|sitio), data=event.col, family=\'binomial\')\r\n\r\nfit1col <- glmer(event ~ ano.scaled + flores.scaled + (1|especie) + (1|sitio), data=event.col, family=\'binomial\')\r\n\r\n#  check for overdispersion - passes\r\ndispersion_glmer(fit0col)\r\ndispersion_glmer(fit1col)\r\n\r\nanova(fit0col, fit1col)\r\nsummary(fit1col)\r\n(fit1.confint.profile.col <- confint.merMod(fit1col, method=""profile""))\r\n\r\nfit1.predicted.col <- ggpredict(fit1col, terms=\'ano.scaled\') %>% mutate(class=""Coleoptera"", ano = sort(unique(event.col$ano)))\r\n\r\n# DIPTERA --------------\r\n\r\nevent.dip <- tmp1 %>% mutate(event=ifelse(!order==\'Diptera\' | is.na(order), 0, 1)) %>% summarize(event=sum(event)) %>% ungroup() %>% mutate(order=\'Diptera\', ano.scaled=scale(ano), flores.scaled=scale(flores)) \r\n\r\nfit0dip <- glmer(event ~ 1 + flores.scaled + (1|especie) + (1|sitio), data=event.dip, family=\'binomial\')\r\n\r\nfit1dip <- glmer(event ~ ano.scaled + flores.scaled + (1|especie) + (1|sitio), data=event.dip, family=\'binomial\')\r\n\r\n#  check for overdispersion - passes\r\ndispersion_glmer(fit0dip)\r\ndispersion_glmer(fit1dip)\r\n\r\nanova(fit0dip, fit1dip)\r\nsummary(fit1dip)\r\n(fit1.confint.profile.dip <- confint.merMod(fit1dip, method=""profile""))\r\n\r\nfit1.predicted.dip <- ggpredict(fit1dip, terms=\'ano.scaled\') %>% mutate(class= ""Diptera"", ano = sort(unique(event.dip$ano)))\r\n\r\n# HYMENOPTERA --------------\r\n\r\nevent.hym <- tmp1 %>% mutate(event=ifelse(!order==\'Hymenoptera\' | is.na(order), 0, 1)) %>% summarize(event=sum(event)) %>% ungroup() %>% mutate(order=\'Hymenoptera\', ano.scaled=scale(ano), flores.scaled=scale(flores)) \r\n\r\nfit0hym <- glmer(event ~ 1 + flores.scaled + (1|especie) + (1|sitio), data=event.hym, family=\'binomial\')\r\n\r\nfit1hym <- glmer(event ~ ano.scaled + flores.scaled + (1|especie) + (1|sitio), data=event.hym, family=\'binomial\')\r\n\r\n#  check for overdispersion - passes\r\ndispersion_glmer(fit0hym)\r\ndispersion_glmer(fit1hym)\r\n\r\nanova(fit0hym, fit1hym)\r\nsummary(fit1hym)\r\n(fit1.confint.profile.hym <- confint.merMod(fit1hym, method=""profile""))\r\n\r\nfit1.predicted.hym <- ggpredict(fit1hym, terms=\'ano.scaled\') %>% mutate(class =""Hymenoptera"", ano = sort(unique(event.hym$ano)))\r\n\r\n# LEPIDOPTERA --------------\r\n\r\nevent.lep <- tmp1 %>% mutate(event=ifelse(!order==\'Lepidoptera\' | is.na(order), 0, 1)) %>% summarize(event=sum(event)) %>% ungroup() %>% mutate(order=\'Lepidoptera\', ano.scaled=scale(ano), flores.scaled=scale(flores)) \r\n\r\nfit0lep <- glmer(event ~ 1 + flores.scaled + (1|especie) + (1|sitio), data=event.lep, family=\'binomial\')\r\n\r\nfit1lep <- glmer(event ~ ano.scaled + flores.scaled + (1|especie) + (1|sitio), data=event.lep, family=\'binomial\')\r\n\r\n#  check for overdispersion - passes\r\ndispersion_glmer(fit0lep)\r\ndispersion_glmer(fit1lep)\r\n\r\nanova(fit0lep, fit1lep)\r\nsummary(fit1lep)\r\n(fit1.confint.profile.lep <- confint.merMod(fit1lep, method=""profile""))\r\n\r\nfit1.predicted.lep <- ggpredict(fit1lep, terms=\'ano.scaled\') %>% mutate(class=""Lepidoptera"", ano = sort(unique(event.lep$ano)))\r\n\r\nfit1.predicted.patch.by.order <- bind_rows(fit1.predicted.col, fit1.predicted.dip, fit1.predicted.hym, fit1.predicted.lep) %>% mutate(tipo.visita=""Patch visitation"")\r\n\r\n#-------------------------------\r\n# FLOWER VISITATION PROBABILITY, \r\n# PLANT COMMUNITY LEVEL,\r\n# BY INSECT ORDERS\r\n# \r\n# TEST #9 IN TABLE 2; Results shown in Table 5\r\n#-------------------------------\r\n\r\ntmp2 <- working.dataset %>% group_by (especie, sitio, ano.as.factor, ano, mes,fecha, censo.num,flores, or', '# -------------------------------------------------------\r\n# Script 7\r\n# TESTS #7 AND #10 IN TABLE 2\r\n# Section ""Community-level trends: selected genera""\r\n# -------------------------------------------------------\r\n\r\n# This script runs with the following versions\r\n# of required packages. May not work with later versions. \r\n\r\nrequire(lme4);      # ver. 1.1-17\r\nrequire(broom);     # ver. 0.4.5\r\nrequire(ggplot2);   # ver. 3.0.0\r\nrequire(ggeffects); # ver. 0.4.0\r\nrequire(ggrepel);   # ver. 0.8.0\r\nrequire(dplyr) ;    # ver. 0.7.6\r\n\r\nmytheme <- theme_bw(22) + theme(legend.position=""none"", legend.key = element_blank(), strip.background = element_blank(), panel.grid = element_blank(), panel.border= element_rect(size=rel(2.5),color=\'black\'), strip.text.x=element_text(vjust = 1))\r\n\r\nworking.dataset <- read.csv(""working.dataset.csv"", stringsAsFactors = FALSE)\r\n\r\ntmp0 <- working.dataset %>% group_by (especie, sitio, ano.as.factor, ano, mes, fecha, censo.num, flores, order, genus) %>% summarize(visitas = sum(visitas)) %>% mutate(event = ifelse(visitas>0, 1, 0), visrate = visitas/flores)\r\n\r\ngenus.list <- c(""Acmaeodera"", ""Anthaxia"", ""Anthrenus"", ""Aplocnemus"", ""Chlorophorus"", ""Chrysanthia"", ""Dasytes"", ""Heliotaurus"", ""Lobonyx"", ""Malachius"", ""Meligethes"", ""Mordella"", ""Mylabris"", ""Oedemera"", ""Orphilus"", ""Pseudovadonia"", ""Stictoleptura"", ""Trichodes"", ""Tropinota"", ""Apolysis"", ""Bombylius"", ""Chrysotoxum"", ""Dilophus"", ""Eristalis"", ""Eumerus"", ""Eupeodes"", ""Hemipenthes"", ""Merodon"", ""Nowickia"", ""Pangonius"", ""Peleteria"", ""Rhyncomyia"", ""Sphaerophoria"", ""Stomorhina"", ""Syrphus"", ""Systoechus"", ""Villa"", ""Volucella"", ""Xanthempis"", ""Amegilla"", ""Andrena"", ""Anthidiellum"", ""Anthidium"", ""Anthophora"", ""Apis"", ""Bembix"", ""Bombus"", ""Ceratina"", ""Colletes"", ""Eucera"", ""Evylaeus"", ""Halictus"", ""Heriades"", ""Hylaeus"", ""Lasioglossum"", ""Megachile"", ""Osmia"", ""Panurgus"", ""Seladonia"", ""Aporia"", ""Argynnis"", ""Aricia"", ""Colias"", ""Hesperia"", ""Iphiclides"", ""Lasiommata"", ""Lycaena"", ""Macroglossum"", ""Maniola"", ""Melanargia"", ""Micropterix"", ""Ochlodes"", ""Plebejus"", ""Polyommatus"", ""Pyrgus"", ""Pyronia"", ""Satyrium"", ""Satyrus"", ""Thymelicus"")\r\n\r\norder.list <- c(rep(""Coleoptera"",19), rep(""Diptera"",20), rep( ""Hymenoptera"",20), rep(""Lepidoptera"", 20))\r\n\r\n#-------------------------------\r\n# PATCH VISITATION PROBABILITY, \r\n# PLANT COMMUNITY LEVEL,\r\n# BY INSECT GENERA\r\n# \r\n# TEST #7 IN TABLE 2\r\n#-------------------------------\r\n\r\n# empty dataframe for GLMM results\r\nGLMM.by.genus.patch <- data.frame(order = character(), genus= character(), chisquared=double(), p.value=double(), estimate=double())\r\n\r\n# loop over 79 genera in genus.list\r\nfor (i in 1:length(genus.list))\r\n  {\r\ndatos <- tmp0 %>% mutate(event = ifelse(!genus == genus.list[i] | is.na(genus), 0, 1)) %>% summarize(event=sum(event)) %>% ungroup() %>% mutate(ano.scaled=scale(ano), flores.scaled=scale(flores))\r\n\r\nfit0 <- glmer(event ~ 1 + flores.scaled + (1|especie) + (1|sitio), data=datos, family=\'binomial\', control=glmerControl(optimizer=""bobyqa"", optCtrl=list(maxfun=100000)))\r\n\r\nfit1 <- glmer(event ~ ano.scaled + flores.scaled + (1|especie) + (1|sitio), data=datos, family=\'binomial\', control=glmerControl(optimizer=""bobyqa"", optCtrl=list(maxfun=100000)))\r\n\r\nGLMM.by.genus.patch[i, c(""chisquared"", ""p.value"")] <- tidy(anova(fit0, fit1))[2,c(""statistic"", ""p.value"")]\r\nGLMM.by.genus.patch[i, ""estimate""] <- tidy(fit1)[2,""estimate""]\r\n}\r\n\r\nGLMM.by.genus.patch <- GLMM.by.genus.patch %>% mutate(order = order.list, genus=genus.list, p.valueBH = p.adjust(p.value, method=""BH"")) %>% select(-p.value)\r\n\r\nrow.names(GLMM.by.genus.patch) <- NULL\r\n\r\n#-------------------------------\r\n# FLOWER VISITATION PROBABILITY, \r\n# PLANT COMMUNITY LEVEL,\r\n# BY INSECT GENERA\r\n# \r\n# TEST #10 IN TABLE 2\r\n#-------------------------------\r\n\r\n# empty dataframe for GLMM results\r\nGLMM.by.genus.flower <- data.frame(order = character(), genus=character(), chisquared=double(), p.value=double(), estimate=double())\r\n\r\n# loop over 79 genera in genus.list\r\nfor (i in 1:length(genus.list))\r\n{\r\n  \r\ndatos <- tmp0 %>% mutate(visrate = ifelse(!genus== genus.list[i] | is.na(genus), 0, visrate)) %>% summarize(visrate=sum(visrate)) %>% ungroup(losdatos) %>% mutate(ano.scaled=scale(ano), flores.scaled=scale(flores)) %>% mutate(visrate=ifelse(visrate>1, 1, visrate))\r\n  \r\n  fit0 <- glmer(visrate ~ 1 + flores.scaled + (1|especie) + (1|sitio), data=datos, family=\'binomial\', weights=flores, control=glmerControl(optimizer=""bobyqa"", optCtrl=list(maxfun=100000)))\r\n  \r\n  fit1 <- glmer(visrate ~ ano.scaled + flores.scaled + (1|especie) + (1|sitio), data=datos, family=\'binomial\', weights=flores, control=glmerControl(optimizer=""bobyqa"", optCtrl=list(maxfun=100000)))\r\n  \r\n  GLMM.by.genus.flower[i, c(""chisquared"", ""p.value"")] <- tidy(anova(fit0, fit1))[2,c(""statistic"", ""p.value"")]\r\n  GLMM.by.genus.flower[i, ""estimate""] <- tidy(fit1)[2,""estimate""]\r\n}\r\n\r\nGLMM.by.genus.flower <- GLMM.by.genus.flower %>% mutate(order = order.list, genus=genus.list, p.valueBH = p.adjust(p.value, method=""BH"")) %>% select(-p']",1,"pollinator decline, pollination crisis, long-term dynamics, undisturbed habitats, Mediterranean montane, pollinator visitation, plant species, major pollinator groups, community-wide inferences, mixed effects models, patch visitation probability, flower"
"Coastal squeeze on temperate reefs: long-term shifts in salinity, water quality, and oyster-associated communities","Foundation species such as mangroves, saltmarshes, kelps, seagrasses, and oysters thrive within suitable environmental envelopes as narrow ribbons along the land-sea margin. Therefore, these habitat-forming species and resident fauna are sensitive to modified environmental gradients. For oysters, many estuaries impacted by sea-level rise, channelization, and municipal infrastructure are experiencing saltwater intrusion and water-quality degradation that may alter reef distributions, functions, and services. To explore decadal-scale oyster-reef community patterns across a temperate estuary in response to environmental change, we resampled reefs in the Newport River Estuary (NRE) during 2013-2015 that were previously studied during 1955-1956. We also coalesced historical NRE reef distribution (1880s-2015), salinity (1913-2015), and water-quality driven shellfish closure boundary (1970s-2015) data to document environmental trends that could influence reef ecology and service delivery. Over the last 60-120 years, the entire NRE has shifted toward higher salinities. Consequently, oyster-reef communities have become less distinct across the estuary, manifest by 20-27% lower species turnover and decreased faunal richness among NRE reefs in the 2010s relative to the 1950s. During the 2010s, NRE oyster-reef communities tended to cluster around a euhaline, intertidal-reef type more so than during the 1950s. This followed from faunal expansions farther up-estuary and biological degradation of subtidal reefs as NRE conditions became more marine and favorable for aggressive, reef-destroying taxa. In addition to these biological shifts, the area of suitable bottom on which subtidal-reefs persist (ultimately regulated by up-estuary intrusion of marine waters) and support human harvest (driven by water quality, eroding from up-estuary) has decreased by >75% since the natural history of NRE reefs was first explored. This ""coastal squeeze"" on harvestable subtidal oysters (reduced from a 4.5-km to a 0.75-km envelope along the NRE's main axis) will likely have consequences regarding the economic incentives for future oyster conservation, as well as the suite of services delivered by remaining shellfish reefs (e.g., biodiversity maintenance, seafood supply). More broadly, these findings exemplify how ""squeeze"" may be a pervasive concern for biogenic habitats along terrestrial or marine ecotones during an era of intense global change.","['require(lubridate)\nrequire(dplyr)\nrequire(plyr)\nrequire(ggplot)\nrequire(cowplot)\nrequire(scales)\nlibrary(readr)\nlibrary(zoo)\nlibrary(tidyr) \nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(ggpmisc)\nlibrary(extrafont)\nlibrary(scales)\nlibrary(ggpubr)\n#set working directory\nsetwd(""~/Dropbox/Lab Projects/Oyster Projects/Oyster Resilence 2013 - 2015/New Versions/CRFL-Wells Comparison/environmentaldata"")\ngetwd()\n\n#read in data - salinity and MSL\nnpre <- read_csv(""~/Dropbox/Lab Projects/Oyster Projects/Oyster Resilence 2013 - 2015/New Versions/CRFL-Wells Comparison/environmentaldata/newport salinity r.csv"")\npimsl <- read_csv(""~/Dropbox/Lab Projects/Oyster Projects/Oyster Resilence 2013 - 2015/New Versions/CRFL-Wells Comparison/environmentaldata/NOAA.msl.csv"")\n\n#make year column posixct for axis formatting later\npimsl$year <- as.Date(as.character(pimsl$year), format = ""%Y"")\npimsl$year <- as.POSIXct(pimsl$year, ""%Y/%m/%d"")\n\n\n##______________________________________________________________________________________________________________________________##\n#npre <- npre %>% filter(source==""wells"" | source==""CRFL"") ##Run if you only want to include Wells and CRFL data ***For revision process check\n#npre <- npre%>% filter(source==""DMF"" | source==""DMFSS"")\n#npre <- npre %>% filter(tide.s != ""h"")\nnpre$date<-as.Date(npre$date, ""%m/%d/%Y"" )\nnpre <- arrange(npre, year)\nnpre$month = cut(npre$date, breaks=""month"")\nnpre$jday <- NA\nnpre$jday <- npre$date\nnpre$jday <- format(npre$jday, ""%j"")\nnpre$jday <- as.numeric(npre$jday)\nnpre$variable <- ""raw""\nnpre$value <- npre$salinity\nnpre$site <- npre$use\nnpre$year <- as.Date(as.character(npre$year), format = ""%Y"")\nnpre$year <- as.POSIXct(npre$year, ""%Y/%m/%d"")\nnpre$beforewells <- NA \nnpre$beforewells[npre$prepost==""pre""] <- ""yes""\nnpre$beforewells[npre$prepost==""post""] <- ""no""\n\n###Do next bit of code to summarise WRR/WR together\nnpre$site <- NA \nnpre$site[npre$use == ""CR""]  <- ""CR""  \nnpre$site[npre$use == ""WR""]  <- ""WR"" \nnpre$site[npre$use == ""WRR""]  <- ""WR""\nnpre$site[npre$use == ""PI""]  <- ""PI""                  \nnpre <- npre %>% drop_na(site)\n\n###Create season variables for npre sheet\nnpre$season <- NA \nnpre$season[npre$jday <= 365] <- ""W""\nnpre$season[npre$jday >= 244 & npre$jday <= 334] <- ""F""  \nnpre$season[npre$jday >= 60 & npre$jday <= 151] <- ""SP""\nnpre$season[npre$jday >= 152 & npre$jday <= 243] <- ""SU""\n#create a year column w/ unformatted years to combine with seasons \nnpre$intyear <- format(as.Date(npre$date, format=""%d/%m/%Y""),""%Y"")\n#create a column of bonded season and year\nnpre$seasonyear <- paste0(npre$intyear,""."",npre$season)\n##create study column \nnpre$study <- NA\nnpre$study[npre$intyear>=2013] <- ""CRFL""\nnpre$study[npre$intyear == 1955 & npre$intyear ==1956] <- ""WELLS""\n\n##make a summary sheet for npre.site.year.season\nnpre.season <- ddply(npre, c(""site"", ""seasonyear""), summarise, Mean = mean(value))\n#write csv file to use in communiy analysis \n#write.csv(npre.season, ""npre.season.summary.csv"")\n#filter npre for a npre crfl wells specific sheet \nnpre$study <- NA\nnpre$study[npre$intyear == 2013 | npre$intyear == 2014 |npre$intyear == 2015] <- ""C""\nnpre$study[npre$intyear == 1955 | npre$intyear == 1956] <- ""W""\nnpre.study.sum <- ddply(npre, c(""study"",""use""), summarise, Mean = mean(value), SEM = sd(value)/sqrt(length(value)), N = length(value))\nnpre.study.sum1 <- ddply(npre, c(""study"",""site""), summarise, Mean = mean(value), SEM = sd(value)/sqrt(length(value)), N = length(value))\n##______________________________________________________________________________________________________________________________##\n##summarise salinity data for monthly min/max values \nnpre.month<-ddply(npre, c(""month"", ""site"", ""sourcedata""), \n                  summarise, Mean = mean(salinity), SD = sd(salinity), \n                  SE = sd(salinity)/sqrt(length(salinity)), Max = max(salinity), Min = min(salinity), N = length(salinity))\n\n##summarise salinity data for yearly min/max\nnpre.year<-ddply(npre, c(""year"", ""site"", ""sourcedata""), \n                 summarise, Mean = mean(salinity), SD = sd(salinity), \n                 SE = sd(salinity)/sqrt(length(salinity)), Max = max(salinity), Min = min(salinity), N = length(salinity), Range = max(salinity) - min(salinity))\nnpre.year$year <- as.Date(as.character(npre.year$year), format = ""%Y"")\nnpre.year$year <- as.POSIXct(npre.year$year, ""%Y/%m/%d"")\nnpre.year$beforewells <- npre.year$beforewells <- NA \n##Run for total analysis\nnpre.year$beforewells[1:20] <- ""yes""\nnpre.year$beforewells[21:156] <- ""no""\n##run for the Eco apps revision crfl vs wells work only\n#npre.year$beforewells[1:15] <- ""no""\n#npre.year$beforewells[1:130] <- ""no""\nnpre.year$diffmmax <- npre.year$Max - npre.year$Mean\nnpre.year$diffmmin <- npre.year$Mean - npre.year$Min\n##_______________________________________________________________________________________________________________________________##\n\n##melt npre.year to make a figure w/ legend\nnpre.year.m <- melt(npre.year, id=c(""year"", ""site"", ""beforewells"", ""', 'library(lubridate)\nlibrary(ggplot2)\nlibrary(ggpmisc)\nlibrary(scales)\nlibrary(gridExtra)\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(readr)\nlibrary(zoo)\nlibrary(tidyr) \nlibrary(data.table)\nlibrary(dtplyr)\nlibrary(reshape2)\nrequire(grid)\n#cowplot theme\nrequire(cowplot) \n\n##### import csv ####\n#setwd\nsetwd(""~/Dropbox/Lab Projects/Oyster Projects/oyster resilence 2013 - 2015/New Versions/CRFL-Wells Comparison"")\n\n\n###Load crfl.wells from csv ####\ncrfl.wells <- #Load .csv file, ""\ncrfl.wells <- crfl.wells[ ,-c(1)]\n\nlibrary(vegan)\n##drop sites w/out salinity data (mean by season/year) \ncrfl.wells <- crfl.wells %>% drop_na(Mean)\ncrfl.wells.env <- crfl.wells %>% select(1:35)\ncrfl.wells.fam <- crfl.wells %>% select(36:92)\n\n## separate data based on study for analyses on individual studies\ncrfl.fam <- crfl.wells.fam %>% slice(c(36:81))\ncrfl.env <- crfl.wells.env %>% slice(c(36:81))\nwells.fam <- crfl.wells.fam %>% slice(c(1:35))\nwells.env <- crfl.wells.env %>% slice(c(1:35))\n\n###Create a dissimilary matric based on B-C\ncwf.dist <- vegdist(crfl.wells.fam, method = ""jaccard"", binary=T)\nprint(cwf.dist)\nsummary(cwf.dist)\nplot(cwf.dist)\n\n##Generate jaccard distance matrix of all faunal data\njaccard.fam <- vegdist(crfl.wells.fam, method = ""jaccard"")\n\n##generate a distance matrix of all salinity differences \nsaldiff.cw <- vegdist(crfl.wells.env$Mean, method = ""euclidean"",na.rm = TRUE)\n\n##perform a mantel test to test the correlation between salinity differences and jaccard distances\nmantel(saldiff.cw, jaccard.fam) \n\n##Generate jaccard distance matrix of crfl data\njaccard.crfl <- vegdist(crfl.fam)\n\n##generate jaccard distance matrix of wells data\njaccard.wells <- vegdist(wells.fam)\n\n\n\n## Run function to make a list of community dissimilarities with columns that will be used for B-diversity comparisons between studies \n## show comparison being made. From: https://stackoverflow.com/questions/23474729/convert-object-of-class-dist-into-data-frame-in-r\ndist.df <- function(inDist) {\n  if (class(inDist) != ""dist"") stop(""wrong input type"")\n  A <- attr(inDist, ""Size"")\n  B <- if (is.null(attr(inDist, ""Labels""))) sequence(A) else attr(inDist, ""Labels"")\n  if (isTRUE(attr(inDist, ""Diag""))) attr(inDist, ""Diag"") <- FALSE\n  if (isTRUE(attr(inDist, ""Upper""))) attr(inDist, ""Upper"") <- FALSE\n  data.frame(\n    row = B[unlist(lapply(sequence(A)[-1], function(x) x:A))],\n    col = rep(B[-length(B)], (length(B)-1):1),\n    value = as.vector(inDist))\n}\n\n##Create column lists of dissilimilarity measures with corresponding sample comparisons\ndist.list.fam <- dist.df(jaccard.fam)\ndist.list.crfl <- dist.df(jaccard.crfl)\ndist.list.wells <- dist.df(jaccard.wells)\n\n##inlet distance\ninlet.crfl <- crfl.env[ ,c(11)]\ninlet.dist <- vegdist(inlet.crfl, method = ""euclidean"")\ninlet.dist.crfl <- dist.df(inlet.dist)\ndist.env <- bind_cols(inlet.dist.crfl, dist.list.crfl)\nnames(dist.env)[3] <- ""inlet.dist""\ndist.env <- dist.env[ ,-c(4,5)]\nnames(dist.env)[4] <- ""jacc.diss""\n\n##seasonal mean salinity\nsalseas.crfl <- crfl.env[ ,c(32)]\nsalseas.dist <- vegdist(salseas.crfl, method = ""euclidean"")#make euclidean distance matrix of refractometer differences          \nsalseas.dist.crfl <- dist.df(salseas.dist)\ndist.env <- bind_cols(salseas.dist.crfl, dist.env)\ndist.env <- dist.env[ ,-c(4,5)]\nnames(dist.env)[3] <- ""seassal.diff""\n\n\n##create factors from distances\ndist.env$site.comp <- NA \ndist.env$site.comp[dist.env$inlet.dist==0] <- NA\ndist.env$site.comp[dist.env$inlet.dist>0 & dist.env$inlet.dist<2] <- ""CR.WRR""\ndist.env$site.comp[dist.env$inlet.dist>2 & dist.env$inlet.dist <10] <- ""WRR.PI""\ndist.env$site.comp[dist.env$inlet.dist>10] <- ""CR.PI""\ndist.env$site.comp[dist.env$row > 0 & dist.env$row < 17 & dist.env$col > 0 & dist.env$col < 17] <- ""CR.CR""\ndist.env$site.comp[dist.env$row > 16 & dist.env$row < 33 & dist.env$col > 16 & dist.env$col < 33] <- ""PI.PI""\ndist.env$site.comp[dist.env$row > 32 & dist.env$row < 47 & dist.env$col > 32 & dist.env$col < 47] <- ""WRR.WRR""\n\n\n\n##__________________________WELLS____________________##\n\n##refractometer readings\nrefract.wells <- wells.env[ ,c(12)]   \nrefract.dist.wells <- vegdist(refract.wells, method = ""euclidean"")#make euclidean distance matrix of refractometer differences          \nrefract.dist.wells <- dist.df(refract.dist.wells)\ndist.env.wells <- bind_cols(refract.dist.wells, dist.list.wells)\ndist.env.wells <- dist.env.wells[ ,-c(4,5)]\nnames(dist.env.wells)[3] <- ""ssal.diff""\nnames(dist.env.wells)[4] <- ""jacc.diss""\n\n##inlet distance\ninlet.wells <- wells.env[ ,c(11)]\ninlet.dist.wells <- vegdist(inlet.wells, method = ""euclidean"")\ninlet.dist.wells <- dist.df(inlet.dist.wells)\ndist.env.wells <- bind_cols(inlet.dist.wells, dist.env.wells)\nnames(dist.env.wells)[3] <- ""inlet.dist""\ndist.env.wells <- dist.env.wells[ ,-c(4,5)]\n\n##seasonal mean salinity\nsalseas.wells <- wells.env[ ,c(32)]\nsalseas.dist.wells <- vegdist(salseas.wells, method = ""euclidean"")#make euclidean distance matrix of refractometer differences          \nsalseas.dist.wells <- d']",1,"Coastal squeeze, temperate reefs, salinity, water quality, oyster-associated communities, foundation species, mangroves, saltmarshes, kelps, seagrasses, estuaries, sea-level rise, channelization, municipal"
Data from: Projected impacts of warming seas on commercially fished species at a biogeographic boundary of the European continental shelf,"1. Projecting the future effects of climate change on marine fished populations can help prepare the fishing industry and management systems for resulting ecological, social and economic changes. Generating projections using multiple climate scenarios can provide valuable insights for fisheries stakeholders regarding uncertainty arising from future climate data.2. Using a range of climate projections based on the Intergovernmental Panel on Climate Change A1B, RCP4.5 and RCP8.5 climate scenarios, we modelled abundance of eight commercially important bottom dwelling fish species across the Celtic Sea, English Channel and southern North Sea through the 21st century. This region spans a faunal boundary between cooler northern waters and warmer southern waters, where mean sea surface temperatures are projected to rise by 2 to 4C by 2098.3. For each species, Generalised Additive Models were trained on spatially explicit abundance data from six surveys between 2001 and 2010. Annual and seasonal temperatures were key drivers of species abundance patterns. Models were used to project species abundance for each decade through to 2090.4. Projections suggest important future changes in the availability and catchability of fish species, with projected increases in abundance of red mullet (Mullus surmuletus L.), Dover sole (Solea solea L.), John dory (Zeus faber L.) and lemon sole (Microstomus kitt L.) and decreases in abundance of Atlantic cod (Gadus morhua L.), anglerfish (Lophius piscatorius L.) and megrim (Lepidorhombus whiffiagonis L.). European plaice (Pleuronectes platessa L.) appeared less affected by projected temperature changes. Most projected abundance responses were comparable among climate projections, but uncertainty in the rate and magnitude of changes often increased substantially beyond 2040.5. Synthesis and applications. These results indicate potential risks as well as some opportunities for demersal fisheries under climate change. These changes will challenge current management systems, with implications for decisions on target fishing mortality rates, fishing effort and allowable catches. Increasingly flexible and adaptive approaches that reduce climate impacts on species while also supporting industry adaptation are required.","['# This script has been developed to give an overall idea of the process by which the analysis outlined in Maltby et al 2020 took place. The script falls into three main sections:\n# Section A) an example of trialling a GAM on a single species and climate projection, and extracting different model statistics; \n# Section B) using least square means to identify the model to use over all species-climate projection combinations, using the data within the Supplementary Table S8 outputs (which were all derived using a process similar to that outlined in section A of this script); \n# Section C) using the \'final\' GAM to generate future projections, and tidying up the resulting projections.\n\n\n############################################################\n\n# Set up the workspace\n\n############################################################\n\n# Set the working directory for where the files are found \nsetwd("""")\n\n# Load relevant packages (to install use install.packages() first)\nlibrary(mgcv)\nlibrary(reshape2)\nlibrary(emmeans)\nlibrary(plyr)\nlibrary(MuMIn)\nlibrary(nlme)\n\n# Load in the data for the model to run off initially, which for this example contains CPUE and environmental data for the 2000s decade for the SRES Ens_00 climate projection.\n\ntraining<-read.csv(""Ens_00_CPUE_Env_2000s.csv"")\n# Remove NAs\ntraining <- training[!is.na(training$Plaice_CPUE),]\n\n\n#########################################################################################################\n\n                                # Section A #\n\n# Trialling GAMs over species and climate projections. \n\n# This could be looped so that the same GAM runs over every species-climate projection combination, but for the purposes of this script, a single species and climate projection example is used.\n\n#########################################################################################################\n\n\n# Run a GAM over one species - this example is the \'Full Model\' - Model A, for European plaice and Ens_00 climate projection.\n\nEns_00_mod_plaice <- gam(Plaice_CPUE ~ \n             s(Depth, k=5) + \n             s(Av_Effort, k=5) + \n             s(Grain_Size, k=5) +  \n             s(Dec_Av_NBS, k=5) +   \n             s(Dec_Av_SST, k=5) + \n             s(Dec_Av_NBT, k=5) +\n             s(Summer_Av_SST, k=5) + \n             s(Summer_Av_NBT, k=5) + \n             s(Winter_Av_SST, k=5) + \n             s(Winter_Av_NBT, k=5), \n             family=gaussian (link=""identity""), data=training) \n\n# Look at some of the model statistics.\n\n# Summary of the model\nEns_00_mod_plaice_summary <- summary.gam(Ens_00_mod_plaice) # Need to store this as an object to allow extraction of deviance explained, GCV and R Squared scores.\n\n# Model diagnostic plots\npar(mfrow=c(2,2)) # Change the panel layout to 2 x 2 so it plots all 4 plots together\ngam.check(Ens_00_mod_plaice) # Diagnostics and plots provided\n\n# Look at and extract the model test statistics:\n\nEns_00_mod_plaice_AICc <- AICc(Ens_00_mod_plaice) # AICc score\nEns_00_mod_plaice_rsq <- Ens_00_mod_plaice_summary$r.sq # Adjusted R Squared\nEns_00_mod_plaice_dev_expl <- Ens_00_mod_plaice_summary$dev.expl # Deviance Explained\nEns_00_mod_plaice_GCV <- unname(Ens_00_mod_plaice_summary$sp.criterion) # GCV\n\n# Compare modelled projections for 2000s with observations for 2000s through a correlation test\ntraining_projections<-predict(Ens_00_mod_plaice, type = ""response"")\nEns_00_mod_plaice_cor_test<-cor.test(training$Plaice_CPUE,training_projections)\nEns_00_mod_plaice_correlation <- unname(Ens_00_mod_plaice_cor_test$estimate) # Correlation score\n\n# Store these test statistics in a dataframe. We suggest then saving these in a csv/txt file to allow easier comparison across trialled GAM models, species and climate projections. \nEns_00_mod_plaice_test_statistics <- c(Ens_00_mod_plaice_AICc, Ens_00_mod_plaice_rsq, Ens_00_mod_plaice_dev_expl, Ens_00_mod_plaice_GCV, Ens_00_mod_plaice_correlation) # Make a vector with scores\nTest_statistic <- c(""AICc"", ""Adjusted_R_Squared"", ""Deviance_Explained"", ""GCV"", ""Correlation"") # Make vector with score names\nEns_00_mod_plaice_test_statistics<-data.frame(Test_statistic, Ens_00_mod_plaice_test_statistics) # Bring together\n\n# Some \'cleaning\' to make the dataframe more interpretable\ncolnames(Ens_00_mod_plaice_test_statistics)[2] <- ""Score""\nEns_00_mod_plaice_test_statistics$Model <- rep(""Model A"",5)\nEns_00_mod_plaice_test_statistics$Species <- rep(""European_plaice"", 5)\nEns_00_mod_plaice_test_statistics$Climate_Projection <- rep(""Ens_00"", 5)\n\n# Generating Delta AICc and Akaike weights can be done after all models have been run for each species- climate projection combination.\n\n#########################################################################################################\n                      # Section B #\n\n# Using least square means to identify an optimal GAM to use across all species-climate projection combinations.\n# The example uses the ready-made file which contains GAM test statstic outputs from all trialled GAMs u']",1,"climate change, marine fished populations, fishing industry, management systems, ecological changes, social changes, economic changes, climate scenarios, fisheries stakeholders, Intergovernmental Panel on Climate Change, A1B, RCP4.5, RCP"
Data from: Is it worthwhile scaring geese to alleviate damage to crops?  an experimental study,"Increasing population sizes of geese are the cause of numerous agricultural conflicts in many regions of the Northern Hemisphere. Scaring is often used as a tool to chase geese away from fields, either as a means to protect vulnerable crops or as part of goose management schemes to drive geese to accommodation areas. Geese are quick to habituate to stationary scaring devices; hence, active scaring by humans is often employed. However, it remains undocumented how much effort is required for active scaring to be effective. We explored the relationship between intensity of active human scaring on field use and behaviour by geese. Using an experimental framework, we applied four different scaring doses per day (geese were scared either 2, 5, 7 or 10 times per day), to random pastures in a pink-footed goose spring staging area in mid-Norway, and recorded goose flock sizes, fleeing response distances, and average weekly goose densities assessed by dropping densities. In addition, we counted droppings in fields without scaring. We used mixed models to test for changes in the effects of different scaring doses over time and compared observed with predicted dropping levels. Cumulative dropping densities increased at different rates depending on the scaring dose. Scaring dosage did not affect flock size and fleeing response distance during the study period, but both flock sizes and fleeing response distances changed with time. Scaring dose 2 did not show any decrease in relative goose use compared to the fields without scaring, whereas doses 5, 7 and 10 all showed 7478% fewer droppings by the end of the spring staging period, indicating a possible threshold between dose 2 and 5. The largest effect of scaring appeared during the first week of scaring. Synthesis and applications. This study is the first to show a doseresponse relationship between active scaring and field use of flocking geese. For individual farmers, the study provides guidance on the level of scaring effort needed to be cost-effective. If implemented as part of a management scheme with subsidy/accommodation areas in combination with systematic and persistent scaring, it can be used as a tool to keep geese away from areas where they are not wanted, thereby assisting in the alleviation of gooseagriculture conflicts. The approach in this study can be adapted and used in a wider range of wildlife interactions with human economic interests.","['\r\n#---------start of dropping count analyses-------------------------------------------------\r\n\r\n#figs are exported as \'copy to clipboard\', width 265, maintain ratio, export at bitmap\r\n#rm(list = ls())\r\n#removes everyting in global environment\r\n\r\ncount=read.table(file=""count_final.txt"",header=T,sep=""\\t"")\r\npoly=read.table(file=""poly_final.txt"",header=T,sep=""\\t"")\r\n\r\n#summary(count)\r\n#summary(poly)\r\n\r\n\r\n#this is all based on scaring dose, but as some fields with high scaring dose\r\n#never (or one to two times) had geese they should be removed from the data: 66, 75, 401\r\n#excluding 55, 76 as they don\'t share scaring dose with other fields\r\n#excluding polygon 344 because of ploughing\r\n\r\ndata1<-merge(poly, count, by.x=""polygon"", by.y=""polygon"")\r\n\r\ncount1<-data1[data1$polygon!=66 & data1$polygon!=75 & data1$polygon!=401 & data1$polygon!=344 & data1$polygon!=76 & data1$polygon!=55, ]\r\n#summary(count1)\r\n\r\nlibrary(ggplot2)\r\nlibrary(lme4)\r\nlibrary(grid)\r\nlibrary(gridExtra)\r\nlibrary(plyr)\r\n\r\ncount0<-subset(count1, week==3)\r\n# overview of no. of fields investigated (part of fig. 1)\r\ncolors = c(rep(""grey"",1),rep(""goldenrod1"",1),rep(""darkorange"",1),rep(""orangered"",1),rep(""orangered4"",1))\r\nqplot(as.factor(scardose), data=count0, geom=""histogram"",binwidth = 1) +\r\n  geom_histogram(colour=""black"", fill=colors)+ xlab(""Scaring dosage"") +\r\n  ylab(""No. of fields"") + scale_y_continuous( breaks=c(seq(0,10,by=2)))  + theme_bw() +\r\n  theme(axis.title.x = element_text(face=""plain"", colour=""black"", size=25),\r\n        axis.text.x  = element_text(angle=0, vjust=0, size=20)) +\r\n  theme(axis.title.y = element_text(face=""plain"", colour=""black"", size=25),\r\n        axis.text.y  = element_text(angle=0, vjust=0, size=20))\r\n\r\n#######################################################################\r\n###### Does scaring dose have a negative effect on goose use?  ########\r\n\r\n#is goose dropping density influenced by scaring vs. no scaring (1)\r\n#and\r\n#is goose dropping density influenced by scaring dose? (2)\r\n\r\n#####   (1)         SCARDOSE          ############\r\n\r\ncolorspoly = c(rep(""grey"",2),rep(""goldenrod1"",2),rep(""grey"",1),rep(""goldenrod1"",2),rep(""grey"",2),rep(""orangered4"",2),\r\n               rep(""orangered"",3),rep(""darkorange"",1),rep(""goldenrod1"",1),rep(""grey"",3),rep(""orangered"",2),rep(""darkorange"",1),\r\n               rep(""orangered"",1),rep(""grey"",1),rep(""darkorange"",1),rep(""grey"",1))\r\n\r\nggplot(aes(y = accudrop, x = day, colour = as.factor(polygon)), data=count1) +\r\n  geom_point() + geom_line() +\r\n  scale_colour_manual(values=colorspoly) +\r\n  xlab(""Days since experiment start"") + ylab(""Cumulative no. of droppings per 2-m radius circle"") +\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), \r\n        panel.background = element_blank(), axis.line = element_line(colour = ""black"")) +\r\n  theme(axis.title.x = element_text(face=""plain"", colour=""black"", size=10),\r\n        axis.text.x  = element_text(angle=0, vjust=0, size=10)) +\r\n  theme(axis.title.y = element_text(face=""plain"", colour=""black"", size=10),\r\n        axis.text.y  = element_text(angle=0, vjust=0, size=10)) +\r\n  theme(legend.position=""none"")\r\n\r\n#relationship does not look linear - can\'t use accumulative dropping count\r\n#split up in weeks, model with scardose + without -> compare the AIC of the two models\r\n#if the scardose does not add to model strength it should be left out which means\r\n#there is no effect of scaringdose \r\n\r\n\r\n###########################################################################\r\n####### compare observed with predicted values ############################\r\n\r\ncount2<-subset(count1, week !=0)\r\n\r\n### observed ###\r\n\r\nsummary(count1)\r\nddply(count2, .(week,scardose), summarize, med = median(log((drop+1)/(inidrop.x+1))))\r\nmeans<-ddply(count1, .(week,scardose), summarize,  drop=mean(drop))\r\nmeans0<-ddply(means, .(week), summarize,  drop=mean(drop[scardose==0]))\r\nmeans2<-ddply(means, .(week), summarize,  drop=mean(drop[scardose==2]))\r\nmeans5<-ddply(means, .(week), summarize,  drop=mean(drop[scardose==5]))\r\nmeans7<-ddply(means, .(week), summarize,  drop=mean(drop[scardose==7]))\r\nmeans10<-ddply(means, .(week), summarize,  drop=mean(drop[scardose==10]))\r\n\r\nmeanchg01<-((means0$drop[means0$week==1]-means0$drop[means0$week==0])/means0$drop[means0$week==0])\r\nmeanchg12<-((means0$drop[means0$week==2]-means0$drop[means0$week==1])/means0$drop[means0$week==1])\r\nmeanchg23<-((means0$drop[means0$week==3]-means0$drop[means0$week==2])/means0$drop[means0$week==2])\r\n\r\nobs0<-means$drop[means$week==0]\r\nobs1<-means$drop[means$week==1]\r\nobs2<-means$drop[means$week==2]\r\nobs3<-means$drop[means$week==3]\r\nobssum<-obs0+obs1+obs2+obs3\r\nobsall<-cbind(obs0,obs1,obs2,obs3)\r\n\r\nobsplot<-cbind(obs0,obs1,obs2,obs3,scardose=c(0,2,5,7,10))\r\nrownames(obsplot, do.NULL = TRUE, prefix = ""row"")\r\nrownames(obsplot)<-c(0,2,5,7,10)\r\n\r\nobsplot1 <- as.data.frame(obsplot)\r\nobsplot2 <- data.frame(scardose= obsplot1$scardose,stack(obsplot1,select=-scardose))\r\n\r\nggplot(data=obsplot2, aes(x=ind, y=values, g']",1,"geese, agricultural conflicts, scaring, crop damage, habitat management, goose management schemes, flock sizes, fleeing response distance, dropping densities, doseresponse relationship, active scaring, cost-effective, subsidy, wildlife interactions, human economic interests."
Data from: Using plant functional traits and phylogenies to understand patterns of plant community assembly in a seasonal tropical forest in Lao PDR,"Plant functional traits reflect different evolutionary responses to environmental variation, and among extant species determine the outcomes of interactions between plants and their environment, including other plant species. Thus, combining phylogenetic and trait-based information can be a powerful approach for understanding community assembly processes across a range of spatial scales. We used this approach to investigate tree community composition at Phou Khao Khouay National Park (1814'-1832'N; 10238'- 10259'E), Laos, where several distinct forest types occur in close proximity. The aim of our study was to examine patterns of plant community assembly across the strong environmental gradients evident at our site. We hypothesized that differences in tree community composition were being driven by an underlying gradient in soil conditions. Thus, we predicted that environmental filtering would predominate at the site and that the filtering would be strongest on sandier soil with low pH, as these are the conditions least favorable to plant growth. We surveyed eleven 0.25 ha (50x50 m) plots for all trees above 10 cm dbh (1221 individual trees, including 47 families, 70 genera and 123 species) and sampled soils in each plot. For each species in the community, we measured 11 commonly studied plant functional traits covering both the leaf and wood economic spectrum traits and we reconstructed a phylogenetic tree for 115 of the species in the community using rbcL and matK sequences downloaded from Genebank (other species were not available). Finally we compared the distribution of trait values and species at two scales (among plots and 10x10m subplots) to examine trait and phylogenetic community structures. Although there was strong evidence that an underlying soil gradient was determining patterns of species composition at the site, our results did not support the hypothesis that the environmental filtering dominated community assembly processes. For the measured plant functional traits there was no consistent pattern of trait dispersion across the site, either when traits were considered individually or when combined in a multivariate analysis. However, there was a significant correlation between the degree of phylogenetic dispersion and the first principle component axis (PCA1) for the soil parameters. Moreover, the more phylogenetically clustered plots were on sandier soils with lower pH. Hence, we suggest that the community assembly processes across our site may reflect the influence of more conserved traits that we did not measure. Nevertheless, our results are equivocal and other interpretations are possible. Our study illustrates some difficulties in combining trait and phylogenetic approaches that may result from the complexities of integrating spatial and evolutionary processes that vary at different scales.","['library(ape)\nlibrary(picante)\nlibrary(geiger)\nlibrary(spacodiR)\n\n  ## convert community dataset to be Spacodi format\ncomm <- read.table(file=""community.csv"", header=T, row.names=1, sep="","")\n\ncomm.new <- as.spacodi(data=comm)\nwrite.csv(comm.new, file=""community dataset Spacodi format.csv"")\n\n ## randomize a community phylogenetic matrix to null model 1s in Hardy 2008 by using SpacodiR package\ncommunity.spacodi <- read.table(file=""community dataset Spacodi format.csv"", header=T, row.names=1, sep="","")\nrandomize.1s <- resamp.1s(obj=community.spacodi)\n\n  ## convert randomized community data to picante format\nnew.community<-as.picante(ramdomize.1s)\n\n\t## phylogenic tree (newick format), use read.nexus for nexus format file\ntree <- read.tree(file=""rooted tree.NWK"")\n\n\n\t## apply branch length on phylogenetic tree (incase if tree has no branch lenght)\ntree.branch <- compute.brlen(tree, method=""power"")\n\n\t### check for mismatches/missing species between phylogenetic tree and community data\ncombined <- match.phylo.comm(tree.branch,new.community)\n\n\t## convert phylogenetic tree to distance matrix\nphydist<-cophenetic(tree.branch)\n\n\t### calculate SES Mean Pairwise distance and nri\nses.fri<-ses.mpd(new.community, phy, null.model=NULL, abundance.weighted=T)\t\n\nnri.fri<- -1*((ses.fri$mpd.obs-ses.fri$mpd.rand.mean)/(ses.fri$mpd.rand.sd))\n\n\n', 'library(picante)\nlibrary(geiger)\nlibrary(spacodiR)\nlibrary(vegan)\nlibrary(FD)\n\n##### I. Examine phylogenetic similarity \n## community data\ncom.new<-read.table(file=""community.csv"", header=T, row.names=1, sep="","")\n\n## apply null model\nrandomize.1s<-resamp.1s(obj=com.new)\n\n## phylogeny file tree\ntree<-read.tree(file=""Phylogenetic tree-rooted.NWK"")\n\n## apply branch length\ntree.branch <- compute.brlen(tree, method=""power"")\n\n## check for mismatches/missing species\ncombined <- match.phylo.comm(tree.branch,randomize.1s)\n\n### phylogenetic similarity\npp <- phylosor (randomize.1s, tree.branch)\n\n\n\n#### II. Examine trait similarity\n##trait data\ntrait <- read.table(file=""trait.csv"",header=T, row.names=1, sep="","")\n\n## tranform traits data to be distance matrix\nD <- vegdist(trait,""gower"")\n\n## run cluster analysis\ntree <- hclust(D,""average"")\n\n## tranform cluster dendrogram to phylogetic tree (tips are species names, branch lengths are distance matrix)\nrequire(graphics)\ntree.p <- as.phylo(tree)\n\n##exporting tree from R\nwrite.tree(tree.p, file=""tree.nex"")\n\n## community data\ncom <-read.table (file=""community.csv"", header = T, row.names = 1, sep="","")\n\n## apply null model\nrando.1s<-resamp.1s(obj=com)\ncomm.1s<-as.picante(rando.1s)\n\n## check for mismatches/missing species\ncombined <- match.phylo.comm (tree.p, comm.1s)\n\n### trait similarity distance matrix\np <- phylosor (rando.1s, tree.p)\n\n\n', 'library(FD)\nlibrary(spacodiR)\n\n  ## community basal area data\nbasal <- read.table(file=""community basal area.csv"", header=T, row.names=1, sep="","")\nnew.basal <- data.matrix (basal)\n\n## community data with relative abundance\nabundance<-read.table(file=""community_sp richness.csv"", header=T, row.names=1, sep="","")\n\n  ## randomize a community phylogenetic matrix to null model 1s in Hardy 2008 by using SpacodiR package\nbasal.1s <- resamp.1s(obj=basal)\nnew.1s <- data.matrix(basal.1s)\n\nabun.1s <- resamp.1s (obj=abundnace)\nnew.abun <- data.matrix (abun.1s)\n\n  ## import trait data\ntrait <- read.table(file=""trait data.csv"", header=T, row.names =1, sep="","")\n  \n  ## log transformed trait data\nlogtrait <- log10(trait)\n\n  ## convert trait data to distance matrix\ndis.trait <- gowdis(logtrait)\n\n  ## calculate FDis.obs\nFDis.obs <- fdisp(dis.trait, new.basal)\n\n  ## to see FDis values\nFDisobs <- FDis.obs$FDis\n\n  ## calculate FDis.null\nFDis.null <- fdisp(dis.trait, new.1s)\nFDisnull <- FDis.null$FDis\n  \n  ## FDis.sd\nFDissd <- sd(FDisnull)\n  \n  ## calculate ZFDis\nZFDis <- ((FDisobs - FDisnull)/FDissd)']",1,"Plant functional traits, phylogenies, community assembly, seasonal tropical forest, Lao PDR, environmental variation, interactions, spatial scales, tree community composition, Phou Khao Khouay National Park, soil conditions, environmental filtering, plant growth"
Local versus broad scale environmental drivers of continental beta diversity patterns in subterranean spider communities across Europe,"Macroecologists seek to identify drivers of community turnover (-diversity) through broad spatial scales. Yet, the influence of local habitat features in driving broad-scale -diversity patterns remains largely untested, due to the objective challenges of associating local-scale variables to continental-framed datasets. We examined the relative contribution of local- versus broad-scale drivers of continental -diversity patterns, using a uniquely suited dataset of cave-dwelling spider communities across Europe (3570 latitude). Generalized dissimilarity modeling showed that geographical distance, mean annual temperature, and size of the karst area in which caves occurred drove most of -diversity, with differential contributions of each factor according to the level of subterranean specialization. Highly specialized communities were mostly influenced by geographical distance, while less specialized communities were mostly driven by mean annual temperature. Conversely, local-scale habitat features turned out to be meaningless predictors of community change, which emphasizes the idea of caves as the human accessible fraction of the extended network of fissures that more properly represents the elective habitat of the subterranean fauna. To the extent that the effect of local features turned to be inconspicuous, caves emerge as experimental model systems in which to study broad biological patterns without the confounding effect of local habitat features.","['######################### APPENDIX S5 #######################\n\n## Local versus broad scale environmental drivers of continental beta diversity patterns in subterranean spider communities across Europe\n## Mammola S., et al.\n\n## R code for running the main analyses\n\n##############################################################\n####################### Set working directory ################\n\nsetwd("" "") #<---- specify here your working directory\n\n##############################################################\n######################### Load R packages ####################\n\nlibrary(""gdm"")\nlibrary(""ggplot2"")\nlibrary(""gambin"")\nlibrary(""BAT"")\nlibrary(""gridExtra"")\n\n##############################################################\n################### plotting parameters ######################\n\nggplot_Theme =theme(axis.text = element_text(size = 10),\n            axis.title.x=element_text(size=12),\n            axis.title.y=element_text(size=12, angle=90),\n            panel.grid.major = element_line(colour = ""grey90"",linetype=""dotted"",size=0.3),\n            legend.background = element_rect(fill=""gray97"", size=0.3, linetype=""solid"",color=""black""),\n            legend.position = c(0.60, 0.7),\n            legend.text = element_text(colour=""black"", size=10),\n            legend.title=element_blank())\n\n##############################################################\n################ Load environmental table ####################\n\nenvTab=read.table(""Appendix_S2_data_env.txt"")\n\nstr(envTab)\n\n## These are both local and broad scale variables (extracted for the coordinates of the localities)  in the format useful for Gdm analyses\n## Only uncollinear variables are in this dataset\n## For the full local scale dataset see the data paper associated to this manuscript \n## For repositories where to download broad scale predictors see details in the method section\n\n##############################################################\n##############################################################\n##############################################################\n\n################ Generalized Dissimilarity Modelling \n################ Troglophiles\n\n##############################################################\n##############################################################\n##############################################################\n\n## Table for Troglophiles (TF)\n\nTabTF=read.table(""Appendix_S4_data_troglophiles.txt"")\n\n## Extract environmental variables for TF sites\n\nenvTabTF <- envTab[envTab$site %in% TabTF$site, ]\n\n## Creating a SitePair table object for GDM\n\nsitePairTabTF <- formatsitepair(TabTF[,1:4], \n                                2, \n                                dist=""bray"",\n                                XColumn=""x"", \n                                YColumn=""y"",\n                                sppColumn=""species"",\n                                siteColumn=""site"", \n                                weightType=""richness"",\n                                predData=envTabTF,\n                                sppFilter=4)\n\n\nsitePairTabTF= na.omit(sitePairTabTF)\n\n## Fitting the model\n\ngdmTabModTF <- gdm::gdm(sitePairTabTF,geo=T)\nsummary(gdmTabModTF)\nplot(gdmTabModTF)\n\n## Calculating p-values and variable importance (Warning: computation time is long)\n\nmodTest <- gdm.varImp(sitePairTabTF, geo=T, nPerm=50, parallel=T, cores=10)\n\n## Plotting model oputput\n\npar(mfrow=c(1,1))\nbarplot(sort(modTest[[2]][,1], decreasing=T))\n\nplotUncertainty(sitePairTabTF, sampleSites=0.7, bsIters=5, geo=TRUE, plot.layout=c(1,3),errCol=""grey90"",splineCol=""black"")\n\n## Model selection\n\nsitePairTabTF <- formatsitepair(TabTF[,1:4], \n                                2, \n                                dist=""bray"",\n                                XColumn=""x"", \n                                YColumn=""y"",\n                                sppColumn=""species"",\n                                siteColumn=""site"", \n                                weightType=""richness"",\n                                predData=envTabTF[,c(1:3,5:9,12)],\n                                sppFilter=4)\n\nsitePairTabTF= na.omit(sitePairTabTF)\n\n## Fitting the model\n\ngdmTabModTF <- gdm::gdm(sitePairTabTF,geo=TRUE)\nsummary(gdmTabModTF)\nplot(gdmTabModTF)\n\n## Calculating p-values and variable importance (Warning: computation time is long)\n\nmodTest <- gdm.varImp(sitePairTabTF, geo=T, nPerm=50, parallel=T, cores=10)\n\n## Plotting model oputput\n\npar(mfrow=c(1,1))\nbarplot(sort(modTest[[2]][,1], decreasing=T))\n\nplotUncertainty(sitePairTabTF, sampleSites=0.7, bsIters=5, geo=TRUE, plot.layout=c(1,3),errCol=""grey90"",splineCol=""black"")\n\n##############################################################\n##############################################################\n##############################################################\n\n################ Generalized Dissimilarity Modelling \n################ Troglobionts\n\n##############################################################\n##############################################################\n#######']",1,"local habitat, broad-scale, environmental drivers, continental beta diversity patterns, subterranean spider communities, Europe, macroecologists, community turnover, geographical distance, mean annual temperature, size of the karst area, subterranean specialization, highly specialized"
Gut Microbiome Dysbiosis is Associated with Increased Mortality following Solid Organ Transplantation,"Supporting codes and testing data for manuscript ""Gut Microbiome Dysbiosis is Associated with Increased Mortality following Solid Organ Transplantation"". Also available at github: https://github.com/GRONINGEN-MICROBIOME-CENTRE/TransplantLines . Readme file is included in the archive.","['\n\nlibrary(microbiome)\n\n# filter data\n\nps_all <- phyloseq(otu_table(CDtaxa_All_only_s, taxa_are_rows=F),\n                   sample_data(CDmeta_All),\n                   tax_table(CDtaxatbl_All)) %>% \n  aggregate_rare(level = ""species"", detection = .1/100, prevalence = 10/100)\n\n\nps_hc <- prune_samples(samples=CDmeta_All[CDmeta_All$exclude_liver_long %in% ""Healthy_Control_DAG3 Control"",]$ID, x=ps_all)\n\nps_hc_rtr <- prune_samples(samples=CDmeta_All[CDmeta_All$exclude_liver_long %in% c(""RTR_CS_TxN"",""3months_TxN"",""6months_TxN"",""12months_TxN"",""24months_TxN"",""PreTxRTR_TxN"",""Healthy_Control_DAG3 Control""),]$ID, x=ps_all)\n\nps_rtr_long <- prune_samples(samples=CDmeta_All[CDmeta_All$exclude_liver_long %in% c(""PreTxRTR_TxN"",""3months_TxN"",""6months_TxN"",""12months_TxN"",""24months_TxN""),]$ID, x=ps_all)\n']",1,"Gut microbiome, dysbiosis, increased mortality, solid organ transplantation, supporting codes, testing data, github, Groningen Microbiome Centre, TransplantLines, archive."
FixYourRain,"Aim""FixYourRain"" is a simple tool, built under free R code, whith main goal to facilitate the organization of precipitation data downloaded or requested from the Spanish Meteorological Agency (AEMET).Data SourcePrecipitation data sets requested from AEMET (https://www.aemet.es/es/portada).MotivationThe daily precipitation data provided by AEMET is delivered in a file/spreadsheet where each day of the month occupies a different column in a spreadsheet. Ususally, the users, for treatment or analysis, organize the data with daily dates continuously, and each AEMET rain gauge occupying a column. Runing ""FixYourRain"" tool, make possible to carry out these changes of format with a minimum investment of time and zero cost of economic resources. HOW TO USE IT1. Organize input dataCreate a CSV in your Spreadsheet program or download the sample file (precipitation_to_fix.csv).Copy and paste the original data according to the following columns:Column 1 (INDICATIVO): the name of the rain gauge must not begin with a number. It is convenient to rename those stations that start their code with a number using the Excel concatenate tool (or the similar tool in the software you are using) and adding a letter before the station name.Columns 2 and 3 (ANO and MES):refers to the year and month of measurement.Columns from 4 to 34: contains the information of RAIN GAUGES from AEMET, day 1 to 31. (Note: do not fill the gaps in days 29,30 or 31 when are empty).2. Download and Install R y RStudioDownload and Install R: (https://cran.r-project.org/bin/windows/base/)Download and Install RStudio Deskstop: (https://www.rstudio.com/products/rstudio/)3. Download the script from this repositoryDownload the script of this repository and save it on a known directory.Both, the script and input data must be on the same directory.4. Start RStudio and run the scriptStart the script FixYourRain: File > OpenFile and select FixYourRain.RBefore running the script, change the directory of your files at row 12 > setwd C:/Users/miequipo/Documents/ *(read below)*Be sure that the link use this separator / and no \Run the scriptOutput data could be found on selected directory as a .csv file.5. Check output dataTake in account the output data have been transformated following these chenges:Data relocated in columns.Units changed: from tenths of mm to mm.Negative data have been replace by 0.6. Treat output data to be able to explore and explote itRemember to check decimal separators on your settings (periods or commas). Also you can replace it masivement on Notepad.","['# Instalar y cargar los paquetes necesarios\r\n# Installing and loading packages\r\ninstall.packages(""pacman"")\r\npacman::p_load(readr, tidyverse, tidyr, zoo, reshape2, dplyr, padr)\r\n\r\n# Definir formato anglosajon para las fechas\r\n# Set datetime format to English   \r\nSys.setlocale(""LC_TIME"", ""English"")\r\n\r\n# Escribir directorio donde estan los datos de entrada\r\n# Set working directory with input data file\r\nsetwd(""My/Path"")\r\n\r\n# Abrir el archivo de datos de entrada y asignarlo a un data frame\r\n# Load input data as a data frame\r\ndf <- as.data.frame(read_delim(""precipitation_to_fix.csv"", \r\n                                         delim = "";"", escape_double = FALSE, trim_ws = TRUE))\r\n\r\n# Preprocesado de nombres de columnas\r\n# Column names preprocessing\r\nnames(df)[2] <- \'ANO\'\r\nnames(df)[4:34] <- as.character(seq(1:31))\r\n\r\n# Definir el formato de la fecha\r\n# Specify date format\r\ndf$fecha <- as.yearmon(paste(df$ANO, df$MES), ""%Y %m"") \r\nhead(df$fecha)\r\n\r\n# Reformatear el data frame de acuerdo a nombre de la estacion y fecha\r\n# Melt data frame for casting based on rain gauge station and date\r\ndf.m <- melt(df[,-c(2:3)], id.vars = c(\'INDICATIVO\', \'fecha\'))\r\ndf.m$date <-paste(df.m$variable, df.m$fecha, sep = """")\r\ndf.m$date <- as.Date(df.m$date, ""%d%B%Y"")\r\n\r\n# Ordenar y limpiar el data frame, e indicar que la columna INDICATIVO va a ser el factor que define cada estacion\r\n# Sort and clean data frame, and encode rain gauge station name (column INDICATIVO) as factor\r\ndf.m <- arrange(df.m, INDICATIVO, date)\r\ndf.m <- df.m[,-c(2:3)]\r\ndf.m$INDICATIVO <- as.factor(df.m$INDICATIVO)\r\n\r\n# Renombrar columnas\r\n# Rename columns\r\ncolnames(df.m) <- c(\'Station\',\'Precipitation\',\'Date\')\r\n\r\n# Pasar la precipitacion de decimas de mm a mm\r\n# Transform precipitation from tenths of mm to mm\r\ndf.m$Precipitation <- df.m$Precipitation/10\r\n\r\n# Eliminar los negativos en el caso de la precipitacion. ¡¡¡¡ATENCION!!!!: esta linea no debo correrla si estoy tratando temperatura\r\n# Deleting negative values. WARNING!!!!: do not not run this line if processing temperature data \r\ndf.m$Precipitation <- with(df.m, ifelse(Precipitation < 0, 0, Precipitation))\r\n\r\n# Limpiar la columna date de NA\r\n# Removing NAs from date column\r\nsummary(df.m)\r\ndf.m <- df.m %>% drop_na(Date)\r\n\r\n# Colocar las filas en columnas segun el nombre de la estacion y rellenar los datos faltantes con NA\r\n# Relocating rows as a columns according to rain gauge name and refill missing values with NA\r\nclean_series <- df.m %>%\r\n  spread(key = Station, value = Precipitation)\r\n\r\nclean_series <- pad(clean_series)\r\n\r\n# Exportar archivo de salida\r\n# Export clean data as csv file\r\nwrite_delim(clean_series, file=""YourRain_fixed.csv"", delim ="";"")\r\n']",1,"FixYourRain, precipitation data, AEMET, rain gauge, daily dates, format, organization, data source, CSV, R code, RStudio, script, input data, output data, data transformation, decimal separators, cost-saving."
Database populated with European diversification experiences,"The EU Horizon 2020 project DiverIMPACTS aims to promote the realisation of the full potential of crop diversification through rotation, multicropping and intercropping by demonstrating technical, economic and environmental benefits for famers, along the value chain and for society at large, and by providing innovations that can remove existing barriers and lock-ins of practical diffusion.DiverIMPACTS does so by combining findings from several participatory case studies with a set of field experiments across Europe, and translating these into strategies, recommendations and fit-for-purpose tools developed with and for farmers, advisors and other actors along the value chain.To first gain a good overview of the current situation, i.e. the existing success stories and challenges of crop diversification in Europe, Work Package 1 (WP 1) identified and analysed factors of success and failure associated with a variety of crop diversification experiences (CDEs) outside those already represented in the consortium (see Deliverable 1.1). WP 1 thus makes sure that the rich experience with crop diversification initiatives across Europe (e.g. from other Horizon 2020 projects) is taken into account for developing strategies, recommendations and tools.Deliverable 1.1 provided i) a list of key drivers (ex ante occurrence of market opportunities, environmental constraints, availability of enabling advisory services, land and workforce availability etc.) to be further considered in WP3, and WP5; and ii) a comprehensive and exhaustive description of the links between key factors and CDE types. This analysis is the basis for consolidating or updating the tentative typology of crop diversification situations used for setting up DiverIMPACTS (case studies), and was used for selecting experiences for more detailed investigations in T1.2. It also complements the identification and characterisation of lock-ins and barriers to crop diversification, and serves their overcoming. During the process of collecting, cleaning and analysing the survey data, a Database of European diversification experiences was created.All together 128 valid responses from 15 European countries  mainly from the project countries Belgium, France, Germany, Hungary, Italy, the Netherlands, Poland, Romania, Sweden, Switzerland, and UK, but also from Denmark, Finland, Luxemburg and Spain were received in T1.1, and were included in the database.The database is stored in original and back-up form in a tabular ='.csv'= format that can be opened in Excel on the Sharepoint system of the project and now on Zenodo, under restricted WP1 area. A further ='.csv'= file was created to store the metadata of the database. This file helps to have a better overview of the questions and sub-questions that were asked in the survey and the type of answer that could be provided to each of them (e.g. factor, Yes-No selection or character).Using the meta data and the database, a selection of personal data fields has been made (e.g. email addresses and names of people) that cannot be published with open access, and needs special attention and data handling. These variables were removed from the original database, and a public version of the database was created that can be shared with third parties. Links to the data files will be shared here after.Developing a Shiny(c) application in R was chosen as a solution to visualize the public data, and make it possible for Partners and all interested parties to interactively view the survey results. The Shiny application is shared as an R-package and are freely accessible on the internet. The users have the possibility to download application and public data in order to visualize them on their own computer. A remote solution, facilitating the consultation of the data, will be installed in CRA-W, where the open data analyses module will be hosted. A short user guide and tutorial is part of this deliverable for helping interested parties to use the Shiny interface.The chosen approach, linking R scripts, R packages and data files, will be useful in the future in order to continiously complete the data base and to update the application (new graphs, new functions regarding the demand of the main users). The release of the application will be shared using modern technologies of information and communication : project website, newsletter, blogs, twitter and other social networks.The main deliverable (D1.2) which is public, is available here : 10.5281/zenodo.3966852","['## Copyright (c) 2018 Frédéric Vanwindekens, Dóra Drexler\n## Centre wallon de Recherches agronomiques (CRA-W)\n## Hungarian Research Institute of Organic Agriculture (ÖMKi),\n## Projet DiverIMPACTS (https://www.diverimpacts.net/)\n## License cc-by\n\ndata.full <- read.csv(\n    ""./data-raw/survey_438634_R_data_file-names.csv"",\n    quote = ""\'\\"""",\n    na.strings = c("""", ""\\""\\""""),\n    stringsAsFactors = FALSE,\n    skip = 5)\n', '## Copyright (c) 2018 Frédéric Vanwindekens, Dóra Drexler\n## Centre wallon de Recherches agronomiques (CRA-W)\n## Hungarian Research Institute of Organic Agriculture (ÖMKi),\n## Projet DiverIMPACTS (https://www.diverimpacts.net/)\n## License cc-by\n\ndata <- read.csv(""./survey_438634_R_data_file.csv"",\n                 quote = ""\'\\"""",\n                 na.strings = c("""", ""\\""\\""""),\n                 stringsAsFactors = FALSE,\n                 skip = 5) ## the five-lines-license\n\n# LimeSurvey Field type: F\ndata[, 1] <- as.numeric(data[, 1])\nattributes(data)$variable.labels[1] <- ""id""\nnames(data)[1] <- ""id""\n# LimeSurvey Field type: DATETIME23.2\ndata[, 2] <- as.character(data[, 2])\nattributes(data)$variable.labels[2] <- ""submitdate""\nnames(data)[2] <- ""submitdate""\n# LimeSurvey Field type: \ndata[, 3] <- as.character(data[, 3])\nattributes(data)$variable.labels[3] <- ""lastpage""\nnames(data)[3] <- ""lastpage""\n# LimeSurvey Field type: A\ndata[, 4] <- as.character(data[, 4])\nattributes(data)$variable.labels[4] <- ""startlanguage""\nnames(data)[4] <- ""startlanguage""\n# LimeSurvey Field type: A\ndata[, 5] <- as.character(data[, 5])\nattributes(data)$variable.labels[5] <- ""token""\nnames(data)[5] <- ""token""\n# LimeSurvey Field type: DATETIME23.2\ndata[, 6] <- as.character(data[, 6])\nattributes(data)$variable.labels[6] <- ""startdate""\nnames(data)[6] <- ""startdate""\n# LimeSurvey Field type: DATETIME23.2\ndata[, 7] <- as.character(data[, 7])\nattributes(data)$variable.labels[7] <- ""datestamp""\nnames(data)[7] <- ""datestamp""\n# LimeSurvey Field type: A\ndata[, 8] <- as.character(data[, 8])\nattributes(data)$variable.labels[8] <- ""ipaddr""\nnames(data)[8] <- ""ipaddr""\n# LimeSurvey Field type: A\ndata[, 9] <- as.character(data[, 9])\nattributes(data)$variable.labels[9] <- ""1.1 Title/Name of diversification initiative""\nnames(data)[9] <- ""A1""\n# LimeSurvey Field type: A\ndata[, 10] <- as.character(data[, 10])\nattributes(data)$variable.labels[10] <- ""[Country] 1.2 Geographical location of diversification initiative""\nnames(data)[10] <- ""A2_SQ001""\n# LimeSurvey Field type: A\ndata[, 11] <- as.character(data[, 11])\nattributes(data)$variable.labels[11] <- ""[Region(s)] 1.2 Geographical location of diversification initiative""\nnames(data)[11] <- ""A2_SQ002""\n# LimeSurvey Field type: F\ndata[, 12] <- as.numeric(data[, 12])\nattributes(data)$variable.labels[12] <- ""[Rotation was applied (different crops in successive growing years) Please describe the typical plant order.] 1.3 Please describe the production practice before the diversification initiative. Use the comment box to complete your answer.""\ndata[, 12] <- factor(data[, 12], levels=c(1,0),labels=c(""Yes"", ""Not selected""))\nnames(data)[12] <- ""A3_SQ001""\n# LimeSurvey Field type: A\ndata[, 13] <- as.character(data[, 13])\nattributes(data)$variable.labels[13] <- ""[Comment] 1.3 Please describe the production practice before the diversification initiative. Use the comment box to complete your answer.""\nnames(data)[13] <- ""A3_SQ001comment""\n# LimeSurvey Field type: F\ndata[, 14] <- as.numeric(data[, 14])\nattributes(data)$variable.labels[14] <- ""[Multicropping was applied (different crops within one growing year) Please describe the typical plant order.] 1.3 Please describe the production practice before the diversification initiative. Use the comment box to complete your answer.""\ndata[, 14] <- factor(data[, 14], levels=c(1,0),labels=c(""Yes"", ""Not selected""))\nnames(data)[14] <- ""A3_SQ002""\n# LimeSurvey Field type: A\ndata[, 15] <- as.character(data[, 15])\nattributes(data)$variable.labels[15] <- ""[Comment] 1.3 Please describe the production practice before the diversification initiative. Use the comment box to complete your answer.""\nnames(data)[15] <- ""A3_SQ002comment""\n# LimeSurvey Field type: F\ndata[, 16] <- as.numeric(data[, 16])\nattributes(data)$variable.labels[16] <- ""[Intercropping was applied (growing different species or cultivars in proximity in the same field) Please describe the typical intercropping (e.g. mixed, row or strip intercropping).] 1.3 Please describe the production practice before the diversification initiative. Use the comment box to complete your answer.""\ndata[, 16] <- factor(data[, 16], levels=c(1,0),labels=c(""Yes"", ""Not selected""))\nnames(data)[16] <- ""A3_SQ003""\n# LimeSurvey Field type: A\ndata[, 17] <- as.character(data[, 17])\nattributes(data)$variable.labels[17] <- ""[Comment] 1.3 Please describe the production practice before the diversification initiative. Use the comment box to complete your answer.""\nnames(data)[17] <- ""A3_SQ003comment""\n# LimeSurvey Field type: F\ndata[, 18] <- as.numeric(data[, 18])\nattributes(data)$variable.labels[18] <- ""[None of these were applied] 1.3 Please describe the production practice before the diversification initiative. Use the comment box to complete your answer.""\ndata[, 18] <- factor(data[, 18], levels=c(1,0),labels=c(""Yes"", ""Not selected""))\nnames(data)[18] <- ""A3_SQ004""\n# LimeSurvey Field type: A\ndata[, 19] <- as.character(data[, 19])\nattributes(data)$variable.labels[19] <- ""[Comment]']",1,"e with the information. The keywords for this text are: EU Horizon 2020, DiverIMPACTS, crop diversification, rotation, multicropping, intercropping, farmers, value chain, innovations, participatory case studies,"
Machine Learning in R: Random Forest for Cybersport and iGaming,"Machine Learning in R: Random Forest for Cybersport and iGaming1.Why we choose RandomForestSimulation of socio-economic processes is a rather labour-consuming task associated with aggregation of huge amount of heterogeneous data. But theres no more unpredictable system than a man. Over the past 5 years, predicting any processes of human creativity has reached an unprecedented level.All sorts of betting companies offer a wide range of betting options. Moreover, nowadays people want to get a double dose of adrenaline, so they not only make bets, but also choose gaming for it. Today the most popular are football and eSports. Since all the data thats a consequence of human decisions arent homogeneous, the question about the choice of non-parametric methods, for example, decision trees arises from mathematical point of view.Decision trees are a good family of basic classifiers, since theyre quite complex and can reach zero error on any sample. Special attention should be paid to the child method - random forest. Thiss a situation when the method of random subspaces is added in combination with the basic principles of decision tree building.This process reduces the correlation between the resulting decision trees, besides, it allows to avoid overfitting, due to the fact that the base trees are trained on different subspaces of the features determined randomly. The technical aspects of RandomForest method are widely represented at free access, therefore, I will not focus them in this article. Let's talk more about practice!Currently, this algorithm is practically a basic one for machine learning, therefore its easy to find its implementation in a matter of seconds for any platform and programming language for a data scientist.If youre working with Python, then you should be familiar with the scikit-learn module. It includes RandomForest:>>> from sklearn.ensemble import RandomForestClassifier>>> from sklearn.datasets import make_classificationThe scikit-learn documentation is distinguished by its comprehensive and accessible presentation, as well as several practical examples are always demonstrated with different settings of parameters. If you need implementing RandomForest in Python, then I advise you referring to the official documentation. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.htmlIf youre working with R, then theres a specific randomForest library and documentationhttps://cran.r-project.org/web/packages/randomForest/randomForest.pdf .A distinctive feature of all official documentation for R is the detailed elaboration of modelling parameters, but on the other side - absence of practical examples, in general form only. If youre not accustomed to this type of information, then this may discourage you at the initial stage, but for advanced users its convenient.It should be noted that some difficulties in both cases may be the obtaining of decision rules as a result of modelling, therefore the interpretation of the model. Thus, developers often stop at the stage of obtaining a model with a high degree of fidelity, stability, but they give interpretation based on 1-5 particular cases of decision trees. As a data scientist, it hurts me watching this picture from year to year. Since the high-end model turns out to be a black box with a ghostly effect of understanding the processes occurring within the model.2.Application of machine learning in cybergame2.1 Basic modellingThe principles of randomForest implementation for socio-economic processes are obvious. I set the goal to show an example where this particular interpretation will have a key indicator and it will be interesting to observe the models development.In general, the sphere of Casino Now project is related to online casinos, however were at the stage of expanding our interests recently and such an environment as eSports has become a new area of interest. At the moment, its one of the most developing spheres, which goes not only to professional sports, but also as a type of employment by law.One of the most popular eSports disciplines - Dota 2 was a platform for data collection. If theres any doubt about this particular discipline, the answer is extremely simple - its absolute popularity. As proof, theres a development of the prize pool of The International (one of the largest competitions).For those who are interested in the predictive nature of the matches, Ive decided to make a little research, this dataset was usedhttps://archive.ics.uci.edu/ml/datasets/Dota2+Games+Results#from free database https://archive.ics.uci.edu/ml/datasets.html?sort=nameUp&view=list .The study was conducted using the R language and the randomForest library accordingly. The original sample has the following format:Each row of the dataset is a single game with the following features (in the order in the vector):Team won the game (1 or -1)Cluster ID (related to location)Game mode (eg All Pick)Game type (eg. Ranked)- end: Each element is an indicator for a hero. Value of 1 indicates that a player from team '1' played as that hero and '-1' for the other team. A hero can be selected by only one player each game. This means that each row has five '1' and five '-1' values.The set goal is to predict the outcome of the game based on the location of teams, modification and games type, as well as considering the pick of teams. For this purpose, a small script was written without complicated parameters manipulation, since the main goal for me was to demonstrate the possibilities of interpretation, and not to build a statistically significant model. For those whore interested in perfecting the parameters and getting a high quality model, Ive attached the script. Data can be downloaded at the link above.library(readr)library(randomForest)dota2Train <- read_csv(""dota2Train.csv"")colnames(dota2Train) <- append(c('TeamWon','Cluster','GameMode','GameType'),c(1:113))cnames <- append(c('TeamWon','Cluster','GameMode','GameType'),c(1:113))dota2Train[cnames] <- lapply(dota2Train[cnames], factor)# to devide the data to ""train"" and ""test""ind <- sample(2, nrow(df), replace=TRUE, prob=c(0.8,0.2))train <- dota2Train [ind==1,]test <- dota2Train [ind==2,]xxtrain <- trainyytrain <- train$TeamWonxxtrain <- subset(xxtrain, select = -TeamWon)xxtest <- testyytest <- test$TeamWonxxtest <- subset(xxtest, select = -TeamWon)testmodel <- randomForest(xxtrain,yytrain, ntree = 300, importance=T, na.action = na.roughfix)sum_mod <- summary(testmodel)2.2 InterpretationAfter you get a trained model, lets interpret the results based on decision rules of decision trees. To do this, use another package:inTrees https://cran.r-project.org/web/packages/inTrees/inTrees.pdfFirst, we want to get all the branch rules of all built random trees from subspaces.treeList <- RF2List(testmodel)exec <- extractRules(treeList, xxtrain) # R-executable conditionsexec[1:2,]Measure rules. len is the number of variable-value pairs in a condition, freq is the percentage of data satisfying a condition, pred is the outcome of a rule, i.e., condition => pred, err is the error rate of a rule.ruleMetric <- getRuleMetric(exec,xxtrain,yytrain) # get rule metricsruleMetric[1:2,]Prune each rule:ruleMetric <- pruneRule(ruleMetric, xxtrain, yytrain)ruleMetric[1:2,]Make rules more readable:readableRules <- presentRules(ruleMetric, colnames(xxtrain))readableRules[1:2, ]readableRules <- readableRules[order(readableRules[,5]),]readableRules <- as.data.frame(readableRules) Lets choose only success rules from the point of view of model prediction to develop recommendations.newTable <- subset(readableRules, pred==1)newTable <- newTable[order(newTable[,2],decreasing = TRUE ),]Then compile a results table for clarity.newTable <- newTable[,-5]table <- as.data.frame(matrix(data = NA, nrow = length(newTable$len), ncol = 2))table[,1] <- newTable$freqtable[,2] <- newTable$conditiontable <- table[order(table[,1],decreasing = TRUE ),]colnames(table) <- c(""len"", ""condition"")table$len <- as.numeric(table$len)table$len <- abs(table$len - max(table$len))In order to determine which factors are most significant, you can build a graph of factors significance depending on the Gini indexvarImpPlot(testmodel)Formation of significance dataframe is carried out in 3 lines:imp <- as.data.frame(importance(testmodel))imp <- imp[,-1]imp <- imp[,-1]imp <- imp[order(imp[,2],decreasing = TRUE ),]ConclusionThus we obtained a complete interpretation of the model which regressors are exclusively factorial characteristics: significance and a set of conditions. Parsing the rules, we can identify the key ones, since we can judge its authenticity and get statistically significant combinations of heroes for winning purpose based on the frequency of this rules implementation.","['\r\nlibrary(readr)\r\nlibrary(randomForest)\r\n\r\ndota2Train <- read_csv(""dota2Train.csv"")\r\n\r\n\r\ncolnames(dota2Train) <- append(c(\'TeamWon\',\'Cluster\',\'GameMode\',\'GameType\'),c(1:113))\r\ncnames <- append(c(\'TeamWon\',\'Cluster\',\'GameMode\',\'GameType\'),c(1:113))\r\n\r\ndota2Train[cnames] <- lapply(dota2Train[cnames], factor)\r\n\r\nmaxtrue <- 0 \r\nmintrue <- 1\r\n\r\ndf <- dota2Train\r\n\r\n\r\n# to devide the data to ""train"" and ""test""\r\nind <- sample(2, nrow(df), replace=TRUE, prob=c(1,0))\r\ntrain <- df[ind==1,]\r\ntest <- df[ind==2,]\r\n  \r\nxxtrain <- train\r\nyytrain <- train$TeamWon\r\nxxtrain <- subset(xxtrain, select = -TeamWon)\r\nxxtest <- test\r\nyytest <- test$TeamWon\r\nxxtest <- subset(xxtest, select = -TeamWon)\r\n\r\nModelka <- randomForest(xxtrain,yytrain, ntree = 30, importance=T, na.action = na.roughfix)\r\nsum_mod <- summary(Modelka)\r\n\r\nModelka\r\n\r\nlibrary(inTrees)\r\n\r\ntreeList <- RF2List(Modelka)  # transform rf object to an inTrees\' format\r\nexec <- extractRules(treeList, xxtrain)  # R-executable conditions\r\nexec[1:2,]\r\nruleMetric <- getRuleMetric(exec,xxtrain,yytrain)  # get rule metrics\r\nruleMetric[1:2,]\r\nruleMetric <- pruneRule(ruleMetric, xxtrain, yytrain)\r\nruleMetric[1:2,]\r\nreadableRules <- presentRules(exec, colnames(xxtrain))\r\nreadableRules[1:2, ]\r\nreadableRules <- readableRules[order(readableRules[,5]),]\r\nreadableRules <- as.data.frame(readableRules)\r\n\r\nnewTable <- subset(readableRules, pred==1)\r\nnewTable <- readableRules\r\nnewTable <- newTable[order(newTable[,2],decreasing = TRUE ),]\r\n\r\n\r\n#cool tabel all of the rules\r\nnewTable <- newTable[,-5]\r\n\r\ntable <- as.data.frame(matrix(data = NA, nrow = length(newTable$len), ncol = 2))\r\ntable[,1] <- newTable$freq\r\ntable[,2] <- newTable$condition\r\n\r\ntable <- table[order(table[,1],decreasing = TRUE ),]\r\ncolnames(table) <- c(""len"", ""condition"")\r\n\r\ntable$len <- as.numeric(table$len)\r\ntable$len <- abs(table$len - max(table$len))\r\n\r\n\r\n# write.table(readableRules, \r\n#             file =""Правила.csv"",\r\n#             row.names=TRUE,col.names = TRUE, \r\n#             sep="";"", dec=""."")\r\n\r\nplot(Modelka)\r\nimportance(Modelka)\r\nvarImpPlot(Modelka)\r\nimp <- as.data.frame(importance(meanmodelka))\r\nimp <- imp[,-1]\r\nimp <- imp[,-1]\r\nimp <- imp[order(imp[,2],decreasing = TRUE ),]\r\n# write.table(importance(meanmodelka), \r\n#             file =""Важность.csv"",\r\n#             row.names=TRUE,col.names = TRUE, \r\n#             sep="";"", dec=""."")\r\n\r\n  \r\n']",1,"forest algorithm, decision trees, machine learning, R programming language, cybersport, iGaming, predicting human creativity, betting companies, non-parametric methods, scikit-learn module, randomForest library, interpretation of model, socio-economic processes"
Population collapse in viviparid gastropods of the Lake Victoria ecoregion started before the Last Glacial Maximum,"For the purpose of reproducibility, we here provide the datasets and R script supporting the analyses of the paper ""Population collapse in viviparid gastropods of the Lake Victoria ecoregion started before the Last Glacial Maximum"" by Van Bocxlaer et al. This paper has been accepted for publication in Molecular Ecology on 31 July 2020. In this study, we examine the population structure of the clade of Bellamya gastropods that occupies the Lake Victoria ecoregion with the aim to relate past environmental change with demography and diversification dynamics. The here provided datasets include 1) an alignment of a fragment of the gene cytochrome c oxidase subunit 1 for 60 specimens; 2) genotype data for 321 individuals from 39 localities for 15 microsatellite loci (total dataset); 3) a regrouped genotype dataset (282 specimens from 21 localities for 15 microsatellite loci), which was used for some analyses in our study. Analyses were performed with various programs as reported in our paper. Here we provide input and result files (33 files in total) for these analyses, complemented with an R script that readily allows reproducing the majority of our inquiries.","['#------------------------------------------------------------------------------------------------#\r\n#                                                                                                #\r\n#              Historic demographic decline preceded the ongoing population collapse             #\r\n#                         in gastropods of the Lake Victoria ecoregion                           #\r\n#                                             by                                                 #\r\n#       Bert Van Bocxlaer, Catharina Clewing, Anne Duptié, Camille Roux, Christian Albrecht      #\r\n#                                                                                                #\r\n#             Code by: Bert Van Bocxlaer, email: bert.van-bocxlaer|at|univ-lille.fr              #\r\n#                                                                                                #\r\n#------------------------------------------------------------------------------------------------#\r\n\r\n#------------------------------------------------------------------------------------------------\r\n# Reproducibility data\r\n#------------------------------------------------------------------------------------------------\r\n\r\n# Session info ------------------------------------------------------------------------------------------------------------\r\n\r\n#version  R version 3.6.1 (2019-07-05)\r\n#os       Windows 7 x64 SP 1          \r\n#system   x86_64, mingw32             \r\n#ui       RStudio                     \r\n#language (EN)                        \r\n             \r\n# Package info ----------------------------------------------------------------------------------------------------------------\r\n#  package      * version  date       lib source        \r\n#  ade4         * 1.7-13   2018-08-31 [1] CRAN (R 3.6.1)\r\n#adegenet     * 2.1.1    2018-02-02 [1] CRAN (R 3.6.1)\r\n#ape          * 5.3      2019-03-17 [1] CRAN (R 3.6.1)\r\n#assertthat     0.2.1    2019-03-21 [1] CRAN (R 3.6.1)\r\n#backports      1.1.5    2019-10-02 [1] CRAN (R 3.6.1)\r\n#bitops         1.0-6    2013-08-17 [1] CRAN (R 3.6.0)\r\n#boot         * 1.3-22   2019-04-02 [1] CRAN (R 3.6.1)\r\n#callr          3.3.2    2019-09-22 [1] CRAN (R 3.6.1)\r\n#caTools        1.17.1.2 2019-03-06 [1] CRAN (R 3.6.1)\r\n#class          7.3-15   2019-01-01 [1] CRAN (R 3.6.1)\r\n#classInt       0.4-2    2019-10-17 [1] CRAN (R 3.6.1)\r\n#cli            1.1.0    2019-03-19 [1] CRAN (R 3.6.1)\r\n#cluster        2.1.0    2019-06-19 [1] CRAN (R 3.6.1)\r\n#coda           0.19-3   2019-07-05 [1] CRAN (R 3.6.1)\r\n#colorspace     1.4-1    2019-03-18 [1] CRAN (R 3.6.1)\r\n#crayon         1.3.4    2017-09-16 [1] CRAN (R 3.6.1)\r\n#DBI            1.0.0    2018-05-02 [1] CRAN (R 3.6.1)\r\n#deldir         0.1-23   2019-07-31 [1] CRAN (R 3.6.1)\r\n#desc           1.2.0    2018-05-01 [1] CRAN (R 3.6.1)\r\n#devtools     * 2.2.1    2019-09-24 [1] CRAN (R 3.6.1)\r\n#digest         0.6.22   2019-10-21 [1] CRAN (R 3.6.1)\r\n#dplyr          0.8.3    2019-07-04 [1] CRAN (R 3.6.1)\r\n#e1071          1.7-2    2019-06-05 [1] CRAN (R 3.6.1)\r\n#ellipsis       0.3.0    2019-09-20 [1] CRAN (R 3.6.1)\r\n#eulerr       * 6.0.0    2019-09-27 [1] CRAN (R 3.6.1)\r\n#expm           0.999-4  2019-03-21 [1] CRAN (R 3.6.1)\r\n#fastmap        1.0.1    2019-10-08 [1] CRAN (R 3.6.1)\r\n#fs             1.3.1    2019-05-06 [1] CRAN (R 3.6.1)\r\n#gdata          2.18.0   2017-06-06 [1] CRAN (R 3.6.0)\r\n#genepop      * 1.1.3    2019-08-22 [1] CRAN (R 3.6.1)\r\n#ggplot2        3.2.1    2019-08-10 [1] CRAN (R 3.6.1)\r\n#glue           1.3.1    2019-03-12 [1] CRAN (R 3.6.1)\r\n#gmodels        2.18.1   2018-06-25 [1] CRAN (R 3.6.1)\r\n#gplots       * 3.0.1.1  2019-01-27 [1] CRAN (R 3.6.1)\r\n#gtable         0.3.0    2019-03-25 [1] CRAN (R 3.6.1)\r\n#gtools         3.8.1    2018-06-26 [1] CRAN (R 3.6.0)\r\n#hierfstat    * 0.04-22  2015-12-04 [1] CRAN (R 3.6.1)\r\n#htmltools      0.4.0    2019-10-04 [1] CRAN (R 3.6.1)\r\n#httpuv         1.5.2    2019-09-11 [1] CRAN (R 3.6.1)\r\n#igraph         1.2.4.1  2019-04-22 [1] CRAN (R 3.6.1)\r\n#KernSmooth     2.23-15  2015-06-29 [1] CRAN (R 3.6.1)\r\n#later          1.0.0    2019-10-04 [1] CRAN (R 3.6.1)\r\n#lattice        0.20-38  2018-11-04 [1] CRAN (R 3.6.1)\r\n#lazyeval       0.2.2    2019-03-15 [1] CRAN (R 3.6.1)\r\n#LearnBayes     2.15.1   2018-03-18 [1] CRAN (R 3.6.0)\r\n#magrittr       1.5      2014-11-22 [1] CRAN (R 3.6.1)\r\n#mapdata      * 2.3.0    2018-03-30 [1] CRAN (R 3.6.1)\r\n#mapproj      * 1.2.6    2018-03-29 [1] CRAN (R 3.6.1)\r\n#maps         * 3.3.0    2018-04-03 [1] CRAN (R 3.6.1)\r\n#MASS         * 7.3-51.4 2019-03-31 [1] CRAN (R 3.6.1)\r\n#Matrix         1.2-17   2019-03-22 [1] CRAN (R 3.6.1)\r\n#mclust       * 5.4.5    2019-07-08 [1] CRAN (R 3.6.1)\r\n#memoise        1.1.0    2017-04-21 [1] CRAN (R 3.6.1)\r\n#mgcv           1.8-28   2019-03-21 [1] CRAN (R 3.6.1)\r\n#mime           0.7      2019-06-11 [1] CRAN (R 3.6.0)\r\n#munsell        0.5.0    2018-06-12 [1] CRAN (R 3.6.1)\r\n#nlme           3.1-140  2019-05-12 [1] CRAN (R 3.6.1)\r\n#pegas        * 0.12     2019-10-05 [1] CRAN (R 3.6']",1,"Population collapse, viviparid gastropods, Lake Victoria, ecoregion, Last Glacial Maximum, datasets, R script, reproducibility, Bellamya gastropods, population structure, environmental change, demography, divers"
The LUCKINet land use ontology,"1. RationaleThe LUCKINet land use ontology expands AGROVOC (https://agrovoc.fao.org/browse/agrovoc/en/), which was developed by the Food and Agriculture Organization of the United Nations (FAO). Like AGROVOC, the LUCKINet land use ontology uses semantic web technology powered by the Simple Knowledge Organization System (SKOS).The ontology has been primarily developed to support the LUCKINet effort to standardize approaches that develop gridded information on land use dynamics. The ontology covers concepts of landcover, land use and agricultural commodity labels and contains:harmonized concept of the LUCKINet and GeoKur projects and the semantic matches between them, andmappings to other external ontologies, vocabularies or lists of concepts from databases or publications.We built the ontology with a strong focus on mapping LUCKINet concepts to FAO/AGROVOC concepts to foster interoperability with the wealth of other projects building on AGROVOC. Moreover, concepts from different state-of-the-art datasets are considered, for example, on land use from statistical agencies worldwide and the (grey) literature or landcover data such as CORINE Land Cover because its nomenclature is very well defined and documented. Furthermore, the hierarchical structure of CORINE allows mapping various other land cover and land use datasets into a common ontology. This is a crucial milestone necessary to consistently allocate land use statistics across time and space into widely available landcover data products, the primary goal of LUCKINet.Over time, more concepts from external ontologies, vocabularies or lists will be added to this ontology within the LUCKINet project. Moreover, community development is possible by replicating the procedures used here, which may ultimately lead to improved interoperability between the ontological basis of many projects. 2. MethodWe have built the ontology with the R-package ontologics v0.5.2, which enables reproducibility and transparency in creating an ontology. A script documenting this process is part of this publication.2.1 SourcesThe ontology is an amalgamation of various datasets:The FAO Indicative Crop Classification (ICC) v1.1 provides a hierarchical structure of crop and livestock concepts.The FAO Crops and livestock products (QCL) dataset contains a range of concepts used by the FAO and frequently by statistical offices of nations that make their census data publicly available.The FAO Land Use (RL) dataset contains a hierarchical set of land-use concepts that represent the most harmonized representation of land-use concepts to date.The FAO Forest Resource Assessment (FRA) contains a detailed set of forest classes that fit well with the FAO Land Use vocabulary and represent relevant classes at a coarse scale (Sections 1b, 1e, 1f).CORINE Land Cover (CLC) classes are used as the basis to define the (hierarchical) organization of land cover and land use classes (into which the crop and livestock commodities are nested).2.2 ClassesThe ontology is hierarchically structured into the following classes:domain: the overarching dimensions into which all other concepts are nested. These represent conceptually different kinds of classification systems. At the current stage of this ontology, these are surface types to describe the configuration of the earth's surface (essentially replicating the CORINE ontology) and production systems to describe the hierarchical organization of commodities (mainly replicating the FAO classification).landcover group: groups of landcover types (as the top-most surface types class) describing the respective areas, such as AGRICULTURAL AREAS, FOREST AND SEMI-NATURAL AREAS, WETLANDS and others.Landcover: concepts that describe a unique cover type, taken mainly from the CORINE classification, with minor adaptions.land use: the socio-economic dimension, i.e., how the land under a particular cover is used. At the current state of this ontology, only the landcover groups AGRICULTURAL AREAS and FOREST AND SEMI-NATURAL AREAS are detailed because they are the focus of the first iteration of the LUCKINet land use time series.group: groups of crop or livestock commodities as the top-most class of production systemsclass: classes of similar commodities.aggregate: some commodities are very similar and thus reported together by some national statistical agencies or the FAO (this class is at the same level as commodities).commodity: individual commodities (typically at the level of species or variety).2.3 ConceptsThe bulk content of the ontology consists of a collection of concepts, where each concept is an instance of a specific class. The concepts also bear hierarchical relations determined by their class affiliation. For example, the concept of Permanent Grazing (class: land use) is semantically narrower than Permanent Cropland (class: landcover), which in turn is narrower than the concept of Agricultural Areas (class: lancover group), which, finally, is narrower than surface types (class: domain).2.4 MappingsAs far as possible, concepts are mapped to all corresponding wikidata concepts. For example, the concept of maize is mapped to wikidata concepts Q11575 and Q25618328. Moreover, concepts are mapped to thelife-form concepts graminoid, forb, herb, shrub, treeuse-type concepts bioenergy, fibre, food, wood, forage, silage, fodder, industrial, recreation, medicinal and labourthe persistence concepts temporary and permanent 3. Acknowledgements- This work was supported by funding to Carsten Meyer through the Flexpool mechanism of the German Centre for Integrative Biodiversity Research (iDiv) (FZT-118, DFG).- Several developments in this software stem from work on the GeoKur research data infrastructure. GeoKur is granted by the BMBF (Federal Ministry of Education and Research) under number 16QK04A. Files:build_luckinet_landuse_ontology.R: The script used to build this ontologyluckiOnto.rds: the ontology to access and manipulate with R and the ontologics R-packageluckiOnto.ttl: the semantic web compatible turtle file","['# load packages ----\r\n#\r\nlibrary(readr)\r\nlibrary(tibble)\r\nlibrary(dplyr)\r\nlibrary(ontologics)\r\nlibrary(here)\r\n\r\n\r\n# script arguments ----\r\n#\r\nmessage(""\\n---- build landuse ontology ----"")\r\n\r\n\r\n# data processing ----\r\n#\r\n# start a new ontology\r\nmessage("" --> initiate ontology"")\r\nluckiOnto <- start_ontology(name = ""luckiOnto"", path = here(),\r\n                            version = ""1.0.0"",\r\n                            code = "".xxx"",\r\n                            description = ""the intial LUCKINet commodity ontology"",\r\n                            homepage = ""http://www.luckinet.org"",\r\n                            uri_prefix = ""http://luckinet.org"",\r\n                            license = ""CC-BY-4.0"")\r\n\r\n# define new classes\r\nluckiOnto <- new_class(new = ""domain"", target = NA,\r\n                       description = ""the type of description system"", ontology = luckiOnto) %>%\r\n  new_class(new = ""landcover group"", target = ""domain"",\r\n            description = ""groups of landcover concepts"", ontology = .) %>%\r\n  new_class(new = ""landcover"", target = ""landcover group"",\r\n            description = ""concepts that describe the general type of the land surface cover"", ontology = .) %>%\r\n  new_class(new = ""land use"", target = ""landcover"",\r\n            description = ""concepts that describe the socio-economic dimension (how land is used) of the land surface cover"", ontology = .) %>%\r\n  new_class(new = ""group"", target = ""domain"",\r\n            description = ""broad groups of concepts that describe crop and livestock commodities"", ontology = .) %>%\r\n  new_class(new = ""class"", target = ""group"",\r\n            description = ""classes or types of concepts that describe crop and livestock commodities"", ontology = .) %>%\r\n  new_class(new = ""aggregate"", target = ""class"",\r\n            description = ""aggregates of (commodities that often occurr together) concepts that describe crop and livestock commodities"", ontology = .) %>%\r\n  new_class(new = ""commodity"", target = ""class"",\r\n            description = ""concepts that describe crop or livestock commodities"",  ontology = .)\r\n\r\n# define the harmonized concepts\r\ndomain <- tibble(new = c(""surface types"", ""production systems""),\r\n                 description = c(""land use and landcover concepts describing the surface of the earth"",\r\n                                 ""production systems described by the crop or livestock commodities grown there""))\r\n\r\nluckiOnto <- new_concept(new = domain$new,\r\n                         description = domain$description,\r\n                         class = ""domain"",\r\n                         ontology =  luckiOnto)\r\n\r\nmessage(""     landcover group"")\r\nlcGroup <- tibble(concept = c(\r\n  ""ARTIFICIAL SURFACES"",\r\n  ""AGRICULTURAL AREAS"",\r\n  ""FOREST AND SEMI-NATURAL AREAS"",\r\n  ""WETLANDS"",\r\n  ""WATER BODIES""),\r\n  broader = ""surface types"")\r\n\r\nluckiOnto <- new_concept(new = lcGroup$concept,\r\n                         broader = get_concept(x = lcGroup %>% select(label = broader), ontology = luckiOnto),\r\n                         class = ""landcover group"",\r\n                         ontology =  luckiOnto)\r\n\r\nmessage(""     landcover"")\r\nlc <- tibble(concept = c(\r\n  ""Urban fabric"",\r\n  ""Industrial, commercial and transport units"",\r\n  ""Mine, dump and construction sites"",\r\n  ""Artificial, non-agricultural vegetated areas"",\r\n  ""Temporary cropland"",\r\n  ""Permanent cropland"",\r\n  ""Heterogeneous agricultural areas"",\r\n  ""Forests"",\r\n  ""Other Wooded Areas"",\r\n  ""Shrubland"",\r\n  ""Herbaceous associations"",\r\n  ""Heterogeneous semi-natural areas"",\r\n  ""Open spaces with little or no vegetation"",\r\n  ""Inland wetlands"",\r\n  ""Marine wetlands"",\r\n  ""Inland waters"",\r\n  ""Marine waters""),\r\n  broader = c(rep(lcGroup$concept[1], 4), rep(lcGroup$concept[2], 3),\r\n             rep(lcGroup$concept[3], 6), rep(lcGroup$concept[4], 2),\r\n             rep(lcGroup$concept[5], 2)))\r\n\r\nluckiOnto <- new_concept(new = lc$concept,\r\n                         broader = get_concept(x = lc %>% select(label = broader), ontology = luckiOnto),\r\n                         class = ""landcover"",\r\n                         ontology =  luckiOnto)\r\n\r\nmessage(""     land use"")\r\nlu <- tibble(concept = c(\r\n  ""Fallow"",\r\n  ""Herbaceous crops"",\r\n  ""Temporary grazing"",\r\n  ""Permanent grazing"",\r\n  ""Shrub orchards"",\r\n  ""Palm plantations"",\r\n  ""Tree orchards"",\r\n  ""Woody plantation"",\r\n  ""Protective cover"",\r\n  ""Agroforestry"",\r\n  ""Mosaic of agricultural-uses"",\r\n  ""Mosaic of agriculture and natural vegetation"",\r\n  ""Undisturbed Forest"",\r\n  ""Naturally Regenerating Forest"",\r\n  ""Planted Forest"",\r\n  ""Temporally Unstocked Forest""),\r\n  broader = c(rep(lc$concept[5], 3), rep(lc$concept[6], 6),\r\n             rep(lc$concept[7], 3), rep(lc$concept[8], 4)))\r\n\r\nluckiOnto <- new_concept(new = lu$concept,\r\n                         broader = get_concept(x = lu %>% select(label = broader), ontology = luckiOnto),\r\n                         class = ""land use"",\r\n                         ontology =  luckiOnto)\r\n\r\nmessage(""     crop and livestock concepts"")\r\ngroup <- tibble(concept = c(\r\n  ""BIOENERG']",1,"cations).Landcover: the physical land cover classes such as forest, grassland, water bodies, etc.Land use: the ways in which land is used, such as agriculture, urbanization, conservation, etc.Agricultural commodity: the"
"Rapid Creation of a Data Product for the World's Specimens of Horseshoe Bats and Relatives, a Known Reservoir for Coronaviruses","This repository is associated with NSF DBI 2033973, RAPID Grant: Rapid Creation of a Data Product for the World's Specimens of Horseshoe Bats and Relatives, a Known Reservoir for Coronaviruses (https://www.nsf.gov/awardsearch/showAward?AWD_ID=2033973). Specifically, this repository contains (1) raw data from iDigBio (http://portal.idigbio.org) and GBIF (https://www.gbif.org), (2) R code for reproducible data wrangling and improvement, (3) protocols associated with data enhancements, and (4) enhanced versions of the dataset published at various project milestones. Additional code associated with this grant can be found in the BIOSPEX repository (https://github.com/iDigBio/Biospex). Long-term data management of the enhanced specimen data created by this project is expected to be accomplished by the natural history collections curating the physical specimens, a list of which can be found in this Zenodo resource.Grant abstract: ""The award to Florida State University will support research contributing to the development of georeferenced, vetted, and versioned data products of the world's specimens of horseshoe bats and their relatives for use by researchers studying the origins and spread of SARS-like coronaviruses, including the causative agent of COVID-19. Horseshoe bats and other closely related species are reported to be reservoirs of several SARS-like coronaviruses. Species of these bats are primarily distributed in regions where these viruses have been introduced to populations of humans. Currently, data associated with specimens of these bats are housed in natural history collections that are widely distributed both nationally and globally. Additionally, information tying these specimens to localities are mostly vague, or in many instances missing. This decreases the utility of the specimens for understanding the source, emergence, and distribution of SARS-COV-2 and similar viruses. This project will provide quality georeferenced data products through the consolidation of ancillary information linked to each bat specimen, using the extended specimen model. The resulting product will serve as a model of how data in biodiversity collections might be used to address emerging diseases of zoonotic origin. Results from the project will be disseminated widely in opensource journals, at scientific meetings, and via websites associated with the participating organizations and institutions. Support of this project provides a quality resource optimized to inform research relevant to improving our understanding of the biology and spread of SARS-CoV-2. The overall objectives are to deliver versioned data products, in formats used by the wider research and biodiversity collections communities, through an open-access repository; project protocols and code via GitHub and described in a peer-reviewed paper, and; sustained engagement with biodiversity collections throughout the project for reintegration of improved data into their local specimen data management systems improving long-term curation.This RAPID award will produce and deliver a georeferenced, vetted and consolidated data product for horseshoe bats and related species to facilitate understanding of the sources, distribution, and spread of SARS-CoV-2 and related viruses, a timely response to the ongoing global pandemic caused by SARS-CoV-2 and an important contribution to the global effort to consolidate and provide quality data that are relevant to understanding emergent and other properties the current pandemic. This RAPID award is made by the Division of Biological Infrastructure (DBI) using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.""Files included in this resource9d4b9069-48c4-4212-90d8-4dd6f4b7f2a5.zip: Raw data from iDigBio, DwC-A format0067804-200613084148143.zip: Raw data from GBIF, DwC-A format0067806-200613084148143.zip: Raw data from GBIF, DwC-A format1623690110.zip: Full export of this project's data (enhanced and raw) from BIOSPEX, CSV formatbionomia-datasets-attributions.zip: Directory containing 103 Frictionless Data packages for datasets that have attributions made containing Rhinolophids or Hipposiderids, each package also containing a CSV file for mismatches in person date of birth/death and specimen eventDate. File bionomia-datasets-attributions-key_2021-02-25.csv included in this directory provides a key between dataset identifier (how the Frictionless Data package files are named) and dataset name.bionomia-problem-dates-all-datasets_2021-02-25.csv: List of 21 Hipposiderid or Rhinolophid records whose eventDate or dateIdentified mismatches a wikidata recipients date of birth or death across all datasets.flagEventDate.txt: file containing term definition to reference in DwC-AflagExclude.txt: file containing term definition to reference in DwC-AflagGeoreference.txt: file containing term definition to reference in DwC-AflagTaxonomy.txt: file containing term definition to reference in DwC-AgeoreferencedByID.txt: file containing term definition to reference in DwC-AidentifiedByNames.txt: file containing term definition to reference in DwC-Ainstructions-to-get-people-data-from-bionomia-via-datasetKey: instructions given to data providersRAPID-code_collection-date.R: code associated with enhancing collection datesRAPID-code_compile-deduplicate.R: code associated with compiling and deduplicating raw dataRAPID-code_external-linkages-bold.R: code associated with enhancing external linkagesRAPID-code_external-linkages-genbank.R: code associated with enhancing external linkagesRAPID-code_external-linkages-standardize.R: code associated with enhancing external linkagesRAPID-code_people.R: code associated with enhancing data about peopleRAPID-code_standardize-country.R: code associated with standardizing country dataRAPID-data-dictionary.pdf: metadata about terms included in this projects data, in PDF formatRAPID-data-dictionary.xlsx: metadata about terms included in this projects data, in spreadsheet formatrapid-data-providers_2021-05-03.csv: list of data providers and number of records provided to rapid-joined-records_country-cleanup_2020-09-23.csvrapid-final-data-product_2021-06-29.zip: Enhanced data from BIOSPEX, DwC-A formatrapid-final-gazetteer.zip: Gazetteer providing georeference data and metadata for 10,341 localities assessed as part of this projectrapid-joined-records_country-cleanup_2020-09-23.csv: data product initial version where raw data has been compiled and deduplicated, and country data has been standardizedRAPID-protocol_collection-date.pdf: protocol associated with enhancing collection datesRAPID-protocol_compile-deduplicate.pdf: protocol associated with compiling and deduplicating raw dataRAPID-protocol_external-linkages.pdf: protocol associated with enhancing external linkagesRAPID-protocol_georeference.pdf: protocol associated with georeferencingRAPID-protocol_people.pdf: protocol associated with enhancing data about peopleRAPID-protocol_standardize-country.pdf: protocol associated with standardizing country dataRAPID-protocol_taxonomic-names.pdf: protocol associated with enhancing taxonomic name dataRAPIDAgentStrings1_archivedCopy_30March2021.ods: resource used in conjunction with RAPID people protocolrecordedByNames.txt: file containing term definition to reference in DwC-ARhinolophid-HipposideridAgentStrings_and_People2_archivedCopy_30March2021.ods: resource used in conjunction with RAPID people protocolwikidata-notes-for-bat-collectors_leachman_2020: please see https://zenodo.org/record/4724139 for this resource","['# RAPID code for enhancing collection dates\n# This code accompanies document \'RAPID-protocol_collection-dates.pdf\'\n# Code written by Katelin Pearson, 2021-01-19.\n# Latest update 2021-04-02.\n\n# Load core libraries; install these packages if you have not already\nlibrary(tidyverse)\nlibrary(splitstackshape)\n\n# Read into R.....\nspecs <- read.csv(""C:/Users/Katie/Desktop/agents_gbifIDs_combined_NEW.csv"")\ncols <- read.csv(""C:/Users/Katie/Desktop/Horseshoe_bat_people_combined.csv"")\n\n# PART 1\n# Join agent birth and death years, when available from Bionomia, to specimen\n# data\n\n#Convert columns to character type, because R is finicky\nspecs$recordedBy_gbifR <- as.character(specs$recordedBy_gbifR)\nspecs$identifiedBy_gbifR <- as.character(specs$identifiedBy_gbifR)\ncols$birth_year <- as.character(cols$birth_year)\ncols$death_year <- as.character(cols$death_year)\nspecs$recordedByBirthYear <- NA\nspecs$recordedByDeathYear <- NA\nspecs$identifiedByBirthYear <- NA\nspecs$identifiedByDeathYear <- NA\n\n#separate cases of multiple collectors into 4 new columns\nspecs$recordedByID_split <- specs$recordedByID\nspecs <- cSplit(specs,""recordedByID_split"",sep=""|"",type.convert = FALSE)\n\n#More character conversions\nspecs$recordedByID_split_1 <- as.character(specs$recordedByID_split_1)\nspecs$recordedByID_split_2 <- as.character(specs$recordedByID_split_2)\nspecs$recordedByID_split_3 <- as.character(specs$recordedByID_split_3)\nspecs$recordedByID_split_4 <- as.character(specs$recordedByID_split_4)\n\n#Assign birth and death years of collectors of each specimen\n#In cases of multiple collectors, the birth dates and death dates are concatenated\nfor(i in 1:dim(specs)[1]){\n  print(i)\n  if(is.na(specs$recordedByID[i])){\n    next\n  } else {\n    m <- match(specs$recordedByID_split_1[i],cols$bionomia_url)\n    if(is.na(m)){\n      next\n    }else{\n      p <- match(specs$recordedByID_split_2[i],cols$bionomia_url)\n      if(is.na(p)){\n        specs$recordedByBirthYear[i] <- as.character(cols$birth_year[m])\n        specs$recordedByDeathYear[i] <- as.character(cols$death_year[m])\n      } else {\n        q <- match(specs$recordedByID_split_3[i],cols$bionomia_url)\n        if(is.na(q)){\n          specs$recordedByBirthYear[i] <- as.character(paste(cols$birth_year[m],cols$birth_year[p],sep=""|""))\n          specs$recordedByDeathYear[i] <- as.character(paste(cols$death_year[m],cols$death_year[p],sep=""|""))\n        } else {\n          r <- match(specs$recordedByID_split_4[i],cols$bionomia_url)\n          if(is.na(r)){\n            specs$recordedByBirthYear[i] <- as.character(paste(cols$birth_year[m],cols$birth_year[p],cols$birth_year[q], sep=""|""))\n            specs$recordedByDeathYear[i] <- as.character(paste(cols$death_year[m],cols$death_year[p],cols$death_year[q], sep=""|""))\n          } else {\n            specs$recordedByBirthYear[i] <- as.character(paste(cols$birth_year[m],cols$birth_year[p],cols$birth_year[q],cols$birth_year[r], sep=""|""))\n            specs$recordedByDeathYear[i] <- as.character(paste(cols$death_year[m],cols$death_year[p],cols$death_year[q],cols$death_year[r], sep=""|""))\n          }\n        }\n      }\n    }\n  }\n}\n\n#Combine identifier birth and death years with specimen data\n#NOTE: this loop does not handle instances of multiple identifiers\nfor(i in 1:dim(specs)[1]){\n  if(is.na(specs$identifiedByID[i])){\n    next\n  } else {\n    n <- match(specs$identifiedByID[i],cols$bionomia_url)\n    #if no duplicate found, go to the next record\n    if(is.na(n)){\n      next\n    }\n    else {\n      specs$identifiedByBirthYear[i] <- cols$birth_year[n]\n      specs$identifiedByDeathYear[i] <- cols$death_year[n]\n    }\n  }\n}\n\nwrite.csv(specs, ""C:/Users/Katie/Desktop/specs_after_part2.csv"")\n\n####PART 2###############\n#In PART 2, the collection data are augmented with collection date data (this was not present\n#in the initial download, and this part could be skipped if the initial download\n#contains all the collection date information.)\n####Combine specimen date data with specimen recorded data\n#load the old dataset\nall_data <- read.csv(""C:/Users/Katie/Desktop/rapid-joined-records_country-cleanup_2020-09-23.csv"")\n#isolate only time/date data from old dataset\ntime_data <- all_data %>%\n  select(gbifID_gbifR,eventDate_gbifP,eventDate_gbifR,eventDate_idbP,eventDate_idbR,verbatimEventDate_gbifR,verbatimEventDate_gbifP,verbatimEventDate_idbP,verbatimEventDate_idbR,year_gbifP,year_gbifR,year_idbR,month_gbifR,month_gbifP,month_idbR,day_gbifP,day_gbifR,day_idbR,issue_gbifP)\n#remove unnecessary dataset (for memory)\nrm(all_data)\n\ntime_data$eventDate_gbifP <- as.character(time_data$eventDate_gbifP)\ntime_data$eventDate_gbifR <- as.character(time_data$eventDate_gbifR)\ntime_data$eventDate_idbP <- as.character(time_data$eventDate_idbP)\ntime_data$eventDate_idbR <- as.character(time_data$eventDate_idbR)\ntime_data$verbatimEventDate_gbifR <- as.character(time_data$verbatimEventDate_gbifR)\ntime_data$verbatimEventDate_gbifP <- as.character(time_data$verbatimEventDate_gbifP)\ntime_data$verbatimEventDate_idbP <-', '# RAPID code for compiling raw data from multiple sources and deduplicating records\n# This code accompanies document \'RAPID-protocol_compile-deduplicate.pdf\'\n# Code written by Erica Krimmel, 2020-07-31. Updated 2020-09-23.\n\n# Load core libraries; install this package if you have not already\nlibrary(tidyverse)\n\n# Load data GBIF R package for data quality control further down; install\n# this package if you have not already\nlibrary(rgbif)\n\n# PREPARE DATA FROM IDIGBIO\n\n# Read into R the raw occurrence data from iDigBio, which should be whatever was\n# published by the data provider (e.g. the collection)\nidb_raw <- read_csv(""9d4b9069-48c4-4212-90d8-4dd6f4b7f2a5/occurrence_raw.csv"", \n                    na = c("""", ""NA""),\n                    col_types = cols(.default = col_character()))\n\n# Rename columns to reflect provenance and remove colon characters\nidb_raw <- idb_raw %>% \n  rename_all(function(x){paste0(x, ""_idbR"")}) %>% \n  rename_all(funs(str_replace_all(., ""dwc:"", """"))) %>% \n  rename_all(funs(str_replace_all(., "":"", ""_"")))\n\n# Read into R the version of occurrence data processed by iDigBio\nidb_processed <- read_csv(""9d4b9069-48c4-4212-90d8-4dd6f4b7f2a5/occurrence.csv"", \n                          na = c("""", ""NA""),\n                          col_types = cols(.default = col_character()))\n\n# Rename columns to reflect provenance and remove illegal characters\nidb_processed <- idb_processed %>% \n  rename_all(function(x){paste0(x, ""_idbP"")}) %>% \n  rename_all(funs(str_replace_all(., ""dwc:"", """"))) %>% \n  rename_all(funs(str_replace_all(., "":"", ""_"")))\n\n# Join raw and processed iDigBio data together\nidb_joined <- idb_raw %>% \n  left_join(idb_processed, by = c(""coreid_idbR"" = ""coreid_idbP"")) %>% \n  # Subset by families of interest\n  mutate(family_idbR = tolower(family_idbR)) %>% \n  filter(family_idbR %in% c(""rhinolophidae"", \n                            ""hipposideridae"", \n                            ""rhinonycteridae"") |\n           family_idbP %in% c(""rhinolophidae"", \n                              ""hipposideridae"", \n                              ""rhinonycteridae"")) %>% \n  # Expand the idbP field `geopoint` into two fields to reflect the same\n  # structure as the other datasets here\n  separate(idigbio_geoPoint_idbP, \n           c(""decimalLatitude_idbP"", ""decimalLongitude_idbP""), \n           sep = "","") %>% \n  mutate(decimalLatitude_idbP = parse_number(decimalLatitude_idbP)) %>% \n  mutate(decimalLongitude_idbP = parse_number(decimalLongitude_idbP)) %>% \n  # Create a new column to identify duplicate records between iDigBio and GBIF\n  # in future steps\n  unite(matchDuplicates, \n        c(""institutionCode_idbR"", \n          ""collectionCode_idbR"", \n          ""catalogNumber_idbR"",\n          ""occurrenceID_idbR""), \n        sep = "" "", \n        remove = FALSE) %>% \n  # Normalize text differences by lowercasing\n  mutate(matchDuplicates = str_squish(tolower(matchDuplicates)))\n\n# PREPARE DATA FROM GBIF\n\n# Read into R the raw occurrence data from GBIF, which should be whatever was\n# published by the data provider (e.g. the collection); this should ostensibly\n# be the same as what is in `idb_raw`\ngbif_subset_1R <- read_tsv(""0067804-200613084148143/verbatim.txt"", \n                           na = c("""", ""NA""),\n                           col_types = cols(.default = col_character()))\n\n# Rename columns to reflect provenance\ngbif_subset_1R <- gbif_subset_1R %>% \n  rename_all(function(x){paste0(x, ""_gbifR"")})\n\n# Read into R the version of occurrence data processed by GBIF\ngbif_subset_1P <- read_tsv(""0067804-200613084148143/occurrence.txt"", \n                           na = c("""", ""NA""),\n                           col_types = cols(acceptedTaxonKey = col_integer(),\n                                            classKey = col_integer(),\n                                            familyKey = col_integer(),\n                                            genusKey = col_integer(),\n                                            kingdomKey = col_integer(),\n                                            orderKey = col_integer(),\n                                            phylumKey = col_integer(),\n                                            speciesKey = col_integer(),\n                                            .default = col_character()))\n\n# Rename columns to reflect provenance\ngbif_subset_1P <- gbif_subset_1P %>% \n  rename_all(function(x){paste0(x, ""_gbifP"")})\n\n# Join raw and processed GBIF subset data together\ngbif_subset_1 <- gbif_subset_1R %>% \n  left_join(gbif_subset_1P, by = c(""gbifID_gbifR"" = ""gbifID_gbifP"")) %>% \n  # Get rid of these columns because I can\'t figure out how to coerce them into\n  # the same data class to later join GBIF subsets\n  select(-organismQuantity_gbifR, -organismQuantity_gbifP)\n\n# Read into R the raw occurrence data from GBIF, which should be whatever was\n# published by the data provider (e.g. the collection); this should ostensibly\n# be the same as what is in `idb_raw`\ngbif_subset_2R <- read_tsv(""0067806-200613084148143/verbatim.txt"", \n  ', '##############\r\n###Title: RAPID Code to Find and Link Barcode of Life External Linkages\r\n###Author: Katelin D. Pearson\r\n###Date: February 12, 2021\r\n#############\r\n\r\nlibrary(DataCombine)\r\n\r\n#load in all Hipposideridae and Rhinolophidae sequences from BOLD\r\nboldbats <- read.csv(""C:/Users/Katie/Documents/FSU_RAPID/bold_hippo_rhino.csv"")\r\n\r\n#exclude records that were already mined from NCBI\r\nboldbats <- subset(boldbats,boldbats$institutionCode!=""Mined from GenBank, NCBI"")\r\n\r\n#standardize the ROM and MZB catalog numbers\r\n#no other catalog numbers were identified as needing standardization\r\ntofrom <- matrix(ncol = 2, nrow = 2)\r\ntofrom <- as.data.frame(tofrom)\r\ncolnames(tofrom) <- c(""from"",""to"")\r\ntofrom$from[1] <- ""ROM:MAM:""\r\ntofrom$to[1] <- ""ROM ""\r\ntofrom$from[2] <- ""MZB_""\r\ntofrom$to[2] <- ""MZB ""\r\nboldbats <- FindReplace(boldbats, ""catalogNumber"", tofrom, from = ""from"", to = ""to"", exact = FALSE)\r\nboldbats$catalogNumber <- as.character(boldbats$catalogNumber)\r\n\r\n#need to add ROM when belongs to Royal Ontario Museum (some were missing acronym in catalog number)\r\nfor(i in 1:dim(boldbats)[1]){\r\n  if(grepl(""Royal Ontario"",boldbats$institutionCode[i])){\r\n    if(is.na(boldbats$catalogNumber[i])){\r\n      next\r\n    } else {\r\n      if(grepl(""ROM"",boldbats$catalogNumber[i])==FALSE){\r\n        boldbats$catalogNumber[i] <- paste(""ROM"",boldbats$catalogNumber[i])\r\n      }\r\n    }\r\n  } else {\r\n    next\r\n  }\r\n}\r\n\r\n#load the previous specimen data\r\nspecs <- read.csv(""C:/Users/Katie/Documents/FSU_RAPID/FinalParsedFiles/final_standardized_assocSeq.csv"")\r\n\r\n#add a flag for specimens that already have sequences from NCBI\r\n#these are expected to be dupliates, not new sequences\r\nspecs$inBOLD <- NA\r\nspecs$associatedSequences_rapid <- as.character(specs$associatedSequences_rapid)\r\n\r\n#find BOLD sequences corresponding to the specimens in our dataset\r\nfor(i in 1:dim(specs)[1]){\r\n  matches <- which(boldbats$catalogNumber %in% specs$unifiedCatNum[i])\r\n  if(length(matches)==0){\r\n    next\r\n  } else {\r\n    if(!is.na(specs$associatedSequences_rapid[i])){\r\n     specs$inBOLD[i] <- TRUE\r\n    } else {\r\n      specs$associatedSequences_rapid[i] <- paste(paste(""http://www.barcodinglife.org/index.php/Public_RecordView?processid="",as.character(boldbats$occurrenceID[matches]),sep=""""), collapse = ""|"")\r\n    }\r\n  }\r\n}\r\n\r\n#how many specimens were from BOLD?\r\nboldIDs <- subset(specs,grepl(""barcoding"",specs$associatedSequences_rapid))\r\n\r\n#how many specimens were both in NCBI and BOLD?\r\nboldAndNCBI <- subset(specs, specs$inBOLD==TRUE)\r\n\r\n#output data file\r\nwrite.csv(specs,""C:/Users/Katie/Documents/FSU_RAPID/associatedSequences_after_bold.csv"")\r\n', '##############\r\n###Title: RAPID Code to Find GenBank Sequences\r\n###Author: Katelin D. Pearson\r\n###Date: February 4, 2021\r\n#############\r\n\r\nlibrary(splitstackshape)\r\nlibrary(tidyverse)\r\nlibrary(DataCombine)\r\n\r\n##Part 1: Extract Catalog Numbers from GenBank Results\r\n\r\n#Read in dataset (a gff3 file converted to csv from NCBI Nuccore)\r\nsample <- read_csv(""PATH"",\r\n  col_types = cols(\r\n  text = col_character()\r\n))\r\n\r\n#Make a dataframe to hold each line that contains a catalog number\r\npositives <- matrix(NA,nrow=100000)\r\npositives <- as.data.frame(positives)\r\ncolnames(positives) <- c(""text"")\r\n\r\n#Go through the dataset and extract only lines with catalog numbers\r\n#In GenBank, these are values in the ""specimen-voucher"" field\r\nk = 1\r\n#ptm <- proc.time() #for if you want to time the loop\r\nfor(i in 1:dim(sample)[1]){\r\n  if(sample$text[i]==""""){\r\n    next\r\n  } else {\r\n    if(is.na(sample$text[i])){\r\n      next\r\n    } else {\r\n      if(grepl(""specimen-voucher="",sample$text[i])){\r\n        positives$text[k] <- as.character(sample$text[i])\r\n        k <- k + 1\r\n      }\r\n    }\r\n  }\r\n}\r\n#ptm - proc.time() #for if you want to time the loop\r\n\r\n#Remove any blank rows\r\npositives <- positives %>% filter_all(any_vars(!is.na(.)))\r\n\r\n#Split the results into two columns\r\npositives <- cSplit(positives, ""text"", sep = ""+"")\r\n\r\n#Because the first column always has the GenBankID at the beginning followed by a tab\r\n#We can extract this value using cSplit and put it into its own column\r\npositives <- cSplit(positives, ""text_1"", sep = ""/t"")\r\ncolnames(positives)[2] <- ""GenBankID""\r\n\r\n#Make a new catalogNumber field\r\npositives$catalogNumber <- NA\r\n\r\n#Extract the catalogNumbers from each specimen record\r\nfor(i in 1:dim(positives)[1]){\r\n  cn <- sub("".*specimen-voucher="","""", positives$text_2[i])\r\n  positives$catalogNumber[i] <- cn\r\n}\r\n\r\n#Remove garbage on the other side of the catalog number\r\npositives <- cSplit(positives,""catalogNumber"", sep = "";"")\r\n\r\n#Make a cleaned dataframe of only catalog number and GenBankID\r\ncleaned <- cbind(catalogNumber = as.character(positives$catalogNumber_1), GenBankID = as.character(positives$GenBankID))\r\ncleaned <- as.data.frame(cleaned)\r\n\r\n#standardize ROM catalog numbers that say ROM MAM or ROM: into just ROM\r\ntofrom <- matrix(ncol = 2, nrow = 1)\r\ntofrom <- as.data.frame(tofrom)\r\ncolnames(tofrom) <- c(""from"",""to"")\r\ntofrom$from[1] <- ""ROM MAM""\r\ntofrom$to[1] <- ""ROM""\r\ncleaned <- FindReplace(cleaned, ""catalogNumber"", tofrom, from = ""from"", to = ""to"", exact = FALSE)\r\ntofrom$from[1] <- ""ROM:""\r\ntofrom$to[1] <- ""ROM ""\r\ncleaned <- FindReplace(cleaned, ""catalogNumber"", tofrom, from = ""from"", to = ""to"", exact = FALSE)\r\n\r\n#standardize Western Australian Museum records\r\nfor(i in 1:dim(cleaned)[1]){\r\n  if(grepl(""Western Australian"", cleaned$catalogNumber[i])){\r\n    this1 <- sub("" Western Australian Museum.*"", """", as.character(cleaned$catalogNumber[i]))\r\n    this2 <- paste(""WAM "",this1)\r\n    cleaned$catalogNumber[i] <- this2\r\n  } else {\r\n    next\r\n  }\r\n}\r\n\r\nwrite.csv(cleaned, ""PATH"")\r\n\r\n##Part 2: Unify Catalog Number in Specimen Dataset\r\n#The GenBank specimen voucher numbers are generally the institution code followed by the number\r\n#Often the catalog number in the specimen dataset is only the catalog number\r\n#We need to make a catalog number field that matches the GenBank format for matching purposes\r\n\r\n#load GenBank dataset if need be \r\n#cleaned <- read.csv(""PATH"")\r\n\r\nspecs <- read.csv(""PATH"")\r\n\r\nspecs$unifiedCatNum <- paste(as.character(specs$institutionCode_gbifP),as.character(specs$catalogNumber_gbifP),sep = "" "")\r\n\r\n#look at what else needs to be cleaned or standardized:\r\n\r\n#look at values in the catalogNumber field for the GenBank data\r\n# table(substr(as.character(cleaned$catalogNumber), 1, 4))\r\n#look at values in the unifiedCatNum field for the specimen data\r\n# table(substr(specs$institutionCode_gbifP, 1, 4))\r\n\r\n##Part 3: Compare Catalog Number in Specimen Dataset to Catalog Numbers for GenBank sequences\r\n\r\n#only uncomment the first time you are making this column\r\n# specs$associatedSequences_rapid <- NA\r\n\r\nfor(i in 1:dim(specs)[1]){\r\n  matches <- which(cleaned$catalogNumber %in% specs$unifiedCatNum[i])\r\n  if(length(matches)==0){\r\n    next\r\n  } else {\r\n    specs$associatedSequences_rapid[i] <- paste(paste(""http://www.ncbi.nlm.nih.gov/nuccore/"",as.character(cleaned$GenBankID[matches]),sep=""""), collapse = ""|"")\r\n  }\r\n}\r\n\r\nView(specs$associatedSequences_rapid)\r\nt <- table(specs$associatedSequences_rapid)\r\nView(t)\r\n\r\nwrite.csv(specs, ""PATH"")', '##############\n###Title: RAPID Code to Standardize External Linkages\n###Author: Katelin D. Pearson\n###Date: February 4, 2021\n#############\n\nlibrary(splitstackshape)\n\n#load the dataset of specimen records\n#the default associatedSequences field in our dataset is named associatedSequences_gbifP\nbats <- read.csv(""PATH"")\n\n#make a subset of the dataset for which the associatedSequences field is not empty\n#(to get a simple count)\nassocSeq <- subset(bats, bats$associatedSequences_gbifP!="""")\n\n#make a new column for the re-formatted sequences\n#DO NOT RUN THIS if you have already run the GenBank-Specimen Linkage code on your dataset\n#bats$associatedSequences_rapid <- NA\n\nbats$associatedSequences_rapid <- as.character(bats$associatedSequences_rapid)\n\n#populate the new (or previously existing) column\n#we are using the NCBI URL as recommended by the Darwin Core\nfor(i in 1:dim(bats)[1]){\n  if(bats$associatedSequences_gbifP[i]==""""){\n    next\n  } else {\n    if(is.na(bats$associatedSequences_gbifP[i])){\n      next\n    } else {\n      #transfer over the sequences that are already correctly formatted with the nuccure URL\n      if(grepl(""nuccore"",as.character(bats$associatedSequences_gbifP[i]))){\n        bats$associatedSequences_rapid[i] <- as.character(bats$associatedSequences_gbifP[i])\n      } else {\n        #reformat the records that are only an NCBI number\n        if(nchar(as.character(bats$associatedSequences_gbifP[i]))<10){\n          bats$associatedSequences_rapid[i] <- as.character(paste(""http://www.ncbi.nlm.nih.gov/nuccore/"",as.character(bats$associatedSequences_gbifP[i]),sep=""""))\n        }\n      }\n    }\n  }\n}\n\n#make sure it works\nt <- table(bats$associatedSequences_rapid)\nt <- as.data.frame(t)\n\n#now to deal with columns that have two sequences\n#preserve the original column\nbats$associatedSequences_gbifP_orig <- bats$associatedSequences_gbifP\n\n#split the column into two columns, assuming the sequences are separated by a |\nbats <- cSplit(bats,""associatedSequences_gbifP"",sep=""|"",type.convert = FALSE)\n\n#isolate each incorrect URL and apply the correct URL, then join them together\n#into the associatedSequences_rapid column\nfor(i in 1:dim(bats)[1]){\n  if(!is.na(bats$associatedSequences_gbifP_2[i])){\n    first <- sub("".*term="", """", as.character(bats$associatedSequences_gbifP_1[i]))\n    second <- sub("".*term="", """", as.character(bats$associatedSequences_gbifP_2[i]))\n    bats$associatedSequences_rapid[i] <- paste(paste(""http://www.ncbi.nlm.nih.gov/nuccore/"",first, sep = """"), paste(""http://www.ncbi.nlm.nih.gov/nuccore/"",second,sep = """"),sep = ""|"")\n    print(i)\n  }\n}\n\n#remove erroneous associatedSequences\nfor(i in 1:dim(bats)[1]){\n  if(is.na(bats$associatedSequences_rapid[i])){\n    next\n  } else {\n    if(nchar(as.character(bats$associatedSequences_rapid[i]))<10){\n      bats$associatedSequences_rapid[i] <- NA\n    }\n  }\n}', '# RAPID code for fetching data from Bionomia to reintegrate with data in BIOSPEX\n# This code accompanies document \'RAPID-protocol_people.pdf\'\n# Code written by Katelin Pearson, 2020-12-16. Updated by Erica Krimmel on\n# 2021-02-02.\n\n# Load core libraries; install these packages if you have not already\nlibrary(tidyverse)\nlibrary(jsonlite)\n\n# Read into R the data exported from Bionomia and specify column types (doing\n# this is recommended when dataset is large and columns may be sparsely\n# populated). Change filename for reuse.\nspecs <- read_csv(""biospex_people_2020-12-15.csv"",\n                  col_types = cols(\'_id\' = col_character(),\n                                   gbifID = col_double(),\n                                   recordedBy = col_character(),\n                                   recordedByID = col_character(),\n                                   identifiedBy = col_character(),\n                                   identifiedByID = col_character()))\n\n# Modify `specs` for data enhancement process\nspecs <- specs %>% \n  # Create column to use when fetching data from Bionomia\n  mutate(gbifIDurl = paste0(""https://bionomia.net/occurrence/"",\n                            specs$gbifID,\n                            "".json"")) %>% \n  # Create columns for the enhanced people data from Bionomia\n  mutate(recordedBy_rapid = NA) %>% \n  mutate(recordedByID_rapid = NA) %>% \n  mutate(identifiedBy_rapid = NA) %>% \n  mutate(identifiedByID_rapid = NA)\n\n# Define function to apply a NULL value to any JSON queries returning an error\nfromJSON_possibly <- possibly(fromJSON,\n                              otherwise = NULL)\n\n# Loop through records in \'spec\' to fetch person data from Bionomia\nfor(i in 1:dim(specs)[1]){\n  print(i)\n  # Skip record if it lacks a value for `gbifID`\n  if(is.na(specs$gbifID[i])){\n    next\n  }\n  # Use record to fetch data from Bionomia\n  thisSpec <- fromJSON_possibly(specs$gbifIDurl[i],\n                                simplifyDataFrame = TRUE,\n                                flatten = TRUE)\n  # Skip record if it lacks Bionomia data\n  if(is.null(thisSpec)){\n    next\n  }else{\n    # Check for collector person data from Bionomia\n    if(length(thisSpec$recorded) > 0){\n      # Create table of collector person data\n      recby <- as_tibble(thisSpec$recorded)\n      \n      # Check for multiple collectors. If there are multiple collectors,\n      # concatenate values that will be transferred to `recordedBy_rapid` and\n      # `recordedByID_rapid`\n      if(dim(recby)[1]>1){\n        # Make placeholders for concatenated values\n        allids <- c()\n        allnam <- c()\n        # Concatenate values\n        for(k in 1:dim(recby)[1]){\n          allids <- paste(allids,\n                          recby$`@id`[k],\n                          sep="" | "")\n          allnam <- paste(allnam,\n                          paste0(recby$givenName[k],\n                                "" "",\n                                recby$familyName[k]),\n                          sep="" | "")\n        }\n        \n        # Remove extra pipes from strings\n        allids <- sub(\' \\\\| \', \'\', allids)\n        allnam <- sub(\' \\\\| \', \'\', allnam)\n        \n        # Put concatenated values for `id` into `recordedByID_rapid`\n        specs$recordedByID_rapid[i] <- allids\n       \n        # Put concatenated values for `recorded` into `recordedBy_rapid`\n        specs$recordedBy_rapid[i] <- allnam\n        \n      # If there is a single collectors, transfer values directly into\n      # `recordedBy_rapid` and `recordedByID_rapid`\n      }else{\n        # Put value for `id` in `recordedByID_rapid`\n        specs$recordedByID_rapid[i] <- recby$`@id`\n        # Put value for `recorded` into `recordedBy_rapid`\n        specs$recordedBy_rapid[i] <- paste0(recby$givenName,\n                                           "" "",\n                                           recby$familyName)\n      }\n    }\n    \n    # Check for identifier person data from Bionomia\n    if(length(thisSpec$identified) > 0){\n      # Create table of identifier person data\n      idby <- as_tibble(thisSpec$identified)\n      \n      # Check for multiple identifiers. If there are multiple identifiers,\n      # concatenate values that will be transferred to `identifiedBy_rapid` and\n      # `identifiedByID_rapid`\n      if(dim(idby)[1]>1){\n        # Make placeholders for concatenated values\n        allids <- c()\n        allnam <- c()\n        # Concatenate values\n        for(k in 1:dim(idby)[1]){\n          allids <- paste(allids,\n                          idby$`@id`[k],\n                          sep="" | "")\n          allnam <- paste(allnam,\n                          paste0(idby$givenName[k],\n                                "" "",\n                                idby$familyName[k]),\n                          sep="" | "")\n        }\n\n        # Remove the extra pipes from strings\n        allids <- sub(\' \\\\| \', \'\', allids)\n        allnam <- sub(\' \\\\| \', \'\', allnam)\n        \n        # Put concatenated values for `id` into `identifiedByID_rapid`\n   ', '# RAPID code for standardizing country data\n# This code accompanies document \'RAPID-protocol_standardize-country.pdf\'\n# Code written by Erica Krimmel, 2020-07-31. Updated 2020-09-23.\n\n# Load core libraries; install this package if you have not already\nlibrary(tidyverse)\n\n# Read into R the primary records dataset\nrecords <- read_csv(""rapid-joined-records_2020-09-23.csv"", \n                    na = c("""", ""NA""),\n                    col_types = cols(.default = col_character()))\n\n# Extract distinct values for country information\ncountryCleanup <- records %>% \n  select(starts_with(""country""), idigbio_isoCountryCode_idbP) %>% \n  distinct() %>% \n  unite(countryKey, everything(), remove = FALSE) %>% \n  mutate_at(vars(starts_with(""country_"")), tolower) %>% \n  mutate_at(vars(contains(""countryCode"")), toupper) %>% \n  mutate(country_rapid = coalesce(country_idbP,\n                                  country_gbifR,\n                                  country_idbR)) %>% \n  mutate(countryCode_rapid = coalesce(countryCode_gbifP,\n                                      idigbio_isoCountryCode_idbP,\n                                      countryCode_gbifR,\n                                      countryCode_idbR))\n\n# Save `countryCleanup` as file to do cleanup work in OpenRefine\nwrite_csv(countryCleanup,\n          paste(""rapid-country-cleanup_"", Sys.Date(), "".csv"", sep = """"),\n          na = """")\n\n# Read in cleaned up country data file saved from OpenRefine\ncountryCleanup_done <- read_csv(""rapid-country-cleanup_2020-09-23.csv"",\n                                na = c("""", ""NA""),\n                                col_types = cols(.default = col_character()))\n\n# Join cleaned up country data back into `records`\nrecords_countryCleanup <- records %>% \n  unite(countryKey,\n        country_gbifR,\n        country_idbP,\n        country_idbR,\n        countryCode_gbifP,\n        countryCode_gbifR,\n        countryCode_idbR,\n        idigbio_isoCountryCode_idbP,\n        remove = FALSE) %>% \n  left_join(select(countryCleanup_done, \n                   countryKey, country_rapid, countryCode_rapid),\n            by = ""countryKey"") %>% \n  select(-countryKey)\n\n# Save `records` as CSV file\nwrite_csv(records_countryCleanup, \n          paste(""rapid-joined-records_country-cleanup_"", Sys.Date(), "".csv"", sep = """"),\n          na = """")']",1,"- horseshoe bats
- coronaviruses
- NSF grant
- data product
- iDigBio
- GBIF
- R code
- data management
- natural history collections
- georeferenced
- vet"
Data from: Continuous corn and cornsoybean profits over a 45-year tillage and fertilizer experiment,"Studies comparing profitability of tillage systems often examine narrow historic windows or exclude annual price fluctuations. This study uses a continuous corn (Zea mays L.) (CC; 19701990) and cornsoybean [Glycine max (L.) Merr.] (CS; 19912014) Tillage  Fertilizer study in somewhat poorly drained soils in southern Illinois to reconstruct partial annual budgets with historical prices for crops, fertilizers, lime, herbicides, fuel, labor, and machinery. Combinations of tillage (moldboard plow [MP], chisel tillage [ChT], alternate tillage [AT], and no-till [NT]) and fertilizer (Control, N-only, N+NPK starter, NPK+NPK starter, and NPK broadcast) treatments were evaluated. The CC profits were highest in NPK-applied treatments followed by N-only and Control. The MP treatments were similar to ChT and more profitable than NT, while AT fell between. In CS, NPK-applied treatments were similar regardless of tillage. Combined costs for herbicide, machinery, labor, and diesel were higher in MP and ChT systems than AT and lowest in NT, but were a small percentage of total costs (26.6, 26.0, 21.5, and 18.2%, respectively). Nitrogen fertilizer offered a return on investment of 396% in CC and 133% in CS while P & K returned 78% in CC and 109% in CS. Sensitivity analysis in CS showed that NT would be less profitable than MP if herbicide costs increased 850%. A 300% machinery cost increase would have made MP less profitable than NT. These findings suggest that since 1991 CS under NT carried the same potential for profit as other tillage systems under full fertility management.","['setwd(""~/Dropbox/Tillage x Fertility trial/Economics paper/Data/final_data_files/"") ## user to define file structure \n\nyld <- read.csv(""Master_Yield_1970-2014_econ.csv"")\nyld <- yld[,1:15] ### cut unnecessary columns\nyld$yield_bu_ac <- as.character(yld$yield_bu_ac)\nyld$yield_bu_ac <- as.numeric(yld$yield_bu_ac)\nyld$yield_kg_ha <- as.character(yld$yield_kg_ha)\nyld$yield_kg_ha <- as.numeric(yld$yield_kg_ha)\n\n### crop prices, $/bu, taken from USDA NASS quick stats\ncorn_pr <- read.csv(""corn_price_USDANASS.csv"")\ncorn_pr <- corn_pr[which(corn_pr$Period==""MARKETING YEAR""),]\ncorn_pr <- corn_pr[,c(""Year"", ""Value"")]\ncolnames(corn_pr)[2] <- c(""Corn_pr_nominal"")\n\nsoy_pr <- read.csv(""soy_price_USDANASS.csv"")\nsoy_pr <- soy_pr[,c(""Year"", ""Value"")]\ncolnames(soy_pr)[2] <- c(""Soy_pr_nominal"")\n\nprices <- merge(corn_pr, soy_pr, by=""Year"", all=TRUE)\n\n#### reconstruct PPITW to update prices\nppitw1990 <- read.csv(""PPITW_2013-1975-1990index.csv"")\nppitw1990 <- ppitw1990[, c(""Year"", ""Value"")]\ncolnames(ppitw1990)[2] <- c(""PPITW90"")\nppitw2011 <- read.csv(""PPITW_2014-1990-2011index.csv"")\nppitw2011 <- ppitw2011[, c(""Year"", ""Value"")]\ncolnames(ppitw2011)[2] <- c(""PPITW11"")\n\n### 1967-indexed PPITW to get index to reach back from 1970-1975 (values copied from scanned pdfs of NASS annual price summaries, 1971-1980)\nppitw1967 <- data.frame(c(114, #1970\n                       120,\n                       127,\n                       145,\n                       168,\n                       185,\n                       192,\n                       202,\n                       219,\n                       250,\n                       280)) #1980\nppitw1967$Year <- seq(1970,1980, by=1)\ncolnames(ppitw1967)[1] <- c(""PPITW67"")\n\nppitw_comp <- merge(ppitw1967, ppitw1990, by=""Year"", all = TRUE)\nppitw_comp <- merge(ppitw_comp, ppitw2011, by=""Year"", all=TRUE)\nppitw_comp$PPITWcomp <- ppitw_comp$PPITW11\n\n### back-calculate 2011-indexed PPITW to from 1989 to 1975 using the 1990-indexed PPITW\nppitw_comp$PPITWcomp[which(ppitw_comp$Year<=1989 & ppitw_comp$Year>=1975)] <- 100*(ppitw_comp$PPITW90[which(ppitw_comp$Year<=1989 & ppitw_comp$Year>=1975)]/ppitw_comp$PPITW90[44])\n\n### back-calculate 2011-indexed PPITW from 1974 to 1970 using the 1967-indexed PPITW (use 1980 PPITWcomp as fulcrum)\nppitw_comp$PPITWcomp[which(ppitw_comp$Year<=1974 & ppitw_comp$Year>=1970)] <- ppitw_comp$PPITWcomp[11]*(ppitw_comp$PPITW67[which(ppitw_comp$Year<=1974 & ppitw_comp$Year>=1970)]/ppitw_comp$PPITW67[11])\nprices <- merge(prices, ppitw_comp[,c(""Year"", ""PPITWcomp"")], by=""Year"", all=TRUE)\n\n### Fertilizer costs collation\n### Ammonium nitrate, urea, and ag. lime prices from USDA NASS\namm_nit <- read.csv(""Amm_nit_2014_1970_price.csv"")\namm_nit <- amm_nit[,c(""Year"", ""Value"")]\ncolnames(amm_nit)[2] <- c(""Amm_nit_pr_nominal"")\nurea <- read.csv(""Urea_2014_1970_price.csv"")\nurea <- urea[,c(""Year"", ""Value"")]\ncolnames(urea)[2] <- c(""urea_pr_nominal"")\nlime <- read.csv(""Ag_lime_2014-1970_price.csv"")\nlime <- lime[,c(""Year"", ""Value"")]\ncolnames(lime)[2] <- c(""Lime_pr_nominal"")\n\ntab7 <- read.csv(""Table7_Fert_prices_1960.csv"") ### table 7 from a separate USDA report, provides clearer data on KCl and TSP prices \ncolnames(tab7)[1] <- c(""Year"")\ntab7 <- tab7[,c(""Year"", ""X.8"", ""X.10"")]\ncolnames(tab7)[2:3] <- c(""TSP_pr_nominal"", ""KCl_pr_nominal"")\ntab7[,2] <- as.character(tab7[,2])\ntab7[,3] <- as.character(tab7[,3])\ntab7 <- tab7[6:112,]\ntab7$Year <- as.character(tab7$Year)\ntab7$Year <- as.numeric(tab7$Year)\ntab7 <- tab7[is.finite(tab7$Year),]\ntab7[,2] <- as.numeric(tab7[,2])\ntab7[,3] <- as.numeric(tab7[,3])\ntab7 <- rbind(tab7, c(2014, 621, 601)) ### 2014 US total annual average values, as reported in NASS Quick Stats\ntab7 <- tab7[which(tab7$Year>=1970),] \nfert <- merge(amm_nit, urea, by=""Year"", all=TRUE)\nfert <- merge(fert, tab7, by=""Year"", all=TRUE)\nfert <- merge(fert, lime, by=""Year"", all=TRUE)\n\nprices <- merge(prices, fert, by=""Year"", all=TRUE)\n\n### labor prices\n####read in unsorted file from USDA NASS quick stats (filters:  Economics/Expenses/Labor/Wage Rate/2015:1978)\nlabor <- read.csv(""Labor_Unsorted.csv"")\nlabor.nat <- labor[which(labor$Geo.Level==""NATIONAL""),]\nlabor.nat.ann <- labor.nat[which(labor.nat$Period==""YEAR""),]\npick <- labor.nat.ann$Data.Item[1]\nlabor.nat.ann.type <- labor.nat.ann[which(labor.nat.ann$Data.Item==pick),]### Type = Labor, hired, wage rate, measured in $/hour\nwage_90_14 <- labor.nat.ann.type[1:25,c(""Year"", ""Value"")] ## select 2014-1990\nwage_90_14$Value <- as.character(wage_90_14$Value)\nwage_90_14$Value <- as.numeric(wage_90_14$Value)\n\n#### manually read in average annual labor wages from old .wk1 files from USDA.\nwage_70_89 <- c(1.64, ### 1970, ""per hour, wages only"" wage from 70-73\n                1.73,\n                1.84,\n                2.00,\n                2.25, ### 1974, ""Machine operator"" wage from 74-79\n                2.43,\n                2.66,\n                2.87,\n                3.09,\n                3.39,\n                3.65, ### 1980, ""all workers"" wage from 80-89\n            ']",1,"continuous corn, corn soybean, tillage, fertilizer, profitability, historic windows, annual price fluctuations, southern Illinois, partial annual budgets, historical prices, crops, fertilizers, lime, herbicides, fuel, labor, machinery, moldboard pl"
Data and Code for the manuscript PhD students in life sciences can benefit from team cohesion,"This folder contains data and code to reproduce regression results of the manuscript ""PhD students in life sciences can benefit from team cohesion"".","['phd=read.table(""phd_data.csv"",sep="","",header=T)\r\n\r\nphd$global_clustering_no_ego_2=phd$`global clustering without ego_2`\r\n\r\nphd$constraint_2=phd$`constraint of ego_2`\r\nphd$constraint_4=phd$`constraint of ego_4`\r\nphd$constraint_6=phd$`constraint of ego_6`\r\n\r\nhist(log(phd$degree_0))\r\n\r\nnrow(phd[])\r\n\r\nhist(log(phd$betweenness_0))\r\n\r\nm1=lm(log(citations_cum_8+1) ~ \r\n        log(citations_cum_2+1) + \r\n        log(papers_cum_2+1) + \r\n        log(papers_cum_8-papers_cum_2+1) + \r\n        #log(degree_2+1) + \r\n        #log(global_clustering_no_ego_2+1) +\r\n        discipline +  as.factor(def_year)\r\n      , data=phd[which(phd$def_year<2011),])\r\n#summary(m1)\r\n\r\nm2=lm(log(citations_cum_8+1) ~ \r\n        log(citations_cum_2+1) + \r\n        log(papers_cum_2+1) + \r\n        log(papers_cum_8-papers_cum_2+1) + \r\n        log(degree_2+1) + \r\n        #log(global_clustering_no_ego_2+1) +\r\n        discipline +  as.factor(def_year)\r\n      , data=phd[which(phd$def_year<2011),])\r\n#summary(m2)\r\n\r\nm3=lm(log(citations_cum_8+1) ~ \r\n        log(citations_cum_2+1) + \r\n        log(papers_cum_2+1) + \r\n        log(papers_cum_8-papers_cum_2+1) + \r\n        log(degree_2+1) + \r\n        log(constraint_2+1) +\r\n        discipline +  as.factor(def_year)\r\n      , data=phd[which(phd$def_year<2011),])\r\n#summary(m3)\r\n\r\ncor_d=phd[which(phd$def_year<2011),c(""citations_cum_8"", ""citations_cum_2"",\r\n                                     ""papers_cum_2"", ""degree_2"", ""constraint_2"")]\r\n\r\ncor(cor_d, use=""pairwise.complete.obs"", method=""pearson"")\r\n\r\nlength(unique(phd$discipline))\r\n\r\n#library(stargazer)\r\ncapture.output(stargazer(m1, m2, m3, type=""text"", \r\n                         dep.var.labels=""Citations 8 years post-defence"",\r\n                         column.labels = c(""M1"",""M2"", ""M3""),\r\n                                                #  covariate.labels=c(""CIT_2"",""PAPERS_2"", ""PAPERS_8_2""), \r\n                                                                     # ""DEG_0 ? CON_0"",\r\n                                                                     # ""PAPERS_0"", ""PAPERS_8"",\r\n                                                                     # ""DEG_0_2"", ""DEG_2_4"", ""DEG_4_6"",\r\n                                                                     # ""CON_0_2"", ""CON_2_4"", ""CON_4_6""),\r\n                         omit.stat=c(""LL"",""ser"",""f""), ci=F,  single.row=F,\r\n                         out=""dec14_1.txt""))\r\n\r\n\r\n###############################################xx\r\n\r\n\r\n# non-live science\r\nm4=lm(log(citations_cum_8+1) ~ \r\n        log(citations_cum_2+1) + \r\n        log(papers_cum_2+1) + \r\n        log(papers_cum_8-papers_cum_2+1) + \r\n        log(degree_2+1) + \r\n        log(constraint_2+1) +\r\n        discipline +  as.factor(def_year)\r\n      , data=phd[which(phd$def_year<2011 & phd$Research_Area==""non-live science""),])\r\n\r\n# live science\r\nm5=lm(log(citations_cum_8+1) ~ \r\n        log(citations_cum_2+1) + \r\n        log(papers_cum_2+1) + \r\n        log(papers_cum_8-papers_cum_2+1) + \r\n        log(degree_2+1) + \r\n        log(constraint_2+1) +\r\n        discipline +  as.factor(def_year)\r\n      , data=phd[which(phd$def_year<2011 & phd$Research_Area==""live science""),])\r\n\r\n# engineering\r\nm6=lm(log(citations_cum_8+1) ~ \r\n        log(citations_cum_2+1) + \r\n        log(papers_cum_2+1) + \r\n        log(papers_cum_8-papers_cum_2+1) + \r\n        log(degree_2+1) + \r\n        log(constraint_2+1) +\r\n        discipline +  as.factor(def_year)\r\n      , data=phd[which(phd$def_year<2011 & phd$Research_Area==""engineering""),])\r\n\r\n# social science\r\nm7=lm(log(citations_cum_8+1) ~ \r\n        log(citations_cum_2+1) + \r\n        log(papers_cum_2+1) + \r\n        log(papers_cum_8-papers_cum_2+1) + \r\n        log(degree_2+1) + \r\n        log(constraint_2+1) +\r\n        discipline +  as.factor(def_year)\r\n      , data=phd[which(phd$def_year<2011 & phd$Research_Area==""social science""),])\r\n\r\n\r\ncapture.output(stargazer(m4, m5, m6, m7, type=""text"", \r\n                         dep.var.labels=""Citations 8 years post-defence"",\r\n                         column.labels = c(""Sciences"",""Life Sci"", ""Engineer"", ""Social Sci""),\r\n                         #                         covariate.labels=c(""HIV_0"",""DEG_0"", ""CON_0"", \r\n                         #                                            ""DEG_0 ? CON_0"",\r\n                         #                                            ""PAPERS_0"", ""PAPERS_8"",\r\n                         #                                            ""DEG_0_2"", ""DEG_2_4"", ""DEG_4_6"",\r\n                         #                                            ""CON_0_2"", ""CON_2_4"", ""CON_4_6""),\r\n                         omit.stat=c(""LL"",""ser"",""f""), ci=F,  single.row=F,\r\n                         out=""dec14_2.txt""))\r\n\r\n']",1,"data, code, manuscript, PhD students, life sciences, team cohesion, regression results."
Data and Code for the paper Can hesitancy be mitigated by free choice across COVID-19 vaccine types?,"This folder contains data and code to reproduce results of the manuscript ""Can hesitancy be mitigated by free choice across COVID-19 vaccine types?"".","['# Network to matrices\r\nsetwd(""c:/Users/bleng/Documents/1 munka/0 járvány/vaccine/preferences/package"")\r\n\r\nd=read.table(""MASZK_dataCore.csv"", sep="","", header = T)\r\n\r\nedges=d[d$Vaccinated==1 & d$RejectedAny==1,c(5:10)]\r\ne_c=data.frame(unique(edges$VaccineType),c(""Moderna"", ""Sinopharm"", ""Pfizer"", ""AstraZeneca"", ""Sputnyk"", ""Janssen""))\r\nnames(e_c)=c(""VaccineType"", ""vaccine"")\r\nedges=merge(edges, e_c, by=""VaccineType"")\r\n#edges$VaccineType=edges$vaccine.y\r\n\r\nlibrary(reshape2)\r\ne=melt(edges, id.vars = ""VaccineType"")\r\ne=e[e$value==1,]\r\ne$value=1\r\n\r\ne_c2=data.frame(unique(e$variable),c(""Pfizer"", ""Moderna"", ""AstraZeneca"", ""Sputnyk"", ""Sinopharm""))\r\nnames(e_c2)=c(""variable"", ""vaccine"")\r\ne=merge(e, e_c2, by=""variable"")\r\nnames(e)[c(2,4)]=c(""Target"", ""Source"")\r\ne=e[,c(4,2)]\r\ne$val=1\r\ne_to_gephi=aggregate(e$val, by=list(e$Source, e$Target), FUN=sum)\r\nnames(e_to_gephi)=c(""Source"", ""Target"", ""Weight"")\r\n\r\n\r\nlibrary(igraph)\r\nnet=graph_from_data_frame(e_to_gephi, directed = T)\r\n\r\nplot(net)\r\n\r\nwrite.table(e_to_gephi, file = ""e_to_gephi.csv"", col.names = T, row.names = F, sep="","")\r\n\r\n', '# 11 Oct 2021\r\n# Kristof Kutasi\r\n# Running time: 1 minute\r\npackages <- c(""dplyr"",""lubridate"",""xtable"",""tidyr"",""qdapTools"",""gghighlight"",""hexbin"",\r\n              ""ggpubr"",""censReg"",""VGAM"",""stargazer"",""gridExtra"",""reshape2"",""svglite"",\r\n              ""forcats"",""grid"")\r\nlapply(packages, require, character.only = TRUE)\r\n\r\n\r\ndataCore=read.table(""MASZK_dataCore.csv"", sep="","", header = T)\r\n\r\n# Table 1\r\n# Sumary table ""Table 1""\r\nt1 = dataCore[!is.na(dataCore$VaccineType),] %>% select(c(Id, IsFemale, Schooling, ChronicIll, RejectedAny, Age, VaccineType,\r\n                           PfizerRating,ModernaRating,AstraRating,SputRating,SinoRating))\r\nt1$Category = ifelse(t1$VaccineType == ""NoVaccine"", 3,\r\n                     ifelse(t1$RejectedAny == 0, 1, 2))\r\nt1$RejectedAny = NULL\r\nt1$AgeGroup = ifelse(t1$Age >= 20 & t1$Age < 40, 1, \r\n                     ifelse(t1$Age >= 40 & t1$Age < 60,2,\r\n                            ifelse(t1$Age >= 60 & t1$Age < 80,3,\r\n                                   ifelse(t1$Age >= 80,4,NA))))\r\nt1$Education = ifelse(t1$Schooling == 1, 1,\r\n                      ifelse(t1$Schooling == 2 | t1$Schooling == 3, 2,3))\r\n\r\nt12 = t1[!is.na(t1$PfizerRating) & t1$PfizerRating == 1 & !is.na(t1$ModernaRating) & \r\n           t1$ModernaRating == 1 & !is.na(t1$AstraRating) & t1$AstraRating == 1 & \r\n           !is.na(t1$SputRating) & t1$SputRating == 1 & !is.na(t1$SinoRating) & \r\n           t1$SinoRating == 1,]\r\n\r\nt13 = t1[(!is.na(t1$PfizerRating) & t1$PfizerRating == 1) | (!is.na(t1$ModernaRating) & \r\n                                                               t1$ModernaRating == 1) | (!is.na(t1$AstraRating) & t1$AstraRating == 1) | \r\n           (!is.na(t1$SputRating) & t1$SputRating == 1) | (!is.na(t1$SinoRating) & \r\n                                                             t1$SinoRating == 1),]\r\n\r\nt1 = t1 %>% select(c(Id,IsFemale,Education,ChronicIll,AgeGroup,VaccineType,Category)) %>%\r\n  group_by(Category)\r\nt1 = t1 %>% summarise(Men = sum(IsFemale == 0), Women = sum(IsFemale == 1),\r\n                      University = sum(Education ==3),""High-school"" = sum(Education ==2), Elementary = sum(Education ==1),\r\n                      ""Not Chronic Illness"" = sum(ChronicIll == 0), ""Chronic Illness"" = sum(ChronicIll == 1),\r\n                      ""Age 20-39"" = sum(AgeGroup ==1, na.rm = T), ""Age 40-59"" = sum(AgeGroup ==2, na.rm = T), \r\n                      ""Age 60-79"" = sum(AgeGroup ==3, na.rm = T), ""Age 80+"" = sum(AgeGroup ==4, na.rm = T),\r\n                      Pfizer = sum(VaccineType == ""Pfizer""), Moderna = sum(VaccineType == ""Moderna""), AstraZeneca = sum(VaccineType == ""Astrazeneca""),\r\n                      Sputnik = sum(VaccineType == ""Sputnik""), Sinopharm = sum(VaccineType == ""Sinopharm""))\r\nt1 = as.data.frame(t(t1[2:17])); colnames(t1) = c(""Vaccinated without rejection"",""Vaccinated after rejection"", ""Non-vaccinated"")\r\n\r\nt12 = t12 %>% summarise(Men = sum(IsFemale == 0), Women = sum(IsFemale == 1),\r\n                        University = sum(Education ==3),""High-school"" = sum(Education ==2), Elementary = sum(Education ==1),\r\n                        ""Not Chronic Illness"" = sum(ChronicIll == 0), ""Chronic Illness"" = sum(ChronicIll == 1),\r\n                        ""Age 20-39"" = sum(AgeGroup ==1, na.rm = T), ""Age 40-59"" = sum(AgeGroup ==2, na.rm = T), \r\n                        ""Age 60-79"" = sum(AgeGroup ==3, na.rm = T), ""Age 80+"" = sum(AgeGroup ==4, na.rm = T),\r\n                        Pfizer = sum(VaccineType == ""Pfizer""), Moderna = sum(VaccineType == ""Moderna""), AstraZeneca = sum(VaccineType == ""Astrazeneca""),\r\n                        Sputnik = sum(VaccineType == ""Sputnik""), Sinopharm = sum(VaccineType == ""Sinopharm""))\r\nt12 = as.data.frame(t(t12)); colnames(t12) = c(""Rates all vaccines unacceptable"")\r\n\r\nt13 = t13 %>% summarise(Men = sum(IsFemale == 0), Women = sum(IsFemale == 1),\r\n                        University = sum(Education ==3),""High-school"" = sum(Education ==2), Elementary = sum(Education ==1),\r\n                        ""Not Chronic Illness"" = sum(ChronicIll == 0), ""Chronic Illness"" = sum(ChronicIll == 1),\r\n                        ""Age 20-39"" = sum(AgeGroup ==1, na.rm = T), ""Age 40-59"" = sum(AgeGroup ==2, na.rm = T), \r\n                        ""Age 60-79"" = sum(AgeGroup ==3, na.rm = T), ""Age 80+"" = sum(AgeGroup ==4, na.rm = T),\r\n                        Pfizer = sum(VaccineType == ""Pfizer""), Moderna = sum(VaccineType == ""Moderna""), AstraZeneca = sum(VaccineType == ""Astrazeneca""),\r\n                        Sputnik = sum(VaccineType == ""Sputnik""), Sinopharm = sum(VaccineType == ""Sinopharm""))\r\nt13 = as.data.frame(t(t13)); colnames(t13) = c(""Rates one vaccine acceptable but another unacceptable"")\r\nTable1 = cbind(t12,t13,t1)\r\nTable1 = round(Table1/nrow(dataCore)*100,1)\r\nprint(xtable(Table1,digits=1), type=""latex"", file=""table1.tex"")\r\n\r\n# Figure 4\r\nfig4Data = dataCore[dataCore$Vaccinated == 1,]\r\n# Figure 4A\r\nfig4NoReject = fig4Data[fig4Data$RejectedAny == 0,] %>% \r\n  filter(!is.na(fig4Data[fig4Data$RejectedAny == 0,]', 'packages <- c(""dplyr"",""lubridate"",""xtable"",""tidyr"",""qdapTools"",""gghighlight"",\r\n              ""hexbin"",""ggpubr"",""censReg"",""VGAM"",""stargazer"",""gridExtra"",""reshape2"")\r\nlapply(packages, require, character.only = TRUE)\r\n\r\n\r\nlibrary(foreign)\r\n\r\ndataCore=read.table(""MASZK_dataCore.csv"", sep="","", header = T)\r\ndata <- read.spss(""MASZK_2021_05.sav"", to.data.frame=TRUE)\r\n\r\nd=data[,c(""K24"", ""K25_1_1"", ""K25_3"", ""K25_7"",\r\n          ""K25_8C1"", ""K25_8C2"", ""K25_8C3"", ""K25_8C4"", ""K25_8C5"", ""K25_8C6"",\r\n          ""K26_2C1"", ""K26_2C2"", ""K26_2C3"", ""K26_2C4"", ""K26_2C5"", ""K26_2C6"",\r\n          ""K26_3"",\r\n          ""K88_2C1"", ""K88_2C2"", ""K88_2C3"", ""K88_2C4"", ""K88_2C5"", ""K88_2C6"", ""K88_2C7"", ""K88_2C8"",\r\n          ""K88_3C1"", ""K88_3C2"", ""K88_3C3"", ""K88_3C4"", ""K88_3C5"", ""K88_3C6"", ""K88_3C7"")]\r\n\r\nnames(d)=c(""vaccinated"", ""month"", ""vaccine_type"", ""reject"",\r\n           ""rej_Pfi"", ""rej_Mod"", ""rej_Ast"", ""rej_Spu"", ""rej_Sin"", ""rej_Joh"",\r\n           ""pref_Pfi"", ""pref_Mod"", ""pref_Ast"", ""pref_Spu"", ""pref_Sin"", ""pref_Joh"",\r\n           ""wait"",\r\n           ""adv_doc"", ""adv_sci"", ""adv_fake"", ""adv_pol"", ""adv_fam"", ""adv_fri"", ""adv_jou"", ""adv_cel"",\r\n           ""source_int"", ""source_soc"", ""source_print"", ""source_radio"", ""source_tv"", ""source_family"", ""source_friends"")\r\n\r\nh=data.frame(data$RecordNo, d, \r\n             data$K23_5_1, data$K23_5_2, data$K23_5_3, data$K23_5_4, data$K23_5_5, data$K23_5_6,\r\n             data$K23_6_1, data$K23_6_1O, data$K23_6_2, data$K23_6_2O, data$K23_6_3,\r\n             data$K23_6_3O, data$K23_6_4, data$K23_6_4O, data$K23_6_5, data$K23_6_5O, data$K23_6_6, data$K23_6_6O)\r\n\r\nnames(h)=c(""Id"",  ""vaccinated"", ""month"", ""vaccine_type"", ""reject"",\r\n           ""rej_Pfi"", ""rej_Mod"", ""rej_Ast"", ""rej_Spu"", ""rej_Sin"", ""rej_Joh"",\r\n           ""pref_Pfi"", ""pref_Mod"", ""pref_Ast"", ""pref_Spu"", ""pref_Sin"", ""pref_Joh"",\r\n           ""wait"",\r\n           ""adv_doc"", ""adv_sci"", ""adv_fake"", ""adv_pol"", ""adv_fam"", ""adv_fri"", ""adv_jou"", ""adv_cel"",\r\n           ""source_int"", ""source_soc"", ""source_print"", ""source_radio"", ""source_tv"", ""source_family"", ""source_friends"",\r\n           ""ass_pf"", ""ass_mo"", ""ass_as"", ""ass_sp"", ""ass_si"", ""ass_ja"",\r\n           ""ass_ch_pf"", ""ass_ch_pf_v"",""ass_ch_mo"", ""ass_ch_mo_v"",""ass_ch_as"", ""ass_ch_as_v"",\r\n           ""ass_ch_sp"", ""ass_ch_sp_v"",""ass_ch_si"", ""ass_ch_si_v"",""ass_ch_ja"", ""ass_ch_ja_v"")\r\n\r\n\r\nfig1Data = dataCore %>% select(c(Id,VaccineType,Vaccine1Date,Age,Group)) %>% drop_na()\r\n\r\nfig1Data$reject=0\r\nfig1Data$reject[fig1Data$Group==""Chr=0&Rej=1"" | fig1Data$Group==""Chr=1&Rej=1""]=1\r\n\r\ntable(fig1Data$reject)\r\n\r\nh$pf_deny=0\r\nh$pf_deny[h$ass_pf==""Elfogadhatatlan""]=1\r\nh$mo_deny=0\r\nh$mo_deny[h$ass_mo==""Elfogadhatatlan""]=1\r\nh$as_deny=0\r\nh$as_deny[h$ass_as==""Elfogadhatatlan""]=1\r\nh$sp_deny=0\r\nh$sp_deny[h$ass_sp==""Elfogadhatatlan""]=1\r\nh$si_deny=0\r\nh$si_deny[h$ass_si==""Elfogadhatatlan""]=1\r\n\r\n\r\nh$sex=data$nem\r\nh$birthyear=data$K1\r\nh$age_d=""80<""\r\nh$age_d[h$birthyear>1941 & h$birthyear<1960]=""60<80""\r\nh$age_d[h$birthyear>1959 & h$birthyear<1980]=""40<60""\r\nh$age_d[h$birthyear>1979 & h$birthyear<2000]=""20<40""\r\nh$age_d[h$birthyear>1999]=""<20""\r\n\r\nh$age_d=as.factor(h$age_d)\r\n\r\nh$chronic=data$K8 # chronic illness\r\nh$citytype=data$teltip # city type\r\nh$covid=data$K18_4 # COVID positive test\r\nh$covidinfamily=data$K21 # COVID in family\r\nh$ser_covid=data$K23_2 # in hospital with COVID\r\nh$ser_covid_family=data$K23_3 # family member in hospital with COVID\r\nh$schooling=data$K10 # schooling\r\nh$schooling=as.character(h$schooling)\r\n\r\nh$chronic_family=""Nem"" # chronic illness in family\r\nh$chronic_family[data$K17_2_1==""Igen"" | data$K17_2_2==""Igen"" | data$K17_2_3==""Igen"" | data$K17_2_4==""Igen"" | data$K17_2_5==""Igen"" | data$K17_2_6==""Igen"" | data$K17_2_7==""Igen""]=""Igen""\r\n\r\n# schooling\r\nh$schooling[h$schooling==""Ã‰rettsÃ©gire Ã©pÃ¼lÅ‘ szakkÃ©pesÃ\xadtÅ‘ bizonyÃ\xadtvÃ¡ny""]=""Közép fokú""\r\nh$schooling[h$schooling==""Ã‰rettsÃ©gi bizonyÃ\xadtvÃ¡ny szakkÃ©pesÃ\xadtÃ©s nÃ©lkÃ¼l""] =""Közép fokú""\r\nh$schooling[h$schooling==""Egyetemi (vagy azzal egyenÃ©rtÃ©kÅ±, pl. MA/MSc) oklevÃ©l""]=""Felsõ fokú""\r\nh$schooling[h$schooling==""FÅ‘iskolai (vagy azzal egyenÃ©rtÃ©kÅ±, pl. BA/BSc) oklevÃ©l""]=""Felsõ fokú""\r\nh$schooling[h$schooling==""FelsÅ‘fokÃº (akkreditÃ¡lt is) szakkÃ©pesÃ\xadtÅ‘ bizonyÃ\xadtvÃ¡ny""]=""Felsõ fokú""\r\nh$schooling[h$schooling==""Szakiskolai oklevÃ©l, bizonyÃ\xadtvÃ¡ny""]=""Közép fokú""\r\nh$schooling[h$schooling==""SzakmunkÃ¡skÃ©pzÅ‘ iskolai bizonyÃ\xadtvÃ¡ny""]=""Közép fokú""\r\nh$schooling[h$schooling==""Ã‰rettsÃ©gi bizonyÃ\xadtvÃ¡ny szakkÃ©pesÃ\xadtÃ©ssel, kÃ©pesÃ\xadtÅ‘ bizonyÃ\xadtvÃ¡ny (az Ã©rettsÃ©givel egyÃ¼tt szerzett szakma)""]=""Közép fokú""\r\nh$schooling[h$schooling==""Ã\\u0081ltalÃ¡nos (elemi, polgÃ¡ri) iskola 8. osztÃ¡ly, Ã©vfolyam""]=""Általános""\r\nh$schooling[h$schooling==""Ã\\u0081ltalÃ¡nos (elemi, polgÃ¡ri) iskola 8. osztÃ¡lynÃ¡l, Ã©vfolyamnÃ¡l alacsonyabb""]=""Általános""\r\n\r\n# handle NA\r\nh$covid=as.character(h$covid)\r\nh$covid[h$covid==0]=""Nem""\r\nh$covid[is.na(h$covid)]=""Nem""\r\n\r\n\r\nh$ser_covid=as.character(h$ser_covid)\r\nh$ser_covid[h$ser_covid==""0""]=""Nem""\r\nh$ser_covid[is.na(h$ser_covid)]=""Nem""\r\n\r\nh$Id=as.numeric(h$Id)\r\n\r\nh$I', '# load packages and data\r\nrm(list = ls())\r\npackages <- c(""dplyr"",""lubridate"",""xtable"",""tidyr"",""qdapTools"",""gghighlight"",\r\n              ""hexbin"",""ggpubr"",""censReg"",""VGAM"",""stargazer"",""gridExtra"",""tree"", ""glmnet"", ""tidyverse"", \r\n              ""ISLR"", ""randomForest"", ""matrixStats"")\r\nlapply(packages, require, character.only = TRUE)\r\nsetwd(""C:/Users/szabomorvai.agnes/Dropbox/Research/MASZK/Lasso"")\r\ndata <- read.csv2(""MASZK_dataCore.csv"", header=T, sep="","", dec = ""."")\r\n\r\nIdVacc = data %>% select(c(Id, VaccineType)) \r\nIdVacc$NoVacc = as.factor(ifelse(IdVacc$VaccineType == ""NoVaccine"", 1, 0))\r\nIdVacc$TypePf = as.factor(ifelse(IdVacc$VaccineType == ""Pfizer"", 1, 0))\r\nIdVacc$TypeMo = as.factor(ifelse(IdVacc$VaccineType == ""Moderna"", 1, 0))\r\nIdVacc$TypeAs = as.factor(ifelse(IdVacc$VaccineType == ""Astrazeneca"", 1, 0))\r\nIdVacc$TypeSp = as.factor(ifelse(IdVacc$VaccineType == ""Sputnik"", 1, 0))\r\nIdVacc$TypeSi = as.factor(ifelse(IdVacc$VaccineType == ""Sinopharm"", 1, 0))\r\n#IdVacc$TypeJa = as.factor(ifelse(IdVacc$VaccineType == 6, 1, 0))\r\n\r\nCharShort = data %>% select(c(Id,Age,IsFemale,CitySize,Schooling,Smoking,ChronicIll,AcuteIll,WorkLastWeek,WealthPreCovid,WealthNow))\r\nId <- data %>% select(c(Id))\r\nPrefHet <- data %>% select(c(PfizerRating, ModernaRating, AstraRating, SputRating, SinoRating)) \r\n\r\nPrefHet <- data.matrix(PrefHet) \r\nPrefVar <- rowVars(PrefHet, na.rm = TRUE)\r\nPrefMean <- rowMeans(PrefHet, na.rm = TRUE)\r\nPrefMin <- rowMins(PrefHet, na.rm = TRUE)\r\nPrefMax <- rowMaxs(PrefHet, na.rm = TRUE)\r\nPrefHetMat <- cbind(Id, PrefVar, PrefMean, PrefMin, PrefMax)\r\nPrefHetNew <- as.data.frame(PrefHetMat)\r\nPrefHetNew$UnaccAny <- PrefMin\r\nPrefHetNew[PrefHetNew$UnaccAny > 1,]$UnaccAny = 0\r\n\r\ndataListen <- data %>% select(c(Id, VaccineDoctor, VaccineScientist, VaccineSceptical, VaccinePolitician, VaccineFamily, VaccineFriends, VaccineJournalist, VaccineCelebrity, SourceWebNews, SourceSocialNetwork, SourcePress, SourceRadio, SourceTV, SourceFamily, SourceFriends, SourceOther))\r\n\r\nLassoVars = CharShort %>% \r\n  left_join(dataListen, by=""Id"") %>%    \r\n  left_join(PrefHetNew, by=""Id"")\r\n# eliminate missing variables\r\ndim(LassoVars)\r\nLassoVars = na.omit(LassoVars)\r\ndim(LassoVars)\r\n# Lasso\r\n# outcome: PrefVar\r\nx = model.matrix(PrefVar~ . -Id -PrefMean -PrefMax -PrefMin ,LassoVars )[,-1]\r\ny = LassoVars$PrefVar\r\n\r\n#outcome: PrefMean\r\n#x = model.matrix(PrefMean  ~ . -Id -PrefVar -PrefMax -PrefMin ,LassoVars )[,-1]\r\n#y = LassoVars$PrefMean\r\n\r\n#outcome: PrefMin\r\n#x = model.matrix(PrefMin  ~ . -Id -PrefVar -PrefMax -PrefMean ,LassoVars )[,-1]\r\n#y = LassoVars$PrefMin\r\n\r\n#outcome: PrefMax\r\n#x = model.matrix(PrefMax  ~ . -Id -PrefVar -PrefMin -PrefMean ,LassoVars )[,-1]\r\n#y = LassoVars$PrefMax\r\n\r\n#outcome: UnaccAny\r\n#x = model.matrix(UnaccAny  ~ . -Id -PrefVar -PrefMin -PrefMean -PrefMax ,LassoVars )[,-1]\r\n#y = LassoVars$UnaccAny\r\n\r\ngrid =10^seq(10,-2, length =100)\r\nset.seed (1)\r\ntrain=sample (1: nrow(x), nrow(x)/2)\r\n\r\ntest=(- train )\r\ny.test=y[test]\r\n\r\n# Lasso\r\nlasso.mod =glmnet (x[train ,],y[train],alpha =1, lambda =grid)\r\nplot(lasso.mod)\r\n\r\n\r\nset.seed (1)\r\ncv.out =cv.glmnet (x[train ,],y[train],alpha =1)\r\nplot(cv.out)\r\nbestlam =cv.out$lambda.min\r\nlasso.pred=predict (lasso.mod ,s=bestlam ,newx=x[test ,])\r\nout=glmnet (x,y,alpha =1, lambda =grid)\r\nlasso.coef=predict (out ,type = ""coefficients"",s=bestlam )[1:20 ,]\r\nlasso.coef\r\nlasso.coef[lasso.coef !=0]\r\n\r\n', '# RUN AFTER 4_TableS3-S7.R\r\n\r\nt=coeftest(md_pf, vcov = vcovHC(md_pf, ""HC1""))\r\n\r\nvar1_pf <- data.frame(Variable = ""Advice Doctor"",\r\n                      Coefficient = t[2,1],\r\n                      SE = t[2,2], \r\n                      p = t[2,4])\r\nvar2_pf <- data.frame(Variable = ""Advice Scientist"",\r\n                      Coefficient = t[3,1],\r\n                      SE = t[3,2], \r\n                      p = t[3,4])\r\nvar3_pf <- data.frame(Variable = ""Advice Anti-Vaccine Propagist"",\r\n                      Coefficient = t[4,1],\r\n                      SE = t[4,2], \r\n                      p = t[4,4])\r\nvar4_pf <- data.frame(Variable = ""Advice Politicians"",\r\n                      Coefficient = t[5,1],\r\n                      SE = t[5,2], \r\n                      p = t[5,4])\r\nvar5_pf <- data.frame(Variable = ""Advice Family"",\r\n                      Coefficient = t[6,1],\r\n                      SE = t[6,2], \r\n                      p = t[6,4])\r\nvar6_pf <- data.frame(Variable = ""Advice Friends"",\r\n                      Coefficient = t[7,1],\r\n                      SE = t[7,2], \r\n                      p = t[7,4])\r\nvar7_pf <- data.frame(Variable = ""Advice Journalists"",\r\n                      Coefficient = t[8,1],\r\n                      SE = t[8,2], \r\n                      p = t[8,4])\r\nvar8_pf <- data.frame(Variable = ""Advice Celebrities"",\r\n                      Coefficient = t[9,1],\r\n                      SE = t[9,2], \r\n                      p = t[9,4])\r\nvar9_pf <- data.frame(Variable = ""Age"",\r\n                      Coefficient = t[10,1],\r\n                      SE = t[10,2], \r\n                      p = t[10,4])\r\nvar10_pf <- data.frame(Variable = ""Male"",\r\n                       Coefficient = t[11,1],\r\n                       SE = t[11,2], \r\n                       p = t[11,4])\r\nvar11_pf <- data.frame(Variable = ""Schooling"",\r\n                       Coefficient = t[12,1],\r\n                       SE = t[12,2], \r\n                       p = t[12,4])\r\nvar12_pf <- data.frame(Variable = ""City Category by Size"",\r\n                       Coefficient = t[13,1],\r\n                       SE = t[13,2], \r\n                       p = t[13,4])\r\nvar13_pf <- data.frame(Variable = ""Smoking"",\r\n                       Coefficient = t[14,1],\r\n                       SE = t[14,2], \r\n                       p = t[14,4])\r\nvar14_pf <- data.frame(Variable = ""Chronic Illness"",\r\n                       Coefficient = t[15,1],\r\n                       SE = t[15,2], \r\n                       p = t[15,4])\r\nvar15_pf <- data.frame(Variable = ""COVID Previously"",\r\n                       Coefficient = t[16,1],\r\n                       SE = t[16,2], \r\n                       p = t[16,4])\r\nvar16_pf <- data.frame(Variable = ""Serious COVID Previously"",\r\n                       Coefficient = t[17,1],\r\n                       SE = t[17,2], \r\n                       p = t[17,4])\r\n\r\nt=coeftest(md_mo, vcov = vcovHC(md_mo, ""HC1""))\r\n\r\nvar1_mo <- data.frame(Variable = ""Advice Doctor"",\r\n                      Coefficient = t[2,1],\r\n                      SE = t[2,2], \r\n                      p = t[2,4])\r\nvar2_mo <- data.frame(Variable = ""Advice Scientist"",\r\n                      Coefficient = t[3,1],\r\n                      SE = t[3,2], \r\n                      p = t[3,4])\r\nvar3_mo <- data.frame(Variable = ""Advice Anti-Vaccine Propagist"",\r\n                      Coefficient = t[4,1],\r\n                      SE = t[4,2], \r\n                      p = t[4,4])\r\nvar4_mo <- data.frame(Variable = ""Advice Politicians"",\r\n                      Coefficient = t[5,1],\r\n                      SE = t[5,2], \r\n                      p = t[5,4])\r\nvar5_mo <- data.frame(Variable = ""Advice Family"",\r\n                      Coefficient = t[6,1],\r\n                      SE = t[6,2], \r\n                      p = t[6,4])\r\nvar6_mo <- data.frame(Variable = ""Advice Friends"",\r\n                      Coefficient = t[7,1],\r\n                      SE = t[7,2], \r\n                      p = t[7,4])\r\nvar7_mo <- data.frame(Variable = ""Advice Journalists"",\r\n                      Coefficient = t[8,1],\r\n                      SE = t[8,2], \r\n                      p = t[8,4])\r\nvar8_mo <- data.frame(Variable = ""Advice Celebrities"",\r\n                      Coefficient = t[9,1],\r\n                      SE = t[9,2], \r\n                      p = t[9,4])\r\nvar9_mo <- data.frame(Variable = ""Age"",\r\n                      Coefficient = t[10,1],\r\n                      SE = t[10,2], \r\n                      p = t[10,4])\r\nvar10_mo <- data.frame(Variable = ""Male"",\r\n                       Coefficient = t[11,1],\r\n                       SE = t[11,2], \r\n                       p = t[11,4])\r\nvar11_mo <- data.frame(Variable = ""Schooling"",\r\n                       Coefficient = t[12,1],\r\n                       SE = t[12,2], \r\n                       p = t[12,4])\r\nvar12_mo <- data.frame(Variable = ""City Category by Size"",\r\n                       Coefficient = t[13,1],\r\n                       SE = t[13,2], \r\n                       p = t[13,4])\r\nvar13_mo <- data.frame(Variable = ""Smo', 'library(foreign)\r\nlibrary(stargazer)\r\nlibrary(sandwich)\r\nlibrary(lmtest)\r\n#library(tidyverse)\r\n#library(caret)\r\nlibrary(regclass)\r\n\r\n### 1. PREPARE DATA TABLE\r\ndataCore=read.table(""MASZK_dataCore.csv"", sep="","", header = T)\r\n\r\n\r\ndataCore$pf_deny=0\r\ndataCore$pf_deny[dataCore$PfizerRating==1]=1\r\ndataCore$mo_deny=0\r\ndataCore$mo_deny[dataCore$ModernaRating==1]=1\r\ndataCore$as_deny=0\r\ndataCore$as_deny[dataCore$AstraRating==1]=1\r\ndataCore$sp_deny=0\r\ndataCore$sp_deny[dataCore$SputRating==1]=1\r\ndataCore$si_deny=0\r\ndataCore$si_deny[dataCore$SinoRating==1]=1\r\n\r\n\r\ndataCore$hedu=0\r\ndataCore$hedu[dataCore$Schooling==4]=1\r\n\r\n# handle NA\r\ndataCore$Covid[is.na(dataCore$Covid)]=0\r\n\r\ndataCore$SerCovid[is.na(dataCore$SerCovid)]=0\r\n\r\n# add month factor ???\r\n\r\ndataCore$Vaccine1Date\r\n\r\n# standardize\r\n\r\ndata_reg=dataCore[,c(""pf_deny"", ""mo_deny"", ""as_deny"", ""sp_deny"", ""si_deny"", ""VaccineDoctor"", ""VaccineScientist"", ""VaccineSceptical"", ""VaccinePolitician"", ""VaccineFamily"", ""VaccineFriends"",\r\n                     ""VaccineJournalist"", ""VaccineCelebrity"", ""Age"", ""IsFemale"", ""hedu"", ""CitySize"", ""WealthPreCovid"", ""Smoking"",\r\n                       ""ChronicIll"", ""Covid"",  ""SerCovid"")]\r\nsc_data_reg=as.data.frame(scale(data_reg))\r\n\r\n### 2. Prepare Empirics\r\n\r\n# LPM on unacceptable\r\n\r\nmd_pf=lm(data=dataCore, pf_deny ~ VaccineDoctor + VaccineScientist + VaccineSceptical + VaccinePolitician + VaccineFamily + VaccineFriends + VaccineJournalist + VaccineCelebrity +\r\n           Age + IsFemale + hedu + CitySize \r\n         #+ WealthPreCovid \r\n         + Smoking + ChronicIll \r\n         + Covid + SerCovid\r\n)\r\nmd_mo=lm(data=dataCore, mo_deny ~ VaccineDoctor + VaccineScientist + VaccineSceptical + VaccinePolitician + VaccineFamily + VaccineFriends + VaccineJournalist + VaccineCelebrity +\r\n           Age + IsFemale + hedu + CitySize \r\n         #+ WealthPreCovid \r\n         + Smoking + ChronicIll  \r\n         + Covid + SerCovid\r\n)\r\nmd_as=lm(data=dataCore, as_deny ~ VaccineDoctor + VaccineScientist + VaccineSceptical + VaccinePolitician + VaccineFamily + VaccineFriends + VaccineJournalist + VaccineCelebrity +\r\n           Age + IsFemale + hedu + CitySize \r\n         #+ WealthPreCovid \r\n         + Smoking + ChronicIll \r\n         + Covid + SerCovid\r\n)\r\nmd_sp=lm(data=dataCore, sp_deny ~ VaccineDoctor + VaccineScientist + VaccineSceptical + VaccinePolitician + VaccineFamily + VaccineFriends + VaccineJournalist + VaccineCelebrity +\r\n           Age + IsFemale + hedu + CitySize \r\n         #+ WealthPreCovid \r\n         + Smoking + ChronicIll \r\n         + Covid + SerCovid\r\n)\r\nmd_si=lm(data=dataCore, si_deny ~ VaccineDoctor + VaccineScientist + VaccineSceptical + VaccinePolitician + VaccineFamily + VaccineFriends + VaccineJournalist + VaccineCelebrity +\r\n           Age + IsFemale + hedu + CitySize \r\n         #+ WealthPreCovid \r\n         + Smoking + ChronicIll \r\n         + Covid + SerCovid\r\n)\r\n\r\ncov_pf         <- vcovHC(md_pf, type = ""HC1"")\r\nrobust_pf    <- sqrt(diag(cov_pf))\r\n\r\ncov_mo         <- vcovHC(md_mo, type = ""HC1"")\r\nrobust_mo    <- sqrt(diag(cov_mo))\r\n\r\ncov_as         <- vcovHC(md_as, type = ""HC1"")\r\nrobust_as    <- sqrt(diag(cov_as))\r\n\r\ncov_sp         <- vcovHC(md_sp, type = ""HC1"")\r\nrobust_sp    <- sqrt(diag(cov_sp))\r\n\r\ncov_si         <- vcovHC(md_si, type = ""HC1"")\r\nrobust_si    <- sqrt(diag(cov_si))\r\n\r\n\r\n# Stargazer output (with and with RSE)\r\nstargazer(md_pf, md_mo, md_as, md_sp, md_si, \r\n          se=list(robust_pf, robust_mo, robust_as, robust_sp, robust_si),\r\n          dep.var.labels=""Vaccine Denial, linear probability models"",\r\n          column.labels = c(""Pfizer"",""Moderna"", ""Astrazeneca"", ""Sputnik"", ""Sinopharm""),\r\n          covariate.labels=c(""Advice from Doctors"", ""Advice from Scientists"", ""Advice from Anti-Vaccine Propagators"", ""Advice from Politicians"", \r\n                             ""Advice from Family"", ""Advice from Friends"", ""Advice from Journalists"", ""Advice from Celebrities"",\r\n                             ""Age"", ""Female"", ""University"", ""City Category by Size"", ""Smoking"", ""Chronic Illness"", ""Covid Previously"",\r\n                             ""Serious Covid Previously""),\r\n          omit.stat=c(""LL"",""ser"",""f""), ci=F,  single.row=F)\r\n\r\n# Standardized table\r\nst_pf=lm(data=sc_data_reg, pf_deny ~ VaccineDoctor + VaccineScientist + VaccineSceptical + VaccinePolitician + VaccineFamily + VaccineFriends + VaccineJournalist + VaccineCelebrity +\r\n           Age + IsFemale + hedu + CitySize \r\n         #+ WealthPreCovid \r\n         + Smoking + ChronicIll \r\n         + Covid + SerCovid\r\n)\r\nst_mo=lm(data=sc_data_reg, mo_deny ~ VaccineDoctor + VaccineScientist + VaccineSceptical + VaccinePolitician + VaccineFamily + VaccineFriends + VaccineJournalist + VaccineCelebrity +\r\n           Age + IsFemale + hedu + CitySize \r\n         #+ WealthPreCovid \r\n         + Smoking + ChronicIll \r\n         + Covid + SerCovid\r\n)\r\nst_as=lm(data=sc_data_reg, as_deny ~ VaccineDoctor + VaccineScientist + VaccineSceptical + VaccinePolitician + VaccineFamily + VaccineFriends + VaccineJournalist ', 'library(foreign)\r\nlibrary(sandwich)\r\n#library(tidyverse)\r\n#library(caret)\r\n\r\n### 1. PREPARE DATA TABLE\r\ndataCore=read.table(""MASZK_dataCore.csv"", sep="","", header = T)\r\n\r\n#not=dataCore[dataCore$Vaccinated==0,c(""Id"",""PfizerRating"", ""ModernaRating"", ""AstraRating"", ""SputRating"", ""SinoRating"")]\r\nnot=dataCore[,c(""Id"",""PfizerRating"", ""ModernaRating"", ""AstraRating"", ""SputRating"", ""SinoRating"")]\r\n\r\n\r\nnot[is.na(not)]=0\r\nnot$s=not$PfizerRating+ not$ModernaRating + not$AstraRating + not$SputRating + not$SinoRating\r\n\r\nnot2=not[not$s>0,]\r\n\r\nnot2=not2[,2:6]\r\n\r\n\r\n\r\n# Acceptance by the number of vaccines taken out\r\n\r\nacc=not2\r\nacc[acc<2]=0\r\nacc[acc>0]=1\r\nacc$s=acc$PfizerRating+ acc$ModernaRating + acc$AstraRating + acc$SputRating + acc$SinoRating\r\n\r\ntable(acc$s)\r\n\r\nacc1=acc[acc$s==1,]\r\n\r\n\r\nlibrary(reshape2)\r\n\r\n# co-acceptance matrix\r\nacc_net=acc[acc$s>1 & acc$s<4,1:5]\r\n\r\nnames(acc_net)=c(""Pfizer"", ""Moderna"", ""AstraZeneca"", ""Sputnik"", ""Sinopharm"")\r\n\r\npflinks=melt(acc_net, id.vars=c(""Pfizer""))\r\npflinks=pflinks[pflinks$Pfizer!=0,]\r\npflinks=aggregate(pflinks$value, by=list(pflinks$variable), FUN=sum)\r\npflinks$From=""Pfizer""\r\npflinks=pflinks[,c(3,1,2)]\r\nnames(pflinks)=c(""From"", ""To"", ""weight"")\r\n\r\nmolinks=melt(acc_net, id.vars=c(""Moderna""))\r\nmolinks=molinks[molinks$Moderna!=0,]\r\nmolinks=aggregate(molinks$value, by=list(molinks$variable), FUN=sum)\r\nmolinks$From=""Moderna""\r\nmolinks=molinks[,c(3,1,2)]\r\nnames(molinks)=c(""From"", ""To"", ""weight"")\r\n\r\naslinks=melt(acc_net, id.vars=c(""AstraZeneca""))\r\naslinks=aslinks[aslinks$AstraZeneca!=0,]\r\naslinks=aggregate(aslinks$value, by=list(aslinks$variable), FUN=sum)\r\naslinks$From=""AstraZeneca""\r\naslinks=aslinks[,c(3,1,2)]\r\nnames(aslinks)=c(""From"", ""To"", ""weight"")\r\n\r\nsplinks=melt(acc_net, id.vars=c(""Sputnik""))\r\nsplinks=splinks[splinks$Sputnik!=0,]\r\nsplinks=aggregate(splinks$value, by=list(splinks$variable), FUN=sum)\r\nsplinks$From=""Sputnik""\r\nsplinks=splinks[,c(3,1,2)]\r\nnames(splinks)=c(""From"", ""To"", ""weight"")\r\n\r\nsilinks=melt(acc_net, id.vars=c(""Sinopharm""))\r\nsilinks=silinks[silinks$Sinopharm!=0,]\r\nsilinks=aggregate(silinks$value, by=list(silinks$variable), FUN=sum)\r\nsilinks$From=""Sinopharm""\r\nsilinks=silinks[,c(3,1,2)]\r\nnames(silinks)=c(""From"", ""To"", ""weight"")\r\n\r\n\r\nacc_links=rbind(pflinks, molinks, aslinks, splinks, silinks)\r\n\r\nlibrary(igraph)\r\nacc_g=graph_from_data_frame(acc_links, directed = F)\r\n\r\nacc_g=simplify(acc_g, remove.multiple = T, edge.attr.comb = ""min"")\r\n\r\ncoord=layout_(acc_g, in_circle())\r\n\r\npng(""Acceptance on non-vaccinated.png"", width=500, height=500)\r\nplot(acc_g, layout=coord, edge.width=E(acc_g)$weight/2.5, edge.label=E(acc_g)$weight, edge.color=""gray90"", \r\n     edge.label.cex=2, vertex.label.cex=2)\r\ndev.off()\r\n']",1,"data, code, paper, hesitancy, mitigated, free choice, COVID-19, vaccine types, manuscript, reproduce results, folder"
Cost-effectiveness modeling for neuropathic pain treatments: investigating the relative importance of parameters using an open-source model,"A health economic model developed by NICE to assess the cost effectiveness of alternative pharmaceutical treatments for neuropathic pain was replicated using R and used to compare a hypothetical neuropathic pain drug to pregabalin. Model parameters were sourced from NICEs clinical guideline and associated with probability distributions to account for underlying uncertainty. A simulation-based scenario analysis was conducted to assess how uncertainty in efficacy and adverse events affected the net monetary benefit for the hypothetical treatment at a cost-effectiveness threshold of 20,000 per QALY.","['######################\n# SAMPLING FUNCTIONS #\n######################\nset.seed(seed)\n\n#Intermediate functions to calculate alpha and beta parameters\nrbeta_alpha <- function(mean, se){\n  alpha = ((1-mean)/se^2-1/mean)*mean^2\n  return(alpha)\n}\n\nrbeta_beta <- function(mean, se){\n  beta = rbeta_alpha(mean,se) * (1/mean-1)\n  return(beta)\n}\n\nrdirichlet = function(n, alpha) {\n  k = length(alpha)\n  r = matrix(0, nrow=n, ncol=k) \n  for (i in 1:k) {\n    r[,i] = rgamma(n, alpha[i], 1)\n  }\n  r = matrix(mapply(function(r, s) {return (r/s)}, r, rowSums(r)), ncol=k)\n  return (r)\n}\n\n\n#######################\n# SIMULATION FUNCTION #\n#######################\n\n# use for scenario analysis\nscenario_simulations <- function(efficacy_scale, AEs_scale, mode = ""default"") {\n  \n  #####\n  #SIMULATION PARAMETERS \n  #####\n  \n  #Efficacy of productx\n  \n  #define the efficacy matrix for productX according to the scale factors defined above\n  productx_efficacy <- preg_efficacy[2:3] * efficacy_scale\n  alpha <- c(100 - sum(productx_efficacy), productx_efficacy)\n  efficacy_productx <- rdirichlet(nsim, alpha)\n  \n  #check that efficacy sums to 1 \n  if (sum(rowSums(efficacy_productx)) != nsim) {stop(""Efficacy does not sum to 1"")}\n  \n  #AEs of productx\n  AEs_productx <- t(t(AEs_preg_matrix) * AEs_scale)\n  \n  #####\n  #RESULTS\n  #####\n  \n  #QALYS OF PRODUCTX\n  #calculates qalys of productx patients that do not withdraw from treatment\n  productx_utils_nowithdraw <- (1-AEs_productx[ ,1])*yearproportion_treatmentduration\n  #calculates qalys of productx patients that experience AEs\n  productx_utils_AEdecrements <- (AEs_productx[ ,2]*yearproportion_dizziness*aes_utils[ ,2])+(AEs_productx[ ,3]*yearproportion_nausea*aes_utils[ ,3])\n  #calculates qalys of productx patients withdrawing from treatment\n  productx_utils_withdraw_first4weeks <- AEs_productx[ ,1]*aes_utils[ ,1]*yearproportion_withdraw\n  productx_utils_withdraw_rem16weeks <- AEs_productx[ ,1]*(yearproportion_treatmentduration-yearproportion_withdraw)\n  #calculates the total QALYs of productx patients <30% pain relief \n  productx_totalqalys_30 <- ((productx_utils_withdraw_first4weeks + productx_utils_nowithdraw)*efficacy_utils[ ,1] + productx_utils_withdraw_rem16weeks*efficacy_utils[ ,1]+ productx_utils_AEdecrements)*efficacy_productx[ ,1]\n  #calculates the total QALYs of productx patients 30% to 49% pain relief\n  productx_totalqalys_30to49 <- ((productx_utils_withdraw_first4weeks + productx_utils_nowithdraw)*efficacy_utils[ ,2] + productx_utils_withdraw_rem16weeks*efficacy_utils[ ,1]+ productx_utils_AEdecrements)*efficacy_productx[ ,2]\n  #calculates the total QALYs of productx patients >50% pain relief\n  productx_totalqalys_over50 <- ((productx_utils_withdraw_first4weeks + productx_utils_nowithdraw)*efficacy_utils[ ,3] + productx_utils_withdraw_rem16weeks*efficacy_utils[ ,1]+ productx_utils_AEdecrements)*efficacy_productx[ ,3]\n  #calculates the total QALYs of productx\n  productx_totalqalys <- productx_totalqalys_30 + productx_totalqalys_30to49 + productx_totalqalys_over50\n\n  \n  #COSTS OF PRODUCTX\n  #cost of AEs productx\n  productx_AEs_cost_total <- rowSums(AE_costs_total * AEs_productx)\n  #drug cost product x\n  productx_drug_cost_total <- productx_cost*(1-AEs_productx[ ,1])+AEs_productx[ ,1]*productx_cost_withdraw\n  #total productx cost \n  productx_cost_total <- productx_drug_cost_total + productx_AEs_cost_total\n  \n  #####\n  #SIMULATION RESULTS\n  #####\n  \n  #store sim_results in a matrix\n  sim_results <- matrix(data = NA, nrow = nsim, ncol = 4)\n  sim_results <- cbind(preg_cost_total, productx_cost_total, preg_totalqalys, productx_totalqalys)  \n  # A big matrix with the simulations for all the ""parameters""\n#  mat = cbind(AEs_productx,productx_utils_nowithdraw,productx_utils_AEdecrements,\n#\t\tproductx_utils_withdraw_first4weeks,productx_utils_withdraw_rem16weeks,productx_totalqalys_30,\n#\t\tproductx_totalqalys_30to49,productx_totalqalys_over50,productx_AEs_cost_total) \n  mat = cbind(AEs_productx,aes_utils,efficacy_utils,AE_costs_total,efficacy_preg_matrix)\n  colnames(mat) = c(""Withdrawal"",""Dizziness"",""Nausea"",""Utility withdrawal"",""Utility dizziness"",""Utility nausea"",\n\t\t""Utility pain relief (<30%)"",""Utility pain relief (30-49%)"",""Utility pain relief (>50%)"",\n\t\t""Cost GP withdrawal"",""Cost GP dizziness"",""Cost GP nausea"",\n\t\t""Efficacy Preg (<30%)"",""Efficacy Preg (30-49%)"",""Efficacy Preg (>50%)"")\n  if (mode == ""dsa"") {\n    list(sim_results=colMeans(sim_results, na.rm = TRUE),mat=mat) \n  } else {\n    list(sim_results=sim_results,mat=mat)\n  }\n}  \n\n\n###############################\n# PRICE OPTIMIZATION FUNCTION #\n###############################\n\nprice_optimization <- function(preg_cost_total, incremental_qalys, AEs_scale){\n  \n  # calculate the cost vectors for finding the EJP\n  drug_cost_multipliers <- seq(0, 5, by = 0.01)\n  productx_cost_opt <- preg_cost * drug_cost_multipliers\n  productx_cost_withdraw_opt <- productx_cost_opt * (treatmentwithdraw_time_weeks / treatmentduration_time_weeks)\n  \n  # determine the drug', ""####################\n# GLOBAL VARIABLES #\n####################\n\nset.seed(seed)\n\n#####\n#MODEL DURATION\n#####\n\n#the model duration is 20 weeks\n#treatment duration (model time horizon)\ntreatmentduration_time_weeks <- 20\n#Withdrawal of the drug due to adverse events occurs at 4 weeks\n#time of withdrawal due to AEs\ntreatmentwithdraw_time_weeks <- 4\n#proportion of a year (used to calculate QALYs)\nyearproportion_treatmentduration <- treatmentduration_time_weeks / (365.25/7)\nyearproportion_withdraw <- treatmentwithdraw_time_weeks / (365.25/7)\n\n#####\n#EFFICACY\n#####\n\n#EFFICACY OF PREGABALIN obtained from NICE CG173\n#efficacy determined by a reduction in pain relief of <30%, 30-49%, =50%\n#NICE CG173 used random draws from the posterior distribution of the NMA for efficacy parameters\n#as no information was available on the posterior distribution or the covariance between efficacy \n#variables, a dirichlet distribution was assumed for efficacy sampling\npreg_efficacy <- c(43, 16, 41) \nefficacy_preg_matrix <- rdirichlet(nsim, preg_efficacy)\ncolnames(efficacy_preg_matrix, do.NULL = FALSE)\ncolnames(efficacy_preg_matrix) <- c('<30','30-49','50andover')\n\n#ADVERSE EVENTS (AEs) of pregabalin -- withd: withdawal; dizzi: dizziness; nause: nausea.\n#In line with the nice report, the AEs are not modelled as mutually exclusive\n#as the posterior distribution was not provided in the NICE CG173, a beta distribution was assumed for AEs\nae_withd <- rbeta(n=nsim,rbeta_alpha(0.19,(0.26-0.13)/(2*qnorm(0.975))),rbeta_beta(0.19,(0.26-0.13)/(2*qnorm(0.975))))\nae_dizzi <- rbeta(n=nsim,rbeta_alpha(0.36,(0.51-0.24)/(2*qnorm(0.975))),rbeta_beta(0.36,(0.51-0.24)/(2*qnorm(0.975))))\nae_nause <- rbeta(n=nsim,rbeta_alpha(0.12,(0.23-0.05)/(2*qnorm(0.975))),rbeta_beta(0.12,(0.23-0.05)/(2*qnorm(0.975))))\nAEs_preg_matrix <- cbind(ae_withd,ae_dizzi,ae_nause)\ncolnames(AEs_preg_matrix, do.NULL = FALSE)\ncolnames(AEs_preg_matrix) <- c('wd','dizziness','naus')\n\n#####\n#DRUG COSTS\n#####\n\n#20 week pregabalin cost from NICE CG173\npreg_cost <- 322.00\n#inflation indices obtained from Curtis 2014\n#inflation index 2013/2014: 290.5\n#inflation index 2012/2013: 287.3\ninflation_index <- 290.5/287.3\n#preg cost\npreg_cost <- preg_cost * inflation_index\n#productx cost\nproductx_cost <- preg_cost * drug_cost_scale\n\n#cost of pregabalin when patients withdraw at 4 weeks\npreg_cost_withdraw <- preg_cost * (treatmentwithdraw_time_weeks / treatmentduration_time_weeks )\n#cost of productx when patients withdraw at 4 weeks\nproductx_cost_withdraw <- productx_cost * (treatmentwithdraw_time_weeks / treatmentduration_time_weeks)\n\n#####\n#ADVERSE EVENTS and RESOURCE USE\n#####\n\n#AES RESOURCE USE AND COSTS OF pregbalin\n#duration of AEs; nausea and dizziness (assumed withdrawal = 0)\n#randomize duration of nausea\nnauseaduration_days <- runif(n=nsim, min=7, max=14)\n#randomize duration of dizziness\ndizzinessduration_days <- runif(n=nsim, min=7, max=14)\n#calculates proportion of a year that AEs apply\nyearproportion_nausea <- nauseaduration_days / 365.25\nyearproportion_dizziness <- dizzinessduration_days /365.25\n\nAE_duration <- cbind(0,dizzinessduration_days,nauseaduration_days)\ncolnames(AE_duration, do.NULL = FALSE)\ncolnames(AE_duration) <- c('AE days wd','AE days dizziness', 'AE days nausea')\n\n#COSTS OF AES\n#GP Cost obtained from CG173\nGPvisit_cost_pervisit <- c(63,63,63)\n#inflating GP costs using Curtis 2014\n#GP costs were sourced from PSSRU 2012 and had to be first inflated to 2013 levels\nGPvisit_cost_pervisit <- GPvisit_cost_pervisit * inflation_index\nnames(GPvisit_cost_pervisit) <- c('GP visit costs wd','GP visit costs dizziness','GP visit costs nausea')\n#AE_drugcosts_day refers to cyclizine hydrochloride used to treat nausea--cost obtained from NICE CG173\nAE_drugcosts_day <- c(0,0,0.4407)\n#inflating drug costs using Curtis 2014\nAE_drugcosts_day <- AE_drugcosts_day * inflation_index\nnames(AE_drugcosts_day) <- c('AE drug cost wd','AE drug cost dizziness','AE drug cost nausea')\n\n#RESOURCE USE OF AES\n#randomize the number of gp visits using distributions from NICE CG173\ngp_wd <- runif(n=nsim, min=2, max=4)\ngp_dizziness <- runif(n=nsim, min=1, max=2)\ngp_nausea <- runif(n=nsim, min=1, max=2)\nGPvisits_total <- cbind(gp_wd,gp_dizziness,gp_nausea)\ncolnames(GPvisits_total, do.NULL = FALSE)\ncolnames(GPvisits_total) <- c('GP wd','GP dizziness','GP nausea')\n\n#GP costs over the 20 week period \nGP_costs_total <- t(t(GPvisits_total) * GPvisit_cost_pervisit) \n#drug costs due to AEs over 20 week period\nAE_drugcost_total <- t(t(AE_duration) * AE_drugcosts_day)\n#TOTAL AE COSTS\nAE_costs_total <- GP_costs_total + AE_drugcost_total\n\n#####\n#UTILITIES\n#####\n\n#utilities of pain relief of <30%, 30-49%, =50% obtained from NICE CG173\nutil_eff1=rbeta(n=nsim,rbeta_alpha(0.16,0.036),rbeta_beta(0.16,0.036))\nutil_eff2=rbeta(n=nsim,rbeta_alpha(0.46,0.015),rbeta_beta(0.46,0.015))\nutil_eff3=rbeta(n=nsim,rbeta_alpha(0.67,0.015),rbeta_beta(0.67,0.015))\nefficacy_utils <- cbind(util_eff1,util_eff2,util_eff3)\n\n#utilities associated with "", 'rm(list = ls())\nlibrary(BCEA)\nlibrary(ggplot2)\n\n# Sets BASIC parameters for the simulation exercise\n#set the increment value \nincr <- 0.05\n#set ICER threshold\nICER_threshold <- 20000\n#set number of simulations\nnsim <- 10000\n# efficacy of producx calculated as a scale benefit of pregabalin\nefficacy_scale <- c(0.9,0.9)\nnames(efficacy_scale) <- c(\'30-49\',\'50 and over\')\n# adverse events of productx calculated as a scale benefit of pregbalin \nAEs_scale <- c(0.6,0.6,0.6)\nnames(AEs_scale) <- c(\'wd\',\'dizziness\',\'naus\')\n# cost scale to calculate productx cost as a multiple of pregabalin cost in the base case\ndrug_cost_scale <- 1\n\n## Loads data & functions\n#insure that results are reproducible\nseed=76\nsource(""functions.R"")\nsource(""global_variables.R"")\n\n### Runs the analysis for the scenarios in the paper\nms1=make.scenario(c(.9,.9),c(.6,.6,.6))\nms2=make.scenario(c(1,1),c(.8,.8,.8))\nms3=make.scenario(c(1.1,1.1),c(1,1,1))\nms4=make.scenario(c(1.2,1.2),c(1.2,1.2,1.2))\nms5=make.scenario(c(1.3,1.3),c(1.4,1.4,1.4))\n\n# Creates table 2 (differences due to rounding and simulation error):\ntab2=rbind(ms1$tab,ms2$tab,ms3$tab,ms4$tab,ms5$tab)\nrownames(tab2) = paste0(""Scenario"",1:5)\n\n# Makes figure II\nrg = range(ms1$m$eib,ms2$m$eib,ms3$m$eib,ms4$m$eib,ms5$m$eib)\npng(file=""../EIB.png"",width=800,height=800,res=100)\nplot(ms1$m$k,ms1$m$eib,ylim=c(rg[1],550),t=""l"",ylab=""Net Monetary Benefit"",xlab=""Cost-effectiveness threshold (GBP)"",\n     axes=F,xlim=c(0,30000),main=""Net Monetary Benefit of Product X vs pregabalin"")\nrect(20000,-200,30000,1000,col=adjustcolor(""light grey"",.2),border=NA)\naxis(1); axis(2,at=seq(-100,800,100))\nabline(h=0,col=""dark grey"")\ntext(20000,-50,""Scenario 1"",cex=.8,pos=4)\npoints(ms1$m$k,ms2$m$eib,ylim=rg,t=""l"")\ntext(14000,60,""Scenario 2"",cex=.8,pos=1)\npoints(ms1$m$k,ms3$m$eib,ylim=rg,t=""l"")\ntext(16000,130,""Scenario 3"",cex=.8,pos=1)\npoints(ms1$m$k,ms4$m$eib,ylim=rg,t=""l"")\ntext(16800,200,""Scenario 4"",cex=.8,pos=1)\npoints(ms1$m$k,ms5$m$eib,ylim=rg,t=""l"")\ntext(17600,270,""Scenario 5"",cex=.8,pos=2)\n# These are the values for the EIB in Table 2\npoints(20000,ms1$tab[7],pch=20,cex=.7)\npoints(20000,ms2$tab[7],pch=20,cex=.7)\npoints(20000,ms3$tab[7],pch=20,cex=.7)\npoints(20000,ms4$tab[7],pch=20,cex=.7)\npoints(20000,ms5$tab[7],pch=20,cex=.7)\ndev.off()\n\n\n']",1,"cost-effectiveness modeling, neuropathic pain treatments, open-source model, NICE, pharmaceutical treatments, R, simulation-based scenario analysis, efficacy, adverse events, net monetary benefit, QALY, cost-effectiveness threshold, probability distributions, clinical"
Modelling interactions between farmer practices and fattening pig performances with an individual-based model,"The fattening unit model is a discrete-event mechanistic model with stochastic biological traits (pig feed intake, growth potential and risk of mortality) and a one-day time step. The pig fattening system is composed of pig herd, farm management and farm infrastructure. The pig herd is divided into successive batches of pigs of the same age which are reared in the same room from the beginning of the fattening period until shipping to the slaughterhouse. Each pig is represented using an individual-based model adapted from the InraPorc model (van Milgen et al., 2008). The InraPorc model simulates feed intake, body protein and lipid depositions, and the resulting growth and nutrient excretion of each pig. Each pig is attributed a profile which includes an initial body weight (BW70), the mean protein deposition (PDm), a shape parameter of the protein deposition function (BGompertz), and two parameters describing its feed intake (a, b), set to generate the appropriate structure of a pig herd according to Vautier et al. (2013). Farm management is represented by farmers practices and a calendar of events containing tasks to perform. At each time step of the simulation, the events corresponding to the current day are read in the calendar and processed. The practices include batch management, allocation of pigs to pens, feeding practices and slaughter shipping practices. Farm infrastructure is represented by a number of fattening rooms, each with a number of pens of a given size, which are provided as input parameters to the model. A buffer room can be used at the end of the fattening period to extend the fattening period of the lightest pigs which have not reached the minimum slaughter weight without economic penalties. Once the last pigs of a batch are moved to the slaughterhouse or to a buffer room, the fattening room is considered empty after a disinfection period and ready for a new batch. The model calculates technical, economic and environmental results for each fattening pig and globally for the unit. Environmental impacts of each slaughtered pig are estimated using LCA, taking into account impacts from the extraction of raw materials to the farm gate. More detailed description of the model and the LCA performed by the model can be found in Cadero et al. (2017).","['Chemin=readline(prompt = ""Please indicate the path of your working directory: "")\r\nsetwd(Chemin)\r\n\r\nsource(paste(Chemin,""/fonction_Animal.R"", sep =""""))\r\n\r\n\r\nlistparam = c(""choix_plan"", ""choix_seq"")\r\n\r\nmydata <- read.table(paste(Chemin,""/fichier_de_parametres_defaut.csv"", sep =""""), sep ="";"", quote = ""\\"""",\r\n                     header = TRUE, stringsAsFactors = FALSE)\r\n\r\nvecteur = mydata$Nom%in%listparam\r\n\r\n\r\n# CREER LA MATRICE DES COMBINAISONS D\'ENTREES / SCENARIOS --> expand.grid\r\n\r\nmat.param = expand.grid(c(""Ad libitum"", ""Restriction a 2.5 kg/j""), c(""Bi-phase AB"",""Multiphase en 10""))\r\ncolnames(mat.param)= listparam\r\n\r\nid=1\r\nsystem.time(list.result1 <- FUNCANIMAL(id, mat.param, vecteur, mydata, listparam, Chemin))\r\n\r\nid=2\r\nsystem.time(list.result2 <- FUNCANIMAL(id, mat.param, vecteur, mydata, listparam, Chemin))\r\n\r\nid=3\r\nsystem.time(list.result3 <- FUNCANIMAL(id, mat.param, vecteur, mydata, listparam, Chemin))\r\n\r\nid=4\r\nsystem.time(list.result4 <- FUNCANIMAL(id, mat.param, vecteur, mydata, listparam, Chemin))\r\n\r\nmat.result = rbind(list.result1, list.result2, list.result3, list.result4)\r\n\r\ncolnames(mat.result)=c(""Simu"", ""CC"", ""CC_F"", ""CC_H"", ""CC_M"",\r\n                       ""AC"", ""AC_F"", ""AC_H"", ""AC_M"",\r\n                       ""EU"", ""EU_F"", ""EU_H"", ""EU_M"",\r\n                       ""CED"", ""CED_F"", ""CED_H"", ""CED_M"",\r\n                       ""Land"", ""Land_F"", ""Land_H"", ""Land_M"", \r\n                       ""CC_Eng"", ""CC_F_Eng"", ""CC_H_Eng"", ""CC_M_Eng"",\r\n                       ""AC_Eng"", ""AC_F_Eng"", ""AC_H_Eng"", ""AC_M_Eng"",\r\n                       ""EU_Eng"", ""EU_F_Eng"", ""EU_H_Eng"", ""EU_M_Eng"",\r\n                       ""CED_Eng"", ""CED_F_Eng"", ""CED_H_Eng"", ""CED_M_Eng"",\r\n                       ""Land_Eng"", ""Land_F_Eng"", ""Land_H_Eng"", ""Land_M_Eng"",\r\n                       ""prixvente"", ""produit"",\r\n                       ""marge_Eng_reelle"", ""marge_Eng_estimee"",\r\n                       ""CoutAlimTot"", ""CoutAlim_Eng"", \r\n                       ""IC_Eng"", ""RejeteN_Eng"", ""RejeteP_Eng"",\r\n                       ""MOexc_Eng"", ""Resdig_Eng"",\r\n                       ""PVFin"", ""AgeFin"", ""Gamme"",\r\n                       ""Coeur_gamme"", ""Lourds"", ""Legers"", ""Perte"")\r\n\r\n\r\n\r\nmat.tot = cbind(mat.param,mat.result)\r\nsave(\'mat.tot\', file = \'mat.tot_Animal.RData\')\r\n\r\nwrite.table(mat.tot,""mat_tot_Animal.csv"",sep="";"", col.names = TRUE,\r\n            row.names = FALSE)\r\n\r\n', 'FUNCANIMAL <- function(id, mat, vecteur, mydata, listparam, Chemin){\r\n  \r\n  \r\n  tablo = mydata\r\n  j=1\r\n  for (i in (1:length(vecteur))){\r\n    if (vecteur[i]==TRUE){\r\n      tablo[i, ""Valeur""] = paste(rep(mat[id,listparam[j]], tablo[i,""nbcols""]), collapse="" "")\r\n      j = j+1\r\n    }\r\n  }\r\n\r\n  write.table(tablo,""fichier_entrees.csv"",sep="";"", col.names = TRUE,\r\n              row.names = FALSE)\r\n  \r\n\r\n  shell(""C:/Users/acadero/AppData/Local/Programs/Python/Python36/python.exe Main_python.py"") # Windows version\r\n  # system(""python3.4 Main_python.py"")               # Linux version\r\n  \r\n  performances <- read.table(""Performances_bilan.txt"", sep ="""", header = TRUE)\r\n  Aliment_cout <- read.table(""Aliment_cout.txt"", sep ="""", header = TRUE)\r\n  \r\n\r\n\r\n# Indicateurs economiques ---------------------------------------------------  \r\n\r\n  tauxCC_fr=0.765       # rendement carcasse froid\r\n  tauxCC_ch=0.79        # rendement carcasse chaud\r\n  \r\n  Coeffcor_TMP = 61.082/57.2648277921286\r\n  #   La référence GTE 2015 la plus proche des caractéristiques de ta \r\n  #   simulation est celle des élevages naisseurs engraisseurs conduits en 7 \r\n  #   bandes (toutes les 3 semaines). Si on prend les 213 élevages correspondant \r\n  #   au tiers supérieur de cette référence (les 33% meilleurs élevages triés selon la marge),\r\n  #   on se rapproche des caractéristiques de ta simulation (moyenne de 232 truies présentes, engraissement entre 31 et 120kg avec GMQ \r\n  #   observé de 826g/j).\r\n  #   On a TMP=61.082 et écart type 0,752.\r\n  #   Sachant comme on le disait qu\'une majorité des élevages allote au poids, \r\n  #   distribue un aliment biphase et rationne en engraissement (même si on ne \r\n  #   gère pas ces codifications en GTE).\r\n  \r\n  performances$TMPcor= performances$TMP*Coeffcor_TMP\r\n  \r\n  GDP1 <- read.csv(paste(Chemin,""/GDP_2015.csv"", sep =""""), header=TRUE, sep="";"")\r\n  \r\n  performances$plusvalue<-0\r\n  for (j in 3:17) {\r\n    for (i in 3:15) {\r\n      if (nrow(performances[(performances$TMPcor>GDP1$TMPmin[j])&\r\n                            (performances$TMPcor<=GDP1$TMPmax[j])&\r\n                            (performances$PVFin>(as.numeric(GDP1[1,i])/tauxCC_ch))&\r\n                            (performances$PVFin<=(as.numeric(GDP1[2,i])/tauxCC_ch)),])>0) {  \r\n        \r\n        performances[(performances$TMPcor>GDP1$TMPmin[j])&\r\n                       (performances$TMPcor<=GDP1$TMPmax[j])&\r\n                       (performances$PVFin>(as.numeric(GDP1[1,i])/tauxCC_ch))&\r\n                       (performances$PVFin<=(as.numeric(GDP1[2,i])/tauxCC_ch)),]$plusvalue<-GDP1[j,i]\r\n      }\r\n    }\r\n  }\r\n  \r\n  performances$plusvalue[performances$Depart==""mort""]=0\r\n  \r\n  \r\n  # Creation colonnes des indicateurs economiques\r\n  prixbase=1.314                     # euros/kg en f?vrier 2014\r\n  \r\n  performances$prixvente <- ifelse (performances$Depart==""mort"",\r\n                                    0,\r\n                                    performances$plusvalue+prixbase+0.02)\r\n  # euros/ kg carcasse\r\n  # 0.02 plus-value tracabilit?\r\n  \r\n  performances$produit<- (performances$prixvente*performances$PVFin*tauxCC_fr)   # euros/ carcasse de porc\r\n  \r\n  performances$marge_Eng <- (performances$produit - (2.35*25 + (performances$PVInit-25)*0.67)- performances$CoutAlimTot)\r\n  # euros/ porc\r\n  # Seules les charges alimentaires en engraissement \r\n  # et le prix d\'achat du porcelet à la fin du post-sevrage sont déduits \r\n  # Cout de revient du porcelet de 25 kg en fevrier 2014 : 2,35 €/kg\r\n  # Cout du kg vif supplémentaire : 0,67 €/kg\r\n  \r\n# Indicateurs environnementaux ----------------------------------------------  \r\n\r\n  \r\n#----------------------------------------------------------------------------\r\n  #FATTENING\r\n#----------------------------------------------------------------------------\r\n  \r\n  \r\n  # N COMPOUNDS\r\n  \r\n  # Housing Emissions (kg manure N ex-animal *emission factor) \r\n  \r\n  performances$N_NH3EmitH= (performances$RejeteN / 1000) * 0.24          # kg N-NH3 / pig\r\n  # Rigolot et al.(2010b): EF is 0.24 kg N-NH3/kg N excreted\r\n  \r\n  performances$N_N2OEmitH= (performances$RejeteN / 1000) * 0.002         # kg N-N2O / pig\r\n  # IPCC (2006): TABLE 10.21-Pit storage bellow animal confinements\r\n  \r\n  performances$N_NOxEmitH= (performances$RejeteN / 1000) * 0.002         # kg N-NOx / pig\r\n  # Dammgen & Hutchings cited by IPCC (2006)\r\n  \r\n  performances$N_NO3EmitH=(performances$RejeteN / 1000) * 0              # kg N-NO3 / pig\r\n  \r\n  # N leaving after housing emissions (kg N / pig)\r\n  performances$N_St = (performances$RejeteN/1000 - performances$N_NH3EmitH \r\n                       - performances$N_N2OEmitH - performances$N_NOxEmitH)\r\n  \r\n  \r\n  # Storage Emissions (kg manure N ex-housing *emission factor) \r\n  \r\n  performances$N_NH3EmitSt= performances$N_St * 0.1                      # kg N-NH3 / pig\r\n  # Rigolot et al.(2010b): Table 4\r\n  \r\n  performances$N_N2OEmitSt= performances$N_St * 0.001                    # kg N-N2O / pig\r\n  # IPCC (2006)\r\n  \r\n  performances$N_NOxEmitSt= pe']",1,"Modelling, interactions, farmer practices, fattening pig performances, individual-based model, discrete-event mechanistic model, stochastic biological traits, pig feed intake, growth potential, risk of mortality, pig fattening system, pig herd, farm management, farm"
Survey on Internet attitudes,"The dataset on Alternative Internet survey is part of the research on Alternative Internets carried out in the project netCommons: Network Infrastructure as Commons H2020 EU project (Grant Number 688768, project URL: http://netcommons.eu).The netCommons project aspires to study, support and further promote an emerging trend, community-based networking and communication services that can offer a complement, or even a sustainable alternative, to the global Internets current dominant model. Community networks not only provide citizens with access to a neutral, bottom-up network infrastructure, which naturally increases the transparency of data flow, but they also represent an archetype of networked collective cooperation and action, mixing common or communal ownership and management of an infrastructure with a balanced set of services supported by the local stakeholders. Community networks, however, are complex systems that require multiple skills to thrive: technical, legal, socio-economic, and political. They face many challenges and they also need abstractions, models and practical tools to grow and produce a higher beneficial impact on our society.The Alternative Internet survey examines the concerns about internet usage that can be identified among sufficiently competent and regular internet users. Such concerns provide useful input both to community networks and to other stakeholders such as policy-makers and regulators who play significant roles in the telecommunications and internet landscape and, consequently, need to take informed steps as to the regulation of the internet and the ways in which community networks can be part of this landscape. In addition, the survey results provide input as to the attitudes of those internet users regarding the possibility of using community networks.The online survey run on platform limesurvey, which is built on open source code, whilst it also presents convenient functionality, including the possibility of anonymisation of the user. It run for seven months (from June 2017 until 22 January 2018) and resulted in 1000 completed online questionnaires. Respondents were targeted through specific mailing lists and have included: academics/university researchers, university administrators, IT professionals in products and services, and students, while some clerical workers have also filled in the questionnaire. The selected groups were assumed to be both regular and competent internet users and therefore suitable for providing rich and relevant responses on Internet usage.The data presented are both quantitative and qualitative. Whilst the full results are included in the above mentioned file (Survey_357528_Survey_on_Internet_Attitudes.pdf ), we have also produced a number of graphs, as described in the relevant README file.The data are responses to different categories of questions.Question QA1 is the relevant consent form.Questions QB1, QB2, QB3, QB5 and QB6 are about some dimensions of the Internet usage of the respondent. QB8A asks the respondent to grade their skills on a number of different Internet uses.Question QC1A asks whether the user has experienced privacy violations, with certain options provided as answers. QC1B requests more details about the kind of privacy violation experienced.Questions QC2A, QC2B, QC2C, QC2D, QC2E address privacy concerns, while questions QC3A, QC3b, QC3C are about possible steps taken by the respondent to address such concerns. QC4 is about consideration of alternative platforms.Questions QC5, QC7, QC8 provide a measure of advertising and commercialisation concerns, while QC9 is about consideration of alternatives to address such concerns.Questions QC10A, QC11A, QC11B, QC12A, QC12B are on monopolies, and examine attitudes towards the dominant presence of an Internet service provider, social networking site, or search engine. QC13 is about consideration of alternatives to address monopoly concerns.Questions QC15, QC16, QC17, QC18 and QC18B relate to the theme of Internet governance and electronic democracy; more specifically on: taxation of large Internet corporations, equality of access and skills (digital divide), unequal online visibility on social networks, and access to online content. Questions QD1, QD2 and QD2B explicitly ask respondents to consider community networks as an alternative and also seeks to elicit their views as to the potential of such networks.Finally, questions QE1 to QE8 include demographics of the respondents, as well as certain attitudes that they might have towards life and society, which might be indicative of the likelihood to support community initiatives.The elaboration of the data available here (mainly the analysis of open, qualitative answers) has been published as netCommons Deliverable 5.4 ""Alternative Internet's Political Economy"" available at: https://www.netcommons.eu/?q=content/alternative-internets-political-economyThe quantitative answers are available through the netCommons website at: https://www.netcommons.eu/?q=surveyFor any information concerning the Survey, please contact Dimitris Boucas, email: D.Boucas(at)westminster.ac.uk and Maria Michalis, email: M.Michalis(at)westminster.ac.uk.For details about the quantitative data processing, please contact Renato Lo Cigno, web: http://disi.unitn.it/locigno, email: locigno(at)disi.unitn.it.","['###############\nlibrary(RColorBrewer)\nlibrary(gplots)\nlibrary(\'corrplot\')\n\nmake.dir <- function(fp) {\n  if(!file.exists(fp)) {  # If the folder does not exist, create a new one\n    dir.create(fp)\n  } else {   # If it existed, delete and replace with a new one  \n    unlink(fp, recursive = TRUE)\n    dir.create(fp)\n  }\n} \n\nwraplabels <- function(longnames, w=10) {\n  retval=c()\n  for (n in longnames) {\n    sn = paste(strwrap(n, width = w), collapse=""\\n"")\n    sn = substr(sn,0,30)\n    retval=c(retval, sn)\n  }\n  return(retval)\n}\n\ntrunclabs <- function(longnames, stopAt=30) {\n  retval=c()\n  for (n in longnames) {\n    sn=n\n    if (nchar(n)>stopAt) {\n      sn = paste(substr(n, 0, stopAt-3), ""..."")  \n    }\n    if(sn=="""")\n      sn=""N/A""\n    retval=c(retval, sn)\n  }\n  return(retval)\n}\n\n###############\n\n###  START  ###\nprint(""Importing data"")\ndata = read.csv(""selected-results.csv"", header = TRUE, sep="","", quote = ""\\"""", check.names=FALSE)\n\n### categorical ordinal answers, answers replaced by integer code to enable correlation analysis\ndataCORR = read.csv(""catordQUCODANSFULL_process.csv"", header = TRUE, sep = "","",\n stringsAsFactors = TRUE, quote = ""\\"""", check.names=FALSE)\n\npaste(""Discovered"", nrow(data), ""completed responses answering"", ncol(data), ""questions each"")\n\n### OUTPUT FOLDER  ###\nprint(""Creating output folder"")\nmake.dir(""out"")\nsetwd(""out"")\n\n#https://stackoverflow.com/questions/23913276/texture-in-barplot-for-7-bars-in-r\ncols = c(""firebrick2"", ""blue3"",""forestgreen"",""purple2"",""darkorange2"",""darkturquoise"",""cornsilk4"",""navy"")\n\n# questions\' strings manipulation\nqnames = names(data)\nfnames = make.names(qnames)\n\nfor (i in 1:length(data)) {\n  # again strings manipulation\n  fullqn = qnames[i]\n  codes = unlist(strsplit(fullqn, ""[.]""))[1]\n  qtitle = substr(fullqn, nchar(codes)+3, nchar(fullqn))\n  \n  seccod = substr(codes,0,2)\n  qcod = substr(codes,3,nchar(codes))\n\n  qn = substr(paste(strwrap(qtitle, width = 60), collapse = ""\\n""),0,160)\n  qn = """"\n  \n  fn = fnames[i]\n  fn = substr(gsub(""\\\\."", ""_"", fn),0,25)\n  \n  # Section folder\n  if(!file.exists(seccod))\n    dir.create(seccod)\n  \n  cdir = paste(seccod,""/"",qcod,sep="""")\n  make.dir(cdir) \n  setwd(cdir)\n  \n  toplot = sort(table(data[i]), decreasing = TRUE)\n  rn = rownames(toplot)\n  rn = trunclabs(rn,250)\n  mylabels = wraplabels(rn,10)\n  leglabs = trunclabs(rn,40)\n  \n  # Plot histograms with and without titles\n  for (dev in 1:2) {\n    \n    if(dev==1) {\n      pdf(paste(codes,""_h.pdf"",sep=""""))\n    }\n    if(dev==2) {\n      qn = qnames[i]\n      qn = substr(paste(strwrap(qn, width = 60), collapse =""\\n""),0,160)\n      pdf(paste(codes,""_hWT.pdf"",sep=""""))\n    }\n    \n    barplot(toplot/sum(toplot)*100, \n            las=2,\n            main=qn,\n            ylab = ""Percentage [%]"",\n            ylim=c(0,100),\n            col=cols,\n            legend = leglabs,\n            names.arg="""",\n            angle=c(45,0,90), density=seq(10,30,20)\n    )\n    dev.off()\n  }\n  setwd(""../.."")\n}\n  \n# Preparing folders for extra plots\nmake.dir(""extra"")\n\nsetwd(""./extra"")\nmake.dir(""corrplotMAT"")\nmake.dir(""corrplotUPPER"")\nmake.dir(""heatmap_rainbow"")\n\n### correlation diagrams\nM <- cor(dataCORR)\n\nfor (i in 1:2) {\n  \n  if (i %% 2) {\n    mytitle = ""Pearson Correlation Index""\n    myfname = ""corrplotMAT/corrplotMAT_WT.pdf""\n  } else {\n    mytitle = """"\n    myfname = ""corrplotMAT/corrplotMAT.pdf""\n  }\n  pdf(myfname)\n  par(xpd=TRUE)\n  corrplot(M, method = ""square"", col = colorRampPalette(c(""navyblue"",""white"",""red3""))(100),\n           tl.col = ""black"",title=mytitle, mar = c(2, 2, 2, 2), tl.cex=0.7)\n  dev.off()\n  \n  if (i %% 2) {\n    mytitle = ""Pearson Correlation Index""\n    myfname = ""corrplotUPPER/corrplotUPPER_WT.pdf""\n  } else {\n    mytitle = """"\n    myfname = ""corrplotUPPER/corrplotUPPER.pdf""\n  }\n  pdf(myfname)\n  corrplot(M, type = ""upper"", order = ""hclust"", tl.col = ""black"",\n           title=mytitle, tl.srt = 75, tl.cex=0.7, mar = c(2, 2, 2, 2))\n  dev.off()\n  \n  if (i %% 2) {\n    mytitle = ""Heatmap""\n    myfname = ""heatmap_rainbow/heatmap_rainbow_WT.pdf""\n  } else {\n    mytitle = """"\n    myfname = ""heatmap_rainbow/heatmap_rainbow.pdf""\n  }\n  pdf(myfname)\n  par(mar=c(7,4,4,2)+0.1) \n  heatmap.2(x=M, Rowv=NULL,Colv=""Rowv"",\n            main=mytitle,\n            cexRow=1,\n            cexCol=1.2,\n            col = rev(rainbow(20*10, start = 0/6, end = 4/6)), \n            scale=""none"",\n            margins=c(8,2.5), # (""margin.Y"", ""margin.X"")\n            trace=\'none\', \n            symkey=FALSE, \n            symbreaks=FALSE, \n            dendrogram=\'none\',\n            density.info=\'histogram\', \n            denscol=""black"",\n            keysize=1, \n            key.par=list(mar=c(3.5,0,3,0)),\n            lmat=rbind(c(5, 4, 2), c(6, 1, 3)), lhei=c(2.5, 5), lwid=c(1, 10, 1)\n  )\n  dev.off()\n  \n}']",1,"ative internet options and attitudes towards community networks. Other keywords include: netCommons project, community-based networking, telecommunications, policy-makers, open source code, anonymisation, quantitative and qualitative data, privacy violations, privacy concerns, alternative internet options."
Data from: Correlational selection on personality and social plasticity: morphology and social context determine behavioural effects on mating success,"Despite a central line of research aimed at quantifying relationships between mating success and sexually dimorphic traits (e.g., ornaments), individual variation in sexually selected traits often explains only a modest portion of the variation in mating success.Another line of research suggests that a significant portion of the variation in mating success observed in animal populations could be explained by correlational selection, where the fitness advantage of a given trait depends on other components of an individual's phenotype and/or its environment. We tested the hypothesis that interactions between multiple traits within an individual (phenotype dependence) or between an individual's phenotype and its social environment (context dependence) can select for individual differences in behaviour (i.e., personality) and social plasticity.To quantify the importance of phenotype- and context-dependent selection on mating success, we repeatedly measured the behaviour, social environment and mating success of about 300 male stream water striders, Aquarius remigis. Rather than explaining individual differences in long-term mating success, we instead quantified how the combination of a male's phenotype interacted with the immediate social context to explain variation in hour-by-hour mating decisions. We suggest that this analysis captures more of the mechanisms leading to differences in mating success.Males differed consistently in activity, aggressiveness and social plasticity. The mating advantage of these behavioural traits depended on male morphology and varied with the number of rival males in the pool, suggesting mechanisms selecting for consistent differences in behaviour and social plasticity. Accounting for phenotype and context dependence improved the amount of variation in male mating success we explained statistically by 30274%.Our analysis of the determinants of male mating success provides important insights into the evolutionary forces that shape phenotypic variation. In particular, our results suggest that sexual selection is likely to favour individual differences in behaviour, social plasticity (i.e., individuals adjusting their behaviour), niche preference (i.e., individuals dispersing to particular social conditions) or social niche construction (i.e., individuals modifying the social environment). The true effect of sexual traits can only be understood in interaction with the individual's phenotype and environment.","[""library(lme4)\n\nsource('scripts.r')\n\n############################################################\n####   FIGURE 1 variation in activity\n############################################################\ndev.new()\npar(mfrow = c(2,1), mar=c(3.5, 3.5, 0.5, 0.5))\n\nhist(striders.males.unmated$nbmales.c, freq = F, col = 'gray'\n\t, ylim = c(0, 0.20), axes = F, main = '', xlab = '', ylab = '')\naxis(side = 1, at = seq(0, 15, 3))\naxis(side = 2, at = seq(0, 0.20, .05), labels = seq(0, 20, 5), las=1)\nbox()\ntext(18.9, 0.195, 'N = 6746 Observations', pos = 2)\nmtext(side = 1, line = 1.8, 'Number of unmated males')\nmtext(side = 1, line = 2.4, 'on the water')\nmtext(side = 2, line = 2.6, 'Frequency (%)')\n\nnbmales.seq <- seq(-5.31, 12.69, 0.1)\nplot(nbmales.seq, plogis(fixef(model.activity.final.glmer)[1] + fixef(model.activity.final.glmer)[3] * nbmales.seq)\n, axes = F, xlab = '', ylab = ''\n, type = 'l', col = 'black', ylim=c(0,1), lwd = 2)\naxis(side = 1, at = seq(-5.31, 9.69, 3), labels=seq(0, 15, 3))\naxis(side = 2, at = seq(0, 1, .25), las = 1)\n\nfor (i in 1:length(males$id_exp)){\nseq <- seq(males$min[i], males$max[i], 0.1)\npoints(seq, plogis(fixef(model.activity.used.for.blups.glmer)[1] + males$coef.activity[i] + (fixef(model.activity.used.for.blups.glmer)[3] + males$slope.activity.nbmales.c[i]) * seq), type = 'l', col = 'darkgray')\n}\nseq <- seq(males$min[20], males$max[20], 0.1)\n\tpoints(seq, plogis(fixef(model.activity.used.for.blups.glmer)[1] + males$coef.activity[20] + (fixef(model.activity.used.for.blups.glmer)[3] + males$slope.activity.nbmales.c[20]) * seq), type = 'l', col = 'darkblue', lwd = 2, lty = 4)\nseq <- seq(males$min[151], males$max[151], 0.1)\n\tpoints(seq, plogis(fixef(model.activity.used.for.blups.glmer)[1] + males$coef.activity[151] + (fixef(model.activity.used.for.blups.glmer)[3] + males$slope.activity.nbmales.c[151]) * seq), type = 'l', col = 'darkred', lwd = 2, lty = 4)\n\tseq <- seq(males$min[248], males$max[248], 0.1)\npoints(seq, plogis(fixef(model.activity.used.for.blups.glmer)[1] + males$coef.activity[248] + (fixef(model.activity.used.for.blups.glmer)[3] + males$slope.activity.nbmales.c[248]) * seq), type = 'l', col = 'darkgreen', lwd = 2, lty = 4)\n\npoints(nbmales.seq, plogis(fixef(model.activity.final.glmer)[1] + fixef(model.activity.final.glmer)[3] * nbmales.seq), type = 'l', col = 'black', lwd = 2)\npoints(nbmales.seq, plogis(fixef(model.activity.final.glmer)[1] + fixef(model.activity.final.glmer)[2] + (fixef(model.activity.final.glmer)[3] + fixef(model.activity.final.glmer)[5]) * nbmales.seq), type = 'l', col = 'black', lwd = 2, lty = 2)\nbox()\ntext(13.5, 0.98, 'N = 316 males', pos = 2)\nmtext(side = 1, line = 1.8, 'Number of unmated males')\nmtext(side = 1, line = 2.4, 'on the water')\nmtext(side = 2, line = 2.6, 'Activity')\ndev.copy2eps(file='figure1.eps', width=3.5, height=7)\ndev.off()\n\n############################################################\n####   FIGURE 2 interaction activity by nb males on mating rate \n############################################################\ndev.new()\npar(mfrow=c(1,3), mar=c(3.5, 3.7, 0.5, 0.5))\nquant <- quantile(striders.males.start[is.na(striders.males.start$coef.activity)==F,]$nbmales.c, c(1/3, 2/3))\nf = function(x,y) {plogis(fixef(model.start.final.glmer)[1] + x*fixef(model.start.final.glmer)[6] + y*fixef(model.start.final.glmer)[9] + x*y*fixef(model.start.final.glmer)[13])} \nseq.act <- seq(-2, 2, .1)\n\nwith(striders.males.start[striders.males.start$nbmales.c<quant[1],], plot(jitter(mating, amount = 0) ~ coef.activity \n, xlim=c(-2, 2), ylim=c(0, 1), las=1, cex=1, pch=16\n, type='p', axes = F\n, xlab='', ylab=''))\npoints(seq.act, f(seq.act, mean(striders.males.start[striders.males.start$nbmales.c<quant[1],]$nbmales.c))\n, type='l', lwd = 2)\naxis(side = 1, at = seq(-2, 2, 1))\naxis(side = 2, at = seq(0, 1, .25), las = 1)\nbox()\ntext(-2, 0.8, 'Low competition', cex=1, pos=4)\nmtext(side=1, line=2, 'Individual activity')\nmtext(side=2, line=2.6, 'Probability of initiating mating')\n\nwith(striders.males.start[striders.males.start$nbmales.c>=quant[1]&striders.males.start$nbmales.c<quant[2],], plot(jitter(mating, amount = 0) ~ coef.activity \n, xlim=c(-2, 2), ylim=c(0, 1), las=1, cex=1, pch=16\n, type='p', axes = F\n, xlab='', ylab=''))\npoints(seq.act, f(seq.act, mean(striders.males.start[striders.males.start$nbmales.c>=quant[1]&striders.males.start$nbmales.c<quant[2],]$nbmales.c))\n, type='l', lwd = 2)\naxis(side = 1, at = seq(-2, 2, 1))\naxis(side = 2, at = seq(0, 1, .25), las = 1)\nbox()\ntext(-2, 0.8, 'Intermediate competition', cex=1, pos=4)\nmtext(side=1, line=2, 'Individual activity')\nmtext(side=2, line=2.6, 'Probability of initiating mating')\n\nwith(striders.males.start[striders.males.start$nbmales.c>=quant[2],], plot(jitter(mating, amount = 0) ~ coef.activity \n, xlim=c(-2, 2), ylim=c(0, 1), las=1, cex=1, pch=16\n, type='p', axes = F\n, xlab='', ylab=''))\npoints(seq.act, f(seq.act, mean(striders.males.start[striders.males.start$nbmales.c>=quant[2],]$nbmales.c))\n, type='l', lwd = 2)\naxis(side = 1, "", ""library(lme4)\n\n\n### analysing male activity\n\nstriders.males.unmated <- read.delim2('data.act.2011.csv', sep = ';', dec = '.')\n\n# model used to compute repeatability\nmodel.activity.used.for.repeatability.glmer <- glmer(activity ~ treatment + (1|id_exp) + (1|stream) + (1|date) + (1|timestep)\n\t, data = striders.males.unmated\n\t, family = binomial)\n\n# model used to compute blups and slope\nmodel.activity.used.for.blups.glmer <- glmer(activity ~  treatment * (nbmales.c + nbfemales.c)\n\t\t\t\t\t+ (nbmales.c + nbfemales.c|id_exp) + (1|stream) + (1|date) + (1|timestep)\n\t, data = striders.males.unmated, family = binomial)\n\n\n# model with explaining strider activity\nmodel.activity.initial.glmer <- glmer(activity ~ treatment * (total.l.c + fore.w.c)\n\t\t\t\t\t+ treatment * (nbmales.c + nbfemales.c)\n\t\t\t\t\t+ total.l.c * (nbmales.c + nbfemales.c)\n\t\t\t\t\t+ fore.w.c * (nbmales.c + nbfemales.c)\n\t\t\t\t\t+ (nbmales.c|id_exp) + (1|stream) + (1|date) + (1|timestep)\n\t, data = striders.males.unmated, family = binomial)\n\n\nmodel.activity.final.glmer <- glmer(activity ~ treatment * (total.l.c + fore.w.c)\n\t\t\t\t\t+ treatment * (nbmales.c + nbfemales.c)\n\t\t\t\t\t+ total.l.c * (nbmales.c + nbfemales.c)\n\t\t\t\t\t+ fore.w.c * (nbmales.c + nbfemales.c)\n\t- fore.w.c:nbfemales.c \n\t- treatment:fore.w.c \n\t- total.l.c:nbfemales.c \n\t- treatment:nbfemales.c \n\t- treatment:total.l.c \n\t- fore.w.c:nbmales.c \n\t- fore.w.c \n\t- total.l.c\n\t- total.l.c:nbmales.c\n\t\t\t\t\t+ (nbmales.c|id_exp) + (1|stream) + (1|date) + (1|timestep)\n\t, data = striders.males.unmated, family = binomial)\n\n\n### extract blups\nmales <- cbind(rownames(ranef(model.activity.final.glmer)$id_exp), ranef(model.activity.final.glmer)$id_exp)\nmales <- unique(males)\nnames(males) <- c('id_exp', 'coef.activity', 'slope.activity.nbmales.c')\nmin <- as.data.frame(with(striders.males.unmated, aggregate(nbmales.c, by = list(id_exp), FUN = min)))\nnames(min) <- c('id_exp', 'min')\nmax <- as.data.frame(with(striders.males.unmated, aggregate(nbmales.c, by = list(id_exp), FUN = max))) \nnames(max) <- c('id_exp', 'max')\nmales <- merge(males, min, all.x = T)\nmales <- merge(males, max, all.x = T)\n\n## final model explaining activity using standardized variables.\nstriders.males.unmated$total.l.c.z <- scale(striders.males.unmated$total.l.c, scale = T)\nstriders.males.unmated$fore.w.c.z <- scale(striders.males.unmated$fore.w.c, scale = T)\nstriders.males.unmated$nbmales.c.z <- scale(striders.males.unmated$nbmales.c, scale = T)\nstriders.males.unmated$nbfemales.c.z <- scale(striders.males.unmated$nbfemales.c, scale = T)\n\nmodel.activity.final.glmer.standardized <- glmer(activity ~ treatment * (total.l.c.z + fore.w.c.z)\n\t\t\t\t\t+ treatment * (nbmales.c.z + nbfemales.c.z)\n\t\t\t\t\t+ total.l.c.z * (nbmales.c.z + nbfemales.c.z)\n\t\t\t\t\t+ fore.w.c.z * (nbmales.c.z + nbfemales.c.z)\n\t- fore.w.c.z:nbfemales.c.z \n\t- treatment:fore.w.c.z \n\t- total.l.c.z:nbfemales.c.z \n\t- treatment:nbfemales.c.z \n\t- treatment:total.l.c.z \n\t- fore.w.c.z:nbmales.c.z \n\t- fore.w.c.z \n\t- total.l.c.z\n\t- total.l.c.z:nbmales.c.z\n\t\t\t\t\t+ (nbmales.c|id_exp) + (1|stream) + (1|date) + (1|timestep)\n\t, data = striders.males.unmated, family = binomial)\n\n### analysing male aggressiveness\n\nagg <- read.delim2('data.agg.2011.csv', header = T, sep = ';', dec = '.')\n\n#model used to compute repeatability\nmodel.agg.1.glmer <- glmer(agg.score ~ treatment + (1|id_exp) + (1|stream) + (1|date) + (1|type)\n\t, data=agg)\nsummary(model.agg.1.glmer)\n\n#extract aggressiveness BT coefficients for all males.\nmales.agg <- cbind(rownames(ranef(model.agg.1.glmer)$id), ranef(model.agg.1.glmer)$id)\nmales.agg <- as.data.frame(unique(males.agg))\nnames(males.agg) <- c('id_exp', 'coef.agg')\nmales <- merge(males, males.agg)\n\n#extract table on the presence of HAM in pools each day\ntable.ham <- agg[agg$agg.score>2,c('id_exp', 'pool', 'stream', 'date', 'time', 'type')]\nnames(table.ham) <- c('HAM', 'pool', 'stream', 'date', 'time.ham.check', 'type')\n\n# model analysing aggressiveness with everything\nmodel.agg.lmer.final <- lmer(agg.score ~ treatment * (total.l.c + fore.w.c) + (fore.w.c + total.l.c) * (coef.activity)\n\t- treatment:total.l.c - fore.w.c:coef.activity - total.l.c:coef.activity - total.l.c + (1|id_exp) + (1|stream) + (1|date) + (1|type), data=agg)\n\nagg$total.l.c.z <- scale(agg$total.l.c, scale = T)\nagg$fore.w.c.z <- scale(agg$fore.w.c, scale = T)\nagg$coef.activity.z <- scale(agg$coef.activity, scale = T)\nagg$slope.activity.nbmales.c.z <- scale(agg$slope.activity.nbmales.c, scale = T)\n\nmodel.agg.lmer.final.standardized <- lmer(agg.score ~ treatment * (total.l.c.z + fore.w.c.z) + (fore.w.c.z + total.l.c.z) * (coef.activity.z)\n\t- treatment:total.l.c.z - fore.w.c.z:coef.activity.z - total.l.c.z:coef.activity.z - total.l.c.z + (1|id_exp) + (1|stream) + (1|date) + (1|type), data=agg) \n\n\n\n\n\n### analysing mating rate\n\nstriders.males.start <- read.delim2(file = 'data.start.2011.csv', sep = ';', dec = '.')\n\nmodel.start.glmer.initial <- glmer(mating ~ treatment + previous.activity \n\t\t\t\t\t+ treatment * (total.l.c + fore.w.c + coef.activity +""]",2,"correlational selection, personality, social plasticity, morphology, mating success, sexually dimorphic traits, fitness advantage, phenotype, environment, social context, animal populations, behaviour, Aquarius remigis, activity, aggressiveness, social environment,"
Data from: Stronger social bonds do not always predict greater longevity in a gregarious primate,"In group-living species, individuals often have preferred affiliative social partners, with whom ties or bonds can confer advantages that correspond with greater fitness. For example, in adult female baboons and juvenile horses, individuals with stronger or more social ties experience greater survival. We used detailed behavioral and life history records to explore the relationship between tie quality and survival in a gregarious monkey (Cercopithecus mitis stuhlmanni), while controlling for dominance rank, group size, and life history strategy. We used Cox proportional hazards regressions to model the cumulative (multi-year) and current (single-year) relationships of social ties and the hazard of mortality in 83 wild adult females of known age, observed 28 years each (437 subject-years) in eight social groups. The strength of bonds with close partners was associated with increased mortality risk under certain conditions: Females that had strong bonds with close partners that were inconsistent over multiple years had a higher risk of mortality than females adopting any other social strategy. Within a given year, females had a higher risk of death if they were strongly bonded with partners that changed from the previous year versus with partners that remained consistent. Dominance rank, number of adult female groupmates, and age at first reproduction did not predict the risk of death. This study demonstrates that costs and benefits of strong social bonds can be context-dependent, relating to the consistency of social partners over time.","['\n# 1. Repeatability (strength, rank, peers) ----\nsetwd(...)\ntdc<-read.csv(""tdc.data.csv"",stringsAsFactors = F) #annual subject values\nlibrary(rptR)\n\n\npR<-rptGaussian(peers ~ 1 + (1|subj),\n                grname =""subj"", data=tvc, nboot=1000, npermut=1000)\npR\n\nrR<-rptGaussian(rank ~ 1 + (1|subj),\n                grname =""subj"", data=tvc, nboot=1000, npermut=1000)\nrR\n\nsR<-rptGaussian(strength ~ peers + (1|subj),\n                grname =""subj"", data=tvc, nboot=1000, npermut=1000)\nsR\n\n\n\n# 2. Fixed effects Cox models-----\nsetwd(...)\nfe<-read.csv(""fixedeffects.data.csv"", stringsAsFactors = F)\nfe$st.co3<-as.factor(fe$st.co3)\nfe$st.co6<-as.factor(fe$st.co6)\nlibrary(survival) #for Coxph & cox.zph\nlibrary(car) #for vif\n\nz.<-function(x) scale(x)\n\n#top 3 partners\nfe$st.co3<-C(fe$st.co3, contr.treatment, base=3) #set reference level of strength-consistency class (reset as needed)\na3<-coxph(Surv(entry.years,censor.years,death)~st.co3 + z.(rank) + z.(age.first.rep), data=fe)\nb3<-coxph(Surv(entry.years,censor.years,death)~st.co3 + z.(af.groupmates) + z.(age.first.rep), data=fe)\nsummary(a3)\nsummary(b3)\nvif(a3)\nvif(b3)\nAICc(a3)\nAICc(b3)\n\ncox.zph(a3)\ncox.zph(b3)\n\n\n#top 6 partners\nfe$st.co6<-C(fe$st.co6, contr.treatment, base=3) #set reference level of strength-consistency class (reset as needed)\na6<-coxph(Surv(entry.years,censor.years,death)~st.co6 + z.(rank) + z.(age.first.rep), data=fe)\nb6<-coxph(Surv(entry.years,censor.years,death)~st.co6 + z.(af.groupmates) + z.(age.first.rep), data=fe)\nsummary(a6)\nsummary(b6)\nvif(a6)\nvif(b6)\nAICc(a6)\nAICc(b6)\n\ncox.zph(a6)\ncox.zph(b6)\n\n\n# 2a. Testing quadratic term - adult female groupmates^2 -------\n\nfe$st.co3<-C(fe$st.co3, contr.treatment, base=3)\nqrtest3<-coxph(Surv(entry.years,censor.years,death)~ st.co3 + z.(af.groupmates) + I(z.(af.groupmates)^2) + z.(age.first.rep), data=fe)\nsummary(qrtest3)\nAICc(qrtest3)\ncox.zph(qrtest3)\nvif(qrtest3)\n\nfe$st.co6<-C(fe$st.co6, contr.treatment, base=3)\nqrtest6<-coxph(Surv(entry.years,censor.years,death)~ st.co6 + z.(af.groupmates) + I(z.(af.groupmates)^2) + z.(age.first.rep), data=fe)\nsummary(qrtest6)\nAICc(qrtest6)\ncox.zph(qrtest6)\n\n\n\n\n# 2b. Model averaging strength consistency class coefficients----\n\nlibrary(AICcmodavg)\n\ncand.mod<-list(a3,b3)\ncand.names<-c(""m1.wrank"",""m2.wpeers"")\naictab(cand.mod,cand.names, sort=T)\na<-modavg(cand.mod, parm=""st.co31"",modnames = cand.names)\na$Mod.avg.beta\nexp(a$Mod.avg.beta) #hazard ratio\nb<-modavg(cand.mod, parm=""st.co32"",modnames = cand.names)\nb$Mod.avg.beta\nexp(b$Mod.avg.beta)\nc<-modavg(cand.mod, parm=""st.co33"",modnames = cand.names)\nc$Mod.avg.beta\nexp(c$Mod.avg.beta)\nd<-modavg(cand.mod, parm=""st.co34"",modnames = cand.names)\nd$Mod.avg.beta\nexp(d$Mod.avg.beta)\ne<-modavg(cand.mod, parm=""z.(age.first.rep)"",modnames = cand.names)\ne$Mod.avg.beta\nexp(e$Mod.avg.beta) \n\ncand.mod<-list(a6,b6)\ncand.names<-c(""m1.wrank"",""m2.wpeers"")\naictab(cand.mod,cand.names, sort=T)\na<-modavg(cand.mod, parm=""st.co61"",modnames = cand.names)\na$Mod.avg.beta\nexp(a$Mod.avg.beta) #hazard ratio\nb<-modavg(cand.mod, parm=""st.co62"",modnames = cand.names)\nb$Mod.avg.beta\nexp(b$Mod.avg.beta)\nc<-modavg(cand.mod, parm=""st.co63"",modnames = cand.names)\nc$Mod.avg.beta\nexp(c$Mod.avg.beta)\nd<-modavg(cand.mod, parm=""st.co64"",modnames = cand.names)\nd$Mod.avg.beta\nexp(d$Mod.avg.beta)\ne<-modavg(cand.mod, parm=""z.(age.first.rep)"",modnames = cand.names)\ne$Mod.avg.beta\nexp(e$Mod.avg.beta) \n\n\n\n# 3. Time dependent covariate Cox model-----\nsetwd(...)\nlibrary(survival)\nlibrary(car)\ntdc<-read.csv(""tdc.data.csv"",stringsAsFactors = F)\ntdc$a.st.co3<-as.factor(tdc$a.st.co3)\ntdc$a.st.co6<-as.factor(tdc$a.st.co6)\n\nz.<-function(x) scale(x)\n\n#top 3 TDC\n#base 3 (+/-)\ntdc$a.st.co3<-C(tdc$a.st.co3, contr.treatment, base=3)\ntdc3.3<-coxph(Surv(start.age, stop.age, death)~ a.st.co3 + z.(rank) + z.(af.groupmates), data=tdc)\nsummary(tdc3.3)\nvif(tdc3.3)\n\n#base 4 (+/+)\ntdc$a.st.co3<-C(tdc$a.st.co3, contr.treatment, base=4)\ntdc3.4<-coxph(Surv(start.age, stop.age, death)~ a.st.co3 + z.(rank) + z.(af.groupmates), data=tdc)\nsummary(tdc3.4)\nvif(tdc3.4)\n\n#base 1 (-/-)\ntdc$a.st.co3<-C(tdc$a.st.co3, contr.treatment, base=1)\ntdc3.1<-coxph(Surv(start.age, stop.age, death)~ a.st.co3 + z.(rank) + z.(af.groupmates), data=tdc)\nsummary(tdc3.1)\nvif(tdc3.1)\n\n#top 6 TDC\n#base 3 (+/-)\ntdc$a.st.co6<-C(tdc$a.st.co6, contr.treatment, base=3)\ntdc6.3<-coxph(Surv(start.age, stop.age, death)~ a.st.co6 + z.(rank) + z.(af.groupmates), data=tdc)\nsummary(tdc6.3)\nvif(tdc6.3)\n\n#base 4 (+/+)\ntdc$a.st.co6<-C(tdc$a.st.co6, contr.treatment, base=4)\ntdc6.4<-coxph(Surv(start.age, stop.age, death)~ a.st.co6 + z.(rank) + z.(af.groupmates), data=tdc)\nsummary(tdc6.4)\nvif(tdc6.4)\n\n#base 1 (+/+)\ntdc$a.st.co6<-C(tdc$a.st.co6, contr.treatment, base=1)\ntdc6.1<-coxph(Surv(start.age, stop.age, death)~ a.st.co6 + z.(rank) + z.(af.groupmates), data=tdc)\nsummary(tdc6.1)\nvif(tdc6.1)\n\n# 4. Compare observed st-co class coefficients to coefficients of permuted st.co classes-----\n\nsetwd(...)\nload(""permuted strength-consistency class coefficients.']",2,"social bonds, longevity, primates, group-living, affiliative, fitness, survival, behavioral records, life history records, tie quality, dominant rank, group size, life history strategy, Cox proportional hazards regressions, mortality risk, wild adult"
Male-male behavioral interactions drive social-dominance mediated differences in ejaculate traits,"Higher social status is expected to result in fitness benefits as it secures access to potential mates. In promiscuous species, male reproductive success is also determined by an individual's ability to compete for fertilization after mating by producing high quality ejaculates. However, the complex relationship between a male's investment in social status and ejaculates remains unclear. Here we examine how male social status influences ejaculate quality under a range of social contexts in the pygmy halfbeak Dermogenys cf. collettei, a small, group-living, internally fertilizing freshwater fish. We show that male social status influences ejaculate traits, both in the presence and absence of females. Dominant males produced faster swimming and more viable sperm, two key determinants of ejaculate quality, but only under conditions with frequent male-male behavioral interactions. When male-male interactions were experimentally reduced through the addition of a refuge, differences in ejaculate traits of dominant and subordinate males disappeared. Furthermore, dominant males were in better condition, growing faster and possessing larger livers, highlighting a possible condition-dependence of competitive traits. Contrary to expectations, female presence or absence did not affect sperm swimming speed or testes mass. Together, these results suggest a positive relationship between social status and ejaculate quality in halfbeaks, and highlight that the strength of behavioral interactions between males is a key driver of social-status dependent differences in ejaculate traits.","['setwd(""G:/Social_status_x_sperm_2019/Ejaculate_traits/All_first_strip"")\r\n\r\nsetwd(""F:/Social_status_x_sperm_2019/Ejaculate_traits/All_first_strip"")\r\n#Packages\r\n#install.packages(c(\'tidyverse\', \'broom\', \'dplyr\', \'forcats\', \'ggplot2\', \'lubridate\', \'magrittr\', \'modelr\', \'purrr\', \'readr\', \'readxl\', \'stringr\', \'tibble\', \'tidyr\', \'effects\', \'lsmeans\'))\r\n#install.packages(c(""ggsci"", ""ggthemes"", ""cowplot""))\r\n#install.packages(c(""xlsx"", ""RVAideMemoire"", ""lme4"", ""effects""))\r\n#install.packages(""Hmisc"")\r\n#install.packages(""bestNormalize"")\r\n#install.packages(c(""sjPlot"", ""sjmisc""))\r\n#install.packages(c(""tidyverse""))\r\n#install.packages(c(""compute.es""))\r\n#install.packages(c(""Hmisc""))\r\n#install.packages(c(""metafor""))\r\n#install.packages(c(""reshape2""))\r\n#install.packages(c(""installr""))\r\n#library(installr)\r\n#install.packages(""lmerTest"")\r\n#install.packages(c(""psych""))\r\n#install.packages(c(""VGAM""))\r\n##updateR()\r\n#library(installr)\r\n\r\n#updateR()\r\nR.Version()\r\ncitation()\r\n\r\nlibrary(Hmisc)\r\nlibrary(RVAideMemoire)\r\nlibrary(lme4)\r\nlibrary(effects)\r\nlibrary(tidyverse)\r\nlibrary(Hmisc)\r\nlibrary(ggsci)\r\nlibrary(ggthemes)\r\nlibrary(lsmeans)\r\nlibrary(effects)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(xlsx)\r\nlibrary(bestNormalize)\r\nlibrary(VGAM)\r\nlibrary(sjPlot)\r\nlibrary(sjmisc)\r\nlibrary(broom)\r\nlibrary(dplyr)\r\nlibrary(compute.es)\r\nlibrary(metafor)\r\nlibrary(reshape2)\r\nlibrary(lmerTest)\r\nlibrary(psych)\r\nlibrary(ggpubr)\r\n\r\nSIMPLE_THEME<-theme_bw() +\r\n  theme(axis.line=element_line(colour= ""black""),\r\n        panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        panel.border = element_blank(),\r\n        panel.background = element_blank(),\r\n        legend.key=element_blank())\r\n\r\n##Load the data\r\nSocioSperm<- read.csv(file=""Experiment_2.csv"",header=TRUE,sep="","") \r\n\r\n#DO NOT USE THIS CODE IF YOU DONT WANT CODE FILTERED ACCORDING TO DOMINANCE CRITERIA!\r\n####Filter out dyads where dominant male had dominance index <0.7 (less than 70 percent of the fights won)\r\nSocioSperm_rmv <- SocioSperm %>% filter(between(Dominance_index, 0.30, 0.70))\r\nSocioSperm <- SocioSperm %>% filter(!between(Dominance_index, 0.30, 0.70))\r\n\r\n######################################################################################################\r\n####################Calculating means for relevent metrics############################################\r\n\r\n##General overview (flip Sociosperm to include or exclude 8 dyads with dominant males 50 - 70% of fights won)\r\nSocioSperm <- SocioSperm %>%\r\n  filter(Population == ""Tebrau"")\r\n\r\nSocioVCL<-psych::describeBy(SocioSperm$VCL, SocioSperm$Dom_status)\r\nViamean<-  SocioSperm %>% filter(Total_sum > 100) #Filtering data points where less than 100 cells were recorded for viability\r\nSocioVia<-psych::describeBy(Viamean$Percentage_live, Viamean$Dom_status)\r\nCountmean<- SocioSperm  %>% filter(Total_count_of_sperm_cells > 200) #Filtering data points where less than 200 cells were recorded for count\r\nSocioCount<-psych::describeBy(Countmean$Total_count_of_sperm_cells,Countmean$Dom_status)\r\nSocioHead<-psych::describeBy(SocioSperm$MeanHead, SocioSperm$Dom_status)\r\nSocioMid<-psych::describeBy(SocioSperm$MeanMidpiece, SocioSperm$Dom_status)\r\nSocioTail<-psych::describeBy(SocioSperm$MeanTail, SocioSperm$Dom_status)\r\nSocioTotal<-psych::describeBy(SocioSperm$Total_sperm_length, SocioSperm$Dom_status)\r\nSocioDom <- psych::describeBy(SocioSperm$Dominance_index, SocioSperm$Dom_status)\r\n\r\n###Only trials with barrier ""+ Barrier""\r\nSocioSpermR <- SocioSperm[(SocioSperm$Barrier==""Yes""), ]\r\n\r\nSocioVCLR<-psych::describeBy(SocioSpermR$VCL, SocioSpermR$Dom_status)\r\nViameanR<-  SocioSpermR %>% filter(Total_sum > 100) #Filtering data points where less than 100 cells were recorded for viability\r\nSocioViaR<-psych::describeBy(ViameanR$Percentage_live, ViameanR$Dom_status)\r\nCountmeanR<- SocioSpermR  %>% filter(Total_count_of_sperm_cells > 200) #Filtering data points where less than 200 cells were recorded for count\r\nSocioCountR<-psych::describeBy(CountmeanR$Total_count_of_sperm_cells,CountmeanR$Dom_status)\r\nSocioHeadR<-psych::describeBy(SocioSpermR$MeanHead_um, SocioSpermR$Dom_status)\r\nSocioMidR<-psych::describeBy(SocioSpermR$MeanMidpiece_um, SocioSpermR$Dom_status)\r\nSocioTailR<-psych::describeBy(SocioSpermR$MeanTail_um, SocioSpermR$Dom_status)\r\nSocioTotalR<-psych::describeBy(SocioSpermR$Total_sperm_length_um, SocioSpermR$Dom_status)\r\n\r\n###Only trials with no barrier ""- Barrier""\r\nSocioSpermF <- SocioSperm[(SocioSperm$Barrier==""No""), ]\r\n\r\nSocioVCLF<-psych::describeBy(SocioSpermF$VCL, SocioSpermF$Dom_status)\r\nViameanF<-  SocioSpermF %>% filter(total_sum > 100) #Filtering data points where less than 100 cells were recorded for viability\r\nSocioViaF<-psych::describeBy(ViameanF$Percentage_live, ViameanF$Dom_status)\r\nCountmeanF<- SocioSpermF  %>% filter(Total_count_of_sperm_cells > 200) #Filtering data points where less than 200 cells were recorded for count\r\nSocioCountF<-psych::describeBy(CountmeanF$Total_count_of_sperm_cells,CountmeanF$Dom_status)\r\nSoc', '####################################################################################################################################\r\n####################################################################################################################################\r\n################################# Experiment 1######################################################################################\r\n################################# Dyads with no female, visual access or full access to females#####################################\r\n####################################################################################################################################\r\n####################################################################################################################################\r\nsetwd(""F:/Male Contests w Females NEWEST"")\r\n\r\nsetwd(""E:/Male Contests w Females NEWEST"")\r\n\r\n#Packages\r\n#install.packages(c(\'tidyverse\', \'broom\', \'dplyr\', \'forcats\', \'ggplot2\', \'lubridate\', \'magrittr\', \'modelr\', \'purrr\', \'readr\', \'readxl\', \'stringr\', \'tibble\', \'tidyr\', \'effects\', \'lsmeans\'))\r\n#install.packages(c(""ggsci"", ""ggthemes"", ""cowplot""))\r\n#install.packages(c(""xlsx"", ""RVAideMemoire"", ""lme4"", ""effects""))\r\n#install.packages(""Hmisc"")\r\n#install.packages(""bestNormalize"")\r\n#install.packages(c(""sjPlot"", ""sjmisc""))\r\n#install.packages(c(""tidyverse""))\r\n#install.packages(c(""compute.es""))\r\n#install.packages(c(""Hmisc""))\r\n#install.packages(c(""metafor""))\r\n#install.packages(c(""reshape2""))\r\n#install.packages(c(""installr""))\r\n#install.packages(c(""psych""))\r\n#install.packages(c(""ggpubr""))\r\n#install.packages(c(""multcompView""))\r\n#library(installr)\r\n#updateR()\r\n\r\nlibrary(Hmisc)\r\nlibrary(RVAideMemoire)\r\nlibrary(lme4)\r\nlibrary(effects)\r\nlibrary(tidyverse)\r\nlibrary(Hmisc)\r\nlibrary(ggsci)\r\nlibrary(ggthemes)\r\nlibrary(lsmeans)\r\nlibrary(effects)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(xlsx)\r\nlibrary(bestNormalize)\r\nlibrary(VGAM)\r\nlibrary(sjPlot)\r\nlibrary(sjmisc)\r\nlibrary(broom)\r\nlibrary(dplyr)\r\nlibrary(compute.es)\r\nlibrary(metafor)\r\nlibrary(reshape2)\r\nlibrary(lmerTest)\r\nlibrary(psych)\r\nlibrary(multcompView)\r\nlibrary(ggpubr)\r\n\r\nSIMPLE_THEME<-theme_bw() +\r\n  theme(axis.line=element_line(colour= ""black""),\r\n        panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        panel.border = element_blank(),\r\n        panel.background = element_blank(),\r\n        legend.key=element_blank())\r\n\r\n####Load the data####\r\ndataS<- read.csv(file=""Experiment1.csv"",header=TRUE,sep="","")\r\ndataS$Status.Treatment <- interaction(dataS$Status, dataS$Treatment)\r\n\r\n###########################################################\r\n########Summary of relevant metrics########################\r\ndataSVCL<-psych::describeBy(dataS$VCL, dataS$Status)\r\ndataSLiver<-psych::describeBy(dataS$Liver_weight_mg, dataS$Status)\r\ndataSGonad<-psych::describeBy(dataS$Gonad_weight_mg, dataS$Status)\r\ndataSCond<-psych::describeBy(dataS$AF_CF_No_beak, dataS$Status)\r\ndataSChange<-psych::describeBy(dataS$Change_Body_area_No_Beak_daily, dataS$Status)\r\ndataSBodylength<-psych::describeBy(dataS$BF_Body_length, dataS$Status)\r\ndataSDom<-psych::describeBy(dataS$Dominance_index, dataS$Status)\r\n\r\n###########################################################################\r\n####Were male dyads correctly size matched?##############################\r\ndataSize<-dataS %>%\r\n  group_by(Pair_ID)\r\n\r\ndataA<-dataSize %>%\r\n  group_by(Pair_ID) %>% \r\n  filter(ID_2 == ""A"") #Filters for every observation male ""A"" (randomly assigned)\r\ndataA<- arrange(dataA, Pair_ID) #Sort them according to trial number\r\n\r\ndataB<-dataSize %>%\r\n  group_by(Pair_ID) %>% \r\n  filter(ID_2 == ""B"") #Filters for every observation male ""B"" (randomly assigned)\r\ndataB<- arrange(dataB, Pair_ID) #Sort them according to trial number\r\n\r\nvar.test(dataA$BF_Body_length, dataB$BF_Body_length, alternative = ""two.sided"")\r\nt.test(dataA$BF_Body_length, dataB$BF_Body_length, alternative = ""two.sided"", var.equal = TRUE, paired = TRUE)\r\n\r\n\r\n###########################################################\r\n#########Do sperm traits correlate with condition?#########\r\n#########Simple analysis ignoring male status##############\r\n\r\n##Model as differences to account for dominance affecting condition and sperm speed\r\n#As we measure dyad differences, we have to remove dyads where only one male was present (Dyad NoF_8)\r\nOnlydya<-dataS %>%\r\n  filter(Pair_ID != ""NoF_8"")\r\n\r\n#Differences in VCl\r\nDyadiffVCL <- Onlydya %>% \r\n  dcast(Pair_ID ~ Status, value.var = ""VCL"", fill = 0) %>% \r\n  mutate(Diff_VCL = Dom - Sub) %>% \r\n  select(Pair_ID, Diff_VCL)\r\n\r\n#Differences in Condition to compare with VCL\r\nDyadiffCF <- Onlydya %>% \r\n  dcast(Pair_ID ~ Status, value.var = ""AF_CF_No_beak"", fill = 0) %>% \r\n  mutate(Diff_CF = Dom - Sub) %>% \r\n  select(Pair_ID, Diff_CF)\r\n\r\n#Model the relationship\r\n#Putting everything together into a large dataset\r\nVCL_CF_diff <- dplyr::full_join(DyadiffVCL, DyadiffCF, by = ""Pair_ID"")\r\n\r\n#Lastly, include Treatment to every corresponding Dyad (has ']",2,"male-male interactions, social dominance, ejaculate traits, fitness benefits, reproductive success, promiscuous species, fertilization, high quality ejaculates, male investment, social context, pygmy halfbeak, Dermogenys cf. col"
Possibilistic Network. Social Polarization in the Metropolitan Area of Marseille (France).,"This R-script builds and executes a possibilistic network which has the same structure as the Baysesian network proposed by F. Scarella [2] in order to model social polarization in the metropolitan area of Marseille (France). The model can be used to infer a trend scenario of social polarization of the 439 municipalities in the metropolitan area of Marseille (France) in a 10 year time. Within the model, a valorized municipality is defined as a municipality where executives and professionals are overrepresented within its resident population; a devalorized municipality is defined as a municipality where the unemployed are overrepresented within its resident population.The initial data for the study area were elaborated for 2009 and are in the Data_Marseille_2009.txt file. Other auxiliary files are: - DependentVariables.txt containing the dependency structure of the possibilistic network - VariableModalities.txt containing the values of each variable of the network- VariableModalities_PlainEnglish.txt gives plain English names to variables and modalities, but is not used by the R-script.- ModelStructure.png visualizes the DAG structure of the possibilistic networkThe network is build using uncertain logical gates ([3], [4]), which are the possibilistic counterpart of the noisy logical gates used in Bayesian networks [2]. More thorough presentations of the model and of the model results are available in [5] and in [6].Model results and comparison with Bayesian network results can be explored through an interactive data-visualization at the following address: https://public.tableau.com/profile/fusco#!/vizhome/RepresentingUncertainFutures/Story1References[1] Francisco Dez and Marek Druzdzel. ""Canonical Probabilistic Models for Knowledge Engineering"", Tech. Rep. CISIAN-06-01, version 0.9, April 28, 2007.[2] Floriane Scarella, La sgrgation rsidentielle dans l'espace-temps mtropolitain: analyse spatiale et go-prospective des dynamiques rsidentielles de la mtropole azurenne, PhD dissertation, University of Nice Sophia Antipolis, 2014.[3] Matteo Caglioni, Didier Dubois, Giovanni Fusco, Diego Moreno, Henri Prade, Floriane Scarella, and Andrea Tettamanzi. ""Mise en oeuvre pratique de rseaux possibilistes pour modliser la spcialisation sociale dans les espaces mtropoliss"", LFA 2014 - Cargse 22-24 novembre 2014, Cpadus, Toulouse, ISBN : 9782364931565, pp. 267-274.[4] Didier Dubois, Giovanni Fusco, Henri Prade, and Andrea Tettamanzi, ""Uncertain Logical Gates in Possibilistic Networks. An Application to Human Geography"". In Ch. Beierle and A. Dekhtyar (Eds.). Scalable Uncertainty Management - 9th International Conference, SUM 2015, Qubec City, QC, Canada, September 16-18, 2015. Proceedings (ISBN: 978-3-319-23539-4), Lecture Notes in Artificial Intelligence, vol. 9310, Springer, pp. 249-263.[5] Didier Dubois, Giovanni Fusco, Henri Prade, and Andrea Tettamanzi, ""Uncertain Logical Gates in Possibilistic Networks: Theory and application to human geography"", International Journal of Approximate Reasoning, 2016 (in progress).[6] Giovanni Fusco, Cristina Cao, Didier Dubois, Henri Prade, Floriane Scarella, and Andrea Tettamanzi, Social polarization in the metropolitan area of Marseille. Modelling uncertain knowledge with probabilistic and possibilistic networks, ECTQG 2015 - XIX European Colloquium on Theoretical and Quantitative Geography, Bari (Italy), September 3rd-7th 2015, Proceedings, Plurimondi. An International Forum for Research and Debate on Human Settlements, 8 p., 2015.","['###############################################################\r\n#                                                             #\r\n# Possibilistic Network.                                      #\r\n# Social Polarization in the Metropolitan Area of Marseille.  #\r\n# An R script to build and execute a possibilistic network.   #\r\n#                                                             #\r\n# AuthorS: Andrea G. B. Tettamanzi                            #\r\n#          Université Côte d\'Azur, CNRS, I3S, UMR7271         #\r\n#          06903 Sophia Antipolis, France                     #\r\n#          E-mail: andrea.tettamanzi@unice.fr                 #\r\n#                                                             #\r\n#          Giovanni Fusco                                     #\r\n#          Université Côte d\'Azur, CNRS, ESPACE, UMR7300      #\r\n#          98 Bd Herriot, BP 3209, 06204 Nice, France         #\r\n#          E-mail: giovanni.fusco@unice.fr                    #\r\n#                                                             #\r\n###############################################################\r\n\r\n# This script builds and executes a possibilistic network which has the same structure as the Baysesian network \r\n# proposed by F. Scarella [2] in order to model social polarization in the metropolitan area of Marseille (France).\r\n# The model can be used to infer a trend scenario of social polarization of the 439 municipalities in the metropolitan\r\n# area of Marseille (France) in a 10 years time.\r\n# Within the model, a valorized municipality is defined as a municipality where executives and professionals are\r\n# overrepresented within its resident population; a devalorized municipality is defined as a municipality where the\r\n# unemployed are overrepresented within its resident population.\r\n# The initial data for the study area were elaborated for 2009 and are in the Data_Marseille_2009.txt file.\r\n# Other auxiliary files are:\r\n#   - DependentVariables.txt containing the dependency structure of the possibilistic network\r\n#   - VariableModalities.txt containing the values of each variable of the network\r\n#   - VariableModalities_PlainEnglish.txt gives plain English names to variables and modalities, but is not used by \r\n#     the R-script.\r\n#   - ModelStructure.png visualizes the DAG structure of the possibilistic network\r\n#\r\n# The network is build using uncertain logical gates ([3], [4]), which are the possibilistic counterpart of\r\n# the noisy logical gates used in Bayesian networks [2]. More thorough presentations of the model and of the model \r\n# results are available in [5] and in [6].\r\n# Model results and comparison with Bayesian network results can be explored through an interactive data-visualization \r\n# at the following address: https://public.tableau.com/profile/fusco#!/vizhome/RepresentingUncertainFutures/Story1\r\n\r\n# Acknowledgement:\r\n# This script was produced within the Géo-Incertitude project (2014-2015, CNRS grant of the PEPS HuMaIn program). \r\n\r\n# References:\r\n# [1] Francisco Díez and Marek Druzdzel. ""Canonical Probabilistic Models for Knowledge Engineering"",\r\n#     Tech. Rep. CISIAN-06-01, version 0.9, April 28, 2007.\r\n# [2] Floriane Scarella, La ségrégation résidentielle dans l\'espace-temps métropolitain: \r\n#     analyse spatiale et géo-prospective des dynamiques résidentielles de la métropole azuréenne, \r\n#     PhD dissertation, University of Nice Sophia Antipolis, 2014.\r\n# [3] Matteo Caglioni, Didier Dubois, Giovanni Fusco, Diego Moreno, Henri Prade, Floriane Scarella,\r\n#     and Andrea Tettamanzi. ""Mise en oeuvre pratique de réseaux possibilistes pour modéliser\r\n#     la spécialisation sociale dans les espaces métropolisés"", LFA 2014 - Cargèse 22-24 novembre 2014, \r\n#     Cépaduès, Toulouse, ISBN : 9782364931565,  pp. 267-274\r\n# [4] Didier Dubois, Giovanni Fusco, Henri Prade, and Andrea Tettamanzi, ""Uncertain Logical Gates in Possibilistic Networks.\r\n#     An Application to Human Geography"". In Ch. Beierle and A. Dekhtyar (Eds.). Scalable Uncertainty Management - \r\n#     9th International Conference, SUM 2015, Québec City, QC, Canada, September 16-18, 2015. Proceedings (ISBN: 978-3-319-23539-4), \r\n#     Lecture Notes in Artificial Intelligence, vol. 9310, Springer, pp. 249-263.\r\n# [5] Didier Dubois, Giovanni Fusco, Henri Prade, and Andrea Tettamanzi, ""Uncertain Logical Gates in Possibilistic Networks:\r\n#     Theory and application to human geography"", International Journal of Approximate Reasoning, 2016 (in progress)\r\n# [6] Giovanni Fusco, Cristina Cao, Didier Dubois, Henri Prade, Floriane Scarella, and Andrea Tettamanzi, 2015, \r\n#     Social polarization in the metropolitan area of Marseille. Modelling uncertain knowledge with probabilistic and \r\n#     possibilistic networks, ECTQG 2015 - XIX European Colloquium on Theoretical and Quantitative Geography, Bari (Italy),\r\n#     September 3rd-7th 2015, Proceedings, Plurimondi. An International Forum for Research and Debate on Human Settlements, 8 p.\r\n\r\n# To i']",2,"possibilistic network, social polarization, metropolitan area, Marseille, France, R-script, Bayesian network, trend scenario, valorized municipality, devalorized municipality, executives, professionals, unemployed, resident population, uncertain logical gates, noisy logical gates,"
"Data from: Ecological divergence, adaptive diversification, and the evolution of social signaling traits: an empirical study in arid Australian lizards","Species diversification often results from divergent evolution of ecological or social signaling traits. Theoretically, a combination of the two may promote speciation; however empirical examples studying how social signal and ecological divergence might be involved in diversification are rare in general, and typically do not consider range overlap as a contributing factor. We show that ecologically distinct lineages within the Australian sand dragon species complex (including (Ctenophorus maculatus, C. fordi, and C. femoralis) have diversified recently, diverging in ecologically relevant and social signaling phenotypic traits as arid habitats expanded and differentiated. Diversification has resulted in repeated and independent invasion of distinct habitat-types, driving convergent evolution of similar phenotypes. Our results suggest parapatry facilitates diversification in visual signals through reinforcement as a hybridization avoidance mechanism. We show particularly striking variation in visual social signaling traits is better explained by the extent of lineage parapatry relative to ecological or phylogenetic divergence, suggesting these traits reinforce divergence among lineages initiated by ecologically adaptive evolution. This study provides a rare empirical example of a repeated, intricate relationship between ecological and social signal evolution during diversification driven by ecological divergence and the evolution of new habitats. Therefore, supporting emergent theories regarding the importance of both ecological and social trait evolution throughout speciation.","['C_m_dualis_ext<-extent(117,136,-35.251,-31.13)\n\nC_for_clade2_ext<-extent(132.25,139.8952,-35.17,-31.583)\n\nC_fem_ext<-extent(113.016,116.392,-25.84,-18.925)\n\nC_fordi_ext<-extent(118,133,-35,-25)\n\nC_m_gris_ext<-extent(114,124,-35,-28)\n\nC_for_clade1_ext<-extent(115.993,139.083,-33.974,-25.535)\n\nC_m_mac_ext<-extent(111.703,116.79,-32.966,-20.589)\n\nC_for_clade4_ext<-extent(138.616,147.563,-36.08,-29.156)\n\nC_mac_clade1_ext<-extent(111.716,114.765,-27.275,-23.115)\n\nC_for_clade5_ext<-extent(137.359,144.368,-38.412,-33.478)\n\nC_for_clade3_ext<-extent(134.531,144.398,-34.44,-26.137)\n', '#DTT accounting for phylogenetic uncertainty\n\n\n\nmdi_phy<-function(tree,x){\n \t\tdisp<-dtt(tree,x,index=""avg.sq"",nsim=1000,plot=FALSE)\n \t\tmdi<-disp$MDI\n \t\tval<-disp$dtt\n \t\ttime<-disp$times\n \t\tsim<-t(disp$sim)\n \t\treturn(list(MDI=mdi,time=time,DTT=val,SIM=sim))\n }\n\nplots2<-function(x,y) {\n\tx.x<-apply(x,1,function (x) data.frame(x))\n\ty.y<-apply(y,1,function (y) data.frame(y))\n\tcomb<-mapply(data.frame, y.y, x.x, SIMPLIFY=FALSE)\n\tlapply(comb,function (x) lines(as.data.frame(x),col=rgb(1,0,0,0.1)))\n\trm(x.x)\n\trm(y.y)\n\trm(comb)\n}\n\nelems<-c(""MDI"",""time"",""DTT"",""SIM"") \n\nmdi<-lapply(posterior_trees,mdi_phy,trait_data)\n\nres<-sapply(elems,function(E){do.call(rbind,lapply(mdi,function(X){X[[E]]}))},simplify=FALSE)\n \nplot(NA,NA,xlim=c(0.0,1.0),ylim=c(0.0,1.25),xlab=""Time (my bp)"",ylab=""Disparity (MDI)"")\naxis(1,at=c(0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0))\naxis(2,at=c(0.0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0))\n\n\ns<-data.frame(res$SIM)\nt<-data.frame(res$time)\nv<-data.frame(res$DTT)\nx.MDI<-mean(res$MDI)\nx.MDI\n\n\nsim.median<-apply(s,2,median)\ntime.median<-apply(t,2,median)\nval.median<-apply(v,2,median)\nsd<-apply(s,2,sd)\nmean<-apply(s,2,mean)\nupper<-mean+(2*sd)\nlower<-mean-(2*sd)\ns.u<-data.frame(time.median,upper)\ns.l<-data.frame(rev(time.median),rev(lower))\ncolnames(s.l)<-colnames(s.u)\npoly<-rbind(s.u,s.l)\npolygon(poly,col=grey(0.1,0.3),border=NA)\n\nplots2(v,t)\n\nobs<-dtt(median_tree,trait_data,index=""avg.sq"",nsim=1000,plot=FALSE)\nobs.dtt<-obs$dtt\nobs.time<-obs$times\nlines(time.median,sim.median,col=rgb(1,1,0,1),lty=2,lwd=4)\nlines(obs.time,obs.dtt,col=rgb(0,0,0,1),lwd=4)\nlines(time.median,val.median,col=rgb(0,0.9,1,1),lty=2,lwd=4)\n\n\ndev.copy2pdf(file=""trait_dtt.pdf"")\n', '## Tests accounting for phylogenetic uncertainty\n\nff<-function(tree,x,y){\n\tpic.x<-pic.ortho(x,tree,intra=T)\n\tpic.y<-pic.ortho(y,tree,intra=T)\n\tfit<-lm(pic.y~pic.x-1)\n\tsum<-as.numeric(summary.lm(fit)$adj.r.squared)\n\tsetNames(c(coef(fit),vcov(fit),sum),c(""beta"",""var(beta)"",""R2""))\n}\n# now apply to all trees in your sample\n\nBB<-t(sapply(trees,ff,x,y))\n\n\n# total variance in beta estimated by\nvarBeta<-var(BB[,""beta""])+mean(BB[,""var(beta)""])\nt.beta<-mean(BB[,""beta""])/sqrt(varBeta)\nP.beta<-2*pt(abs(t.beta),df=length(trees[[1]]$tip)-2,lower.tail=FALSE)\n', 'Species\tMinTraining+Specificity\nC_m_dualis\t0.268336364\nC_for_clade2\t0.205963636\nC_femoralis\t0.228536364\nC_fordi\t0.166\nC_m_griseus\t0.229118182\nC_for_clade1\t0.230027273\nC_m_maculatus\t0.134181818\nC_for_clade4\t0.2151\nC_m_clade1\t0.276927273\nC_for_clade5\t0.156863636\nC_for_clade3\t0.276927273']",2,"Ecological divergence, adaptive diversification, social signaling traits, empirical study, arid Australian lizards, species diversification, divergent evolution, theoretical, speciation, social signal, ecological divergence, empirical examples, range overlap, ecologically distinct line"
Data from: Scale dependence of sex ratio in wild plant populations: implications for social selection,"Social context refers to the composition of an individual's social interactants, including potential mates. In spatially structured populations, social context can vary among individuals within populations, generating the opportunity for social selection to drive differences in fitness functions among individuals at a fine spatial scale. In sexually polymorphic plants, the local sex ratio varies at a fine scale and thus has the potential to generate this opportunity. We measured the spatial distribution of two wild populations of the gynodioecious plant Silene vulgaris and show that there is fine-scale heterogeneity in the local distribution of the sexes within these populations. We demonstrate that the largest variance in sex ratio is among nearest neighbors. This variance is greatly reduced as the spatial scale of social interactions increases. These patterns suggest the sex of neighbors has the potential to generate fine-scale differences in selection differentials among individuals. One of the most important determinants of social interactions in plants is the behavior of pollinators. These results suggest that the potential for selection arising from sex ratio will be greatest when pollen is shared among nearest neighbors. Future studies incorporating the movement of pollinators may reveal whether and how this fine-scale variance in sex ratio affects the fitness of individuals in these populations.","['require(reshape2)\nmerged <- read.delim(""mergedUTM.tsv"", stringsAsFactors=F)\n\n# Make distance matrices for each population, then melt them into long form\n# data frames\n\n# Create a new matrix for the UTM coordinates and name the rows of the matrix\n# by plant\npreBurk1 <- as.matrix(cbind(merged$Northing[merged$population==""Burk1""], merged$Easting[merged$population==""Burk1""]))\nrownames(preBurk1) <- merged$name[merged$population==""Burk1""]  \n\n# Make a matrix of all pairwise distances between plants\ndistBurk1 <- as.matrix(dist(preBurk1))\n\n# Use the melt function to convert the distance matrix to a long-form data frame\nmeltBurk1 <- melt(distBurk1)\nburk1Out <- meltBurk1\n\n# Create new variables for the sex of the focal and partner plants\nburk1Out$focalSex <- NA\nburk1Out$partnerSex <- NA\ncolnames(burk1Out) <- c(""focal"", ""partner"", ""distance"", ""focalSex"", ""partnerSex"")\nburk1Out$focal <- as.character(burk1Out$focal)\nburk1Out$partner <- as.character(burk1Out$partner)\n\n# Iterate through the long-form data frame and add in the sex of focal and \n# partners\nfor (i in 1:nrow(burk1Out)) {\n  fName <- burk1Out$focal[i]\n  pName <- burk1Out$partner[i]\n  burk1Out$focalSex[i] <- unique(merged$sex[merged$name==fName])\n  burk1Out$partnerSex[i] <- unique(merged$sex[merged$name==pName])\n}\n\n# Create a new matrix for the UTM coordinates and name the rows of the matrix\n# by plant (these are the same steps, but repeated for the Burk2 pop)\npreBurk2 <- as.matrix(cbind(merged$Northing[merged$population==""Burk2""], merged$Easting[merged$population==""Burk2""]))\nrownames(preBurk2) <- merged$name[merged$population==""Burk2""]  \n\n# Make a matrix of all pairwise distances between plants\ndistBurk2 <- as.matrix(dist(preBurk2))\n\n# Use the melt function to convert the distance matrix to a long-form data frame\nmeltBurk2 <- melt(distBurk2)\nburk2Out <- meltBurk2\n\n# Create new variables for the sex of the focal and partner plants\nburk2Out$focalSex <- NA\nburk2Out$partnerSex <- NA\ncolnames(burk2Out) <- c(""focal"", ""partner"", ""distance"", ""focalSex"", ""partnerSex"")\nburk2Out$focal <- as.character(burk2Out$focal)\nburk2Out$partner <- as.character(burk2Out$partner)\n\n# Iterate through the long-form data frame and add in the sex of focal and \n# partners\nfor (i in 1:nrow(burk2Out)) {\n  fName <- as.character(burk2Out$focal[i]) \n  pName <- as.character(burk2Out$partner[i])\n  burk2Out$focalSex[i] <- unique(merged$sex[merged$name==fName])\n  burk2Out$partnerSex[i] <- unique(merged$sex[merged$name==pName])\n}\n\n# combine all of the long-form data frames into one long data frame\nlongestFrame <- rbind(burk1Out, burk2Out)\n\n# make a new data frame that has rows for all individuals in all populations\nlocalSex <- as.character(merged$name)\nlocalSex <- as.data.frame(localSex)\ncolnames(localSex) <- ""name""\n\n# initilize variables for local ratios and pop sizes (laborious)\nlocalSex$sex <- NA\nlocalSex$population <- NA\nlocalSex$sexRatio <- NA\nlocalSex$popSize <- NA\nlocalSex$r025 <- NA\nlocalSex$r05 <- NA\nlocalSex$r10 <- NA\nlocalSex$r15 <- NA\nlocalSex$r20 <- NA\nlocalSex$r25 <- NA\nlocalSex$r30 <- NA\nlocalSex$r35 <- NA\nlocalSex$r40 <- NA\nlocalSex$r45 <- NA\nlocalSex$r50 <- NA\nlocalSex$r55 <- NA\nlocalSex$r60 <- NA\n\nlocalSex$d025 <- NA\nlocalSex$d05 <- NA\nlocalSex$d10 <- NA\nlocalSex$d15 <- NA\nlocalSex$d20 <- NA\nlocalSex$d25 <- NA\nlocalSex$d30 <- NA\nlocalSex$d35 <- NA\nlocalSex$d40 <- NA\nlocalSex$d45 <- NA\nlocalSex$d50 <- NA\nlocalSex$d55 <- NA\nlocalSex$d60 <- NA\n\n\n# look up sex and population name  from the source data frame\nfor (i in 1:nrow(localSex)) {\n  name <- as.character(localSex$name[i])\n  localSex$sex[localSex$name == name] <- as.character(merged$sex[merged$name == name])\n  localSex$population[localSex$name == name] <- as.character(merged$population[merged$name == name])\n}\n\n# convert sex, and population into factors\nlocalSex$sex <- factor(localSex$sex, levels=c(""hermaphrodite"", ""female""))\nlocalSex$population <- factor(localSex$population, levels=c(""Burk1"", ""Burk2""))\n\n# calculate sex ratio and pop size for each population\nfor (i in 1:length(levels(localSex$population))) {\n  nHerm = 0\n  nFem = 0\n  pop = levels(localSex$population)[i]\n  subByPop <- localSex[localSex$population==pop,]\n  popSize = nrow(subByPop)\n  for (j in 1:nrow(subByPop)) {\n    if (subByPop$sex[j] == ""hermaphrodite"") {\n      nHerm <- nHerm + 1\n    } \n    \n    if (subByPop$sex[j] == ""female"") {\n      nFem <- nFem + 1\n    }\n  }\n  localSex$sexRatio[localSex$population==pop] <- (nHerm / (nHerm + nFem))\n  localSex$popSize[localSex$population==pop] <- popSize\n}\n\n# iterate through the list, subset all of the distances for that individual\n# calculate the local sex ratios and densities at the different radii\n\n\nfor (i in 1:nrow(localSex)) {\n  # A vector to specify the radius lengths \n  distVector = c(0.25, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6)\n  # A vector to store the individual sex ratios across spatial scales\n  ratVector = rep(NA, length(distVector))\n  # A vector to store individual sex ratios weighted by fruit number\n  #fRatVecto', '# Load required packages\nrequire(car)\nrequire(lme4)\nrequire(ggplot2)\nrequire(psych)\nrequire(AICcmodavg)\nrequire(boot)\n\n# This helper script imports and formats the data, then creates the distance\n# matricies and calculates the local sex ratios for all individuals across\n# all scales. It is not well vectorized and so it takes a long time to run.\n\nsource(\'formatData.R\')\n\n\n# Test whether mean sex ratio changes among levels using a general linear mixed\n# model\n\nvarModel <- glmer(sexRatio ~ sex*level + (1|name), weights=density, \n                  data=varAnalysis, family=binomial(logit), \n                  control=glmerControl(optimizer=""bobyqa""))\nvarAnalysis$resid <- resid(varModel)\nAnova(varModel, type=3)\n\n# Visually insepct the histogram of residuals and a plot of the \n# predicted values against the residuals to test for deviations from\n# regression model assumptions\npar(mfrow=c(1,2))\nhist(resid(varModel))\nplot(resid(varModel) ~ predict(varModel))\ndev.off()\n\n### calculate Cij, Pii, Pjj to compare the components of the selection\n### differential\n\n## Cij is the covariance between an individual\'s sex and social partner sex\n## ratios calculated as the sum of products of the deviation of individual sex\n## from mean sex of population and the deviation of local sex ratio from the \n## population sex ratio\n\ncij <- as.character(levels(varAnalysis$level))\ncij <- as.numeric(levels(varAnalysis$level))\ncij <- rbind(cij, rep(NA, length(levels(varAnalysis$level))), \n             rep(NA, length(levels(varAnalysis$level))))\n            \nfor (i in 1:length(levels(varAnalysis$level))) {\n  cij[2,i] <- cov(varAnalysis$sexEncode[varAnalysis$level==levels(varAnalysis$level)[i]], varAnalysis$sexRatio[varAnalysis$level==levels(varAnalysis$level)[i]])\n  cij[3,i] <- cor.test(varAnalysis$sexEncode[varAnalysis$level==levels(varAnalysis$level)[i]], varAnalysis$sexRatio[varAnalysis$level==levels(varAnalysis$level)[i]])$p.value\n}\n\n## Pii is just the variance in sex at each level. It should ideall be the same...\n\npii <- as.character(levels(varAnalysis$level))\npii <- as.numeric(levels(varAnalysis$level))\npii <- rbind(pii, rep(NA, length(levels(varAnalysis$level))))\n\nfor (i in 1:length(levels(varAnalysis$level))) {\n  pii[2,i] <- var(varAnalysis$sexEncode[varAnalysis$level==levels(varAnalysis$level)[i]])\n}\n\n## Pjj is the variance in local sex ratios at each level\n\npjj <- as.character(levels(varAnalysis$level))\npjj <- as.numeric(levels(varAnalysis$level))\npjj <- rbind(pjj, rep(NA, length(levels(varAnalysis$level))))\npjj <- rbind(pjj, rep(NA, length(levels(varAnalysis$level))))\npjj <- rbind(pjj, rep(NA, length(levels(varAnalysis$level))))\n\nfor (i in 1:length(levels(varAnalysis$level))) {\n  pjj[2,i] <- var(varAnalysis$sexRatio[varAnalysis$level==levels(varAnalysis$level)[i] & varAnalysis$sex==""hermaphrodite""])\n  pjj[3,i] <- var(varAnalysis$sexRatio[varAnalysis$level==levels(varAnalysis$level)[i] & varAnalysis$sex==""female""])\n  pjj[4,i] <- var(varAnalysis$sexRatio[varAnalysis$level==levels(varAnalysis$level)[i]])\n}\n\n\n## Cjj is the correlation of social phenotypes\n\ncjj <- corr.test(localSex[,8:19])\n\n\n######################\n# Supplemental methods\n######################\n\n#Calculate what a hypothetical beta-N would be if females have\n#a two-fold fitness advantage over hermaphrodites\n\n# Dummy variable for fitness\nlocalSex$fakeFit <- NA\nlocalSex$sexCode <- NA\n\nlocalSex$sex <- as.factor(localSex$sex)\nlocalSex$sexCode[localSex$sex==""hermaphrodite""] = 1\nlocalSex$sexCode[localSex$sex==""female""] = 0\nlocalSex$fakeFit[localSex$sex==""hermaphrodite""] = 1\nlocalSex$fakeFit[localSex$sex==""female""] = 2\n\n# Calculate a variance-standardized value of the 0-1 index of sex\nlocalSex$sexStd <- standardize(localSex$sexCode)\n\n# Calculate mean-relativized value of fitness\nlocalSex$relFit <- relativize(localSex$fakeFit)\n\n# A simple linear regression of relative fitness on standardized sex\nsummary(lm(relFit ~ sexStd, data=localSex))\n\n\n############################################## \n#Plotting functions\n##############################################\n\n### FIGURE 1\n### Can use to plot the local sex ratio variance thing (like from arc)\nsub <- varAnalysis[varAnalysis$level==0.5 | varAnalysis$level==1.5 | \n                     varAnalysis$level==3.5 | varAnalysis$level==4.0,]\nsub$level <- factor(sub$level)\ng1 <- (ggplot(data=sub, \n             aes(x=Easting, y=Northing))\n      + geom_point(aes(fill = sexRatio), pch=21, color=""#484848"", size=1.5) \n      + scale_fill_gradientn(colours = gray.colors(n=2, start = 1, end = 0))\n      + scale_y_continuous(breaks = c(4137080:4137129), labels=rep(\'\', length(c(4137080:4137129))))\n      + scale_x_continuous(breaks = c(557111:557177), labels=rep(\'\', length(c(557111:557177))))\n      + theme (panel.grid.major = element_blank(),\n               panel.grid.minor = element_blank(),\n               panel.border = element_blank(),\n               panel.background = element_blank())\n      + facet_wrap(~level, scales=""free"")\n      )\n\ng1 \n\n\n### FIGURE 2\n# Crea']",2,"scale dependence, sex ratio, wild plant populations, social selection, spatially structured populations, fitness functions, sexually polymorphic plants, local sex ratio, gynodioecious plant Silene vulgaris, spatial distribution, fine-scale heterogeneity,"
Data from: Assortative interactions revealed in a fission-fusion society of Australian humpback dolphins,"Understanding individual interactions within a community or population provides valuable insight into its social system, ecology and, ultimately, resilience against external stimuli. Here, we used photo-identification data, generalised affiliation indices and social network analyses to investigate dyadic relationships, assortative interactions and social clustering in the Australian humpback dolphin (Sousa sahulensis). Boat-based surveys were conducted between May 2013 and October 2015 around the North West Cape, Western Australia. Our results indicated a fission-fusion society, characterised by non-random dyadic relationships. Assortative interactions were identified both within and between sexes, and were higher amongst members of the same sex, indicating same-sex preferred affiliations and sexual segregation. Assortative interactions by geographic locations were also identified, but with no evidence of distinct social communities or clusters, or affiliations based on residency patterns. We noted high residency amongst females. Models of temporal patterns of association demonstrated variable levels of stability, including stable (preferred companionships) as well as fluid (casual acquaintances) associations. We also demonstrated some social avoidance. Our results point to greater social complexity than previously recognised for humpback dolphins and, along with knowledge of population size and habitat use, provide the necessary baseline upon which to assess the influence of increasing human activities on this endemic, Vulnerable species.","['install.packages(""maptools"")\r\ninstall.packages(""sp"")\r\ninstall.packages(""rgeos"")\r\ninstall.packages(""adehabitatHR"")\r\n\r\nrequire(maptools)\r\n\r\nsetwd(""C:/Workspace/hunt0176/Workspace/R Working Directory/Ss_HomeRange_Adehabitat"")\r\n\r\nfn <- ""C:/Workspace/hunt0176/Workspace/R Working Directory/Ss_HomeRange_Adehabitat/Ss_HR_Adehabitat_setup_UTM.shp""\r\n\r\nshp.pts <- readShapePoints(fn, verbose=T)\r\n#MUST MATCH YOUR NUMBER OF RECORDS\r\n\r\nxys <- coordinates(shp.pts)\r\nnames(shp.pts)\r\n\r\nids <- shp.pts$ID\r\nidsp <- data.frame(ids) \r\ncoordinates(idsp) <- xys\r\nclass(idsp)\r\nhead(idsp)\r\n\r\n#Give a UTM reference system:\r\nproj4string(idsp) <- CRS(""+proj=utm +zone=50S +ellps=WGS84"") #NB. NWC region is UTM50S\r\nhead(idsp)\r\nidsp #will show you the list\r\n\r\nlibrary(adehabitatHR)\r\n\r\nclu2 <- clusthr(idsp)\r\nclass(clu2)\r\nclu2\r\n\r\nlength(clu2)  #NB. This number has to correspond with the total number of individuals (i.e. 50)\r\n\r\nplot(clu2)\r\n\r\nkud <- kernelUD(idsp[,1], \r\n         h = ""href"", \r\n         grid = 200, \r\n         same4all = TRUE, \r\n         hlim = c(0.1, 1.5), \r\n         kern = c(""bivnorm""), \r\n         extent = 0.5) \r\nkud\r\nimage(kud)\r\n#THIS WILL JUST PLOT THE HOME RANGES ON A GRID; CHECK THAT MAKES SENSE WITH YOUR INDIVIDUAL\'S DISTRIBUTION\r\n\r\n#You may need to alter your h-value, so you need to determine what \'href\' values for each individual are\r\n#You can query this specifically, where 1,2,21,52 etc are your individual IDs \r\n#You can do this for all or some IDs, so you get an idea of what h values href is using\r\n\r\nkud[[1]]@h\r\nkud[[2]]@h\r\nkud[[21]]@h\r\nkud[[52]]@h\r\n\r\n#You then need to convert the object estUD (i.e. kud) in homeranges to shp to open and view in ArcMap\r\n#The purpose of doing this is so you can see all individual homeranges and whether or not\r\n#they are too expanded or overlap land\r\n\r\nhomerange<- getverticeshr(kud)\r\nwritePolyShape (homerange, ""utilizationhref"")\r\n\r\n#If homeranges of IDs seem suitable (in ArcMap), then proveed with calculation of homerange overlap (below)\r\n##############################################################################\r\n#If homeranges of IDs not suitable (in ArcMap), you will need to estimate the UD with a different value of h:\r\n#E.g. h=400\r\n\r\nkudh400 <- kernelUD(idsp[,1], \r\n                h = 400, \r\n                grid = 200, \r\n                same4all = TRUE, \r\n                hlim = c(0.1, 1.5), \r\n                kern = c(""bivnorm""), \r\n                extent = 0.5) \r\nhomerange<- getverticeshr(kudh400)\r\nwritePolyShape (homerange, ""utilizationh400"")\r\n\r\n#Re-check in ArcMap again and if needed, re-do above steps with different h-value until it is suitable.\r\n##############################################################################\r\n\r\n#Estimating kernel home range overlap:\r\n\r\nkov<-kerneloverlaphr(kud, meth=""UDOI"",percent=95, conditional=TRUE) #THIS WILL CALCULATE THE OVERLAP\r\n\r\nwrite.table(kov, file = ""UDOI.csv"", sep = "","", col.names = NA,qmethod = ""double"")\r\n#THIS IS THE RESULTING FILE and WILL BE STORED IN THE DIRECTORY WHERE R FILES ARE STORED;; \r\n#then you have to open it with Excel and check the matrix, check that the names and the rest \r\n#matches exactly as the ID names in your input file in SOCPRoG. \r\n#For example - input file for R the IDs were 1,2,3,4 etc, but in SOCROG are S001, S002, S003 etc\r\n# So in the resulting matrix you will have to add those letters/prefix numbers to each ID, use the concatenate function in Excel if needed.  \r\n\r\n# Next steps:\r\n#1. Upload the Excel csv file to SOCPROG in the GAI module.\r\n#2. Remember to add the short name at the bottom of the upload screen or it will make problems (e.g. UDOI or HR, similar)\r\n#3. Check that the matrix looks fine in the SOCPROG/DOS screen\r\n#4. Add gregariousnes & temporal overlap predictor variables (if haven\'t already)\r\n#5. Do MRQAP tests to check if variables are significant - if yes, click the GAI button\r\n#6. If variables not significant, remove, re-do MRQAP\r\n#7. Once all variables significant, click GAI button, you will enter next screen, then you can do the rest of your analyses\r\n']",2,"Assortative interactions, fission-fusion society, Australian humpback dolphins, photo-identification data, social network analyses, dyadic relationships, ecological resilience, external stimuli, generalised affiliation indices, social clustering, North West Cape, Western Australia"
"Sociality and tattoo skin disease among bottlenose dolphins in Shark Bay, Australia","Social behavior is an important driver of infection dynamics, though identifying the social interactions that foster infectious disease transmission is challenging. Here we examine how social behavior impacts disease transmission in Indo-Pacific bottlenose dolphins (Tursiops aduncus) using an easily identifiable skin disease and social network data. We analyzed tattoo skin disease (TSD) lesions based on photographs collected as part of a 34-year longitudinal study in relation to the sociality of T. aduncus using three metrics (degree, time spent socializing, and time in groups) and network structure, using the k-test. We show that calves with TSD in the second year of life associated more with TSD-positive individuals in the first year of life compared with calves that did not have TSD. Additionally, the network k-test showed that the social network links are epidemiologically relevant for transmission. However, degree, time spent in groups, and time spent socializing were not significantly different between infected and uninfected groups. Our findings indicate that association with infected individuals is predictive of an individual's risk for TSD and that the social association network can serve as a proxy for studying the epidemiology of skin diseases in bottlenose dolphins.","['###Get averaged degree, actual degree, infected degree\n\noptions(stringsAsFactors = FALSE)\n\nSurveyFile<-read.csv(""allsightings.csv"")\nFocalFile<-read.csv(""allfocals.csv"")\n\ncalves<-unique(FocalFile$dolphin_id)\nn<-length(calves)\n\n#format dates\nSurveyFile$date<-as.Date(SurveyFile$date)\nFocalFile$birthdate<-as.Date(FocalFile$birthdate)\n\n#calculate average degree by resampling sets of 5 surveys\nindivs<-split(SurveyFile, SurveyFile$dolphin_id)\n\nresults<-list()\n\nset.seed(2019) \n\nfor (i in 1:n){\n  \n  focal<-calves[i]\n  start<-FocalFile$birthdate[which(FocalFile$dolphin_id==focal)]\n  end<-start+365\n  surveys<-indivs[[focal]]\n  trim_surveys<-surveys[(surveys$date>=start & surveys$date<=end),]\n  observation_ids<-unique(trim_surveys$observation_id)\n  #number of surveys\n  total<-length(observation_ids)\n  \n  if(length(observation_ids)<5) {next} else{\n    \n    subsample_degree<-replicate(1000,{\n      \n      sobservation_ids<-sample(observation_ids, 5)\n      associates<-SurveyFile$dolphin_id[which(SurveyFile$observation_id %in% sobservation_ids)]\n      nassoc<-length(unique(associates[which(associates!=focal)]))\n      return(nassoc)})\n  }\n  \n  mean_degree<-mean(unlist(subsample_degree))\n  \n  #total and infected number of associates\n  associates<-SurveyFile[,c(""dolphin_id"", ""infection_status"")][which(SurveyFile$observation_id %in% observation_ids),]\n  assoc<-aggregate(infection_status~dolphin_id, data=associates, max)\n  assoc<-assoc[assoc$dolphin_id!=focal,]\n\n  ind_results<-data.frame(dolphin_id=focal, \n                          avg_degree=mean_degree,\n                          true_degree=nrow(assoc),\n                          infected_degree=sum(assoc$infection_status),\n                          total_surveys=total)\n  results[[i]]<-ind_results\n}\n\nmodeldata<-as.data.frame(do.call(""rbind"", results))\n\nmodeldata$proportion_infected<-modeldata$infected_degree/modeldata$true_degree\n\n#Merge with FocalFile\n\nmodeldata<-merge(FocalFile, modeldata, by=""dolphin_id"")\n\nwrite.csv(modeldata, ""modeldata.csv"", row.names = FALSE)\n', '###Run models on real data \n\noptions(stringsAsFactors = FALSE)\n\nmodeldata<-read.csv(""modeldata.csv"")\n\nmodeldatasexed<-modeldata[-which(is.na(modeldata$sex)),]\n\nmodelfocal<-modeldatasexed[-which(is.na(modeldatasexed$proportion_groups)),]\n\nlibrary(logistf)\n\n#effect on degree on disease\ndegree_log<-logistf(formula = tsd ~ avg_degree + sex, data = modeldatasexed, pl = TRUE,\n                    alpha = 0.05, firth = TRUE)\n\nsummary(degree_log)\n\ninfdegree_log<-logistf(formula = tsd ~ proportion_infected + sex, data = modeldatasexed, pl = TRUE,\n                       alpha = 0.05, firth = TRUE)\n\nsummary(infdegree_log)\n\nsocial_log<-logistf(formula = tsd ~ proportion_social + sex, data = modelfocal, pl = TRUE,\n                    alpha = 0.05, firth = TRUE)\n\nsummary(social_log)\n\ngroups_log<-logistf(formula = tsd ~ proportion_groups + sex, data = modelfocal, pl = TRUE,\n                    alpha = 0.05, firth = TRUE)\n\nsummary(groups_log)\n', '#Randomzing Functions\n\n#Directly from Strona et al. 2014 (Nature Communications)\n\ncurve_ball<-function(m){\n  RC=dim(m)\n  R=RC[1]\n  C=RC[2]\n  hp=list()\n  for (row in 1:dim(m)[1]) {hp[[row]]=(which(m[row,]==1))}\n  l_hp=length(hp)\n  for (rep in 1:(5*l_hp)){\n    AB=sample(1:l_hp,2)\n    a=hp[[AB[1]]]\n    b=hp[[AB[2]]]\n    ab=intersect(a,b)\n    l_ab=length(ab)\n    l_a=length(a)\n    l_b=length(b)\n    if ((l_ab %in% c(l_a,l_b))==F){\n      tot=setdiff(c(a,b),ab)\n      l_tot=length(tot)\n      tot=sample(tot, l_tot, replace = FALSE, prob = NULL)\n      L=l_a-l_ab\n      hp[[AB[1]]] = c(ab,tot[1:L])\n      hp[[AB[2]]] = c(ab,tot[(L+1):l_tot])}\n    \n  }\n  rm=matrix(0,R,C)\n  for (row in 1:R){rm[row,hp[[row]]]=1}\n  dimnames(rm)<-dimnames(m) #added retention of dimnames\n  rm\n}\n\n\nlibrary(SocGen) #devtools::install_github(""vjf2/SocGen"")\n\nmake_random<-function(sf){\n  rand1 <- list()\n  \n  for (i in unique(sf$twomonth_interval)) {\n    thesemonths <- sf[sf$twomonth_interval == i, ]\n    \n    infected <- unique(thesemonths$dolphin_id[which(thesemonths$infection_status == 1)])\n    \n    #make matrix\n    m <- dat2mat(data.frame(thesemonths$observation_id, thesemonths$dolphin_id, 1))\n    m[is.na(m)] <- 0\n    \n    m1 <- curve_ball(m)\n    \n    #convert back to original format\n    d1 <- mat2dat(m1)\n    d1$infection_status <- ifelse(d1$ID2 %in% infected, 1, 0)\n    \n    #add infection status\n    rand1[[i]] <- d1[d1$values == 1, c(1,2,4)]\n  }\n  \n  randsurv <- data.table::rbindlist(rand1)\n  \n  randsurv$date <- sf$date[match(randsurv$ID1, sf$observation_id)]\n  \n  names(randsurv)[1:2]<-c(""observation_id"", ""dolphin_id"")\n  \n  return(as.data.frame(randsurv))\n  \n}\n', '#Randomize data using curveball algorithm\n\noptions(stringsAsFactors = FALSE)\n\nsf<-read.csv(""allsightings.csv"")\nmodeldata<-read.csv(""modeldata.csv"")\nmodeldatasexed<-modeldata[-which(is.na(modeldata$sex)),]\nmodeldatasexed$birthdate<-as.Date(modeldatasexed$birthdate)\n\n#set up empty matrices\n\ncalves<-sort(modeldatasexed$dolphin_id)\nn<-length(calves)\n\nmonths<-unique(sf$twomonth_interval)\ndegree_matrix<-matrix(NA, nrow=n, ncol=1000)\ninfected_degree_matrix<-matrix(NA, nrow=n, ncol=1000)\nrownames(degree_matrix)<-calves\n\ntp<-aggregate(observation_id~twomonth_interval, data=sf, function(x) length(unique(x)))\nmean(tp$observation_id) #169 sightings per period\n\nsource(\'3aRandomizingFunctions.R\')\n\nset.seed(2019)\n\nfor (q in 1:1000) {\n  \n  randsurveys<-make_random(sf)\n  \n  #now calculate degree and infected degree for all focal individuals\n  \n  indivs<-split(randsurveys, randsurveys$dolphin_id)\n  \n  results<-list()\n  \n  for (i in 1:n){\n\n    focal<-calves[i]\n    start<-modeldatasexed$birthdate[which(modeldatasexed$dolphin_id==focal)]\n    end<-start+365\n    surveys<-indivs[[focal]]\n    trim_surveys<-surveys[(surveys$date>=start & surveys$date<=end),]\n    observation_ids<-unique(trim_surveys$observation_id)\n    \n    #total and infected number of associates\n    associates<-randsurveys[,c(""dolphin_id"", ""infection_status"")][which(randsurveys$observation_id %in% observation_ids),]\n    assoc<-aggregate(infection_status~dolphin_id, data=associates, max)\n    assoc<-assoc[assoc$dolphin_id!=focal,]\n    \n    ind_results<-data.frame(dolphin_id=focal, \n                            true_degree=nrow(assoc),\n                            infected_degree=sum(assoc$infection_status))\n    results[[i]]<-ind_results\n    }\n  \n  fakemodeldata<-as.data.frame(do.call(""rbind"", results))\n  \n  degree_matrix[,q]<-fakemodeldata$true_degree\n  infected_degree_matrix[,q]<-fakemodeldata$infected_degree\n}\n\nproportion_infected_matrix<-infected_degree_matrix/degree_matrix\n\nwrite.csv(proportion_infected_matrix, ""curveball_proportion_infected.csv"")\n', '#Compare real and random\n\noptions(stringsAsFactors = FALSE)\n\nmodeldata<-read.csv(""modeldata.csv"")\nmodeldatasexed<-modeldata[-which(is.na(modeldata$sex)),]\n\nproportion_infected_matrix<-read.csv(""curveball_proportion_infected.csv"", row.names = 1)\n\nrealnsim<-merge(modeldatasexed, proportion_infected_matrix, \n                by.x=""dolphin_id"", by.y=0,all.x=TRUE, all.y=FALSE)\n\nlibrary(logistf)\n\nflog<-logistf(formula = tsd ~ proportion_infected + sex, data = modeldatasexed, pl = TRUE,\n              alpha = 0.05, firth = TRUE)\n\nactualp<-flog$prob[2]\nactualB<-coef(flog)[2]\nactualse<-sqrt(flog$var[2,2])\n\nrandp<-list()\nrandB<-list()\nrandse<-list()\n\nfor (i in 1:1000){\n  \n  flog<-logistf(formula = tsd ~ realnsim[,i+11] + sex, data = realnsim, pl = TRUE,\n                alpha = 0.05, firth = TRUE)\n  \n  randp[[i]]<-flog$prob[2]\n  randB[[i]]<-flog$coefficients[2]\n  randse[[i]]<-sqrt(flog$var[2,2])\n}\n\n\nallrandB<-unlist(randB)\nallrandse<-unlist(randse)\nallrandz<-allrandB/allrandse\nactualz<-actualB/actualse\n\nBtile<-round(ecdf(allrandB)(actualB)*100,0)\nZtile<-round(ecdf(allrandz)(actualz)*100,0)\n\n\n\npdf(file=""supplemental_histograms.pdf"", width=9.5, height=5.9)\npar(mfrow=c(1,2), xpd=NA, mar=c(5.1, 4.1, 2.1, 2.1))\nhg<-hist(allrandB, probability = TRUE, col=""grey85"", border=NA,\n         main=NA, \n         xlab=""Coefficients generated from randomized data"", \n         ylab=NA,\n         axes=FALSE, \n         cex.lab=1.3)\n\naxis(1, cex.axis=1.3)\n\nsegments(actualB,0,actualB, max(hg$density)*1.02, col=""red"", lty=3, lwd=2)\ntext(x=actualB, y=max(hg$density)*1.05,\n     labels=expression(paste(""Observed "", beta, "" = 23rd %tile"")))\n\ny1<-density(allrandB, adjust=1.25, from=min(hg$breaks), to=max(hg$breaks))\nlines(y1$x, y1$y, lwd=2, col=""darkgrey"", lty=2)\n\n##plot 2\nhg<-hist(allrandz, probability = TRUE, col=""grey85"", border=NA,\n         main=NA, \n         xlab=""Z-values generated from randomized data"", \n         ylab=NA,\n         xlim=c(min(allrandz),max(c(allrandz, actualz))),\n         axes=FALSE, \n         cex.lab=1.3)\n\naxis(1, cex.axis=1.3)\n\nsegments(actualz,0,actualz, max(hg$density)*1.02, col=""red"", lty=3, lwd=2)\ntext(x=actualz, y=max(hg$density)*1.05, labels=""Observed z-value = 80th %tile"")\n\ny1<-density(allrandz, adjust=1.25, from=min(hg$breaks), to=max(hg$breaks))\nlines(y1$x, y1$y, lwd=2, col=""darkgrey"", lty=2)\n\ndev.off()\n\n\n\n\n\n\n']",2,"Sociality, tattoo skin disease, bottlenose dolphins, Shark Bay, Australia, infection dynamics, social behavior, infectious disease transmission, Indo-Pacific bottlenose dolphins, Tursiops aduncus, skin disease, social network data, tattoo"
"Experimental disruption of social structure reveals totipotency in the orchid bee, Euglossa dilemma","Eusociality has evolved multiple times across the insect phylogeny. Social insects with greater levels of social complexity tend to exhibit specialized castes with low levels of individual phenotypic plasticity. In contrast, species with simple social groups may consist of totipotent individuals that transition among behavioral and reproductive states. However, recent work has shown that in simple social groups, there can still be constraint on individual plasticity, caused by differences in maternal nourishment or social interaction. It is not well understood how these constraints arise, ultimately leading to the evolution of nonreproductive workers. Some species of orchid bees form social groups of a dominant and 1-2 subordinate helpers where all individuals are reproductive. Females can also disperse to start their own nest as a solitary foundress, which includes a nonreproductive phase, characterized by ovary inactivation, not typically expressed by subordinates. Little is known about individual flexibility across these trajectories. Here, using the orchid bee Euglossa dilemma, we assess the plasticity of subordinate helpers, finding that they are capable of the same behavioral, physiological, transcriptomic, and chemical changes seen in foundresses. Our results suggest that the lack of nonreproductive workers in E. dilemma is not due to a lack of subordinate plasticity.","['###### Euglossa dilemma experimental disruption analysis #####################\r\n\r\n#Variables for each behavioral group are represented by a single letter throughout all analyses\r\n# Group ID: D = dominant, G = Natural Guard, R = Isolated Subordinate, S = Subordinate\r\n# depending on the data set/analysis these groups may be represented by upper or lowercase letters, which will be specified before each analysis\r\n\r\n\r\n### Brood size (number of brood cells) comparison between isolated subordinates and natural guards ######\r\n\r\n#required packaged for analysis\r\nlibrary(car)\r\nlibrary(Steel.Dwass.test)\r\nlibrary(ggplot2)\r\nlibrary(ggpubr)\r\nlibrary(cowplot)\r\nlibrary(vegan)\r\nlibrary(ecodist)\r\nlibrary(gplots)\r\nlibrary(MASS)\r\nlibrary(RVAideMemoire)\r\nlibrary(corrplot)\r\nlibrary(randomForest)\r\n\r\nBroodData = read.csv(""BroodSize.csv"")\r\nBehavior = BroodData$Behavior\r\nCells = BroodData$Cells\r\n\r\n\r\n#Brood Cells vs Behavior\r\nfit <- aov(Cells ~ Behavior , data=BroodData)\r\nsummary(fit)\r\nleveneTest(Cells~Behavior, data =BroodData)\r\nshapiro.test(Cells)\r\n\r\n#data pass homogeneity of variances and normality so no need for kruskal test\r\nkruskal.test(Cells~Behavior, data = BroodData)\r\n\r\n\r\n#Boxplots\r\n\r\ngp = ggplot(BroodData, aes(x = Behavior, y = Cells))+\r\n        geom_boxplot(width=0.25)+\r\n        #geom_point(aes())+\r\n        theme_bw()+\r\n        theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+\r\n        geom_point(aes(fill = Behavior,shape = Behavior), size = 2, position=position_jitter(0.1)) +\r\n        scale_shape_manual(values = c(22,23,23,24,25))+\r\n        scale_fill_manual(breaks = c(""G"", ""R""),\r\n                          values=c(""purple"", ""palegreen4""))+\r\n        ylim(0,15)\r\n\r\ngp\r\n\r\n### Ovary size comparison among Reproductives, Natural guards, and Isolated subordinates ###\r\n\r\n#Note: In this data set dominants and subordinates are lumped together and labeled ""A"" instead of the ""D"" and ""S"" throughout the other files\r\n\r\nOvData = read.csv(""OvaryData.csv"")\r\n\r\nBehavior = OvData$Reproductive_Behavior\r\nOvary_Index = OvData$TotalIndex\r\n\r\n#Ovary Index vs Behavior\r\nfit <- aov(Ovary_Index ~ Behavior , data=OvData)\r\nsummary(fit)\r\nTukeyHSD(fit)\r\nleveneTest(Ovary_Index~Behavior, data =OvData)\r\nshapiro.test(Ovary_Index)\r\n\r\n# homogeneity of variances and normality confirmed, kruskal test not needed\r\nkruskal.test(Ovary_Index~Behavior, data = OvData)\r\n\r\n##Boxplots ###\r\n\r\n#behavior with dominants and subordinates separate for plotting\r\nBehavior_fine = OvData$Behavior_finescale\r\n\r\ngp = ggplot(OvData, aes(x = Behavior, y = Ovary_Index))+\r\n        geom_boxplot(width=0.25)+\r\n        geom_point(aes(fill = Behavior_fine,shape = Behavior_fine), size = 3, position=position_jitter(0.1)) +\r\n        scale_shape_manual(values = c(21,22,23,24,25))+\r\n        theme_bw()+\r\n        theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\r\n\r\ngp + scale_fill_manual(breaks = c(""D"", ""G"", ""R"", ""S""),\r\n                       values=c(""red"", ""purple"", ""palegreen4"", ""blue""))\r\n\r\n## Body size test ##\r\n\r\nBodySize = OvData$Intertegular\r\n\r\n\r\nfit <- aov(BodySize ~ Behavior , data=OvData)\r\nsummary(fit)\r\nTukeyHSD(fit)\r\nleveneTest(BodySize~Behavior, data =OvData)\r\nshapiro.test(BodySize)\r\n\r\n#fails normality so kruskal instead\r\nkruskal.test(BodySize~Behavior, data = OvData)\r\nSteel.Dwass(BodySize, OvData$Reproductive_Behavior_numbered)\r\n\r\n##body size boxplot\r\ngp = ggplot(OvData, aes(x = Behavior, y = BodySize))+\r\n        geom_boxplot(width=0.25)+\r\n        geom_point(aes(fill = Behavior_fine,shape = Behavior_fine), size = 2, position=position_jitter(0.1)) +\r\n        scale_shape_manual(values = c(21,22,23,24,25))+\r\n        theme_bw()+\r\n        theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\r\n\r\ngp + scale_fill_manual(breaks = c(""D"", ""G"", ""R"", ""S""),\r\n                       values=c(""red"", ""purple"", ""palegreen4"", ""blue""))\r\n\r\n\r\n##### Ovary size comparison with Saleh and Ramirez, 2019 ##########\r\n\r\n## ""OG"" in the data file refers to the samples from Saleh and Ramirez, 2019 and ""G"" refers to samples from this study\r\n\r\nOvData = read.csv(""Ovaries_Cross_Study_Comparison.csv"")\r\nBehavior = OvData$Behavior\r\nBehavior_fine = OvData$Behavior_fine\r\nOvarySize = OvData$Ovary_Index\r\n\r\n#Peaks vs Behavior\r\nfit <- aov(OvarySize ~ Behavior , data=OvData)\r\nsummary(fit)\r\n\r\n## Boxplot ###\r\n\r\ngp = ggplot(OvData, aes(x = Behavior, y = OvarySize))+\r\n        geom_boxplot(width=0.25)+\r\n        geom_point(aes(fill = Behavior_fine,shape = Behavior_fine), size = 2, position=position_jitter(0.1)) +\r\n        scale_shape_manual(values = c(21,22,23,24,25))+\r\n        theme_bw()+\r\n        theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\r\n\r\ngp\r\n\r\n########################## CHC analysis ##########################\r\n### NMDS first round is done with rare polymorphism samples present. Then the samples will be removed for Random Forest analysis as in the main text###########\r\n\r\nCHC = read.csv(""CHCs_all_samples.csv"")\r\ndim(CHC)\r\n\r\n##f']",2,"orchid bee, Euglossa dilemma, eusociality, social structure, totipotency, insect phylogeny, specialized castes, individual phenotypic plasticity, simple social groups, behavioral states, reproductive states, maternal nour"
"Data from: Singing behaviour of Ruby-crowned Kinglets (Regulus calendula) in relation to time-of-day, time-of-year, and social context","Observational field studies provide insight on the multifunctional nature of birdsong. For example, if song production were limited to pre-fertilization, then that would suggest a mate attraction function. If it were used throughout the breeding season and in response to intruding males, then that would suggest a territorial defence function. In the present study, we determined the daily and seasonal singing patterns of male Ruby-crowned Kinglets (Regulus calendula) in Labrador, Canada, using microphone arrays in two breeding seasons. Using a playback experiment, we simulated a territorial intrusion to compare the structure of songs produced while defending a territory to the structure of songs produced during solo and contest singing. Singing peaked in the early part of the breeding season and then declined continuously for the remainder of the season, which suggests that the songs function in mate attraction. Singing peaked 2-3 h after dawn, and then declined steadily until it stopped at 2200 h. Some nocturnal singing was observed, but no dawn singing was observed. A high probability of signal overlap by heterospecific songs at dawn would hinder signal recognition and explain the observed delay in peak singing activity. Vocal responses to playback suggested a function in territory defence. However, there were no significant differences in the duty cycle, frequency modulation, and bandwidth of songs in relation to the context of song production, though songs were shorter in the intrusion context than during solo singing. Overall, the study provides the first quantitative description of the effects of time of day, time of year, and social context on singing behaviour in this understudied species.","['# this script calculates several structural variables from raw data created by Luscinia; the signals\r\n# are ruby-crowned kinglet songs recorded during solo singing, contest competitions, or during simulated territorial intrusions in\r\n# Labrador, Canada in 2016; solo and contest songs were recorded with an SM3 microphone array; those during territorial\r\n# intrusions were recorded with a Marantz PMD-660 and Sennheiser mic; only those songs without interference, and those\r\n# from within ~15 m of the SM3 recorder, were included in this analysis\r\n\r\n#Required packages\r\nlibrary(""lme4"")\r\nlibrary(""lmerTest"")\r\nlibrary(""dplyr"")\r\nlibrary(""ggplot2"")\r\nlibrary(""multcomp"")\r\nlibrary(""gridExtra"") #load gridExtra to put all the plots together\r\n\r\nrm(list = ls()) # clear working memory\r\nsetwd(""C:/Users/Mohammad Fahmy/Desktop/Honours 2016-2017/Operation Kinglet/Localisation Script/Dryad"") #set working directory\r\n                                       #####Response variables calculation#####\r\n# open datafiles\r\ndata.song <- read.csv(""song level data v02.csv"")\r\ndata.element <- read.csv(""element level data v02.csv"")\r\ndata.spectrum <- read.csv(""peak frequency data v02.csv"") # open data file with peak frequency values for each spectrum\r\ndata.spectrum$songID <- paste(data.spectrum$songID, "".wav"", sep="""") # add \'.wav\' to each songID to match data.song\r\n\r\n# add absolute change in peak frequency from one Tbin to the next; first Tbin of a syllable is left blank\r\nfor(i in 1:nrow(data.spectrum)) {\r\n\tif(data.spectrum$Tbin[i] == 1) {\r\n\t\tdata.spectrum$peak.freq.change[i] <- """"} else {data.spectrum$peak.freq.change[i] <- abs(data.spectrum$peak.freq[i] - data.spectrum$peak.freq[i-1])\r\n\t}\r\n}\r\n\r\ndata.spectrum$peak.freq.change <- as.numeric(data.spectrum$peak.freq.change) # change to numeric\r\n\r\n# calculate element rate (elements / second)\r\ndata.song$element.rate <- data.song$Nelements / data.song$song.duration * 1000\r\n\r\n# calculate other song-level parameters\r\nfor (i in 1:nrow(data.song)) {\r\n\r\n# calculate duty cycle of song\r\n\tdata.song$duty.cycle[i] <- sum(data.element$duration[data.element$songID == data.song$songID[i]]) / data.song$song.duration[i]\r\n\r\n# calculate percentile5, percentile50, percentile 95, and bandwidth (i.e., percentile95-percent5) from all peak frequency values\r\n# of a given song\r\n\tdata.song$percentile05[i] <- quantile(data.spectrum$peak.freq[data.spectrum$songID == data.song$songID[i]], 0.05)\r\n\tdata.song$percentile50[i] <- quantile(data.spectrum$peak.freq[data.spectrum$songID == data.song$songID[i]], 0.50)\r\n\tdata.song$percentile95[i] <- quantile(data.spectrum$peak.freq[data.spectrum$songID == data.song$songID[i]], 0.95)\r\n\tdata.song$bandwidth[i] <- data.song$percentile95[i] - data.song$percentile05[i]\r\n\t\r\n# calculate frequency modulation in song as sum of all absolute changes in peak frequency, divided by sum of all element durations\r\n\tdata.song$freq.mod[i] <- sum(na.omit(data.spectrum$peak.freq.change[data.spectrum$songID == data.song$songID[i]])) / sum(data.element$duration[data.element$songID == data.song$songID[i]])\r\n\t\r\n}\r\n\r\n#View frequency of songs in each context for each array \r\nTABLE<-table(data.song$array, data.song$context)\r\nView(TABLE)\r\ntable(data.song$context)\r\n\r\n#Make a column with duration in seconds instead of milliseconds\r\ndata.song$duration.sec <- data.song$song.duration / 1000\r\n\r\n\r\n                                       #####Mixed effects models#####\r\n#Relevel the treatments to obtain the mean and st error of the response variable from the model. \r\n#Calculating mean and st error from raw dataset is not accurate when one has repeated measures for each individual\r\n\r\ndata.song <- within(data.song, context <- relevel(context, ref = ""solo"")) #change the \'ref =\' to the different and rerun the models \r\n\r\n#Document the mean and st error of the intercept of the models in a spreadsheet to be used in ggplot\r\n\r\n#model1: response= song duration, fixed factor=context(3 levels), random effect=array\r\nsong.duration<- lmer(duration.sec~context+(1|array), data.song)\r\nsummary(song.duration)\r\nanova(song.duration)\r\n#multiple comparisons \r\nsummary(glht(song.duration, linfct = mcp(context = ""Tukey"")),test = adjusted(""holm""))\r\n\r\n\r\n#Assumptions of normality\r\n#Check for fanning, skewing, no patterns\r\nres1<-resid(song.duration)\r\nfit1<-fitted(song.duration)\r\nlag.plot(res1,\r\n         main = ""Figure 1"",\r\n         diag = FALSE,\r\n         do.lines = FALSE)\r\n#should have the same vertical spreading for homogeneity \r\nplot(fit1,res1,\r\n     xlab = ""Fits"",\r\n     ylab = ""Residuals"",\r\n     main = ""Figure 2"")\r\n#should be a straight line\r\nqqnorm(res1,\r\n       main = ""Figure 3"")\r\nhist(res1)\r\n\r\n\r\n###model2: response= Number of elements, fixed factor=context(3 levels), random effect=array\r\nelements<- lmer(Nelements~context+(1|array), data.song)\r\nsummary(elements)\r\nanova(elements)\r\nsummary(glht(elements, linfct = mcp(context = ""Tukey"")),test = adjusted(""holm""))\r\n\r\n\r\n#Assumptions of normality\r\n#Check for fanning, skewing, no patterns\r\nres2<-resid(elements)\r\nf']",2,"Ruby-crowned Kinglets, birdsong, time-of-day, time-of-year, social context, observational field studies, multifunctional, mate attraction, breeding season, territorial defence, Labrador, Canada, microphone arrays, playback experiment, signal recognition,"
Data from: Achromatic plumage brightness predicts stress resilience and social interactions in tree swallows (Tachycineta bicolor),"Theory suggests that signal honesty may be maintained by differential costs for high and low quality individuals. For signals that mediate social interactions, costs can arise from the way that a signal changes the subsequent social environment via receiver responses. These receiver-dependent costs may be linked with individual quality through variation in resilience to environmental and social stress. Here, we imposed stressful conditions on female tree swallows (Tachycineta bicolor) by attaching groups of feathers during incubation to decrease flight efficiency and maneuverability. We simultaneously monitored social interactions using an RFID network that allowed us to track the identity of every individual that visited each nest for the entire season. Prior to treatments, plumage coloration was correlated with baseline and stress-induced corticosterone. Relative to controls, experimentally challenged females were more likely to abandon their nest during incubation. Overall, females with brighter white breasts were less likely to abandon, but this pattern was only significant under stressful conditions. In addition to being more resilient, brighter females received more unique visitors at their nest box and tended to make more visits to other active nests. In contrast, dorsal coloration did not reliably predict abandonment or social interactions. Taken together, our results suggest that females differ in their resilience to stress and that these differences are signaled by plumage brightness, which is in turn correlated with the frequency of social interactions. While we do not document direct costs of social interaction, our results are consistent with models of signal honesty based on receiver-dependent costs.","['### Code to analyze color and abandonment data. Written by Conor Taff. \n##\tRun in R version 3.3.3. Last updated on 1/11/2019. cct63@cornell.edu or cct663@gmail.com\n\tlibrary(lme4)\n\tlibrary(rethinking)\n\tlibrary(blmeco)\n\tlibrary(plotrix)\n\tlibrary(MuMIn)\n\tlibrary(sjPlot)\n\tlibrary(optimx)\n\n## Run a separate script that inputs the full set of RFID records and then calculated the number of visits\n\t## and unique visits for each day of the season rather than at a season long level. A second data frame\n\t## is also created that has the average and sd for unique visitors across days of the season.\n\t## This takea a few minutes to run because it is loading the big table of raw RFID reads.\n\t\n\tsource(\'Code_Make_DailyRFID.r\',print.eval=TRUE)\n\n## Read in the master file that has every RFID read from all boxes for the entire\n\t## 2015 season in Ithaca.\n\t\tall<-d0\t\t# Depends on the script above having been run already\n\t\tall.orig<-all\t# saving this version unmodified\n\t\t# Saving separate subsets for the two sites\n\t\t\tall.cu2<-subset(all,all$Site==""Unit 2"")\n\t\t\tall.cu4<-subset(all,all$Site==""Unit 4"")\n\t\tall<-all.orig\t\t# Change here if you want to run separately for sites\n\t\t\n## Read in other files. First is main data with treatments, etc. Second two are generated by script\n\t## run in line 12 above.\n\t\tmain.d<-read.delim(""Input_Main_Data.txt"")\n\t\tday.d<-read.csv(""Input_Visits_By_Day.csv"")\n\t\tday.av.d<-read.csv(""Input_Avg_Daily_Visits.csv"")\n\t\tday.av.d$UnitBox<-gsub(""-"",""_"",day.av.d$UnitBox)\n\t\tcolnames(day.av.d)[2]<-""unitbox""\n\t\tmain.d<-join(main.d,day.av.d,""unitbox"")\n\t\t\n## This calculates and plots the total number of RFID reads on every day of the season pooling all units.\n\t\tdate<-as.data.frame(seq(min(all$JDate),max(all$JDate),by=1))\n\t\tcolnames(date)<-""JDate""\n\t\tfor(i in 1:nrow(date)){\n\t\t\tdate$Count[i]<-nrow(subset(all.orig,all.orig$JDate==date$JDate[i]))\n\t\t}\n\t\tpdf(""Output_Fig_Total_RFID_Reads.pdf"",width=11,height=5)\n\t\t\tplot(date$JDate,date$Count,xlab=""Julian Date"",ylab=""Total RFID Reads"",\n\t\t\t\t\tpch=16,main=""Ithaca"",xaxt=""n"")\n\t\t\tlines(date$JDate,date$Count,lwd=2,col=""black"")\n\t\t\ttext(176,200000,""Total of 2,433,455 reads"")\n\t\t\taxis(1,at=seq(120,190,2))\n\t\tdev.off()\n\n## Make a figure that shows the number of active PIT tags and RFIDs on each day of season\n\t## Generate counts for how many unique birds were observed on any RFID on each day\n\t\t# of the breeding season.\n\t\t\tall.rfid<-as.data.frame(unique(all$RFID))\n\t\t\tcolnames(all.rfid)<-c(""RFID"")\t\n\t\t\tfor(i in 1:nrow(all.rfid)){\n\t\t\t\tsub<-subset(all,all$RFID==all.rfid$RFID[i])\n\t\t\t\tall.rfid$First[i]<-min(na.omit(sub$JDate))\n\t\t\t\tall.rfid$Second[i]<-max(na.omit(sub$JDate))\n\t\t\t}\n\t\t\tall.rfid$Diff<-all.rfid$Second-all.rfid$First\n\t\t\tdates<-as.data.frame(seq(min(all.rfid$First),max(all.rfid$Second),1))\n\t\t\tcolnames(dates)<-""JDate""\n\t\t\tfor(i in 1:nrow(dates)){\n\t\t\t\tdates$Count[i]<-nrow(subset(all.rfid,all.rfid$First<dates$JDate[i]&\n\t\t\t\t\tall.rfid$Second>dates$JDate[i]))\n\t\t\t}\t\n\t\tpdf(""Output_Fig_Active_Tags.pdf"",width=6,height=6)\t\n\t\t\tplot(dates$JDate,dates$Count,xlab=""Julian Date"",ylab=""Count"",\n\t\t\t\tpch=16,xlim=c(135,185),ylim=c(0,100),main="""")\n\t\t\tlines(dates$JDate,dates$Count,lwd=2,col=""black"")\n\t\t\t#barplot(dates$Count)\n\t\t\t#axis(1,at=c(-10,100),labels=FALSE)\n\t## Generate counts for the number of RFID units running on each day of the breeding season\n\t\t\tall.unit<-as.data.frame(unique(all$UnitBox))\n\t\t\tcolnames(all.unit)<-c(""UnitBox"")\t\n\t\t\tfor(i in 1:nrow(all.unit)){\n\t\t\t\tsub<-subset(all,all$UnitBox==all.unit$UnitBox[i])\n\t\t\t\tall.unit$First[i]<-min(na.omit(sub$JDate))\n\t\t\t\tall.unit$Second[i]<-max(na.omit(sub$JDate))\n\t\t\t}\n\t\t\tall.unit$Diff<-all.unit$Second-all.unit$First\n\t\t\tdates2<-as.data.frame(seq(min(all.unit$First),max(all.unit$Second),1))\n\t\t\tcolnames(dates2)<-""JDate""\n\t\t\tfor(i in 1:nrow(dates2)){\n\t\t\t\tdates2$Count[i]<-nrow(subset(all.unit,all.unit$First<dates2$JDate[i]&\n\t\t\t\t\tall.unit$Second>dates2$JDate[i]))\n\t\t\t}\t\t\n\t\t\tpoints(dates2$JDate,dates2$Count,xlab=""Julian Date"",ylab=""Number of active RFID units"",\n\t\t\t\tcol=""slateblue"",pch=16)\n\t\t\tlines(dates2$JDate,dates2$Count,lwd=2,col=""slateblue"")\n\t\t\tlegend(""topleft"",c(""Active PITs"",""Active RFID""),lwd=2,col=c(""Black"",""Slateblue""))\n\t\t\t#barplot(dates2$Count)\n\t\t\t#axis(1,at=c(-10,100),labels=FALSE)\n\t\t\tdev.off()\n\n## Make a plot showing number of active PIT tags in a different way\t\t\t\n\t## Plot number of PITs a different way\n\t\tpdf(""Output_Fig_Active_PIT.pdf"",width=5.7,height=9.1)\n\t\tall.rfid$seq<-seq(1,nrow(all.rfid),1)\n\t\tplot(1,1,xlim=c(min(all.rfid$First),max(all.rfid$Second)),ylim=c(0,nrow(all.rfid)),type=""n"",yaxt=""n"",\n\t\t\txlab=""Julian Date"",ylab=""Individual Birds (one per line)"",main=""Active PITs"")\n\t\tfor(i in 1:nrow(all.rfid)){\n\t\t\tlines(c(all.rfid$First[i],all.rfid$Second[i]),rep(all.rfid$seq[i],2))\n\t\t}\n\t\tdev.off()\n\t\t\n## Calculate number of trips for every day that PIT was detected for each bird\n\ttrips<-NA\n\ttemp<-NA\n\tall2<-subset(all,all$focal==""No"")\n\tfor(i in 1:nrow(all.rfid)){\n\t\tfor(k in 1:all.rfid[i,4]){\n\t\t\ttemp[k]<-paste(all.rfid[i,1],all.rfid[i,3]+k-1,sep=""_"")\n\t\t}\n\t\ttrips<-append(trips', '\n## Load library\n\tlibrary(plyr)\n\n## Read data\n\td0<-read.csv(""Input_AllVisitsMerged.csv"")\n\t\n\td<-read.delim(""Input_ties_by_row.txt"")\n\t\t\n## Make a new column with unique ID for each RFID/day combo\n\td0$Visited.ubd<-paste(d0$UnitBox,d0$JDate,sep=""_"")\n\n## Make a data frame with this unique code\n\td0$Visited.ubd<-gsub(""-"",""_"",d0$Visited.ubd)\n\tlists<-as.data.frame(unique(d0$Visited.ubd))\n\tcolnames(lists)<-""Visited.ubd""\n\td2<-join(lists,d0,""Visited.ubd"",match=""first"")\n\td2<-d2[,c(""Visited.ubd"",""JDate"",""Year"",""Offset"",""UnitBox"",""FemaleRFID"",""MaleRFID"")]\n\td2$fRFID_Day<-as.character(paste(d2$FemaleRFID,""_"",d2$JDate,sep=""""))\n\td2$mRFID_Day<-as.character(paste(d2$MaleRFID,""_"",d2$JDate,sep=""""))\n\t\n\td$Visited.ubd<-paste(d$VisitedUnitBox,d$JDate,sep=""_"")\n\t\n\td$fRFID_Day<-as.character(paste(d$VisitorRFID,""_"",d$JDate,sep=""""))\n\td$mRFID_Day<-as.character(paste(d$VisitorRFID,""_"",d$JDate,sep=""""))\n\n## For each day, count number of male and female visits total and unique\n\tfor(i in 1:nrow(d2)){\n\t\tsub.m<-subset(d,d$Visited.ubd==d2$Visited.ubd[i]&d$VisitorSex==""M"")\n\t\tsub.f<-subset(d,d$Visited.ubd==d2$Visited.ubd[i]&d$VisitorSex==""F"")\n\t\tsub.t<-subset(d,d$fRFID_Day==d2$fRFID_Day[i])\n\t\tsub.t.male<-subset(d,d$mRFID_Day==d2$mRFID_Day[i])\n\t\td2$All.m[i]<-nrow(sub.m)\n\t\td2$Uni.m[i]<-length(unique(sub.m$VisitorBand))\n\t\td2$All.f[i]<-nrow(sub.f)\n\t\td2$Uni.f[i]<-length(unique(sub.f$VisitorBand))\n\t\td2$All.t[i]<-nrow(sub.t)\n\t\td2$Uni.t[i]<-length(unique(sub.t$VisitedUnitBox))\n\t\td2$All.t.m[i]<-nrow(sub.t.male)\n\t\td2$All.t.m[i]<-length(unique(sub.t.male$VisitedUnitBox))\n\t}\n\n\td2$All.v<-d2$All.m+d2$All.f\n\td2$All.u<-d2$Uni.m+d2$Uni.f\n\t\n## Write the output to a new file to be used in the other code\n\twrite.csv(d2,""Input_Visits_By_Day.csv"")\n\t\n## Make a new object that is the averge number of visitors across all days \n\t# with sd and sample size.\n\t\n\td3<-as.data.frame(unique(d2$UnitBox))\n\tcolnames(d3)<-""UnitBox""\n\tfor(i in 1:nrow(d3)){\n\t\tsub<-subset(d2,d2$UnitBox==d3$UnitBox[i])\n\t\td3$n[i]<-nrow(sub)\n\t\td3$Avg.All.m[i]<-mean(sub$All.m)\n\t\td3$SD.All.m[i]<-sd(sub$All.m)\n\t\td3$Avg.Uni.m[i]<-mean(sub$Uni.m)\n\t\td3$SD.Uni.m[i]<-sd(sub$Uni.m)\n\t\td3$Avg.All.f[i]<-mean(sub$All.f)\n\t\td3$SD.All.f[i]<-sd(sub$All.f)\n\t\td3$Avg.Uni.f[i]<-mean(sub$Uni.f)\n\t\td3$SD.Uni.f[i]<-sd(sub$Uni.f)\n\t\td3$Avg.All.v[i]<-mean(sub$All.v)\n\t\td3$SD.All.v[i]<-sd(sub$All.v)\n\t\td3$Avg.Uni.v[i]<-mean(sub$All.u)\n\t\td3$SD.Uni.v[i]<-sd(sub$All.u)\n\t\td3$Avg.All.t[i]<-mean(sub$All.t)\n\t\td3$SD.All.t[i]<-sd(sub$All.t)\n\t\td3$Avg.Uni.t[i]<-mean(sub$Uni.t)\n\t\td3$SD.Uni.t[i]<-sd(sub$Uni.t)\n\t}\n\t\n## Write the output to a new file to be used in other script\n\twrite.csv(d3,""Input_Avg_Daily_Visits.csv"")']",2,"- Achromatic plumage brightness
- Stress resilience
- Social interactions
- Tree swallows
- Signal honesty
- Individual quality
- Environmental stress
- Social stress
- Female swallows
- Flight efficiency
- Maneuver"
Data used in: Social status does not predict in-camp integration among egalitarian hunter-gatherer men,"In the last few decades, there has been much research regarding the importance of social prestige in shaping the social structure of small-scale societies. While recent studies show that social prestige may have important health consequences, little is known about the extent to which prestige translates into actual in-person interactions and proximity, even though the level of integration into such real-life social networks has been shown to have important health consequences. Here, we determine the extent to which two different domains of social prestige, popularity (being perceived as a friend by others) and hunting reputation (being perceived as a good hunter), translate into GPS-derived in- and out-of-camp proximity networks in a group of egalitarian hunter-gatherer men, the Hadza. We show that popularity and hunting reputation differ in the extent to which they are translated into time spent physically close to each other. Moreover, our findings suggest that in-camp proximity networks, which are commonly applied in studies of small-scale societies, do not show the full picture of Hadza men's social preferences. While men are in camp, neither popularity nor hunting reputation is associated with being central in the proximity network; however, when out of camp, Hadza men who are popular are more integrated in the proximity networks while men with better hunting reputations are less integrated. Overall, our findings suggest that, in order to fully understand social preferences among hunter-gatherers, both in-camp and out-of-camp proximity networks should be considered.","['library(tnet)\nlibrary(bipartite)\nlibrary(lmtest)\nlibrary(car)\nlibrary(rcompanion)\n\n#CAMP1\n#out of camp proximity networks camp 1\nCAMP1gpsOUT_OF_CAMP=read.csv(""~CAMP1gpsOUT_OF_CAMP.csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP1gpsOUT_OF_CAMP<-as.matrix(CAMP1gpsOUT_OF_CAMP)\n#in of camp proximity networks camp 1\nCAMP1gpsIN_CAMP=read.csv(""~CAMP1gpsIN_CAMP.csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP1gpsIN_CAMP<-as.matrix(CAMP1gpsIN_CAMP)\n#friendship  nomination camp 1\nCAMP1friendship=read.csv(""~CAMP1friendship.csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP1friendship<-as.matrix(CAMP1friendship)\nnrow(CAMP1friendship)\n#kinship matrix camp 1\nCAMP1kinship<-read.csv(""~CAMP1kinship.csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP1kinship<-as.matrix(CAMP1kinship)\n#hunting nomination matrix camp 1\nCAMP1hunter<-read.csv(""~CAMP1hunter.csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP1hunter<-as.matrix(CAMP1hunter)\n\n#CAMP2\n#out os of camp proximity networks camp 2\nCAMP2gpsOUT_OF_CAMP=read.csv(""~CAMP2gpsOUT_OF_CAMP.csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP2gpsOUT_OF_CAMP<-as.matrix(CAMP2gpsOUT_OF_CAMP)\n#in of camp proximity networks camp 2\nCAMP2gpsIN_CAMP=read.csv(""~CAMP2gpsIN_CAMP.csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP2gpsIN_CAMP<-as.matrix(CAMP2gpsIN_CAMP)\n#firenship network camp 2\nCAMP2friendship=read.csv(""CAMP2friendship~csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP2friendship<-as.matrix(CAMP2friendship)\n#kinship matrix camp 2\nCAMP2kinship<-read.csv(""~CAMP2kinship.csv"",header=TRUE,row.names=1,check.names=FALSE)\nCAMP2kinship<-as.matrix(CAMP2kinship)\n#hunting nomination matrix camp 2\nCAMP2hunter<-read.csv(""~CAMP2hunter.csv"",header=TRUE,row.names=1,check.names=FALSE)\nCAMP2hunter<-as.matrix(CAMP2hunter)\n\n#CAMP3\n#out os of camp proximity networks camp 3\nCAMP3gpsOUT_OF_CAMP=read.csv(""~CAMP3gpsOUT_OF_CAMP.csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP3gpsOUT_OF_CAMP<-as.matrix(CAMP3gpsOUT_OF_CAMP)\n#in of camp proximity networks camp 3\nCAMP3gpsIN_CAMP=read.csv(""~CAMP3gpsIN_CAMP.csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP3gpsIN_CAMP<-as.matrix(CAMP3gpsIN_CAMP)\n#firenship network camp 3\nCAMP3friendship=read.csv(""~CAMP3friendship.csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP3friendship<-as.matrix(CAMP3friendship)\n#kinship matrix camp 3\nCAMP2kinship=read.csv(""~CAMP2kinship.csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP2kinship<-as.matrix(CAMP2kinship)\n#hunting nomination matrix camp 3\nCAMP3hunter=read.csv(""~CAMP3hunter.csv"",header=TRUE,row.names=1,check.names=FALSE) \nCAMP3hunter<-as.matrix(CAMP3hunter)\n\n\n#FRIENDSHIP POPULARITY\n\nM<-as.matrix(CAMP1friendship)\nb<-web2edges(M, webName=NULL, weight.column=TRUE, both.directions=FALSE,is.one.mode=T, out.files=c(""edges"", ""names"", ""groups"")[1:2],return=T, verbose=T)\nb<- as.tnet(b, type=""weighted one-mode tnet"")\nv<-degree_w(b, measure=c(""alpha""), alpha=1, type=""in"")\nv<-v[,-1]\nfriend_strength1<-v\n\n\nM<-as.matrix(CAMP2friendship)\nb<-web2edges(M, webName=NULL, weight.column=TRUE, both.directions=FALSE,is.one.mode=T, out.files=c(""edges"", ""names"", ""groups"")[1:2],return=T, verbose=T)\nb<- as.tnet(b, type=""weighted one-mode tnet"")\nv<-degree_w(b, measure=c(""alpha""), alpha=1, type=""in"")\nv<-v[,-1]\nfriend_strength2<-v\n\n\n\nM<-as.matrix(CAMP3friendship)\nb<-web2edges(M, webName=NULL, weight.column=TRUE, both.directions=FALSE,is.one.mode=T, out.files=c(""edges"", ""names"", ""groups"")[1:2],return=T, verbose=T)\nb<- as.tnet(b, type=""weighted one-mode tnet"")\nv<-degree_w(b, measure=c(""alpha""), alpha=1, type=""in"")\nv<-v[,-1]\nfriend_strength3<-v\n\n\nfriend_strength<-c(friend_strength1,friend_strength2,friend_strength3)\n\n#FORAGING REPUTATION\n\nM<-as.matrix(CAMP1hunter)\nb<-web2edges(M, webName=NULL, weight.column=TRUE, both.directions=FALSE,is.one.mode=T, out.files=c(""edges"", ""names"", ""groups"")[1:2],return=T, verbose=T)\nb<- as.tnet(b, type=""weighted one-mode tnet"")\nv<-degree_w(b, measure=c(""alpha""), alpha=1, type=""in"")\nv<-v[,-1]\nforaging_strength1<-v\n\n\nM<-as.matrix(CAMP2hunter)\nb<-web2edges(M, webName=NULL, weight.column=TRUE, both.directions=FALSE,is.one.mode=T, out.files=c(""edges"", ""names"", ""groups"")[1:2],return=T, verbose=T)\nb<- as.tnet(b, type=""weighted one-mode tnet"")\nv<-degree_w(b, measure=c(""alpha""), alpha=1, type=""in"")\nv<-v[,-1]\nforaging_strength2<-v\n\n\n\nM<-as.matrix(CAMP3hunter)\nb<-web2edges(M, webName=NULL, weight.column=TRUE, both.directions=FALSE,is.one.mode=T, out.files=c(""edges"", ""names"", ""groups"")[1:2],return=T, verbose=T)\nb<- as.tnet(b, type=""weighted one-mode tnet"")\nv<-degree_w(b, measure=c(""alpha""), alpha=1, type=""in"")\nv<-v[,-1]\nforaging_strength3<-v\n\n\nforaging_strength<-c(foraging_strength1,foraging_strength2,foraging_strength3)\n\n\n#gps_strengthOUT_OF_CAMP\nM<-as.matrix(CAMP1gpsOUT_OF_CAMP)\nM<-M/10000#returning the original proximity values, see README \nb<-web2edges(M, webName=NULL, weight.column=TRUE, both.directions=FALSE,is.one.mode=T, out.files=c(""edges"", ""names"", ""groups"")[1:2],return=T, verbose=T)\nb<- as.']",2,"Social status, small-scale societies, social prestige, health consequences, in-person interactions, proximity networks, GPS-derived, egalitarian hunter-gatherer men, Hadza, popularity, hunting reputation, physical closeness, in-camp, out-of-c"
Data from: Shifts in reproductive investment in response to competitors lowers male reproductive success,"In many species, males exhibit phenotypic plasticity in sexually selected traits when exposed to social cues about the intensity of sexual competition. To date, however, few studies have tested how this plasticity affects male reproductive success. We initially tested whether male mosquitofish, Gambusia holbrooki (Poeciliidae), change their investment in traits under pre- and post- copulatory sexual selection depending on the social environment. Focal males were exposed, for a full spermatogenesis cycle, to visual and chemical cues of rivals that were either present (competitive treatment) or absent (control). Males from the competitive treatment had significantly slower swimming sperm, but did not differ in sperm count from control males. When two males competed for a female, competitive treatment males also made significantly fewer copulation attempts and courtship displays than control males. Further, paternity analysis of 708 offspring from 148 potential sires, testing whether these changes in reproductive traits affected male reproductive success, showed that males previously exposed to cues about the presence of rivals sired significantly fewer offspring when competing with a control male. We discuss several possible explanations for these unusual findings.","['#~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\r\n# Clear working space\r\n#~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\r\n\r\nrm(list=ls())\r\n\r\n\r\n#~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\r\n# Packages\r\n#~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\r\n\r\nlibrary(lme4)\r\nlibrary(car)\r\nlibrary(DHARMa)\r\n\r\n\r\n#~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\r\n# UPLOAD Data\r\n#~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\r\n\r\ndata_P<-read.csv(""Spagopoulou-etal_AmNat2020.csv"", header=TRUE)\r\n\r\n\r\n#Order rows according to Pair\r\ndata_P<-data_P[ order(data_P[,2]), ]\r\n\r\n\r\n#~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\r\n# SPERM ANALYSIS\r\n#~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\r\n\r\n#~#~#~#~#~#~#~#~#~#~#~# Data Subsets #~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#\r\n#For Sperm Number\r\ndata_sperm <- subset(data_P, Trial==""A"")\r\n\r\n# For Sperm Velocity we need to use only the individuals that have more that 10 sperm cells recorded\r\ndata_sperm_tracks <- subset(data_sperm, Sperm_cells > 10)\r\n\r\n\r\n\r\n#~#~#~#~#~#~#~#~#~#~#~# MODELS #~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#\r\n\r\n##############____ Sperm Number ____###############\r\n\r\n## Model with Interaction term\r\nsn_int <- lm((sqrt(data_sperm$Sperm_number)) ~ Trt *scSL, data = data_sperm,  contrasts = list(Trt = contr.sum))\r\n\r\n# Plot model\r\npar(mfrow=c(2,2))\r\nplot(sn_int)\r\n\r\n# Summary statistics\r\nsummary(sn_int)\r\nAnova(sn_int, type=3)\r\n\r\n\r\n\r\n## Additive Model without interaction\r\nsn_add <- lm((sqrt(data_sperm$Sperm_number)) ~ Trt + scSL, data = data_sperm, contrasts = list(Trt = contr.sum))\r\n\r\n# Plot model\r\npar(mfrow=c(2,2))\r\nplot(sn_add)\r\n\r\n# Summary statistics\r\nsummary(sn_add)\r\nAnova(sn_add, type=3)\r\n\r\n##############____ Sperm Velocity ____###############\r\n#______________________ VCL _______________________#\r\n## Model with Interaction term\r\nsvcl_int <- lm(VCL_average ~ Trt * scSL, data = data_sperm_tracks, contrasts = list(Trt = contr.sum))\r\n\r\n# Plot model\r\npar(mfrow=c(2,2))\r\nplot(svcl_int)\r\n\r\n# Summary statistics\r\nsummary(svcl_int)\r\nAnova(svcl_int, type=3)\r\n\r\n\r\n\r\n## Additive Model without interaction\r\nsvcl_add <- lm(VCL_average ~ Trt + scSL, data = data_sperm_tracks, contrasts = list(Trt = contr.sum))\r\n\r\n# Plot model\r\npar(mfrow=c(2,2))\r\nplot(svcl_add)\r\n\r\n# Summary statistics \r\nsummary(svcl_add)\r\nAnova(svcl_add, type=3)\r\n\r\n\r\n#______________________ VAP _______________________#\r\n## Model with Interaction term\r\nsvap_int <- lm(VAP_average ~ Trt * scSL, data = data_sperm_tracks, contrasts = list(Trt = contr.sum))\r\n\r\n# Plot model\r\npar(mfrow=c(2,2))\r\nplot(svap_int)\r\n\r\n# Summary statistics\r\nsummary(svap_int)\r\nAnova(svap_int, type=3)\r\n\r\n\r\n\r\n## Additive Model without interaction\r\nsvap_add <- lm(VAP_average ~ Trt + scSL, data = data_sperm_tracks, contrasts = list(Trt = contr.sum))\r\n\r\n# Plot model\r\npar(mfrow=c(2,2))\r\nplot(svap_add)\r\n\r\n# Summary statistics\r\nsummary(svap_add)\r\nAnova(svap_add, type=3)\r\n\r\n\r\n#~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\r\n# BEHAVIOURAL ANALYSIS\r\n#~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*~*\r\n\r\n#~#~#~#~#~#~#~#~#~#~#~# Data Subsets #~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#\r\n\r\n# We excluded two individuals that had a problem with their air bludder after the sperm assay\r\ndata_behav<-subset(data_P, Pair!=""19"")\r\ndata_behav<-subset(data_behav, Pair!=""42"")\r\n\r\n\r\n### Separate the dataset in each Day of behavioural observations\r\ndata_behav_A <- subset(data_behav, Trial==""A"")\r\n#table(data_behav_A$Trial)\r\n\r\ndata_behav_C <- subset(data_behav, Trial==""C"")\r\n#table(data_behav_C$Trial)\r\n\r\n\r\n##############____ Copulation Atempts ____###############\r\n#____________________ Rate of CA _____________________#\r\n\r\n# Remove zeros, to investigate the rate of CA in the males that did perform this behaviour\r\ndata_CArate_A<-subset(data_behav_A, !CA==""0"")\r\ndata_CArate_C<-subset(data_behav_C, !CA==""0"")\r\n\r\n#____________________ Binomial CA ____________________#\r\n\r\n# Binomial CA response of males (Yes/No)\r\ndata_CAbinom_A<-data_behav_A\r\ndata_CAbinom_A$CA[data_CAbinom_A$CA>0] <- 1\r\n\r\ndata_CAbinom_C<-data_behav_C\r\ndata_CAbinom_C$CA[data_CAbinom_C$CA>0] <- 1\r\n\r\n\r\n##############____ Courtship Displays ____###############\r\n#__________________ Rate of Displays __________________#\r\n\r\n# Remove zeros, to investigate the rate of displays in the males that did perform this behaviour\r\ndata_DisplayRate_A<-subset(data_behav_A, !Display==""0"")\r\ndata_DisplayRate_C<-subset(data_behav_C, !Displ']",2,"shifts, reproductive investment, competitors, male reproductive success, phenotypic plasticity, sexually selected traits, social cues, sexual competition, mosquitofish, Gambusia holbrooki, Poeciliidae, pre-copulatory"
Repercussions of Patrilocal Residence on Mothers' Social Support Networks Among Tsimane Forager-Farmers,"While it is commonly thought that patrilocality is associated with worse outcomes for women 27 and their children due to lower social support, few studies have examined whether the structure 28 of female social networks covaries with post-marital residence. Here we analyze scan sample 29 data collected among Tsimane forager-farmers. We compare the social groups and activity 30 partners of 181 women residing in the same community as their parents, their husband's parents, 31 both or neither. Relative to women living closer to their in-laws, women living closer to their 32 parents are less likely to be alone or solely in the company of their nuclear family (OR: 0.6, 33 95%CI: 0.3-0.9), and more likely to be observed with others when engaging in food processing 34 and manufacturing of market or household goods, but not other activities. Women are slightly 35 more likely to receive childcare support from outside the nuclear family when they live closer to 36 their parents (OR=1.8, 95%CI 0.8 - 3.9). Their social group size and their children's probability 37 of receiving allocate decrease significantly with distance from their parents, but not their in-laws. 38 Our findings highlight the importance of women's proximity to kin, but also indicate that 39 patrilocality per se is not costly to Tsimane women.","['# library(devtools)\n# install_github(""edseab/Counterfact"")\nlibrary(Counterfact)\nlibrary(sjPlot)\nlibrary(lme4)\nlibrary(brms)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(glmmTMB)\n\n\n## Load in the datasets ##\ndmom <- read.csv(""final_analysis_table_mothers.csv"")\nd <- read.csv(""final_analysis_table_children.csv"")\nsocial_group_plot <- read.csv(""social_group_plot.csv"",row.names=1)\nactivity_group_plot <- read.csv(""activity_group_plot.csv"",row.names=1)\nsexbias <- read.csv(""activity_sexbias.csv"",row.names=1)\n\n####################################\n##                                ##\n##            MODELS              ##\n##                                ##\n####################################\n\n## First, the 4 models of social group size, poisson, negative binomial, and zero-inflated poisson\nmm1 <- glmmTMB(out_sgroupsize ~ Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid)+ (1|Comunidad), data=dmom,family=poisson)\n\nnb1 <- glmmTMB(out_sgroupsize ~ Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid)+ (1|Comunidad), data=dmom,family=nbinom2)\n\nzip1nosquare <- glmmTMB(out_sgroupsize~Residence_Pattern+ age_centered + Morning   + (1|pid) + (1|Comunidad),data=dmom,ziformula= ~Residence_Pattern + age_centered + Morning + (1|pid) + (1|Comunidad),family=poisson)\n\nzip1 <- glmmTMB(out_sgroupsize~Residence_Pattern+ age_centered +age_centered2 + Morning   + (1|pid) + (1|Comunidad),data=dmom,ziformula= ~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),family=poisson)\n\n\n\n## Now active care and number of helpers\n{\n  mmcare <- glmmTMB(active_care ~ Residence_Pattern + age_centered +age_centered2 + nkids_under7 + (1|pid)+ Morning +  (1|Comunidad), data=dmom,family=binomial)\n  \n  mmhelpfood <- glmmTMB(help_with_food ~ Residence_Pattern + age_centered +age_centered2 + Morning +  (1|pid)+ (1|Comunidad), data=dmom,family=poisson)\n  \n  \n  ziphelpfood <- glmmTMB(help_with_food~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),data=dmom,ziformula=~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),family=poisson)\n  \n  mmhelpwork <- glmmTMB(help_with_labor~ Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid)+ (1|Comunidad), data=dmom,family=poisson)\n  \n  ziphelpwork <- glmmTMB(help_with_labor~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),data=dmom,ziformula=~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),family=poisson)\n  \n  mmhelpresources <- glmmTMB(resource_acquisition_helpers~ Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid)+ (1|Comunidad), data=dmom,family=poisson)\n  \n  \n  ziphelpresources <- glmmTMB(resource_acquisition_helpers~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),data=dmom,ziformula=~Residence_Pattern + Morning+ age_centered +age_centered2 + (1|pid) + (1|Comunidad),family=poisson)\n  \n  mmhelpmanufacture <- glmmTMB(help_with_manufacturing~ Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid)+ (1|Comunidad), data=dmom,family=poisson)\n  \n  ziphelpmanufacture <- glmmTMB(help_with_manufacturing~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),data=dmom,ziformula=~Residence_Pattern + age_centered +age_centered2 + Morning +  (1|pid) + (1|Comunidad),family=poisson)\n}\n\n\n## Now Activity group models\n{\n  mmpart <- glmmTMB(out_agroupsize~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),data=dmom,family=poisson)\n  nbpart <- glmmTMB(out_agroupsize~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),data=dmom,family=nbinom2)\n  \n  zippart <- glmmTMB(out_agroupsize~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),data=dmom,ziformula=~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),family=poisson)\n  \n  mmpartfood <- glmmTMB(food_partners ~ Residence_Pattern + Morning + age_centered +age_centered2 + (1|pid)+ (1|Comunidad), data=dmom,family=poisson)\n  \n  zippartfood <- glmmTMB(food_partners~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),data=dmom,ziformula=~Residence_Pattern + Morning +  age_centered + (1|pid) + (1|Comunidad),family=poisson)\n  \n  mmpartwork <- glmmTMB(labor_partners~ Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid)+ (1|Comunidad), data=dmom,family=poisson)\n  \n  zippartwork <- glmmTMB(labor_partners~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),data=dmom,ziformula=~Residence_Pattern + age_centered +age_centered2 + Morning + (1|pid) + (1|Comunidad),family=poisson)\n  \n  mmpartresources <- glmmTMB(resource_partners~ Residence_Pattern + age_centered +age_centered2 + (1|Comunidad), data=dmom,family=poisson)\n  \n  zippartresources <- glmmTMB(resource_partners~ Residence_Pattern + age_centered', '# library(devtools)\n# install_github(""edseab/Counterfact"")\nlibrary(Counterfact)\nlibrary(dplyr)\n\n## RAW DATASETS ##\n## ta <- ta_full <- read.csv("""")  ## the full raw time allocation database\n## census_ta_data<- read.csv("""")  ## the contemporaneous census data \ncodes <- read.csv(""codes.csv"")  ## the activity codes file\n## com <- read.csv("""")  ## data on communities\n## r_matrix<- read.csv("""") ## relatedness matrix for all communities\n## reg <-read.csv("""") ## full population register with data on parents\n## med <- read.csv("""") ## medical database with some additional data for cleaning and verification\n## vis <- read.csv("""") ## visit register with family ID and residence data\n\nreg$pid<-as.character(reg$pid)\nreg$father_pid<-as.character(reg$father_pid)\nreg$mother_pid<-as.character(reg$mother_pid)\n\n## Here we fun a family function which returns the relationship between any 2 individuals as a categorical variable (including affinal relationships) \n\n## First an ""identical"" function\ni<-function(x,y){\n  if(is.na(x)|is.na(y))return(FALSE)\n  else if(x==y) return(TRUE)\n  else return(FALSE)}\n\n## Now the ""family"" function\n\nfamily<-function (pid1,pid2, paste=F){\n  \n  \n  pid1<-as.character(pid1)\n  pid2<-as.character(pid2)\n  \n  a<-reg[match(pid1,reg$pid),]\n  b<-reg[match(pid2,reg$pid),]\n  c<-character(0)\n  \n  d1<-data.frame(pid=as.character(pid1))\n  d2<-data.frame(pid=as.character(pid2))\n  \n  #parents\n  d1$f<-a$father_pid\n  d1$m<-a$mother_pid\n  d2$f<-b$father_pid\n  d2$m<-b$mother_pid\n  #grandparents\n  d1$pgf<-reg[match(d1$f, reg$pid),""father_pid""]\n  d1$mgf<-reg[match(d1$m, reg$pid),""father_pid""]\n  d1$pgm<-reg[match(d1$f, reg$pid),""mother_pid""]\n  d1$mgm<-reg[match(d1$m, reg$pid),""mother_pid""]\n  d2$pgf<-reg[match(d2$f, reg$pid),""father_pid""]\n  d2$mgf<-reg[match(d2$m, reg$pid),""father_pid""]\n  d2$pgm<-reg[match(d2$f, reg$pid),""mother_pid""]\n  d2$mgm<-reg[match(d2$m, reg$pid),""mother_pid""]\n  \n  d1[is.na(d1)] <- ""12345placeholder""\n  d2[is.na(d2)] <- ""67890placeholder""\n  \n  #siblings \n  as<-reg$pid[which((reg$father_pid %in% d1$f | reg$mother_pid %in% d1$m)& reg$pid!=pid1)]\n  bs<-reg$pid[which((reg$father_pid %in% d2$f | reg$mother_pid %in% d2$m)& reg$pid!=pid2)]\n  \n  #joint children\n  children<-intersect(reg[match(pid1,reg$father_pid),""pid""], reg[match(pid2,reg$mother_pid),""pid""])\n  children<-append(children,intersect(reg[match(pid2,reg$father_pid),""pid""], reg[match(pid1,reg$mother_pid),""pid""]))\n  \n  \n  #spouse\n  \n  aspouse<-rep(NA,nrow(a))\n  aspouse <- ifelse(a$male==1,unique(reg$mother_pid[match(pid1,reg$father_pid)]),\n                    unique(reg$father_pid[match(pid1,reg$mother_pid)]))\n  \n  bspouse<-rep(NA,nrow(b))\n  bspouse<-ifelse(b$male==1,unique(reg$mother_pid[match(pid2,reg$father_pid)]),\n                  unique(reg$father_pid[match(pid2,reg$mother_pid)]))\n  \n  aspouse[which(is.na(aspouse))] <- ""aspouseplaceholder""\t\n  bspouse[which(is.na(bspouse))] <- ""bspouseplaceholder""\t\n  \n  #same person\n  \n  if(pid1==pid2){c<-""same person""} \n  \n  if(pid1!=pid2){\n    if(i(pid1,d2$f)) c<-append(c,""father"")\n    if(i(pid1,d2$m)) c<-append(c,""mother"")\n    if(i(pid2,d1$f) | i(pid2,d1$m)) c<-append(c,""child"")\n    if(i(pid1,d1$pgf)) c<-append(c,""paternal grandfather"")\n    if(i(pid1,d1$mgf)) c<-append(c,""maternal grandfather"")\n    if(i(pid1,d1$pgm)) c<-append(c,""paternal grandmother"")\n    if(i(pid1,d1$mgm)) c<-append(c,""maternal grandmother"")\n    if(i(pid2,d2$pgf) | i(pid2,d2$pgm))c<-append(c,""son\'s child"")\n    if(i(pid2,d2$mgf) | i(pid2,d2$mgm))c<-append(c,""daughter\'s child"")\n    if(i(d1$f,d2$f) & (i(d1$m,d2$m)==FALSE))c<-append(c,""paternal half-sibling"")\n    if(i(d1$m,d2$m) & (i(d1$f,d2$f)==FALSE))c<-append(c,""maternal half-sibling"")\n    if(i(d1$f,d2$f) & i(d1$m,d2$m)) c<-append(c,""full sibling"")\n    if(length(bspouse)!=0) {if(pid1 %in% reg$father_pid[match(bspouse,reg$pid)]==TRUE) c<-append(c,""father-in-law"")\n    if(pid1 %in% reg$mother_pid[match(bspouse,reg$pid)]==TRUE)c<-append(c,""mother-in-law"")}\n    if(!i(d1$f,d2$f)&!i(d1$m,d2$m) & (i(d1$pgf,d2$pgf) | i(d1$pgm,d2$pgm)))c<-append(c,""paternal parallel cousin"")\n    if(!i(d1$f,d2$f)&!i(d1$m,d2$m) & (i(d1$mgf,d2$mgf) | i(d1$mgm,d2$mgm)))c<-append(c,""maternal parallel cousin"")\n    if(!i(d1$f,d2$f)&!i(d1$m,d2$m) & (i(d1$pgf,d2$mgf) | i(d1$pgm,d2$mgm)))c<-append(c,""maternal cross cousin"")\n    if(!i(d1$f,d2$f)&!i(d1$m,d2$m) & (i(d1$mgf,d2$pgf) | i(d1$mgm,d2$pgm)))c<-append(c,""paternal cross cousin"")\n    if(length(bspouse)!=0){if(pid2 %in% reg$father_pid[match(aspouse,reg$pid)] == TRUE| pid2 %in% reg$mother_pid[match(aspouse,reg$pid)] == TRUE) c<-append(c,""offspring-in-law"")\n    if(pid1 %in% bspouse==TRUE) c<-append(c,""married"")}\n    if(length(bspouse)!=0 & length(as)!=0){if(bspouse %in% as )c<-append(c,""sibling\'s spouse"")}\n    if(length(aspouse)!=0 & length(bs)!=0){if(aspouse %in% bs )c<-append(c,""spouse\'s sibling"")}\n    if(length(aspouse)!=0 & length(d2$f)!=0){if(!i(pid1,d2$m) & d2$f %in% aspouse==TRUE)c<-append(c,""stepmother"")}\n    if(length(bspouse)!=0 & length(d1$f)!=0){if(!i(pid2,d1$m) & d']",2,"patrilocal residence, social support networks, Tsimane forager-farmers, post-marital residence, female social networks, social groups, activity partners, in-laws, nuclear family, childcare support, proximity to kin, patriloc"
Data from: General rules for environmental management to prioritise social-ecological systems research based on a value of information approach,"1. Globally, billions of dollars are invested each year to help understand the dynamics of social-ecological systems (SES) in bettering both social and environmental outcomes. However, there is no scientific consensus on which aspect of an SES is most important and urgent to understand; particularly given the realities of limited time and money. 2. Here we use a simulation-based ""value of information"" approach to examine where research will deliver the most important information for environmental management in four social-ecological systems representing a range of real-life environmental issues. 3. We find that neither social nor ecological information is consistently the most important: instead, researchers should focus on understanding the primary effects of their management actions. 4. Thus, when managers are undertaking social actions the highest research priority should be understanding the dynamics of social groups. Alternatively, when manipulating ecological systems it will be most important to quantify ecological population dynamics. 5. Synthesis and applications. Our results provide a standard assessment to determine the uncertain SES component with the highest expected impact for management outcomes. First, managers should determine the structure of their SES by identifying social and ecological nodes. Second, managers should identify the qualitative nature of the network, by determining which nodes are linked, but not the strength of those interactions. Finally, managers should identify the actions available to them to intervene in the SES. From these steps, managers will be able to identify the SES components that are closest to the management action(s), and it is these nodes and interactions that should receive priority research attention to achieve effective environmental decision making.","['# ****************************************************************************\r\n# MODEL CODE FOR:\r\n#   ""General rules for environmental management to prioritise social-ecological\r\n#   systems research based on a value of information approach""\r\n# AUTHORS:\r\n#   Katrina J Davis, Iadine Chadès, Jonathan R Rhodes & Michael Bode\r\n# DESCRIPTION:\r\n#   Code recreates Figure 4 in the manuscript (but at lower discretisation)\r\n# NOTE: \r\n#   This code should be run after Matlab file \'Code_EVPXI\'\r\n# ****************************************************************************\r\n\r\n# Update with personal file path\r\nworkDir <- """"\r\n\r\n# Set working directory\r\nsetwd(workDir) \r\n\r\n# libraries\r\nlibrary(grDevices) # colours\r\n\r\n# clear environment\r\nrm(list=ls())\r\n\r\n# Color settings ---------------------------------------------------------\r\n\r\n# Colours\r\nDBlue = ""#1D7BE5"" # Social\r\nLBlue = ""#00B0F0"" # Social-ecological\r\nLGreen = ""#64C39B"" # Ecological\r\nWhite = ""White""\r\n\r\n# Order for figure: Soc, Soc, Ecol, Ecol, Soc-Ecol, space, Soc, Ecol, Soc-Ecol\r\ncolor <- c(DBlue,DBlue,LGreen,LGreen,LBlue,White,DBlue,LGreen,LBlue)\r\n\r\n\r\n# Aggregate systems data ------------------------------------------------------------------\r\n\r\n\r\n# Read in systems data\r\nS1 <- read.csv(""S1.csv"", header = F) # System 1\r\nS2 <- read.csv(""S2.csv"", header = F) # System 2\r\nS3 <- read.csv(""S3.csv"", header = F) # System 3\r\nS4 <- read.csv(""S4.csv"", header = F) # System 4\r\n\r\n# Combine data into single dataframe\r\nd <- rbind(S1, S2, S3, S4)\r\n\r\n# Specify column names\r\ncolnames(d) <- c(""I"",""q"",""r"",""C"",""H"",""Soc"",""Eco"",""Soc-Eco"")\r\n# Add an empty column\r\nd$No <- NA\r\n# Re-order\r\nd <- d[,c(""I"",""q"",""r"",""C"",""H"",""No"",""Soc"",""Eco"",""Soc-Eco"")]\r\n\r\n# Data to percentages\r\nd=d*100\r\n\r\n# Create Manuscript Figure 4 as PDF ---------------------------------------------------------------------\r\n\r\n# Individual plot titles\r\nmains <- c(""1. Territorial use rights fishery"", \r\n           ""2. Salmon fishery"", \r\n           ""3. Agricultural system"", \r\n           ""4. Non-timber forest products"")\r\n\r\npdf(file = ""EVPXI.pdf"", width = 3.5, height = 8) # call pdf (units in inches)\r\npar(mfrow=c(4,1), # multipane plot\r\n    mar=c(0,5,2,0.5), # inner plot margins (space btw plots)\r\n    oma=c(5,0,0,0)) # outer margins (space outside plots)\r\n\r\nj <- 0 # initialise counter j\r\nfor (i in c(2,5,8,11)){\r\n  j=j+1\r\n  barCenters <- barplot(as.numeric(d[i,]), col=color,\r\n                        names.arg = colnames(d),\r\n                        xaxt=""n"",\r\n                        axes=T,\r\n                        ylim = c(0, 100),\r\n                        main = mains[j],\r\n                        ylab = """", cex.lab=1, cex.main=1.25, cex.names=0.5, cex.axis=1)\r\n  segments(6.7, 0, 6.7, 85, lwd=1.5, lty=2, col=""gray"")\r\n  \r\n  if (j==4) {\r\n    par(xpd=NA)\r\n    text(x=barCenters[1:5],y = par(""usr"")[3] - 9,labels=colnames(d[1:5]), \r\n         cex = 1, pos = 1, offset = 0.01)\r\n    text(x=barCenters[7:9],y = par(""usr"")[3] - 9,labels=colnames(d[7:9]), \r\n         cex = 1, pos = 1, offset = 0.3, srt=45)\r\n  }\r\n  \r\n  segments(barCenters,as.matrix(d[i-1,]), barCenters, as.matrix(d[i+1,]), lwd = 1)\r\n  arrows(barCenters, as.matrix(d[i-1,]), barCenters, as.matrix(d[i+1,]), lwd = 1, \r\n         angle = 90,code = 3, length = 0.05)\r\n}\r\npar(xpd=NA)\r\n\r\n# y axis label\r\ntext(x = -2, y = 240, labels = ""Expected value of partial information (%)"", srt=90, cex=1.25)\r\n\r\ndev.off() # close graph']",2,"social-ecological systems, environmental management, value of information, research, dynamics, social groups, ecological population dynamics, management outcomes, uncertain SES component, qualitative nature, network, management action, effective environmental decision making."
Restart Data Workbench Motivation Survey,"The Restart Data Workbench motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting soil pollution through waste reduction/management in the Restart Data Workbench pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/restart-data-workbench/.The Restart Data Workbench motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation","['setwd(""restart-data-workbench"")\r\n\r\n## DATA PREPARATION\r\nraw.data<- read.csv(""restart-data-workbench-results.csv"", header=T)\r\n\r\n# Keep only completed surveys\r\nraw.data.unfinished<-raw.data[raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data.unfinished$user))\r\n\r\nraw.data<-raw.data[!raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data$user))\r\n\r\n# Keep only questions related to motivation \r\nmotivation.questions<-raw.data[raw.data$tag %in% c(""achievement"", ""conformity"", ""self-direction"", ""stimulation"", ""routine"", ""hedonism"", ""power"", ""belongingness"", ""benevolence"", ""universalism"", ""global motivation""),]\r\nunique(motivation.questions$tag)\r\n\r\n# Create unique id made usign tag+id.question to perform pivoting of the table  \r\nmotivation.questions<-cbind(motivation.questions, tag.question = paste0(motivation.questions$tag, motivation.questions$questionId))\r\n\r\n\r\n## --  pivoting table (one row for each user and one column for each question. As value the numerical value given as answer)\r\nlibrary(reshape2) \r\nlibrary(reshape) \r\n\r\nmatch.tag.question<-unique(motivation.questions[, c(""tag.question"", ""question"")])\r\n# Rows now represent users, each column is related to a tag.question. Value of each cell is taken from the ""value"" column of the original file (default behaviour of the function, if a ""value"" column doesn\'t exist in the original table you need to specify which column should be used).\r\npivot.motivation.questions<-cast(motivation.questions, user ~ tag.question , fun.aggregate = mean)\r\npivot.motivation.questions<-pivot.motivation.questions[ , !(names(pivot.motivation.questions) %in% c(""global motivation1492""))]\r\n\r\n\r\n# Modify question names to uniform analysis according to the survey-motivation-template\r\nnames(pivot.motivation.questions) <- c(""user"", ""achievement.1"", ""achievement.2"", ""belongingness.2"", ""belongingness.1"", \r\n  ""benevolence.1"", ""benevolence.2"",  ""conformity.2"", ""conformity.1"", ""global motivation"",\r\n  ""hedonism.2"", ""hedonism.1"", ""power.1"", ""power.2"", ""routine.2"", ""routine.1"",""self-direction.1"", \r\n  ""self-direction.2"", ""stimulation.1"", ""stimulation.2"", ""universalism.1"", ""universalism.2"")\r\n\r\nwrite.csv(pivot.motivation.questions, ""restart-data-workbench-pivot-questions.csv"", row.names = F)\r\n\r\n#--------------------------- ANALYSIS RESULTS BY QUESTION\r\n\r\n# Compute average and variance for each question \r\nans<- read.csv(""restart-data-workbench-pivot-questions.csv"", header=TRUE)\r\n\r\nmean.question<-round(sapply(ans[,c(2:22)], mean),2)\r\nvar.questions<-round(sapply(ans[,c(2:22)], var), 2)\r\n\r\ndf.questions<- data.frame(mean = mean.question, var = var.questions)\r\n\r\n\r\n#--------------------------- ANALYSIS RESULTS BY MOTIVATING FACTOR\r\n\r\n# Compute average and variance for each motivating factor \r\nans<- read.csv(""restart-data-workbench-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n\r\nfactors.summary= data.frame(factor= character(), mean=numeric(), var=numeric())\r\n\r\nall.factors<-c(""Achievement"", ""Belongingness"", ""Benevolence"", ""Conformity"", ""Hedonism"", ""Power"", ""Routine"", ""Self.direction"", ""Stimulation"", ""Universalism"", ""Global.motivation"")\r\n\r\nfor (k in all.factors){\r\n  f<-ans %>% select(starts_with(k)) \r\n  num.col<-ncol(f)\r\n  v.final=vector()\r\n  for(i in 1:num.col){\r\n    v<-as.vector(f[,i])\r\n    v.final<-c(v.final, v)\r\n  }\r\n  \r\n  f.mean<-round(mean(v.final), 2)\r\n  f.var<-round(var(v.final),2)\r\n  new.row<-data.frame(factor= k, mean=f.mean, var=f.var)\r\n  factors.summary<-rbind(factors.summary, new.row) # Add row to the dataframe\r\n  \r\n  rm(f, f.mean, f.var, new.row, v.final, v, num.col)\r\n  \r\n}\r\n\r\n\r\n#--------------------------- CORRELATION ANALYSIS\r\n\r\ncompleted<- read.csv(""restart-data-workbench-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n# average questions for each tag\r\n# average results for each tag\r\n\r\n#achievement\r\nach.subset<-completed %>% select(starts_with(""ach""))\r\ncompleted$ach <- rowMeans(ach.subset, na.rm = TRUE)\r\n\r\n#belongingness\r\nbel.subset<-completed %>% select(starts_with(""bel""))\r\ncompleted$bel <- rowMeans(bel.subset, na.rm = TRUE)\r\n\r\n#benevolence\r\nben.subset<-completed %>% select(starts_with(""ben""))\r\ncompleted$ben <- rowMeans(ben.subset, na.rm = TRUE)\r\n\r\n#conformity\r\nconf.subset<-completed %>% select(starts_with(""conf""))\r\ncompleted$conf <- rowMeans(conf.subset, na.rm = TRUE)\r\n\r\n#hedonism\r\nhed.subset<-completed %>% select(starts_with(""hed""))\r\ncompleted$hed <- rowMeans(hed.subset, na.rm = TRUE)\r\n\r\n#power\r\npwr.subset<-completed %>% select(starts_with(""pow""))\r\ncompleted$pwr <- rowMeans(pwr.subset, na.rm = TRUE)\r\n\r\n#routine\r\nrout.subset<-completed %>% select(starts_with(""rout""))\r\ncompleted$rout <- rowMeans(rout.subset, na.rm = TRUE)\r\n\r\n#self-direction\r\nself.subset<-completed %>% select(starts_with(""self""))\r\ncompleted$self <- rowMeans(self.subset, na.rm = TRUE)\r\n\r\n#stimulation\r\nstim.subset<-completed %>% select(starts_with(""stim""))\r\ncompleted$stim <- rowMeans(stim.subset, na.rm = TRUE)\r\n\r\n#universalism\r\nuniv.subset<-completed %>% select(starts_with(""univ""))\r\ncompleted$univ']",2,"Restart Data Workbench, Motivation, Survey, H2020, ACTION project, citizen science, volunteers, personal enjoyment, improvement, achievement, personal relationships, care for the environment, participation, factors, engagement tasks, long-term engagement, soil pollution"
Wow Nature Motivation Survey,"The Wow Nature motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting air pollution in the Wow Nature pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/wow-nature/.The Wow Nature motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation","['setwd(""wow-nature"")\r\n\r\n## DATA PREPARATION\r\nraw.data<- read.csv(""wow-nature-results.csv"", header=T)\r\n\r\n# Keep only completed surveys\r\nraw.data.unfinished<-raw.data[raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data.unfinished$user))\r\n\r\nraw.data<-raw.data[!raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data$user))\r\n\r\n# Keep only questions related to motivation \r\nmotivation.questions<-raw.data[raw.data$tag %in% c(""achievement"", ""conformity"", ""self-direction"", ""stimulation"", ""routine"", ""hedonism"", ""power"", ""belongingness"", ""benevolence"", ""universalism"", ""global motivation""),]\r\nunique(motivation.questions$tag)\r\n\r\n# Create unique id made usign tag+id.question to perform pivoting of the table  \r\nmotivation.questions<-cbind(motivation.questions, tag.question = paste0(motivation.questions$tag, motivation.questions$questionId))\r\n\r\n\r\n## --  pivoting table (one row for each user and one column for each question. As value the numerical value given as answer)\r\nlibrary(reshape2) \r\nlibrary(reshape) \r\n\r\nmatch.tag.question<-unique(motivation.questions[, c(""tag.question"", ""question"")])\r\n# Rows now represent users, each column is related to a tag.question. Value of each cell is taken from the ""value"" column of the original file (default behaviour of the function, if a ""value"" column doesn\'t exist in the original table you need to specify which column should be used).\r\npivot.motivation.questions<-cast(motivation.questions, user ~ tag.question , fun.aggregate = mean)\r\npivot.motivation.questions<-pivot.motivation.questions[ , !(names(pivot.motivation.questions) %in% c(""global motivation3906""))]\r\n\r\n\r\n# Modify question names to uniform analysis according to the survey-motivation-template\r\nnames(pivot.motivation.questions) <- c( ""user"", ""achievement.1"", ""achievement.2"", ""belongingness.1"", ""belongingness.2"", ""benevolence.1"", ""benevolence.2"", ""benevolence.3"", \r\n        ""conformity.2"", ""conformity.1"", ""global motivation"", ""hedonism.2"", ""hedonism.1"", ""power.1"", ""power.2"", ""routine.1"", ""routine.2"", \r\n        ""self-direction.1"", ""self-direction.2"", ""stimulation.1"", ""stimulation.2"", ""universalism.1"", ""universalism.2"" )\r\n\r\nwrite.csv(pivot.motivation.questions, ""wow-nature-pivot-questions.csv"", row.names = F)\r\n\r\n#--------------------------- ANALYSIS RESULTS BY QUESTION\r\n\r\n# Compute average and variance for each question \r\nans<- read.csv(""wow-nature-pivot-questions.csv"", header=TRUE)\r\n\r\nmean.question<-round(sapply(ans[,c(2:23)], mean), 2)\r\nvar.questions<-round(sapply(ans[,c(2:23)], var), 2)\r\n\r\ndf.questions<- data.frame(mean = mean.question, var = var.questions)\r\n\r\n\r\n#--------------------------- ANALYSIS RESULTS BY MOTIVATING FACTOR\r\n\r\n# Compute average and variance for each motivating factor \r\nans<- read.csv(""wow-nature-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n\r\nfactors.summary= data.frame(factor= character(), mean=numeric(), var=numeric())\r\n\r\nall.factors<-c(""Achievement"", ""Belongingness"", ""Benevolence"", ""Conformity"", ""Hedonism"", ""Power"", ""Routine"", ""Self.direction"", ""Stimulation"", ""Universalism"", ""Global.motivation"")\r\n\r\nfor (k in all.factors){\r\n  f<-ans %>% select(starts_with(k)) \r\n  num.col<-ncol(f)\r\n  v.final=vector()\r\n  for(i in 1:num.col){\r\n    v<-as.vector(f[,i])\r\n    v.final<-c(v.final, v)\r\n  }\r\n  \r\n  f.mean<-round(mean(v.final), 2)\r\n  f.var<-round(var(v.final),2)\r\n  new.row<-data.frame(factor= k, mean=f.mean, var=f.var)\r\n  factors.summary<-rbind(factors.summary, new.row) # Add row to the dataframe\r\n  \r\n  rm(f, f.mean, f.var, new.row, v.final, v, num.col)\r\n  \r\n}\r\n\r\n\r\n#--------------------------- CORRELATION ANALYSIS\r\n\r\ncompleted<- read.csv(""wow-nature-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n# average questions for each tag\r\n# average results for each tag\r\n\r\n#achievement\r\nach.subset<-completed %>% select(starts_with(""ach""))\r\ncompleted$ach <- rowMeans(ach.subset, na.rm = TRUE)\r\n\r\n#belongingness\r\nbel.subset<-completed %>% select(starts_with(""bel""))\r\ncompleted$bel <- rowMeans(bel.subset, na.rm = TRUE)\r\n\r\n#benevolence\r\nben.subset<-completed %>% select(starts_with(""ben""))\r\ncompleted$ben <- rowMeans(ben.subset, na.rm = TRUE)\r\n\r\n#conformity\r\nconf.subset<-completed %>% select(starts_with(""conf""))\r\ncompleted$conf <- rowMeans(conf.subset, na.rm = TRUE)\r\n\r\n#hedonism\r\nhed.subset<-completed %>% select(starts_with(""hed""))\r\ncompleted$hed <- rowMeans(hed.subset, na.rm = TRUE)\r\n\r\n#power\r\npwr.subset<-completed %>% select(starts_with(""pow""))\r\ncompleted$pwr <- rowMeans(pwr.subset, na.rm = TRUE)\r\n\r\n#routine\r\nrout.subset<-completed %>% select(starts_with(""rout""))\r\ncompleted$rout <- rowMeans(rout.subset, na.rm = TRUE)\r\n\r\n#self-direction\r\nself.subset<-completed %>% select(starts_with(""self""))\r\ncompleted$self <- rowMeans(self.subset, na.rm = TRUE)\r\n\r\n#stimulation\r\nstim.subset<-completed %>% select(starts_with(""stim""))\r\ncompleted$stim <- rowMeans(stim.subset, na.rm = TRUE)\r\n\r\n#universalism\r\nuniv.subset<-completed %>% select(starts_with(""univ""))\r\ncompleted$univ <- rowMeans(univ.subset, na.rm = TRUE)\r\n\r\n']",2,"Wow Nature, motivation, survey, H2020 project, citizen science, personal enjoyment, improvement, achievement, personal relationships, environment, participation, analysis, communities, engagement, air pollution, pilot, Coney toolkit, RO-Crate specification, procedure"
Open Soil Atlas Motivation Survey,"The Open Soil Atlas motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting soil pollution in the Open Soil Atlas pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/open-soil-atlas/.The Open Soil Atlas motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation","['setwd(""open-soil-atlas"")\r\n\r\n## DATA PREPARATION\r\nraw.data<- read.csv(""open-soil-atlas-results.csv"", header=T)\r\n\r\n# Keep only completed surveys\r\nraw.data.unfinished<-raw.data[raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data.unfinished$user))\r\n\r\nraw.data<-raw.data[!raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data$user))\r\n\r\n# Keep only questions related to motivation \r\nmotivation.questions<-raw.data[raw.data$tag %in% c(""achievement"", ""conformity"", ""self-direction"", ""stimulation"", ""routine"", ""hedonism"", ""power"", ""belongingness"", ""benevolence"", ""universalism"", ""global motivation""),]\r\nunique(motivation.questions$tag)\r\n\r\n# Create unique id made usign tag+id.question to perform pivoting of the table  \r\nmotivation.questions<-cbind(motivation.questions, tag.question = paste0(motivation.questions$tag, motivation.questions$questionId))\r\n\r\n\r\n## --  pivoting table (one row for each user and one column for each question. As value the numerical value given as answer)\r\nlibrary(reshape2) \r\nlibrary(reshape) \r\n\r\nmatch.tag.question<-unique(motivation.questions[, c(""tag.question"", ""question"")])\r\n# Rows now represent users, each column is related to a tag.question. Value of each cell is taken from the ""value"" column of the original file (default behaviour of the function, if a ""value"" column doesn\'t exist in the original table you need to specify which column should be used).\r\npivot.motivation.questions<-cast(motivation.questions, user ~ tag.question , fun.aggregate = mean)\r\npivot.motivation.questions<-pivot.motivation.questions[ , !(names(pivot.motivation.questions) %in% c(""global motivation1188""))]\r\n\r\n\r\n# Modify question names to uniform analysis according to the survey-motivation-template\r\nnames(pivot.motivation.questions) <- c(""user"", ""achievement.2"", ""achievement.1"", ""belongingness.1"", ""benevolence.1"", ""benevolence.2"", ""conformity.1"", \r\n""global motivation"", ""hedonism.1"", ""power.2"", ""routine.1"", ""self-direction.1"", ""self-direction.2"", ""stimulation.1"", \r\n""stimulation.2"", ""universalism.1"", ""universalism.2"")\r\n\r\nwrite.csv(pivot.motivation.questions, ""open-soil-atlas-pivot-questions.csv"", row.names = F)\r\n\r\n#--------------------------- ANALYSIS RESULTS BY QUESTION\r\n\r\n# Compute average and variance for each question \r\nans<- read.csv(""open-soil-atlas-pivot-questions.csv"", header=TRUE)\r\n\r\nmean.question<-round(sapply(ans[,c(2:17)], mean),2)\r\nvar.questions<-round(sapply(ans[,c(2:17)], var), 2)\r\n\r\ndf.questions<- data.frame(mean = mean.question, var = var.questions)\r\n\r\n\r\n#--------------------------- ANALYSIS RESULTS BY MOTIVATING FACTOR\r\n\r\n# Compute average and variance for each motivating factor \r\nans<- read.csv(""open-soil-atlas-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n\r\nfactors.summary= data.frame(factor= character(), mean=numeric(), var=numeric())\r\n\r\nall.factors<-c(""Achievement"", ""Belongingness"", ""Benevolence"", ""Conformity"", ""Hedonism"", ""Power"", ""Routine"", ""Self.direction"", ""Stimulation"", ""Universalism"", ""Global.motivation"")\r\n\r\nfor (k in all.factors){\r\n  f<-ans %>% select(starts_with(k)) \r\n  num.col<-ncol(f)\r\n  v.final=vector()\r\n  for(i in 1:num.col){\r\n    v<-as.vector(f[,i])\r\n    v.final<-c(v.final, v)\r\n  }\r\n  \r\n  f.mean<-round(mean(v.final), 2)\r\n  f.var<-round(var(v.final),2)\r\n  new.row<-data.frame(factor= k, mean=f.mean, var=f.var)\r\n  factors.summary<-rbind(factors.summary, new.row) # Add row to the dataframe\r\n  \r\n  rm(f, f.mean, f.var, new.row, v.final, v, num.col)\r\n  \r\n}\r\n\r\n\r\n#--------------------------- CORRELATION ANALYSIS\r\n\r\ncompleted<- read.csv(""open-soil-atlas-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n# average questions for each tag\r\n# average results for each tag\r\n\r\n#achievement\r\nach.subset<-completed %>% select(starts_with(""ach""))\r\ncompleted$ach <- rowMeans(ach.subset, na.rm = TRUE)\r\n\r\n#belongingness\r\nbel.subset<-completed %>% select(starts_with(""bel""))\r\ncompleted$bel <- rowMeans(bel.subset, na.rm = TRUE)\r\n\r\n#benevolence\r\nben.subset<-completed %>% select(starts_with(""ben""))\r\ncompleted$ben <- rowMeans(ben.subset, na.rm = TRUE)\r\n\r\n#conformity\r\nconf.subset<-completed %>% select(starts_with(""conf""))\r\ncompleted$conf <- rowMeans(conf.subset, na.rm = TRUE)\r\n\r\n#hedonism\r\nhed.subset<-completed %>% select(starts_with(""hed""))\r\ncompleted$hed <- rowMeans(hed.subset, na.rm = TRUE)\r\n\r\n#power\r\npwr.subset<-completed %>% select(starts_with(""pow""))\r\ncompleted$pwr <- rowMeans(pwr.subset, na.rm = TRUE)\r\n\r\n#routine\r\nrout.subset<-completed %>% select(starts_with(""rout""))\r\ncompleted$rout <- rowMeans(rout.subset, na.rm = TRUE)\r\n\r\n#self-direction\r\nself.subset<-completed %>% select(starts_with(""self""))\r\ncompleted$self <- rowMeans(self.subset, na.rm = TRUE)\r\n\r\n#stimulation\r\nstim.subset<-completed %>% select(starts_with(""stim""))\r\ncompleted$stim <- rowMeans(stim.subset, na.rm = TRUE)\r\n\r\n#universalism\r\nuniv.subset<-completed %>% select(starts_with(""univ""))\r\ncompleted$univ <- rowMeans(univ.subset, na.rm = TRUE)\r\n\r\n\r\n# Correlation between motivating factors and global motivation\r\nd.cor<-comple']",2,"Open Soil Atlas, motivation study, citizen science, participation, personal enjoyment, improvement, achievement, personal relationships, environment, factors, engagement, soil pollution, pilot, ACTION project, Coney toolkit, RO-Crate, conversational survey, Survey Ont"
Walk Up Aniene Motivation Survey,"The Walk Up Aniene motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting soil and water pollution in the Walk Up Aniene pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/walk-up-aniene/.The Walk Up Aniene motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation","['setwd(""walk-up-aniene"")\r\n\r\n## DATA PREPARATION\r\nraw.data<- read.csv(""walk-up-aniene-results.csv"", header=T)\r\n\r\n# Keep only completed surveys\r\nraw.data.unfinished<-raw.data[raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data.unfinished$user))\r\n\r\nraw.data<-raw.data[!raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data$user))\r\n\r\n# Keep only questions related to motivation \r\nmotivation.questions<-raw.data[raw.data$tag %in% c(""achievement"", ""conformity"", ""self-direction"", ""stimulation"", ""routine"", ""hedonism"", ""power"", ""belongingness"", ""benevolence"", ""universalism"", ""global motivation""),]\r\nunique(motivation.questions$tag)\r\n\r\n# Create unique id made usign tag+id.question to perform pivoting of the table  \r\nmotivation.questions<-cbind(motivation.questions, tag.question = paste0(motivation.questions$tag, motivation.questions$questionId))\r\n\r\n\r\n## --  pivoting table (one row for each user and one column for each question. As value the numerical value given as answer)\r\nlibrary(reshape2) \r\nlibrary(reshape) \r\n\r\nmatch.tag.question<-unique(motivation.questions[, c(""tag.question"", ""question"")])\r\n# Rows now represent users, each column is related to a tag.question. Value of each cell is taken from the ""value"" column of the original file (default behaviour of the function, if a ""value"" column doesn\'t exist in the original table you need to specify which column should be used).\r\npivot.motivation.questions<-cast(motivation.questions, user ~ tag.question , fun.aggregate = mean)\r\npivot.motivation.questions<-pivot.motivation.questions[ , !(names(pivot.motivation.questions) %in% c(""global motivation739""))]\r\n\r\n\r\n# Modify question names to uniform analysis according to the survey-motivation-template\r\nnames(pivot.motivation.questions) <- c(""user"", ""achievement.1"", ""achievement.2"", ""achievement.3"", ""belongingness.1"", ""belongingness.2"", \r\n  ""benevolence.2"", ""benevolence.1"", ""conformity.2"", ""conformity.1"", ""global motivation"", ""hedonism.2"", ""hedonism.1"",\r\n  ""power.2"", ""power.1"", ""routine.1"", ""routine.2"", ""self-direction.2.1"", ""self-direction.2.2"", ""self-direction.1"", \r\n  ""self-direction.2.3"", ""self-direction.2.4"", ""stimulation.2"", ""stimulation.1"", ""universalism.1"", ""universalism.2"")\r\n\r\nwrite.csv(pivot.motivation.questions, ""walk-up-aniene-pivot-questions.csv"", row.names = F)\r\n\r\n#--------------------------- ANALYSIS RESULTS BY QUESTION\r\n\r\n# Compute average and variance for each question \r\nans<- read.csv(""walk-up-aniene-pivot-questions.csv"", header=TRUE)\r\n\r\nmean.question<-round(sapply(ans[,c(2:26)], mean),2)\r\nvar.questions<-round(sapply(ans[,c(2:26)], var), 2)\r\n\r\ndf.questions<- data.frame(mean = mean.question, var = var.questions)\r\n\r\n\r\n#--------------------------- ANALYSIS RESULTS BY MOTIVATING FACTOR\r\n\r\n# Compute average and variance for each motivating factor \r\nans<- read.csv(""walk-up-aniene-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n\r\nfactors.summary= data.frame(factor= character(), mean=numeric(), var=numeric())\r\n\r\nall.factors<-c(""Achievement"", ""Belongingness"", ""Benevolence"", ""Conformity"", ""Hedonism"", ""Power"", ""Routine"", ""Self.direction"", ""Stimulation"", ""Universalism"", ""Global.motivation"")\r\n\r\nfor (k in all.factors){\r\n  f<-ans %>% select(starts_with(k)) \r\n  num.col<-ncol(f)\r\n  v.final=vector()\r\n  for(i in 1:num.col){\r\n    v<-as.vector(f[,i])\r\n    v.final<-c(v.final, v)\r\n  }\r\n  \r\n  f.mean<-round(mean(v.final), 2)\r\n  f.var<-round(var(v.final),2)\r\n  new.row<-data.frame(factor= k, mean=f.mean, var=f.var)\r\n  factors.summary<-rbind(factors.summary, new.row) # Add row to the dataframe\r\n  \r\n  rm(f, f.mean, f.var, new.row, v.final, v, num.col)\r\n  \r\n}\r\n\r\n\r\n#--------------------------- CORRELATION ANALYSIS\r\n\r\ncompleted<- read.csv(""walk-up-aniene-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n# average questions for each tag\r\n# average results for each tag\r\n\r\n#achievement\r\nach.subset<-completed %>% select(starts_with(""ach""))\r\ncompleted$ach <- rowMeans(ach.subset, na.rm = TRUE)\r\n\r\n#belongingness\r\nbel.subset<-completed %>% select(starts_with(""bel""))\r\ncompleted$bel <- rowMeans(bel.subset, na.rm = TRUE)\r\n\r\n#benevolence\r\nben.subset<-completed %>% select(starts_with(""ben""))\r\ncompleted$ben <- rowMeans(ben.subset, na.rm = TRUE)\r\n\r\n#conformity\r\nconf.subset<-completed %>% select(starts_with(""conf""))\r\ncompleted$conf <- rowMeans(conf.subset, na.rm = TRUE)\r\n\r\n#hedonism\r\nhed.subset<-completed %>% select(starts_with(""hed""))\r\ncompleted$hed <- rowMeans(hed.subset, na.rm = TRUE)\r\n\r\n#power\r\npwr.subset<-completed %>% select(starts_with(""pow""))\r\ncompleted$pwr <- rowMeans(pwr.subset, na.rm = TRUE)\r\n\r\n#routine\r\nrout.subset<-completed %>% select(starts_with(""rout""))\r\ncompleted$rout <- rowMeans(rout.subset, na.rm = TRUE)\r\n\r\n#self-direction\r\nself.subset<-completed %>% select(starts_with(""self""))\r\ncompleted$self <- rowMeans(self.subset, na.rm = TRUE)\r\n\r\n#stimulation\r\nstim.subset<-completed %>% select(starts_with(""stim""))\r\ncompleted$stim <- rowMeans(stim.subset, na.rm = TRUE)\r\n\r\n#universalism\r\nuniv.subset<-completed %>% select(']",2,"Walk Up Aniene, motivation study, citizen science, H2020 project, ACTION project, volunteer participation, personal enjoyment, improvement, achievement, personal relationships, care for environment, analysis, factors, engagement, fighting soil pollution, water pollution, pilot"
Immune challenges increase network centrality in a queenless ant,"Social animals display a wide range of behavioural defences against infectious diseases, some of which inherently increase social contacts with infectious individuals (e.g., mutual grooming), while others decrease them (e.g., social exclusion). These defences often rely on the detection of infectious individuals, but this can be achieved in several ways that are difficult to differentiate. Here, we combine non-pathogenic immune challenges with automated tracking in colonies of the clonal raider ant to ask whether ants can detect the immune status of their social partners and to quantify their behavioural responses to this perceived infection risk. We first show that a key behavioural response elicited by live pathogens (allogrooming) can be qualitatively recapitulated by immune challenges alone. Automated scoring of interactions between all colony members reveals that this behavioural response increases the network centrality of immune-challenged individuals through a general increase in physical contacts. These results show that ants can detect the immune status of their nestmates and respond with a general ""caring"" strategy, rather than avoidance, towards social partners that are perceived to be infectious. Finally, we find no evidence that changes in cuticular hydrocarbon profiles drive these behavioural effects.","['#set directory\nsetwd(\'/Users/Desktop/Obiroi_ImmuneChallenge\')\n\n#clear workspace\nrm(list=ls())\n\n#load packages\nlibrary(survival)\nlibrary(multcomp)\nlibrary(glmmTMB)\nlibrary(DHARMa)\nlibrary(ggplot2)  \nlibrary(survminer)\nlibrary(coxme)\nlibrary(emmeans)\nlibrary(EnvStats)\nlibrary(readxl)\nlibrary(nlme)\nlibrary(plyr)\nlibrary(gridExtra)\nlibrary(igraph)\nlibrary(vegan)\nlibrary(PerformanceAnalytics)\nlibrary(adegenet)\nlibrary(reshape2)\nlibrary(randomForest)\n\n#Define palette plots\ncolTr=""#0072B2""  \ncolSh=""#D55E00""\ncolNV=""#006400""\n\n#Create folder to save plots\nifelse(!dir.exists(file.path(getwd(),\'Plots\')), dir.create(file.path(getwd(),\'Plots\')), FALSE)\n\n#Create folder to save confidence intervals\nifelse(!dir.exists(file.path(getwd(),\'BootstrappedCI\')), dir.create(file.path(getwd(),\'BootstrappedCI\')), FALSE)\n\n###Fig.S1, Pathogen Exposure Survival###\nobiroi.survival=read.table(file=""FigS1_PathogenExposureSurvival/PathogenExposure.survival.txt"", header=T, sep=""\\t"")\n\n#Create survival curves for plotting\nfit=survfit(Surv(day.of.death, status) ~ treatment.rearing, data = obiroi.survival) \n\n#Plot\nobiroiSurv=ggsurvplot(\n  fit,                     \n  data = obiroi.survival,  # data used to fit survival curves. \n  risk.table = FALSE,       # show risk table.\n  pval = FALSE,             # show p-value of log-rank test.\n  conf.int = TRUE,         # show confidence intervals \n  break.time.by = 1,     \n  palette=c(colTr,colTr,colSh,colSh),\n  censor=FALSE,\n  linetype=c(1,3,1,3),\n  size=0.3,\n  legend=""none"",\n  ggtheme = theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n                  panel.background = element_blank(), axis.line = element_line(colour = ""black""),\n                  text=element_text(size=10,family=""Helvetica""),axis.text = element_text(size=9)),\n  risk.table.y.text.col = FALSE, \n  risk.table.y.text = FALSE, \n  xlab=(\'Days after Treatment\'),\n  ylab=(\'Proportion Surviving\')\n)\nggsave(filename=""Plots/FigS1_PathogenExpSurv.pdf"",print(obiroiSurv),width=3.625,height=2,unit=""in"")\n\n#Statistical model\ncox_Surv=coxme(Surv(day.of.death, status) ~ treatment*rearing+age + (1|replicate), data = obiroi.survival)\nanova(cox_Surv) #age has no effect, removing from future models and experiments\n\ncox_Surv=coxme(Surv(day.of.death, status) ~ treatment*rearing + (1|replicate), data = obiroi.survival)\nanova(cox_Surv)\n\npairwise_survdiff(Surv(day.of.death, status) ~ treatment.rearing, data = obiroi.survival, p.adjust.method = ""BH"" ) #pairwise comparisons\n\nconfint(glht(cox_Surv)) #confidence intervals\n\n\n###Fig.S2 Pathogen Exposure Allogrooming###\nPathogenExpGrooming<-read.table(\'FigS2_PathogenExposureGrooming/PathogenExposure.grooming.txt\',header=TRUE,na.strings=""NA"","""","" "",fill=TRUE) \n\nPathogenExpGrooming$treatment=as.factor(PathogenExpGrooming$treatment) #Treatment as factor\nPathogenExpGrooming$age=as.factor(PathogenExpGrooming$age) #Young/Old\nPathogenExpGrooming$allogroom.time<-as.numeric(PathogenExpGrooming$allogroom.time) #Duration allogrooming\nPathogenExpGrooming$timenum=as.numeric(PathogenExpGrooming$time) #Time of observation\nPathogenExpGrooming$rep<-as.factor(PathogenExpGrooming$rep) #Batch\nPathogenExpGrooming$time<-as.factor(PathogenExpGrooming$time) #Time of observation as factor\n\nPathogenExpGrooming$treat_time<-interaction(PathogenExpGrooming$treatment,PathogenExpGrooming$time) # 90-level variable coding treatment at each time\nPathogenExpGrooming$indID<-interaction(PathogenExpGrooming$rep,PathogenExpGrooming$age,PathogenExpGrooming$treatment) # 56-level variable coding individual ID\n\nSubPathogenGrooming<-subset(PathogenExpGrooming,allogroom.time!=\'NA\') #subset for which allogrooming data is available (final=427 lines)\n\nSubPathogenGrooming<-droplevels(SubPathogenGrooming) # drop unused levels (18 levels for treat_time, 48 levels for indID)\nSubPathogenGrooming$index<-as.factor(seq_len(nrow(SubPathogenGrooming)))  #include index unique for each observation\n\n#Statistical Model\nmPEG<-glmmTMB(allogroom.time~treatment*time+(1|rep/indID), data=SubPathogenGrooming, dispformula=~ time*treatment,\n              start=list(thetaf=1.5), family = tweedie)\nplot(simulateResiduals(mPEG)) #Diagnostic model\n\nmPEGNoInt<-glmmTMB(allogroom.time~treatment+time+(1|rep/indID), data=SubPathogenGrooming, dispformula=~ treatment+time,\n                       start=list(thetaf=1.565), family = tweedie)\nanova(mPEG,mPEGNoInt) #compare to model without interaction treatment*time\n\nemmeans(mPEG, specs = pairwise ~ treatment|time)\n\n#Plot\nSubPathogenGroomingPlot=  ggplot(aes(x=time, y=allogroom.time, color=treatment, shape=treatment), data=SubPathogenGrooming)+\n    geom_point(data=SubPathogenGrooming,aes(x=time,y=allogroom.time,color=treatment,shape=treatment),position=position_jitterdodge(jitter.width=0.6),size=1,alpha=0.5)+\n    stat_summary(fun=mean, geom=""point"",position=position_nudge(x=c(-0.2,0.2)),size=2)+\n    stat_summary(fun.data=mean_cl_boot, geom=""errorbar"", width=0.15,position=position_nudge(x=c(-0.2,0.2))) +\n    scale_y_continuous(br']",2,"Immune challenges, network centrality, queenless ant, social animals, infectious diseases, behavioural defences, mutual grooming, social exclusion, detection of infectious individuals, non-pathogenic, automated tracking, clonal raider ant, live pathogens, al"
Unequal reproduction early in a social transition,"In eusocial insects, nestmate queens can differ in their reproductive output, causing asymmetries in the distribution of mutual benefits. However, little is known about how reproductive success is partitioned in incipiently polygynous species, which would provide clues about the evolutionary forces shaping the emergence of polygyny. Here, we leverage a recent transition from predominantly single-queen (monogyne) to multiple-queen (polygyne) colonies in an invasive yellowjacket to investigate whether queens in incipiently polygyne colonies invest equally in reproductive effort or vary in their relative investment in each caste. We excavated nine polygyne Vespula pensylvanica colonies in Hawaii and used RAD-sequencing to infer the parentage of worker, male, and gyne (daughter queen) pupae from each nest comb. In four colonies with at least eight gyne pupae, a single queen produced most or all gynes. These queens had no male offspring and few worker offspring, suggesting that a subset of nestmate queens might exploit the collective benefits of newly polygyne societies. In contrast to most queens, gyne-producers had offspring distributed non-randomly across nest combs. Nestmate queens generally exhibited low relatedness levels. Our results suggest that rapid, ecologically driven transitions to polygyny among unrelated queens may, at their onset, be vulnerable to reproductive asymmetries that are likely evolutionarily unstable. More broadly, this study contributes to the understanding of social evolution by uncovering asymmetric partitioning of reproduction in a population with newly evolved polygyny and raises questions about the future trajectories of introduced populations.","['####\n######## Working Directory and Libraries ###########\n####\n\n#set WD to a folder containing the allele frequencies (a csv called allele_freqs.csv), and include the datfile_part1.txt and datfile_part2.txt files if you want to specify the COLONY parameters and then have the script generate a formatted .dat file to be run directly in COLONY.  \nsetwd(""/Users/kloope/Dropbox/Projects/Madison_Vpenn/simulating_data_for_colony/Supp Info/"")\n\n#Libraries\nlibrary(tidyr)\nlibrary(ggplot2)\n\n####\n######## Functions ################################\n####\n\n#***************************************************\n#SAMPLE_ALLELE: make a sampler for any number of loci with any number of alleles, input is a data frame with columns titled ""marker"", ""allele"",""freq""\nsample_allele<-function(df){\n  markers<-df[,c(""marker"")]\n  alleles<-df[,c(""allele"")]\n  freq<-df[,c(""freq"")]\n  umarkers<-unique(markers) #make a list of the markers\n  outvector<-vector(length=length(umarkers)) #make an empty output vector of the right length\n  for(i in c(1:length(umarkers))){ #for each of the markers...\n    sub<-markers==umarkers[i] #these are the subset of the rows that are for the current marker\n    outvector[i]<-sample(x = alleles[sub], 1, replace = T, prob = freq[sub])  #sample the alleles from the current marker with probability according to the freqs of current marker (with replacement)\n  }\n  outvector #return the vector\n}\n\n\n#___________________________________________________\n\n\n\n#***************************************************\n#NEWQUEENS: function that generates n random queens in a list from a specified allele frequency df\nnewqueens <- function(n,allele_freq){\n  outlist<- vector(mode = ""list"", length = n)\n  for(i in c(1:n)){\n    outlist[[i]]<-as.matrix(rbind(sample_allele(allele_freq),sample_allele(allele_freq)))\n  }\n  outlist\n}\n#___________________________________________________\n\n\n\n#***************************************************\n#NEWMALES: function that generates n random males in a list from a specified alllele frequency df\nnewmales <- function(n,allele_freq){\n  outlist<- vector(mode = ""list"", length = n)\n  for(i in c(1:n)){\n    outlist[[i]]<-as.matrix(sample_allele(allele_freq))\n  }\n  outlist\n}\n#___________________________________________________\n\n\n#***************************************************\n#SAMPLE_QUEEN: function to sample randomly from a queen\'s alleles to generate offspring types\nsample_queen<-function(queen){\n  apply(queen, 2, function(x) x[sample(c(1,2),1)]) #randomly choose element 1 or 2 from a column, then apply across columns for the matrix queen\n}\n#___________________________________________________\n\n\n\n#***************************************************\n#MAKE_DAUGHTER: make a daughter from a queen and a male\nmake_daughter<-function(queen,male){\n  cbind(as.matrix(sample_queen(queen)),male) #makes a daughter (2 column format!)\n}\n#___________________________________________________\n\n\n#***************************************************\n#MAKE_HALF_SIBS: This function generates n half-siblings from a random mother given an allele_freq df.  use this to generate sib queens (output of females is in 2 column format)\nmake_half_sibs<-function(n,allele_freq){\n  mother<-newqueens(1,allele_freq)[[1]] #generate a random mother from the allele freqs\n  siblist<- vector(mode = ""list"", length = n)\n  for (i in c(1:n)){\n    siblist[[i]]<-t(make_daughter(mother,sample_allele(allele_freq)))\n  }\n  siblist\n}\n#___________________________________________________\n\n\n\n#***************************************************\n#MAKE_FATHERS: make fathers for a given list of queens.  this will make a list of lists, each ""outer"" list corresponds to a queen, and the inner list is a list of each of her male mates.  requires an allele freq df as well\nmake_fathers<-function(queenlist,nmates=5,allele_freq){\n  l<-length(queenlist)\n  outlist<-vector(mode=""list"",length=l) #make a list of lists...one for each female\n  for (i in c(1:l)){\n    outlist[[i]]<-newmales(nmates,allele_freq)\n  }\n  outlist\n}\n#___________________________________________________\n\n\n#***************************************************\n#MAKE_COLONY: here\'s a function to make a colony of n daughters from a randomly sampled queen from a pregenerated list of queens (using newqueens() or make_half_sibs()) and a randomly sampled male mate from a pre-generated list of male mates (function make_fathers())\nmake_colony<-function(ndaughter,nson,queens,mates,colnum){\n  outlist<-vector(mode=""list"",length=ndaughter+nson)\n  if(ndaughter>0){ #only make daughters if ndaughter > 0\n    for(i in c(1:ndaughter)){\n      q<-sample(c(1:length(queens)),1) #sample a queen\n      m<-sample(c(1:length(mates[[q]])),1) #sample a mate of that queen\n      outlist[[i]]<-c(paste0(""C"",colnum),paste0(""daughter"",sprintf(""%03d"", i)),paste0(""Q"",q),paste0(""M"",m),t(make_daughter(queens[[q]],mates[[q]][[m]])))#make a daughter from those parents and add the colony, queen and male info at the start.  transpose the makedaughter output so ']",2,"unequal reproduction, social transition, eusocial insects, nestmate queens, reproductive output, mutual benefits, asymmetries, polygyny, evolutionary forces, yellowjacket, reproductive effort, monogyny, multiple-queen, colonies,"
"Manhattan, New York City, 2020 Traffic Time Series + R Code for Analysis","This dataset includes (1) a .txt file of processed time-series with four traffic congestion levels for the borough of Manhattan, NYC, averaged every 3 hours for the duration of 2020, and (2) an R script for completing analysis of the traffic time series to determine patterns in traffic over the year 2020, and to evaluate the impact of stay-at-home orders implemented in response to the COIVD-19 pandemic.","['##############################\n###                        ###\n###  Stay-at-home Fatigue  ###\n###       09/11/2020       ###\n###      J. Shearston      ###\n###                        ###\n##############################\n\n# Last Updated: February 19, 2021\n\n# Summary\n# This script contains code for doing a time series analysis of traffic data\n# from Google traffic maps. It loads data that has already been cleaned and\n# prepared through image analysis in MatLab, by Markus Hilpert. It includes descriptive\n# analysis, modeling with GAMs, seasonal trend decomposition, and the creation\n# of several figures for a manuscript. Multiple exploratory GAM models are\n# constructed before a final set of models is selected. We attempt to fit GAMs \n# that most appropriately fit the observed time series. \n\n# Table of Contents\n# 1: Preparation\n# 2: Load data and add variables\n# 3: Review time series\n# 4: Exploratory models for each intervention level (separate models for each level)\n# 5: Exploratory model with intervention levels as one categorical variable (one full model)\n# 6: Determine optimal df values for full model with complete time series\n# 7: Construct nested models with full time series to determine which variables are needed\n# 8: AIC and Likelihood Ratio tests for comparing full models\n# 9: Evaluate residual plots for model on full time series\n# 10: Determine breakpoints for three models, using time series to drive cutpoints\n# 11: Three model solution\n# 12: Four model solution\n# 13: Predictions for ""typical"" hours of day, by weekend and weekday\n# 14: Seasonal trend decomposition (STL)\n# 15: Figures for manuscript\n# 16: Table 1 for manuscript (Supplementary table 1 uses data collated separately)\n\n\n##########################################################################\n# 1: Preparation\n##########################################################################\n\n# 1A Load Packages\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(mgcv)\nlibrary(visreg)\nlibrary(lmtest)\nlibrary(tidymv)\nlibrary(grid)\nlibrary(gridExtra)\nrequire(stats)\n\n\n##########################################################################\n# 2: Load Data & Add Vars\n##########################################################################\n\n# Note: Data is in a txt file, with the DateNum variable representing Matlab\'s\n# datetime structure. However, a character string of datetime is also provided\n\n# 2A Read in data\ngcc <- read_csv(""./CCC_COVID19.txt"") %>% janitor::clean_names()\n\n# 2B Reformat date variable and create additional variables\ngcc <- gcc %>% \n  mutate(date_time = as.POSIXct(date_string,format=c(\'%d-%b-%Y %H:%M:%S\')),\n         hour_of_day = hour(date_time),\n         dow = wday(date_time, week_start = 1),                  # 1=Monday; 7=Sunday\n         weeknum = week(date_time),\n         daynum = yday(date_time),\n         intervention = case_when(\n           daynum < 82 ~ ""Pre-Intervention"",\n           daynum >= 82 & daynum < 160 ~ ""PAUSE"",                # March 22 - June 7\n           daynum >= 160 & daynum < 174 ~ ""Phase 1"",             # June 8 - June 21\n           daynum >= 174 & daynum < 188 ~ ""Phase 2"",             # June 22 - July 5\n           daynum >= 188 & daynum < 199 ~ ""Phase 3"",             # July 6 - July 16\n           daynum >= 199 ~ ""Phase 4""                             # July 16 onward\n         ),\n         intervention = as_factor(intervention))\n\n# 2C Check levels of new ""intervention"" factor\nlevels(gcc$intervention)\ntable(gcc$intervention, useNA = ""always"")\n\n# 2D Converting day of week variable (dow) to a factor\ngcc2 <- gcc %>% \n  mutate(dow = as_factor(as.character(dow)))\n\n# 2E Add new weekday/weekend dummy vars to dataframe\ngcc2 <- gcc2 %>% \n  mutate(weekday = ifelse(dow==1 | dow==2 | dow==3 | dow==4 | dow==5, 1, 0),\n         weekend = ifelse(dow==6 | dow ==7, 1, 0),\n         weekday_hourofday = weekday*hour_of_day,\n         weekend_hourofday = weekend*hour_of_day)\n\n# 2F Restrict to only 2020\ngcc2 <- gcc2 %>% filter(date_num_matlab < 738157.0)\n# Note: Should have n=2928 observations, or 8 observations/day * 366 days (2020 is a leap year)\n\n\n##########################################################################\n# 3: Review time series\n##########################################################################\n\n# Note: We would like to review each color\'s time series to identify which \n# color might be best to complete analysis on. We choose red because of its\n# clear pandemic related signal (decrease in April) and easier interpretability\n# e.g., a decrease in percent roads with a red color is a decrease in traffic\n# congestion \n\n\n# 3A View time series of all colors\ngcc2 %>% \n  pivot_longer(maroon:gray, names_to = ""gcc_color"", values_to = ""percent_roads"") %>% \n  ggplot(aes(x = date_time, y = percent_roads)) +\n  geom_line() +\n  facet_wrap(~ gcc_color)\n\n# 3B View time series of red color\ngcc2 %>% \n  pivot_longer(maroon:gray, names_to = ""gcc_color"", values_to = ""percent_roads"") %>% \n  filter(gcc_color == ""red"") %>% \n  ggplot(aes(x = d']",2,"Manhattan, New York City, traffic, time series, R code, analysis, dataset, congestion levels, borough, COVID-19, pandemic, stay-at-home orders, patterns, impact, 2020."
"The modularity of a social group does not affect the transmission speed of a novel, socially learned behaviour, or the formation of local variants","Data set used within the Article ""The modularity of a social group does not affect the transmission speed of a novel, socially learned behaviour, or the formation of local variants."" The structure of a group is critical in determining how a socially learnt behaviour will spread. Predictions from theoretical models indicate that specific parameters of social structure differentially influence social transmission. Modularity describes how the structure of a group or network is divided into distinct subgroups or clusters. Theoretical modelling indicates that the modularity of a network will predict the rate of behavioural spread within a group, with higher modularity slowing the rate of spread and facilitating the establishment of local behavioural variants which can prelude local cultures. Despite prolific modelling approaches, empirical tests via manipulations of group structure remain scarce.We experimentally manipulated the modularity of populations of domestic fowl chicks, Gallus gallus domesticus, to affect the transmission of a novel foraging behaviour. We compared the spread of behaviour in populations with networks of high or low modularity against a control population where social transmission was prevented. We found the foraging behaviour to spread socially between individuals when the social transmission was permitted; however, modularity did not increase the speed of behavioural spread nor lead to the initial establishments of shared behavioural variants. This result suggests that factors in the social transmission process additional to the network structure may influence behavioural spread. This dataset includes:The manipulated social networks of our 6 populations of domestic chicks (2 of high modularity, 2 of low modularity and 2 asocial control populations where social transmission was prevented).The order in which chicks learnt the solving behaviour alongside information on individual level variables.The solving techniques used by chicks in all their solves throughout the experiment.The R code use to perform the analyses (Coxph analysis, OADA, testing for assortment)","['############## Does the solving behaviour spread socially throughout the populations and do rates differ between social network structures? ########\r\n\r\n\r\nlibrary(NBDA) #downloaded from WH github\r\n\r\n#Create NBDA object for all pens combined\r\n\r\nload(""cumulativeLearningPotentialNetworksChicksModularityCombined.RData"")\r\n#Here I made the 15 \'big\' matrices showing which birds went into the testing together each round for all 6 pens together with zero\'s between the different pens\r\n#then I used the learning indicator array to get learning potential per round and then cumulative learning potential\r\n#Id\'s are recoded so that pens: 4A=1-27, 4B=28-54, 5A=55-78, 5B=79-102, 7A=103-126, 7B=127-150)\r\n\r\ndim(cumulativeLearningPotential)\r\n\r\n#We do want 6 networks- with the connections from each group, and with zeroes for the other group, allowing us to test for differences in social\r\n#transmission as before. This is easily done:\r\n\r\ncumulativeLearningPotential2<-array(0,dim=c(150,150,6,15))\r\n\r\n#WH remember in a stratified OADA the network for group 1 includes the individuals from other groups, but just has zero connections for any dyads where both individuals are not in group 1\r\n#and the same for group 2 etc.\r\n\r\ncumulativeLearningPotential2[1:27,1:27,1,]<-cumulativeLearningPotential[1:27,1:27,]\r\ncumulativeLearningPotential2[28:54,28:54,2,]<-cumulativeLearningPotential[28:54,28:54,]\r\ncumulativeLearningPotential2[55:78,55:78,3,]<-cumulativeLearningPotential[55:78,55:78,]\r\ncumulativeLearningPotential2[79:102,79:102,4,]<-cumulativeLearningPotential[79:102,79:102,]\r\ncumulativeLearningPotential2[103:126,103:126,6,]<-cumulativeLearningPotential[103:126,103:126,]\r\ncumulativeLearningPotential2[127:150,127:150,6,]<-cumulativeLearningPotential[127:150,127:150,]\r\n\r\n#here is the aquisition order for all new coded birds:\r\nAq_order_all<-c(65,84,61,14,95,81,35,12,21,29,34,47,39,62,77,85,96,98,106,7,8,30,51,43,44,49,31,68,79,86,100,115,1,10,2,15,24,28,38,72,75,78,88,83,91,99,133,4,18,26,32,54,53,67,93,97,127,131,11,13,19,41,45,56,71,3,22,69,76,87,102,17,27,36,57,89,116,25,40,55,70,90)\r\n\r\n#here is the round those birds learnt in:\r\nassMatrixIndexAll<-c(2,2,5,6,6,6,7,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,13,13,13,13,13,13,14,14,14,14,14,14,15,15,15,15,15)\r\n\r\n#and the true ties list:\r\ntrueTiesAll<-list(c(8,9),c(11,12),c(22,23),c(33,34),c(36,37),c(44,45,46),c(51,52))\r\n\r\ndemons = c(0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\r\n\r\n#Individual level variables:\r\n#WH in the NBDA package we specify each ILV as a matrix- this is to allow for the possibility we have time-dependent ILVs with multiple columns, one for each event\r\nsex4A<-cbind(c(0,0,0,0,0,1,0,1,1,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0)) \r\nmass24A<-cbind(c(133,106,148,121,143,121,121,103,114,126,119,120,122,143,95,104,108,120,122,106,120,88,94,102,111,136,115))\r\n\r\nsex4B<-cbind(c(0,0,0,0,1,1,1,0,1,1,0,0,1,1,1,1,1,1,0,0,1,0,1,1,1,0,1))\r\nmass24B<-cbind(c(124,94,121,120,90,107,105,121,121,104,123,120,102,103,101,79,102,96,143,107,92,94,84,104,107,128,114))\r\n\r\nsex5A<-cbind(c(0,0,1,0,0,0,1,1,1,0,0,0,0,1,0,1,1,1,0,0,1,1,1,1))\r\nmass25A<-cbind(c(124,124,116,120,130,137,111,111,134,133,124,156,123,122,129,131,134,142,142,133,136,128,110,139))\r\n\r\nsex5B<-cbind(c(0,0,1,1,0,0,1,0,0,1,0,1,1,1,0,0,1,0,1,0,1,1,0,1))\r\nmass25B<-cbind(c(118,132,137,124,120,133,138,114,142,132,125,122,132,136,142,135,129,114,133,144,115,122,117,143))\r\n\r\nsex7A<-cbind(c(0,0,0,1,1,0,1,1,0,0,1,1,0,1,0,0,0,1,0,1,1,1,0,1))\r\nmass27A<-cbind(c(105,110,134,128,108,116,121,129,120,118,133,128,130,119,119,116,115,112,112,128,142,122,119,117))\r\n\r\nsex7B<-cbind(c(0,1,1,0,1,0,1,0,0,0,0,1,1,1,1,1,0,0,0,0,1,0,1,1))\r\nmass27B<-cbind(c(127,137,146,120,121,121,125,118,114,127,116,131,128,148,125,129,98,132,116,118,120,127,124,119))\r\n\r\nsex<-rbind(sex4A,sex4B,sex5A,sex5B,sex7A,sex7B)\r\nmass2<-rbind(mass24A, mass24B,mass25A, mass25B, mass27A, mass27B)\r\n\r\n#Usually in a multi-diffusion using TADA or stratified OADA we would want to have a set of binary ILVs (amounting to a factor) controlling for the possibility \r\n#that each group might have a different rate of asocial learning- which might look like social transmission.\r\n#However, here is a little different. Individuals were assigned to groups at random, and there is no reason to assume that different groups would\r\n#have different rates of asocial learning.\r\n\r\nasoc_ilv<-c(""sex"", ""mass2"") \r\nint_ilv<-c(""sex"",""mass2"")\r\n\r\n\r\nAllPens<-nbdaData(label=""AllPens"", assMatrix=cumulativeLearningPotential2,assMatrixIndex=assMatrixIndexAll,orderAcq=Aq_order_all, demons=demons,asoc_ilv=asoc_ilv,int_ilv=int_ilv,trueTies = trueTiesAll)\r\n\r\nmodel_t<-oadaFit(AllPens)\r\n\r\n#']",2,"modularity, social group, transmission speed, socially learned behaviour, local variants, theoretical models, social structure, network, subgroups, clusters, behavioural spread, local cultures, domestic fowl chicks, Gallus gallus domesticus, foraging behaviour"
Winter associations predict social and extra-pair mating patterns in a wild songbird,"Despite decades of research, our understanding of the underlying causes of within-population variation in patterns of extra-pair paternity (EPP) remains limited. Previous studies have shown that extra-pair mating decisions are linked to both individual traits and ecological factors. Here, we examine whether social associations among individuals prior to breeding also shape mating patterns, specifically the occurrence of EPP, in a small songbird, the blue tit. We test whether associations during the non-breeding period predict (1) future social pairs, (2) breeding proximity, i.e. the distance between breeding individuals, and (3) the likelihood that individuals have extra-pair young together. Individuals that were more strongly associated (those that foraged more often together) during winter tended to nest closer together. This, by itself, predicts EPP patterns, because most extra-pair sires are close neighbours. However, even after controlling for spatial effects, female-male dyads with stronger social associations prior to breeding were more likely to have extra-pair young. Our findings reveal a carry-over from social associations into future mating decisions. Quantifying the long-term social environment of individuals and studying its dynamics is a promising approach to enhance our understanding of the process of (extra-) pair formation.","['\n\n#  Data and script to accompany the manuscript: \n#  ""Winter associations predict social and extra-pair mating patterns in a wild songbird"" \'\n\n\n#  Load required packages\nsapply(c(\'lme4\',\'sna\',\'data.table\',\'dplyr\'),\n       function(x) suppressMessages(require (x ,character.only =TRUE) ) )\n\n#  Load data \n#list incluidng all datasets \n#(dataset excluding social pairs, dataset including social pairs, the generated spatial null model and the raw data collected at the bird feeders)\nload(""all_data.RData"")\n\n\n\n\n#   1. Do winter associations predict spatial breeding arrangement?\n####--------------------------------------------------------------------------------------------------\n\n#  Select data \ndf <- all_data[[1]]\n\n# Create matrices\npro <- df[,c(""ID1"",""ID2"",""rank"")]   # proximity data\nspa <- df[,c(""ID1"",""ID2"",""total_activity"")]   # spatial overlap data\nwa <- df[,c(""ID1"",""ID2"",""rank_asso_avg"")]    # winter associations data\n\nprox <- with(pro, {\n  out <- matrix(nrow=nlevels(ID1), ncol=nlevels(ID2),dimnames=list(levels(ID1), levels(ID2)))\n  out[cbind(ID1, ID2)] <- rank\n  out\n})\ndiag(prox) <- NA\n\nwinter_asso <- with(wa, {\n  out <- matrix(nrow=nlevels(ID1), ncol=nlevels(ID2),dimnames=list(levels(ID1), levels(ID2)))\n  out[cbind(ID1, ID2)] <- rank_asso_avg\n  out\n})\ndiag(winter_asso) <- NA\n\nspatial_overlap <- with(spa, {\n  out <- matrix(nrow=nlevels(ID1), ncol=nlevels(ID2),dimnames=list(levels(ID1), levels(ID2)))\n  out[cbind(ID1, ID2)] <- total_activity\n  out\n})\ndiag(spatial_overlap) <- NA\n\n\n#   Linear matrix regression model\nx<-rgraph(221,2)\nx[1,,] <- winter_asso\nx[2,,] <- spatial_overlap\n\nlm <- netlm(prox, x, nullhyp = ""classical"", test.statistic = ""beta"", mode=""graph"")\nsummary(lm)\nobs <- lm$coefficients[2]  # coefficient of observed data\n\n\n# Permutation test\n\n# load spatial null model, see main text for details\nspatial_null <- all_data[[3]]\nma_co <- rep(0,1000)\n\nfor (i in c(1:1000)){\n  b <- spatial_null[[i]]\n  b$ID1 <- as.factor(b$ID1)\n  b$ID2 <- as.factor(b$ID2)\n  bb <- b[order(ID1),]\n  bb <- bb[order(ID2),]\n  h <- merge(pro, bb, by.x=c(""ID1"",""ID2""),by.y=c(""ID1"",""ID2""), all.x=TRUE)\n  h <- unique(h)\n  hh <- h[,c(""ID1"",""ID2"",""ranked_asso"")]\n  b_matrix <- with(hh, {\n    out <- matrix(nrow=nlevels(ID1), ncol=nlevels(ID2),\n                  dimnames=list(levels(ID1), levels(ID2)))\n    out[cbind(ID1, ID2)] <- ranked_asso\n    out\n  })\n  diag(b_matrix) <- NA\n  x<-rgraph(221,2)\n  x[1,,] <- b_matrix\n  x[2,,] <- spatial_overlap\n  lm_random <- netlm(prox, x, nullhyp = ""classical"", test.statistic = ""beta"", mode=""graph"")\n  ma_co[i] <- lm_random[[1:2]]  \n  \n}\n\nhist(ma_co,breaks=100)\nabline(v=obs, col=""red"")\nsum(abs(obs) < abs(ma_co))/1000  # p-value\n\n####--------------------------------------------------------------------------------------------------\n\n\n\n#   2. Do winter associations predict future social pairs?\n####--------------------------------------------------------------------------------------------------\n\n# Select data\ndf <- all_data[[2]]\n\n# Social pair matrix\nsocial_pairs <- with(df, {\n  out <- matrix(nrow=nlevels(ID1), ncol=nlevels(ID2),\n                dimnames=list(levels(ID1), levels(ID2)))\n  out[cbind(ID1, ID2)] <- social_pair\n  out\n})\n\n# Winter association matrix\nwinter_asso <- with(df, {\n  out <- matrix(nrow=nlevels(ID1), ncol=nlevels(ID2),\n                dimnames=list(levels(ID1), levels(ID2)))\n  out[cbind(ID1, ID2)] <- rank_asso_avg\n  out\n})\n\n#  Logistic network regression model\nkk_m <- netlogit(social_pairs, winter_asso, nullhyp = ""classical"",test.statistic = ""beta"") # tests based on classical asymptotics\nsummary(kk_m)\nobs <- kk_m[[1:2]]  # to get the coefficient\n\n\n#  Permutation\ndata_to_swap <- df[order(rank)]  \nrand_coef_winter <- rep(0,1000)\n\nfor (i in 1:1000) {\n  \n  data_random <- data_to_swap[, random:=sample(rank_asso_avg),by=rank]\n  d_rand <- data_random[,c(""ID1"",""ID2"",""random"")]\n  m_rand <- with(d_rand, {\n    out <- matrix(nrow=nlevels(ID1), ncol=nlevels(ID2),\n                  dimnames=list(levels(ID1), levels(ID2)))\n    out[cbind(ID1, ID2)] <- random\n    out\n  })\n  \n  lm_r <- netlogit(social_pairs, m_rand, nullhyp = ""classical"",test.statistic = ""beta"")\n  rand_coef_winter[i] <- lm_r[[1:2]]\n  \n}\n\nhist(rand_coef_winter,breaks=100)\nabline(v=obs, col=""red"")\nsum(abs(obs) < abs(rand_coef_winter))/1000   # p-value\n\n\n####--------------------------------------------------------------------------------------------------\n\n\n\n#   3. Do winter associations predict extra-pair paternity?\n####--------------------------------------------------------------------------------------------------\n\n#  Select data \ndf <- all_data[[1]]\n# If analyses should be repeated selecting only the 1st an 2nd direct neighbours\n#df <- df[rank<3,]\n\nepp <- df[,c(""ID1"",""ID2"",""epp"")]\ndistance <- df[,c(""ID1"",""ID2"",""rank"")]\nage <- df[,c(""ID1"",""ID2"",""age"")]\nbox_visit <- df[,c(""ID1"",""ID2"",""box_visit"")]\nwinter_asso <- df[,c(""ID1"",""ID2"",""rank_asso_avg"")]\ndiff_arrival <- df[,c(""ID1"",""ID2"",""diff_arrival"")]\n\n# Create matrices\nepp1 <- with(epp, {\n  out <- ']",2,"Winter, social associations, extra-pair paternity, EPP, individual traits, ecological factors, songbird, blue tit, non-breeding period, social pairs, breeding proximity, extra-pair young, foraging, neighbours, spatial effects,"
"Data from: Social disappointment and partner presence affect long-tailed macaque refusal behaviour in an ""inequity aversion"" experiment","Protest in response to unequal reward distribution is thought to have played a central role in the evolution of human cooperation. Some animals refuse food and become demotivated when rewarded more poorly than a conspecific, and this has been taken as evidence that non-human animals, like humans, protest in the face of inequity. An alternative explanation - social disappointment  shifts the cause of this discontent away from the unequal reward, to the human experimenter who could  but elects not to  treat the subject well. This study investigates whether social disappointment could explain frustration behaviour in long-tailed macaques, Macaca fascicularis. We tested 12 monkeys in a novel `inequity aversion' paradigm. Subjects had to pull a lever and were rewarded with low-value food; in half of the trials, a partner worked alongside the subjects receiving high-value food. Rewards were distributed either by a human or a machine. In line with the social disappointment hypothesis, monkeys rewarded by the human refused food more often than monkeys rewarded by the machine. Our study extends previous findings in chimpanzees and suggests that social disappointment plus social facilitation or food competition effects drive food refusal patterns.","['rm(list=ls())\n\n#model 1A\n#tests whether distributor and partner present interact to affect refusal behaviour in test (inequality) conditions\n#note that throughout the script below, ""counterbalance"" is used to refer to the condition presentation order (referred to as ""order"" in the main paper)\n\n#libraries\nlibrary(lme4)\nlibrary(car)\n\n#import data\nsetwd(""/..."")\nxdata <- read.csv(""model1A.csv"", header = TRUE, sep = "","", stringsAsFactors = TRUE)\n\n#add subject_session column\nxdata$subj.session = as.factor(paste(xdata$subject, xdata$session, sep = \'_\'))\n\n#random slopes\nsource(""/.../diagnostic_fcns.r"") #function provided by Roger Mundry\nxx.fe.re = fe.re.tab(fe.model = ""refusal ~ distributor * partner_presence + trial + session + sex + counterbalance"", re = ""(1|subject) + (1|subj.session)"", data = xdata)\nxx.fe.re$summary\n\n#replacing ""xdata"" with the ""t.data"" dataframe which arose from the xx.fe.re function\n#the advantage in using t.data is that this is free of NAs (in case it was not already) and variables are already dummy-coded\nt.data = xx.fe.re$data\n\n#center the already dummy-coded factors\nt.data$sex.male <- t.data$sex.male - mean(t.data$sex.male)\nt.data$counterbalance.partner <- t.data$counterbalance.partner - mean(t.data$counterbalance.partner)\nt.data$partner_presence.y <- t.data$partner_presence.y - mean(t.data$partner_presence.y)\n\n#z-transform the covariates\nt.data$z.trial <- as.vector(scale(t.data$trial))\nt.data$z.session <- as.vector(scale(t.data$session))\n\n#fit the full model (notation: suffix "".wc"" = with correlations) \ncontr = glmerControl(optimizer = ""nlminbwrap"", optCtrl = list(maxfun = 1e+05))\nfull.wc = glmer(refusal ~ distributor * partner_presence + z.trial +\n                  z.session + sex + counterbalance + \n                  (1 + partner_presence.y + z.trial + z.session | subject) + (1 + z.trial|subj.session),\n                data = t.data, control = contr, family = binomial)\n\n#check whether to keep or remove correlations \nsummary(full.wc)$varcor\n#decision: there is a high correlation (1.000) but the other correlations are low, so fine to continue to continue with full.wc\n\n#check BLUPs\n#BLUPs should be normally distributed and the range on the x-axis should not be too large i.e not greater than c. 3 or -3  \nranef.diagn.plot(full.wc) #one extreme plot\n\n#check model stability\nsource(""/.../glmm_stability.r"") #function provided by Roger Mundry\nm.stab = glmm.model.stab(model.res = full.wc, contr = contr)\nm.stab.plot(m.stab$summary[,-1]) \ntable(m.stab$detailed$warnings)\nround(m.stab$summary[, -1], 3)\n\n#check collinearity \n#all Variance Inflation Factors (VIFs) should be close to 1\nxx = lm(refusal ~ distributor + partner_presence + z.trial + z.session + \n          sex + counterbalance, data = t.data)\nround(vif(xx), digits = 2)\n\n#fit null model\nnull.wc = glmer(refusal ~ z.trial + z.session + \n                  sex + counterbalance + \n                  (1 + partner_presence.y + z.trial + z.session | subject) + (1 + z.trial|subj.session),\n                data = t.data, control = contr, family = binomial)\n\n#compare full and null models\nround(anova(full.wc, null.wc, test = ""Chisq""), digits = 3)\n\n#full model output\nround(summary(full.wc)$coefficients, digits = 3)\n\n#more reliable P-values are derived from the code below\n#this code uses likelihood ratio tests and compares the full model with the respective reduced models.\ntests.full <- round(as.data.frame(drop1(full.wc, test = ""Chisq"")), 3)\ntests.full\n\n#confidence intervals full model\nsource(""/.../boot_glmm.r"") #function provided by Roger Mundry\nboot.full = boot.glmm.pred(model.res = full.wc, excl.warnings = F, nboots = 1000, para = F, resol = 1000, level = 0.95, use = NULL) \nround(boot.full$ci.estimates, 3) \nm.stab.plot(boot.full$ci.estimates)\n\n#fit reduced model\nred.wc = glmer(refusal ~ distributor + partner_presence + z.trial +\n                 z.session + sex + counterbalance + \n                 (1 + partner_presence.y + z.trial + z.session | subject) + (1 + z.trial|subj.session), \n               data = t.data, control = contr, family = binomial)\n\n#output reduced model\nround(summary(red.wc)$coefficients, digits = 3)\n\n#p-values reduced model\n#this code uses likelihood ratio tests and compares the full model with the respective reduced models.  \ntests.red <- round(as.data.frame(drop1(red.wc, test = ""Chisq"")), 3)\ntests.red\n\n#confidence intervals reduced model\nsource(""/.../boot_glmm.r"") #function provided by Roger Mundry\nboot.red = boot.glmm.pred(model.res = red.wc, excl.warnings = F, nboots = 1000, para = F, resol = 1000, level = 0.95, use = NULL) #run on institute computer\nround(boot.red$ci.estimates, 3)\nm.stab.plot(boot.red$ci.estimates)\n\n#effect size\nlibrary(MuMIn)\n(effect_size = r.squaredGLMM(red.wc))\n\n#citation information\nsessionInfo()\nR.Version()\ncitation()\n', 'rm(list=ls())\n\n#model 1B\n#tests whether distributor and partner_presence interact to affect refusal behaviour\n#model 1B is identical in structure to model 1A but it is run with only a subset of the data (597 trials, all the trials carried out prior to Ilja\'s death).\n#below is a list which details the date for each subject from which trials were excluded onward: \n\n#Mars (completed prior to ilja\'s death, no data excluded)  \n#Moritz (06/06/2019 onwards)  \n#Sherry (14/06/2019 onwards)  \n#Mona (07/06/2019 onwards)  \n#Mila (04/06/2019 onwards)  \n#Sambia (euthanised on same date as Ilja, no data excluded)  \n#Lukas (11/06/2019)  \n#Meiwi (04/06/2019)  \n#Mara (11/06/2019)  \n#Smilla (14/06/2019)  \n#Max (04/06/2019)  \n#Simon (06/06/2019) \n\n#note that throughout the script below, ""counterbalance"" is used to refer to the condition presentation order (referred to as ""order"" in the main paper)\n\n#libraries\nlibrary(knitr)\nlibrary(lme4)\nlibrary(car)\n\n#import data\nsetwd(""/..."")\nxdata <- read.csv(""model1B.csv"", \n                  header = TRUE, \n                  sep = "","", \n                  stringsAsFactors = TRUE)\n\n#add subject_session column\nxdata$subj.session = as.factor(paste(xdata$subject, xdata$session, sep = \'_\'))\n\n#preparatory steps  \n#although it is an experimental design and the data should be balanced, the section below checks this. \nkable(xx <- table(xdata$sex, xdata$subject)) #checking each subj. only assigned 1 sex\nkable(xx <- table(xdata$counterbalance, xdata$subject)) #checking each subj. only assigned 1 counterbalance order\nkable(xx <- table(xdata$partner_presence, xdata$subject)) #checking balanced\nkable(xx <- table(xdata$distributor, xdata$partner_presence)) # Reason for slight imbalance (more human than machine trials) is that we have only 5 animals in the machine group compared to 7 in the human group. In addition to this some machine trials were discounted due to apparatus malfunctions.\n\n#random slopes\nsource(""/.../diagnostic_fcns.r"") #function provided by Roger Mundry\nxx.fe.re = fe.re.tab(fe.model = ""refusal ~ distributor * partner_presence + trial + session + sex + counterbalance"", re = ""(1|subject) + (1|subj.session)"", data = xdata)\nxx.fe.re$summary\n\n#replacing ""xdata"" with the ""t.data"" dataframe which arose from the xx.fe.re function\n#the advantage in using t.data is that it is free of NAs and variables are already dummy-coded.\nt.data = xx.fe.re$data\n\n#center the already dummy-coded factors:\nt.data$sex.male <- t.data$sex.male - mean(t.data$sex.male)\nt.data$counterbalance.partner <- t.data$counterbalance.partner - mean(t.data$counterbalance.partner)\nt.data$partner_presence.y <- t.data$partner_presence.y - mean(t.data$partner_presence.y)\n\n#z.transform the covariates:\nt.data$z.trial <- as.vector(scale(t.data$trial))\nt.data$z.session <- as.vector(scale(t.data$session))\n\n#fit the full model (notation: suffix "".wc"" = with correlations)\ncontr = glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))\nfull.wc = glmer(refusal ~ distributor * partner_presence + z.trial + \n                              z.session + sex + counterbalance + (1 + partner_presence.y + z.trial + z.session | subject) + (1 + z.trial|subj.session), \n                            data = t.data, control = contr, family = binomial)\n\n#check whether to keep or remove correlations \n#as we are looking to compare the output of model 1B with that of model 1A, we will keep the model structure as similar as possible (i.e. we will keep the correlations in for model 1B).  \nsummary(full.wc)$varcor \n\n#check BLUPs\n#BLUPs should be normally distributed and the range on the x-axis should not be too large i.e not greater than c. 3 or -3  \nranef.diagn.plot(full.wc) #two extreme plots \n\n#check model stability\nsource(""/.../glmm_stability.r"") #function provided by Roger Mundry\nm.stab = glmm.model.stab(model.res = full.wc, contr = contr) \nm.stab.plot(m.stab$summary[,-1]) \ntable(m.stab$detailed$warnings)\nround(m.stab$summary[, -1], 3)\n\n#fit null model\nnull.wc = glmer(refusal ~ z.trial + z.session + \n                  sex + counterbalance + \n                  (1 + partner_presence.y + z.session + z.trial | subject) + (1 + z.trial|subj.session),\n                data = t.data, control = contr, family = binomial)\n\n#compare full and null models\nround(anova(full.wc, null.wc, test = ""Chisq""), digits = 3)\n\n#full model output\nround(summary(full.wc)$coefficients, digits = 3)\n\n#more reliable P-values are derived from the code below\n#this code uses likelihood ratio tests and compares the full model with the respective reduced models.\n#note as p-values are a function of sample size, for this model we do not report the p-values in the ESM\ntests <- round(as.data.frame(drop1(full.wc, test = ""Chisq"")), 3) \ntests\n\n#confidence intervals full model\nsource(""/.../boot_glmm.r"") #function provided by Roger Mundry\nboot.full = boot.glmm.pred(model.res = full.wc, excl.warnings = F, nboots = 1000, para = F, resol = 1000, level = 0.95, use = NULL)\nround(boot.full$ci.estimates, 3)\nm.', 'rm(list=ls())\n\n#model 2\n#tests whether distributor affects refusal behaviour in control (equality) conditions\n#note that throughout the script below, ""counterbalance"" is used to refer to the condition presentation order (referred to as ""order"" in the main paper)\n\n#libraries\nlibrary(knitr)\nlibrary(lme4)\nlibrary(car)\n\n#import data\nsetwd(""/..."")\nxdata <- read.csv(""model2.csv"", \n                  header = TRUE, \n                  sep = "","", \n                  stringsAsFactors = TRUE)\n\n#add subject_session column\nxdata$subj.session = as.factor(paste(xdata$subject, xdata$session, sep = \'_\'))\n\n#preparatory steps  \nkable(xx <- table(xdata$sex, xdata$subject)) #checked each subj. was only assigned 1 sex\nkable(xx <- table(xdata$distributor, xdata$subject)) #checked each subj. was only assigned 1 distributor\n\n#random slopes\nsource(""/.../diagnostic_fcns.r"") #function provided by Roger Mundry\nxx.fe.re = fe.re.tab(fe.model = ""refusal ~ distributor + trial + session + sex"", re = ""(1|subject) + (1|subj.session)"", data = xdata)\nxx.fe.re$summary\n\n#replace ""xdata"" with the ""t.data"" dataframe which arose from the xx.fe.re function\n#the advantage in using t.data is that it is free of NAs and variables are already dummy-coded.\nt.data = xx.fe.re$data\n(nrow(t.data))\n\n#z.transform the two covariates:\nt.data$z.trial <- as.vector(scale(t.data$trial))\nt.data$z.session <- as.vector(scale(t.data$session))\n\n#fit the full model (notation: suffix "".wc"" = with correlations) \ncontr = glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))\nfull.wc = glmer(refusal ~ distributor + z.trial + z.session + sex + (1 + z.trial + z.session | subject) + (1 + z.trial|subj.session), data = t.data, control = contr, family = binomial)\n\n#check whether to keep or remove correlations \nsummary(full.wc)$varcor\n#decision: all correlations high, remove from the random slopes structure.  \n\n#double pipe syntax removes the correlations from the random slopes structure (suffix "".nc"" = no correlations)\nfull.nc = glmer(refusal ~ distributor + z.trial + z.session + sex + (1 + z.trial + z.session || subject) + (1 + z.trial||subj.session), data = t.data, control = contr, family = binomial)\n\n#check it was right decision to remove the correlations\nround(logLik(full.wc), digits = 3)\nround(logLik(full.nc), digits = 3)\n#decision: proceed with full.nc\n\n#check BLUPs\n#BLUPs should be normally distributed and the range on the x-axis should not be too large i.e not greater than c. 3 or -3\nranef.diagn.plot(full.nc) #look fine\n\n#check model stability\nsource(""/.../glmm_stability.r"") #function provided by Roger Mundry\nm.stab = glmm.model.stab(model.res = full.nc, contr = contr)\nm.stab.plot(m.stab$summary[,-1])\ntable(m.stab$detailed$warnings)\nround(m.stab$summary[, -1], 3)\n\n#check collinearity \n#all Variance Inflation Factors (VIFs) should be close to 1\nxx = lm(refusal ~ distributor + z.trial + z.session + \n          sex, data = t.data)\nvif(xx)\n\n#fit null model\nnull.nc = glmer(refusal ~ z.trial + z.session + sex + (1 + z.session + z.trial || subject) + (1 + z.trial||subj.session), data = t.data, control = contr, family = binomial)\n\n#compare full and null models\nanova(full.nc, null.nc, test = ""Chisq"")\n\n#full model output\nround(summary(full.nc)$coefficients, digits = 3)\n\n#more reliable P-values are derived from the code below\n#this code uses likelihood ratio tests and compares the full model with the respective reduced models.  \ntests <- round(as.data.frame(drop1(full.nc, test = ""Chisq"")), 3)\ntests\n\n#confidence intervals full model\nsource(""/.../boot_glmm.r"") #function provided by Roger Mundry\nboot.full = boot.glmm.pred(model.res = full.nc, excl.warnings = F, nboots = 1000, para = F, resol = 1000, level = 0.95, use = NULL) #run on institute computer, change para = to T\nround(boot.full$ci.estimates, 3)\nm.stab.plot(boot.full$ci.estimates)\n\n#citation information\nsessionInfo()\nR.Version()\ncitation()\n', 'rm(list=ls())\n\n#model 3 (survival analysis)\n#explores whether pull-latency differs with test condition\n#note that throughout the script below, ""counterbalance"" is used to refer to the condition presentation order (referred to as ""order"" in the main paper)\n\n#libraries\nlibrary(coxme)\n\n#import data (dataset has all timeout- and pull-latency events, 1066 events in total)\nsetwd(""/..."") \nxdata <- read.csv(""model3.csv"", header = TRUE, sep = "","", stringsAsFactors = TRUE)\n\n#add subject_session column\nxdata$subj.session = as.factor(paste(xdata$subject, xdata$session, sep = \'_\'))\n\n#add rv variable to data set\nxdata$rv = Surv(time = xdata$pull_latency, event = xdata$behavior == \'S_latency_to_pull\')\n\n#random slopes\nsource(""/.../diagnostic_fcns.r"") #function provided by Roger Mundry\nxx.fe.re = fe.re.tab(fe.model = ""rv~distributor*partner_presence + trial + session + sex + counterbalance"", re = \'(1|subject) + (1|subj.session)\', data=xdata)\nnrow(xx.fe.re$data)\nnrow(xdata)\n(xx.fe.re$summary)\n\n#z.transform the covariates\nxdata$z.trial <- as.vector(scale(xdata$trial))\nxdata$z.session <- as.vector(scale(xdata$session))\n\n#dummy-code\nxdata$partner_presence.code = as.numeric(xdata$partner_presence == levels(xdata$partner_presence)[2])\nxdata$partner_presence.code = xdata$partner_presence.code - mean(xdata$partner_presence.code)\n\n#fit the full model\nfull = coxme(rv~distributor*partner_presence + z.trial + z.session + sex + counterbalance + \n               (1 |subject) + (0 + partner_presence.code |subject)+(0 + z.trial |subject)+(0 + z.session |subject)+\n               (1 |subj.session)+(0 + z.trial |subj.session), \n             data=xdata)\nsummary(full)\n\n#check model stability\nsource(""/.../coxme_stab.r"") #function provided by Roger Mundry\nf=""rv~distributor*partner_presence + z.trial + z.session + sex + counterbalance + (1 |subject) + (0 + partner_presence.code |subject)+(0 + z.trial |subject)+(0 + z.session |subject)+(1 |subj.session)+(0 + z.trial |subj.session)""\nfull.stab = coxme.stab(mres=full, data=xdata, formula=f, para=T, n.cores=c(""all-1"", ""all""), contr=NULL)\nwarnings()\nround(full.stab$summary, digits = 3)\n\n#fit null model\nnull = coxme(rv~ z.trial + z.session + sex + counterbalance + \n               (1 |subject) + (0 + partner_presence.code |subject)+(0 + z.trial |subject)+(0 + z.session |subject)+\n               (1 |subj.session)+(0 + z.trial |subj.session), \n             data=xdata)\nsummary(null)\n\n#compare full and null models\nas.data.frame(anova(null, full, test = ""Chisq""))\n\n#coefficients full\nsource(""/.../coef_from_coxme.r"") #function provided by Roger Mundry\nround(coef.from.coxme(m=full), digits = 3)\n\n#p-values full\nsource(""/.../drop1_para_coxme.r"") #function provided by Roger Mundry\ntests.full=drop1p(model.res=full, para=F, data=xdata)\nround(tests.full, 3)\n\n#fit reduced model\nred = coxme(rv~distributor + partner_presence + z.trial + z.session + sex + counterbalance + \n               (1 |subject) + (0 + partner_presence.code |subject)+(0 + z.trial |subject)+(0 + z.session |subject)+\n               (1 |subj.session)+(0 + z.trial |subj.session), \n             data=xdata)\n\n#coefficients reduced model\nround(coef.from.coxme(m=red),3)\n\n#p-values reduced model\ntests.red=drop1p(model.res=red, para=F, data=xdata)\nround(tests.red, 3)\n\n#citation information\nsessionInfo()\nR.Version()\ncitation()\n', 'rm(list=ls())\n\n---\ntitle: ""Subsetting the raw data to create subsets needed for models 1A, 1B, 2 and 3""\nauthor: ""Rowan Titchener""\ndate: ""2021""\noutput: html_document\n---\n\n#set working directory\n#setwd(""/..."")  \n#setwd(""/Users/rowantitchener/Documents/00_PhD/Study1_social_disappointment/15_Submissions/10_Royal Society Open Science (acceptance)/04_Upload-to-figshare/0_raw-data"")\n\n#""Pre-R"" data handling:\n\n#1. The videos were coded using the Mangold Interact software. Mangold Interact creates a .act file. This .act file was exported to Excel (.xlsx)\n#Prior to export from Mangold Interact RT manually checked that all \'y\' entries in the discount_trial column were justified as per the following 5 criteria:  \n#S_platform_travels_empty  \n#S_platform_travels_with_more_than_1_item  \n#P_platform_travels_empty  \n#P_platform_travels_with_more_than_1_item  \n#S_accidentally_drops_food  \n#For detail on discounts refer to section 3 of the electronic supplementary materials  \n\n#2. Within Excel spreadsheet\n#Dates were printed in ISO 8601 format (yyyy-mm--dd)  \n\n#All subsequent data-handling took place in R  \n\n#Libraries\n# library(knitr)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(forcats)\n\n#Import data\nxdata <- read_csv(""2021-06-02_social-disappointment_data.csv"", col_names = TRUE)\n\n#Tidy\n\n#remove the rows comprised solely of NAs (these rows are an artefact of the Mangold Interact export process)\nxdata <- xdata %>% drop_na(date)\n\n#rename the Duration_Time column\nxdata <- xdata %>% rename(pull_latency = Duration_Time)\n\n#add a column which documents the proportion of refusals per session \nxdata <- mutate(xdata, proportion = xdata$number_refusals_in_session / xdata$number_trials_in_session)\n\n#re-classify columns as factors\nlevels_subject <- c(""lukas"", ""mara"", ""mars"", ""max"", ""meiwi"", ""mila"", ""mona"", ""moritz"", ""sambia"", ""sherry"", ""simon"", ""smilla"") \nxdata$subject <-factor(xdata$subject, levels = levels_subject)\n\nlevels_sex <- c(""female"", ""male"")\nxdata$sex <- factor(xdata$sex, levels = levels_sex )  \n\nlevels_condition <- c(""inequality"", ""equality"")\nxdata$condition <- factor(xdata$condition, levels = levels_condition )\nxdata$condition <- fct_recode(xdata$condition, \n                              ""test"" = ""inequality"", \n                              ""control"" = ""equality"")\n\nlevels_distributor <- c(""human"", ""machine"") \nxdata$distributor <- factor(xdata$distributor, levels = levels_distributor )  \n\nlevels_partner_presence <- c(""n"", ""y"") \nxdata$partner_presence <- factor(xdata$partner_presence, levels = levels_partner_presence )  \n\nlevels_partner_identity <- c(""ghost"", ""ilja"", ""linus"") \nxdata$partner_identity <- factor(xdata$partner_identity, levels = levels_partner_identity )\n\nlevels_counterbalance <- c(""partner"", ""no_partner"") \nxdata$counterbalance <- factor(xdata$counterbalance, levels = levels_counterbalance )\n\nlevels_refusal <- c(""1"", ""0"")\nxdata$refusal <- factor(xdata$refusal, levels = levels_refusal )\n\nlevels_discount_trial <- c(""n"", ""y"")\nxdata$discount_trial <-factor(xdata$discount_trial, levels = levels_discount_trial)\n\n#Create data subsets for each model (Models 1A, 1B, 2 & 3)\n\n#Subset 1 - saved as a .csv file titled model1a.csv\n#Subset 1 contains:\n#all test condition trials\n#all non-discounted trials\n#all food refusal/food acceptance behaviours\n\nmodel1a_subset <-\n  filter(xdata, condition == ""test"",\n         discount_trial == ""n"",\n         behavior == ""S_eats_food"" |\n           behavior == ""S_refuses_to_consume_food"" |\n           behavior == ""S_refuses_to_participate"" |\n           behavior == ""S_refuses_to_take_food"") %>%\n  select(date, condition, refusal, distributor, partner_presence, session, trial,\n         sex, counterbalance, subject)\n\nlength(model1a_subset$trial) #1066\n\nwrite_excel_csv(model1a_subset, ""model1a.csv"")\n\n#Subset 2 - saved as a .csv file titled model1B.csv   \n#Subset 2 was identical to Subset 1 except it only contains the trials that were carried out while ilja was in the partner role.\n\nmodel1b_subset <- filter(xdata, condition == ""test"") %>%\n  filter(discount_trial == ""n"") %>%\n  filter(behavior == ""S_eats_food"" | \n           behavior == ""S_refuses_to_consume_food"" |\n           behavior == ""S_refuses_to_participate"" | \n           behavior == ""S_refuses_to_take_food"") %>%\n  filter(subject == ""mars"" |\n           subject == ""moritz"" & \n           session %in% c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"") |\n           subject == ""sherry"" & session %in% c(""1"", ""2"", ""3"", ""4"") |\n           subject == ""mona"" & session %in% c(""1"", ""2"", ""3"") |\n           subject == ""mila"" & session %in% c(""1"", ""2"", ""3"", ""4"") |\n           subject == ""sambia"" |\n           subject == ""lukas"" & session %in% c(""1"", ""2"", ""3"", ""4"")|\n           subject == ""meiwi"" & session %in% c(""1"", ""2"", ""3"", ""4"", ""5"") |\n           subject == ""mara"" & session %in% c(""1"", ""2"") |\n           subject == ""smilla"" & session %in% c(""1"", ""2"", ""3"", ""4"") |\n           subject == ""max"" & session %in% c(""1"", ""2"") |\n           subject == ""simon"" &session %in% c(""1"",']",2,"social disappointment, partner presence, long-tailed macaque, refusal behavior, inequity aversion experiment, protest, unequal reward distribution, human cooperation, non-human animals, frustration behavior, Macaca fascicularis, novel paradigm, lever pulling,"
"Past, present and future of chamois science","The chamois Rupicapra spp. is the most abundant mountain ungulate of Europe and the Near East, where it occurs as two species, the Northern chamois R. rupicapra and the Southern chamois R. pyrenaica. Here, we provide a state-of-the-art overview of research trends and the most challenging issues in chamois research and conservation, focusing on taxonomy and systematics, genetics, life history, ecology and behavior, physiology and disease, management, and conservation. Research on Rupicapra has a longstanding history and has contributed substantially to the biological and ecological knowledge of mountain ungulates. Although the number of publications on this genus has markedly increased over the past two decades, major differences persist with respect to knowledge of species and subspecies, with research mostly focusing on the Alpine chamois R. r. rupicapra and, to a lesser extent, the Pyrenean chamois R. p. pyrenaica. In addition, a scarcity of replicate studies of populations of different subspecies and/or geographic areas limits the advancement of chamois science. Since environmental heterogeneity impacts behavioral, physiological and life history traits, understanding the underlying processes would be of great value from both an evolutionary and conservation/management standpoint, especially in the light of ongoing climatic change. Substantial contributions to this challenge may derive from a quantitative assessment of reproductive success, investigation of fine-scale foraging patterns, and a mechanistic understanding of disease outbreak and resilience. Improving conservation status, resolving taxonomic disputes, identifying subspecies hybridization, assessing the impact of hunting and establishing reliable methods of abundance estimation are of primary concern. Despite being one of the most well-known mountain ungulates, substantial field efforts to collect paleontological, behavioral, ecological, morphological, physiological and genetic data on different populations and subspecies are still needed to ensure a successful future for chamois conservation and research.","['###############################################################################\n# ANALYSIS OF CHAMOIS SCIENCE\n# .R code for data analysis                                                   \n###############################################################################\n\n\ntrend_chamois <- read.csv2(file.choose())\ntrend_chamois\n\nlibrary(MuMIn)\nlibrary(DHARMa)\nlibrary(performance)\nlibrary(visreg)\n\nmod1 <- glm(Rupicapra ~ YEAR + Rupicapra_1, data=trend_chamois, family=poisson)\nmod2 <- glm(Rupicapra ~ poly(YEAR,2) + Rupicapra_1, data=trend_chamois, family=poisson)\nmod3 <- glm(Rupicapra ~ poly(YEAR,3) + Rupicapra_1, data=trend_chamois, family=poisson)\nmod4 <- glm(Rupicapra ~ poly(YEAR,4) + Rupicapra_1, data=trend_chamois, family=poisson)\nmod5 <- glm(Rupicapra ~ poly(YEAR,5) + Rupicapra_1, data=trend_chamois, family=poisson)\nmod6 <- glm(Rupicapra ~ poly(YEAR,6) + Rupicapra_1, data=trend_chamois, family=poisson)\nmodel.sel(mod2, mod3, mod4, mod5, mod6)\n\ncheck_autocorrelation(mod3) #OK!\n\nsim.mod3 <- simulateResiduals(mod3)\nplot(sim.mod3)\nsummary(mod3)\n\nvisreg::visreg(mod3, xvar=""YEAR"", scale=""response"")\n\npar(mfrow = c(1,1), mar=c(4.5,5,1,2), oma = c(0, 0, 0, 0))\n\nvisreg(mod3, xvar=""YEAR"", scale=""response"", # export 6x8 inches\n       rug=FALSE,\n       ylim = c(0,50),\n       overlay = F, \n       xlab=""Year"", \n       ylab=""Number of chamois publications"",\n       fill=list(col=grey(c(0.7), alpha=0.4)),\n       line=list(lty=1:3, col = ""black"", lwd = 1.5),\n       points=list(cex=1, pch=16, col = ""black""), # partial residuals\n       partial = FALSE,\n       cex.lab = 1.25)\nwith(trend_chamois, points(YEAR, Rupicapra, pch = 16, col = ""black"")) # real data\ntext(1990, 50, ""n = 160"", cex = 1, font = 2)\ntext(2010, 50, ""n = 596"", cex = 1, font = 2)\nabline(v=2000, col = ""black"", lty=2)\n\n\n# proportion of Caprinae papers\n\nmod0b <- lm(Prop ~ 1, data=trend_chamois)\nmod1b <- lm(Prop ~ YEAR, data=trend_chamois)\nmod2b <- lm(Prop ~ poly(YEAR,2), data=trend_chamois)\nmod3b <- lm(Prop ~ poly(YEAR,3), data=trend_chamois)\nmod4b <- lm(Prop ~ poly(YEAR,4), data=trend_chamois)\nmod5b <- lm(Prop ~ poly(YEAR,5), data=trend_chamois)\nmod6b <- lm(Prop ~ poly(YEAR,6), data=trend_chamois)\nmodel.sel(mod0b, mod1b, mod2b, mod3b, mod4b, mod5b, mod6b)\n\ncheck_autocorrelation(mod1b) #OK!\ncheck_heteroscedasticity(mod1b) #OK!\n\nsim.mod1b <- simulateResiduals(mod1b)\nplot(sim.mod1b)\nsummary(mod1b)\n\nvisreg::visreg(mod1b, scale=""response"")\n\npar(mfrow = c(1,1), mar=c(4.5,5,1,2), oma = c(0, 0, 0, 0))\n\nvisreg(mod1b, xvar=""YEAR"", scale=""response"", # export 6x8 inches\n       rug=FALSE,\n       ylim = c(0.00,0.30),\n       overlay = F, \n       xlab=""Year"", \n       ylab=""Proportion of chamois publications"",\n       fill=list(col=grey(c(0.7), alpha=0.4)),\n       line=list(lty=1:3, col = ""black"", lwd = 1.5),\n       points=list(cex=1, pch=16, col = ""black""), # partial residuals\n       partial = FALSE,\n       cex.lab = 1.25)\nwith(trend_chamois, points(YEAR, Prop, pch = 16, col = ""black"")) # real data\ntext(1990, 0.30, ""mean = 0.15"", cex = 1, font = 2)\ntext(2010, 0.30, ""mean = 0.17"", cex = 1, font = 2)\nabline(v=2000, col = ""black"", lty=2)\n']",2,"Chamois, Rupicapra spp., Northern chamois, Southern chamois, taxonomy, systematics, genetics, life history, ecology, behavior, physiology, disease, management, conservation, Alpine chamois, Pyrenean"
Water Sentinels Motivation Survey,"The Water Sentinels motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting water pollution in the Water Sentinels pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/water-sentinels/.The Water Sentinels motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed using the Coney toolkit and administered using Google Forms.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-results-google-forms.csv contains the CSV of the collected answers exported from Google Forms*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation","['setwd(""water-sentinels"")\r\n\r\n\r\n##---------- PREPARAZIONE DATI\r\nraw.data<- read.csv(""water-sentinels-results.csv"", header=T)\r\nraw.data<-raw.data[1:4,8:28]\r\n\r\nraw.data.backup<-raw.data\r\n\r\nlibrary(stringr)\r\n# Keep only tag as header for the column\r\nnames(raw.data)<-str_replace(names(raw.data), ""\\\\.\\\\.\\\\..+"","""")\r\n\r\n# Substitute answers with their numeric value\r\nlibrary(plyr)\r\n#Stimulation.1\r\nlevels(raw.data$Stimulation.1)\r\nraw.data$Stimulation.1<- revalue(raw.data$Stimulation.1, c(""em parte""= 4, ""exactamente""= 5, ""n達o influenciou""=3) )\r\nraw.data$Stimulation.1<-as.numeric(as.character(raw.data$Stimulation.1))\r\n\r\n#Stimulation.2\r\nlevels(raw.data$Stimulation.2)\r\nraw.data$Stimulation.2<- revalue(raw.data$Stimulation.2, c(""em parte""= 4, ""exactamente""= 5) )\r\nraw.data$Stimulation.2<-as.numeric(as.character(raw.data$Stimulation.2))\r\n\r\n#Routine.1\r\nlevels(raw.data$Routine.1)\r\nraw.data$Routine.1<- revalue(raw.data$Routine.1, c(""nunca""= 1) )\r\nraw.data$Routine.1<-as.numeric(as.character(raw.data$Routine.1))\r\n\r\n#Achievement.1\r\nlevels(raw.data$Achievement.1)\r\nraw.data$Achievement.1<- revalue(raw.data$Achievement.1, c(""sim"" = 5, ""sim, um pouco""=4) )\r\nraw.data$Achievement.1<-as.numeric(as.character(raw.data$Achievement.1))\r\n\r\n#Achievement.2\r\nlevels(raw.data$Achievement.2)\r\nraw.data$Achievement.2<- revalue(raw.data$Achievement.2, c(""sim"" = 5) )\r\nraw.data$Achievement.2<-as.numeric(as.character(raw.data$Achievement.2))\r\n\r\n#Power.1\r\nlevels(raw.data$Power.1)\r\nraw.data$Power.1<- revalue(raw.data$Power.1, c(""n達o muito"" = 2,""n達o, de todo"" = 1, ""sim""= 5, ""sim, um pouco"" = 4 ) )\r\nraw.data$Power.1<-as.numeric(as.character(raw.data$Power.1))\r\n\r\n#Power.2\r\nlevels(raw.data$Power.2)\r\nraw.data$Power.2<- revalue(raw.data$Power.2, c(""nada"" = 1) )\r\nraw.data$Power.2<-as.numeric(as.character(raw.data$Power.2))\r\n\r\n#Belongingness.1\r\nlevels(raw.data$Belongingness.1)\r\nraw.data$Belongingness.1<- revalue(raw.data$Belongingness.1, c(""muito influenciada"" = 5 , ""n達o, de todo"" = 1, ""neutro"" = 3))\r\nraw.data$Belongingness.1<-as.numeric(as.character(raw.data$Belongingness.1))\r\n\r\n\r\n#Belongingness.2\r\nlevels(raw.data$Belongingness.2)\r\nraw.data$Belongingness.2<- revalue(raw.data$Belongingness.2, c(""sim"" = 5))\r\nraw.data$Belongingness.2<-as.numeric(as.character(raw.data$Belongingness.2))\r\n\r\n#Conformity.1\r\nlevels(raw.data$Conformity.1)\r\nraw.data$Conformity.1<- revalue(raw.data$Conformity.1, c(""algumas pessoas"" = 4, ""poucos participantes"" = 3))\r\nraw.data$Conformity.1<-as.numeric(as.character(raw.data$Conformity.1))\r\n\r\n#Benevolence.2\r\nlevels(raw.data$Benevolence.2)\r\nraw.data$Benevolence.2<- revalue(raw.data$Benevolence.2, c(""Sim, definitivamente"" = 5, ""Sim, principalmente por isso"" =4))\r\nraw.data$Benevolence.1<-as.numeric(as.character(raw.data$Benevolence.1))\r\n\r\n#Universalism.1\r\nlevels(raw.data$Universalism.1)\r\nraw.data$Universalism.1<- revalue(raw.data$Universalism.1, c(""Sim, definitivamente"" = 5))\r\nraw.data$Universalism.1<-as.numeric(as.character(raw.data$Universalism.1))\r\n\r\nwrite.csv(raw.data, ""water-sentinels-pivot-questions.csv"", row.names = F)\r\n\r\n\r\n#--------------------------- ANALYSIS RESULTS BY QUESTION\r\n\r\n# Compute average and variance for each question \r\nans<- read.csv(""water-sentinels-pivot-questions.csv"", header=TRUE)\r\n\r\nmean.question<-round(sapply(ans[,c(1:21)], mean),2)\r\nvar.questions<- round(sapply(ans[,c(1:21)], var), 2)\r\n\r\ndf.questions<- data.frame(mean= mean.question, var = var.questions)\r\n\r\n\r\n#--------------------------- ANALYSIS RESULTS BY MOTIVATING FACTOR\r\n\r\n# Compute average and variance for each motivating factor \r\nans<- read.csv(""water-sentinels-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n\r\nfactors.summary= data.frame(factor= character(), mean=numeric(), var=numeric())\r\n\r\nall.factors<-c(""Achievement"", ""Belongingness"", ""Benevolence"", ""Conformity"", ""Hedonism"", ""Power"", ""Routine"", ""Self.direction"", ""Stimulation"", ""Universalism"", ""Global.motivation"")\r\n\r\nfor (k in all.factors){\r\n  f<-ans %>% select(starts_with(k)) \r\n  num.col<-ncol(f)\r\n  v.final=vector()\r\n  for(i in 1:num.col){\r\n    v<-as.vector(f[,i])\r\n    v.final<-c(v.final, v)\r\n  }\r\n  \r\n  f.mean<-round(mean(v.final), 2)\r\n  f.var<-round(var(v.final),2)\r\n  new.row<-data.frame(factor= k, mean=f.mean, var=f.var)\r\n  factors.summary<-rbind(factors.summary, new.row) #aggiunta di riga al dataframe\r\n  \r\n  rm(f, f.mean, f.var, new.row, v.final, v, num.col)\r\n  \r\n}\r\n\r\n\r\n#--------------------------- CORRELATION ANALYSIS\r\n\r\ncompleted<- read.csv(""water-sentinels-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n# average questions for each tag\r\n# average results for each tag\r\n\r\n#achievement\r\nach.subset<-completed %>% select(starts_with(""ach""))\r\ncompleted$ach <- rowMeans(ach.subset, na.rm = TRUE)\r\n\r\n#belongingness\r\nbel.subset<-completed %>% select(starts_with(""bel""))\r\ncompleted$bel <- rowMeans(bel.subset, na.rm = TRUE)\r\n\r\n#benevolence\r\nben.subset<-completed %>% select(starts_with(""ben""))\r\ncompleted$ben <- rowMeans(ben.subset, na.rm = TRUE)\r\n\r\n#conformity\r\nconf.subset<-completed %>% select(starts_with(""']",2,"Water Sentinels, motivation study, H2020 project, ACTION, citizen science, volunteers, personal enjoyment, improvement, achievement, personal relationships, environment, factors, participation, engagement, community, water pollution, pilot, Coney toolkit, Google"
Data from: Interest in nonsocial novel stimuli as a function of age in rhesus monkeys,"Human cognitive and affective life changes with healthy aging; cognitive capacity declines while emotional life becomes more positive and social relationships are prioritized. This may reflect an awareness of limited lifetime unique to humans, leading to a greater interest in maintaining social relationships at the expense of the nonsocial world in the face of limited cognitive and physical resources. Alternately, fundamental biological processes common to other primate species may direct preferential interest in social stimuli with increasing age. Inspired by a recent study that described sustained interest in social stimuli but diminished interest in nonsocial stimuli in aged Barbary macaques, we carried out a conceptual replication to test whether old rhesus monkeys lost interest in nonsocial stimuli. Male and female macaques (Macaca mulatta; N=243) 4-30 years old were tested with a food puzzle outfitted with an activity monitor to evaluate their propensity to manipulate the puzzle in order to free a food reward.","['library(tidyverse)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(gridExtra)\nsetwd(""~/Dropbox/EBM07 Shaker Study"")\n\nacti_data <- read_csv(""activity_data.csv"")\nveggie_relish <- read_csv(""veggie_data.csv"")\n\nactivity8 <- acti_data %>% filter(obs<9) %>% group_by(subj_id, Day) %>% \n  summarize(total=sum(Activity), active_bins = sum(Activity > 0), age = min(age))\nactivity80 <- acti_data %>% filter(obs<81) %>% group_by(subj_id, Day) %>% \n  summarize(total=sum(Activity), active_bins = sum(Activity > 0), age = min(age))\n\nlmer(active_bins ~ age * Day + (1|subj_id), data = activity8) %>% anova()\nlmer(total ~ age * Day + (1|subj_id), data = activity8) %>% anova()\n\nlmer(active_bins ~ age * Day + (1|subj_id), data = activity80) %>% anova()\nlmer(total ~ age * Day + (1|subj_id), data = activity80) %>% anova()\n\n# First 2 minutes: how many of 8 bins have any activity (Figure 3A)\n\nday_names <- list(""1"" = ""Day 1"", ""2"" = ""Day 2"")\nday_labeller <- function(variable, value) {return(day_names[value])}\n\nlmer(active_bins ~ age * Day + (1|subj_id), data = activity8) %>% anova()\n(plot3a <- ggplot(activity8, aes(x=age, y=active_bins, color=factor(Day))) + \n  geom_jitter(width=0.1, height=0.1, size = 3, show.legend = FALSE) + \n  geom_smooth(method=""lm"", show.legend = FALSE) + \n  facet_grid(. ~ Day, labeller = day_labeller) + scale_color_manual(values=c(""navy"", ""darkgoldenrod1"")) + \n  xlab(""Age (years)"") + ylab(""Number of 15-sec bins with any activity in first 2 min"") +\n  theme_bw() + theme(strip.background=element_rect(fill=""white"")) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(text = element_text(size=20)) +\n  labs(subtitle = ""A"") + theme(plot.subtitle=element_text(size=27)) +\n  theme(plot.margin = unit(c(1,1,1,1), ""cm"")))\n\n# First 2 minutes: total activity counts across 2 minutes (Figure 3B)\n\nlmer(total ~ age * Day + (1|subj_id), data = activity8) %>% anova()\n(plot3b <- ggplot(activity8, aes(x=age, y=total, color=factor(Day))) + \n  geom_jitter(width=0.1, height=0.1, size = 3, show.legend = FALSE) + \n  geom_smooth(method=""lm"", show.legend = FALSE) + \n  facet_grid(. ~ Day, labeller = day_labeller) + scale_color_manual(values=c(""navy"", ""darkgoldenrod1"")) + \n  xlab(""Age (years)"") + ylab(""Total activity counts across 2 minutes"") +\n  theme_bw() + theme(strip.background=element_rect(fill=""white"")) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(text = element_text(size=20)) +\n  labs(subtitle = ""B"") + theme(plot.subtitle=element_text(size=27)) +\n  theme(plot.margin = unit(c(1,1,1,1), ""cm"")))\n\n# All 20 minutes: how many of 80 bins have any activity (Figure 3C)\n\nlmer(active_bins ~ age * Day + (1|subj_id), data = activity80) %>% anova()\n(plot3c <- ggplot(activity80, aes(x=age, y=active_bins, color=factor(Day))) + \n    geom_jitter(width=0.1, height=0.1, size = 3, show.legend = FALSE) + \n    geom_smooth(method=""lm"", show.legend = FALSE) + \n    facet_grid(. ~ Day, labeller = day_labeller) + scale_color_manual(values=c(""navy"", ""darkgoldenrod1"")) + \n    xlab(""Age (years)"") + ylab(""Number of 15-sec bins with any activity in entire 20 min"") +\n    theme_bw() + theme(strip.background=element_rect(fill=""white"")) +\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n    theme(text = element_text(size=20)) +\n    labs(subtitle = ""C"") + theme(plot.subtitle=element_text(size=27)) +\n    theme(plot.margin = unit(c(1,1,1,1), ""cm"")))\n\n# All 20 minutes: total activity counts across 20 minutes\n\nlmer(total ~ age * Day + (1|subj_id), data = activity80) %>% anova()\n(plot3d <- ggplot(activity80, aes(x=age, y=total, color=factor(Day))) + \n    geom_jitter(width=0.1, height=0.1, size=3, show.legend = FALSE) + \n    geom_smooth(method=""lm"", show.legend = FALSE) + \n    facet_grid(. ~ Day, labeller = day_labeller) + scale_color_manual(values=c(""navy"", ""darkgoldenrod1"")) + \n    xlab(""Age (years)"") + ylab(""Total activity counts across 20 minutes"") +\n    theme_bw() + theme(strip.background=element_rect(fill=""white"")) +\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n    theme(text = element_text(size=20)) +\n    labs(subtitle = ""D"") + theme(plot.subtitle=element_text(size=27)) +\n    theme(plot.margin = unit(c(1,1,1,1), ""cm"")))\n\npng(""Figure3.png"", width = 1600, height = 1419)\ngrid.arrange(plot3a, plot3b, plot3c, plot3d, nrow = 2)\ndev.off()\n\n# Veggie relish: how much is lost after test period\n\nlmer(loss ~ age * day + (1|subj_id), data = veggie_relish) %>% anova()\npng(""Figure4.png"", width = 800, height = 709)\nggplot(veggie_relish, aes(x=age, y=loss, color=factor(day))) + \n  geom_jitter(width=0.25, height=0.25, size=3, show.legend = FALSE) + \n  geom_smooth(method=""lm"", show.legend = FALSE) + \n  facet_grid(. ~ day, labeller = day_labeller) + scale_color_manual(values=c(""navy"", ""darkgoldenrod1"")) + \n  xlab(""Age (years)"") + ylab(""Veggie relish lost during test period (g)"") +\n  theme_bw() + theme(strip.background=element_rect(fill=""white""))', 'library(tidyverse)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(gridExtra)\nsetwd(""~/Dropbox/EBM07 Shaker Study"")\n\nacti_data <- read_csv(""activity_data.csv"")\nveggie_relish <- read_csv(""veggie_data.csv"")\n\nactivity8 <- acti_data %>% filter(obs<9) %>% group_by(subj_id, Day) %>% \n  summarize(total=sum(Activity), active_bins = sum(Activity > 0), age = min(age))\nactivity80 <- acti_data %>% filter(obs<81) %>% group_by(subj_id, Day) %>% \n  summarize(total=sum(Activity), active_bins = sum(Activity > 0), age = min(age))\n\nlmer(active_bins ~ age * Day + (1|subj_id), data = activity8) %>% anova()\nlmer(total ~ age * Day + (1|subj_id), data = activity8) %>% anova()\n\nlmer(active_bins ~ age * Day + (1|subj_id), data = activity80) %>% anova()\nlmer(total ~ age * Day + (1|subj_id), data = activity80) %>% anova()\n\n# First 2 minutes: how many of 8 bins have any activity (Figure 3A)\n\nday_names <- list(""1"" = ""Day 1"", ""2"" = ""Day 2"")\nday_labeller <- function(variable, value) {return(day_names[value])}\n\nlmer(active_bins ~ age * Day + (1|subj_id), data = activity8) %>% anova()\n(plot3a <- ggplot(activity8, aes(x=age, y=active_bins, color=factor(Day))) + \n  geom_jitter(width=0.1, height=0.1, size = 3, show.legend = FALSE) + \n  geom_smooth(method=""lm"", show.legend = FALSE) + \n  facet_grid(. ~ Day, labeller = day_labeller) + scale_color_manual(values=c(""navy"", ""darkgoldenrod1"")) + \n  xlab(""Age (years)"") + ylab(""Number of 15-sec bins with any activity in first 2 min"") +\n  theme_bw() + theme(strip.background=element_rect(fill=""white"")) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(text = element_text(size=20)) +\n  labs(subtitle = ""A"") + theme(plot.subtitle=element_text(size=27)) +\n  theme(plot.margin = unit(c(1,1,1,1), ""cm"")))\n\n# First 2 minutes: total activity counts across 2 minutes (Figure 3B)\n\nlmer(total ~ age * Day + (1|subj_id), data = activity8) %>% anova()\n(plot3b <- ggplot(activity8, aes(x=age, y=total, color=factor(Day))) + \n  geom_jitter(width=0.1, height=0.1, size = 3, show.legend = FALSE) + \n  geom_smooth(method=""lm"", show.legend = FALSE) + \n  facet_grid(. ~ Day, labeller = day_labeller) + scale_color_manual(values=c(""navy"", ""darkgoldenrod1"")) + \n  xlab(""Age (years)"") + ylab(""Total activity counts across 2 minutes"") +\n  theme_bw() + theme(strip.background=element_rect(fill=""white"")) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(text = element_text(size=20)) +\n  labs(subtitle = ""B"") + theme(plot.subtitle=element_text(size=27)) +\n  theme(plot.margin = unit(c(1,1,1,1), ""cm"")))\n\n# All 20 minutes: how many of 80 bins have any activity (Figure 3C)\n\nlmer(active_bins ~ age * Day + (1|subj_id), data = activity80) %>% anova()\n(plot3c <- ggplot(activity80, aes(x=age, y=active_bins, color=factor(Day))) + \n    geom_jitter(width=0.1, height=0.1, size = 3, show.legend = FALSE) + \n    geom_smooth(method=""lm"", show.legend = FALSE) + \n    facet_grid(. ~ Day, labeller = day_labeller) + scale_color_manual(values=c(""navy"", ""darkgoldenrod1"")) + \n    xlab(""Age (years)"") + ylab(""Number of 15-sec bins with any activity in entire 20 min"") +\n    theme_bw() + theme(strip.background=element_rect(fill=""white"")) +\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n    theme(text = element_text(size=20)) +\n    labs(subtitle = ""C"") + theme(plot.subtitle=element_text(size=27)) +\n    theme(plot.margin = unit(c(1,1,1,1), ""cm"")))\n\n# All 20 minutes: total activity counts across 20 minutes\n\nlmer(total ~ age * Day + (1|subj_id), data = activity80) %>% anova()\n(plot3d <- ggplot(activity80, aes(x=age, y=total, color=factor(Day))) + \n    geom_jitter(width=0.1, height=0.1, size=3, show.legend = FALSE) + \n    geom_smooth(method=""lm"", show.legend = FALSE) + \n    facet_grid(. ~ Day, labeller = day_labeller) + scale_color_manual(values=c(""navy"", ""darkgoldenrod1"")) + \n    xlab(""Age (years)"") + ylab(""Total activity counts across 20 minutes"") +\n    theme_bw() + theme(strip.background=element_rect(fill=""white"")) +\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n    theme(text = element_text(size=20)) +\n    labs(subtitle = ""D"") + theme(plot.subtitle=element_text(size=27)) +\n    theme(plot.margin = unit(c(1,1,1,1), ""cm"")))\n\npng(""Figure3.png"", width = 1600, height = 1419)\ngrid.arrange(plot3a, plot3b, plot3c, plot3d, nrow = 2)\ndev.off()\n\n# Veggie relish: how much is lost after test period\n\nlmer(loss ~ age * day + (1|subj_id), data = veggie_relish) %>% anova()\npng(""Figure4.png"", width = 800, height = 709)\nggplot(veggie_relish, aes(x=age, y=loss, color=factor(day))) + \n  geom_jitter(width=0.25, height=0.25, size=3, show.legend = FALSE) + \n  geom_smooth(method=""lm"", show.legend = FALSE) + \n  facet_grid(. ~ day, labeller = day_labeller) + scale_color_manual(values=c(""navy"", ""darkgoldenrod1"")) + \n  xlab(""Age (years)"") + ylab(""Veggie relish lost during test period (g)"") +\n  theme_bw() + theme(strip.background=element_rect(fill=""white""))']",2,"age, rhesus monkeys, cognitive decline, emotional life, social relationships, biological processes, primate species, social stimuli, nonsocial stimuli, interest, aged Barbary macaques, conceptual replication, male, female, Macaca mulatta"
"No, you go first: phenotype and social context affect house sparrow neophobia","Novel object trials are commonly used to assess aversion to novelty (neophobia), and previous work has shown neophobia can be influenced by the social environment, but whether the altered behaviour persists afterwards (social learning) is largely unknown in wild animals. We assessed house sparrow (Passer domesticus) novel object responses before, during, and after being paired with a conspecific of either similar or different behavioral phenotype. During paired trials, animals housed with a similar or more neophobic partner demonstrated an increased aversion to novel objects. This change did not persist a week after unpairing, but neophobia decreased after unpairing in birds previously housed with a less neophobic partner. We also compared novel object responses to non-object control trials to validate our experimental procedure. Our results provide evidence of social learning in a highly successful invasive species, and an interesting asymmetry in the effects of social environment on neophobia behavior depending on the animal's initial behavioral phenotype.","['# Supplementary Material for:\r\n# ""No, you go first: effects of phenotype and social context on neophobia in the house sparrow""\r\n# Kelly, T. R.(1), Kimball, M. G.(1), Stansberry, K. R.(1), and Lattin, C. R.(1)\r\n# 1 - Biological Sciences Department, Louisiana State University, Baton Rouge, Louisiana, USA.\r\n# Author for correspondence: christinelattin@lsu.edu\r\n\r\n# Cox Proportional Hazard Models & Kaplan-Meir Survival Curves\r\n# using coxme and survminer packages\r\n# Started: March 04, 2020\r\n# Finalized: June 15, 2020\r\n\r\n# THIS SCRIPT IS TO INVESTIGATE EFFECT OF OBJECTS\r\n## all subjects included, object and control trials\r\n\r\n# notes on data:\r\n## excludes the escapee (HOSP 46) and cage mate (HOSP 70; Trial 11)\r\n## missing Trial 11 for HOSP 47 & 68 (camera failed)\r\n\r\n# resources for packages:\r\n## coxme command is from the \'coxme\' package - can handle random effects: https://cran.r-project.org/web/packages/coxme/coxme.pdf\r\n### interpretation of cox regression model coefficients:\r\n### http://www.sthda.com/english/wiki/cox-proportional-hazards-model (~ 1/2 way down the page)\r\n\r\n#packages\r\ninstall.packages(c(""survival"",""survminer""))\r\nlibrary(""survival"") #not used in manuscript\r\nlibrary(""survminer"") \r\nlibrary(""coxme"")\r\n\r\n#Reset R\'s brain\r\nrm(list=ls())\r\n#getwd tells you where R is currently looking\r\ngetwd()\r\n#setwd tells R where to look\r\nsetwd(""C:/Users/Tosh/Box Sync/Neophobia - 2020 - Cage Mate/R Survival Code & Data"")\r\n#use getwd to confirm that R is now looking there\r\ngetwd()\r\n#name the datafile\r\ntosh<-read.csv(""Approach times_all phenotypes all weeks.csv"")\r\nhead(tosh)\r\nstr(tosh)\r\n#make appropriate data types (must all be numeric for survival plot)\r\ntosh$ID<-factor(tosh$ID)\r\ntosh$OBJECT<-factor(tosh$OBJECT)\r\ntosh$TIME<-as.numeric(tosh$TIME)\r\ntosh$WEEK<-as.numeric(tosh$WEEK)\r\ntosh$STATUS<-as.numeric(tosh$STATUS)\r\nstr(tosh)\r\n\r\n##################\r\n# OBJECT EFFECTS #\r\n##################\r\n##controlling for individual effects\r\ncox.lessneopair.ID<-coxme(Surv(TIME,STATUS)~OBJECT+(1|ID),data=tosh)\r\ncox.lessneopair.ID\r\n# object plot\r\nfit.object<-survfit(Surv(TIME,STATUS)~OBJECT,data=tosh)\r\nggsurvplot(fit.object,\r\n           data=tosh,\r\n           size=1, #change line size\r\n           #palette = c(""#E7B800"",""#2E9FDF"",""#9F18C4""), #custom colours via https://www.google.com/search?q=color+picker\r\n           #conf.int = TRUE, #add confidence interval\r\n           #conf.int.style = ""ribbon"", #""ribbon or ""step""\r\n           #conf.int.alpha = 0.3, #0 = transparent & 1 = not transparent\r\n           pval=FALSE, #add p-value\r\n           pval.coord=c(2700,0.9), #change position of p-value\r\n           risk.table = TRUE, #add risk table\r\n           risk.table.col = ""strata"", #risk table colour by groups\r\n           legend.labs = c(""Control"",""egg"",""cover"",""red dish"",""pipe cleaners"",""light"",""bells"",""foil hood"",""puffs"",""umbrella""), #change legend labels\r\n           xlab = ""Time (seconds)"", #change x-axis label\r\n           ylab = ""Proportion yet to feed"", #change y-axis label\r\n           break.time.by = 300,\r\n           risk.table.height = 0.4, #useful to change when you have multiple groups\r\n           risk.table.title = ""Number yet to feed"",\r\n           ggtheme = theme_bw() #change ggplot theme\r\n)\r\n', '# Supplementary Material for:\r\n# ""No, you go first: effects of phenotype and social context on neophobia in the house sparrow""\r\n# Kelly, T. R.(1), Kimball, M. G.(1), Stansberry, K. R.(1), and Lattin, C. R.(1)\r\n# 1 - Biological Sciences Department, Louisiana State University, Baton Rouge, Louisiana, USA.\r\n# Author for correspondence: christinelattin@lsu.edu\r\n\r\n# Cox Proportional Hazard Models & Kaplan-Meir Survival Curves\r\n# using coxme and survminer packages\r\n# Started: March 04, 2020\r\n# Finalized: June 15, 2020\r\n\r\n# for help on plots: https://cran.r-project.org/web/packages/survminer/readme/README.html\r\n\r\n# THIS SCRIPT IS FOR CONTROL TRIALS (NO OBJECT PRESENTED)\r\n\r\n# notes on data:\r\n## excludes the escapee (HOSP 46) and cage mate (HOSP 70; Trial 10) -- controls\r\n## missing Trial 10 for HOSP 47 & 68 (camera failed) -- mixed pair\r\n\r\n# notes on script organization:\r\n## Want to see the effect of pairing, so spreadsheets are sorted by pairing\r\n## pairing = control, more neophobic individual, less neophobic individual \r\n\r\n# resources for packages:\r\n## coxph command [not used in MS, here for reference] is from the \'survival package\' - cannot handle random effects: https://cran.r-project.org/web/packages/survival/survival.pdf\r\n### requires most data types to be numeric and does not break down levels of a factor (need to use pairwise_survdiff command)\r\n## survminer package required to make plots (survfit command)\r\n## coxme command is from the \'coxme\' package - can handle random effects: https://cran.r-project.org/web/packages/coxme/coxme.pdf\r\n### interpretation of cox regression model coefficients:\r\n### http://www.sthda.com/english/wiki/cox-proportional-hazards-model (~ 1/2 way down the page)\r\n\r\n\r\n################################################\r\n### CONTROLS = EFFECT OF HAVING A CAGE MATE ####\r\n################################################\r\n#Reset R\'s brain\r\nrm(list=ls())\r\n#packages\r\nlibrary(coxme)\r\nlibrary(survival) #not used in manuscript\r\nlibrary(survminer)\r\n#setwd tells R where to look\r\nsetwd(""C:/Users/Tosh/Box Sync/Neophobia - 2020 - Cage Mate/R Survival Code & Data"")\r\n#name the datafile\r\ntosh<-read.csv(""Approach times_controls all weeks_control trials only.csv"") #control pairing, control trials\r\nhead(tosh)\r\nstr(tosh)\r\n#make appropriate data types \r\ntosh$ID<-factor(tosh$ID)\r\ntosh$TIME<-as.numeric(tosh$TIME)\r\ntosh$WEEK<-factor(tosh$WEEK)\r\ntosh$STATUS<-as.numeric(tosh$STATUS)\r\ntosh$TRIAL<-as.numeric(tosh$TRIAL)\r\nstr(tosh) #confirm data types\r\n# AN EFFECT OF CAGE MATE would be noted by differences between weeks\r\n##controlling for individual effects using coxme package\r\ncox.control.ID<-coxme(Surv(TIME,STATUS)~WEEK+(1|ID),data=tosh)\r\ncox.control.ID\r\n# plot control pairing control trials by week\r\nfit.control.week<-survfit(Surv(TIME,STATUS)~WEEK,data=tosh)\r\ncontrol.plot<-ggsurvplot(fit.control.week,\r\n           data=tosh,\r\n           title = ""Control pairing & control trials only (no object)"",\r\n           size=1, #change line size\r\n           palette = c(""#0A85FF"",""#FF7300"",""#2724FF""), #custom colour\r\n           conf.int = TRUE, #add confidence interval\r\n           conf.int.style = ""ribbon"", #""ribbon or ""step""\r\n           conf.int.alpha = 0.3, #0 = transparent & 1 = not transparent\r\n           pval=FALSE, #add p-value\r\n           pval.coord=c(2700,0.9), #change position of p-value\r\n           risk.table =  TRUE,#add risk table\r\n           risk.table.col = ""strata"", #risk table colour by groups\r\n           legend.labs = c(""Week 1: Solo"", ""Week 3: Paired"", ""Week 5: Unpaired""), #change legend labels\r\n           xlab = ""Time (seconds)"", #change x-axis label\r\n           ylab = ""Proportion yet to feed"", #change y-axis label\r\n           break.time.by = 300,\r\n           risk.table.height = 0.3, #useful to change when you have multiple groups\r\n           risk.table.title = ""Number yet to feed"",\r\n           ggtheme = theme_bw() #change ggplot theme\r\n) ##nice to see how week 3 falls back to week 1\r\ncontrol.plot\r\n\r\n\r\n#############################\r\n### MORE NEOPHOBIC BIRDS #### (paired with less neophobic; partner quicker to appraoch)\r\n#############################\r\n#setwd tells R where to look\r\nsetwd(""C:/Users/Tosh/Box Sync/Neophobia - 2020 - Cage Mate/R Survival Code & Data"")\r\n#name the datafile\r\ntosh<-read.csv(""Approach times_more neophobic all weeks_control trials only.csv"") #more neophobic, control trials\r\nhead(tosh)\r\nstr(tosh)\r\n#make appropriate data types \r\ntosh$ID<-factor(tosh$ID)\r\ntosh$TIME<-as.numeric(tosh$TIME)\r\ntosh$WEEK<-factor(tosh$WEEK)\r\ntosh$STATUS<-as.numeric(tosh$STATUS)\r\ntosh$TRIAL<-as.numeric(tosh$TRIAL)\r\nstr(tosh)\r\n# AN EFFECT OF CAGE MATE would be noted by differences between weeks \r\n##controlling for individual effects\r\ncox.lessneopair.ID<-coxme(Surv(TIME,STATUS)~WEEK+(1|ID),data=tosh)\r\ncox.lessneopair.ID\r\n# how does this look?\r\nfit.lessneopair.week<-survfit(Surv(TIME,STATUS)~WEEK,data=tosh)\r\nlessneo.plot<-ggsurvplot(fit.lessneopair.week,\r\n           data=tosh,\r\n           title=""With less neophobic par', '# Supplementary Material for:\r\n# ""No, you go first: effects of phenotype and social context on neophobia in the house sparrow""\r\n# Kelly, T. R.(1), Kimball, M. G.(1), Stansberry, K. R.(1), and Lattin, C. R.(1)\r\n# 1 - Biological Sciences Department, Louisiana State University, Baton Rouge, Louisiana, USA.\r\n# Author for correspondence: christinelattin@lsu.edu\r\n\r\n# Cox Proportional Hazard Models & Kaplan-Meir Survival Curves\r\n# using coxme and survminer packages\r\n# Started: March 04, 2020\r\n# Finalized: June 15, 2020\r\n\r\n# for help on plots: https://cran.r-project.org/web/packages/survminer/readme/README.html\r\n\r\n# THIS SCRIPT IS FOR OBJECT TRIALS ONLY\r\n\r\n# notes on data:\r\n## excludes the escapee (HOSP 46) and cage mate (HOSP 70; Trial 10) -- controls\r\n## missing Trial 10 for HOSP 47 & 68 (camera failed) -- mixed pair\r\n\r\n# notes on script organization:\r\n## Want to see the effect of pairing, so spreadsheets are sorted by pairing\r\n## pairing = control, more neophobic individual, less neophobic individual \r\n\r\n# resources for packages:\r\n## coxph command [not used in MS] is from the \'survival package\' - cannot handle random effects: https://cran.r-project.org/web/packages/survival/survival.pdf\r\n### requires most data types to be numeric and does not break down levels of a factor (need to use pairwise_survdiff command)\r\n## survminer package required to make plots (survfit command)\r\n## coxme command is from the \'coxme\' package - can handle random effects: https://cran.r-project.org/web/packages/coxme/coxme.pdf\r\n### interpretation of cox regression model coefficients:\r\n### http://www.sthda.com/english/wiki/cox-proportional-hazards-model (~ 1/2 way down the page)\r\n\r\n\r\n################################################\r\n### CONTROLS = EFFECT OF HAVING A CAGE MATE ####\r\n################################################\r\n#Reset R\'s brain\r\nrm(list=ls())\r\n#packages\r\nlibrary(coxme)\r\nlibrary(survival) #not used in manuscript\r\nlibrary(survminer)\r\n#setwd tells R where to look\r\nsetwd(""C:/Users/Tosh/Box Sync/Neophobia - 2020 - Cage Mate/R Survival Code & Data"")\r\n#name the datafile\r\ntosh<-read.csv(""Approach times_controls all weeks_no control trials.csv"") #control pairing, object trials\r\nhead(tosh)\r\nstr(tosh)\r\n#make appropriate data types \r\ntosh$ID<-factor(tosh$ID)\r\ntosh$OBJECT<-factor(tosh$OBJECT)\r\ntosh$TIME<-as.numeric(tosh$TIME)\r\ntosh$WEEK<-factor(tosh$WEEK)\r\ntosh$STATUS<-as.numeric(tosh$STATUS)\r\ntosh$TRIAL<-as.numeric(tosh$TRIAL)\r\nstr(tosh)\r\n# AN EFFECT OF CAGE MATE would be noted by differences between weeks\r\n##controlling for individual effects\r\ncox.control.ID<-coxme(Surv(TIME,STATUS)~WEEK+OBJECT+(1|ID),data=tosh)\r\ncox.control.ID\r\n# how does this look?\r\nfit.control.week<-survfit(Surv(TIME,STATUS)~WEEK,data=tosh)\r\ncontrol.plot<-ggsurvplot(fit.control.week,\r\n           data=tosh,\r\n           title = ""Control pairing"",\r\n           size=1, #change line size\r\n           palette = c(""#0A85FF"",""#FF7300"",""#2724FF""), #custom colour\r\n           conf.int = TRUE, #add confidence interval\r\n           conf.int.style = ""ribbon"", #""ribbon or ""step""\r\n           conf.int.alpha = 0.3, #0 = transparent & 1 = not transparent\r\n           pval=FALSE, #add p-value\r\n           pval.coord=c(2700,0.9), #change position of p-value\r\n           risk.table =  TRUE,#add risk table\r\n           risk.table.col = ""strata"", #risk table colour by groups\r\n           legend.labs = c(""Week 1: Solo"", ""Week 3: Paired"", ""Week 5: Unpaired""), #change legend labels\r\n           xlab = ""Time (seconds)"", #change x-axis label\r\n           ylab = ""Proportion yet to feed"", #change y-axis label\r\n           break.time.by = 300,\r\n           risk.table.height = 0.3, #useful to change when you have multiple groups\r\n           risk.table.title = ""Number yet to feed"",\r\n           ggtheme = theme_bw() #change ggplot theme\r\n) \r\ncontrol.plot\r\n\r\n\r\n#############################\r\n### MORE NEOPHOBIC BIRDS #### (paired with less neophobic; partner quicker to approach)\r\n#############################\r\n#setwd tells R where to look\r\nsetwd(""C:/Users/Tosh/Box Sync/Neophobia - 2020 - Cage Mate/R Survival Code & Data"")\r\n#name the datafile\r\ntosh<-read.csv(""Approach times_more neophobic all weeks_no control trials.csv"") #more neophobic, object trials\r\nhead(tosh)\r\nstr(tosh)\r\n#make appropriate data types \r\ntosh$ID<-factor(tosh$ID)\r\ntosh$OBJECT<-factor(tosh$OBJECT)\r\ntosh$TIME<-as.numeric(tosh$TIME)\r\ntosh$WEEK<-factor(tosh$WEEK)\r\ntosh$STATUS<-as.numeric(tosh$STATUS)\r\ntosh$TRIAL<-as.numeric(tosh$TRIAL)\r\nstr(tosh)\r\n# AN EFFECT OF CAGE MATE would be noted by differences between weeks \r\n##controlling for individual effects\r\ncox.lessneopair.ID<-coxme(Surv(TIME,STATUS)~WEEK+OBJECT+(1|ID),data=tosh)\r\ncox.lessneopair.ID\r\n# how does this look?\r\nfit.lessneopair.week<-survfit(Surv(TIME,STATUS)~WEEK,data=tosh)\r\nlessneo.plot<-ggsurvplot(fit.lessneopair.week,\r\n           data=tosh,\r\n           title=""With less neophobic partner"",\r\n           size=1, #change line size\r\n           palette = c(""#0A85FF"",""#FF7300"",""#2724FF""), #cus']",2,"- phenotype
- social context
- house sparrow
- neophobia
- novel object trials
- aversion to novelty
- social environment
- social learning
- wild animals
- conspecific
- behavioral phenotype
- paired trials"
Neural priming of adipose-derived stem cells by cell-imprinted substrates,Raw data and figure evaluation scripts for the manuscript Neural Priming of Adipose-Derived Stem Cells by Cell-Imprinted Substrates,"['library(readxl) # To run this R-script, the R-library readxl needs to be installed and functional\r\n\r\n# Change this to wherever you decide to store pneurite length.xlsx\r\nroot_folder=""/path/to/excel/file""\r\n\r\nsetwd(root_folder)\r\n\r\nneurite_data = as.data.frame(read_excel(""neurite length.xlsx""))\r\n\r\n# Helper function for plotting (Thomas Braschler)\r\n\r\nsignificance_labels<-function (x, levels = c(0.1, 0.05, 0.01, 0.001), codes = c(""."",\r\n""*"", ""**"", ""***""))\r\n{\r\n    s = x\r\n    s[] = """"\r\n    for (level_index in 1:length(levels)) {\r\n        s[x <= levels[level_index]] = codes[level_index]\r\n    }\r\n    return(s)\r\n}\r\n\r\n\r\nbarplot_with_errorbars<-function (height, sd_height = NULL, beside = FALSE, horiz = FALSE,\r\ngroup_order = NULL, sig_codes = NULL, sig.cex = 1, ...)\r\n{\r\n    args = list(...)\r\n    if (!is.null(group_order)) {\r\n        if (is.matrix(height)) {\r\n            height = height[, group_order]\r\n            sd_height = sd_height[, group_order]\r\n            if (!is.null(sig_codes)) {\r\n                sig_codes = sig_codes[, group_order]\r\n            }\r\n        }\r\n        else {\r\n            height = height[group_order]\r\n            sd_height = sd_height[group_order]\r\n            if (!is.null(sig_codes)) {\r\n                sig_codes = sig_codes[group_order]\r\n            }\r\n        }\r\n        if (!is.null(args[[""names.arg""]])) {\r\n            args[[""names.arg""]] = args[[""names.arg""]][group_order]\r\n        }\r\n    }\r\n    y = c(height)\r\n    if (is.null(sd_height)) {\r\n        sd_y = 0\r\n    }\r\n    else {\r\n        sd_y = c(sd_height)\r\n    }\r\n    if (is.null(args$ylim)) {\r\n        upper_y = max(c(y, max(y + sd_y * 2, na.rm = TRUE)),\r\n        na.rm = TRUE)\r\n        lower_y = 0\r\n        limit_y = c(lower_y, upper_y)\r\n    }\r\n    else {\r\n        limit_y = args$ylim\r\n    }\r\n    args$ylim = limit_y\r\n    args[[""height""]] = height\r\n    args[[""beside""]] = beside\r\n    args[[""horiz""]] = horiz\r\n    theLength = NULL\r\n    if (!is.null(args[[""length""]])) {\r\n        theLength = args[[""length""]]\r\n        args[[""length""]] = NULL\r\n    }\r\n    w = c(do.call(barplot, args))\r\n    if (!beside & is.matrix(height)) {\r\n        xpoints = matrix(data = w, nrow = nrow(height), ncol = length(w),\r\n        byrow = TRUE)\r\n    }\r\n    else {\r\n        xpoints = w\r\n    }\r\n    ypoints_start = height\r\n    if (!beside) {\r\n        if (is.matrix(height)) {\r\n            ypoints_start = apply(height, 2, cumsum)\r\n        }\r\n    }\r\n    if (horiz) {\r\n        errorbar_args_positive = list(x = ypoints_start[ypoints_start >=\r\n        0], y = xpoints[ypoints_start >= 0], sd_y = sd_y[ypoints_start >=\r\n        0], code = 2)\r\n        errorbar_args_negative = list(x = ypoints_start[ypoints_start <\r\n        0], y = xpoints[ypoints_start < 0], sd_y = sd_y[ypoints_start <\r\n        0], code = 1)\r\n        errorbar_args_positive[[""horiz""]] = TRUE\r\n        errorbar_args_negative[[""horiz""]] = TRUE\r\n    }\r\n    else {\r\n        errorbar_args_positive = list(x = xpoints[ypoints_start >=\r\n        0], y = ypoints_start[ypoints_start >= 0], sd_y = sd_y[ypoints_start >=\r\n        0], code = 2)\r\n        errorbar_args_negative = list(x = xpoints[ypoints_start <\r\n        0], y = ypoints_start[ypoints_start < 0], sd_y = sd_y[ypoints_start <\r\n        0], code = 1)\r\n        if (!is.null(args[[""offset""]])) {\r\n            errorbar_args_positive[[""y""]] = errorbar_args_positive[[""y""]] +\r\n            args[[""offset""]]\r\n            errorbar_args_negative[[""y""]] = errorbar_args_negative[[""y""]] +\r\n            args[[""offset""]]\r\n        }\r\n    }\r\n    if (!is.null(theLength)) {\r\n        errorbar_args_positive[[""length""]] = theLength\r\n        errorbar_args_negative[[""length""]] = theLength\r\n    }\r\n    do.call(errorbars, errorbar_args_positive)\r\n    do.call(errorbars, errorbar_args_negative)\r\n    if (horiz) {\r\n        if (!is.null(sig_codes)) {\r\n            text(y + sd_y + limit_y[2] * 0.03, c(xpoints), c(sig_codes))\r\n        }\r\n    }\r\n    else {\r\n        if (!is.null(sig_codes)) {\r\n            if (beside) {\r\n                text(c(xpoints), y + sd_y + limit_y[2] * 0.03,\r\n                c(sig_codes), cex = sig.cex)\r\n            }\r\n            else {\r\n                text(c(xpoints), ypoints_start + sd_y + limit_y[2] *\r\n                0.03, c(sig_codes), cex = sig.cex)\r\n            }\r\n        }\r\n    }\r\n    invisible(w)\r\n}\r\n\r\nerrorbars<-function (x, y, sd_y = 0, angle = 90, code = 3, horiz = FALSE,\r\n...)\r\n{\r\n    ystart = y\r\n    ystop = y\r\n    if (code == 2 || code == 3) {\r\n        ystop = y + sd_y\r\n    }\r\n    if (code == 1 || code == 3) {\r\n        ystart = y - sd_y\r\n    }\r\n    if (horiz) {\r\n        arrows(x0 = ystart, y0 = c(x), x1 = ystop, y1 = c(x),\r\n        angle = angle, code = code, ...)\r\n    }\r\n    else {\r\n        arrows(x0 = x, y0 = ystart, x1 = x, y1 = ystop, angle = angle,\r\n        code = code, ...)\r\n    }\r\n}\r\n\r\n\r\n\r\nneurite_data$Substrate = factor(neurite_data$Substrate,levels=c(""TCP"",""imprinted""))\r\n\r\nneurite_data$Neurite_length = as.numeric(neurite_data[,""Average Neurite Length (micrometers)""]) # The long nam', 'library(readxl) # To run this R-script, the R-library readxl needs to be installed and functional\r\nlibrary(multcomp) # library for multiple comparisons\r\n\r\n# Change this to wherever you decide to store qPCR_Comparison.xlsx\r\nroot_folder=""/path/to/excel_file""\r\n\r\nsetwd(root_folder)\r\n\r\nqPCR_data = as.data.frame(read_excel(""qPCR_Comparison.xlsx""))\r\n\r\n\r\n# Helper function for plotting (Thomas Braschler)\r\n\r\nsignificance_labels<-function (x, levels = c(0.1, 0.05, 0.01, 0.001), codes = c(""."",\r\n""*"", ""**"", ""***""))\r\n{\r\n    s = x\r\n    s[] = """"\r\n    for (level_index in 1:length(levels)) {\r\n        s[x <= levels[level_index]] = codes[level_index]\r\n    }\r\n    return(s)\r\n}\r\n\r\n\r\nbarplot_with_errorbars<-function (height, sd_height = NULL, beside = FALSE, horiz = FALSE,\r\ngroup_order = NULL, sig_codes = NULL, sig.cex = 1, ...)\r\n{\r\n    args = list(...)\r\n    if (!is.null(group_order)) {\r\n        if (is.matrix(height)) {\r\n            height = height[, group_order]\r\n            sd_height = sd_height[, group_order]\r\n            if (!is.null(sig_codes)) {\r\n                sig_codes = sig_codes[, group_order]\r\n            }\r\n        }\r\n        else {\r\n            height = height[group_order]\r\n            sd_height = sd_height[group_order]\r\n            if (!is.null(sig_codes)) {\r\n                sig_codes = sig_codes[group_order]\r\n            }\r\n        }\r\n        if (!is.null(args[[""names.arg""]])) {\r\n            args[[""names.arg""]] = args[[""names.arg""]][group_order]\r\n        }\r\n    }\r\n    y = c(height)\r\n    if (is.null(sd_height)) {\r\n        sd_y = 0\r\n    }\r\n    else {\r\n        sd_y = c(sd_height)\r\n    }\r\n    if (is.null(args$ylim)) {\r\n        upper_y = max(c(y, max(y + sd_y * 2, na.rm = TRUE)),\r\n        na.rm = TRUE)\r\n        lower_y = 0\r\n        limit_y = c(lower_y, upper_y)\r\n    }\r\n    else {\r\n        limit_y = args$ylim\r\n    }\r\n    args$ylim = limit_y\r\n    args[[""height""]] = height\r\n    args[[""beside""]] = beside\r\n    args[[""horiz""]] = horiz\r\n    theLength = NULL\r\n    if (!is.null(args[[""length""]])) {\r\n        theLength = args[[""length""]]\r\n        args[[""length""]] = NULL\r\n    }\r\n    w = c(do.call(barplot, args))\r\n    if (!beside & is.matrix(height)) {\r\n        xpoints = matrix(data = w, nrow = nrow(height), ncol = length(w),\r\n        byrow = TRUE)\r\n    }\r\n    else {\r\n        xpoints = w\r\n    }\r\n    ypoints_start = height\r\n    if (!beside) {\r\n        if (is.matrix(height)) {\r\n            ypoints_start = apply(height, 2, cumsum)\r\n        }\r\n    }\r\n    if (horiz) {\r\n        errorbar_args_positive = list(x = ypoints_start[ypoints_start >=\r\n        0], y = xpoints[ypoints_start >= 0], sd_y = sd_y[ypoints_start >=\r\n        0], code = 2)\r\n        errorbar_args_negative = list(x = ypoints_start[ypoints_start <\r\n        0], y = xpoints[ypoints_start < 0], sd_y = sd_y[ypoints_start <\r\n        0], code = 1)\r\n        errorbar_args_positive[[""horiz""]] = TRUE\r\n        errorbar_args_negative[[""horiz""]] = TRUE\r\n    }\r\n    else {\r\n        errorbar_args_positive = list(x = xpoints[ypoints_start >=\r\n        0], y = ypoints_start[ypoints_start >= 0], sd_y = sd_y[ypoints_start >=\r\n        0], code = 2)\r\n        errorbar_args_negative = list(x = xpoints[ypoints_start <\r\n        0], y = ypoints_start[ypoints_start < 0], sd_y = sd_y[ypoints_start <\r\n        0], code = 1)\r\n        if (!is.null(args[[""offset""]])) {\r\n            errorbar_args_positive[[""y""]] = errorbar_args_positive[[""y""]] +\r\n            args[[""offset""]]\r\n            errorbar_args_negative[[""y""]] = errorbar_args_negative[[""y""]] +\r\n            args[[""offset""]]\r\n        }\r\n    }\r\n    if (!is.null(theLength)) {\r\n        errorbar_args_positive[[""length""]] = theLength\r\n        errorbar_args_negative[[""length""]] = theLength\r\n    }\r\n    do.call(errorbars, errorbar_args_positive)\r\n    do.call(errorbars, errorbar_args_negative)\r\n    if (horiz) {\r\n        if (!is.null(sig_codes)) {\r\n            text(y + sd_y + limit_y[2] * 0.03, c(xpoints), c(sig_codes))\r\n        }\r\n    }\r\n    else {\r\n        if (!is.null(sig_codes)) {\r\n            if (beside) {\r\n                text(c(xpoints), y + sd_y + limit_y[2] * 0.03,\r\n                c(sig_codes), cex = sig.cex)\r\n            }\r\n            else {\r\n                text(c(xpoints), ypoints_start + sd_y + limit_y[2] *\r\n                0.03, c(sig_codes), cex = sig.cex)\r\n            }\r\n        }\r\n    }\r\n    invisible(w)\r\n}\r\n\r\nerrorbars<-function (x, y, sd_y = 0, angle = 90, code = 3, horiz = FALSE,\r\n...)\r\n{\r\n    ystart = y\r\n    ystop = y\r\n    if (code == 2 || code == 3) {\r\n        ystop = y + sd_y\r\n    }\r\n    if (code == 1 || code == 3) {\r\n        ystart = y - sd_y\r\n    }\r\n    if (horiz) {\r\n        arrows(x0 = ystart, y0 = c(x), x1 = ystop, y1 = c(x),\r\n        angle = angle, code = code, ...)\r\n    }\r\n    else {\r\n        arrows(x0 = x, y0 = ystart, x1 = x, y1 = ystop, angle = angle,\r\n        code = code, ...)\r\n    }\r\n}\r\n\r\n\r\n\r\n\r\n# Statistical analysis on the log transformed expression, this normalizes the variance (and also corresponds to the original data which is Ct', 'library(readxl) # To run this R-script, the R-library readxl needs to be installed and functional\r\nlibrary(multcomp) # library for multiple comparisons\r\n\r\n# Change this to wherever you decide to store positive_cell_fraction.xlsx\r\nroot_folder=""/path/to/excel_file""\r\n\r\nsetwd(root_folder)\r\n\r\ncell_positive_data = as.data.frame(read_excel(""positive_cell_fraction.xlsx""))\r\n\r\n# Helper function for plotting (Thomas Braschler)\r\n\r\nsignificance_labels<-function (x, levels = c(0.1, 0.05, 0.01, 0.001), codes = c(""."",\r\n""*"", ""**"", ""***""))\r\n{\r\n    s = x\r\n    s[] = """"\r\n    for (level_index in 1:length(levels)) {\r\n        s[x <= levels[level_index]] = codes[level_index]\r\n    }\r\n    return(s)\r\n}\r\n\r\n\r\nbarplot_with_errorbars<-function (height, sd_height = NULL, beside = FALSE, horiz = FALSE,\r\ngroup_order = NULL, sig_codes = NULL, sig.cex = 1, ...)\r\n{\r\n    args = list(...)\r\n    if (!is.null(group_order)) {\r\n        if (is.matrix(height)) {\r\n            height = height[, group_order]\r\n            sd_height = sd_height[, group_order]\r\n            if (!is.null(sig_codes)) {\r\n                sig_codes = sig_codes[, group_order]\r\n            }\r\n        }\r\n        else {\r\n            height = height[group_order]\r\n            sd_height = sd_height[group_order]\r\n            if (!is.null(sig_codes)) {\r\n                sig_codes = sig_codes[group_order]\r\n            }\r\n        }\r\n        if (!is.null(args[[""names.arg""]])) {\r\n            args[[""names.arg""]] = args[[""names.arg""]][group_order]\r\n        }\r\n    }\r\n    y = c(height)\r\n    if (is.null(sd_height)) {\r\n        sd_y = 0\r\n    }\r\n    else {\r\n        sd_y = c(sd_height)\r\n    }\r\n    if (is.null(args$ylim)) {\r\n        upper_y = max(c(y, max(y + sd_y * 2, na.rm = TRUE)),\r\n        na.rm = TRUE)\r\n        lower_y = 0\r\n        limit_y = c(lower_y, upper_y)\r\n    }\r\n    else {\r\n        limit_y = args$ylim\r\n    }\r\n    args$ylim = limit_y\r\n    args[[""height""]] = height\r\n    args[[""beside""]] = beside\r\n    args[[""horiz""]] = horiz\r\n    theLength = NULL\r\n    if (!is.null(args[[""length""]])) {\r\n        theLength = args[[""length""]]\r\n        args[[""length""]] = NULL\r\n    }\r\n    w = c(do.call(barplot, args))\r\n    if (!beside & is.matrix(height)) {\r\n        xpoints = matrix(data = w, nrow = nrow(height), ncol = length(w),\r\n        byrow = TRUE)\r\n    }\r\n    else {\r\n        xpoints = w\r\n    }\r\n    ypoints_start = height\r\n    if (!beside) {\r\n        if (is.matrix(height)) {\r\n            ypoints_start = apply(height, 2, cumsum)\r\n        }\r\n    }\r\n    if (horiz) {\r\n        errorbar_args_positive = list(x = ypoints_start[ypoints_start >=\r\n        0], y = xpoints[ypoints_start >= 0], sd_y = sd_y[ypoints_start >=\r\n        0], code = 2)\r\n        errorbar_args_negative = list(x = ypoints_start[ypoints_start <\r\n        0], y = xpoints[ypoints_start < 0], sd_y = sd_y[ypoints_start <\r\n        0], code = 1)\r\n        errorbar_args_positive[[""horiz""]] = TRUE\r\n        errorbar_args_negative[[""horiz""]] = TRUE\r\n    }\r\n    else {\r\n        errorbar_args_positive = list(x = xpoints[ypoints_start >=\r\n        0], y = ypoints_start[ypoints_start >= 0], sd_y = sd_y[ypoints_start >=\r\n        0], code = 2)\r\n        errorbar_args_negative = list(x = xpoints[ypoints_start <\r\n        0], y = ypoints_start[ypoints_start < 0], sd_y = sd_y[ypoints_start <\r\n        0], code = 1)\r\n        if (!is.null(args[[""offset""]])) {\r\n            errorbar_args_positive[[""y""]] = errorbar_args_positive[[""y""]] +\r\n            args[[""offset""]]\r\n            errorbar_args_negative[[""y""]] = errorbar_args_negative[[""y""]] +\r\n            args[[""offset""]]\r\n        }\r\n    }\r\n    if (!is.null(theLength)) {\r\n        errorbar_args_positive[[""length""]] = theLength\r\n        errorbar_args_negative[[""length""]] = theLength\r\n    }\r\n    do.call(errorbars, errorbar_args_positive)\r\n    do.call(errorbars, errorbar_args_negative)\r\n    if (horiz) {\r\n        if (!is.null(sig_codes)) {\r\n            text(y + sd_y + limit_y[2] * 0.03, c(xpoints), c(sig_codes))\r\n        }\r\n    }\r\n    else {\r\n        if (!is.null(sig_codes)) {\r\n            if (beside) {\r\n                text(c(xpoints), y + sd_y + limit_y[2] * 0.03,\r\n                c(sig_codes), cex = sig.cex)\r\n            }\r\n            else {\r\n                text(c(xpoints), ypoints_start + sd_y + limit_y[2] *\r\n                0.03, c(sig_codes), cex = sig.cex)\r\n            }\r\n        }\r\n    }\r\n    invisible(w)\r\n}\r\n\r\nerrorbars<-function (x, y, sd_y = 0, angle = 90, code = 3, horiz = FALSE,\r\n...)\r\n{\r\n    ystart = y\r\n    ystop = y\r\n    if (code == 2 || code == 3) {\r\n        ystop = y + sd_y\r\n    }\r\n    if (code == 1 || code == 3) {\r\n        ystart = y - sd_y\r\n    }\r\n    if (horiz) {\r\n        arrows(x0 = ystart, y0 = c(x), x1 = ystop, y1 = c(x),\r\n        angle = angle, code = code, ...)\r\n    }\r\n    else {\r\n        arrows(x0 = x, y0 = ystart, x1 = x, y1 = ystop, angle = angle,\r\n        code = code, ...)\r\n    }\r\n}\r\n\r\n\r\n\r\ncell_positive_data$Substrate = factor(cell_positive_data$Substrate,levels=c(""TCP"",""PDMS"",""imprinted""))\r\n\r\nnestin_data = ce', 'library(readxl) # To run this R-script, the R-library readxl needs to be installed and functional\r\nlibrary(multcomp) # library for multiple comparisons\r\n\r\n# Change this to wherever you decide to store qPCR_Comparison.xlsx\r\nroot_folder=""/path/to/excel_file""\r\n\r\nsetwd(root_folder)\r\n\r\nqPCR_data = as.data.frame(read_excel(""qPCR_Comparison.xlsx""))\r\n\r\n\r\n# Helper function for plotting (Thomas Braschler)\r\n\r\nsignificance_labels<-function (x, levels = c(0.1, 0.05, 0.01, 0.001), codes = c(""."",\r\n""*"", ""**"", ""***""))\r\n{\r\n    s = x\r\n    s[] = """"\r\n    for (level_index in 1:length(levels)) {\r\n        s[x <= levels[level_index]] = codes[level_index]\r\n    }\r\n    return(s)\r\n}\r\n\r\n\r\nbarplot_with_errorbars<-function (height, sd_height = NULL, beside = FALSE, horiz = FALSE,\r\ngroup_order = NULL, sig_codes = NULL, sig.cex = 1, ...)\r\n{\r\n    args = list(...)\r\n    if (!is.null(group_order)) {\r\n        if (is.matrix(height)) {\r\n            height = height[, group_order]\r\n            sd_height = sd_height[, group_order]\r\n            if (!is.null(sig_codes)) {\r\n                sig_codes = sig_codes[, group_order]\r\n            }\r\n        }\r\n        else {\r\n            height = height[group_order]\r\n            sd_height = sd_height[group_order]\r\n            if (!is.null(sig_codes)) {\r\n                sig_codes = sig_codes[group_order]\r\n            }\r\n        }\r\n        if (!is.null(args[[""names.arg""]])) {\r\n            args[[""names.arg""]] = args[[""names.arg""]][group_order]\r\n        }\r\n    }\r\n    y = c(height)\r\n    if (is.null(sd_height)) {\r\n        sd_y = 0\r\n    }\r\n    else {\r\n        sd_y = c(sd_height)\r\n    }\r\n    if (is.null(args$ylim)) {\r\n        upper_y = max(c(y, max(y + sd_y * 2, na.rm = TRUE)),\r\n        na.rm = TRUE)\r\n        lower_y = 0\r\n        limit_y = c(lower_y, upper_y)\r\n    }\r\n    else {\r\n        limit_y = args$ylim\r\n    }\r\n    args$ylim = limit_y\r\n    args[[""height""]] = height\r\n    args[[""beside""]] = beside\r\n    args[[""horiz""]] = horiz\r\n    theLength = NULL\r\n    if (!is.null(args[[""length""]])) {\r\n        theLength = args[[""length""]]\r\n        args[[""length""]] = NULL\r\n    }\r\n    w = c(do.call(barplot, args))\r\n    if (!beside & is.matrix(height)) {\r\n        xpoints = matrix(data = w, nrow = nrow(height), ncol = length(w),\r\n        byrow = TRUE)\r\n    }\r\n    else {\r\n        xpoints = w\r\n    }\r\n    ypoints_start = height\r\n    if (!beside) {\r\n        if (is.matrix(height)) {\r\n            ypoints_start = apply(height, 2, cumsum)\r\n        }\r\n    }\r\n    if (horiz) {\r\n        errorbar_args_positive = list(x = ypoints_start[ypoints_start >=\r\n        0], y = xpoints[ypoints_start >= 0], sd_y = sd_y[ypoints_start >=\r\n        0], code = 2)\r\n        errorbar_args_negative = list(x = ypoints_start[ypoints_start <\r\n        0], y = xpoints[ypoints_start < 0], sd_y = sd_y[ypoints_start <\r\n        0], code = 1)\r\n        errorbar_args_positive[[""horiz""]] = TRUE\r\n        errorbar_args_negative[[""horiz""]] = TRUE\r\n    }\r\n    else {\r\n        errorbar_args_positive = list(x = xpoints[ypoints_start >=\r\n        0], y = ypoints_start[ypoints_start >= 0], sd_y = sd_y[ypoints_start >=\r\n        0], code = 2)\r\n        errorbar_args_negative = list(x = xpoints[ypoints_start <\r\n        0], y = ypoints_start[ypoints_start < 0], sd_y = sd_y[ypoints_start <\r\n        0], code = 1)\r\n        if (!is.null(args[[""offset""]])) {\r\n            errorbar_args_positive[[""y""]] = errorbar_args_positive[[""y""]] +\r\n            args[[""offset""]]\r\n            errorbar_args_negative[[""y""]] = errorbar_args_negative[[""y""]] +\r\n            args[[""offset""]]\r\n        }\r\n    }\r\n    if (!is.null(theLength)) {\r\n        errorbar_args_positive[[""length""]] = theLength\r\n        errorbar_args_negative[[""length""]] = theLength\r\n    }\r\n    do.call(errorbars, errorbar_args_positive)\r\n    do.call(errorbars, errorbar_args_negative)\r\n    if (horiz) {\r\n        if (!is.null(sig_codes)) {\r\n            text(y + sd_y + limit_y[2] * 0.03, c(xpoints), c(sig_codes))\r\n        }\r\n    }\r\n    else {\r\n        if (!is.null(sig_codes)) {\r\n            if (beside) {\r\n                text(c(xpoints), y + sd_y + limit_y[2] * 0.03,\r\n                c(sig_codes), cex = sig.cex)\r\n            }\r\n            else {\r\n                text(c(xpoints), ypoints_start + sd_y + limit_y[2] *\r\n                0.03, c(sig_codes), cex = sig.cex)\r\n            }\r\n        }\r\n    }\r\n    invisible(w)\r\n}\r\n\r\nerrorbars<-function (x, y, sd_y = 0, angle = 90, code = 3, horiz = FALSE,\r\n...)\r\n{\r\n    ystart = y\r\n    ystop = y\r\n    if (code == 2 || code == 3) {\r\n        ystop = y + sd_y\r\n    }\r\n    if (code == 1 || code == 3) {\r\n        ystart = y - sd_y\r\n    }\r\n    if (horiz) {\r\n        arrows(x0 = ystart, y0 = c(x), x1 = ystop, y1 = c(x),\r\n        angle = angle, code = code, ...)\r\n    }\r\n    else {\r\n        arrows(x0 = x, y0 = ystart, x1 = x, y1 = ystop, angle = angle,\r\n        code = code, ...)\r\n    }\r\n}\r\n\r\n\r\n\r\n# Statistical analysis on the log transformed expression, this normalizes the variance (and also corresponds to the original data which is Ct v', 'library(readxl) # To run this R-script, the R-library readxl needs to be installed and functional\r\n\r\n# Change this to wherever you decide to store ellipsoid_evaluation.xlsx\r\nroot_folder=""/path/to/excel_file""\r\n\r\nsetwd(root_folder)\r\n\r\nellipsoids=as.data.frame(read_excel(""ellipsoid_evaluation.xlsx""))\r\n\r\n\r\nellipsoids$Substrate=factor(ellipsoids$Substrate,levels=c(""TCP"",""PDMS"",""imprinted""))\r\n\r\n\r\n\r\nbreaks=c(seq(from=1,to=3,by=1/3),50)\r\n\r\n\r\nfor_barplot_ecc = matrix(ncol=length(levels(ellipsoids$Substrate)),nrow=length(breaks)-1)\r\n\r\n\r\ncolnames(for_barplot_ecc)=levels(ellipsoids$Substrate)\r\n\r\nfor_chisq=for_barplot_ecc\r\n\r\nfor(theSubstrate in colnames(for_barplot_ecc))\r\n{\r\n    \r\n    for_barplot_ecc[,theSubstrate]=hist(ellipsoids$Eccentricity[ellipsoids$Substrate==theSubstrate],breaks=breaks,plot=FALSE)$counts\r\n    \r\n    for_barplot_ecc[,theSubstrate]=for_barplot_ecc[,theSubstrate]/sum(for_barplot_ecc[,theSubstrate])\r\n    \r\n    for_chisq[,theSubstrate]=hist(ellipsoids$Eccentricity[ellipsoids$Substrate==theSubstrate],breaks=breaks,plot=FALSE)$counts\r\n    \r\n    for_barplot_ecc[,theSubstrate]=for_barplot_ecc[,theSubstrate]/sum(for_barplot_ecc[,theSubstrate])\r\n    \r\n    \r\n    \r\n    \r\n    \r\n}\r\n\r\n\r\n\r\nnames.arg= round((breaks[1:(length(breaks)-1)]+breaks[2:(length(breaks))])/2*10)/10\r\n\r\n\r\nbarplot(t(for_barplot_ecc),beside=TRUE,legend=TRUE, col=c(""#eeeeeeff"",""#aaaaaaff"",""#555555ff""),names.arg=names.arg,xlab=""Eccentricity"",ylab=""Frequency"",las=2,ylim=c(0,0.3))\r\n\r\n# Chisquare test of the distribution against the TCP\r\n# Consider all categories except for the very large eccentricities where there are almost no cells\r\n\r\ncat(""Chi-square test for the distribution, PDMS vs TCP"")\r\n\r\ntheSubstrate=""PDMS""\r\n\r\nei_sub=sum(for_chisq[-dim(for_chisq)[1],theSubstrate])*(for_barplot_ecc[-dim(for_chisq)[1],""TCP""]+for_barplot_ecc[-dim(for_chisq)[1],theSubstrate])/2\r\nei_TCP=sum(for_chisq[-dim(for_chisq)[1],""TCP""])*(for_barplot_ecc[-dim(for_chisq)[1],""TCP""]+for_barplot_ecc[-dim(for_chisq)[1],theSubstrate])/2\r\n\r\nei=c(ei_sub,ei_TCP)\r\n\r\noi=c(for_chisq[-dim(for_chisq)[1],theSubstrate],for_chisq[-dim(for_chisq)[1],""TCP""])\r\n\r\nchisq=sum((oi-ei)^2/ei)\r\n\r\n# degrees of freedom: for each ei substrate (or TCP) one comme P-value is estimated, and we use the two totals\r\n\r\ndf=length(oi)-2- length(ei_sub)\r\n\r\n3*pchisq(chisq,df,lower.tail=FALSE)\r\n\r\ncat(""Chi-square test for the distribution, imprinted vs TCP"")\r\n\r\ntheSubstrate=""imprinted""\r\n\r\nei_sub=sum(for_chisq[-dim(for_chisq)[1],theSubstrate])*(for_barplot_ecc[-dim(for_chisq)[1],""TCP""]+for_barplot_ecc[-dim(for_chisq)[1],theSubstrate])/2\r\nei_TCP=sum(for_chisq[-dim(for_chisq)[1],""TCP""])*(for_barplot_ecc[-dim(for_chisq)[1],""TCP""]+for_barplot_ecc[-dim(for_chisq)[1],theSubstrate])/2\r\n\r\nei=c(ei_sub,ei_TCP)\r\n\r\noi=c(for_chisq[-dim(for_chisq)[1],theSubstrate],for_chisq[-dim(for_chisq)[1],""TCP""])\r\n\r\nchisq=sum((oi-ei)^2/ei)\r\n\r\n# degrees of freedom: for each ei substrate (or TCP) one comme P-value is estimated, and we use the two totals\r\n\r\ndf=length(oi)-2- length(ei_sub)\r\n\r\n3*pchisq(chisq,df,lower.tail=FALSE)\r\n\r\ncat(""Chi-square test for the distribution, imprinted vs PDMS"")\r\n\r\ntheSubstrate=""imprinted""\r\n\r\nei_sub=sum(for_chisq[-dim(for_chisq)[1],theSubstrate])*(for_barplot_ecc[-dim(for_chisq)[1],""PDMS""]+for_barplot_ecc[-dim(for_chisq)[1],theSubstrate])/2\r\nei_PDMS=sum(for_chisq[-dim(for_chisq)[1],""PDMS""])*(for_barplot_ecc[-dim(for_chisq)[1],""PDMS""]+for_barplot_ecc[-dim(for_chisq)[1],theSubstrate])/2\r\n\r\nei=c(ei_sub,ei_PDMS)\r\n\r\noi=c(for_chisq[-dim(for_chisq)[1],theSubstrate],for_chisq[-dim(for_chisq)[1],""PDMS""])\r\n\r\nchisq=sum((oi-ei)^2/ei)\r\n\r\n# degrees of freedom: for each ei substrate (or TCP) one comme P-value is estimated, and we use the two totals\r\n\r\ndf=length(oi)-2- length(ei_sub)\r\n\r\n3*pchisq(chisq,df,lower.tail=FALSE)\r\n\r\n\r\n\r\n\r\n\r\n\r\n', 'library(readxl) # To run this R-script, the R-library readxl needs to be installed and functional\nlibrary(multcomp) # library for multiple comparisons\n\n# Change this to wherever you decide to store cell_number_evaluation.xlsx\nroot_folder=""/path/to/excel_file""\n\nsetwd(root_folder)\n\ncount_data=as.data.frame(read_excel(""cell_number_evaluation.xlsx""))\n\n\n# Helper function for plotting (Thomas Braschler)\n\nsignificance_labels<-function (x, levels = c(0.1, 0.05, 0.01, 0.001), codes = c(""."",\n""*"", ""**"", ""***""))\n{\n    s = x\n    s[] = """"\n    for (level_index in 1:length(levels)) {\n        s[x <= levels[level_index]] = codes[level_index]\n    }\n    return(s)\n}\n\n\nbarplot_with_errorbars<-function (height, sd_height = NULL, beside = FALSE, horiz = FALSE,\ngroup_order = NULL, sig_codes = NULL, sig.cex = 1, ...)\n{\n    args = list(...)\n    if (!is.null(group_order)) {\n        if (is.matrix(height)) {\n            height = height[, group_order]\n            sd_height = sd_height[, group_order]\n            if (!is.null(sig_codes)) {\n                sig_codes = sig_codes[, group_order]\n            }\n        }\n        else {\n            height = height[group_order]\n            sd_height = sd_height[group_order]\n            if (!is.null(sig_codes)) {\n                sig_codes = sig_codes[group_order]\n            }\n        }\n        if (!is.null(args[[""names.arg""]])) {\n            args[[""names.arg""]] = args[[""names.arg""]][group_order]\n        }\n    }\n    y = c(height)\n    if (is.null(sd_height)) {\n        sd_y = 0\n    }\n    else {\n        sd_y = c(sd_height)\n    }\n    if (is.null(args$ylim)) {\n        upper_y = max(c(y, max(y + sd_y * 2, na.rm = TRUE)),\n        na.rm = TRUE)\n        lower_y = 0\n        limit_y = c(lower_y, upper_y)\n    }\n    else {\n        limit_y = args$ylim\n    }\n    args$ylim = limit_y\n    args[[""height""]] = height\n    args[[""beside""]] = beside\n    args[[""horiz""]] = horiz\n    theLength = NULL\n    if (!is.null(args[[""length""]])) {\n        theLength = args[[""length""]]\n        args[[""length""]] = NULL\n    }\n    w = c(do.call(barplot, args))\n    if (!beside & is.matrix(height)) {\n        xpoints = matrix(data = w, nrow = nrow(height), ncol = length(w),\n        byrow = TRUE)\n    }\n    else {\n        xpoints = w\n    }\n    ypoints_start = height\n    if (!beside) {\n        if (is.matrix(height)) {\n            ypoints_start = apply(height, 2, cumsum)\n        }\n    }\n    if (horiz) {\n        errorbar_args_positive = list(x = ypoints_start[ypoints_start >=\n        0], y = xpoints[ypoints_start >= 0], sd_y = sd_y[ypoints_start >=\n        0], code = 2)\n        errorbar_args_negative = list(x = ypoints_start[ypoints_start <\n        0], y = xpoints[ypoints_start < 0], sd_y = sd_y[ypoints_start <\n        0], code = 1)\n        errorbar_args_positive[[""horiz""]] = TRUE\n        errorbar_args_negative[[""horiz""]] = TRUE\n    }\n    else {\n        errorbar_args_positive = list(x = xpoints[ypoints_start >=\n        0], y = ypoints_start[ypoints_start >= 0], sd_y = sd_y[ypoints_start >=\n        0], code = 2)\n        errorbar_args_negative = list(x = xpoints[ypoints_start <\n        0], y = ypoints_start[ypoints_start < 0], sd_y = sd_y[ypoints_start <\n        0], code = 1)\n        if (!is.null(args[[""offset""]])) {\n            errorbar_args_positive[[""y""]] = errorbar_args_positive[[""y""]] +\n            args[[""offset""]]\n            errorbar_args_negative[[""y""]] = errorbar_args_negative[[""y""]] +\n            args[[""offset""]]\n        }\n    }\n    if (!is.null(theLength)) {\n        errorbar_args_positive[[""length""]] = theLength\n        errorbar_args_negative[[""length""]] = theLength\n    }\n    do.call(errorbars, errorbar_args_positive)\n    do.call(errorbars, errorbar_args_negative)\n    if (horiz) {\n        if (!is.null(sig_codes)) {\n            text(y + sd_y + limit_y[2] * 0.03, c(xpoints), c(sig_codes))\n        }\n    }\n    else {\n        if (!is.null(sig_codes)) {\n            if (beside) {\n                text(c(xpoints), y + sd_y + limit_y[2] * 0.03,\n                c(sig_codes), cex = sig.cex)\n            }\n            else {\n                text(c(xpoints), ypoints_start + sd_y + limit_y[2] *\n                0.03, c(sig_codes), cex = sig.cex)\n            }\n        }\n    }\n    invisible(w)\n}\n\nerrorbars<-function (x, y, sd_y = 0, angle = 90, code = 3, horiz = FALSE,\n...)\n{\n    ystart = y\n    ystop = y\n    if (code == 2 || code == 3) {\n        ystop = y + sd_y\n    }\n    if (code == 1 || code == 3) {\n        ystart = y - sd_y\n    }\n    if (horiz) {\n        arrows(x0 = ystart, y0 = c(x), x1 = ystop, y1 = c(x),\n        angle = angle, code = code, ...)\n    }\n    else {\n        arrows(x0 = x, y0 = ystart, x1 = x, y1 = ystop, angle = angle,\n        code = code, ...)\n    }\n}\n\n\ncount_data$Substrate=factor(count_data$Substrate,levels=c(""TCP"",""PDMS"",""imprinted""))\n\ngrowth_data = count_data[count_data$Day==14,]\n\nseeding_density_cells_mm2 = 10000/1.9/100\n\n\ngrowth_data$relative_increase = growth_data$cell_per_mm2/seeding_density_cells_mm2\n\ngrowth_data$k_per_day = log(gr', 'library(readxl) # To run this R-script, the R-library readxl needs to be installed and functional\nlibrary(multcomp) # library for multiple comparisons\n\n# Change this to wherever you decide to store qPCR_Comparison.xlsx\nroot_folder=""/path/to/excel_file""\n\n\nsetwd(root_folder)\n\nqPCR_data = as.data.frame(read_excel(""qPCR_Comparison.xlsx""))\n\n\n# Helper function for plotting (Thomas Braschler)\n\nsignificance_labels<-function (x, levels = c(0.1, 0.05, 0.01, 0.001), codes = c(""."",\n""*"", ""**"", ""***""))\n{\n    s = x\n    s[] = """"\n    for (level_index in 1:length(levels)) {\n        s[x <= levels[level_index]] = codes[level_index]\n    }\n    return(s)\n}\n\n\nbarplot_with_errorbars<-function (height, sd_height = NULL, beside = FALSE, horiz = FALSE,\ngroup_order = NULL, sig_codes = NULL, sig.cex = 1, ...)\n{\n    args = list(...)\n    if (!is.null(group_order)) {\n        if (is.matrix(height)) {\n            height = height[, group_order]\n            sd_height = sd_height[, group_order]\n            if (!is.null(sig_codes)) {\n                sig_codes = sig_codes[, group_order]\n            }\n        }\n        else {\n            height = height[group_order]\n            sd_height = sd_height[group_order]\n            if (!is.null(sig_codes)) {\n                sig_codes = sig_codes[group_order]\n            }\n        }\n        if (!is.null(args[[""names.arg""]])) {\n            args[[""names.arg""]] = args[[""names.arg""]][group_order]\n        }\n    }\n    y = c(height)\n    if (is.null(sd_height)) {\n        sd_y = 0\n    }\n    else {\n        sd_y = c(sd_height)\n    }\n    if (is.null(args$ylim)) {\n        upper_y = max(c(y, max(y + sd_y * 2, na.rm = TRUE)),\n        na.rm = TRUE)\n        lower_y = 0\n        limit_y = c(lower_y, upper_y)\n    }\n    else {\n        limit_y = args$ylim\n    }\n    args$ylim = limit_y\n    args[[""height""]] = height\n    args[[""beside""]] = beside\n    args[[""horiz""]] = horiz\n    theLength = NULL\n    if (!is.null(args[[""length""]])) {\n        theLength = args[[""length""]]\n        args[[""length""]] = NULL\n    }\n    w = c(do.call(barplot, args))\n    if (!beside & is.matrix(height)) {\n        xpoints = matrix(data = w, nrow = nrow(height), ncol = length(w),\n        byrow = TRUE)\n    }\n    else {\n        xpoints = w\n    }\n    ypoints_start = height\n    if (!beside) {\n        if (is.matrix(height)) {\n            ypoints_start = apply(height, 2, cumsum)\n        }\n    }\n    if (horiz) {\n        errorbar_args_positive = list(x = ypoints_start[ypoints_start >=\n        0], y = xpoints[ypoints_start >= 0], sd_y = sd_y[ypoints_start >=\n        0], code = 2)\n        errorbar_args_negative = list(x = ypoints_start[ypoints_start <\n        0], y = xpoints[ypoints_start < 0], sd_y = sd_y[ypoints_start <\n        0], code = 1)\n        errorbar_args_positive[[""horiz""]] = TRUE\n        errorbar_args_negative[[""horiz""]] = TRUE\n    }\n    else {\n        errorbar_args_positive = list(x = xpoints[ypoints_start >=\n        0], y = ypoints_start[ypoints_start >= 0], sd_y = sd_y[ypoints_start >=\n        0], code = 2)\n        errorbar_args_negative = list(x = xpoints[ypoints_start <\n        0], y = ypoints_start[ypoints_start < 0], sd_y = sd_y[ypoints_start <\n        0], code = 1)\n        if (!is.null(args[[""offset""]])) {\n            errorbar_args_positive[[""y""]] = errorbar_args_positive[[""y""]] +\n            args[[""offset""]]\n            errorbar_args_negative[[""y""]] = errorbar_args_negative[[""y""]] +\n            args[[""offset""]]\n        }\n    }\n    if (!is.null(theLength)) {\n        errorbar_args_positive[[""length""]] = theLength\n        errorbar_args_negative[[""length""]] = theLength\n    }\n    do.call(errorbars, errorbar_args_positive)\n    do.call(errorbars, errorbar_args_negative)\n    if (horiz) {\n        if (!is.null(sig_codes)) {\n            text(y + sd_y + limit_y[2] * 0.03, c(xpoints), c(sig_codes))\n        }\n    }\n    else {\n        if (!is.null(sig_codes)) {\n            if (beside) {\n                text(c(xpoints), y + sd_y + limit_y[2] * 0.03,\n                c(sig_codes), cex = sig.cex)\n            }\n            else {\n                text(c(xpoints), ypoints_start + sd_y + limit_y[2] *\n                0.03, c(sig_codes), cex = sig.cex)\n            }\n        }\n    }\n    invisible(w)\n}\n\nerrorbars<-function (x, y, sd_y = 0, angle = 90, code = 3, horiz = FALSE,\n...)\n{\n    ystart = y\n    ystop = y\n    if (code == 2 || code == 3) {\n        ystop = y + sd_y\n    }\n    if (code == 1 || code == 3) {\n        ystart = y - sd_y\n    }\n    if (horiz) {\n        arrows(x0 = ystart, y0 = c(x), x1 = ystop, y1 = c(x),\n        angle = angle, code = code, ...)\n    }\n    else {\n        arrows(x0 = x, y0 = ystart, x1 = x, y1 = ystop, angle = angle,\n        code = code, ...)\n    }\n}\n\n\nqPCR_data$log_expression = log(qPCR_data$Normalized_expression)\n\nqPCR_data$condition=factor(rep(""other"",dim(qPCR_data)[1]),levels=c(""negative"",""test"",""positive"",""other""))\n\nqPCR_data$condition[(qPCR_data$Cell==""ADSC"" & qPCR_data$Substrate==""imprinted"")]=""test""\n\nqPCR_data$condition[(qPCR_data$Cell==""Renc']",2,"Keywords: Neural priming, Adipose-derived stem cells, Cell-imprinted substrates, Raw data, Figure evaluation scripts, Manuscript."
Data and files for: Decoupling cooperation and punishment in humans shows that punishment is not an altruistic trait,"Economic experiments have suggested that cooperative humans will altruistically match local levels of cooperation ('conditional cooperation') and pay to punish non-cooperators ('altruistic punishment'). Evolutionary models have suggested that if altruists punish non-altruists this could favour the evolution of costly helping behaviours (cooperation) among strangers. An often-key requirement is that helping behaviours and punishing behaviours form one single, conjoined trait ('strong reciprocity'). Previous economics experiments have provided support for the hypothesis that punishment and cooperation form one conjoined, altruistically motivated, trait. However, such a conjoined trait may be evolutionarily unstable, and previous experiments have confounded a fear of being punished with being surrounded by cooperators, two factors that could favour cooperation. Here, we experimentally decouple the fear of punishment from a cooperative environment and allow cooperation and punishment behaviour to freely separate (420 participants). We show, that if a minority of individuals are made immune to punishment, they (1) learn to stop cooperating on average despite being surrounded by high levels of cooperation, contradicting the idea of conditional cooperation; and (2) often continue to punish, 'hypocritically', showing that cooperation and punishment do not form one, altruistically motivated, linked trait.","['#Analysis file for ""Decoupling cooperation and punishment in humans shows that punishment is not an altruistic trait."" by Burton-Chellew, M.N. & Guerin, C. 2021\n\n#note the experiment/project had an official codename of ""PIN"".\n\n\n# Step 1: \nprint(""Step 1: import the z-Tree data"")\n# using package zTree (from Kirchkamp_2019_JBEF) to upload all the data\n\nif(!require(zTree)){\n  install.packages(""zTree"")\n  library(zTree)\n}\n\n#uploading data according to their instructions\n(myFilenames <- list.files(pattern = ""*.xls""))\nallData <- zTreeTables(myFilenames)\nsummary(allData)\n\n#Here is the dimensions and a subset of the globals tables:\ndim(allData[[""globals""]])\n\n\n#40 variables, here is a description of them all\n# Date = the date and time of the session\n# Treatment = does not mean experimental treatment, but means z-Tree file order in experiment (1 = instructions, 2 = strategy method stage, 3 = public good game)\n# Period = period/round of that stage (\'treatment\')\n# NumPeriods = how many that stage had (1, 1, or 5 for the repeated pgg)\n# RepeatTreatment = ignore\n# ExchangeRate = the exchange rate we used between Monetary Units and Swiss CHF, note this was reduced from 0.05 to 0.04 after session 3\n# Ascending/Descending = coding of the words for coding purposes in the subjects table\n# Random1-3 = random number from 0-20 to simulate computer players in strategy method\n# Computer1-3,..Sum,...Average,...AverageInteger = integer version of Random1-3 to represent computer players and summaries\n# AuctionStop / noStop = ignore\n# N = group size for public good (4)\n# Endowment = endowment for public good (20)\n# MPCR = marginal per capita return for public good (0.4)\n# EfficientyFactor = N*MPCR (1.6)\n# PunishmentFactor = how much deductions are multiplied, so punisher spends 1 MU to deduct 3 MU from punishee\n# PunishmentLimit = the maximum deduction a player could spend on another individual (spend 6 to deduct 18), NOTE: THIS CHANGED FROM 6 TO 10 after session 2\n# PunishmentBudget = The fresh endowment given to players to enable them to punish others (3*limit because 3 other group members), NOTE: THIS CHANGED FROM 18 TO 30 AFTER SESSION 2, this was so a punisher could feasibly equalize payoffs between themself and a free rider who had 20 MU more than them (each unit spent on punishment reduces the inequality by 2)\n# Yes/ No = coding of the words for subjects coding purposes\n# Tyrant / Diplomat / Solider / Peasant = coding of those words numerically as 1-4 for subjects table \n# AllSoldiers, TyrantvSoldiers, TyrantvPeasants, DiplomatvSoldiers = coding of those names for \'Worlds\'\n# World = the World (aka \'Scenario\') that session modeled \n#(each session was either AllSoldiders(1); TyrantvSoldiers(2); or TyrantvPeasants(3), and DiplomatvSoldiers was not conducted in the end)\n# \'AllSoldiers\', i.e. same as Fehr & Gachter 2002 design, aka \'Mutual Punishers\')\n# TyrantsvSoldiers (1 immune punisher, aka ""Immune Punisher\') \n# TyrantsvPeasants (1 punisher, aka \'Sole Punisher\')\n# DiplomatvSoldiers (1 immune player who cannot punish, 3 punishers) - not conducted\n# Session = Session number (1-20)\n# Special = Tyrant (not important, can ignore)\n# Normal = either Soldier or Peasant (not important, can ignore)\n\n#And here is a subset of the subjects tables: \nallData[[""subjects""]][100:105,1:5]\ndim(allData[[""subjects""]])\n#$subjects:\'data.frame\': 2964 obs. of  205 variables:\n\n#Importing questionnaires: z-Tree stores information from the questionnaire at the end of the experiment in a table with the extension\n#.sbj.\n( myQuestFiles <- list.files(pattern=""*.sbj"") )\n## [1] ""160215_0810.sbj"" ""160215_0949.sbj""\n\n#Now myQuestFiles is a vector of filenames. Each name refers to the subjects from one session of the experiment (here we have only two sessions). While the .xls table stores subject specific information as one line per subject, the .sbj table uses one column per subject. The function zTreeSbj reads a vector of files and transposes them to one line per subject. We read subjects from all sessions with the following command:\nallQuest <- zTreeSbj(myQuestFiles) ## reading 160215_0810.sbj ... ## reading 160215_0949.sbj ...\n\n#The data frame allQuest contains all subjects from all experiments. Here is a subset:\nallQuest[,5:6]\nlevels(as.factor(allQuest$Gender))\nlevels(as.factor(allQuest$Age))\nsummary(as.factor(allQuest$Gender))\nsummary(as.factor(allQuest$Age))\n\n#We can easily merge data from the questionnaire with the subjects table:\nsubjectsJoined <- merge(allData[[""subjects""]],allQuest)\n\n#Here is a subset:\nsubjectsJoined[990:1000,c(""Date"",""Subject"", ""Group"",""Profit"",""Age"",""Gender"", ""Period"", ""Contribution"")]\ndim(subjectsJoined)\n#each subject (participant) has 7 rows of data, 1 for instructions phase, 1 for strategy method phase, and 1-5 for rPGG phase.\n#I do not know why the periods do not seem to be in order\n\n\n###################### Step 2: data wrangling #####################################\nprint(""Step 2 - renaming and creating variables"")\n####################']",2,"Decoupling cooperation, punishment, humans, altruistic punishment, conditional cooperation, evolutionary models, helping behaviors, strong reciprocity, economics experiments, fear of punishment, cooperative environment, immunity to punishment, minority, average, contradicts, hypocritical."
Wild Dictyostelium discoideum social amoebae show plastic responses to the presence of nonrelatives during multicellular development,"When multiple strains of microbes form social groups, such as the multicellular fruiting bodies of Dictyostelium discoideum, conflict can arise regarding cell fate. Both fixed and plastic differences among strains can contribute to cell fate, and plastic responses may be particularly important if social environments frequently change. We used RNA-sequencing and photographic time series analysis to detect possible conflict-induced plastic differences between wild D. discoideum aggregates formed by single strains compared to mixed pairs of strains (chimeras). We found one hundred and two differentially expressed genes that were enriched for biological processes including cytoskeleton organization and cyclic-AMP response (up-regulated in chimeras), and DNA replication and cell cycle (down-regulated in chimeras). In addition, our data indicate that in reference to a time series of multicellular development in the lab strain AX4, chimeras may be slightly behind clonal aggregates in their development. Finally, phenotypic analysis supported slower splitting of aggregates and a nonsignificant trend for larger group sizes in chimeras. The transcriptomic comparison and phenotypic analyses support discoordination among aggregate group members due to social conflict. These results are consistent with previously observed factors that affect cell fate decision in D. discoideum and provide evidence for plasticity in cAMP signaling and phenotypic coordination during development in response to social conflict in D. discoideum and similar microbial social groups.","['### R/bioconductor differential expression (DESeq2) workflow using counted reads (htseq-count)\n## edited for publication: Oct 11, 2019\n\nlibraries <- c(""DESeq2"", ""ggplot2"", ""gplots"", ""RColorBrewer"", ""gridExtra"", ""GenomicFeatures"", ""Gviz"", ""gage"", ""GOstats"", ""GSEABase"", ""ShortRead"", ""reshape2"", ""mgcv"", ""AICcmodavg"", ""GGally"", ""lsmeans"")\nlapply(libraries, require, character.only=T) \n\n# used for formatting figures, adjust parameters as necessary\nmanuscript_theme = theme_classic() + theme(axis.line.x = element_line(color=""black"", size = .7),axis.line.y = element_line(color=""black"", size = .7),text=element_text(size=20))\nmanuscript_theme_grid = theme_classic() + theme(axis.line.x = element_line(color=""black"", size = .7),axis.line.y = element_line(color=""black"", size = .7),text=element_text(size=16))\n\n\n#### QC of fastq and aligned bams ####\n## summary stats on sequence quality in R using ShortRead\n# quick quality assessment of sequenced reads, by default using sample of 1 mil reads per file\nfls <- dir(""./"", ""*clip.fastq$"", full=TRUE)\nqaSummary <- qa(fls, type=""fastq"")\nreport(qaSummary,type=""html"",dest=""/personal/snoh/chimera2_redo/bamQAreport"")\n# Comments: ShortRead â€“ base call frequencies suggests a5,ab5,b5 should not be used\n\n## summary stats on alignment from picard CollectAlignmentSummaryMetrics\nbam.stat <- read.table(""all.alignment.stats.txt"", h=T, row.names=1, sep=""\\t"", na.strings ="""")\nsummary(bam.stat)\n\n\n#### Read in count data and generate metadata table ####\n## full dataset including samples to be removed\ncounts.chi <- read.table(""all.gmap.count.txt"", sep=""\\t"", h=T, row.names=1)\n\nsampleTable <- data.frame(shortname=c(""a.3"",""a.4"",""a.5"",""ab.3"",""ab.4"",""ab.5"",""b.3"",""b.4"",""b.5"",""c.1"",""c.2"",""c.3"",""cd.1"",""cd.2"",""cd.3"",""d.1"",""d.2"",""d.3"",""e.1"",""e.2"",""e.3"",""ef.1"",""ef.2"",""ef.3"",""f.1"",""f.2"",""f.3"",""g.1"",""g.2"",""g.3"",""gh.1"",""gh.2"",""gh.3"",""h.1"",""h.2"",""h.3""),\n                          condition=factor(c(rep(c(""clonal"",""clonal"",""clonal"",""chimeric"",""chimeric"",""chimeric"",""clonal"",""clonal"",""clonal""),4)), levels=c(""clonal"",""chimeric"")),\n                          rep=factor(c(rep(""ab"",9),rep(""cd"",9),rep(""ef"",9),rep(""gh"",9))),\n                          batch=factor(c(rep(c(""lane4"",""lane5"",""lane6""),12))),\n                          clone=factor(c(""a"",""a"",""a"",""ab"",""ab"",""ab"",""b"",""b"",""b"",""c"",""c"",""c"",""cd"",""cd"",""cd"",""d"",""d"",""d"",""e"",""e"",""e"",""ef"",""ef"",""ef"",""f"",""f"",""f"",""g"",""g"",""g"",""gh"",""gh"",""gh"",""h"",""h"",""h"")))\n\n## doublecheck outlier status of ab5 replicate with a basic DESeq2 analysis without removing these samples\ndds.all <- DESeqDataSetFromMatrix(countData=counts.chi, colData=sampleTable, tidy=F, design=~batch+rep+condition)\n\ncor.ab <- as.data.frame(cor(assay(dds.all)[,c(1:9)]))\ncor.cd <- as.data.frame(cor(assay(dds.all)[,c(10:18)]))\ncor.ef <- as.data.frame(cor(assay(dds.all)[,c(19:27)]))\ncor.gh <- as.data.frame(cor(assay(dds.all)[,c(28:36)]))\n\n# check how sample read counts relate to each other\nsample.quality <- data.frame(in.ab = c(cor.ab[""a.3"",""a.4""],cor.ab[""b.3"",""b.4""]),\n                             in.cd = c(mean(cor.cd[""c.1"",""c.2""],cor.cd[""c.1"",""c.3""],cor.cd[""c.2"",""c.3""]),mean(cor.cd[""d.1"",""d.2""],cor.cd[""d.1"",""d.3""],cor.cd[""d.2"",""d.3""])),\n                             in.ef = c(mean(cor.ef[""e.1"",""e.2""],cor.ef[""e.1"",""e.3""],cor.ef[""e.2"",""e.3""]),mean(cor.ef[""f.1"",""f.2""],cor.ef[""f.1"",""f.3""],cor.ef[""f.2"",""f.3""])),\n                             in.gh = c(mean(cor.gh[""g.1"",""g.2""],cor.gh[""g.1"",""g.3""],cor.gh[""g.2"",""g.3""]),mean(cor.gh[""h.1"",""h.2""],cor.gh[""h.1"",""h.3""],cor.gh[""h.2"",""h.3""])),\n                             out.ab = c(mean(cor.ab[""a.3"",""a.5""],cor.ab[""a.4"",""a.5""]),mean(cor.ab[""b.3"",""b.5""],cor.ab[""b.4"",""b.5""]))\n)\nsample.quality\n\nmean(unlist(sample.quality[,1:4])) # mean pairwise correlation within each pair across all included samples 0.93\nmean(unlist(sample.quality[,5])) # mean pairwise correlation within ab with excluded samples 0.63\nrm(dds.all, cor.ab, cor.cd, cor.ef, cor.gh, sample.quality)\n\n\n#### Final differential expression analysis after ab5 removal ####\ntemp1 <- counts.chi[,-c(3,6,9)]\ncounts.chi <- temp1\ntemp2 <- sampleTable[-c(3,6,9),]\nsampleTable <- temp2\n\ndds.chi <- DESeqDataSetFromMatrix(countData=counts.chi, colData=sampleTable, tidy=F, design=~batch+rep+condition)\ndds.chi <- estimateSizeFactors(dds.chi)\ndds.chi <- DESeq(dds.chi)\nres.chi <- results(dds.chi)\nres.chi <- res.chi[order(res.chi$padj),]\nsummary(res.chi)\ntopGene <- rownames(subset(res.chi,padj<0.1))\ntopGene.05 <- rownames(subset(res.chi,padj<0.05))\n\n# IMPORTANT doublecheck levels of chimeric vs clonal to interpret foldchange properly\nmcols(res.chi)\n\n# DE genes are slightly different if the version of DESeq2 is different. To recreate the exact analysis we have provided the original list of up- and down- regulated genes \n# export results tables\ndf.res.chi <- as.data.frame(res.chi)\nsummary(df.res.chi) #padj==NA are genes with no counts or those that don\'t pass the minimum baseMean level (4 reads)\nwrite.table(df.res.chi, file=paste(""chimera_deseq2_results"",form']",2,"Wild Dictyostelium discoideum, social amoebae, plastic responses, nonrelatives, multicellular development, strains, microbes, conflict, cell fate, fixed differences, RNA-sequencing, photographic time series analysis, chimer"
Drivers of alloparental provisioning of fledglings in a colonially-breeding bird,"Offspring provisioning represents a major reproductive cost. However, evidence suggests that parents sometimes feed unrelated offspring. Several hypotheses could explain this puzzling phenomenon. Adults could feed unrelated offspring that are (1) of close social associates to facilitate these juveniles' integration into their social network (resulting in social inheritance), (2) potential extra-pair offspring, (3) at a similar developmental stage as their own, (4) coercing feeding by begging, or (5) less-developed and who's enhanced survival would benefit the adult or its own offspring (the group augmentation hypothesis). Colonial breeders are ideal for investigating the relative importance of these hypotheses because offspring are often kept in crches where adults can exhibit allofeeding. Using automated monitoring of replicated captive zebra finch (Taeniopygia guttata) colonies, we found that while parents selectively fed their own offspring, they also consistently fed unrelated offspring (32.48% of feeding events). Social relationships among adults prior to breeding did not predict allofeeding, nor was provisioning directed towards unrelated offspring directed to potential genetic offspring. Instead, adults preferentially fed less-developed non-offspring, despite these not begging more frequently than larger ones did. Our study suggests that allofeeding is consistent with group augmentation, which could be beneficial through colony maintenance or increased offspring survival.","['# feeding data\ndata <- read.csv(""dataQ.csv"",stringsAsFactors=FALSE, sep="","")\n\n\n# chick info\nchicks <- read.csv(""dataChickID_toPublish20200223.csv"",stringsAsFactors=FALSE,sep="","")\n\n\n# parent info\nparents <- read.csv(""dataParentsID.csv"",stringsAsFactors=FALSE,sep="","")\n\n\n# FLEDGE DAY (TO FIX)\nfledge_day <- 17\n\n# only feeding data\n#data <- data[data$fed == ""y"",]\n\n# necessary feeding data\ndata <- data[,which(colnames(data) %in% c(""Day"", ""Aviary"", ""AdultID"", ""fed"", ""Who"", ""Count"",""EventID"", ""IDchicks_AnyTime"", ""Num_NQ.anytime""))]\ndata$Day <- as.numeric(gsub(""-"","""",data$Day))\n\n# cohorts (1) 20180810 to 20180829, (2) 20180926 - 20181020\ndata$Cohort <- NA\ndata$Cohort[which(data$Day >= 20180810 & data$Day <= 20180829)] <- 1\ndata$Cohort[which(data$Day >= 20180926 & data$Day <= 20181020)] <- 2\n\n# remove non-cohort data\ndata <- data[which(data$Cohort %in% c(1,2)), ]\n\n# necessary chick data\n#chicks <- chicks[which(is.na(chicks$deathdate)),]\nchicks <- chicks[,which(colnames(chicks) %in% c(""fosteraviary"",""Cohort"",""infidelity"",""geneticmother"", ""geneticfather"", ""Idfosterbrood"", ""backpack"", ""datebackpack"", ""hatchdate"", ""deathdate""))]\nchicks$datebackpack <- as.numeric(gsub(""-"","""",chicks$datebackpack))\nchicks$hatchdate <- as.numeric(gsub(""-"","""",chicks$hatchdate))\nchicks$deathdate <- as.numeric(gsub(""-"","""",chicks$deathdate))\nchicks$fledgedate <- as.character(as.Date(as.character(chicks$hatchdate), format=""%Y%m%d"") + fledge_day)\nchicks$fledgedate <- as.numeric(gsub(""-"","""",chicks$fledgedate))\n\n# add a date that has either the backpacking (if occurred) OR death (if not backpacked)\nchicks$lastNQ <- chicks$datebackpack\nchicks$lastNQ[is.na(chicks$lastNQ)] <- chicks$deathdate[is.na(chicks$lastNQ)]\n\n# necessary adult data\nparents <- parents[,which(colnames(parents) %in% c(""ind_ID"", ""aviary"", ""qr_code"",""sex""))]\n\n# only males\nparents <- parents[which(parents$sex == ""m""),]\n\n# find offspring of parents\nparents.offspring_c1 <- list()\nparents.offspring_c2 <- list()\nparents$first_backpack_c1 <- NA\nparents$first_backpack_c2 <- NA\nparents$all_backpack_c1 <- NA\nparents$all_backpack_c2 <- NA\nparents$first_c2_chick <- NA\nfor (i in 1:nrow(parents)) {\n\tparents.offspring_c1[[i]] <- as.numeric(chicks$backpack[which(chicks$geneticfather == parents$ind_ID[i] & chicks$Cohort == 1 & chicks$infidelity == 1)])\n\tparents.offspring_c2[[i]] <- as.numeric(chicks$backpack[which(chicks$geneticfather == parents$ind_ID[i] & chicks$Cohort == 2 & chicks$infidelity == 1)])\n\tif (sum(!is.na(parents.offspring_c1[[i]])) > 0) {\n\t\t# first add the other chicks from the same brood\n\t\tparents.offspring_c1[[i]] <- as.numeric(chicks$backpack[which(chicks$Idfosterbrood %in% chicks$Idfosterbrood[which(chicks$backpack %in% parents.offspring_c1[[i]])])])\n\n\t\tparents$first_backpack_c1[i] <- min(chicks$datebackpack[which(chicks$backpack %in% parents.offspring_c1[[i]])], na.rm=T)\n\t\tparents$all_backpack_c1[i] <- max(chicks$lastNQ[which(chicks$backpack %in% parents.offspring_c1[[i]])])\n\t}\n\tif (sum(!is.na(parents.offspring_c2[[i]])) > 0) {\n\t\t# first add the other chicks from the same brood\n\t\tparents.offspring_c2[[i]] <- as.numeric(chicks$backpack[which(chicks$Idfosterbrood %in% chicks$Idfosterbrood[which(chicks$backpack %in% parents.offspring_c2[[i]])])])\n\n\t\tparents$first_backpack_c2[i] <- min(chicks$datebackpack[which(chicks$backpack %in% parents.offspring_c2[[i]])], na.rm=T)\n\t\tparents$all_backpack_c2[i] <- max(chicks$lastNQ[which(chicks$backpack %in% parents.offspring_c2[[i]])])\n\t\tparents$first_c2_chick[i] <- min(chicks$fledgedate[which(chicks$backpack %in% parents.offspring_c2[[i]])], na.rm=T)\n\t}\n}\n\n\n# add metadata to feeding data\ndata$allbackpacked <- FALSE\ndata$anybackpacked <- FALSE\ndata$noc2chicks <- FALSE\n\ndata$offspring_present <- """"\ndata$non_offspring_present <- """"\n\ndata$offspring_fed <- """"\ndata$non_offspring_fed <- """"\n\ndata$n_offspring_present <- 0\ndata$n_non_offspring_present <- 0\ndata$n_unknown_present <- 0\n\ndata$offspring_fed_count <- 0\ndata$nonoffspring_fed_count <- 0\ndata$unknown_fed_count <- 0\ndata$offspring_unique_fed <- 0\ndata$nonoffspring_unique_fed <- 0\ndata$unknown_unique_fed <- 0\n\n# remove data from females\ndata <- data[which(data$AdultID %in% parents$qr_code),]\n\n# loop through non-empty feeds\nfor (i in 1:nrow(data)) {\n\n\tid <- which(parents$qr_code == data$AdultID[i])\n\tch_fed <- as.numeric(strsplit(data$Who[i], "" "")[[1]])\n\tcounts <- as.numeric(strsplit(data$Count[i], "" "")[[1]])\n\tids_present <- as.numeric(strsplit(data$IDchicks_AnyTime[i], "" "")[[1]])\n\n\tif (data$Cohort[i] == 1) {\n\t\toff <- parents.offspring_c1[[id]]\n\t\tif (!is.na(parents$all_backpack_c1[id])) {\n\t\tif (data$Day[i] >= parents$all_backpack_c1[id]) {\n\t\t\tdata$allbackpacked[i] <- TRUE\n\t\t}}\n\t\tif (!is.na(parents$first_backpack_c1[id])) {\n\t\tif (data$Day[i] >= parents$first_backpack_c1[id]) {\n\t\t\tdata$anybackpacked[i] <- TRUE\n\t\t}}\n\t\tif (!is.na(parents$first_c2_chick[id])) {\n\t\t#if (data$Day[i] >= parents$first_c2_chick[id]) {  # check if they ever had c2 chicks\n\t\t\tdata$noc2chicks[i] <- TRUE\n\t\t}#}\n\t} else {\n\t\toff <- pare', 'data <- read.csv(""massChecksChicksCross-foster.csv"", stringsAsFactors=FALSE, sep="";"")\n\n# remove excess columns\ndata <- data[,which(!(colnames(data) %in% c(""X"",""X.1"",""X.2"",""X.3"",""X.4"",""X.5"",""X.6"")))]\n\n# get dates\ndates <- colnames(data)\nwhich.cols.dates <- grep(""X"", dates)\n\n#dates <- dates[which.cols.dates]\ndates <- strptime(dates, format=""X%d.%m.%y"")\n\n# convert hatch date into dates\ndata$hatchdate <- strptime(data$hatchdate, format=""%d.%m.%y"")\n\ndata2 <- data.frame()\n\nfor (i in 1:nrow(data)) {\nfor (j in which.cols.dates) {\n\t\n\tif (!is.na(data[i,j])) {\n\t\tage <- as.numeric(dates[j] - data$hatchdate[i] + 1)\n\t\tweight <- data[i,j]\n\t\tdata2 <- rbind(data2, data.frame(ring=data$RING[i], age=age, weight=weight, stringsAsFactors=FALSE))\n\t}\n}\n}\n\ndata2$age2 <- data2$age^2\n\nplot(data2$age,data2$weight, pch=20, xlab=""Age"", ylab=""Weight"", xlim=c(15,50) ,ylim = c(6,18))\n\nabline(h=10.5, lty=2)\nabline(v=18, lty=2)\n\nmod <- lm(weight ~ age + age2, data=data2)\nnewx <- 18:45\nlines(newx, predict(mod, newdata=list(age=newx, age2=newx^2)), col=""red"", lwd=2)\n\n\n']",2,"1. Offspring provisioning 
2. Alloparental provisioning 
3. Reproductive cost 
4. Fledglings 
5. Colonially-breeding birds 
6. Social network 
7. Social inheritance 
8. Extra-p"
Data from: Quantifying variation in female internal genitalia: no evidence for plasticity response to sexual conflict risk in a seed beetle,"Sexually antagonistic coevolution can drive the evolution of male traits that harm females, and female resistance to those traits. While males have been found to vary their harmfulness to females in response to social cues, whether female resistance traits vary in response to social cues remains to be examined. Among seed beetles, male genital spines harm females during copulation and females might resist male harm via thickening of the reproductive tract walls. Here we develop a novel Micro-CT imaging technique to quantify female reproductive tract thickness in 3-dimensional space. We compared the reproductive tracts of female Callosobruchus maculatus from populations that had evolved under high and low levels of sexual conflict, and for females reared under a social environment that predicted either high or low levels of sexual conflict. We show that neither social environment nor evolutionary history significantly affected reproductive tract thickness. Our novel methodology allows for the measurement of fine-scale changes in the internal female reproductive tract, and will allow future investigations into the internal organs of insects and other animals.","['### Read Data #############################################################\r\ndata <- read.csv(""CT_data.csv"")\r\nrptdata <- read.csv(""Repeatability.csv"")\r\n\r\n### Packages ##############################################################\r\nlibrary(pbkrtest) # Enables Kenward-Roger F-tests\r\nlibrary(lmerTest) # Enables Kenward-Roger F-tests\r\nlibrary(FactoMineR) #PCA package\r\nlibrary(lme4) # Linear models\r\nlibrary(car) # Linear models\r\nlibrary(Hmisc) #Correlations\r\nlibrary(OutlierDetection) # Outlier testing\r\nlibrary(rptR) # Repeatability analysis\r\nlibrary(scales)# Re-scaling of response variable\r\nlibrary(predictmeans)\r\nlibrary(EMAtools)#Cohens d effect size\r\nlibrary(simr)\r\nlibrary(effectsize)\r\nlibrary(nlme)\r\nlibrary(MuMIn)\r\nlibrary(RColorBrewer)\r\n\r\n### Repeatability #########################################################\r\n\r\nUMe <- rpt(U.mean ~ (1|ID), grname = ""ID"", data= rptdata,\r\n           datatype= ""Gaussian"", nboot=10000, npermut=10000)\r\nUMa <- rpt(U.max ~ (1|ID), grname = ""ID"", data= rptdata,\r\n           datatype= ""Gaussian"", nboot=10000, npermut=10000)\r\nUMi <- rpt(U.min ~ (1|ID), grname = ""ID"", data= rptdata,\r\n           datatype= ""Gaussian"", nboot=10000, npermut=10000)\r\n\r\nLMe <- rpt(L.mean ~ (1|ID), grname = ""ID"", data= rptdata,\r\n           datatype= ""Gaussian"", nboot=10000, npermut=10000)\r\nLMa <- rpt(L.max ~ (1|ID), grname = ""ID"", data= rptdata,\r\n           datatype= ""Gaussian"", nboot=10000, npermut=10000)\r\nLMi <- rpt(L.min ~ (1|ID), grname = ""ID"", data= rptdata,\r\n           datatype= ""Gaussian"", nboot=10000, npermut=10000)\r\n\r\nprint(UMe) #R=0.635, CI(0.147,0.856), p=0.00105\r\nprint(UMa) #R=0.604, CI(0.11,0.838), p=0.00199\r\nprint(UMi) #R=0, CI(0,0.4), p=0.5\r\nprint(LMe) #R=0.769, CI(0.368,0.914), p=2.54e-05\r\nprint(LMa) #R=0.741, CI(0.33,0.903), p=6.66e-05\r\nprint(LMi) #R=0, CI(0,0.406), p=1\r\n\r\n# As minimum values for the upper and lower tract are non-repeatable, we will not use them in subsequent analyses.\r\n\r\n#### Correlation Tests ####################################################\r\nnames(data)\r\nvars=c(""U.mean"",""U.max"",""U.min"",""L.mean"",""L.max"",""L.min"")\r\nCorr=data[vars]\r\nmat=as.matrix(Corr)\r\nmode(mat) = ""numeric""\r\nrcorr(mat,type=c(""pearson""))\r\n\r\n#### PCA and Data Compilation #############################################\r\nnames(data)\r\npc.subset <- data[,c(10:11,13:14)]\r\npc.comp <- PCA(pc.subset, scale.unit=TRUE, graph = TRUE)\r\ndata.pca <- cbind(data[,c(1:6)],pc.comp$ind$coord[,1:3]) # Adds loading values for the first three PC\'s\r\n\r\npc.comp$eig # PC eigenvalues\r\npc.comp$var # PC trait loadings\r\n\r\ndata.pca$Bias <- as.factor(data.pca$Bias)\r\ndata.pca$Conflict <- as.factor(data.pca$Conflict)\r\ndata.pca$Voltage <- as.factor(data.pca$Voltage)\r\ndata.pca$Population <- as.factor(data.pca$Population)\r\n\r\n### Re-scale Dim.1 ###\r\n#Data fails to converge with negative eigenvalue\r\ndata.pca$Dim.1 <- rescale(data.pca$Dim.1, to = c(0, 1)) \r\ndata.pca$Dim.2 <- rescale(data.pca$Dim.2, to = c(0, 1)) \r\n\r\n\r\n#### Checking for Outliers################################################\r\n\r\nnames(data.pca)\r\noutlier.data <- data.pca[, c(5, 11)]\r\noutlier.data\r\ndens(outlier.data, k=3, boottimes=50000, rnames=TRUE)\r\n\r\n\r\n#### PC1 Model Reduction #################################################\r\npc1.model1 <- lmer(Dim.1 ~ Conflict+Bias+Weight+(1 + Weight * Conflict|Population) + (1|Voltage)\r\n                   + Conflict * Bias\r\n                   + Conflict * Weight\r\n                   + Bias * Weight, data=data.pca, REML=TRUE)\r\n\r\npc1.model2 <- lmer(Dim.1 ~ Conflict+Bias+Weight+(1 + Weight * Conflict|Population) + (1|Voltage)\r\n                  + Conflict * Bias\r\n                  + Conflict * Weight, data=data.pca, REML=TRUE)\r\n\r\nKR.A1 <- KRmodcomp(pc1.model1, pc1.model2)\r\nsummary(KR.A1) #Non-sig (remove)\r\n\r\npc1.model3 <- lmer(Dim.1 ~ Conflict+Bias+Weight+(1 + Weight * Conflict|Population) + (1|Voltage)\r\n                   + Conflict * Bias, data=data.pca, REML=TRUE)\r\n\r\nKR.A2 <- KRmodcomp(pc1.model2, pc1.model3)\r\nsummary(KR.A2) #Non-sig (remove)\r\n\r\npc1.model4 <- lmer(Dim.1 ~ Conflict+Bias+Weight+(1 + Weight * Conflict|Population) + (1|Voltage), data=data.pca, REML=TRUE)\r\n\r\nKR.A3 <- KRmodcomp(pc1.model3, pc1.model4)# Non-sig (remove)\r\nsummary(KR.A3)\r\n\r\n#Final model results#\r\nanova(pc1.model4, type = c(""III"",""II"",""I""), ddf=""Kenward-Roger"")\r\nsummary(pc1.model4, ddf=""Kenward-Roger"")\r\n\r\n\r\npc1.nlme <- lme(Dim.1 ~ Conflict + Bias + Weight, random = ~1 + Conflict|Population, data=data.pca, method = ""REML"")\r\nanova(pc1.nlme)\r\n\r\n\r\n\r\n# Means, CI\'s and Effect Size ##############################################\r\nemmeans(pc1.model4, ~ Conflict) # Estimated means\r\nemmeans(pc1.model4, ~ Bias)\r\n\r\nStandardized.model <- standardize(pc1.model4)\r\nsummary(Standardized.model, type = c(""III"",""II"",""I""), ddf=""Kenward-Roger"")\r\n\r\nstandardize_parameters(pc1.model4, method = ""refit"", ci=0.95) # Coefficients match those of above, but 95% CI are provided. \r\n\r\n\r\n  \r\n# Standardization method: refit\r\n\r\n#            | Coefficient (std.) |        95% CI\r\n#-----------------------------']",2,"Quantifying variation, female internal genitalia, plasticity response, sexual conflict risk, seed beetle, sexually antagonistic coevolution, male traits, female resistance, harmfulness, social cues, female reproductive tract thickness, Micro-CT imaging technique,"
Social and abiotic factors differentially affect plumage ornamentation of young and old males in an Australian songbird,"Both abiotic environmental conditions and variation in social environment are known to impact the acquisition of sexual signals. However, the influences of abiotic environmental and social factors are rarely compared to each other. Here we test the relative importance of these factors in determining whether and when male red-backed fairywrens (Malurus melanocephalus) moult into a known sexual signal, ornamented breeding plumage. One-year-old male red-backed fairywrens vary in whether or not they acquire ornamentation, whereas males age two and older vary in their timing of ornament acquisition. It is unclear whether these processes are determined by the same or different factors and we examine both events using a combination of long-term breeding records and non-breeding social networks. We found that one-year-old males that paired prior to the start of the breeding season were more likely to acquire ornamented plumage, but rainfall did not influence whether one-year-old males acquired ornamented plumage. Thus, for young individuals, social cues appear to play a larger role than abiotic environmental factors in determining ornament acquisition. For older males, timing of ornamented plumage acquisition was constrained by rainfall, with drier non-breeding seasons leading to poorer physiological condition and later moult dates. Thus, sexual signal variation in older males appears to be a condition-dependent trait, driven by abiotic environmental and physiological factors rather than social cues. These findings reveal that factors influencing sexual signal expression can vary with age when age classes exhibit different forms of signal variation. Our results suggest that social environment may drive sexual signal variation in young individuals, whereas abiotic environmental variation may drive sexual signal variation in older individuals.","### Read Data #############################################################
data <- read.csv(""CT_data.csv"")
rptdata <- read.csv(""Repeatability.csv"")

### Packages ##############################################################
library(pbkrtest) # Enables Kenward-Roger F-tests
library(lmerTest) # Enables Kenward-Roger F-tests
library(FactoMineR) #PCA package
library(lme4) # Linear models
library(car) # Linear models
library(Hmisc) #Correlations
library(OutlierDetection) # Outlier testing
library(rptR) # Repeatability analysis
library(scales)# Re-scaling of response variable
library(predictmeans)
library(EMAtools)#Cohens d effect size
library(simr)
library(effectsize)
library(nlme)
library(MuMIn)
library(RColorBrewer)

### Repeatability #########################################################

UMe <- rpt(U.mean ~ (1|ID), grname = ""ID"", data= rptdata,
           datatype= ""Gaussian"", nboot=10000, npermut=10000)
UMa <- rpt(U.max ~ (1|ID), grname = ""ID"", data= rptdata,
           datatype= ""Gaussian"", nboot=10000, npermut=10000)
UMi <- rpt(U.min ~ (1|ID), grname = ""ID"", data= rptdata,
           datatype= ""Gaussian"", nboot=10000, npermut=10000)

LMe <- rpt(L.mean ~ (1|ID), grname = ""ID"", data= rptdata,
           datatype= ""Gaussian"", nboot=10000, npermut=10000)
LMa <- rpt(L.max ~ (1|ID), grname = ""ID"", data= rptdata,
           datatype= ""Gaussian"", nboot=10000, npermut=10000)
LMi <- rpt(L.min ~ (1|ID), grname = ""ID"", data= rptdata,
           datatype= ""Gaussian"", nboot=10000, npermut=10000)

print(UMe) #R=0.635, CI(0.147,0.856), p=0.00105
print(UMa) #R=0.604, CI(0.11,0.838), p=0.00199
print(UMi) #R=0, CI(0,0.4), p=0.5
print(LMe) #R=0.769, CI(0.368,0.914), p=2.54e-05
print(LMa) #R=0.741, CI(0.33,0.903), p=6.66e-05
print(LMi) #R=0, CI(0,0.406), p=1

# As minimum values for the upper and lower tract are non-repeatable, we will not use them in subsequent analyses.

#### Correlation Tests ####################################################
names(data)
vars=c(""U.mean"",""U.max"",""U.min"",""L.mean"",""L.max"",""L.min"")
Corr=data[vars]
mat=as.matrix(Corr)
mode(mat) = ""numeric""
rcorr(mat,type=c(""pearson""))

#### PCA and Data Compilation #############################################
names(data)
pc.subset <- data[,c(10:11,13:14)]
pc.comp <- PCA(pc.subset, scale.unit=TRUE, graph = TRUE)
data.pca <- cbind(data[,c(1:6)],pc.comp$ind$coord[,1:3]) # Adds loading values for the first three PC's

pc.comp$eig # PC eigenvalues
pc.comp$var # PC trait loadings

data.pca$Bias <- as.factor(data.pca$Bias)
data.pca$Conflict <- as.factor(data.pca$Conflict)
data.pca$Voltage <- as.factor(data.pca$Voltage)
data.pca$Population <- as.factor(data.pca$Population)

### Re-scale Dim.1 ###
#Data fails to converge with negative eigenvalue
data.pca$Dim.1 <- rescale(data.pca$Dim.1, to = c(0, 1)) 
data.pca$Dim.2 <- rescale(data.pca$Dim.2, to = c(0, 1)) 


#### Checking for Outliers################################################

names(data.pca)
outlier.data <- data.pca[, c(5, 11)]
outlier.data
dens(outlier.data, k=3, boottimes=50000, rnames=TRUE)


#### PC1 Model Reduction #################################################
pc1.model1 <- lmer(Dim.1 ~ Conflict+Bias+Weight+(1 + Weight * Conflict|Population) + (1|Voltage)
                   + Conflict * Bias
                   + Conflict * Weight
                   + Bias * Weight, data=data.pca, REML=TRUE)

pc1.model2 <- lmer(Dim.1 ~ Conflict+Bias+Weight+(1 + Weight * Conflict|Population) + (1|Voltage)
                  + Conflict * Bias
                  + Conflict * Weight, data=data.pca, REML=TRUE)

KR.A1 <- KRmodcomp(pc1.model1, pc1.model2)
summary(KR.A1) #Non-sig (remove)

pc1.model3 <- lmer(Dim.1 ~ Conflict+Bias+Weight+(1 + Weight * Conflict|Population) + (1|Voltage)
                   + Conflict * Bias, data=data.pca, REML=TRUE)

KR.A2 <- KRmodcomp(pc1.model2, pc1.model3)
summary(KR.A2) #Non-sig (remove)

pc1.model4 <- lmer(Dim.1 ~ Conflict+Bias+Weight+(1 + Weight * Conflict|Population) + (1|Voltage), data=data.pca, REML=TRUE)

KR.A3 <- KRmodcomp(pc1.model3, pc1.model4)# Non-sig (remove)
summary(KR.A3)

#Final model results#
anova(pc1.model4, type = c(""III"",""II"",""I""), ddf=""Kenward-Roger"")
summary(pc1.model4, ddf=""Kenward-Roger"")


pc1.nlme <- lme(Dim.1 ~ Conflict + Bias + Weight, random = ~1 + Conflict|Population, data=data.pca, method = ""REML"")
anova(pc1.nlme)



# Means, CI's and Effect Size ##############################################
emmeans(pc1.model4, ~ Conflict) # Estimated means
emmeans(pc1.model4, ~ Bias)

Standardized.model <- standardize(pc1.model4)
summary(Standardized.model, type = c(""III"",""II"",""I""), ddf=""Kenward-Roger"")

standardize_parameters(pc1.model4, method = ""refit"", ci=0.95) # Coefficients match those of above, but 95% CI are provided. 


  
# Standardization method: refit

#            | Coefficient (std.) |        95% CI
#------------------------------------------------
#(Intercept) |               0.65 | [-0.17, 1.46]
#ConflictL   |              -0.43 | [-0.92, 0.05]
#BiasMale    |              -0.28 | [-0.76, 0.21]
#Weight      |               0.49 | [ 0.20, 0.78]
",2,"Social factors, abiotic factors, plumage ornamentation, young males, old males, Australian songbird, sexual signals, environmental conditions, acquisition, red-backed fairywrens, moult, breeding plumage, breeding records, non-breeding social"
Data from: Self-organizing dominance hierarchies in a wild primate population,"Linear dominance hierarchies, which are common in social animals, can profoundly influence access to limited resources, reproductive opportunities and health. In spite of their importance, the mechanisms that govern the dynamics of such hierarchies remain unclear. Two hypotheses explain how linear hierarchies might emerge and change over time. The 'prior attributes hypothesis' posits that individual differences in fighting ability directly determine dominance ranks. By contrast, the 'social dynamics hypothesis' posits that dominance ranks emerge from social self-organization dynamics such as winner and loser effects. While the prior attributes hypothesis is well supported in the literature, current support for the social dynamics hypothesis is limited to experimental studies that artificially eliminate or minimize individual differences in fighting abilities. Here, we present the first evidence supporting the social dynamics hypothesis in a wild population. Specifically, we test for winner and loser effects on male hierarchy dynamics in wild baboons, using a novel statistical approach based on the Elo rating method for cardinal rank assignment, which enables the detection of winner and loser effects in uncontrolled group settings. Our results demonstrate (i) the presence of winner and loser effects, and (ii) that individual susceptibility to such effects may have a genetic basis. Taken together, our results show that both social self-organization dynamics and prior attributes can combine to influence hierarchy dynamics even when agonistic interactions are strongly influenced by differences in individual attributes. We hypothesize that, despite variation in individual attributes, winner and loser effects exist (i) because these effects could be particularly beneficial when fighting abilities in other group members change over time, and (ii) because the coevolution of prior attributes and winner and loser effects maintains a balance of both effects.","['\r\n#################################################################################\r\n# functions for fitting the extended Elo rating model ###########################\r\n#################################################################################\r\n\r\n# function to run the extended Elo rating method for a single group and either return the log-likelihood or Elo scores and k values for each individual in an interaction\r\nelo_one_group <- function(par, elo_data, burn_in = 100, select_par = c(1,0,0,0,0,0,0,0,0,0,0), return_elo_scores=F)\r\n{    \r\n  # initialize parameters; selected parameters are initialized with the set value, all other parameters are set to zero \r\n  k_par <- rep(0,11)\r\n  k_par[select_par==1] <- par\r\n\r\n  # get all individuals\r\n  all_ids <- unique(c(as.character(elo_data$Winner),as.character(elo_data$Loser)))\r\n\r\n  # current Elo scores\r\n  currentELO <- rep(0,length(all_ids)); names(currentELO) <- all_ids\r\n  \r\n  # the date of the most recent interaction, needed for recentering Elo scores\r\n  prev_date <- rep(0,length(all_ids)); names(prev_date) <- all_ids\r\n  \r\n  # initialize columns to save Elo scores and k values\r\n  if (return_elo_scores==T) \r\n  {\r\n    elo_data$elo_w <- NA  #Elo score of the winner\r\n    elo_data$elo_l <- NA  #Elo score of the loser\r\n    elo_data$k_w <- NA  #k value of the winner\r\n    elo_data$k_l <- NA  #k value of the loser\r\n  }\r\n  \r\n  L <- 0  #log-likelihood\r\n  \r\n  for(i in 1:nrow(elo_data)) \r\n  {       \r\n    ind1 <- which(names(currentELO)==elo_data$Winner[i])  #Winner in this interaction\r\n    ind2 <- which(names(currentELO)==elo_data$Loser[i])   #Loser in this interaction\r\n    \r\n    p_win <- 1/(1+exp(-.01*(currentELO[ind1] - currentELO[ind2]))) # winning chance of the winner\r\n    \r\n    if (i <= burn_in)   #during burn-in period all k values are fixed to 100\r\n    {\r\n      currentELO[ind1] <- currentELO[ind1] + 100 * (1 - p_win)  #new Elo score of the Winner\r\n      currentELO[ind2] <- currentELO[ind2] - 100 * (1 - p_win)  #new Elo score of the Loser\r\n    }\r\n    else  #after the burn-in period k values are a function of selected predictor variables\r\n    {\r\n      k1 <- exp(sum(k_par[1:2]) + sum(k_par[3:4]) * elo_data$hybrid_w[i] + sum(k_par[5:6]) * elo_data$Age_w[i] + sum(k_par[7:8]) * elo_data$Lag_day_w[i] + sum(k_par[9:10]) * elo_data$aggr_index[i] + k_par[11] * elo_data$hybrid_w[i] * elo_data$aggr_index[i])\r\n      k2 <- exp(k_par[1]        +        k_par[3] * elo_data$hybrid_l[i] +        k_par[5] * elo_data$Age_l[i] +        k_par[7] * elo_data$Lag_day_l[i] +         k_par[9] * elo_data$aggr_index[i] + k_par[11] * elo_data$hybrid_l[i] * elo_data$aggr_index[i])\r\n\r\n      currentELO[ind1] <- currentELO[ind1] + k1 * (1 - p_win) #new Elo score of the Winner\r\n      currentELO[ind2] <- currentELO[ind2] - k2 * (1 - p_win) #new Elo score of the Loser\r\n\r\n      # save Elo scores and k values for output\r\n      if (return_elo_scores==T) \r\n      {\r\n        elo_data$elo_w[i] <- currentELO[ind1]\r\n        elo_data$elo_l[i] <- currentELO[ind2]\r\n        elo_data$k_w[i] <- k1\r\n        elo_data$k_l[i] <- k2  \r\n      }\r\n      else L <- L + log(p_win)  #update log-likelihood \r\n    }\r\n    \r\n    #recenter Elo scores around mean 0, consider only individuals that interacted within the past 90 days\r\n    currentELO[elo_data$date[i] - prev_date <= 90] <- currentELO[elo_data$date[i] - prev_date <= 90] - mean(currentELO[elo_data$date[i] - prev_date <= 90])\r\n\r\n    #update date of last interaction\r\n    prev_date[ind1] <- prev_date[ind2] <- elo_data$Date[i]\r\n    \r\n  }  \r\n  if (return_elo_scores==T) return(elo_data)    #return data frame with added Elo scores and k values\r\n  else return(-1*L)   #return negative log-likelihood\r\n}\r\n\r\n\r\n\r\n\r\n#################################################################################\r\n\r\n# function to combine results of multiple group \r\nelo_multiple_groups <- function(par, ago_data, select_par, return_elo_scores)\r\n{\r\n  groups <- unique(ago_data$Group)  # get all group IDs\r\n  \r\n  if (return_elo_scores) #return Elo scores and k values\r\n  {\r\n    new_data <- data.frame()\r\n    for (i in groups) new_data <- rbind(new_data, elo_one_group(par, subset(ago_data, Group == i), burn_in=100, select_par=select_par, return_elo_scores=return_elo_scores))\r\n    return(new_data)\r\n  }\r\n  else  # return negative log-likelihood\r\n  {\r\n    log_lik <- 0\r\n    for (i in groups) log_lik <- log_lik + elo_one_group(par, subset(ago_data, Group == i), burn_in=100, select_par=select_par, return_elo_scores=F)\r\n    return(log_lik)\r\n  }\r\n}\r\n\r\n\r\nelo_multiple_groups(c(4.6), ago_data = ago_data, select_par = c(1,0,0,0,0,0,0,0,0,0,0), return_elo_scores=F)\r\nres <- elo_multiple_groups(c(4.6,0,0,0,0,0,0,0,0,0,0), ago_data = ago_data, select_par = c(1,1,1,1,1,1,1,1,1,1,1), return_elo_scores=T)\r\n\r\n\r\n\r\n\r\n#################################################################################\r\n# read data file  ###############################################################\r\n##########################################################']",2,"- Self-organizing dominance hierarchies
- Linear dominance hierarchies
- Limited resources
- Reproductive opportunities
- Health
- Prior attributes hypothesis
- Social dynamics hypothesis
- Fighting ability
- Winner and loser effects
"
"Data from: Genetic, maternal, and environmental influences on sociality in a pedigreed primate population","Various aspects of sociality in mammals (e.g., dyadic connectedness) are linked with measures of biological fitness (e.g., longevity). How within- and between-individual variation in relevant social traits arises in uncontrolled wild populations is challenging to determine but is crucial for understanding constraints on the evolution of sociality. We use an advanced statistical method, known as the 'animal model', which incorporates pedigree information, to look at social, genetic, and environmental influences on sociality in a long-lived wild primate. We leverage a longitudinal database spanning 20 years of observation on individually recognized white-faced capuchin monkeys (Cebus capucinus imitator), with a multi-generational pedigree. We analyze two measures of spatial association, using repeat sampling of 376 individuals (mean: 53.5 months per subject, range: 6-185 months per subject). Conditioned on the effects of age, sex, group size, seasonality , and El NioSouthern Oscillation phases, we show low to moderate long-term repeatability (across years) of the proportion of time spent social (posterior mode [95% Highest Posterior Density interval]: 0.207 [0.169, 0.265]) and of average number of partners (0.144 [0.113, 0.181]) (latent scale). Most of this long-term repeatability could be explained by modest heritability (h2social: 0.152 [0.094, 0.207]; h2partners: 0.113 [0.076, 0.149]) with small long-term maternal effects (m2social: 0.000 [0.000, 0.045]; m2partners: 0.000 [0.000, 0.041]). Our models capture the majority of variance in our behavioral traits, with much of the variance explained by temporally changing factors, such as group of residence, highlighting potential limits to the evolvability of our trait due to social and environmental constraints.","['### load packages\nlibrary(MCMCglmm)\nlibrary(viridis)\nlibrary(bayesplot)\nlibrary(coda)\nlibrary(dplyr)\nlibrary(ggpubr)\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(patchwork)\nif (!require(""postMCMCglmm"")) {\n  devtools::install_github(""JWiley/postMCMCglmm"")\n  library(""postMCMCglmm"", force=TRUE) # for extracting fixed effects\n}\n\n###############################################################################\n###############################################################################\n####### Figure 1: Distributions (raw data) #####################################\n###############################################################################\n###############################################################################\n\n### load in original data\nload(""cebus.RData"")\n\ndf <- cebus\ndf$p_soc <- df$social / df$n\ndf$p_par <- df$partners / df$n\n\n###############################################################################\n\n## plot distribution of age, group size, and sociality in population\ntheme_set(theme_classic(base_size = 7, base_family = \'Helvetica\'))\n\n### b) sampling ###\n###################\n\nind1 <- as.data.frame(table(df$id, df$Year)) #sampling across diff years\nind1 <- ind1 %>% filter(Freq > 0)\nind1 <- as.data.frame(table(ind1$Var1))      #calendar years per id\n\nind1$Years <- ""1-4""\nind1$Years[ind1$Freq>4] <- ""5-9""\nind1$Years[ind1$Freq>9] <- ""10+""\nind1$lev <- 1\nind1$lev[ind1$Freq>4] <- 2\nind1$lev[ind1$Freq>9] <- 3\nind1$Years <- factor(ind1$Years, levels = unique(ind1$Years[order(ind1$lev)]))\ntable(ind1$Freq)\n# 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \n# 1 32 52 38 18 33 36 19 22 36 20  9 10 17 10  2  6  2  9  4 \n\n# plot distribution of calendar years per subjects\np3_yrs <- ggplot(data=ind1, aes(Freq, fill=Years)) + \n\tgeom_bar(col=""black"", alpha=.5, size=0.2) +\n\tscale_x_continuous(breaks=seq(0,20,5)) + \n\tlabs(title=""b) Sampling"", \n\t\t\t x=""years per subject"") + \n\tscale_fill_manual(values = c(""grey90"",""grey60"",""grey10"")) + \n\ttheme(axis.text=element_text(size=5), \n\t\t\t\taxis.title.y=element_text(size=5), \n\t\t\t\tlegend.key.size = unit(0.2, \'cm\'), \n\t\t\t\tlegend.position = c(0.9, 0.9))\n(p3_yrs)\n\nind2 <- as.data.frame(table(df$id)) #number of months per individual\nmean(ind2$Freq)  #53.51862\nrange(ind2$Freq) #6 185\n\n### c) group size ###\n#####################\n\np5_grp <- ggplot(data=df, \n\t\t\t\t\t\t\taes(x = grp_size)) + \n\tgeom_bar(col=""black"", \n\t\t\t\t\t fill=""#999999"", \n\t\t\t\t\t size=0.2, \n\t\t\t\t\t alpha=.5) + \n\tscale_x_continuous(breaks=seq(0,50,5)) + \n\tlabs(title=""c) Group size"", \n\t\t\t x=""group size"") +\n\ttheme(axis.text=element_text(size=5), \n\t\t\t\taxis.title.y=element_text(size=5))\n(p5_grp)\n\n# d) tenure length #\n####################\n\nalpha <- as.data.frame(table(df$Group,df$Year))\nalpha <- alpha %>% filter(Freq > 0)\nalpha <- as.data.frame(table(alpha$Var1)) #calendar years per alpha\nmean(alpha$Freq)  #3.571429\nrange(alpha$Freq)  #1 14\n\np6_alp <- ggplot(\n\tdata=alpha, \n\taes(x = Freq)) + \n\tgeom_bar(col=""black"", \n\t\t\t\t\t fill=""#999999"", \n\t\t\t\t\t size=0.2, \n\t\t\t\t\t alpha=.5) + \n\tscale_x_continuous(breaks=seq(1,15,2)) + \n\tscale_y_continuous(breaks=seq(0,15,3)) + \n  labs(title=""d) Alpha tenure length"", \n  \t\t x=""alpha tenure length (years)"") +\n\ttheme(axis.text=element_text(size=5), \n\t\t\t\taxis.title.y=element_text(size=5))\n(p6_alp)\n\n##### combine 3 figures together #####\n#####################################\n\nplt_3 <- plot_grid(p3_yrs, p5_grp, p6_alp, \n               ncol = 1, nrow = 3)\n(plt_3)\n\n##### ##### ##### ##### ##### #####\n\n# generate capuchin image\nlibrary(sketcher)\nimg <- sketcher::im_load(\'MEE.2022.02.19.NSC.IMG_9201.JPG\')\nMEE <- sketcher::sketch(img, \n\t\t\t\t\t\t\t\t\t\t\t\tlineweight = 4, \n\t\t\t\t\t\t\t\t\t\t\t\tsmooth = 0.05)\nplot(MEE)\n\n# plot distributions of sociality scores\np <- ggplot(df, aes(x=p_soc, y=p_par, size=n)) + \n\tgeom_point(alpha=0.1) + scale_size_area(max_size=1) + \n\ttheme_bw(base_size=7, base_family=\'Helvetica\') + \n\txlab(""proportion of time social"") + \n\tylab(""average number of partners"") + \n\tggtitle(""a) Sociality"") +\n\ttheme(axis.text=element_text(size=5),\n\t\t\t\tlegend.position=""none"")\np1 <- ggExtra::ggMarginal(p, type = ""histogram"")\n\nplt_soc <- cowplot::ggdraw() +\n\tdraw_plot(p1) +\n\tdraw_image(plot(MEE), \n\t\t\t\t\t\t x = 0.131, y = 0.53, \n\t\t\t\t\t\t width = 0.4/1.25, height = 0.3/1.25)\n\nFig1 <- plot_grid(plt_soc, plt_3, \n\t\t\t\t\t\t\t\t\tncol = 2, nrow = 1)\npdf(""plots/Fig1_distributionsNew.pdf"", width = 7.08661, height = 3.543305)\nFig1\ndev.off()\n\n###############################################################################\n###############################################################################\n###### Figure 2: Age by Sex interaction plot ######\n###############################################################################\n###############################################################################\n\n### load in data\nload(""cebus.RData"")               #original data\nload(""Models1_fullModels.RData"")  #full models\n\nlogit2prob <- function(logit){\n\todds <- exp(logit)\n\tprob <- odds / (1 + odds)\n\treturn(prob)\n}\n\nxvals <- seq(min(scale(cebus$age)', '### load packages\nlibrary(MCMCglmm)  #loads coda too\nlibrary(parallel)\nlibrary(lattice)\nlibrary(tictoc)\n\n### load data and pedigree\nload(""cebus.RData"")\n\n### ENSO phases\ncebus$ENSO_ <- \'Average/Neutral\'\ncebus$ENSO_[cebus$ENSO<=-0.5] <- \'Cool/La_Niña\'\ncebus$ENSO_[cebus$ENSO>=0.5] <- \'Warm/El_Niño\'\n\n### create sine and cosine of month to capture annual seasonality\nrequire(plyr)\ncebus$rad_12 <- mapvalues(\n\tcebus$Month, \n\tfrom=c(""01/Jan"", ""02/Feb"", ""03/Mar"", ""04/Apr"", ""05/May"",\n\t\t\t\t ""06/Jun"", ""07/Jul"", ""08/Aug"", ""09/Sep"", ""10/Oct"",\n\t\t\t\t ""11/Nov"", ""12/Dec""), \n\tto=seq(12)*(2*pi/12))\ncebus$rad_12 <- as.numeric(cebus$rad_12)\n\n### format factor variables (data)\ncols <- c(""id"", ""animal"", ""Mother"", ""Group"", ""Sex"", ""Year"", ""Month"", ""ENSO_"")\ncebus[cols] <- lapply(cebus[cols], factor)\n\n### format factor variables (pedigree)\ncols <- c(""animal"", ""Mother"", ""Father"")\nPed[cols] <- lapply(Ped[cols], factor)\n\nrm(cols)\n\n###############################################################################\n###############################################################################\n###############################################################################\n\n### NOTE: to set the prior\n# one G structure needs to be specified for each random effect\n# one R structure (for residuals) needs to be specified; \n### univariate models have only one\n\n### Parameter expanded prior ###\n### taken from Tutorial by Pierre de Villemereuil\n### Estimation of a biological trait Heritability using the animal model \n#### and MCMCglmm (version 2), 2021-09-22\n\nprior_m9a <- list(\n\tR=list(V=1, nu=0.002),  \n\tG=list(\n\t\tG1=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),  \n\t\tG2=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG3=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG4=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG5=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG6=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG7=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG8=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG9=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000)))\n\n## weakly informative inverse-Wishart prior (V=1, nu=0.002)\n## performs poorly if variance is close to zero\n## in case better to use a stronger prior (increase nu)\n# lower values for nu create fatter tails \n## less conservative, will sample a broader parameter space\n## danger of weird sampling issues at the tails\nprior_m9b <- list(R=list(V=1, nu=0.002),  \n\t\t\t\t\t\t\t\t\tG=list(G1=list(V=1, nu=0.002),  \n\t\t\t\t\t\t\t\t\t\t\t\t G2=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\t\t\t G3=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\t\t\t G4=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\t\t\t G5=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\t\t\t G6=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\t\t\t G7=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\t\t\t G8=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\t\t\t G9=list(V=1, nu=0.002)))\nprior_m9c <- list(R=list(V=1, nu=0.02),  \n\t\t\t\t\t\t\t\t\tG=list(G1=list(V=1, nu=0.02),  \n\t\t\t\t\t\t\t\t\t\t\t\t G2=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\t\t\t G3=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\t\t\t G4=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\t\t\t G5=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\t\t\t G6=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\t\t\t G7=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\t\t\t G8=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\t\t\t G9=list(V=1, nu=0.02)))\n\n## Priors for poisson models ##\n###############################\n\nk <- 14 # number of fixed effects plus intercept\n\nprior_p9a <- list(\n\tB=list(V=diag(k)*1e7, mu=rep(0,k)),  #priors for fixed effects\n\tR=list(V=1, nu=1),  #prior for response\n\tG=list(\n\t\tG1=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),  \n\t\tG2=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG3=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG4=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG5=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG6=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG7=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG8=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG9=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000)))\n# set strong prior for log of sampling effort\n# Note: k-3 is because log(n) is 3rd to last\nprior_p9a$B$mu[k-3] <- 1 \nprior_p9a$B$V[k-3,k-3] <- 1e-7\n\nprior_p9b <- list(B=list(V=diag(k)*1e7, mu=rep(0,k)),  #priors for fixed effects\n\t\t\t\t\t\t\t\t\tR=list(V=1, nu=0.002),  \n\t\t\t\t\t\t\t\t\tG=list(\n\t\t\t\t\t\t\t\t\t\tG1=list(V=1, nu=0.002),  \n\t\t\t\t\t\t\t\t\t\tG2=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\tG3=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\tG4=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\tG5=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\tG6=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\tG7=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\tG8=list(V=1, nu=0.002),\n\t\t\t\t\t\t\t\t\t\tG9=list(V=1, nu=0.002)))\n# set strong prior for log of sampling effort\nprior_p9b$B$mu[k-3] <- 1 \nprior_p9b$B$V[k-3,k-3] <- 1e-7\n\nprior_p9c <- list(B=list(V=diag(k)*1e7, mu=rep(0,k)),  #priors for fixed effects\n\t\t\t\t\t\t\t\t\tR=list(V=1, nu=0.02),  \n\t\t\t\t\t\t\t\t\tG=list(\n\t\t\t\t\t\t\t\t\t\tG1=list(V=1, nu=0.02),  \n\t\t\t\t\t\t\t\t\t\tG2=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\tG3=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\tG4=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\tG5=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\tG6=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\tG7=list(V=1, nu=0.02),\n\t\t\t\t\t\t\t\t\t\tG8=list(', '### load packages\nlibrary(MCMCglmm)  #also loads coda\nlibrary(parallel)\nlibrary(lattice)\nlibrary(tictoc)\n\n### obtain the data\nload(""objects/cebus.RData"")\n\ncebus$ENSO_ <- \'Average/Neutral\'\ncebus$ENSO_[cebus$ENSO<=-0.5] <- \'Cool/La_Niña\'\ncebus$ENSO_[cebus$ENSO>=0.5] <- \'Warm/El_Niño\'\n\n### create sine and cosine of month to capture annual seasonality\nrequire(plyr)\ncebus$rad_12 <- mapvalues(\n\tcebus$Month, \n\tfrom=c(""01/Jan"", ""02/Feb"", ""03/Mar"", ""04/Apr"", ""05/May"",\n\t\t\t\t ""06/Jun"", ""07/Jul"", ""08/Aug"", ""09/Sep"", ""10/Oct"",\n\t\t\t\t ""11/Nov"", ""12/Dec""), \n\tto=seq(12)*(2*pi/12))\ncebus$rad_12 <- as.numeric(cebus$rad_12)\n\n### format factor variables\ncols <- c(""id"", ""animal"", ""Mother"", ""Group"", ""Sex"", ""Year"", ""Month"", ""ENSO_"")\ncebus[cols] <- lapply(cebus[cols], factor)\n\n### Pedigree\nload(""objects/cebusPed.RData"")\ncols <- c(""animal"", ""Mother"", ""Father"")\nPed[cols] <- lapply(Ped[cols], factor)\n\nrm(cols)\n\n###############################################################################\n###############################################################################\n###############################################################################\n\n### multinomial2 models ###\n\n### NOTE: to set the prior\n# one G structure needs to be specified for each random effect\n# one R structure (for residuals) needs to be specified; bivariate model has two, univariate has one\n\n### Parameter expanded prior ###\n### taken from Tutorial by Pierre de Villemereuil\n### Estimation of a biological trait Heritability using the animal model \n#### and MCMCglmm (version 2), 2021-09-22\n\nprior_m9a <- list(\n\tR=list(V=1, nu=0.002),  \n\tG=list(\n\t\tG1=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),  \n\t\tG2=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG3=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG4=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG5=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG6=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG7=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG8=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),\n\t\tG9=list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000)))\n\n### Intercept only\n\nset.seed(1980)  #The Empire Strikes Back\ntic() \nm09a.0 <- mclapply(1:4, function(i) {\n\tMCMCglmm(\n\t\tcbind(social, n-social) ~ 1,  #intercept only\n\t\trandom = ~ animal + id + id:Year + \n\t\t\tMother + Mother:Group:Year +  \n\t\t\tGroup + Group:Year + \n\t\t\tYear + Month:Year,\n\t\tfamily = ""multinomial2"", \n\t\tdata = cebus, pedigree = Ped, \n\t\tthin = 1000, burnin = 50000, nitt = 2050000,\n\t\tprior = prior_m9a)\n}, mc.cores=4)\ntoc()  #119562.022 sec elapsed\n\n### Age\n\nset.seed(1983)  #Return of the Jedi\ntic() \nm09a.1 <- mclapply(1:4, function(i) {\n\tMCMCglmm(\n\t\tcbind(social, n-social) ~ \n\t\t\tpoly(scale(age), degree = 3, raw = TRUE),  \n\t\trandom = ~ animal + id + id:Year + \n\t\t\tMother + Mother:Group:Year +  \n\t\t\tGroup + Group:Year + \n\t\t\tYear + Month:Year,\n\t\tfamily = ""multinomial2"", \n\t\tdata = cebus, pedigree = Ped, \n\t\tthin = 500, burnin = 50000, nitt = 2050000,\n\t\tprior = prior_m9a)\n}, mc.cores=4)\ntoc()  #146878.393 sec elapsed\n\n### Age*Sex\n\nset.seed(2015)  #The Force Awakens\ntic() \nm09a.2 <- mclapply(1:4, function(i) {\n\tMCMCglmm(\n\t\tcbind(social, n-social) ~ \n\t\t\tpoly(scale(age), degree = 3, raw = TRUE)*Sex, \n\t\trandom = ~ animal + id + id:Year + \n\t\t\tMother + Mother:Group:Year +  \n\t\t\tGroup + Group:Year + \n\t\t\tYear + Month:Year,\n\t\tfamily = ""multinomial2"", \n\t\tdata = cebus, pedigree = Ped, \n\t\tthin = 500, burnin = 50000, nitt = 2050000,\n\t\tprior = prior_m9a)\n}, mc.cores=4)\ntoc()  #56076.345 sec elapsed\n\n### Age*Sex + group size\n\nset.seed(2017)  #The Last Jedi\ntic() \nm09a.3 <- mclapply(1:4, function(i) {\n\tMCMCglmm(\n\t\tcbind(social, n-social) ~ \n\t\t\tpoly(scale(age), degree = 3, raw = TRUE)*Sex + \n\t\t\tscale(grp_size), \n\t\trandom = ~ animal + id + id:Year + \n\t\t\tMother + Mother:Group:Year +  \n\t\t\tGroup + Group:Year + \n\t\t\tYear + Month:Year,\n\t\tfamily = ""multinomial2"", \n\t\tdata = cebus, pedigree = Ped, \n\t\tthin = 500, burnin = 50000, nitt = 2050000,\n\t\tprior = prior_m9a)\n}, mc.cores=4)\ntoc()  #61084.651 sec elapsed\n\n### Age*Sex + group size + seasonality\n\nset.seed(2019)  #The Rise of Skywalker\ntic() \nm09a.4 <- mclapply(1:4, function(i) {\n\tMCMCglmm(\n\t\tcbind(social, n-social) ~ \n\t\t\tpoly(scale(age), degree = 3, raw = TRUE)*Sex + \n\t\t\tscale(grp_size) + \n\t\t\tsin(rad_12) + cos(rad_12), \n\t\trandom = ~ animal + id + id:Year + \n\t\t\tMother + Mother:Group:Year +  \n\t\t\tGroup + Group:Year + \n\t\t\tYear + Month:Year,\n\t\tfamily = ""multinomial2"", \n\t\tdata = cebus, pedigree = Ped, \n\t\tthin = 500, burnin = 50000, nitt = 2050000,\n\t\tprior = prior_m9a)\n}, mc.cores=4)\ntoc()  #65323.258 sec elapsed\n\n###############################################################################\n###############################################################################\n###############################################################################\n\n## weakly informative inverse-Wishart prior (V=1, nu=0.002)\n## performs poorly if variance is close to zero\nprior_m9b <- list(R=list(V=1, nu=0.002),  \n\t\t\t\t\t\t\t\t\tG=lis']",2,"genetics, maternal effects, environment, sociality, mammals, biological fitness, longevity, animal model, white-faced capuchin monkeys, spatial association, repeat sampling, age, sex, group size, seasonality, El Nio-Southern"
Widespread reticulate evolution in an adaptive radiation,"A fundamental assumption of evolutionary biology is that phylogeny follows a bifurcating process. However, hybrid speciation and introgression are becoming more widely documented in many groups. Hybrid inference studies have been historically limited to small sets of taxa, while exploration of the prevalence and trends of reticulation at deep time scales remains unexplored. We study the evolutionary history of an adaptive radiation of 109 gemsnakes in Madagascar (Pseudoxyrhophiinae) to identify potential instances of introgression. Using several network inference methods, we find twelve reticulation events within the 22-million-year evolutionary history of gemsnakes, producing 28% of the diversity for the group, including one reticulation that resulted in the diversification of an 18 species radiation. These reticulations occur at nodes with high gene tree discordance. Hybridization events occurred between north-south distributed parentals that share similar ecologies. Younger hybrids occupy intermediate contact zones between the parentals, showing that post-speciation dispersal in this group has not eroded the spatial signatures of introgression. Reticulations accumulated consistently over time, despite drops in overall speciation rates during the Pleistocene. This suggests that while bifurcating speciation may decline as the result of species accumulation and environmental change, speciation by hybridization may be more robust to these processes.","['##Written by: Dylan DeBaun\nlibrary(\'MSCquartets\')\nlibrary(\'ape\')\nlibrary(\'gtools\')\nlibrary(\'stringr\')\n\nargs = commandArgs(trailingOnly=TRUE)\nout=args[1] #name of the group that we will be looking at\n\n#RUN NANUQ TO CALCULATE PROBABILITIES\n#check for if you already ran this part, if so, don\'t run it again\nif(!file.exists(paste0(out,""z"","".csv""))){\n#read in CF file (built in prep.R)\nu <- read.csv(paste0(out,""_fullindivCFs"","".csv""))\ncolnames(u)[dim(u)[2]-2] = ""12|34""\ncolnames(u)[dim(u)[2]-1] = ""13|24""\ncolnames(u)[dim(u)[2]] = ""14|23""\n}else{\n#read in previously made nanuq file\nu <- read.csv(paste0(out,""z"","".csv""))\ncolnames(u)[dim(u)[2]-4] = ""12|34""\ncolnames(u)[dim(u)[2]-3] = ""13|24""\ncolnames(u)[dim(u)[2]-2] = ""14|23""\n}\n\n#FOR PICKING CHOICE IN ALPHA/BETA\n#after running NANUQ for the first time, to make the output easier to read to choose alpha/beta, concatonate all individuals together (i.e. make it like species level matrix) \n#this assumes your indivdiuals are labelled as such GENUS_SPECIES_* or GENUS_SPECIES_CF_* (assuming the first 2 or 3 words are the species name)\nif(!file.exists(paste0(out,""zedited"","".csv""))){\n  newz <- c(rep(0,dim(z)[1]))\n  i=1\n  while(i != (dim(z)[2]-4)){\n    start = i\n    if(sapply(str_split(colnames(z)[i],""_""),length) == 5){\n      x = 3\n    }else{\n      x = 2\n    }\n    while(isTRUE(word(colnames(z)[i],x,x,sep=""_"")==word(colnames(z)[i+1],x,x,sep=""_""))){\n      i=i+1\n    }\n    end = i\n    sum = rep(0,dim(z)[1])\n    for(j in start:end){\n      sum = z[,j] + sum\n    }\n    newz<- cbind(newz,sum)\n    colnames(newz)[dim(newz)[2]] = word(colnames(z)[start],x,x,sep=""_"")\n    i=i+1\n  }\n  newz<- cbind(newz,z[,(dim(z)[2]-1):(dim(z)[2])])\n  write.csv(newz[,2:dim(newz)[2]],paste0(out,""zedited"","".csv""),row.names = F)\n}\n\n#TO RUN NANUQ W CHOSEN ALPHA/BETAS\n#if you already ran the NANUQ command, you will now have created a list of alpha and beta values you want to test. These should be in csv files with a column labelled either ""alphas"" or ""betas""\nif(file.exists(paste0(out,""z"","".csv""))){\nalpha <- read.csv(paste0(out,""alpha.csv""))\nbeta <- read.csv(paste0(out,""beta.csv""))\n#for every combo of alpha and beta, fun the NANUQ simulation to create the network\nfor(a in 1:dim(alpha)[1]){\n  for(b in 1:dim(beta)[1]){\n          cat(alpha[a,1])\n          z<- NANUQ(as.matrix(u), outfile = paste0(out), alpha =as.numeric(alpha[a,1]), beta =as.numeric(beta[b,1]), plot = TRUE)\n  }\n}\n}else{ #if first time running NANUQ, run it with arbitrary alpha and beta to get the matrix\n  z<- NANUQ(as.matrix(u), outfile = paste0(out), alpha =0.01, beta =0.01, plot = TRUE)\n  write.csv(z,paste0(out,""z"",uninf,"".csv""),row.names=F)\n}\n']",2,"reticulate evolution, adaptive radiation, phylogeny, bifurcating process, hybrid speciation, introgression, hybrid inference, taxa, deep time scales, gemsnakes, Madagascar, Pseudoxyrhophiinae, diversity"
Testing hormonal responses to real and simulated social challenges in a competitive female bird,"Competitive interactions often occur in series; therefore animals may respond to social challenges in ways that prepare them for success in future conflict. Changes in the production of the steroid hormone testosterone (T) are thought to mediate phenotypic responses to competition, but research over the past few decades has yielded mixed results, leading to several potential explanations as to why T does not always elevate following a social challenge. Here, we measured T levels in tree swallows (Tachycineta bicolor), a system in which females compete for limited nesting cavities and female aggression is at least partially mediated by T. We experimentally induced social challenges in two ways: (1) using decoys to simulate territorial intrusions and (2) removing subsets of nesting cavities to increase competition among displaced and territory-holding females. Critically, these experiments occurred pre-laying, when females are physiologically capable of rapidly increasing circulating T levels. However, despite marked aggression in both experiments, T did not elevate following real or simulated social challenges, and in some cases, socially-challenged females had lower T levels than controls. Likewise, the degree of aggression was negatively correlated with T levels following a simulated territorial intrusion. Though not in line with the idea that social challenges prompt T elevation in preparation for future challenges, these patterns nevertheless connect T to territorial aggression in females. Coupled with past work showing that T promotes aggression, these results suggest that T may act rapidly to allow animals to adaptively respond to the urgent demands of a competitive event.","['\n#libraries\nlibrary(ggplot2)\nlibrary(magrittr)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(glmmTMB)\nlibrary(DHARMa)\nlibrary(Rmisc)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(Hmisc)\n\n\n#input Experiment 1 data\n\n#set wd\n\n#set file name\nfn<- ""Exp_1_data.csv""\n#import data\nSTIdata<- read.csv(fn, na.strings=c("""",""NA""))\n\n\n#test that time of day does not affect T\n\ncor.test(STIdata$capture_time_of_day_h, STIdata$logT, method=""spearman"")\n\n# test that latency from capture to bleed does not affect T\n\ncor.test(STIdata$bleed_latency_h, STIdata$logT, method=""spearman"")\n\n#test of decoy and playback IDs on T in the experimental group (supplmental table 1)\n\nSTI<- STIdata %>% filter(Treatment==""STI"")\n\nm<-lm(logT~STI.decoy, data=STI)\nanova(m)\n\nm<-lm(logT~STI.audio, data=STI)\nanova(m)\n\nm<-lm(STI.prop.min.agg~STI.decoy, data=STI)\nanova(m)\n\nm<-lm(STI.prop.min.agg~STI.audio, data=STI)\nanova(m)\n\n## Statistical test for results in Table 1, Model 1\n\n\nm<-lm(logT~Treatment+Mass_g+Age, data=STIdata)\nanova(m)\n\n\n## Statistical test for results in Table 1, Model 2\n\nm<-lm(logT~STI.prop.min.agg+Captime.since.30min.end +Mass_g+Age, data=STI)\nanova(m)\n\n## code for making Figure 2a\n\n\nsum<-summarySE(STIdata, measurevar = ""logT"", groupvars = ""Treatment"", na.rm=T)\nSTI <- STIdata[ which(STIdata$Treatment==\'STI\'), ]\nCTL <- STIdata[ which(STIdata$Treatment==\'Control\'), ]\n\n#create T plot\nconst=0.25\npar(mar=c(4.5,4.5,0.5,0.5), mgp=c(2.5,1.2,0), oma=c(0,0,0,0)) #margins\npar(bty=""l"")\nset.seed(5)\nep=0.5\ncp= 1.5\nstripchart(logT~Treatment,data=STIdata, tck=-0.02, bty=""l"",vertical=T, las=1, xaxt=""n"",\n           cex=0, cex.lab=1.2, bg=""black"", xlim=c(0,2), ylim=c(-3.5,0.5),\n          axes=T,  ylab=""log Testosterone (ng/mL plasma)"", xlab=""Treatment"",\n           group.names=c(""Control"",""STI"")) \naxis(side=1, tck=-0.02, cex.lab=2.5,  at= c(ep, cp), labels=c(""Control \\n"",""Experimental \\n (STI)""))\n\n\npcol=adjustcolor(""darkgray"",alpha.f=0.5)\nmcol= adjustcolor(""darkgray"", alpha.f=0.5)\nrect(ep-const,sum[1,""logT""]-sum[1,""se""],ep+const,sum[1,""logT""]+sum[1,""se""],col=pcol,border=NA)\nrect(cp-const,sum[2,""logT""]-sum[2,""se""],cp+const,sum[2,""logT""]+sum[2,""se""],col=mcol,border=NA)\n\nlines(c(ep-const,ep+const),rep(sum[1,""logT""],2),col=""black"",lwd=2)\nlines(c(cp-const,cp+const),rep(sum[2,""logT""],2),col=""black"",lwd=2)\n\nstripchart(CTL$logT,  tck=-0.01,las=1,vertical=T,\n           method=""jitter"",jitter=0.1, pch=1,  cex=1, \n           add=T, at=ep,\n           axes=T, bty=""l"")\nstripchart(STI$logT, tck=-0.01,las=1,vertical=T,\n           method=""jitter"",jitter=0.1, pch=19, cex=1, \n           add=T, at= cp,\n           axes=T, bty=""l"")\n\n\n### making Figure 2b\n\nggplot(data=STI, aes(x=STI.prop.min.agg, y=logT)) + \n  geom_smooth(method=\'lm\',formula=y~x, color=""black"", se=TRUE)+ \n  xlab(""Proportion of minutes with aggression"")+ ylab(""Log testosterone (ng/mL plasma)"")+ \n  theme_classic()+ theme(text=element_text(size=13))+  geom_point(size=2)\n\n############## Exp 2: effects of treatment on rates of aggression\n\n#load Y1 observational data\n\nfn<-""Exp2_obs_data.csv""\nExp2_obs<- read.csv(fn, na.strings=c("""",""NA""))\n\nExp2_Y1_obs <- Exp2_obs %>% filter(Year==""Y1"")\n\nExp2_Y1_obs$exp_day<-as.factor(Exp2_Y1_obs$exp_day)\n\n#### model to obtain results in Table 2\n\nm<-glmmTMB(physagg_permin_perpair~Treatment*exp_day+start_time_h+(1|Site.block),\n           ziformula=~1,family=ziGamma (link=""log""), data=Exp2_Y1_obs)\nsummary(m)\n\n#check model residuals\nsim_res<-simulateResiduals(m)\nplot(sim_res)\n\n### make figure 3a\n\nEXP <- Exp2_Y1_obs[ which(Exp2_Y1_obs$Treatment==""EXP""), ]\nCTL <- Exp2_Y1_obs[ which(Exp2_Y1_obs$Treatment==""CTL""), ]\n\n\nconst=0.25\npar(mar=c(4.5,4.5,0.5,0.5), mgp=c(3, 1, 0),oma=c(0,0,0,0)) #margins\npar(bty=""l"")\nset.seed(11)\ncp=0.5\nep= 1.5\nstripchart(physagg_permin_perpair~Treatment, data=Exp2_Y1_obs, tck=-0.02, bty=""l"",vertical=T, las=1, xaxt=""n"",\n           cex=0, cex.lab=1.2, bg=""black"", xlim=c(0,2), ylim=c(0,0.26),\n           ylab=""rate of physical aggression"",\n           xlab=""Y1 Treatment"", axes=T,  \n           group.names=c(""Control"",""Experimental"")) \naxis(side=1, tck=-0.02, cex.lab=2.5,  at= c(cp, ep), labels=c("""",""""))\n\nboxplot(physagg_permin_perpair~Treatment, axes = FALSE, outline=FALSE, add=TRUE, at= c(cp, ep),col=""white"", data=Exp2_Y1_obs)\n\nstripchart(CTL$physagg_permin_perpair,  tck=-0.01,las=1,vertical=T,\n           method=""jitter"",jitter=0.3, pch=1,  cex=1, \n           add=T, at=cp,\n           axes=T, bty=""l"", col=alpha(""black"", 1))\nstripchart(EXP$physagg_permin_perpair, tck=-0.01,las=1,vertical=T,\n           method=""jitter"",jitter=0.3, pch=19, cex=1, \n           add=T, at= ep,\n           axes=T, bty=""l"", col=alpha(""black"", 1))\n\n#load Y2 obs data\n\n\nExp2_Y2_obs<- Exp2_obs %>% filter(Year==""Y2"")\n\n#model for Table 2 results, Y2\nm2<-glmmTMB(physagg_permin_perpair~Treatment+start_time_h+ (1|Site.block), ziformula=~1,family=ziGamma (link=""log""),data=Exp2_Y2_obs)\nsummary(m2)\n\n#check residuals\nsim_resm2<-simulateResiduals(m2)\nplot(sim_resm2)\n\n# make figure 3b\n\n\nEXP <- Exp2_Y2_obs[ which(Exp2_Y2_obs$T']",2,"- hormonal responses
- social challenges
- competitive female bird
- steroid hormone testosterone
- phenotypic responses
- tree swallows
- limited nesting cavities
- female aggression
- experimentally induced
- territorial intrusions
-"
Genomic signatures of recent convergent transitions to social life in spiders,comparative genomics of social and nonsocial spiders.,"['require(phangorn)\nrequire(tools)\nsetwd("""")\n\n#create function for pruning alignment from species tree\npruneAlnFromTree = function(alnfile, treefile, type = ""AA"", format = ""fasta"", writealn=TRUE)\n{\n  #prune the alignment to have only the species in the tree\n  #read in the alignment\n  alnPhyDat = read.phyDat(alnfile, type = type, format = format)\n  #read in the treefile\n  genetree = read.tree(treefile)\n  #eliminate species in the alignment but not the tree\n  inboth = intersect(names(alnPhyDat),genetree$tip.label)\n  alnPhyDat = subset(alnPhyDat, subset = inboth)\n  if (writealn) {\n    #write the new alignment with a revised filename\n    fe = file_ext(alnfile)\n    fpse = file_path_sans_ext(alnfile)\n    write.phyDat(alnPhyDat, file=paste(fpse,"".pruned."",fe,sep=""""), format = format)\n  }\n  return(alnPhyDat)\n}\n\n\npruneTreeFromAln = function (treefile, alnfile, type = ""AA"", format = ""fasta"", writetree=TRUE)\n{\n  #prune the tree to have only the species in the alignment\n  #read in the alignment\n  alnPhyDat = read.phyDat(alnfile, type = type, format = format)\n  #read in the treefile\n  genetree = read.tree(treefile)\n  #eliminate species in the alignment but not the tree and vice versa\n  inboth = intersect(names(alnPhyDat),genetree$tip.label)\n  todrop = genetree$tip.label[genetree$tip.label %in% inboth == FALSE]\n  if (length(todrop) > 0) {\n    genetree = drop.tip(genetree, todrop)\n  }\n  #unroot the tree\n  genetree = unroot(genetree)\n  if (writetree) {\n    #write the new tree with a revised filename\n    fe = file_ext(treefile)\n    fpse = file_path_sans_ext(treefile)\n    write.tree(genetree, file=paste(fpse,"".pruned."",fe,sep=""""))\n  }\n  return(genetree)\n}\n\nestimatePhangornTree = function(alnfile, treefile, submodel=""LG"", type = ""AA"",\n                                format = ""fasta"", k=4, ...)\n{\n  alnPhyDat = read.phyDat(alnfile, type = type, format = format)\n  genetree = read.tree(treefile)\n  inboth = intersect(names(alnPhyDat),genetree$tip.label)\n  todropg = genetree$tip.label[genetree$tip.label %in% inboth == FALSE]\n  if (length(todropg) > 0) {\n    genetree = drop.tip(genetree, todropg)\n  }\n  if (length(inboth) < length(names(alnPhyDat))) {\n    alnPhyDat = subset(alnPhyDat, subset = inboth)\n  }\n  genetree = unroot(genetree)\n  genetree$edge.length = c(rep(1,length(genetree$edge.length)))\n  lgptree = pml(genetree, alnPhyDat, model = submodel, k = k, rearrangement=""none"", ...)\n  lgopttree = optim.pml(lgptree,optInv=T,optGamma=T,optEdge=T,rearrangement=""none"",\n                        model=submodel, ...)\n  return(list(""results.init""=lgptree,""results.opt""=lgopttree, ""tree.opt""=lgopttree$tree))\n}\n\nestimatePhangornTreeAll = function(alnfiles=NULL, alndir=NULL, pattern=NULL, treefile, output.file=NULL,\n                                   submodel=""LG"", type = ""AA"", format = ""fasta"", k=4, ...)\n{\n  if(is.null(output.file)){\n    stop(""output.file must be supplied"")\n  }\n  if (is.null(alnfiles)&&is.null(alndir)){\n    stop(""Either alnfiles or alndir must be supplied"")\n  }\n\n  if (!is.null(alnfiles)&&!is.null(alndir)){\n    stop(""Only one of alnfiles or alndir must be supplied"")\n  }\n\n  if (!is.null(alndir)){\n    alnfiles=list.files(path=alndir, pattern=pattern, full.names = TRUE)\n  }\n  names=basename(alnfiles)\n  names=sub(""\\\\.\\\\w*$"", """", names)\n\n  treesL=vector(mode = ""list"", length = length(names))\n  for(i in 1:length(alnfiles)){\n   message(paste(""Processing"", alnfiles[i]))\n tree.res=estimatePhangornTree(alnfiles[i], treefile, submodel=submodel, type = type,\n                                          format = format, k=k, ...)\n   treesL[[i]]=tree.res$tree\n  }\n  fc=file(output.file, ""wt"")\n  for(i in 1:length(alnfiles)){\n  writeLines(paste(names[i], write.tree(treesL[[i]]), sep = ""\\t""), fc)\n  }\n  close(fc)\n}\n\nestimatePhangornTreeAll(treefile=""phylogeny_spider25.txt"",alndir=""/"",output.file=""output.tre"",pattern=""*.pruned.fas"")\n', '\n#install.packages(""remotes"")\n#library(remotes)\n#remotes::install_github(""nclark-lab/RERconverge"")\n\nlibrary(RERconverge)\nlibrary(topGO)\nsource(""Permulationfun.R"")\n\nsetwd("""")\n#setwd("""")\n\nspidertreefile = ""branch_length_spider25.txt""\nspidertrees = readTrees(spidertreefile, max.read = 7540)\n\nspidernames <- c(""r1"",""r2"",""r3"",""r4"",""r5"",""r6"",""r7"",""r8"",""Smim"",""Sdum"",""r20"",""r21"",""r12"",""r13"",""r14"",""r15"",""r16"",""r22"",""r23"",""r17"",""r18"",""r19"",""r24"",""r25"",""Agen"")\nspiderRER = getAllResiduals(spidertrees, useSpecies = spidernames, transform = ""sqrt"", weighted = T, scale = T, min.sp = 3)\n\n#saveRDS(spiderRER, file = ""spiderRER.rds"")\n#newspiderRER =  readRDS(""spiderRER.rds"")\n#multirers = returnRersAsTreesAll(spidertrees,spiderRER)\n#write.tree(multirers, file=\'spiderRER.nwk\', tree.names=TRUE)\n\n#generate a binary tree from a vector of foreground species using foreground2Tree\nsocial.foreground = c(""r1"",""r2"",""r4"",""r5"",""r6"",""Smim"",""Sdum"",""r12"",""r17"")\nsisters.social=list(""clade1""=c(""r1"",""r2""))\nsocial.Fg.tree = foreground2TreeClades(social.foreground, sisters.social, spidertrees,plotTree=F)\n\nphenvsocial=tree2PathsClades(social.Fg.tree, spidertrees)\n\n#min.sp: the minimum number of species in the gene tree for that gene to be included in the analysis.\n#The default is ""10"", but you may wish to modify it depending upon the number of species in your mastertree.\n#min.pos: the minimum number of independent foreground (non-zero) lineages represented in the gene tree\n#for that gene to be included in the analysis. The default is 2, requiring at least two foreground\n#lineages to be present in the gene tree.\ncorsocial=correlateWithBinaryPhenotype(spiderRER, phenvsocial, min.sp=3, min.pos=2,weighted=""auto"")\n\n#head(corsocial[order(corsocial$P),])\n#write.csv((corsocial[order(corsocial$P),]), file = ""RER_spider_N24_7539_OGG_Phangorn.csv"")\n\n\n#creating a gene2GO list from the list of 7590 shared orthologs, where there is a gene name and list of GO terms\ngene.BPGO.list=read.csv(""go_annotation.csv"")\ngene.BPGO=strsplit(gene.BPGO.list$go_id,"","")\nnames(gene.BPGO)=gene.BPGO.list$OGG_ID\nGO2geneID=inverseList(gene.BPGO)\n\nannot=list(""GO""=list(""genesets""=GO2geneID,""geneset.names""=names(GO2geneID)))\n\n\n#getStat converts correlation results to Rho-signed negative log p-values, removes NA values, and returns a named numeric vector, with names \n#corresponding to rownames of correlation results (i.e. gene names)\n#[ see https://cdn.rawgit.com/nclark-lab/RERconverge/master/vignettes/FullWalkthroughUTD.html  ""Deriving a ranked gene list"" ]\nstats=getStat(corsocial)\n\nenrichment=fastwilcoxGMTall(stats, annot, outputGeneVals=T, num.g=10)\n\n\nroot_sp = ""Agen""\nmasterTree = spidertrees$masterTree\n\n\nperms = getPermsBinary(numperms=10000, fg_vec=social.foreground, sisters_list=sisters.social, root_sp=root_sp, \n                       RERmat=spiderRER, trees=spidertrees,mastertree=masterTree,permmode=""cc"",\n                       min.pos=2,calculateenrich=T,annotlist=annot)\n\n\n\ncorpermpvals=permpvalcor(corsocial,perms)\nenrichpermpvals=RERconverge::permpvalenrich(enrichment, perms)\n\nres=corsocial\n\n# add permulations to real results\nres$permpval=corpermpvals[match(rownames(res), names(corpermpvals))]\nres$permpvaladj=p.adjust(res$permpval, method=""BH"")\ncount=1\nwhile(count<=length(enrichment)){\n  enrichment[[count]]$permpval=enrichpermpvals[[count]][match(rownames(enrichment[[count]]),\n                                                              names(enrichpermpvals[[count]]))]\n  enrichment[[count]]$permpvaladj=p.adjust(enrichment[[count]]$permpval, method=""BH"")\n  count=count+1\n}\n\nwrite.csv(res, file = ""gene_RER_output.csv"")\nwrite.csv(enrichment, file = ""GO_RER_output.csv"")\n\n\n#plot correlations and check Spearman rank correlations between p.adj and perm p, etc.\n#plot(enrichment$GO$permpval,enrichment$GO$p.adj)\n#plot(res$p.adj,res$permpval)\n\n#cor.test(res$P,res$permpval,test=""spearman"")\n#cor.test(res$p.adj,res$permpval,test=""spearman"")\n#cor.test(enrichment$GO$permpval,enrichment$GO$pval,test=""spearman"")\n#cor.test(enrichment$GO$permpval,enrichment$GO$p.adj,test=""spearman"")\n', 'library(topGO)\nsetwd(""path to file"")\n\ngene.BPGO.list=read.csv(""{go_annotation.csv}"")\ngene.BPGO=strsplit(gene.BPGO.list$go_id,"","")\nnames(gene.BPGO)=gene.BPGO.list$OGG_ID\n\ngeneNames=names(gene.BPGO)\ngene.list=read.csv(""{gene.csv}"")\nsig.list=subset(gene.list.gT)$OGG_ID\n\nGOfunc <- function(myInterestingGenes){\n  geneList <- factor(as.integer(geneNames %in% myInterestingGenes))\n  names(geneList) <- geneNames\n  GOdata <- new(""topGOdata"", ontology = ""BP"", allGenes = geneList,\n                annot = annFUN.gene2GO, gene2GO = gene.BPGO, nodeSize=10)\n  resultFisher <- runTest(GOdata, algorithm = ""classic"", statistic = ""fisher"")\n  tab <- GenTable(GOdata,resultFisher, topNodes = 500,numChar=500)\n  sig.tab=subset(tab,as.numeric(result1)<0.05)\n  return(sig.tab)\n}\n\nGOfunc(sig.list)\n\ngene_result=GOfunc(sig.list)\nwrite.csv(gene_result, file = ""{output_enriched_GO.csv}"")\n']",2,"genomics, signatures, recent, convergent, transitions, social life, spiders, comparative, social, nonsocial."
Data from: The repeatable opportunity for selection differs between pre- and post-copulatory fitness components,"In species with multiple mating, intense sexual selection may occur both before and after copulation. However, comparing the strength of pre- and postcopulatory selection is challenging, because i) postcopulatory processes are generally difficult to observe and ii) the often-used opportunity for selection (I) metric contains both deterministic and stochastic components. Here, we quantified pre- and postcopulatory male fitness components of the simultaneously hermaphroditic flatworm, Macrostomum lignano. We did this by tracking fluorescent spermusing transgenicsthrough the transparent body of sperm recipients, enabling to observe postcopulatory processes in vivo. Moreover, we sequentially exposed focal worms to three independent mating groups, and in each assessed their mating success, sperm-transfer efficiency, sperm fertilising efficiency, and partner fecundity. Based on these multiple measures, we could, for each fitness component, combine the variance (I) with the repeatability (R) in individual success to assess the amount of repeatable variance in individual successa measure we call the repeatable opportunity for selection (IR). We found higher repeatable opportunity for selection in sperm-transfer efficiency and sperm fertilising efficiency compared to mating success, which clearly suggests that postcopulatory selection is stronger than precopulatory selection. Our study demonstrates that the opportunity for selection contains a repeatable deterministic component, which can be assessed and disentangled from the often large stochastic component, to provide a better estimate of the strength of selection.","['## Source R script associated to the manuscript entitled \r\n## ""The repeatable opportunity for selection differs between pre- and postcopulatory fitness components"" \r\n## by L. Marie-Orleach, N. Vellnow, and L. Schärer\r\n\r\n\r\n## Function to relativise a data column\r\nREL <- function (x) { x/mean(x, na.rm=TRUE) }\r\n\r\n\r\n## Function to compute the fitness components (as explained on Figure 1).\r\nFITCOMP <- function(dataset) {\r\n  dataset$mRS <- dataset$focal_offspring # mRS\r\n  dataset$F   <- dataset$total_offspring # F\r\n  dataset$MS  <- dataset$focal_matings / dataset$total_matings # MS\r\n  dataset$STE <- (dataset$focal_sperm / dataset$total_sperm) / (dataset$focal_matings / dataset$total_matings) # STE\r\n  dataset$SFE <- (dataset$focal_offspring / dataset$total_offspring) / (dataset$focal_sperm / dataset$total_sperm) # SFE\r\n  dataset$SFE[which(!is.finite(dataset$SFE))] <- NaN\r\n  \r\n  return(dataset)\r\n}\r\n\r\n\r\n## Function to compute the standard variance in a fitness component\r\nSTDVAR <- function(fit.comp, dataset) {\r\n  var(s(dataset[,fit.comp])/mean(s(dataset[,fit.comp]), na.rm=TRUE)) \r\n}\r\n\r\n\r\n## Function to estimate the binomial sampling error arising from STE and SFE\r\n## see Pelissie et al, Evolution, 2012 (Appendix A) and Marie-Orleach et al, Evolution, 2016 (Supp. Info. E) for more details\r\nBSE <- function(fit.comp, dataset) {\r\n  if(fit.comp==""STE""){\r\n    dataset$V_STS_exp <- ((dataset$focal_sperm/dataset$total_sperm) * (1 - (dataset$focal_sperm/dataset$total_sperm))) / (dataset$MS^2) # expected variance in each STS estimate\r\n    bse_STE_rel <- (sum(dataset$V_STS_exp) / (sum(dataset$total_sperm))) / (mean(dataset$STE)^2) # binomial sampling error in STE\r\n    return (bse_STE_rel)\r\n  }\r\n  \r\n  if(fit.comp==""SFE""){\r\n    dataset$STS       <- dataset$focal_sperm / dataset$total_sperm # sperm transfer success\r\n    dataset$V_PS_exp  <- (dataset$focal_offspring / dataset$total_offspring) * (1 - (dataset$focal_offspring /dataset$total_offspring)) # expected variance in each PS estimate\r\n    dataset$V_SFE_exp <- dataset$V_PS_exp / (dataset$STS^2) # expected variance in each SFE estimate\r\n    dataset_147 <- subset(dataset, focal_sperm!=0) # subset\r\n    bse_SFE_rel <- (sum(dataset_147$V_SFE_exp) / sum(dataset_147$total_offspring)) / (mean(dataset_147$SFE)^2) # binomial sampling error in SFE\r\n    return (bse_SFE_rel)\r\n  }\r\n}\r\n\r\n\r\n## Function to compute the standard covariance between two fitness components\r\nSTDCOV <- function (fit.comp1, fit.comp2, dataset){\r\n  if (fit.comp1==""SFE"" | fit.comp2==""SFE"") { dataset <- dataset[complete.cases(dataset),] }\r\n  CovArray <- (REL(dataset[,fit.comp1])-mean(REL(dataset[,fit.comp1]), na.rm=TRUE)) * (REL(dataset[,fit.comp2])-mean(REL(dataset[,fit.comp2]), na.rm=TRUE))\r\n  return (mean(CovArray))\r\n}\r\n\r\n\r\n## Function to compute the total variance explained by our model\r\nTOTVAR <- function(dataset){\r\n  stdvar_F   <- STDVAR (""F""  , dataset)\r\n  stdvar_MS  <- STDVAR (""MS"" , dataset)\r\n  stdvar_STE <- STDVAR (""STE"", dataset)\r\n  stdvar_SFE <- STDVAR (""SFE"", dataset)\r\n  \r\n  bse_STE <- BSE (""STE"", dataset)\r\n  bse_SFE <- BSE (""SFE"", dataset)\r\n  \r\n  stdcov_F.MS    <- STDCOV (""F"",   ""MS"",  dataset)\r\n  stdcov_F.STE   <- STDCOV (""F"",   ""STE"", dataset)\r\n  stdcov_F.SFE   <- STDCOV (""F"",   ""SFE"", dataset)\r\n  stdcov_MS.STE  <- STDCOV (""MS"",  ""STE"", dataset)\r\n  stdcov_MS.SFE  <- STDCOV (""MS"",  ""SFE"", dataset)\r\n  stdcov_STE.SFE <- STDCOV (""STE"", ""SFE"", dataset)\r\n  \r\n  return(stdvar_F + stdvar_MS + (stdvar_STE-bse_STE) + (stdvar_SFE-bse_SFE) +\r\n    (2*stdcov_F.MS) + (2*stdcov_F.STE) + (2*stdcov_F.SFE) + (2*stdcov_MS.STE) + (2*stdcov_MS.SFE) + (2*stdcov_STE.SFE) +\r\n    bse_STE + bse_SFE)\r\n}\r\n\r\n\r\n## Function to bootstrapp the variances and covariances to assess their 95 CI\r\n## Outcomes are here expressed as percentages of total variance\r\nBOOT_VARCOVAR <- function (dataset, iteration){\r\n  N <- NROW(dataset)\r\n  \r\n  stor.data <- data.frame(stdvar_mRS=numeric(iteration), stdvar_F=numeric(iteration), stdvar_MS=numeric(iteration), stdvar_STE=numeric(iteration), stdvar_SFE=numeric(iteration),\r\n                        bse_STE=numeric(iteration), bse_SFE=numeric(iteration),\r\n                        stdcov_F.MS=numeric(iteration), stdcov_F.STE=numeric(iteration), stdcov_F.SFE=numeric(iteration), stdcov_MS.STE=numeric(iteration), stdcov_MS.SFE=numeric(iteration), stdcov_STE.SFE=numeric(iteration))\r\n  \r\n  for(i in 1:iteration){\r\n    rand = sample(1:N, N, replace=TRUE)\r\n    dataset$nb <- 1:N\r\n    newdataset = dataset[match(rand, dataset$nb), ]\r\n    \r\n    stdvar_mRS <- STDVAR (""mRS"", newdataset)\r\n    stdvar_F   <- STDVAR (""F""  , newdataset)\r\n    stdvar_MS  <- STDVAR (""MS"" , newdataset)\r\n    stdvar_STE <- STDVAR (""STE"", newdataset)\r\n    stdvar_SFE <- STDVAR (""SFE"", newdataset)\r\n    \r\n    bse_STE <- BSE (""STE"", newdataset)\r\n    bse_SFE <- BSE (""SFE"", newdataset)\r\n    \r\n    stdcov_F.MS    <- STDCOV (""F"",   ""MS"",  newdataset)\r\n    stdcov_F.STE   <- STDCOV (""F"",   ""STE"", newdataset)\r\n    stdcov_F.SFE   <- STDCOV (""F"",   ""SFE"", newdataset)\r\n', '## Main R script associated to the manuscript entitled \r\n## ""The repeatable opportunity for selection differs between pre- and postcopulatory fitness components"" \r\n## by L. Marie-Orleach, N. Vellnow, and L. Schärer\r\n\r\nlibrary(rptR) #for rptR::rpt\r\nlibrary(hablar) #for hablar::s\r\nlibrary(lmerTest) #for lmerTest::lmer\r\n\r\nsource("".../Marie-Orleach.et.al_R.SCRIPT_FUNCTIONS.R"")\r\niteration <- 10000\r\n\r\n#### 1. variance and covariance in mRS and all four fitness components ####\r\ndataset_focal <- read.delim("".../Marie-Orleach.et.al_DATASET.FOCAL.txt"")\r\ndataset_focal <- FITCOMP (dataset_focal)\r\n\r\n#### 1.1 standard variance in mRS and all four fitness components ####\r\nstdvar_mRS <- STDVAR (""mRS"", dataset_focal)\r\nstdvar_F   <- STDVAR (""F""  , dataset_focal)\r\nstdvar_MS  <- STDVAR (""MS"" , dataset_focal)\r\nstdvar_STE <- STDVAR (""STE"", dataset_focal)\r\nstdvar_SFE <- STDVAR (""SFE"", dataset_focal)\r\n \r\n#### 1.2 binomial sampling error in STE and SFE ####\r\nbse_STE <- BSE (""STE"", dataset_focal)\r\nbse_SFE <- BSE (""SFE"", dataset_focal)\r\n\r\n#### 1.3 covariances ####\r\nstdcov_F.MS    <- STDCOV (""F"",   ""MS"",  dataset_focal)\r\nstdcov_F.STE   <- STDCOV (""F"",   ""STE"", dataset_focal)\r\nstdcov_F.SFE   <- STDCOV (""F"",   ""SFE"", dataset_focal)\r\nstdcov_MS.STE  <- STDCOV (""MS"",  ""STE"", dataset_focal)\r\nstdcov_MS.SFE  <- STDCOV (""MS"",  ""SFE"", dataset_focal)\r\nstdcov_STE.SFE <- STDCOV (""STE"", ""SFE"", dataset_focal)\r\n\r\n#### 1.4 total variance ####\r\ntotvar <- TOTVAR (dataset_focal)\r\n\r\n#### 1.5 variance and covariance bootstrap (95% CIs) ####\r\nboot_varcovar_outcome <- BOOT_VARCOVAR (dataset_focal, iteration)\r\n\r\n#### 1.6 variance pairwise comparisons (P values) ####\r\npvar_F.MS    <- PWCOMP_VAR (""F"",   ""MS"",  dataset_focal, iteration)\r\npvar_F.STE   <- PWCOMP_VAR (""F"",   ""STE"", dataset_focal, iteration)\r\npvar_F.SFE   <- PWCOMP_VAR (""F"",   ""SFE"", dataset_focal, iteration)\r\npvar_MS.STE  <- PWCOMP_VAR (""MS"",  ""STE"", dataset_focal, iteration)\r\npvar_MS.SFE  <- PWCOMP_VAR (""MS"",  ""SFE"", dataset_focal, iteration)\r\npvar_STE.SFE <- PWCOMP_VAR (""STE"", ""SFE"", dataset_focal, iteration)\r\n\r\n\r\n\r\n#### 2. repeatability in mRS and all four fitness components ####\r\ndataset_group <- read.delim("".../Marie-Orleach.et.al_DATASET.MATING.GROUP.txt"")\r\ndataset_group <- FITCOMP (dataset_group) \r\n\r\n## /!\\ warnings \'boundary (singular) fit: see ?isSingular\' arise when the random effect (1|focal) explains 0 variance.\r\n## /!\\ These warnings arise for F, and for the permutation tests of all fitness components (which is expected).\r\n## /!\\ These warnings appear in all steps in which repeatability is assessed (3.2 & 3.3)\r\nrpt_mRS <- RPT (""mRS"", TRANS_mRS, dataset_group, iteration) \r\nrpt_F   <- RPT (""F"",   TRANS_F,   dataset_group, iteration)\r\nrpt_MS  <- RPT (""MS"",  TRANS_MS,  dataset_group, iteration)\r\nrpt_STE <- RPT (""STE"", TRANS_STE, dataset_group, iteration)\r\nrpt_SFE <- RPT (""SFE"", TRANS_SFE, dataset_group, iteration)\r\n\r\n\r\n\r\n#### 3. repeatable variance in mRS and all four fitness components ####\r\ndataset_focal <- read.delim("".../Marie-Orleach.et.al_DATASET.FOCAL.txt"")\r\ndataset_focal <- FITCOMP (dataset_focal)\r\ndataset_group <- read.delim("".../Marie-Orleach.et.al_DATASET.MATING.GROUP.txt"")\r\ndataset_group <- FITCOMP (dataset_group) \r\n\r\n#### 3.1 repeatable variance ####\r\n## /!\\ missing data are explained in the Methods of the article\r\nrptvar_mRS <- RPTVAR (""mRS"", TRANS_mRS, dataset_focal, dataset_group)\r\nrptvar_F   <- RPTVAR (""F"",   TRANS_F,   dataset_focal, dataset_group)\r\nrptvar_MS  <- RPTVAR (""MS"",  TRANS_MS,  dataset_focal, dataset_group)\r\nrptvar_STE <- RPTVAR (""STE"", TRANS_STE, dataset_focal, dataset_group)\r\nrptvar_SFE <- RPTVAR (""SFE"", TRANS_SFE, dataset_focal, dataset_group)\r\n\r\n#### 3.2 repeatable variance bootstrap (95% CIs) ####\r\nboot_rptvar_outcome <- BOOT_RPTVAR (dataset_focal, dataset_group, iteration)\r\n\r\n#### 3.3 pairwise comparisons in repeatable variance ####\r\nprptvar_F.MS    <- PWCOMP_RPTVAR (""F"",   ""MS"",  TRANS_F,   TRANS_MS,  dataset_focal, dataset_group, iteration)\r\nprptvar_F.STE   <- PWCOMP_RPTVAR (""F"",   ""STE"", TRANS_F,   TRANS_STE, dataset_focal, dataset_group, iteration)\r\nprptvar_F.SFE   <- PWCOMP_RPTVAR (""F"",   ""SFE"", TRANS_F,   TRANS_SFE, dataset_focal, dataset_group, iteration)\r\nprptvar_MS.STE  <- PWCOMP_RPTVAR (""MS"",  ""STE"", TRANS_MS,  TRANS_STE, dataset_focal, dataset_group, iteration)\r\nprptvar_MS.SFE  <- PWCOMP_RPTVAR (""MS"",  ""SFE"", TRANS_MS,  TRANS_SFE, dataset_focal, dataset_group, iteration)\r\nprptvar_STE.SFE <- PWCOMP_RPTVAR (""STE"", ""SFE"", TRANS_STE, TRANS_SFE, dataset_focal, dataset_group, iteration)\r\n\r\n\r\n\r\n#### 4. group and batch effects ####\r\ndataset_group <- read.delim("".../Marie-Orleach.et.al_DATASET.MATING.GROUP.txt"")\r\ndataset_group <- FITCOMP (dataset_group) \r\n\r\nm_mRS <- lmer(REL(TRANS_mRS(mRS)) ~  (1|focal) + as.factor(mating_group) + as.factor(batch) + as.factor(mating_group)*as.factor(batch), data=dataset_group)\r\nm_F   <- lmer(REL(TRANS_F(F))     ~  (1|focal) + as.factor(mating_group) + as.factor(batch) + as.factor(mating_group)*as.factor(batch), data=dat']",2,"Selection, Pre-copulatory fitness, Post-copulatory fitness, Sexual selection, Opportunity for selection, Macrostomum lignano, Fluorescent sperm, Transgenics, Sperm recipients, In vivo, Mating success, Sperm"
Datasets and R code for: Brood as booty: The effect of colony size and resource value in social insect contests,"From the Manuscript: Animals engage in contests for access to resources like food, mates, and space. Intergroup contests between groups of organisms have received little attention, and it remains unresolved what information groups might use collectively to make contest decisions. We staged whole-colony contests using ant colonies (Temnothorax rugatulus), which perceive conspecific colonies as both a threat and resource from which to steal brood. We recorded individual behaviors and used demographic characteristics as proxies for resource value (number of brood items) and fighting ability (number of workers). We found that ants altered their fighting effort depending on the relative number of workers of their opponent. While the proximate mechanism for this ability remains uncertain, we found that colonies increased fighting when their opponent had relatively more brood, but not if opposing colonies had relatively many more workers. This suggests that ant colonies can use information about opposing colonies that shapes contest strategies. Further, the behavior of opposing colonies were strongly correlated with each other despite colony size differences ranging from 4-51%, consistent with the hypothesis that colonies can use opponent information. The behavior of a distributed, collective system of many individuals, like a eusocial insect colony, thus fits several predictions of contest models designed for individuals if we consider the gain and loss of worker ants analogous to energetic costs accrued during typical dyadic contests.","['### R script for the publication: Brood as booty: the effect of colony size and resource value in social insect contests\r\n### Authors: Kenneth James Chapin, Victor Alexander Paat, Anna Dornhaus\r\n### Contact: chapinkj@gmail.com\r\n\r\n##### Packages\r\nrequire(effects); \r\n##### LOAD DATA\r\na <- read.csv(\'Chapin_Paat_Dornhaus_data.csv\'); \r\n#obs <- read.csv(\'Chapin_Paat_Dornhaus_observationaldata.csv\') \r\n\r\ncolnames(a)\r\nfor(ii in 1:length(a$workersl)){\r\n        if(a$workersl[ii] > a$workersr[ii]){ # convert ""left"" and ""right"" colonies to large (L) and small (S).\r\n                a$workersLG[ii]  <- a$workersl[ii]\r\n                a$workersSM[ii]  <- a$workersr[ii]\r\n                a$broodLG[ii]    <- a$broodl[ii] # note that ""larger"" colonies may have fewer brood\r\n                a$broodSM[ii]    <- a$broodr[ii]\r\n                a$fightsSM[ii]   <- a$fightr[ii]\r\n                a$fightsLG[ii]   <- a$fightl[ii]\r\n                a$entranceSM[ii] <- a$entrr[ii]\r\n                a$entranceLG[ii] <- a$entrl[ii]\r\n        }else{\r\n                a$workersSM[ii]  <- a$workersl[ii]\r\n                a$workersLG[ii]  <- a$workersr[ii]\r\n                a$broodLG[ii]    <- a$broodr[ii]\r\n                a$broodSM[ii]    <- a$broodl[ii]\r\n                a$fightsSM[ii]   <- a$fightl[ii]\r\n                a$fightsLG[ii]   <- a$fightr[ii]\r\n                a$entranceSM[ii] <- a$entrl[ii]\r\n                a$entranceLG[ii] <- a$entrr[ii]\r\n        }\r\n        a$workerdif[ii] <- (a$workersL[ii] - a$workersS[ii])/a$workersL[ii] # make proportions and ratios\r\n        a$ratiodif[ii] <- ((a$broodL[ii]/a$workersL[ii]) - (a$broodS[ii]/a$workersS[ii]))/(a$broodL[ii]/a$workersl[ii])\r\n        a$brooddif[ii] <-  (a$broodL[ii] - a$broodS[ii])/a$broodL[ii]\r\n}\r\nmax(a$brooddif)\r\n\r\nfight <- glm(fightsSM ~  workerdif * brooddif * time, family=""poisson"", data=a, na.action=na.fail)\r\nentr <- glm(entranceSM ~ workerdif * brooddif * time, family=""poisson"", data=a, na.action=na.fail)\r\nsummary(fight)\r\nsummary(entr)\r\nef <- allEffects(fight, xlevels=list(workerdif = seq(0, 0.5, 0.1), brooddif = seq(-1, 1, 0.5), time = seq(0, 180, 15)))\r\nplot(ef, lines=list(multiline=TRUE, se=TRUE), rug=FALSE, colors=grey.colors(8), lwd=4)\r\n\r\n#\r\nfight <- glm(fightsSM ~  fightsLG * time, family=""poisson"", data=a, na.action=na.fail)\r\nentr <- glm(entranceSM ~ entranceLG * time, family=""poisson"", data=a, na.action=na.fail)\r\nsummary(fight)\r\nsummary(entr)\r\n\r\nef <- effect(""fightsLG:time"", fight, xlevels=list(time = seq(0, 180, 15), fightsLG = seq(0, 10, 1)))\r\nplot(ef, lines=list(multiline=TRUE, se=TRUE), rug=FALSE, colors=grey.colors(11), lwd=4)\r\n\r\n\r\n\r\n# Supplemental\r\nx <- unique(data.frame(""value"" = c(a$broodSM, a$broodLG, a$workersSM, a$workersLG),\r\n                       ""measure"" = c(rep(""brood"", length(c(a$broodSM, a$broodLG))), \r\n                                     rep(""workers"", length(c(a$workersSM, a$workersLG)))),\r\n                       ""ids"" = c(paste(""SM"", a$trial, sep=""""), paste(""LG"", a$trial, sep=""""))))\r\n\r\nx$measure <- as.factor(x$measure)\r\nx$ids <- as.factor(x$ids)\r\npar(mfrow=c(1,1), mar=c(4,4,1,1))\r\nbarplot(x$value ~ x$measure + x$ids, beside=TRUE, las=1, ylim=c(0, 250),\r\n        names.arg = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18),\r\n        ylab=""Total Number"", xlab=""Colonies"")\r\nlegend(""topleft"", legend=c(""Brood"", ""Workers""), fill=c(""black"", ""grey""), bty=""n"");\r\nbox()\r\n\r\n\r\ny <- c(a$broodl, a$broodr)\r\nx <- c(a$workersl, a$workersr)\r\nplot(y ~ x)\r\nmod <- lm(y ~ x)\r\nsummary(mod)\r\nabline(mod)\r\n\r\n\r\nhist(a$smallFighters)\r\n\r\n\r\ntrials <- unique(a$trial)\r\n        par(mfrow=c(1,2), mar=c(4, 4, 1, 1))\r\n        clrs <- rainbow(n=length(trials), 0.6, 0.7)\r\n        plot(0, 0, type=""n"", ylim=c(0, 15), xlim=c(0, 180), ylab=""Individuals Fighting"", xlab=""Time (minutes)"", xaxt=""n"")\r\n        axis(1, at = seq(0, 180, 15))\r\n        for(ii in 1:length(trials)){\r\n                b <- subset(a, a$trial == trials[ii])\r\n        lines(jitter(b$fightl, 1) ~ jitter(b$time, 3), lty=1, col = clrs[ii], lwd=3)\r\n        lines(jitter(b$fightr, 1) ~ jitter(b$time, 3), lty=2, col = clrs[ii], lwd=3)\r\n}\r\n\r\nplot(0, 0, type=""n"", ylim=c(0, 5), xlim=c(0, 180), ylab=""Individuals Fighting"", xlab=""Time (minutes)"", xaxt=""n"")\r\naxis(1, at = seq(0, 180, 15))\r\nfor(ii in 1:length(trials)){\r\n        b <- subset(a, a$trial == trials[ii])\r\n        lines(jitter(b$entrl, 1) ~ jitter(b$time, 3), lty=1, col = clrs[ii], lwd=3)\r\n        lines(jitter(b$entrr, 1) ~ jitter(b$time, 3), lty=2, col = clrs[ii], lwd=3)\r\n}\r\n#\r\n\r\n### continous study\r\nobs <- read.csv(""Chapin_Paat_Dornhaus_observationaldata.csv"") \r\nobs$start <- as.numeric(obs$start) # data formatting\r\nobs$end <- as.numeric(obs$end) \r\nobs$id <- as.factor(paste(substr(obs$ant1nest, 0, 1), sep="""", obs$trial))  # make unique ids for colonies\r\n\r\ncols <- c(""grey"", ""grey"",""black"", ""black"") # set colors\r\nltys <- c(1, 2, 1, 2) # set line types\r\nbehaviors <- unique(obs$behavior) \r\nids <- unique(obs$id)\r\npar(mfrow=c(4, 2), mar=c(0, 0, 3, 1), oma=c(4, 5, 0.5, 0), mgp=c']",2,"Datasets, R code, colony size, resource value, social insect contests, intergroup contests, organisms, contest decisions, ant colonies, Temnothorax rugatulus, brood, individual behaviors, demographic characteristics, fighting ability, workers"
"Code and data of MS ""Null models for animal social network analysis and data collected via focal sampling: pre-network or node network permutation?""","In social networks analysis, two different approaches have predominated in creating null models for hypothesis testing, namely pre-network and node network permutation approaches. Although the pre-network permutation approach appears more advantageous, its use has mainly been restricted to data on associations and sampling methods such as 'group follows.'The pre-network permutation approach has recently been adapted to data on interactions and the focal sampling method, but its performance in different scenarios has not been thoroughly explored. Here, we assessed the performance of the pre- and node network permutation approach in several simulated scenarios based on proneness to false positive or false negatives and with or without observation bias.Our results showed that the pre-network permutation was sensitive to false positives in scenarios with or without observation bias. The node network permutation approach produced fewer false positives and negatives than the pre-network approach, but only in scenarios without observation bias. In scenarios with observation bias, the node network permutation approach was outperformed by pre-network permutation. Caution should be taken when using the pre- and node network permutations to create null models with data collected via focal sampling. This study provides future methodological research perspectives for social network analyses.","['#############################################################################################\r\n### DATA ANALYSIS FEMALES WITH NO OBSERVATION BIAS (FEM AND MAL OBSERVED EQUALLY FREQUENT)###\r\n#############################################################################################\r\n\r\n### FALSE POSITIVES, NO FEMALE PHENOTYPE BIAS (EQUAL DEGREE MAL AND FEM) ###\r\nsetwd(""C:/My_Directory/..."")\r\n### load simulated data\r\nload(""AllSimResultsLHSNoFemPhenotypeBias_NoFemObsBias1000Perm.RData"")\r\n#load(""AllSimResultsLHSNoObsBias_NoPhenBias10000Perm.RData"")\r\n### remove na\r\nresult<-result[!is.na(result$MEDIAN.DEGREE.FEMALES.BIAS),]\r\nresult<-result[!is.na(result$MEDIAN.DEGREE.MALES.BIAS),]\r\ndim(result)\r\ndf<-result\r\nResult<-df\r\n#### CREATE TWO NEW COLUMNS WITH LOGICAL VALUES FOR SIGNIFICANCE OF CORRELATIONS #############\r\n### 1 --> DETECTING DIFFERENCE WHEN IT SHOULD NOT, FAILED! TYPE I ERROR\r\n### 0 --> NO DIFFERENCE DETECTED, CORRECT!\r\n### PRENET PERM\r\nP.VALUE_PRE.LOG<-c(rep(0,nrow(Result)))\r\nP.VALUE_PRE.LOG[which(Result$P.VALUE_PRE>=0.975)]=1\r\nP.VALUE_PRE.LOG[which(Result$P.VALUE_PRE<=0.025)]=1\r\n### NODES PERM\r\nP.VALUE_NODES.LOG<-c(rep(0,nrow(Result)))\r\nP.VALUE_NODES.LOG[which(Result$P.VALUE_NODES>=0.975)]=1\r\nP.VALUE_NODES.LOG[which(Result$P.VALUE_NODES<=0.025)]=1\r\nResult<-cbind(Result,P.VALUE_PRE.LOG,P.VALUE_NODES.LOG)\r\n\r\n### Detection percentage of false positives\r\nsum(P.VALUE_PRE.LOG==1)/length(P.VALUE_PRE.LOG)\r\nsum(P.VALUE_NODES.LOG==1)/length(P.VALUE_NODES.LOG)\r\n\r\n### LOGISTISC REGRESSION PRE\r\nlibrary(DescTools)\r\nLogModelPre<-glm(P.VALUE_PRE.LOG ~GROUP.SIZE + FEM.SEXRATIO + FOCALS.NUM ,family=binomial(link=\'logit\'),data=Result)\r\nsummary(LogModelPre)\r\nPseudoR2(LogModelPre,which = ""all"")\r\n\r\n############  FIG S2 A ########\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nResult$fit<-predict(LogModelPre,type=""response"",se.fit = T)$fit\r\nResult$fit.se<-predict(LogModelPre,type=""response"",se.fit = T)$se.fit\r\n\r\n# Summarise data to create histogram counts\r\nhSex = Result %>% group_by(P.VALUE_PRE.LOG) %>%  mutate(breaks = cut(FEM.SEXRATIO, breaks=seq(0.2,0.8,0.05), labels=seq(0.21,0.8,0.05), include.lowest=TRUE),\r\n                                                        breaks = as.numeric(as.character(breaks))) %>%  group_by(P.VALUE_PRE.LOG, breaks) %>%   summarise(n = n()) %>%  mutate(pct = ifelse(P.VALUE_PRE.LOG==0, n/500, 1 - n/500)) \r\nhSiz = Result %>% group_by(P.VALUE_PRE.LOG) %>%  mutate(breaks = cut(GROUP.SIZE, breaks=seq(10,100,5), labels=seq(11,100,5), include.lowest=TRUE),\r\n                                                        breaks = as.numeric(as.character(breaks))) %>% group_by(P.VALUE_PRE.LOG, breaks) %>%   summarise(n = n()) %>%  mutate(pct = ifelse(P.VALUE_PRE.LOG==0, n/500, 1 - n/500)) \r\nhSam = Result %>% group_by(P.VALUE_PRE.LOG) %>%  mutate(breaks = cut(FOCALS.NUM, breaks=seq(100,2000,100), labels=seq(101,2000,100), include.lowest=TRUE),\r\n                                                        breaks = as.numeric(as.character(breaks))) %>% group_by(P.VALUE_PRE.LOG, breaks) %>%   summarise(n = n()) %>%  mutate(pct = ifelse(P.VALUE_PRE.LOG==0, n/500, 1 - n/500)) \r\n\r\n### Female sex ratio\r\nh=hSex\r\nggplot() +\r\n  geom_segment(data=h, size=13, show.legend=FALSE,aes(x=breaks, xend=breaks, y=P.VALUE_PRE.LOG, yend=pct, colour=factor(P.VALUE_PRE.LOG))) +\r\n  geom_pointrange(data=Result, aes(x=FEM.SEXRATIO,y=fit,ymin=fit-fit.se,ymax=fit+fit.se), colour=""grey50"", lwd=0.5) +\r\n  stat_smooth(data=Result, aes(x=FEM.SEXRATIO,y=fit), method = \'glm\', method.args=list(family=""binomial""), se=TRUE) +\r\n  scale_y_continuous(limits=c(0.0,1)) + scale_x_continuous(limits=c(0.2,0.8)) + theme_bw(base_size=12) +\r\n  labs(y=""Prob false positives"",x=""Fem Sex Ratio"") + theme(text = element_text(size = 20))\r\n### GroupSize\r\nh=hSiz\r\nggplot() +\r\n  geom_segment(data=h, size=8, show.legend=FALSE,aes(x=breaks, xend=breaks, y=P.VALUE_PRE.LOG, yend=pct, colour=factor(P.VALUE_PRE.LOG))) +\r\n  geom_pointrange(data=Result, aes(x=GROUP.SIZE,y=fit,ymin=fit-fit.se,ymax=fit+fit.se), colour=""grey50"", lwd=0.5) +\r\n  stat_smooth(data=Result, aes(x=GROUP.SIZE,y=fit), method = \'glm\', method.args=list(family=""binomial""), se=TRUE) +\r\n  scale_y_continuous(limits=c(0.0,1)) + scale_x_continuous(limits=c(10,100)) + theme_bw(base_size=12) +\r\n  labs(y=""Prob false positives"",x=""Group Size"") + theme(text = element_text(size = 20))\r\n### Sample size\r\nh=hSam\r\nggplot() +\r\n  geom_segment(data=h, size=8, show.legend=FALSE,aes(x=breaks, xend=breaks, y=P.VALUE_PRE.LOG, yend=pct, colour=factor(P.VALUE_PRE.LOG))) +\r\n  geom_pointrange(data=Result, aes(x=FOCALS.NUM,y=fit,ymin=fit-fit.se,ymax=fit+fit.se), colour=""grey50"", lwd=0.5) + \r\n  stat_smooth(data=Result, aes(x=FOCALS.NUM,y=fit), method = \'glm\', method.args=list(family=""binomial""), se=TRUE) +\r\n  scale_y_continuous(limits=c(0.0,1)) + scale_x_continuous(limits=c(100,2000)) + theme_bw(base_size=12) +\r\n  labs(y=""Prob false positives"",x=""Number of Samples"") + theme(text = element_text(size = 20))\r\n\r\n#####\r\n####### LOGISTIC REGRESSION NODES\r\nLogMod', '###########################################################################################################\r\n###              DATA ANALYSIS FEMALES WITH OBSERVATION BIAS (LESS FREQUENTLY OBSERVED)                 ###\r\n###########################################################################################################\r\n\r\n#############################################################################\r\n### FALSE POSITIVES (NO FEMALE PHENOTYPE BIAS (EQUAL DEGREE MAL AND FEM) ###\r\nsetwd(""C:/My_Directory/..."")\r\n##### DATA ####\r\nload(""AllSimResultsLHSNoFemPhenotypeBias_1000Perm.RData"")\r\n### remove na\r\nresult<-result[!is.na(result$MEDIAN.DEGREE.FEMALES.BIAS),]\r\nresult<-result[!is.na(result$MEDIAN.DEGREE.MALES.BIAS),]\r\ndim(result)\r\ndf<-result\r\nResult<-df\r\n#### CREATE TWO NEW COLUMNS WITH LOGICAL VALUES FOR SIGNIFICANCE OF CORRELATIONS #############\r\n### 1 --> DETECTING DIFFERENCE WHEN IT SHOULD NOT, FAILED! TYPE I ERROR\r\n### 0 --> NO DIFFERENCE DETECTED, CORRECT!\r\n\r\n### PRENET PERM\r\nP.VALUE_PRE.LOG<-c(rep(0,nrow(Result)))\r\nP.VALUE_PRE.LOG[which(Result$P.VALUE_PRE>=0.975)]=1\r\nP.VALUE_PRE.LOG[which(Result$P.VALUE_PRE<=0.025)]=1\r\n### NODES PERM\r\nP.VALUE_NODES.LOG<-c(rep(0,nrow(Result)))\r\nP.VALUE_NODES.LOG[which(Result$P.VALUE_NODES>=0.975)]=1\r\nP.VALUE_NODES.LOG[which(Result$P.VALUE_NODES<=0.025)]=1\r\nResult<-cbind(Result,P.VALUE_PRE.LOG,P.VALUE_NODES.LOG)\r\n\r\n### Detection percentage of false positives\r\nsum(P.VALUE_PRE.LOG==1)/length(P.VALUE_PRE.LOG)\r\nsum(P.VALUE_NODES.LOG==1)/length(P.VALUE_NODES.LOG)\r\n\r\n####################################\r\n### LOGISTISC REGRESSION PRE NETWORK\r\nlibrary(DescTools)\r\nLogModelPre<-glm(P.VALUE_PRE.LOG ~GROUP.SIZE + FEM.REMOVAL + FEM.SEXRATIO + FOCALS.NUM ,family=binomial(link=\'logit\'),data=Result)\r\nLogModelPreQuasi<-glm(P.VALUE_PRE.LOG ~GROUP.SIZE + FEM.REMOVAL + FEM.SEXRATIO + FOCALS.NUM ,family=quasibinomial(link=\'logit\'),data=Result)\r\nsummary(LogModelPre)\r\n\r\n#### FIG S4\r\n#######################\r\nlibrary(dplyr)\r\nResult$fit<-predict(LogModelPre,type=""response"",se.fit = T)$fit\r\nResult$fit.se<-predict(LogModelPre,type=""response"",se.fit = T)$se.fit\r\n\r\n# Summarise data to create histogram counts\r\nhObs = Result %>% group_by(P.VALUE_PRE.LOG) %>%  mutate(breaks = cut(FEM.REMOVAL, breaks=seq(0.5,1,0.05), labels=seq(0.51,1,0.05), include.lowest=TRUE),\r\n                                                        breaks = as.numeric(as.character(breaks))) %>%  group_by(P.VALUE_PRE.LOG, breaks) %>%   summarise(n = n()) %>%  mutate(pct = ifelse(P.VALUE_PRE.LOG==0, n/500, 1 - n/500)) \r\nhSex = Result %>% group_by(P.VALUE_PRE.LOG) %>%  mutate(breaks = cut(FEM.SEXRATIO, breaks=seq(0.2,0.8,0.05), labels=seq(0.21,0.8,0.05), include.lowest=TRUE),\r\n                                                        breaks = as.numeric(as.character(breaks))) %>%  group_by(P.VALUE_PRE.LOG, breaks) %>%   summarise(n = n()) %>%  mutate(pct = ifelse(P.VALUE_PRE.LOG==0, n/500, 1 - n/500)) \r\nhSiz = Result %>% group_by(P.VALUE_PRE.LOG) %>%  mutate(breaks = cut(GROUP.SIZE, breaks=seq(10,100,5), labels=seq(11,100,5), include.lowest=TRUE),\r\n                                                        breaks = as.numeric(as.character(breaks))) %>% group_by(P.VALUE_PRE.LOG, breaks) %>%   summarise(n = n()) %>%  mutate(pct = ifelse(P.VALUE_PRE.LOG==0, n/500, 1 - n/500)) \r\nhSam = Result %>% group_by(P.VALUE_PRE.LOG) %>%  mutate(breaks = cut(FOCALS.NUM, breaks=seq(100,2000,100), labels=seq(101,2000,100), include.lowest=TRUE),\r\n                                                        breaks = as.numeric(as.character(breaks))) %>% group_by(P.VALUE_PRE.LOG, breaks) %>%   summarise(n = n()) %>%  mutate(pct = ifelse(P.VALUE_PRE.LOG==0, n/500, 1 - n/500)) \r\n\r\n### Female obs bias\r\nh=hObs\r\nggplot() +\r\n  geom_segment(data=h, size=13, show.legend=FALSE,aes(x=breaks, xend=breaks, y=P.VALUE_PRE.LOG, yend=pct, colour=factor(P.VALUE_PRE.LOG))) +\r\n  geom_pointrange(data=Result, aes(x=FEM.REMOVAL,y=fit,ymin=fit-fit.se,ymax=fit+fit.se), colour=""grey50"", lwd=0.5) +\r\n  stat_smooth(data=Result, aes(x=FEM.REMOVAL,y=fit), method = \'glm\', method.args=list(family=""binomial""), se=TRUE) +\r\n  scale_y_continuous(limits=c(0.0,1)) + scale_x_continuous(limits=c(0.5,1)) + theme_bw(base_size=12) +\r\n  labs(y=""Prob false positives"",x=""Fem Obs Bias"") + theme(text = element_text(size = 20))\r\n\r\n### Female sex ratio\r\nh=hSex\r\nggplot() +\r\n  geom_segment(data=h, size=13, show.legend=FALSE,aes(x=breaks, xend=breaks, y=P.VALUE_PRE.LOG, yend=pct, colour=factor(P.VALUE_PRE.LOG))) +\r\n  geom_pointrange(data=Result, aes(x=FEM.SEXRATIO,y=fit,ymin=fit-fit.se,ymax=fit+fit.se), colour=""grey50"", lwd=0.5) +\r\n  stat_smooth(data=Result, aes(x=FEM.SEXRATIO,y=fit), method = \'glm\', method.args=list(family=""binomial""), se=TRUE) +\r\n  scale_y_continuous(limits=c(0.0,1)) + scale_x_continuous(limits=c(0.2,0.8)) + theme_bw(base_size=12) +\r\n  labs(y=""Prob false positives"",x=""Fem Sex Ratio"") + theme(text = element_text(size = 20))\r\n### GroupSize\r\nh=hSiz\r\nggplot() +\r\n  geom_segment(data=h, size=8, show.legend=', 'library(igraph)\r\nlibrary(sna)\r\n\r\n### BASIC SIMULATION FUNCTIONS\r\n\r\n### function to create a network (matrix n x n) from data collection (obs) and focal id using simple ratio index (sri) \r\nmake_network <- function(obs, focal.id) {\r\n  N <- ncol(obs)\r\n  network <- matrix(0,nrow=N, ncol=N)\r\n  \r\n  for (i in 1:(N-1)) {\r\n    for (j in (i+1):N) {\r\n      x <- sum(obs[which(focal.id %in% c(i,j)),c(i,j)]>0)\r\n      fa <- sum(focal.id == i)\r\n      fb <- sum(focal.id == j)\r\n      sri <- x/(fa+fb)\r\n      if(!is.nan(sri)){\r\n        network[i,j] <- sri\r\n        network[j,i] <- sri\r\n      }else{\r\n        network[i,j] <- 0\r\n        network[j,i] <- 0\r\n        }\r\n    }\r\n  }\r\n  return(network)\r\n}\r\n\r\n### function to generate pre-network permutations (swaps of individuals between focals)\r\nrand_network2 <- function(obs.p, focal.id, n.perm,n_focals) {\r\n  N <- ncol(obs.p)\r\n  networks_rand <- array(0, c(n.perm,N,N))\r\n  for (i in 1:n.perm) {\r\n    # first randomly select two focal observations\r\n    repeat {\r\n      o <- 1:n_focals\r\n      a <- sample(o,1)\r\n      b <- sample(o[-a],1)\r\n      \r\n      # check if these are different individuals and they have associates\r\n      if ((focal.id[a] != focal.id[b]) & (sum(obs.p[a,])>0) & (sum(obs.p[b,])>0)) {\r\n        # next select two associates to swap\r\n        d <- sample(which(obs.p[a,] > 0),1)\r\n        e <- sample(which(obs.p[b,] > 0),1)\r\n        \r\n        # check they do not occur in the other focal\r\n        if ((obs.p[a,e] == 0) & obs.p[b,d] == 0) {\r\n          \r\n          # now check we have 4 distinct individuals, otherwise repeat this process\r\n          if (!(d %in% c(focal.id[a], focal.id[b], e)) & !(e %in% c(focal.id[a], focal.id[b], d))) {\r\n            break;\r\n          }\r\n        }\r\n      }\r\n    }\r\n    \r\n    # swap individuals\r\n    obs.p[a,d] <- 0\r\n    obs.p[b,d] <- 1\r\n    obs.p[b,e] <- 0\r\n    obs.p[a,e] <- 1\r\n    # caculate network\r\n    networks_rand[i,,] <- make_network(obs.p,focal.id)\r\n  }\r\n  return(networks_rand)\r\n}\r\n\r\n### Function to allocate number of observations to groups\r\nrand_vect <- function(N, M, sd = 1, pos.only = TRUE) {\r\n  vec <- rnorm(N, M/N, sd)\r\n  if (abs(sum(vec)) < 0.01) vec <- vec + 1\r\n  vec <- round(vec / sum(vec) * M)\r\n  deviation <- M - sum(vec)\r\n  for (. in seq_len(abs(deviation))) {\r\n    vec[i] <- vec[i <- sample(N, 1)] + sign(deviation)\r\n  }\r\n  if (pos.only) while (any(vec < 0)) {\r\n    negs <- vec < 0\r\n    pos  <- vec > 0\r\n    vec[negs][i] <- vec[negs][i <- sample(sum(negs), 1)] + 1\r\n    vec[pos][i]  <- vec[pos ][i <- sample(sum(pos ), 1)] - 1\r\n  }\r\n  vec\r\n}\r\n\r\n### MAIN SIMULATION FUNCTION ###\r\n### Arguments ###\r\n# GS --> numeric argument indicating group size\r\n# ObsBia --> numeric argument indicating the degree of observation bias [0.5-1.0]\r\n# FemPhenotypeBias --> boolean argument indicating whether a phenotype bias is present among females\r\n# nfocals --> numeric argument indicating number of focal samples\r\n# N.perm --> numeric argument indicating number of permutations \r\nSimulation<-function(GS,ObsBias,FemSexRatio,FemPhenotypeBias,nfocals,N.Perm)\r\n{\r\n  # Set parameters\r\n  N <- GS\r\n  n_focals <- nfocals\r\n  # Generate nodes\r\n  NumFem<-round(GS * FemSexRatio)\r\n  NumMal<-GS - NumFem\r\n  Sex<-c(rep(""F"",NumFem),rep(""M"",NumMal))\r\n  Sex<-sample(Sex,GS,replace=F)\r\n  ids <- data.frame(ID=1:(N),SEX=Sex)\r\n  # Generate a distribution of group sizes\r\n  group_size <- sample(c(1:(N/2)),n_focals,replace=TRUE)\r\n  # Create blank observation matrix\r\n  obs <- matrix(0,nrow=n_focals,ncol=N)\r\n  ## set number of observations of an individual in a group per individual\r\n  ids$OBS <- rand_vect(N,sum(group_size),pos.only=TRUE)\r\n  ## Variables to Allocate individuals to groups, \r\n  GroupID<-c(1:n_focals)\r\n  group_size.tmp <- group_size\r\n  # IF Fem phenotype is stronger than males, start with males so that they end up in smaller groups\r\n  if(FemPhenotypeBias == T)\r\n  {\r\n    which.males <- which(ids$SEX==""M"")\r\n    which.females <- which(ids$SEX==""F"")\r\n    for (i in which.males) \r\n    {\r\n      g <- sample(GroupID[which(group_size.tmp>0)],ids$OBS[i])\r\n      group_size.tmp[g] <- group_size.tmp[g]-1\r\n      obs[g,i] <- 1\r\n    }\r\n    for (i in which.females) \r\n    {\r\n      if ((sum(group_size.tmp>0) < ids$OBS[i])) \r\n      {\r\n        Needed<-ids$OBS[i]-(sum(group_size.tmp>0))\r\n        group.tmp<-group_size\r\n        group.tmp[group_size.tmp>0]=0\r\n        BiggestGroups<-sort(group.tmp,decreasing = T,index.return=T)$ix\r\n        ExtraGroups<-BiggestGroups[1:Needed]\r\n        g<-c(GroupID[which(group_size.tmp>0)],ExtraGroups)\r\n      }else \r\n      {\r\n        g<-sample(GroupID[which(group_size.tmp>0)],ids$OBS[i])\r\n      }\r\n      group_size.tmp[g] <- group_size.tmp[g]-1\r\n      obs[g,i] <- 1\r\n    }\r\n  }else # IF Fem phenotype is equal to males, allocate indivdiuals to groups at random\r\n  {\r\n    Inds<-c(1:GS)\r\n    for (. in 1:GS) \r\n    {\r\n      id<-Inds[1]\r\n      if(length(Inds)>1){id<-sample(Inds,1)}\r\n      Inds<-Inds[-which(Inds==id)]\r\n      if ((sum(group_size.tmp>0) < ids$OBS[id])) \r\n      {\r\n        Nee']",2,"Keywords: social networks analysis, null models, pre-network permutation approach, node network permutation approach, associations, sampling methods, interactions, focal sampling method, false positives, false negatives, observation bias, simulated scenarios, methodological research."
Evolutionary Game Theory: Simulations,"The software examples are all derived in the book by Daniel Friedman and Barry Sinervo, ""Evolutionary Games in Natural, Social, and Virtual Worlds"", published by Oxford University Press (2016). These Excel simulations, R workspaces, and mathematica programs are designed to illustrate diverse games that can be developed for one-population games such as Hawk Dove, RPS, Defect-Cooperate-TFT, to two population games such as Baseball (pitchers versus batters) or Buyer-Seller. The Excel games D-C-TFT and Baseball are derived from Joseph E. Harrington's examples from his book ""Games, Strategies, and Decision Making"" (Publisher MacMillan, 2009). We also develop continuous time versions of several games in the R programming environment, including an r-K strategy game, a two-population buyer-seller game, and several variations on RPS (all discussed in Chapters 1-4 of Friedman and Sinervo, 2016). The software repository also includes a cellular automata (written by Morgan Maddren, Barry Sinervo and Daniel Friedman) implemented in the R programming environment. The package is described in Chapter 6 of Friedman and Sinervo (2016). The software requires the R packages vcd and deSolve to run. We also include a mathematica version, based on William Sandholm's Mathematica package Dynamo (2013) of a predator (Naive-Responsive) playing against alternative prey types of Aposematic Model, Batesian Mimic, and Cryptic type, referred to as ABC prey game vs NR Predator. Version 1 of ABC-NR includes the simple version introduced in Chapter 7 of Friedman and Sinervo (2016) as well as the more complex version with additional own population effects for prey. Also included in this software repository are diverse examples of RPS games using Sandholm's mathematica package Dynamo, such as the yeast RPS of Question 6 in Chapter 3, and diverse RPS mating systems found in Chapter 7 of Friedman and Sinervo (2016).These examples are intended to spur on the development of projects by students who use the book as a text for a class.","['library(rpanel)\nlibrary(tkrplot)\nlibrary(tcltk)\n\n# packages needed to be installed and loaded\n# rpanel, tkrplot, tcltk\n\ndebugflag <<- TRUE\n\n# for stochastic, make it an exponential distribution e^beta*fi / normalization\n# beta is a parameter along with death rate\n# look up cran documentation\n# contour plotting for tirangles or hex? low priority\n# fitness should get from cell i,j on other pop\n# read snowdrift journal of theoretical biology to doublecheck parameters and try to replicate game dynamics\n# read speciation paper\n# tag and preference and tolerance\n\n# every period, highest fitness neighbor wins\n# simultaneous or sequential, sequential\n# every tick, random neighborhood chosen to update, \n# sweep as well\n\n# prioritize death rates\n# frequency dependence would be each cell gets taken over by max fitness\n# public goods check paper because death is differently structured\n# make CA work within a package style system\n\n\ndrawGrid <- function(panel)\n{\n\tdbg(\'drawGrid\')\n\n    par(mar=rep(0,4))\n    plot.new()\n    rect(0, 0, 1, 1)\n\t#print(""draw"")\n    #print(ca$curgrid)\n\t\n\tfor (n in 1:length(ca$populations))\n\t{\n\t\tcg = ca$populations[[n]]$curgrid\n\t\tog = ca$populations[[n]]$oldgrid\n\t\t\n\t\tfor (i in 1:ncol(cg))\n\t\t{\n\t\t\tfor (j in 1:nrow(cg))\n\t\t\t{\n\t\t\t\tif (cg[i, j] == 0)\n\t\t\t\t\tcolor <- ""#FFFFFF""\n\t\t\t\telse\n\t\t\t\t\tcolor <- ca$populations[[n]]$popColors[cg[i, j]]\n\t\t\t\t\n\t\t\t\t# {\n\t\t\t\t\t# if (og[i, j] == 1)\n\t\t\t\t\t# {\n\t\t\t\t\t\t# color <- ""#FF8080""\n\t\t\t\t\t# }\n\t\t\t\t\t# else if (og[i, j] == 2)\n\t\t\t\t\t# {\n\t\t\t\t\t\t# color <- ""#FFD280""\n\t\t\t\t\t# }\t\n\t\t\t\t\t# else if (og[i, j] == 3)\n\t\t\t\t\t# {\n\t\t\t\t\t\t# color <- ""#8080FF""\n\t\t\t\t\t# }\n\t\t\t\t# }\n\t\t\t\t# if (cg[i, j] == 1)\n\t\t\t\t# {\n\t\t\t\t\t# color <- ""#FF0000""\n\t\t\t\t# }\n\t\t\t\t# else if (cg[i, j] == 2)\n\t\t\t\t# {\n\t\t\t\t\t# color <- ""#FFA500""\n\t\t\t\t# }\n\t\t\t\t# else if (cg[i, j] == 3)\n\t\t\t\t# {\n\t\t\t\t\t# color <- ""#0000FF""\n\t\t\t\t# }\n\t\t\t\t#n = paste(""cell_"", as.character(i), ""_"", as.character(j), sep="""")\n                if (ca$bwOnly)\n                    color = ca$bwColors[cg[i, j] + 1]\n\t\t\t\tif (n == 1)\n                {\n\t\t\t\t\trect((i - 1) / nrow(cg), (j - 1) / ncol(cg), i / nrow(cg), j / ncol(cg), col=color)\n\t\t\t\t\tif (ca$viewFitness)\n\t\t\t\t\t\ttext((i - 0.5) / nrow(cg), (j - 0.5) / ncol(cg), ca$populations[[n]]$fitnessMatrix[i, j])\n                }\n\t\t\t\telse if (n == 2)\n\t\t\t\t{\n\t\t\t\t\tif (ca$panel$drawStyle == ca$drawStyles[1])\n\t\t\t\t\t\trect((i - 0.75) / nrow(cg), (j - 0.75) / ncol(cg), (i - 0.25) / nrow(cg), (j - 0.25) / ncol(cg), col=color)\n\t\t\t\t\telse if (ca$panel$drawStyle == ca$drawStyles[2])\n\t\t\t\t\t\trect((i - 0.5) / nrow(cg), (j - 0.5) / ncol(cg), (i - 0) / nrow(cg), (j - 0) / ncol(cg), col=color)\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t}\n    rect(0, 0, 1, 1)\n\ttext(0.25, -0.02, paste(ca$gameType, "", generation "", ca$tick))\n\tdbg(\'end drawGrid\')\n    panel\n}\n\ncaStop <- function(panel)\n{\n\tca$running <<- FALSE\n\trp.tkrreplot(ca$panel, tkrp)\n\tpanel\n}\n\ncaStart <- function(panel)\n{\n\tca$running <<- TRUE\n\tpanel\n}\n\nmakeFile <- function()\n{\n\tsettingsline = list()\n\tsettingsline[[1]] = paste(""game="", ca$gameType, "","", sep="""")\n\tsettingsline[[2]] = paste(""size="", ca$size$x, ""x"", ca$size$y, "","", sep="""")\n\tsettingsline[[3]] = paste(""surface="", ca$world, "","", sep="""")\n\tsettingsline[[4]] = ""TODO: full game parameters go here!""\n\n\tfirstline = list()\n\tfirstline[[1]] = ""generation""\n\tfor (n in 1:length(ca$populations))\n\t{\n\t\tfor (i in 1:ca$populations[[n]]$players)\n\t\t{\n\t\t\tfirstline[[length(firstline)+1]] = paste(""shares_pop"", n, ""_strat"", i, collapse="""", sep="""")\n\t\t}\n\t}\n\tfor (i in 1:length(settingsline))\n\t{\n\t\twriteLines(settingsline[[i]])\n\t}\n\t#s = paste(settingsline, collapse="","", sep="""")\n\t#writeLines(s)\n\t\n\ts = paste(firstline, collapse="","", sep="""")\n\t#print(s)\n\twriteLines(s)\n\t\n\t#s = lapply(firstline, paste, collapse="""", sep="","")\n\t#write(s, file="""", append=FALSE, ncolumns=1)\n\t#write(firstline, file="""", ncolumns=length(firstline), append=FALSE, sep="","")\n}\n\nwriteLine <- function()\n{\n\tcaline = list()\n\tcaline[[1]] = ca$tick\n\tfor (n in 1:length(ca$populations))\n\t{\n\t\tfor (i in 1:ca$populations[[n]]$players)\n\t\t{\n\t\t\tcaline[[length(caline)+1]] = ca$cellCounts[[n]][i]\n\t\t}\n\t}\n\ts = paste(caline, collapse="","", sep="""")\n\t#print(s)\n\twriteLines(s)\n\t\n\t#s = lapply(caline, paste, collapse="""", sep="","")\n\t#write(s, file="""", append=FALSE, ncolumns=1)\n\t#write(caline, file="""", ncolumns=length(caline), append=TRUE, sep="","")\n}\n\ncountCells <- function()\n{\n    counts = list()\n    for (n in 1:length(ca$populations))\n\t{\n        counts[[n]] = numeric(length=ca$populations[[n]]$players)\n\t\t\n\t\tfor (i in 1:ncol(ca$populations[[n]]$curgrid))\n\t\t{\n\t\t\tfor (j in 1:nrow(ca$populations[[n]]$curgrid))\n\t\t\t{\n\t\t\t\t# first update cell counts for this generation\n\t\t\t\tcounts[[n]][ca$populations[[n]]$curgrid[i, j]] = counts[[n]][ca$populations[[n]]$curgrid[i, j]] + 1\n\t\t\t}\n\t\t}\n\t}\n    return(counts)\n}\n\nupdateFitnessMatrix <- function()\n{\n    for (n in 1:length(ca$populations))\n\t{\n\t\tca$populations[[n]]$fitnessMatrix <<- matrix(ncol=ncol(ca$populations[[n]]$curgrid), nrow=nrow(ca$populations[[n]]$curgrid), byrow=TRUE)\n\n\t\tfor (i in 1:ncol(ca$populations[[n]]$curgrid))\n\t\t{\n\t\t\tf']",2,"Evolutionary Game Theory, Simulations, Software Examples, Daniel Friedman, Barry Sinervo, Evolutionary Games, Natural, Social, Virtual Worlds, Oxford University Press, Excel Simulations, R Workspaces, Mathematica Programs, One-Population"
Code for: A metapopulation model of social group dynamics and disease  applied to Yellowstone wolves,"AbstractThe population structure of social species has important consequences for both their demography and transmission of their pathogens. We develop a new form of metapopulation model that tracks two key components of a species' social system: average group size and number of groups within a population. While the model is general, we parameterize it to mimic the dynamics of the Yellowstone wolf population and two associated pathogens: sarcoptic mange and canine distemper. In the initial absence of disease, we show that group size is mainly determined by the birth, death rates, and the rates at which groups fission to form new groups. The total number of groups is determined by rates of fission and fusion, as well as upon environmental resources and rates of intergroup aggression. Incorporating pathogens into the models reduces the size of the host population, predominantly by reducing the number of social groups. Average group size responds in more subtle ways: infected groups decrease in size, but uninfected groups may increase when disease reduces the number of groups and thereby reduces intraspecific aggression. Our modeling approach allows for easy calculation of prevalence at multiple scales (within group, across groups, and population level), illustrating that aggregate population-level prevalence can be misleading for group-living species. The model structure is general, can be applied to other social species, and allows for a dynamic assessment of how pathogens can affect social structure and vice versa. SignificanceHow do social and infectious disease dynamics interact in group-living mammals? A significant cost to group living is increased transmission of pathogens. When a pathogen invades a group, members will be more vulnerable to mortality, Allee effects, and ultimately group extinction. The presence of a pathogen reduces the size of the population by reducing the number of social groups, allowing uninfected groups to grow larger from a reduction in inter-group aggression. Concomitantly, Allee effects are exacerbated in infected groups; this reduces the probability of pathogen persistence as infected groups die out more rapidly. Social structuring changes prevalence across scales and influences pathogen invasion and persistence. The models described here provide a new framework for understanding the dynamics of these interactions. AboutHere is the R code associated with the paperBrandell et al. 2021. A metapopulation model of social group dynamics and disease applied to Yellowstone wolves. Proceedings of the National Academy of Sciences.We provide code for the basic metapopulation models (central_allee.R) and the disease models: susceptible-infected-susceptible (SIS_model.R) and susceptible-infected-recovered (SIR_model.R). We have also included the parameters used (parameters.R) and the wolf count data used in Figure 1 of the main text (wolf_counts.csv).","['###### CORE GROUPS MODEL ###### \n# state variables:\n# g = mean group size\n# G = number of groups\n\n# parameters:\n# Bf = number of breeding females\n# b = birth \n# bf = number of offspring/breeding female\n# d = death\n# f = fission\n# c = fusion\n# a = aggression/attack (intraspecific mortality)\n# e = group extinction rate\n\n\nGrpDynCore <- function(t, x, params){\n  \n  ## create vectors to hold states through time\n  g <- x[1]\n  G <- x[2]\n  \n  ## model\n  with(as.list(params),{\n    \n    ## equation 6 in text\n    dg <- b*g - d*g - (f*g*g/(gf+g))*(d/(c+d)) - (e*G)*g\n    ## equation 7 in text\n    dG <- (c*f*g*g*G)/((gf+g)*(d+c)) - (a+e*G)*G\n    \n    ## output\n    res <- c(dg, dG)\n    list(res)\n  })\n}\n\n\nGrpDyn <- function(t, x, params){\n  \n  ## create vectors to hold states through time\n  g <- x[1]\n  G <- x[2]\n  \n  ## model\n  with(as.list(params),{\n    \n    ## equation 6 in text + breeding females specified\n    dg <- bf*(g/(Bf+g)) - d*g - (f*g*g/(gf+g))*(d/(c+d)) - (a+e*G)*g\n    ## equation 7 in text\n    dG <- (c*f*g*g*G)/((gf+g)*(d+c)) - a*G - e*G*G\n    \n    ## output\n    res <- c(dg, dG)\n    list(res)\n  })\n}\n\n\n\n###### CORE GROUPS MODEL + ALLEE ###### \n# additional parameter:\n# A = allee threshold\n\nGrpDynAllee <- function(t, x, params){\n  \n  ## create vectors to hold states through time\n  g <- x[1]\n  G <- x[2]\n  \n  ## model\n  with(as.list(params),{\n    \n    ## equation 6 in text + breeding females specified + Allee term\n    dg <- bf*(g/(Bf+g))*(g/(g+A)) - d*g - (f*g*g/(gf+g))*(d/(c+d)) - (a+e*G)*g\n    ## equation 7 in text\n    dG <- (c*f*g*g*G)/((gf+g)*(d+c)) - a*G - e*G*G\n    \n    ## output\n    res <- c(dg, dG)\n    list(res)\n  })\n}\n\n', '### set parameter values\n\nBf <- 1         # number of breeding females\nbf <- 3         # number of offspring/female (b in SIS/SIR models in text)\nb <- 0.63       # per capita birth rate\nd <- 0.12       # per capita death rate\nf <- 0.08       # fission rate\na <- 0.1        # aggression rate\nc <- 4          # fusion rate\nA <- 2          # Allee effect\ne <- 0.02       # background extinction rate\ngf <- 8         # critical group size that increases fission rates\n', '###### SIR GROUPS + DISEASE MODEL ###### \n\n# state variables: \n# g = mean number of uninfected individuals in uninfected groups\n# s = mean number of uninfected individuals in infected groups\n# i = mean number of infected individuals in infected groups\n# r = mean number of recovered individuals in infected groups\n# sR = mean number of uninfected individuals in recovered groups\n# rR = mean number of recovered individuals in recovered groups\n# G = number of uninfected groups\n# I = number of infected groups\n# R = number of recovered groups\n\n# group parameters:\n# Bf = number of breeding females\n# b = birth \n# bf (just b in text) = number of offspring/breeding female\n# d = death\n# f = fission\n# c = fusion\n# a = aggression/attack (intraspecific mortality)\n# e = group extinction rate\n\n# disease parameters:\n# beta = transmission rate between G/R and I groups\n# betaW = within group transmission rate\n# sigma = recovery rate\n# alpha = disease-induced mortality\n# phi = rate that immunity wanes\n\nGrpDynSIR <- function(t, x, params){\n\n  ## create vectors to hold states through time\n  G <- x[1]\n  I <- x[2]\n  g <- x[3]\n  s <- x[4]\n  i <- x[5]\n  r <- x[6]\n  R <- x[7]\n  sR <- x[8]\n  rR <- x[9]\n\n  ## model\n  with(as.list(params),{\n\n    ## equation 13 in text\n    dg <- Bf*bf - (d+a)*g - (g*g*f)/((gf+g)*(d/(c+d))) - e*g*(G+I+R)\n    ## equation 14 in text\n    ds <- Bf*bf*((s+r)/(0.001+s+r)) - (d+a)*s + phi*r - s*f*(s+i+r)/((gf+s+i+r)*(d/(c+d))) - e*s*(G+I+R) - (betaW*s*i)/(1+s+i+r)\n    ## equation 15 in text\n    di <- (betaW*s*i)/(1+s+i+r) - (d+sigma+alpha+a)*i - e*i*(G+I+R)\n    ## equation 16 in text\n    dr <- sigma*i - (d+phi+a)*r - r*f*(s+i+r)/((gf+s+i+r)*(d/(c+d))) - e*r*(G+I+R)\n    ## equation 17 in text\n    dsR <- Bf*bf*((sR+rR)/(0.001+sR+rR)) - (d+a)*sR + phi*rR - ((sR+rR)*f*sR)/((gf+sR+rR)*(d/(c+d))) - e*sR*(G+I+R)\n    ## equation 18 in text\n    drR <- sigma*i/(1+i) - (d+phi+a)*rR - ((sR+rR)*rR*f)/((gf+sR+rR)*(d/(c+d))) - e*rR*(G+I+R)\n    \n    ## equation 19 in text\n    dG <- (G*c*f*g*g)/((gf+g)*(d+c)) - a*G - e*G*(G+I+R) + R*phi/(1+rR) - beta*I*G/(G+I+R)\n    ## equation 20 in text\n    dI <- beta*I*(G+(R*(sR/(sR+rR))))/(G+I+R) - a*I - e*I*(G+I+R) - I*((alpha+sigma)/(1+i))\n    ## equation 21 in text\n    dR <- I*sigma/(1+i) + (c*f*R*((sR+rR)^2))/((gf+sR+rR)*(c+d)) - a*R - e*R*(G+I+R) - R*phi/(1+rR) - beta*I*(R*(sR/(sR+rR)))/(G+I+R)\n    \n    ## output\n    res <- c(dG, dI, dg, ds, di, dr, dR, dsR, drR)\n    list(res)\n  })\n}\n\n']",2,"Metapopulation model, social group dynamics, disease, Yellowstone wolves, pathogens, sarcoptic mange, canine distemper, group size, number of groups, birth rates, death rates, fission, fusion, environmental resources, intergroup"
Data from: Cooperation with closely bonded individuals reduces cortisol levels in long-tailed macaques,"Many animal species cooperate with conspecifics in various social contexts. While ultimate causes of cooperation are being studied extensively, its proximate causes, particularly endocrine mechanisms, have received comparatively little attention. Here, we present a study investigating the link between the hormone cortisol, cooperation and social bonds in long-tailed macaques (Macaca fascicularis). We tested 14 macaques in a dyadic cooperation task (loose-string paradigm), each with two partners of different social bond strength and measured their salivary cortisol before and after the task. We found no strong link between the macaques' cortisol level before the task and subsequent cooperative success. In contrast, we did find that the act of cooperating in itself led to a subsequent decrease in cortisol levels, but only when cooperating with closely bonded individuals. Two control conditions showed that this effect was not due to the mere presence of such an individual or the pulling task itself. Consequently, our study shows an intricate way in which the hypothalamic-pituitary-adrenal axis is involved in cooperation. Future studies should reveal whether and how our findings are driven by the anxiolytic effect of oxytocin, which has been associated with social bonding.","['\n# Stocker et al. 2019: Cooperation with closely bonded individuals reduces cortisol levels in long-tailed macaques\n# Q1: IS AN INDIVIDUAL\'S COOPERATIVE BEHAVIOUR MODULATED BY ITS CORTISOL LEVEL BEFORE COOPERATION, SOCIAL BOND AND/OR RANK?\n#--------------------------------------------------------------------\n\n\nsetwd(""your_working_directory"")\n\nLTM <- read.csv(""Stocker_Data_Nov2019.csv"")\nstr(LTM)\n\n# Dataset with cooperation condition only\nLTM.Coop <- LTM[!is.na(LTM$Success), ]\nLTM.Coop <- LTM.Coop[!is.na(LTM.Coop$cort.1), ]\n\n\n#---------------------\n# check distribution of cooperative success (number of trials)\n\nhist(LTM.Coop$Success, 10)\n\n# check beta distribution\nsuc_prop<-LTM.Coop$Success/15\nLTM.Coop$suc_prop <- suc_prop\nLTM.Coop$suc_prop[LTM.Coop$suc_prop==1] <- 0.9999  #cannot be 0 or 1 with beta distribution\n\nlibrary(fitdistrplus)\nfit.beta<-fitdist(LTM.Coop$suc_prop, ""beta"")  \nplot(fit.beta)  # better fit than other distributions\n\n\n\n#---------------------\n# full model\n\nlibrary(glmmADMB)\nsummary(m.Q1 <- glmmadmb (suc_prop ~ cort.1 + Soc.bond.prop + Rank.indiv + \n                            (1|Individual) + (1|Partner) , data = LTM.Coop, family = ""beta""))\n\n\nlibrary(car)\nvif(m.Q1)  #no mulit-collinearity issue\n\n\n# Model averaging\nlibrary(MuMIn)\noptions(na.action=na.fail)\nall.m.Q1 <- dredge(m.Q1, rank=""AICc"") #all model combinations\nall.m.Q1\nsub.m.Q1 <- subset(all.m.Q1, delta<2) # model combinations less than 2 AIC difference to the \'best\' models\nsub.m.Q1 # subset includes null model\n#avg.m.Q1 <- model.avg(sub.m.Q1)\n\n', '\n# Stocker et al. 2019: Cooperation with closely bonded individuals reduces cortisol levels in long-tailed macaques\n# Q2: ARE CHANGES IN CORTISOL CONNECTED WITH THE INDIVIDUALS\' COOPERATIVE SUCCESS AND/OR WITH OTHER SOCIAL FACTORS?\n#--------------------------------------------------------------------\n\n\nsetwd(""your_working_directory"")\n\nLTM <- read.csv(""Stocker_Data_Nov2019.csv"")\nstr(LTM)\n\n# Q2a dataset with cooperation condition only\nLTM.Coop <- LTM[!is.na(LTM$Success), ]\nLTM.Coop <- LTM.Coop[!is.na(LTM.Coop$cort.1), ]\n# ... and no missing delta cort\nLTM.Coop.Q2 <- LTM.Coop[!is.na(LTM.Coop$cort.2), ]\nLTM.Coop.Q2$Kin.maternal <- droplevels(LTM.Coop.Q2$Kin.maternal)\n\n\n# Q2b dataset with social control condition only\nLTM_noZ <- LTM[!is.na(LTM$delta.2.1), ]\nLTM.SocCont <- LTM_noZ[LTM_noZ$Condition == ""Control"",]\nLTM.SocCont$Kin.maternal <- droplevels(LTM.SocCont$Kin.maternal)\n\n\n#--------------------------------------------------------------------\n# full model Q2a\n\nlibrary(lme4)\nsummary(m.Q2.a <- lmer(delta.2.1 ~ Success + Soc.bond.prop + Sex.indiv + Rank.indiv + Baby + Kin.maternal +  \n                         (1|Individual) , data=LTM.Coop.Q2, REML=FALSE))\n\n\nlibrary(car)\nvif(m.Q2.a)    # no mulit-collinearity issue\n\n#standardize model for averaging\nlibrary(arm)\nm.Q2.a_stdz <- standardize(m.Q2.a)\n\n# Model averaging\nlibrary(MuMIn)\noptions(na.action=na.fail)\nall.m.Q2.a <- dredge(m.Q2.a_stdz, rank=""AICc"") #all model combinations\nall.m.Q2.a\nsub.m.Q2.a <- subset(all.m.Q2.a, delta<2) # model combinations less than 2 AIC difference to the \'best\' models\nsub.m.Q2.a\navg.m.Q2.a <- model.avg(sub.m.Q2.a)\nsummary(avg.m.Q2.a)\nimportance(avg.m.Q2.a)\n\nconfint(avg.m.Q2.a)\n\n\n\n#--------------------------------------------------------------------\n# full model Q2b\n\nlibrary(lme4)\nsummary(m.Q2.b <- lmer(delta.2.1 ~ Soc.bond.prop + Sex.indiv + Rank.indiv + Baby + Kin.maternal +\n                            (1|Individual), data=LTM.SocCont, REML = F))\n\nlibrary(car)\nvif(m.Q2.b) #no mulit-collinearity issue\n\nlibrary(arm)\nm.Q2.b_stdz <- standardize(m.Q2.b)\n\n# Model averaging\nlibrary(MuMIn)\noptions(na.action=na.fail)\nall.m.Q2.b <- dredge(m.Q2.b_stdz, rank=""AICc"")\nall.m.Q2.b\nsub.m.Q2.b <- subset(all.m.Q2.b, delta<2)\nsub.m.Q2.b                      # null model is the best!\n\n\n\n', '\n# Stocker et al. 2019: Cooperation with closely bonded individuals reduces cortisol levels in long-tailed macaques\n# Q3: ARE CHANGES IN CORTISOL CONNECTED WITH THE TEST CONDITIONS?\n#--------------------------------------------------------------------\n\nsetwd(""your_working_directory"")\n\nLTM <- read.csv(""Stocker_Data_Nov2019.csv"")\nstr(LTM)\n\n# delete rows in which delta.2.1 has no values\nLTM_noZ <- LTM[!is.na(LTM$delta.2.1), ]\nstr(LTM_noZ)\n\n\n#---------------------\n# plot\npar(mfrow=c(1,1))\nplot(LTM_noZ$delta.2.1 ~ LTM_noZ$Condition, ylab = ""Change in cortisol [ng/ml]"", xlab = ""Condition"")\n\n\n#---------------------\n# check distribution\nhist(LTM_noZ$delta.2.1)\n\nlibrary(fitdistrplus)\nfit.norm<-fitdist(LTM_noZ$delta.2.1, ""norm"")\nplot(fit.norm)\n\n\n#---------------------\n# full model\n\nlibrary(lme4)\nsummary(m.Q4 <- lmer(delta.2.1 ~ Condition + Sex.indiv + Rank.indiv + Baby +\n                      (1|Individual), data=LTM_noZ, REML=FALSE))\n\nlibrary(car)\nvif(m.Q4)   # no mulit-collinearity issue\n\n\n#standardize model\nlibrary(arm)\nm.Q4_stdz <- standardize(m.Q4)\n\n\n# Model averaging\nlibrary(MuMIn)\noptions(na.action=na.fail)\nall.m.Q4 <- dredge(m.Q4_stdz, rank=""AICc"") #all model combinations\nall.m.Q4\nsub.m.Q4 <- subset(all.m.Q4, delta<2) # model combinations less than 2 AIC difference to the \'best\' models\nsub.m.Q4\navg.m.Q4 <- model.avg(sub.m.Q4)\nsummary(avg.m.Q4)\nimportance(avg.m.Q4)\n\nconfint(avg.m.Q4)\n']",2,"cooperation, social bonds, long-tailed macaques, cortisol levels, hormone cortisol, dyadic cooperation task, loose-string paradigm, salivary cortisol, hypothalamic-pituitary-adrenal axis, oxytocin, anxi"
Data from: How to make methodological decisions when inferring social networks.,"Social network analyses allow studying the processes underlying the associations between individuals and the consequences of those associations. Constructing and analysing social networks can be challenging, especially when designing new studies as researchers are confronted with decisions about how to collect data and construct networks, and the answers are not always straightforward. The current lack of guidance on building a social network for a new study system might lead researchers to try several different methods, and risk generating false results arising from multiple hypotheses testing. Here, we suggest an approach for making decisions when starting social network research in a new study system that avoids the pitfall of multiple hypotheses testing. We argue that best edge definition for a network is a decision that can be made using a priori knowledge about the species, and that is independent from the hypotheses that the network will ultimately be used to evaluate. We illustrate this approach with a study conducted on a colonial cooperatively breeding bird, the sociable weaver. We first identified two ways of collecting data using different numbers of feeders and three ways to define associations among birds. We then evaluated which combination of data collection and association definition maximised (i) the assortment of individuals into previously known 'breeding groups' (birds that contribute towards the same nest and maintain cohesion when foraging), and (ii) socially differentiated relationships (more strong and weak relationships than expected by chance). This evaluation of different methods based on a priori knowledge of the study species can be implemented in a diverse array of study systems and makes the case for using existing, biologically meaningful knowledge about a system to help navigate the myriad of methodological decisions about data collection and network inference.","['#R script to replicate all the analyses from the manuscript: How to make methodological decisions when inferring social networks. (Ecology and Evolution)\r\n\r\n#Convert RFID data to continuous time\r\nRFID_data_to_continuous(RFID_data, tags, tags$Last_colony_captured, same_tags_as_col=TRUE, keeptesttags= TRUE, keepfeeders=TRUE)\r\nrm(RFID_data)\r\n#add time bins to the RIFD\r\nRFID_data_to_birds_fit_time_bin(RFID_continuous_time, 60, tags, duplicates=FALSE, removetest=TRUE)\r\n\r\n#Remove test tags and birds not captured\r\nTag_time_bin_remove_tags(Tag_time_bin, tags, ""2017-01-01"")\r\n\r\n#remove df not needed\r\nrm(RFID_continuous_time)\r\n\r\n#save to gmmevents\r\npath=""PATH_TO_SAVE_GMMM_EVENTS""\r\n\r\n#Change here for the colony that you want to use (colonies: 11,20,27,43,71)\r\n#colonyID\r\ncolonyID=71\r\n\r\nGmmevents_within_Gmmevents(Tag_time_bin, colonyID, path, all=TRUE)\r\n\r\n########## co-occurrence network ############\r\n#get the network of the co-occurrences\r\nlibrary(asnipe)\r\nnet_occorrence <-get_network(gbi2)\r\ncv_occorrence<-cv_network(net_occorrence)\r\n\r\n#test if cv is generated randomly\r\n\r\n#create a storage matrix for the results\r\nrandomcv_occorrence<- matrix(NA, nrow=1000, ncol=1)\r\nnetworks.rand <- network_permutation(gbi2, association_matrix=net_occorrence, identities = as.character(colnames(net_occorrence)), within_location=FALSE, within_day=FALSE)\r\n#loop\r\nstr(networks.rand)\r\nfor(i in 1:1000) {\r\n  #extract the association matrix from the networks.rand variable\r\n  net.tmp <- networks.rand[i,,]\r\n  #calculate individuals\' eigenvector centralities in the network\r\n  #take this code from the above (on the original netowrk)\r\n  cv_occorrence_rand<-cv_network(net.tmp)\r\n  #now do the same statistical test as above\r\n  randomcv_occorrence[i,1]<-cv_occorrence_rand\r\n  print(i)\r\n}\r\n\r\nhist(randomcv_occorrence[,1])\r\nabline(v=cv_occorrence, col=""red"")\r\nP <- 2*sum(randomcv_occorrence[,1] >= cv_occorrence)/1000\r\nP\r\nplot(randomcv_occorrence)\r\nabline(h=cv_occorrence, col =""red"")\r\n\r\n\r\n\r\n##Test assortment\r\ngroups<-groups[!is.na(groups$tag),]\r\n\r\n#erase groups not duplicated (just one individual)\r\ngroups<-groups[ groups$group %in% groups$group[duplicated(groups$group)],]\r\n\r\n\r\n#get the group ID\r\ngroups<-groups[groups$tag %in% colnames(net_occorrence),]\r\nnet_occorrence<-net_occorrence[(rownames(net_occorrence) %in% groups$tag),]\r\nnet_occorrence<-net_occorrence[,(colnames(net_occorrence) %in% groups$tag)]\r\nnet_occorrence<-net_occorrence[order(colnames(net_occorrence)),]\r\nnet_occorrence<-net_occorrence[,order(colnames(net_occorrence))]\r\ngroups<-groups[order(groups$tag),]\r\n\r\nlibrary(asnipe)\r\nlibrary(assortnet)\r\nassort_occorrence <- assortment.discrete(net_occorrence, groups$group, weighted=TRUE, SE=TRUE)\r\nassort_occorrencer<-assort_occorrence$r\r\n#erase individuals that are not in the group ID\r\ngbi2<-gbi2[,colnames(gbi2) %in% groups$tag]\r\n#order gbi2\r\ngbi2<-gbi2[,order(colnames(gbi2))]\r\nnetworks.rand <- network_permutation(gbi2, association_matrix=net_occorrence, identities = as.character(colnames(net_occorrence)), within_location=FALSE, within_day=FALSE)\r\n\r\n#create a storage matrix for the results\r\nrandomassort_occorrencer<- matrix(NA, nrow=1000, ncol=1)\r\n\r\n#loop\r\nstr(networks.rand)\r\nfor(i in 1:1000) {\r\n  #extract the association matrix from the networks.rand variable\r\n  net.tmp <- networks.rand[i,,]\r\n  #calculate individuals\' eigenvector centralities in the network\r\n  #take this code from the above (on the original netowrk)\r\n  assorti <- assortment.discrete(net.tmp, groups$group, weighted=TRUE, SE=TRUE)\r\n  #now do the same statistical test as above\r\n  randomassort_occorrencer[i,1]<- assorti$r\r\n  print(i)\r\n}\r\n\r\nhist(randomassort_occorrencer[,1])\r\nabline(v=assort_occorrencer, col=""red"")\r\nP <- 2*sum(randomassort_occorrencer[,1] >=assort_occorrencer )/1000\r\nP\r\nplot(randomassort_occorrencer)\r\nabline(h=assort_occorrencer, col =""red"")\r\n\r\n#########Overlap of time network#############\r\n\r\n#convert to start end events\r\nContinuous_time_to_start_end(Tag_time_bin)\r\n\r\n#get network of overlapping times\r\nnetwork_overlap_time<-Start_end_to_edgelist(Start_end_events, 5, directed=FALSE)$network_overlap_time\r\n\r\nStart_end_events$Start<-as.numeric(Start_end_events$Start)\r\n\r\nStart_end_events$End<-as.numeric(Start_end_events$End)\r\nevents$Start<-as.numeric(events$Start)\r\nevents$End<-as.numeric(events$End)\r\n\r\n###########CV  and assortment of the overlap networkcv\r\ncv_overlap_time<-cv_network(network_overlap_time)\r\n\r\n#erase groups not duplicated (i.e. it is not a group because it has only one individual)\r\ngroups<-groups[ groups$group %in% groups$group[duplicated(groups$group)],]\r\n\r\n#set the groups in same order\r\ngroups<-groups[(groups$tag %in% rownames(network_overlap_time)),]\r\nnetwork_overlap_time<-network_overlap_time[(rownames(network_overlap_time) %in% groups$tag),]\r\nnetwork_overlap_time<-network_overlap_time[,(colnames(network_overlap_time) %in% groups$tag)]\r\ngroups<-groups[order(groups$tag),]\r\nnetwork_overlap_time<-network_overlap_time[order(rownames(network_overlap_time)),]\r\nnetwork_o']",2,"Social network analyses, associations, individuals, consequences, constructing, analysing, challenging, new studies, decision making, data collection, constructing networks, guidance, multiple hypotheses testing, edge definition, a priori knowledge, independent, evaluation, breeding groups,"
"Data for: Zebra finch song ecology: monitoring of breeding, observational transects, focal and year-round acoustic recordings, and a large-scale simultaneous playback experiment","Male songbirds sing to establish territories and to attract mates. However, increasing reports of singing in non-reproductive contexts and by females show that song use is more diverse than previously considered. Therefore, alternative functions of song, such as social cohesion and synchronisation of breeding, by and large were overlooked even in such well-studied species as the zebra finch (Taeniopygia guttata). In these social songbirds only the males sing and pairs breed synchronously in loose colonies following aseasonal rain events in their arid habitat. As males are not territorial, and pairs form long-term monogamous bonds early in life, conventional theory predicts that zebra finches should not sing much at all; yet they do and their song is the focus of hundreds of lab-based studies. We hypothesise that zebra finch song functions to maintain social cohesion and to synchronise breeding. Here we test this idea using data from five years of field studies, including observational transects, focal and year-round audio recordings, and a large-scale playback experiment. We show that zebra finches frequently sing while in groups, that breeding status influences song output at the nest and at aggregations, that they sing year-round, and that they predominantly sing when with their partner, suggesting that song remains important after pair formation. Our playback reveals that song actively features in social aggregations as it attracts conspecifics. Together, these results demonstrate that birdsong has important functions beyond territoriality and mate choice, illustrating its importance in coordination and cohesion of social units within larger societies.","['# Data analysis and figure plotting script for zebra finch song ecology data \r\n# By Hugo Loning, 2022\r\n\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\nlibrary(glmmTMB)\r\nlibrary(ggpubr)\r\n\r\n##### GANTT CHART 2016-2020 #####\r\n\r\ndata_info <- read_csv(""breeding_over_the_years_2016-2020.csv"")\r\n\r\n{\r\n  tick_marks <- data.frame(xa = c(12.5,12.5), ya = c(-100,0))\r\n  \r\nplot_rainfall_breeding <- ggplot(data = data_info, mapping = aes(x = month_index)) +\r\n    geom_bar(mapping = aes(y = monthly_rainfall_mm/150, fill = ""Rainfall""), stat = ""identity"", color = ""black"", size = 0.3, alpha = 0.8) +\r\n    geom_bar(mapping = aes(y = clutches_laid/140, fill = ""Clutches""), stat = ""identity"", color = ""black"", size = 0.3, alpha = 0.75) +\r\n    geom_line(data = tick_marks, aes(x = xa-12, y = ya), size = 0.3) +\r\n    geom_line(data = tick_marks, aes(x = xa, y = ya), size = 0.3) +\r\n    geom_line(data = tick_marks, aes(x = xa+12, y = ya), size = 0.3) +\r\n    geom_line(data = tick_marks, aes(x = xa+24, y = ya), size = 0.3) +\r\n    geom_line(data = tick_marks, aes(x = xa+36, y = ya), size = 0.3) +\r\n    scale_x_continuous(breaks = seq(6.5, 54.5, 12), \r\n                       labels = c(""2016"", ""2017"", ""2018"", ""2019"", ""2020"")) +\r\n    scale_fill_manual(values = c(""Clutches"" = ""#DDAA33"", ""Rainfall"" = ""#004488""), name = ""Type"") +\r\n    scale_y_continuous(breaks = seq(0, 1, 1/7), labels = c(""0"", ""20"", ""40"", ""60"", ""80"", ""100"", ""120"", ""140""),\r\n                       sec.axis = sec_axis(~ ., \r\n                                           seq(0,1,1/3), \r\n                                           labels = c(""0"", ""50"", ""100"", ""150""), \r\n                                           name = ""Monthly rainfall (mm)"")) +\r\n    coord_cartesian(xlim = c(3.25, 57.75), ylim = c(0, 1)) +\r\n    labs(x = ""Year"", y = ""Clutches laid per month"") +\r\n    theme_classic() +\r\n    theme(axis.title.y.right = element_text(angle=90),\r\n          legend.position = c(0.55, 0.85),\r\n          #legend.background = element_rect(color = ""black""),\r\n          legend.key.height= unit(0.2, \'cm\'),\r\n          legend.key.width= unit(0.2, \'cm\')); plot_rainfall_breeding\r\n}\r\n\r\n{\r\ntick_marks <- data.frame(xa = c(12.5,12.5), ya = c(-100,0.05))\r\n\r\nnest_recs2016 <- data.frame(xa = c(9.5,10.5), ya = c(0.3,0.3))\r\nplayback2017 <- data.frame(xa = c(21.5,22.5), ya = c(0.1,0.1))\r\nfocal_recs2018 <- data.frame(xa = c(32.5,34.5), ya = c(0.4,0.4))\r\nsongmeters2018 <- data.frame(xa = c(33.5,46.5), ya = c(0.2,0.2))\r\ntransects_2018 <- data.frame(xa = c(32.5,36.5), ya = c(0.5,0.5))\r\ntransects_2019 <- data.frame(xa = c(44.5,48.5), ya = c(0.5,0.5))\r\ntransects_2020 <- data.frame(xa = c(58.5,59.5), ya = c(0.5,0.5))\r\n\r\nplot_breeding2 <- ggplot(data = data_info, mapping = aes(x = month_index)) +\r\n  geom_bin2d(mapping = aes(y = rep.int(0.5,60), fill = clutches_laid/max(clutches_laid)), stat = ""identity"", alpha = 1) +\r\n  scale_fill_gradient(low=""white"", high = ""#DDAA33"", guide = ""none"") +\r\n  geom_line(data = tick_marks, aes(x = xa-12, y = ya), size = 0.3) +\r\n  geom_line(data = tick_marks, aes(x = xa, y = ya), size = 0.3) +\r\n  geom_line(data = tick_marks, aes(x = xa+12, y = ya), size = 0.3) +\r\n  geom_line(data = tick_marks, aes(x = xa+24, y = ya), size = 0.3) +\r\n  geom_line(data = tick_marks, aes(x = xa+36, y = ya), size = 0.3) +\r\n  geom_line(data = tick_marks, aes(x = xa+48, y = ya), size = 0.3) +\r\n  geom_line(data = tick_marks, aes(x = xa+48, y = ya), size = 0.3) +\r\n  \r\n  geom_line(data = nest_recs2016, aes(x = xa, y = ya), size = 3) +\r\n  geom_line(data = playback2017, aes(x = xa, y = ya), size = 3) +\r\n  geom_line(data = focal_recs2018, aes(x = xa, y = ya), size = 3) +\r\n  geom_line(data = songmeters2018, aes(x = xa, y = ya), size = 3) +\r\n  geom_line(data = transects_2018, aes(x = xa, y = ya), size = 3) +\r\n  geom_line(data = transects_2019, aes(x = xa, y = ya), size = 3) +\r\n  geom_line(data = transects_2020, aes(x = xa, y = ya), size = 3) +\r\n  scale_x_continuous(breaks = seq(6.5, 54.5, 12), \r\n                     labels = c(""2016"", ""2017"", ""2018"", ""2019"", ""2020"")) +\r\n  scale_y_continuous(breaks = seq(0.1,0.5,0.1),\r\n                     labels = c(""playback exp."", ""year-round rec."", ""nest rec."", ""focal rec."", ""transects"")) +\r\n  coord_cartesian(xlim = c(3.25, 57.9), ylim = c(0.05, 0.5)) +\r\n  labs(x = ""Year"", y = ""Activity"") +\r\n  theme_classic(); plot_breeding2\r\n}\r\n\r\n\r\n##### TRANSECT OBSERVATIONS 2018-2020 #####\r\n\r\n### Read + tidy data\r\ndata_transect <- read_csv(""transects_2018-2020.csv"")\r\ndata_transect <- data_transect %>% \r\n  filter(location != ""Unknown"" & # don\'t use unknown location\r\n           type != ""f"" & # don\'t use flying observations since zebra finches don\'t sing while flying (and there were probably some mistakes in the 2020 data in this regard by slightly inexperienced observers)\r\n           !is.na(song) # don\'t use observations where it is unclear whether it is song or not\r\n         ) %>% \r\n  mutate(song_binary = if_else(song == \'s\', 1, 0), # add numerical equivalent for song for the geom_smooth logistic regression\r\n         ']",2,"zebra finch, song ecology, breeding, monitoring, observational transects, focal recordings, acoustic recordings, playback experiment, social cohesion, synchronisation, male songbirds, non-reproductive contexts, female song, social songbirds, monogamous"
Data from: Male responses to sperm competition risk when rivals vary in their number and familiarity,"Males of many species adjust their reproductive investment to the number of rivals present simultaneously. However, few studies have investigated whether males sum previous encounters with rivals, and the total level of competition has never been explicitly separated from social familiarity. Social familiarity can be an important component of kin recognition and has been suggested as a cue that males use to avoid harming females when competing with relatives. Previous work has succeeded in independently manipulating social familiarity and relatedness among rivals, but experimental manipulations of familiarity are confounded with manipulations of the total number of rivals that males encounter. Using the seed beetle Callosobruchus maculatus we manipulated three factors: familiarity among rival males, the number of rivals encountered simultaneously, and the total number of rivals encountered over a 48-hour period. Males produced smaller ejaculates when exposed to more rivals in total, regardless of the maximum number of rivals they encountered simultaneously. Males did not respond to familiarity. Our results demonstrate that males of this species can sum the number of rivals encountered over separate days, and therefore the confounding of familiarity with the total level of competition in previous studies should not be ignored.","['\r\nsetwd(""C:/Users/samue/OneDrive/PhD/PhD OneDrive/Chapter Two Familiarity or Intensity/Data V2"")\r\n\r\nlibrary(lme4)\r\nlibrary(glmmML)\r\nlibrary(pscl)\r\nlibrary(AER)\r\nlibrary(MASS)\r\nlibrary(RVAideMemoire)\r\nlibrary(sjPlot)\r\n\r\nopt.expon.mod <- function(model, expon=seq(4/100, 4, length=100)) {\r\n  # Function that establishes to which exponent (power) a variable \r\n  # should be raised to maximise the probability of the supplied\r\n  # model\'s residuals testing as being normally distributed using \r\n  # the Shapiro-Wilks test.\r\n  #\r\n  # The function takes two arguments: the base model and the\r\n  # range of exponents to be tested. This range is set by default\r\n  # to 100 values between 0 (but not including 0) and 4.\r\n  #\r\n  # Author: Emile van Lieshout\r\n  # Date: 1/8/2013\r\n  \r\n  \r\n  # Could use the commented-out code below to test whether specific\r\n  # exponents < 1 equivalent to even roots are inappropriate if the\r\n  # response contains negative numbers.\r\n  #\tform <- formula(model)\r\n  #\tdep <- as.character(form[2])\r\n  #\trest <- as.character(c(form[c(1,3)]))\r\n  \r\n  start <- proc.time()\r\n  pvals <- numeric(length(expon))\r\n  opt_expon <- 0\r\n  resid.best <- NULL\r\n  \r\n  for(i in 1:length(expon)) {\r\n    form.upd <- as.formula(paste("".^"", expon[i], "" ~ ."", sep=\'\'))\r\n    model.upd <- update(model, form.upd)\r\n    \r\n    pvals[i] <- shapiro.test(residuals(model.upd))$p.value\r\n    \r\n    if(pvals[i] >= max(pvals)) {\r\n      opt_expon <- expon[i]\r\n      resid.best <- residuals(model.upd)\r\n    }\r\n  }\r\n  \r\n  cat(\'Maximum observed Shapiro-Wilks p =\', max(pvals), \'at exponent\', opt_expon, \'\\n\')\r\n  \r\n  layout(matrix(1:2, ncol=1))\r\n  plot(expon, pvals, \r\n       type=\'l\', \r\n       xlab=""Dependent variable exponent used"", \r\n       ylab=""Observed Shapiro-Wilk test p"")\r\n  hist(resid.best)\r\n  layout(1)\r\n  \r\n  cat(""Calculated in"", (proc.time() - start)[3], ""seconds\\n"")\r\n  \r\n}\r\n\r\n###EJACULATE WEIGHT###\r\n\r\ned=read.csv(""EjacData3F.csv"")\r\ned$Treatment=as.factor(ed$Treatment)\r\ned$FW=scale(ed$F.Weight)\r\ned$MW=scale(ed$W1)\r\nnames(ed)\r\n\r\n#Test patterns in male weight\r\nmw1=lmer(W1~Total+Sim*Fam+(1|Eb.ID),data=ed)\r\nplot(mw1)\r\nmw2=lm(W1~Total+Sim*Fam,data=ed)\r\nplot(mw2)\r\nanova(mw1)\r\nanova(mw2)\r\nmw3=lmer(W1~Total+Sim+Fam+(1|Eb.ID),data=ed)\r\nanova(mw1,mw3)\r\n\r\n###############\r\n\r\n\r\nEW1=lmer(Ejac~(Sim+Total)*Fam*MW+(1|Eb.ID),data=ed)\r\nEW1b=lm(Ejac~(Sim+Total)*Fam*MW,data=ed)\r\nplot(EW1)\r\nplot(EW1b)\r\nshapiro.test(EW1b$residuals)\r\nopt.expon.mod(EW1)\r\nopt.expon.mod(EW1b)\r\n#Can\'t correct\r\n#Remove 131, 149, 182\r\n\r\ned2=read.csv(""EjacData3Fb.csv"")\r\ned2$Treatment=as.factor(ed2$Treatment)\r\ned2$FW=scale(ed2$F.Weight)\r\ned2$MW=scale(ed2$W1)\r\nnames(ed2)\r\n\r\nEW1=lmer(Ejac~(Sim+Total)*Fam*MW+(1|Eb.ID),data=ed2)\r\nEW1b=lm(Ejac~(Sim+Total)*Fam*MW,data=ed2)\r\nplot(EW1)\r\nplot(EW1b)\r\nshapiro.test(EW1b$residuals)\r\nopt.expon.mod(EW1)\r\nopt.expon.mod(EW1b)\r\n#Can\'t correct\r\n#Remove 31, 131, 152\r\n\r\ned3=read.csv(""EjacData3Fc.csv"")\r\ned3$Treatment=as.factor(ed3$Treatment)\r\ned3$FW=scale(ed3$F.Weight)\r\ned3$MW=scale(ed3$W1)\r\nnames(ed3)\r\n\r\nT1=subset(ed3,Treatment==""1"")\r\nlength(T1$Ejac)\r\nT2=subset(ed3,Treatment==""2"")\r\nlength(T2$Ejac)\r\nT3=subset(ed3,Treatment==""3"")\r\nlength(T3$Ejac)\r\nT4=subset(ed3,Treatment==""4"")\r\nlength(T4$Ejac)\r\nT5=subset(ed3,Treatment==""5"")\r\nlength(T5$Ejac)\r\n\r\nEW1=lmer(Ejac~(Sim+Total)*Fam*MW+(1|Eb.ID),data=ed3)\r\nEW1b=lm(Ejac~(Sim+Total)*Fam*MW,data=ed3)\r\nplot(EW1)\r\nplot(EW1b)\r\nshapiro.test(EW1b$residuals)\r\nopt.expon.mod(EW1)\r\nopt.expon.mod(EW1b)\r\n#Now it is normal (p=0.071)\r\n\r\n###Test whether MW differs among treatments###\r\nmw1=lm(W1~Sim+Total+Fam,data=ed3)\r\nmw2=lm(W1~Total+Fam,data=ed3)\r\nanova(mw1,mw2)\r\n#No effect of Sim\r\nmw3=lm(W1~Sim+Fam,data=ed3)\r\nanova(mw1,mw3)\r\n#Marginally non-sig effect of total\r\nmw4=lm(W1~Sim+Total,data=ed3)\r\nanova(mw1,mw4)\r\n#Marginally non sig effect of Fam\r\n#Include MW as a covariate\r\n\r\n#Comparison of T1 and T4, to determine whether absolute exposure time is a problem.\r\nT1T4=rbind(T1,T4)\r\ntime1=lmer(Ejac~Treatment*MW+(1|Eb.ID),data=T1T4)\r\ntime2=lmer(Ejac~Treatment+MW+(1|Eb.ID),data=T1T4)\r\nanova(time1,time2)\r\n#Remove MW\r\ntime3=lmer(Ejac~MW+(1|Eb.ID),data=T1T4)\r\nanova(time2,time3)\r\n#No sig effect of treatment\r\n\r\n#Calculate number of groups\r\nlevels(ed3$Eb.ID)\r\n\r\n###Now test for ejaculate weight across all combinations\r\new1=lmer(Ejac~Sim+Total+Fam+MW\r\n         +Total*Fam\r\n         +Sim*MW\r\n         +Total*MW\r\n         +Fam*MW\r\n         +(1|Eb.ID),\r\n           data=ed3)\r\new2=lm(Ejac~Sim+Total+Fam+MW\r\n         +Total*Fam\r\n         +Sim*MW\r\n         +Total*MW\r\n         +Fam*MW,\r\n         data=ed3)\r\nanova(ew1,ew2)\r\n#Keep Eb.ID\r\new3=lmer(Ejac~Sim+Total+Fam+MW\r\n         +Total*Fam\r\n         +Sim*MW\r\n         +Total*MW\r\n         +Fam+MW\r\n         +(1|Eb.ID),\r\n         data=ed3)\r\nanova(ew1,ew3)\r\n#Fam by MW non sig\r\new4=lmer(Ejac~Sim+Total+Fam+MW\r\n         +Total*Fam\r\n         +Sim*MW\r\n         +Total+MW\r\n         +Fam*MW\r\n         +(1|Eb.ID),\r\n         data=ed3)\r\nanova(ew1,ew4)\r\n#Total by MW non sig\r\new5=lmer(Ejac~Sim+Total+Fam+MW\r\n     +Total*Fam\r\n     +Sim+MW\r\n    ']",3,"Keywords: sperm competition, male responses, rivals, social familiarity, reproductive investment, kin recognition, relatedness, experimental manipulations, seed beetle, ejaculates, separate days."
IPBES regions and sub-regions,"This dataset describes the IPBES regions and sub-regions and their corresponding countries or areas.ipbes_regions_subregions.csvGID_0: Corresponds to the GID from GADM (https://gadm.org/) and offered to facilitates the preparation of mapsISO_3166_alpha_3: Corresponds to the International Standard for country codes and codes for their subdivisions (https://www.iso.org/iso-3166-country-codes.html)Region: Corresponds to IPBES regionsSub-region: Corresponds to IPBES subregionsipbes_regions.png: map of the IPBES regions (Robinson projection https://epsg.io/54030)ipbes_sub_regions.png: map of the IPBES sub-regions (Robinson projection https://epsg.io/54030)ipbes_regions_shapefile.R: The R code used to link the ""ipbes_regions_subregions.csv"" to GADM (https://gadm.org/) boundariesipbes_region_mapping.R The R code used to map the IPBES regions and sub-regions.Inquiries: IPBES technical support unit on knowledge and data - tsu.data@ipbes.net","['#####################\r\n## IPBES regions and sub-regions\r\n## ploting maps\r\n## Technical Support Unit on Data\r\n## DOI 10.5281/zenodo.3923634\r\n## Contact: Joy Kumagai (joy.kumagai@senckenberg.de)\r\n#####################\r\n\r\nrm(list=ls())\r\n##### Load Packages ######\r\nlibrary(sf) #spatial \r\nlibrary(graticule)\r\n\r\nipbes <- st_read(""Results/IPBES_Regions_Subregions2.shp"")\r\n#### Mapping ####\r\n# Projection \r\ncrs_robin <-  ""+proj=robin +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs""\r\nipbes_robin <- st_transform(ipbes, crs_robin)\r\n\r\nipbes_regions <- ipbes_robin %>% \r\n        dplyr::mutate(count = 1) %>% \r\n        dplyr::group_by(Region) %>% \r\n        dplyr::summarise(region_count = sum(count)) %>% \r\n        sf::st_cast()\r\n\r\n# add graticules\r\nlat <- c(-90, -60, -30, 0, 30, 60, 90)\r\nlon <- c(-180, -120, -60, 0, 60, 120, 180)\r\nxl <- range(lon)\r\nyl <- range(lat)\r\ngrat <- graticule(lon, lat, proj = crs_robin, xlim = xl, ylim = yl)\r\nlabs <- graticule_labels(lon, lat, xline = min(xl), yline = min(yl), proj = crs_robin)\r\n\r\n# Regions Map \r\npng(""ipbes_regions_no_borders.png"", width = 10, height = 8, units = ""in"", res = 600)\r\npalette_r <- c(""#da511f"",""#669437"", ""grey"", ""#0c7573"", ""#891635"") # colors\r\nplot(grat, lty = 3, col = ""grey"",  main = ""\\n \\n \\n \\n IPBES Regions"") # Plot graticules \r\nplot(ipbes_regions[,-c(2)], lwd = .5, pal = palette_r, add= TRUE) # Plot Countries\r\ntext(subset(labs, labs$islon), lab = parse(text = labs$lab[labs$islon]), pos = 1) # Degree labels \r\ntext(subset(labs, !labs$islon), lab = parse(text = labs$lab[!labs$islon]), pos = 2) # Degree Labels \r\nlegend(""bottomleft"", # legend\r\n       legend = sort(as.character(unique(ipbes$Region))),\r\n       title = """", \r\n       fill = c(palette_r),\r\n       cex = 0.9,\r\n       bty = ""n"")\r\n\r\ndev.off()\r\n\r\nipbes_subregions <- ipbes_robin %>% \r\n        dplyr::mutate(count = 1) %>% \r\n        dplyr::group_by(Sub_Region) %>% \r\n        dplyr::summarise(region_count = sum(count)) %>% \r\n        sf::st_cast()\r\n\r\n# Sub Regions Map\r\npng(""ipbes_subregions_no_borders.png"", width = 10, height = 8, units = ""in"", res = 600)\r\npalette_s <- c(""grey49"", ""#5D3A9B"", ""#009e73"", ""#fae442"", ""#0072b2"", ""#E66100"", ""#D35FB7"", ""#e69d00"", ""#994F00"", ""#E1BE6A"", ""#56b4e9"", ""#4B0092"", ""#669437"", ""pink"", ""grey"",""#0c7573"", ""#891635"", ""aliceblue"") \r\nplot(grat, lty = 3, col = ""grey"", main = ""\\n \\n \\n \\n IPBES Sub-Regions"")\r\nplot(ipbes_subregions[,-c(2)], lwd = .5, pal = palette_s, add=TRUE)\r\ntext(subset(labs, labs$islon), lab = parse(text = labs$lab[labs$islon]), pos = 1)\r\ntext(subset(labs, !labs$islon), lab = parse(text = labs$lab[!labs$islon]), pos = 2)\r\nlegend(""bottom"", \r\n       legend =  sort(as.character(unique(ipbes_robin$Sub_Region))),\r\n       title = """",\r\n       fill = palette_s,\r\n       cex = 0.75,\r\n       bty = ""n"",\r\n       ncol = 4)\r\ndev.off()\r\n\r\n## The assignment of countries or areas to specific groupings is for statistical convenience and does not imply any assumption regarding political or other affiliation of countries or territories by the United Nations and IPBES. For more details please consult the United Nations publication ""Standard Country or Area Codes for Statistical Use"", commonly referred to as the M49 standard.https://unstats.un.org/unsd/methodology/m49/\r\n#######################\r\n#####################']",3,"IPBES, regions, sub-regions, countries, GID, GADM, ISO_3166_alpha_3, international standard, IPBES map, Robinson projection, shapefile, R code, mapping, technical support, knowledge,"
"Artificial light at night, in interaction with spring temperature, modulates timing of reproduction in a passerine bird","The ecological impact of artificial light at night (ALAN) on phenological events such as reproductive timing is increasingly recognized. In birds, previous experiments under controlled conditions showed that ALAN strongly advances gonadal growth, but effects on egg-laying date are less clear. In particular, effects of ALAN on timing of egg-laying are found to be year-dependent, suggesting an interaction with climatic conditions such as spring temperature, which is known have strong effects on the phenology of avian breeding. Thus, we hypothesized that ALAN and temperature interact to regulate timing of reproduction in wild birds. Field studies have suggested that sources of ALAN rich in short wavelengths can lead to stronger advances in egg-laying date. We therefore tested this hypothesis in the great tit (Parus major), using a replicated experimental setup where eight previously unlit forest transects were illuminated with either white, green, or red LED light, or left dark as controls. We measured timing of egg-laying for 619 breeding events spread over six consecutive years and obtained temperature data for all sites and years. We detected overall significantly earlier egg-laying dates in the white and green light versus the dark treatment, and similar trends for red light. However, there was a strong inter-annual variability in mean egg-laying dates in all treatments, which was explained by spring temperature. We did not detect any fitness consequence of the changed timing of egg-laying due to ALAN, which suggests that advancing reproduction in response to ALAN might be adaptive.","['###################################\r\n## Load and prep data\r\n\r\nlibrary(""raster"")\r\nlibrary(""sp"")\r\nlibrary(""plyr"")\r\nlibrary(""climwin"")\r\nlibrary(""lme4"")\r\n\r\n\r\nsetwd(""C:/Users/DavideD/Dropbox/Lavoro/R work/LON/phenology paper"")\r\nsetwd(""C:/Users/dd114x/Dropbox/Lavoro/R work/LON/phenology paper"")\r\n\r\nclim <- read.csv(""temperatureAllSites3.csv"", sep="";"") #load data\r\nclim$year <- substring(clim$date,7,10) #add column with year\r\n\r\nclimT <- read.csv(""allMinTemp2011-2018Clean.csv"", sep="";"") #load data\r\nclimT$year <- substring(climT$date,7,10) #add column with year\r\n\r\nclimP <- read.csv(""allPrecip2011-2018Clean.csv"", sep="";"") #load data\r\nclimP$year <- substring(climP$date,7,10) #add column with year\r\n\r\n\r\n\r\nallbroods <- read.csv(""all broods_lighton.csv"", sep="","") #load data\r\ndistances <- read.csv(""distances.csv"", sep="","")\r\n\r\nallbroods <- merge(allbroods, distances, by=""UserPlaceName"") #merge nestboxes and light data\r\nGreatTit <- allbroods[which(allbroods$BroodSpecies==""14640""),] #only use great tits\r\nGreatTitFirst <- GreatTit[(GreatTit$BroodType==""0""),] #only use first clutch\r\n\r\nGreatTitFirst$LayDateA<-as.POSIXct(GreatTitFirst$LayDate,format = ""%d/%m/%Y"", sep=""/"") #convert dates\r\nGreatTitFirst$LayDateJ<-as.integer(format(GreatTitFirst$LayDateA, ""%j"")) \r\nGreatTitFirst<-GreatTitFirst[complete.cases(GreatTitFirst[ , 10]),]\r\n\r\nGreatTitFirst <- GreatTitFirst[which(GreatTitFirst$LayDateJ>72),] #remove odd record 13th march\r\nfirsteggs <- ddply(GreatTitFirst, c(""BroodYear"",""Site""), summarise, \r\n                   FirstEgg    = min(LayDateJ))\r\n\r\nGreatTitFirst <- merge(GreatTitFirst,firsteggs,by=c(""BroodYear"",""Site"")) #merge the datasets\r\nGreatTitFirst$eggdiff <- GreatTitFirst$LayDateJ-GreatTitFirst$FirstEgg #days after sites earliest\r\nGreatTitFirst <- GreatTitFirst[which(GreatTitFirst$eggdiff<=30),] #remove outliers over 30 days from first egg\r\n\r\nGreatTitFirst$LayDateA<-format.Date(GreatTitFirst$LayDateA, format = ""%d/%m/%Y"", sep=""/"") #convert dates\r\n\r\n\r\n\r\n########################################################\r\n## ClimWin Analysis for average temperature\r\nrandWin <- randwin(repeats = 250, window = ""sliding"", #repeats are no. of iterations\r\n                   xvar=list(Temp = climT$minTemp),\r\n                   cdate=climT$date, bdate=GreatTitFirst$LayDateA,\r\n                   baseline = lmer(LayDateJ ~ 1 + (1|Site), data = GreatTitFirst),\r\n                   range = c(151, 0), #31 of May is day 151, 1st of Jan is 0\r\n                   stat = ""mean"", func = ""lin"", type = ""absolute"", \r\n                   refday = c(31, 5), \r\n                   spatial = list(Bio = GreatTitFirst$Site2, Temp = climT$Site),\r\n                   cmissing = FALSE, cinterval = ""day"")\r\n\r\nhead(randWin)\r\nrandWin\r\n\r\n\r\nslidWin <- slidingwin(xvar=list(Temp = clim$temp),\r\n                      cdate=clim$date, bdate=GreatTitFirst$LayDateA,\r\n                      baseline = lmer(LayDateJ ~ 1 + (1|Site), data = GreatTitFirst),\r\n                      range = c(151, 0), \r\n                      stat = ""mean"", func = ""lin"", type = ""absolute"", \r\n                      refday = c(31, 5), #1 of May is julian = 31\r\n                      spatial = list(Bio = GreatTitFirst$Site2, Temp = clim$Site),\r\n                      cmissing = FALSE, cinterval = ""day"")\r\n\r\n\r\nhead(slidWin[[1]]$Dataset)\r\nsummary(slidWin[[1]]$BestModel)\r\n\r\n\r\nMassOutput <- slidWin[[1]]$Dataset\r\nMassRand <- randWin[[1]]\r\n\r\npvalue(dataset = slidWin[[1]]$Dataset, datasetrand = randWin[[1]], metric = ""C"", sample.size = 5)\r\n#p value=0.01126379\r\n\r\nplothist(dataset = MassOutput, datasetrand = MassRand)\r\nplotdelta(dataset = MassOutput)\r\nplotweights(dataset = MassOutput)\r\nplotbetas(dataset = MassOutput)\r\nplotwin(dataset = MassOutput)\r\n\r\nsave(file=""d:/climRun_Davide_minT.RData"")\r\n\r\n\r\n## ClimWin Analysis for min temperature\r\nrandWin <- randwin(repeats = 250, window = ""sliding"", #repeats are no. of iterations\r\n                   xvar=list(Temp = clim$temp),\r\n                   cdate=clim$date, bdate=GreatTitFirst$LayDateA,\r\n                   baseline = lmer(LayDateJ ~ 1 + (1|Site), data = GreatTitFirst),\r\n                   range = c(151, 0), #31 of May is day 151, 1st of Jan is 0\r\n                   stat = ""mean"", func = ""lin"", type = ""absolute"", \r\n                   refday = c(31, 5), \r\n                   spatial = list(Bio = GreatTitFirst$Site2, Temp = clim$Site),\r\n                   cmissing = FALSE, cinterval = ""day"")\r\n\r\nhead(randWin)\r\nrandWin\r\n\r\n\r\nslidWin <- slidingwin(xvar=list(Temp = climT$minTemp),\r\n                      cdate=climT$date, bdate=GreatTitFirst$LayDateA,\r\n                      baseline = lmer(LayDateJ ~ 1 + (1|Site), data = GreatTitFirst),\r\n                      range = c(151, 0), \r\n                      stat = ""min"", func = ""lin"", type = ""absolute"", \r\n                      refday = c(31, 5), #1 of May is julian = 31\r\n                      spatial = list(Bio = GreatTitFirst$Site2, Temp = climT$Site),\r\n                      cmissing = FALSE, cinterval = ""day"")\r\n\r\n\r\nhead(slidWin[[1]]$Dataset)\r\nsumm', 'require(lmerTest)\r\nrequire(ggplot2)\r\nrequire(plyr)\r\nrequire(dplyr)\r\nrequire(tidyr)\r\nrequire(broom)\r\nrequire(emmeans)\r\nrequire(ggpubr)\r\ntheme_set(theme_bw())\r\n\r\nsetwd(""D:/Users/DavideD/Dropbox/Lavoro/R work/LON/phenology paper"")\r\nsetwd(""C:/Users/dd114x/Dropbox/Lavoro/R work/LON/phenology paper"")\r\n\r\n#load, merge and subset datasets\r\nallbroods <- read.csv(""all broods_lighton_allyears.csv"", sep="","")\r\ndistances <- read.csv(""distances.csv"", sep="","")\r\nallbroods <- merge(allbroods,distances,by=""UserPlaceName"")\r\nGT <- allbroods[ which(allbroods$BroodSpecies==""14640""),]\r\nGTfirst <- GT[ which(GT$BroodType==""0""),]\r\n\r\n# LAY DATES\r\n#convert lay dates into julian days\r\n#GTfirst$LayDateA<-anydate(as.factor(GTfirst$LayDate))\r\nGTfirst$LayDateA<-as.POSIXct(GTfirst$LayDate,format = ""%d/%m/%Y"")\r\nGTfirst$LayDateJ<-as.integer(format(GTfirst$LayDateA, ""%j""))\r\nGTfirst<-GTfirst[complete.cases(GTfirst[ , 10]),]\r\n\r\n##define first broods: find first egg laid per year and delete entries 30 days after the first egg for each year\r\nGTfirst<-GTfirst[which(GTfirst$LayDateJ>72),] #remove odd record 13th march\r\nfirsteggs <- ddply(GTfirst, c(""BroodYear"",""Site""), summarise, \r\n                   FirstEgg    = min(LayDateJ))\r\n\r\nGTfirst<- merge(GTfirst,firsteggs,by=c(""BroodYear"",""Site""))\r\nGTfirst$eggdiff<-GTfirst$LayDateJ-GTfirst$FirstEgg\r\nGTfirst <- GTfirst[which(GTfirst$eggdiff<=30),]\r\n\r\n\r\n##calculate mean egg lay date in each treatment in each year\r\nmeans_first <- ddply(GTfirst, c(""Treatment"",""Site"",""BroodYear""), summarise, \r\n                     MeanLayDate  = mean(LayDateJ))\r\nmeans_dark <- means_first[which(means_first$Treatment==""Dark""),]\r\nmeans_white <- means_first[which(means_first$Treatment==""White""),]\r\nmeans_green <- means_first[which(means_first$Treatment==""Green""),]\r\nmeans_red<- means_first[which(means_first$Treatment==""Red""),]\r\n\r\n\r\n## merge all treatments, then calculate differences in mean egg laying dates \r\n## between each light treatment and the baseline (dark), per year per site\r\ndf_list <- list(means_dark,means_white,means_red,means_green)\r\nannual_means<-Reduce(function(x, y) merge(x, y, all=TRUE), df_list, accumulate=FALSE)\r\nfinal_means<-gather(annual_means, var, val, MeanLayDate) %>% \r\n  spread(Treatment, val) %>% \r\n  mutate(diffW = White-Dark, \r\n         diffG = Green-Dark,\r\n         diffR = Red-Dark)\r\n\r\nwrite.csv(final_means,""deviation_means.csv"") #this file is then manually transposed to conform it to the next analysis\r\n\r\n\r\n\r\n######################################################################################\r\n#RUN ANALYSIS USING CLIMWIN-DERIVED WINDOW (FIGURE 3 AND SUPPL.TABLE 3A)\r\n\r\n#load, merge and subset climate datasets\r\ntempall <- read.csv(""allMeanTemp2011-2018.csv"", sep="","",as.is=T) #mean temperature dataset\r\ndevmeans <- read.csv(""deviation_means_transposed.csv"", sep="","",as.is=T) #egg-laying deviation dataset\r\n\r\n\r\n#extract the best time window only\r\ntempall$date<-as.POSIXct(tempall$date,format = ""%d/%m/%Y"")\r\ny2013<-subset(tempall,date>=""2013/03/26"" & date<=""2013/04/11"")\r\ny2014<-subset(tempall,date>=""2014/03/26"" & date<=""2014/04/11"")\r\ny2015<-subset(tempall,date>=""2015/03/26"" & date<=""2015/04/11"")\r\ny2016<-subset(tempall,date>=""2016/03/26"" & date<=""2016/04/11"")\r\ny2017<-subset(tempall,date>=""2017/03/26"" & date<=""2017/04/11"")\r\ny2018<-subset(tempall,date>=""2018/03/26"" & date<=""2018/04/11"")\r\n\r\nwindowall<-rbind(y2013,y2014,y2015,y2016,y2017,y2018)\r\nwindowall$year<-as.integer(format(windowall$date, ""%Y""))\r\nwindowall$temp<-as.numeric(windowall$temp)\r\nwindowall$BroodYear<-windowall$year\r\n\r\nmeansT <- ddply(windowall, c(""BroodYear"",""Site""), summarise, \r\n                      N    = length(temp),\r\n                      avg  = mean(temp),\r\n                      sd   = sd(temp),\r\n                      se   = sd / sqrt(N))\r\n\r\nggplot(meansT, aes(x=BroodYear,y=avg))+\r\n  geom_errorbar(data=meansT,size=0.5,width=0.1,aes(ymin=avg-se,ymax=avg+se))+\r\n  geom_point(size=4)+\r\n  scale_x_discrete(""\\n Year"")+\r\n  scale_y_continuous(""Mean T (C) \\n  Mar 26th to Apr 11th     \\n"")+\r\n  theme(axis.line = element_line(colour = ""black""),\r\n        #panel.border = element_blank(),\r\n        #panel.background = element_blank(),\r\n        legend.position=""none"",\r\n        #legend.position = c(0.3,0.8),\r\n        #legend.title = element_blank(), \r\n        #legend.text = element_text(size=20),\r\n        #legend.key = element_blank(),\r\n        panel.grid.major=element_blank(),\r\n        panel.grid.minor=element_blank(),\r\n        #strip.text.x = element_text(size=16,family=""Times""),\r\n        axis.text.x = element_text(family=""Times"",size=22,colour = ""black""),\r\n        axis.text.y = element_text(family=""Times"",size=22,colour = ""black""),\r\n        axis.title.x=element_text(family=""Times"",size=28,,colour = ""black""),\r\n        axis.title.y=element_text(family=""Times"",size=28,angle=90))\r\n\r\n\r\ntemplay<-merge(meansT,devmeans,by=c(""Site"",""BroodYear""))\r\n\r\ntemp_all<-ggplot(templay, aes(x=avg,y=Deviation,col=Treatment,fill=Treatment))+\r\n  geom_point(size=4)+\r\n  geom_smooth(method=""l']",3,"- Artificial light at night
- Phenological events
- Reproductive timing
- Passerine bird
- Ecological impact 
- Gonadal growth 
- Egg-laying date 
- Year-dependent 
- Climatic conditions 
- Spring"
Images and statistical analysis of alfalfa root crowns from inside and outside disease rings caused by cotton root rot,"This repository contains raw image data of root crowns imaged using the backlit RhizoVision Crown platform of alfalfa plants from either inside or outside disease rings caused by cotton root rot for a manuscript to be submitted. Data files and the R scripts are included for complete statistical analysis associated with the imaged root crown set.Please cite both this repository and below journal article if reusing this data for a publication.Manuscript Title: Digital imaging to evaluate root system architectural changes associated with soil biotic factorsAuthors: Chakradhar Mattupalli, Anand Seethepalli, Larry M. York, Carolyn A. YoungJournal Article: https://doi.org/10.1094/PBIOMES-12-18-0062-R (open access)Image Filesafalfa_roots_raw_images.zip - 264 images in PNG format directly from a monochrome camera with gamma at 3.9 to make near-segmented raw images in greyscalealfalfa_feature_images.zip - 264 images in PNG format with a subset of computed features overlaidalfalfa_segmented_images.zip - 264 black and white, binary images in PNG format that result from simple thresholding of the raw imagesI_scale_1.png - an image of a 6 inch ruler for extracting pixel to physical unit conversionmetadata.csv - metadata output from RhizoVision Analyzer with diameter ranges and other options used for image analysisStatistical AnalysisalfalfaCRR_features_10012018.csv - extracted features from RhizoVision Analyzer v1.0.3 - called by name in R scriptRootArchitectureFieldStudyimagetoIDmap.csv - mapping of image file names to plot identity - called by name in R scriptmanuscript complete root rot RVC analysis.R - R script with all analysis that used the data from the imaged root crowns","['#Full statistical analysis and plotting for cotton root rot crowns\r\n#Authors: Larry York \r\n#for Cotton Root Rot manuscript using RhizoVision-Crown\r\n#December 12, 2018\r\n#Assume the input files below are in the current working directory\r\n\r\n#data loading ----\r\nlibrary(tidyverse)\r\n#first load larry\'s plotting themes\r\nelitetheme <-  theme(panel.grid.major = element_blank(), #save my ggplot2 theme\r\n                     panel.grid.minor = element_blank(),\r\n                     panel.background = element_blank(),\r\n                     axis.title.x = element_text(size=25,vjust=-0.5),\r\n                     axis.title.y = element_text(size=25,angle=90, vjust=.3),\r\n                     axis.text.x = element_text(colour=""black"", size=20),\r\n                     #axis.text.x = element_text(colour=""black"", size=20, \r\n                     #                          angle=45, hjust=.25, vjust=-.0001),\r\n                     axis.text.y = element_text(colour=""black"", size=20),\r\n                     plot.title = element_text(colour=""black"", size=25),\r\n                     legend.title = element_text(colour=""black"", size=25, face=""plain""),  \r\n                     legend.text = element_text(colour=""black"", size=20),\r\n                     legend.key = element_blank(), #get rid of boxes around legend lines)\r\n                     #legend.position =c(.1, .8),\r\n                     axis.line=element_line(),\r\n                     #legend.key.width = unit(2, ""cm""),\r\n                     plot.margin = unit(c(.5,.5,.5,.5), ""cm""),\r\n                     axis.ticks =  element_line(colour = ""black""),\r\n                     strip.background = element_rect(colour = NA, fill = ""white""),\r\n                     strip.text.x = element_text(size = 20, hjust=.05)\r\n                     )\r\n\r\nelitetheme2 <-  theme(panel.grid.major = element_blank(), #save my ggplot2 theme\r\n                      panel.grid.minor = element_blank(),\r\n                      panel.background = element_blank(),\r\n                      axis.text.x=element_blank(),\r\n                      axis.ticks.x=element_blank(), \r\n                      axis.title.x = element_text(size=25,vjust=-0.5),\r\n                      axis.title.y = element_text(size=25,angle=90, vjust=.3),\r\n                      #axis.text.x = element_text(colour=""black"", size=20),\r\n                      #axis.text.x = element_text(colour=""black"", size=20, \r\n                      #                          angle=45, hjust=.25, vjust=-.0001),\r\n                      axis.text.y = element_text(colour=""black"", size=20),\r\n                      plot.title = element_text(colour=""black"", size=25),\r\n                      legend.title = element_text(colour=""black"", size=25, face=""plain""),  \r\n                      legend.text = element_text(colour=""black"", size=20),\r\n                      legend.key = element_blank(), #get rid of boxes around legend lines)\r\n                      #legend.position =c(.1, .8),\r\n                      axis.line=element_line(),\r\n                      #legend.key.width = unit(2, ""cm""),\r\n                      plot.margin = unit(c(.5,.5,.5,.5), ""cm""),\r\n                      axis.ticks =  element_line(colour = ""black""),\r\n                      strip.background = element_rect(colour = NA, fill = ""white""),\r\n                      strip.text.x = element_text(size = 20, hjust=.05)\r\n                      \r\n                      \r\n)\r\n\r\n#read in files from current working directory\r\n\r\nCottonRootArchitectureFieldStudy <- read.csv(""RootArchitectureFieldStudyimagetoIDmap.csv"")\r\ncottonrootrotfeatures <- read.csv(""alfalfaCRR_features_10012018.csv"") #current paper version as of 12/5/2018\r\n\r\n\r\nnames2 <- function(x) as.data.frame(names(x)) #vertical list of column names with numbers\r\n\r\nnames(cottonrootrotfeatures)[1] <- ""FileName""\r\n\r\ncottonrootrotcombine <- merge(CottonRootArchitectureFieldStudy, cottonrootrotfeatures, by=""FileName"")\r\n\r\n#Means SE 3 zones ----\r\n\r\nrot.mean <-\r\n  cottonrootrotcombine %>% group_by(Zone) %>% summarize_at(vars(Median.Number.of.Roots:Coarse.Diameter.Frequency),mean, na.rm=T) #and you can do it for all the columns too...\r\n\r\nstd.error <- function(x) sd(na.omit(x))/sqrt(length(na.omit(x))) #use na.omit to drop NAs\r\n\r\nrot.se <-\r\n  cottonrootrotcombine %>% group_by(Zone) %>% summarize_at(vars(Median.Number.of.Roots:Coarse.Diameter.Frequency),std.error) #and you can do it for all the columns too...\r\n\r\nrot.mean.g <-\r\n  rot.mean %>% gather(key=""phene"", value=""mean"", -Zone)\r\n\r\nrot.se.g <-\r\n  rot.se %>% gather(key=""phene"", value=""se"", -Zone)\r\n\r\nrot.long.mese <- cbind(rot.mean.g, rot.se.g[3])\r\n\r\nalltraits3zones.MESE.plot <-\r\nggplot(rot.long.mese, aes(x=Zone, y=mean, colour=Zone)) +\r\n  geom_point(size=6)+\r\n  geom_linerange(aes(ymin= mean - se, ymax = mean + se), colour=""grey10"", size=1) +\r\n  #scale_color_manual(values=c(""firebrick4"", ""dodgerblue4""))+\r\n  ylab(""Feature Mean and SE"") +\r\n  facet_wrap(~phene, scales = ""free"") + \r\n  elitetheme2 + theme(legend.position = ""top"")\r\n#expand_limits(x = 0, y = 0)\r\n\r\nggsave']",3,"Images, statistical analysis, alfalfa root crowns, inside disease rings, outside disease rings, cotton root rot, repository, raw image data, backlit RhizoVision Crown platform, manuscript, data files, R scripts, complete statistical analysis, im"
Data from: Factors contributing to the accumulation of reproductive isolation: A mixed model approach,"The analysis of large datasets describing reproductive isolation between species has been extremely influential in the study of speciation. However, the statistical methods currently used for these data limit the ability to make direct inferences about the factors predicting the evolution of reproductive isolation. As a result, our understanding of iconic patterns and rules of speciation rely on indirect analyses that have clear statistical limitations. Phylogenetic mixed models are commonly used in ecology and evolution, but have not been applied to studies of reproductive isolation. Here I describe a flexible framework using phylogenetic mixed models to analyze data collected at different evolutionary scales, to test both categorical and continuous predictor variables, and to test the effect of multiple predictors on rates and patterns of reproductive isolation simultaneously. I demonstrate the utility of this framework by re-analyzing four classic datasets, from both animals and plants, and evaluating several hypotheses that could not be tested in the original studies: In the Drosophila and Bufonidae datasets, I found support for more rapid accumulation of reproductive isolation in sympatric species pairs compared to allopatric species pairs. Using Silene and Nolana, I found no evidence supporting the hypothesis that floral differentiation elevates postzygotic reproductive isolation. The faster accumulation of postzygotic isolation in sympatry is likely the result of species coexistence determined by the level of postzygotic isolation between species. In addition, floral trait divergence does not appear to translate into pleiotropic effects on postzygotic reproductive isolation. Overall, these methods can allow researchers to test new hypotheses using a single statistical method, while remedying the statistical limitations of several previous methods.","['#Sample analysis for Nolana using the genetic distance matrix insead of phylogeny\n\n#load libraries that you will need\nlibrary(MCMCglmm)\nlibrary(MASS)\n\n\n#Read genetic distance matrix and assign names\ngd<-read.csv(""nolanagd.csv"",header=F)\nmat<-as.matrix(gd[,2:12])\n\n\n#Take generalized inverse and format for MCMCglmm\n#generalized inverse of 1-gd matrix is required for the analysis\nM1<-1-mat\nM1a<- as(ginv(M1), ""dgCMatrix"")\ndimnames(M1a)<-list(gd[,1],gd[,1])\n#might get warning about supressing column names\n\n\n#If using phylogeny instead\n#Take generalized inverse and format for MCMCglmm\ntree<-read.tree(""nolana_ultrametric_tree.txt"")\nsp1inv<-inverseA(tree,nodes=""TIPS"")$Ainv\nsp2inv<-inverseA(tree,nodes=""TIPS"")$Ainv\n#use these matrices in place of M1a below\n\n\n#Trait data\n#Names in dataset need to match names for genetic distance matrix and/or phylogeny exactly\ndata<-read.csv(""nolana.csv"")\n\n#Creating subsets of the data\n#There should be no missing values for RI or the explanatory variables\ntotal<-subset(data,!is.na(data$Total_Isolation))\n\n#setting up uninformative priors based on y variable of interest\np.var<-var(total$Total_Isolation)\n\n#Model potential correlation between fixed effects by making variance-covariance matrix\nBvar<-diag(4)*p.var\nBvar[Bvar==0]<-p.var/4\n\n#prior for correlated fixed effects, residuals and two random effects ""G""\n\nprior1=list(B=list(mu=rep(0,4),V=Bvar),R=list(V=p.var/3, n=1), G=list(G1=list(V=p.var/3, n=1),G2=list(V=p.var/3,n=1)))\n\n\n#Run two MCMC chains for each analysis so that we can look at diagnostics\n#set seed so results can be reproduced\nset.seed(1001)\nm1a_1<-MCMCglmm(Total_Isolation~GeneticDistance+GeographicDistance+CorolDiam_Diff,random=~Species1+Species2,ginverse=list(Species1=M1a,Species2=M1a),prior=prior1,data=total,nitt=26000,thin=20,burnin=6000,verbose=F)\n\nset.seed(3001)\nm1a_2<-MCMCglmm(Total_Isolation~GeneticDistance+GeographicDistance+CorolDiam_Diff,random=~Species1+Species2,ginverse=list(Species1=M1a,Species2=M1a),prior=prior1,data=total,nitt=26000,thin=20,burnin=6000,verbose=F)\n\n#Diagnostics based on gelman-rubin approach\n#looking for values to be close to 1, values less than 1.1 acceptable\ngelman.diag(mcmc.list(m1a_1$Sol,m1a_2$Sol))\ngelman.diag(mcmc.list(m1a_1$VCV,m1a_2$VCV))\n\n#Adding the two chains together to profile posterior distribution\nm1asum<-as.mcmc(rbind(m1a_1$Sol,m1a_2$Sol))\nHPDinterval(m1asum)\n\n\n']",3,"Keywords: reproductive isolation, speciation, large datasets, statistical methods, phylogenetic mixed models, evolution, predictor variables, rates, patterns, sympatric species, allopatric species, Silene, Nolana, floral differentiation, post"
Estimation of excess mortality of patients with rheumatoid arthritis (RA) in Germany,Source code and data of estimation of excess mortality of people with RA in Germany - for use with the statistical software R (The R Foundation of Statistical Computing),"['rm(list=ls(all=TRUE))\r\n\r\n#######################\r\n#\r\n###   Input data    ###\r\n#\r\n#######################\r\n\r\n\r\n### Prevalence of RA in German men and women 2009 and 2015\r\n# Steffen A et al. Epidemiologie der rheumatoiden Arthritis in Deutschland, 2017, DOI 10.20364/VA-17.08, Fig 1, p. 8\r\np09.m  <- 1e-2*c(0.026, 0.043, 0.065, 0.088, 0.153, 0.233, 0.334, 0.489, # Men in 2009\r\n                 0.684, 0.811, 1.014, 1.200, 1.329, 1.292, 1.037, 0.837)\r\np15.m  <- 1e-2*c(0.030, 0.052, 0.076, 0.116, 0.165, 0.260, 0.401, 0.577, \r\n                 0.838, 1.091, 1.210, 1.483, 1.714, 1.710, 1.473, 1.068)\r\np09.f  <- 1e-2*c(0.078, 0.126, 0.192, 0.279, 0.424, 0.603, 0.856, 1.287, \r\n                 1.786, 2.027, 2.353, 2.590, 2.547, 2.198, 1.622, 0.972)\r\np15.f  <- 1e-2*c(0.094, 0.172, 0.243, 0.339, 0.505, 0.733, 1.013, 1.451, # Women in 2015\r\n                 2.008, 2.443, 2.670, 3.004, 3.295, 2.945, 2.240, 1.469)\r\nages.p <- c(17.5+5*(0:14), 92) # midpoints of reported age groups, 92 is approx life expectancy of 90y old\r\n\r\n\r\n# Incidence of RA (DOI 10.20364/VA-17.08, Tab 3, p. 10)\r\ni.m    <- 1e-5*c( 9.0, 30.2,  75.4, 119.9,  97.3) # Men 2014\r\ni.f    <- 1e-5*c(26.1, 73.3, 148.1, 188.4, 119.5) # Women 2014\r\nages.i <-      c(  25, 42.5,  57.5,  72.5,    87) # midpoints of reported age groups, 87 is approx life expectancy of 80y old\r\n\r\n\r\n### General mortality in Germany (logarithmic scale)\r\n# Period life table 2012 from Genesis database of the \r\n# Federal Statistical Office of Germany (DESTATIS)\r\nlog.gm.m  <- c(-8.795179291, -7.599213083, -7.548995919, -7.347645087, # men\r\n               -7.131449529, -6.676275331, -6.145331329, -5.573655173, \r\n               -5.020471797, -4.564083507, -4.183849644, -3.799568641, \r\n               -3.314886683, -2.732839266, -2.151835594, -1.583090808)\r\nlog.gm.f  <- c(-9.741849652, -9.005136484, -8.603149634, -8.436980597, # women\r\n               -8.179973895, -7.762456033, -7.255301367, -6.739186028, \r\n               -6.151846527, -5.654838109, -5.238719625, -4.836743928, \r\n               -4.434604996, -3.893039584, -3.187707288, -2.472702287)\r\nages.gm <- 15+5*0:15\r\n\r\n### Data from Danish RA register \r\n# Table 1 (right part) of Loppenthin et al, Morbidity and mortality in patients with\r\n# rheumatoid arthritis, J Comorb (9) 2019, DOI 10.1177/2235042X19853484\r\nagPc.P  <- 1e-2*c(.9, 2.9, 6.4, 12.4, 21.4, 24.3, 21.9,  9.8) # Age distribution RA pat\r\nagPc.C  <- 1e-2*c(.8, 2.8, 6.2, 12.0, 20.8, 24.3, 22.8, 10.4) # Age distribution controls\r\nmenPc.P <- 0.27                                               # Percentage of men in RA pat \r\nmenPc.C <- 0.277                                              # Percentage of men in controls  \r\nN.P     <- 21777                                              # number of RA pat\r\nN.C     <- 91024                                              # number of controls\r\nlogHR   <- log(0.59)                                          # hazard ratio (see last paragraph of Results section)\r\n\r\n#######################\r\n#\r\n###  aux functions  ###\r\n#\r\n#######################\r\n\r\nlogit <- function(x){\r\n  return(log(x/(1-x)))\r\n}\r\n\r\ninv.logit <- function(x){\r\n  return(exp(x)/(1+exp(x)))\r\n}\r\n\r\n#######################\r\n#\r\n### pre-processing  ###\r\n#\r\n#######################\r\n\r\nages.e  <- seq(from = 17.5, to = 92.5, by = 7.5) # ages where we want results\r\n\r\n# sex-specific prevalences (needed for 2nd part)\r\nlogit_p1.m <- smooth.spline(ages.p, logit(p09.m), df = 7)\r\nlogit_p2.m <- smooth.spline(ages.p, logit(p15.m), df = 7)\r\n\r\nlogit_p1.f <- smooth.spline(ages.p, logit(p09.f), df = 7)\r\nlogit_p2.f <- smooth.spline(ages.p, logit(p15.f), df = 7)\r\n\r\nget_p.m <- function(t_, a_){\r\n  h_  <- t_ - 9.0\r\n  p1_ <- inv.logit(predict(logit_p1.m, x = a_ - h_    )$y)\r\n  p2_ <- inv.logit(predict(logit_p2.m, x = a_ - h_ + 6)$y)\r\n  # affine-linear interpolation btw 2009 and 2015\r\n  return((1 - h_/6)*p1_ + h_*p2_/6)\r\n}\r\n\r\nget_p.f <- function(t_, a_){\r\n  h_  <- t_ - 9.0\r\n  p1_ <- inv.logit(predict(logit_p1.f, x = a_ - h_    )$y)\r\n  p2_ <- inv.logit(predict(logit_p2.f, x = a_ - h_ + 6)$y)\r\n  # affine-linear interpolation btw 2009 and 2015\r\n  return((1 - h_/6)*p1_ + h_*p2_/6)\r\n}\r\n\r\nget_p <- function(t_, a_, isMale = TRUE){\r\n  if(isMale)\r\n     return(get_p.m(t_, a_))\r\n  else\r\n     return(get_p.f(t_, a_))\r\n}\r\n\r\nlog_i.m  <- smooth.spline(ages.i, log(i.m), df = 4)\r\nlog_i.f  <- smooth.spline(ages.i, log(i.f), df = 4)\r\n\r\nget_i <- function(a_, isMale = TRUE){\r\n  if(isMale)\r\n     return(exp(predict(log_i.m, x = a_)$y))\r\n  else\r\n     return(exp(predict(log_i.f, x = a_)$y))\r\n}\r\n\r\nget_m <- function(a_, isMale = TRUE){\r\n   if(isMale)\r\n      return(exp(approx(ages.gm, log.gm.m, xout = ages.e, rule = 2)$y))\r\n   else\r\n      return(exp(approx(ages.gm, log.gm.f, xout = ages.e, rule = 2)$y))\r\n}\r\n\r\n\r\n\r\n#\r\n# Estimation of range of meaningful hazard ratios\r\n#\r\n\r\nnSensScen <- 1e5 # number of required samples of the sensitivity\r\n\r\nset.seed(18720518) # birthday of Bertrand Russell\r\nsnsSc.mat <- matrix(data = c( runif(length(ages.e) * nSensScen, m']",3,"excess mortality, estimation, patients, rheumatoid arthritis, Germany, source code, data, statistical software, R Foundation, Statistical Computing"
R code and example data for using genogeographic clustering approach,"While in recent years there have been considerable advances in discerning spatial genetic patterns within species, the task of identifying common patterns across species is still challenging. Approaches using new data from co-sampled species permit rigorous statistical analysis but are often limited to a small number of species; meta-analyses of published data can encompass a much broader range of species, but are usually restricted by uneven data properties. There is a need for new approaches that bring greater statistical rigour to meta-analyses, and are also able to discern more than a single spatial pattern among species.We propose a new approach for comparative multi-species meta-analyses of published population genetic data that addresses many existing limitations. This analysis takes a three-stage approach: (i) use common genetic metrics to measure location-specific diversity across the sampled range of each species, (ii) use an innovative graphing technique to describe spatial patterns within each species, and (iii) quantitatively cluster species by their similarity in pattern. We apply this technique to 21 species of intertidal invertebrate from the New Zealand coastline, to resolve common spatial patterns from disparate profiles of genetic diversity.The genogeographic curves are shown to successfully capture the known spatial patterns within each intertidal species, and readily permit statistical comparison of those patterns, regardless of sampling and marker inconsistencies. The species clustering technique is shown to discern groups of species that clearly share spatial patterns within groups but differ significantly among groups. The species groups defined were not identifiable a priori from their taxonomy or life history, but their spatial genetic patterns appear biologically relevant.Genogeographic species clustering provides a novel approach to discerning multiple common spatial patterns of diversity among a large number of species. It will permit more rigorous comparative studies from diverse published data, and can be easily extended to a wide variety of alternative measures of genetic diversity or divergence. We see the approach best used as an exploratory method, to uncover the patterns often hidden in multi-species communities, likely to be followed by more targeted model-testing analyses.","['## =============================================\n## R CODE FOR GENOGEOGRAPHIC SPECIES CLUSTERING\n## =============================================\n## This file contains R code to perform the analyses in the following paper:\n## ""Genogeographic clustering to identify cross-species concordance of spatial genetic patterns"",\n## by V. Arranz, R.M. Fewster, and S.D. Lavery.\n##\n## All code in this file is written by Rachel Fewster, r.fewster@auckland.ac.nz.\n\n## =============================================\n## DATA FILES NEEDED:\n## =============================================\n## Two data files are needed:\n##\n## 1. Coastline data: e.g. New-Zealand-Coastline-Lat-Long.csv.\n## This CSV contains three columns: X (longitude), Y (latitude), and Distance.\n## X and Y give the coordinates of the coastline of interest at a fine scale.\n## Distance is the value of the linear spatial coordinate corresponding to each (X, Y) location.\n## The code below assumes that the coastline follows a closed loop; otherwise  some adjustments are needed.\n##\n## 2. Species data: e.g. All-Species-Data-for-H.csv.\n## This has three columns: Species, Distance, and a third column that gives the value of a genetic metric\n## of interest. The name of the third column will be automatically detected, so different genetic metrics can\n## easily be used.  However, to detect the correct measure, it is assumed that the genetic metric is in the\n## column immediately after the Distance column. Other arrangements of columns will be incorrectly interpreted.\n## Data columns are as follows:\n## - The Species column gives the species sampled at a location.\n## - The Distance column gives the value of the linear spatial coordinate at the sampling location.\n## This is the same linear spatial coordinate used in the coastline data file. In our case it is the distance of\n## the sampling location from Cape Reinga, in km, following a clockwise trajectory around New Zealand.\n## - The third column gives the value of the genetic measure for the corresponding species at the sampling\n## location.  Any measure can be inserted here, but it is assumed that the measure is a number between 0 and 1.\n\n\n## =============================================\n## PRELIMINARY SET-UP:\n## =============================================\n## The code below establishes the coastal coordinates and species list.\n## It is run every time this code file is sourced.\n\n## Load packages (these must be installed first):\nrequire(RColorBrewer)\n\n## Read in the coordinates of the coastline from the coastline data file, e.g. New-Zealand-Coastline-Lat-Long.csv.\n## This CSV contains three columns: X (Longitude), Y (Latitude), and Distance, where Distance is the value of\n## the linear spatial coordinate corresponding to each (X, Y) location.  The resulting R object ""coastdat"" is used\n## as a lookup table for converting between Distance and (X, Y), and also for plotting colour-maps of the results.\ncoastdat <- read.csv(""New-Zealand-Coastline-Lat-Long.csv"", as.is=T)\n\n## Define the species included in the analyses: there are 21 species in our dataset.\n## Each of the species named must be included in the data CSV (see All-Species-Data-for-H.csv for an example).\nall.species <- c(""Asqu"", ""Astu"", ""Aten"", ""CglaspA"", ""CglaspC"", ""Corn"",\n                 ""Crad"", ""Echlo"", ""Hiris"", ""Hsex"", ""Lsma"", ""Paus"", ""Pcan"", ""Pelo"",\n                 ""Pnov"", ""Preg"", ""Psub"", ""Sbre"", ""Spel"", ""Zlut"", ""Zsub"")\n\n## Define a fixed set of colours so that each species is always plotted in the same colour:\ncolour.standard <- c(""black"", ""red"", ""darksalmon"", ""blue"", ""cyan"", ""magenta"", ""yellow"", ""gray"",\n                     ""maroon"", ""orange"", ""purple"", ""green3"", ""sienna"", ""#66A61E"", ""#E7298A"",\n                     ""#7570B3"", ""#D95F02"", ""#1B9E77"", ""plum3"", ""tan2"", ""steelblue"")\n\n\n## =============================================\n## COMMANDS TO RUN ANALYSES:\n## =============================================\n## The code below is enclosed in an if(FALSE){..} statement, which acts as a large comment block.\n## To run the code, copy and paste the commands below into R.\n\nif(F){\n        ## Begin by sourcing this code:\n        source(""Genogeographic-Cluster-Code.R"")\n\n        ## =========================================\n        ## FIT THE GENOGEOGRAPHIC CURVES\n        ## =========================================\n        ## Fit genogeographic curves to the data supplied using default selections for all options.\n        ## Press Enter to scroll to the second page, or use argument autoscroll=TRUE.\n        res.curves.H <- fitspline.wrap(""All-Species-Data-for-H.csv"")\n        ## NOTE: fitspline.wrap is set to scale-and-centre curves by default. If this is not wanted, use argument\n        ## scaleAndCentre=FALSE.\n\n        ## Under the default options, the object returned in res.curves.H is a data-frame with a column for\n        ## each species in the CSV file, and with 1000 rows giving the curve height for each species at each\n        ## of 1000 equally-spaced points along the distanc']",3,"genogeographic, clustering approach, R code, example data, spatial genetic patterns, species, co-sampled, statistical analysis, meta-analyses, published data, common genetic metrics, location-specific diversity, graphing technique, intertidal in"
Robust SOM Clustering,"This R script performs a combined SOM/SuperSOM clustering of the 640 administrative districs of India. Fowlkes-Mallows Similarity Index is used to identify robust initializations of the clustering. Data_India.txt is a specially conceived geographic database of 55 indicators, covering issues of economic activity, urban structure, socio-demographic development, consumption levels, infrastructure endowment and basic geographical positioning within the Indian space. Data refer to 2011 or to 2001-2011 evolutions.Warnng: This is an old version of the project. Version 1.2 available at https://zenodo.org/record/2563213#.XGQl01xKiiz","['####################################################################################\r\n#                                                                                  #\r\n# An R script to perform combined SOM/superSOM data clustering                     #\r\n#                                                                                  #\r\n# AuthorS: Joan Perez                                                              #\r\n#          Univ. Avignon Pays du Vaucluse, CNRS, ESPACE, UMR7300                   #\r\n#          Case Pasteur, Avignon                                                   #\r\n#          E-mail: joan.perez.etu@gmail.com                                        # \r\n#                                                                                  #\r\n#          Giovanni Fusco                                                          #\r\n#          Univ. Côte d\'Azur, CNRS, ESPACE, UMR7300                                #\r\n#          98 Bd Herriot, BP 3209, 06204 Nice, France                              #\r\n#          E-mail: giovanni.fusco@unice.fr                                         #\r\n#                                                                                  #\r\n####################################################################################\r\n\r\n# This script performs a combined SOM/SuperSOM clustering of the 640 administrative districts of India.\r\n# Fowlkes-Mallows Similarity Index is used to identify robust initializations of the clustering.\r\n# Data_India.txt is a specially conceived geographic database of 55 indicators, covering issues of economic activity,\r\n# urban structure, socio-demographic development, consumption levels, infrastructure endowment and \r\n# basic geographical positioning within the Indian space. Data refer to 2011 or to 2001-2011 evolutions.\r\n\r\n# References:\r\n# [1] Wehrens R., Buydens L., 2007, Self- and Super-organizing Maps in R: The ko-honen Package,\r\n#     Journal of Statistical Software, Vol. 21, Issue 5.\r\n# [2] Fusco G., Perez J., 2015, Spatial Analysis of the Indian Subcontinent: the Complexity Investigated \r\n#     through Neural Networks, CUPUM 2015 - 14th International Conference on Computers in Urban Planning \r\n#     and Urban Management, MIT, Cambridge (Ma.), July 5th-7th 2015, Proceedings, 287, 1-20,\r\n#     http://web.mit.edu/cron/project/CUPUM2015/proceedings/Content/analytics/287_fusco_h.pdf \r\n# [3] Perez J., 2015, Spatial Structures in India in the Age of Globalisation. A Data-Driven Approach, Phd in geography,\r\n#     University of Avignon (France)\r\n\r\n\r\n####################################################################################\r\n# 0.1 : Environment Preparation                                                    #\r\n####################################################################################\r\n\r\n# Packages loading (needs to be previously installed)\r\nlibrary(MASS)\r\nlibrary(class)\r\nlibrary(kohonen)\r\nsuppressPackageStartupMessages(library(dendextend))\r\n\r\n####################################################################################\r\n# 0.2. General Functions and Utilities                                             #\r\n####################################################################################\r\n\r\n## 0.2.1 Automated Number of Primes Selection / Initialization                       \r\n\r\nselect.primes <- function(nb)\r\n{\r\n  primes <- vector()\r\n  t <- 0\r\n  a <- 3\r\n  while (length(primes)<nb) {for (i in a) { for (j in 2:(i-1)) {if (i %% j == 0){t <- (t+1)} } \r\n                                            if (t==0){primes<- c(primes,i)}\r\n                                            t <- 0\r\n  }\r\n  a <- (a+1)\r\n  }\r\n  return(primes)\r\n}\r\n\r\n## 0.2.2 Calculation of the Best Similarity Score for each Seed\r\n\r\n# function returns two arguments, first one is the best seed in chr\r\n# second one is the average similarity score for each seed\r\nFM.Similarity <- function(results.list, clusters)\r\n{\r\nFM.matrix <- matrix(nrow=length(results.list), ncol=length(results.list))\r\nfor(i in 1:length(results.list)) {for(j in 1:length(results.list)) \r\n{FM.matrix[i,j]<-FM_index_R(clusters[,i+1],clusters[,j+1],assume_sorted_vectors = TRUE) }}\r\nrownames(FM.matrix) <- c(paste0(""seed"", primes))\r\ncolnames(FM.matrix) <- c(paste0(""seed"", primes))\r\naverage <- apply(FM.matrix, 1, mean)\r\nFM.df <- as.data.frame(cbind(FM.matrix, average))\r\nbest.seed <- row.names (FM.df[which.max(FM.df$average),])\r\nbest.seed <- paste0(""cluster."",best.seed)\r\noutput <- list(best.seed, average)\r\nreturn(output)\r\n}\r\n\r\n####################################################################################\r\n# 0.3 Prerequise on Df Origin                                                      #\r\n####################################################################################\r\n\r\n# Importation of the main database completed using bayesian inference, including NA\r\ndf.origin <- read.delim2(""Data_India.txt"")\r\n\r\n# Base 10 log transformation of Air_Flows\r\n# Replace -Inf by 0 within the dataframe\r\n# Replace N']",3,"Robust SOM, Clustering, R Script, India, Administrative districts, Fowlkes-Mallows Similarity Index, Data_India.txt, Geographic database, Economic activity, Urban structure, Socio-demographic development, Consumption levels, Infrastructure endowment"
Involvement of Burkholderiaceae and sulfurous volatiles in disease suppressive soils,"Scripts for statistical analysis and graphs of the publication "" Involvement of Burkholderiaceae and sulfurous volatiles in disease suppressive soils "".","['############################################################################################\r\n#                                                                                          #\r\n# Metabolomics analysis - PCA/PLSDA/SPLSDA                                                 #\r\n#                                                                                          #\r\n#                                                                                          #\r\n# Victor J. Carrion                                                                        #\r\n# Netherlands Institute of Ecology                                                         #\r\n# v.carrion@nioo.knaw.nl                                                                   #  \r\n# victorcarrionbravo@hotmail.com                                                           #\r\n#                                                                                          #\r\n############################################################################################\r\n\r\nlibrary(mixOmics)\r\nlibrary(ggplot2)\r\nlibrary(reshape2)\r\nlibrary(ggdendro)\r\nlibrary(grid)\r\n\r\n\r\n#Figure 2####\r\n#Figure 2B\r\ninputVOCs <- read.table(""1_input_DI.txt"", header = TRUE, sep = ""\\t"")\r\np <- ggplot(data = inputVOCs, aes(x=Treatment, y=DI)) + geom_boxplot(aes(fill=Treatment), width = 0.8) +\r\n  theme_bw() + \r\n  theme(axis.line = element_line(colour = ""black""),\r\n        panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        panel.background = element_blank())\r\np + \r\n  scale_fill_manual(values=c(""white"",""white"",""white"",""white"",""white"",""white"",""gray"",""black"",""white""))  + theme (axis.text=element_text(size=15))\r\n\r\n#Figure 3####\r\n#Figure 3B\r\ninputVOCs <- read.table(""2_input_in_vitro_VOCs.txt"", header = TRUE, sep = ""\\t"")\r\np <- ggplot(data = inputVOCs, aes(x=Treatment, y=day4)) + geom_boxplot(aes(fill=Treatment), width = 0.8) +\r\n  theme_bw() +\r\n  theme(axis.line = element_line(colour = ""black""),\r\n        panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        panel.background = element_blank())\r\np + \r\n  scale_fill_manual(values=c(""green4"", ""royalblue"",""sienna2"",""grey"",""palevioletred3"", ""white"")) + theme (axis.text=element_text(size=15))\r\n\r\n\r\n#Figure 3C\r\n\r\nwild <- read.table(""3_input_pareto_norm.txt"", sep=""\\t"", header = TRUE)\r\n\r\nhead(wild)\r\ntail(wild)\r\nX_wild <- (wild[, 3:503])\r\nhead(X_wild)\r\ntail(X_wild)\r\ngroup_wild <- wild[, 2]\r\nhead(group_wild)\r\n\r\n#Preliminary analysis with PCA: PCA analysis on the volatomic data.\r\npca.wild <- pca(X_wild, ncomp = 3, center = TRUE, scale = TRUE)\r\npca.wild\r\nplot(pca.wild)\r\n\r\n#PCA with centroids\r\nplotIndiv(pca.wild, group = wild$Group, ind.names = FALSE, \r\n          ellipse = TRUE, legend = TRUE, title = \'VOCs Burkholderia spp.\', \r\n          size.xlabel = 20, size.ylabel = 20, size.axis = 25, pch = 15, cex = 5)\r\n\r\n#PCA without centroids\r\nplotIndiv(pca.wild, group = wild$Group, ind.names = FALSE, \r\n          ellipse = FALSE, legend = TRUE, title = \'VOCs Burkholderia spp.\', \r\n          size.xlabel = 20, size.ylabel = 20, size.axis = 25, pch = 15, cex = 5)\r\n\r\n\r\n#PLS-DA analysis\r\n#The PLS-DA and sPLS-DA analyses below will help refine the clusters of samples in a supervised fashion. \r\nY_wild <- wild$Group\r\nsummary(Y_wild)\r\n\r\n\r\n#this chunk takes ~ 5 min to run\r\nset.seed(32) # for reproducibility of the outputs of this code that performs random cross-validation sampling. \r\nwild.plsda.perf <- plsda(X_wild, Y_wild, ncomp = 6)\r\n# to speed up computation in this example we choose 5 folds repeated 10 times:\r\nwild.perf.plsda <- perf(wild.plsda.perf, validation = \'Mfold\', folds = 6,\r\n                        progressBar = FALSE, nrepeat = 10)\r\n\r\nhead(wild.perf.plsda$error.rate)\r\n\r\nplot(wild.perf.plsda, overlay = \'measure\', sd=TRUE)\r\n\r\nwild.plsda <- plsda(X_wild, Y_wild, ncomp = 10)\r\n\r\n#PLS-DA with centroids\r\nplotIndiv(wild.plsda , comp = c(1,2),\r\n          group = wild$Group, ind.names = FALSE, \r\n          ellipse = TRUE, legend = TRUE, title = \'wild, PLSDA comp 1 - 2\')\r\n\r\nplot(wild.perf.plsda, overlay = \'measure\', sd=TRUE)\r\n\r\n\r\n\r\nwild.plsda <- plsda(X_wild, Y_wild, ncomp = 10)\r\n\r\n#PLS-DA without centroids\r\nplotIndiv(wild.plsda , comp = c(1,2),\r\n          group = wild$Group, style = \'ggplot2\', ind.names = FALSE, \r\n          ellipse = TRUE, legend = TRUE, title = \'VOCs Burkholderia spp.\', \r\n          size.xlabel = 20, size.ylabel = 20, size.axis = 25, pch = 15, cex = 5)\r\n\r\n#Figure 3D\r\n#differential abundance wild\r\n\r\ndf <-read.delim(""4_stats_KW_BH_dot_plot.txt"", header = T, sep = ""\\t"")\r\nhead(df, 10)\r\n\r\ng<-ggplot(df, aes(x=RT, y=Pgave_ratio)) +\r\n  geom_point(size=6, colour=df$Class_ratio_stats, pch=20)  +\r\n  theme_bw() \r\ng +   theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), \r\n            panel.background = element_rect(fill = ""white"", colour = ""black"",\r\n                                            linetype = 0.5, size = 1.5),\r\n            axis.ticks = element_line(colour = ""black"")']",3,"Burkholderiaceae, sulfurous volatiles, disease suppressive soils, statistical analysis, graphs, publication."
Data from: Transcriptomic plasticity of mesophotic corals among natural populations and transplants of Montastraea cavernosa in the Gulf of Mexico and Belize,"While physiological responses to low-light environments have been studied among corals on mesophotic coral ecosystems worldwide (MCEs; 30150 m), the mechanisms behind acclimatization and adaptation to depth are not well understood for most coral species. Transcriptomic approaches based on RNA sequencing are useful tools for quantifying gene expression plasticity, particularly in slow-growing species such as scleractinian corals, and for identifying potential functional differences among conspecifics. A tag-based RNA-Seq (Tag-Seq) pipeline was applied to quantify transcriptional variation in natural populations of the scleractinian coral Montastraea cavernosa from mesophotic and shallower environments across five sites in Belize and the Gulf of Mexico: Carrie Bow Cay, West and East Flower Garden Banks, Pulley Ridge, and Dry Tortugas. Regional site location was a stronger driver of gene expression patterns than depth. However, mesophotic corals among all sites shared similar regulation of metabolic and cell growth functional pathways that may represent common physiological responses to environmental conditions at depth. Additionally, in a transplant experiment at West and East Flower Garden Banks, colonies transplanted from mesophotic to shallower habitats diverged from the control mesophotic group over time, indicating depth-regulated plasticity of gene expression. When the shallower depth zone experienced a bleaching event, bleaching severity did not differ significantly between transplants and shallow controls, but gene expression patterns indicated variable regulation of stress responses among depth treatments. Coupled observational and experimental studies of gene expression among mesophotic and shallower M. cavernosa provide insights into the ability of this depth-generalist coral species to persist under varying environmental conditions.","['setwd(""~/path/to/local/directory"")\n\n# run these once, then comment out\n# source(""https://bioconductor.org/biocLite.R"")\n# biocLite(""BiocUpgrade"")\n# biocLite(""DESeq2"",dependencies=T)\n# biocLite(""arrayQualityMetrics"",dependencies=T)  # requires Xquartz, xquartz.org\n# biocLite(""BiocParallel"")\n# \n# install.packages(""pheatmap"")\n# install.packages(""VennDiagram"")\n# install.packages(""gplots"")\n# install.packages(""vegan"")\n# install.packages(""plotrix"")\n# install.packages(""ape"")\n# install.packages(""ggplot2"")\n# install.packages(""rgl"")\n# install.packages(""adegenet"")\n\n#---------------------\n# assembling data, running outlier detection, and fitting models\n# (skip this section if you don\'t need to remake models)\n\nlibrary(DESeq2)\nlibrary(arrayQualityMetrics)\n\n#read in counts\ncounts = read.table(""allcounts_enviro_mcav.txt"")\n\n# how many genes we have total?\nnrow(counts) \nncol(counts)\n\n# how does the data look? \nhead(counts)\n\n#---------------------\nkeep <- rowSums(counts) >= 10\ncountData <- counts[keep,]\nnrow(countData)\nncol(countData)\nwrite.csv(countData, file=""countData.csv"")\n\n# importing a design .csv file\ndesign = read.csv(""design_enviro.csv"", head=TRUE)\ndesign\nstr(design)\n\n# make big dataframe including all factors and interaction, getting normalized data for outlier detection\ndds = DESeqDataSetFromMatrix(countData=countData, colData=design, design=~ site+depth+site:depth)\n\n# reorders treatment factor according to ""control"" vs ""treatment"" levels\ndds$depth <- factor(dds$depth, levels = c(""shallow"",""mesophotic""))\n\n# for large datasets, rlog may take too much time, especially for an unfiltered dataframe\n# vsd is much faster and still works for outlier detection\nVsd=varianceStabilizingTransformation(dds)\n# rl=rlog(dds)\n\nlibrary(Biobase)\ne=ExpressionSet(assay(Vsd), AnnotatedDataFrame(as.data.frame(colData(Vsd))))\n# e=ExpressionSet(assay(rl), AnnotatedDataFrame(as.data.frame(colData(rl))))\n\n# running outlier detection\narrayQualityMetrics(e,intgroup=c(""site"",""depth""),force=T)\n# dev.off()\n# open the directory ""arrayQualityMetrics report for e"" in your working directory and open index.html\n# Array metadata and outlier detection overview gives a report of all samples, and which are likely outliers according to the 3 methods tested. I typically remove the samples that violate *1 (distance between arrays).\n# Figure 2 shows a bar plot of array-to-array distances and an outlier detection threshold based on your samples. Samples above the threshold are considered outliers\n# under Figure 3: Principal Components Analyses, look for any points far away from the rest of the sample cluster\n# use the array number for removal in the following section\n\n## if there were outliers, say, sample number MS_043 (array 42):\nouts=c(150)\ncountData=countData[,-outs]\nVsd=Vsd[,-outs]\n# rl=rl[,-outs]\ndesign=design[-outs,]\n\n# remaking model with outliers removed from dataset\ndds = DESeqDataSetFromMatrix(countData=countData, colData=design, design=~ site+depth+site:depth)\ndds$depth <- factor(dds$depth, levels = c(""shallow"",""mesophotic""))\n\n# save all these dataframes as an Rdata package so you don\'t need to rerun each time\nsave(dds,design,countData,Vsd,file=""initial.RData"")\n# save(dds,design,countData,rl,file=""initial.RData"")\n\n#---------------------\n# generating normalized variance-stabilized data for PCoA, heatmaps, etc\n\nload(""initial.RData"")\nlibrary(DESeq2)\nlibrary(BiocParallel)\n\n# creating normalized dataframe\nvsd=assay(Vsd)\n# vsd=assay(rl)\n# takes the sample IDs and factor levels from the design to create new column names for the dataframe\nsnames=paste(colnames(countData),design[,2],design[,3],sep=""."")\n# renames the column names\ncolnames(vsd)=snames\nsave(vsd,design,file=""vsd.RData"")\n\n#-------------------\n# EXPLORING SIMILARITIES AMONG SAMPLES\n\n# heatmap and hierarchical clustering:\nload(""vsd.RData"")\nlibrary(pheatmap)\n# similarity among samples\npdf(file=""heatmap_enviro_mcav.pdf"", width=50, height=50)\npheatmap(cor(vsd))\ndev.off()\n\n# Principal coordinates analysis\nlibrary(vegan)\nlibrary(rgl)\nlibrary(ape)\n\nconditions=design\nconditions$site <- factor(conditions$site, levels = c(""BLZ"",""WFGB"",""EFGB"",""PRG-DRT""))\n\n# creating a PCoA eigenvalue matrix\ndds.pcoa=pcoa(dist(t(vsd),method=""manhattan"")/1000)\nscores=dds.pcoa$vectors\n\n# how many good PC\'s do we have? Compared to random (""broken stick"") model\n# plotting PCoA eigenvalues\npdf(file=""PCoA_Manhattan.pdf"", width=6, height=6)\nplot(dds.pcoa$values$Relative_eig)\npoints(dds.pcoa$values$Broken_stick,col=""red"",pch=3)\ndev.off()\n# the number of black points above the line of red crosses (random model) corresponds to the number of good PC\'s\n\n# plotting PCoA\npdf(file=""PCoA_enviro_mcav.pdf"", width=12, height=6)\npar(mfrow=c(1,2))\nplot(scores[,1], scores[,2],col=c(""#f6e8c3"",""#01665e"",""#5ab4ac"",""#8c510a"")[as.numeric(as.factor(conditions$site))],pch=c(19,17)[as.numeric(as.factor(conditions$depth))], xlab=""Coordinate 1"", ylab=""Coordinate 2"", main=""Site"")\n# cluster overlay of site\nordiellipse(scores, conditions$site, label=F, draw= ""po', 'setwd(""~/path/to/local/directory"")\ngetwd()\n\n# run these once, then comment out\n# source(""http://bioconductor.org/biocLite.R"")\n# biocLite(""DESeq2"",dependencies=T)\n# biocLite(""arrayQualityMetrics"",dependencies=T)  # requires Xquartz, xquartz.org\n# biocLite(""BiocParallel"")\n\n# install.packages(""pheatmap"")\n# install.packages(""VennDiagram"")\n# install.packages(""gplots"")\n# install.packages(""vegan"")\n# install.packages(""plotrix"")\n# install.packages(""ape"")\n# install.packages(""ggplot2"")\n# install.packages(""rgl"") \n\n#---------------------\n# assembling data, running outlier detection, and fitting models\n# (skip this section if you don\'t need to remake models)\n\nlibrary(DESeq2)\nlibrary(arrayQualityMetrics)\n\n#read in counts\ncounts = read.table(""allcounts_enviro_zoox.txt"")\n\n# how many genes we have total?\nnrow(counts) \n\n# how does the data look? \nhead(counts)\n\n#---------------------\nkeep <- rowSums(counts) >= 10\ncountData <- counts[keep,]\nnrow(countData)\nncol(countData)\nwrite.csv(countData, file=""countData.csv"")\n\n# importing a design .csv file\ndesign = read.csv(""design_enviro.csv"", head=TRUE)\ndesign\nstr(design)\n\n# make big dataframe including all factors and interaction, getting normalized data for outlier detection\ndds = DESeqDataSetFromMatrix(countData=countData, colData=design, design=~ site+depth+site:depth)\n\n# reorders treatment factor according to ""control"" vs ""treatment"" levels\ndds$depth <- factor(dds$depth, levels = c(""shallow"",""mesophotic""))\n\n# for large datasets, rlog may take too much time, especially for an unfiltered dataframe\n# vsd is much faster and still works for outlier detection\nVsd=varianceStabilizingTransformation(dds)\n# rl=rlog(dds)\n\nlibrary(Biobase)\ne=ExpressionSet(assay(Vsd), AnnotatedDataFrame(as.data.frame(colData(Vsd))))\n# e=ExpressionSet(assay(rl), AnnotatedDataFrame(as.data.frame(colData(rl))))\n\n# running outlier detection\narrayQualityMetrics(e,intgroup=c(""site"",""depth""),force=T)\n# dev.off()\n# open the directory ""arrayQualityMetrics report for e"" in your working directory and open index.html\n# Array metadata and outlier detection overview gives a report of all samples, and which are likely outliers according to the 3 methods tested. I typically remove the samples that violate *1 (distance between arrays).\n# Figure 2 shows a bar plot of array-to-array distances and an outlier detection threshold based on your samples. Samples above the threshold are considered outliers\n# under Figure 3: Principal Components Analyses, look for any points far away from the rest of the sample cluster\n# use the array number for removal in the following section\n\n## if there were outliers, say, sample number MS_043 (array 42):\nouts=c(29,89,138,146,153,161,205,215,223,244)\ncountData=countData[,-outs]\nVsd=Vsd[,-outs]\n# rl=rl[,-outs]\ndesign=design[-outs,]\n\n# remaking model with outliers removed from dataset\ndds = DESeqDataSetFromMatrix(countData=countData, colData=design, design=~ site+depth+site:depth)\ndds$depth <- factor(dds$depth, levels = c(""shallow"",""mesophotic""))\n\n# save all these dataframes as an Rdata package so you don\'t need to rerun each time\nsave(dds,design,countData,Vsd,file=""initial.RData"")\n# save(dds,design,countData,rl,file=""initial.RData"")\n\n#---------------------\n# generating normalized variance-stabilized data for PCoA, heatmaps, etc\n\nload(""initial.RData"")\nlibrary(DESeq2)\nlibrary(BiocParallel)\n\n# creating normalized dataframe\nvsd=assay(Vsd)\n# vsd=assay(rl)\n# takes the sample IDs and factor levels from the design to create new column names for the dataframe\nsnames=paste(colnames(countData),design[,2],design[,3],sep=""."")\n# renames the column names\ncolnames(vsd)=snames\nsave(vsd,design,file=""vsd.RData"")\n\n#-------------------\n# exploring similarities among samples\n\n# heatmap and hierarchical clustering:\nload(""vsd.RData"")\nlibrary(pheatmap)\n# similarity among samples\npdf(file=""heatmap_enviro_zoox.pdf"", width=50, height=50)\npheatmap(cor(vsd))\ndev.off()\n\n# Principal coordinate analysis\nlibrary(vegan)\nlibrary(rgl)\nlibrary(ape)\n\nconditions=design\nconditions$site <- factor(conditions$site, levels = c(""BLZ"",""WFGB"",""EFGB"",""PRG-DRT""))\n\n# creating a PCoA eigenvalue matrix\ndds.pcoa=pcoa(dist(t(vsd),method=""manhattan"")/1000)\nscores=dds.pcoa$vectors\n\n# how many good PC\'s do we have? Compared to random (""broken stick"") model\n# plotting PCoA eigenvalues\npdf(file=""PCoA_Manhattan.pdf"", width=6, height=6)\nplot(dds.pcoa$values$Relative_eig)\npoints(dds.pcoa$values$Broken_stick,col=""red"",pch=3)\ndev.off()\n# the number of black points above the line of red crosses (random model) corresponds to the number of good PC\'s\n# there are 4 PC\'s in this case\n\n# plotting PCoA\npdf(file=""PCoA_enviro_zoox.pdf"", width=12, height=6)\npar(mfrow=c(1,2))\nplot(scores[,1], scores[,2],col=c(""#f6e8c3"",""#01665e"",""#5ab4ac"",""#8c510a"")[as.numeric(as.factor(conditions$site))],pch=c(19,17)[as.numeric(as.factor(conditions$depth))], xlab=""Coordinate 1"", ylab=""Coordinate 2"", main=""Site"")\n# cluster overlay of site\nordiellipse(scores, conditions$site, label=F, draw= ""p', 'setwd(""~/path/to/local/directory"")\n\n# run these once, then comment out\n# source(""http://bioconductor.org/biocLite.R"")\n# biocLite(""DESeq2"",dependencies=T)\n# biocLite(""arrayQualityMetrics"",dependencies=T)  # requires Xquartz, xquartz.org\n# biocLite(""BiocParallel"")\n\n# install.packages(""pheatmap"")\n# install.packages(""VennDiagram"")\n# install.packages(""gplots"")\n# install.packages(""vegan"")\n# install.packages(""plotrix"")\n# install.packages(""ape"")\n# install.packages(""ggplot2"")\n# install.packages(""rgl"") \n\n#---------------------\n# assembling data, running outlier detection, and fitting models\n# (skip this section if you don\'t need to remake models)\n\nlibrary(DESeq2)\nlibrary(arrayQualityMetrics)\n\n#read in counts\ncounts = read.table(""allcounts_trans_mcav.txt"")\n\n# how many genes we have total?\nnrow(counts) \nncol(counts)\n\n# how does the data look? \nhead(counts)\n\n#---------------------\nkeep <- rowSums(counts) >= 10\ncountData <- counts[keep,]\nnrow(countData)\nncol(countData)\nwrite.csv(countData, file=""countData.csv"")\n\n# importing a design .csv file\ndesign = read.csv(""design_trans.csv"", head=TRUE)\ndesign\nstr(design)\n\n# make big dataframe including all factors and interaction, getting normalized data for outlier detection\ndds = DESeqDataSetFromMatrix(countData=countData, colData=design, design=~ time+depth+time:depth)\n\n# reorders treatment factor according to ""control"" vs ""treatment"" levels\ndds$depth <- factor(dds$depth, levels = c(""shallow"",""mesophotic"",""transplant""))\ndds$time <- factor(dds$time, levels = c(""zero"",""six"",""twelve""))\n\n# for large datasets, rlog may take too much time, especially for an unfiltered dataframe\n# vsd is much faster and still works for outlier detection\nVsd=varianceStabilizingTransformation(dds)\n#rl=rlog(dds)\n\nlibrary(Biobase)\ne=ExpressionSet(assay(Vsd), AnnotatedDataFrame(as.data.frame(colData(Vsd))))\n#e=ExpressionSet(assay(rl), AnnotatedDataFrame(as.data.frame(colData(rl))))\n\n# running outlier detection\narrayQualityMetrics(e,intgroup=c(""time"",""depth""),force=T)\n# dev.off()\n# open the directory ""arrayQualityMetrics report for e"" in your working directory and open index.html\n# Array metadata and outlier detection overview gives a report of all samples, and which are likely outliers according to the 3 methods tested. I typically remove the samples that violate *1 (distance between arrays).\n# Figure 2 shows a bar plot of array-to-array distances and an outlier detection threshold based on your samples. Samples above the threshold are considered outliers\n# under Figure 3: Principal Components Analyses, look for any points far away from the rest of the sample cluster\n# use the array number for removal in the following section\n\n# not removing outliers to preserve experimental design\n# if there were outliers, say, arrays 138, 168, and 170:\n# outs=c(138, 168, 170)\n# countData=countData[,-outs]\n# Vsd=Vsd[,-outs]\n# rl=rl[,-outs]\n# design=design[-outs,]\n\n# not necessary if no outliers were found and removed\n# remaking model with outliers removed from dataset\n# dds = DESeqDataSetFromMatrix(countData=countData, colData=design, design=~ time+depth+time:depth)\n# dds$depth <- factor(dds$depth, levels = c(""shallow"",""mesophotic"",""transplant""))\n\n# save all these dataframes as an Rdata package so you don\'t need to rerun each time\nsave(dds,design,countData,Vsd,file=""initial.RData"")\n# save(dds,design,countData,rl,file=""initial.RData"")\n\n#---------------------\n# generating normalized variance-stabilized data for PCoA, heatmaps, etc\n\nload(""initial.RData"")\nlibrary(DESeq2)\nlibrary(BiocParallel)\n\n# creating normalized dataframe\nvsd=assay(Vsd)\n# vsd=assay(rl)\n# takes the sample IDs, colony IDs, and factor levels from the design to create new column names for the dataframe\nsnames=paste(colnames(countData),design[,3],design[,5],design[,8],sep=""."")\n# renames the column names\ncolnames(vsd)=snames\nsave(vsd,design,file=""vsd.RData"")\n\n#-------------------\n# EXPLAINING SIMILARITIES AMONG SAMPLES\n\n# heatmap and hierarchical clustering:\nload(""vsd.RData"")\n\nlibrary(pheatmap)\n# similarity among samples\npdf(file=""heatmap_trans_mcav.pdf"", width=20, height=20)\npheatmap(cor(vsd))\ndev.off()\n\n# Principal coordinates analysis\nlibrary(vegan)\nlibrary(rgl)\nlibrary(ape)\n\nconditions=design\nconditions$depth <- factor(conditions$depth, levels = c(""shallow"",""mesophotic"",""transplant""))\nconditions$time <- factor(conditions$time, levels = c(""zero"",""six"",""twelve""))\n\n# creating a PCoA eigenvalue matrix\ndds.pcoa=pcoa(dist(t(vsd),method=""manhattan"")/1000)\nscores=dds.pcoa$vectors\n\n# how many good PC\'s do we have? Compared to random (""broken stick"") model\n# plotting PCoA eigenvalues\npdf(file=""PCoA_Manhattan.pdf"", width=6, height=6)\nplot(dds.pcoa$values$Relative_eig)\npoints(dds.pcoa$values$Broken_stick,col=""red"",pch=3)\ndev.off()\n# the number of black points above the line of red crosses (random model) corresponds to the number of good PC\'s\n\n# plotting PCoA\npdf(file=""PCoA_trans_mcav.pdf"", width=12, height=6)\npar(mfrow=c(1,2))\nplot(scores[,1], scores[,2],col=c(""#225ea8"",""', 'setwd(""~/path/to/local/directory"")\n\n# run these once, then comment out\n# source(""http://bioconductor.org/biocLite.R"")\n# biocLite(""DESeq2"",dependencies=T)\n# biocLite(""arrayQualityMetrics"",dependencies=T)  # requires Xquartz, xquartz.org\n# biocLite(""BiocParallel"")\n\n# install.packages(""pheatmap"")\n# install.packages(""VennDiagram"")\n# install.packages(""gplots"")\n# install.packages(""vegan"")\n# install.packages(""plotrix"")\n# install.packages(""ape"")\n# install.packages(""ggplot2"")\n# install.packages(""rgl"") \n\n#---------------------\n# assembling data, running outlier detection, and fitting models\n# (skip this section if you don\'t need to remake models)\n\nlibrary(DESeq2)\nlibrary(arrayQualityMetrics)\n\n#read in counts\ncounts = read.table(""allcounts_trans_zoox.txt"")\n\n# how many genes we have total?\nnrow(counts) \nncol(counts)\n\n# how does the data look? \nhead(counts)\n\n#---------------------\nkeep <- rowSums(counts) >= 10\ncountData <- counts[keep,]\nnrow(countData)\nncol(countData)\nwrite.csv(countData, file=""countData.csv"")\n\n# importing a design .csv file\ndesign = read.csv(""design_trans.csv"", head=TRUE)\ndesign\nstr(design)\n\n# make big dataframe including all factors and interaction, getting normalized data for outlier detection\ndds = DESeqDataSetFromMatrix(countData=countData, colData=design, design=~ time+depth+time:depth)\n\n# reorders treatment factor according to ""control"" vs ""treatment"" levels\ndds$depth <- factor(dds$depth, levels = c(""shallow"",""mesophotic"",""transplant""))\ndds$time <- factor(dds$time, levels = c(""zero"",""six"",""twelve""))\n\n# for large datasets, rlog may take too much time, especially for an unfiltered dataframe\n# vsd is much faster and is recommended for datasets with more than 20 or so samples\nVsd=varianceStabilizingTransformation(dds)\n#rl=rlog(dds)\n\nlibrary(Biobase)\ne=ExpressionSet(assay(Vsd), AnnotatedDataFrame(as.data.frame(colData(Vsd))))\n#e=ExpressionSet(assay(rl), AnnotatedDataFrame(as.data.frame(colData(rl))))\n\n# running outlier detection\narrayQualityMetrics(e,intgroup=c(""time"",""depth""),force=T)\n# dev.off()\n# open the directory ""arrayQualityMetrics report for e"" in your working directory and open index.html\n# Array metadata and outlier detection overview gives a report of all samples, and which are likely outliers according to the 3 methods tested. I typically remove the samples that violate *1 (distance between arrays).\n# Figure 2 shows a bar plot of array-to-array distances and an outlier detection threshold based on your samples. Samples above the threshold are considered outliers\n# under Figure 3: Principal Components Analyses, look for any points far away from the rest of the sample cluster\n# use the array number for removal in the following section\n\n# not removing outliers to preserve experimental design\n# if there were outliers, say, sample number MS_145 (array 138):\n# outs=c(138, 168, 170)\n# countData=countData[,-outs]\n# Vsd=Vsd[,-outs]\n# rl=rl[,-outs]\n# design=design[-outs,]\n\n# not necessary if no outliers were found and removed\n# remaking model with outliers removed from dataset\n# dds = DESeqDataSetFromMatrix(countData=countData, colData=design, design=~ time+depth+time:depth)\n# dds$depth <- factor(dds$depth, levels = c(""shallow"",""mesophotic"",""transplant""))\n\n# save all these dataframes as an Rdata package so you don\'t need to rerun each time\nsave(dds,design,countData,Vsd,file=""initial.RData"")\n# save(dds,design,countData,rl,file=""initial.RData"")\n\n#---------------------\n# generating normalized variance-stabilized data for PCoA, heatmaps, etc\n\nload(""initial.RData"")\nlibrary(DESeq2)\nlibrary(BiocParallel)\n\n# creating normalized dataframe\nvsd=assay(Vsd)\n# vsd=assay(rl)\n# takes the sample IDs, colony IDs, and factor levels from the design to create new column names for the dataframe\nsnames=paste(colnames(countData),design[,3],design[,5],design[,8],sep=""."")\n# renames the column names\ncolnames(vsd)=snames\nsave(vsd,design,file=""vsd.RData"")\n\n#-------------------\n# EXPLAINING SIMILARITIES AMONG SAMPLES\n\n# heatmap and hierarchical clustering:\nload(""vsd.RData"")\n\nlibrary(pheatmap)\n# similarity among samples\npdf(file=""heatmap_trans_zoox.pdf"", width=20, height=20)\npheatmap(cor(vsd))\ndev.off()\n\n# Principal coordinate analysis\nlibrary(vegan)\nlibrary(rgl)\nlibrary(ape)\n\nconditions=design\nconditions$depth <- factor(conditions$depth, levels = c(""shallow"",""mesophotic"",""transplant""))\nconditions$time <- factor(conditions$time, levels = c(""zero"",""six"",""twelve""))\n\n# creating a PCoA eigenvalue matrix\ndds.pcoa=pcoa(dist(t(vsd),method=""manhattan"")/1000)\nscores=dds.pcoa$vectors\n\n# how many good PC\'s do we have? Compared to random (""broken stick"") model\n# plotting PCoA eigenvalues\npdf(file=""PCoA_Manhattan.pdf"", width=6, height=6)\nplot(dds.pcoa$values$Relative_eig)\npoints(dds.pcoa$values$Broken_stick,col=""red"",pch=3)\ndev.off()\n# the number of black points above the line of red crosses (random model) corresponds to the number of good PC\'s\n\n# plotting PCoA\npdf(file=""PCoA_trans_zoox.pdf"", width=12, height=6)\npar(mfrow=c(1,2))\nplot(scores[', 'setwd(""~/path/to/local/directory"")\ngetwd()\n# install.packages(""KOGMWU"")\nlibrary(KOGMWU)\n\n#---------------------------\n# full model, assessing changes in KOG expression across site and depth\n\n# loading KOG annotations\ngene2kog=read.table(""Mcavernosa_Cladocopium_iso2kogClass.tab"",sep=""\\t"")\nhead(gene2kog)\n\nadd=load(\'depth_lpv.RData\')\nadd # names of datasets in the package\nlpv.d=kog.mwu(depth.p,gene2kog) \nlpv.d \n\nads=load(\'site_lpv.RData\')\nads # names of datasets in the package\nlpv.s=kog.mwu(site.p,gene2kog) \nlpv.s \n\nadi=load(\'int_lpv.RData\')\nadi # names of datasets in the package\nlpv.i=kog.mwu(int.p,gene2kog) \nlpv.i\n\nbarplot(lpv.d$delta.rank,horiz=T)\n\n# compiling a table of delta-ranks to compare these results:\nktable=makeDeltaRanksTable(list(""Site""=lpv.s,""Depth""=lpv.d,""Site:Depth""=lpv.i))\n\nlibrary(RColorBrewer)\ncolor = colorRampPalette(rev(c(brewer.pal(n = 7, name =""RdBu""),""royalblue"",""darkblue"")))(100)\n  \n# Making a heatmap with hierarchical clustering trees: \npdf(file=""enviro_mcav_heatmap_enviro_lpv.pdf"", width=7, height=7)\npheatmap(as.matrix(ktable),clustering_distance_cols=""correlation"",color=color, cellwidth=15, cellheight=15, border_color=""white"") \nwhile (!is.null(dev.list()))  dev.off()\n\n# exploring correlations between datasets\npdf(file=""enviro_mcav_corrplot_enviro_lpv.pdf"", width=6, height=6)\npairs(ktable, lower.panel = panel.smooth, upper.panel = panel.cor)\nwhile (!is.null(dev.list()))  dev.off()\n#scatterplots between pairs\n# p-values of these correlations in the upper panel:\npdf(file=""enviro_mcav_pvalplot_enviro_lpv.pdf"", width=6, height=6)\npairs(ktable, lower.panel = panel.smooth, upper.panel = panel.cor.pval)\nwhile (!is.null(dev.list()))  dev.off()\n\n# plotting individual delta-rank correlations:\n# produces individual plots from the grid above\n# corrPlot(x=""Site"",y=""Depth"",ktable)\n# corrPlot(x=""Site"",y=""Site:Depth"",ktable)\n# corrPlot(x=""Depth"",y=""Site:Depth"",ktable)\n\n#---------------------------\n# assessing KOG differences across depth within site\n\n# loading KOG annotations\ngene2kog=read.table(""Mcavernosa_Cladocopium_iso2kogClass.tab"",sep=""\\t"")\nhead(gene2kog)\n\nblz=load(\'BLZ_lpv.RData\')\nblz # names of datasets in the package\nBLZ=kog.mwu(BLZ.p,gene2kog) \nBLZ \n\nwfgb=load(\'WFGB_lpv.RData\')\nwfgb # names of datasets in the package\nWFGB=kog.mwu(WFGB.p,gene2kog) \nWFGB \n\nefgb=load(\'EFGB_lpv.RData\')\nefgb # names of datasets in the package\nEFGB=kog.mwu(EFGB.p,gene2kog) \nEFGB \n\nprg_drt=load(\'PRG-DRT_lpv.RData\')\nprg_drt # names of datasets in the package\nPRG_DRT=kog.mwu(PRG_DRT.p,gene2kog) \nPRG_DRT \n\n# compiling a table of delta-ranks to compare these results:\nktable=makeDeltaRanksTable(list(""BLZ""=BLZ,""WFGB""=WFGB,""EFGB""=EFGB,""PRG-DRT""=PRG_DRT))\n\n# library(RColorBrewer)\n# color = colorRampPalette(rev(c(brewer.pal(n = 7, name =""RdYlBu""),""royalblue"",""darkblue"")))(100)\n  \n# Making a heatmap with hierarchical clustering trees: \npdf(file=""enviro_mcav_heatmap_site_lpv.pdf"", width=7, height=7)\npheatmap(as.matrix(ktable),clustering_distance_cols=""correlation"",color=color, cellwidth=15, cellheight=15, border_color=""white"") \nwhile (!is.null(dev.list()))  dev.off()\n\n# exploring correlations between datasets\npdf(file=""enviro_mcav_corrplot_site_lpv.pdf"", width=6, height=6)\npairs(ktable, lower.panel = panel.smooth, upper.panel = panel.cor)\nwhile (!is.null(dev.list()))  dev.off()\n#scatterplots between pairs\n# p-values of these correlations in the upper panel:\npdf(file=""enviro_mcav_pvalplot_site_lpv.pdf"", width=6, height=6)\npairs(ktable, lower.panel = panel.smooth, upper.panel = panel.cor.pval)\nwhile (!is.null(dev.list()))  dev.off()\n\n# creating a pub-ready corr plot\npdf(file=""enviro_mcav_corr_site_lpv.pdf"", width=7, height=5)\npar(mfrow=c(2,3))\ncorrPlot(x=""BLZ"",y=""WFGB"",ktable)\ncorrPlot(x=""BLZ"",y=""EFGB"",ktable)\ncorrPlot(x=""BLZ"",y=""PRG-DRT"",ktable)\ncorrPlot(x=""WFGB"",y=""EFGB"",ktable)\ncorrPlot(x=""WFGB"",y=""PRG-DRT"",ktable)\ncorrPlot(x=""EFGB"",y=""PRG-DRT"",ktable)\ndev.off()\n#---------------------------\n# assessing KOG differences within conserved genes across depth and site\n\n# loading KOG annotations\ngene2kog=read.table(""Mcavernosa_Cladocopium_iso2kogClass.tab"",sep=""\\t"")\nhead(gene2kog)\n\nblz.common=load(\'BLZ_common_lpv.RData\')\nblz.common # names of datasets in the package\nBLZ.common=kog.mwu(BLZ.common,gene2kog) \nBLZ.common \n\nwfgb.common=load(\'WFGB_common_lpv.RData\')\nwfgb.common # names of datasets in the package\nWFGB.common=kog.mwu(WFGB.common,gene2kog) \nWFGB.common \n\nefgb.common=load(\'EFGB_common_lpv.RData\')\nefgb.common # names of datasets in the package\nEFGB.common=kog.mwu(EFGB.common,gene2kog) \nEFGB.common \n\nprg_drt.common=load(\'PRG-DRT_common_lpv.RData\')\nprg_drt.common # names of datasets in the package\nPRG_DRT.common=kog.mwu(PRG_DRT.common,gene2kog) \nPRG_DRT.common \n\n# compiling a table of delta-ranks to compare these results:\nktable=makeDeltaRanksTable(list(""BLZ""=BLZ.common,""WFGB""=WFGB.common,""EFGB""=EFGB.common,""PRG-DRT""=PRG_DRT.common))\n\n# library(RColorBrewer)\n# color = colorRampPalette(rev(c(brewer.pal(n = 7, name =""RdYlBu""),""', 'setwd(""~/path/to/local/directory"")\ngetwd()\n# install.packages(""KOGMWU"")\nlibrary(KOGMWU)\n\n#---------------------------\n# full model, assessing changes in KOG expression across site and depth\n\n# loading KOG annotations\ngene2kog=read.table(""Mcavernosa_Cladocopium_iso2kogClass.tab"",sep=""\\t"")\nhead(gene2kog)\n\nadd=load(\'depth_lpv.RData\')\nadd # names of datasets in the package\nlpv.d=kog.mwu(depth.p,gene2kog) \nlpv.d \n\nads=load(\'site_lpv.RData\')\nads # names of datasets in the package\nlpv.s=kog.mwu(site.p,gene2kog) \nlpv.s \n\nadi=load(\'int_lpv.RData\')\nadi # names of datasets in the package\nlpv.i=kog.mwu(int.p,gene2kog) \nlpv.i\n\nbarplot(lpv.d$delta.rank,horiz=T)\n\n# compiling a table of delta-ranks to compare these results:\nktable=makeDeltaRanksTable(list(""Site""=lpv.s,""Depth""=lpv.d,""Site:Depth""=lpv.i))\n\nlibrary(RColorBrewer)\ncolor = colorRampPalette(rev(c(brewer.pal(n = 7, name =""RdBu""),""royalblue"",""darkblue"")))(100)\n  \n# Making a heatmap with hierarchical clustering trees: \npdf(file=""enviro_zoox_heatmap_enviro_lpv.pdf"", width=7, height=7)\npheatmap(as.matrix(ktable),clustering_distance_cols=""correlation"",color=color, cellwidth=15, cellheight=15, border_color=""white"") \nwhile (!is.null(dev.list()))  dev.off()\n\n# exploring correlations between datasets\npdf(file=""enviro_zoox_corrplot_enviro_lpv.pdf"", width=6, height=6)\npairs(ktable, lower.panel = panel.smooth, upper.panel = panel.cor)\nwhile (!is.null(dev.list()))  dev.off()\n#scatterplots between pairs\n# p-values of these correlations in the upper panel:\npdf(file=""enviro_zoox_pvalplot_enviro_lpv.pdf"", width=6, height=6)\npairs(ktable, lower.panel = panel.smooth, upper.panel = panel.cor.pval)\nwhile (!is.null(dev.list()))  dev.off()\n\n# plotting individual delta-rank correlations:\n# produces individual plots from the grid above\n# corrPlot(x=""Site"",y=""Depth"",ktable)\n# corrPlot(x=""Site"",y=""Site:Depth"",ktable)\n# corrPlot(x=""Depth"",y=""Site:Depth"",ktable)\n\n#---------------------------\n# assessing KOG differences across depth within site\n\n# loading KOG annotations\ngene2kog=read.table(""Mcavernosa_Cladocopium_iso2kogClass.tab"",sep=""\\t"")\nhead(gene2kog)\n\nblz=load(\'BLZ_lpv.RData\')\nblz # names of datasets in the package\nBLZ=kog.mwu(BLZ.p,gene2kog) \nBLZ \n\nwfgb=load(\'WFGB_lpv.RData\')\nwfgb # names of datasets in the package\nWFGB=kog.mwu(WFGB.p,gene2kog) \nWFGB \n\nefgb=load(\'EFGB_lpv.RData\')\nefgb # names of datasets in the package\nEFGB=kog.mwu(EFGB.p,gene2kog) \nEFGB \n\nprg_drt=load(\'PRG-DRT_lpv.RData\')\nprg_drt # names of datasets in the package\nPRG_DRT=kog.mwu(PRG_DRT.p,gene2kog) \nPRG_DRT \n\n# compiling a table of delta-ranks to compare these results:\nktable=makeDeltaRanksTable(list(""BLZ""=BLZ,""WFGB""=WFGB,""EFGB""=EFGB,""PRG-DRT""=PRG_DRT))\n\n# library(RColorBrewer)\n# color = colorRampPalette(rev(c(brewer.pal(n = 7, name =""RdYlBu""),""royalblue"",""darkblue"")))(100)\n  \n# Making a heatmap with hierarchical clustering trees: \npdf(file=""enviro_zoox_heatmap_site_lpv.pdf"", width=7, height=7)\npheatmap(as.matrix(ktable),clustering_distance_cols=""correlation"",color=color, cellwidth=15, cellheight=15, border_color=""white"") \nwhile (!is.null(dev.list()))  dev.off()\n\n# exploring correlations between datasets\npdf(file=""enviro_zoox_corrplot_site_lpv.pdf"", width=6, height=6)\npairs(ktable, lower.panel = panel.smooth, upper.panel = panel.cor)\nwhile (!is.null(dev.list()))  dev.off()\n#scatterplots between pairs\n# p-values of these correlations in the upper panel:\npdf(file=""enviro_zoox_pvalplot_site_lpv.pdf"", width=6, height=6)\npairs(ktable, lower.panel = panel.smooth, upper.panel = panel.cor.pval)\nwhile (!is.null(dev.list()))  dev.off()\n\n# creating a pub-ready corr plot\npdf(file=""enviro_zoox_corr_site_lpv.pdf"", width=7, height=5)\npar(mfrow=c(2,3))\ncorrPlot(x=""BLZ"",y=""WFGB"",ktable)\ncorrPlot(x=""BLZ"",y=""EFGB"",ktable)\ncorrPlot(x=""BLZ"",y=""PRG-DRT"",ktable)\ncorrPlot(x=""WFGB"",y=""EFGB"",ktable)\ncorrPlot(x=""WFGB"",y=""PRG-DRT"",ktable)\ncorrPlot(x=""EFGB"",y=""PRG-DRT"",ktable)\ndev.off()\n\n#---------------------------\n# assessing KOG differences within conserved genes across depth and site\n\n# loading KOG annotations\ngene2kog=read.table(""Mcavernosa_Cladocopium_iso2kogClass.tab"",sep=""\\t"")\nhead(gene2kog)\n\nblz.common=load(\'BLZ_common_lpv.RData\')\nblz.common # names of datasets in the package\nBLZ.common=kog.mwu(BLZ.common,gene2kog) \nBLZ.common \n\nwfgb.common=load(\'WFGB_common_lpv.RData\')\nwfgb.common # names of datasets in the package\nWFGB.common=kog.mwu(WFGB.common,gene2kog) \nWFGB.common \n\nefgb.common=load(\'EFGB_common_lpv.RData\')\nefgb.common # names of datasets in the package\nEFGB.common=kog.mwu(EFGB.common,gene2kog) \nEFGB.common \n\nprg_drt.common=load(\'PRG-DRT_common_lpv.RData\')\nprg_drt.common # names of datasets in the package\nPRG_DRT.common=kog.mwu(PRG_DRT.common,gene2kog) \nPRG_DRT.common \n\n# compiling a table of delta-ranks to compare these results:\nktable=makeDeltaRanksTable(list(""BLZ""=BLZ.common,""WFGB""=WFGB.common,""EFGB""=EFGB.common,""PRG-DRT""=PRG_DRT.common))\n\n# library(RColorBrewer)\n# color = colorRampPalette(rev(c(brewer.pal(n = 7, name =""RdYlBu""),', '# run these once, then comment out\n# install.packages(""MANOVA.RM"")\n# install.packages(""vegan"")\n# install.packages(""RVAideMemoire"")\n\nlibrary(MANOVA.RM)\nlibrary(vegan)\nlibrary(RVAideMemoire)\nlibrary(rcompanion)\n\nsetwd(""~/path/to/local/directory"")\n#--------------------------------------------\n# Data import\ntransplant <- read.csv(file=""FGB_Oct15-Sep16_transplant.csv"",header=TRUE,sep="","")\n\n#displays the format of all variables in the dataframe\nstr(transplant)\n# converts colony variable from integer to factor for statistical analyses\ntransplant$colony <- as.factor(transplant$colony)\n# transplant$time <- as.numeric(transplant$time)\nstr(transplant)\ntransplant\n\n# Data transformations for large numbers\n# performs linear transformation on variables with numbers in the millions to make it easier to graph\ntransplant$zoox_cm <- transplant$zoox_cm/1000000\n# transplant$chla_cm <- transplant$chla_cm/1000000\n# transplant$chlc_cm <- transplant$chlc_cm/1000000\nstr(transplant)\n\n#--------------------------------------------\n# Normality tests\n\n# creates a PDF for all the histograms\npdf(""transplant_hist.pdf"", height=6, width=12)\n# creates a plot window with 2 columns and 3 rows\npar(mfrow=c(2,3))\n# creates a histogram of data values for each variable\nhist(transplant$zoox_cm, breaks=15, xlab= expression(10^6 ~ cells ~ cm^-2), main=expression(Symbionts ~ cm^-2), col=""grey50"")\nhist(transplant$chla_cm, breaks=15, xlab= expression(ug ~ cm^-2), main=expression(Chl ~ a ~ cm^-2), col=""grey50"")\nhist(transplant$chlc_cm, breaks=15, xlab= expression(ug ~ cm^-2), main=expression(Chl ~ c[2] ~ cm^-2), col=""grey50"")\nhist(transplant$chl_ac, breaks=15, xlab= NA, main=expression(Chl ~ a:c[2]), col=""grey50"")\nhist(transplant$chla_cell, breaks=15, xlab= expression(pg ~ cell^-1), main=expression(Chl ~ a ~ cell^-1), col=""grey50"")\nhist(transplant$chlc_cell, breaks=15, xlab= expression(pg ~ cell^-1), main=expression(Chl ~ c[2] ~ cell^-1), col=""grey50"")\n# stops plotting, final save of the file\ndev.off()\n\n# runs a statistical test (Shapiro test) to determine if the data violates the assumptions of normality\n# p < 0.05 means non-normal data\nshapiro.test(transplant$zoox_cm)\nshapiro.test(transplant$chla_cm)\nshapiro.test(transplant$chlc_cm)\nshapiro.test(transplant$chl_ac)\nshapiro.test(transplant$chla_cell)\nshapiro.test(transplant$chlc_cell)\n# copy the output from each of these tests into a .txt file\n\n#----------------------------------------------\n# univariate repeated measures PERMANOVAs\n# run one for each response variable\n\nzoox_cm.rm <- RM(zoox_cm ~ site * depth * time, data=transplant, subject=""colony"", resampling=""Perm"", CPU=8)\nchla_cm.rm <- RM(chla_cm ~ site * depth * time, data=transplant, subject=""colony"", resampling=""Perm"", CPU=8)\nchlc_cm.rm <- RM(chlc_cm ~ site * depth * time, data=transplant, subject=""colony"", resampling=""Perm"", CPU=8)\nchl_ac.rm <- RM(chl_ac ~ site * depth * time, data=transplant, subject=""colony"", resampling=""Perm"", CPU=8)\nchla_cell.rm <- RM(chla_cell ~ site * depth * time, data=transplant, subject=""colony"", resampling=""Perm"", CPU=8)\nchlc_cell.rm <- RM(chlc_cell ~ site * depth * time, data=transplant, subject=""colony"", resampling=""Perm"", CPU=8)\n\n# copy the outputs of each of these objects into the same .txt file as before\n# the ANOVA-Type Statistic tables contain the p values you want to report\nzoox_cm.rm\nchla_cm.rm\nchlc_cm.rm\nchl_ac.rm\nchla_cell.rm\nchlc_cell.rm\n\n#----------------------------------------------\n# 3-way PERMANOVA\n# non-parametric multivariate test to identify effects of site, depth, and time on the 6 response variables\n# using the adonis function, creates a dissimilarity matrix of all the response variables ""transplant[c(7:12)]"" and tests it against all factors\npermanova <- adonis(transplant[c(6:11)] ~ site * depth * time, data=transplant, method=""euclidian"")\n# copy the outputs of this object into the same .txt file as before\npermanova\n# pairwise comparisons for depth and time\n# no need for site as there are only 2 factor levels (wb, eb)\ndepth.pair <- pairwise.perm.manova(transplant[c(6:11)], transplant$depth, nperm=999)\ntime.pair <- pairwise.perm.manova(transplant[c(6:11)], transplant$time, nperm=999)\n# copy the outputs of each of these objects into the same .txt file as before\ndepth.pair\ntime.pair\n\n#----------------------------------------------\n#----------------------------------------------\n# % change zoox density as a bleaching metric\n\nbleach <- read.csv(file=""FGB_Oct15-Sep16_bleaching.csv"",header=TRUE,sep="","")\nstr(bleach)\nbleach$colony <- as.factor(bleach$colony)\nstr(bleach)\n\nshapiro.test(bleach$bleach)\n\n# log transformation\nbleach$bleach.log <- log(bleach$bleach+1)\nstr(bleach)\n\nshapiro.test(bleach$bleach.log)\n\npdf(""bleaching_hist.pdf"", height=8, width=5)\npar(mfrow=c(2,1))\nhist(bleach$bleach, breaks=15, xlab= ""% remaining"", main=""Symbiont density May 2016-Sept 2016"", col=""grey50"")\nhist(bleach$bleach.log, breaks=15, xlab= ""log % remaining"", main=""Log symbiont density May 2016-Sept 2016"", col=""grey50"")\ndev.off()\n\n#-', '# install.packages(""EnvStats"")\nlibrary(EnvStats)\n\n#------------------------------------------------\n#data import\nsetwd(""~/path/to/local/directory"")\nfgb_templight = read.csv(""FGBOct15-Oct16_HOBO_templight.csv"", head=TRUE)\nhead(fgb_templight)\ntail(fgb_templight)\nstr(fgb_templight)\n\n#-------------------------------------------------\n# converting lux to PAR (following equation in Valiela (1995))\nfgb_templight$maxPAR <- fgb_templight$maxlight/51.2\nfgb_templight$avgPAR <- fgb_templight$avglight/51.2\nstr(fgb_templight)\n\n#---------------------------------------------------\n#subsetting dataset to exclude McGrail for full factorial design\nfgb_templight_EW <- subset(fgb_templight, Site != ""McGrail"")\nfgb_templight_EW$Site=factor(fgb_templight_EW$Site, levels=unique(fgb_templight_EW$Site)) \nhead(fgb_templight_EW)\ntail(fgb_templight_EW)\nstr(fgb_templight_EW)\n\n#---------------------------------------------------\n#calculating means for Site and depth\n\nmintemp.mean <- aggregate(fgb_templight_EW$mintemp, by=list(fgb_templight_EW$Site, fgb_templight_EW$Depth),FUN=\'mean\')\ncolnames(mintemp.mean) <- c(""Site"", ""Depth"", ""mintemp"")\nmintemp.mean\n\nmaxtemp.mean <- aggregate(fgb_templight_EW$maxtemp, by=list(fgb_templight_EW$Site, fgb_templight_EW$Depth),FUN=\'mean\')\ncolnames(maxtemp.mean) <- c(""Site"", ""Depth"", ""maxtemp"")\nmaxtemp.mean\n\navgtemp.mean <- aggregate(fgb_templight_EW$avgtemp, by=list(fgb_templight_EW$Site, fgb_templight_EW$Depth),FUN=\'mean\')\ncolnames(avgtemp.mean) <- c(""Site"", ""Depth"", ""avgtemp"")\navgtemp.mean\n\nmaxPAR.mean <- aggregate(fgb_templight_EW$maxPAR, by=list(fgb_templight_EW$Site, fgb_templight_EW$Depth),FUN=\'mean\', na.rm=TRUE)\ncolnames(maxPAR.mean) <- c(""Site"", ""Depth"", ""maxPAR"")\nmaxPAR.mean\n\navgPAR.mean <- aggregate(fgb_templight_EW$avgPAR, by=list(fgb_templight_EW$Site, fgb_templight_EW$Depth),FUN=\'mean\', na.rm=TRUE)\ncolnames(avgPAR.mean) <- c(""Site"", ""Depth"", ""avgPAR"")\navgPAR.mean\n\n#---------------------------------------------------\n#normality tests\n\n#Q-Q plots\ndata=fgb_templight_EW\npdf(""Q-Q Plot mintemp.pdf"")\nqqnorm(fgb_templight_EW$mintemp, main=""Q-Q Plot mintemp"")\nqqline(fgb_templight_EW$mintemp)\ndev.off()\n\npdf(""Q-Q Plot maxtemp.pdf"")\nqqnorm(fgb_templight_EW$maxtemp, main=""Q-Q Plot maxtemp"")\nqqline(fgb_templight_EW$maxtemp)\ndev.off()\n\npdf(""Q-Q Plot avgtemp.pdf"")\nqqnorm(fgb_templight_EW$avgtemp, main=""Q-Q Plot avgtemp"")\nqqline(fgb_templight_EW$avgtemp)\ndev.off()\n\npdf(""Q-Q Plot maxPAR.pdf"")\nqqnorm(fgb_templight_EW$maxPAR, main=""Q-Q Plot maxPAR"")\nqqline(fgb_templight_EW$maxPAR)\ndev.off()\n\npdf(""Q-Q Plot avgPAR.pdf"")\nqqnorm(fgb_templight_EW$avgPAR, main=""Q-Q Plot avgPAR"")\nqqline(fgb_templight_EW$avgPAR)\ndev.off()\n\n#Shapiro-Wilk test, p<0.05 indicates non-normality\nshapiro.test(fgb_templight_EW$mintemp)\nshapiro.test(fgb_templight_EW$maxtemp)\nshapiro.test(fgb_templight_EW$avgtemp)\n\nshapiro.test(fgb_templight_EW$maxPAR)\nshapiro.test(fgb_templight_EW$avgPAR)\n\n#--------------------------------------------------\n#data transformation\n\nboxcox(fgb_templight_EW$mintemp, optimize=TRUE)\n# Optimal Value:                   lambda = -1.404298\nboxcox(fgb_templight_EW$maxtemp, optimize=TRUE)\n# Optimal Value:                   lambda = -1.133448\nboxcox(fgb_templight_EW$avgtemp, optimize=TRUE)\n# Optimal Value:                   lambda = -1.188773\n\nboxcox(fgb_templight_EW$maxPAR, optimize=TRUE)\n# Optimal Value:                   lambda = 0.03973649\nboxcox(fgb_templight_EW$avgPAR, optimize=TRUE)\n# Optimal Value:                   lambda = 0.07758381\n\n\n# applying the BoxCox recommended transformation\nfgb_templight_EW$mintemp.log <- (fgb_templight_EW$mintemp ^ -1.404298 - 1)/-1.404298\nfgb_templight_EW$maxtemp.log <- (fgb_templight_EW$maxtemp ^ -1.133448 - 1)/-1.133448\nfgb_templight_EW$avgtemp.log <- (fgb_templight_EW$avgtemp ^ -1.188773 - 1)/-1.188773\n\nfgb_templight_EW$maxPAR.log <- (fgb_templight_EW$maxPAR ^ 0.03973649 - 1)/0.03973649\nfgb_templight_EW$avgPAR.log <- (fgb_templight_EW$avgPAR ^ 0.07758381 - 1)/0.07758381\n\nhead(fgb_templight_EW)\nstr(fgb_templight_EW)\n\n#Q-Q plots\npdf(""Q-Q Plot mintemp-log.pdf"")\nqqnorm(fgb_templight_EW$mintemp.log, main=""Q-Q Plot mintemp.log"")\nqqline(fgb_templight_EW$mintemp.log)\ndev.off()\n\npdf(""Q-Q Plot maxtemp-log.pdf"")\nqqnorm(fgb_templight_EW$maxtemp.log, main=""Q-Q Plot maxtemp.log"")\nqqline(fgb_templight_EW$maxtemp.log)\ndev.off()\n\npdf(""Q-Q Plot avgtemp-log.pdf"")\nqqnorm(fgb_templight_EW$avgtemp.log, main=""Q-Q Plot avgtemp.log"")\nqqline(fgb_templight_EW$avgtemp.log)\ndev.off()\n\npdf(""Q-Q Plot maxPAR-log.pdf"")\nqqnorm(fgb_templight_EW$maxPAR.log, main=""Q-Q Plot maxPAR.log"")\nqqline(fgb_templight_EW$maxPAR.log)\ndev.off()\n\npdf(""Q-Q Plot avgPAR-log.pdf"")\nqqnorm(fgb_templight_EW$avgPAR.log, main=""Q-Q Plot avgPAR.log"")\nqqline(fgb_templight_EW$avgPAR.log)\ndev.off()\n\n#Shapiro-Wilk test, p<0.05 indicates non-normality\nshapiro.test(fgb_templight_EW$mintemp.log)\nshapiro.test(fgb_templight_EW$maxtemp.log)\nshapiro.test(fgb_templight_EW$avgtemp.log)\n\nshapiro.test(fgb_templight_EW$maxPAR.log)\nshapiro.test(fgb_templight_EW$avgP']",3,"Transcriptomic plasticity, Mesophotic corals, Montastraea cavernosa, Gulf of Mexico, Belize, RNA sequencing, gene expression, acclimatization, adaptation, depth, scleractinian corals, metabolic pathways,"
"Data from: Morphometrics and redescription of Aphyllon fasciculatum and Aphyllon franciscanum (Orobanchaceae), two widespread but previously conflated species in western North America","We continue the taxonomic reevaluation of Aphyllon sect. Aphyllon by describing a widespread species throughout Western North America previously recognized within a polyphyletic A. fasciculatum. To support our description and revised key, we analyzed fifteen continuous and discrete characters across 156 herbarium specimens and iNaturalist observations sampled from across the geographic range. Principal component and multiple correspondence analyses reveal clear variation in floral characters. Discriminant analyses show that floral color, the calyx cup to calyx lobe ratio, calyx lobe length, and degree of bend in the corolla tube are useful distinguishing features, but not diagnostic in every case.","['## Analyses for Schneider and Benton 2021. Morphometrics and redescription of Aphyllon fasciculatum and Aphyllon franciscanum, two widespread but previously conflated species in western North America. Systematic Botany.\n\n## LINEAR DISCRIMINANT ANALYSIS##\nlibrary(tidyverse) #version 1.3.0\nlibrary(MASS) #version 7.3-51.4\nlibrary(klaR) #version 06-14\nlda_data=read.csv(file.choose()) ## use ""PCA_normalized_data.csv"" to be sure data is normalized with mean=0 sd=1\n\ncomplete = lda(Species~., data=lda_data[,1:9])\ncomplete ##prints coefficients and group means\n\n##Measuring accuracy via jackknife validation\njackknife <- lda(Species~., data=lda_data, CV=TRUE)\naccJack <- table(lda_data$Species[,1:9], jackknife$class)\naccJack\nsum(accJack[row(accJack) == col(accJack)]) / sum(accJack)\n\n## MULTIPLE CORRESPONDENCE ANALYSIS ##\nlibrary(""FactoMineR"") #version 2.3\nlibrary(""factoextra"") #version 1.0.7\n\n#read in data\nMCA_data=read.csv(file.choose()) # file= ""MCA_all.csv"" or ""MCA_confirmed_host.csv""\nsummary(MCA_data)\n\ndata= MCA_data[,1:7] #remove Species ID from dataset\n\nAph.mca <-MCA(data, graph=FALSE)\n\n##plot variable groups vs. principal dimensions\nfviz_mca_biplot(Aph.mca, choice = ""mca.cor"", \n            repel = TRUE,            \n            ggtheme = theme_minimal(),\n            shape.ind = 1,\n            shape.var = 2)\n            \n##plot variables vs. principal dimensions\nfviz_mca_biplot(Aph.mca, \n            repel = TRUE,            \n            ggtheme = theme_minimal(),\n             col.ind= ""gray40"",geom.ind= ""point"")\n\n## DISCRIMINANT CORRESPONDENCE ANALYSIS ##\n##TExPosition manual: https://cran.r-project.org/web/packages/TExPosition/TExPosition.pdf\n\nlibrary(TExPosition) #version 2.6.10.1\nDCA_data=read.csv(file.choose()) #read in same files as for MCA.\nsummary(DCA_data)\n\ndata = DCA_data[,1:7]\ndesign=DCA_data[,14:15]\ndca=tepDICA(data, make_data_nominal = T, DESIGN=design)\n\ndca$TExPosition.Data$fj #variable loadings\ndca$TExPosition.Data$eigs #eigenvalue\ndca$TExPosition.Data$assign$confusion #confusion matrix\ndca$TExPosition.Data$assign$r2 #r squared\n\n## To investigate a reduced data set, simply drop/subset columns\nreduced.data = data[,c(1,2,3,5)]\nreduced.dca=tepDICA(reduced.data, make_data_nominal = T, DESIGN=design)']",3,"morphometrics, redescription, Aphyllon fasciculatum, Aphyllon franciscanum, Orobanchaceae, taxonomic reevaluation, Western North America, polyphyletic, herbarium specimens, i"
Extracted data from primary literature examining impacts of recreational activities on freshwater ecosystems,"Aquatic ecosystems are attractive sites for recreation. However, human presence at or on aquatic ecosystems can have a range of ecological impacts, creating trade-offs between recreation as ecosystem service and biodiversity conservation. There is currently no synthesis of evidence regarding the ecological impacts associated with various forms of aquatic recreation, to compare the magnitude of effects between types of recreation. Therefore, conservation conflicts surrounding water-based recreation are difficult to manage. We conducted a global meta-analysis, differentiating various recreational impacts and the type of recreational uses in four categories: shore use, shoreline angling, swimming and boating; and studied ecological impacts directed at three levels of biological organization: individuals, populations, and communities. We screened over 13,000 articles and identified 94 suitable studies providing 701 effect sizes for inclusion in the meta-analysis. Aggregated across all animal and plant taxa, impacts of boating and shore use resulted in highly significant effects on almost all levels of biological organization. Regarding taxonomic groups, the most negative effects of water-based recreation were observed in invertebrates, whereas effects on birds were most pronounced at individual levels and not significant at community levels. From a conservation perspective, fostering water-based recreation and the ecological services they provide must be balanced with ecological impacts associated with the activities. Although generalizations are challenging, local scale effects of activity-specific constraints seem unlikely to be effective if other forms of water-based recreation continue.","['library(rlang)\r\nlibrary(metafor)\r\nlibrary(effsize)\r\nlibrary(compute.es)\r\nlibrary(esc)\r\nlibrary(meta)\r\nlibrary(forestplot)\r\nlibrary(MAd)\r\nlibrary(tidyr)\r\nlibrary(plyr)\r\nlibrary(dplyr)\r\nlibrary(splitstackshape)\r\nlibrary(schoolmath)\r\nlibrary(Gmisc)\r\n\r\n\r\n# first set working directory\r\nsetwd("""")\r\n# - where you stored the file ""Data_Meta-analysis_Schafft_etal.csv""\r\n\r\n####load data set ####\r\nrecrfull <- read.csv(""Data_Meta-analysis_Schafft_etal.csv"", sep="";"", header=T, dec=""."")\r\n\r\n#include only those that are definately freshwater studies\r\nrecrfull <- recrfull[(recrfull$Freshwater== 2),]\r\n\r\n# convert variables to numeric\r\nrecrfull$Longitud <-as.numeric(gsub("","","""",recrfull$Longitud,fixed=TRUE))\r\nrecrfull$C.Mean <-as.numeric(gsub("","","""",recrfull$C.Mean,fixed=TRUE))\r\nrecrfull$C.neg <-as.numeric(as.character(recrfull$C.neg))\r\nrecrfull$C.pos <-as.numeric(as.character(recrfull$C.pos))\r\nrecrfull$C.N<-as.numeric(as.character(recrfull$C.N))\r\nrecrfull$T.Mean <-as.numeric(gsub("","","""",recrfull$T.Mean,fixed=TRUE))\r\nrecrfull$F.Test <-as.numeric(gsub("","","""",recrfull$F.Test,fixed=TRUE))\r\nrecrfull$T.neg <-as.numeric(as.character(recrfull$T.neg))\r\nrecrfull$T.pos <-as.numeric(as.character(recrfull$T.pos))\r\nrecrfull$T.positive... <-as.numeric(as.character(recrfull$T.positive...))\r\nrecrfull$T.N<-as.numeric(as.character(recrfull$T.N))\r\nrecrfull$C.SD <-as.numeric(as.character(recrfull$C.SD))\r\nrecrfull$C.SE <-as.numeric(as.character(recrfull$C.SE))\r\nrecrfull$T.SD <-as.numeric(as.character(recrfull$T.SD))\r\nrecrfull$T.SE <-as.numeric(as.character(recrfull$T.SE))\r\nrecrfull$tvalue <-as.numeric(as.character(recrfull$t.value))\r\nrecrfull$df <-as.numeric(as.character(recrfull$df))\r\nrecrfull$p.value <-as.numeric(gsub("","","""",recrfull$p.value,fixed=TRUE))\r\nrecrfull$C.SE <-as.numeric(as.character(recrfull$C.SE))\r\n\r\nrec <- recrfull\r\nnames(rec)\r\n\r\n# exclude studies that compare different recreational activties \r\n#instead of comparing to control or different intensities \r\n\r\nrec$Real.control.............[is.na(rec$Real.control.............)]<-\'999\'\r\nrec<-subset(rec, Real.control............. %in% c(\'1\', \'2\', \'3\', \'5\',\'999\'))\r\n\r\n#### correcting direction of values ####\r\n\r\n#first get rid of NA\r\nrec$Meaning[is.na(rec$Meaning)]<-\'1\'\r\n\r\nfor(r in 1:nrow(rec)) {\r\n  if (rec[r,43]==\'2\') {\r\n    rec[r,49]<-rec[r,49]*-1;\r\n    rec[r,53]<-rec[r,53]*-1\r\n  } else \r\n    if (rec[r,43]==\'3\') {\r\n      if (rec[r,42]==""+""){\r\n        rec[r,49]<-rec[r,49]*-1;\r\n        rec[r,53]<-rec[r,53]*-1\r\n      }\r\n    }\r\n}\r\n\r\n####calculating effect size ####\r\n\r\nrec.sd <- rec[!is.na(rec$T.SD),]\r\n\r\nESg<-esc_mean_sd(grp1m = as.numeric(rec.sd$T.Mean), \r\n                 grp1sd = rec.sd$T.SD, \r\n                 grp1n = as.numeric(rec.sd$T.N),\r\n                 grp2m =as.numeric(rec.sd$C.Mean), \r\n                 grp2sd = rec.sd$C.SD, \r\n                 grp2n = as.numeric(rec.sd$C.N),\r\n                 es.type = ""g"")\r\n\r\nrec.sd$es<-ESg$es\r\nrec.sd$se<-ESg$var\r\n\r\n### create colomn es and se \r\n### to have identical number of rows in rec and rec.sd#\r\n\r\nrec$es<-NA\r\nrec$se<-NA\r\n\r\n#create the complementary to rec.sd# to be able to combine the data again \r\n\r\nrec.sd.t.na <- rec[is.na(rec$T.SD),]\r\nrec<-rbind(rec.sd,rec.sd.t.na) #combine rows to have have full data set again\r\n\r\nrec$Cohen.s.d<-NA\r\nrec$Cohen.s.d.1<- NA\r\n\r\n# calculate Hedges g with t-value / p-value\r\n\r\nrec.p<- rec[!is.na(rec$p.value),]\r\n\r\nEStg<-esc_t(t = as.numeric(rec.p$t.value), \r\n            p = as.numeric(rec.p$p.value), \r\n            grp1n = as.numeric(rec.p$T.N), \r\n            grp2n = as.numeric(rec.p$C.N), \r\n            es.type = ""g"")\r\n\r\n\r\nrec.p$Cohen.s.d<-EStg$es\r\nrec.p$Cohen.s.d.1<- EStg$var\r\n\r\n###combine data to full data set ###\r\nrec.p.na<- rec[is.na(rec$p.value),] #rec without rec.p\r\nrec<-rbind(rec.p,rec.p.na) #combine rows to have have full data set again\r\n\r\n\r\n#put values in right columns\r\nfor(r in 1:nrow(rec)) {if (is.na(rec[r,96])) {\r\n  rec[r,96]<-rec[r,60];\r\n  rec[r,97]<-rec[r,84] \r\n}\r\n}\r\n\r\n### calculate Odds Ratio ####\r\n\r\nES.OR<-escalc(measure = ""OR"", ai = T.pos, bi = T.neg, \r\n              ci = C.pos, di = C.neg, data = rec)\r\n\r\nrec$Odds.ratio<-ES.OR$yi\r\nrec$Risk.ratio<-ES.OR$vi\r\n\r\n\r\n### Converting Effect Sizes ####\r\n\r\nfor(r in 1:nrow(rec)) {if (!is.na(rec[r,68])) \r\n  {rec[r,96]<-rec[r,68]*sqrt(3)/pi;rec[r,97]<-rec[r,69]*sqrt(3)/pi^2 }}\r\n\r\n\r\n#### Effect size of correlations ####\r\nr.es <- res(Correlation.coefficient......R.., var.r = NULL, sample.size, id = rec$Study_ID ,data = rec)\r\n\r\nrec$Fishers.z <- r.es$d\r\nrec$Variance <- r.es$var.d\r\n\r\n# put values in right colums\r\nfor(r in 1:nrow(rec)) {if (is.na(rec[r,96])) {\r\n  rec[r,96]<-rec[r,91];\r\n  rec[r,97]<-rec[r,89] \r\n}\r\n}\r\n\r\n#### Effect size with F statistics ####\r\n\r\nf.es<-fes(F.Test, T.N, C.N, \r\n          level = 95, \r\n          cer = 0.2, dig = 2, \r\n          verbose = TRUE, id=Study_ID, data=rec)\r\n\r\nrec$Cohen.s.d <- f.es$g\r\nrec$Cohen.s.d.1 <- f.es$var.g\r\n\r\n#put values in right columns\r\nfor(r in 1:nrow(rec)) {if (is.na(rec[r,96])) {\r\n  rec[r,96]<-rec[r,60];\r\n  rec[r,97]<-rec[r']",3,"recreational activities, freshwater ecosystems, human presence, ecological impacts, trade-offs, biodiversity conservation, evidence synthesis, aquatic recreation, conservation conflicts, global meta-analysis, shore use, shoreline angling, swimming, boating, biological organization, individuals,"
Data and R code for statistical analyses on trap experiments,"Data and R code for statistical analyses related to trap experiments used in the article titled ""Attractant activity of host-related chemical blends on the Poultry Red Mite at different spatial scales"".Data are provided as two .csv files and codes as one .rmd file and one .R file as follows:- 6-trap laboratory experiments (data_PRM_lab_6-trap_experiments.csv and Lab_6-trap-experiments.rmd)- field experiments (data_PRM_field_trapping.csv and Field_trap_analyses.R).","['install.packages(""ggplot2"")\r\nlibrary(""ggplot2"")\r\ninstall.packages(""forcats"")\r\nlibrary(""forcats"")\r\n\r\nField<-read.table(""data_PRM_field_trapping.csv"", sep="";"", header=TRUE, dec="","", fill=TRUE)\r\n# subset per period\r\nempty<-subset(Field, period==""empty"")\r\nflock<-subset(Field, period==""flock"")\r\n\r\n# Subset per farm / empty period\r\nDIN <- subset(empty,  FarmOK == ""DIN"")\r\nDINplus <- subset(empty,  FarmOK == ""DINplus"")\r\nTES <- subset(empty,  FarmOK == ""TES"")\r\nSPT <- subset(empty,  FarmOK == ""SPT"")\r\nFOU <- subset(empty,  FarmOK == ""FOU"")\r\n\r\n# Subset per farm / flock period\r\nflock_DIN <- subset(flock,  FarmOK == ""DIN"")\r\nflock_SPT <- subset(flock,  FarmOK == ""SPT"")\r\n\r\n# paired samples Wilcoxon tests\r\nWilc_empty_DIN <- wilcox.test(no_mites ~ treatment, data = DIN, paired = TRUE)\r\nWilc_empty_DINplus<- wilcox.test(no_mites ~ treatment, data = DINplus, paired = TRUE)\r\nWilc_empty_TES<- wilcox.test(no_mites ~ treatment, data = TES, paired = TRUE)\r\nWilc_empty_SPT<- wilcox.test(no_mites ~ treatment, data = SPT, paired = TRUE)\r\nWilc_empty_FOU<- wilcox.test(no_mites ~ treatment, data = FOU, paired = TRUE)\r\nWilc_flock_DIN <- wilcox.test(no_mites ~ treatment, data = flock_DIN, paired = TRUE)\r\nWilc_flock_SPT<- wilcox.test(no_mites ~ treatment, data = flock_SPT, paired = TRUE)\r\nWilc_empty_DIN\r\nWilc_empty_DINplus\r\nWilc_empty_TES\r\nWilc_empty_SPT\r\nWilc_empty_FOU\r\nWilc_flock_DIN \r\nWilc_flock_SPT\r\n\r\n# boxplots\r\nempty$Farm <- fct_relevel(empty$Farm, c(""DIN(n=8)"", ""DIN+(n=13)"", ""TES(n=10)"", ""SPT(n=10)"",""FOU(n=10)""))\r\n\r\nggplot(empty, aes(x = Farm, y = (no_mites+1), fill=treatment)) + \r\n  geom_boxplot() + \r\n  scale_fill_manual(values=c(""blue"",""red"")) +\r\n  scale_y_continuous(trans=\'log10\')+\r\n  xlab("""") + ylab(""Mean no of trapped mites (log-scale)"") +\r\n  theme_classic(base_size = 15)\r\n\r\nggplot(flock, aes(x = Farm, y = (no_mites+1), fill=treatment)) + \r\n  geom_boxplot() + \r\n  scale_fill_manual(values=c(""blue"",""red"")) +\r\n  scale_y_continuous(trans=\'log10\')+\r\n  xlab("""") + ylab(""Mean no of trapped mites (log-scale)"") +\r\n  theme_classic(base_size = 15)\r\n\r\n']",3,"Data, R code, statistical analyses, trap experiments, attractant activity, host-related chemical blends, Poultry Red Mite, spatial scales, .csv files, .rmd file, .R file, laboratory experiments, field experiments, data_PR"
Removing invasive giant reed reshapes desert riparian butterfly and bird communities,"Giant reed (Arundo donax) is a prevalent invasive plant in desert riparian ecosystems that threatens wildlife habitat. From 2008 to 2018, under a United StatesMexico partnership, prescribed burns and herbicide applications were used to remove giant reed and promote native revegetation along the Rio Grande  Ro Bravo floodplain in west Texas, USA, and Mexico. Our goal was to explore the effects of the removal efforts on butterfly and bird communities and their habitat along the United States portion of the Rio Grande  Ro Bravo floodplain in Big Bend National Park, Texas. During spring and summer, 20162017, we surveyed butterflies, birds, and their habitat using ground-collected and remotely sensed data. Using a variety of generalized linear and N-mixture modeling routines and multivariate analyses, we found that the initial giant reed removal efforts removed key components of riparian habitat leading to reduced butterfly and bird communities. Within several years following management, giant reed levels remained low, while riparian habitat conditions and butterfly and bird communities largely rebounded, including many disturbance-sensitive butterfly species and riparian-associated bird species. Butterflies were most consistently associated with forb and grass cover, and birds with a remotely sensed index of greenness (the normalized difference vegetation index), several vegetation cover types, and habitat heterogeneity, habitat elements that were most common in locations that had the longest time to recover following management actions. Our results suggest that prescribed burns and herbicide applications, when used following protocols to minimize risk to wildlife, can limit the spread of giant reed in desert riparian systems and introduce habitat conditions that support diverse and abundant butterfly and bird communities.","[""\r\n###########  N-mixture models and model selection, birds\r\n\r\nlibrary(unmarked)\r\nlibrary(MuMIn)\r\n\r\n# covariate data \r\ncovs <- read.csv('visit and site covariates_allsites.csv')\r\n\r\n# detection data, one species (specify species file)\r\ndets <- read.csv('ISAbird_dets/BTSP_detection history.csv')\r\n\r\n# merge and reduce to analysis sites only\r\nNdat <- merge(dets,covs, by='site')\r\n\r\ngrps <- read.csv('site_groups.csv')\t\t# treatment groups\r\ntreats <- c('untreated_cane','new_treated','old_treated','reference')\r\ngrps <- grps[which(grps$treat %in% treats),]\r\nNdat <- Ndat[which(Ndat$site %in% grps$site),]\t# subset Ndat to groups\r\n\r\n\r\n### make unmarked data frame from Ndat\r\n# subset different data types\r\n\tcounts <- Ndat[,2:7]\t\t\t\t# detection history\r\n\tobs.cov <- Ndat[,8:19]\t\t\t\t# obs-level covariates\r\n\tsite.cov <- Ndat[,20:27]\t\t\t# site level covariates\r\n\r\n# unmarked data frame for Royle Count model (N-mixture model)\r\n\tdf <- unmarkedFramePCount(y=counts, siteCovs = site.cov, \r\n\t\tobsCovs = list(obs=obs.cov[,1:6], year=obs.cov[,7:12]))\r\n\tsummary(df)\r\n\r\n# models with det covars only\r\n\tm1 <- pcount(~1~1, df, K=100) \t# model with no covariates\r\n\tm2 <- pcount(~obs~1, df, K=100) \t# models with only det covariates\r\n\tm3 <- pcount(~year ~ 1, df, K=100)\t\t\r\n\tm4 <- pcount(~obs + year ~ 1, df, K=100)\r\n\r\n# model selection for detection only\r\n\tmlist<-fitList(m1=m1,m2=m2,m3=m3,m4=m4)\r\n\tmodSel(mlist)\r\n\r\n### models with best det covars (see above) and adding site covariates.\r\n### up to two site covars per model, no interactions\r\n\t# one var\r\n\tm5 <- pcount(~obs+year~p_arundo, df, K=100)\t\r\n\tm6 <- pcount(~obs+year~p_woody, df, K=100)\t\t\r\n\tm7 <- pcount(~obs+year~p_willow, df, K=100)\t\r\n\tm8 <- pcount(~obs+year~p_herb, df, K=100) \r\n\tm9 <- pcount(~obs+year~ndvi, df, K=100)\r\n\tm10 <- pcount(~obs+year~text.cv, df, K=100)\r\n\t\r\n\t# two vars\r\n\tm11 <- pcount(~obs+year~p_arundo+p_woody, df, K=100)\t\r\n\tm12 <- pcount(~obs+year~p_arundo+p_willow, df, K=100)\t\r\n\tm13 <- pcount(~obs+year~p_arundo+p_herb, df, K=100)\r\n\tm14 <- pcount(~obs+year~p_arundo+ndvi, df, K=100)\r\n\tm15 <- pcount(~obs+year~p_arundo+text.cv, df, K=100)\r\n\tm16 <- pcount(~obs+year~p_woody+p_willow, df, K=100)\t\t\r\n\tm17 <- pcount(~obs+year~p_woody+p_herb, df, K=100)\t\r\n\tm18 <- pcount(~obs+year~p_willow+p_herb, df, K=100) \r\n\tm19 <- pcount(~obs+year~p_willow+ndvi, df, K=100) \r\n\tm20 <- pcount(~obs+year~p_willow+text.cv, df, K=100) \r\n\tm21 <- pcount(~obs+year~p_herb+ndvi, df, K=100) \r\n\tm22 <- pcount(~obs+year~p_herb+text.cv, df, K=100) \r\n\tm23 <- pcount(~obs+year~ndvi+text.cv, df, K=100)\r\n\r\n# model selection. note m4 is 'null', i.e., no site covars.\t\r\nseltab <- model.sel(m4,m5,m6,m7,m8,m9,m10,m11,\r\n\tm12,m13,m14,m15,m16,m17,m18,m19,m20,m21,m22,m23)\r\nseltab\r\n\r\n## variable importance, across models\r\nimportance(seltab)\r\n\r\n\r\n\r\n\r\n"", '\r\n### BUTTERFLY GLM models and model selection  ###\r\n\r\nlibrary(MASS)\r\nlibrary(MuMIn)\r\n\r\n# butterfly abundances\r\nbydat <- read.csv(""Butterfly_rel_abun.csv"")\r\n\r\n# subset to indicator species\r\ni.s <- c(\'PACR\',\'LAOS\',\'SLOR\',\'FAME\',\'QUEE\',\'REBL\')\r\nis.idx <- which(names(bydat) %in% i.s)\r\nbydat <- bydat[,c(1,is.idx)]\r\n\r\n# veg data \r\nvegdat <- read.csv(\'VEGandRSdata_73 analysis sites.csv\')\r\n\r\n# merge species abundances and habitat data\r\ndat <- merge(vegdat,bydat, by=\'site\')\r\n\r\n### negative binomial GLMs\r\n\r\n# choose species from i.s (e.g., PACR)\r\nsp <- i.s[6]\r\ndat$spec <- dat[,which(names(dat)==sp)]\r\n\r\n## model set -- up to two covariates, no interactions\r\n\t# one var\r\n\tm1 <- glm.nb(spec~p_arundo, dat, control=glm.control(maxit=50))\t\r\n\tm2 <- glm.nb(spec~p_woody, dat, control=glm.control(maxit=50))\t\t\r\n\tm3 <- glm.nb(spec~p_willow, dat, control=glm.control(maxit=50))\t\r\n\tm4 <- glm.nb(spec~herb.t, dat, control=glm.control(maxit=50)) \r\n\tm5 <- glm.nb(spec~grass.t, dat, control=glm.control(maxit=50))\r\n\tm6 <- glm.nb(spec~ndvi, dat, control=glm.control(maxit=50))\r\n\tm7 <- glm.nb(spec~text.cv, dat, control=glm.control(maxit=50))\r\n\t# two vars\r\n\tm8 <- glm.nb(spec~p_arundo+p_woody, dat, control=glm.control(maxit=50))\t\r\n\tm9 <- glm.nb(spec~p_arundo+p_willow, dat, control=glm.control(maxit=25))\t\r\n\tm10 <- glm.nb(spec~p_arundo+herb.t, dat, control=glm.control(maxit=50))\r\n\tm11 <- glm.nb(spec~p_arundo+grass.t, dat, control=glm.control(maxit=50))\r\n\tm12 <- glm.nb(spec~p_arundo+ndvi, dat, control=glm.control(maxit=50))\r\n\tm13 <- glm.nb(spec~p_arundo+text.cv, dat, control=glm.control(maxit=25))\r\n\r\n\tm14 <- glm.nb(spec~p_woody+p_willow, dat, control=glm.control(maxit=25))\t\t\r\n\tm15 <- glm.nb(spec~p_woody+herb.t, dat, control=glm.control(maxit=50))\t\r\n\tm16 <- glm.nb(spec~p_woody+grass.t, dat, control=glm.control(maxit=50))\t\r\n\r\n\tm17 <- glm.nb(spec~p_willow+herb.t, dat, control=glm.control(maxit=50)) \r\n\tm18 <- glm.nb(spec~p_willow+grass.t, dat, control=glm.control(maxit=50)) \r\n\tm19 <- glm.nb(spec~p_willow+ndvi, dat, control=glm.control(maxit=25)) \r\n\tm20 <- glm.nb(spec~p_willow+text.cv, dat, control=glm.control(maxit=75)) \r\n\t\r\n\tm21 <- glm.nb(spec~herb.t+grass.t, dat, control=glm.control(maxit=50)) \r\n\tm22 <- glm.nb(spec~herb.t+ndvi, dat, control=glm.control(maxit=50)) \r\n\tm23 <- glm.nb(spec~herb.t+text.cv, dat, control=glm.control(maxit=50)) \r\n\r\n\tm24 <- glm.nb(spec~grass.t+ndvi, dat, control=glm.control(maxit=50)) \r\n\tm25 <- glm.nb(spec~grass.t+text.cv, dat, control=glm.control(maxit=50)) \r\n\tm26 <- glm.nb(spec~ndvi+text.cv, dat, control=glm.control(maxit=25))\r\n\r\n\t# \'null\' model (no habitat covariates)\r\n\tm.nul <- glm.nb(spec~1, dat, control=glm.control(maxit=50))\r\n\r\n# model selection\r\nseltab <- model.sel(m.nul,m1,m2,m3,m4,m5,m6,m7,m8,m9,m10,m11,\r\n\tm12,m13,m14,m15,m16,m17,m18,m19,m20,m21,m22,m23,\r\n\tm24,m25,m26)\r\nseltab\r\n\r\n## variable importance, across models\r\nimportance(seltab)\r\n\r\n\r\n\r\n', '########## look at bird/butterfly rio grande data files\r\n\r\nlibrary(vegan)\r\nlibrary(pairwiseAdonis)\r\nlibrary(indicspecies)\r\n\r\n\r\n#### BIRDS ####\r\n\r\n# bird data\r\ndat2 <- read.csv(\'Bird_abun_Nmixture.csv\')\r\n\r\n## bray-curtis dissimilarity\r\nbcdist <- vegdist(dat2[,4:26])\t\t\r\nhist(bcdist, breaks=40, col=\'gray\')\r\n### There is a good dstn of dissimilarity values.\r\n\r\n# NMDS\r\nbirdord <- metaMDS(dat2[,4:26], k=3, trymax=1000)\t\r\nstressplot(birdord)\r\nbirdord\r\n\r\n# indicator species analysis (ISA)\r\nsp <- dat2[,4:26]\t\t#species \r\nind <- multipatt(sp, dat2$grp, func=\'r\', \t#\'r\' for abun data\r\n\tcontrol = how(nperm=9999)) \t\t#perumations\r\nsummary(ind, alpha=0.10)\r\nind$sign\r\n\r\n\r\n# test for group differences (permanova)\r\ngrp.sim <- adonis2(dat2[,4:26] ~ as.factor(dat2$grp), permutations = 9999, distance = \'bray\')\r\ngrp.sim\r\n# evidence for some differences\r\n\r\n# post-hoc pairwise tests\r\ngrp.pw <- pairwise.adonis(dat2[,4:26], dat2$treat, sim.method = \'bray\', perm=9999)\r\ngrp.pw\r\n# there are multiple group differences\r\n\r\n# betadisper: test for differences in dispersion among groups, birds\r\nmod <- betadisper(bcdist, dat2$grp, type=\'centroid\',add=TRUE)\r\nmod\r\nplot(mod)\r\n# permutation test for F\r\nper <- permutest(mod, pairwise = TRUE, permutations = 9999)\r\nper\r\n# Tukey\'s Honest Significant Differences\r\nmod.HSD <- TukeyHSD(mod)\r\nplot(mod.HSD)\r\n# no difference among groups in dispersion.\r\n\r\n\r\n# plot NMDS results, birds\r\n# colors with transparency\r\n## RGB values for named color\r\nrgb.1 <- col2rgb(\'black\')\r\nrgb.2 <- col2rgb(\'orange\')\r\nrgb.3 <- col2rgb(\'blue\')\r\nrgb.4 <- col2rgb(\'red\')\r\n\r\n# with transparency\r\ncol.1 <- rgb(rgb.1[1], rgb.1[2], rgb.1[3], max = 255, alpha = 150)\r\ncol.2 <- rgb(rgb.2[1], rgb.2[2], rgb.2[3], max = 255, alpha = 150)\r\ncol.3 <- rgb(rgb.3[1], rgb.3[2], rgb.3[3], max = 255, alpha = 150)\r\ncol.4 <- rgb(rgb.4[1], rgb.4[2], rgb.4[3], max = 255, alpha = 150)\r\n\r\ngrps.tran <- c(col.1,col.2,col.3,col.4)\r\n\r\n# without transparency\r\ncolgrps <- c(""black"", ""orange"", ""blue"", ""red"")\r\n\r\n## export plot\r\nfilename <- ""bird abun NMDS_polys.tiff""\r\ntiff(filename, width = 4, height = 4.5, units=\'in\', res=1200, compression=""lzw"")\t#open tiff render\r\n\t\r\n\tplot(birdord$points, xlim=c(-0.26,0.32), ylim=c(-0.23,0.32), type=\'n\',\r\n\t\taxes=F, frame=F)\r\n\taxis(1, tck= -0.02, padj= -2.2, hadj=0.65, las=1, cex.axis=0.65)\t# x-axis\r\n\taxis(2, tck= -0.02, mgp=c(1,0.5,0), las=1, cex.axis=0.65)\t\t# y-axis\r\n\tpoints(birdord, display=\'sites\', pch=16, cex=0.7, col=grps.tran[dat2$grp])\r\n\tordiellipse(birdord, dat2$grp, col=colgrps, kind=\'sd\', draw=\'polygon\', alpha=100, border=\'transparent\')\r\n\tordiellipse(birdord, dat2$grp, col=colgrps, kind=\'se\', draw=\'polygon\', alpha=100, border=\'transparent\')\r\n\r\ndev.off()\t\t\t# close tiff render\r\n\r\n\r\n\r\n\r\n#### BUTTERFLIES  ####\r\n\r\n# data file\r\ndat2 <- read.csv(""Butterfly_rel_abun.csv"")\r\n\r\n## look at bray-curtis dissimilarity\r\nbcdist <- vegdist(dat2[,4:24])\t\t\r\nhist(bcdist, breaks=40, col=\'gray\')\r\n## There is a good dstn of dissimilarity values.\r\n\r\n# NMDS\r\nbyord <- metaMDS(dat2[,4:24], k=3, trymax=1000)\t\r\nstressplot(byord)\r\nbyord\r\n\r\n# indicator species analysis (ISA)\r\nsp <- dat2[,4:24]\r\nind <- multipatt(sp, dat2$grp, func=\'r\', control = how(nperm=9999)) \r\nsummary(ind, alpha=0.1)\r\nind$sign\r\n\r\n\r\n## test for group differences, butterflies\r\ngrp.sim <- adonis2(dat2[,4:24] ~ as.factor(dat2$grp), permutations = 9999, distance = \'bray\')\r\ngrp.sim\r\n# there is evidence for differences\r\n\r\n# post-hoc pairwise tests\r\ngrp.pw <- pairwise.adonis(dat2[,4:24], dat2$treat, sim.method = \'bray\', perm=9999)\r\ngrp.pw\r\n# there are several group differences, with weaker evidence than birds\r\n\r\n# differences in dispersion among groups (betadisper), butterflies\r\nmod <- betadisper(bcdist, dat2$grp, type=\'centroid\', add=TRUE)\r\nmod\r\nplot(mod)\r\n# permutation test for F\r\npermutest(mod, pairwise = TRUE, permutations = 9999)\r\n# Tukey\'s Honest Significant Differences\r\nmod.HSD <- TukeyHSD(mod)\r\nplot(mod.HSD)\r\n# possibly weak evidence for some differences among\r\n# groups in dispersion, but no single group-group difference\r\n# with strong evidence.\r\n\r\n\r\n# plot NMDS results, butterflies\r\n## export plot\r\nfilename <- ""butterfly abun NMDS_polys.tiff""\r\ntiff(filename, width = 4, height = 4.5, units=\'in\', res=1200, compression=""lzw"")\t#open tiff render\r\n\t\r\n\tplot(byord$points, xlim=c(-1.1,1.1), ylim=c(-1,1), type=\'n\', axes=F, frame=F)\r\n\taxis(1, tck= -0.02, padj= -2.2, hadj=0.65, las=1, cex.axis=0.65)\t\t# x-axis\r\n\taxis(2, tck= -0.02, mgp=c(1,0.5,0), las=1, cex.axis=0.65)\t\t# y-axis\r\n\tpoints(byord, display=\'sites\', pch=16, cex=0.7, col=grps.tran[dat2$grp])\r\n\tordiellipse(byord, dat2$grp, col=colgrps, kind=\'sd\', draw=\'polygon\', alpha=70, border=\'transparent\')\r\n\tordiellipse(byord, dat2$grp, col=colgrps, kind=\'se\', draw=\'polygon\', alpha=100, border=\'transparent\')\r\n\r\ndev.off()\t\t\t# close tiff render\r\n\r\n\r\n\r\n########## vegetation/habitat variable group differences\r\n\r\n# data file\r\ndata <- read.csv(\'VEGandRSdata_73 analysis sites.csv\')\r\n\r\n## look at bray-curtis dissimilarity\r\nbcdist <- vegdist(data[,4:11], meth']",3,"giant reed, invasive plant, desert riparian ecosystem, wildlife habitat, prescribed burns, herbicide applications, United States-Mexico partnership, native revegetation, Rio Grande/Ro Bravo floodplain, west Texas, butterfly communities, bird"
Data from: Does pollen limitation limit plant ranges? Evidence and implications,"Sexual reproduction often declines towards range edges, reducing fitness, dispersal, and adaptive potential at range edges. For plants, sexual reproduction is frequently limited by inadequate pollination. While case studies show that pollen limitation can limit plant distributions, the extent to which pollination commonly declines toward plant range edges is unknown. Here, we leverage global databases of pollen-supplementation experiments and plant occurrence data to test whether pollen limitation increases toward plant range edges, using a phylogenetically controlled meta-analysis. While there was significant pollen limitation across studies, we found little evidence that pollen limitation increases toward plant range edges. Pollen limitation was not stronger toward the tropics, nor at species' equatorward vs poleward range limits. Meta-analysis results are consistent with results from targeted experiments, in which pollen limitation increased significantly toward only 14% of 14 plant range edges, suggesting that pollination contributes to range limits less often than do other interactions. Together, these results suggest pollination is one of the rich variety of potential ecological factors that can contribute to range limits, rather than a generally important constraint on plant distributions.","['##Dawson-Glass: Does pollen limitation limit plant ranges? Evidence and implications 2021 manuscript submission\n##GloPL dataset cleaning and GBIF download for all species\n##1st R script of 3 on pollen limitation and range data gathering\n##\n\n# LOAD PACKAGES ###############################################################\nlibrary(rgbif)\n#library(plyr) #dont think need this anymore& gets confusing w dplyr loaded too\nlibrary(""dplyr"")\nlibrary(devtools)\nlibrary(raster)\nlibrary(rgeos)\nlibrary(""ggplot2"")\ntheme_set(theme_bw())\nlibrary(""sf"")\nlibrary(""rnaturalearth"")\nlibrary(""rnaturalearthdata"")\nlibrary(sp)\nlibrary(""rworldmap"")\nlibrary(rworldxtra)\n\nsetwd(""~/Desktop"") #for Emma\n\n\n# DATA EXPLANATION #########################################################\n#This is the 1st of 3 R scripts for the ""Does pollen limitation limit plant ranges? Evidence and implications"" 2021 manuscript\n#This study uses GloPL database from Bennett et. al., 2018, link: https://rdcu.be/bYy3D downloaded 4/24/20\n#Because this data is available in its original form online, we will not be providing the GloPL dataset in our supplemental material\n#GBIF data downloaded and used to generate range maps for each species in the GloPL database, which in a later script will be used to measure distance\n\n#Relevant variables: note first data cleaning makes use of full GloPL dataset, and full metadata description available in supporting docs\n#In this script, variables used:\n###Species_accepted_names: correct name of species as determined by the GloPL authors\n    #NB, Species_accepted_names must be cleaned to only species level, not including varieties or subspecies (done in this script)\n###Latitude and Longitude: self explanatory, corresponding lat long of each study point\n###occurrence_no: unique occurrence number for each entry in the GloPL dataset, that will be used in later loops to make sure all data matches up and\n  #simplifies editing. NB below we create the ""Master"" GloPL dataset with occurrence nos, which we will use for the remainder of all code (will not change or update occurrence no again) \n\n# Load master GloPL dataset (RUN THIS CODE ONCE - then skip to L47)\nGloPL.orig <-read.csv(""GloPL_04_20/GloPL_Download_04_24_20/GloPL.csv"")\n # add a unique occurrence number for each entry to ease data cleaning and editing\nGloPL.orig$occurrence_no <- 1:nrow(GloPL.orig)\n# #Save as the master dataset you will be referencing from now on\nwrite.csv(GloPL.orig, ""GloPL_04_20/GloPL_Download_04_24_20/GloPL_MASTER_occ_no.csv"")\n# ###########\n\n# PREP GLOPL DATA #########################################################\n#Starting with GloPL dataset with occurrence numbers\nGloPL <- read.csv(""GloPL_04_20/GloPL_Download_04_24_20/GloPL_MASTER_occ_no.csv"", stringsAsFactors =T)\nGloPL$X <- NULL\nsummary(GloPL)\n\n#How many species do we start with?\nog_no_sps <- as.data.frame(unique(GloPL$Species_accepted_names))\ndim(og_no_sps)\n#starting with 1265 species--including varieties and subspecies (consistent with GloPL publication: ""1265 wild plant species across the globe"")\n\n#we don\'t want to model variety/subsp range- only the species generally, so we create a new column with only species name (not including\n#var or subsp)\n\n#Save original species name in a new column-- will need it later\nGloPL$SpeciesGloPLNames <- GloPL$Species_accepted_names\n\n#modify species name to be the full species name without variety or subspecies, as there is not enough data in GBIF to create range for these\n#this old code was deleting anything with the phrase var in it, replacing with more inclusive code\n#GloPL$Species_accepted_names<-sub(""_var._*"", """", GloPL$Species_accepted_names)\n\n#delete subspecies  (ok, doesn\'t delete anything incorrectly)\nsubsp <- GloPL[grepl(""subsp."", GloPL[[""Species_accepted_names""]]), ]\nunique(subsp$Species_accepted_names)\n#you should have 22 subsp\n#Remove the subspecies naming info from column\nGloPL$Species_accepted_names <- sub(""_subsp.*"", """", GloPL$Species_accepted_names)\n\n#delete variety from species names\nvarieties <- GloPL[grepl(""var._"", GloPL[[""Species_accepted_names""]]), ]\nunique(varieties$Species_accepted_names)\n#should have 17 species with a  variety\n#delete var.... from species name\nvarieties$Species_accepted_names<-sub(""_var.*"", """", varieties$Species_accepted_names)\n#subset out only the name and occurrence no\nvarieties<-varieties %>%\n  select(Species_accepted_names, occurrence_no)\n#using loop (there is probably a better way to do this but this is the way I know how to do and it works)\n#match the edited names with the master occurrence no in the GloPL dataset to have corrected name in species accepted name column\nfor (j in 1:nrow(varieties))\n{\tcat(j, ""out of"", nrow(varieties), ""at"", date(), varieties$occurrence_no[j], ""\\n"")\n  {\n    GloPL$Species_accepted_names[GloPL$occurrence_no==varieties$occurrence_no[j]] = varieties$Species_accepted_names[j]\n}}\n\n#How many species do we have now?\nnum_sps_no_v.sbs <- as.data.frame(unique(GloPL$Species_accepted_names))\ndim(num_sps_no_v.sbs) #Sho', '##Dawson-Glass: Does pollen limitation limit plant ranges? Evidence and implications 2021 \n##Map generation and measurements for target species in GloPL\n##2nd R script of 3 on pollen limitation and range data gathering\n\n#This is the 2nd of 3 R scripts for the ""Where and how often does pollen limitation limit species range distributions"" project\n#This script generates a range map using species occurrences downloaded in the RGBIF script and measures range distances, among other variables needed for analysis\n#NB, this is a long code that will require certain components to be done manually, where noted\n#Specifically, the individual will have to visually evaluate the ""validity"" of the species range maps to ensure they are accurate in conveying the species range\n# LOAD PACKAGES ##############################################################\nlibrary(""ggplot2"")\ntheme_set(theme_bw())\nlibrary(""sf"")\nlibrary(""rnaturalearth"")\nlibrary(""rnaturalearthdata"")\nlibrary(""geosphere"")\nlibrary(rgeos)\nlibrary(sp)\nlibrary(dplyr)\nlibrary(rgdal)\nlibrary(tidyverse)\nlibrary(fields)\nlibrary(""concaveman"")\nlibrary(""janitor"")\nlibrary(""countrycode"")\nlibrary(""lwgeom"")\nlibrary(""CoordinateCleaner"")\nlibrary(rworldmap)\nlibrary(rworldxtra)\nlibrary(""prevR"")\n\nsetwd(""~/Desktop/R_2_GloPL_data"") \n\n# PREP DATA #########################################################\n\n#Data: load cleaned GloPL dataset from R_EDG_GloPL_GBIF code\n#reading in our species lists from the first R script (R_EDG_GloPL_1)\nGloPL <- read.csv(""GloPL_1_datasets/GloPL_cleaned_21_04_18.csv"")\nGloPL$X <- NULL\ndim(GloPL) #should have 2788 datapoints\n#Important variables: \n#Species_accepted_names: correct name of species as determined by the GloPL authors, excluding subspecies and variety names\n#Latitude and longitude of studies - these will be used to plot where in the species\' range the study was done\n#Continent- the continent where the study was done, this will help us with thinning out species occurrence data and only generating maps over 1 continuous land mass\n\n#Species list\nSortedSpeciesMaster <- read.csv(""GloPL_1_datasets/Species_list_R_1_21_04_18.csv"")\nSortedSpeciesMaster$X <- NULL\ndim(SortedSpeciesMaster) #1175 species (could get the same list using unique())\n#Important variables: \n#Species_accepted_names: same as above, and should match the GloPL species names\n\n\n# GENERATE RANGE MAP FOR EACH SPECIES ##############################\n#In this script, make a polygon for each target species and calculate distance from GloPL point to nearest edge\n\n#_World polygons---------------\n#Call in world polygon\nworld <- ne_countries(scale = ""medium"", returnclass = ""sf"")\n\n#Call in ocean polygon\n\n#NB update:\n#changing ocean polygon used, the 50m resolution one has too fine detail for st lawrence river and other esturine channels, so at times map would code that to be the closest range edge when in fact is not really a range edge, but in middle of range, doesn\'t seem like it would be a biologically meaningful result, so using lower resolution ocean map to cut out ocean part of polygon and fixes that issue\n#polygon downloaded 4/29 from https://www.naturalearthdata.com/downloads/110m-physical-vectors/110m-ocean/\n\n#only need the below once\nuzp <- ""ne_polygons/ne_110m_ocean.zip""\nfils<-unzip(uzp, exdir = ""ne_polygons/"")\n\n#Call in ocean polygon shapefile\nocean <- readOGR(grep(""shp$"", fils, value=TRUE), ""ne_110m_ocean"",\n                 stringsAsFactors=FALSE, verbose=FALSE)\n#convert to sf object\nocean<-st_as_sf(ocean)\n#Combine geometries for polygon (was having an issue where it had 2 rows of entries, this combines those rows for geometry and produces accurate results re https://github.com/r-spatial/sf/issues/459)\nocean <- sf::st_as_sf(ocean) %>% st_combine() %>% st_sf()\n\n#NB, as of 4/27 r downloads of natural earth are not working and anything that requires a ne polygon needs to be done manually, can do this by downloading from ne website\n#the land polygon is needed for the CoordinateCleaner protocol to delete ocean occurrences, using this below downloaded 5/1-- using manual polygon produces the same results, just annoying to have to do\n#also produces a warning, but based on https://github.com/earthlab/cft/issues/127 seems like it can be ignored and is just a minor annoyance\nland <- st_read(""ne_polygons/ne_50m_land/ne_50m_land.shp"")#same polygon used by default system\nland<-as_Spatial(land)\n\n\n#old:\n#Call in ocean polygon (downloaded from https://www.naturalearthdata.com/downloads/50m-physical-vectors/50m-ocean/ May 2020)\n#ocean <- st_read(""ne_50m_ocean/ne_50m_ocean.shp"")\n\n\n\n#Step 1) generate 1st (rough) map and polygon ---------------------------\n#the below loop will clean the GBIF occurrence data, create a concave polygon, and generate range data for each species with available data\n#note this is just the first pass of several that will be used in cleaning and generating the final maps that will be used for data analysis\n#note you will need to create folders for the generated maps somewhere in your work', '##Dawson-Glass: Does pollen limitation limit plant ranges? Evidence and implications 2021\n## Statistical analyses and figure making\n##3rd R script of 3 \n\n#This is the statistical analysis script for  ""Does pollen limitation limit plant ranges? Evidence and implications"" \n\n#This script analyses the data collected from the polygon script and the GloPL database, using the rma.mv function from metafor, combined with a information theoretic approach for model selection and visualization\n\n# LOAD PACKAGES ##############################################################\nlibrary(""ggplot2"")\nlibrary(""metafor"")\nlibrary(""fitdistrplus"")\nlibrary(""logspline"")\nlibrary(""lme4"")\nlibrary(""taxize"")\nlibrary(""rnaturalearth"")\nlibrary(""rnaturalearthdata"")\nlibrary(ape)\nlibrary(stats)\nlibrary(plyr)\nlibrary(minqa)\nlibrary(grDevices)\nlibrary(varhandle)\nlibrary(""pbkrtest"")\nlibrary(""geiger"")\nlibrary(""arm"") #NB load arm after geiger-- need to use rescale from arm, geiger has same command and will mess up results if it\'s run after\nlibrary(MuMIn)\nlibrary(ggpubr)\n\n#set working directory\nsetwd(""~/Desktop"") \n\n\n#LOAD DATA #########################################################\n#Load datasets generated and cleaned in the previous polygon script\n\n#_MULTI OCC (""MULTI-EXPERIMENT"") DATASET########################\n#Only includes species with multiple occurrences from the GloPL dataset with adequate GBIF data\nGloPL_multi<-read.csv(""R_2_GloPL_data_inputs/GloPL_2_master_datasets/GloPL_multi_expt_DS_21_05_24.csv"")\nGloPL_multi$X <- NULL\nsummary(GloPL_multi)\ndim(GloPL_multi)#878 datapoints\n\n#_BIG (""FULL"") DATASET########################\n#Big dataset containing data from all species with adequate range data from GBIF (not restricted to multi occurrence)\nGloPL_big<-read.csv(""R_2_GloPL_data_inputs/GloPL_2_master_datasets/GloPL_full_DS_21_05_24.csv"")\nGloPL_big$X <- NULL\nsummary(GloPL_big)\ndim(GloPL_big) #1609 datapoints\n\n#Explanation of relevant variables (both datasets have same variables):\nnames(GloPL_big)\n\n#Study: concatenation from GloPL database of author name and year PL effect size extracted from\n#occurrence_no: unique number for each row of data\n#Species_accepted_names: name of study species, as correctly determined by original GloPL authors, EXCLUDING subspecies and varieties \n#SpeciesGloPLNames: Full species name listed in original GloPL database, including subspecies and varieties, (necessary to match with phylogenetic matrix)\n#PL_Effect_Size: log response ratio pollen limitation effect size, calculated in the original GLoPL database\n#Sampling_variance: sampling variance of the pollen limitation effect size, as calculated in script R1, based on equations from Gurevitch et al., 2001 \n#Prop_dist: a proportional distance measurement of distance, calculated dist to edge/dist to edge+ dist to centroid\n#Polarity: the angle relative to the poles of the nearest range edge from the GloPL study point, denoting if the nearest range edge is closer to the poles or the equator\n#Edge_type: ""land"" or ""ocean"" corresponding to if the nearest edge occured on land or ocean\n#Lat.abs.val: Absolute latitude \n#Field_method: The method of supplementation used in the original pollen supplementation experiment\n\n#_PHYLOGENETIC TREE#############\n#load the phylogenetic tree generated by Bennett et al. for the original GloPL database, which will be used to construct a variance-covariance matrix to conduct a phylogenetic meta-analysis (downloaded 4/24/20, https://doi.org/10.5061/dryad.dt437)\nGloPLphyltree<-read.tree(file= ""GloPL_04_20/GloPL_Download_04_24_20/SiteTree_VS.tree"")\n\n#_VECTOR DEPENDENCE#############\n#At the request of reviewers, we updated our analyses to include ""vector dependence"" (ie if a plant is dependent on a vector to transport pollen for reproduction, or is auto-fertile). Vector dependence for species in the GloPL dataset was downloaded from https://doi.org/10.5061/dryad.dt437 (Bennett et al 2020, downloaded August 2021). Some vector dependencies are missing in this dataset (not enough information available) or were excluded because of wind dependency. For missing species, we attempt to find their depednence in the literature. For species dependent on wind pollination, we deem them ""vector dependent."" More information on this in Methods section of publication.\n\nload(""glopl_data.rda"") #will automatically load as ""dt""\n\n\n#PREP DATA#####################\n\n#_Prep: Multi occurrence dataset######\n\n#Change names that were edited to create multiple polygons for a species on different continents back to original naming\nGloPL_multi$Species_accepted_names[GloPL_multi$Species_accepted_names==""Rubus_chamaemorus_Eu""] = ""Rubus_chamaemorus""\n\n#Add small constant to sampling variance in each dataset. Because some sampling variance is coded to 0, get a convergence issue in later rma.mv model. Adding constant prevents this issue. Consistent with Burns et. al. (2019)  (they used 0.0001, our datset smaller so can use even smaller constant)\nGloPL_multi$SV0.00001<-GloPL_multi$Sampling_var', '##Dawson-Glass: Does pollen limitation limit plant ranges? Evidence and implications 2021\n## Conceptual figure (Figure 1 in manuscript) code\n#Accompanies script 3 in main series for generating figrues\n#This is a supplement to the Stats and figures code, and creates a map for a target species used in conceptualizig the methods of the project\n#This entire code essentially reruns what was done in R_DG_polygon_code but for a single species. Please refer to that code\n#for more details on the methods/ meaning of the code here\n\n# LOAD PACKAGES ##############################################################\nlibrary(""ggplot2"")\ntheme_set(theme_bw())\nlibrary(""sf"")\nlibrary(""rnaturalearth"")\nlibrary(""rnaturalearthdata"")\nlibrary(""geosphere"")\nlibrary(rgeos)\nlibrary(sp)\nlibrary(dplyr)\nlibrary(plyr)\nlibrary(rgdal)\nlibrary(tidyverse)\nlibrary(fields)\nlibrary(""concaveman"")\nlibrary(""janitor"")\nlibrary(""countrycode"")\nlibrary(""lwgeom"")\nlibrary(""CoordinateCleaner"")\nlibrary(rworldmap)\nlibrary(rworldxtra)\nlibrary(""prevR"")\n\nsetwd(""~/Desktop/R_2_GloPL_data_inputs"") \n\n#read in full dataset\nGloPL<-read.csv(""R_2_GloPL_data_inputs/GloPL_2_master_datasets/GloPL_full_DS_21_05_24.csv"")\nGloPL<-GloPL_big\n\ngbifocc <- read.csv(file = paste(""/Volumes/2020HAR/Species_occurrence_Gbif.nosync/"", ""Arisaema_triphyllum.csv"", sep = """")) #Upload csv of target species, downloaded from GBIF August 2020\n\nglopp <- subset(GloPL, Species_accepted_names %in% ""Arisaema_triphyllum"") #subset targeted species from larger glopl subset\n#Give unique occurrence number to each row\nglopp$occurrence<-1:nrow(glopp)\nGloPLMASTER<-glopp\n\n#call in world polygon\nworld <- ne_countries(scale = ""medium"", returnclass = ""sf"")\n\n#Call in ocean polygon shapefile\nuzp <- ""ne_polygons/ne_110m_ocean.zip""\nfils<-unzip(uzp, exdir = ""ne_polygons/"")\n\nocean <- readOGR(grep(""shp$"", fils, value=TRUE), ""ne_110m_ocean"",\n                 stringsAsFactors=FALSE, verbose=FALSE)\n#convert to sf object\nocean<-st_as_sf(ocean)\n#Combine geometries for polygon (was having an issue where it had 2 rows of entries, this combines those rows for geometry and produces accurate results re https://github.com/r-spatial/sf/issues/459)\nocean <- sf::st_as_sf(ocean) %>% st_combine() %>% st_sf()\n\n\n#Cleaning GBIF occ\n#Clean GBIF data to make more streamlined\ngbifocc <- gbifocc %>%\n  dplyr::select(species, decimalLongitude, decimalLatitude, countryCode,\n                gbifID, family, taxonRank, year,\n                basisOfRecord, institutionCode, datasetName)#reduce many columns in GBIF occurance file to a few relevant ones\ngbifocc <- gbifocc%>%\n  filter(!is.na(decimalLongitude))%>%\n  filter(!is.na(decimalLatitude))#get rid of NA values\n#change country code system to iso3c\ngbifocc$countryCode <-  countrycode(gbifocc$countryCode, origin =  \'iso2c\', destination = \'iso3c\')\n# remove zeroes\nif (any(gbifocc$decimalLongitude == 0 | gbifocc$decimalLatitude == 0, na.rm = TRUE))\n{gbifocc <- gbifocc[-which(gbifocc$decimalLongitude == 0 | gbifocc$decimalLatitude == 0), ]\n}\ngbifocc <- data.frame(gbifocc)\n# Remove duplicate records (remove if not desired, can greatly reduce number of observations)\nif (nrow(gbifocc) >= 1)  #ie ignore header row \n  #SpatialPoints creates object of class SpatialPoints (from sp package)\n{spgeo <- SpatialPoints(data.frame(lon = gbifocc$decimalLongitude, lat = gbifocc$decimalLatitude)) \nzd <- zerodist(spgeo) #zerodist finds point pairs with equal spatial coordinates\nif (length(zd) > 0) #ie if found any duplicates\n{spgeo <- spgeo[-zd[, 2], ] #spgeo df without anything in the second column of zd (1st column of z gives 1st occurrence, 2nd column of z gives the duplicate row)\ngbifocc <- gbifocc[-zd[, 2], ] #trim spgbif in same way\n}}\n#\n#use clean coordinate tests to get rid of occurance points inside big cities (probably inaccurate/mislabeled), occuring in the ocean\n#occuring at the GBIF headquarters, etc (see) ?clean_coordinates() for full details\nflags <- clean_coordinates(x = gbifocc, lon = ""decimalLongitude"", lat = ""decimalLatitude"",\n                           countries = ""countryCode"", \n                           species = ""species"",\n                           tests = c(""capitals"", ""centroids"", ""equal"",""gbif"", ""institutions"",\n                                     ""seas"")) # most test are on by default\n\ngbifocc <- gbifocc[flags$.summary,]#sub out flagged coordinates\n#sub out uncertainty of greater than 100 m\n#GBIFocc <- GBIFocc %>%\n# filter(coordinateUncertaintyInMeters / 1000 <= 100 | is.na(coordinateUncertaintyInMeters)) \n#only include observation methods below (excludes fossils, machine observation, living specimin (like in a botanical garden))\ngbifocc <- filter(gbifocc, basisOfRecord == ""HUMAN_OBSERVATION"" | \n                    basisOfRecord == ""OBSERVATION"" |\n                    basisOfRecord == ""PRESERVED_SPECIMEN"")\n#gave NA values an arbitrary number because were being deleted erroneously \ngbifocc$year[is.na(gbifocc$year)] <- 3000\n\ngbifocc<-gbifocc[ which(gbifocc$year >1945), ] # remove records from before', '##Dawson-Glass: Does pollen limitation limit plant ranges? Evidence and implications 2021\n#Code for GBIF data citation\n#Accompanies script 1 in main series for citing GBIF datasets\n#This code pulls out the dataset key info to be used in Derived Dataset tool for accurate GBIF citation\n\n#Load packages\nlibrary(plyr)\n\n#Set working directory\nsetwd(""~/Desktop"")\n\n#Load full dataset\nGloPL_big_gbif<-read.csv(""R_2_GloPL_data_inputs/GloPL_2_master_datasets/GloPL_full_DS_21_05_24.csv"")\nGloPL_big_gbif$X <- NULL\nsummary(GloPL_big_gbif)\ndim(GloPL_big_gbif) #1609 datapoints\n\n#Full dataset prep##\n#Change names back\n#Canna_indica \nGloPL_big_gbif$Species_accepted_names[GloPL_big_gbif$Species_accepted_names==""Canna_indica_Af""] = ""Canna_indica""\nGloPL_big_gbif$Species_accepted_names[GloPL_big_gbif$Species_accepted_names==""Canna_indica_SA""] = ""Canna_indica""\n\n#Eulophia_alta \nGloPL_big_gbif$Species_accepted_names[GloPL_big_gbif$Species_accepted_names==""Eulophia_alta_SA""] = ""Eulophia_alta""\nGloPL_big_gbif$Species_accepted_names[GloPL_big_gbif$Species_accepted_names==""Eulophia_alta_NA""] = ""Eulophia_alta""\n\n\n#Lythrum_salicaria\nGloPL_big_gbif$Species_accepted_names[GloPL_big_gbif$Species_accepted_names==""Lythrum_salicaria_NA""] = ""Lythrum_salicaria""\n\n#Rubus_chamaemorus \nGloPL_big_gbif$Species_accepted_names[GloPL_big_gbif$Species_accepted_names==""Rubus_chamaemorus_NA""] = ""Rubus_chamaemorus""\nGloPL_big_gbif$Species_accepted_names[GloPL_big_gbif$Species_accepted_names==""Rubus_chamaemorus_Eu""] = ""Rubus_chamaemorus""\n\n#Acacia_dealbata\nGloPL_big_gbif$Species_accepted_names[GloPL_big_gbif$Species_accepted_names==""Acacia_dealbata_Eu""] = ""Acacia_dealbata""\nGloPL_big_gbif$Species_accepted_names[GloPL_big_gbif$Species_accepted_names==""Acacia_dealbata_Af""] = ""Acacia_dealbata""\n\n\n#Remove data from two studies with very low (negative) effect sizes-- presumably as a result of error or poor flower response to supplementation. We do not believe this data accureately reflects pollen limitation\nsummary(GloPL_big_gbif$PL_Effect_Size)#Current min is -3.37873\nGloPL_big_gbif<-subset(GloPL_big_gbif, !(Study %in% c(""McKinney_&_Goodell_2010"", ""Matias_&_Consolaro_2014"")))\n\nsummary(GloPL_big_gbif$PL_Effect_Size) #Min now -2.31206\ndim(GloPL_big_gbif)#1605 datapoints\n#There are other study locations associated with this study species (Geranium_maculatum), but Matias_&_Consolaro_2014 was the only study on their target species, so we still have 559 unique species\nunique(GloPL_big_gbif$Species_accepted_names)\n\n#remove species with no vector dependence data\nGloPL_big_gbif<-subset(GloPL_big_gbif, !(SpeciesGloPLNames %in% c(""Caryocar_villosum"",""Celtis_iguanaea"", ""Croton_sonderianus"", ""Cypripedium_candidum"", ""Haemanthus_sanguineus"", ""Oxypetalum_appendiculatum"", ""Talbotiella_gentii"")))\n\nunique(GloPL_big_gbif$Species_accepted_names)\n#now have 552\n\n#Make dataframe of just species names\nsps_in_data<-as.data.frame(unique(GloPL_big_gbif$Species_accepted_names))\nnames(sps_in_data)[1]<-""Species_accepted_names""\n\n#Create dataframe for extracted info to be entered into\ndatasetKey<-""test""\nfreq<-""0""\nSpecies<-""test""\nData_key_GloPL<-data.frame(datasetKey, freq, Species)\n\n#Create a loop to call in all GBIF data we used and extract all datasetKeys in data we use\nfor (i in 1:nrow(sps_in_data))\n  \n{\tcat(i, ""out of"", nrow(sps_in_data), ""at"", date(), sps_in_data$Species_accepted_names[i], ""\\n"")\n  tryCatch(\n    {\n      #_Load data---------------\n      #Load target species GBIF files (on exteral hard drive)\n      GBIFocc <- read.csv(file = paste(""/Volumes/2020HAR/Species_occurrence_Gbif.nosync/"", sps_in_data$Species_accepted_names[i], "".csv"", sep = """"))\n      key_info<-count(GBIFocc, ""datasetKey"")#get unique keys used\n      key_info$Species<-sps_in_data$Species_accepted_names[i]\n      \n      Data_key_GloPL<-rbind(Data_key_GloPL, key_info)#just keep pasting on to the end of the created dataset\n    }, error=function(e){cat(""ERROR :"",conditionMessage(e), ""\\n"")}) #Keeps loop running even if there\'s an error\n}\n#delete test variable\nData_key_GloPL_final<-subset(Data_key_GloPL, !(Species %in% ""test""))\n#count how many species there are (552)\nunique(Data_key_GloPL_final$Species)\n#verify all species accounted for\ntest<-subset(sps_in_data, !(Species_accepted_names %in% Data_key_GloPL_final$Species))\n#reorder so species is first\nnames(Data_key_GloPL_final)\nData_key_GloPL_final<-Data_key_GloPL_final[, c(""Species"", ""datasetKey"", ""freq"")]\n#save file \nwrite.csv(Data_key_GloPL_final, ""Poll_lim_phil_trans_GBIF_dataKey_21_09_05.csv"")\n']",3,"pollen limitation, plant ranges, sexual reproduction, fitness, dispersal, adaptive potential, inadequate pollination, case studies, plant distributions, global databases, pollen-supplementation experiments, plant occurrence data, phylogenetically controlled meta-analysis, tropics,"
"Competition and co-association, but not phosphorous availability, shape the benefits of phosphate solubilising root bacteria for maize (Zea mays).","Predicting the conditions under which rhizobacteria benefit plant growth remains challenging. Here we tested the hypothesis that benefits from inoculation with phosphate-solubilising rhizobacteria will depend upon two environmental conditions: phosphate availability and competition between bacteria. Maize-associated rhizobacteria with varying phosphate solubilisation ability were used in experiments in soil, sterilised soil and gnotobiotic microcosms under conditions of varying orthophosphate availability, while we manipulated intensity of competition by varying the number of isolates in plant inocula. Growth promotion by microbes did not depend on phosphate availability but was affected by interactions between inoculants: beneficial effects of one Serratia isolate were detectable only when plants were inoculated with a single strain and beneficial effects of a competition sensitive Rhizobium was only detectable in sterilised soil or in microcosms inoculated with single strains. Moreover, microcosm experiments suggested that facilitation of a parasitic isolate, not competitive interactions between bacteria, prevented plants from gaining benefits from a potential mutualist. Competition and facilitation affected colonization of plants in microcosms but growth promotion by Serratia was more affected by inoculation treatment than culturable densities on roots. Experimental manipulation of seed inocula can reveal whether plant growth stimulation is robust with respect to competition, as well as the ecological strategies of different rhizobacteria. From an applied perspective, phosphate solubilisation may not provide the mechanism for bacterial growth promotion but may indicate mutualistic potential due to phylogenetic associations. Importantly, benefits to plants are vulnerable to interactions between rhizobacteria and may not persist in mixed inoculations.","['# R script to look at CFUs for PSB project\n# Get the path to current open R script and find main dir\n\nsetwd(""~/Dropbox/Andy M/Shared with BEN/Phosphate manuscript/PHOSPHATE DATA JO"")\nrm( list = ls( ) )\nif(!require(rstudioapi)){install.packages(\'rstudioapi\')}\nlibrary( rstudioapi ) \npath_to_file <- getActiveDocumentContext()$path\nwd <- paste( dirname( path_to_file ), ""/"", sep = """" )\nsetwd( wd )\n\n# load packages with pacman\nif(!require(pacman)){install.packages(\'pacman\')}\nlibrary(pacman)\npacman::p_load(corrplot,cowplot,forcats,ggplot2,MASS,viridisLite)\n\n#read\'em back in after painful cleaning\n\ncfu_trt<-read.table(""cfu_by_trt_no_pseudo.txt"",header = TRUE)\nhead(cfu_trt)\ncfu_no_fungi<-cfu_trt[cfu_trt$Fungal==""Not"",]\nhead(cfu_no_fungi)\ncfu_simp<-cfu_no_fungi[cfu_no_fungi$t2==""one""|cfu_no_fungi$t2==""CONTROL"",]\nhead(cfu_simp)\ncfu_no_W<-cfu_no_fungi[cfu_no_fungi$Treatment==""CONTROL""|cfu_no_fungi$Treatment==""X""|cfu_no_fungi$Treatment==""XY""|cfu_no_fungi$Treatment==""XYZ""|cfu_no_fungi$Treatment==""XZ""|cfu_no_fungi$Treatment==""YZ""|cfu_no_fungi$Treatment==""Z""|cfu_no_fungi$Treatment==""Y"",]\nhead(cfu_no_W)\ncfu_nofu_nomul<-cfu_no_fungi[cfu_no_fungi$Treatment!=""MULTI"",]\ncfu_nomulti<-cfu_trt[cfu_trt$Treatment!=""MULTI"",]\n\ncommunity <- cfu_nomulti[,1:4]\n\ncolnames(community) <- c(\'Y\',\'X\',\'W\',\'Z\') # correct names now\n\n# set up some vectors for the loops\np_vec <- vector()\nchi_vec <- vector()\nchi_matrix_list <- list()\ncombo_counter <- 0\n\n# run over the community column names to make pairs of strains (i and j)\nfor(i in 1:length(colnames(community))){\n  for(j in 1:length(colnames(community))){\n    \n    # only run if not comparing a strain with itself and avoid running a pair twice\n    if(i != j & i < j){\n      combo_counter <- combo_counter + 1 # count what number pairing this is\n      # set counts to 0\n      n_i <- 0\n      n_j <- 0\n      n_ij <- 0\n      n_none <- 0\n      \n      # count the rows with presence absence combos\n      for(row in 1:length(community[,i])){ \n        if(community[row,i]>0 & community[row,j]==0){\n          n_i <- n_i + 1\n        } else if(community[row,i]==0 & community[row,j]>0){\n          n_j <- n_j + 1\n        } else if(community[row,i]>0 & community[row,j]>0){\n          n_ij <- n_ij + 1\n        } else if(community[row,i]==0 & community[row,j]==0){\n          n_none <- n_none + 1\n        }\n      }\n      \n      # run chi square tests, pull out p value and test statistic into two vectors \'p_vec\' \'chi_vec\' respectively, and name the vector elements according to the combo of strains\n      chi_mat <- matrix(c(n_ij,n_j,n_i,n_none), nrow = 2)\n      p_vec[combo_counter] <- chisq.test(chi_mat)$p.value\n      if(length(p_vec)==1){\n        names(p_vec) <- c(names(p_vec),paste(colnames(community)[i],colnames(community)[j],sep = ""*""))\n      } else {\n        names(p_vec) <- c(names(p_vec)[1:length(p_vec)-1],paste(colnames(community)[i],colnames(community)[j],sep = ""*""))\n      }\n      chi_vec[combo_counter] <- chisq.test(chi_mat)$statistic\n      names(chi_vec) <- names(p_vec)\n      \n      # name the matrix columns and rows and save as a list so we can view them \n      colnames(chi_mat) <- c(paste(colnames(community)[j],\'present\',sep = ""_""),\n                             paste(colnames(community)[j],\'absent\',sep = ""_""))\n      rownames(chi_mat) <- c(paste(colnames(community)[i],\'present\',sep = ""_""),\n                             paste(colnames(community)[i],\'absent\',sep = ""_""))\n      chi_matrix_list[[combo_counter]] <- chi_mat\n      names(chi_matrix_list) <- names(chi_vec)\n    }\n  }\n}\n\n# use FDR correction on p values for multiple testing (NB not sure what method is used in microbiology, I use FDR in my gene expression work)\np_adjust_vec <- p.adjust(p_vec, method = \'fdr\', n = length(p_vec))\nchi_matrix_list\np_adjust_vec\nchi_vec\n\n#BR graphical exploration\n\nw1<-ggplot(data=cfu_nomulti, aes(x=Treatment, y=(mean_w+1)))+\ngeom_boxplot()+scale_y_log10(name=""Isolate W CFU / ml"")+guides(x = guide_axis(angle = -45))\nw1\ny1<-ggplot(data=cfu_nomulti, aes(x=Treatment, y=(mean_y+1)))+\n  geom_boxplot()+scale_y_log10(name=""Isolate Z CFU / ml"")+guides(x = guide_axis(angle = -45))\ny1\nx1<-ggplot(data=cfu_nomulti, aes(x=Treatment, y=(mean_x+1)))+\n  geom_boxplot()+scale_y_log10(name=""Isolate Y CFU / ml"")+guides(x = guide_axis(angle = -45))\nx1\nsw1<-ggplot(data=cfu_nomulti, aes(x=Treatment, y=(mean_swd+1)))+\n  geom_boxplot()+scale_y_log10(name=""Isolate X CFU / ml "")+guides(x = guide_axis(angle = -45))\nsw1\n\nlibrary(cowplot)\n#all the boxplots\nraw_cfu<-plot_grid(w1,y1,x1,sw1,ncol = 2)\nraw_cfu\n\n\n#simplify competition treatments to number of competitors in inoculum\ncfu_trt$t2<-as.factor(cfu_trt$Treatment)\nlevels(cfu_trt$t2)\nlevels(cfu_trt$t2)[6]<-""many""\nlevels(cfu_trt$t2)[2]<-""many""\nlevels(cfu_trt$t2)[5:6]<-""three""\nlevels(cfu_trt$t2)[7]<-""three""\nlevels(cfu_trt$t2)[10]<-""three""\nlevels(cfu_trt$t2)[3]<-""one""\n\nlevels(cfu_trt$t2)[8]<-""one""\n\nlevels(cfu_trt$t2)[4]<-""two""\nlevels(cfu_trt$t2)[10]<-""one""\nlevels(cfu_trt$t2)[11]<-""one""\nlevels(cfu_trt$t2)[6:10]<-""two""\n\n#code', 'setwd(""~/Dropbox/Andy M/Shared with BEN/Phosphate manuscript"")\nlibrary(ggplot2)\n\nmicro<-read.table(""MicrocosmJW.csv"",sep="","",header = TRUE)\n1\nmicro2<-micro[micro$Fungal==""Not"",]\nmicro3<-micro2[micro2$RS_ratio!=""na"",]\nmicro3$RS_ratio<-as.numeric(micro3$RS_ratio)\n\nmicro4<-micro[micro$Treatment!=""MULTI"",]\nmicro4.1<-micro4[micro4$Fungal==""Not"",]\n\nrequire(ggplot2)\n\nrich1<-glm(WetTotal~Phosphate*Strain_rich,data=micro4)\nsummary(rich1)\nanova(rich1,test=""F"")\n\nrich2<-glm(WetTotal~Treatment*Phosphate,data=micro4,subset=(Fungal==""Not""))\nanova(rich2,test=""F"")\nsummary(rich2)\n\nrich2.1<-glm(WetTotal~Treatment*Phosphate+Fungal,data=micro4)\nanova(rich2.1,test=""F"")\nsummary(rich2.1)\n\nrich3<-glm(WetTotal~Treatment,data=micro4,subset=(Fungal==""Not""))\nsummary(rich3)\n\n\n\n\nrich3.1<-glm(WetTotal~T3,data=micro4,subset=(Fungal==""Not""))\nsummary(rich3.1)\nanova(rich3,rich3.1,test=""F"")\nanova(rich3.1,test=""F"")\n\nrich3.2<-glm(WetTotal~T3*Phosphate,data=micro4,subset=(Fungal==""Not""))\nsummary(rich3.2)\nanova(rich3.2,test=""F"")\n\nrich3.3<-glm(WetTotal~T2,data=micro4,subset=(Fungal==""Not""))\nsummary(rich3.3)\nanova(rich3,rich3.3,test=""F"")\n#clearer impact of X on its own but trend not bad for X+ inoculation\n\nmicro2$Treatment<-as.factor(micro2$Treatment)\nlevels(micro2$Treatment)\nlevels(micro2$Treatment)[1]<-""CON""\nmicro2$T4<-as.factor(micro2$Treatment)\nlevels(micro2$T4)[2:10]<-""with W""\nlevels(micro2$T4)\nlevels(micro2$T4)[3:6]<-""with X(-W)""\nlevels(micro2$T4)\n\nrich3.4<-glm(WetTotal~T4,data=micro2)\nsummary(rich3.4)\nanova(rich3.4,test=""F"")\n\n\n# 3.4 one way anova sign but differences relative to control not evident with full pooling\nAIC(rich3.4)\nAIC(rich3.3)  #lowest 997.5  more selective pooling (W in threes, X alone)\nAIC(rich3.1)  #also low 998.8  (pools all Xs)\nAIC(rich3)\n\nplot(rich3.3)\n\n\nrs1<-glm(as.numeric(RS_ratio)~Treatment,data=micro4.1)\nsummary(rs1)\n#little variation in RS ratios at all\nrs2<-glm(as.numeric(RS_ratio)~Treatment*Phosphate,data=micro4.1)\nsummary(rs2)\nanova(rs2,test=""F"")\n\nh1<-glm(Height~Treatment,data=micro4.1,subset=(Fungal==""Not""))\nsummary(h1)\nanova(h1,test=""F"")\n#nothing interesting here really\n\nroot1<-glm(RootI~Treatment*Phosphate,data=micro,subset=(Fungal==""Not""))\nsummary(root1)\nanova(root1,test=""F"")\n#Root length boring too\n\n\n\n\nmicfig1<-ggplot(data=micro4.1,aes(y=WetTotal,x=Treatment))+\n  geom_boxplot()+scale_x_discrete(name=""Bacterial inocula"")+scale_y_continuous(name = ""Total Fresh Mass (g)"",limits =c(0.5,4.5))+\n  theme_bw()+theme(panel.grid.major = element_blank(),panel.grid.minor  = element_blank())\n  micfig1\nmicfig2<-ggplot(data=micro2,aes(y=WetTotal,x=T4))+\n  geom_boxplot()+scale_x_discrete(name=""Treatments"")+scale_y_continuous(name = ""Total Fresh Mass (g)"")+\n  theme_bw()+theme(panel.grid.major = element_blank(),panel.grid.minor  = element_blank())\nmicfig2\n\nmicfig3<-ggplot(data=micro3,aes(y=RS_ratio,x=Treatment))+\n  geom_boxplot()+scale_x_discrete(name=""Treatments"")+scale_y_continuous(name = ""RS ratio"")+\n  theme_bw()+theme(panel.grid.major = element_blank(),panel.grid.minor  = element_blank())\nmicfig3\n\nmicfig4<-ggplot(data=micro2,aes(y=Height,x=Treatment))+\n  geom_boxplot()+scale_x_discrete(name=""Treatments"")+scale_y_continuous(name = ""Height"")+\n  theme_bw()+theme(panel.grid.major = element_blank(),panel.grid.minor  = element_blank())\nmicfig4\n', 'setwd(""/Users//Joe//Documents//Silwood//Phosphate_Project//Stats//Code"")\nsetwd(""~/Dropbox/Andy M/Shared with BEN/Phosphate manuscript/GhouseData/JoeCode"")\n#nb big data entry errors in root mass corrected in this version (decimal point shifts)\n\n\n# The palette with black:\ncbbPalette <- c(""#000000"", ""#E69F00"", ""#56B4E9"", ""#009E73"", ""#F0E442"", ""#0072B2"", ""#D55E00"", ""#CC79A7"")\n\n#miss first three colours\ncbbPalette2 <- c(""#009E73"", ""#0072B2"", ""#D55E00"", ""#CC79A7"")\n#another version\n\ncbbPalette3 <- c( ""#000000"",""#D55E00"", ""#CC79A7"")\n\n# To use for fills, add\nscale_fill_manual(values=cbbPalette)\n\n# To use for line and point colors, add\nscale_colour_manual(values=cbbPalette)\n\ngh1<-read.csv(""GlasshouseData.csv"")\nstr(gh1)\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(AER)\nlibrary(sciplot)\nlibrary(qcc)\nlibrary(agricolae)\n\n#treat dates as dates\ngh1$Emerg <-strptime(as.character(gh1$Emerg), ""%d/%m/%Y"")\ngh1$Sow<-strptime(as.character(gh1$Sow), ""%d/%m/%Y"")\ngh1$Innoc <-strptime(as.character(gh1$Innoc), ""%d/%m/%Y"")\ngh1$DateH <-strptime(as.character(gh1$DateH), ""%d/%m/%Y"")\nstr(gh1)\n\n\n#get rid of new stuff and extra column of notes\ngh1<-subset(gh1, subset=ID<=271)\ngh1$X<-NULL\n\n#add ages for each height column\n#gh1$Age2<-gh1$AgeI+2\n#gh1$Age4<-gh1$AgeI+4\n#gh1$Age6<-gh1$AgeI+6\n#gh1$Age8<-gh1$AgeI+8\n#gh1$Age10<-gh1$AgeI+10\n#gh1$Age12<-gh1$AgeI+12\n#gh1$Age14<-gh1$AgeI+14\n#gh1$Age18<-gh1$AgeI+18\n#gh1$Age16<-gh1$AgeI+16\n#gh1$Age20<-gh1$AgeI+20\n#gh1$Age22<-gh1$AgeI+22\n#gh1$Age24<-gh1$AgeI+24\n#gh1$Age26<-gh1$AgeI+26\n#gh1$Age28<-gh1$AgeI+28\n\n##LETS USE AGE AS AGE SINCE INOC\ngh1$AgeI<-0\ngh1$Age2<-2\ngh1$Age4<-4\ngh1$Age6<-6\ngh1$Age8<-8\ngh1$Age10<-10\ngh1$Age12<-12\ngh1$Age14<-14\ngh1$Age18<-18\ngh1$Age16<-16\ngh1$Age20<-20\ngh1$Age22<-22\ngh1$Age24<-24\ngh1$Age26<-26\ngh1$Age28<-28\n\n#make a binary column for survival\ngh1$SurvBinary <- as.numeric(gh1$Survival)\ngh1$SurvBinary[gh1$SurvBinary==2]<-0\n\n###GET RID OF UNFINISHED DATA###\ngh1<-subset(gh1, subset=ID<=271)\n\n#lets get rid of any ones that died\nghSurv<-gh1\ngh<-subset(gh1, subset=Survival==""Alive"")\ngh<-subset(gh, subset=AgeI>=0)\n\n\n#subset into a sterile and not sterile df\nghs<-subset(gh, subset=Soil==""Sterile"")\nghns<-subset(gh, subset=Soil==""Not Sterile"")\nghsSurv<-subset(ghSurv, subset=Soil==""Sterile"")\nghnsSurv<-subset(ghSurv, subset=Soil==""Not Sterile"")\n\n#get rid of empty levels\nghsSurv$Bacteria<-factor(ghsSurv$Bacteria)\nlevels(ghsSurv$Bacteria)\nghs$Bacteria<-factor(ghs$Bacteria)\nlevels(ghs$Bacteria)\n\n#alive NS growth \nplot(HeightI~AgeI, data=ghns, xlim=c(0,35), ylim=c(0,80), xlab=""Age(days)"", ylab=""Height(cm)"", col=Phosphate)\npoints(Height2~Age2, data=ghns, col=Phosphate)\npoints(Height4~Age4, data=ghns, col=Phosphate)\npoints(Height6~Age6, data=ghns, col=Phosphate)\npoints(Height8~Age8, data=ghns, col=Phosphate)\npoints(Height10~Age10, data=ghns, col=Phosphate)\npoints(Height12~Age12, data=ghns, col=Phosphate)\npoints(Height14~Age14, data=ghns, col=Phosphate)\npoints(Height16~Age16, data=ghns, col=Phosphate)\npoints(Height18~Age18, data=ghns, col=Phosphate)\npoints(Height20~Age20, data=ghns, col=Phosphate)\npoints(Height22~Age22, data=ghns, col=Phosphate)\npoints(Height24~Age24, data=ghns, col=Phosphate)\npoints(Height26~Age26, data=ghns, col=Phosphate)\npoints(Height28~Age28, data=ghns, col=Phosphate)\n\n#make subsets of phosphate sol and insol\nghnsp<-subset(ghns, subset=ghns$Phosphate==""Soluble"")\nghnsi<-subset(ghns, subset=ghns$Phosphate==""Insoluble"")\n\n\n\n\n\n\n     \n # glms sterile wet mass\nster.wmass.gl1<-glm(log(WetTotal)~(Phosphate*Bacteria), data=ghs)\nsummary(ster.wmass.gl1) \nanova(ster.wmass.gl1,test=""F"") \nster.wmass.gl2<-glm(log(WetTotal)~(Phosphate+Bacteria), data=ghs)\nsummary(ster.wmass.gl2) \n\n# only phosphate is significant\n# sterile dry mass\nsster.dmass.gl1<-glm(log(DryTotal)~(Phosphate*Bacteria), data=ghs)\nanova(sster.dmass.gl1,test=""F"")\nsster.dmass.gl2<-glm(log(DryTotal)~(Phosphate+Bacteria), data=ghs)\nsster.dmass.gl2.1<-glm(log(DryTotal)~(Phosphate+Bact3), data=ghs)\nsummary(sster.dmass.gl2.1)\nplot(sster.dmass.gl2)\n\n#actually might need to log this one\nsster.dmass.gl3<-glm(DryTotal~Bacteria, data=ghs,subset=(Phosphate==""Soluble""))\nanova(sster.dmass.gl3,test=""F"")\nsummary(sster.dmass.gl3)\n\nghs$Bact3<-as.factor(ghs$Bacteria)\nlevels(ghs$Bact3)\nlevels(ghs$Bact3)[2:5]<-""W,X,Y or Z""\n\n\n\n# non sterile wet mass\nster.wmass.gl1<-glm(log(WetTotal)~(Phosphate*Bacteria), data=ghns)\nanova(ster.wmass.gl1,test=""F"")\nster.wmass.gl1.1<-glm(log(WetTotal)~(Phosphate+Bacteria), data=ghns)\nsummary(ster.wmass.gl1.1)\nplot(ster.wmass.gl1.1)\n#lovely\n#lots of differences no sensible simplification\nster.dmass.gl1<-glm(log(DryTotal)~(Phosphate*Bacteria), data=ghns)\nanova(ster.dmass.gl1,test=""F"")\nster.dmass.gl1.1<-glm(log(DryTotal)~(Phosphate+Bacteria), data=ghns)\nsummary(ster.dmass.gl1.1)\nplot(ster.dmass.gl1.1)\n\nster.dmass.gl1<-glm(DryTotal~(Phosphate*Bacteria), data=ghns)\n\nanova(ster.dmass.gl1,test=""F"")\nster.dmass.gl1.1<-glm(DryTotal~(Phosphate+Bacteria), data=ghns)\nsummary(ster.dmass.gl1.1)\n\nplot(ster.dmass.gl1.1) # no reason to log']",3,"phosphate solubilising rhizobacteria, maize, competition, co-association, environmental conditions, orthophosphate availability, Serratia, Rhizobium, sterilised soil, gnotobiotic microcosms, growth promotion"
"Code + simulated + publically accessable data for ""Evaluating health facility access using Bayesian spatial models and location analysis methods""","# READMEThese files contain r data objects and R files that represent the key details of the paper, ""Evaluating health facility access using Bayesian spatial models and location analysis methods"".The following datasources are available for simulation of some of the ideas in the paper.- dat_grid_sim: simulated data of the grid and grid cells- dat_ohca_cv_sim: simulated data containing the cross validated test/training sets of OHCA data- dat_ohca_sim: simulated OHCA event data- dat_aed_sim: simulated AED location data- dat_bldg_sim: simulated building location data- dat_municipality_sim: simulated municipality information- table_1: Table 1 information containing key demographic dataThese data were produced using the code in 01-create-sim-data.R, and one of the statistical models is demonstrated in 02-demo-inla-model.RIn terms of the paper itself, the functions and code used in the manuscript are located in:* 01_tidy.Rmd - analysis code used to tidy up the data* 02_fit_fixed_all_cv.Rmd - analysis code used to place AEDs* 02_model.Rmd - analysis code used to fit the model in INLA* 03_manuscript.Rmd - Full code and text used to create the paper* 04_supp_materials.Rmd - full code and text used to create the supplementary materialsThe following files are a part of an R package ""swatial"" that was developed along with the paper. These files are:* DESCRIPTION* NAMESPACE* LICENSE* LICENSE.md* decay.R* spherical-distance.R* test-figure-data-matches.R* test-table-data-matches.R* testthat.R* tidy-inla.R* tidy-posterior-coefs.R* tidy-predictions.R* utils-pipe.R* All files that end in .Rd are documentation files for the functions.## Regarding data sourcesCensus information for Ticino was transcribed from the Annual Statistical Report of Canton Ticino from years 2010 to 2015. This data was taken from their publicly accessible annual reports - for example: (https://www3.ti.ch/DFE/DR/USTAT/allegati/volume/ast_2015.pdf). The raw data was extracted from these annual reports, and placed into the file: ""swiss_census_popn_2010_2015.xlsx"". These data are put into analysis ready format in the file 01_tidy.RmdHousing and other relevant geospatial data can be accessed via http://map.housing-stat.ch/ and https://data.geo.admin.ch/. The maps of buildings from the REA (Register of Buildings and Dwellings) can be found here: https://map.geo.admin.ch/?zoom=11&bgLayer=ch.swisstopo.pixelkarte-grau&lang=en&topic=ech&layers=ch.bfs.gebaeude_wohnungs_register,ch.swisstopo.swissboundaries3d-gemeinde-flaeche.fill,ch.bfs.volkszaehlung-gebaeudestatistik_gebaeude,ch.bfs.volkszaehlung-gebaeudestatistik_wohnungen,ch.swisstopo.swissbuildings3d_1.metadata,ch.swisstopo.swissbuildings3d_2.metadata&E=2717616.28&N=1096597.25&catalogNodes=687,696&layers_timestamp=,,2016,2016,,&layers_visibility=true,false,false,false,false,false&layers_opacity=1,1,1,1,1,0.75For further enquiries on this data, contact the Swiss federal Office of Statistics at the details listed here: https://www.bfs.admin.ch/bfs/en/home/services/contact.htmlThe shapefiles of the Comuni can be accessed here: https://www4.ti.ch/dfe/de/ucr/documentazione/download-file/?noMobile=1Data from the people living in the Municipalities in Ticino can be downloaded here: https://www3.ti.ch/DFE/DR/USTAT/index.php?fuseaction=dati.home&tema=33&id2=61&id3=65&c1=01&c2=02&c3=02## Future workIn the future, these functions from the paper may be generalised and put into their own package. If that happens, this repository will be updated with a link to updated functions.","['# simulate the data for the swiss spatial paper """"\n# Author: Nicholas Tierney\n\n# R packages used: sf, tidyverse, here\nlibrary(sf)\nlibrary(tidyverse)\n\ngrid_sim_prep <-\n  readr::read_rds(\nhere::here(""manuscript/outputs/01_tidy_output/rpolycount.rds"")\n    ) %>%\n  st_as_sf() %>%\n  rename(pop_dens = `Density_per_km2`,\n         prop_men = `1002_02_Pop Men_Perc`,\n         prop_65 = `1010_02_Pop 65 years and above_Perc`,\n         prop_transport = `01_Set_Urb_03_Transport`,\n         prop_industr_comm = `01_Set_Urb_01_IndComm`,\n         prop_bldg = `01_Set_Urb_02_Building`,\n         prop_rec = `01_Set_Urb_05_RecrCem`,\n         prop_spec_urb = `01_Set_Urb_04_Special`,\n         prop_finance = `1050_01_Financial strength index_Value`) %>%\n  mutate(prop_transport = prop_transport/area_hectare,\n         prop_industr_comm = prop_industr_comm/area_hectare,\n         prop_bldg = prop_bldg/area_hectare,\n         prop_rec = prop_rec/area_hectare,\n         prop_spec_urb = prop_spec_urb/area_hectare) %>% \n  mutate(prop_transport = prop_transport/n_grid,\n         prop_industr_comm = prop_industr_comm/n_grid,\n         prop_bldg = prop_bldg/n_grid,\n         prop_rec = prop_rec/n_grid,\n         prop_spec_urb = prop_spec_urb/n_grid,\n         pop_dens = pop_dens / n_grid,\n         prop_men = prop_men / n_grid,\n         prop_65 = prop_65 / n_grid,\n         prop_finance = prop_finance / n_grid) %>%\n  select(count,\n         prop_men,\n         pop_dens,\n         prop_65,\n         prop_transport,\n         prop_industr_comm,\n         prop_bldg,\n         prop_rec,\n         prop_spec_urb,\n         prop_finance)\n\n# simulation: find min and max, generate values between them uniformly.\n\ndat_grid_sim <- grid_sim_prep %>%\n  dplyr::transmute_if(.predicate = is.numeric,\n                      .funs = function(x){\n                        runif(n = x, min = min(x), max = max(x))\n                        }) %>%\n  mutate(count = round(count))\n\n# make this an sp object.\ndat_grid_sim <- as(dat_grid_sim, ""Spatial"")\n\n# write the data\nreadr::write_rds(dat_grid_sim,\n                 here::here(""PLoS_ONE_submission/simulated-data/dat_grid_sim.rds""))\n\n# OHCA data --------------------------------------------------------------------\n\n# extract relevant data\ndat_ohca_sim_prep <- \n  readr::read_rds(""manuscript/outputs/01_tidy_output/dat_ohca.rds"") %>%\n  select(event_id,\n         lat,\n         long,\n         urban_rural_ohca,\n         municipality)\n\ndat_ohca_sim <- dat_ohca_sim_prep %>%\n  # keep event ids unique, but recode them\n  mutate(event_id = 1:n()) %>%\n  # generate lat/long values within the bounding box\n  mutate_at(.vars = vars(lat, long),\n            .funs = function(x){\n              runif(n = x, min = min(x), max = max(x))\n            }) %>%\n  # randomly select urban or rural and municipality values\n  mutate_at(.vars = vars(urban_rural_ohca, municipality),\n            .funs = sample) %>%\n  as_tibble() %>%\n  # randomly select 10 rows of the data\n  sample_n(100)\n\nreadr::write_rds(dat_ohca_sim,\n                 here::here(""PLoS_ONE_submission/simulated-data/dat_ohca_sim.rds""))\n\n\n# OHCA cross validated data ----------------------------------------------------\n\nlibrary(modelr)\nset.seed(2018-05-17-1139)\ndat_ohca_cv_sim <- crossv_kfold(dat_ohca_sim, 5) %>% \n  # we change the test and train sets from the `resample`\n  mutate(test = map(test,as_tibble),\n         train = map(train,as_tibble))\n# This creates a dataframe with test and training sets\n\nreadr::write_rds(dat_ohca_cv_sim,\n                 here::here(""PLoS_ONE_submission/simulated-data/dat_ohca_cv_sim.rds""))\n\n# AED data ---------------------------------------------------------------------\n\ndat_aed_sim_prep <- \n  readr::read_rds(""manuscript/outputs/01_tidy_output/dat_aed.rds"") %>%\n  select(aed_id,\n         lat,\n         long,\n         urban_rural_aed,\n         town)\n\ndat_aed_sim <- dat_aed_sim_prep %>%\n# keep event ids unique, but recode them\nmutate(aed_id = 1:n()) %>%\n  # generate lat/long values within the bounding box\n  mutate_at(.vars = vars(lat, long),\n            .funs = function(x){\n              runif(n = x, min = min(x), max = max(x))\n            }) %>%\n  # randomly select urban or rural and town values\n  mutate_at(.vars = vars(urban_rural_aed, town),\n            .funs = sample) %>%\n  as_tibble() %>%\n  # randomly select 10 rows of the data\n  sample_n(100)\n\nreadr::write_rds(dat_aed_sim,\n                 here::here(""PLoS_ONE_submission/simulated-data/dat_aed_sim.rds""))\n\n# building data ---------------------------------------------------------------\n\ndat_bldg_sim_prep <-\nreadr::read_rds(""manuscript/outputs/01_tidy_output/dat_building.rds"") %>%\n  select(lat,\n         long,\n         municipality_name,\n         urban_rural_bldg) \n\ndat_bldg_sim <- dat_bldg_sim_prep %>%\n  mutate_at(.vars = vars(lat, long),\n            .funs = function(x){\n              runif(n = x, min = min(x), max = max(x))\n            }) %>%\n  # randomly select urban or rural and town values\n  mutate_at(.vars = vars(municipality_name, urban_', '# code to fit the model, demonstrated using the simulation data\n# Author: Nicholas Tierney\n\n# Packages used: spdep, INLA.\n\nlibrary(INLA)\n\nbsa_adj_swiss <- spdep::poly2nb(dat_grid_sim_sp, queen = FALSE)\n\nadj_swiss <- spdep::nb2mat(bsa_adj_swiss,\n                           style = ""B"",\n                           zero.policy = TRUE) %>% as(""dgTMatrix"")\n\nswiss_idx <- 1:nrow(dat_grid_sim_sp)\n\nbtdf <- as.data.frame(dat_grid_sim_sp) \n\nsystem.time(\n  model_inla_bym_grid <- \n    inla(formula = \n           count ~ prop_men + \n           pop_dens +\n           prop_65 + \n           prop_transport + \n           prop_industr_comm + \n           prop_bldg + \n           prop_rec + \n           prop_spec_urb + \n           prop_finance +\n           f(swiss_idx, \n             model = ""bym"", \n             graph = adj_swiss), \n         family = ""zeroinflatedpoisson1"", \n         data = btdf, \n         control.compute = list(dic = TRUE, \n                                config = TRUE, # to calculate posterior draws\n                                cpo = TRUE), \n         control.predictor = list(compute = TRUE,\n                                  link = 1))\n)\n\n# took me about 300 seconds to fit the model', '#\' Gaussian Decay function\n#\' \n#\' This is a function that creates some decay between some distance and the \n#\' maximum distance.\n#\'\n#\' @param d_kj given distance\n#\' @param d_0 maximum distance. When d_kj == d_0, you will get 0\n#\'\n#\' @return numeric\n#\' \n#\' @export\n#\'\n#\' @examples\n#\' \n#\' gauss_decay(1,100)\n#\' gauss_decay(50,100)\n#\' gauss_decay(99,100)\n#\' \ngauss_decay <- function(d_kj, \n                        d_0) {\n  if (d_kj <= d_0) {\n    (exp(-0.5 * (d_kj / d_0) ^ 2)\n     - exp(-0.5)) / (1 - exp(-0.5))\n  } else 0\n  # warning(""d_0 should not be greater than d_kj"")\n}\n\n# A note here that the input for d_kj is all of those OHCA events that are \n# within 100m of the given AED / EMS location\n# So this means that there will be some really cool function\n# that will use the distance calculation between the AEDs and OHCA\n# (grab from maxcovr)\n\n\n#\' Stepwise decay function\n#\'\n#\' @param d_kj given distance\n#\' @param d_0 maximum distance. When d_kj == d_0, you will get 0\n#\' @param jump specified jump distance - default of 0.09\n#\' @param n_steps The number of steps to include, default is 5\n#\'\n#\' @return single numeric number of the decay value\n#\' @export\n#\'\n#\' @examples\n#\' step_decay(0, 100)\n#\' step_decay(1, 100)\n#\' step_decay(50, 100)\n#\' step_decay(99, 100)\n#\' step_decay(100, 100)\nstep_decay <- function(d_kj,\n                       d_0,\n                       jump = 0.09,\n                       n_steps = 5) {\n  \n  steps <- 1 - jump * 1:n_steps\n  \n  size  <- 1 / n_steps\n  \n  out <- ifelse(test = (d_kj <= d_0),\n                yes = steps[floor(d_kj / d_0 / size) + 1],\n                no = 0)\n  \n  out[is.na(out)] <- 0\n  \n  return(out)\n  \n}\n', ""#' Calculate the distance between two lat/lon\n#'\n#' This function uses the haversine formula to calculate the great circle\n#' distance between two locations, identified by their latitudes and \n#' longitudes. It is borrowed from rnoaa\n#' (https://github.com/ropenscilabs/rnoaa/blob/master/R/meteo_distance.R) \n#' I have renamed it from meteo_spherical_distance to spherical_distance\n#'\n#' @param lat1 Latitude of the first location.\n#' @param long1 Longitude of the first location.\n#' @param lat2 Latitude of the second location.\n#' @param long2 Longitude of the second location.\n#'\n#' @return A numeric value giving the distance in meters between the\n#'    pair of locations.\n#'\n#' @note This function assumes an earth radius of 6,371 km.\n#'\n#' @examples\n#'\n#' spherical_distance(lat1 = -27.4667,\n#'                    long1 = 153.0217,\n#'                    lat2 = -27.4710,\n#'                    long2 = 153.0234)\n#'\n#' @export\nspherical_distance <- function(lat1,\n                               long1,\n                               lat2,\n                               long2) {\n  \n  radius_earth <- 6371\n  \n  deg2rad <- function(deg) return(deg*pi/180)\n  # Convert angle values into radians\n  lat1 <- deg2rad(lat1)\n  long1 <- deg2rad(long1)\n  lat2 <- deg2rad(lat2)\n  long2 <- deg2rad(long2)\n  \n  # Determine distance using the haversine formula, assuming a spherical earth\n  a <- sin((lat2 - lat1) / 2) ^ 2 + cos(lat1) * cos(lat2) *\n    sin((long2 - long1) / 2) ^ 2\n  \n  d <- 2 * atan2(sqrt(a), sqrt(1 - a)) * radius_earth\n  \n  # return distance in kilometres\n  d <- d * 1000\n  # return(d)\n  # return the distance in kilometers\n  return(d)\n  \n} # End function"", 'context(""Figure data"")\n\nlibrary(zeallot)\nlibrary(purrr)\n\noriginal_figures <- fs::dir_map(here::here(""manuscript"",\n                                          ""06_figure_data"",\n                                          ""2019-05-03""),\n                               readr::read_rds)\n\ncurrent_figures <- fs::dir_map(here::here(""manuscript"",\n                                         ""06_figure_data"",\n                                         ""current""),\n                              readr::read_rds)\n\nc(og_figure_1_data,\n  og_figure_2_data,\n  og_figure_3_data,\n  og_figure_4_data,\n  og_figure_5_data) %<-% original_figures\n\nc(current_figure_1_data,\n  current_figure_2_data,\n  current_figure_3_data,\n  current_figure_4_data,\n  current_figure_5_data) %<-% current_figures\n\ntest_that(""figure 1 data matches"", {\n  expect_identical(og_figure_1_data, current_figure_1_data)\n  expect_true(all(map2_lgl(og_figure_1_data, current_figure_1_data, identical)))\n})\n\ntest_that(""figure 2 data matches"", {\n  expect_identical(og_figure_2_data, current_figure_2_data)\n  expect_true(all(map2_lgl(og_figure_2_data, current_figure_2_data, identical)))\n})\n\ntest_that(""figure 3 data matches"", {\n  expect_identical(og_figure_3_data, current_figure_3_data)\n  expect_true(all(map2_lgl(og_figure_3_data, current_figure_3_data, identical)))\n})\ntest_that(""figure 4 data matches"", {\n  expect_identical(og_figure_4_data, current_figure_4_data)\n  expect_true(all(map2_lgl(og_figure_4_data, current_figure_4_data, identical)))\n})\ntest_that(""figure 5 data matches"", {\n  expect_identical(og_figure_5_data, current_figure_5_data)\n  expect_true(all(map2_lgl(og_figure_5_data, current_figure_5_data, identical)))\n})\n', 'context(""Table data"")\n\nlibrary(zeallot)\nlibrary(purrr)\n\noriginal_tables <- fs::dir_map(here::here(""manuscript"",\n                                          ""05_table_data"",\n                                          ""2019-05-03""),\n                               readr::read_rds)\n\ncurrent_tables <- fs::dir_map(here::here(""manuscript"",\n                                         ""05_table_data"",\n                                         ""current""),\n                              readr::read_rds)\n\nc(og_table_1_data,\n  og_table_2_data,\n  og_table_3_data,\n  og_table_4_data) %<-% original_tables\n\nc(current_table_1_data,\n  current_table_2_data,\n  current_table_3_data,\n  current_table_4_data) %<-% current_tables\n\ntest_that(""the table data matches"", {\n  expect_identical(og_table_1_data, current_table_1_data)\n  expect_identical(og_table_2_data, current_table_2_data)\n  expect_identical(og_table_3_data, current_table_3_data)\n  expect_identical(og_table_4_data, current_table_4_data)\n  \n  expect_true(all(map2_lgl(og_table_1_data, current_table_1_data, identical)))\n  expect_true(all(map2_lgl(og_table_1_data, current_table_1_data, identical)))\n  expect_true(all(map2_lgl(og_table_1_data, current_table_1_data, identical)))\n  expect_true(all(map2_lgl(og_table_1_data, current_table_1_data, identical)))\n  expect_true(all(map2_lgl(og_table_1_data, current_table_1_data, identical)))\n})\n\n# new_false_table <- current_table_1_data\n# new_false_table[1,1] <- NA\n# \n# test_that(""data breaks when doesn\'t match table data matches"", {\n#   expect_identical(og_table_1_data, new_false_table)\n# })\n', 'library(testthat)\nlibrary(swatial)\n\ntest_check(""swatial"")\n', '#\' Tidy up INLA model output\n#\'\n#\' @param x an INLA model\n#\'\n#\' @return a dataframe with columns ""terms"" ""mean"" ""sd"" ""0.025quant"" ""0.5quant"" ""0.975quant"" ""mode"" ""kld""\n#\' \n#\' @export\n#\'\n#\' @examples\n#\' \n#\' \\dontrun{\n#\' \n#\' tidy(model_inla_bym_grid)\n#\' \n#\' }\n#\' \ntidy.inla <- function(x){\n  \n  tibble::as_tibble(x$summary.fixed) %>%\n    dplyr::mutate(terms = rownames(.)) %>%\n    dplyr::select(terms,\n                  dplyr::everything())\n  \n}', '#\' Tidy INLA Posterior Coefficients\n#\'\n#\' Creates a dataframe where each row is a posterior sample, and the \n#\' columns are the coefficients from the model. Still need to work out how\n#\' to get the inla\n#\'\n#\' @param samples \n#\' @param model \n#\'\n#\' @return dataframe of posterior coefficients\n#\' @export\n#\'\n#\' @examples\n#\' \n#\' \\dontrun{\n#\' tidy_posterior_coefs(inla_bym_samples,model_inla_bym_grid)\n#\' }\n#\' \ntidy_posterior_coefs <- function(samples, \n                                 model, \n                                 new_covars = TRUE){\n  \n  latent_rows <- rownames(samples[[1]]$latent)\n  coef_in_samples_lgl <- latent_rows %in% model$names.fixed\n  coef_names_in_samples <- latent_rows[coef_in_samples_lgl]\n  \n  coef_names_in_samples\n  \n  if (new_covars == TRUE) {\n    \n    test_inla_sample <- samples %>%\n      purrr::map_dfr(\n        .f = function(x){\n          tibble::tibble(""(Intercept)"" = x$latent[coef_in_samples_lgl][1],\n                 ""prop_men"" = x$latent[coef_in_samples_lgl][2],\n                 ""pop_dens"" = x$latent[coef_in_samples_lgl][3],\n                 ""prop_65"" = x$latent[coef_in_samples_lgl][4],\n                 ""prop_transport"" = x$latent[coef_in_samples_lgl][5],\n                 ""prop_industr_comm"" = x$latent[coef_in_samples_lgl][6],\n                 ""prop_bldg"" = x$latent[coef_in_samples_lgl][7],\n                 ""prop_rec"" = x$latent[coef_in_samples_lgl][8],\n                 ""prop_spec_urb"" = x$latent[coef_in_samples_lgl][9],\n                 ""prop_finance"" = x$latent[coef_in_samples_lgl][10])\n        }\n      )\n    \n  } else if (new_covars == FALSE) {\n    test_inla_sample <- samples %>%\n      purrr::map_dfr(\n        .f = function(x){\n          tibble::tibble(""(Intercept)"" = x$latent[coef_in_samples_lgl][1],\n                 ""prop_65_and_up"" = x$latent[coef_in_samples_lgl][2],\n                 ""prop_male"" = x$latent[coef_in_samples_lgl][3],\n                 ""population_density"" = x$latent[coef_in_samples_lgl][4])\n        }\n      )\n  }\n  return(test_inla_sample)\n  \n} # end function', ""#' Tidy up the predictions from an INLA model\n#' \n#' Create a dataframe of the predictions\n#'\n#' @param x \n#'\n#' @return data.frame\n#' \n#' @export\n#'\n#' @examples\n#' \n#' \\dontrun{\n#' tidy_predictions(model_inla_bym_grid)\n#' }\ntidy_predictions <- function(x){\n  \n  tibble::as_tibble(x$summary.linear.predictor) %>%\n    dplyr::mutate(terms = rownames(.)) %>%\n    dplyr::mutate(predictor_id = readr::parse_number(terms)) %>%\n    dplyr::select(\n      predictor_id,\n      dplyr::everything()\n    )\n  \n}"", ""#' Pipe operator\n#'\n#' See \\code{magrittr::\\link[magrittr]{\\%>\\%}} for details.\n#'\n#' @name %>%\n#' @rdname pipe\n#' @keywords internal\n#' @export\n#' @importFrom magrittr %>%\n#' @usage lhs \\%>\\% rhs\nNULL\n\n\n#' @importFrom generics tidy\n#' @export\ngenerics::tidy""]",3,"Code, simulated data, publically accessible data, health facility access, Bayesian spatial models, location analysis methods, r data objects, OHCA data, AED location data, building location data, municipality information, demographic data, statistical models, INLA"
Data from: The role of ecotype-environment interactions in intraspecific trophic niche partitioning subsequent to stocking,"Worldwide, stocking of fish represent a valuable tool for conservation and maintenance of species exploited by recreational fishing. Releases of hatchery-reared fish are more and more recognized to have numerous demographic, ecological and genetic impacts on wild populations. However, consequences on intraspecific trophic relationships have rarely been investigated. In this study, we assessed the impacts of supplementation stocking and resulting introgressive hybridization on the trophic niches occupied by stocked, local and hybrid Lake Trout (Salvelinus namaycush) within populations of piscivorous and planktivorous ecotypes stocked from a wild piscivorous source population. We compared trophic niches using stable isotope analysis (13C and 15N) and trophic position among the three genetic origins. Putative genetic effects were tested with phenotype-genotype association of ""life history"" ecological traits (body size, growth rate, condition index and trophic niche) and genotypes (RADseq SNP markers) using redundant discriminant analysis (RDA). Results showed that sympatry resulting from the stocking of contrasting ecotypes is a risk factor for niche partitioning. Planktivorous populations are more susceptible to niche partitioning, by competitive exclusion of the local fish from a littoral niche to an alternative pelagic/profundal niche. Observed niche partitioning is probably a manifestation of competitive interactions between ecotypes. Our results emphasize that ecotypic variation should be considered for more efficient management and conservation practices and in order to mitigate negative impact of supplementation stocking.","['####### Cleaning workspace #########\r\nrm(list=ls())\r\n\r\n####### Loading required librairies #########\r\nlibrary(FSA)\r\nlibrary(nlstools)\r\nlibrary(dplyr)\r\nlibrary(SIBER)\r\nlibrary(lme4)\r\nlibrary(nlme)\r\n\r\n####### Establish a work directory #########\r\nsetwd(""/your_work_directory/"")\r\n\r\n############################ Comparison of isotopic values between muscle types (neck and dorsal) in same individuals ###################\r\n####### Loading the data #########\r\ndata <- read.csv(""/your_work_directory/SIA_muscle_comparison.csv"", header=TRUE, sep="";"", dec=""."")\r\n\r\n####### ANOVA among muscle type #########\r\nfit_aov_C <- lm(d13C ~ muscle, data = data)\r\nanova(fit_aov_C)\r\n\r\nfit_aov_N <- lm(d15N ~ muscle, data = data)\r\nanova(fit_aov_N)\r\n\r\n############################ Calculation of Standard Ellipse Area (SEA) using the SIBER R package (Jackson et al. 2011) ###################\r\n#This R script is largely inspired of SIBER vignette bye Andrew Jackson, available online.\r\n\r\n####### Loading the data #########\r\nrm(list=ls())\r\ndata <- read.csv(""/your_work_directory/SIBER_all_populations.csv"", header=TRUE, sep="";"", dec=""."")\r\n\r\n####### Creating a siber object #########\r\nsiber.data <- createSiberObject(data)\r\n\r\n\r\n####### Calculating the group metrics, including SEA and Total area (TA) #########\r\ngroup.ML <- groupMetricsML(siber.data)\r\nprint(group.ML)\r\n\r\n############################ Modelling of the Linear mixed effect model (LMEM) on SEA values ###################\r\n\r\n####### Loading the data #########\r\nrm(list=ls())\r\ndata <- read.csv(""/your_work_directory/SEA_by_pop.csv"", header=TRUE, sep="";"", dec=""."")\r\n\r\n####### Valudation of data distribution #########\r\nhist(sqrt(data$SEA), breaks=7)\r\n\r\n####### Computation of the LMEM on SEA data #########\r\nfit <- lme(fixed=sqrt(SEA) ~ ecotype + origin %in% ecotype, random = ~1|lake, data = data, na.action = na.omit)\r\nsummary(fit)\r\nanova(fit)\r\nlsmeans(fit, pairwise~origin, adjust=""tukey"")\r\n\r\n####### Summary figure, figure 2 in Morissette et al. (2019) #########\r\ndata$origin <- ordered(data$origin, levels=c(""Wild"", ""Local"", ""Hybrid"", ""Stocked""))\r\ntiff(""/your_work_directory/Figure_2.tiff"", width = 3, height = 3, units = ""in"", res = 300)\r\npar(mar = c(2,2.5,1,1))\r\nboxplot(sqrt(SEA)~origin, data=data,\r\n        cex=0.6, col = ""light gray"", boxwex = 0.6, ylim = c(0,3), cex.axis=0.6, cex.lab=0.7)\r\nmtext(""Log(Standard ellipse area)"", side = 2, line=1.7, cex=0.7)\r\ntext(1,2.3,""A,B"", cex=0.6)\r\ntext(2,1.8,""A"", cex=0.6)\r\ntext(3,1.7,""A"", cex=0.6)\r\ntext(4,2.5,""B"", cex=0.6)\r\n\r\ndev.off()\r\n\r\n\r\n\r\n############################ Modelling of the Linear mixed effect model (LMEM) on stable isotope values ###################\r\n\r\n####### Loading the data #########\r\nrm(list=ls())\r\ndata <- read.csv(""/your_work_directory/SIA_lake_trout.csv"", header=TRUE, sep="";"", dec=""."")\r\n\r\n####### LMEM of trophic position data ########\r\ntp_model <- lme(fixed= trophic_position ~ ecotype + origin %in% ecotype + tl_cap %in% ecotype, random = ~1|lake, data = stk, na.action = na.omit)\r\nsummary(tp_model)\r\nanova(tp_model)\r\nlsmeans(tp_model, pairwise~origin, adjust=""tukey"")\r\n\r\n####### LMEM of pelagic distance data ########\r\npel_model <- lme(fixed= delta_pelagic ~ ecotype + origin %in% ecotype + tl_cap %in% ecotype, random = ~1|lake, data = data, na.action = na.omit)\r\nsummary(pel_model)\r\nanova(pel_model)\r\nlsmeans(pel_model, pairwise~origin, adjust=""tukey"")\r\n\r\n\r\n\r\n############################ Summary figure, figure 3 in Morissette et al. (2019) ############################\r\n\r\n####### Data filtering #######\r\npisc <- filter(data, ecotype == ""I"") %>% droplevels()\r\npisc_L <- filter(pisc, origin ==""L"") %>% droplevels()\r\npisc_H <- filter(pisc, origin ==""H"") %>% droplevels()\r\npisc_E <- filter(pisc, origin ==""E"") %>% droplevels()\r\nunstk_pisc <- filter(data, stocked == ""N"" & ecotype == ""I"" & lake != ""Opeongo"")%>% droplevels()\r\n\r\n\r\n####### Data filtering #######\r\nplank <- filter(data, ecotype == ""P"") %>% droplevels()\r\nplank_L <- filter(plank, origin ==""L"") %>% droplevels()\r\nplank_H <- filter(plank, origin ==""H"") %>% droplevels()\r\nplank_E <- filter(plank, origin ==""E"") %>% droplevels()\r\nunstk_plank <- filter(data, stocked == ""N"" & ecotype == ""P"")%>% droplevels()\r\n\r\n\r\n####### Figure creation #######\r\ntiff(""/your_work_directory/Figure_3.tiff"", height = 6, width =6, units = ""in"", res = 300)\r\npar(mfrow = c(2,2), \r\n    mar = c(2,1,0,0), \r\n    oma = c(1,2,1,1))\r\n\r\nboxplot(unstk_pisc$trophic_position,pisc_L$trophic_position, pisc_H$trophic_position, pisc_E$trophic_position, col = ""light gray"", \r\n        names = c(""Wild"", ""Local"",""Hybrid"", ""Stocked""), outline=F, \r\n        ylim = c(4.5,6.4 ), cex = 0.7, boxwex = 0.7, axes = F)\r\naxis(1, c(1,2,3,4), labels = c(""Wild"", ""Local"",""Hybrid"", ""Stocked""), cex.axis = 0.8, mgp = c(0,0.5,0))\r\naxis(2, seq(4.5, 6, 0.5), cex.axis = 0.8)\r\ntext(3.8,6.4, ""Piscivorous"", cex = 0.8)\r\ntext(1,6.4, ""A"")\r\n\r\nboxplot(unstk_pisc$delta_pelagic,pisc_L$delta_pelagic, pisc_H$delta_pelagic, pisc_E$delta_pelagic, col = ""light gray"", \r\n        names = c(""Wild"", ""L']",3,"1. Ecotype
2. Environment interactions
3. Trophic niche
4. Stocking
5. Fish
6. Conservation
7. Recreational fishing
8. Demographic impacts
9. Ecological impacts
10"
chemo-diversity_fungal_volatile__organic_compounds,"The R codes for statistical analyses of fungal volatilome data from 43 fungal species applied in the research article ""Volatile organic compound patterns predict fungal trophic mode and lifestyle"" by Guo et al. The R files include the codes for:Heatmap visualization of the volatilome from 43 examined fungal species.Chemical diversity of volatile profiles of 43 examined fungal species.Chemotaxonomic and functional characterization (DAPC) of fungal volatilome.Phenotypic integration analysis.Machine learning for identification of volatile biomarkers.","['# Load library\r\nlibrary(vegan)\r\nlibrary(data.table) \r\nlibrary(ggpubr)\r\n# load data set used for analysis\r\n#ã€€gc data and ptr data were computed seperatly\r\nData <- read.csv(""Emission_intensity_of_all_detected_compounds_from_all_fungi.csv"", header = TRUE)\r\n# calculation of Pilou_evenness\r\n   # calculate sum of compounds in each species\r\n    S <- apply(Data[-1]>0, 1, sum)\r\n    Diversity <- diversity(Data[-1], index=""simpson"")/log(S)\r\n\r\n    #compounds_richness, Menhinick\'s index \r\n    n<-apply(Data[,-1]>0, 1, sum)\r\n    N <- apply(Data[,-1], 1, sum)\r\n    Diversity <- n/sqrt(N)\r\n\r\n# bar plot\r\nggbarplot(Diversity, x = ""species"", y = ""Diversity"",  \r\n               color = ""grey87"", fill = ""grey87"", add.params = list(color = ""black"",shape=1,size=.5),\r\n               add = c(""mean_se"", ""jitter""),  \r\n               palette = ""jco"",x.text.angle = 90,\r\n               position = position_dodge(.2), orientation = ""horizontal"") + \r\n               theme(text = element_text(size=5)) +\r\n               theme(axis.text.y = element_blank()\r\n               )\r\n\r\n        \r\n', '# Load library\r\n  library(""ggplot2"")\r\n  library(""adegenet"")\r\n# Combined gc and ptr data set were used for Discriminant analysis\r\n# load data set used for analysis\r\n    Data <- read.csv("" Emission_intensity_of_all_detected_compounds_from_all_fungi.csv"", header = TRUE)\r\n    Data_Mat <- as.matrix(Data[-c(class)])\r\n# standardize data to remove the influence of scale of gc and ptr data\r\n    Norm_Data_Mat <- scale(Data_Mat, center = TRUE, scale = TRUE)\r\n# put rownames of standardized data\r\n    Names <- Data[,class]\r\n#using cross-validation to determine the optimal number of PCs to retain\r\n    xval <- xvalDapc(Norm_Data_Mat, Data$class, n.pca.max = 300, training.set = 0.75,result = ""groupMean"", center = FALSE, scale =FALSE, n.pca = NULL, n.rep = 30, xval.plot = TRUE) \r\n#check the optamial number of PCs Achieving Highest Mean Success and Lowest MSE \r\n    xval$DAPC\r\n# computation of Discriminant analysis\r\n  dapc_class <- dapc(Norm_Data_Mat, Data$class) \r\n# get scatter plot\r\n  scatter(dapc_phylum, scree.da=FALSE, cex = 3, \r\n    col = c(""darkblue"", ""red"", ""purple"",""orange""), \r\n    # color configuration for each class\r\n    # PHYLUM: use default color\r\n    # Class: library(""RColorBrewer""), col = brewer.pal(n=11, name=""Paired""),\r\n    # Order: library(RColorBrewer), pch=15:20, col = brewer.pal(8,""Dark2""),\r\n    # Family: library(RColorBrewer), pch=15:20, col = brewer.pal(11,""Paired""),\r\n    # Trophic mode: col = c(""darkblue"", ""red"", ""purple"",""orange""),\r\n    # lifestyle : col = c(""green"", ""blue""),\r\n    # Substrate-use:  col = c(""yellow"", ""yellowgreen"",""yellow4""),\r\n    # Host-type: col = c(""gray"", ""gold""),\r\n    leg = TRUE,clab=0, cleg=1, solid=0.5, cellipse=0, posi.leg=""topleft"",cstar=0,label.inds = list(air=0.1, pch=NA)) \r\n\r\n  #supplymentary figure 4a\r\n    scatter(dapc_phylum, scree.da=FALSE, cex = 3,\r\n    col = c(""red"", ""purple"",""orange""),\r\n    leg = TRUE,clab=0, cleg=1, solid=0.5, cellipse=0, posi.leg=""topleft"",cstar=0, label.inds = list(air=0.1, pch=NA)) \r\n\r\n', '# emission_profiles_of_43_fungal_species\r\n# Load library\r\nlibrary(ComplexHeatmap)\r\n# load data set used for analysis\r\nData <- read.csv(""Emission_intensity_of_all_detected_compounds_from_all_fungi.csv"", header = TRUE)\r\n# calculate means of each species\r\nMean_Data <- aggregate(. ~ Species, Data, mean)\r\n# standardize data to remove the influence of scale of gc and ptr data\r\nNorm_Mean_Data <- scale(Mean_Data, center = TRUE, scale = TRUE)\r\n# put rownames of prepared data\r\nrow.names(Norm_Mean_Data) <- Mean_Data[,2]\r\n#plot heatmap\r\n    Heatmap(Norm_Mean_Data, cluster_rows=FALSE,\r\n      clustering_distance_columns = ""spearman"", clustering_method_columns = ""complete"",\r\n      column_dend_height = unit(1, ""cm""), \r\n      row_names_gp = gpar(fontsize = 6),\r\n      column_names_gp = gpar(fontsize = 3),\r\n      )', '# gc and ptr data were used for modeling seperatly\r\n## train the random forest model using Recursive feature elimination (RFE)\r\n  library(caret)\r\n  library(randomForest)\r\n  library(dplyr)\r\n  Data <- read.csv(""Data.csv"", header = TRUE)\r\n  # subset data for each class\r\n  Data_Mat <- Raw[-class]\r\n  class <- as.factor(Data[,i]) # i indicidate each class\r\n  i_Data <- cbind(class, Data_Mat)\r\n  # in the model of substrate_use, factor ""root_shoot"" were removed due to the limited number of factors\r\n  # model training\r\n  # spliting data to training and testing subsetï¼Œrandom sampling occurs within each class and should preserve the overall class distribution of the data \r\n  set.seed(1234)\r\n  Index <- createDataPartition(i_Data$class, p = .75, list = FALSE, times = 1)\r\n  Train_i <- i_Data[Index,] \r\n  Test_i <- i_Data[-Index,]\r\n  # to fix the class imblance influence on model, we choose the ""up"" sampling outside the cross validation, becasue rfe do not support sampling inside CV  \r\n  #create up-samplled train data\\\r\n  set.seed(1234)\r\n  Train_i_up_1 <- upSample(x = Train_i[, -ncol(Train_i)], y = as.factor(Train_i$class))\r\n  #remove gernerated ""Class"" column\r\n  Train_i_up <- subset(Train_i_up_1, select=-Class)\r\n  #build rf-rfe model\r\n  # controlling feature selection algorithm\r\n  Control_Fea_phy <- rfeControl(functions = rfFuncs, method = ""cv"", number = 10, saveDetails = TRUE, p = 0.75, returnResamp = ""final"", verbose = FALSE)\r\n  set.seed(1234)\r\n  Rf_Rfe_i <- rfe(Train_i_up[,-1], Train_i_up[,1], \r\n        sizes = c(5, 10, 15), \r\n        metric = ""Accuracy"", \r\n        rfeControl = Control_Fea_phy, \r\n        tuneLength=9) \r\n  Rf_Rfe_i #check the best subset size and corresponding accuracy subset  top 15\r\n  # save variable importance subset\r\n  Imp_Rf_i <- varImp(Rf_Rfe_i)\r\n  write.csv(Imp_Rf_i, ""PTR_Imp_i.csv"") \r\n  # validation using testing subset (obain model accuray and sensitity)\r\n  set.seed(1234)\r\n  Rf_Rfe_i_Valid <- predict(Rf_Rfe_i, newdata = Test_i)\r\n  #calculate validation performance\r\n  Valid_rf_i <- confusionMatrix(Rf_Rfe_i_Valid$pred, Test_i$class)\r\n  #show information of validation\r\n  Valid_rf_i\r\n  summary(Valid_rf_i)\r\n  #overall accuracy\r\n  Overall_Accuracy_rf_i<- Valid_rf_phy$overall\r\n  write.csv(Overall_Accuracy_rf_i, ""Overall_Accuracy_rf_valid.csv"") \r\n  #accuracy by class\r\n  ClassAccuracy_rf_i <- Valid_rf_i$byClass\r\n  write.csv(ClassAccuracy_rf_i, ""ClassAccuracy_rf_valid.csv"") \r\n  #using the top 15 compounds to build new model\r\n  library(dplyr)\r\n  #create name index based on variable importance \r\n  col_index <- varImp(Rf_Rfe_i)%>% mutate(names=row.names(.)) %>% arrange(-Overall)\r\n  # extract colnames of the top 15 variable\r\n  imp_names <- col_index$names[1:15]\r\n  # subset top15 dataset from raw dataset\r\n  Top15_Subset <- i_Data[,imp_names]\r\n  #set rownames of the Top15 subset \r\n  Top15_i_rf_rfe <- cbind(class, Top15_Subset)\r\n  # save the predictor subset for further visulaization\r\n  write.csv(Top15_i_rf_rfe, ""Top15_rf_rfe.csv"")\r\n  # run model again using top15 subset to valid the model accuracy, sensitivity and specificity.\r\n  \r\n## treebag model\r\n # ensure reproducibility \r\n  set.seed(1234)\r\n # model control\r\n  Treebag_Control_phy <- rfeControl(functions = treebagFuncs, method= ""cv"",  number = 10, p = 0.75, verbose = FALSE)\r\n  set.seed(1234)\r\n  Treebag_i <- rfe(Train_i_up[,-1], Train_i_up[,1], \r\n    size = c(5, 10, 15),\r\n    rfeControl = Treebag_Control_i, \r\n    preProc=c(""center"", ""scale""), \r\n    metric = ""Accuracy"") \r\n  #check the best subset size and corresponding accuracy subset top 10 or top 15\r\n  Treebag_i \r\n  #validation using testing subset (obain model accuray and sensitity)\r\n  set.seed(1234)\r\n  Treebag_i_Valid <- predict(Treebag_i, newdata = Test_i)\r\n  #calculate validation performance\r\n  Valid_Treebag_i <- confusionMatrix(Treebag_i_Valid$pred, Test_i$class)\r\n  #overall accuracy\r\n  Overall_Accuracy_Treebag_i<- Valid_Treebag_i$overall\r\n  #accuracy by class\r\n  ClassAccuracy_Treebag_i <- Valid_Treebag_i$byClass\r\n  write.csv(ClassAccuracy_Treebag_i, ""ClassAccuracy_Treebag_valid.csv"") \r\n\r\n  #using top 15 compounds to build new model\r\n  library(dplyr)\r\n  #create name index based on variable importance \r\n  col_index <- varImp(Treebag_i)%>% mutate(names=row.names(.)) %>% arrange(-Overall)\r\n  # extract colnames of the top 15 variable\r\n  imp_names <- col_index$names[1:15]\r\n  # subset top15 dataset from raw dataset\r\n  Top15_Subset <- i_Data[,imp_names]\r\n  #set rownames of the Top15 subset \r\n  Top15_treebag_rfe <- cbind(class, Top15_Subset)\r\n  #save predictor subset\r\n  write.csv(Top15_treebag_rfe, ""Top15_treebag_rfe.csv"")\r\n  # run model again using top15 subset to valid the model accuracy, sensitivity and specificity.\r\n\r\n\r\n## embedded random forest using train()\r\n # ensure reproducibility \r\n  set.seed(1234)\r\n # model control\r\n  Rf_Control_phy <- trainControl(method= ""cv"",  number = 10, p = 0.75, returnResamp = ""final"", verbose = FALSE)\r\n  set.seed(1234)\r\n  Rf_i <- train(Train_i_up[,-1],  Train_i_up[,1], \r\n  ', '##########################################\r\n##### Co-variation in Scent Bouquets #####\r\n##########################################\r\n\r\nlibrary(vegan)\r\nlibrary(MASS)\r\nlibrary(MVA)\r\nlibrary(stats)\r\nlibrary(cluster)\r\nlibrary(ggpubr)\r\n\r\nlist.files()\r\n\r\ndata <-read.table(""Dataset_phenotypic_intergration.txt"", header = T) \r\nclasses <- read.table(""Dataset_phenotypic_intergration_classes.txt"", header = T)\r\n\r\n# normalize ----\r\nnormalize <- function(x) {\r\n  return((x-min(x))/(max(x)-min(x)))\r\n}\r\n\r\nbouquet <- as.data.frame(lapply(data[2:ncol(data)], normalize))\r\nDataLabels <- data[,1]\r\n\r\n# Phenotypic Integration  - all samples ----\r\n     \r\ncor.bouquet<-cor(bouquet, use=""pairwise.complete.obs"", method=""pearson"")\r\neig.bouquet<-eigen(cor.bouquet,symmetric=TRUE, only.values=FALSE, EISPACK=FALSE)\r\neig.var<-as.matrix(eig.bouquet $values)\r\nvar.bouquet<-var(eig.var)\r\n\r\n####integration under no selection ((number of traits measured -1)/ sample size)\r\n\r\nn.sub<-ncol(bouquet)\r\nn.sample<-nrow(bouquet)\r\n\r\ninte.bouquet<-(n.sub-1)/n.sample\r\n\r\n#####Standardized index\r\n\r\nPI.bouquet<-as.numeric(((var.bouquet-inte.bouquet)/n.sub)*100)\r\nPI.bouquet\r\n\r\n\r\n# Phenotypic Integration  - mycorrhiza ----\r\nlevels(DataLabels)\r\nbouquet_sub <- subset(bouquet, DataLabels == ""mycorrhiza"")\r\nbouquet_sub <- bouquet_sub[rowSums(bouquet_sub) >0, colSums(bouquet_sub) >0]\r\n\r\ncor.bouquet<-cor(bouquet_sub, use=""pairwise.complete.obs"", method=""pearson"")\r\neig.bouquet<-eigen(cor.bouquet,symmetric=TRUE, only.values=FALSE, EISPACK=FALSE)\r\neig.var<-as.matrix(eig.bouquet $values)\r\nvar.bouquet<-var(eig.var)\r\n\r\n####integration under no selection ((number of traits measured -1)/ sample size)\r\n\r\nn.sub<-ncol(bouquet_sub)\r\nn.sample<-nrow(bouquet_sub)\r\n\r\ninte.bouquet<-(n.sub-1)/n.sample\r\n\r\n#####Standardized index\r\n\r\nPI.bouquet<-as.numeric(((var.bouquet-inte.bouquet)/n.sub)*100)\r\nPI.bouquet\r\n\r\n\r\n# Phenotypic Integration  - mycoparasitism ----\r\nlevels(DataLabels)\r\nbouquet_sub <- subset(bouquet, DataLabels == ""mycoparasitism"")\r\nbouquet_sub <- bouquet_sub[rowSums(bouquet_sub) >0, colSums(bouquet_sub) >0]\r\n\r\ncor.bouquet<-cor(bouquet_sub, use=""pairwise.complete.obs"", method=""pearson"")\r\neig.bouquet<-eigen(cor.bouquet,symmetric=TRUE, only.values=FALSE, EISPACK=FALSE)\r\neig.var<-as.matrix(eig.bouquet $values)\r\nvar.bouquet<-var(eig.var)\r\n\r\n####integration under no selection ((number of traits measured -1)/ sample size)\r\n\r\nn.sub<-ncol(bouquet_sub)\r\nn.sample<-nrow(bouquet_sub)\r\n\r\ninte.bouquet<-(n.sub-1)/n.sample\r\n\r\n#####Standardized index\r\n\r\nPI.bouquet<-as.numeric(((var.bouquet-inte.bouquet)/n.sub)*100)\r\nPI.bouquet\r\n\r\n\r\n\r\n# Phenotypic Integration  - phytopathogen ----\r\nlevels(DataLabels)\r\nbouquet_sub <- subset(bouquet, DataLabels == ""phytopathogen"")\r\nbouquet_sub <- bouquet_sub[rowSums(bouquet_sub) >0, colSums(bouquet_sub) >0]\r\n\r\ncor.bouquet<-cor(bouquet_sub, use=""pairwise.complete.obs"", method=""pearson"")\r\neig.bouquet<-eigen(cor.bouquet,symmetric=TRUE, only.values=FALSE, EISPACK=FALSE)\r\neig.var<-as.matrix(eig.bouquet $values)\r\nvar.bouquet<-var(eig.var)\r\n\r\n####integration under no selection ((number of traits measured -1)/ sample size)\r\n\r\nn.sub<-ncol(bouquet_sub)\r\nn.sample<-nrow(bouquet_sub)\r\n\r\ninte.bouquet<-(n.sub-1)/n.sample\r\n\r\n#####Standardized index\r\n\r\nPI.bouquet<-as.numeric(((var.bouquet-inte.bouquet)/n.sub)*100)\r\nPI.bouquet\r\n\r\n\r\n# Phenotypic Integration  - saprophyte ----\r\nbouquet_sub <- subset(bouquet, DataLabels == ""saprophyte"")\r\nbouquet_sub <- bouquet_sub[rowSums(bouquet_sub) >0, colSums(bouquet_sub) >0]\r\n\r\ncor.bouquet<-cor(bouquet_sub, use=""pairwise.complete.obs"", method=""pearson"")\r\neig.bouquet<-eigen(cor.bouquet,symmetric=TRUE, only.values=FALSE, EISPACK=FALSE)\r\neig.var<-as.matrix(eig.bouquet $values)\r\nvar.bouquet<-var(eig.var)\r\n\r\n####integration under no selection ((number of traits measured -1)/ sample size)\r\n\r\nn.sub<-ncol(bouquet_sub)\r\nn.sample<-nrow(bouquet_sub)\r\n\r\ninte.bouquet<-(n.sub-1)/n.sample\r\n\r\n#####Standardized index\r\n\r\nPI.bouquet<-as.numeric(((var.bouquet-inte.bouquet)/n.sub)*100)\r\nPI.bouquet\r\n\r\n\r\n\r\n# PI for Functional group * compound class ----\r\n\r\nFGs <- levels(DataLabels)\r\ntable(DataLabels)\r\n\r\ntable(classes$Class)\r\n\r\nclass_include <- c(""Alkanes"",""Alkenes"",""Benzenoids"", ""Carbonyl_compounds"",  \r\n                   ""Carboxylic_acid_derivatives"", ""Carboxylic_acids"",\r\n                   ""Fatty_acid_esters"", ""Fatty_alcohol_esters"", ""Fatty_alcohols"",\r\n                   ""Monoterpenoids"", ""Nitrogenous"", ""Sesquiterpenoids"")\r\n\r\n\r\nbouquet_sub <- bouquet[, classes$Class %in% class_include]\r\nclass_sub <- droplevels(classes$Class[classes$Class %in% class_include])\r\ncla <- levels(class_sub)\r\n\r\n\r\ncombi <- expand.grid(FGs, cla)\r\n\r\nPI <- rep(NA, nrow(combi))\r\n\r\nfor(i in 1:nrow(combi)){\r\n  bouquet_temp <- bouquet_sub[DataLabels == combi[i,1], class_sub == combi[i,2]]\r\n  \r\n  bouquet_temp <- bouquet_temp[rowSums(bouquet_temp) >0, colSums(bouquet_temp) >0]\r\n  dim(bouquet_temp)\r\n  \r\n  if(is.data.frame(bouquet_temp) == FALSE) next\r\n  if(ncol(bouquet_temp) < 3) next\r\n  if(nrow(bouquet_temp)']",3,"chemo-diversity, fungal, volatile organic compounds, statistical analyses, R codes, fungal species, research article, volatilome data, trophic mode, lifestyle, Heatmap visualization, chemical diversity, chemotaxonomic, functional"
"Supplemental R code and csv files for statistical analysis on Doi et al. ""Effects of species traits and ecosystem  characteristics on species detection by eDNA metabarcoding  in lake fish communities""","Supplemental R code and csv files for statistical analysis on Doi et al. ""Effects of species traits and ecosystem characteristics on species detection by eDNA metabarcoding in lake fish communities""","['\n### 2022.2.14 \n### Doi et al. “Effects of species traits and ecosystem \n# characteristics on species detection by eDNA metabarcoding \n# in lake fish communities” \n\n#library loading -------\nlibrary(tidyverse) \nlibrary(lmerTest)\nlibrary(lme4)\nlibrary(lmer)\nlibrary(nlme)\nlibrary(lsmeans)\nlibrary(hier.part)\nlibrary(vegan)\nlibrary(iNEXT)\nlibrary(VennDiagram)\nlibrary(stringr)\nlibrary(gridExtra)\nlibrary(car)\nlibrary(ggpubr)\nlibrary(patchwork)\nlibrary(rfishbase)\n\n#Deta (CSV) import -------\neDNAdata<-read.csv(""eDNAlakes_data-re.csv"")\neDNAsite<-read.csv(""eDNAlakes_site.csv"")\nFishdata<-read.csv(""fish_historical_ori.csv"")\n\neDNAsitelakemo<-read.csv(""eDNAlakes_site_mor.csv"")\neDNAsite2<-merge(eDNAsite,eDNAsitelakemo,by=""lake"")\n\n\n#Dataframe subset -------\neDNAcomMor<-merge(eDNAsite2,eDNAdata, by=""code"")\neDNAcomMor<-subset(eDNAcomMor,eDNAcomMor$SampleType!=""NC"")\neDNAcomMor<-subset(eDNAcomMor,eDNAcomMor$SampleType!=""Blank"")\neDNAcom<-eDNAcomMor\n\neDNAcom2<-subset(eDNAcom,eDNAcom$SampleType==""Individual"")\neDNAcom1L<-subset(eDNAcom,eDNAcom$SampleType==""Individual"")\neDNAcom3<-subset(eDNAcom,eDNAcom$SampleType==""Mix_cold"")\neDNAcomMc<-subset(eDNAcom,eDNAcom$SampleType==""Mix_cold"")\neDNAcom4<-subset(eDNAcom,eDNAcom$SampleType==""Mix_freeze"")\neDNAcomMf<-subset(eDNAcom,eDNAcom$SampleType==""Mix_freeze"")\n\n\n#Fig. 2 ---------\nggplot(data=eDNAcom5,aes(col=SampleType, x=SampleType, y=Total))+\n  geom_boxplot()+\n  geom_violin(alpha=0.2)+\n  geom_jitter(width=0.2)+\n  ylab(""Number of fish taxa"")+\n  xlab(""Sampling method"")+\n  theme_bw()+\n  theme(legend.position = \'none\')\n\neDNAcom2<-eDNAcom %>% group_by(SampleType) %>% \n  summarize(.[20:ncol(eDNAcom)], sum = mean())\n\neDNAcomlong<-pivot_longer(eDNAcom, 20:137, names_to = ""spp"", values_to = ""reads"")\n\neDNAcomL2<-eDNAcomlong %>% group_by(lake,SampleType,spp) %>% \n  summarise(sum = sum(reads)) \n  \neDNAcomL3<- eDNAcom2 %>% group_by(lake) %>% \n    summarise(Total=sum((sum>1)))\n\neDNAcomL3$SampleType<-""Individual total""\neDNAcomL4<-merge(eDNAcomL3,eDNAcom,by=""lake"")\neDNAcom_ext<-data.frame(eDNAcom[,c(2,5,19)])\neDNAcomL5<-rbind(eDNAcomL3,eDNAcom_ext)\n\n\n\n#Nested ANOVA with LMM for Total spp.---------\n\n\nmodel = lme(Total ~ SampleType, random=~1|lake, \n            data=eDNAcom5, \n            method=""REML"")\n\nanova.lme(model, \n          type=""sequential"", \n          adjustSigma = FALSE)\n\nleastsquare<-lsmeans(model, \n                      pairwise ~ SampleType, \n                      adjust=""tukey"")  \nleastsquare\n\n\n\n#list comparison --------\n\nFishdataM<-Fishdata[,c(-2,-3,-4,-5)]\neDNAcom2M<-eDNAcom2[,c(-2,-3,-6,-7)]\nFishdataM<-gather(FishdataM,key = species, value=detect, -lake)\neDNAcom2M<-gather(eDNAcom2M,key = species, value=detect, -lake,-SampleType,-code)\n\nFishdataM<-subset(FishdataM,FishdataM$detect>3)\neDNAcom2M<-subset(eDNAcom2M,eDNAcom2M$detect>2)\n\nFisheDNA<-merge(FishdataM,eDNAcom2M, by=""lake"")\nwrite.csv(FisheDNA,file=""FisheDNAMC.csv"")\nFisheDNA1L<-read.csv(""FisheDNA_4.csv"")\nFisheDNAMc<-read.csv(""FisheDNAMc.csv"")\nFisheDNAMf<-read.csv(""FisheDNAMf.csv"")\n\ncounttotal4<-FisheDNA4 %>%group_by(lake) %>% dplyr::summarise(count=sum(match)) \ncounttotalMc<-FisheDNAMc %>%group_by(lake) %>% dplyr::summarise(count=sum(match)) \ncounttotalMf<-FisheDNAMf %>%group_by(lake) %>% dplyr::summarise(count=sum(match)) \n\ncounttotal4<-read.csv(""counttotal-lake.csv"")\n\n\n#Fig.2 ------------\n\naa1<-ggplot(counttotal4, aes(col=method, y=count/Total*100, x=method))+\n  geom_boxplot()+\n  geom_violin(alpha=0.2)+\n  geom_jitter(width=0.3)+\n  ylab(""Percentage of record covered"")+\n  xlab(""Sampling method"")+\n  theme_bw()+\n  theme(legend.position = \'none\')\naa2<-ggplot(data=eDNAcom5,aes(col=SampleType, x=SampleType, y=Total))+\n  geom_boxplot()+\n  geom_violin(alpha=0.2)+\n  geom_jitter(width=0.2)+\n  ylab(""Number of fish taxa"")+\n  xlab(""Sampling method"")+\n  theme_bw()+\n  theme(legend.position = \'none\')\n\naa2+aa1+\n  plot_annotation(\n    tag_levels = ""a"",\n    tag_suffix = "")""\n  )\n\n\nanov<-aov(count/Total~method:lake,counttotal4)\nsummary(anov)\n\n\nlibrary(nlme)\nlibrary(lsmeans)\n\ncounttotal4$count12<-counttotal4$count/counttotal4$Total12\ncounttotal4$count4<-counttotal4$count/counttotal4$Total\ncounttotal6<-counttotal4 %>% \n  tidyr::pivot_longer(c(-1,-2,-3,-4,-5,-6,-7), names_to = ""counttype"", values_to = ""per"")\n\n\nmodel = lme(count/Total12 ~ method, random=~1|lake, \n            data=counttotal4, \n            method=""REML"")\nmodel = lme(per ~ counttype+method, random=~1|lake, \n            data=counttotal6, \n            method=""REML"")\n\n\nanova.lme(model, \n          type=""sequential"", \n          adjustSigma = FALSE)\n\nleastsquare<-lsmeans(model, \n                     pairwise ~ method, \n                     adjust=""tukey"")  \nleastsquare\n\n\n#Fig.4----------\n\ncounttotal5<-merge(counttotal4,eDNAsitelakemo,by=""lake"")\ncounttotal5$MeanDepth<-as.numeric(counttotal5$MeanDepth)\ncounttotal5$Vol<-as.numeric(counttotal5$Vol)\n\n\ng1<-ggplot(counttotal5, aes(col=method, y=count/Total*100, x=Lat))+\n  geom_point()+\n  xlab(""Latitude"")+\n  stat_smooth(method=""lm"",fill = ""grey90"")+\n  theme_bw(']",3,"Supplemental, R code, csv files, statistical analysis, Doi et al., Effects, species traits, ecosystem characteristics, species detection, eDNA, metabarcoding, lake fish communities."
"Data and analysis for ""Fldgen v1.0: An Emulator with Internal Variability and Space-Time Correlation for Earth System Models""","This is an archive of the raw data and analysis source code for the paper ""Fldgen v1.0: An Emulator with Internal Variability and Space-Time Correlation for Earth System Models"". The archive contains:devel.Rmd : Source code for the worksheet that contains the early development and figures for the paper.devel.html : HTML rendering of devel.Rmdlg-ensemble-stats.Rmd : Source code for the worksheet that contains the statistical analysis described in the paper.lg-ensemble-stats.html : HTML rendering of lg-ensemble-stats.Rmdcc-analysis.Rmd : Analysis of the compromise conjecture raised by some readers of the papercc-analysis.nb.html : HTML rendering of cc-analysis.Rmddata.tar.bz2 : Input data for the analyses above.The source code in this archive is written in R and requires the R runtime environment. It also uses the fldgen package, version 1.0.0, which is available at https://github.com/JGCRI/fldgen","['## Function for calculating the power of a two-tailed F test for a hypothesized effect\n## size.\n\n## params:\n## n1 - Number of degrees of freedom for the test set\n## n2 - Number of degrees of freedom for the reference set\n## delta - Effect size.  The fractional difference between the population\n##         variances of the reference set and the test set.  So, if var(x2) =\n##         1.3 var(x1), delta would be 0.3\n## alpha - Intended p-value for the test\npower.f <- function(n1, n2, delta, alpha)\n{\n    falpha <- qf(c(alpha/2.0, 1-alpha/2.0), n1, n2)\n\n    pr <- sapply(c(1+delta, 1-delta),\n                 function(k) {\n                    fk <- falpha / k\n                    pf(fk[1], n1, n2) + pf(fk[2], n1, n2, lower.tail=FALSE)\n                 })\n    min(pr)\n}\n', '## Determine the parameters for the beta function to use as a comparison\n## hypothesis in determining the power of the Shapiro-Wilk test for normality.\n## For each candidate value of n, compute the fraction of positive results in a\n## suite of F-tests, at a specified p-value.  The two comparison distributions\n## have the same variance (by construction), so there should be roughly that\n## fraction of failures in the tests.  Substantially lower or higher failure\n## rates indicate that the departure from normality is big enough to skew an\n## F-test, and that is therefore a proxy for the threshold for a ""de minimis""\n## departure.\nswcmpfn <- function(nsampobs, nsampmod, pval) {\n    ## nsampobs: number of observational samples in each trial.  This should be similar to the\n    ##        actual sample sizes.\n    ## nsampmod: number of model samples in each trial\n    ## pval: intended p-value to use for the f-test\n\n    ntrial <- 2500          # number of trials for each candidate n value\n    nvals <- seq(1:20)      # n values to try\n    trialvec <- seq(1,ntrial)\n\n    ## Sample a beta function with a specified n (= n1 = n2)\n    sampbeta <- function(n) {\n        rbeta(nsampmod, n, n)\n    }\n\n    ## Sample a normal distribution centered at 0.5 with the same variance as a beta distribution\n    ## with the specified n\n    sampnorm <- function(n) {\n        s <- sqrt(1/(4*(1+2*n)))\n        rnorm(nsampobs, mean=0.5, sd=s)\n    }\n\n    falpha <- c(pval/2, 1-pval/2)\n    fc <- qf(falpha, nsampmod, nsampobs) # assumes F := var(model)/var(obs)\n\n    ## Function to run the trials for each n\n    runtrials <- function(n) {\n        ## Function to run a single trial.  This will close over n\n        run1trial <- function(index) {\n            ## index is not used\n            vbeta <- sampbeta(n)\n            vnorm <- sampnorm(n)\n\n            f <- var(vbeta)/var(vnorm)\n\n            f < fc[1] | f > fc[2]       # return a single true-false value\n        }\n\n        ## return the fraction of positive (i.e., true) values\n        sum(sapply(trialvec, run1trial)) / length(trialvec)\n    }\n\n    ## Run that function over all candidate n-values\n    ffail <- sapply(nvals, runtrials)\n\n    data.frame(n=nvals, frac.fail=ffail)\n}\n\n\n## Based on the tests above, even the 5,5 Beta distribution doesn\'t distort the\n## F-test unduly, so we will use that as our alternative distribution.\npwr.sw <- function(nsampmod, pval, ntrial=1000)\n{\n    run1trial <- function(index) {\n        ## index is not used\n\n        ## generate a beta distribution data set.\n        d <- rbeta(nsampmod, 5, 5)\n        ## return true/false based on the p-value\n        swpval(d) < pval\n    }\n\n    sum(sapply(seq(1:ntrial), run1trial)) / ntrial\n}\n\n\n## Helper function to run a Shapiro-Wilk test and get the p-value from the\n## result structure, discarding the other stuff.\nswpval <- function(x)\n{\n    tst <- shapiro.test(x)\n    tst$p.value\n}\n']",3,"data, analysis, Fldgen v1.0, emulator, internal variability, space-time correlation, earth system models, archive, raw data, source code, paper, worksheet, development, figures, statistical analysis, compromise conjecture, input data"
"Data from: Statistical analysis of dental variation in the Oligocene equid Miohippus (Mammalia, Perissodactyla) of Oregon","As many as eight species of the ""anchitherine"" equid Miohippus have been identified from the John Day Formation of Oregon, but no statistical analysis of variation in these horses has yet been conducted to determine if that level of diversity is warranted. Variation of the anterior-posterior length and transverse width of upper and lower teeth of Turtle Cove Member Miohippus was compared to that of M. equinanus, Mesohippus bairdii, Equus quagga, and Tapirus terrestris using t tests of their coefficients of variation (V). None of the t tests are significant, indicating that the variation seen in Turtle Cove Miohippus is not significantly different from any of the populations of other perissodactyls examined in this study. Data also indicate that Mesohippus is present in the Turtle Cove Member. Additionally, hypostyle condition, used to diagnose all species of Miohippus, was found to be related to stage of wear using an ordered logistic regression. Only two species of equid, one Miohippus and one Mesohippus, in the Turtle Cove Member can be identified, therefore only Miohippus annectens, the genotype and first species described from the region, can be recognized as the sole Miohippus species known from the Turtle Cove assemblage. There are insufficient data to determine which species of Mesohippus is present. The dependence of hypostyle condition on crown height in Miohippus implies that wear stage must also be considered in investigations of dental morphology in the ""Anchitheriinae.""","['#This is code written by Nick Famoso to calculate an ordinal logit regression on data collected for Miohippus wear stages and Hypostyle condition\r\n#Date:December 2014\r\n\r\n#clear the memory\r\nrm(list=ls())\r\n\r\n#load the library\r\nlibrary(ordinal)\r\nlibrary(MASS)\r\n\r\ndata<-read.csv(""Miohippus_Hypostyle.csv"")\r\n\r\nHypostyle<-data$Hypostyle_condition\r\nHypostyle1<-data$Hypostyle_conditionA\r\nWear<-data$HI\r\n\r\n#perform ordinal logistic regression\r\nz<-polr(Hypostyle1~Wear, data = data, Hess=TRUE, method=""logistic"")\r\nsummary(z)\r\n\r\n#present results with no p-value\r\n(ctable <- coef(summary(z)))\r\n#Get p-values\r\np<-pnorm(abs(ctable[,""t value""]), lower.tail=FALSE)*2\r\n(ctable<-cbind(ctable, ""pvalue"" = p))\r\n\r\n\r\n#run is without the M3\r\nnoM3<-data[ which(data$tooth!=\'M3\'), ]\r\n\r\nM3Hypostyle<-noM3$Hypostyle_condition\r\nM3Hypostyle1<-noM3$Hypostyle_conditionA\r\nM3Wear<-noM3$HI\r\n\r\n#perform ordinal logistic regression\r\nM3z<-polr(M3Hypostyle1~M3Wear, data = data, Hess=TRUE, method=""logistic"")\r\nsummary(M3z)\r\n\r\n#present results with no p-value\r\n(M3ctable <- coef(summary(M3z)))\r\n#Get p-values\r\nM3p<-pnorm(abs(M3ctable[,""t value""]), lower.tail=FALSE)*2\r\n(M3ctable<-cbind(M3ctable, ""pvalue"" = M3p))']",3,"dental variation, Oligocene equid, Miohippus, statistical analysis, Oregon, species diversity, upper teeth, lower teeth, perissodactyls, coefficients of variation, hypostyle condition, wear stage, Turtle Cove"
Reduced seed set under water deficit is driven mainly by reduced flower numbers and not by changes in flower visitations and pollination,"Water deficit can alter floral traits with cascading effects on flower-visitor interactions and plant fitness. Water stress induction can diminish productivity, directly resulting in lower flower production and consequently seed set. Changes in floral traits, such as floral scent or reward amount, may in turn alter pollinator visitations and behavior and consequently can reduce pollination services resulting in lower reproduction output. However, the relative contribution of this indirect in comparison to the direct effects of changes in seed set are not fully understood.We manipulated water availability using rain-out shelters in a field experiment and measured effects on floral scent bouquet, morphology, phenology, flower-visitor interactions, pollination, and seed set. Plant individuals of Sinapis arvensis (Brassicaceae) were randomly assigned to one of three treatments: mean precipitation (= control), reduced mean precipitation, or drought period treatment.Our results show that decreasing water availability lowers the number of flowers and seed set. This indicates a direct link between water stress and seed set, as seed mass increases with increasing flower number. The indirect link of water stress via floral traits, pollinator visits, and pollination has weaker effects on seed set. However, floral traits remain relatively stable under decreased water availability, whereas plant growth and flower abundance decrease, potentially in order to allow investment in more resources in fewer flowers to maintain pollination success. Thus, plants are able to compensate for water stress and can maintain floral trait expression, such as a stable scent emission and bouquet, to retain pollinator attraction.These findings indicate that the direct link from water stress to seed set has a stronger impact on plants' reproductive success than the indirect link through altered floral trait expression and pollinator visits in a generalist plant species.","['######Libraries######\r\n#Basic\r\nlibrary(data.table) #function setcolorder()\r\nlibrary(dplyr)\r\nlibrary(tidyr) ##data exploration and tidy up\r\nlibrary(plyr) ##Tools for Splitting, Applying and Combining Data\r\nlibrary(utils)\r\n\r\n#Graphics\r\nlibrary(ggfortify)\r\nlibrary(ggplot2)\r\nlibrary(ggpubr) ## for scatter plots/ regressions\r\nlibrary(RColorBrewer)\r\nlibrary(semPlot) #plot piecewiseSEM \r\nlibrary(rockchalk)\r\nlibrary(PupillometryR) # violine plots\r\n\r\n#PCA\r\nlibrary(ade4) ## function dudi.pca()\r\nlibrary(factoextra) ## PCA\r\n\r\n#Statistic\r\nlibrary(car)\r\nlibrary(corrplot)\r\nlibrary(DHARMa)\r\nlibrary(emmeans)\r\nlibrary(glmmTMB)\r\nlibrary(lme4) ##GLMMS, LMMS\r\nlibrary(mosaic) ## for function favstats()\r\nlibrary(piecewiseSEM) #Structural Equation Modelling\r\nlibrary(Matrix)\r\nlibrary(multcomp) ## Tukey-test\r\nlibrary(nlme)\r\nlibrary(report)\r\nlibrary(vegan) ##NMDS\r\n\r\n\r\n#Create theme for graphs-----------------------------------------------\r\ntheme_MS <- function(){\r\n  theme_bw() +\r\n    theme(axis.text = element_text(size = 16), \r\n          axis.title = element_text(size = 18, face = ""bold""),\r\n          axis.line.x = element_line(color=""black""), \r\n          axis.line.y = element_line(color=""black""),\r\n          panel.border = element_blank(),\r\n          panel.grid.major.x = element_blank(),                                          \r\n          panel.grid.minor.x = element_blank(),\r\n          panel.grid.minor.y = element_blank(),\r\n          panel.grid.major.y = element_blank(),  \r\n          plot.margin = unit(c(1, 1, 1, 1), \r\n                             units = , ""cm""),\r\n          plot.title = element_text(size = 18, \r\n                                    vjust = 1, \r\n                                    hjust = 0),\r\n          legend.text = element_text(size = 12),          \r\n          legend.title = element_blank(),                              \r\n          legend.position = ""NONE"")\r\n}\r\n\r\n#********************************************************************************\r\n#*********************************************************************************\r\n\r\n# Set working directory and load data --------------------------------------------------------- \r\n\r\nsetwd("""")\r\n\r\n\r\n###Import all data files----------------------------------------------------------------------------------------------------\r\n\r\nplantlist <- read.csv(""Plantlist.csv"", sep = "";"") %>% # file with plant individual information (motherID, treatment...)\r\n             mutate(treatment = factor(treatment,\r\n                            levels = c(""Control"", ""PrecipRed"", ""Drought""))) \r\n\r\n\r\nloggers <- read.csv(""Loggerdata.csv"", header=TRUE) # climate data logger files\r\n\r\n\r\nsoil <- read.csv(""Soil_humi_temp.csv"", header=TRUE, sep = "";"") %>% # Soil conditions data file\r\n        inner_join(plantlist) %>%\r\n        group_by(plantID) %>%\r\n        mutate(meantemperature = mean(temperature, na.rm=TRUE)) %>%\r\n        mutate(meanhumidity = mean(humidity, na.rm=TRUE)) %>%\r\n        distinct(plantID,.keep_all = TRUE)\r\n\r\n\r\n# Floral trait files\r\nFloral_trait_data <- read.csv(""Floral_traits.csv"", header=TRUE, sep="";"")\r\n\r\nscent_wide<- read.csv(""Scent_data.csv"", header=TRUE, sep = "","")\r\n\r\nFloral_trait_data_complete <- Floral_trait_data %>%\r\n                              left_join(scent_wide %>% dplyr::select(plantID, totalscent), \r\n                                        by=""plantID"") %>%\r\n                              mutate(treatment = factor(treatment,\r\n                                                        levels = c(""Control"", ""PrecipRed"", ""Drought"")))\r\n\r\n#Load floral visitor data\r\nInteraction_data <- read.csv(""Int_data.csv"", header=TRUE, sep="";"")\r\n\r\n  #Create wide format data table\r\n      Interaction_data_wide <- Interaction_data %>%\r\n                               dplyr::select(plantID, visitorgroup, NrInt_day_flower, treatment, shelter, motherID)%>%\r\n                               group_by(visitorgroup) %>%\r\n                               spread(visitorgroup, NrInt_day_flower)\r\n\r\n      Interaction_data_wide[, 5:18][is.na(Interaction_data_wide[, 5:18])] <- 0\r\n      Interaction_data_wide$totalint <- rowSums(Interaction_data_wide[,5:18]) # totalint = sum of all visits per day per flower for each plant \r\n\r\n\r\n      \r\n#Summary statistic of interactions-------------------------------------------------------------------------------------------------------    \r\n  Interaction_data_stat <- Interaction_data %>%\r\n        group_by(plantID) %>%\r\n        distinct(plantID,.keep_all = TRUE)\r\n      \r\n      ##Mean days plants were observed\r\n      mean(Interaction_data_stat$sumday) # mean number of days plants were observed 12.45\r\n      sd(Interaction_data_stat$sumday) # standard deviation of number of days plants were observed = 3.65\r\n      \r\n      ##Mean observation time \r\n      mean(Interaction_data_stat$observetime) # mean observation time in min = 191.89\r\n      sd(Interaction_data_stat$observetime) # standard deviation of observation time in min = 44.98\r\n      \r\n      \r\n#Statistic stem water potential treatments---']",3,"Water deficit, flower numbers, pollination, seed set, floral traits, flower-visitor interactions, plant fitness, productivity, floral scent, reward amount, pollinator behavior, reproduction output, rain-out shelters, Sinapis arvensis, Brassic"
Data from: Multi-allelic exact tests for Hardy-Weinberg equilibrium that account for gender,"Statistical tests for Hardy-Weinberg equilibrium are important elementary tools in genetic data analysis. X-chromosomal variants have long been tested by applying autosomal test procedures to females only, and gender is usually not considered when testing autosomal variants for equilibrium. Recently, we proposed specific X-chromosomal exact test procedures for bi-allelic variants that include the hemizygous males, as well as autosomal tests that consider gender. In this paper we present the extension of the previous work for variants with multiple alleles. A full enumeration algorithm is used for the exact calculations of triallelic variants. For variants with many alternate alleles we use a permutation test. Some empirical examples with data from the 1000 genomes project are discussed.","['#\r\n# R-code reconstructing tables of the paper ""Multi-allelic exact tests for Hardy-Weinberg equilibrium that account for gender""\r\n#\r\n# Molecular Ecology Resources: http://dx.doi.org/10.1111/1755-0998.12748\r\n#\r\n\r\n#install.packages(HardyWeinberg)\r\nlibrary(HardyWeinberg)\r\n\r\n#\r\n# Reconstructing Table 5 in Graffelman & Weir (2017) \r\n#\r\n\r\ndata(""JPTtriallelicsChrX"")\r\nJPTtriallelicsChrX\r\n\r\nMales   <- as.matrix(JPTtriallelicsChrX[,c(""A"",""B"",""C"")],ncol=3)\r\nFemales <- as.matrix(JPTtriallelicsChrX[,c(""AA"",""AB"",""AC"",""BB"",""BC"",""CC"")],ncol=6)\r\n\r\n# Exact test HWP & EAF\r\n\r\nout1.exa <- HWTriExact(Females[1,],Males[1,])\r\nout2.exa <- HWTriExact(Females[2,],Males[2,])\r\nout3.exa <- HWTriExact(Females[3,],Males[3,])\r\nout4.exa <- HWTriExact(Females[4,],Males[4,])\r\nout5.exa <- HWTriExact(Females[5,],Males[5,])\r\nhwpandeafpval <- c(out1.exa$pval,\r\n                   out2.exa$pval,\r\n                   out3.exa$pval,\r\n                   out4.exa$pval,\r\n                   out5.exa$pval)\r\n\r\n# Permutation tests for HWP & EAF\r\n\r\nset.seed(123)\r\nout1.perm <- HWPerm.mult(Males[1,],Females[1,])\r\nout2.perm <- HWPerm.mult(Males[2,],Females[2,])\r\nout3.perm <- HWPerm.mult(Males[3,],Females[3,])\r\nout4.perm <- HWPerm.mult(Males[4,],Females[4,])\r\n#out5.perm <- HWPerm.mult(Males[5,],Females[5,]) # time consuming\r\npermpval <- c(out1.perm$pval,\r\n              out2.perm$pval,\r\n              out3.perm$pval,\r\n              out4.perm$pval,\r\n              NA)\r\n\r\n# Exact test using females only\r\n\r\nout1.exa.fem <- HWTriExact(Females[1,])\r\nout2.exa.fem <- HWTriExact(Females[2,])\r\nout3.exa.fem <- HWTriExact(Females[3,])\r\nout4.exa.fem <- HWTriExact(Females[4,])\r\nout5.exa.fem <- HWTriExact(Females[5,])\r\neaffempval <- c(out1.exa.fem$pval,\r\n             out2.exa.fem$pval,\r\n             out3.exa.fem$pval,\r\n             out4.exa.fem$pval,\r\n             out5.exa.fem$pval)\r\n\r\n# Exact test for EAF only\r\n\r\nout1.eaf <- EAFExact(Males[1,],toTriangular(Females[1,]))\r\nout2.eaf <- EAFExact(Males[2,],toTriangular(Females[2,]))\r\nout3.eaf <- EAFExact(Males[3,],toTriangular(Females[3,]))\r\nout4.eaf <- EAFExact(Males[4,],toTriangular(Females[4,]))\r\nout5.eaf <- EAFExact(Males[5,],toTriangular(Females[5,]))\r\neafpval <- c(out1.eaf$pval,\r\n             out2.eaf$pval,\r\n             out3.eaf$pval,\r\n             out4.eaf$pval,\r\n             out5.eaf$pval)\r\nTable5 <- round(cbind(hwpandeafpval,permpval,eaffempval,eafpval),digits=4)\r\nTable5\r\n\r\n#\r\n# Reconstructing Table 7 in Graffelman & Weir (2017) \r\n#\r\n\r\ndata(""JPTmultiallelicsChrX"")\r\nattach(JPTmultiallelicsChrX)\r\n\r\nout.hwp_eaf.4 <- HWPerm.mult(m4,f4)\r\nout.hwp_fem.4 <- HWPerm.mult(f4)\r\nout.eaf.4 <- EAFExact(m4,f4)\r\nresults.4 <- c(out.hwp_eaf.4$pval,out.hwp_fem.4$pval,out.eaf.4$pval)\r\n\r\nout.hwp_eaf.5 <- HWPerm.mult(m5,f5)\r\nout.hwp_fem.5 <- HWPerm.mult(f5)\r\nout.eaf.5 <- EAFExact(m5,f5)\r\nresults.5 <- c(out.hwp_eaf.5$pval,out.hwp_fem.5$pval,out.eaf.5$pval)\r\n\r\nout.hwp_eaf.6 <- HWPerm.mult(m6,f6)\r\nout.hwp_fem.6 <- HWPerm.mult(f6)\r\nout.eaf.6 <- EAFExact(m6,f6)\r\nresults.6 <- c(out.hwp_eaf.6$pval,out.hwp_fem.6$pval,out.eaf.6$pval)\r\n\r\nout.hwp_eaf.7 <- HWPerm.mult(m7,f7)\r\nout.hwp_fem.7 <- HWPerm.mult(f7)\r\nout.eaf.7 <- EAFExact(m7,f7)\r\nresults.7 <- c(out.hwp_eaf.7$pval,out.hwp_fem.7$pval,out.eaf.7$pval)\r\n\r\nTable7 <- round(rbind(results.4,results.5,results.6,results.7),digits=4)\r\nTable7\r\n\r\ndetach(JPTmultiallelicsChrX)\r\n\r\n#\r\n# Reconstructing Table 8 in Graffelman & Weir (2017) \r\n#\r\n\r\ndata(""JPTtriallelicsChr7"")\r\n\r\nMales <- JPTtriallelicsChr7[,3:8]\r\ncolnames(Males) <- substr(colnames(Males),2,3)\r\n\r\nFemales <- JPTtriallelicsChr7[,9:14]\r\ncolnames(Females) <- substr(colnames(Females),2,3)\r\n\r\n# test HWP & EAF\r\n\r\nout1.hwe_eaf <- HWTriExact(Males[1,],Females[1,])\r\nout2.hwe_eaf <- HWTriExact(Males[2,],Females[2,])\r\nout3.hwe_eaf <- HWTriExact(Males[3,],Females[3,])\r\nout4.hwe_eaf <- HWTriExact(Males[4,],Females[4,])\r\nout5.hwe_eaf <- HWTriExact(Males[5,],Females[5,])\r\nout6.hwe_eaf <- HWTriExact(Males[6,],Females[6,])\r\nResults.hwe_eaf <- c(out1.hwe_eaf$pval,out2.hwe_eaf$pval,out3.hwe_eaf$pval,\r\n                     out4.hwe_eaf$pval,out5.hwe_eaf$pval,out6.hwe_eaf$pval)\r\n\r\n# test HWP in the total sample\r\n\r\nTotal <- Males+Females\r\n\r\nout1.t <- HWTriExact(Total[1,])\r\nout2.t <- HWTriExact(Total[2,])\r\nout3.t <- HWTriExact(Total[3,])\r\nout4.t <- HWTriExact(Total[4,])\r\nout5.t <- HWTriExact(Total[5,])\r\nout6.t <- HWTriExact(Total[6,])\r\nResults.hwp.t <- c(out1.t$pval,out2.t$pval,out3.t$pval,\r\n                   out4.t$pval,out5.t$pval,out6.t$pval)\r\n\r\n# test HWP in females only\r\n\r\nout1.f <- HWTriExact(Females[1,])\r\nout2.f <- HWTriExact(Females[2,])\r\nout3.f <- HWTriExact(Females[3,])\r\nout4.f <- HWTriExact(Females[4,])\r\nout5.f <- HWTriExact(Females[5,])\r\nout6.f <- HWTriExact(Females[6,])\r\nResults.hwp.f <- c(out1.f$pval,out2.f$pval,out3.f$pval,\r\n                   out4.f$pval,out5.f$pval,out6.f$pval)\r\n\r\n\r\n# test HWP in males only\r\n\r\nout1.m <- HWTriExact(Males[1,])\r\nout2.m <- HWTriExact(Males[2,])\r\nout3.m <- HWTriExact(Males[3,])\r\nout4.m <- HWTriExact(Males[4,])\r\nout5.m <- HWTriExact(M']",3,"Hardy-Weinberg equilibrium, genetic data analysis, statistical tests, gender, X-chromosomal variants, autosomal variants, bi-allelic variants, hemizygous males, multiple alleles, full enumeration algorithm, permutation test, "
Future number of people with diagnosed type 1 diabetes in Germany until 2040,Data and source file for estimation of the incidence of type 1 diabetes (T1D) and projection of the prevalence and number of people living with T1D in future Germany. Prevalence input is based on a huge German claims data set based on aggregated data. The additional data files (text files) provide population projections published by the German Federal Statistical Office (see https://www.destatis.de/EN/Themes/Society-Environment/Population/Population-Projection/_node.html;jsessionid=6642087EED703DD77DFB9CB076205246.live712).All files are text files. The file with the extension .R is the source file for use with the open statistical software R (The R Foundation of Scientific Computing).,"['# clear environment \r\nrm(list=ls(all=TRUE))\r\n# set working directory accordingly \r\n#setwd() \r\n\r\n################################################################################\r\n##\r\n##      Estimating the incidence of type 1 diabetes (T1D) in Germany and\r\n##\r\n##     projecting the prevalence and number of people with T1D until 2040\r\n##\r\n##                from aggregated claims data covering 85% of the \r\n##\r\n##                       overall German population 2009-10\r\n##\r\n################################################################################\r\n\r\n# load required packages\r\nlibrary(deSolve)\r\nlibrary(stargazer)\r\nlibrary(ggplot2)\r\n\r\n################################################################################\r\n#\r\n##  Incidence estimation  \r\n#\r\n################################################################################\r\n\r\n##------------------------------------------------------------------------------\r\n##\r\n## Load age- and sex-specific mortality rate functions \r\n##\r\n##------------------------------------------------------------------------------\r\n\r\n# mortality of the general population based on FSO data\r\n# (Sterbetabelle des Statistischen Bundesamts 1970-2010)\r\n# -> m_\r\ngetMort <- function(t, a, isMale)\r\n{\r\n  if(isMale){\r\n    decr <- 0.982 # annual decrement, geometric mean over last 40 years\r\n    alpha <- -10.36\r\n    beta <- 9.652e-02\r\n  }\r\n  else{\r\n    decr <- 0.981\r\n    alpha <- -11.47\r\n    beta <- 1.056e-01\r\n  }\r\n  return(exp((t-7)*log(decr) + (alpha + beta*a)))\r\n}\r\n# mortality of people with diagnosed diabetes compared to healthy population\r\n# (based on Carstensen, May 2019, Fig 7.8 & 7.9)\r\n# -> R_\r\nget_MRR_DK <- function(a, isMale, fac = 1.0)\r\n{\r\n  if(isMale){\r\n    x_age <- c(10, 15, 25, 35, 45, 55, 65, 76, 80)\r\n    y_smr <- c(3.648,3.815,4.15,4.55,4.95,4.85,4.175,2.444,2.319)\r\n    # y_smr = SMR of male T1D relative to 2015-01-01\r\n  }\r\n  else{\r\n    x_age <- c(10, 15, 25, 35, 45, 55, 65, 76, 80)\r\n    y_smr <- c(7.042,6.708,6.214,5.697,5.182,4.525,3.722,2.653,2.042)\r\n    # y_smr = SMR of female T1D relative to 2015-01-01\r\n  }\r\n  #initiate/ smoothing spline\r\n  smr <- smooth.spline(x_age, y_smr)\r\n  p_smr <- predict(smr, a)$y\r\n  return(fac*p_smr)\r\n}\r\n    \r\n##------------------------------------------------------------------------------\r\n##\r\n## Load and prepare input data for incidence estimation\r\n##\r\n##------------------------------------------------------------------------------\r\n\r\n# read age-specific prevalence data from 2009 and 2010 for men and women \r\nages <- c(0.5, 3, 7.5+5*0:18, 100)\r\nprev1.m <- c(NA, 0.000530514, 0.001362272, 0.002195808, 0.002853293, \r\n             0.002960097, 0.003091387, 0.003514712, 0.004042108, 0.004056383, \r\n             0.004071315, 0.003957671, 0.003953044, 0.004167662, 0.004101669, \r\n             0.004379181, 0.004512438, 0.004670023, 0.004715352, 0.005022321, \r\n             0.004271869, NA)\r\nprev2.m <- c(NA, 0.0005430915, 0.0014138410, 0.0023054613, 0.0029331250, \r\n             0.0030923048, 0.0031264062, 0.0036067355, 0.0039317276, \r\n             0.0041253953, 0.0041178487, 0.0039103910, 0.0038365692, \r\n             0.0039234103, 0.0038337967, 0.0039545987, 0.0042733887, \r\n             0.0044278245, 0.0044102578, 0.0044573882, 0.0040218132, NA)\r\nprev1.f <- c(NA, 0.0004776393, 0.0013507536, 0.0021989486, 0.0023718039, \r\n             0.0022482700, 0.0024763126, 0.0027615586, 0.0026959898, \r\n             0.0027252619, 0.0025334451, 0.0025143993, 0.0026057772, \r\n             0.0029261234, 0.0031045979, 0.0036698890, 0.0042646550, \r\n             0.0047036318, 0.0051996053, 0.0055755204, 0.0050969430, \r\n             0.0048398723)\r\nprev2.f <- c(NA, 0.0004679733, 0.0013986678, 0.0022943281, 0.0024446233, \r\n             0.0023245007, 0.0025394190, 0.0027934020, 0.0027682406, \r\n             0.0027130633, 0.0025529751, 0.0024890763, 0.0025437367, \r\n             0.0027875775, 0.0029933896, 0.0033679294, 0.0038750234, \r\n             0.0042147696, 0.0044657944, 0.0045918258, 0.0043793162, \r\n             0.0041506078)\r\n\r\n# prepare constants for later analysis\r\nnYears <- 2\r\nnSexes <- 2\r\nnAgeGroups <- 22\r\nnDiagGroups <- 6\r\nages <- c(0.5, 3, 7.5+5*0:18, 100) # specify mid of each age group interval\r\naidx <- 2:21\r\nmya <- ages[aidx]\r\n\r\n# T1D prevalence for total population for men/women in 2009/2010\r\np1.m <- 348.65\r\np1.f <- 278.66\r\np2.m <- 343.88\r\np2.f <- 271.36\r\n\r\n##------------------------------------------------------------------------------\r\n##\r\n## Incidence estimation \r\n##\r\n##------------------------------------------------------------------------------\r\n\r\n# we consider a partial differential equation (PDE) that relates prevalence, \r\n# incidence and mortality based on the illness-death-model\r\n\r\n# age- and sex-specific T1D incidence in 2009/10 -------------------------------\r\ncalcInc <- function(a, a_t, p_t1, p_t2, isMale, Rfac = 1.0){\r\n  sp1   <- smooth.spline(a_t, p_t1, df = 4)\r\n  sp2   <- smooth.spline(a_t, p_t2, df = 4)\r\n  \r\n  dp_   <- (predict(sp2, a+0.5)$y - predict(sp1,']",3,"Type 1 diabetes, Germany, incidence, prevalence, projection, population, statistics, data set, claims data, Federal Statistical Office, R software, estimation."
Data for: Open COVID Trials (OCT) Project,"The COVID-19 pandemic has brought substantial attention to the systems used to communicate biomedical research. In particular, the need to rapidly and credibly communicate research findings has led many stakeholders to encourage researchers to adopt open science practices such as posting preprints and sharing data. To examine the degree to which this has led to the actual adoption of such practices, we examined the ""openness"" of a sample of 539 published papers describing the results of randomized controlled trials testing interventions to prevent or treat COVID-19. The majority (56%) of the papers in this sample were free to read at the time of our investigation and 23.56% were preceded by preprints. However, there is no guarantee that the papers without an open license will be available without a subscription in the future, and only 49.61% of the preprints we identified were linked to the subsequent peer-reviewed version. Of the 331 papers in our sample with statements identifying if (and how) related datasets were available, only a paucity indicated that data was available in a repository that facilitates rapid verification and reuse. Our results demonstrate that, while progress has been made, there is still a significant mismatch between aspiration and actual practice in the adoption of open science in an important area of the COVID-19 literature.","['## Covid clinical trial meta-analysis\n## cyp v-2022\n\nlibrary(""ggplot2"")\nlibrary(""waffle"")\nlibrary(""dplyr"")\n\n# XXX set working directory\nwd <- ""~/Desktop/oct_dataset""\nsetwd(wd)\n# load data\ndata <- read.csv(""./oct-dataset_for-manuscript_2022-08-03.csv"",header=T)\n\n### Paper t-tests\n## ICMJE member journals vs not for citations\n# t = -5.3182, df = 104.35, p-value = 6.007e-07 (mean no: 41.98152, mean yes: 617.79048 ) - S\nt.test(as.numeric(citations_dimensions)~icmje_member,data=data)\n## ICMJE member journals vs not for altmetrics\n# t = -6.3867, df = 106.79, p-value = 4.51e-09 (mean no: 306.1039, mean yes: 3419.8491 ) - S\nt.test(as.numeric(altmetric_score)~icmje_member,data=data)\n## ICMJE recommendations (i.e. follow?) vs not for citations\n# t = -4.7315, df = 275.35, p-value = 3.567e-06 (mean no: 45.38148, mean yes: 264.15299) - S\nt.test(as.numeric(citations_dimensions)~icmje_follow,data=data)\n## ICMJE recommendations (i.e. follow?) vs not for altmetrics\n# t = -5.37, df = 306.75, p-value = 1.558e-07 (mean no: 317.3444, mean yes: 1521.7993) - S\nt.test(as.numeric(altmetric_score)~icmje_follow,data=data)\n## Preprints found vs not for citations\n# t = -0.55581, df = 198.6, p-value = 0.579 (mean no: 146.8297, mean yes: 178.7323) - NS\nt.test(as.numeric(citations_dimensions)~preprint_found,data=data)\n## Preprints found vs not for altmetrics\n# t = -0.41217, df = 345.58, p-value = 0.6805 (mean no: 897.6156, mean yes: 985.3672) - NS\nt.test(as.numeric(altmetric_score)~preprint_found,data=data)\n\n\n## Paper figures\ndata_avail_request <- paste(data$data_avail_statement,data$data_avail_request_author,sep=""_"")\n# A) to photoshop legend\n# availability statement? no, yes --> available by request? no, yes\nwaffle(table(data_avail_request),colors=c(""#D3D3D3"",""#708090"",""#DA614E""),xlab=""1 square = 1 publication"",rows=20,size=0.5)\n\n# B)\ndata_avail_repo <- paste(data$data_avail_statement,data$data_avail_repo,sep=""_"")\n# availability statement? no, yes --> available by repo? no, yes\nwaffle(table(data_avail_repo),colors=c(""#D3D3D3"",""#708090"",""#4EC7DA""),xlab=""1 square = 1 publication"",rows=20,size=0.5)\n\n']",3,"open COVID trials, COVID-19 pandemic, biomedical research, open science, preprints, data sharing, randomized controlled trials, interventions, open license, subscription, peer-reviewed version, datasets, repository, verification, reuse, aspiration, actual practice."
MCC-Analysis and Data,This archive holds data (tpm and counts) to implement and produce plots and analysis for the paper xyz,"['# Read the raw counts\nlibrary(DESeq2);library(ggplot2);library(reshape2);library(RColorBrewer)\nlibrary(EnsDb.Hsapiens.v86);library(ggrepel);library(fgsea);library(tidyverse);library(corrplot);\nlibrary(Hmisc);\n\ngtf = rtracklayer::import(\'Data/Homo_sapiens.GRCh38.103.gtf\')\ngtf_df = as.data.frame(gtf)\nheat_colors = brewer.pal(6, ""YlOrRd"")\n\n###############################\n### WAGA ####\n###############################\nwaga = read.table(""Data/ont_pipeline/WAGA/counts/waga_all_gene_counts.tsv"")\ncolnames(waga) = gsub(""_.+"","""",colnames(waga))\n\n# Specific to MKL1 since barcode 3 failed\ndesignMatrix = read.table(""Data/ont_pipeline/WAGA/DesignMatrixWAGA.txt"")\ncolnames(designMatrix) = c(""Samples"",""Condition"")\ndesignMatrix$Samples = colnames(waga)\ndds = DESeqDataSetFromMatrix(countData = waga,colData = designMatrix,design = ~Condition)\ndds = DESeq(dds)\nplotDispEsts(dds, main= ""dispEst: waga"")\n\n# Lists the coefficients\nres = results(dds, name=resultsNames(dds)[2])\nres = as.data.frame(res)\nres = res[complete.cases(res),]\nresSig = res[res$padj < 0.1 & abs(res$log2FoldChange) > 1,]\n\n# Formating of ENSG\ngeneIDs = ensembldb::select(EnsDb.Hsapiens.v86, keys = rownames(res), keytype = ""GENEID"", columns = c(""SYMBOL"",""GENEID""))\nrownames(geneIDs) = geneIDs$GENEID\nres$symbol = geneIDs[rownames(res),1]\nres[""LargeT-Antigen"",""symbol""] = ""Large-T-Antigen""\nresSig$symbol = geneIDs[rownames(resSig),1]\nrm(geneIDs)\n\nres = res[,c(""symbol"",""baseMean"",""log2FoldChange"",""pvalue"",""padj"")]\nresSig = resSig[,c(""symbol"",""baseMean"",""log2FoldChange"",""padj"")]\n\nnorm_counts = as.data.frame(counts(dds,normalized = T))\nnorm_counts_waga = norm_counts\nres = cbind(res,norm_counts[rownames(res),])\nres = res[complete.cases(res),]\noutpath = paste0(""Analysis/"",""waga-results-unfiltered.tsv"")\nwrite.table(res,file=outpath,sep=""\\t"",row.names = F)\n\nthreshold = 0.01\nlibrary(ggrepel)\n\n# VOLCANO PLOT\nmycolors = c(""green"", ""red"", ""black"")\nnames(mycolors) = c(""UP"", ""DOWN"", ""NO"")\n\nres$diff = ""NO""\nres$diff[res$log2FoldChange > 1 & res$padj < 0.1] <- ""UP""\nres$diff[res$log2FoldChange < -1 & res$padj < 0.1] <- ""DOWN""\nres$label[res$diff != ""NO""] = res$symbol[res$diff != ""NO""]\n\nggplot(res)+\n  geom_point(aes(x = log2FoldChange, y = -log10(pvalue),col=diff)) +\n  geom_text_repel(data=res,mapping = aes(x = log2FoldChange,-log10(pvalue),label = label)) +\n  scale_colour_manual(values = mycolors)+\n  ggtitle(""WAGA"") +\n  theme_bw(base_size = 15)+\n  xlab(""log2 fold change"") + \n  ylab(""-log10 p-value"") +\n  #geom_vline(xintercept=c(-1, 1), col=""red"",linetype=""longdash"")+\n  #geom_hline(yintercept=-log10(0.1), col=""red"",linetype=""longdash"")+\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 15)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +\n  theme(legend.position = ""none"",\n        plot.title = element_text(size = rel(1.5), hjust = 0.5),\n        axis.title = element_text(size = rel(1.25))) \n\n####\nrownames(designMatrix) = designMatrix$Sample\n\nvsd = vst(dds, blind=FALSE)\npcaData = plotPCA(vsd, intgroup=c(""Condition""), returnData=TRUE)\npercentVar = round(100 * attr(pcaData, ""percentVar""))\nggplot(pcaData, aes(PC1, PC2, color=Condition)) +\n  geom_point(size=3) +\n  xlab(paste0(""PC1: "",percentVar[1],""% variance"")) +\n  ylab(paste0(""PC2: "",percentVar[2],""% variance"")) + \n  coord_fixed()\n\nvsd = as.data.frame(assay(vsd))\nvsd = vsd[row.names(resSig),]\nrownames(designMatrix) = designMatrix$Sample\nresSig = resSig[order(resSig$log2FoldChange,decreasing = T),]\nvsd = vsd[rownames(resSig),]\n\npheatmap::pheatmap(vsd,\n                   color = heat_colors,\n                   cellwidth = 30,\n                   cellheight = 30,\n                   show_colnames = F,\n                   cluster_rows = F,\n                   cluster_cols =  T,\n                   border_color = NA,\n                   labels_row = resSig$symbol,\n                   fontsize = 7,\n                   annotation_col = designMatrix[,c(2),drop=F],\n                   annotation_row = resSig[,c(4,3),drop = F],\n                   main = ""WAGA - VSD normaliszed"",\n                   angle_col = ""45"")\n\n\n#############################\n####### MKL1 ################\n#############################\n\nmkl1 = read.table(""Data/ont_pipeline/MKL1/counts/mkl1_all_gene_counts.tsv"")\ncolnames(mkl1) = gsub(""_.+"","""",colnames(mkl1))\n# Specific to MKL1 since barcode 3 failed\nmkl1 = mkl1[,-3]\ndesignMatrix = read.table(""Data/ont_pipeline/MKL1/DesignMatrixMKL1.txt"")\ncolnames(designMatrix) = c(""Samples"",""Condition"")\ndesignMatrix$Samples = colnames(mkl1)\ndds = DESeqDataSetFromMatrix(countData = mkl1,colData = designMatrix,design = ~Condition)\ndds = DESeq(dds)\nplotDispEsts(dds, main= ""dispEst: mkl1"")\n\n# Lists the coefficients\nres = results(dds, name=resultsNames(dds)[2])\nres = as.data.frame(res)\nres = res[complete.cases(res),]\nresSig = res[res$padj < 0.1 & abs(res$log2FoldChange)>1,]\n\n# Formating of ENSG\ngeneIDs = ensembldb::select(EnsDb.Hsapiens.v86, keys = rownames(res), keytype = ""GENEID"", columns = c(""SYMBOL"",""GENEID""))\nrownames(geneIDs) = geneIDs$GEN']",3,"MCC-analysis, data archive, tpm, counts, implementation, plots, analysis, paper xyz."
Analysis and data set,Code and data set for analysing eco button trial.,"['library(lme4)\nlibrary(lmerTest)\nlibrary(sciplot)\nlibrary(apex)\nlibrary(piecewiseSEM)\n\n\nrm(list = ls(all = TRUE))\nload(""/Users/floriankutzner/Documents/Academia/AAA_Projekte/horizon 2020 EVs/Electrific CelFlo/WP6 Research/EcoButtonTrial/Data/how_far.Rda"")\n\n\n#social norm condition contrast\nda$sticker_contrast<- -1\nda[which(da$sticker.condition==""CAR"" | da$sticker.condition==""EWALD""),""sticker_contrast""] <- 1\n\n#correct km measure from OBD (/2 because THD recorded 2 bit for every instance)\nda$km<-da$km/2\n\n#####data cleaning \n#remove those with loss from OBD transmission\nda$broken<- 0\nda[(da$milage.trip-da$km)/da$milage.trip >.10,""broken""]<-1\n#plot(da$milage.trip,da$km)\nchisq.test( table(da$broken,da$eco_start))\nchisq.test( table(da$broken,da$sticker_contrast))\n\nda<-da[-which(da$broken==1),]\n\n#get at number of participants in trial\nda$survey.code<-droplevels(da$survey.code)\ncodes<-table(da$survey.code)\nda$nr_part<-NA\nfor (j in 1:dim(codes)){da[which(da$survey.code == names(codes[j])),""nr_part""]<-codes[j][[1]]}\n#table(da$nr_part)\nunique(da$survey.code)\nsummary(da$milage.trip)\n\nhist(rowSums(table(da$survey.code,da$nr_part)),breaks = 50)\nsummary(rowSums(table(da$survey.code,da$nr_part)))\n\n#average consumption\nda$av_consumption <-(da$soc_change/1000)/(da$km/100)\n\n#variables for regressions\n#recode variable to 1 = on at start and 0 = off at start\nda$eco_start<-abs(da$eco_start-1)\nda$eco_start_factor<-as.factor(da$eco_start)\nlevels(da$eco_start_factor)<-c(""OFF at start"",""ON at start"")\n#levels(da$eco_start_factor)<-c(""AUS beim Start"",""An beim Start"")\nda$sticker_contrast_factor<-as.factor(da$sticker_contrast)\nlevels(da$sticker_contrast_factor)<-c(""Control"",""Social norm"")\n#levels(da$sticker_contrast_factor)<-c(""Kontrolle"",""Soziale Norm"")\n\n##########\n#z-standardization\n##########\nda$eco_start_cent<-scale(da$eco_start,scale=T)\nda$sticker_contrast_cent<-scale(da$sticker_contrast,scale=T)\nda$km_cent<-scale(da$km,scale=T)\nda$eco_share<-scale(da$eco_share,scale=T)\nda$av_consumption<-scale(da$av_consumption,scale=T)\nda$av_abs_accel<-scale(da$av_abs_accel,scale=T)\nda$av_pedal<-scale(da$av_pedal,scale=T)\n###########\n## cell sizes\ntable(da$sticker_contrast_factor, da$eco_start_factor)\ntable(da$sticker.condition,da$eco_start)\n\n###########\n#Regressions\n###########\nshare<-lmer(eco_share~eco_start_cent*sticker_contrast_cent*km_cent+\n                      (eco_start_cent*sticker_contrast_cent*km_cent||survey.code),data=da)\nsummary(share)\neffectsize::standardize_parameters(share)\n\n\naccel<-lmer(av_abs_accel~eco_start_cent*sticker_contrast_cent*km_cent+\n                         (eco_start_cent*sticker_contrast_cent*km_cent||survey.code),data=da)\nsummary(accel)\neffectsize::standardize_parameters(accel)\n\ncons<-lmer(av_consumption~eco_start_cent*sticker_contrast_cent*km_cent+\n                           (eco_start_cent*sticker_contrast_cent*km_cent||survey.code),data=da)\nsummary(cons)\neffectsize::standardize_parameters(cons)\n\n#habituation to pedal sensitivity\npedal<-lmer(av_pedal~eco_start_cent*sticker_contrast_cent*km_cent+\n                     (eco_start_cent*sticker_contrast_cent*km_cent||survey.code),data=da)\nsummary(pedal)\neffectsize::standardize_parameters(pedal)\n\nM <- lmer(eco_share~eco_start_cent*sticker_contrast_cent*km_cent+\n       (eco_start_cent*sticker_contrast_cent*km_cent||survey.code),data=da)\nM <- lmer(av_abs_accel~eco_start_cent*sticker_contrast_cent*km_cent+\n            (eco_start_cent*sticker_contrast_cent*km_cent||survey.code),data=da)\nM <- lmer(av_consumption~eco_start_cent*sticker_contrast_cent*km_cent+\n            (eco_start_cent*sticker_contrast_cent*km_cent||survey.code),data=da)\nM <- lmer(av_pedal~eco_start_cent*sticker_contrast_cent*km_cent+\n             (eco_start_cent*sticker_contrast_cent*km_cent||survey.code),data=da)\n\nconfint.merMod(M, devtol = 1e-6)\nconfint(M, maxpts = 8, devtol = 1e-6)\nconfint(M, method=""Wald"")\n###########\n#Plotting\n###########\npar(mfrow=c(2,2))\nbargraph.CI(eco_start_factor,eco_share, group = sticker_contrast_factor, data=subset(da),\n            main=""(A) Eco mode ON"",legend = T,x.leg=0.7, y.leg=1,cex.leg = .9,\n            ylim=c(0,1),ylab=""Proportion of drive time"")\nbargraph.CI(eco_start_factor,av_abs_accel, group = sticker_contrast_factor, data=subset(da),\n            main=""(B) Car acceleration"",legend = F,x.leg=1,y.leg = .9,cex.leg = .9,\n            ylim=c(.5,.8),ylab=expression(paste(m/s^2, "" (absolute values)"")))\nbargraph.CI(eco_start_factor,av_consumption, group = sticker_contrast_factor, data=subset(da),\n            main=""(C) Consumption"",legend = F,x.leg=1,y.leg = 50,cex.leg = .9,\n            ylim=c(0,40),ylab=""kWh/100km"")\nbargraph.CI(eco_start_factor,av_pedal, group = sticker_contrast_factor, data=subset(da),\n            main=""(D) Accelerator pedal"",legend = F,x.leg=1,y.leg = 52,cex.leg = .9,\n            ylim=c(40,50),ylab=""Pedal position (0 - 250)"")\n\npar(mfrow=c(2,2))\nbargraph.CI(eco_start_factor,eco_share, group = sticker_contrast_factor, data=subset(da),\n            main=""(A) Ante']",3,"Analysis, data set, code, eco button, trial, environment, energy conservation, sustainability, experimental design, statistics, data analysis, energy usage, conservation behavior, research."
"Incidence estimation from prevalence of a chronic disease (without remission, without excess mortality)","The script comprises:1) a data set of aggregated data (cases and population stratified by age) from two cross-sections about a hypothetical chronic disease motivated by type 1 diabetes in the SEARCH study2) an algorithm programmed in the statistical software R (The R Foundation for Statistical Computing) to estimate the age-specific incidence rate from the prevalence data given in 1) using the assumptions that is no remission (no cure) and no excess mortalityImportant note: the data in 1) are mock-up data, which are only given to demonstrate the methodological approach described in 2). These data are not intended for any other purpose.","['rm(list=ls(all=TRUE))\r\n\r\n##################################################################\r\n#\r\n#          Estimation of incidence from prevalence\r\n#               (without remission, without excess mortality)\r\n# \r\n##################################################################\r\n\r\n# Steps:\r\n# 1) data preparation\r\n# 2) definition of auxiliary functions\r\n# 3) data analysis\r\n# 4) representation of results\r\n\r\n\r\n## ====================\r\n## 1) data preparation\r\n## ====================\r\n\r\n# The data set comprises aggregated data (cases and population\r\n# stratified by age) from two cross-sections about a hypothetical \r\n# chronic disease motivated by type 1 diabetes in the SEARCH study\r\n\r\n# Important note: these are mock-up data, which are only given to \r\n# demonstrate the methodological approach. These data are not \r\n# intended for any other purpose.\r\n\r\n\r\n# number of cases and population size in one-year age groups \r\n# (0-<1, 1-<2, ..., 19-<20) and years 2001 and 2009\r\ncases01 <- c(    1,    2,    3,    6,    9,   11,   17,   20,   21,   34,\r\n                37,   45,   54,   62,   51,   51,   45,   48,   43,   50)\r\npopul01 <- c(23802,19411,18486,20071,23290,19324,23174,24442,21967,26500,\r\n             23229,24680,26293,27131,21795,20607,19240,23721,21705,21242)\r\ncases09 <- c(    1,    2,    4,    7,   11,   19,   19,   23,   34,   43,\r\n                58,   68,   69,   67,   61,   73,   70,   79,   69,   73)\r\npopul09 <- c(24734,25880,25013,20779,21384,23525,20369,20802,24058,21595,\r\n             22862,23895,23808,23549,22278,26443,25724,30333,24970,26639)\r\n\r\n# prevalence\r\np01     <- cases01/popul01\r\np09     <- cases09/popul09\r\n\r\nages    <- 0.5 + 0:19 # middle of age groups\r\n\r\nif(FALSE){\r\n# check if input data look ok\r\nmatplot(ages, 1e3*p01, type = ""l"", lty = 4, xlab = ""Ages (years)"", \r\n     ylim = c(0, 4), col = ""gray5"", ylab = ""Prevalence (per 1000)"")\r\nmatplot(ages, 1e3*p09, type = ""l"", add = TRUE)\r\nlegend(""topleft"", legend=c(""2009"", ""2001""), lty = c(1, 4), \r\n     col = c(""black"", ""gray5""))\r\n}\r\n\r\n## ===================\r\n## 2) aux functions\r\n## ===================\r\n\r\nlogit <- function(x){log(x/(1-x))}       # logit function\r\nexpit <- function(x){exp(x)/(1+exp(x))}  # inverse of logit function\r\n\r\n\r\n\r\n## ================\r\n## 3) data analysis\r\n## ================\r\n\r\n# Modelling of prevalence\r\n# -----------------------\r\n\r\ntt <- rep(c(1, 9), each = length(ages)) # years after 2000\r\naa <- rep(ages, 2)\r\n\r\n\r\n# fit a logit model to prevalence data\r\nvec.p       <- as.numeric(c(p01, p09))\r\nl.p         <- lm(logit(vec.p) ~ tt*poly(aa, 3))\r\n#summary(l.p) #-> looks good\r\n# note: other models are possible, eg, affine linear interpolation\r\n\r\n# make prevalence easily accessible\r\nfct_p <- function(t, a){\r\n   return(expit(as.numeric(predict(l.p, newdata = data.frame(tt = rep(t, length(a)), aa = a))))) \r\n}\r\n\r\n\r\n# partial derivative of the prevalence (numerically approximated)\r\nfct_dp <- function(t, a){\r\n   return((fct_p(t+.001, a+.001) - fct_p(t-.001, a-.001))/.002) \r\n}\r\n\r\n# Incidence estimation\r\n# --------------------\r\n\r\ne.ages <- seq(0.5, 19.5, by = 1) # ages at which incidence is estimated\r\n\r\n# Incidence in 2005 (t=5, middle btw survey years 2001 and 2009)\r\ndp_    <- sapply(e.ages, FUN = fct_dp, t=5)   # partial derivative of prev\r\np_     <- sapply(e.ages, FUN = fct_p,  t=5)   # prev\r\n\r\n# Equation (4) from this paper: https://doi.org/10.1371/journal.pone.0118955\r\n# with R = 1 (i.e., no excess mortality) and r = 0 (i.e., no remission)\r\ninc    <- dp_/(1-p_) \r\n\r\n\r\n## ================\r\n## 4) representation of results\r\n## ================\r\n\r\nmatplot(e.ages, 1e5*inc, col = ""black"",  type = ""b"", pch = 1, las = 1, \r\n   xlab = ""Age (years)"", ylab = ""Incidence (per 100,000 py)"", \r\n   main = ""Incidence"")\r\n\r\n']",3,"Chronic disease, prevalence, incidence estimation, remission, excess mortality, data set, aggregated data, age, cross-sections, type 1 diabetes, SEARCH study, algorithm, statistical software, R Foundation, age-specific incidence rate, assumptions,"
"Data from: Phylogenetic ANOVA: group-clade aggregation, biological challenges, and a refined permutation procedure","Phylogenetic regression is frequently utilized in macroevolutionary studies, and its statistical properties have been thoroughly investigated. By contrast, phylogenetic ANOVA has received relatively less attention, and the conditions leading to incorrect statistical and biological inferences when comparing multivariate phenotypes among groups remains under-explored. Here we propose a refined method of randomizing residuals in a permutation procedure (RRPP) for evaluating phenotypic differences among groups while conditioning the data on the phylogeny. We show that RRPP displays appropriate statistical properties for both phylogenetic ANOVA and regression models, and for univariate and multivariate datasets. For ANOVA, we find that RRPP exhibits higher statistical power than methods utilizing phylogenetic simulation. Additionally, we investigate how group dispersion across the phylogeny affects inferences, and reveal that highly aggregated groups generate strong and significant correlations with the phylogeny, which reduce statistical power and subsequently affect biological interpretations. We discuss the broader implications of this phylogenetic group aggregation, and its relation to challenges encountered with other comparative methods where one or a few transitions in discrete traits are observed on the phylogeny. Finally, we recommend that phylogenetic comparative studies of continuous trait data utilize RRPP for assessing the significance of indicator variables as sources of trait variation.","['#Empirical Example: Plethodon body proportions\r\n\r\ndevtools::install_github(\'geomorphR/geomorph\',ref=""Develop"")  #new version of method until manuscript acceptance\r\nlibrary(ape)\r\nlibrary(geomorph)\r\nlibrary(geiger)\r\n\r\n#Tree and data\r\ntree.best<-read.nexus(""Consensus of 1000 salamander trees.nex"") #Maximum Credible Tree\r\nplot(tree.best)\r\nplethdata<-read.csv(""meandata-CinGlutOnly.csv"",row.names=1, header=TRUE)\r\nplethtree<-drop.tip(tree.best,setdiff(tree.best$tip.label,rownames(plethdata)))\r\nplot(plethtree, edge.width = 3)\r\naxisPhylo(1)\r\n \r\n#isometric size adjustment to get to relative proportions\r\ngroup<-as.factor(plethdata[,1]); names(group)<-rownames(plethdata)\r\nsize<-as.matrix(plethdata[,2]); rownames(size)<-rownames(plethdata)\r\n  body<-as.matrix(plethdata[,-(1:3)])\r\nY<-apply(body,2,function(x) x/size); rownames(Y)<-rownames(body)\r\n  gdf<-geomorph.data.frame(Y=Y, BW = Y[,3],\r\n                           size=size,group=group)\r\n\r\n##Group Aggregation\r\nC<-vcv.phylo(plethtree)\r\nX<-model.matrix(~group); rownames(X) <- names(group)\r\n\r\ntwo.b.pls(C, X,iter=9999)  #Near perfect correlation: PLS-r = 0.99, P = 0.0001\r\n  \r\n##Analyses  \r\nprocD.lm(Y~size, data=gdf)\r\nprocD.lm(Y~group, data=gdf)\r\nprocD.lm(Y~group*size, data=gdf, SS.type = ""II"")\r\n\r\n# is it the correlation between X variables or the C matrix causing the different results?\r\n# repeat these steps a few times\r\n\r\nnewsize <- sim.char(plethtree, par=1)[,,1]\r\ngdf$size <- newsize\r\nprocD.pgls(Y~size*group, data=gdf, phy = plethtree, SS.type = ""I"")\r\nprocD.pgls(Y~group*size, data=gdf, phy = plethtree, SS.type = ""I"")\r\n\r\n\r\n##Interpreting\r\nres<-procD.pgls(Y~size*group, data=gdf, phy = plethtree)\r\n means<-array(NA,dim=c(2,ncol(res$pgls.fitted)))\r\n for (i in 1:ncol(res$pgls.fitted)){\r\n   means[,i]<-tapply(res$pgls.fitted[,i],group,mean)\r\n }\r\n colnames(means)<-colnames(res$pgls.fitted)\r\n rownames(means)<-c(""Glut"",""Cin"")\r\n means   #Gluts proportionally larger, especially limbs\r\n \r\n### PLOTS\r\nplot(plethtree)\r\nplotGMPhyloMorphoSpace(phy=plethtree,A=Y,node.labels = FALSE,tip.labels = FALSE)\r\n\r\n\r\n###################################\r\n#PhyloMorphoSpace By Hand (to adapt for this dataset)\r\nN <- length(plethtree$tip.label)\r\nx<-Y\r\nx <- x[plethtree$tip.label, ]\r\ngp.plot<-group[plethtree$tip.label]\r\nsz.plot<-size[plethtree$tip.label,]\r\nnames <- row.names(x)\r\nanc.states <- NULL\r\nfor (i in 1:ncol(x)) {\r\n  options(warn = -1)\r\n  tmp <- as.vector(ace(x[, i], compute.brlen(plethtree, 1), type = ""continuous"", \r\n                       method = ""ML"")$ace)\r\n  anc.states <- cbind(anc.states, tmp)\r\n}\r\ncolnames(anc.states) <- NULL\r\nall.data <- rbind(x, anc.states)\r\nphylo.PCA <- prcomp(all.data)$x\r\n\r\nlimits = function(x, s) {\r\n  r = range(x)\r\n  rc = scale(r, scale = F)\r\n  l = mean(r) + s * rc }\r\n\r\n# Plotting regular phylomorphospace\r\nlibrary(calibrate)\r\nplot(phylo.PCA, type = ""n"", xlim = limits(phylo.PCA[, 1], 1.1), \r\n     ylim = limits(phylo.PCA[, 2], 1.1), asp = 1)\r\nfor (i in 1:nrow(plethtree$edge)) {\r\n  lines(phylo.PCA[(plethtree$edge[i, ]), 1], phylo.PCA[(plethtree$edge[i, ]), 2],\r\n        type = ""l"", pch = 21, col = ""black"", lwd = 2)}\r\nN <- length(plethtree$tip.label)\r\n\r\npoints(phylo.PCA[(N + 1):nrow(phylo.PCA), ], pch = 21, bg = ""white"", cex= 1.00) #nodes\r\npoints(phylo.PCA[which(gp.plot==""Large""), ], pch = 22, \r\n       bg = ""gray"", cex= 2) # tips\r\npoints(phylo.PCA[which(gp.plot==""Small""), ], pch = 21, \r\n       bg = ""gray"", cex= 2) # tips\r\n#textxy(phylo.PCA[1:N, 1], phylo.PCA[1:N, 2], rownames(phylo.PCA), cex= 0.5) # tip labels\r\n\r\n']",3,"Phylogenetic ANOVA, group-clade aggregation, randomizing residuals, permutation procedure, multivariate phenotypes, statistical properties, phylogenetic regression, univariate datasets, multivariate datasets, statistical power, phylogenetic simulation, group"
Dataset of Impact of pre-breeding feeding practices on rabbit mammary gland development at mid-pregnancy,"The dataset includes search results used in article Impact of pre-breeding feeding practices on rabbit mammary gland development at mid-pregnancy biorXiv, 2022.01.17.476562, ver. 3 peer-reviewed and recommended by Peer Community in Animal Science. https://doi.org/10.1101/2022.01.17.476562 Search Results Description: Please use Figure 1 from paper to trace the data made available and experimental group.The excel raw data 2022-06-24 file contains the following dataBody weight of each rabbit on a weekly basisAnalysis of breeding parameters at mid-pregnancyHistological areas of each mammry tissue measuredOptical density values obtained for biochemical leptin concentration determinationOptical density values obtained for biochemical triglyceride concentration determinationOptical density values obtained for biochemical glucose concentration determinationOptical density values obtained for biochemical cholesterol concentration determinationRT-qPCR results (Ct) from QuantStudio export for milk protein analysisRT-qPCR results (Ct) from QuantStudio export for lipid metabolism analysisenStatistical analysis.doc contained the description of the statistics used in Excel and the description of the linear mixed model analysis , the reference of the script available by the CRAN project is also include.The R scipt file for using the linear mixed model in R added with the ""data-croissance-analysis"" file.","['setwd(""D:/Florence Jaffrezic/Professionnel/Documents/CATHY/Avril_2022"")\r\n\r\ndata_croissance = read.table(""data_croissance_analyses.txt"",sep=""\\t"",header=TRUE)\r\n\r\nlibrary(nlme);library(lattice);library(car);library(lsmeans)\r\n\r\noptions(contrasts = c(""contr.sum"", ""contr.poly"")) \r\n\r\n# post-weaning and pubertal\r\n\r\ndata1 = data_croissance[data_croissance$time<=11,]\r\n\r\nxyplot(mesures~time|group,data=data1,groups=as.character(animal),type=""l"",\r\n\tmain=""Period 1"")\r\n\r\nmodel1_1 = lme(mesures ~ group + time + group:time, random= ~1|animal,\r\ndata=data1,correlation=corAR1(form=~1|animal),na.action=na.omit)\r\n\r\nsummary(model1_1)\r\nAnova(model1_1,type=3)\r\n\r\nmodel2_1 = lme(mesures ~ 0 + group:as.factor(time), random= ~1|animal,\r\ndata=data1,correlation=corAR1(form=~1|animal),na.action=na.omit)\r\n\r\nlsmeans(model2_1,list(pairwise~group|time)) \r\n\r\n# fattening period\r\n\r\ndata2 = data_croissance[data_croissance$time>=12,]\r\n\r\nxyplot(mesures~time|group,data=data2,groups=as.character(animal),type=""l"",\r\n\tmain=""Period 2"")\r\n\r\nmodel1_2 = lme(mesures ~ group + time + group:time, random= ~1|animal,\r\ndata=data2,correlation=corAR1(form=~1|animal),na.action=na.omit)\r\n\r\nsummary(model1_2)\r\nAnova(model1_2,type=3)\r\n\r\nmodel2_2 = lme(mesures ~ 0 + group:as.factor(time), random= ~1|animal,\r\ndata=data2,correlation=corAR1(form=~1|animal),na.action=na.omit)\r\n\r\nlsmeans(model2_2,list(pairwise~group|time)) \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n']",3,"Dataset, pre-breeding feeding practices, rabbit, mammary gland development, mid-pregnancy, biorXiv, peer-review, animal science, body weight, breeding parameters, histological areas, mammry tissue, leptin concentration, trig"
Data from: Diagnostic gene expression biomarkers of coral thermal stress,"Gene expression biomarkers can enable rapid assessment of physiological conditions in situ, providing a valuable tool for reef managers interested in linking organism physiology with large-scale climatic conditions. Here, we assessed the ability of quantitative PCR (qPCR) based gene expression biomarkers to evaluate (1) the immediate cellular stress response (CSR) of Porites astreoides to incremental thermal stress and (2) the magnitude of CSR and cellular homeostasis response (CHR) during a natural bleaching event. Expression levels largely scaled with treatment temperature, with the strongest responses occurring in heat shock proteins. This is the first demonstration of a ""tiered"" CSR in a coral, where the magnitude of expression change is proportional to stress intensity. Analysis of a natural bleaching event revealed no signature of an acute CSR in normal or bleached corals, indicating that the bleaching stressor(s) had abated by the day of sampling. Another long-term stress CHR-based indicator assay was significantly elevated in bleached corals; though assay values overall were low, suggesting good prospects for recovery. This study represents the first step in linking variation in gene expression biomarkers to stress tolerance and bleaching thresholds in situ by quantifying the severity of ongoing thermal stress and its accumulated long-term impacts.","['#############for plotting primer efficiencies######################\n\nprimereff=read.csv(file.choose()) #select primer efficiency .csv file of gene, ct value and concentration\nhead(primereff) #check to see everything looks okay\nsummary(primereff$gene) #to view list of gene names\ngene=subset(primereff, gene==""SCF"") #you will have to change the gene name in quotes to cycle through all the genes\nsummary(lm(concentration~ct1,gene)) #This gives you the summary of the regression - the slope is the value at the bottom of the first column (Estimate:ct1), the R2 is listed in the paragraph at the bottom of the summary\nsummary(lm(concentration~ct2,gene))\npar(mfrow=c(3,8)) #a plot function to split up your graph into multiple windows if you want to plot all genes in the same figure\nplot(concentration~ct1,gene,main=""SCF"",sub=""slope= 1.02  R2=0.93"") #you will need to manually change the gene name and the slope and R2 values in quotes depending on which gene you\'re analyzing and the results of the summary command\nabline(lm(concentration~ct1,gene)) #this plots a best-fit line for your regression\nplot(concentration~ct2,gene,main=""SCF"",sub=""slope= 0.96  R2=0.99"") #you will need to manually change the gene name and the slope and R2 values in quotes depending on which gene you\'re analyzing and the results of the summary command\nabline(lm(concentration~ct2,gene))\n\n##############Reformatting column based data in excel to proper format for analysis\ngenes=levels(testCq$gene)\ntest=unstack(testCq,avgct~gene*sample*origin*transplant)\n\n\n##################################################################Normalization of qPCR data\n#install.packages(""/carlykenkel/Downloads/CAmethod_1.0.tar.gz"") #select local package download, click install and navigate to location\nlibrary(MCMC.qpcr)\n\ntestCq=read.csv(""2010BNB_revival.csv"") # loading raw Cq data: whateveryounamedyourfile.txt and rembering it as testCq\n\neff=read.table(file.choose(),header=T) # loading efficiencies: whateveryounamedyourefficiencies.txt and remembering it as eff\n\nhead(testCq) # to see how the first few lines in the file look\nhead(eff)\ngenecolumns=c(4:20) # gene values are in the columns number 4-19 (modify as needed for your own dataset)\n\nefc=EffCorr(testCq,eff) #correcting Cqs for efficiencies and remembering it as efc\n\nhead(efc) # see how the first few lines in the corrected file, efc, look\n\nefc$dblgene=-(efc$aBcryst-efc$Actin)\nplot(dblgene~condition,efc)\nsummary(aov(dblgene~condition,efc))\nm1=aov(dblgene~treatment,efc)\nsummary(glht(m1,linfct = mcp (treatment = ""Tukey"")))\n\n\nwrite.csv(efc, file=""BNBEffCor.csv"", quote=F)\n#library(VGAM)\nconditioncolumns=c(1:2) #the columns with your experimental conditions (modify as needed for your own dataset)\ngenecolumns=c(3:20)\n#CAexplore(efc,genecolumns,conditioncolumns,1)\n\ncenter=CenterCq(efc,genecolumns) # transforming data so Ct values center around 0 instead of being all positive and rembering it as center\nhead(center)\n#NormFinder(q2,c(4:7),c(1:3)) #skip this for now; will sort out with Misha\n\ncalibrators=c(""eif3h"",""rpl11"") #These genes will be our normalizers for heat stress expts\n\nnormalized=PCRnormalize(center,calibrators,genecolumns) #taking center and using normalizer genes to correct for template loading variation across samples\n\nstack=stackCq(normalized,genecolumns,conditioncolumns) #re-formatting data from multiple columns to single column\n\nwrite.csv(normalized, file=""BNB2010_Normalized.csv"", quote=F) #writing output file of normalized data - can run this in any stats program you like if R isn\'t your thing...otherwise, continue below!\n\nsummary(aov(actin~condition,normalized))\np.adjust(c(0.918,0.357,0.232,0.544,0.912,0.0958,0.37,0.014,0.808,0.881,0.294,0.0194,0.428),method=""BH"") #bleachnobleach\n\n######################################################Data Analysis\n\nlibrary(lattice) # this loads the library containing the function bwplot()\nlibrary(lme4)\nlibrary(multcomp)\n\nnormalized=read.csv(""BNB2010_Normalized.csv"")\n\nnormalized$sample=as.factor(normalized$sample)\nsummary(normalized)\n\n#x18s2,abcryst, actin, adk, c3, chrom, tgoln, ubl3, acad, agxt, malsyn, pepck, pyrca\n\ngene=""abcryst""\nCq=normalized[,gene]\nanova(lm(Cq~condition,normalized))\np.adjust(c(0.357,0.2328,0.5468,0.9134,0.09557,0.3797,0.01385,0.8806,0.2863,0.01947,0.4287,0.1048,0.3866,0.04959,0.2769,0.003269,0.693),method=""BH"")#g3pdh, spon2, slc26, lts significant; only LTS remains marginally significant following MTC for all at p=0.056\n\nnames(normalized)\ngenecolumns=c(3:21)\nconditioncolumns=c(1:2)\nstack=stack(normalized,genecolumns,conditioncolumns)\ncolnames(stack)<-c(""Cq"",""gene"")\nstack$sample<-normalized$sample\nstack$condition<-normalized$condition\n\n#m0.1=lmer(Cq~(1|sample)+(1|tank),normalized)\n#m0=lmer(Cq~(1|sample),normalized)\n#anova(m0,m0.1)\n\nHGnormalized=read.csv(""HeatGradationNormalized_finalized.csv"")\n\nHGnormalized$treatment=as.factor(HGnormalized$treatment)\nsummary(HGnormalized)\n\n#""abcryst""   ""actin""     ""adk""       ""c3""        ""chrom""     ""g3pdh""     ""hsp60""     ""hsp']",3,"Diagnostic gene expression biomarkers, Coral thermal stress, rapid assessment, physiological conditions, in situ, valuable tool, reef managers, organism physiology, large-scale climatic conditions, quantitative PCR (qPCR), cellular stress response, Porites astreoides"
Data from: High-density cultivation of microalgae continuously fed with unfiltered water from a recirculating aquaculture system,"Water from recirculating aquaculture systems (RAS) has been shown to be a suitable growth medium for microalgae and their cultivation can, therefore, be used to reduce RAS emissions. However, while efficient wastewater treatment is possible, the nutrient content of RAS water limits attainable microalgae biomass densities to 12 g l1 at best, which requires frequent harvesting of microalgae. We have taken advantage of the constant evaporation of water from an open thin-layer photobioreactor (200 l volume, 18 m2 illuminated surface, artificial supply of CO2) to continuously add water from RAS to a microalgae culture and thereby provide nutrients for continued growth while evaporating all water. To test for a possible inhibitory effect of RAS water on microalgae growth, components of mineral medium were omitted stepwise in subsequent cultivations and replaced by RAS water as the only source of nutrients. This approach showed that microalgae can be grown successfully for up to three weeks in RAS water without additional nutrients and that high (20 g l1) biomass densities can be attained. While growth in wastewater did not reach productivities measured in mineral medium, analysis of growth data suggested that this reduction was not due to an inhibitory effect of the RAS water but due to an insufficient supply rate of nutrients, even though RAS water contained up to 158 mg l1 NO3-N. It is, therefore, concluded that this method can be used to fully treat the wastewater discharge of a RAS. Furthermore, because both water evaporation from and microalgae growth in the photobioreactor correlated positively with each other due to their shared dependency on solar radiation, supply of nutrients continuously adjusts to changes in demand. It is estimated that the area of a photobioreactor required to treat all emissions of a RAS requires approximately 6.5 times the area of the latter.","['#set the path to your working directory, where all .csv files must be placed\nsetwd(""~/SWITCHdrive/3_Projects/Anschubfinanzierung/Data analysis/Data for dryad"")\n\n#collect all data, add some variables, and save them as dataframe ""all.data""\nfiles <- data.frame(\n  run  = c(""Run140514"",""Run140708"",""Run140722"",""Run140814"",""Run140902"",""Run140619"",""Run150624"",""Run150817"",""Run150909""),\n  type = c(""std.1"",    ""std.2"",    ""std.3"",    ""std.4"",    ""aq.fert"",  ""aq.no.n"",  ""aq.no.me"", ""aq.only.1"",""aq.only.2""),\n  use.for.growth = c(F,F,F,T,T,T,F,F,F),\n  use.for.evap   = c(T,T,T,F,T,T,F,F,F))\n\nall.data <- data.frame()\n\n# Factors\npar2watt <- 1/4.94 # PPFD to Watts per m2. Use conversion factor as given by Doucha and Livansky (2013)\nn.cont <- 0.08493 # N content of microalgae biomass (w/w) (Doucha and Livansky, 2006).\np.cont <- 0.00899 # P content of microalgae biomass (w/w) (Doucha and Livansky, 2006).\nenergy.in.biomass <- 23 # Energy content per g biomass in kJ\nkWh2kJ <- 3600 # kWh to kJ\npe.assumed <- 0.05 # Energy in PAR converted into energy in Biomass\n\npar(mfrow = c(3,3), mar = c(4,4,1,1))\nfor(i in 1:nrow(files)){\n  run <- files$run[i]\n  type <- files$type[i]\n  cat(paste(""\\nProcesssing now:"",run))\n  raw.data <- read.csv(paste(run,"".csv"", sep = """"), sep = "","", dec = ""."", header = T)\n  raw.data$time.string <- as.POSIXct(raw.data$time.string)\n  if(""dryweight.not.washed"" %in% colnames(raw.data)){\n    names(raw.data)[names(raw.data)==""dryweight.not.washed""] <- ""dryweight""\n  }\n  \n\t#Remove data that are before or after runs (e.g. if inoculum size was so small that dryweight measurements were not possible for a few days)\n\tif(run == ""Run140708""){\n\t  cat(paste(""\\n\\tData before or after run are truncated.""))\n\t  index <- which(raw.data$time.string == ""2014-07-12 08:30:00"")\n\t  raw.data <- raw.data[index:nrow(raw.data),]\n\t  }\n  \n  #Now, the beginning of raw.data is the beginning of the experiment\n  plot(raw.data$time.string, raw.data$TURBIDITY, ylim = c(0,35), xlab="""", ylab=""Turbidity (g solids/l)"", type = ""l"", main = paste(run, type, sep = "",""), lwd = 2)\n\n  #Create new variable ""days"", which is time as days since midnight before the start of the experiment\n\tfirst.time <- as.POSIXct(raw.data$date[1])\n\traw.data$days <- as.numeric(difftime(raw.data$time.string, first.time, units = ""days""))\n\tcat(paste(""\\n\\tDuration of run"",files$run[i],""is"",round(raw.data$days[nrow(raw.data)], digits = 1),""days.""))\n\t\n\t#Create new variable ""water"" which is the cumulative amount of water added since the beginning of the experiment\n\traw.data$water <- raw.data$SUM.OF.WATER-raw.data$SUM.OF.WATER[1]\n\t#Create new variable ""delta.water"" which is the amount of water added per 10-min time step\n\traw.data$delta.water <- c(0,diff(raw.data$water, lag = 1))\n\t\n\t#remove water data where the water meter was broken or where readings were erroneous. Replace water data in Run140902 with manually acquired data\n\tif(run == ""Run140514""){\n\t  start <- which(raw.data$time.string == ""2014-05-21 13:40:00"")\n\t  end <- which(raw.data$time.string == ""2014-05-28 08:00:00"")\n\t  raw.data$water[start:end] <- NA\n\t  index <- which(raw.data$time.string == ""2014-05-28 06:00:00"")\n\t  raw.data$dryweight[index] <- NA\n\t  raw.data <- raw.data[c(1:1980,1990:nrow(raw.data)),]\n\t  }\n\tif(run == ""Run140619""){\n\t  raw.data <- raw.data[1:(nrow(raw.data)-1),]\n\t  }\n\tif(run == ""Run140814""){\n\t  raw.data$water <- NA\n\t  raw.data <- raw.data[1:(nrow(raw.data)-1),]\n\t  #remove erroneous turbidity measurement\n\t  index <- which(raw.data$time.string == ""2014-08-26 07:40:00"")\n\t  raw.data$TURBIDITY[index] <- NA\n\t  }\n\tif(run == ""Run140722""){\n\t  #at July 29 the water meter stopped working\n\t  index <- which(raw.data$time.string == ""2014-07-29 00:00:00"")\n\t  raw.data$water[index:nrow(raw.data)] <- NA\n\t  #dry weight is too low\n\t  index <- which(raw.data$time.string == ""2014-08-02 09:00:00"")\n\t  raw.data$dryweight[index] <- NA\n\t  }\n\tif(run == ""Run140902""){\n\t  #two times the numbers on the replacement water meter were not correctly read\n\t  raw.data$water.m3[which(raw.data$time.string == ""2014-10-04 09:30:00"")] <- 56.1188\n\t  raw.data$water.m3[which(raw.data$time.string == ""2014-10-05 11:00:00"")] <- 56.1759\n\t  #water meter is broken. Replace values with those that have been collected with the replacement meter.\n\t  raw.data$water <- NA\n\t  raw.data$water <- (raw.data$water.m3 - raw.data$water.m3[1])*1000\n\t  #after October 4th, dryweight reached a plateau.\n\t  index <- which(raw.data$time.string == ""2014-10-04 08:30:00"")\n\t  raw.data$dryweight[(index+1):nrow(raw.data)] <- NA\n\t}\n\tif(run == ""Run150909""){\n\t  #remove erroneous turbidity measurement\n\t  index <- which(raw.data$time.string == ""2015-09-17 17:40:00"")\n\t  raw.data$TURBIDITY[index] <- NA\n\t}\n  lines(raw.data$time.string, raw.data$TURBIDITY, col = ""red"", lwd = 2)\n  points(raw.data$time.string, raw.data$dryweight, col = ""red"")\n\n  # calculate for every day (midnight to midnight) some variables. Store them at the end of the day at 00:00\n  # amount of water evaporated\n  raw.data$evap.24h <- ', '#set the path to your working directory, where all .csv files must be placed\nsetwd(""~/SWITCHdrive/3_Projects/Anschubfinanzierung/Data analysis/Data for dryad"")\n#library(plyr)\nload(""all.data.RData"")\ndf <- all.data\nrm(all.data)\nnames(df)\ndf$name <- df$type\nlevels(df$name) <- gsub(""std.1"", ""mineral fertilizer (replicate 1)"", levels(df$name))\nlevels(df$name) <- gsub(""std.2"", ""mineral fertilizer (replicate 2)"", levels(df$name))\nlevels(df$name) <- gsub(""std.3"", ""mineral fertilizer (replicate 3)"", levels(df$name))\nlevels(df$name) <- gsub(""std.4"", ""mineral fertilizer (replicate 4)"", levels(df$name))\nlevels(df$name) <- gsub(""aq.fert"", ""RAS effluent with mineral fertilizer"", levels(df$name))\nlevels(df$name) <- gsub(""aq.no.n"", ""RAS effluent with mineral fertilizer without N"", levels(df$name))\nlevels(df$name) <- gsub(""aq.no.me"", ""RAS effluent with microelements"", levels(df$name))\nlevels(df$name) <- gsub(""aq.only.1"", ""RAS effluent only (replicate 1)"", levels(df$name))\nlevels(df$name) <- gsub(""aq.only.2"", ""RAS effluent only (replicate 2)"", levels(df$name))\ndf$name <- factor(df$name,levels(df$name)[c(6,7,8,9,1,3,2,4,5)])\nfor(i in 1:nlevels(df$name)){\n  index <- match(levels(df$name)[i], df$name)\n  cat(paste(""\\nRun: "",df$run[index],""\\tName: "",df$name[index], sep = """"))\n}\n\n# Plot growth of every run\npdf(file = ""Figure2.pdf"", width = 14.5 * 0.394, height = 12 * 0.394, pointsize = 10, family = ""Times"", colormodel = ""gray"")\npar(mfrow = c(3,3), mar = c(3.5,3.4,0.1,0), xpd = TRUE)\nfor (i in 1:nlevels(df$name)){\n  ssdf <- df[which(df$name == levels(df$name)[i]),]\n  cat(paste(""\\n"",ssdf$name[i],"":\\n\\tDays: "",round(max(ssdf$days),digits = 2),""\\n\\tMaximum density: "",max(ssdf$dw, na.rm = T), sep = """"))\n  #cat(paste(""\\n\\tMean daily evaporation:"",round(mean(ssdf$evap.24h/18, na.rm = T),digits = 2)))\n  #cat(paste(""\\n\\tSD daily evaporation:"",round(sd(ssdf$evap.24h/18, na.rm = T),digits = 2)))\n  #cat(paste(""\\n\\tMedian daily growth:"",round(median(ssdf$delta.dw/ssdf$time.dw, na.rm = T),digits = 2)))\n  #cat(paste(""\\n\\tPercentiles daily growth:"",round(quantile(ssdf$delta.dw/ssdf$time.dw, probs = 0.25, na.rm = T),digits = 2),"","",round(quantile(ssdf$delta.dw/ssdf$time.dw, probs = 0.75, na.rm = T),digits = 2)))\n  plot(ssdf$days,ssdf$dw, las = 1, xlim = c(0,35), ylim = c(0,30), xlab = """", ylab = """", axes = F)\n  title(ylab = expression(Biomass~(g~l^-1)), line = 2.1)\n  title(xlab = ""Time (days)"", line = 2.1)\n  model1 <- summary(lm(ssdf$dw ~ ssdf$days))\n  #clip(ssdf$days[which.min(ssdf$dw)],ssdf$days[which.max(ssdf$dw)],ssdf$dw[which.min(ssdf$dw)],ssdf$dw[which.max(ssdf$dw)])\n  #abline(model1, lty = ""dashed"")\n  #clip(0,100,0,100)\n  #parttext <- """"\n  #plottext <- bquote(.(parttext)~.(round(model1$coefficients[2], digits = 2)) ~ g~l^{-1}~d^{-1})\n  #text(12, 2, plottext, pos = 4)\n  cat(paste(""\\n\\tDaily growth:"",round(model1$coefficients[2,1],digits = 2)))\n  cat(paste(""\\n\\tDaily growth SEM:"",round(model1$coefficients[2,2],digits = 2)))\n  lines(ssdf$days,ssdf$dw.pred)\n  axis(1, at = c(0:7)*5, labels = c(0:7)*5)\n  axis(2, at = c(0:6)*5, labels = c(0:6)*5, las = 1)\n  text(2,28,paste(i,"")"",sep=""""))\n}\ndev.off()\n\nsummary.aov(aov(df$dw ~ df$days * df$name))\n\nmean(df$evap.24h/18, na.rm = T)\nsd(df$evap.24h/18, na.rm = T)\npar(mar = c(4,4,1,1))\nboxplot(df$evap.24h/18,las = 1, ylab = ""Evaporation in 24 hours"")\nquantile(df$evap.24h/18, probs = c(0.25,0.5,0.75,0.9,1), na.rm = T)\n\n#compare PEs\nboxplot(df$pe ~ df$name, las = 2, ylim = c(-0.2,0.4), yaxt = ""n"", ylab = ""Photosynthetic efficiency"", xaxt = ""n"")\naxis(1, at = c(1:9), labels = c(1:9))\naxis(2,at = c(0,0.05,0.1,0.15,0.2), labels = c(""0%"",""5%"",""10%"",""15%"",""20%""), las = 2)\nkruskal.test(df$pe ~ df$name)\nfor(i in 1:nlevels(df$name)){\n  cat(paste(""PE of run "",levels(df$name)[i],"" = "",median(df$pe[which(df$name == levels(df$name)[i])], na.rm = TRUE),""\\n"", sep = """"))\n}\n#Compare total sunlight and total biomass\npar(mar = c(0,0,0,0))\nfor(i in 1:nlevels(df$name)){\n  if(levels(df$name)[i] != ""mineral fertilizer (replicate 1)""){\n    temp <- df[which(df$name == levels(df$name)[i]),]\n    first <- min(which(!is.na(temp$dw)))\n    last <- max(which(!is.na(temp$dw)))\n    temp <- temp[first:last,]\n    plot(temp$days,temp$par.w.m2, type = ""l"")\n    totalsunkwh <- sum(temp$par.w.m2) / 6 / 1000\n    totaldw <- temp$dw[nrow(temp)] - temp$dw[1]\n    pe <- (totaldw * 200 * 23)/(totalsunkwh * 18 * 3600)\n    cat(paste(""PE of run "",levels(df$name)[i],"" = "",round(pe, digits = 3),""\\n"", sep = """"))\n  }\n}\nt.test(x = c(0.078,0.08,0.069,0.069), y = c(0.023,0.042,0.048,0.05), var.equal = T)\n\n#concentrations of no3 in RAS water\nfor (i in 6:9){\n  temp <- df[which(df$name == levels(df$name)[i]),]\n  cat(paste(""\\n"",levels(df$name)[i]))\n  cat(paste(""\\n\\t Average:"",round(mean(temp$n.inflow, na.rm = T), digits = 5)))\n  cat(paste(""\\n\\t SD:"",round(sd(temp$n.inflow, na.rm = T), digits = 5)))\n}\n\n#plot n in pbr for cultivations 6-9\npdf(file = ""Figure3.pdf"", width = 14.5*2/3 * 0.394, height = 8 * 0.394, pointsize = 10, colormodel = ""gray"")\npar(mfrow = c(2,2), mar =']",3,"High-density cultivation, microalgae, unfiltered water, recirculating aquaculture system, wastewater treatment, nutrient content, biomass densities, constant evaporation, thin-layer photobioreactor, CO2, inhibitory effect, mineral medium,"
Computer codes of Simultaneous analysis of mutations and methylations in circulating cell-free DNA for hepatocellular carcinoma detection,"""Simultaneous analysis of mutations and methylations in circulating cell-free DNA for hepatocellular carcinoma detection"", a research paper, will be published. These are compute codes about this research, including the HCC detection model and the statistical and graph generation scripts.","['##Boxplot Script ---   start   ----\r\n\r\n# load library\r\nlibrary(ggplot2)\r\nlibrary(ggbeeswarm)\r\nlibrary(scales)\r\nlibrary(ggpubr)\r\n\r\nmytheme <- theme(\r\n  panel.background = element_blank(), \r\n  panel.border = element_rect(linetype = ""solid"", colour = ""black"", fill = NA,size = 0.5), \r\n  panel.grid.minor = element_blank(),\r\n  panel.grid.major = element_blank()\r\n)\r\n\r\nsetwd(""~/Desktop"")\r\n\r\nmydf <- read.csv(\r\n  file = ""boxplot.csv"", header=T\r\n)\r\n# check columns of dataframe\r\n# mydf$sample_ori_name\r\ntable(mydf$group)\r\n# mydf[1:4,1:3]\r\n# head(mydf)\r\nclass(mydf$group)\r\n\r\n# specify order of x axis\r\nmydf$group <- factor(as.character(mydf$group), levels = c(""A"",""B"",""C"",""D""))\r\nclass(mydf$group)\r\ntable(mydf$group)\r\n\r\n#my_trans <- function() {trans_new(""log2_1p"", function(x) {log2(x + 1)}, function(x) {2 ^ x - 1})}\r\n\r\nmy_comparisons <- list(c(""A"",""B""),c(""B"",""C""),c(""C"",""D""))\r\n\r\nmytheme <- theme(\r\n  panel.background = element_blank(), \r\n  panel.border = element_rect(linetype = ""solid"", colour = ""black"", fill = NA,size = 0.5), \r\n  panel.grid.minor = element_blank(),\r\n  panel.grid.major = element_blank()\r\n)\r\n\r\nggplot(\r\n  data = mydf, # dataframe\r\n  aes(x = group, y = methylation)) +  \r\n  #geom_violin() + \r\n  geom_boxplot(outlier.shape = NA ) + \r\n  stat_boxplot(geom =\'errorbar\', width = 0.4) +\r\n  geom_quasirandom(aes(color=group),size=0.1) +\r\n  scale_color_manual(values = c(""blue"",""green"",""orange"",""red"")) +\r\n  scale_fill_manual(values = c(""blue"",""green"",""orange"",""red"")) +\r\n  labs(title="""", x=\'\', y=\'Methylation value\') + \r\n  #scale_y_continuous(trans=my_trans(), breaks=c(0,1,2,4,8,16,32,64,128,256,512,1024))+ \r\n  stat_compare_means(comparisons = my_comparisons, method=""wilcox.test"",label = ""p.signif"") +\r\n  stat_compare_means(comparisons = my_comparisons, method=""wilcox.test"") +\r\n  mytheme \r\nggsave(""hcc_boxplot.pdf"", height = 4, width = 5)\r\n\r\n\r\n##Boxplot Script ---   end   ----\r\n\r\n\r\n\r\n##Heatmap Script---   start   ----\r\n\r\n\r\nlibrary(pheatmap)\r\ndata <- read.table(\'data.xls\',header=T,row.names= 1)\r\nanno <- read.table(\'annotation.list\',header=T,row.names= 1)\r\n#ann_colors <- list(CLASS = c(class1 = ""#D95F02"", class0 = ""#189E77""))\r\nann_colors <- list(CLASS = c(class1 = ""#D95F02"", class0 = ""#6495ED""))\r\npdf(\'GCGC_heatmap.pdf\',width=12,height=8)\r\nexprTable_t <- as.data.frame(t(data))\r\nmat <- dist(exprTable_t)\r\nhclust_mat <- hclust(mat)\r\nindex <- seq(1,60, by = 1)\r\nhclust_mat$order <- index\r\npheatmap(data, cluster_cols = hclust_mat,\r\n         #color = colorRampPalette(c(""#1B9E77"", ""white"", ""#D95F02""))(60), \r\n         color = colorRampPalette(c(""#6495ED"", ""white"", ""#D95F02""))(60), \r\n         show_rownames=T,show_colnames=T,\r\n         display_numbers = FALSE,\r\n         treeheight_row = 0, treeheight_col = 0,\r\n         fontsize = 9,\r\n         cellwidth = 10, cellheight = 10,\r\n         legend_labels = c(\'0\',\'0.2\',\'0.4\',\'0.6\',\'0.8\',\'1\'),\r\n         annotation_col = anno, annotation_colors = ann_colors\r\n)\r\ndev.off()\r\n\r\n\r\n##Heatmap Script---   end   ----\r\n\r\n\r\n\r\n\r\n##Venn Script---   start   ----\r\n\r\nlibrary(tidyverse)\r\nlibrary(ggplot2)\r\nlibrary(pROC)\r\nlibrary(rstatix)\r\nlibrary(ggpubr)\r\nlibrary(ggbeeswarm)\r\nlibrary(venn)\r\nlibrary(eulerr)\r\nlibrary(VennDiagram)\r\n\r\ndata_temp <- c(""A"" = 15978,\r\n               ""B"" = 17359,\r\n               ""C"" = 17556,\r\n               ""A&B"" = 5033,\r\n               ""B&C"" = 5879,\r\n               ""A&C"" = 5171,\r\n               ""A&B&C"" = 218241)\r\n\r\npdf(file = ""/Users/wuxingyu/Desktop/venn.pdf"", height = 4.5, width = 4)\r\nplot(euler(data_temp), \r\n     fills = list(fill = c(""mintcream"", ""tan1"", ""cornflowerblue""),\r\n                  alpha = 0.8), \r\n     quantities = list(type = c(""counts""), cex = 1.5), \r\n     control = list(extraopt = FALSE), \r\n     labels = list(cex = 1.5),\r\n     #legend = list(side = ""right""), \r\n     #quantities = T\r\n)\r\ndev.off()\r\n\r\n##Venn Script---   end   ----\r\n\r\n\r\n\r\n##Correlation Script---   start   ----\r\nlibrary(bdpv)\r\nlibrary(epiR)\r\n\r\nepi.tests(data_temp, method = ""clopper-pearson"")\r\n\r\n\r\nspearman_all <- corr.test(data_1[, c(2,3)], method = ""spearman"")\r\nspearman_all\r\nspearman_all$ci\r\nspearman_all$p\r\n\r\nprint(spearman_all, short = F)\r\n##Correlation Script---   end   ----\r\n\r\n\r\n##Wilcox Test Script---   start   ----\r\nstat_01 <- data_1 %>% wilcox_test(Methylation.Score ~ Tumor.size, p.adjust.method = ""bonferroni"")\r\n\r\nwilcox.test(data_m_h$Methylation.Score, data_m_n$Methylation.Score, alternative = ""greater"")\r\n##Wilcox Test Script---   end   ----\r\n\r\n\r\n\r\n']",3,"computer codes, simultaneous analysis, mutations, methylations, circulating cell-free DNA, hepatocellular carcinoma detection, research paper, HCC detection model, statistical scripts, graph generation scripts."
Sublethal exposure to deltamethrin stimulates reproduction and has limited effects on post-hatching maternal care in the European earwig,Data set and associated R script used to obtain the statistical results presented in the corresponding study.,"['\r\n#####################################################################################\r\n########                 - R SCRIPT ASSOCIATED WITH THE PUBLICATION -\r\n########    Effects of deltamethrin on posthatching care and future reproduction\r\n########                  E MAUDIT E, LECUREUIL C & MEUNIER J                    \r\n######## \r\n#####################################################################################\r\n\r\n  library(car); library(emmeans); library(ggplot2); library(gplots); library(DHARMa); library(MASS); library(ggpubr)\r\n  Alldata <- read.delim(""Data set.txt"")\r\n  Alldata$Traitement_F<-factor(Alldata$Traitement_F)\r\n  Alldata$Traitement_F<-factor(Alldata$Traitement_F, levels = c(""Control"",""Solvent"", ""Low"", ""High""))\r\n \r\n  table(Alldata$Future_Femelle,Alldata$Traitement_F)\r\n  table(Alldata$Traitement_F)\r\n\r\n## Data description\r\n    summary(Alldata$J1_Nb_N)\r\n    sd(Alldata$J1_Nb_N, na.rm=T)/sqrt(length((na.omit(Alldata$J1_Nb_N))))\r\n    hist(Alldata$J1_Nb_N_manip, breaks=100)\r\n    \r\n\r\n#=======================================================================================================================       \r\n#=========== A. EFFECTS ON MATERNAL CARE ===============================================================================  \r\n  \r\n### A.1 Mothers close to the brood -----         \r\n          Alldata$F.POS<-ifelse(Alldata$J7_Femelle_position>=3, 0, 1)\r\n          table(Alldata$F.POS, Alldata$Traitement_F)\r\n          model.Pokeb<-glm(F.POS ~ Traitement_F, family=binomial(cloglog), data=Alldata)\r\n          simulationOutput <- simulateResiduals(fittedModel = model.Pokeb, plot = T)\r\n          testResiduals(model.Pokeb)\r\n          Anova(model.Pokeb) # Traitement_F, p = 0.06476 .\r\n\r\n### A.2 Maternal defense of the brood against predator attack -----         \r\n          Alldata$F.POS<-ifelse(Alldata$J7_Femelle_position>=3, ""Away"", ""Close"")\r\n          table(Alldata$F.POS, Alldata$Traitement_F)\r\n          Alldata.closeE<-subset(Alldata, F.POS==""Close"")\r\n          hist(Alldata.closeE$J7_Nb_poke)\r\n          model.Poke<-lm(log(J7_Nb_poke) ~ Traitement_F, data=Alldata.closeE)\r\n          simulationOutput <- simulateResiduals(fittedModel = model.Poke, plot = T)\r\n          testResiduals(model.Poke)\r\n          Anova(model.Poke) # Traitement_F, p = 0.005287 **\r\n          emmeans(model.Poke, pairwise~Traitement_F)  # => S = L > H \r\n\r\n### A.3 Occurence of maternal contacts to the nymphs -----       \r\n          Alldata$MO.Interactions<-(Alldata$J8_Time_Grooming+Alldata$J8_Time_Tropha); hist(Alldata$MO.Interactions)\r\n          Alldata$CONTACTS<-ifelse(Alldata$MO.Interactions>=1,1,0); hist(Alldata$CONTACTS)\r\n          table(Alldata$CONTACTS, Alldata$Traitement_F)\r\n          model.Contact.Bin<-glm(CONTACTS ~ Traitement_F, data=Alldata, family=binomial(cloglog))\r\n          simulationOutput <- simulateResiduals(fittedModel = model.Contact.Bin, plot = T)\r\n          testResiduals(model.Contact.Bin)\r\n          Anova(model.Contact.Bin) # Traitement_F, p = 0.4466\r\n\r\n### A.4 Total duration of maternal contacts to the nymphs (when present) ----- \r\n          Alldata.C<-subset(Alldata, CONTACTS==1)\r\n          table(Alldata.C$CONTACTS, Alldata.C$Traitement_F)\r\n          hist(Alldata.C$MO.Interactions)\r\n          model.Contact<-lm(log(MO.Interactions) ~ Traitement_F, data=Alldata.C)\r\n          simulationOutput <- simulateResiduals(fittedModel = model.Contact, plot = T)\r\n          testResiduals(model.Contact)\r\n          Anova(model.Contact) # Traitement_F, p = 0.7616       \r\n\r\n### A. FIGURES -----\r\n\r\n          Alldata$EXPO.FIG<-factor(as.factor(ifelse(Alldata$Traitement_F==""Solvent"",""0"",ifelse(Alldata$Traitement_F==""High"", ""68.750"", ""6.875""))),levels = c(""0"", ""6.875"",""68.750""))\r\n          Alldata.closeE$EXPO.FIG<-factor(as.factor(ifelse(Alldata.closeE$Traitement_F==""Solvent"",""0"",ifelse(Alldata.closeE$Traitement_F==""High"", ""68.750"", ""6.875""))),levels = c(""0"", ""6.875"",""68.750""))\r\n          Alldata.C$EXPO.FIG<-factor(as.factor(ifelse(Alldata.C$Traitement_F==""Solvent"",""0"",ifelse(Alldata.C$Traitement_F==""High"", ""68.750"", ""6.875""))),levels = c(""0"", ""6.875"",""68.750""))\r\n          \r\n          Log.poke<-log(Alldata.closeE$J7_Nb_poke)\r\n          Fig.Poke<-ggboxplot(Alldata.closeE, x = ""EXPO.FIG"", y =""Log.poke"", color = ""EXPO.FIG"", palette =c(""#357AB7"",""#FFCB60"",""#BF3030""), add = ""jitter"", xlab=""Deltamethrin (ng/cm2)"", ylab=""No. pokes (log)"", shape=""EXPO.FIG"", outlier.shape = NA)+\r\n            theme_classic()+ rremove(""legend"")+\r\n            annotate(""text"", x = 1.1, y = 3.5, label = ""a"", size=5)+ annotate(""text"", x = 2.1, y = 3.5, label = ""a"", size=5)+ annotate(""text"", x = 3.1, y = 3.5, label = ""b"", size=5)\r\n          \r\n          TT<-table(Alldata$F.POS, Alldata$EXPO.FIG)\r\n          DAT.TT<-data.frame(Prop.F=c(TT[2,]/(TT[1,]+TT[2,])),Treat=c(""0"", ""6.875"",""68.750""))\r\n          DAT.TT$Treat <- factor(DAT.TT$Treat, levels = c(""0"", ""6.875"",""68.750""))\r\n          Fig.position<-ggplot(data = DAT.TT, aes(x = Treat, y = Prop.F*100, fill=Treat))+\r\n     ']",3,"- sublethal exposure
- deltamethrin 
- reproduction
- post-hatching maternal care
- European earwig
- statistical results
- data set
- R script
- study"
Strong decreases in genetic diversity despite high gene flow for a solitary bee,"R code and data files used to generate the statistical results and figures in Suni & Hernandez 2022, submitted to Conservation Genetics","['# Code used to produce figures and statistical results from Suni & Hernandez\n\nlibrary(lme4)\nlibrary(sjPlot)\nlibrary(ggplot2)\n\nsetwd(""~/Dropbox/allDocuments/GenCoStruct/suni&hernandez"")\n\n# Hypothesis 1: genetic structure will be higher between sites separated by deforested landscapes\n\n### Hypothesis 1: genetic structure is higher between sites separated by deforested landscapes\n# MMRR using geo dist matrix, gen dist matrix, and forest cover matrix\n# code from Wang 2013 Evolution http://datadryad.org/bitstream/handle/10255/dryad.48462/MMRR.R?sequence=1\n# Testing independent association btwn genetic distances and geo dist or forest cover\n# Matrices are (1) gen distance (2) geo dist (3) forest cover\n# Y is a dependent distance matrix\n# X is a list of independent distance matrices (with optional names)\n# Y is matrix of haploid genetic distances HGD\n# X1 is overland geo dist\n# X2 specifies % forest cover\n# The read.matrix function requires {tseries} package to be installed and loaded.\n\n# Do for broken-stick and euclidian distances\n# 2009 doesn\'t have enough comparisons\n# This uses sites that have 10 or more samples\n\nrequire(tseries)\n\n# defining the MMRR function\nMMRR<-function(Y,X,nperm=999){\n  #compute regression coefficients and test statistics\n  nrowsY<-nrow(Y)\n  y<-unfold(Y)\n  if(is.null(names(X)))names(X)<-paste(""X"",1:length(X),sep="""")\n  Xmats<-sapply(X,unfold)\n  fit<-lm(y~Xmats)\n  coeffs<-fit$coefficients\n  summ<-summary(fit)\n  r.squared<-summ$r.squared\n  tstat<-summ$coefficients[,""t value""]\n  Fstat<-summ$fstatistic[1]\n  tprob<-rep(1,length(tstat))\n  Fprob<-1\n  \n  #perform permutations\n  for(i in 1:nperm){\n    rand<-sample(1:nrowsY)\n    Yperm<-Y[rand,rand]\n    yperm<-unfold(Yperm)\n    fit<-lm(yperm~Xmats)\n    summ<-summary(fit)\n    Fprob<-Fprob+as.numeric(summ$fstatistic[1]>=Fstat)\n    tprob<-tprob+as.numeric(abs(summ$coefficients[,""t value""])>=abs(tstat))\n  }\n  \n  #return values\n  tp<-tprob/(nperm+1)\n  Fp<-Fprob/(nperm+1)\n  names(r.squared)<-""r.squared""\n  names(coeffs)<-c(""Intercept"",names(X))\n  names(tstat)<-paste(c(""Intercept"",names(X)),""(t)"",sep="""")\n  names(tp)<-paste(c(""Intercept"",names(X)),""(p)"",sep="""")\n  names(Fstat)<-""F-statistic""\n  names(Fp)<-""F p-value""\n  return(list(r.squared=r.squared,\n              coefficients=coeffs,\n              tstatistic=tstat,\n              tpvalue=tp,\n              Fstatistic=Fstat,\n              Fpvalue=Fp))\n}\n\n# unfold converts the lower diagonal elements of a matrix into a vector\n# unfold is called by MMRR\n\nunfold<-function(X){\n  x<-vector()\n  for(i in 2:nrow(X)) x<-c(x,X[i,1:i-1])\n  return(x)\n}\n\n# Read in the matrices\n\ngenMat<-read.matrix(""championi_2010_genMat.csv"",header=FALSE, sep="","")\ngeoMat<-read.matrix(""championi_2010_geoDist_euclidian.csv"",header=FALSE, sep="","")\nforest<-read.matrix(""championi_2010_forest_euclidian.csv"",header=FALSE,sep="","")\n\n# Standardize each matrix\ngenMat <- (genMat-mean(genMat))/sd(genMat)\ngeoMat <- (geoMat-mean(geoMat))/sd(geoMat)\nforest<- (forest-mean(forest))/sd(forest)\n\nmantel.partial(genMat, geoMat, forest, permutations = 100000)\n\n# Make a list of the explanatory (X) matrices.\n# Names are optional.  Order doesn\'t matter.\n# Can include more than two matrices, if desired.\n# Xmats<-list(geoDist=geoMat)\nXmats<-list(geoDist=geoMat,forDist=forest)\n\n# Run MMRR function using genMat as the response variable and Xmats as the explanatory variables.\n# nperm does not need to be specified, default is nperm=999)\nMMRR(genMat,Xmats,nperm=100000)\n\n# $r.squared\n# r.squared \n# 0.4851949 \n\n# $coefficients\n\n# Intercept    geoDist    forDist \n# 0.8300779 -0.1501772 -0.7360781 \n\n# $tstatistic\n# Intercept(t)   geoDist(t)   forDist(t) \n# 2.1345982   -0.3254996   -1.2894355 \n\n# $tpvalue\n# Intercept(p)   geoDist(p)   forDist(p) \n# 0.1250387    0.8331717    0.3753262 \n\n# $Fstatistic\n# F-statistic \n# 1.413724 \n\n# $Fpvalue\n# F p-value \n# 0.3753262 \n\n# Using broken stick distance\ngeoMat<-read.matrix(""championi_2010_geo_bs.csv"",header=FALSE, sep="","")\nforest<-read.matrix(""championi_2010_forest_bs.csv"",header=FALSE,sep="","")\n\ngeoMat <- (geoMat-mean(geoMat))/sd(geoMat)\nforest<- (forest-mean(forest))/sd(forest)\n\nXmats<-list(geoDist=geoMat,forDist=forest)\n\nMMRR(genMat,Xmats,nperm=10000)\n\n# $r.squared\n# r.squared \n# 0.8077765 \n\n# $coefficients\n# Intercept    geoDist    forDist \n# -0.5833206 -0.3243602  2.1283222 \n\n# $tstatistic\n# Intercept(t)   geoDist(t)   forDist(t) \n# -1.452533    -1.211345     3.375055 \n\n# $tpvalue  \n# Intercept(p)   geoDist(p)   forDist(p) \n# 0.29147709   0.28992710   0.04149959 \n\n# $Fstatistic\n# F-statistic \n# 6.303415 \n\n# $Fpvalue\n# F p-value \n# 0.08255917 \n\n# forDist is signinficant, but the F p-value is not. \n\n\n# 2017 doesn\'t have enough comparisons with big enough sample sizes\n\n# 2018 Euclidian\n\n# Using broken stick distance\ngenMat<-read.matrix(""championi_2018_genMat.csv"", header=FALSE, sep="","")\ngeoMat<-read.matrix(""championi_2018_geo_euclidian.csv"",header=FALSE, sep="","")\nforest<-read.matrix(""championi_2018_forest_euclidian.csv"",header=FALSE,sep=']",3,"keywords: genetic diversity, gene flow, solitary bee, R code, data files, statistical results, figures, Suni, Hernandez, Conservation Genetics."
"Statistical Analysis Models for ""Conservation Opportunities and Challenges in Brazil's Roadless and Railroad-Less Areas""","Input Spatial and supporting data and R Code to run the Statistical Analyses for Conservation Opportunities and Challenges in Brazils Roadless and Railroad-Less AreasInput data include raster (.tiff), shapefiles (.shp), and delimited text files (csv), as well as organized R-scripts (originally written and run in the R-Studio environment). All program libraries are included in a ""Library List"" at each section of each sub-model script.***Read all instructions for remodeling included in the .txt files.***","['### Basic Data Prep ###\r\n\r\n# LIBRARIES #\r\nlibrary(here)\r\n# library(rgdal)\r\n# library(sp)\r\n# library(sf)\r\n#library(raster)\r\nlibrary(terra)\r\n\r\n# library(jtools)\r\n# library(stargazer)\r\n# library(expss)\r\n# library(nnet)\r\nlibrary(tidyverse)\r\n\r\nwd <- here()\r\nsetwd(wd)\r\nwd\r\n\r\nBrazil_Biomes <- vect(""C:/Spatial Data/Final RLRL Data/Review Remodel/z_BrazilLandcover/Brazil_Biomes_StudyArea.shp"")\r\nBrazil_ExtentPoly <- vect(""C:/Spatial Data/Final RLRL Data/Review Remodel/z_BrazilLandcover/Brazil_ExtentPoly.shp"")\r\nview(as.data.frame(Brazil_Biomes))\r\n\r\ncrs(Brazil_Biomes)\r\n\r\n\r\n### Amazon ###\r\nAmazon <- Brazil_Biomes[1,]\r\nplot(Amazon)\r\nAmazon_ext <- ext(Amazon)\r\nAmazon_ExtentPoly <- as.polygons(Amazon_ext)\r\nis(Amazon)\r\nis(Amazon_ExtentPoly)\r\ncrs(Amazon_ExtentPoly) <- Brazil_Biomes\r\nwriteVector(Amazon, filename=""Amazon/Amazon_Study_Area.shp"", filetype=\'ESRI Shapefile\', overwrite=TRUE)\r\nwriteVector(Amazon_ExtentPoly, filename=""Amazon/Amazon_ExtentPoly.shp"", filetype=\'ESRI Shapefile\', overwrite=TRUE)\r\n\r\n\r\n\r\n### AtlanticForest ###\r\nAtlanticForest <- Brazil_Biomes[2,]\r\nplot(AtlanticForest)\r\nAtlanticForest_ext <- ext(AtlanticForest)\r\nAtlanticForest_ExtentPoly <- as.polygons(AtlanticForest_ext)\r\nis(AtlanticForest)\r\nis(AtlanticForest_ExtentPoly)\r\ncrs(AtlanticForest_ExtentPoly) <- Brazil_Biomes\r\nwriteVector(AtlanticForest, filename=""AtlanticForest/AtlanticForest_Study_Area.shp"", filetype=\'ESRI Shapefile\', overwrite=TRUE)\r\nwriteVector(AtlanticForest_ExtentPoly, filename=""AtlanticForest/AtlanticForest_ExtentPoly.shp"", filetype=\'ESRI Shapefile\', overwrite=TRUE)\r\n\r\n\r\n\r\n### Caatinga ###\r\nCaatinga <- Brazil_Biomes[3,]\r\nplot(Caatinga)\r\nCaatinga_ext <- ext(Caatinga)\r\nCaatinga_ExtentPoly <- as.polygons(Caatinga_ext)\r\nis(Caatinga)\r\nis(Caatinga_ExtentPoly)\r\ncrs(Caatinga_ExtentPoly) <- Brazil_Biomes\r\nwriteVector(Caatinga, filename=""Caatinga/Caatinga_Study_Area.shp"", filetype=\'ESRI Shapefile\', overwrite=TRUE)\r\nwriteVector(Caatinga_ExtentPoly, filename=""Caatinga/Caatinga_ExtentPoly.shp"", filetype=\'ESRI Shapefile\', overwrite=TRUE)\r\n\r\n\r\n\r\n### Cerrado ###\r\nCerrado <- Brazil_Biomes[4,]\r\nplot(Cerrado)\r\nCerrado_ext <- ext(Cerrado)\r\nCerrado_ExtentPoly <- as.polygons(Cerrado_ext)\r\nis(Cerrado)\r\nis(Cerrado_ExtentPoly)\r\ncrs(Cerrado_ExtentPoly) <- Brazil_Biomes\r\nwriteVector(Cerrado, filename=""Cerrado/Cerrado_Study_Area.shp"", filetype=\'ESRI Shapefile\', overwrite=TRUE)\r\nwriteVector(Cerrado_ExtentPoly, filename=""Cerrado/Cerrado_ExtentPoly.shp"", filetype=\'ESRI Shapefile\', overwrite=TRUE)\r\n\r\n\r\n\r\n### Pampas ###\r\nPampas <- Brazil_Biomes[5,]\r\nplot(Pampas)\r\nPampas_ext <- ext(Pampas)\r\nPampas_ExtentPoly <- as.polygons(Pampas_ext)\r\nis(Pampas)\r\nis(Pampas_ExtentPoly)\r\ncrs(Pampas_ExtentPoly) <- Brazil_Biomes\r\nwriteVector(Pampas, filename=""Pampas/Pampas_Study_Area.shp"", filetype=\'ESRI Shapefile\', overwrite=TRUE)\r\nwriteVector(Pampas_ExtentPoly, filename=""Pampas/Pampas_ExtentPoly.shp"", filetype=\'ESRI Shapefile\', overwrite=TRUE)\r\n\r\n\r\n\r\n### Pantanal ###\r\nPantanal <- Brazil_Biomes[6,]\r\nplot(Pantanal)\r\nPantanal_ext <- ext(Pantanal)\r\nPantanal_ExtentPoly <- as.polygons(Pantanal_ext)\r\nis(Pantanal)\r\nis(Pantanal_ExtentPoly)\r\ncrs(Pantanal_ExtentPoly) <- Brazil_Biomes\r\nwriteVector(Pantanal, filename=""Pantanal/Pantanal_Study_Area.shp"", filetype=\'ESRI Shapefile\', overwrite=TRUE)\r\nwriteVector(Pantanal_ExtentPoly, filename=""Pantanal/Pantanal_ExtentPoly.shp"", filetype=\'ESRI Shapefile\', overwrite=TRUE)\r\n', '# SET WD #\r\n\r\n### BUREAUCRACY !!! ###\r\nlibrary(here)\r\nhere()\r\nwd <- here()\r\nsetwd(wd)\r\n\r\n# wd <- setwd(""C:/Spatial Data/Final RLRL Data/Review Remodel/z_BrazilProtectedAreas/"")\r\n\r\n# LIBRARIES #\r\n\r\nlibrary(rgdal)\r\nlibrary(sp)\r\nlibrary(sf)\r\nlibrary(raster)\r\nlibrary(terra)\r\nlibrary(tidyverse)\r\n\r\n\r\nBrazil_Biomes <- vect(""z_BrazilLandcover/Brazil_Biomes_StudyArea.shp"")\r\nBrazil_ExtentPoly <- vect(""z_BrazilLandcover/Brazil_ExtentPoly.shp"")\r\n\r\n#_______# 1 - LPA Types #_______#\r\n\r\nBrazil_LPAsID_V1 <- rast(""C:/Spatial Data/Final RLRL Data/Review Remodel/z_BrazilProtectedAreas/Brazil_LPAType_ID_V2.tif"")\r\n# plot(Brazil_LPAsID_V1)\r\n\r\n\r\nBrazil_LPAsID_V2 <- crop(Brazil_LPAsID_V1, Brazil_ExtentPoly, snap=""near"", overwrite=TRUE, filename = ""z_Temp/Brazil_LPAsID_V2.tif"", datatype=\'INT1U\', filetype=\'GTiff\')\r\nBrazil_LPAsID_V3 <- extend(Brazil_LPAsID_V2, Brazil_ExtentPoly)\r\n\r\n\r\next(Brazil_LPAsID_V2)\r\next(Brazil_LPAsID_V3)\r\n\r\n\r\nBrazil_LPAsID_V4 <- mask(Brazil_LPAsID_V3, Brazil_Biomes, updatevalue=NA, touches=TRUE)\r\next(Brazil_LPAsID_V2)\r\next(Brazil_LPAsID_V3)\r\next(Brazil_LPAsID_V4)\r\n\r\nplot(Brazil_LPAsID_V4)\r\nplot(Brazil_Biomes, add = TRUE, col=\'transparent\')\r\nplot(Brazil_ExtentPoly, add = TRUE, col=\'transparent\')\r\n\r\nwriteRaster(Brazil_LPAsID_V4, overwrite=TRUE, filename = ""C:/Spatial Data/Final RLRL Data/Review Remodel/z_BrazilProtectedAreas/Brazil_All_LPAsIDs.tif"", datatype=\'INT1U\', filetype=\'GTiff\')\r\nBrazil_All_LPAsIDs <- Brazil_LPAsID_V4\r\n\r\n\r\nrm(Brazil_LPAsID_V1)\r\nrm(Brazil_LPAsID_V2)\r\nrm(Brazil_LPAsID_V3)\r\nrm(Brazil_LPAsID_V4)\r\ngc()\r\n\r\next(Brazil_All_LPAsIDs)\r\n\r\n\r\n\r\n\r\n\r\n\r\n# \r\n# \r\n# \r\n# #_______# 2 - Protection Types #_______#\r\n# \r\n# Brazil_ProtectTypeID_V1 <- rast(""C:/Spatial Data/Final RLRL Data/Review Remodel/z_BrazilProtectedAreas/ProtectType_ID_V2.tif"")\r\n# # plot(Brazil_LPAsID_V1)\r\n# \r\n# Brazil_ProtectTypeID_V2 <- crop(Brazil_ProtectTypeID_V1, Brazil_ExtentPoly, snap=""near"", overwrite=TRUE, filename = ""z_Temp/Brazil_ProtectTypeID_V2.tif"", datatype=\'INT1U\', filetype=\'GTiff\')\r\n# Brazil_ProtectTypeID_V3 <- extend(Brazil_ProtectTypeID_V2, Brazil_ExtentPoly)\r\n# \r\n# \r\n# ext(Brazil_ProtectTypeID_V2)\r\n# ext(Brazil_ProtectTypeID_V3)\r\n# \r\n# \r\n# Brazil_ProtectTypeID_V4 <- mask(Brazil_ProtectTypeID_V3, Brazil_Biomes, updatevalue=NA, touches=TRUE)\r\n# ext(Brazil_ProtectTypeID_V2)\r\n# ext(Brazil_ProtectTypeID_V3)\r\n# ext(Brazil_ProtectTypeID_V4)\r\n# \r\n# plot(Brazil_ProtectTypeID_V4)\r\n# plot(Brazil_Biomes, add = TRUE, col=\'transparent\')\r\n# plot(Brazil_ExtentPoly, add = TRUE, col=\'transparent\')\r\n# \r\n# writeRaster(Brazil_ProtectTypeID_V4, overwrite=TRUE, filename = ""C:/Spatial Data/Final RLRL Data/Review Remodel/z_BrazilProtectedAreas/Brazil_All_ProtectTypeIDs.tif"", datatype=\'INT1U\', filetype=\'GTiff\')\r\n# Brazil_All_ProtectTypeIDs <- Brazil_ProtectTypeID_V4\r\n# \r\n# \r\n# rm(Brazil_ProtectTypeID_V1)\r\n# rm(Brazil_ProtectTypeID_V2)\r\n# rm(Brazil_ProtectTypeID_V3)\r\n# rm(Brazil_ProtectTypeID_V4)\r\n# gc()\r\n# \r\n# \r\n# \r\n# ext(Brazil_All_ProtectTypeIDs)\r\n# \r\n# \r\n# \r\n# \r\n# \r\n# #_______# 3 - Protection Status #_______#\r\n# \r\n# Brazil_ProtectStatusID_V1 <- rast(""C:/Spatial Data/Final RLRL Data/Review Remodel/z_BrazilProtectedAreas/ProtectStatus_ID_V2.tif"")\r\n# # plot(Brazil_LPAsID_V1)\r\n# \r\n# Brazil_ProtectStatusID_V2 <- crop(Brazil_ProtectStatusID_V1, Brazil_ExtentPoly, snap=""near"", overwrite=TRUE, filename = ""z_Temp/Brazil_ProtectStatusID_V2.tif"", datatype=\'INT1U\', filetype=\'GTiff\')\r\n# Brazil_ProtectStatusID_V3 <- extend(Brazil_ProtectStatusID_V2, Brazil_ExtentPoly)\r\n# \r\n# \r\n# ext(Brazil_ProtectStatusID_V2)\r\n# ext(Brazil_ProtectStatusID_V3)\r\n# \r\n# \r\n# Brazil_ProtectStatusID_V4 <- mask(Brazil_ProtectStatusID_V3, Brazil_Biomes, updatevalue=NA, touches=TRUE)\r\n# ext(Brazil_ProtectStatusID_V2)\r\n# ext(Brazil_ProtectStatusID_V3)\r\n# ext(Brazil_ProtectStatusID_V4)\r\n# \r\n# plot(Brazil_ProtectStatusID_V4)\r\n# plot(Brazil_Biomes, add = TRUE, col=\'transparent\')\r\n# plot(Brazil_ExtentPoly, add = TRUE, col=\'transparent\')\r\n# \r\n# writeRaster(Brazil_ProtectStatusID_V4, overwrite=TRUE, filename = ""C:/Spatial Data/Final RLRL Data/Review Remodel/z_BrazilProtectedAreas/Brazil_All_ProtectStatusIDs.tif"", datatype=\'INT1U\', filetype=\'GTiff\')\r\n# Brazil_All_ProtectStatusIDs <- Brazil_ProtectStatusID_V4\r\n# \r\n# \r\n# rm(Brazil_ProtectStatusID_V1)\r\n# rm(Brazil_ProtectStatusID_V2)\r\n# rm(Brazil_ProtectStatusID_V3)\r\n# rm(Brazil_ProtectStatusID_V4)\r\n# gc()\r\n# \r\n# \r\n# \r\n# ext(Brazil_All_ProtectStatusIDs)\r\n# \r\n', '\r\n\r\n\r\n#_#_#_#_#_#_#_# FOR GOOD HOUSE KEEPING, YOU CAN RESTART YOUR R SESSSION BEFORE CONTINUING #_#_#_#_#_#_#\r\n\r\n### BUREAUCRACY !!! ###\r\nlibrary(here)\r\nwd <- here()\r\nsetwd(wd)\r\n\r\n\r\n### WARNING - THERE IS PARALLEL PROCESSING AHEAD SUBTRACT 1 FROM\r\n### THE NUMBER OF CORES ON YOUR COMPUTER BEFORE PROCEEDING, OTHERWISE\r\n### YOUR PC MAY STALL ONCE YOU ARRIVE AT THE PARALLEL OPPERATIONS\r\nnumCores <- 7\r\n\r\n\r\n\r\n# LIBRARIES #\r\n\r\nlibrary(readr)\r\nlibrary(beepr)\r\n\r\nlibrary(rgdal)\r\nlibrary(sp)\r\nlibrary(sf)\r\nlibrary(raster)\r\nlibrary(terra)\r\n\r\n\r\n\r\n\r\n#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_#_# \r\n\r\n\r\nBrazil_ImafloraLandTenure <- rast(""C:/Spatial Data/Final RLRL Data/Review Remodel/z_BrazilTenure/Brazil_ImafloraLandTenure_Final.tif"")\r\nBrazil_All_LPAsIDs <- rast(""C:/Spatial Data/Final RLRL Data/Review Remodel/z_BrazilProtectedAreas/Brazil_All_LPAsIDs.tif"")\r\n\r\n\r\nplot(Brazil_ImafloraLandTenure)\r\nplot(Brazil_All_LPAsIDs)\r\n\r\n\r\n\r\n\r\n\r\nMB3_Anthropic_2 <- app(MB3_Anthropic, fun=function(x){x[x==1] <- 2; return(x)})\r\nMB3_NatVeg_3 <- app(MB3_NatVeg, fun=function(x){x[x==1] <- 3; return(x)})\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n#______________# STEP 2 - ADDING LAND USE AND LAND COVER DATA\r\n#______________# TO THE RLRL DATA\r\n\r\n\r\n#_#_# Pulling Only the Variables Needed #_#_#\r\n\r\nstart <- proc.time()\r\nPampas_Test_w_Landcover <- read_csv(""z_PampasStatsdata/Pampas_Test_w_LandcoverTenure.csv"", col_names = TRUE)\r\n\r\n\r\n\r\n\r\n\r\n#_________________ Step 9 _________________#\r\n# WRITING RASTER GeoTIFFs for: \r\n# Final RLRL Distance (RLRL_DIST)\r\n# ""Dominant"" Infrastructure (RLRL_Infra)\r\n# Combination of ""Dominant"" Infrastructure (RLRL_Combo)\r\n\r\nPampas_RLRLDist_DF4Rast <- as.data.frame(Pampas_Test_Final[c(""x"", ""y"", ""RLRL_Dist"")])\r\nPampas_RLRLDist_Raster <- rasterFromXYZ(Pampas_RLRLDist_DF4Rast)\r\nrm(Pampas_RLRLDist_DF4Rast)\r\ngc()\r\n']",3,"Statistical Analysis Models, Conservation Opportunities, Challenges, Brazil, Roadless, Railroad-Less Areas, Spatial data, R Code, R-scripts, R-Studio, raster, .tiff, shapefiles, .shp, delimited text files,"
Code_vanAert_vanAssen16,This R code is used for analytically approximating the statistical properties of the snapshot Bayesian hybrid meta-analysis.,"['#################################################################################\r\n##### ANALYTICALLY APPROXIMATE THE PERFORMANCE OF SNAP SHOT BAYESIAN HYBRID #####\r\n##### METHOD                                                                #####\r\n#################################################################################\r\n\r\nrm(list = ls()) ### Clean workspace\r\n\r\n################\r\n### PACKAGES ###\r\n################\r\n\r\nlibrary(parallel)\r\n\r\n#################\r\n### FUNCTIONS ###\r\n#################\r\n\r\n### Function for transforming correlations to Fisher Z values and vice versa\r\nfis.trans <- function(r, fis) {\r\n  if (!missing(""r"")) {\r\n    out <- 0.5*log((1+r)/(1-r))\r\n  }\r\n  \r\n  if (!missing(""fis"")) {\r\n    out <- (exp(2*fis) - 1)/(exp(2*fis) + 1)\r\n  }\r\n  return(out)\r\n}\r\n\r\n### Function for snap shot Bayesian hybrid method\r\nhybrid.snap <- function(yi, vi, alpha) {\r\n  \r\n  ycvi <- qnorm(alpha, lower.tail = FALSE)*sqrt(vi) # Critical value\r\n  \r\n  ### Transform small, medium, and large effect size to Fisher-z\r\n  fis.sm <- fis.trans(r = 0.1)\r\n  fis.me <- fis.trans(r = 0.3)\r\n  fis.la <- fis.trans(r = 0.5)\r\n  \r\n  ### Compute probabilities\r\n  f.0 <- (dnorm(yi[1],0,sqrt(vi[1]))/pnorm(ycvi[1],0,sqrt(vi[1]),\r\n                                           lower.tail=FALSE))*dnorm(yi[2],0,sqrt(vi[2]))\r\n  f.sm <- (dnorm(yi[1],fis.sm,sqrt(vi[1]))/pnorm(ycvi[1],fis.sm,sqrt(vi[1]),\r\n                                                 lower.tail=FALSE)) * dnorm(yi[2],fis.sm,sqrt(vi[2]))\r\n  f.me <- (dnorm(yi[1],fis.me,sqrt(vi[1]))/pnorm(ycvi[1],fis.me,sqrt(vi[1]),\r\n                                                 lower.tail=FALSE)) * dnorm(yi[2],fis.me,sqrt(vi[2]))\r\n  f.la <- (dnorm(yi[1],fis.la,sqrt(vi[1]))/pnorm(ycvi[1],fis.la,sqrt(vi[1]),\r\n                                                 lower.tail=FALSE)) * dnorm(yi[2],fis.la,sqrt(vi[2]))\r\n  \r\n  ## Posterior probabilities of hypotheses\r\n  p.0 <- f.0/(f.0+f.sm+f.me+f.la)\r\n  p.sm <- f.sm/(f.0+f.sm+f.me+f.la)\r\n  p.me <- f.me/(f.0+f.sm+f.me+f.la)\r\n  p.la <- f.la/(f.0+f.sm+f.me+f.la)\r\n  \r\n  return(data.frame(p.0 = p.0, p.sm = p.sm, p.me = p.me, p.la = p.la))\r\n  \r\n}\r\n\r\n### Function for Bayesian uncorrected snap shot\r\nuncor.snap <- function(yi, vi) {\r\n  \r\n  ### Transform small, medium, and large effect size to Fisher-z\r\n  fis.sm <- fis.trans(r = 0.1)\r\n  fis.me <- fis.trans(r = 0.3)\r\n  fis.la <- fis.trans(r = 0.5)\r\n  \r\n  ### Compute probabilities\r\n  f.0 <- dnorm(yi[1],0,sqrt(vi[1]))*dnorm(yi[2],0,sqrt(vi[2]))\r\n  f.sm <- dnorm(yi[1],fis.sm,sqrt(vi[1]))*dnorm(yi[2],fis.sm,sqrt(vi[2]))\r\n  f.me <- dnorm(yi[1],fis.me,sqrt(vi[1]))*dnorm(yi[2],fis.me,sqrt(vi[2]))\r\n  f.la <- dnorm(yi[1],fis.la,sqrt(vi[1]))*dnorm(yi[2],fis.la,sqrt(vi[2]))\r\n  \r\n  ## Posterior probabilities of hypotheses\r\n  p.0 <- f.0/(f.0+f.sm+f.me+f.la)\r\n  p.sm <- f.sm/(f.0+f.sm+f.me+f.la)\r\n  p.me <- f.me/(f.0+f.sm+f.me+f.la)\r\n  p.la <- f.la/(f.0+f.sm+f.me+f.la)\r\n  \r\n  return(data.frame(p.0 = p.0, p.sm = p.sm, p.me = p.me, p.la = p.la))\r\n  \r\n}\r\n\r\n### Function for fixed-effect meta-analysis\r\nfix <- function(yi, vi) {\r\n  wi <- 1/vi # Weight per study\r\n  est.fis <- sum(yi*wi)/sum(wi) # FE meta-analytic estimate (Fisher scale)\r\n  est <- fis.trans(fis = est.fis) # FE meta-analytic estimate (correlation)\r\n  \r\n  return(est)\r\n}\r\n\r\n##################\r\n### CONDITIONS ###\r\n##################\r\n\r\nalpha <- .025\r\nnis <- c(31, 55, 96, 300, 1000, 10000) # Sample sizes\r\nmus <- c(0, 0.1, 0.3, 0.5)\r\ncond <- expand.grid(nis = nis, mus = mus) # Create grid with all conditions\r\nnworkers <- detectCores()\r\npoints <- 5001\r\nsplits <- 250 # Number of times datasets are splitted\r\nsteps <- seq(0, (points-1)*(points-1), (points-1)*(points-1)/splits) # Steps for creating datasets\r\n\r\n### Save parts of the dataset for the analyses to avoid memory problems\r\nfor (ni in nis) {\r\n\r\n  sei <- 1/sqrt(ni-3) # Standard error\r\n\r\n  for (mu in mus) {\r\n\r\n    pcv <- pnorm(qnorm(1-alpha)*sei, mean = fis.trans(r = mu), sd = sei, lower.tail = FALSE) # Probability of a significant study given mu and sei\r\n    ps.o <- seq(pcv/points, pcv/points*(points-1), pcv/points) # Vector of possible p-vales for original study (all p-values < 0.025)\r\n    ps.r <- seq(1/points, 1/points*(points-1), 1/points) # Vector of possible p-values for replication (p-values between 0 and 1)\r\n    tmp.grid <- expand.grid(ps.o = ps.o, ps.r = ps.r) # Create grid with all possible combinations of p-values\r\n\r\n    for (s in 1:(length(steps)-1)) {\r\n\r\n      grid <- tmp.grid[(1+steps[s]):(steps[s+1]), ]\r\n      dump(""grid"", file = paste(""grid_"", s, ""_"", mu, ""_"", ni, "".txt"", sep = """"))\r\n\r\n    }\r\n  }\r\n}\r\n\r\n#########################\r\n### START SIMULATIONS ###\r\n#########################\r\n\r\ndo.sim <- function(pos, cond, points, alpha, steps, splits) {\r\n  \r\n  ### Select condition\r\n  ni <- cond$nis[pos]\r\n  mu <- cond$mus[pos]\r\n  \r\n  sei <- 1/sqrt(ni-3) # Standard error\r\n  \r\n  ### Load datasets\r\n  for (s in 1:splits) {\r\n    \r\n    source(paste(""grid_"", s, ""_"", mu, ""_"", ni, "".txt"", sep = """"))\r\n    \r\n    fis.o <- qnorm(grid$ps.o,']",3,"Keywords: R code, analytically, approximating, statistical properties, snapshot, Bayesian, hybrid meta-analysis."
Meta-analysis of multiple ion uptake kinetics in crop roots,"This dataset collates measures of Michaelis-Menten root uptake parameters for several nutrients based on reported literature. The analysis appears in a review article by Marcus Griffiths (mdgriffiths at noble.org) and Larry York (lmyork at noble.org).Please cite this repository and the review article if the data is reused.Citation:Griffiths, M. G., York, L. M. (2020). Targeting root ion uptake kinetics to increase plant productivity and nutrient use efficiency. Plant Physiology 182, 2854 - 1868. https://doi.org/10.1104/pp.19.01496File descriptions and useThe R script assumes that all data files are in the working directory and that the ""tidyverse"" meta-package is installed.Kinetics_metaanalysis_all_files_v2.zip - Contains all the files.Kinetics_metaanalysis_column_definitions.txt - Definitions of column names used in data files (not used by R script)Kinetics_metaanalysis_csv.csv - Complete dataset in CSV format (used by R script)Kinetics_metaanalysis_excel.xlsx - Complete dataset in Excel format (not used by R script)Kinetics_metaanalysis_figure_subset.csv - Subset of total data used for figure generation (used by R script)Kinetics_metaanalysis_figures_code.R - Required R script for statistical analysis Version 2 of this repository includes minor updates to include new papers and data.","['## Griffiths M & York LM (2020) Targeting root nutrient uptake kinetics for increasing plant productivity\r\n\r\n#################\r\n## User setup  ##\r\n#################\r\n# 1) Set R working directory to folder with kinetics meta-analysis csv data\r\n# 2) Install or load following packages\r\nlibrary(tidyverse) #ggplot2, purrr, tibble, dplyr, tidyr, stringr, readr, forcats\r\n\r\n##############################\r\n## Metaanalysis Regressions ##\r\n##############################\r\ndat <- read_csv(file=paste(""Kinetics_metaanalysis_figure_subset.csv""), na = c(""NA"", ""na"", ""n.a."", """"))\r\n\r\ndat <- mutate(dat,\r\n              LUR.HUR = (LUR / HUR)\r\n\t\t\t  )\r\n\r\ndat <- mutate(dat,\r\n              HUR.LUR = (HUR / LUR)\r\n\t\t\t )\r\n\r\ndat <- mutate(dat,\r\n              Imax.Km = (Imax / Km)\r\n\t\t\t  )\r\n\r\ndat$Geno <- as.factor(dat$Geno)\r\n\r\ndat <- dat %>% filter(Nutrient==""Nitrate"")\r\n\r\n###########################\r\n## Metaanalysis Boxplots ##\r\n###########################\r\ntempdf <- dat %>% group_by(Geno) %>% summarise(avgdep = median(Imax, na.rm = TRUE))\r\ntempdf <- tempdf %>% arrange(avgdep)\r\ndat$Geno <- factor(dat$Geno, levels = unique(tempdf$Geno), ordered = TRUE);\r\nggplot(dat, aes(x=Geno, y=Imax)) +\r\n  geom_boxplot(outlier.shape = NA) +\r\n  geom_jitter(aes(shape=Geno), position=position_jitter(0.05)) +\r\n  theme_bw() + \r\n  theme(\r\n    plot.background = element_blank()\r\n    ,panel.grid.major = element_blank()\r\n    ,panel.grid.minor = element_blank()\r\n    ,axis.text.x = element_text(angle=45,vjust=0.5)\r\n  ) + \r\n  xlab(bquote("""")) +\r\n  ylab(bquote(""Imax (痠ol"" ~ g^-1 ~ h^-1*"")""))\r\nggsave(file=paste(""Nitrate_Imax_boxplot.png""), width=7, height=3, dpi=300)\r\nggsave(file=paste(""Nitrate_Imax_boxplot.pdf""), width=7, height=3, dpi=300, useDingbats=FALSE)\r\n\r\nggplot(dat, aes(x=Geno, y=Km)) +\r\n  geom_boxplot(outlier.shape = NA) +\r\n  geom_jitter(aes(shape=Geno), position=position_jitter(0.05)) +\r\n  theme_bw() + \r\n  theme(\r\n    plot.background = element_blank()\r\n    ,panel.grid.major = element_blank()\r\n    ,panel.grid.minor = element_blank()\r\n    ,axis.text.x = element_text(angle=45,vjust=0.5)\r\n  ) + \r\n  xlab(bquote("""")) +\r\n  ylab(bquote(""Km (然)""))\r\nggsave(file=paste(""Nitrate_Km_boxplot.png""), width=7, height=3, dpi=300)\r\nggsave(file=paste(""Nitrate_Km_boxplot.pdf""), width=7, height=3, dpi=300, useDingbats=FALSE)\r\n\r\n#################\r\n## Regressions ##\r\n#################\r\n## part 1 define model\r\nggplotRegression  <- function(dat, xvar, yvar){\r\n  fml <- paste(yvar, ""~"", xvar)\r\n  fit <- lm(fml, dat)\r\n  ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) + \r\n    geom_point(data = dat, aes(shape=Geno)) +\r\n    stat_smooth(method = ""lm"", col = ""red"") +\r\n    theme_bw() + \r\n    theme(\r\n      plot.background = element_blank()\r\n      ,panel.grid.major = element_blank()\r\n      ,panel.grid.minor = element_blank()\r\n    ) +\r\n    \r\n    geom_text(aes(x = 6, y = max(dat$Imax*0.95)), hjust = 0, \r\n              label = paste(""R2 = "",signif(summary(fit)$r.squared, 5),\r\n                            "" Intercept ="",signif(fit$coef[[1]],5)\r\n              )) +\r\n    geom_text(aes(x = 6, y = max(dat$Imax*0.90)), hjust = 0, \r\n              label = paste(""Slope ="",signif(fit$coef[[2]], 5),\r\n                            "" P ="",signif(summary(fit)$coef[2,4], 5)\r\n              ))\r\n}\r\n\r\n## part 2 plot graph\r\np <- ggplotRegression(dat, ""HUR"", ""Imax"")\r\np + scale_x_continuous(expand = c(0, 0)) +\r\n  scale_y_continuous(expand = c(0, 0)) +\r\n  xlab(bquote(""Highest uptake rate reported (痠ol"" ~ g^-1 ~ h^-1*"")"")) +\r\n  ylab(bquote(""Imax (痠ol"" ~ g^-1 ~ h^-1*"")""))\r\nggsave(file=paste(""Imax_vs_HUR_all.png""), width=4.75, height=2.75, dpi=300)\r\nggsave(file=paste(""Imax_vs_HUR_all.pdf""), width=4.75, height=2.75, dpi=300, useDingbats=FALSE)\r\n\r\np <- ggplotRegression(dat, ""Km"", ""Imax"")\r\np + scale_x_continuous(expand = c(0, 0)) +\r\n  scale_y_continuous(expand = c(0, 0)) +\r\n  xlab(bquote(""Km (然)"")) + \r\n  ylab(bquote(""Imax (痠ol"" ~ g^-1 ~ h^-1*"")""))\r\nggsave(file=paste(""Imax_vs_Km_all.png""), width=4.75, height=2.75, dpi=300)\r\nggsave(file=paste(""Imax_vs_Km_all.pdf""), width=4.75, height=2.75, dpi=300, useDingbats=FALSE)\r\n\r\nfiltereddata <- dat %>% filter(Range_high<5000)\r\np <- ggplotRegression(filtereddata, ""Range_high"", ""Imax"")\r\np + scale_x_continuous(expand = c(0, 0)) +\r\n  scale_y_continuous(expand = c(0, 0)) +\r\n  xlab(bquote(""Highest concentration tested (然)"")) +\r\n  ylab(bquote(""Imax (痠ol"" ~ g^-1 ~ h^-1*"")""))\r\nggsave(file=paste(""Imax_vs_conc_all.png""), width=4.75, height=2.75, dpi=300)\r\nggsave(file=paste(""Imax_vs_conc_all.pdf""), width=4.75, height=2.75, dpi=300, useDingbats=FALSE)\r\n\r\np <- ggplotRegression(dat, ""at.50uM"", ""Imax"")\r\np + scale_x_continuous(expand = c(0, 0)) +\r\n  scale_y_continuous(expand = c(0, 0)) +\r\n  xlab(bquote(""Uptake rate at 50然 (痠ol"" ~ g^-1 ~ h^-1*"")"")) +\r\n  ylab(bquote(""Imax (痠ol"" ~ g^-1 ~ h^-1*"")""))\r\nggsave(file=paste(""Imax_vs_50uM_all.png""), width=4.75, height=2.75, dpi=300)\r\nggsave(file=paste(""Imax_vs_50uM_all.pdf""), width=4.75, height=2.75, dpi=300, useDingbats=FALSE)\r\n\r\n########']",3,"Meta-analysis, ion uptake, kinetics, crop roots, Michaelis-Menten, nutrients, review article, Marcus Griffiths, Larry York, plant productivity, nutrient use efficiency, data reuse, citation, R script, tidyverse, column definitions,"
Data from: Permutation tests for phylogenetic comparative analyses of high-dimensional shape data: what you shuffle matters,"Evaluating statistical trends in high-dimensional phenotypes poses challenges for comparative biologists, because the high-dimensionality of the trait data relative to the number of species can prohibit parametric tests from being computed. Recently, two comparative methods were proposed to circumvent this difficulty. One obtains phylogenetic independent contrasts for all variables, and statistically evaluates the linear model by permuting the PICs of the response data. The other uses a distance-based approach to obtain coefficients for generalized least squares models (D-PGLS), and subsequently permutes the original data to evaluate the model effects. Here we show that permuting PICs is not equivalent to permuting the data prior to the analyses as in D-PGLS. We further explain why PICs are not the correct exchangeable units under the null hypothesis, and demonstrate that this mis-specification of permutable units leads to inflated type I error rates of statistical tests. We then show that simply shuffling the original data and re-calculating the independent contrasts with each iteration yields significance levels that correspond to those found using D-PGLS. Thus, while summary statistics from methods based on PICs and PGLS are the same, permuting PICs can lead to strikingly different inferential outcomes with respect to statistical and biological inferences.","['## Simulate X,Y data on random phylogenies, and evaluate Type I error rate of D-PGLS and PIC-RAND\r\n\r\n#############################\r\nrm(list=ls())\r\nlibrary(MASS)\r\nlibrary(geiger)\r\nlibrary(geomorph)\r\nlibrary(gplots)\r\nsource(\'procD.pic.r\')\r\n\r\n###################  SIMULATION\r\nNspec<-32\r\nNtraits<-10 \r\neffect<-0\r\nNsimul<-100\r\nNtree<-1000\r\npermute<-99\r\n\r\nout.pic<-out.pgls<-NULL\r\nfor (k in 1:Nsimul){\r\n  tree1<-rtree(Nspec)\r\n  x<-rnorm(Nspec)\r\n  y<-matrix(rnorm(Nspec*Ntraits),ncol=Ntraits)\r\n  rownames(y)<-tree1$tip.label; names(x)<-tree1$tip.label\r\n  for (m in 1:Ntree){\r\n    tree.sim<-compute.brlen(rtree(Nspec)) #random tree   \r\n    pic.x<-pic(x,tree.sim)\r\n    pic.y<-NULL\r\n    for (ii in 1:ncol(y)){\r\n      tmp<-pic(y[,ii],tree.sim)\r\n      pic.y<-cbind(pic.y,tmp)\r\n    }\r\n    P.PIC<-cbind(k,procD.pic(pic.y~pic.x,iter=permute)[1,6])\r\n    P.PGLS<-cbind(k,procD.pgls(y~x,tree.sim,iter=permute)[1,7])\r\n    out.pic<-rbind(out.pic,P.PIC)\r\n    out.pgls<-rbind(out.pgls,P.PGLS)    \r\n  }\r\n}\r\n\r\nNiter<-table(out.pic[,1])\r\nbin.pic<-ifelse(out.pic[,2]<=0.05,1,0)\r\nbin.pgls<-ifelse(out.pgls[,2]<=0.05,1,0)\r\nprob.PIC<-rowsum(bin.pic, out.pic[,1])/as.vector(Niter)\r\nprob.PGLS<-rowsum(bin.pgls, out.pgls[,1])/as.vector(Niter)\r\n\r\nres<-cbind(prob.PIC,prob.PGLS)\r\ntypeI.mn<-apply(res,2,mean)\r\ntypeI.sd<-apply(res,2,sd)\r\n  error <- qnorm(0.975)*typeI.sd/10    #std/sqrt(N)\r\n  CI.l <- typeI.mn-error; CI.u <- typeI.mn+error  #lower and upper CI\r\n\r\nplotCI(typeI.mn,li=CI.l,ui=CI.u,xlab=""PIC-Rand                            D-PGLS"",\r\n       ylab=""Type I Error"",pch=21,pt.bg=""black"",cex=1.5,ylim=c(0,.35))\r\n\r\n\r\n', '# SSE via squared distances  (for procD.lm RRPP method et al.)\r\nSSE <- function(L){# L is a linear model\r\n  r <- as.matrix(resid(L))\r\n  S <- r%*%t(r)\r\n  sse <- sum(diag(S))\r\n  sse\r\n}\r\n\r\n# P-values  (for procD.lm RRPP method)\r\npval = function(s){# s = sampling distribution\r\n  p = length(s)\r\n  r = rank(s)[1]-1\r\n  pv = 1-r/p\r\n  pv\r\n}\r\n\r\n# ProcD.lm modified to do analysis through the origin\r\nprocD.pic <- function(f1, iter = 999){ \r\n  data=NULL\r\n\tform.in <- formula(f1)\r\n    \tTerms <- terms(form.in, keep.order = TRUE)\r\n    \tY <- eval(form.in[[2]], parent.frame())\r\n    \tdf <- df.tmp <- SS.tmp <- SS.obs <- F <- Rsq <- array()\r\n    \tdat <- model.frame(form.in, data=NULL)\r\n\tXdims <- dim(as.matrix(model.matrix(Terms)))\r\n\tj <- ncol(attr(Terms, ""factors""))\r\n\tif(j==1){\r\n\t\tmod.mat <- model.matrix(Terms[1], data = dat)\r\n\t\tSS.res <- SSE(lm(Y~mod.mat[,-1]-1))    #NOTE, This is correct RESID SS!\r\n\t\tpredmod<-predict(lm(Y~mod.mat[,-1]-1))  #brute-force SSModel.  Not elegant, but works. \r\n    SS.obs<-sum(diag(predmod%*%t(predmod)))\r\n\t\tSS.tot <- SS.obs+SS.res\r\n\t\tdf <- ncol(mod.mat)-1\r\n\t\tMS <- SS.obs/df\r\n\t\tdf.tot <- nrow(Y)   #NOTE: not n-1 b/c PIC are already N-1.  Otherwise, F ratios won\'t perfectly match\r\n        df.res <- nrow(Y) - df ##  Ditto here, forget the -1\r\n        MS.tot <- SS.tot/df.tot\r\n        MS.res <- SS.res/df.res\r\n        Rsq <- SS.obs/SS.tot\r\n        F <- MS/MS.res\r\n        P <- array(c(SS.obs,rep(0,iter)))\r\n    \tfor(i in 1:iter){\r\n\t\t\tYr <- Y[sample(nrow(Y)),]\r\n\t\t\tpredmod.r<-predict(lm(Yr~mod.mat[,-1]-1))  #brute-force SSModel.  Not elegant, but works. \r\n\t\t\tP[i+1]<-sum(diag(predmod.r%*%t(predmod.r)))\r\n    \t#\tP[i+1] <- SS.null-SS.tmp\t\t\r\n\t\t}\r\n    \tP.val <- pval(P)\r\n\t}\r\n\t##NOTE: haven\'t fixed factorial version yet....\r\n\tif(j>1){\r\n\tXs <- array(0, c(Xdims, (j+1)))\r\n\tXs[,1,1] <- 1\r\n\tfor(i in 1:j){\r\n\t\tx <- as.matrix(model.matrix(Terms[1:i], data=dat))\r\n\t\tXs[,1:ncol(x),(i+1)] <- x\r\n        SS.tmp[i] <- SSE(lm(Y ~ x -1))\r\n        df.tmp[i] <- ifelse(ncol(x) == 1, 1, (ncol(x) - 1))\r\n        ifelse(i == 1, df[i] <- df.tmp[i], df[i] <- (df.tmp[i] - df.tmp[i - 1]))\r\n    \t}\r\n    SS.null <- (c(SSE(lm(Y~1)),SS.tmp))[1:j]\r\n    SS.obs <- SS.null - SS.tmp\r\n    MS <- SS.obs/df\r\n    SS.tot <- SSE(lm(Y~1))\r\n    SS.res <- SS.tot - sum(SS.obs)\r\n    df.tot <- nrow(Y) - 1\r\n    df.res <- nrow(Y) - 1 - sum(df)\r\n    MS.tot <- SS.tot/df.tot\r\n    MS.res <- SS.res/df.res\r\n    Rsq <- SS.obs/SS.tot\r\n    F <- MS/MS.res\r\n    P <- array(0,c(dim(matrix(SS.obs)),iter+1))\r\n    P[,,1] <- SS.obs\r\n  \r\n\t\tfor(i in 1:iter){\r\n\t\t\tYr <- Y[sample(nrow(Y)),]\r\n    \t\t\tfor (ii in 1:j) {\r\n        \t\t\tSS.tmp[ii] <- SSE(lm(Yr ~ Xs[,,ii+1] -1))\t\r\n        \t\t\tSS.null <- (c(SSE(lm(Y~1)),SS.tmp))[1:j]\t\r\n\t\t\t}\r\n    \t  \tP[,,i+1] <- SS.null-SS.tmp\t\t\r\n    \t\t}\r\n\t\tP.val <- Pval.matrix(P)\r\n    }\r\n    \r\n    anova.tab <- data.frame(df = c(df,df.res,df.tot), \r\n    \tSS = c(SS.obs, SS.res, SS.tot), \r\n    \tMS = c(MS, MS.res, MS.tot),\r\n    \tRsq = c(Rsq, NA, NA),\r\n    \tF = c(F, NA, NA),\r\n    \tP.val = c(P.val, NA, NA))\r\n    \trownames(anova.tab) <- c(attr(Terms, ""term.labels""), ""Residuals"",""Total"")\r\n      anova.title = ""\\nRandomization of Raw Values used\\n""\r\n    attr(anova.tab, ""heading"") <- paste(""\\nType I (Sequential) Sums of Squares and Cross-products\\n"",anova.title)\r\n    class(anova.tab) <- c(""anova"", class(anova.tab))\r\n    \t\treturn(anova.tab)\r\n}  ']",3,"- Permutation tests 
- Phylogenetic comparative analysis 
- High-dimensional shape data 
- Species 
- Parametric tests 
- Phenotypes 
- Independent contrasts 
- Linear models 
- Distance-based approach 
- Generalized least squares models 
"
Neo-taphonomic analysis of the Misiam leopard lair,"The data set presented here contains the MAU% data for the selected hyena-made and leopard-made faunal assemblages with which the Misiam assemblage is compared. Misiam is a recently discovered modern faunal accumulation found at Olduvai Gorge (Tanzania) interpreted as a palimpsest resulting from the action of leopards (main transporting agents) and hyenas (secondary scavengers). It is the first open-air reported leopard-made faunal accumulation. Defining the anatomical and taphonomic characteristics of such an assembllage is important for the interpretation of prehistoric faunal assemblages created by carnivores. It is also relevant for modern ecological studies. In this particular case, the bulk of the assemblage is composed of wildebeests. This is usually not the target of leopards; however, their seasonal abundance during the wildebeest migration on the plains adjacent to Olduvai Gorge prompts this rather exceptional highly-specialized behavior by usually eclectic leopards. In the present work, a thorough taphonomic analysis is carried out and the main taxonomic, anatomical and taphonomic characteristics of this felid-hyenic modified assemblage is decribed. The analytical approach adopted uses the data presented here.","['# Helper packages\nlibrary(dplyr)    # for data manipulation\nlibrary(ggplot2)  # for data visualization\nlibrary(tidyr)    # for data reshaping\n\n# Modeling packages\nlibrary(h2o)  # for fitting GLRMs\n\na<-read.table(file.choose(),header=T)#correr primero MAUs y luego Sites\n\nh2o.no_progress()  # turn off progress bars\nh2o.init(max_mem_size = ""5g"")  # connect to H2O instance\n\n# convert data to h2o object\nmy_basket.h2o <- as.h2o(a)\n\n# run basic GLRM\nbasic_glrm <- h2o.glrm(\n  training_frame = my_basket.h2o,\n  k = 10, \n  loss = ""Quadratic"",\n  regularization_x = ""None"", \n  regularization_y = ""None"", \n  transform = ""STANDARDIZE"", \n  max_iterations = 2000,\n  seed = 123\n)\n\nsummary(basic_glrm)\n\n\nplot(basic_glrm)\n\n\nbasic_glrm@model$importance\n\ndata.frame(\n  PC  = basic_glrm@model$importance %>% seq_along(),\n  PVE = basic_glrm@model$importance %>% .[2,] %>% unlist(),\n  CVE = basic_glrm@model$importance %>% .[3,] %>% unlist()\n) %>%\n  gather(metric, variance_explained, -PC) %>%\n  ggplot(aes(PC, variance_explained)) +\n  geom_point() +\n  facet_wrap(~ metric, ncol = 1, scales = ""free"")\n\n\nt(basic_glrm@model$archetypes)[1:5, 1:5]\n\n\n#------------------------------\n#grafico\n\np1 <- t(basic_glrm@model$archetypes) %>% \n  as.data.frame() %>% \n  mutate(feature = row.names(.)) %>%\n  ggplot(aes(Arch1, reorder(feature, Arch1))) +\n  geom_point()\n\np2 <- t(basic_glrm@model$archetypes) %>% \n  as.data.frame() %>% \n  mutate(feature = row.names(.)) %>%\n  ggplot(aes(Arch1, Arch2, label = feature)) +\n  geom_text()\n\ngridExtra::grid.arrange(p1, p2, nrow = 1)\n\n#---------------------------------\n\n#componentes de cada row\nt(basic_glrm@model$archetypes)\n\n#clasificacion y prediccion\n\npp<-h2o.predict(basic_glrm, my_basket.h2o)\n\nas.data.frame(h2o.cbind(my_basket.h2o$group,pp$reconstr_group))\n\n#--------------------------\n\n# Use non-negative regularization\nk8_glrm_regularized <- h2o.glrm(\n  training_frame = my_basket.h2o,\n  k = 10, \n  loss = ""Quadratic"",\n  regularization_x = ""NonNegative"", \n  regularization_y = ""NonNegative"",\n  gamma_x = 0.5,\n  gamma_y = 0.5,\n  transform = ""STANDARDIZE"", \n  max_iterations = 2000,\n  seed = 123\n)\n\n# Show predicted values\npredict(k8_glrm_regularized, my_basket.h2o)\n\n?//////////////////////////////\n  \n  # Split data into train & validation\nsplit <- h2o.splitFrame(my_basket.h2o, ratios = 0.75, seed = 123)\ntrain <- split[[1]]\nvalid <- split[[2]]\n\n# Create hyperparameter search grid\nparams <- expand.grid(\n  regularization_x = c(""None"", ""NonNegative"", ""L1""),\n  regularization_y = c(""None"", ""NonNegative"", ""L1""),\n  gamma_x = seq(0, 1, by = .25),\n  gamma_y = seq(0, 1, by = .25),\n  error = 0,\n  stringsAsFactors = FALSE\n)\n\n# Perform grid search\nfor(i in seq_len(nrow(params))) {\n  \n  # Create model\n  glrm_model <- h2o.glrm(\n    training_frame = train,\n    k = 10, \n    loss = ""Quadratic"",\n    regularization_x = params$regularization_x[i], \n    regularization_y = params$regularization_y[i],\n    gamma_x = params$gamma_x[i],\n    gamma_y = params$gamma_y[i],\n    transform = ""STANDARDIZE"", \n    max_runtime_secs = 1000,\n    seed = 123\n  )\n  \n  # Predict on validation set and extract error\n  validate <- h2o.performance(glrm_model, valid)\n  params$error[i] <- validate@metrics$numerr\n}\n\n# Look at the top 10 models with the lowest error rate\nparams %>%\n  arrange(error) %>%\n  head(10)\n#------------------------------\n\n# Apply final model with optimal hyperparamters\nfinal_glrm_model <- h2o.glrm(\n  training_frame = my_basket.h2o,\n  k = 8, \n  loss = ""Quadratic"",\n  regularization_x = ""L1"", \n  regularization_y = ""NonNegative"",\n  gamma_x = 1,\n  gamma_y = 0.25,\n  transform = ""STANDARDIZE"", \n  max_iterations = 2000,\n  seed = 123\n)\n\n# New observations to score\nnew_observations <- as.h2o(sample_n(my_basket, 2))\n\n#--------------------------------\n\n#--------------------------------\n#UMAP\nlibrary(umap) \na<-read.table(file.choose(),header=T)#correr MAU LEO\nswissTib<-a\n\n# CREATE GRID OF UMAP HYPERPARAMETERS ----\numapHyperPars <- expand.grid(n_neighbors = seq(3, 12, 4),\n                             min_dist    = seq(0.1, 0.5, 0.1),\n                             metric      = c(""euclidean"", ""manhattan""),\n                             n_epochs    = seq(50, 400, 75))\n\numap <- pmap(umapHyperPars, umap, d = swissTib[, -1], verbose = TRUE)\n\numapTib <- tibble(n_neighbors = rep(umapHyperPars$n_neighbors, each = 12),#numero rows\n                  min_dist      = rep(umapHyperPars$min_dist, each = 12),\n                  metric        = rep(umapHyperPars$metric, each = 12),\n                  n_epochs   = rep(umapHyperPars$n_epochs, each = 12),\n                  UMAP1      = unlist(map(umap, ~.$layout[, 1])),\n                  UMAP2      = unlist(map(umap, ~.$layout[, 2])))\n\nfilter(umapTib, metric == ""euclidean"", n_epochs == 200) %>%\n  ggplot(aes(UMAP1, UMAP2)) +\n  facet_grid(n_neighbors ~ min_dist) +\n  geom_point() +\n  theme_bw()\nggsave(""umap1.pdf"", width = 10, height = 6)\n\nfilter(umapTib, n_neighbors == 3, min_dist == 0.1) %>%\n  ggplot(aes(UMAP1, UMAP2)) +\n  facet_grid(metric ~ n_']",3,"Neo-taphonomy, Misiam, leopard lair, MAU%, hyena-made, leopard-made, faunal assemblage, Olduvai Gorge, Tanzania, palimpsest, transporting agents, scavengers, prehistoric, anatom"
AI-RISKI: A tool for examining the spatio-temporal risk of transmission of Avian Influenza to poultry in Finland,"This release provides data files on the assessment of the spatio-temporal risk of transmission of Avian Influenza to poultry in Finland, and the source code of the web application AI-RISKI, a tool for examining the assessment results. The application is available at https://ai-riski.rahtiapp.fi/.The assessment of the spatio-temporal risk of transmission of Avian Influenza to poultry in Finland was done and the AI-RISKI application developed in the Risk Assessment Unit of the Finnish Food Authority as part of a project Mapping the spatiotemporal distribution of risk factors affecting the spread of Avian Influenza in egg production and broiler farms in Finland funded by the Ministry of Agriculture and Forestry of Finland (The Development Fund for Agriculture and Forestry - Makera). The project was a co-operation between the Finnish Food Authority, Finnish Museum of Natural History (LUOMUS) and BirdLife Finland. The assessment is more detailly described in the risk profile of Avian Influenza in Finnish poultry (in Finnish) (Rossow et al., 2023).AI-RISKI web application is written in R (R Core Team, 2022) with its package shiny (Chang et al., 2022). R packages sf (Pebesma, 2018), rgdal (Bivand et al., 2022), rgeos (Bivand & Rundel, 2021) and raster (Hijmans, 2022) are used to process the GIS data and the R package leaflet (Cheng et al., 2022) to create interactive maps for the visual examination of the assessment results. R packages shinythemes (Chang, 2021), shinyhelper (Mason-Thom, 2019), shinybusy (Meyer & Perrier, 2022), shinywidgets (Perrier et al., 2022), promises (Cheng, 2021) and future (Bengtsson, 2021) are used to finalize the user experience. The application is in Finnish, but the source code is documented in English.Please note that data on the borders of regions (National Land Survey of Finland, 2021) and municipalities (Statistics Finland, 2022) in Finland and the observation records of Avian Influenza in Finland in 2021-2022 that are available in the AI-RISKI application at https://ai-riski.rahtiapp.fi/ are not available in this release.To run the source code locally in your computer you need to:Install R from https://www.r-project.org/Save the file app.R in your working directory.Create a folder into your working directory called data, and unzip the file data.zip there.Install R packages shiny, shinythemes, shinyhelper, shinybusy, shinywidgets, sf, rgdal, rgeos, raster, leaflet, promises and future.Run the command runApp() in R.ReferencesBengtsson H (2021). A Unifying Framework for Parallel and Distributed Processing in R using Futures, The R Journal 13:2, pages 208-227, doi:10.32614/RJ-2021-048Bivand R, Rundel C (2021). rgeos: Interface to Geometry Engine - Open Source ('GEOS'). R package version 0.5-9, https://CRAN.R-project.org/package=rgeosBivand R, Keitt T, Rowlingson B (2022). rgdal: Bindings for the 'Geospatial' Data Abstraction Library. R package version 1.532, https://CRAN.R-project.org/package=rgdalChang W (2021). shinythemes: Themes for Shiny. R package version 1.2.0, https://CRAN.R-project.org/package=shinythemesChang W, Cheng J, Allaire J, Sievert C, Schloerke B, Xie Y, Allen J, McPherson J, Dipert A, Borges B (2022). shiny: Web Application Framework for R. R package version 1.7.2, https://CRAN.R-project.org/package=shinyCheng J (2021). promises: Abstractions for Promise-Based Asynchronous Programming. R package version 1.2.0.1, https://CRAN.R-project.org/package=promisesCheng J, Karambelkar B, Xie Y (2022). leaflet: Create Interactive Web Maps with the JavaScript 'Leaflet' Library. R package version 2.1.1, https://CRAN.R-project.org/package=leafletHijmans R (2022). raster: Geographic Data Analysis and Modeling. R package version 3.5-29, https://CRAN.R-project.org/package=rasterMason-Thom C (2019). shinyhelper: Easily Add Markdown Help Files to 'shiny' App Elements. R package version 0.3.2, https://CRAN.R-project.org/package=shinyhelperMeyer F, Perrier V (2022). shinybusy: Busy Indicators and Notifications for 'Shiny' Applications. R package version 0.3.1, https://CRAN.R-project.org/package=shinybusyNational Land Survey of Finland (2021). Administrative borders, 20212022, 1:100 000. CSC  IT Center for Science. http://urn.fi/urn:nbn:fi:att:dc71353c-8063-4c67-abc1-3193a130b990Pebesma, E (2018). Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10 (1), 439-446, https://doi.org/10.32614/RJ-2018-009Perrier V, Meyer F, Granjon D (2022). shinyWidgets: Custom Inputs Widgets for Shiny. R package version 0.7.3, https://CRAN.R-project.org/package=shinyWidgetsR Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/Rossow H, Sepp-Lassila L, Tuomola J, Lehtiniemi T, Valkama J, Tammiranta N, Gadd T, Tuominen P (2023) Siipikarjan lintuinfluenssa Suomessa - riskiprofiili. Ruokaviraston tutkimuksia 1/2023. ISBN PDF 978-952-358-046-6. pp. 33.Statistics Finland (2022). Municipalities, 2022 (1:4 500 000). The material was downloaded from Statistics Finland's interface service on 22 October 2022 with the licence CC BY 4.0","['\r\n# This is the source code of the web application AI-RISKI\r\n\r\nlibrary(shiny)\r\nlibrary(shinythemes)\r\nlibrary(shinyhelper)\r\nlibrary(shinybusy)\r\nlibrary(shinyWidgets)\r\nlibrary(sf)\r\nlibrary(rgdal)\r\nlibrary(sp)\r\nlibrary(rgeos)\r\nlibrary(raster)\r\nlibrary(leaflet)\r\nlibrary(promises)\r\nlibrary(future)\r\nplan(multisession)\r\n\r\nhelp_risk <- function() {\r\n  tags$i() %>%\r\n    helper(\r\n      type = ""inline"",\r\n      size = ""m"",\r\n      title = ""Määritä tarkasteleva riskikartta ja klikkaa \'Näytä kartta\' -painiketta"",\r\n      content = HTML(\r\n        ""Valitse ensin tarkasteltava riskitaso. Seuraavat valintapaneelit määräytyvät valitun riskitason perusteella."",\r\n        ""</br>"",\r\n        ""</br>"",\r\n        ""Kun kaikki valinnat ovat määritetty, klikkaa \'Näytä kartta\' painiketta ja valittu kartta ilmestyy ruutuun."",\r\n        ""</br>"",\r\n        ""</br>"",\r\n        ""HUOM, Riskiin vaikuttavilla luonnonvaraisilla linnuilla viitataan niihin luonnonvaraisiin lintuihin, joilla on merkittävä vaikutus lintuinfluenssariskiin. Tästä kerrotaan tarkemmin \'Lisätietoja\' välilehdellä."",\r\n        ""</br>"",\r\n        ""</br>"",\r\n        ""Riskikartoille on mahdollista lisätä myös oma aineisto. Oman aineiston tulee kuvata pistemäisiä sijainteja ja se täytyy ladata sovellukseen csv-tiedostona. Tiedostojen tulee sisältää pistemäisten kohteiden ETRS-TM35FIN-järjestelmän tai WGS84-järjestelmän mukaiset koordinaatit. \r\n        Leveysasteen koordinaatit tulee olla sarakkeessa, jonka nimi on \'Latitude\', ja pituusasteen koordinaatit sarakkeessa, jonka nimi on \'Longitude\'. Desimaalina tulee käyttää pistettä ja arvojen erottajana pilkkua."",\r\n        ""</br>"",\r\n        ""</br>"",\r\n        ""Oma aineisto näkyy kartalla punaisina pisteinä. Jos csv-tiedostoon lisää sarakkeen nimeltä \'Data\', luo sovellus omaan aineistoon ponnahdusikkunan. Tällöin pisteitä klikattaessa sovellus näyttää ponnahdusikkunassa \'Data\'-sarakkeessa oleviva tietoja"",\r\n        ""</br>"",\r\n        ""</br><b>Esimerkki sopivasta csv-tiedostosta:</b>"",\r\n        ""</br><ul><i>Data, Latitude, Longitude</i>"",\r\n        ""</br><i>Site1, 6759644, 306126</i>"",\r\n        ""</br><i>Site2, 6724101, 298494</i>"",\r\n        ""</br><i>Site3, 7047451, 306395</i></ul>""\r\n      ),\r\n      buttonLabel = ""OK"",\r\n      easyClose = TRUE,\r\n      icon = ""question-circle"",\r\n      colour = ""orangered"",\r\n      fade = FALSE\r\n    )\r\n}\r\n\r\nui <-\r\n  navbarPage(\r\n    title = h3(strong(\r\n      ""AI-RISKI - Lintuinfluenssan alueellinen ja ajallinen esiintymisriski Suomen siipikarjatuotannossa""\r\n    )),\r\n    theme = shinytheme(""flatly""),\r\n    tabPanel(\r\n      h4(""Riskikartat""),\r\n      \r\n      fluidPage(\r\n        sidebarLayout(\r\n          sidebarPanel(\r\n            #h3(strong(""Valitse tarkasteltava riskitaso"")),\r\n            help_risk(),\r\n            awesomeRadio(\r\n              inputId = ""risk"",\r\n              label = h4(strong(""Valitse tarkasteltava riskitaso"")),\r\n              choices = c(\r\n                ""Riskiin vaikuttavien luonnonvaraisten lintujen esiintyminen"" = 1,\r\n                ""Siipikarjan tautisuojaus"" = 2,\r\n                ""Siipikarjan lintuinfluenssatartuntariski"" = 3\r\n              ),\r\n              selected = 1\r\n            ),\r\n            uiOutput(outputId = ""ulkoilu""),\r\n            uiOutput(outputId = ""vuodenaika""),\r\n            br(""""),\r\n            awesomeRadio(\r\n              inputId = ""own_data1"",\r\n              label = h4(strong(\r\n                ""Haluatko lisätä kartalle myös oman aineiston?""\r\n              )),\r\n              choices = c(""En"" = 1,\r\n                          ""Kyllä"" = 2),\r\n              inline = TRUE,\r\n              selected = 1\r\n            ),\r\n            uiOutput(outputId = ""owndata2""),\r\n            uiOutput(outputId = ""CRC2""),\r\n            br(""""),\r\n            actionBttn(\r\n              ""run"",\r\n              label = h5(""Näytä kartta""),\r\n              style = ""gradient"",\r\n              color = ""primary"",\r\n              size = ""md"")\r\n          ),\r\n          mainPanel(\r\n            shinycssloaders::withSpinner(\r\n              leafletOutput(""view"", height = 800),\r\n              type = 5,\r\n              color = ""#45B39D"",\r\n              size = 2\r\n            ),\r\n          )\r\n        ))),\r\n    tabPanel(h4(""Lisätietoja""),\r\n             fluidRow(\r\n               column(\r\n                 7,\r\n                 tabsetPanel(\r\n                   tabPanel(\r\n                     ""Johdanto"",\r\n                     tags$br(),\r\n                     wellPanel(\r\n                       p(\r\n                         tags$b(\r\n                           ""AI-RISKI -sovelluksella voi tarkastella \'Lintuinfluenssan leviämiseen vaikuttavien riskitekijöiden kartoitus\' -hankkeessa tehtyjä arvioita lintuinfluenssan alueellisesta ja ajallisesta \r\n                            leviämisriskistä Suomen siipikarjatuotannossa. Riskiä arvioitiin lintuinfluenssan leviämisen kannalta keskeisten luonnonvaraisten lintujen alueellisen ja ajallisen esiintymisen, sekä\r\n                            siipikarjatuotannon alueellisen esiintymisen ja tuotannon alttiu']",3,"- AI-RISKI
- Avian Influenza
- poultry
- Finland
- spatio-temporal risk
- web application
- assessment
- data files
- source code
- Risk Assessment Unit
- Finnish Food"
Simulation of the Resilience of Cultural Diversity in CulSim,"The collection contains:data.csv: data obtained using the CulSim(2016) cultural simulator, and presented in presented in Ulloa & Kacperski (2020)code.R: statistical analysis Ulloa, R. (2016). CulSim: A simulator of emergence and resilience of cultural diversity. SoftwareX, 5, 150155. https://doi.org/10.1016/j.softx.2016.07.005Ulloa, R., & Kacperski, C. (2020). A Simulation of the Resilience of Cultural Diversity in the Face of Large-Scale Events. ArXiv:2003.05322 [Nlin, Physics:Physics]. http://arxiv.org/abs/2003.05322","['library(lme4)\nlibrary(lmerTest)\nlibrary(optimx)\n\n# read data frome file\ndata <- read.csv(\'data.csv\')\n\n# create factors\ndata <- within(data, {\n  distribution <- as.factor(distribution)\n  event <- as.factor(event)\n  diversity <- as.factor(diversity)\n  scenario <- as.factor(scenario)\n  event_level <- as.factor(event_size)\n})\n\n# Specify the order of the events according to the paper\ndata$distribution <- factor(data$distribution, levels=c(""Uniform"",""Normal""))\ndata$event <- factor(data$event, levels=c(""Decimation"", \n                                          ""Settlement"", ""Outsiders"",\n                                          ""Apostasy"",  ""Destruction"",\n                                          ""Partial Content Removal"", ""Full Content Removal"", \n                                          ""Partial Conversion"", ""Full Conversion""))\n\n\n# Appendix A\nsink(""appendix_a.txt"")\n\nfor (ev in levels(as.factor(data$event))){\n  for (dist in levels(as.factor(data$distribution))){\n    DatasetHV <- subset(data, event==ev & distribution==dist)\n    \n    cat(paste(\'\\n\\n# \', ev, dist, \'\\n\'))\n    \n    # Anova for main effects\n    aov1 <- aov(full_sim ~ diversity * event_size * scenario, data=DatasetHV)\n    print(summary(aov1))\n  }\n}\nsink()\n\n\n\n# Table 5 in the paper\nsink(""table_5.txt"")\n\n#  Only applies to divers cultures (only for event sizes between 0.2 and 0.8)\ndiverseData <- subset(data, (diversity == \'Many cultures\' & event_size > 0.0 & event_size < 1.0))\nfor (ev in levels(as.factor(diverseData$event))){\n  for (dist in levels(as.factor(diverseData$distribution))){\n    subDatasetHV <- subset(diverseData, event==ev & distribution==dist)\n    print(paste(ev, dist))\n    \n    # Linear Mixed Effects model to accumulate the effect of pre_cultures\n    lmer1 <- lmer(full_sim ~ pre_cultures_at_least_3 + (scenario|event_level), data=subDatasetHV, \n                  control = lmerControl(\n                    optimizer =\'optimx\', optCtrl=list(method=\'nlminb\')))\n    print(summary(lmer1))\n    \n  }\n}\nsink()\n\n\n\n# Appendix B (correlations of Table 5)\nsink(""appendix_b.txt"")\n\n# Only applies to diverse cultures (extrem event sizes of 0.0 and 1.0 included)\ndiverseData <- subset(data, (diversity == \'Many cultures\'))\ncat(""Event,Distribution,Scenario,Event Size, Correlation\\n"")\nfor (ev in levels(as.factor(diverseData$event))){\n  for (dist in levels(as.factor(diverseData$distribution))){\n    for (sc in levels(as.factor(diverseData$scenario))){\n      for (lvl in levels(as.factor(diverseData$event_size))){\n        subDatasetHV <- subset(diverseData, event==ev & distribution==dist \n                               & scenario == sc & event_size == lvl)\n        cat(paste(ev, dist, sc, lvl, cor(subDatasetHV$full_sim, subDatasetHV$pre_cultures_at_least_3), sep="",""))\n        cat(""\\n"")\n      }\n    }\n  }\n}\nsink()\n\n']",3,"CulSim, cultural diversity, resilience, simulation, large-scale events, data, statistical analysis, software, emergence, ArXiv, Ulloa, Kacperski."
Species Portfolio Effects Dominate Seasonal Zooplankton Stabilization Within a Large Temperate Lake,"The raw data file is available online for public access (https://data.ontario.ca/dataset/lake-simcoe-monitoring). Download the 1980-2019 csv files and open up the file named ""Simcoe_Zooplankton&Bythotrephes.csv"". Copy and paste the zooplankton sheet into a new excel file called ""Simcoe_Zooplankton.csv"". The column ZDATE in the excel file needs to be switched from GENERAL to SHORT DATE so that the dates in the ZDATE column read ""YYYY/MM/DD"". Save as .csv in appropriate R folder. The data file ""simcoe_manual_subset_weeks_5"" is the raw data that has been subset for the main analysis of the article using the .R file ""Simcoe MS - 5 Station Subset Data"". The .csv file produced from this must then be manually edited to remove data points that do not have 5 stations per sampling period as well as by combining data points that should fall into a single week. The ""simcoe_manual_subset_weeks_5.csv"" is then used for the calculation of variability, stabilization, asynchrony, and Shannon Diversity for each year in the .R file ""Simcoe MS - 5 Station Calculations"". The final .R file ""Simcoe MS - 5 Station Analysis contains the final statistical analyses as well as code to reproduce the original figures. Data and code for main and supplementary analyses are also available on GitHub (https://github.com/reillyoc/ZPseasonalPEs).","['simcoe <- read.csv(\'simcoe_final_weeks_5.csv\', header = T)\nsimcoe_div <- read.csv(\'simcoe_final_5_weeks_diversity\', header = T)\nsimcoe_corr <- read.csv(\'simcoe_final_corr_weeks_5.csv\', header = T)\nsimcoe_pairst <- read.csv(\'simcoe_final_pair_stab_weeks_5.csv\', header = T)\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(ggpubr)\n\nYear <- c(1986:1999)\nGlobal_Change <- c(\'Pre\')\nsimcoe_gc_pre <- data.frame(Year, Global_Change)\n\nYear <- c(2008:2019)\nGlobal_Change <- c(\'Post\')\nsimcoe_gc_post <- data.frame(Year, Global_Change)\n\nsimcoe_gc <- rbind(simcoe_gc_pre, simcoe_gc_post)\n\nsimcoe <- merge(simcoe_gc, simcoe, by = c(\'Year\'))\n\n###### Create new data frame subset with categories for Variability for t.test #####\nsimcoe_gcv <- simcoe %>% select(gcv) %>%\n  summarize(Variability = gcv,\n            Scale = \'Metacommunity\')\nsimcoe_lcv <- simcoe %>% select(lcv) %>%\n  summarize(Variability = lcv,\n            Scale = \'Weighted Average Population\')\n\nsimcoe_var <- rbind(simcoe_gcv, simcoe_lcv)\n\nt.test(Variability~Scale, data = simcoe_var)\n\n\n##### Create new data frame subset with categories for Variability for gg barplot #####\n\ngg_simcoe_mc_variability <- simcoe %>% select(gcv) %>%\n  summarize(CV = mean(gcv),\n            sd_variability = sd(gcv),\n            count = n(),\n            se_variability = (sd_variability/(sqrt(count))),\n            Scale = \'Metacommunity\',\n            Color = \'blue\')\ngg_simcoe_wap_variability <- simcoe %>% select(lcv) %>%\n  summarize(CV = mean(lcv),\n            sd_variability = sd(lcv),\n            count = n(),\n            se_variability = (sd_variability/(sqrt(count))),\n            Scale = \'Local Population\',\n            Color = \'green\')\n\ngg_simcoe_variability <- rbind(gg_simcoe_mc_variability, gg_simcoe_wap_variability)\ngg_simcoe_variability$Scale <- factor(gg_simcoe_variability$Scale, levels = c(""Metacommunity"", \n                                                                              ""Local Population""))\n\ngg_box_sim_variability <- ggplot(data = gg_simcoe_variability, aes(x = Scale, y = CV, fill = Color)) +\n  geom_bar(stat = \'identity\',\n           color = \'black\',\n           width = 0.3,\n           position = position_dodge()) +\n  geom_errorbar(aes(ymin = CV - se_variability, ymax = CV + se_variability), width = 0.1) +\n  ylab(""Average Annual Variability (CV Squared)"") +\n  xlab(""Scale"") +\n  theme_classic() +\n  theme(legend.position = \'none\') +\n  scale_fill_brewer(palette = \'Set1\')\ngg_box_sim_variability\n\n\n###### Create new data frame subset with categories for Stabilization for anova ##### \nsimcoe_W <- simcoe %>% select(W) %>%\n  summarize(Stabilization = W,\n            Scale = \'Total\')\nsimcoe_lc_stab <- simcoe %>% select(lc_stab) %>%\n  summarize(Stabilization = lc_stab,\n            Scale = \'Local Community\')\nsimcoe_mp_stab <- simcoe %>% select(mp_stab) %>%\n  summarize(Stabilization = mp_stab,\n            Scale = \'Metapopulation\')\nsimcoe_cc_stab <- simcoe %>% select(cc_stab) %>%\n  summarize(Stabilization = cc_stab,\n            Scale = \'Cross-Community\')\n\nsimcoe_stab <- rbind(simcoe_lc_stab, simcoe_mp_stab, simcoe_cc_stab)\n\nstab_aov <- aov(Stabilization~Scale, data = simcoe_stab)\nsummary(stab_aov)\nTukeyHSD(stab_aov)\n\n\n##### Create new data frame subset with categories for Stabilization for gg barplot #####\n\ngg_simcoe_mc_stab <- simcoe %>% select(W) %>%\n  summarize(CV = mean(W),\n            sd_stab = sd(W),\n            count = n(),\n            se_stab = (sd_stab/(sqrt(count))),\n            Scale = \'Total\',\n            Color = \'blue\')\ngg_simcoe_lc_stab <- simcoe %>% select(lc_stab) %>%\n  summarize(CV = mean(lc_stab),\n            sd_stab = sd(lc_stab),\n            count = n(),\n            se_stab = (sd_stab/(sqrt(count))),\n            Scale2 = \'Stabilization\',\n            Scale = \'Local Community\',\n            Color = \'blue\')\ngg_simcoe_mp_stab <- simcoe %>% select(mp_stab) %>%\n  summarize(CV = mean(mp_stab),\n            sd_stab = sd(mp_stab),\n            count = n(),\n            se_stab = (sd_stab/(sqrt(count))),\n            Scale2 = \'Stabilization\',\n            Scale = \'Metapopulation\',\n            Color = \'green\')\ngg_simcoe_cc_stab <- simcoe %>% select(cc_stab) %>%\n  summarize(CV = mean(cc_stab),\n            sd_stab = sd(cc_stab),\n            count = n(),\n            se_stab = (sd_stab/(sqrt(count))),\n            Scale2 = \'Stabilization\',\n            Scale = \'Cross-Community\',\n            Color = \'red\')\n\ngg_simcoe_stab <- rbind(gg_simcoe_mp_stab, gg_simcoe_lc_stab)\ngg_simcoe_stab <- rbind(gg_simcoe_stab, gg_simcoe_cc_stab)\n\ngg_simcoe_stab$Scale <- factor(gg_simcoe_stab$Scale, levels = c(""Total"", \n                                                                ""Local Community"", \n                                                                ""Metapopulation"", \n                                                                ""Cross-Community""))\n\n\ngg_box_sim_stab <- ggplot(data = gg_simcoe_stab, aes(x = Scale2, y = CV, fill = Color)) +\n  geom_bar(stat = \'identity\',\n           ', 'sim_start <- read.csv(""simcoe_manual_subset_weeks_5.csv"", header = T)\n\nlibrary(tidyverse)\nlibrary(reshape2)\n\nset.seed(017)\n\nsimcoe <- sim_start %>% select(Year, Week, Station_ID, Species, Density)\n\n##### Shannon Diversity & Evenness #####\n\nlibrary(vegan)\n\nsimcoe_diversity <- simcoe %>% group_by(Year, Species, Station_ID) %>%\n  summarize(Density = sum(Density))\n\nsimcoe_diversity <- dcast(simcoe_diversity, Year + Station_ID ~ Species, fun.aggregate = sum, value.var = ""Density"")\n\n\nsimcoe_diversity$Shannon_Div_Local <- diversity(simcoe_diversity[,c(3:53)])\nsimcoe_diversity$Evenness_Local <- diversity(simcoe_diversity[,c(3:53)])/log(specnumber(simcoe_diversity[,c(3:53)]))\n\nsimcoe_diversity <- simcoe_diversity %>% select(Year, Shannon_Div_Local, Evenness_Local) %>%\n  group_by(Year) %>%\n  summarize(Shannon_Div_Local = mean(Shannon_Div_Local),\n            Evenness_Local = mean(Evenness_Local))\n\n\n#Calculating regional diversity (# of metapopulations per year)\nsimcoe_richness <- simcoe %>% group_by(Year, Species) %>%\n  summarize(Density = sum(Density))\n\nsimcoe_richness <- dcast(simcoe_richness, Year  ~ Species, fun.aggregate = sum, value.var = ""Density"")\n\nsimcoe_richness$Species_Richness <- specnumber(simcoe_richness[,c(2:52)])\n\nsimcoe_richness <- simcoe_richness %>% select(Year, Species_Richness)\n\ndetach(""package:vegan"", unload=TRUE)\n\nsimcoe_diversity <- merge(simcoe_diversity, simcoe_richness, by = c(""Year""))\n\nwrite.csv(simcoe_diversity, ""simcoe_final_5_weeks_diversity"")\n\n##### Variability, Stabilization, & Asynchrony Calculations as per Hammond et al. 2020 #####\n\n\n##### 1986 #####\nsimcoe_1986 <- subset(simcoe, Year == \'1986\')\n\n\n#Transpose dataframe to add 0s within years\nsimcoe_1986_t1 <- dcast(simcoe_1986, Year + Week + Station_ID ~ Species, fun.aggregate = sum, value.var = ""Density"")\nsimcoe_1986_m <- melt(simcoe_1986_t1, id.vars = c(""Year"", ""Week"", ""Station_ID""),\n                      variable.name = ""Species"",\n                      value.name = ""Density"")\n\n#subset dataframes by station to add a station tag to species id\nsimcoe_c6_1986 <- subset(simcoe_1986_m, Station_ID == \'C6\')\nsimcoe_c6_1986$Species <- paste(""C6"", simcoe_c6_1986$Species, sep="" "")\nsimcoe_c9_1986 <- subset(simcoe_1986_m, Station_ID == \'C9\')\nsimcoe_c9_1986$Species <- paste(""C9"", simcoe_c9_1986$Species, sep="" "")\nsimcoe_k42_1986 <- subset(simcoe_1986_m, Station_ID == \'K42\')\nsimcoe_k42_1986$Species <- paste(""K42"", simcoe_k42_1986$Species, sep="" "")\nsimcoe_k45_1986 <- subset(simcoe_1986_m, Station_ID == \'K45\')\nsimcoe_k45_1986$Species <- paste(""K45"", simcoe_k45_1986$Species, sep="" "")\nsimcoe_s15_1986 <- subset(simcoe_1986_m, Station_ID == \'S15\')\nsimcoe_s15_1986$Species <- paste(""S15"", simcoe_s15_1986$Species, sep="" "")\n\n#recombine dataframes\nsimcoe_spec_1986 <- rbind(simcoe_c6_1986, simcoe_c9_1986)\nsimcoe_spec_1986 <- rbind(simcoe_spec_1986, simcoe_k42_1986)\nsimcoe_spec_1986 <- rbind(simcoe_spec_1986, simcoe_k45_1986)\nsimcoe_spec_1986 <- rbind(simcoe_spec_1986, simcoe_s15_1986)\nsimcoe_spec_1986 <- simcoe_spec_1986 %>% select(- Station_ID)\n\n#transpose dataframe\nsimcoe_1986_t2 <- dcast(simcoe_spec_1986, Week ~ Species, fun.aggregate = sum, value.var = ""Density"")\nsimcoe_1986_t2 <- log(simcoe_1986_t2 + 1)\nsimcoe_1986_t2 <- simcoe_1986_t2[-1]\n\n##### W - Total Stabilization - asynchrony among all local populations - Hammond et al 2020 #####\n# Equation - W = Sum[(1 - between population Pearson correlation coefficient) * weighted CV population i @ site k * weighted CV population j @ site l]\n#Steps\n# 1 - Calculate weighted population CV for each population\n# 1 A - Calculate weighted-average population variability - lcv = (sum(population mean/metacommunity mean)*population CV)^2\n# 2 - Calculate population pair Pearson correlation coefficients\n# 3 - Plug into equation for each population pair to calculate stabilization from each population pair\n# 4 - Sum all to calculate Total Stabilization (W)\n\n\n#Step 1\nsimcoe_1986_cv <- simcoe_1986_m %>% group_by(Year, Species, Station_ID)%>%\n  summarize(pop_mean_1986 = mean(Density),\n            pop_sd_1986 = sd(Density)) %>%\n  mutate(uw_pop_cv_1986 = pop_sd_1986/pop_mean_1986)\n\nsimcoe_1986_cv <- simcoe_1986_cv %>% group_by(Year) %>%\n  mutate(mc_sum_mean_density = sum(pop_mean_1986),\n         mean_pop_cv = mean(uw_pop_cv_1986, na.rm = T),\n         mean_pop_density = sum(pop_mean_1986),\n         mean_pop_variance = sum(pop_sd_1986))\n\nsimcoe_1986_cv <- simcoe_1986_cv %>% group_by(Species, Station_ID)%>%\n  mutate(weight = pop_mean_1986/mc_sum_mean_density)%>%\n  mutate(w_pop_cv_1986 = (pop_mean_1986/mc_sum_mean_density)*uw_pop_cv_1986) %>%\n  group_by(Year) %>%\n  mutate(sum_weight = sum(weight))\n\n### Step 1 A - lcv - Weighted average population variability ###\nsimcoe_1986_wa_pop_var_1986 <- simcoe_1986_cv %>% group_by(Year) %>%\n  summarize(wa_pop_mean_cv = mean(mean_pop_cv, na.rm = T),\n            wa_pop_mean_density = mean(mean_pop_density),\n            wa_pop_mean_variance = mean(mean_pop_variance),\n            sum_w_pop_cv_1986 =', 'sim_start <- read.csv(""Simcoe_Zooplankton.csv"", header = T)\n\nlibrary(tidyverse)\nlibrary(reshape2)\n\n\n##### Preparing the Dataframe #####\n\n#rename column headers\nsim_start <- sim_start %>%\n  summarize(Date = ZDATE,\n            Station_ID = STN,\n            Species = SPECIES,\n            Density = DENSITY....m3.)\n\n#Pull out year and month columns from date\nsim_start$Year <- format(as.Date(sim_start$Date,format=""%Y-%m-%d""),""%Y"")\nsim_start$Month <- format(as.Date(sim_start$Date,format=""%Y-%m-%d""),""%m"")\n\n\n#Subset .csv to required columns\nsim_start <- sim_start %>% select(Year, Month, Density, Date, Species, Station_ID)\n\n#Species Re-Defined According to Joelle Species List\ndaph_recomb <- subset(sim_start, Species ==\'DAPHNIA (DAPHNIA) PULICARIA\'|\n                        Species == \'DAPHNIA (DAPHNIA) CATAWBA\')\n\ndaph_recomb <- daph_recomb %>% group_by(Year, Month, Date, Station_ID) %>%\n  summarize(Density = sum(Density))\n\nsim_start <- sim_start %>%\n  filter(! Species == \'DAPHNIA (DAPHNIA) PULICARIA\'|\n           Species == \'DAPHNIA (DAPHNIA) CATAWBA\')\n\ndaph_recomb <- daph_recomb %>% mutate(Species = \'DAPHNIA (DAPHNIA) PULICARIA\')\n\nsim_start <- rbind(sim_start, daph_recomb)\n\nsim_start <- subset(sim_start,\n                    Station_ID == \'C6\'|\n                      Station_ID == \'C9\'|\n                      Station_ID == \'K42\'|\n                      Station_ID == \'K45\'|\n                      Station_ID == \'S15\')\n\n#Subset years - remove 2000 to 2007\nsim_start <- filter(sim_start, Year < 2000 | Year > 2007)\n\n#Create a column of weeks\nlibrary(aweek)\nset_week_start(""Sunday"")\nsim_start$Week <- date2week(sim_start$Date, numeric = T)\n\ndetach(""package:aweek"", unload=TRUE)\n\n#Reformat weeks to 1:n per year as time steps and remove weeks that have not all stations sampled\nsim_start <- sim_start %>% select(Year, Week, Density, Station_ID, Species)\n\nsim_start <- sim_start %>% group_by(Year, Week) %>%\n  mutate(station_week = length(unique(Station_ID)))\n\nsim_start <- sim_start %>%\n  filter(! Species == \'CALANOID COPEPODID\')\nsim_start <- sim_start %>%\n  filter(! Species == \'CALANOID NAUPLIUS\')\nsim_start <- sim_start %>%\n  filter(! Species == \'CYCLOPOID COPEPODID\')\nsim_start <- sim_start %>%\n  filter(! Species == \'CYCLOPOID NAUPLIUS\')\nsim_start <- sim_start %>%\n  filter(! Species == \'DREISSENA POLYMORPHA\')\nsim_start <- sim_start %>%\n  filter(! Species == \'BYTHOTREPHES CEDERSTROEMI\')\nsim_start <- sim_start %>%\n  filter(! Species == \'#N/A\')\n\n\nwrite.csv(sim_start, ""sim_start_weeks_manual_subset_5.csv"")\n']",3,"Species Portfolio Effects, Seasonal Zooplankton Stabilization, Large Temperate Lake, Raw Data, Public Access, Ontario, Lake Simcoe Monitoring, CSV Files, Zooplankton Sheet, Excel File, General to Short Date, YYYY"
Data from: Mandated data archiving greatly improves access to research data,"The data underlying scientific papers should be accessible to researchers both now and in the future, but how best can we ensure that these data are available? Here we examine the effectiveness of four approaches to data archiving: no stated archiving policy, recommending (but not requiring) archiving, and two versions of mandating data deposition at acceptance. We control for differences between data types by trying to obtain data from papers that use a single, widespread population genetic analysis, STRUCTURE. At one extreme, we found that mandated data archiving policies that require the inclusion of a data availability statement in the manuscript improve the odds of finding the data online almost 1000-fold compared to having no policy. However, archiving rates at journals with less stringent policies were only very slightly higher than those with no policy at all. We also assessed the effectiveness of asking for data directly from authors and obtained over half of the requested datasets, albeit with 8 d delay and some disagreement with authors. Given the long-term benefits of data accessibility to the academic community, we believe that journal-based mandatory data archiving policies and mandatory data availability statements should be more widely adopted.","['# R 2.15.2 GUI 1.53 Leopard build 64-bit (6335)\n# The R code accompanying the paper by Vines et al.\n# titled Mandated data archiving greatly improves access to research data\n# published in FASEBJ (2013)\n\n# the output of the statistical tests has been added (as comments) where deemed useful\n\n\n# read data file\ninput <- read.table (file.choose(), sep = "","", T)  \n#choose ""Vines_et_al_data""\n\n#load the lme4 library\nlibrary (lme4)\n\n\n#these are the column headings\nnames(input)\n# [1] ""journal""       ""policy_type""   ""data_online""   ""Impact_Factor""\n\n\n#set \'journal\' and \'policy_type\' as factors\ninput$journal <- as.factor (input$journal)\ninput$policy_type <- as.factor (input$policy_type)\n\n\n##################\n#First test\n#having a policy vs not having a policy (first two paragraphs of \'Statistical Analysis\' section)\n\n#create a new factor \'nopolicy_vs_policy\'\ninput$nopolicy_vs_policy<-input$policy_type\n\n#these are the different levels of the nopolicy_vs_policy factor\nlevels(input$nopolicy_vs_policy)\n#[1] ""jdap_DS""   ""jdap_noDS"" ""no_policy"" ""recommend""\n\n#changes \'recommend\', \'jdap_noDS\', and \'jdap_DS\' to \'policy\'\nlevels(input$nopolicy_vs_policy)[c(1,2,4)]<-""policy""\n\n#sets \'no policy\' as the reference level\ninput$nopolicy_vs_policy<-relevel(input$nopolicy_vs_policy, ref=""no_policy"")\n\n\n#these are the final levels of the nopolicy_vs_policy factor\nlevels(input$nopolicy_vs_policy)\n#[1] ""no_policy"" ""policy""\n\n\n#fit a mixed effects logistic regression with journal as a random factor\ntest1 <- lmer (data_online ~ nopolicy_vs_policy + (1|journal), data = input, family = ""binomial"")\n\nsummary(test1)\n\n# Generalized linear mixed model fit by the Laplace approximation \n# Formula: data_online ~ nopolicy_vs_policy + (1 | journal) \n   # Data: input \n   # AIC   BIC logLik deviance\n # 148.8 159.1 -71.42    142.8\n# Random effects:\n # Groups  Name        Variance Std.Dev.\n # journal (Intercept) 3.9472   1.9868  \n# Number of obs: 229, groups: journal, 12\n\n# Fixed effects:\n                         # Estimate Std. Error z value Pr(>|z|)   \n# (Intercept)                -3.784      1.212  -3.122   0.0018 **\n# nopolicy_vs_policypolicy    3.222      1.434   2.247   0.0247 * \n# ---\n# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \n\n# Correlation of Fixed Effects:\n            # (Intr)\n# nplcy_vs_pl -0.845\n\n\n\n# a likelihood ratio test of whether including the factor \'nopolicy_vs_policy\' improves the fit of the model\ndrop1(test1, test=""Chisq"")\n\n# Single term deletions\n\n# Model:\n# data_online ~ nopolicy_vs_policy + (1 | journal)\n                   # Df    AIC    LRT Pr(Chi)  \n# <none>                148.83                 \n# nopolicy_vs_policy  1 151.11 4.2786 0.03859 *\n# ---\n# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \n\n\n\n#the exponential of the Estimate in test1 above (3.222) gives the odds ratio of finding the data online under either policy or no_policy. The odds ratio = 25.07823, so having a policy increased the odds of the data being available online by about 25 times)\n\nexp(3.222)\n# [1] 25.07823 \n\n# 95% confidence intervals for this odds ratio - the 1.434 is the Error in test1 for the nopolicy_policy factor\nlower<- 3.222 + qnorm(0.025)* 1.434\nupper<- 3.222 + qnorm(0.975)* 1.434\n\n\nexp(cbind(3.222, lower, upper))\n               # lower    upper\n# [1,] 25.07823 1.508946 416.7924\n\n\n\n#######################\n# Second test \n#tests for an effect of policy_type on the likelihood of finding data online (see third paragraph of Statistical analysis)\n\n\n#set \'nopolicy\' to be the reference level\ninput$policy_type <- relevel(input$policy_type, ref=""no_policy"")\n\n#these are the final levels\nlevels(input$policy_type)\n#[1] ""no_policy"" ""jdap_DS""   ""jdap_noDS"" ""recommend""\n\n\n\ntest2 <- lmer (data_online ~ policy_type + (1|journal), data = input, family = ""binomial"")\nsummary(test2)\n\n# Generalized linear mixed model fit by the Laplace approximation \n# Formula: data_online ~ policy_type + (1 | journal) \n   # Data: input \n # AIC   BIC logLik deviance\n # 129 146.2 -59.52      119\n# Random effects:\n # Groups  Name        Variance   Std.Dev.\n # journal (Intercept) 4.1088e-13 6.41e-07\n# Number of obs: 229, groups: journal, 12\n\n# Fixed effects:\n                     # Estimate Std. Error z value Pr(>|z|)    \n# (Intercept)           -3.3557     0.5873  -5.714 1.11e-08 ***\n# policy_typejdap_DS     6.8821     1.1723   5.870 4.35e-09 ***\n# policy_typejdap_noDS   2.8449     0.7821   3.638 0.000275 ***\n# policy_typerecommend   1.2889     0.6765   1.905 0.056745 .  \n# ---\n# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \n\n# Correlation of Fixed Effects:\n                # (Intr) polcy_typjdp_DS plcy_typjdp_nDS\n# polcy_typjdp_DS -0.501                                \n# plcy_typjdp_nDS -0.751  0.376                         \n# plcy_typrcm     -0.868  0.435           0.652\n\n\n# likelihood ratio test of whether including the factor \'policy_type\' improves the fit of the model\ndrop1(test2, test=""Chisq"")\n\n# Single term deletions\n\n# Model:\n# data_onl']",3,"Mandated data archiving, research data, data accessibility, archiving policy, data deposition, data availability statement, population genetic analysis, STRUCTURE, journal-based policies, data accessibility statement, academic community."
Data from: Strong patterns of intraspecific variation and local adaptation in Great Basin plants revealed through a review of 75 years of experiments,"Variation in natural selection across heterogeneous landscapes often produces 1) among-population differences in phenotypic traits, 2) trait-by-environment associations, and 3) higher fitness of local populations. Using a broad literature review of common garden studies published between 1941 and 2017, we documented the commonness of these three signatures in plants native to North America's Great Basin, an area of extensive restoration and revegetation efforts, and asked which traits and environmental variables were involved. We also asked, independent of geographic distance, whether populations from more similar environments had more similar traits. From 327 experiments testing 121 taxa in 170 studies, we found 95.1% of 305 experiments reported among-population differences, and 81.4% of 161 experiments reported trait-by-environment associations. Locals showed greater survival in 67% of 24 reciprocal experiments that reported survival, and higher fitness in 90% of 10 reciprocal experiments that reported reproductive output. A meta-analysis on a subset of studies found that variation in eight commonly-measured traits was associated with mean annual precipitation and mean annual temperature at the source location, with notably strong relationships for flowering phenology, leaf size, and survival, among others. Although the Great Basin is sometimes perceived as a region of homogeneous ecosystems, our results demonstrate widespread habitat-related population differentiation and local adaptation. Locally-sourced plants likely harbor adaptations at rates and magnitudes that are immediately relevant to restoration success, and our results suggest that certain key traits and environmental variables should be prioritized in future assessments of plants in this region.","['library(fossil)\nlibrary(vegan)\nlibrary(metacor)\nlibrary(metafor)\n\n\n######################################\n\n\n## get main data\ndatA <- read.csv(""LA review_trimmed_11_30_17.csv"",header=T,na.strings=c("""",""NA""))\ndatA$BreedingSystems <- datA[,13]\ndatA <- datA[,-13]\nnames(datA)\nhead(datA)\nstr(datA)\nunique(datA$SampleGarden)\n\n## excluding wetland species\n## not bothering with Carex cause always <6 (gets excluded anyway)\ndatA <- datA[datA$SampleGarden != ""322A-Mimulus cardinalisJamestown"",]\ndatA <- datA[datA$SampleGarden != ""322A-Mimulus cardinalisMather"",]\ndatA <- datA[datA$SampleGarden != ""322A-Mimulus cardinalisTimberline"",]\ndatA <- datA[datA$SampleGarden != ""322A-Mimulus cardinalisWhite Wolf"",]\ndatA <- datA[datA$SampleGarden != ""322A-Mimulus lewisiiMather"",]\ndatA <- datA[datA$SampleGarden != ""322A-Mimulus lewisiiTimberline"",]\ndatA <- datA[datA$SampleGarden != ""322A-Mimulus lewisiiWhite Wolf"",]\ndatA <- datA[datA$SampleGarden != ""191A-Saxifraga oreganaORPMC"",]\ndatA <- datA[datA$SampleGarden != ""120A-Ranunculus flammulaeugene"",]\n\n## log transformations\ndatA$ShootMass <- log10(datA$ShootMass)\ndatA$Height <- log10(datA$Height)\ndatA$InfFlwrNumber <- log10(datA$InfFlwrNumber + 0.01)\ndatA$LeafLenWid <- log10(datA$LeafLenWid)\ndatA$SeedNumber <- log10(datA$SeedNumber + 0.01)\ndatA$FlowerSize <- log10(datA$FlowerSize)\n\n\n## give unique numbers to studies / sample gardens\nSampleGarden <- unique(datA$SampleGarden)\nlabelNum <- seq(1,length(SampleGarden))\nkeyForOwen <- data.frame(SampleGarden,labelNum)\nwrite.csv(keyForOwen,""keyForOwen.csv"")\ndatB <- merge(datA,keyForOwen,by=""SampleGarden"")\nhead(datB)\n## get it back to the object name used below\ndatA <- datB\nhead(datA)\n\n\n\n###############################################################################################################                                                        \nunique(datA$GrowthForm)\nhead(datA)\ndat1 <- datA\n\n\n## rows below used to select combinations of traits and weather variables\n\nvar1Name <- ""SurvivalPer"";var2Name <- ""MAT""; var1NameForGraph <- ""Survival percentage""   \t\t\t\t\t\t\t\t\t\t                   \n#var1Name <- ""SurvivalPer"";var2Name <- ""MAP""; var1NameForGraph <- ""Survival percentage""  \t\t\t\t\t\t\t\t\t\t                   \n\n#var1Name <- ""Height"";var2Name <- ""MAT""; var1NameForGraph <- ""Height""\t\t\t\t\t\t\t\t\t\t                   \n#var1Name <- ""Height"";var2Name <- ""MAP""; var1NameForGraph <- ""Height""\t\t\t\t\t\t\t\t\t\t\t                   \n\n#var1Name <- ""ShootMass"";var2Name <- ""MAT""; var1NameForGraph <- ""Shoot mass""\t\t\t\t\t\t\t\t\t\t\t                   \n#var1Name <- ""ShootMass"";var2Name <- ""MAP""; var1NameForGraph <- ""Shoot mass""\t\t\t\t\t\t\t\t\t\t                   \n\n#var1Name <- ""InfFlwrNumber"";var2Name <- ""MAT""; var1NameForGraph <- ""Inflorescence""\t\t\t\t\t\t\t\t\t\t                   \n#var1Name <- ""InfFlwrNumber"";var2Name <- ""MAP""; var1NameForGraph <- ""Inflorescence""\t\t\t\t\t\t\t\t\t\t\t                   \n\n#var1Name <- ""LeafLenWid"";var2Name <- ""MAT""; var1NameForGraph <- ""Leaf size""\t\t\t\t\t\t\t\t\t\t\t                   \n#var1Name <- ""LeafLenWid"";var2Name <- ""MAP""; var1NameForGraph <- ""Leaf size""\t\t\t\t\t\t\t\t\t\t                   \n\n#var1Name <- ""SeedNumber"";var2Name <- ""MAT""; var1NameForGraph <- ""Seed number""\t\t\t\t\t\t\t\t\t\t                   \n#var1Name <- ""SeedNumber"";var2Name <- ""MAP""; var1NameForGraph <- ""Seed number""\t\t\t\t\t\t\t\t\t\t                   \n\n#var1Name <- ""FlowerDay"";var2Name <- ""MAT""; var1NameForGraph <- ""Flowering day""\t\t\t\t\t\t\t\t\t\t                   \n#var1Name <- ""FlowerDay"";var2Name <- ""MAP""; var1NameForGraph <- ""Flowering day""\t\t\t\t\t\t\t\t\t\t\t                   \n\n#var1Name <- ""FlowerSize"";var2Name <- ""MAT""; var1NameForGraph <- ""Flower size""\t\t\t\t\t\t\t\t\t\t\t                   \n#var1Name <- ""FlowerSize"";var2Name <- ""MAP""\t; var1NameForGraph <- ""Flower size""\t\t\t\t\t\t\t\t\t\t                   \n\n                                                               \n                                                            \n## find and select columns\nhead(dat1)\ntraitNum <- grep(var1Name,colnames(dat1))\nweatherNum <- grep(var2Name,colnames(dat1))\nnew2 <- dat1[,c(1,10,22,23,traitNum, weatherNum)]                                    \nnew2 <- na.omit(new2)\nlength(unique(new2$SampleGarden))\nhead(new2)\ncolnames(new2)[1] <- ""SampleGarden""\n\n## make tally and exclude small samples\ntally <- aggregate(new2[,5] ~ new2$SampleGarden, FUN=""length""); colnames(tally) <- c(""SampleGarden"",""tally"")  \ntemp <- merge(new2,tally,by=""SampleGarden"")\nnew2 <- temp[temp$tally>4,]                                   #this is the number of populations in a garden\nlength(unique(new2$SampleGarden))\n\n## find SD to exclude no variation\nsdTally <- aggregate(new2[,5] ~ new2$SampleGarden, FUN=""sd""); colnames(sdTally) <- c(""SampleGarden"",""SDtally"")  \nmergeSD <- merge(sdTally, new2, by=""SampleGarden"")\nfilteredNew <- mergeSD[mergeSD$SDtally >0,]\nlength(unique(filteredNew$SampleGarden))\n\n## rename that last object back to the name that goes into the loop\nnew2 <- filteredNew\nhead(new2)\n\n## add the garden number back\nnew3 <- merge(new2,keyForOwen,by=""SampleGarden"")\nhead(new3)\nnew2 <- new3\n\n\n\n##########']",3,"intraspecific variation, local adaptation, Great Basin plants, common garden studies, phenotypic traits, trait-by-environment associations, fitness, geographic distance, environmental variables, restoration, revegetation efforts, survival, reproductive output, meta-analysis,"
R-Tutorials and presentations from the SSS Workshop on Morphometrics,"Morphometrics penetrates most of modern organismal biology. Be it the shape of a flower or the size of a fin  as systematists, we always need to quantify the phenotype. This hands-on morphometrics workshop gave an overview on the topic and let participants work with current analytical tools, in particular Multivariate Ratio Analysis (MRA) using R and MorphoJ.Here I offer some documents used in the workshop for download. Of special interest might be the R-tutorials (Documents 1a-c, 2) that were used to perform the Shape PCA or the LDA Ratio Extractor from a set of distance measurements. In order to run the files on your computer it is necessary to download two source files with MRA functions from Baur and Leuenberger (2020). You also need a dataset of human body measurements, which you get from here. For best results, put the R-scripts together with the data file in the same directory. Then start R-Studio (or any other IDE) by clicking on one of the tutorials. This ensures that R will know the path to source and data files.Geometric morphometrics (GM) of landmark data was the other main topic of the workshop. Hence, I include a few additional slides demonstrating the (unpublished) analysis of shells in Cochlostoma snails using MorphoJ (Documents 3a, b). It is interesting to compare the GM result with the MRA of distance measurements by Reichenbach et al. (2012). The data from the latter study might be obtained from Reichenbach et al. (2020).Concerning GM you should definitively also checkout the work of Christian Klingenberg in the bibliography below, in particular his papers on 'Visualization' (2013) and - brand new - 'Walking on Kendall's Shape Space' (2020). Christian has the rare gift to translate a mathematically complex topic into a beach read.Finally you may find the References section below useful, where I have put a comprehensive list of books and papers mentioned in the course.This online workshop was organized by the Swiss Systematics Society (SSS). I am grateful to the president of the SSS, Dr. Seraina Klopfstein (Natural History Museum Basel), for the kind invitation and invaluable assistance during the course.Citation of this packageUse the citation provided by Zenodo (on this webpage)ContactHannes Baur","['##########################################\n#### Tutorial 1a (SSS Morphometrics Workshop, November 5, 2020, online)\n#### Demonstration of MRA *Shape PCA*, *Ratio Spectra*, and *LDA Ratio Extractor* for male crabs\n#### When using the MRA methods, cite Baur & Leuenberger (2011) https://doi.org/10.1093/sysbio/syr061\n#### always take the newest version of the R-script at https://doi.org/10.5281/zenodo.3890195\n##########################################\n\nrm(list=ls()); ls() # Tidying up workspace\n\n#########################################################################\n### Load libraries needed\nlibrary(ggplot2) \nlibrary(plyr) \nlibrary(MASS)\nlibrary(plotrix)\n\n#########################################################################\n#### Read file with MRA functions\n#### These functions are available with a CC BY licence,\n#### hence when using them, cite \n#### Baur & Leuenberger (2020), see here https://zenodo.org/record/4250142 \nsource(""Shape_PCA_v1.02.R"")\nsource(""LDA_Ratio_Extractor_v1.03.R"")\n\n\n#### Load Leptograpsus crab dataset from\n#### Campbell & Mahon (1974) https://www.publish.csiro.au/zo/ZO9740417\ndat0 <- crabs\n\n\n#### Using a Shape PCA\n\n# Use crab males\n# dat <- dat0\ndat <- subset(dat0, sex == ""M"")\nstr(dat)\n\nX <- dat[,4:8]\nX <- X[, order(names(X))]\notu <- as.factor(dat$sp)\nno <- dat$index \nsex <- as.factor(dat$sex)\n\n# Calculate Shape PCA\nSPCA <- ShapePCA(X)\n\n\n\n#### Using result from the Shape PCA\n\n## Plot variance explained of the first 4 shapePCs by using a screeplot\nbarplot(SPCA$pc_var, main=""Screeplot"", xlab=""shapePC"", ylab=""% variance explained"")\n\n\n# Extract isosize, shapePC1 and shapePC2 to be plotted from the ShapePCA function\nisosize <- as.numeric(SPCA$isosize)\nshapePC1 <- SPCA$PCmatrix[,1] \nshapePC2 <- SPCA$PCmatrix[,2]\n\n# Put the numeric variables together a grouping, a locality and a code variable in a data frame\ndf <- data.frame(isosize, shapePC1, shapePC2, otu, sex, no)\n\n\n# Select colours for symbols in scatterplot \ncolours  <- c(""B""=""#1F78B4"", ""O""=""#FF7F00"")\n# Select shape of symbols\nshapes  <- c(""F""=17, ""M""=18)\n# Specify the sequence of those names\nbreaks  <- c(""O"", ""B"")\n# Specify, how the names should appear in the legend\nlabels  <- c(""orange"", ""blue"")\n\n\n\n## Plot shapePC1 against shapePC2 using ggplot2\nplot <- ggplot(df, aes(shapePC1, shapePC2, shape = sex, color = otu)) # Main ggplot2 function\n# find_hull <- function(df)df[chull(df$shapePC1, df$shapePC2),] # Function used for finding the points for drawing the convex hulls\n# hulls <- ddply(df, ""otu"", find_hull) # Apply the above function by using the grouping variable \'otu\'\n# plot <- plot + geom_polygon(data = hulls, aes(group=otu), colour=""gray80"", fill = NA) # Draw convex hulls around otu\nplot <- plot + geom_point(size=3) # Adjust the size of points\nplot <- plot + theme(aspect.ratio=1) # Determine the aspect ratio of plot\nplot <- plot + labs(title=""Leptograpsus rock craps"") # Main title of plot\nplot <- plot + xlab(paste(""shape PC1 ("",SPCA$pc_var[1],""%)"", sep="""")) # Label for x-axis; note, the percentage of variance explained by shapePC1 is automatically inserted from the output of the ShapePCA function  \nplot <- plot + ylab(paste(""shape PC2 ("",SPCA$pc_var[2],""%)"", sep="""")) # Label for y-axis\nplot <- plot + scale_shape_manual(values=shapes, labels=labels, breaks=breaks) # Applying values and sequence of symbol shape for the plot and its legend\nplot <- plot + scale_colour_manual(values=colours, labels=labels, breaks=breaks) # Applying values and sequence of symbol colours for the plot and its legend\nplot <- plot + labs(colour = ""otu"", shape= ""otu"") # Title of legend\n# plot <- plot + theme(legend.position = c(0.01, 0.18)) # Alternative positioning of legend, e.g. inside plot\n# plot <- plot + geom_text(aes(label=no), hjust=1.4, vjust=0) # Labelling of each point using the variable \'no\'; great for locating outliers\nplot_A <- plot + theme(axis.title.x = element_text(size=14), axis.title.y = element_text(size=14), plot.title = element_text(size=14), legend.text = element_text(face=""italic"", size=13)) # Various adjustments for the plot. Try out yourself and checkout the various information on ggplot2 in the internet!\nplot_A\n\n\n\n## Plot isosize against shapePC1\nplot <- ggplot(df, aes(isosize, shapePC1, shape = sex, color = otu))\nplot <- plot + geom_smooth(method=lm, se=FALSE, size=1)\nplot <- plot + geom_point(size=3) + theme(aspect.ratio=1)\nplot <- plot + labs(title=""Leptograpsus rock craps"")\nplot <- plot + xlab(""isometric size"")\nplot <- plot + ylab(paste(""shape PC1 ("",SPCA$pc_var[1],""%)"", sep=""""))\nplot <- plot + scale_shape_manual(values=shapes, labels=labels, breaks=breaks)\nplot <- plot + scale_colour_manual(values=colours, labels=labels, breaks=breaks)\nplot <- plot + labs(colour = ""otu"", shape= ""otu"")\n# plot <- plot + theme(legend.position = c(0.01, 0.18))\n# plot <- plot + geom_text(aes(label=no), hjust=1.4, vjust=0)\nplot_B <- plot + theme(axis.title.x = element_text(size=14), axis.title.y = element_text(size=14), plot.title = element_text(size=', '##########################################\n#### Tutorial 1b (SSS Morphometrics Workshop, November 5, 2020, online)\n#### Demonstration of MRA *Shape PCA*, *Ratio Spectra*, and *LDA Ratio Extractor* for all crabs\n#### When using the MRA methods, cite Baur & Leuenberger (2011) https://doi.org/10.1093/sysbio/syr061\n#### always take the newest version of the R-script at https://doi.org/10.5281/zenodo.3890195\n##########################################\n\nrm(list=ls()); ls() # Tidying up workspace\n\n#########################################################################\n### Load libraries needed\nlibrary(ggplot2) \nlibrary(plyr) \nlibrary(MASS)\nlibrary(plotrix)\n\n#########################################################################\n#### Read file with MRA functions\n#### These functions are available with a CC BY licence,\n#### hence when using them, cite \n#### Baur & Leuenberger (2020), see here https://zenodo.org/record/4250142 \nsource(""Shape_PCA_v1.02.R"")\nsource(""LDA_Ratio_Extractor_v1.03.R"")\n\n\n#### Load Leptograpsus crab dataset from\n#### Campbell & Mahon (1974) https://www.publish.csiro.au/zo/ZO9740417\ndat0 <- crabs\n\n\n#### Using a Shape PCA\n\n# Use crab males\ndat <- dat0\nstr(dat)\n\nX <- dat[,4:8]\nX <- X[, order(names(X))]\notu <- as.factor(dat$sp)\nno <- dat$index \nsex <- as.factor(dat$sex)\n\n# Calculate Shape PCA\nSPCA <- ShapePCA(X)\n\n\n#### Using result from the Shape PCA\n\n## Plot variance explained of the first 4 shapePCs by using a screeplot\nbarplot(SPCA$pc_var, main=""Screeplot"", xlab=""shapePC"", ylab=""% variance explained"")\n\n\n# Extract isosize, shapePC1 and shapePC2 to be plotted from the ShapePCA function\nisosize <- as.numeric(SPCA$isosize)\nshapePC1 <- SPCA$PCmatrix[,1] \nshapePC2 <- SPCA$PCmatrix[,2]\n\n# Put the numeric variables together a grouping, a locality and a code variable in a data frame\ndf <- data.frame(isosize, shapePC1, shapePC2, otu, sex, no)\n\n\n# Select colours for symbols in scatterplot \ncolours  <- c(""B""=""#1F78B4"", ""O""=""#FF7F00"")\n# Select shape of symbols\nshapes  <- c(""F""=1, ""M""=4)\n# Specify the sequence of those names\nbreaksA  <- c(""O"", ""B""); breaksB  <- c(""M"", ""F"")\n# Specify, how the names should appear in the legend\nlabelsA  <- c(""orange"", ""blue""); labelsB  <- c(""male"", ""female"")\n\n\n\n## Plot shapePC1 against shapePC2 using ggplot2\nplot <- ggplot(df, aes(shapePC1, shapePC2, shape = sex, color = otu)) # Main ggplot2 function\n# find_hull <- function(df)df[chull(df$shapePC1, df$shapePC2),] # Function used for finding the points for drawing the convex hulls\n# hulls <- ddply(df, ""otu"", find_hull) # Apply the above function by using the grouping variable \'otu\'\n# plot <- plot + geom_polygon(data = hulls, aes(group=otu), colour=""gray80"", fill = NA) # Draw convex hulls around otu\nplot <- plot + geom_point(size=3) # Adjust the size of points\nplot <- plot + theme(aspect.ratio=1) # Determine the aspect ratio of plot\nplot <- plot + labs(title=""Leptograpsus rock craps"") # Main title of plot\nplot <- plot + xlab(paste(""shape PC1 ("",SPCA$pc_var[1],""%)"", sep="""")) # Label for x-axis; note, the percentage of variance explained by shapePC1 is automatically inserted from the output of the ShapePCA function  \nplot <- plot + ylab(paste(""shape PC2 ("",SPCA$pc_var[2],""%)"", sep="""")) # Label for y-axis\nplot <- plot + scale_shape_manual(values=shapes, labels=labelsB, breaks=breaksB) # Applying values and sequence of symbol shape for the plot and its legend\nplot <- plot + scale_colour_manual(values=colours, labels=labelsA, breaks=breaksA) # Applying values and sequence of symbol colours for the plot and its legend\nplot <- plot + labs(colour = ""otu"", shape= ""sex"") # Title of legend\n# plot <- plot + theme(legend.position = c(0.01, 0.18)) # Alternative positioning of legend, e.g. inside plot\n# plot <- plot + geom_text(aes(label=no), hjust=1.4, vjust=0) # Labelling of each point using the variable \'no\'; great for locating outliers\nplot_A <- plot + theme(axis.title.x = element_text(size=14), axis.title.y = element_text(size=14), plot.title = element_text(size=14), legend.text = element_text(face=""italic"", size=13)) # Various adjustments for the plot. Try out yourself and checkout the various information on ggplot2 in the internet!\nplot_A\n\n\n\n\n## Plot isosize against shapePC1\nplot <- ggplot(df, aes(isosize, shapePC1, shape = sex, color = otu))\nplot <- plot + geom_smooth(method=lm, se=FALSE, size=1)\nplot <- plot + geom_point(size=3) + theme(aspect.ratio=1)\nplot <- plot + labs(title=""Leptograpsus rock craps"")\nplot <- plot + xlab(""isometric size"")\nplot <- plot + ylab(paste(""shape PC1 ("",SPCA$pc_var[1],""%)"", sep=""""))\nplot <- plot + scale_shape_manual(values=shapes, labels=labelsB, breaks=breaksB) # Applying values and sequence of symbol shape for the plot and its legend\nplot <- plot + scale_colour_manual(values=colours, labels=labelsA, breaks=breaksA) # Applying values and sequence of symbol colours for the plot and its legend\nplot <- plot + labs(colour = ""otu"", shape= ""otu"")\n# plot <- plot + theme(legend.position = c(0.01, 0.18))\n# plot <-', '##########################################\n#### Tutorial 1c\n#### Clustering of crabs data\n#### You may use the code under the following licence: \n#### CC BY-SA 4.0, Hannes Baur, Natural History Museum Bern, https://www.nmbe.ch/en/hannes.baur\n##########################################\n\nrm(list=ls()); ls() # Tidying up workspace\n\n#########################################################################\n### Load libraries needed\nlibrary(cluster)\nlibrary(MASS)\n\n#########################################################################\n#### Read file with MRA functions\n#### These functions are available with a CC BY licence,\n#### hence when using them, cite \n#### Baur & Leuenberger (2020), see here https://zenodo.org/record/4250142 \nsource(""Shape_PCA_v1.02.R"")\n#### Campbell & Mahon (1974) https://www.publish.csiro.au/zo/ZO9740417\ndat0 <- crabs\n\n\n\n#### Cluster Analysis with agnes(). \n#### Many other possibilietes for clustering avaialble that \n#### often give very different results. Unfortunately,\n#### it is impossible to say, which method to choose in first place!\n\n# Use crab males\ndat <- subset(dat0, sex == ""M"")\nstr(dat)\n\nX <- dat[,c(4:8)]; row.names(X) <- paste(c(rep(""B"",50), rep(""O---"",50)), c(101:200), sep="""")\nX <- isofreeshapes(X)\n\n# Clustering\ncluster_ward <- agnes(dist(X),method=""ward""); plot(cluster_ward, which.plots=2, hang=-1, cex=1)\n\ncluster_aver <- agnes(dist(X),method=""average""); plot(cluster_aver, which.plots=2, hang=-1, cex=1)\n\n', '##########################################\n#### Tutorial 2 (SSS Morphometrics Online Workshop, November 5, 2020)\n#### Basic exploration of body measurements \n#### (Reliability analyisis, Matrixplots, PCA, LDA, Clustering)\n#### Data supplied by participants of a morphometrics course, see https://doi.org/10.5281/zenodo.4266229\n#### You may use the code under the following licence: \n#### CC BY 4.0, Hannes Baur, Natural history Museum Bern, https://www.nmbe.ch/en/hannes.baur\n##########################################\n\nrm(list=ls()); ls()  # Tidying up workspace\n\n\n##########################################\n#### Read function for calculating reliability (R). \n#### It is based on calculating measurement error (ME) using Model II ANOVA, \n#### following Bailey & Byrnes (1990, https://doi.org/10.2307/2992450) and Sokal & Rohlf (1995: 196-197, 208-214). \n#### Reliability then is R = 100 - ME.\n#### The function is correct only for equal sample sizes,\n#### hence for each character you need the same number of readings\n\nreliability <- function(x, g, rounding=2) {  \n  x <- data.frame(x)  # data frame of measurements\n  g <- as.factor(g)  # grouping variable, e.g. specimen number\n  n <- length(g)/nlevels(g)  # sample size of each group\n  varname <- colnames(x) # names of variables\n  k <- 0\n  m.error <- matrix(NA, nrow=ncol(x), ncol=1); m.error <- as.vector(m.error) # transforming matrix A to vector as placeholder for measurement errors\n  \n  for (i in 1:ncol(x)) {\n    k <- k+1\n    model <- aov(x[,i] ~ g)\n    anova <- anova(model)\n    msa <- anova[1,3]  # among groups mean sums of squares (= among groups variance)\n    mse <- anova[2,3]  # within groups mean sums of squares (= within groups variance)\n    sa <- 1/n*(msa-mse)  # added variance component among groups (Sokal & Rohlf 1995: 197)\n    merror <- mse/(mse+sa)*100  \n    m.error[k] <- round(merror, rounding)\n  }\n  me <- data.frame(varname, m.error)\n  re <- data.frame(varname, R=(100-m.error))\n  list(me=me, re=re)\n}\n\n##########################################\n\n\n\n\n\n\n\n##########################################\n#### Load dataset and select necessary factor variables\n# Download file with body measurements from here https://doi.org/10.5281/zenodo.4266229\nfilename <- ""Body_Measurements.csv"" \ndat0 <- read.csv(filename)\ndat0 <- dat0\n# str(dat0)\n\n#### Select and order numeric variables\nX0 <- dat0[, c(4:8)]; X0 <- X0[, order(names(X0))]\n# str(X0)\n\n#### Select factor variables\nperson0 <- as.factor(dat0$person); sex0 <- as.factor(dat0$sex); reading0 <- as.factor(dat0$reading)\n\n##########################################\n\n\n\n\n##########################################\n#### Calculation of reliability (R)\n\n#### Compile and print R table\nR.table <- reliability(X0, person0)\nR.table <- R.table$re # table with characters in unordered sequence\n(R.table <- R.table[with(R.table, order(R)), ]) # table with characters ordered by increasing reliability (R)\n\n#### Barplot\nbarplot(R.table$R, main=""Reliability of characters (ordered by increasing R)"", ylab=""% reliability (R)"", ylim=c(1, 100), names.arg=R.table$varname, cex.names=1.2)\n\n##########################################\n\n\n\n\n##########################################\n#### Matrix scatterplot for checking the data\n\nplot(X0, col=c(4,3)[reading0], pch=c(4,1)[sex0], cex=1.5)\n#### Do they look good for you?\n\n##########################################\n\n\n\n\n\n\n\n\n##########################################\n#### For further analysis the separate readings must be aggregated\n#### We do that by calculating the mean of the two readings for each person\n#### We also keep sex as a factor variable\n\ndat <- aggregate(dat0[,4:8], by=dat0[c(""sex"", ""person"")], FUN=mean)\nX <- dat[, c(3:7)]; X <- X[, order(names(X))]\nsex <- as.factor(dat$sex)\n\n##########################################\n\n\n\n\n##########################################\n#### Matrix plot of reading means\n\nplot(X, col=c(4,3)[sex], pch=c(4,1)[sex], cex=1.5)\n#### How does it compare to plot before?\n\n##########################################\n\n\n\n\n\n\n##########################################\n#### Let\'s do a Principal Component Analysis (PCA)\n#### This is a standard PCA after Jolicoeur (1963, http://dx.doi.org/10.2307/2527939)\n#### We will use the covariance matrix of log-transformed values, see\n#### Baur & Leuenberger (2011, https://doi.org/10.1093/sysbio/syr061) for the reason why.\n#### We will again take sex as a factor variable\n\nY <- log(X)\nPCA <- prcomp(Y, scale. = FALSE, center = TRUE) # scale. = FALSEmakes sure that the covariance matrix is used; otherwise it would be a PCA on the correlation matrix, which would be inappropriate here\n\n#### Check variation explained by each principal component\nscreeplot(PCA)\n#### Only the first two are important (as usual)\n\n#### Plot first and second principal component\n#### In such a Jolicoeur PCA of body measurments, \n#### the first PC is always an ALLOMETRIC SIZE axis\n#### PC2 and following are shape axes, or, to put it another way round, size-free axes\n\n#### IMPORTANT NOTE: Bec']",3,"Morphometrics, R-workshops, presentations, SSS workshop, Multivariate Ratio Analysis, MRA, geometric morphometrics, landmark data, Cochlostoma snails, distance measurements, Shape PCA, LDA Ratio Extractor, human body measurements"
Data from: Maternal environment affects the genetic basis of seed dormancy in Arabidopsis thaliana,"The genetic basis of seed dormancy, a key life-history trait important for adaptive evolution in plant populations, has yet been studied only using seeds produced under controlled conditions in greenhouse environments. However, dormancy is strongly affected by maternal environmental conditions, and interactions between seed genotype and maternal environment have been reported. Consequently, the genetic basis of dormancy of seeds produced under natural field conditions remains unclear. We examined the effect of maternal environment on the genetic architecture of seed dormancy using a recombinant inbred line (RIL) population derived from a cross between two locally adapted populations of Arabidopsis thaliana from Italy and Sweden. We mapped quantitative trait loci (QTL) for dormancy of seeds produced in the greenhouse and at the native field sites of the parental genotypes. The Italian genotype produced seeds with stronger dormancy at fruit maturation than did the Swedish genotype in all three environments, and the maternal field environments induced higher dormancy levels compared to the greenhouse environment in both genotypes. Across the three maternal environments, a total of nine dormancy QTL were detected, three of which were only detected among seeds matured in the field, and six of which showed significant QTL  maternal environment interactions. One QTL had a large effect on dormancy across all three environments and colocalized with the candidate gene DOG1. Our results demonstrate the importance of studying the genetic basis of putatively adaptive traits under relevant conditions.","['# R/qtl analysis of the dormancy of seeds produced in 3 different maternal environments.\r\n# For more details on R/qtl, see: Broman KW, Sen S (2009) A Guide to QTL Mapping with R/qtl Springer, New York.\r\n\r\n# The QTL analysis is performed on the untransformed phenotypes and \r\n# on binary and quantile-normalized subsets to check the robustness of the QTL mapping results based on the nonnormal phenotypes\r\n\r\n\r\n#### Load the data ####\r\nrm(list=ls())\r\nlibrary(qtl)\r\nmydata <-read.cross(""csv"", dir="""", ""RILgeno_phenotypes.csv"", na.strings=NA, genotypes = c(""a"", ""b""), alleles=c(""It"",""Sw""))\r\nrildata<-convert2riself(mydata)\r\n\r\n#### Quantile-normalize subset-data ####\r\nquantnorm<-function(x) {\r\n  n=sum(!is.na(x),na.rm=T)\r\n  x=rank(x)/(n+1)\r\n  x=qnorm(x)\r\n  x[is.infinite(x)]=NA\r\n  x }\r\n\r\nnames(rildata$pheno)\r\nrildata$pheno$qn.ItF12<-quantnorm(rildata$pheno$ItF12)\r\nrildata$pheno$qn.SwF12_sub<-quantnorm(rildata$pheno$SwF12_sub)\r\nrildata$pheno$qn.parC_sub<-quantnorm(rildata$pheno$parC_sub)\r\n\r\n\r\n#### Run the permutations ####\r\n# OBS! This code is written to run the permutations on 8 clusters\r\n# The binary permutations take a very long time and were therefore divided in 10x1000 permutations\r\n\r\nrildata<-calc.genoprob(rildata, step=2, error.prob=.0001, map.function=""kosambi"",stepwidth=""max"", off.end=0)\r\n\r\n## Permutations on unmanipulated data ##\r\nperm2_GHwk1<-scantwo(rildata, pheno.col=""GHwk1"", model=""normal"",  method=""hk"",  n.perm=10000, n.cluster=8,\r\n                     addcovar=NULL, intcovar=NULL, weights=NULL, clean.output=FALSE, clean.nmar=FALSE, \r\n                     clean.distance=FALSE, incl.markers=TRUE,  assumeCondIndep=FALSE,  verbose=TRUE)\r\nsave(perm2_GHwk1, file=""perm2_GHwk1_10k.RData"")\r\n\r\nperm2_ItF12<-scantwo(rildata, pheno.col=""ItF12"", model=""normal"",  method=""hk"",  n.perm=10000, n.cluster=8,\r\n                     addcovar=NULL, intcovar=NULL, weights=NULL, clean.output=FALSE, clean.nmar=FALSE, \r\n                     clean.distance=FALSE, incl.markers=TRUE,  assumeCondIndep=FALSE,  verbose=FALSE)\r\nsave(perm2_ItF12, file=""perm2_ItF12_10k.RData"")\r\n\r\nperm2_SwF12<-scantwo(rildata, pheno.col=""SwF12"", model=""normal"",  method=""hk"",  n.perm=10000,n.cluster=8, \r\n                     addcovar=NULL, intcovar=NULL, weights=NULL, clean.output=FALSE, clean.nmar=FALSE, \r\n                     clean.distance=FALSE, incl.markers=TRUE,  assumeCondIndep=FALSE,  verbose=FALSE)\r\nsave(perm2_SwF12, file=""perm2_SwF12_10k.RData"")\r\n\r\nperm2_parC<-scantwo(rildata, pheno.col=""parC"", model=""normal"",  method=""hk"",  n.perm=10000, n.cluster=8,\r\n                    addcovar=NULL, intcovar=NULL, weights=NULL, clean.output=FALSE, clean.nmar=FALSE, \r\n                    clean.distance=FALSE, incl.markers=TRUE,  assumeCondIndep=FALSE,  verbose=FALSE)\r\nsave(perm2_parC, file=""perm2_parC_10k.RData"")\r\n\r\n## Permutations on quantile-normalized subsets of data ##\r\nperm2_qn.SwF12_sub<-scantwo(rildata, pheno.col=""qn.SwF12_sub"", model=""normal"",  method=""hk"",  n.perm=10000, n.cluster=7,\r\n                            addcovar=NULL, intcovar=NULL, weights=NULL, clean.output=FALSE, clean.nmar=FALSE, \r\n                            clean.distance=FALSE, incl.markers=TRUE,  assumeCondIndep=FALSE,  verbose=TRUE)\r\nsave(perm2_qn.SwF12_sub, file=""perm2_qn.SwF12_sub_10k.RData"")\r\n\r\nperm2_qn.parC_sub<-scantwo(rildata, pheno.col=""qn.parC_sub"", model=""normal"",  method=""hk"",  n.perm=10000, n.cluster=7,\r\n                           addcovar=NULL, intcovar=NULL, weights=NULL, clean.output=FALSE, clean.nmar=FALSE, \r\n                           clean.distance=FALSE, incl.markers=TRUE,  assumeCondIndep=FALSE,  verbose=TRUE)\r\nsave(perm2_qn.parC_sub, file=""perm2_qn.parC_sub_10k.RData"")\r\n\r\nperm2_qn.ItF12<-scantwo(rildata, pheno.col=""qn.ItF12"", model=""normal"",  method=""hk"",  n.perm=10000, n.cluster=7,\r\n                        addcovar=NULL, intcovar=NULL, weights=NULL, clean.output=FALSE, clean.nmar=FALSE, \r\n                        clean.distance=FALSE, incl.markers=TRUE,  assumeCondIndep=FALSE,  verbose=TRUE)\r\nsave(perm2_qn.ItF12, file=""perm2_qn.ItF12_10k.RData"")\r\n\r\n\r\n## Binary permutations ##\r\n# These permutation take a very long computation time!\r\n# These permutations were performed 10 times with 1000 permutations each time, and seed set was changed for each time\r\n# After that, the permutations were put together using this code:\r\n#perm2_GHwk1_bin<-c(perm_GHwk1_bin_1, perm_GHwk1_bin_2, perm_GHwk1_bin_3, perm_GHwk1_bin_4, perm_GHwk1_bin_5,perm_GHwk1_bin_6, perm_GHwk1_bin_7, perm_GHwk1_bin_8, perm_GHwk1_bin_9, perm_GHwk1_bin_10)\r\n#save(perm2_GHwk1_bin, file=""perm2_GHwk1_bin_10000p.RData"")\r\n\r\nset.seed(071510)\r\nperm_GHwk1_bin_1 <-scantwo(rildata, pheno.col=""GHwk1_bin"",  model= ""binary"", method=""hk"",  n.perm=1000,\r\n                           n.cluster=8, addcovar=NULL, intcovar=NULL, weights=NULL,\r\n                           clean.output=FALSE, clean.nmar=FALSE, clean.distance=FALSE,\r\n                           incl.markers=TRUE,  assumeCondIndep=FALSE,  verbose=TRUE)\r\nsave(perm_GHwk1_bin_1,']",3,"Maternal environment, genetic basis, seed dormancy, Arabidopsis thaliana, controlled conditions, greenhouse environments, adaptive evolution, plant populations, field conditions, recombinant inbred line, RIL population, Italy, Sweden, quantitative trait"
Evidence of climate-driven selection on tree traits and trait plasticity across the climatic range of a riparian foundation species,"Selection on quantitative traits by heterogeneous climatic conditions can lead to substantial trait variation across a species range. In the context of rapidly changing environments, however, it is equally important to understand selection on trait plasticity. To evaluate the role of selection in driving divergences in traits and their associated plasticities within a widespread species, we compared molecular and quantitative trait variation in Populus fremontii (Fremont cottonwood), a foundation riparian distributed throughout Arizona. Using SNP data and genotypes from 16 populations reciprocally planted in three common gardens, we first performed QST-FST analyses to detect selection on traits and trait plasticity. We then explored the environmental drivers of selection using trait-climate and plasticity-climate regressions. Three major findings emerged: 1) There was significant genetic variation in traits expressed in each of the common gardens and in the phenotypic plasticity of traits across gardens, both of which were heritable. 2) Based on QST-FST comparisons, there was evidence of selection in all traits measured; however, this result varied from no effect in one garden to highly significant in another, indicating that detection of past selection is environmentally dependent. We also found strong evidence of divergent selection on plasticity across environments for two traits. 3) Traits and/or their plasticity were often correlated with population source climate (R2 up to 0.77 and 0.66, respectively). These results suggest that steep climate gradients across the Southwest have played a major role in shaping the evolution of divergent phenotypic responses in populations and genotypes now experiencing climate change.","['### Random sampling 100 times to create plasticity replication in order to calculate Qst values for plasticity.\r\n\nlibrary(dplyr)\n\n## BUDSET \n# Read in .csv file\r\nsetwd(""~/Desktop/Qst-Fst/QSTs/Qst/Bud Flush"")\r\nf<-read.csv(""AllFlush.csv"",stringsAsFactors = T)\r\n\r\nxagg=aggregate(flush ~ Garden+PopGeno,data=f,length) #how many replicates in each geno and garden. flush is the trait of interest\r\nxagg2=aggregate(flush~PopGeno,data=xagg,length) # how many reps across all three gardens \r\npopgenlist=xagg2[xagg2$flush==3,""PopGeno""] #list of only the genotypes that are present in all three gardens \r\n\r\nx=f[f$PopGeno %in% popgenlist,]\r\n\r\nallP=matrix(nrow=487,ncol=100) #nrow = length(plast$P), ncol = 100 randomizations for the plasticity datasets.\r\n  \r\nfor (i in 1:100){\r\n\r\nx$rand=rnorm(length(x$flush))\r\n\r\nxord=x[order(x$PopGeno,x$Garden,x$rand),]\r\nxord$pair=vector(length=nrow(xord))\r\n\r\nrepdata=data.frame()\r\n\r\nfor(pg in popgenlist){\r\n  pgtemp=xord[xord$PopGeno==pg,]\r\n  for(g in levels(x$Garden)){\r\n    pgtemp[pgtemp$Garden==g,""pair""]<-seq(from=1,to=length(pgtemp[pgtemp$Garden==g,""pair""]),by=1)\r\n  }\r\n  pgtempagg=aggregate(flush~pair,data=pgtemp,length)\r\n  pgfin=pgtemp[pgtemp$pair < max(pgtempagg[pgtempagg$flush==3,""pair""]+1),]\r\n\r\nrepdata=rbind(repdata,pgfin)  \r\n  }\r\n  \r\nplast <- repdata %>% \r\n  select(flush, PopGeno, Population, Garden,pair) %>%\r\n  group_by(PopGeno,pair) %>%\r\n  mutate(P = max(flush)-min(flush)) %>% # Generate plasticity scores\r\n  filter(Garden==""AF"") # Just choose one garden since this variable is now irrelevant (plasticity is across the three gardens) \r\n\r\nallP[,i]<-plast$P\r\n\r\n}\r\n\r\nfinal=data.frame(plast[,c(""PopGeno"", ""Population""),],allP)\r\nwrite.csv(final, ""FlushPlastReps.csv"")']",4,"climate-driven selection, tree traits, trait plasticity, Populus fremontii, riparian, Arizona, SNP data, genotypes, QST-FST analyses, heritable, genetic variation, common gardens, phenotypic plasticity, selection"
Data from: Biotic stability mechanisms in Inner Mongolian grassland,"Biotic mechanisms associated with species diversity are expected to stabilize communities in theoretical and experimental studies but may be difficult to detect in natural communities exposed to large environmental variation. We investigated biotic stability mechanisms in a multi-site study across Inner Mongolian grassland characterized by large spatial variations in species richness and composition and temporal fluctuations in precipitation. We used a new additive-partitioning method to separate species synchrony and population dynamics within communities into different species-abundance groups. Community stability was independent of species richness but was regulated by species synchrony and population dynamics, especially of abundant species. Precipitation fluctuations synchronized population dynamics within communities, reducing their stability. Our results indicate generality of biotic stability mechanisms in natural ecosystems and suggest that for accurate predictions of community stability in changing environments uneven species composition should be considered by partitioning stabilizing mechanisms into different species-abundance groups.","['################################################################################\n########               Wang_et_al._2020_R_Codes_&_Results               ########\n################################################################################\n# Please run the ""Wang_et_al._2020_R_Codes_for_Calculations.R"" file first.     #\n################################################################################\n\nf_path = """"\nsetwd(f_path)\n\n# All variables in ""Wang_et_al._2020_Data_Table.csv"" file were calculated\n# using ""Wang_et_al._2020_R_Codes_for_Calculations.R""\ncmda = read.csv(""Wang_et_al._2020_Data_Table.csv"", header=T)\nattach(cmda)\nnames(cmda)\n\n################################################################################\n# Abbreviations and names of variables\n#------------------------------------------------------------------------------#\n# Site      | Site number\n# Latitude  | Latitude\n# Longitude | Longitude\n# VegeType  | Vegetation type\n# MGP       | mean growing-season precipitation\n# CV_MGP    | MGP temporal coefficient of variation (CV)\n# CMU_BM    | Community biomass\n# CMU_SR    | Community species richness\n# D_SR      | Dominant species richness\n# C_SR      | Common species richness\n# R_SR      | Rare species richness\n# CMU_ER    | Community effective species richness\n# D_ER      | Dominant effective species richness\n# C_ER      | Common effective species richness\n# R_ER      | Rare effective species richness\n# CMU_CV    | Community temporal CV\n# CMU_SCV   | Communtiy-wide weighted average species temporal CV\n# CMU_SYNC  | Communtiy-wide species synchrony\n# Taylor_Z  | Mean-variance scaling\n# D_SCV     | Weighted average dominant species temporal CV\n# C_SCV     | Weighted average common species temporal CV\n# R_SCV     | Weighted average rare species temporal CV\n# DOM_SD    | Temporal standard deviation of dominant species group\n# DOM_BM    | Biomass of dominant species group\n# DD_SYNC   | Synchronous species dynamic between dominant species\n# DC_SYNC   | Synchronous species dynamic between dominant and common species\n# DR_SYNC   | Synchronous species dynamic between dominant and rare species\n# CC_SYNC   | Synchronous species dynamic between common species\n# CR_SYNC   | Synchronous species dynamic between common and rare species\n# RR_SYNC   | Synchronous species dynamic between rare species\n#------------------------------------------------------------------------------#\n################################################################################\n\n################################################################################\n# R code and results used in Figure 1\nlibrary(Hmisc)\n\n# Figure 1a\nrcorr(cbind(MGP, CV_MGP, CMU_SR, CMU_ER, Taylor_Z, CMU_SCV, CMU_SYNC, CMU_CV), type=""pearson"")\n#            MGP CV_MGP CMU_SR CMU_ER Taylor_Z CMU_SCV CMU_SYNC CMU_CV\n# MGP       1.00  -0.09   0.64   0.52    -0.11   -0.34    -0.27  -0.50\n# CV_MGP   -0.09   1.00   0.26  -0.03    -0.01   -0.03     0.57   0.22\n# CMU_SR    0.64   0.26   1.00   0.80     0.15    0.13    -0.04  -0.08\n# CMU_ER    0.52  -0.03   0.80   1.00     0.15    0.25    -0.49  -0.20\n# Taylor_Z -0.11  -0.01   0.15   0.15     1.00    0.73     0.04   0.60\n# CMU_SCV  -0.34  -0.03   0.13   0.25     0.73    1.00    -0.10   0.70\n# CMU_SYNC -0.27   0.57  -0.04  -0.49     0.04   -0.10     1.00   0.54\n# CMU_CV   -0.50   0.22  -0.08  -0.20     0.60    0.70     0.54   1.00\n# \n# n= 23 \n# \n# \n# P\n#          MGP    CV_MGP CMU_SR CMU_ER Taylor_Z CMU_SCV CMU_SYNC CMU_CV\n# MGP             0.6674 0.0011 0.0111 0.6037   0.1159  0.2127   0.0143\n# CV_MGP   0.6674        0.2390 0.8982 0.9667   0.8952  0.0046   0.3079\n# CMU_SR   0.0011 0.2390        0.0000 0.4874   0.5460  0.8620   0.7147\n# CMU_ER   0.0111 0.8982 0.0000        0.4876   0.2570  0.0176   0.3578\n# Taylor_Z 0.6037 0.9667 0.4874 0.4876          0.0000  0.8711   0.0024\n# CMU_SCV  0.1159 0.8952 0.5460 0.2570 0.0000           0.6622   0.0002\n# CMU_SYNC 0.2127 0.0046 0.8620 0.0176 0.8711   0.6622           0.0075\n# CMU_CV   0.0143 0.3079 0.7147 0.3578 0.0024   0.0002  0.0075         \n\n# Figure 1b\nrcorr(cbind(MGP, CV_MGP, D_SR, D_ER, Taylor_Z, D_SCV, DD_SYNC, CMU_CV), type=""pearson"")\n#            MGP CV_MGP  D_SR  D_ER Taylor_Z D_SCV DD_SYNC CMU_CV\n# MGP       1.00  -0.09  0.05  0.21    -0.11 -0.56   -0.51  -0.50\n# CV_MGP   -0.09   1.00 -0.42 -0.46    -0.01 -0.12    0.44   0.22\n# D_SR      0.05  -0.42  1.00  0.94     0.12  0.22   -0.70  -0.25\n# D_ER      0.21  -0.46  0.94  1.00     0.06  0.10   -0.77  -0.40\n# Taylor_Z -0.11  -0.01  0.12  0.06     1.00  0.68    0.07   0.60\n# D_SCV    -0.56  -0.12  0.22  0.10     0.68  1.00    0.14   0.70\n# DD_SYNC  -0.51   0.44 -0.70 -0.77     0.07  0.14    1.00   0.53\n# CMU_CV   -0.50   0.22 -0.25 -0.40     0.60  0.70    0.53   1.00\n# \n# n= 23 \n# \n# \n# P\n#          MGP    CV_MGP D_SR   D_ER   Taylor_Z D_SCV  DD_SYNC CMU_CV\n# MGP             0.6674 0.8216 0.3368 0.6037   0.0054 0.0121  0.0143\n# CV_MGP   0.6674        0.0437 0.0285 0.9667   0.5847 0.0363  0.3079\n# D_SR     0.8216 0.04', '################################################################################\n########           Wang_et_al._2020_R_Codes_for_Calculations            ########\n################################################################################\n\n# quartz(width=5, height=7); par(mfrow=c(3, 2))\n\nrm(list=ls())\nf_path = """"\nsetwd(f_path)\n\ncom1 = read.csv(""Wang_et_al._2020_Raw_Community_Dataset.csv"", header=T, fileEncoding=""GBK"")\ncli1 = read.csv(""Wang_et_al._2020_Raw_Climate_Dataset.csv"", header=T, fileEncoding=""GBK"")\n\n################################################################################\n# The following codes were used to calculate targeted variables, such as \n# multiple-year mean growing-season precipitation, community temporal CV, \n# weighted average species temporal CV and species synchrony and so on.\n################################################################################\n\n################################################################################\n# Following codes used to calculate:\n# bm_site: multiple-year mean community biomass of each site\n# pre_site: mean growing-season precipitation of each site\n# pre_cv: CV of growing-season precipitationtemperature of each site\n\ncom1$cBM = ifelse(apply(com1[, 7:251], 1, sum, na.rm=T)==0, NA, apply(com1[, 7:251], 1, sum, na.rm=T))\nbiomass_label = ifelse(is.na(tapply(com1$cBM, list(com1$Site, com1$Year), mean, na.rm=T)), NA, 1)\n\npre = tapply(cli1$Prep, list(cli1$Site, cli1$Year), sum, na.rm=T)\nfor (i in 1:5) {\n    pre[, i] = pre[, i] * biomass_label[, i]\n}\npre_site = apply(pre, 1, function(x)(mean(x, na.rm=T)))\npre_cv = apply(pre, 1, function(x)(sd(x, na.rm=T)))/pre_site\nbm_site = tapply(com1$cBM, list(com1$Site), mean, na.rm=T)\n\n################################################################################\n# Following codes used to calculate:\n# sync_site: species synchrony of each site\n\nspe_bm_year = NULL\nfor (i in 1:length(unique(com1$Site))) {\n    temp1 = subset(com1, Site==unique(com1$Site)[i])\n    \n    for (j in 1:length(unique(temp1$Year))) {\n        temp2 = subset(temp1, Year==unique(temp1$Year)[j])\n        \n        spe_bm_year1 = temp2[1, ]\n        spe_bm_year1[1, 7:251] = apply(temp2[, 7:251], 2, function(x)(sum(x, na.rm=T)))/3\n        spe_bm_year1[1, ] = ifelse(spe_bm_year1[1,]==0, NA, spe_bm_year1[1,])\n        \n        spe_bm_year = rbind(spe_bm_year, spe_bm_year1)\n    }\n}\n\nspe_bmsd_site = as.data.frame(matrix(NA, ncol=251, nrow=23, dimnames =list(NULL, c(names(spe_bm_year)[1:251]))))\nfor (i in 1:23) {\n    spe_bmsd_site[i, 1:6] = spe_bm_year[i, 1:6]\n    for (j in 7:251) {\n        spe_bmsd_site[i, j] = sd(spe_bm_year[spe_bm_year$Site==i, j], na.rm=T)\n    }\n}\n\nbmvar_site = apply(tapply(com1$cBM, list(com1$Site, com1$Year), mean, na.rm=T), 1, function(x)var(x, na.rm=T))\nsumvar_site = (apply(spe_bmsd_site[, 7:251], 1, function(x)sum(x, na.rm=T))^2)\nsync_site = (bmvar_site / ifelse(sumvar_site==0, NA, sumvar_site))\n\n################################################################################\n# Following codes used to calculate:\n# cbm_cv: community temporal CV of each site\n\ncbm_cv = sqrt(bmvar_site)/bm_site\n\n################################################################################\n# Following codes used to calculate:\n# spe_rsb: relative species biomass of individual species of each site\n# spe_bm: mean species biomass individual species of each site\n\nspe_rsb = NULL\nspe_bm = NULL\nfor (i in 1:23) {\n    # i = 1\n    temp1 = subset(com1, Site==unique(Site)[i])\n    \n    spe_rsb1 = temp1[1, ]\n    spe_bm1 = temp1[1, ]\n    \n    total_bm = sum(temp1[, 252], na.rm=T)\n    \n    if (total_bm!=0) {\n        spe_rsb1[, 7:252] = 100 * apply(temp1[, 7:252], 2, function(x)sum(x, na.rm=T))/total_bm\n        spe_bm1[, 7:252] = apply(temp1[, 7:252], 2, function(x)mean(x, na.rm=T))\n    } else {\n        spe_rsb1[1, ] = NA\n        spe_bm1[1, ] = NA\n    }\n    \n    spe_rsb1[1, ] = ifelse(spe_rsb1[1, ]==0, NA, spe_rsb1[1, ])\n    spe_bm1[1, ] = ifelse(spe_bm1[1, ]==0, NA, spe_bm1[1, ])\n    spe_rsb = rbind(spe_rsb, spe_rsb1)\n    spe_bm = rbind(spe_bm, spe_bm1)\n}\n\n################################################################################\n# Following codes used to calculate:\n# w_spe_cv: temporal CVs of indivicual species of each site that weighted by \n#           their contribution to community (site) total community biomass\n# spe_var: temporal variance of individual species of each site\n################################################################################\n# Then, used to calculate the weighted average species temporal CV and its components\n# of different abundance groups\n################################################################################\n\nw_spe_cv = NULL\nspe_var = NULL\nfor (i in 1:23) {\n    # i = 1\n    temp1 = subset(spe_bm_year, Site==unique(Site)[i])\n    \n    w_spe_cv1 = temp1[1, ]\n    spe_var1 = temp1[1, ]\n    mean_cbm = mean(temp1[, 252], na.rm=T)\n    \n    if (mean_cbm!=0 & !is.na(mean_cbm)) {\n        w_spe_cv1[, 7:251] = apply(te']",4,"inner mongolian grassland, biotic stability mechanisms, species diversity, communities, environmental variation, multi-site study, species richness, species composition, temporal fluctuations, precipitation, additive-partitioning method, species synchrony, population dynamics, species-abundance"
Data from: Polyploid plants obtain greater fitness benefits from a nutrient acquisition mutualism,"Polyploidy is a key driver of ecological and evolutionary processes in plants, yet little is known about its effects on biotic interactions. This gap in knowledge is especially profound for nutrient acquisition mutualisms, despite the fact that they regulate global nutrient cycles and structure ecosystems. Generalism in mutualistic interactions depends on the range of potential partners (niche breadth), the benefits obtained, and ability to maintain benefits across a variety of partners (fitness plasticity). Here, we determine how each of these is influenced by polyploidy in the legume-rhizobium mutualism. We inoculated a broad geographic sample of natural diploid and autotetraploid alfalfa (Medicago sativa) lineages with a diverse panel of Sinorhizobium bacterial symbionts. To analyze the extent and mechanism of generalism, we measured host growth benefits and functional traits. Autotetraploid plants obtained greater fitness enhancement from mutualistic interactions and were better able to maintain this across diverse rhizobial partners (i.e., low plasticity in fitness) relative to diploids. These benefits were not attributed to increases in niche breadth, but instead reflect increased rewards from investment in the mutualism. Polyploid plants displayed greater generalization in bacterial mutualisms relative to diploids illustrating another axis of advantage for polyploids over diploids.","['#============\r\n#INPUT DATA\r\n#============\r\np <- read.table(""HB_Data_12-13-19.txt"",sep=""\\t"",header=T)\r\nstr(p)\r\n\r\n#Set desired variables to factors\r\np$Round <- as.factor(p$Round)\r\np$BoxNo <- as.factor(p$BoxNo)\r\np$Acc <- as.factor(p$Acc)\r\np$Pouch <- as.factor(p$Pouch)\r\nstr(p)\r\n\r\n#================================================================\r\n#SPATIAL AND TEMPORAL EFFECTS ON SHOOT BIOMASS AND NODULE TRAITS\r\n#================================================================\r\n#Caclulate trait mean of replicate pouches within accession, including controls\r\np2=ddply(p,c(""Strain"", ""HostSpecies"",""Acc"",""Ploidy"", ""BoxNo"", ""Round""),\r\n         summarise, Shoot.mean=mean(Shoot, na.rm = TRUE),\r\n         Total.mean=mean(Total, na.rm=TRUE),\r\n         QNC.mean=mean(QNodColor, na.rm=TRUE),\r\n         QNC2.mean=mean(QNodColor2, na.rm = TRUE),\r\n         NodNo.mean=mean(NodNo, na.rm=TRUE),\r\n         NodBM.mean=mean(NodBMmg, na.rm=TRUE))\r\nhead(p2)\r\n\r\nst1 <- lm(p2$Shoot.mean ~ p2$Ploidy * p2$Strain + p2$BoxNo)\r\nsummary(st1)\r\nanova(st1)\r\n\r\nst2 <- lm(p2$Shoot.mean ~ p2$Ploidy * p2$Strain + p2$Round)\r\nsummary(st2)\r\nanova(st2)\r\n\r\nst3 <- lm(p2$QNC.mean ~ p2$Ploidy * p2$Strain + p2$BoxNo)\r\nsummary(st3)\r\nanova(st3)\r\n\r\nst4 <- lm(p2$QNC.mean ~ p2$Ploidy * p2$Strain + p2$Round)\r\nsummary(st4)\r\nanova(st4)\r\n\r\nst5 <- lm(p2$NodNo.mean ~ p2$Ploidy * p2$Strain + p2$BoxNo)\r\nsummary(st5)\r\nanova(st5)\r\n\r\nst6 <- lm(p2$NodNo.mean ~ p2$Ploidy * p2$Strain + p2$Round)\r\nsummary(st6)\r\nanova(st6)\r\n\r\nst7 <- lm(p2$NodBM.mean ~ p2$Ploidy * p2$Strain + p2$BoxNO)\r\nsummary(st7)\r\nanova(st7)\r\n\r\nst8 <- lm(p2$NodBM.mean ~ p2$Ploidy * p2$Strain + p2$Round)\r\nsummary(st8)\r\nanova(st8)\r\n\r\n#=======================\r\n#SHOOT BIOMASS\r\n#=======================\r\n#Calculate mean shoot biomass of replicate pouches within accession, including controls\r\np3=ddply(p,c(""Strain"", ""HostSpecies"",""Acc"",""Ploidy""),\r\n         summarise, Shoot.mean=mean(Shoot, na.rm = TRUE),\r\n         Root.mean=mean(Root, na.rm=TRUE),\r\n         Total.mean=mean(Total, na.rm=TRUE),\r\n         QNC.mean=mean(QNodColor, na.rm=TRUE),\r\n         QNC2.mean=mean(QNodColor2, na.rm=TRUE),\r\n         NodNo.mean=mean(NodNo, na.rm=TRUE),\r\n         NodBM.mean=mean(NodBMmg, na.rm=TRUE))\r\nhead(p3)\r\n\r\n#Shoot Biomass Model\r\nshoot <- lmer(Shoot.mean ~ Ploidy * Strain + (1|HostSpecies/Acc), data = p3)\r\nplot(shoot)\r\nE_shoot = resid(shoot, type = ""deviance"")\r\nhist(E_shoot)\r\nqqnorm(E_shoot)\r\nqqline(E_shoot)\r\nsummary(shoot)\r\nanova(shoot)\r\nshootm <- emmeans(shoot, specs = pairwise ~ Ploidy:Strain)\r\nshootm\r\n\r\n\r\n#Shoot summary and plot\r\nshoots <- ddply(p3, c(""Ploidy"", ""Strain""), summarise, N = length(Shoot.mean), mean = mean(Shoot.mean),colors=mean(QNC.mean), sd = sd(Shoot.mean), se = sd/sqrt(N))\r\nshoots\r\n\r\nshootmean <- tapply(p3$Shoot.mean, p3$Ploidy, mean)\r\nshootmean\r\nshootmax <- tapply(p3$Shoot.mean, p3$Ploidy, max)\r\nshootmax\r\nshootmin <- tapply(p3$Shoot.mean, p3$Ploidy, min)\r\nshootmin\r\n\r\n#Shoot Biomass Figure (all 21 strains + controls)\r\nshootsort <- p3[order(p3$Shoot.mean),]\r\nshootsort$Strain=factor(shootsort$Strain,levels=c(""Uninoc"", ""M30"", ""USDA205"", ""USDA4894"", ""USDA207"", ""KH53b"", ""M270"", ""USDA1021"", ""KH36b"", ""WSM419"", ""KH53a"", ""KH36c"", ""1021"", ""KH46c"", ""KH30a"", ""KH16b"", ""USDA1002"", ""KH36d"", ""M210"", ""KH48e"", ""KH35c"", ""USDA4893""))\r\n\r\nshootp <- ggplot(shootsort, aes(x = Strain, y = Shoot.mean, group = Ploidy, colour = Ploidy)) + \r\n  geom_point(size=0.6, alpha=0.6) +\r\n  stat_summary(fun.y=mean, geom=""line"", size=1) +\r\n  stat_summary(fun.y=mean, geom=""point"", size=2) +\r\n  xlab(""Sinorhizobium Strain"") + \r\n  ylab(""Shoot Biomass (g)"") + theme_classic() + scale_color_manual(values=c(""gray50"", ""black"")) + theme(axis.text.x=element_text(angle=90, hjust=1))\r\nshootp\r\n\r\n#Nodule Color Matrix - Shoot Biomass\r\nshootsort2 <- shoots[order(shoots$colors),]\r\nshootsort2\r\n\r\nnod.col=matrix(ncol=nrow(shootsort2)/2,nrow=2, dimnames=list(c(""2X"", ""4X"")))\r\nnod.col[1,]=shootsort2$colors[shootsort2$Ploidy==""2X""]\r\nnod.col[2,]=shootsort2$colors[shootsort2$Ploidy==""4X""]\r\n\r\ncol2 <- colorRampPalette(brewer.pal(9,""YlOrRd""))(50)\r\nlevelplot(nod.col, col.regions= col2)\r\nheatmap(nod.col, Colv=NA, Rowv=NA, col=col2)\r\n\r\n#=========================================\r\n#Test of Lineage Effects - Shoot Biomass\r\n#=========================================\r\nshoot2 <- lm(Shoot.mean ~ Ploidy * Strain, data = p3)\r\nplot(shoot2)\r\nE_shoot2 = resid(shoot2, type = ""deviance"")\r\nhist(E_shoot2)\r\nqqnorm(E_shoot2)\r\nqqline(E_shoot2)\r\nsummary(shoot2)\r\nanova(shoot2)\r\n\r\n#Compare models\r\nanova(shoot,shoot2)\r\n\r\n#Variation explained by accession and host species\r\nVar_Random_effect <- VarCorr(shoot)\r\nVar_Random_effect\r\nvariances <- as.data.frame(Var_Random_effect)\r\nvariances\r\n\r\n#accession\r\nvariances$vcov[1]/sum(variances$vcov)\r\n\r\n#host species\r\nvariances$vcov[2]/sum(variances$vcov)\r\n\r\n#====================\r\n#TOTAL BIOMASS\r\n#====================\r\ntotal <- lmer(Total.mean ~ Ploidy * Strain + (1|HostSpecies/Acc), data = p3)\r\nplot(total)\r\nE_total = resid(total, type = ""deviance"")\r\nhist(E_total)\r\nqqnorm(E_total']",4,"Polyploidy, ecological processes, evolutionary processes, plants, biotic interactions, nutrient acquisition mutualisms, global nutrient cycles, ecosystems, mutualistic interactions, niche breadth, fitness plasticity, legume-rhizobium mutualism, alf"
Was the Devonian placoderm Titanichthys a suspension-feeder?,"Large nektonic suspension-feeders have evolved multiple times. The apparent trend among apex predators for some evolving into feeding on small zooplankton is of interest for understanding the associated shifts in anatomy and behaviour while the spatial and temporal distribution gives clues to an inherent relationship with ocean primary productivity and how past and future perturbations to these may impact on the different tiers of the food web. The evolution of large nektonic suspension-feeders - 'gentle giants' - occurred 4 times among chondrichthyan fishes (e.g. whale sharks, basking sharks and manta rays), as well as in baleen whales (mysticetes), the Mesozoic pachycormid fishes and at least twice in radiodontan stem group arthropods (Anomalocaridids) during the Cambrian Explosion. The Late Devonian placoderm Titanichthys has tentatively been considered to have been a megaplanktivore, primarily due to its gigantic size and narrow, edentulous jaws while no suspension-feeding apparatus have ever been reported. Here the potential for microphagy and other feeding behaviours in Titanichthys is assessed via a comparative study of jaw mechanics in Titanichthys and other placoderms with presumably differing feeding habits (macrophagy and durophagy). Finite element models of the lower jaws of Titanichthys termieri in comparison to Dunkleosteus terrelli and Tafilalichthys lavocati reveal considerably less resistance to von Mises stress in this taxon. Comparisons with a selection of large-bodied extant taxa of similar ecological diversity reveals similar disparities in jaw stress resistance. Our results therefore conform to the hypothesis that Titanichthys was a suspension-feeder with jaws ill-suited for biting and crushing but well suited for gaping ram feeding.","['# PCA intervals\' merhod\r\n# powered by jordi.marce.nogue@uni-hamburg.de\r\n\r\ndevtools::install_github(""kassambara/factoextra"") \r\n\r\n\r\nlibrary(FactoMineR)\r\nlibrary(factoextra)\r\n\r\nsetwd(""F:/Uni stuff/MSc/Bristol return - updated version/Jordi folder/intervals-method-sam/R-part"")\r\n\r\nstress.distrib = read.csv(""intervals-models-sam-nowhales.csv"", row.names=1, header = TRUE, sep = "","")\r\n\r\n\r\n# Multivariate analysis PCA \r\n#--------------------------------------------------------------------------------------------------------\r\n\r\n# ho fem de sb.stress\r\n\r\ncol.number = ncol(stress.distrib)\r\nPCA.stress <- PCA(stress.distrib[,1:col.number],  graph = FALSE)\r\n\r\n# colors by group\r\ngroup.colors = row.names(stress.distrib)\r\n#group.palette = c(""chartreuse"",""cadetblue4"",""darkseagreen4"",""rosybrown3"")\r\n\r\n#colors by variable\r\ninterval.number = nrow(PCA.stress$var$coord)\r\ninterval.vector = seq(from = 1, to = interval.number, by=1 )\r\ninterval.colors = interval.vector\r\ninterval.palette = c(""blue"",""cyan"",""green"",""chartreuse"",""yellow"",""gold"",""orange"",""red"")\r\n\r\n# Biplot: variables coloured by contribution to PCs\r\n\r\nlibrary(ggpubr)\r\n\r\nfviz_pca_biplot(PCA.stress, \r\n                mean.point=F,                     # no vui la mitja de cada esp??cie\r\n                axes.linetype = ""solid"",\r\n    \r\n                # Fill individuals by groups\r\n                geom.ind=c(""point"", ""text""),\r\n                pointshape = 21, \r\n                pointsize = 5,            \r\n                col.ind= ""black"",                 # que no hi hagi el cercle exterior\r\n                fill.ind = group.colors,          # fill amb els grups\r\n                alpha.ind = 1,                    # transparency, no = 1\r\n                \r\n                # Color variable by intervals\r\n                geom.var = ""arrow"",\r\n                col.var = interval.colors,\r\n                arrowsize = 0.5,\r\n                \r\n                repel = TRUE) +                   # Aboid label overplotting\r\n  \r\n  \r\n  #fill_palette(group.palette) +\r\n  gradient_color(interval.palette)+\r\n  labs(x = ""PC1"", y = ""PC2"")+\r\n  \r\n  \r\n  theme(\r\n    panel.grid.major = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n  )\r\n\r\nsummary(PCA.stress)\r\n\r\n\r\n\r\n\r\n\r\n']",4,"Devonian, placoderm, Titanichthys, suspension-feeder, nektonic, zooplankton, anatomy, behaviour, apex predators, ocean primary productivity, food web, chondrichthyan fishes, whale sharks,"
Extracting Duplicate Georeferences from Herbarium Specimen Data,"New version! The March 2021 (2.0) version includes a more informative georeferenceRemarks field in the output table, as well as other improvements. The code documentation has been updated accordingly.This code was developed to rapidly georeference herbarium specimens by importing georeference data that already exist for duplicate specimens. The code looks through each record in a provided dataset and determines whether that record is found in the omoccurduplicatelink table (i.e., a duplicate has been linked to that record in your Symbiota portal). If a georeferenced duplicate is found, the code will add these data into a new output file (newMycoll) such that the unique identifier (occid), catalog number, other catalog number, collector, and collector number corresponds to those fields from the input dataset, and the georeference data is copied from the duplicate record. The user must then clean the output file so it results in one row per specimen record.","['#####\r\n#title: ExtractDuplicateGeoreferences, version 2.0\r\n#author: Katelin (Katie) Pearson\r\n#contact: katelin.d.pearson24@gmail.com\r\n#date: March 25, 2021\r\n#details: This script was developed for the California Phenology Network (capturingcaliforniasflowers.org) and made possible by National Science Foundation Awards 1802312 and 2001500. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.\r\n####\r\n\r\n#load packages\r\nlibrary(dplyr)\r\nlibrary(stringr)\r\nlibrary(gtools)\r\nlibrary(DataCombine)\r\n\r\n#load the table of specimens (occids) that have duplicates\r\n#in most Symbiota instances, this is the omoccurduplicatelink table\r\nduplinks <- read.csv(""PATH"")\r\n\r\n#load the table of georeference data from the database\r\n#we used a modified version of omoccurrences that only includes the columns occid,\r\n#decimalLatitude, decimalLongitude, geodeticDatum, coordinateUncertaintyInMeters, footprintWKT,\r\n#coordinatePrecision, georeferencedBy, georeferenceSources, and georeferenceRemarks\r\n#for specimens for which decimalLatitude IS NOT NULL\r\nothers <- read.csv(""PATH"")\r\n\r\n#load the table of data from the collection for which you are importing data\r\n#make sure that the first column contains the occurrence ID and is called ""id""\r\nmycoll <- read.csv(""PATH"")\r\n\r\n#make a new data frame that will hold all the new georeferences\r\n#note that the maximum is 50,000 records here; increase size as needed\r\nnewMycoll <- matrix(ncol=length(colnames(others)),nrow=50000)\r\ncolnames(newMycoll) <- colnames(others)\r\nnewMycoll <- as.data.frame(newMycoll)\r\nf <- 1\r\n\r\n#look at each record in the mycoll dataframe\r\nfor(i in 1:dim(mycoll)[1]){\r\n  print(i)\r\n  #if desired, uncomment the line below to track progress (it will slow down the code, though)\r\n  #print(paste((i/dim(mycoll)[1])*100,""% complete""))\r\n  \r\n  #uncomment lines 45-47 and line 103 if your input data (from your collection) contains georeferenced specimens\r\n  #if the record in mycoll already has georeference data, go to the next record\r\n  #if(!is.na(mycoll$decimalLatitude[i])){\r\n  #  next\r\n  #} else {\r\n  \r\n  #does the mycoll record have a duplicate?\r\n  m <- match(mycoll$id[i],duplinks$occid)\r\n  \r\n  #if no duplicate found, go to the next record\r\n  if(is.na(m)){\r\n    next\r\n  }  \r\n  else {\r\n    this <- mycoll$id[i]\r\n    #find the id number of the duplicate cluster to which this record belongs\r\n    dupid <- duplinks$duplicateid[duplinks$occid==this]\r\n    #make a temporary dataframe of all the records that belong to this cluster\r\n    dupes <- subset(duplinks,duplicateid==dupid)\r\n    #from the temporary dataframe, remove the record that we are searching against (the original record)\r\n    dupes <- subset(dupes, occid!=mycoll$id[i])\r\n    #if there are no other duplicate records (because of a mistake), go to the next record in mycoll\r\n    if(dim(dupes)[1]<1){\r\n      next\r\n    } else {\r\n      #for every duplicate in the temporary dataframe\r\n      for(j in 1:dim(dupes)[1]){\r\n        #find the occid for that duplicate\r\n        thisdup <- dupes$occid[j]\r\n        #if the duplicate does not exist within the full dataset (i.e., no georeferenced duplicate exists), go to next\r\n        if(thisdup %in% others$occid){\r\n          \r\n          #uncomment lines 77-79 and line 98 if your ""others"" dataset contains specimens that are not georeferenced\r\n          #if the duplicate does not have georeference data, go to the next duplicate in the temporary dataframe\r\n          #if(is.na(others$decimalLatitude[others$occid==thisdup])){\r\n          #  next\r\n          #} else {\r\n          \r\n          #if the duplicate DOES have georeference data, add all the data to the ""newMycoll"" dataframe\r\n          newMycoll$occid[f]=mycoll$id[i]\r\n          newMycoll$decimalLatitude[f] <- as.character(others$decimalLatitude[others$occid==thisdup])\r\n          newMycoll$decimalLongitude[f] <- as.character(others$decimalLongitude[others$occid==thisdup])\r\n          newMycoll$geodeticDatum[f] <- as.character(others$geodeticDatum[others$occid==thisdup])\r\n          newMycoll$coordinateUncertaintyInMeters[f] <- as.character(others$coordinateUncertaintyInMeters[others$occid==thisdup])\r\n          newMycoll$footprintWKT[f] <- as.character(others$footprintWKT[others$occid==thisdup])\r\n          newMycoll$coordinatePrecision[f]=as.character(others$coordinatePrecision[others$occid==thisdup])\r\n          newMycoll$georeferencedBy[f] <- as.character(others$georeferencedBy[others$occid==thisdup])\r\n          newMycoll$georeferenceSources[f] <- as.character(others$georeferenceSources[others$occid==thisdup])\r\n          newMycoll$georeferenceRemarks[f] <- paste(""copied from duplicate collid="",as.character(others$collid[others$occid==thisdup]),"" "",as.character(others$catalogNumber[others$occid==thisdup]),""; "", as.character(others$georeferenceRemarks[others$occid==thisdup]),sep="""")\r\n          newMycoll$catalogNumber[f] <- as.character(mycol']",4,"- Georeferencing 
- Herbarium specimen 
- Duplicate specimens 
- Dataset 
- Symbiota portal 
- Unique identifier 
- Catalog number 
- Collector 
- Collector number 
- Output file 
- March 2021"
"Research outputs associated with ""Policy to Practice: tracking open science funding""","Datasets, code, figures, and other documents assocaited with ""From policy to practice: tracking an open science funding initiative"" (Ratan et al. 2023)Contents:PDF files of figures 1 and 2- Figure_1.pdf- Figure_2.pdfPDF file explaining rules and definitions used when creating an open science compliance report- Compliance_Report_Object_Rules.pdfTabular data sets constructed to produced figure 2- NewSharedData.csv- NewSharedCode.csv- NewSharedLabMat.csv- NewSharedProtocols.csv- ReuseCitedData.csv- ReuseCitedSoftware.csv- ReuseCitedLabMat.csv- ReuseCitedProtocols.csvR Code used to produce figure 2 from the datasets listed above- researchOutputs_pairedData.R","['# Load required packages\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n# Set your working directory, insert file path for the directory where you saved the data accompanying the manuscript between the quotation marks below\r\nsetwd(""C:/Users/Matt/Downloads/updatedProportionData"")\r\n\r\n#######################################################################################################################################################\r\n# Load the .csv file associated with each data set as a data frame ####################################################################################\r\n#######################################################################################################################################################\r\n\r\n# 1) New Data Shared\r\nnewSharedData_df<- read.csv(file = ""NewSharedData.csv"", header = TRUE)\r\n  \r\n# 2) New Code Shared\r\nnewSharedCode_df<- read.csv(file = ""NewSharedCode.csv"", header = TRUE)\r\n\r\n# 3) New Lab Materials Shared\r\nnewSharedLabMat_df<- read.csv(file = ""NewSharedLabMat.csv"", header = TRUE)\r\n\r\n# 4) New Protocols Shared\r\nnewSharedProtocol_df<- read.csv(file = ""NewSharedProtocol.csv"", header = TRUE)\r\n\r\n# 5) Reuse Data Cited\r\nreuseCitedData_df<- read.csv(file = ""ReuseCitedData.csv"", header = TRUE)\r\n\r\n# 6) Reuse Software Cited\r\nreuseCitedSoftware_df<- read.csv(file = ""ReuseCitedSoftware.csv"", header = TRUE)\r\n\r\n# 7) Reuse Lab Materials Cited\r\nreuseCitedLabMat_df<- read.csv(file = ""ReuseCitedLabMat.csv"", header = TRUE)\r\n\r\n# 8) Reuse Protocols Cited\r\nreuseCitedProtocol_df<- read.csv(file = ""ReuseCitedProtocol.csv"", header = TRUE)\r\n\r\n\r\n#######################################################################################################################################################\r\n######################## MAKING PLOTS #################################################################################################################\r\n#######################################################################################################################################################\r\n\r\n#######################################################################################################################################################\r\n###################### Graphs for New Research Outputs ################################################################################################\r\n#######################################################################################################################################################\r\n\r\n## Produces a figure for % new shared data and saves it within the working directory\r\n#pdf(file = ""testing.pdf"", height = 4.0, width = 4.5)\r\npdf(file = ""updatedNewDataShared_plot.pdf"", height = 4.0, width = 4.5)\r\nnewSharedData_df %>%\r\n  ggplot(aes(Stage,PercentShared*100, fill = Stage)) +\r\n  #geom_violin(aes(fill = Stage), color = ""white"", alpha = 0.2) +\r\n  geom_line(aes(group = FirstAuthor), color = ""gray"", linewidth = 0.5, position = position_dodge(0.20)) +\r\n  geom_point(aes(color = Stage, shape = Stage, group = FirstAuthor), size = 2.5, position = position_dodge(0.20)) +\r\n  scale_color_manual(values = c(""#08597b"", ""#34a270"")) +\r\n  theme_classic() + \r\n  theme(legend.position = ""none"") +\r\n  ylab(""% New Datasets Shared"") +\r\n  xlab(""Manuscript Stage"") +\r\n  scale_x_discrete(labels = c(""FS"" = ""First Version"", ""NS"" = ""Second Version"")) +\r\n  theme(axis.title.x = element_text(color=""black"", size=18, face=""bold""),\r\n      axis.title.y = element_text(color=""black"", size=18, face=""bold""),\r\n      axis.text.x = element_text(size = 16),\r\n      axis.text.y = element_text(size = 16))\r\ndev.off()\r\n\r\n## Produces a figure for % new shared code and saves it within the working directory\r\npdf(file = ""updatedNewCodeShared_plot.pdf"", height = 4.0, width = 4.5)\r\nnewSharedCode_df %>%\r\n  ggplot(aes(Stage,PercentShared*100, fill = Stage)) +\r\n  #geom_violin(aes(fill = Stage), color = ""white"", alpha = 0.2) +\r\n  geom_line(aes(group = FirstAuthor), color = ""gray"", linewidth = 0.5, position = position_dodge(0.20)) +\r\n  geom_point(aes(color = Stage, shape = Stage, group = FirstAuthor), size = 2.5, position = position_dodge(0.20)) +\r\n  scale_color_manual(values = c(""#08597b"", ""#34a270"")) +\r\n  theme_classic() + \r\n  theme(legend.position = ""none"") +\r\n  ylab(""% New Software Shared"") +\r\n  xlab(""Manuscript Stage"") + \r\n  scale_x_discrete(labels = c(""FS"" = ""First Version"", ""NS"" = ""Second Version"")) +\r\n  theme(axis.title.x = element_text(color=""black"", size=18, face=""bold""),\r\n        axis.title.y = element_text(color=""black"", size=18, face=""bold""),\r\n        axis.text.x = element_text(size = 16),\r\n        axis.text.y = element_text(size = 16))\r\ndev.off()\r\n\r\n## Produces a figure for % new shared lab materials and saves it within the working directory\r\npdf(file = ""updatedNewLabMatShared_plot.pdf"", height = 4.0, width = 4.5)\r\nnewSharedLabMat_df %>%\r\n  ggplot(aes(Stage,PercentShared*100, fill = Stage)) +\r\n  #geom_violin(aes(fill = Stage), color = ""white"", alpha = 0.2) +\r\n  geom_line(aes(group = Firs']",4,"Open science, funding, policy, practice, research outputs, datasets, code, figures, documents, compliance report, tabular data sets, R code."
Altmetrics and Citation Counts: An Empirical Analysis of the Computer Science Domain,We provide the raw dataset and our analyses scripts for the accepted paper: Altmetrics and Citation Counts: An Empirical Analysis of the Computer Science Domain,"['exx = read.csv(""/2021/data/data_with_plumx.csv"")\r\nattach(exx)\r\nclass(exx)\r\n#cor.test(citation_count,Social_count,method = ""spearm"",exact=FALSE)\r\n#cor.test(citation_count,Mention_count,method = ""kendall"")\r\nmodel <- lm(citation_count ~ Tweet_count, data = exx)\r\nmodel\r\nsummary(model)\r\n\r\n\r\n##-------------scatter plot\r\n\r\nexx = read.csv(""/2021/data/data_with_plumx.csv"")\r\nlibrary(reshape2)\r\nlibrary(ggplot2)\r\nyt<-ggplot(data = extractedPapers, aes(x = citation_count, y =Tweet_count )) +\r\n  geom_point(size=1)+ \r\n  labs(y=""Tweets"", x = ""Citations"")+\r\n  scale_x_continuous(limits = c(1, 500))+\r\n  scale_y_continuous(limits = c(1, 500))+\r\n  theme_bw()+\r\n  theme(axis.title = element_text(size=12),\r\n        axis.text.x= element_text(size=10),\r\n        axis.text.y= element_text(size=10),\r\n        panel.grid.minor.x = element_blank(),\r\n        legend.position = c(0.25, 0.80),\r\n        legend.title = element_blank(),\r\n        legend.key.size = unit(0.5, \'cm\'),  \r\n        legend.key.height = unit(0.4, \'cm\'),  \r\n        legend.key.width = unit(0.5, \'cm\'),\r\n        legend.text=element_text(size=7),\r\n        panel.border = element_rect(size=0.2, linetype=""solid""),\r\n        panel.grid.minor.y = element_blank(),\r\n        panel.grid.major = element_line(size=0.1)\r\n  ) \r\nplot(yt)+\r\n  geom_smooth(method = lm, formula = y ~ splines::bs(x, 3), se = FALSE) +facet_wrap(~ Year)\r\n\r\n#----------------------\r\n#-- Twitter paper barplot\r\nexx = read.csv(""/Users/randalchkr/Dropbox/PHD/work/Yusra/papergraphs/2021/data/groupeddataT.csv"")\r\nlibrary(ggplot2)\r\nplot1<-ggplot(exx, aes(x=cat2, y=value,fill=cate)) + \r\n  geom_bar(stat=""identity"", position=position_dodge(),width = 0.8)+\r\n  labs(y=""Count"", x = ""Years"")+ \r\n  scale_x_continuous(breaks = c(1:7),labels=c(""2015"", ""2016"", ""2017"",""2018"",""2019"",""2020"",""2021""))+\r\n  scale_y_continuous(limits = c(0, 210000),,breaks=seq(0,200000,25000))+\r\n  theme_minimal()+\r\n  theme( \r\n    panel.grid.major.x = element_blank(),\r\n    axis.line = element_blank(),\r\n    axis.ticks.x =  element_line(size=0.1), \r\n    panel.grid.minor.x = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    panel.background = element_rect(fill = ""white"",color=""black"",size=0.2),\r\n    axis.ticks.y =  element_line(size=0.1), \r\n    axis.title = element_text(size=10),\r\n    axis.text.y= element_text(size=8),\r\n    axis.text.x= element_text(size=8), \r\n    legend.position =""top"",\r\n    legend.key.size = unit(0.5, \'cm\'), \r\n    legend.key.height = unit(0.4, \'cm\'), \r\n    legend.key.width = unit(0.5, \'cm\'),\r\n    legend.text=element_text(size=8),\r\n    legend.title = element_blank()\r\n  ) +scale_fill_manual(values=c( ""#41b6c4"",""#a1dab4"",""#0868ac"",""grey"", ""#253494"",""#e0e000""),\r\n                       breaks=c(""a"", ""b"", ""c"",""d"",""e"",""f""),\r\n                       labels=c(""Captures"", ""Usage counts"", ""Citations"",""Social media counts"",""Tweets"",""Mentions"")\r\n  )\r\nplot1\r\n\r\nlibrary(reshape2)\r\nlibrary(ggplot2)\r\nyt<-ggplot(data = groupeddataT, aes(x = citation_count, y =Tweet_count )) +\r\n  geom_point(size=1)+ \r\n  labs(y=""Tweets"", x = ""Citations"")+\r\n  scale_x_continuous(limits = c(1, 500))+\r\n  scale_y_continuous(limits = c(1, 500))+\r\n  theme_bw()+\r\n  theme(axis.title = element_text(size=12),\r\n        axis.text.x= element_text(size=10),\r\n        axis.text.y= element_text(size=10),\r\n        panel.grid.minor.x = element_blank(),\r\n        legend.position = c(0.25, 0.80),\r\n        legend.title = element_blank(),\r\n        legend.key.size = unit(0.5, \'cm\'),  \r\n        legend.key.height = unit(0.4, \'cm\'),  \r\n        legend.key.width = unit(0.5, \'cm\'),\r\n        legend.text=element_text(size=7),\r\n        panel.border = element_rect(size=0.2, linetype=""solid""),\r\n        panel.grid.minor.y = element_blank(),\r\n        panel.grid.major = element_line(size=0.1)\r\n  ) \r\nplot(yt)+\r\n  geom_smooth(method = lm, formula = y ~ splines::bs(x, 3), se = FALSE) +facet_wrap(~ Year)\r\n']",4,"Altmetrics, Citation Counts, Empirical Analysis, Computer Science, Raw Dataset, Analysis Scripts, Accepted Paper."
Data from: A new data-driven mathematical model dissociates attractiveness from sexual dimorphism of human faces,"Human facial attractiveness is evaluated by using multiple cues. Among others, sexual dimorphism (i.e. masculinity for male faces/femininity for female faces) is an influential factor of perceived attractiveness. Since facial attractiveness is judged by incorporating sexually dimorphic traits as well as other cues, it is theoretically possible to dissociate sexual dimorphism from facial attractiveness. This study tested this by using a data-driven mathematical modelling approach. We first analysed the correlation between perceived masculinity/femininity and attractiveness ratings for 400 computer-generated male and female faces (Experiment 1) and found positive correlations between perceived femininity and attractiveness for both male and female faces. Using these results, we manipulated a set of faces along the attractiveness dimension while controlling for sexual dimorphism by orthogonalisation with data-driven mathematical models (Experiment 2). Our results revealed that perceived attractiveness and sexual dimorphism are dissociable, suggesting that there are as yet unidentified facial cues other than sexual dimorphism that contribute to facial attractiveness. Future studies can investigate the true preference of sexual dimorphism or the genuine effects of attractiveness by using well-controlled facial stimuli like those that this study generated. The findings will be of benefit to the further understanding of what makes a face attractive.","['# Autor: Koyo Nakamura\n# Date: 3/9/2020\n\nlibrary(""rstan"")\nlibrary(""brms"")\n\nrstan_options(auto_write = TRUE)\noptions(mc.cores = parallel::detectCores())\n\nload(""Nakamura_Watanabe_2020.Rdata"")\n#################\n# Experiment 1\n#################\n\n# Validation of sexual dimorphism dimension (Best-fitted model)\n\nmodel1 <- brm(Rating ~ Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces +\n                (Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces|Subject_ID) +\n                (Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces|Face_ID),\n              data=dat1,family=""gaussian"", chains=4, iter=13000, warmup=3000, thin=1, seed=1, control = list(adapt_delta = 0.99, max_treedepth = 15))\n\nprint(summary(model1))\n\n\n# Correlation between perceived masculinity/femininity and attractiveness\n\ncor_data <- list(N=200,male_data=cor_data_male[,c(2,3)],female_data=cor_data_female[,c(2,3)])\npar = c(""rho1"",""rho2"",""delta_r"")\n\ncor_anal <- \'\ndata{\n\tint<lower=0>  N; \n\tvector[2] male_data[N];\n\tvector[2] female_data[N];\n}\nparameters{\n\tvector[2] mu1;\n\tvector<lower=0>[2] sigma1;\n\treal<lower=-1,upper=1> rho1;\n\tvector[2] mu2;\n\tvector<lower=0>[2] sigma2;\n\treal<lower=-1,upper=1> rho2;\n}\ntransformed parameters{\n\tvector<lower=0>[2] sig21;\n\tmatrix[2,2] Sigma1;\n\tvector<lower=0>[2] sig22;\n\tmatrix[2,2] Sigma2;\n\n\tsig21[1] = pow(sigma1[1],2);\n\tsig21[2] = pow(sigma1[2],2);\n\tSigma1[1,1] = sig21[1];\n\tSigma1[2,2] = sig21[2];\n\tSigma1[1,2] = sigma1[1]*sigma1[2]*rho1;\n\tSigma1[2,1] = sigma1[1]*sigma1[2]*rho1;\n\n\tsig22[1] = pow(sigma2[1],2);\n\tsig22[2] = pow(sigma2[2],2);\n\tSigma2[1,1] = sig22[1];\n\tSigma2[2,2] = sig22[2];\n\tSigma2[1,2] = sigma2[1]*sigma2[2]*rho2;\n\tSigma2[2,1] = sigma2[1]*sigma2[2]*rho2;\n}\nmodel{\n\tfor(i in 1:N){\n\t\tmale_data[i] ~ multi_normal(mu1,Sigma1);\n\t\tfemale_data[i] ~ multi_normal(mu2,Sigma2);\n\t}\n}\ngenerated quantities{\n\treal delta_r;\n\tdelta_r = rho2 - rho1;\n}\n\';\n\nfit = stan(model_code = cor_anal, data = cor_data,chains=4, iter=13000, warmup=3000,seed=1,cores=4, control = list(adapt_delta = 0.99, max_treedepth = 15))\nprint(fit, pars=par,digits_summary=2)\n\n\n#################\n# Experiment 2\n#################\n\n# Attractiveness orthogonal to sexual dimorphism\n# Attractiveness rating scores (Best-fitted model)\n\nmodel2 <- brm(Rating ~ Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces +\n                (Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces|Subject_ID) +\n                (Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces|Face_ID),\n              data=dat2,family=""gaussian"", chains=4, iter=13000, warmup=3000, thin=1, seed=1, control = list(adapt_delta = 0.99, max_treedepth = 15))\n\nprint(summary(model2))\n\n\n# Attractiveness orthogonal to sexual dimorphism\n# Sexual dimorphism rating scores (Best-fitted model)\n\nmodel3 <- brm(Rating ~ Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces +\n                (Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces|Subject_ID) +\n                (Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces|Face_ID),\n              data=dat3,family=""gaussian"", chains=4, iter=13000, warmup=3000, thin=1, seed=1, control = list(adapt_delta = 0.99, max_treedepth = 15))\n\nprint(summary(model3))\n\n\n# Sexual dimorphism orthogonal to attractiveness\n# Attractiveness rating scores (Best-fitted model)\n\nmodel4 <- brm(Rating ~ Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces +\n                (Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces|Subject_ID) +\n                (Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces|Face_ID),\n              data=dat4,family=""gaussian"", chains=4, iter=13000, warmup=3000, thin=1, seed=1, control = list(adapt_delta = 0.99, max_treedepth = 15))\n\nprint(summary(model4))\n\n\n# Sexual dimorphism orthogonal to attractiveness\n# Sexual dimorphism rating scores (Best-fitted model)\n\nmodel5 <- brm(Rating ~ Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces +\n                (Face_Exaggeration + Sex_of_Faces + Face_Exaggeration:Sex_of_Faces + I(Face_Exaggeration^2) + I(Face_Exaggeration^2):Sex_of_Faces|Su']",4,"facial attractiveness, sexual dimorphism, masculinity, femininity, perceived attractiveness, data-driven mathematical model, computer-generated faces, perceived femininity, correlation, orthoganalisation, unidentified facial cues, future studies, well-controlled facial stimuli, genuine effects"
Data for: Local adaptation of seed and seedling traits along a natural aridity gradient may both predict and constrain adaptive responses to climate change,"Local adaptation of seed and seedling traits along a natural aridity gradient may both predict and constrain adaptive responses to climate change (American Journal of Botany 2022)Premise of study: Variation in seed and seedling traits underlies how plants interact with their environment during establishment, a crucial life history stage. We quantified genetic-based variation in seed and seedling traits in populations of the annual plant Plantago patagonica across a natural aridity gradient, leveraging natural intraspecific variation to predict how populations might evolve in response to increasing aridity associated with climate change in the Southwestern U.S.Methods: We quantified seed size, seed size variation, germination timing, and specific leaf area in a greenhouse common garden, and related these traits to the climates of source populations. We then conducted a terminal drought experiment to determine which traits were most predictive of survival under early-season drought.Key Results: All traits showed evidence of clinal variation  seed size decreased, germination timing accelerated, and specific leaf area increased with increasing aridity. Populations with more variable historical precipitation regimes showed greater variation in seed size, suggestive of past selection shaping a diversified bet-hedging strategy mediated by seed size. Seedling height, achieved via larger seeds or earlier germination, was a significant predictor of survival under drought.Conclusions: We documented substantial interspecific trait variation as well as clinal variation in several important seed and seedling traits, yet these slopes were often opposite to predictions for how individual traits might confer drought tolerance. This work shows that plant populations may adapt to increasing aridity via correlated trait responses associated with alternative life history strategies, but that trade-offs might constrain adaptive responses in individual traits.Keywrds: climate change, drought, intraspecific trait variation, life history strategies, local adaptation, Plantago, precipitation variability, seed size, trade-offsData files: This upload includes all of the input data (.csv and .Rdata) and scripts (.R) for analysis and figure generation in the manuscript. See the included README.txt file for details.Location: This research was conducted at the greenhouse at Nothern Arizona University, using seeds collected from 12 locations in nothern Arizona and southern Utah.","['library(rgdal)\nlibrary(dismo)\nlibrary(raster)\nlibrary(rgeos)\nlibrary(sp)\nlibrary(scales)\n\n\n### 00. import shapefiles (in .Rdata format) -----------------------------------\nsetwd(""~/Documents/Post_Doc/0_Plantago_Research/0_MANUSCRIPTS/01_seed_size_manuscript/02_PUBLICATION_DOCUMENTS/Dryad/"")\n\n# import Four Corners shapefile\nFourCorners <- get(load(file = ""FourCorners_shape.Rdata""))\n\n# import Colorado Plateau shapefile\nCP <- get(load(file = ""Colorado_Plateau_shape.Rdata""))\nCP <-  aggregate(CP, dissolve = T)\n\n# import georeferenced records of Plantago patagonica from the Colorado Plateau from GBIF and SEINET\nPpat_cp <- get(load(file = ""Ppat_records_GBIF_and_SEINET_Colorado_Plateau.Rdata""))\n\n# import study site locations\nsource_dat  <-  read.csv(file = ""Plantago_collection_locations.csv"", header = T, stringsAsFactors = F)\n\npops_xy  <-  source_dat[, c(""Lon"", ""Lat"")]\npops_xy  <-  SpatialPoints(pops_xy,)\n\ncrs(pops_xy) <-  crs(Ppat_cp)\n\n\n### 01. import climate data ----------------------------------------------------\nPpat_clim <- read.csv(file = ""Plantago_WorldClim_climate_data.csv"", header = T)\n\n\n### 02. conduct PCA of Bioclim/WorldClim variables -----------------------------\nlibrary(factoextra)\n\n# combine focal Plantago populations with all other Plantago populations\nALL_Ppat <- rbind(pops_xy, Ppat_cp)\n      # first 12 records seed collection locations\n      # remaining records are collections of P.patagonica from the Colorado Plateau\n\n# pca\nres.pca <- prcomp(Ppat_clim, scale = TRUE)\n\n# plot variables\nfviz_pca_var(res.pca,\n             col.var = ""contrib"", # Color by contributions to the PC\n             gradient.cols = c(""#00AFBB"", ""#E7B800"", ""#FC4E07""),\n             repel = TRUE)\n\n  # Dim1 46.1%\n  # Dim2 29.6%\n\n# isolate individual PCA plotting coordinates\nres.ind <- get_pca_ind(res.pca)\ncoords <-  data.frame(x = res.ind$coord[,1], y = res.ind$coord[,2])\n\n# subset source location sites\nseed_coords <- coords[1:12,]\n\n# add plotting color\nseed_coords$color  <- rep(""black"", times = 12)\n\n\n### 03. plot Figure 1 (with aridity gradient) ----------------------------------\ndev.new()\npar(mfrow = c(1,2))\n\n# collection panel\nplot(FourCorners)\nplot(CP, add = T, border = ""gray"")\nplot(Ppat_cp, add = T, pch = 16, cex = 0.75, col = alpha(""gray"", alpha = 0.45))\n\n# define aridity colors for each population\nseed_coords$Population <- source_dat$Population\n\n# import Aridity Index for all focal populations\narid_dat <- read.csv(file = ""Plantago_climate_data_simple.csv"", header = T, stringsAsFactors = F)\norder(arid_dat$AI)\n\n# generate a continuous color palette and add aridity gradient\nrbPal <- colorRampPalette(c(""blue"", ""red""))\n\narid_dat2  <-  arid_dat[order(arid_dat$AI),]\narid_dat2$colors  <-  rbPal(12)[as.numeric(cut(arid_dat2$AI, breaks = 12))]\narid_dat3  <-  merge(arid_dat2, seed_coords, by.x = ""Population"", by.y = ""Population"")\n\n# plot seed collection locations\nplot(pops_xy, col = alpha(arid_dat3$colors, alpha = 0.95), pch = 16, cex = 1.5, add = T)\n\n# add aridity gradient\nlegend_colors = rbPal(100)\nlegend_image <- as.raster(matrix(rev(legend_colors), ncol = 1))\n\nrasterImage(legend_image,\n            xleft = -105.5,\n            xright = -104.5,\n            ybottom = 32.5,\n            ytop = 36.5)\n\ntext(x = -104, y = c(32.5, 36.5), labels = c(65, 90), font = 1)\n\n\n# PCA panel\nplot(x = coords$x, y = coords$y, col = NULL, cex = 1, pch = 16,\n     xlab = ""PC1 (46.1%)"", ylab = ""PC2 (29.6%)"", \n     main = """")\n\nabline(v = 0, col = ""black"", lwd = 1, lty = 2)\nabline(h = 0, col = ""black"", lwd = 1, lty = 2)\n\npoints(x = coords$x, y = coords$y, col = alpha(""gray"", alpha = 0.5), pch = 16, cex = 1)\npoints(x = seed_coords$x, y = seed_coords$y, col = alpha(arid_dat3$colors, alpha = 0.95),  pch = 16, cex = 1.75)\n\nrasterImage(legend_image,\n            xleft = 7.5,\n            xright = 9,\n            ybottom = 1.5,\n            ytop = 5.5)\n\ntext(x = 9.75, y = c(1.5, 5.5), labels = c(65, 90), font = 1)\n\nquartz.save(file = ""Figure1_collection_locations_and_climate_PCA.jpg"", type = ""jpg"", dpi = 300)\n\n\n\n\n\n', '### SEED SIZE (FIGURE 2A) ###\n\n### 01. import all seed size results from ImageJ particle analysis -------------\nsetwd(""~/Desktop/Dryad/"")\ndat <- read.csv(file = ""2A_seed_size_data.csv"", header = T, stringsAsFactors = F)\n\nnrow(dat) # 33093 seeds\nlength(unique(dat$maternal_line)) # 291 maternal lines\nlength(unique(dat$population)) # 12 populations\n\n\n### 02. assess seed size size number correlation -------------------------------\ncor_dat <- data.frame(\n    maternal_line = names(tapply(dat$seed_area, dat$maternal_line, mean)),\n    avg_seed_size = tapply(dat$seed_area, dat$maternal_line, mean),\n    n_seeds = as.vector(table(dat$maternal_line)))\n\ncor_dat$population <- sapply(strsplit(cor_dat$maternal_line, split = ""_""), function(x) x[1])\n\ncor.test(cor_dat$avg_seed_size, cor_dat$n_seeds) \n    # r = -0.13, p = 0.017, weak correlation between seed size and seed number\n\nplot(avg_seed_size ~ n_seeds, col = ""gray"", pch = 16, data = cor_dat)\n\n# take into account the effect of population\nlibrary(lmerTest)\n\nm0 <- lmer(avg_seed_size ~ n_seeds + (1|population), data = cor_dat)\nsummary(m0) # p = 0.699\n\n\n### 03. import climate data ----------------------------------------------------\nclim_dat <- read.csv(file = ""Plantago_climate_data_simple.csv"", header = T, stringsAsFactors = F)\n\ndat$population[dat$population == ""Huckaby""] <- ""HuckabyTrailhead""\n\n# remove underscores from clim_dat$Population\nclim_dat$Population <-  gsub(pattern = ""_"", replacement = """", x = clim_dat$Population)\n\nsort(unique(dat$population)) %in% clim_dat$Population\n\n# merge data\nall_dat <- merge(dat, clim_dat, by.x = ""population"", by.y = ""Population"")\n\n\n### 05. model seed size ~ aridity of source climate ----------------------------\nlibrary(ggplot2)\nlibrary(ggeffects)\nlibrary(scales)\n\nplot(seed_area ~ jitter(AI, factor = 5), data = all_dat,\n     pch = 16, cex = 0.5, col = alpha(""gray"", alpha = 0.2))\n\n# Aridity Index\nm1.mixed.AI <- lmer(seed_area ~ AI + (1|population) + (1|maternal_line), \n                data = all_dat)\n\nsummary(m1.mixed.AI)\n\nAI_df <- ggpredict(m1.mixed.AI, terms = c(""AI""))\n\np1 <- ggplot(AI_df, aes(x, predicted)) + \n    geom_jitter(data = all_dat, aes(x = AI, y = seed_area), \n        width = 0.125, pch = 16, col = ""darkgray"", alpha = 0.025) +\n    geom_line(aes(linetype=group, color=group), size = 1) +\n    geom_ribbon(aes(ymin=conf.low, ymax=conf.high, fill=group), alpha = 0.1) +\n    scale_linetype_manual(values = c(""solid"", ""dashed"", ""dotted"")) +\n    \n    theme_light() +\n    xlab("""") +\n    theme(axis.text.x = element_blank()) +\n    ylab(expression(bold(""Seed area (mm""^2*"")""))) +\n    theme(axis.title.y = element_text(size = 10, face = ""bold"")) +\n    \n    theme(legend.title = element_blank()) +\n    theme(legend.position = ""none"") \n\n\n# Moisture Index\nm1.mixed.MI <- lmer(seed_area ~ MI + (1|population) + (1|maternal_line), \n                    data = all_dat)\n\nsummary(m1.mixed.MI)\n\nMI_df <- ggpredict(m1.mixed.MI, terms = c(""MI""))\n\nggplot(MI_df, aes(x, predicted)) + \n    geom_line(aes(linetype=group, color=group), size = 1) +\n    geom_ribbon(aes(ymin=conf.low, ymax=conf.high, fill=group), alpha = 0.05) +\n    scale_linetype_manual(values = c(""solid"", ""dashed"", ""dotted"")) +\n    \n    geom_jitter(data = all_dat, aes(x = MI, y = seed_area), width = 0.0015, pch = 16, col = ""darkgray"", alpha = 0.05) +\n    \n    theme_light() +\n    xlab(""Climatic Moisture Index"") +\n    theme(axis.title.x = element_text(size = 12, face = ""bold"")) +\n    ylab(""Seed Area (mm^2)"") +\n    theme(axis.title.y = element_text(size = 12, face = ""bold"")) +\n    \n    theme(legend.title = element_blank()) +\n    theme(legend.position = ""none"") \n\n\n\n### GERMINATION TIMING VARIATION (FIGURE 2B) ###\n### 00. import data ------------------------------------------------------------\ngerm_dat <- read.csv(\n    file = ""2B_germination_data.csv"",\n    header = TRUE,\n    stringsAsFactors = FALSE)\n\n# remove doubled planted and unplanted cells\ngerm_dat2 <- subset(germ_dat, Remove != ""Remove"")\n\n# remove herbarium populations\ngerm_dat3 <- germ_dat2[!grepl(patter = ""herb"", x = germ_dat2$Population),]\nnrow(germ_dat3) # 1726 seeds planted in germination experiment\n\n# merge with climate data\n\n# remove underscores from population name\ngerm_dat3$Population <- gsub(pattern = ""_"", replacement = """", x = germ_dat3$Population)\n\ng_dat <- merge(germ_dat3, clim_dat, by.x = ""Population"", by.y = ""Population"")\nnrow(g_dat) # 1726\n\n\n### 01. explore germination timing in relation to climatic variables -----------\n\n# add days to germination\ng_dat$days_to_germ <- g_dat$Jdate_germ - 151 # seeds planted/begin watering on May 31st.\n\n\n# Aridity Index\nm1.germ.mixed.AI <- lmer(days_to_germ ~ AI + \n    (1|Population) + (1|tray) + (1|Envelope_ID), \n    data = g_dat)\n\nsummary(m1.germ.mixed.AI)\n\ngerm_AI_df <- ggpredict(m1.germ.mixed.AI, terms = c(""AI""))\n\np2 <- ggplot(germ_AI_df, aes(x, predicted)) + \n    geom_jitter(data = g_dat, aes(x = AI, y = days_to_germ), size = 1.5,\n                width = 0.125, pch = 16, col = ""darkgray"", alpha = 0.', '### 00. import all seed size data from ImageJ particle analysis ----------------\n\nsetwd(""~/Desktop/Dryad/"")\ndat <- read.csv(file = ""2A_seed_size_data.csv"", header = T, stringsAsFactors = F)\n\nnrow(dat) # 33093\nlength(unique(dat$maternal_line)) # 291\n\n\n### 02. import long term climate data from PRISM -------------------------------\nCOV_d1 <- read.csv(file = ""PRISM_precip_1967_2017.csv"", header = T, stringsAsFactors = F)\n\n\n### 03. calculate COV for precipitation at each of the sites -------------------\nFind.COV <- function(x){\n    COV = sd(x)/mean(x)\n    return(COV)}\n\n# find COV for Mean Annual Precipitation MAP for each population\ntapply(COV_d1$MAP_inches, COV_d1$Population, Find.COV)\n\nprecip_COV_df <- data.frame(\n    precip_COV_1967 = tapply(COV_d1$MAP_inches, COV_d1$Population, Find.COV),\n    Population = names(tapply(COV_d1$MAP_inches, COV_d1$Population, Find.COV))\n    )\n\nrow.names(precip_COV_df) <- NULL\n\n# remove underscores from Population names\nprecip_COV_df$Population <- gsub(pattern = ""_"", replacement = """", x = precip_COV_df$Population)\n\n# change Huckaby naming convention\nprecip_COV_df$Population[precip_COV_df$Population == ""HuckabyTrailhead""] <- ""Huckaby""\n\n\n### 04. calculate COV for seed size for each maternal line ---------------------\nseed_COV_df <- data.frame(\n    seed_COV = tapply(dat$seed_area, dat$maternal_line, Find.COV),\n    full_line = names(tapply(dat$seed_area, dat$maternal_line, Find.COV)))\n\nseed_COV_df$Population <- sapply(strsplit(seed_COV_df$full_line, ""_""), function(x) x[1])\nseed_COV_df$maternal_line <- sapply(strsplit(seed_COV_df$full_line, ""_""), function(x) x[2])\n\nrow.names(seed_COV_df) <- NULL\n\n\n### 05. merge seed size COV df with precipitation COV df -----------------------\nsort(unique(seed_COV_df$Population)) == sort(unique(precip_COV_df$Population))\n\ndat2 <- merge(seed_COV_df, precip_COV_df, by.x = ""Population"", by.y = ""Population"")\n\n\n# import additional climate data\nclim_dat <- read.csv(file = ""Plantago_climate_data_simple.csv"", header = T, stringsAsFactors = F)\n\n\n# remove underscores from Population names\nclim_dat$Population <- gsub(pattern = ""_"", replacement = """", x = clim_dat$Population)\n\n# change Huckaby naming convention\nclim_dat$Population[clim_dat$Population == ""HuckabyTrailhead""] <- ""Huckaby""\n\ndat3 <- merge(dat2, clim_dat, by.x = ""Population"", by.y = ""Population"")\n\n\n### 06. explore seed size variation --------------------------------------------\nlibrary(emmeans)\nlibrary(scales)\nlibrary(lmerTest)\nlibrary(ggplot2)\nlibrary(ggeffects)\n\n# build linear model \nm1 <- lmer(seed_COV ~ precip_COV_1967 + AI + (1|Population), data = dat3)\nsummary(m1)\n\n# build prediction data.frame\npred_df <- ggpredict(m1, terms = c(""precip_COV_1967""))\n\ndev.new()\n\nggplot(pred_df, aes(x, predicted)) + \n    geom_jitter(data = dat3, aes(x = precip_COV_1967, y = seed_COV), \n        width = 0.00025, size = 3, pch = 16, col = ""darkgray"", alpha = 0.5) +\n    geom_line(aes(linetype=group, color=group), size = 1.5) +\n    geom_ribbon(aes(ymin=conf.low, ymax=conf.high, fill=group), alpha = 0.25) +\n    scale_linetype_manual(values = c(""solid"", ""dashed"", ""dotted"")) +\n    scale_x_continuous(breaks = c(0.18, 0.21, 0.24, 0.27, 0.3, 0.33), labels = c(0.18, 0.21, 0.24, 0.27, 0.3, 0.33), limits = c(0.18, 0.33)) +\n    theme_light() +\n    xlab(""Precipitation COV (1967-2017)"") +\n    theme(axis.title.x = element_text(size = 12, face = ""bold"")) +\n    ylab(""Seed size COV"") +\n    theme(axis.title.y = element_text(size = 12, face = ""bold"")) +\n    theme(legend.title = element_blank()) +\n    theme(legend.position = ""none"") \n\n# quartz.save(file = ""Figure3_seedCOV_and_precip_COV.jpg"", type = ""jpg"", dpi = 300)\n\n\n\n', '### Analyze relative growth rate in relation to seed size \n\nlibrary(lmerTest)\nlibrary(ggplot2)\nlibrary(ggeffects)\n\n\n### 00. import data ------------------------------------------------------------\nsetwd(""~/Desktop/Dryad/"")\n\ndat <- read.csv(\n    file = ""Plantago_seed_weights_and_growth_rates.csv"" ,\n    header = TRUE,\n    stringsAsFactors = FALSE)\n\n# add column for age on July 14th (Julian 195)\ndat$age_Julian195 <- 195 - dat$Jdate_germ\n\n# remove rows for which seed didn\'t germinate\ndat <-  dat[!is.na(dat$Height_July_14_mm), ]\n\n# isolate cohort of seeds that germinated the same day\ndat <- subset(dat, Jdate_germ == 184)\n\n\n\n### 02. model growth rate as a function of seed size ---------------------------\n\n# Growth Rate (RGR)\n    # RGR = log(X2 − X1)/(t2 − t1), \n        # where X1 = height at time 1\n        # where X2 = height at time 2\n        # where t1 = Julian 177\n        # where t2 = Julian 195\n# RGR = log(Plant Height) / time2 - time1\n\n# remove plants that have height = 0\ndat2 <- subset(dat, Height_July_14_mm > 0)\n\ndat2$days_old <- 195 - dat2$Jdate_germ\n\n# calculate RGR\ndat2$RGR <- log(dat2$Height_July_14_mm) / (195 - dat2$Jdate_germ)\n\n### model relative growth rate of all seeds ------------------------------------\nm1 <- lmer(RGR ~ seed_weight_mg + \n        (1|Population) + (1|Maternal_line) + (1|Plate), data = dat2)\nsummary(m1)\n\n\n# prediction data.frame\ndf1 <- ggpredict(m1, terms = c(""seed_weight_mg[all]"")) # constrains prediction interval\n\nggplot(df1, aes(x, predicted)) + \n      geom_point(data = dat2, aes(x = seed_weight_mg, y = RGR), size = 3, pch = 16, col = ""darkgray"", alpha = 0.75) +\n      geom_line(aes(linetype=group, color=group), size = 1) +\n      geom_ribbon(aes(ymin=conf.low, ymax=conf.high, fill=group), alpha = 0.25) +\n      scale_linetype_manual(values = c(""solid"", ""dashed"", ""dotted"")) +\n      scale_x_continuous(breaks = c(0.25, 0.5, 0.75, 1, 1.25, 1.5), labels = c(0.25, 0.5, 0.75, 1, 1.25, 1.5), limits = c(0.25, 1.5)) +\n      theme_light() +\n      ylim(0, 0.3) +\n      xlab(""Seed mass (mg)"") +\n      theme(axis.title.x = element_text(size = 12, face = ""bold"")) +\n      ylab(""Growth rate (cohort of 62 seeds)"") +\n      theme(axis.title.y = element_text(size = 12, face = ""bold"")) +\n      theme(legend.title = element_blank()) +\n      theme(legend.position = ""none"") \n\n#quartz.save(file = ""Figure4_Growth_Rate_07_27_2022.jpg"", type = ""jpg"", dpi = 300)\n\n\n\n', '### Analyze mortality data from drought experiment -----------------------------\n\n### 00. import mortality data --------------------------------------------------\nsetwd(""~/Desktop/Dryad/"")\n\ndat <- read.csv(\n      file = ""mortality_data.csv"" ,\n      header = TRUE,\n      stringsAsFactors = FALSE)\n\n# remove doubled planted and unplanted cells\ndat <- subset(dat, Remove != ""Remove"")\n\n# add column for days to death (stopped watering on June 28th = Julian 179)\ndat$days_to_death <- dat$Jdate_dead - 179\n\n# remove resurrection populations\ndat2 <-  dat[!grepl(pattern = ""herb"", x = dat$Population),]\nnrow(dat2)\n\n\n### 01. import climate data ----------------------------------------------------\nclim_dat <- read.csv(file = ""Plantago_climate_data_simple.csv"", header = T, stringsAsFactors = F)\n\n# merge data\nmort_dat <- merge(dat2, clim_dat, by.x = ""Population"", by.y = ""Population"")\n\n\n### 02. conduct survival analysis ----------------------------------------------\nlibrary(coxme)\nlibrary(survival)\nlibrary(ranger)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggfortify)\nlibrary(coxme)\nlibrary(car)\n\n\n# fit mixed effects Cox survival model\ncox_me <- coxme(Surv(days_to_death) ~ \n                   plant_height_mm_on_06_29 + AI + Jdate_germ +\n                   (1|Population) + (1|Envelope_ID) + (1|tray), \n                data = mort_dat)\nsummary(cox_me)\nAnova(cox_me)\n\n\n# remove plant height from model\ncox_me_test <- coxme(Surv(days_to_death) ~ \n                   AI + Jdate_germ +\n                   (1|Population) + (1|Envelope_ID) + (1|tray), \n                data = mort_dat)\nsummary(cox_me_test)\nAnova(cox_me_test)\n\n\n### 03. plot the data ----------------------------------------------------------\n\n# add categories for plant height\nrange(mort_dat$plant_height_mm_on_06_29, na.rm = T)\n\n\n# 3 categories -----------------------------------------------------------------\nmort_dat$height_cat3 <-  \n   ifelse(mort_dat$plant_height_mm_on_06_29 < 5, ""<5mm"",\n      ifelse(mort_dat$plant_height_mm_on_06_29 < 10, ""5-10mm"", "">10mm""))\n\nheight_fit3 <- survfit(Surv(days_to_death) ~ height_cat3, data = mort_dat)\n\n# convert prediction intervals to data.frame\nhf3_df <- fortify(height_fit3)\nnames(hf3_df)[names(hf3_df) == ""strata""] <- ""Height""\n\nhf3_df$Height = factor(hf3_df$Height, levels = c(""<5mm"", ""5-10mm"", "">10mm""))\n\n# plot data\nggplot(data = hf3_df, aes(x = time, y = surv, color = Height)) +\n   geom_line(size = 1.25) +\n   geom_ribbon(aes(ymin = lower, ymax = upper, fill = Height), linetype = 0, alpha = 0.25) +\n   theme_light() +\n   xlim(5,25) +\n   theme(axis.title.x = element_text(size = 12, face = ""bold"")) +\n   theme(axis.title.y = element_text(size = 12, face = ""bold"")) +\n   xlab(""Days of drought"") +\n   ylab(""Survival"") +\n   theme(legend.position = c(0.15,0.25)) +\n   theme(legend.key.size = unit(1.25, ""cm"")) +\n   theme(legend.title = element_text(size = 12, face = ""bold"")) +\n   theme(legend.text = element_text(size = 10)) +\n   theme(legend.title.align=0.5) \n\n# quartz.save(file = ""Figure5_survival_curves_01_26_22.jpg"", type = ""jpg"", dpi = 300)\n\n\n### 04. conduct survival analysis for plants with SLA data ---------------------\nsla_dat <- read.csv(file = ""SLA_data.csv"", header = T, stringsAsFactors = F)\n\n# add SLA column\nsla_dat$SLA <- sla_dat$leaf_area_mm_squared / sla_dat$leaf_weight_mg\n\n# remove records without SLA data\nsla_dat <- sla_dat[!is.na(sla_dat$SLA),]\n\n# remove resurrection populations\nsla_dat <- sla_dat[!grepl(pattern = ""herb"", x = sla_dat$Population),]\n\n# merge SLA data and climate data\nsla_dat2 <- merge(sla_dat, clim_dat, by.x = ""Population"", by.y = ""Population"")\n\n# add column for days to death (stopped watering on June 28th = Julian 179)\nsla_dat2$days_to_death <- sla_dat2$Jdate_dead - 179\n\n# conduct survival analysis\ncox_me_SLA <- coxme(Surv(days_to_death) ~ \n                     plant_height_mm_on_06_29 + AI + Jdate_germ + SLA +\n                     (1|Population) + (1|Envelope_ID) + (1|tray), \n                     data = sla_dat2)\n\nsummary(cox_me_SLA)\nAnova(cox_me_SLA)\n\n\n\n\n', '### Analyze mortality data from drought experiment with\n### Structural Equation Models (SEM)\n\n### 00. import mortality data --------------------------------------------------\nsetwd(""~/Desktop/Dryad/"")\n\ndat <- read.csv(\n      file = ""mortality_data.csv"" ,\n      header = TRUE,\n      stringsAsFactors = FALSE)\n\n# remove doubled planted and unplanted cells\ndat <- subset(dat, Remove != ""Remove"")\n\n# add column for days to death (stopped watering on June 28th = Julian 179)\ndat$days_to_death <- dat$Jdate_dead - 179\n\n# remove resurrection populations\ndat2 <-  dat[!grepl(pattern = ""herb"", x = dat$Population),]\nnrow(dat2)\n\n\n### 01. import climate data ----------------------------------------------------\nclim_dat <- read.csv(file = ""Plantago_climate_data_simple.csv"", header = T, stringsAsFactors = F)\n\n# merge data\nmort_dat <- merge(dat2, clim_dat, by.x = ""Population"", by.y = ""Population"")\nstr(mort_dat)\n\n# add days to germinate\nmort_dat$days_to_germ <- mort_dat$Jdate_germ - mort_dat$Jdate_planted\n\n# change names of columns for figure\nnames(mort_dat)[names(mort_dat) == ""AI""] <- ""Source.Aridity""\nnames(mort_dat)[names(mort_dat) == ""Jdate_germ""]  <- ""Germ.Time""\nnames(mort_dat)[names(mort_dat) == ""days_to_death""]  <- ""Days.To.Death""\nnames(mort_dat)[names(mort_dat) == ""plant_height_mm_on_06_29""]  <- ""Plant.Height""\n\n# remove NAs\nnrow(mort_dat)\nmort_dat <- mort_dat[!is.na(mort_dat$Germ.Time), ]\nmort_dat <- mort_dat[!is.na(mort_dat$Days.To.Death), ]\nmort_dat <- mort_dat[!is.na(mort_dat$Plant.Height), ]\n\n\n### 02. Structural Equation Model ----------------------------------------------\nlibrary(lavaan)\nlibrary(semPlot)\n\n# lavaan DEFINITIONS\n      # ~ regression (as normal R syntax)\n      # =~ latent variable definition\n      # ~~ covariance operator (unexplained covariance)\n      \n### specify direct and indirect effects in model\nmodel1_specs <- \n      ""# A path    aridity -> days to death\n      Days.To.Death ~ A * Source.Aridity\n      \n      # B path    aridity -> germ time\n      Germ.Time ~ B * Source.Aridity\n      \n      # C path    aridity -> height\n      Plant.Height ~ C * Source.Aridity\n      \n      # D path    germ time -> height\n      Plant.Height ~ D * Germ.Time\n      \n      # E path    height - > days to death\n      Days.To.Death ~ E * Plant.Height \n      \n      ### indirect and total effects\n      CE := C * E\n      BDE := B * D * E\n      DE := D *E\n      total_Aridity := A + CE + BDE\n      ""\n\n# NOTE: 10,000 bootstrap samples takes a while\n# m1 <- sem(model1_specs, data = mort_dat, se = ""bootstrap"", bootstrap = 10000, test = ""Bollen.Stine"")\nm1 <- sem(model1_specs, data = mort_dat, se = ""bootstrap"", bootstrap = 1000, test = ""Bollen.Stine"")\n\nsummary(m1, standardized = T)\nfitMeasures(m1, c(""cfi"", ""rmsea"", ""srmr""))\nparameterestimates(m1)\n\n# guidelines for model fit\n      # chi-squared p < 0.05 (larger p-values here indicate good fit)\n            # chi-square (sensitive to large sample sizes)\n      # CFI Comparative Fit Index > 0.95\n      # RMSEA Root Mean Square Error of Approximation < 0.06\n      # SRMR Standardized Root Mean Square Residual < 0.08\n\n\n# plot model -------------------------------------------------------------------\np1 <-  semPaths(m1, what = ""std"", \n         nCharNodes = 0, \n         sizeMan = 24, \n         sizeMan2 = 8,\n         edge.label.color = ""black"",\n         edge.label.cex = 1.5, \n         fade = F, \n         layout = ""tree3"", \n         curvePivot = T, \n         curvePivotShape = 1,\n         theme = ""colorblind"",\n         rotation = 3, \n         residuals = F,\n         color = ""lightgray"",\n         asize = 5,\n         border.width = 2,\n         label.font = 4,\n         edge.label.font = 2)\n\n# add significance values to paths\nlibrary(semptools)\n\np2  <-  mark_sig(p1, m1)\nplot(p2)\n\n\n### 03. Structural Equation Model for SLA dataset ------------------------------\nsla_dat <- read.csv(file = ""SLA_data.csv"", header = T, stringsAsFactors = F)\n\n# add SLA column\nsla_dat$SLA <- sla_dat$leaf_area_mm_squared / sla_dat$leaf_weight_mg\n\n# remove records without SLA data\nsla_dat <- sla_dat[!is.na(sla_dat$SLA),]\n\n# remove resurrection populations\nsla_dat <- sla_dat[!grepl(pattern = ""herb"", x = sla_dat$Population),]\nnrow(sla_dat)\n\n# merge SLA data and climate data\nsla_dat <- merge(sla_dat, clim_dat, by.x = ""Population"", by.y = ""Population"")\nnrow(sla_dat)\n\n# add column for days to death (stopped watering on June 28th = Julian 179)\nsla_dat$days_to_death <- sla_dat$Jdate_dead - 179\n\n# change names of columns for figure\nnames(sla_dat)[names(sla_dat) == ""AI""] <- ""Source.Aridity""\nnames(sla_dat)[names(sla_dat) == ""Jdate_germ""]  <- ""Germ.Time""\nnames(sla_dat)[names(sla_dat) == ""days_to_death""]  <- ""Days.To.Death""\nnames(sla_dat)[names(sla_dat) == ""plant_height_mm_on_06_29""]  <- ""Plant.Height""\nnames(sla_dat)[names(sla_dat) == ""SLA""]  <- ""Specific.Leaf.Area""\n\n# remove NAs\nsla_dat <- sla_dat[!is.na(sla_dat$Germ.Time), ]\nsla_dat <- sla_dat[!is.na(sla_dat$Days.To.Death), ]\nsla_dat <- sla_dat[!is.na(sla_dat$Plant.Height), ]\n\nnrow(sla_dat)', '### Assess correlation between seed mass and seed area\n\n\n### 01 import data -------------------------------------------------------------\nsetwd(""~/Desktop/Dryad/"")\ndat <- read.csv(file = ""S1_Plantago_seed_weights.csv"", header = T, stringsAsFactors = F)\n\n### 02. assess simple correlation ----------------------------------------------\nplot(seed_weight_mg ~ area, data = dat,\n     pch = 21, bg = ""lightgray"",\n     xlab = ""Seed Area (mm^2)"",\n     ylab = ""Seed mass (mg)"")\n\ncor.test(dat$seed_weight_mg, dat$area)\n\n\n### 03. build linear model to account for population ---------------------------\nlibrary(lmerTest)\nlibrary(ggplot2)\nlibrary(ggeffects)\n\nmm1 <- lmer(seed_weight_mg ~ area + (1|Population), data = dat)\nsummary(mm1)\n\n# calculate R2 for linear mixed model\nlibrary(partR2)\n\npartR2(mm1, data = dat, R2_type = ""marginal"", partvars = c(""area""), nboot = 1000)\n\n\n# build prediction data.frame\ndf <- ggpredict(mm1, terms = c(""area""))\n\nggplot(df, aes(x, predicted)) + \n      geom_point(data = dat, aes(x = area, y = seed_weight_mg), size = 3, pch = 16, col = ""darkgray"", alpha = 0.5) +\n      geom_line(aes(linetype=group, color=group), size = 1, col = ""blue"") +\n      geom_ribbon(aes(ymin=conf.low, ymax=conf.high), alpha = 0.5, fill = ""blue"") +\n      scale_linetype_manual(values = c(""solid"", ""dashed"", ""dotted"")) +\n      theme_light() +\n      xlab(expression(bold(""Seed area (mm""^2*"")""))) +\n      theme(axis.title.x = element_text(size = 12, face = ""bold"")) +\n      ylab(""Seed mass (mg)"") +\n      theme(axis.title.y = element_text(size = 12, face = ""bold"")) +\n      theme(legend.title = element_blank()) +\n      theme(legend.position = ""none"") \n\n# quartz.save(file = ""FigureS1_seed_area_mass_correlation.jpg"", type = ""jpg"", dpi = 300)\n']",4,"local adaptation, seed traits, seedling traits, aridity gradient, climate change, genetic variation, Plantago patagonica, greenhouse common garden, germination timing, specific leaf area, drought, survival, interspecific trait variation, clinal variation"
Asymmetrical effects of temperature on stage-structured predator-prey interactions,"Warming can impact consumer-resource interactions through multiple mechanisms. For example, warming can both alter the rate at which predators consume prey and the rate prey develop through vulnerable life stages. Thus, the overall effect of warming on consumer-resource interactions will depend upon the strength and asymmetry of warming effects on predator and prey performance. Here, we quantified the temperature dependence of both 1) density-dependent predation rates for two dragonfly nymph predators on a shared mosquito larval prey, via the functional response, and 2) the development rate of mosquito larval prey to a predator-invulnerable adult stage. We united the results of these two empirical studies using a temperature- and density-dependent stage-structured predation model. Warming accelerated both larval mosquito development and increased dragonfly consumption. Model simulations suggest that differences in the magnitude and rate of predator and prey responses to warming determined the change in magnitude of the overall effect of predation on prey survival to adulthood. Specifically, we found that depending on which predator species prey were exposed to in the model, the net effect of warming was either an overall reduction or no change in predation strength across a temperature gradient. Our results highlight a need for better mechanistic understanding of the differential effects of temperature on consumer-resource pairs to accurately predict how warming affects food web dynamics.","['## Functional Response Functions\r\n\r\nrogers.sentis <- function( N0, b, T0, Tl, h0, M, bh, Eh, k, TempK, t ){ # Type II FR, fitted with depletion, with a and h modeled as temp-dependent as described in Sentis et al. 2012\r\n  h = h0 * ( M ^ bh ) * exp( ( Eh ) / ( k * TempK ) )\r\n  a = b *( TempK - T0 ) * ( ( Tl - TempK )^( 1 / 2 ) )\r\n  N0 - lambertW( a * h * N0 * exp( -a * ( t - h * N0 ) ) ) / ( a * h )\r\n}\r\n\r\nrogers.sentis.risk <- function( N0, b, T0, Tl, h0, M, bh, Eh, k, TempK, t ){ # The same function, expressed as a risk function\r\n  h = h0 * ( M ^ bh ) * exp( ( Eh ) / ( k * TempK ) )\r\n  a = b * ( TempK - T0 )*( ( Tl - TempK ) ^ ( 1 / 2 ) )\r\n  ( N0 - lambertW( a * h * N0 * exp( -a * ( t - h * N0 ) ) ) / ( a * h ) ) / N0\r\n}\r\n\r\nrogers.sentis.risk.log <- function( N0, b, T0, Tl, h0, M, bh, Eh, k, TempK, t ){ # Log-transformed, to help with fitting the FR\r\n  h = exp( h0 ) * ( M ^ bh ) * exp( exp ( Eh ) / ( k * TempK ) )\r\n  a = exp( b )*( TempK - exp( T0 ) )*( ( exp( Tl ) - TempK ) ^ ( 1 / 2 ) )\r\n  ( N0 - lambertW( a * h * N0 * exp( -a * ( t - h * N0 ) ) ) / ( a * h ) ) / N0\r\n}\r\n\r\nsentis.attack <- function( b, TempK, T0, Tl ){ # Broken down further, this is the temp-dependence of attack rate described by Sentis et al. 2012\r\n  b * ( TempK - T0 ) * ( ( Tl - TempK ) ^ ( 1 / 2 ) )\r\n}\r\n\r\nsentis.handle <- function( h0, M, bh, Eh, k, TempK ){ # And the temp-dependence of handling time\r\n  h0 * (M ^ bh) * exp( Eh / ( k * TempK ) )\r\n}\r\n\r\n\r\n## Development Rate Functions\r\n\r\narr.rate <- function( c_dev, b_dev, temp ) { # The Arrhenius function, which describes the temp-dependence of prey development\r\n  ( c_dev * exp( b_dev / temp ) )\r\n}\r\n\r\narr.time <- function( c_dev, b_dev, temp ) { # The Arrhenius function, which describes the temp-dependence of prey development\r\n  1 / ( c_dev * exp( b_dev / temp ) )\r\n}\r\n\r\nlogisticfun <- function( temp, r_mort, x1, y1 ) { # A logistic function used to model prey mortality as a function of temperature for Ae. atropalpus\r\n    1 / ( 1 + exp( -r_mort * (temp - x1 ) ) ) + y1\r\n} \r\n\r\n\r\n\r\n## Modeling Functions\r\n\r\nStage_pred_devel <-\r\n  function( timesteps,# Number of timesteps for the simulation\r\n           N_vec,# Vector of individuals in each size/stage class at the current time\r\n           mval,# Temperature-dependent prey mortality rate\r\n           rval, # Temperature-dependent prey development rate\r\n           bval,# A fitted constant, part of the equation for attack rate from Sentis et al. 2012\r\n           T0val,# The lower temperature bound for attack rate\r\n           Tlval,# The upper temperature bound for attack rate\r\n           h0val,# A fitted constant, part of the equation for handling time from Sentis et al. 2012\r\n           bhval,# The constant for body mass-scaling, 0.75\r\n           Ehval, # A fitted activation energy, part of the equation for handling time from Sentis et al. 2012\r\n           Mval, # Avg. predator body mass\r\n           kval, # Part of the equation for handling time from Sentis et al. 2012; this is a constant set to 8.617e-5\r\n           tval, # Time to evaluate the functional response for, default is 1\r\n           tempval ) # Current temperature\r\n  {\r\n    emerge <- 0\r\n    new_N_vec <- mat.or.vec( 1, length( N_vec ) ) # Creates a vector to store the prey densities for the next time step\r\n    for ( i in 1:timesteps ){\r\n      # The first loop runs the model for the number of timesteps specified above\r\n      \r\n      for ( j in 2:4 ){ # This loop handles prey development\r\n        #At each timestep a percentage of our larvae in each box moves to the next box according to r, the development rate\r\n        new_N_vec[ j ] <-\r\n          N_vec[ j ] + rval * N_vec[ j - 1 ] - rval * new_N_vec[ j ] # Each cell gains from the cell on the left, and loses to the right.\r\n      }\r\n      \r\n      new_N_vec[ 1 ] <-\r\n        N_vec[ 1 ] - rval * N_vec[ 1 ] # The first cell can only lose individuals w/o reproduction\r\n      emerge <-\r\n        emerge + rval * N_vec[ 5 ] # Larvae that made it to pupation last time step emerge\r\n      new_N_vec[ 5 ] <-\r\n        N_vec[ 5 ] + rval * N_vec[ 4 ] - rval * N_vec[ 5 ] # The last cell gains from cell 4, then loses individuals that emerge\r\n      \r\n      # Calculate p, the proportion of prey eaten, based on current density of prey and temp\r\n      preykilled <- rogers.sentis(\r\n        N0 = sum( N_vec ),\r\n        b = bval,\r\n        T0 = T0val,\r\n        Tl = Tlval,\r\n        h0 = h0val,\r\n        M = Mval,\r\n        bh = bhval,\r\n        Eh = Ehval,\r\n        k = kval,\r\n        TempK = tempval + 273.15,\r\n        t = tval\r\n      )\r\n      \r\n      p <- ifelse( sum( new_N_vec ) == 0, 0, preykilled / sum( new_N_vec ) )\r\n      p <- ifelse( p > 1, 1, p )\r\n\r\n      # Predation/mortality\r\n      \r\n      new_N_vec <-\r\n        new_N_vec - ( p * new_N_vec )\r\n      new_N_vec <- \r\n        new_N_vec - ( mval * new_N_vec )\r\n      N_vec <-\r\n        new_N_vec #At the end of the loop, N_vec (the number of larvae at the most recent time step) is updated with the new values stored in new_N']",4,"Asymmetrical effects, temperature, stage-structured, predator-prey interactions, warming, consumer-resource interactions, predators, prey, rate, development, vulnerable life stages, strength, density-dependent, predation rates, dragonfly nymph, mosquito larval"
Dirt cheap: An experimental test of controls on resource exchange in an ectomycorrhizal symbiosis,"1. To distinguish among hypotheses on the importance of resource-exchange ratios in outcomes of mutualisms, we measured resource (carbon (C), nitrogen (N), and phosphorus (P)) transfers, and their ratios, between Pinus taeda seedlings and two ectomycorrhizal (EM) fungal species, Rhizopogon roseolus and Pisolithus arhizus in a laboratory experiment.2. We evaluated how ambient light affected those resource fluxes and ratios over 3 time periods (10, 20, and 30 weeks), and the consequences for plant and fungal biomass accrual, in environmental chambers.3. Our results suggest that light availability is an important factor driving absolute fluxes of N, P, and C, but not exchange ratios, although its effects vary among EM fungal species. Declines in N:C and P:C exchange ratios over time, as soil nutrient availability likely declined, were consistent with predictions of biological market models. Absolute transfer of P was an important predictor of both plant and fungal biomass, consistent with the excess resource exchange hypothesis, and N transfer to plants was positively associated with fungal biomass.4. Altogether, light effects on resource fluxes indicated mixed support for various theoretical frameworks, while results on biomass accrual better supported the excess resource exchange hypothesis, although among-species variability is in need of further characterization.","['library(lmerTest)\r\nlibrary(interactions)\r\nlibrary(sjPlot)\r\nlibrary(ggplot2)\r\nlibrary(sciplot)\r\nlibrary(MuMIn)\r\nlibrary(multcomp)\r\nlibrary(emmeans)\r\nlibrary(effects)\r\nlibrary(knitr)\r\n\r\nfulldata<-read.csv(file.choose(), header = TRUE) #analysis_final.csv\r\nnames(fulldata)\r\nfulldata$Harvest <- as.factor(fulldata$Harvest)\r\nfulldata$Light <- as.factor(fulldata$Light)\r\nfulldata$Fungi <- as.factor(fulldata$Fungi)\r\nfulldata$Tree <- as.factor(fulldata$Tree)\r\nfulldata$Shelf <- as.factor(fulldata$Shelf)\r\nlevels(fulldata$Fungi)\r\n\r\noptions(scipen = 999) \r\n\r\n####################################################################################\r\n#Question 1:\r\n#HOW DOES LIGHT AVAILABILITY AFFECT RESOURCE EXCHANGE?\r\n\r\nlightNC <- lmer(N.C ~ Harvest + Light + Fungi+\r\n                  Harvest:Light + \r\n                  Light:Fungi +\r\n                  Harvest:Fungi+\r\n                 # Harvest:Light:Fungi+\r\n                  (1|Shelf) + (1|Tree), data=fulldata)\r\n\r\nNCtable <- anova(lightNC) # 3-way interaction highly non-significant, so dropped\r\ncapture.output(NCtable,file=""S2_NC.doc"")\r\n\r\n# \' Simple effect\' contrasts to test effect of Fungi for the 3 Harvests\r\nemmeans(lightNC, pairwise~Fungi|Harvest,adjustSigma=FALSE)\r\n\r\n# Graph: Harvest x Fungi interaction (Fig. 2)\r\nnc.out <- emmeans(lightNC, pairwise~Fungi|Harvest,adjustSigma=FALSE)\r\nnc <- data.frame(head(nc.out,6))\r\nggplot(nc, aes(emmeans.Harvest, emmeans.emmean, fill=emmeans.Fungi)) + geom_bar(stat=""identity"", position=""dodge2"", size=1.0)+ \r\n  scale_fill_manual(""Fungi"", values = c(""Pisolithus"" = ""gray21"", ""Rhizopogon"" = ""gray85""))+\r\n  geom_errorbar(aes(ymin=emmeans.emmean-emmeans.SE, ymax=emmeans.emmean+emmeans.SE), width=0.4, \r\n                position=position_dodge(width=0.9)) + theme_minimal(base_size=16)+\r\n  xlab(""Harvest (weeks)"")+ ylab(""N:C transfer (µmol/µmol)"") +ylim(0, 0.0017)+\r\n  theme_bw()+theme(panel.grid.major=element_blank(), \r\n                   panel.border=element_blank(), axis.line=element_line(colour=""black""))\r\n# to create a high-res version:\r\ndev.copy(png, file=\'Figure_2.png\', width = 6, \r\n         height = 5, units = \'in\', res = 300)\r\ndev.off()\r\n\r\nlightPC <- lmer(P.C ~ Harvest + Light + Fungi+\r\n                  Harvest:Light + \r\n                  Light:Fungi+\r\n                  Harvest:Fungi+\r\n                  #  Harvest:Light:Fungi+\r\n                  (1|Shelf) + (1|Tree), data=fulldata)\r\ncapture.output(anova(lightPC), file=""S3_PC.doc"")\r\n\r\n# Graph: Main effect of Harvest for P:C (Supp Fig. S4)\r\nemmeans(lightPC, pairwise~Harvest)\r\npc.out <- emmeans(lightPC, pairwise~Harvest)\r\npc <- data.frame(head(pc.out))\r\nggplot(pc, aes(emmeans.Harvest, emmeans.emmean)) + geom_bar(stat=""identity"", position=""dodge"")+ \r\n  geom_errorbar(aes(ymin=emmeans.emmean-emmeans.SE, ymax=emmeans.emmean+emmeans.SE), width=0.4, position=position_dodge(width=0.9)) + theme_minimal(base_size=16)+\r\n  xlab(""Harvest (weeks)"")+ ylab(""P:C transfer (µmol/µmol)"")+ ylim(0,0.000035)+\r\n  theme_bw()+theme(panel.grid.major=element_blank(), \r\n                   panel.border=element_blank(), axis.line=element_line(colour=""black""))\r\n\r\nlightC <- lmer(total.C_umol ~ Harvest + Light + Fungi+\r\n                 Harvest:Light + \r\n                 Light:Fungi +\r\n                 Harvest:Fungi+\r\n                 Harvest:Light:Fungi+\r\n                 (1|Shelf) + (1|Tree), data=fulldata)\r\ncapture.output(anova(lightC), file=""S4_Ctot.doc"")\r\n\r\n# Graph: total C: Light x Harvest x Fungi (Fig. 3)\r\nemmeans(lightC, pairwise~Light|Fungi:Harvest,adjustSigma=FALSE)\r\nctot.out <- emmeans(lightC, pairwise~Harvest|Fungi:Light,adjustSigma=FALSE)\r\nctot <- data.frame(head(ctot.out))\r\nggplot(ctot, aes(emmeans.Harvest, emmeans.emmean, fill=emmeans.Fungi, color=emmeans.Light )) + geom_bar(stat=""identity"", position=""dodge2"", size=1.5)+ \r\n  scale_color_manual(""Light"", values = c(""High"" = ""orange"", ""Low"" = ""steelblue3""))+\r\n  scale_fill_manual(""Fungi"", values = c(""Pisolithus"" = ""black"", ""Rhizopogon"" = ""gray85""))+\r\n  geom_errorbar(aes(ymin=emmeans.emmean-emmeans.SE, ymax=emmeans.emmean+emmeans.SE), width=0.4, position=position_dodge(width=0.9)) + theme_minimal(base_size=16)+\r\n  xlab(""Harvest (weeks)"")+ ylab(""Total C transfer to fungi (µmol)"") + ylim(-13000,1500050)+\r\n  theme_bw()+theme(panel.grid.major=element_blank(), \r\n                   panel.border=element_blank(), axis.line=element_line(colour=""black""))\r\n\r\ndev.copy(png, file=\'Figure_3.png\', width = 6, \r\n         height = 5, units = \'in\', res = 300)\r\ndev.off()\r\n\r\n\r\nC.resp <- lmer(respired.C_umol ~ Harvest + Light + Fungi+\r\n                 Harvest:Light + \r\n                 Light:Fungi +\r\n                 Harvest:Light:Fungi+\r\n                 (1|Shelf) + (1|Tree), data=fulldata)\r\nanova(C.resp)\r\n\r\n# Graph: fungal respired C: Light x Harvest x Fungi (very similar to C total, so don\'t use)\r\ncresp.out <- emmeans(C.resp, pairwise~Harvest|Fungi:Light,adjustSigma=FALSE)\r\ncresp <- data.frame(head(cresp.out))\r\nggplot(cresp, aes(emmeans.Harvest, emmeans.emmean, fill=emmeans.Fungi, color=e']",4,"Dirt cheap, experimental, test, controls, resource exchange, ectomycorrhizal symbiosis, carbon, nitrogen, phosphorus, Pinus taeda seedlings, Rhizopogon roseolus, Pisolithus arh"
Genetic evidence for widespread population size expansion in North American boreal birds prior to the Last Glacial Maximum,"Pleistocene climate cycles are well known to have shaped contemporary species distributions and genetic diversity. Northward range expansions in response to deglaciation following the Last Glacial Maximum (LGM; ~21,000 years ago) have been surmised to lead to population size expansions in terrestrial taxa and changes in seasonal migratory behaviour. Recent findings, however, suggest that some northern temperate populations may have been more stable than expected through the LGM. We modelled the demographic history of twenty co-distributed boreal-breeding bird species of North America from full mitochondrial gene sets and species-specific molecular rates. We used these demographic reconstructions to test how species with different migratory strategies were affected by glacial cycles. Our results suggest that effective population sizes increased in response to deglaciation during the middle Wisconsin period (~45,000 years ago) whereas genetic diversity was maintained throughout the LGM despite shifts in geographic range. We conclude that earlier glacial cycles prior to the LGM have most strongly shaped contemporary genetic diversity in these high-latitude species. We did not find differences in historic population dynamics between species differing in migratory behaviour, contributing to growing evidence that major switches in migratory strategy during the Last Glacial Maximum are unnecessary to explain contemporary migratory patterns.","['library(readxl)\nlibrary(dplyr)\nlibrary(ape)\nlibrary(splits)\nlibrary(ggplot2)\nlibrary(ggtree)\n\nrm(list=ls())\n\nsetwd("""")\n\n##### FUNCTIONS #####\n# Define gmyc.summary to scriptable\n# Modifies output text to distinguish between ""cluster"" and ""entity"" CIs for unique grep search\ngmyc.summary <- function (object, second.peak = FALSE, ...) \n{\n  if (second.peak == TRUE) {\n    tmp <- table(cummax(object$likelihood))\n    lik.peaks <- names(tmp[tmp > 20])\n    peak <- which(object$likelihood == lik.peaks[(length(lik.peaks) - \n                                                    1)])\n  }\n  cat(""Result of GMYC species delimitation\\n"")\n  cat(""\\n\\tmethod:\\t"", object[[""method""]], sep = """")\n  cat(""\\n\\tlikelihood of null model:\\t"", object$likelihood[1], \n      sep = """")\n  if (second.peak == FALSE) {\n    cat(""\\n\\tmaximum likelihood of GMYC model:\\t"", max(object$likelihood), \n        sep = """")\n  }\n  else {\n    cat(""\\n\\tmaximum likelihood of GMYC model:\\t"", object$likelihood[peak], \n        sep = """")\n  }\n  if (second.peak == FALSE) {\n    LR <- 2 * (max(object$likelihood) - object$likelihood[1])\n  }\n  else {\n    LR <- 2 * (object$likelihood[peak] - object$likelihood[1])\n  }\n  cat(""\\n\\tlikelihood ratio:\\t"", LR, sep = """")\n  pvalue <- 1 - pchisq(LR, 2)\n  cat(""\\n\\tresult of LR test:\\t"", pvalue, ifelse(pvalue < 0.001, \n                                                 ""***"", ifelse(pvalue < 0.01, ""**"", ifelse(pvalue < 0.05, \n                                                                                           ""*"", ""n.s.""))), sep = """")\n  if (second.peak == FALSE) {\n    cat(""\\n\\n\\tnumber of ML clusters:\\t"", object$cluster[which.max(object$likelihood)], \n        sep = """")\n    tmp <- object$cluster[object$likelihood > (max(object$likelihood) - \n                                                 2)]\n    cat(""\\n\\tcluster confidence interval:\\t"", paste(min(tmp), max(tmp), \n                                                    sep = ""-""), sep = """")\n    cat(""\\n\\n\\tnumber of ML entities:\\t"", object$entity[which.max(object$likelihood)], \n        sep = """")\n    tmp <- object$entity[object$likelihood > (max(object$likelihood) - \n                                                2)]\n    cat(""\\n\\tentity confidence interval:\\t"", paste(min(tmp), max(tmp), \n                                                   sep = ""-""), sep = """")\n    if (object[[""method""]] == ""single"") {\n      cat(""\\n\\n\\tthreshold time:\\t"", object$threshold.time[which.max(object$likelihood)], \n          ""\\n"", sep = """")\n    }\n    else if (object[[""method""]] == ""multiple"" || object[[""method""]] == \n             ""exhaustive"") {\n      cat(""\\n\\n\\tthreshold time:\\t"", object$threshold.time[[which.max(object$likelihood)]], \n          ""\\n"", sep = "" "")\n    }\n    cat(""\\n"")\n  }\n  else {\n    cat(""\\n\\n\\tnumber of ML clusters:\\t"", object$cluster[peak], \n        sep = """")\n    cat(""\\n\\tnumber of ML entities:\\t"", object$entity[peak], \n        sep = """")\n    if (object[[""method""]] == ""single"") {\n      cat(""\\n\\tthreshold time:\\t"", object$threshold.time[peak], \n          ""\\n"", sep = """")\n    }\n    else if (object[[""method""]] == ""multiple"" || object[[""method""]] == \n             ""exhaustive"") {\n      cat(""\\n\\tthreshold time:\\t"", object$threshold.time[[peak]], \n          ""\\n"", sep = "" "")\n    }\n    cat(""\\n"")\n  }\n}\n\n##### GMYC Runs #####\n# Read in data and set up dataframe to store run results\nmetadata <- #load sample list with populations assigned \nsp_summary <- #load outgroup assignments for each species\nnewcols <- c(""LR_Pvalue"",""N_Clusters"",""CI_Cluster_Low"", ""CI_Cluster_High"",""Threshold"")\nsp_summary[newcols] <- NA\n\nsp_list <- sort(unique(sp_summary$species_code))\n# Run GMYC on loop - no outgroup\nfor(i in sp_list){\n  # read in mcc.ca.tre\n  current_spp <- i \n  spp_data <- dplyr::filter(metadata, species_code==paste(current_spp))\n  file_name <- paste0(current_spp,""/"",current_spp,""_concatenated_outgroup_combined.mcc.ca.tre"")\n  mcc_tr <-read.nexus(file_name)\n  \n  # drop outgroup\n  outgroup <- sp_summary$outgroup_individual[sp_summary$species_code==current_spp]\n  mcc_tr_pruned <- ape::drop.tip(mcc_tr, outgroup)\n  \n  # check that tree is fully bifurcating and ultrametric\n  is.binary(mcc_tr_pruned)\n  is.ultrametric(mcc_tr_pruned)\n  \n  # run GMYC\n  gmyc_result <- gmyc(mcc_tr_pruned) # gmyc algorithm\n  gmyc_output <- capture.output(gmyc.summary(gmyc_result)) # store p-value, confidence intervals, threshold number, etc\n  clusters <- spec.list(gmyc_result) # store assignment of individual samples to clusters\n  pops <- dplyr::select(spp_data, fasta_label, population)\n  colnames(clusters) <- c(""GMYC_spec"",""fasta_label"")\n  clusters_pops <- left_join(pops, clusters)\n  \n  # write out species assignment df to csv\n  csv_pathname <- paste0(""gmyc_results/"",current_spp,""_gmyc.csv"")\n  write.csv(clusters_pops, file=csv_pathname)\n  \n  # put p-value, total number of clusters, cluster confidence interval, and threshold time\n  LR_test <- grep(""result of LR test"", gmyc_output, value=TRUE)\n  LR_test <- gsub(""\\tresult of LR test:\\t"", """", LR_test)\n', '## clean environment & plots\nrm(list=ls())\n\n#load packages \nlibrary(tidyverse) \nlibrary(ape)\nlibrary(geiger)\nlibrary(phytools)\nlibrary(Rmisc)\nlibrary(picante)\nlibrary(nlme)\nlibrary(MuMIn)\nlibrary(scales)\nlibrary(cowplot)\n\n#set working directory\n\n#load data\nbeast <- readxl::read_excel(""wd/Kimmitt_etal_beast_summary.xlsx"", na = ""NA"")\n\nother.dat <- read_csv(""wd/migration_distance_Winger&Pegan.csv"")\n\nother.dat <- other.dat %>%\n  select(Species, Migratory_status, boreal_migdist)\n\n\nbeast$Species <- gsub("" "", ""_"", beast$species)\nbeast$Species\nbeast$Species[beast$Species == ""Dryobates_villosus""] <- ""Picoides_villosus""\n\ndat <- beast %>%\n  left_join(other.dat, by = ""Species"")\n\n## set theme\nth=theme_bw()+\n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(), panel.background = element_blank())+\n  theme(panel.grid.major=element_blank(),panel.grid.minor=element_blank())+\n  theme(axis.title.x=element_text(margin=margin(t=10,r=0,b=0,l=0)))+\n  theme(legend.position = ""none"")+\n  theme(axis.title.y=element_text(margin=margin(t=0,r=10,b=0,l=0)))+\n  theme(strip.text=element_text(size=11),\n        axis.text=element_text(size=11),\n        axis.title=element_text(size=12)) \n\n\n#plot the variation in migratory distance as a histogram\nggplot(dat,aes(x=boreal_migdist))+geom_histogram(,color=""black"",fill=""gray"",bins=8)+th + xlab(""Migratory distance (km)"") +\n  ylab(""Number of species"") \n\n\n\n### PGLS \n#load tree \ntree <- read.tree(""/Users/abbykimmitt/Dropbox (University of Michigan)/winger_lab/demographic_history/beast/jetz_matched_names.tre"")\n\n# making sure species names match between dataset and tree \ndat$Species[!dat$Species %in% tree$tip.label]\ndat$Species[dat$Species == ""Regulus_calendula""] <- ""Corthylio_calendula""\ndat$Species[!dat$Species %in% tree$tip.label]\n\n# prune tree down to species we need\ntree <- drop.tip(tree, tree$tip.label[!tree$tip.label %in% dat$Species])\nplot(tree, cex = 0.5)\n\n# assigning row names so that branch lengths are recognized \ndat<- as.data.frame(dat)\nrow.names(dat) <- dat$Species\n\n\n### Historic Ne (Ne1)\n\n#removing missing data\ndat.Ne1_ac2 <- dat[complete.cases(dat$Ne1_ac2, dat$boreal_migdist, dat$mass),]\ntree.Ne1_ac2 <-  drop.tip(tree, tree$tip.label[!tree$tip.label%in%dat.Ne1_ac2$Species])\n\ndat.Ne1_ac4 <- dat[complete.cases(dat$Ne1_ac4, dat$boreal_migdist),]\ntree.Ne1_ac4 <-  drop.tip(tree, tree$tip.label[!tree$tip.label%in%dat.Ne1_ac4$Species])\n\ndat.Ne1_3c2 <- dat[complete.cases(dat$Ne1_3c2, dat$boreal_migdist),]\ntree.Ne1_3c2 <-  drop.tip(tree, tree$tip.label[!tree$tip.label%in%dat.Ne1_3c2$Species])\n\ndat.Ne1_3c4 <- dat[complete.cases(dat$Ne1_3c4, dat$boreal_migdist),]\ntree.Ne1_3c4 <-  drop.tip(tree, tree$tip.label[!tree$tip.label%in%dat.Ne1_3c4$Species])\n\ndat.Ne1_cytb <- dat[complete.cases(dat$Ne1_cytb, dat$boreal_migdist),]\ntree.Ne1_cytb <-  drop.tip(tree, tree$tip.label[!tree$tip.label%in%dat.Ne1_cytb$Species])\n\n\n#Migration distance \n#models without considering phylogeny \nNe1_ac2 <-  lm(Ne1_ac2~ boreal_migdist, data = dat.Ne1_ac2)\nNe1_ac4 <-  lm(Ne1_ac4~ boreal_migdist, data = dat.Ne1_ac4)\nNe1_3c2 <-  lm(Ne1_3c2~ boreal_migdist, data = dat.Ne1_3c2)\nNe1_3c4 <-  lm(Ne1_3c4~ boreal_migdist, data = dat.Ne1_3c4)\nNe1_cytb <-  lm(Ne1_cytb~ boreal_migdist, data = dat.Ne1_cytb)\n\n#checking phylogenetic signal\nres_ac2 = residuals(Ne1_ac2)\npsigNe1_ac2 <- phylosig(tree.Ne1_ac2, res_ac2, method=""lambda"", test=T)\npsigNe1_ac2$P # not significant\n\nres_ac4 = residuals(Ne1_ac4)\npsigNe1_ac4 <- phylosig(tree.Ne1_ac4, res_ac4, method=""lambda"", test=T)\npsigNe1_ac4$P #not significant\n\nres_3c2 = residuals(Ne1_3c2)\npsigNe1_3c2 <- phylosig(tree.Ne1_3c2, res_3c2, method=""lambda"", test=T)\npsigNe1_3c2$P #not significant\n\nres_3c4 = residuals(Ne1_3c4)\npsigNe1_3c4 <- phylosig(tree.Ne1_3c4, res_3c4, method=""lambda"", test=T)\npsigNe1_3c4$P # not significant\n\nres_cytb = residuals(Ne1_cytb)\npsigNe1_cytb <- phylosig(tree.Ne1_cytb, res_cytb, method=""lambda"", test=T)\npsigNe1_cytb$P # not significant\n\n#summarizing OLS without phylogenetic signal \nsummary(Ne1_ac2) #not sig\nsummary(Ne1_ac4) #not sig\nsummary(Ne1_3c2) #not sig\nsummary(Ne1_3c4) #not sig \nsummary(Ne1_cytb) #not sig \n\n##################################\n\n### Pop expansion initiation (time1)\n\n#removing missing data\ndat.time1_ac2 <- dat[complete.cases(dat$time1_ac2, dat$boreal_migdist),]\ntree.time1_ac2 <-  drop.tip(tree, tree$tip.label[!tree$tip.label%in%dat.time1_ac2$Species])\n\ndat.time1_ac4 <- dat[complete.cases(dat$time1_ac4, dat$boreal_migdist),]\ntree.time1_ac4 <-  drop.tip(tree, tree$tip.label[!tree$tip.label%in%dat.time1_ac4$Species])\n\ndat.time1_3c2 <- dat[complete.cases(dat$time1_3c2, dat$boreal_migdist),]\ntree.time1_3c2 <-  drop.tip(tree, tree$tip.label[!tree$tip.label%in%dat.time1_3c2$Species])\n\ndat.time1_3c4 <- dat[complete.cases(dat$time1_3c4, dat$boreal_migdist),]\ntree.time1_3c4 <-  drop.tip(tree, tree$tip.label[!tree$tip.label%in%dat.time1_3c4$Species])\n\ndat.time1_cytb <- dat[complete.cases(dat$time1_cytb, dat$boreal_migdist),]\ntree.time1_cy', '# title: Kimmitt et al. ""Genetic evidence for widespread population size expansion in North American boreal birds prior to the Last Glacial Maximum"" Population Statistics\n# author: abby kimmitt, ben winger\n#this script is evaluating Tajima\'s D and Fst\n\n#load packages \nlibrary(ape)\nlibrary(PopGenome)\nlibrary(adegenet)\nlibrary(hierfstat)\nlibrary(pegas)\n\n#clean environment\nrm(list=ls())\n\n#set working directories \nsnippet=#snippet is the 5 letter code for the species \n\n#define wd1 as where the fasta files are located\n#define wd2 as where the metadata on the files are located\n\n\n#read in the fasta alignment \nfaspath <- paste0(wd1,""/"",snippet,"".fasta"")\nalignment <- read.dna(faspath, format=""fasta"")\n\n#Tajima\'s D\nd<-tajima.test(alignment)\n\n# Import metadata\nmpath<- paste0(wd2,""/"",""Kimmitt_etal_sample_data.csv"")\nmeta <- read.csv(mpath, header = T)\n\n# check to make sure names of all samples match meta file \nname_check <- rownames(alignment)[which(rownames(alignment) %in% meta$fasta_label == F)]\nprint(name_check)\n\nnames<-rownames(alignment)\n# append meta data to rownames\nappend.meta <- function(Seq){\n  for(i in 1:nrow(Seq)){\n    if(any(meta$fasta_label %in% rownames(Seq)[i]) == T){\n      data <- meta[meta$fasta_label == rownames(Seq)[i],]\n      rownames(Seq)[i] <- paste(data$fasta_label, data$population, sep = ""_"") ### add desired metadata here\n    }\n  }\n  return(Seq)\n}\n\nall<- append.meta(alignment) # all populations\n\n# create population labels \nLabels <- rownames(all)\nCN <- grep(""CN"", Labels, ignore.case = F) #MI and MN\nNE <- grep(""NE"", Labels, ignore.case = F) #NY and VT (Northeast)\nMB <- grep(""MB"", Labels, ignore.case = F) #Manitoba\nAB <- grep(""AB"", Labels, ignore.case = F) #Alberta\n\n# Convert alignment to Genind object, preserving populations defined above\npop.factor <- vector(""character"", nrow(all)) \npop.factor[CN] <- ""CN""\npop.factor[NE] <- ""NE""\npop.factor[MB] <- ""MB""\npop.factor[AB] <- ""AB""\n\npop.factor <- as.factor(pop.factor)\n\na.genind <- DNAbin2genind(all, pop = pop.factor)\na.genpop <- genind2genpop(a.genind, pop = pop.factor)\na.genpop\n\n#using hierfstat and adgenet to look at Fst \n#can\'t use Fst function in pegas on haploid data\na.hier<-genind2hierfstat(a.genind)\nfst<-pairwise.WCfst(a.hier, diploid=F)\n']",4,"North American boreal birds, genetic evidence, population size expansion, Last Glacial Maximum, Pleistocene climate cycles, species distributions, genetic diversity, range expansions, deglaciation, terrestrial taxa, migratory behaviour, northern temperate populations, demographic"
Juvenile rank acquisition is associated with fitness independent of adult rank,"Social rank has been identified as a significant determinant of fitness in a variety of species. The importance of social rank suggests that the process by which juveniles come to establish their position in the social hierarchy is a critical component of development. Here, we use the highly predictable process of rank acquisition in spotted hyenas to study the consequences of variation in rank acquisition in early life. In spotted hyenas, rank is 'inherited' through a learning process called 'maternal rank inheritance.' This pattern is highly predictable: ~80% of juveniles acquire the exact rank predicted by the rules of maternal rank inheritance. This predictable nature of rank acquisition in these societies allows the process of rank acquisition to be studied independently from the ultimate rank that each juvenile attains. In this study, we use Elo-deviance scores, a novel application of the Elo-rating method, to calculate each juvenile's deviation from expected pattern of maternal rank inheritance during development. Despite variability in rank acquisition among juveniles, most of these juveniles come to attain the exact rank expected of them according to the rules of maternal rank inheritance. Nevertheless, we find that transient variation in rank acquisition in early life is associated with long term fitness consequences for these individuals: juveniles 'underperforming' their expected ranks show reduced survival and lower lifetime reproductive success than better-performing peers, and this relationship is independent of both maternal rank and rank achieved in adulthood. We also find that multiple sources of early life adversity have cumulative, but not compounding, effects on fitness. Future work is needed to determine variation in rank acquisition directly affects fitness, or if some other variable, such as maternal investment or juvenile condition, causes variation in both of these outcomes.","[""################################################################################\n#                              Define functions                                #\n#                                                                              #\n#                                 Eli Strauss                                  #\n#                                                                              #\n#                                                                              #\n#                               February 2019                                  #\n################################################################################\n\n\nelo_deviance <- function(intx, id, k = 20){\n  if(nrow(intx) < 1){stop('No aggressions supplied')}\n  intx <- intx[order(intx$date, intx$time),]\n  intx$order <- 1:nrow(intx)\n  #remove interactions with no rank\n  intx <- intx[!is.na(intx$recip.rank) & !is.na(intx$agg.rank),]\n  #remove interactions between ids with same rank\n  intx <- intx[intx$agg.rank != intx$recip.rank,]\n  if(nrow(intx) < 1){warning('No interactions with known, non-equal rank'); return(NA)}\n  \n  we_1 <- intx[intx$recip.rank - intx$agg.rank > 0, c('date', 'aggressor', 'recip', 'order')]\n  we_2 <- intx[intx$recip.rank - intx$agg.rank < 0,c('date', 'recip', 'aggressor', 'order')]\n  \n  names(we_1) <- c('date', 'exp.winner', 'exp.loser', 'order')\n  names(we_2) <- c('date','exp.winner', 'exp.loser', 'order')\n  wins_expected <- rbind(we_1, we_2)\n  wins_expected <- wins_expected[order(wins_expected$order),]\n  \n  #ensure that the focal individual is first in the matrix\n  idents <- unique(c(intx$aggressor, intx$recip))\n  idents <- c(idents[which(idents == id)], idents[-which(idents == id)])\n  ##get elo scores for all individuals, although only valid for focal individual\n  elo_observed <- aniDom::elo_scores(winners = intx$aggressor, losers = intx$recip, return.trajectories = T, identities = idents, K = k)\n  elo_expected <- aniDom::elo_scores(winners = wins_expected$exp.winner, losers = wins_expected$exp.loser, return.trajectories = T, identities = idents, K = k)\n  return(elo_observed[1,length(elo_observed[1,])] - elo_expected[1,length(elo_expected[1,])])\n}\n\nelo_obs <- function(intx, id, k = 20){\n  if(nrow(intx) < 1){stop('No aggressions supplied')}\n  intx <- intx[order(intx$date, intx$time),]\n  intx$order <- 1:nrow(intx)\n  #remove interactions with no rank\n  intx <- intx[!is.na(intx$recip.rank) & !is.na(intx$agg.rank),]\n  #remove interactions between ids with same rank\n  intx <- intx[intx$agg.rank != intx$recip.rank,]\n  if(nrow(intx) < 1){warning('No interactions with known, non-equal rank'); return(NA)}\n  \n  we_1 <- intx[intx$recip.rank - intx$agg.rank > 0, c('date', 'aggressor', 'recip', 'order')]\n  we_2 <- intx[intx$recip.rank - intx$agg.rank < 0,c('date', 'recip', 'aggressor', 'order')]\n  \n  names(we_1) <- c('date', 'exp.winner', 'exp.loser', 'order')\n  names(we_2) <- c('date','exp.winner', 'exp.loser', 'order')\n  wins_expected <- rbind(we_1, we_2)\n  wins_expected <- wins_expected[order(wins_expected$order),]\n  \n  #ensure that the focal individual is first in the matrix\n  idents <- unique(c(intx$aggressor, intx$recip))\n  idents <- c(idents[which(idents == id)], idents[-which(idents == id)])\n  ##get elo scores for all individuals, although only valid for focal individual\n  elo_observed <- aniDom::elo_scores(winners = intx$aggressor, losers = intx$recip, return.trajectories = T, identities = idents, K = k)\n  elo_expected <- aniDom::elo_scores(winners = wins_expected$exp.winner, losers = wins_expected$exp.loser, return.trajectories = T, identities = idents, K = k)\n  return(elo_observed[1,length(elo_observed[1,])])\n}\n\n\nelo_deviance_traj <- function(intx, id, k = 20){\n  if(nrow(intx) < 1){stop('No aggressions supplied')}\n  intx <- intx[order(intx$date, intx$time),]\n  intx$order <- 1:nrow(intx)\n  #remove interactions with no rank\n  intx <- intx[!is.na(intx$recip.rank) & !is.na(intx$agg.rank),]\n  #remove interactions between ids with same rank\n  intx <- intx[intx$agg.rank != intx$recip.rank,]\n  if(nrow(intx) < 1){warning('No interactions with known, non-equal rank'); return(NA)}\n  \n  we_1 <- intx[intx$recip.rank - intx$agg.rank > 0, c('date', 'aggressor', 'recip', 'order')]\n  we_2 <- intx[intx$recip.rank - intx$agg.rank < 0,c('date', 'recip', 'aggressor', 'order')]\n  \n  names(we_1) <- c('date', 'exp.winner', 'exp.loser', 'order')\n  names(we_2) <- c('date','exp.winner', 'exp.loser', 'order')\n  wins_expected <- rbind(we_1, we_2)\n  wins_expected <- wins_expected[order(wins_expected$order),]\n  \n  #ensure that the focal individual is first in the matrix\n  idents <- unique(c(intx$aggressor, intx$recip))\n  idents <- c(idents[which(idents == id)], idents[-which(idents == id)])\n  ##get elo scores for all individuals, although only valid for focal individual\n  elo_observed <- aniDom::elo_scores(winners = intx$aggressor, losers = intx$recip, return.trajectories = T, identities = idents, K = k)\n  elo_expected <- aniDom::elo_score"", ""################################################################################\n#                   Identify den cubs and den dependent period                 #\n#                                                                              #\n#                                 Eli Strauss                                  #\n#                                                                              #\n#                                                                              #\n#                               February 2019                                  #\n################################################################################\n\n######Load packages and set global options\nrm(list = ls())\nlibrary(dplyr)\nlibrary(here)\noptions(stringsAsFactors = FALSE)\n\n####Read and tidy data\nload(file = 'data/raw_data.RData')\n\n##############Assemble data#######################\n### Select hyenas with a known birthdate\ncohortInfo <- filter(left_join(tblHyenas, tblLifeHistory.long, by = 'id'), !is.na(DOB))\nnames(cohortInfo) <- tolower(names(cohortInfo))\n\n### Add clan information\ncohortInfo$clan <-  left_join(cohortInfo, filter(tblLifeHistory, event_code == 'DOB'), by = 'id')$event_data\n\n## Restrict to four clans with the best long-term data\ncohortInfo <- filter(cohortInfo, clan %in% c('talek', 'serena.n', 'serena.s', 'happy.zebra'))\n\n## Restrict to known sex, seen in tblHyenasPerSession, not involved in experimental manipulation at den\ncub.fight.ids <- intersect(unique(c(filter(aggsFull, date >= '2015-01-01',\n                                           context == 'experiment')$aggressor,\n                                    filter(aggsFull, date >= '2015-01-01',\n                                           context == 'experiment')$recip)),\n                           filter(tblLifeHistory.long, DOB >= '2014-06-01')$id)\ncohortInfo <- filter(cohortInfo, sex %in% c('m', 'f'), id %in% tblHyenasPerSession$id,\n                     !id %in% cub.fight.ids, !is.na(mom))\n\n### Restrict to cubs who were born at least 1 year before the end of the aggression data\ncohortInfo <- filter(cohortInfo, (clan %in% c('talek', 'happy.zebra') & dob <= (as.Date('2016-06-19') - 365)) |\n                       (clan == 'serena.n' & dob <= (as.Date('2016-12-31') - 365)) |\n                       (clan == 'serena.s' & dob <= (as.Date('2017-03-05') - 365)))\n\n\n\n## Add den graduation dates, binary variable for if cub survived to graduation, disappeared date, and date last observed, lrs\ncohortInfo$survive_to_grad <- 1\ncohortInfo$denend <- NA\ncohortInfo$lrs <- NA\nfor(row in 1:nrow(cohortInfo)){\n  if(is.na(cohortInfo[row,'dengrad'])){\n    if(is.na(cohortInfo[row,'disappeared'])){\n      cohortInfo[row,'denend'] <- cohortInfo[row,'dob']+365 ## If no graduation date and hyena is still alive, use 1 year\n    }else{\n      if(cohortInfo[row,]$dob+365 < cohortInfo[row,]$disappeared){\n        cohortInfo[row,'denend'] <- cohortInfo[row,]$dob+365 ## IF no grad date and hyena died after 1 yr, use 1 yr\n      }else{\n        cohortInfo[row,'denend'] <- cohortInfo[row,]$disappeared ## If no grad date and hyena died 1 yr or before, den end = disappeared date and survive to grad = 0\n        cohortInfo[row,]$survive_to_grad <- 0\n      }\n    }\n  }else{cohortInfo[row,'denend'] <- min(as.numeric(cohortInfo[row,'dengrad']),  as.numeric(cohortInfo[row,'dob']+365), na.rm = T) } # Restrict >1yr grad dates to 1yr\n  \n  cohortInfo$last.seen.sessions <- max(filter(tblHyenasPerSession, id == cohortInfo$id[row])$date, na.rm = TRUE)\n  \n  \n  ## LRS - number of offspring that survived to at least 2 years old\n  if(!is.na(cohortInfo[row,]$disappeared) & cohortInfo[row,]$sex == 'f'){\n    kids <- filter(tblHyenas, mom == cohortInfo$id[row])$id\n    cohortInfo$lrs[row] <- nrow(filter(tblLifeHistory.long, id %in% kids, \n                                   is.na(Disappeared) | Disappeared - DOB >= 365*2))\n  }\n}\n\nsave(cohortInfo, file = '02.cohortInfo.RData')\n"", ""################################################################################\n#                        Calculate Elo-deviance scores                         #\n#                                                                              #\n#                                 Eli Strauss                                  #\n#                                                                              #\n#                                                                              #\n#                               February 2019                                  #\n################################################################################\n\n######Load packages and set global options\nrm(list = ls())\nlibrary(dplyr)\nlibrary(aniDom)\nlibrary(here)\noptions(stringsAsFactors = FALSE)\n\nload('02.cohortInfo.RData')\n\n###Read data files\nload('data/raw_data.RData')\n\nsource('00.define_functions.R')\n\n#### Assemble relevant variables for each cub. \ncub_dev_vars <- list()\nfor(i in 1:nrow(cohortInfo)){\n  if(i %% 100 == 0){\n    cat('Working on row ', i, 'of ', nrow(cohortInfo), '\\n')\n  }\n  \n  ####Basics\n  id <- cohortInfo$id[i]\n  mom <- cohortInfo$mom[i]\n  \n  birthdate <- cohortInfo$dob[i]\n  sex <- cohortInfo$sex[i]\n  den_period <- as.numeric(cohortInfo$denend[i]) - as.numeric(cohortInfo$dob[i])+1\n  clan <- cohortInfo$clan[i]\n  \n  disappeared <- cohortInfo$disappeared[i]\n  last.seen.sessions <- max(filter(tblHyenasPerSession, id == cohortInfo$id[i])$date, na.rm = TRUE)\n  \n  \n  ###Deviance scores\n  ## Filter aggressions such that id is either aggressor or recipient and takes place only among den-dwelling cubs. Aggressor rank and recipient rank can't be the same i.e. no sibs\n  intx <- filter(aggsFull, aggressor == id | recip == id, date <= agg.end, date <= recip.end,\n                 agg.rank != recip.rank)\n  \n  ## skip the rest if there are no interactions from which to calculate deviance scores\n  if(nrow(intx) < 1){next}\n  \n  ## Deviance score\n  end_diff <- elo_deviance(intx, id)\n  ## Observed elo score from same interactions\n  end_obs <- elo_obs(intx, id)\n  \n  ## Number of interactions\n  num_intx = nrow(intx)\n\n  ## Mothers rank. If interactions occur in multiple years, use mean of rank from both years.\n  mom_rank <- mean(as.numeric(filter(tblFemaleRanks, year %in% intx$year, id == cohortInfo$mom[i])$stan_rank), na.rm = T)\n  \n  \n  ####post grad deviance score - based on interactions between focal individuals and subadults\n  intx <- filter(aggsFull, aggressor == id | recip == id, date > agg.end, date > recip.end, \n                 date <= agg.dob+(365*2), date <= recip.dob+(365*2),\n                 agg.rank != recip.rank)\n  postgrad_intx = nrow(intx)\n  if(nrow(intx)){\n    postgrad_diff <- elo_deviance(intx, id)\n  }else(postgrad_diff <- NA)\n  \n  ####first year of adulthood - based on interactions between focal individuals and other adults\n  intx <- filter(aggsFull, aggressor == id | recip == id, \n                 date > agg.dob+(365*2), date > recip.dob+(365*2),\n                 date <= birthdate + (365*3),agg.rank != recip.rank)\n  adult_intx <- nrow(intx)\n  \n  if(nrow(intx)){\n    adult_diff <- elo_deviance(intx, id)\n  }else(adult_diff <- NA)\n\n  ### Does the mother survive to graduation or adulthood?  \n  mom_survive_to_grad <- ifelse(filter(tblLifeHistory.long, id == cohortInfo$mom[i])$Disappeared >= as.Date(cohortInfo$denend[i] , origin = '1970-01-01') | is.na(filter(tblLifeHistory.long, id == cohortInfo$mom[i])$Disappeared),1 , 0) %>%\n    ifelse(length(.), ., NA)\n  mom_survive_to_2 <- ifelse(filter(tblLifeHistory.long, id == cohortInfo$mom[i])$Disappeared >= as.Date(cohortInfo$dob[i] + 365*2 , origin = '1970-01-01') | is.na(filter(tblLifeHistory.long, id == cohortInfo$mom[i])$Disappeared),1 , 0) %>%\n    ifelse(length(.), ., NA)\n  \n  \n  if(id %in% tblFemaleRanks$id){\n    first_adult_rank <- as.numeric(filter(tblFemaleRanks, id == cohortInfo$id[i])[1,]$stan_rank)\n    year.joined <- filter(tblFemaleRanks, id == cohortInfo$id[i])[1,]$year\n    if(mom %in% filter(tblFemaleRanks, year == year.joined)$id){\n      ranks <- filter(tblFemaleRanks, year == year.joined, clan == cohortInfo$clan[i])\n      if(!is.na(as.numeric(cohortInfo$litrank[i]))){\n        expected_spot <- as.numeric(ranks[ranks$id == mom,'rank']) + as.numeric(cohortInfo$litrank[i])\n      }else{expected_spot <- as.numeric(ranks[ranks$id == mom,'rank']) + 1}\n      adult_first_rank_diff <- round(first_adult_rank - \n                                       ((-2*(expected_spot-1)/(nrow(ranks)-1))+1), 4)\n      adult_expected_first_rank <- round(((-2*(expected_spot-1)/(nrow(ranks)-1))+1), 4)\n    }else{adult_first_rank_diff <- NA}\n  }else{first_adult_rank <- NA; adult_first_rank_diff <- NA; adult_expected_first_rank <- NA}\n  \n  ## Survival to den independence\n  survive_to_grad = cohortInfo$survive_to_grad[i]\n  ## Lifetime reproductive success\n  lrs = cohortInfo$lrs[i]\n  \n  ##record summary statistics\n  cub_dev_vars[[i]] <- data.frame(id, mom, sex, birthdate, den_period, num_intx,\n "", '################################################################################\n#                           Conduct survival analysis                          #\n#                                                                              #\n#                                 Eli Strauss                                  #\n#                                                                              #\n#                                                                              #\n#                               February 2019                                  #\n################################################################################\n\n\n######Load packages and set global options\nrm(list = ls())\nlibrary(dplyr)\nlibrary(here)\nlibrary(survival)\nlibrary(survminer)\nlibrary(coxme)\nlibrary(viridis)\nlibrary(sjPlot)\nlibrary(lme4)\noptions(stringsAsFactors = FALSE)\noptions(scipen = 8)\n\nload(\'04.cub_dev_vars.RData\')\nsource(\'00.define_functions.R\')\n\n### Color scheme for plotting\ncolors <- viridis(5)[c(1,4)]\ncolors4 <- viridis(5)[c(1:4)]\n\n####Calculate end date\nall_cub_survival <- list()\nfor(i in 1:nrow(cub_dev_vars)){\n  ID <- cub_dev_vars$id[i]\n  if(is.na(cub_dev_vars$disappeared[i])){\n    #times <- as.numeric(cub_dev_vars$last.seen.sessions[i] - cub_dev_vars$birthdate[i])\n    times <- as.numeric(as.Date(\'2019-06-30\') - cub_dev_vars$birthdate[i])\n    events <- 0\n    #if(is.na(times)) next\n  }else{\n    times <- as.numeric(cub_dev_vars$disappeared[i] - cub_dev_vars$birthdate[i])\n    events <- 1\n  }\n  cub_dat <- cub_dev_vars[i,]\n  #if(length(events)-1) cub_dat <- rbind(cub_dat,cub_dev_vars[i,])\n  all_cub_survival[[i]] <- data.frame(time = times, etype = events, cub_dat)\n}\nall_cub_survival <- do.call(rbind, all_cub_survival)\n\n### Remove individuals without time, etype, end_diff, and mom_rank\nall_survival <- filter(all_cub_survival, !is.na(time), !is.na(etype), !is.na(end_diff), !is.na(mom_rank),\n                           birthdate >= ""1988-06-26"")\n\n##### Right censor data for males that survive to 2 years old at 2####\nall_survival[!is.na(all_survival$time) & all_survival$time >= (365*2) & all_survival$sex == \'m\',\'etype\'] <- 0\nall_survival[!is.na(all_survival$time) & all_survival$time >= (365*2) & all_survival$sex == \'m\',\'time\'] <- 365*2\n\n#### Right censor hyenas who fission to Talek East clan (not well studied) at date of fission ####\neast.membership <- read.csv(\'data/ClanMembership.csv\')\neast.membership <- filter(east.membership, Membership == \'e\' |\n                            Mom %in% filter(east.membership, Membership == \'e\'))\nall_survival[all_survival$id %in% east.membership$ID & (is.na(all_survival$disappeared) | all_survival$disappeared >= \'2000-01-01\'),\'etype\'] <- 0\nall_survival[all_survival$id %in% east.membership$ID & (is.na(all_survival$disappeared) | all_survival$disappeared >= \'2000-01-01\'),\'time\'] <- as.Date(\'2000-01-01\') - \n  all_survival[all_survival$id %in% east.membership$ID & (is.na(all_survival$disappeared) | all_survival$disappeared >= \'2000-01-01\'),\'birthdate\']\n\n\nall_survival$mom_survive_to_2 <- factor(all_survival$mom_survive_to_2, levels = c(1,0),\n                                        labels = c(\'survived\', \'dead\'))\n\n###Restrict to hyenas that survive to graduation\nall_grad <- filter(all_survival, survive_to_grad == 1)\nall_grad$age <- all_grad$time/365\n\n\n#### Some descriptives ####\nmean(all_grad$end_diff)\nsd(all_grad$end_diff)\nsum(!is.na(all_grad$end_diff))\n\nmean(all_grad$postgrad_diff, na.rm = TRUE)\nsd(all_grad$postgrad_diff, na.rm = TRUE)\nsum(!is.na(all_grad$postgrad_diff))\n\nmean(all_grad$adult_diff, na.rm = TRUE)\nsd(all_grad$adult_diff, na.rm = TRUE)\nsum(!is.na(all_grad$adult_diff))\n\n#### Center variables\nall_grad$end_diff_centered <- scale(all_grad$end_diff)\nall_grad$num_intx_centered <- scale(all_grad$num_intx)\nall_grad$end_obs_centered <- scale(all_grad$end_obs)\nall_grad$postgrad_diff_centered <- scale(all_grad$postgrad_diff)\nall_grad$adult_diff_centered <- scale(all_grad$adult_diff)\n\n### Dichotomize variables\nall_grad$diff_class <- cut(all_grad$end_diff, breaks = c(-1000, 0, 1000), labels = c(\'Elo < expected\', \'Elo ≥ expected\'), right = FALSE)\nall_grad$diff_class <- factor(all_grad$diff_class, levels = c(\'Elo ≥ expected\', \'Elo < expected\'))\n\nall_grad$rank_class <- cut(all_grad$mom_rank, breaks = c(-1.1, 0, 1.1), labels = c(\'low rank\', \'high rank\'))\nall_grad$rank_class <- factor(all_grad$rank_class, levels = c(\'high rank\', \'low rank\'))\n\nall_grad$obs_class <- cut(all_grad$end_obs, breaks = c(-1000, mean(all_grad$end_obs), 1000), labels = c(\'below average\', \'above average\'))\nall_grad$obs_class <- factor(all_grad$obs_class, levels = c(\'above average\', \'below average\'))\n\n#### Explore different models relating elo deviance at den independence and survival####\n## Models include number of interactions, rank class (high/low), mom survive to 2\n\n##\nelo.dev.mod <- coxme(Surv(age, etype) ~ end_diff_centered + num_intx_centered + mom_rank + mom_survive_to_2 + (1|clan), data = all_grad)\ndiff.class.mod ', ""################################################################################\n#                              Rank acquisition                                #\n#                                                                              #\n#                                 Eli Strauss                                  #\n#                                                                              #\n#                                                                              #\n#                               February 2019                                  #\n################################################################################\n\n######Load packages and set global options\nrm(list = ls())\nlibrary(dplyr)\nlibrary(aniDom)\nlibrary(here)\nlibrary(ggplot2)\nlibrary(survminer)\nlibrary(viridis)\nlibrary(ggpubr)\noptions(stringsAsFactors = FALSE)\n\nset.seed(1989)\n\nload('02.cohortInfo.RData')\nload('06.rank.acquisition.RData')\nload('data/raw_data.RData')\nsource('00.define_functions.R')\ncolors <- viridis(5)[c(1,4)]\n\n\n##### Rank learning - calculate Elo deviance every month\nrank_learning_builder <- list()\ncounter <- 1\nfor(i in 1:nrow(cohortInfo)){\n  id.intx <- filter(aggsFull, aggressor == cohortInfo$id[i] | recip == cohortInfo$id[i])\n  if(!nrow(id.intx))next\n  max.month <- ceiling(as.numeric(max(id.intx$date, na.rm = TRUE) - cohortInfo$birthdate[i])/30)\n  if(max.month <= 0)next\n  for(m in seq(from = 1, to = max.month, by = 1)){\n    intx <- filter(id.intx, aggressor == cohortInfo$id[i] | recip == cohortInfo$id[i],\n                   agg.rank != recip.rank, \n                   date > cohortInfo$birthdate[i]+(30*(m-1)),\n                   date <= cohortInfo$birthdate[i] + (30*(m)))\n    \n    \n    \n    if(nrow(intx) < 1) next\n    rank_learning_builder[[counter]] <- data.frame(deviance = elo_deviance(intx, cohortInfo$id[i]),\n                                             age.months = m,\n                                             id = cohortInfo$id[i],\n                                             sex = cohortInfo$sex[i])\n    counter <- counter + 1\n  }\n}\nrank_learning <- do.call(rbind, rank_learning_builder)\n\nrank_learning_summary <- rank_learning %>% \n  group_by(age.months) %>%\n  summarize(sd.deviance = sd(deviance), num.ids = length(id)) %>%\n  filter(num.ids >= 20)\n\nsegmented::davies.test(lm(data = rank_learning_summary, sd.deviance ~ age.months), seg.Z = ~age.months)\nseg <- segmented::segmented(lm(data = rank_learning_summary, sd.deviance ~ age.months), seg.Z = ~age.months,\n                             psi = c(18))\n\n## Model of before rank acquisition\nsummary(lm(data = filter(rank_learning_summary, age.months <= seg$psi[2]), sd.deviance ~ age.months))\n\n## Model of before rank acquisition\nsummary(lm(data = filter(rank_learning_summary, age.months > seg$psi[2]), sd.deviance ~ age.months))\n\n\npdf('plots/6_rank_acquisition_timing.pdf', 3.5,3.5)\nggplot(rank_learning_summary, aes(x = age.months, y = sd.deviance)) + \n  geom_jitter() +\n  theme_survminer()+\n  xlab('Age (months)')+\n  ylab('Std. deviation of Elo deviance')+\n  geom_vline(xintercept = 18, lty = 2)+\n  geom_vline(xintercept = seg$psi[2], lty = 3)+\n  geom_smooth(data = filter(rank_learning_summary, age.months <= round(seg$psi[2])), method = 'lm',\n              col = 'red', fill = 'red')+\n  geom_smooth(data = filter(rank_learning_summary, age.months >= round(seg$psi[2])), method = 'lm',\n              col = 'red', fill = 'red')\ndev.off()\n  \n\n#### Do juveniles acquire ranks as expected?\n\nfirst.ranks <- tblFemaleRanks %>%\n  group_by(id) %>%\n  summarize(first.year = min(year), \n            first.rank = as.numeric(stan_rank[which.min(year)]))\n\nfirst.ranks$mom <- left_join(first.ranks, tblHyenas, by = 'id')$mom\nfirst.ranks$mom.rank <- as.numeric(left_join(first.ranks, tblFemaleRanks, \n                                             by = c('mom' = 'id', 'first.year' = 'year'))$stan_rank)\nfirst.ranks$diff_class <- left_join(first.ranks, rank.acquisition, by = 'id')$diff_class\nfirst.ranks <- filter(first.ranks, !is.na(diff_class))\nfirst.ranks$mom.cub.diff <- first.ranks$mom.rank - first.ranks$first.rank\nfirst.ranks$adult_expected_first_rank <- left_join(first.ranks, rank.acquisition, by = 'id')$adult_expected_first_rank\nfirst.ranks$first_adult_rank <- left_join(first.ranks, rank.acquisition, by = 'id')$first_adult_rank\nfirst.ranks$adult_first_rank_diff <- left_join(first.ranks, rank.acquisition, by = 'id')$adult_first_rank_diff\n\ncor.test(first.ranks$first.rank, first.ranks$mom.rank)\n\nrank.acquisition$mri <- ifelse(rank.acquisition$adult_first_rank_diff == 0, 'mri',\n                               ifelse(rank.acquisition$adult_first_rank_diff < 0, 'below', 'above'))\n\nmri.table <- table(rank.acquisition[,c('mri', 'diff_class')])\nchisq.test(mri.table)\n\n\n#cairo_pdf('plots/6_mri.pdf', 3.5, 3.5)\nmri <- ggplot(first.ranks, aes(x = mom.rank, y = first.rank, col = diff_class)) +\n  geom_point(alpha = 0.7) +\n  theme_survminer()+\n  geom_abline(intercept = 0, slope = 1, linetype = 2)+\n  scale_co"", ""################################################################################\n#                        Calculate Elo-deviance scores                         #\n#                                                                              #\n#                                 Eli Strauss                                  #\n#                                                                              #\n#                                                                              #\n#                               February 2019                                  #\n################################################################################\n\n######Load packages and set global options\nrm(list = ls())\nlibrary(dplyr)\nlibrary(aniDom)\nlibrary(here)\noptions(stringsAsFactors = FALSE)\n\nload('02.cohortInfo.RData')\n\n###Read tidy hyena data tables\nload('data/raw_data.RData')\n\nsource('00.define_functions.R')\n\n##remove aggressions where recipient ignores or counterattacks\naggsFull <- filter(tblAggression, \n                   !response1 %in% c('ct', 'ignore'),\n                   !response2 %in% c('ct', 'ignore'),\n                   !response3 %in% c('ct', 'ignore'))\n\n## Add year column\naggsFull$year <- format(aggsFull$date, '%Y')\n\n## Add aggressor and recipient den end date and mother\naggsFull$agg.end <- left_join(aggsFull, cohortInfo, by = c('aggressor' = 'id'))$denend\naggsFull$recip.end <- left_join(aggsFull, cohortInfo, by = c('recip' = 'id'))$denend\naggsFull$agg.mom <- left_join(aggsFull, tblHyenas, by = c('aggressor' = 'id'))$mom\naggsFull$recip.mom <- left_join(aggsFull, tblHyenas, by = c('recip' = 'id'))$mom\n\n#### Add aggressor and recipient rank from rank table. Rank calculated for each female yearly according to Strauss & Holekamp 2019\n###use individual rank if individual is in ranks table (only true for adults)\naggsFull$agg.rank <- left_join(aggsFull, tblFemaleRanks, by = c('year', 'aggressor' = 'id'))$rank %>% as.numeric()\naggsFull$recip.rank <- left_join(aggsFull, tblFemaleRanks, by = c('year', 'recip' = 'id'))$rank %>% as.numeric()\n###otherwise, use maternal rank by selecting rank of mother\naggsFull$agg.rank[is.na(aggsFull$agg.rank)] <- left_join(aggsFull[is.na(aggsFull$agg.rank),], tblFemaleRanks, by = c('year', 'agg.mom' = 'id'))$rank %>% as.numeric()\naggsFull$recip.rank[is.na(aggsFull$recip.rank)] <- left_join(aggsFull[is.na(aggsFull$recip.rank),], tblFemaleRanks, by = c('year', 'recip.mom' = 'id'))$rank %>% as.numeric()\n\n###agg and recip birthdates. Use minimum of date of birth and date first seen to incorporate individuals whose DOB is missing\naggsFull$agg.dob <- pmin(left_join(aggsFull, tblLifeHistory.long, by = c('aggressor' = 'id'))$DOB,\n                         left_join(aggsFull, tblLifeHistory.long, by = c('aggressor' = 'id'))$DFS)\n\naggsFull$recip.dob <- pmin(left_join(aggsFull, tblLifeHistory.long, by = c('recip' = 'id'))$DOB,\n                         left_join(aggsFull, tblLifeHistory.long, by = c('recip' = 'id'))$DFS)\n\n\n#### Assemble relevant variables for each cub. \ncub_dev_vars <- list()\nfor(i in 1:nrow(cohortInfo)){\n  if(i %% 100 == 0){\n    cat('Working on row ', i, 'of ', nrow(cohortInfo), '\\n')\n  }\n  \n  ####Basics\n  id <- cohortInfo$id[i]\n  mom <- cohortInfo$mom[i]\n  \n  birthdate <- cohortInfo$dob[i]\n  sex <- cohortInfo$sex[i]\n  den_period <- as.numeric(cohortInfo$denend[i]) - as.numeric(cohortInfo$dob[i])+1\n  clan <- cohortInfo$clan[i]\n  \n  disappeared <- cohortInfo$disappeared[i]\n  last.seen.sessions <- max(filter(tblHyenasPerSession, id == cohortInfo$id[i])$date, na.rm = TRUE)\n  \n  \n  ###Deviance scores\n  ## Filter aggressions such that id is either aggressor or recipient and takes place only among den-dwelling cubs. Aggressor rank and recipient rank can't be the same i.e. no sibs\n  intx <- filter(aggsFull, aggressor == id | recip == id, date <= agg.end, date <= recip.end,\n                 agg.rank != recip.rank)\n  \n  ## skip the rest if there are no interactions from which to calculate deviance scores\n  if(nrow(intx) < 1){next}\n  \n  ## Deviance score\n  end_diff <- elo_deviance(intx, id, k = 100)\n  ## Observed elo score from same interactions\n  end_obs <- elo_obs(intx, id, k = 100)\n  \n  ## Number of interactions\n  num_intx = nrow(intx)\n\n  ## Mothers rank. If interactions occur in multiple years, use mean of rank from both years.\n  mom_rank <- mean(as.numeric(filter(tblFemaleRanks, year %in% intx$year, id == cohortInfo$mom[i])$stan_rank), na.rm = T)\n  \n  \n  ####post grad deviance score - based on interactions between focal individuals and subadults\n  intx <- filter(aggsFull, aggressor == id | recip == id, date > agg.end, date > recip.end, \n                 date <= agg.dob+(365*2), date <= recip.dob+(365*2),\n                 agg.rank != recip.rank)\n  postgrad_intx = nrow(intx)\n  if(nrow(intx)){\n    postgrad_diff <- elo_deviance(intx, id, k = 100)\n  }else(postgrad_diff <- NA)\n  \n  ####first year of adulthood - based on interactions between focal individuals and other adults\n  intx <- filter("", '################################################################################\n#                           Conduct survival analysis                          #\n#                                                                              #\n#                                 Eli Strauss                                  #\n#                                                                              #\n#                                                                              #\n#                               February 2019                                  #\n################################################################################\n\n\n######Load packages and set global options\nrm(list = ls())\nlibrary(dplyr)\nlibrary(here)\nlibrary(survival)\nlibrary(survminer)\nlibrary(viridis)\nlibrary(coxme)\nlibrary(lme4)\noptions(stringsAsFactors = FALSE)\n\n### Run with Elo deviance calculated with alternative K value\nload(\'09.cub_dev_vars_k100.RData\')\n\n### Color scheme for plotting\ncolors <- viridis(5)[c(1,4)]\ncolors4 <- viridis(5)[c(1:4)]\n\n####Calculate end date\nall_cub_survival <- list()\nfor(i in 1:nrow(cub_dev_vars)){\n  ID <- cub_dev_vars$id[i]\n  if(is.na(cub_dev_vars$disappeared[i])){\n    #times <- as.numeric(cub_dev_vars$last.seen.sessions[i] - cub_dev_vars$birthdate[i])\n    times <- as.numeric(as.Date(\'2019-06-30\') - cub_dev_vars$birthdate[i])\n    events <- 0\n    #if(is.na(times)) next\n  }else{\n    times <- as.numeric(cub_dev_vars$disappeared[i] - cub_dev_vars$birthdate[i])\n    events <- 1\n  }\n  cub_dat <- cub_dev_vars[i,]\n  #if(length(events)-1) cub_dat <- rbind(cub_dat,cub_dev_vars[i,])\n  all_cub_survival[[i]] <- data.frame(time = times, etype = events, cub_dat)\n}\nall_cub_survival <- do.call(rbind, all_cub_survival)\n\n### Remove individuals without time, etype, end_diff, and mom_rank\nall_survival <- filter(all_cub_survival, !is.na(time), !is.na(etype), !is.na(end_diff), !is.na(mom_rank),\n                       birthdate >= ""1988-06-26"")\n\n##### Right censor data for males that survive to 2 years old at 2####\nall_survival[!is.na(all_survival$time) & all_survival$time >= (365*2) & all_survival$sex == \'m\',\'etype\'] <- 0\nall_survival[!is.na(all_survival$time) & all_survival$time >= (365*2) & all_survival$sex == \'m\',\'time\'] <- 365*2\n\n#### Right censor hyenas who fission to Talek East clan (not well studied) at date of fission ####\neast.membership <- read.csv(\'data/ClanMembership.csv\')\neast.membership <- filter(east.membership, Membership == \'e\' |\n                            Mom %in% filter(east.membership, Membership == \'e\'))\nall_survival[all_survival$id %in% east.membership$ID & (is.na(all_survival$disappeared) | all_survival$disappeared >= \'2000-01-01\'),\'etype\'] <- 0\nall_survival[all_survival$id %in% east.membership$ID & (is.na(all_survival$disappeared) | all_survival$disappeared >= \'2000-01-01\'),\'time\'] <- as.Date(\'2000-01-01\') - \n  all_survival[all_survival$id %in% east.membership$ID & (is.na(all_survival$disappeared) | all_survival$disappeared >= \'2000-01-01\'),\'birthdate\']\n\n\nall_survival$mom_survive_to_2 <- factor(all_survival$mom_survive_to_2, levels = c(1,0),\n                                        labels = c(\'survived\', \'dead\'))\n\n###Restrict to hyenas that survive to graduation\nall_grad <- filter(all_survival, survive_to_grad == 1)\nall_grad$age <- all_grad$time/365\n\n\n#### Some descriptives ####\nmean(all_grad$end_diff)\nsd(all_grad$end_diff)\nsum(!is.na(all_grad$end_diff))\n\nmean(all_grad$postgrad_diff, na.rm = TRUE)\nsd(all_grad$postgrad_diff, na.rm = TRUE)\nsum(!is.na(all_grad$postgrad_diff))\n\nmean(all_grad$adult_diff, na.rm = TRUE)\nsd(all_grad$adult_diff, na.rm = TRUE)\nsum(!is.na(all_grad$adult_diff))\n\n#### Center variables\nall_grad$end_diff_centered <- scale(all_grad$end_diff)\nall_grad$num_intx_centered <- scale(all_grad$num_intx)\nall_grad$mom_rank_centered <- scale(all_grad$mom_rank)\nall_grad$end_obs_centered <- scale(all_grad$end_obs)\nall_grad$postgrad_diff_centered <- scale(all_grad$postgrad_diff)\nall_grad$adult_diff_centered <- scale(all_grad$adult_diff)\n\n### Dichotomize variables\nall_grad$diff_class <- cut(all_grad$end_diff, breaks = c(-1000, 0, 1000), labels = c(\'Elo < expected\', \'Elo ≥ expected\'), right = FALSE)\nall_grad$diff_class <- factor(all_grad$diff_class, levels = c(\'Elo ≥ expected\', \'Elo < expected\'))\n\nall_grad$rank_class <- cut(all_grad$mom_rank, breaks = c(-1000, 0, 1000), labels = c(\'low rank\', \'high rank\'))\nall_grad$rank_class <- factor(all_grad$rank_class, levels = c(\'high rank\', \'low rank\'))\n\nall_grad$obs_class <- cut(all_grad$end_obs, breaks = c(-1000, mean(all_grad$end_obs), 1000), labels = c(\'below average\', \'above average\'))\nall_grad$obs_class <- factor(all_grad$obs_class, levels = c(\'above average\', \'below average\'))\n\n#### Explore different models relating elo deviance at den independence and survival####\n## Models include number of interactions, rank class (high/low), mom survive to 2\n\n##\nelo.dev.mod <- coxme(Surv(age, etype) ~ end_diff_centered + num_intx_centered + mom_rank + mom_survi', ""################################################################################\n#                              Rank acquisition                                #\n#                                                                              #\n#                                 Eli Strauss                                  #\n#                                                                              #\n#                                                                              #\n#                               February 2019                                  #\n################################################################################\n\n######Load packages and set global options\nrm(list = ls())\nlibrary(dplyr)\nlibrary(aniDom)\nlibrary(here)\nlibrary(ggplot2)\nlibrary(survminer)\noptions(stringsAsFactors = FALSE)\n\nload('02.cohortInfo.RData')\nload('11.rank.acquisition.k100.RData')\nload('data/raw_data.RData')\nsource('00.define_functions.R')\ncolors <- viridis(5)[c(1,4)]\n\n\n\n##remove aggressions where recipient ignores or counterattacks\naggsFull <- filter(tblAggression, \n                   !response1 %in% c('ct', 'ignore'),\n                   !response2 %in% c('ct', 'ignore'),\n                   !response3 %in% c('ct', 'ignore'))\n\naggsFull$year <- format(aggsFull$date, '%Y')\n\naggsFull$agg.end <- left_join(aggsFull, cohortInfo, by = c('aggressor' = 'id'))$DenEnd\naggsFull$recip.end <- left_join(aggsFull, cohortInfo, by = c('recip' = 'id'))$DenEnd\naggsFull$agg.mom <- left_join(aggsFull, tblHyenas, by = c('aggressor' = 'id'))$mom\naggsFull$recip.mom <- left_join(aggsFull, tblHyenas, by = c('recip' = 'id'))$mom\n\n###use individual rank if individual rank is in ranks table\naggsFull$agg.rank <- left_join(aggsFull, tblFemaleRanks, by = c('year', 'aggressor' = 'id'))$rank %>% as.numeric()\naggsFull$recip.rank <- left_join(aggsFull, tblFemaleRanks, by = c('year', 'recip' = 'id'))$rank %>% as.numeric()\n\n###otherwise, use moms rank\naggsFull$agg.rank[is.na(aggsFull$agg.rank)] <- left_join(aggsFull[is.na(aggsFull$agg.rank),], tblFemaleRanks, by = c('year', 'agg.mom' = 'id'))$rank %>% as.numeric()\naggsFull$recip.rank[is.na(aggsFull$recip.rank)] <- left_join(aggsFull[is.na(aggsFull$recip.rank),], tblFemaleRanks, by = c('year', 'recip.mom' = 'id'))$rank %>% as.numeric()\n\n###agg and recip birthdates - if no birthdate, treat first seen as 'birthdate'\naggsFull$agg.dob <- pmin(left_join(aggsFull, tblLifeHistory.long, by = c('aggressor' = 'id'))$DOB,\n                         left_join(aggsFull, tblLifeHistory.long, by = c('aggressor' = 'id'))$DFS)\n\naggsFull$recip.dob <- pmin(left_join(aggsFull, tblLifeHistory.long, by = c('recip' = 'id'))$DOB,\n                           left_join(aggsFull, tblLifeHistory.long, by = c('recip' = 'id'))$DFS)\n\n\n\n\n##### Rank learning - calculate Elo deviance every month\nrank_learning_builder <- list()\ncounter <- 1\nfor(i in 1:nrow(cohortInfo)){\n  id.intx <- filter(aggsFull, aggressor == cohortInfo$id[i] | recip == cohortInfo$id[i])\n  if(!nrow(id.intx))next\n  max.month <- ceiling(as.numeric(max(id.intx$date, na.rm = TRUE) - cohortInfo$birthdate[i])/30)\n  if(max.month <= 0)next\n  for(m in seq(from = 1, to = max.month, by = 1)){\n    intx <- filter(id.intx, aggressor == cohortInfo$id[i] | recip == cohortInfo$id[i],\n                   agg.rank != recip.rank, \n                   date > cohortInfo$birthdate[i]+(30*(m-1)),\n                   date <= cohortInfo$birthdate[i] + (30*(m)))\n    \n    \n    \n    if(nrow(intx) < 1) next\n    rank_learning_builder[[counter]] <- data.frame(deviance = elo_deviance(intx, cohortInfo$id[i], k = 100),\n                                                   age.months = m,\n                                                   id = cohortInfo$id[i],\n                                                   sex = cohortInfo$sex[i])\n    counter <- counter + 1\n  }\n}\nrank_learning <- do.call(rbind, rank_learning_builder)\n\nrank_learning_summary <- rank_learning %>% \n  group_by(age.months) %>%\n  summarize(sd.deviance = sd(deviance), num.ids = length(id)) %>%\n  filter(num.ids >= 20)\n\nsegmented::davies.test(lm(data = rank_learning_summary, sd.deviance ~ age.months), seg.Z = ~age.months)\nseg <- segmented::segmented(lm(data = rank_learning_summary, sd.deviance ~ age.months), seg.Z = ~age.months,\n                            psi = c(18))\n\npdf('plots/k100/10_rank_acquisition_timing_k100.pdf', 3.5,3.5)\nggplot(rank_learning_summary, aes(x = age.months, y = sd.deviance)) + \n  geom_jitter() +\n  theme_survminer()+\n  xlab('Age (months)')+\n  ylab('Std. deviation of Elo deviance')+\n  geom_vline(xintercept = 18, lty = 2)+\n  geom_vline(xintercept = seg$psi[2], lty = 3)+\n  geom_line(inherit.aes = FALSE,data =\n              data.frame(x = c(1,seg$psi[2]),\n                         y = seg$coefficients[1] + seg$coefficients[2]*c(1, seg$psi[2])),\n            aes(x,y), col = 'red')+\n  geom_line(inherit.aes = FALSE,data =\n              data.frame(x = c(seg$psi[2], 103),\n                         y = c(seg$coefficients[1] + seg$coeffi""]",4,"juvenile rank acquisition, fitness, social rank, spotted hyenas, maternal rank inheritance, Elo-deviance scores, early life, survival, reproductive success, adversity, maternal investment, juvenile condition."
Data and R computer code from: Summer elk calf survival in a partially migratory population,"These data and computer code (written in R, https://www.r-project.org) were created to statistically evaluate a suite of intrinsic and extrinsic risk factors related to calf elk and their mothers' body condition and age. Specifically, known-fate data were collected from 94 elk calves monitored from 2013-2016 in a partially migratory elk (Cervus canadensis) population in Alberta, Canada. Along with adult female data on pregnancy status, age, and body condition, we created a time-to-event dataset that allowed us to analyze calf mortality risk in a time-to-event approach. We also estimated pooled survivorship and cause-specific mortality, as well as stratifying these metrics by migration tactic (resident vs. eastern migrant). Cox proportional hazards models were used to evaluate calf mortality risk in terms of forage biomass (kg/ha), bear predation risk (from an RSF), and other factors that varied between migration tactics. We tested for differences in a number of maternal reproductive parameters (e.g., pregnancy status) and for calf explanatory variables between migrant and resident elk segments. We also use cumulative incidence functions to estimate cause-specific mortality in this multiple carnivore system. Ultimately, we hope that this work helps wildlife managers anticipate how elk calf survival and partial migration dynamics are affected by grizzly bear predation, and our study builds on a long-term partial migration study at the Ya Ha Tinda Ranch in Alberta, Canada.","['# similar to cuminc function but allows more control over time points\n""CumIncidence"" <- function(ftime, fstatus, group, t, strata, rho = 0, \n                           cencode = 0, subset, na.action = na.omit, level,\n                           xlab = ""Time"", ylab = ""Probability"", \n                           col, lty, lwd, digits = 4)\n{\n  # check for the required package\n  if(!require(""cmprsk""))\n  { stop(""Package `cmprsk\' is required and must be installed.\\n \n           See help(install.packages) or write the following command at prompt\n           and then follow the instructions:\\n\n           > install.packages(\\""cmprsk\\"")"") } \n  # collect data\n  mf  <- match.call(expand.dots = FALSE)\n  mf[[1]] <- as.name(""list"")\n  mf$t <- mf$digits <- mf$col <- mf$lty <- mf$lwd <- mf$level <- \n    mf$xlab <- mf$ylab <- NULL\n  mf <- eval(mf, parent.frame())\n  g <- max(1, length(unique(mf$group)))\n  s <- length(unique(mf$fstatus))\n  if(missing(t)) \n  { time <- pretty(c(0, max(mf$ftime)), 6)\n  ttime <- time <- time[time < max(mf$ftime)] }\n  else { ttime <- time <- t }\n  # fit model and estimates at time points\n  fit   <- do.call(""cuminc"", mf)\n  tfit <- timepoints(fit, time)\n  # print result\n  cat(""\\n+"", paste(rep(""-"", 67), collapse=""""), ""+"", sep ="""")\n  cat(""\\n| Cumulative incidence function estimates from competing risks data |"")\n  cat(""\\n+"", paste(rep(""-"", 67), collapse=""""), ""+\\n"", sep ="""")\n  tests <- NULL\n  if(g > 1)\n  { \n    tests <- data.frame(fit$Tests[,c(1,3,2)], check.names = FALSE)\n    colnames(tests) <- c(""Statistic"", ""df"", ""p-value"")\n    tests$`p-value` <- format.pval(tests$`p-value`)\n    cat(""Test equality across groups:\\n"")\n    print(tests, digits = digits) \n  }\n  cat(""\\nEstimates at time points:\\n"")\n  print(tfit$est, digits = digits)\n  cat(""\\nStandard errors:\\n"")\n  print(sqrt(tfit$var), digits = digits)\n  #\n  if(missing(level))\n  { # plot cumulative incidence functions\n    if(missing(t))\n    { time <- sort(unique(c(ftime, time)))\n    x <- timepoints(fit, time) }\n    else x <- tfit\n    col <- if(missing(col)) rep(1:(s-1), rep(g,(s-1))) else col\n    lty <- if(missing(lty)) rep(1:g, s-1) else lty\n    lwd <- if(missing(lwd)) rep(1, g*(s-1)) else lwd      \n    matplot(time, base::t(x$est), type=""s"", ylim = c(0,1), \n            xlab = xlab, ylab = ylab, xaxs=""i"", yaxs=""i"", \n            col = col, lty = lty, lwd = lwd)\n    legend(""topleft"", legend =  rownames(x$est), x.intersp = 2, \n           bty = ""n"", xjust = 1, col = col, lty = lty, lwd = lwd)\n    out <- list(test = tests, est = tfit$est, se = sqrt(tfit$var))\n  }\n  else\n  { if(level < 0 | level > 1) \n    error(""level must be a value in the range [0,1]"")\n    # compute pointwise confidence intervals\n    oldpar <- par(ask=TRUE)\n    on.exit(par(oldpar))\n    if(missing(t))\n    { time <- sort(unique(c(ftime, time)))\n    x <- timepoints(fit, time) }\n    else x <- tfit\n    z <- qnorm(1-(1-level)/2)\n    lower <- x$est ^ exp(-z*sqrt(x$var)/(x$est*log(x$est)))\n    upper <- x$est ^ exp(z*sqrt(x$var)/(x$est*log(x$est)))\n    col <- if(missing(col)) rep(1:(s-1), rep(g,(s-1))) \n    else             rep(col, g*(s-1))\n    lwd <- if(missing(lwd)) rep(1, g*(s-1)) \n    else             rep(lwd, g*(s-1))      \n    # plot pointwise confidence intervals\n    for(j in 1:nrow(x$est))\n    { matplot(time, cbind(x$est[j,], lower[j,], upper[j,]), type=""s"", \n              xlab = xlab, ylab = ylab, xaxs=""i"", yaxs=""i"", \n              ylim = c(0,1), col = col[j], lwd = lwd[j], lty = c(1,3,3))\n      legend(""topleft"", legend =  rownames(x$est)[j], bty = ""n"", xjust = 1) }\n    # print pointwise confidence intervals\n    i <- match(ttime, time)\n    ci <- array(NA, c(2, length(i), nrow(lower)))\n    ci[1,,] <- base::t(lower[,i])\n    ci[2,,] <- base::t(upper[,i])\n    dimnames(ci) <- list(c(""lower"", ""upper""), ttime, rownames(lower))\n    cat(paste(""\\n"", level*100, ""% pointwise confidence intervals:\\n\\n"", sep=""""))\n    print(ci, digits = digits)\n    out <- list(test = tests, est = x$est, se = sqrt(tfit$var), ci = ci)\n  }\n  # return results\n  invisible(out)\n}']",4,"Summer elk calf survival, partially migratory population, intrinsic risk factors, extrinsic risk factors, calf elk, body condition, age, known-fate data, adult female data, pregnancy status, time-to-event dataset, calf mortality risk, pooled survivor"
Data from: Wild Goffin's cockatoos flexibly manufacture and use tool sets,"The use of different tools to achieve a single goal is considered unique to human and primate technology. To unravel the origins of such complex behaviors, it is crucial to investigate tool use that does not occur species wide. These cases can be assumed to have emerged innovatively and be applied flexibly, thus emphasizing creativity and intelligence. However, it is intrinsically challenging to record tool innovations in natural settings that do not occur species-wide. Here we report the discovery of two distinct tool manufacture methods and the use of tool sets in wild Goffin's cockatoos (Cacatua goffiniana). Up to three types of wooden tools, differing in their physical properties and each serving a different function, were manufactured and employed to extract embedded seed matter of Cerbera manghas. While Goffin's cockatoos do not depend on tool-obtained resources, repeated observations of two temporarily kept wild birds and indications from free-ranging individuals suggest this behavior occurs in the wild, albeit not species-wide. The use of a tool set in a non-primate implies convergent evolution of advanced tool use. Furthermore, these observations demonstrate how a species without hands can achieve dexterity in a high-precision task. This finding of flexible use and manufacture of tool sets in animals distantly related to humans significantly diversifies the phylogenetic landscape of technology.","['### Additional used functions in ""Wild Goffinâ€™s cockatoos (Cacatua goffiniana) flexibly manufacture and use tool sets""\n### provided and written by Roger Mundry\n\n\n###fe.re.tab - function (Lines: 17 - 267)\n###diagnostics.plot - function (Lines: 273 - 305)\n###ranef.diagn.plot - function (Lines: 311 - 364)\n###glmm.model.stab - function (Lines: 370 - 568)\n###m.stab.plot - function (Lines: 574 - 608\n###boot.glmm.pred - function (Lines: 614 - 835)\n###overdisp.test - function (Lines: 841 - 940)\n###factor.int.plot - function (Lines: 946 - 1189)\n\n###########################################################################################################\n# fe.re.tab - function\n###########################################################################################################\nfe.re.tab<-function(fe.model, re, other.vars=NULL, data, treat.covs.as.factors=c(NA, F, T)){\n\tprint(""please read the documentation in the beginning of the script"")\n\t#function helping in determinining which random slopes are needed\n\t#last updated: 2018, June 6\n\t#latest updates:\n\t\t#major revision of how interactions are treated; reveals in the summary\n\t\t\t#for combinations factors whether the number of observations is >1 (seperately for each level of the random effect)\n\t\t\t#for combination of covariates whether the number of unique combinations per level of random effect is >2\n\t\t\t#for combinations of factors and covariates whether the number unique values per covariate is larger than 2 (when treated as covariate)\n\t\t\t\t#or larger than 1 (when treated as a factor), separately for each commbination of levels of fixed and random effects factors\n\t\t#treat.covs.as.factors can be NA in which case covariates aretreates as such and as factors\n\t\t\t#this is the default now\n\t\t#output gives information about which fixed effects are covariates and factors aso for interactions\n\t\t\t#and also how covariates were treated\n\t#input/arguments:\n\t\t#data: a dataframe with all relevant variables (including the response)\n\t\t#fe.model: character; the model wrt the fixed effect (including the response); e.g., ""r~f1*c*f2""\n\t\t#re: character; either a vector with the names of the random effects (e.g., c(""re1"", ""re2"")) or a random intercepts expression (e.g., ""(1|re1)+(1|re2)"")\n\t\t#other.vars: character, optional; a vector with the names of variables which are to be kept in the data considered and returned\n\t\t#treat.covs.as.factors: logical, when set to TRUE covariates will be treated like factors (see value/summary for details)\n\t#value: list with the following entries:\n\t\t#detailed: list with cross-tabulations ffor each combination of (main) fixed and random effect \n\t\t#summary: list tables...\n\t\t\t#telling for each combination of (main) fixed and random effect...\n\t\t\t\t#the number of levels of the random effect with a given number of unique values of the fixed effect (in case of a covariate)\n\t\t\t\t#the number of levels of the random effect with a given number of levels of the fixed effect for which at least two cases do exist (in case of a factor)\n\t\t\t#telling for each combination of interaction and random effect...\n\t\t\t\t#the combination of the above two informations, i.e., the number of individuals with a given number of unique values of the covariate\n\t\t\t\t\t#and a given number of factor levels for which at least two cases exist\n\t\t#data: data frame containing all relevant variables (i.e., response, fixed and random effects as well as those indicated in other.vars (e.g., offset terms)\n\t\t\t#also includes columns for dummy variables coding the levels (except the reference level) of all factors\n\t\t#pot.terms: length one vector comprising the model wrt the random slopes (for all combinations of fixed and random effects, i.e., most likely some will need to be omitted)\n\t\t\t#note that this comprises only the random slopes but not the correlations between random slopes and intercepts not the random intercept itself\n\t\t#pot.terms.with.corr: length one vector comprising the model wrt the random slopes (for all combinations of fixed and random effects, i.e., most likely some will need to be omitted)\n\t\t\t#note that this comprises the random slopes and intercepts  and also all correlations among them\n\ttreat.covs.as.factors=treat.covs.as.factors[1]\n\tif(sum(grepl(x=re, pattern="""", fixed=T))>0 & length(re)==1){#if random effects are handed over as formula\n\t\tre=gsub(x=re, pattern=""(1|"", replacement="""", fixed=T)\n\t\tre=gsub(x=re, pattern="")"", replacement="""", fixed=T)\n\t\tre=gsub(x=re, pattern="" "", replacement="""", fixed=T)\n\t\tre=unlist(strsplit(re, split=""+"", fixed=T))\n\t}\n\tfe.model=gsub(x=fe.model, pattern="" "", replacement="""", fixed=T)#remove spaces\n\tmodel.terms=attr(terms(as.formula(fe.model)), ""term.labels"")#get individual terms from fixed effects model\n\tfe.me=model.terms[!grepl(x=model.terms, pattern="":"", fixed=T)]#remove interactions\n\tfe.me=fe.me[!grepl(x=fe.me, pattern=""^"", fixed=T)]#remove squares terms\n\tresp=unlist(strsplit(fe.model, split=""~"", fixed=T))[1]#determine response\n\tif(substr(resp, start=1, stop=6)==""cb', '##################################################################################################\n##    Wild Goffin’s cockatoos flexibly manufacture and use tool sets\t\t##\n##\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t##\n## \tMark O’Hara & Berenika Mioduszewska, Roger Mundry, Yohanna, Tri Haryoko, Rini Rachmatika, \t##\n##\t\t\t\tDewi M. Prawiradilaga, Ludwig Huber, Alice M. I. Auersperg\t\t\t\t\t\t##\n##################################################################################################\n\n#############################################\n## loading required packages ################\n#############################################\nlibrary(dplyr) #for data transformations\nlibrary(plyr) #for data transformations\nlibrary(fpc) #package for cluster analysis\nlibrary(phylogram)#to convert the dendrogram to phylogram\n# BiocManager::install(ggtree) #installing ggtree package\nlibrary(ggtree) #importing the phylogram into ggplot environment \nlibrary(lme4)\nlibrary(lsmeans) #for factorlevel post hoc tests\nlibrary(MASS)\nlibrary(DHARMa) #model validation & overdispersion test\nlibrary(tidyr)\nlibrary(psych)\n\nlibrary(ggplot2) #for nice plotting\nlibrary(ggimage) #for adding image files in plots\n# BiocManager::install(""rgl"") #installing rgl package\nlibrary(rgl) #for 3d presentation and html export of 3d models\nlibrary(scico) #colorscale for sensitive to colorblindness\nlibrary(ggh4x) #for nested facetting\nlibrary(ggpubr) #for arrangements of plots\nlibrary(ggridges) #for arranging multiple density plots of the eventplot\nlibrary(magick) #for loading images into plots\nlibrary(ggforce) #for facetted zooming\nlibrary(grid) \nlibrary(ggrepel) #for non-overlapping text/labels in a graph\n\nsetwd(""~/Downloads/"") # set working directory to location of downloaded files\nsource(\'Functions.r\') # load additional custom functions for model plotting  writ\n############################################\n### Analysis\t############################\n############################################\n\n############################################\n############## Substrate (Wawai)\n############################################\nwawai <-read.table(file=""FruitMeasures.txt"", header=TRUE,na.strings=""na"", stringsAsFactors=T) # load data for individual fruit measures\nwawai$Length_fissure.mm.<-as.numeric(wawai$Length_fissure.mm.)\nstr(wawai)\nsummary(wawai)\n\nsumwawai=subset(wawai,ID!=""NA"") %>% summarise_if(is.numeric, funs(mean =mean(.,na.rm=T), sd=sd(.,na.rm=T), N=length(ID))) #sumarise means and sd for unused Fruit\n(round(sumwawai,1))\n########################\n# Binomial Glmm to test if fruit properties (weight, force to open/fissure width) predict tool use\nwawaiu<-subset(wawai,Individual!=""NA"" & Force_open.kg.!=""NA"") #subsetting all fruit that were provided and measures are available (that have not been lost); fruit with ID 4 had been given to both individuals/was dropped by both (removed it here) #tool use is coded as 0=no tool use/seed dropped after skinning, 1=tools were used\n\nwawaiu$Length_fissure.mm.<-as.numeric(wawaiu$Length_fissure.mm.)\nwawaiu$ID<-as.factor(wawaiu$Length_fissure.mm.)\nnames(wawaiu)\nvars=c(""Total_weight.g."",""Lenght_pericarp.mm."",""Width_pericarp.mm."",""Breadth_pericarp.mm."",""Weight_Exo..Mesocarp"",""Endocarp_weight"",""Length_endocarp"",""Width_endocarp"",""Breadth_endocarp"",""Width_fissure.max."",""Length_fissure.mm."",""Force_open.kg."",""total_Seed_weight.g."",""Thickness_endocarp.mm."",""Depth_fissure.max."")\nxx=cor(wawaiu[,vars])\nhead((abs(xx[,1])),n=15)\nhead(sort(abs(xx[1,]))) #total weight seems to correlate quite well with all measures except for width fissure and force to open\n\n#checking the predictors\nhist(wawaiu$Width_fissure.max.)\nhist((wawaiu$Force_open.kg.))\nhist(wawaiu$Total_weight.g.)\nplot(wawaiu$Total_weight.g.,wawaiu$Width_fissure.max.)\n\n#checking the response\nsum(wawaiu$Tool_use)\n\n#fitting the full model\ncont=glm.control(maxit=100)\nfull.width<-glm(Tool_use~ Individual+ Total_weight.g.+Width_fissure.max.+state, \n\tdata=wawaiu, family=binomial, control=cont) #a case of complete separation, to get realistic test statistics perform repeated simulations with each time one response of single observations to opposite outcome (see below)\n\n\ntable(wawaiu$Individual, wawaiu$Tool_use)\ntable(wawaiu$Total_weight.g., wawaiu$Tool_use)\ntable(wawaiu$Width_fissure.max., wawaiu$Tool_use) #this is where complete separation occurs no tool use if fissure is ≤ 2mm!!!\ntable(wawaiu$state, wawaiu$Tool_use)\nlength(coef(full.width))\nxx=table(wawaiu$Width_fissure.max., wawaiu$Tool_use)\nto.one=which(wawaiu$Width_fissure.max.<=2)\nto.null=which(wawaiu$Width_fissure.max.>=2.7)\n\n## dealing with complete separation by repeatedly setting response of single observations to opposite outcome\nn.sims=1000\nall.res=lapply(1:n.sims, function(x){\n\ti.resp=wawaiu$Tool_use\n\ti.resp[sample(to.one, 1)]=1\n\ti.resp[sample(to.null, 1)]=0\n\ti.full.width<-glm(i.resp~ Individual+ Total_weight.g.+Width_fissure.max.+state, data=wawaiu, family=binomial)\n\ti.null<-glm(i.resp~ 1+Individual,data=wawaiu,family=binomial)\n\tfn=as.data.frame(anova(i.null, i']",4,"wild Goffin's cockatoos, tool use, tool manufacture, tool sets, species-wide, innovation, creativity, intelligence, wooden tools, Cerbera manghas, non-primate, advanced tool use, dexterity, high-precision"
Tropicaltemperate comparisons in insect seed predation vary between study levels and years,"The biotic interaction hypothesis, which states the species interaction becomes stronger in the tropics, is deeply rooted in classic ecological literature and widely accepted to contribute to the latitudinal gradients of biodiversity. Tests in latitudinal insect-plant interaction have emphasized leaf-eating insects on a single or a few plant species rather than within an entire community and mixed accumulating evidence, leaving the biotic interaction hypothesis disputed. We aimed to test the hypothesis by quantifying seed predation by insects in a pair of tropical and temperate forest communities with similar elevations. We applied a consistent study design to sample pre-dispersal seeds with systematically set seed traps in 2019-2020 and examined internally feeding insects. The intensity of seed predation was measured and further applied to tropical versus temperate comparison at two levels (cross-species and community-wide). Our results showed every latitudinal pattern associated with different study levels and years, i.e., negative (greater granivory in the tropics in community-wide comparison in 2020), positive (less granivory in the tropics in community-wide and cross-species comparison in 2019), and missing (similar level of granivory in the tropics in cross-species comparisons in 2020). The cross-species level analyses ignore differences among species in seed production and weaken or even lose the latitudinal trend detected by community-wide comparisons. The between-year discrepancy in tropical-temperate comparisons relates to the highly variable annual seed composition in the temperate forest due to mast seeding of dominant species. Our study highlights that long-term community-level researches across biomes are essential to assess the latitudinal biotic interaction hypothesis.","['###################################################################################################################\n###### Title: Tropical-temperate comparisons in insect seed predation vary between study levels and years #########\n###### Author: Wenlan Wu et al.\n###### Updated Date: 14 August 2022\n###################################################################################################################\n\nrm(list = ls()) # remove all the data in the Global Environment\n\nsetwd(""~/Library/Mobile Documents/com~apple~CloudDocs/Rcode/Wenlan_granivory_Dryad"") # CHANGE to your working directory\n\n\n##########################################  1. Community-wide levels  analyses ####################################\n\nCommunity_wide <- read.csv(\'Seed_per_batch_2019_2020.csv\', header = T, row.names = 1) \n\nhead(Community_wide)\n# Seedmass_insect = The mass of seeds detected with insect predators, which related to Incidence_of_seed_predator\n# Seedmass_attack = The mass of seeds showing signs of insect attack, which related to Seed_predation_rate\n\n\nstr(Community_wide)\n\nCommunity_wide$Site <- as.factor(Community_wide$Site)\n\n##### 1.1 calculate Annual and Community-wide measurements (part of Table 1) #####\n\n##### 1.1.1 Incidence of seed predator in 2019  ####\n\n# CBS\n\nCommunity_wide2019 <- subset(Community_wide, Year==""2019"")\n\nCommunity_wide2019_CBS <- subset(Community_wide2019, Site==""CBS"")\n\n\nhead(Community_wide2019_CBS)\n\n\nsum(Community_wide2019_CBS$Seednumber.total) # 15614\n\nsum(Community_wide2019_CBS$Seedmass.total) # 1093.5g\n\n\nsum(Community_wide2019_CBS$Seedmass_insect)/sum(Community_wide2019_CBS$Seedmass.total) # 0.1792985\n\n\nmean(Community_wide2019_CBS$Incidence_of_seed_predator) # 0.1755801\n\nSD <- sd(Community_wide2019_CBS$Incidence_of_seed_predator) # 0.02841021\n\nSE <- SD/sqrt(7)  # 0.01073805\n\n\n\n# XSBN\n\nCommunity_wide2019_XSBN <- subset(Community_wide2019, Site==""XSBN"")\n\n\nsum(Community_wide2019_XSBN$Seednumber.total) # 15128\n\nsum(Community_wide2019_XSBN$Seedmass.total) # 3958.592g\n\n\nsum(Community_wide2019_XSBN$Seedmass_insect)/sum(Community_wide2019_XSBN$Seedmass.total) # 0.08401151\n\n\nmean(Community_wide2019_XSBN$Incidence_of_seed_predator) # 0.09729547\n\nSD <- sd(Community_wide2019_XSBN$Incidence_of_seed_predator) # 0.08767949\n\nSE <- SD/sqrt(26)  # 0.01719536\n\n\n\n\n##### 1.1.2 Incidence of seed predator in 2020  ####\n\n# CBS\n\nCommunity_wide2020 <- subset(Community_wide, Year==""2020"")\n\nCommunity_wide2020_CBS <- subset(Community_wide2020, Site==""CBS"")\n\n\nsum(Community_wide2020_CBS$Seednumber.total) # 148586\n\nsum(Community_wide2020_CBS$Seedmass.total) # 3262.18g\n\n\nsum(Community_wide2020_CBS$Seedmass_insect)/sum(Community_wide2020_CBS$Seedmass.total) # 0.02001744\n\n\nmean(Community_wide2020_CBS$Incidence_of_seed_predator) # 0.01843841\n\nSD <- sd(Community_wide2020_CBS$Incidence_of_seed_predator) # 0.01533273\n\nSE <- SD/sqrt(13)  # 0.004252534\n\n\n# XSBN\n\nCommunity_wide2020_XSBN <- subset(Community_wide2020, Site==""XSBN"")\n\n\nsum(Community_wide2020_XSBN$Seednumber.total) # 23589\n\nsum(Community_wide2020_XSBN$Seedmass.total) # 4346.749g\n\n\nsum(Community_wide2020_XSBN$Seedmass_insect)/sum(Community_wide2020_XSBN$Seedmass.total) # 0.06717901\n\n\nmean(Community_wide2020_XSBN$Incidence_of_seed_predator) # 0.06784181\n\nSD <- sd(Community_wide2020_XSBN$Incidence_of_seed_predator) # 0.04494625\n\nSE <- SD/sqrt(27)  # 0.008649911\n\n\n\n##### 1.1.3 Seed predation rates in 2020 ####\n\n# CBS\n\nsum(Community_wide2020_CBS$Seedmass_attack)/sum(Community_wide2020_CBS$Seedmass.total) # 0.05231729\n\n\nmean(Community_wide2020_CBS$Seed_predation_rate) # 0.05727119\n\nSD <- sd(Community_wide2020_CBS$Seed_predation_rate) # 0.03673546\n\nSE <- SD/sqrt(13)  # 0.01018858\n\n\n# XSBN\n\nsum(Community_wide2020_XSBN$Seedmass_attack)/sum(Community_wide2020_XSBN$Seedmass.total) # 0.247312\n\n\nmean(Community_wide2020_XSBN$Seed_predation_rate) # 0.2465907\n\nSD <- sd(Community_wide2020_XSBN$Seed_predation_rate) # 0.08440964\n\nSE <- SD/sqrt(27)  # 0.01624464\n\n\n####  *Simulated data of the Korean Pine Pinus koraiensis ####\n\n# change the seed predation rates of late September (2020/9/28) and early October (2020/10/14) batches in CBS in 2020\n\nCommunity_wide2020.pine <- Community_wide2020\n\nCommunity_wide2020.pine[Community_wide2020.pine$Site == \'CBS\' & Community_wide2020.pine$Date == \'2020/9/28\', ]$Seed_predation_rate <- 0.197\n\nCommunity_wide2020.pine[Community_wide2020.pine$Site == \'CBS\' & Community_wide2020.pine$Date == \'2020/10/14\', ]$Seed_predation_rate <- 0.197\n\n\nCommunity_wide2020_CBS_pine <- subset(Community_wide2020.pine, Site==""CBS"")\n\nmean(Community_wide2020_CBS_pine$Seed_predation_rate) # 0.08104514\n\nSD <- sd(Community_wide2020_CBS_pine$Seed_predation_rate) # 0.06253433\n\nSE <- SD/sqrt(13)  # 0.0173439\n\n\n\n#####  1.2 Generalized Least Squares (GLS) analyse  #####\n\n#### 1.2.1 Incidence of seed predator in 2019 ####\n\n# Visualization using the violin Plot (Figure 2a)\n\nlibrary(vioplot)\n\n# par(mfrow=c(1,2), mar=c(2,5,2,0), oma=c(1,0,1,2), bty=""l"") # mar and oma settings correspond to lower, ']",4,"Tropical, temperate, insect seed predation, biotic interaction hypothesis, latitudinal gradient, biodiversity, leaf-eating insects, community-wide, cross-species level, seed traps, granivory, internal feeding insects, study design,"
Data for Contrasting life-history responses to climate variability in eastern and western North Pacific sardine populations,"Massive populations of sardines inhabit both the western and eastern boundaries of the world's subtropical ocean basins, supporting both commercial fisheries and populations of marine predators. Sardine populations in western and eastern boundary current systems have responded oppositely to decadal scale anomalies in ocean temperature, but the mechanism for differing variability has remained unclear. Here, based on otolith microstructure and high-resolution stable isotope analyses, we show that habitat temperature, early life growth rates, energy expenditure, metabolically optimal temperature and, most importantly, the relationship between growth rate and temperature were remarkably different between the two subpopulations in the western and eastern North Pacific. Varying metabolic response to environmental changes partly explain the contrasting growth responses. Consistent differences in the life-history traits are observed between subpopulations in the western and eastern boundary current systems around South Africa. These growth and survival characteristics can facilitate the contrasting responses of sardine populations to climate change.","['library(ggplot2)\r\nrequire(dplyr)\r\n\r\nlibrary(lmerTest)\r\nlibrary(MuMIn)\r\nlibrary(emmeans)\r\n\r\n## Test1.  #############################\r\n## The difference of otolith increment widths \r\n## among JP, CA, SA south-east and SA west subpopulations.\r\n\r\nAll<-read.csv(\'IW_10-100_2.csv\',header = T)# Choose ""IW_10-100_2.csv""\r\nAll$Fish.ID<-as.factor(All$Fish_ID)#set Fish.ID as factors\r\nAll$Region<-as.factor(All$Region)#set Region as factors\r\nAll$Age<-as.factor(All$Age)#set Age as factors\r\n\r\nLMM1<-lmer(IW ~ Age*Region +(1|Fish.ID),All)\r\nems1=emmeans(LMM1, pairwise~Region|Age) #pairwise comparison \r\npair1=pairs(ems1)\r\nEF1=eff_size(ems1, sigma = sigma(LMM1), edf = Inf)\r\nwrite.csv(ems1,\'ems1.csv\')\r\nwrite.csv(pair1,\'pair1.csv\')\r\nwrite.csv(EF1,\'EF1.csv\')\r\n#Plot Supplementary Figure 6\r\npar(mfrow=c(2,2))\r\nqqnorm(resid(LMM1))\r\nypred = predict(LMM1)\r\nres = residuals(LMM1, type = \'deviance\')\r\nplot(ypred,res)\r\nhist(res)\r\n\r\n## Test1 Ends. #############################', '#### Test2 ##############################################\r\n## Fit quadratic relationship between Standard length and temperature\r\n## for JP sardine at 75 dph (end of early juvenile stage).\r\n\r\nlibrary(MASS)\r\n\r\nAll2<-read.csv(\'t-SL_moto.csv\',header = T,stringsAsFactors=T) # Choose ""t-SL_moto.csv""\r\n\r\nmodel1=lm(L4~Tint4+I(Tint4**2),data=All2) \r\nmodel2=lm(L4~Tint4,data=All2) \r\n\r\nAIC(model1,model2)\r\nmodel1.best=stepAIC(model1) # Model selection based on AIC\r\n\r\nsummary(model1) # Supplementary Table 10\r\nsink(""model1_summary.txt"")\r\nsummary(model1)\r\nsink()\r\n#### Test2 Ends. ##############################################\r\n', '### Calculate Moto from otolith d13C ######\r\n\r\nAll<-read.csv(\'Otolith_isotope_ratio\',header = T,stringsAsFactors=T)\r\n\r\nn=10000 #simulation numbers\r\nset.seed(2020) # random choose fit \r\n#####################Moto estimation for JP bu Monte Carlo Simulations ##################################\r\notoCjp<-All[6][(All[1]==""JP""),]#otolith values setting for jp pop\r\ndCjp<-runif(n, min = -22, max = -19)#dietary values setting for jp pop\r\nwCjp<-runif(n, min = 0.53, max = 1.05)#water values setting for jp pop \r\npredMjp<-matrix(NA, ncol=n, nrow=length(otoCjp))\r\nmeanjp=NA\r\nsdjp=NA\r\n\r\nfor(i in 1:length(otoCjp)){\r\n  for (j in 1:n) {\r\n    predMjp[i,j]<-(otoCjp[i]-wCjp[j])/(dCjp[j]-wCjp[j])\r\n  }\r\n  meanjp[i]<-mean(predMjp[i,], na.rm=TRUE)\r\n  sdjp[i]<-sd(predMjp[i,], na.rm=TRUE)\r\n}\r\n\r\n#####################Moto estimation for CA by Monte Carlo Simulations ##################################\r\notoCca<-All[6][(All[1]==""CA""),]#otolith values setting for ca pop\r\ndCca<-runif(n, min = -21.5, max = -18.5)#dietary values setting for ca pop\r\nwCca<-runif(n, min = -0.31, max = 2.2)#water values setting for ca pop\r\npredMca<-matrix(NA, ncol=n, nrow=length(otoCjp))\r\nmeanca=NA\r\nsdca=NA\r\n\r\nfor(i in 1:length(otoCca)){\r\n  for (j in 1:n) {\r\n    predMca[i,j]<-(otoCca[i]-wCca[j])/(dCca[j]-wCca[j])\r\n  }\r\n  meanca[i]<-mean(predMca[i,], na.rm=TRUE)\r\n  sdca[i]<-sd(predMca[i,], na.rm=TRUE)\r\n}\r\n\r\n#####################Combining Data for GAM##################################\r\n\r\nAll$Moto.mean<-c(meanjp,meanca)\r\nAll$Moto.sd<-c(sdjp,sdca)\r\nwrite.csv(All,\'Moto.csv\')', 'library(ggplot2)\r\nrequire(dplyr)\r\n\r\nlibrary(lmerTest)\r\nlibrary(MuMIn)\r\nlibrary(emmeans)\r\n\r\n## Test2.  #############################\r\n## The difference of Moto  \r\n## between JP and CA subpopulations.\r\n\r\n\r\nAll<-read.csv(\'Moto_re_stage_3_R3.csv\',header = T) # Choose ""Moto_re_stage_3_R3.csv""\r\nAll$Fish.ID<-as.factor(All$Fish.ID)#set Fish.ID as factors\r\nAll$Region<-as.factor(All$Region)#set Region as factors\r\nAll$Age.range2<-All$Age.range # Adjust for the different definitions for age range between JP and CA\r\nAll$Age.range2[(All$Region==\'JP\')&(All$Age.range==\'C\')]=\'B\'\r\nAll$Age.range2[(All$Region==\'JP\')&(All$Age.range==\'D\')]=\'C\'\r\nAll$Age.range2[(All$Region==\'JP\')&(All$Age.range==\'E\')]=\'C\'\r\nAll$Age.range2[(All$Region==\'JP\')&(All$Age.range==\'F\')]=\'D\'\r\nAll$Age.range2[(All$Region==\'JP\')&(All$Age.range==\'G\')]=\'D\'\r\n\r\nAll$Age<-as.factor(All$Age.range2)#set Age.range2 as factors\r\nAll=subset(All,All$Age.range2!=\'E\')#Remove 121-150 dph data of CA as there is no data for JP\r\n\r\nhist(All$Moto.mean)\r\n# Remove outliers prior to the test\r\nboxplot(All$Moto.mean, plot=FALSE)$out\r\noutliers <- boxplot(All$Moto.mean, plot=FALSE)$out\r\nAllM<- All[-which(All$Moto.mean %in% outliers),]\r\nhist(AllM$Moto.mean)\r\n\r\nLMM2<-lmer(Moto.mean ~ Age*Region +(1|Fish.ID), AllM)\r\nsummary(LMM2)\r\n\r\n\r\nems2=emmeans(LMM2, pairwise~Region|Age)#pairwise comparison \r\npair2=pairs(ems2)\r\nEF2=eff_size(ems2, sigma = sigma(LMM2), edf = Inf)\r\nwrite.csv(ems2,\'ems2.csv\')\r\nwrite.csv(pair2,\'pair2.csv\')\r\nwrite.csv(EF2,\'EF2.csv\')\r\n#evaluate the normality of residuals (Supplementary Figure 7)\r\npar(mfrow=c(2,2))\r\nqqnorm(resid(LMM2))\r\nypred = predict(LMM2)\r\nres = residuals(LMM2, type = \'deviance\')\r\nplot(ypred,res)\r\nhist(res)\r\n\r\n\r\n## Test2 Ends.  #############################\r\n\r\n\r\n## Test3.  #############################\r\n## The difference of experienced water temperature  \r\n## between JP and CA subpopulations.\r\n\r\nhist(All$Estimated.Temperature)\r\n# Remove outliers prior to the test\r\nboxplot(All$Estimated.Temperature, plot=FALSE)$out\r\noutliers <- boxplot(All$Estimated.Temperature, plot=FALSE)$out\r\nAllT<- All[-which(All$Estimated.Temperature %in% outliers),]\r\nhist(AllT$Estimated.Temperature)\r\n\r\nLMM3<-lmer(Estimated.Temperature ~ Age*Region +(1|Fish.ID), AllT)\r\nsummary(LMM3)\r\n\r\nems3=emmeans(LMM3, pairwise~Region|Age)#pairwise comparison \r\npair3=pairs(ems3)\r\nEF3=eff_size(ems3, sigma = sigma(LMM3), edf = Inf)\r\nwrite.csv(ems3,\'ems3.csv\')\r\nwrite.csv(pair3,\'pair3.csv\')\r\nwrite.csv(EF3,\'EF3.csv\')\r\n#evaluate the normality of residuals (Supplementary Figure 8)\r\npar(mfrow=c(2,2))\r\nqqnorm(resid(LMM3))\r\nypred = predict(LMM3)\r\nres = residuals(LMM3, type = \'deviance\')\r\nplot(ypred,res)\r\nhist(res)\r\n\r\n\r\n## Test3 Ends.  #############################', 'library(ggplot2)\r\nrequire(dplyr)\r\nlibrary(quantreg)\r\n\r\n### Optimal temperature estimation ###############################\r\n### Model upper and lower limits of Moto distribution and find the\r\n### temperature that maximise the gap between upper and lower limits.\r\n\r\nAll<-read.csv(""Moto_re_stage_3_R3.csv"",header = T)\r\n\r\nn_th=4 # use temperature bins that include >=4 individuals\r\n\r\n# Calculate average Moto and temperature for each life stages \r\n# (larva, early juvenile and later juvenile). \r\n# Data with missing data within each life stage were excluded\r\n# Outliers were detected and also removed.\r\n\r\n# Because Age.range ""A"" corresponds 30 days interval and ""B-G"" to 15 days \r\n# intervals for JP sardine, Age.range ""A"" of JP sardine was duplicated \r\n# before averaging for weighting.\r\nJS1=subset(All, All$Age.range==""A"" & All$Region.ID==""JP"") \r\nAll=rbind(All,JS1) # Duplicate Age.range A of JP\r\nJS1=subset(All, All$Stage==""S1"" & All$Region.ID==""JP"") \r\nn = JS1 %>% count(Fish.ID)\r\nn2 = n$Fish.ID[n$n==3] \r\nJS1=filter(JS1, JS1$Fish.ID %in% n2) # remove individuals with missing data\r\nJS1=aggregate(JS1[,10:11], list(JS1$Fish.ID), mean)\r\nJS1$Stage=\'S1\'\r\nJS1$Region.ID=\'JP\'\r\nJS1$TG=round(JS1$Estimated.Temperature) # Temperature bins \r\n\r\nboxplot(JS1$Moto.mean, plot=FALSE)$out # Detect Moto outliers\r\noutliers <- boxplot(JS1$Moto.mean, plot=FALSE)$out\r\nJS1<- JS1[-which(JS1$Moto.mean %in% outliers),] # remove outliers\r\nn = JS1 %>% count(TG)\r\nn2 = n$TG[n$n>=n_th] # remove temperature bins that includes < 4 data\r\nJS1=filter(JS1, JS1$TG %in% n2)\r\n\r\n\r\nJS2=subset(All, All$Stage==""S2"" & All$Region.ID==""JP"")\r\nn = JS2 %>% count(Fish.ID)\r\nn2 = n$Fish.ID[n$n==2]\r\nJS2=filter(JS2, JS2$Fish.ID %in% n2)\r\nJS2=aggregate(JS2[,10:11], list(JS2$Fish.ID), mean)\r\nJS2$Stage=\'S2\'\r\nJS2$Region.ID=\'JP\'\r\n\r\nboxplot(JS2$Moto.mean, plot=FALSE)$out\r\noutliers <- boxplot(JS2$Moto.mean, plot=FALSE)$out\r\nJS2<- JS2[-which(JS2$Moto.mean %in% outliers),]\r\nJS2$TG=round(JS2$Estimated.Temperature)\r\nn = JS2 %>% count(TG)\r\nn2 = n$TG[n$n>=n_th]\r\nJS2=filter(JS2, JS2$TG %in% n2)\r\n\r\n\r\nJS3=subset(All, All$Stage==""S3"" & All$Region.ID==""JP"")\r\nn = JS3 %>% count(Fish.ID)\r\nn2 = n$Fish.ID[n$n==2]\r\nJS3=filter(JS3, JS3$Fish.ID %in% n2)\r\nJS3=aggregate(JS3[,10:11], list(JS3$Fish.ID), mean)\r\nJS3$Stage=\'S3\'\r\nJS3$Region.ID=\'JP\'\r\n\r\nboxplot(JS3$Moto.mean, plot=FALSE)$out\r\noutliers <- boxplot(JS3$Moto.mean, plot=FALSE)$out\r\nJS3<- JS3[-which(JS3$Moto.mean %in% outliers),]\r\nJS3$TG=round(JS3$Estimated.Temperature)\r\nn = JS3 %>% count(TG)\r\nn2 = n$TG[n$n>=n_th]\r\nJS3=filter(JS3, JS3$TG %in% n2)\r\n\r\n\r\n\r\nCS1=subset(All, All$Stage==""S1"" & All$Region.ID==""CA"")\r\nn = CS1 %>% count(Fish.ID)\r\nn2 = n$Fish.ID[n$n==2]\r\nCS1=filter(CS1, CS1$Fish.ID %in% n2)\r\nCS1=aggregate(CS1[,10:11], list(CS1$Fish.ID), mean)\r\nCS1$Stage=\'S1\'\r\nCS1$Region.ID=\'CA\'\r\n\r\nboxplot(CS1$Moto.mean, plot=FALSE)$out\r\n#outliers <- boxplot(CS1$Moto.mean, plot=FALSE)$out # No outliers detected\r\n#CS1<- CS1[-which(CS1$Moto.mean %in% outliers),]\r\nCS1$TG=round(CS1$Estimated.Temperature)\r\nn = CS1 %>% count(TG)\r\nn2 = n$TG[n$n>=n_th]\r\nCS1=filter(CS1, CS1$TG %in% n2)\r\n\r\n\r\n\r\nCS2=subset(All, All$Stage==""S2"" & All$Region.ID==""CA"")\r\nCS2=aggregate(CS2[,10:11], list(CS2$Fish.ID), mean)\r\nCS2$Stage=\'S2\'\r\nCS2$Region.ID=\'CA\'\r\n\r\nboxplot(CS2$Moto.mean, plot=FALSE)$out\r\n#outliers <- boxplot(CS2$Moto.mean, plot=FALSE)$out # No outliers detected\r\n#CS2<- CS2[-which(CS2$Moto.mean %in% outliers),]\r\nCS2$TG=round(CS2$Estimated.Temperature)\r\nn = CS2 %>% count(TG)\r\nn2 = n$TG[n$n>=n_th]\r\nCS2=filter(CS2, CS2$TG %in% n2)\r\n\r\n\r\n\r\nCS3=subset(All, All$Stage==""S3"" & All$Region.ID==""CA"")\r\nCS3=aggregate(CS3[,10:11], list(CS3$Fish.ID), mean)\r\nCS3$Stage=\'S3\'\r\nCS3$Region.ID=\'CA\'\r\nboxplot(CS3$Moto.mean, plot=FALSE)$out\r\noutliers <- boxplot(CS3$Moto.mean, plot=FALSE)$out\r\nCS3<- CS3[-which(CS3$Moto.mean %in% outliers),]\r\nCS3$TG=round(CS3$Estimated.Temperature)\r\nn = CS3 %>% count(TG)\r\nn2 = n$TG[n$n>=n_th]\r\nCS3=filter(CS3, CS3$TG %in% n2)\r\n\r\nAll2=rbind(JS1,JS2,JS3,CS1,CS2,CS3) # Restore filtered data \r\n\r\n# Create boxplot for overall view \r\n\r\nggplot(All2, aes(x=Estimated.Temperature, y=Moto.mean))+\r\n  geom_point()+\r\n  stat_smooth(method=""lm"")+\r\n  facet_grid(Stage~Region.ID)+\r\n  theme_bw( )+  \r\n  theme(axis.title = element_text(size = rel(1.5)),axis.text=element_text(size=rel(1.3),face=""bold"", colour=""black""),\r\n        strip.text=element_text(size = rel(1.3)),legend.text =element_text(size = rel(1.0),face=""bold""),\r\n        legend.title =element_text(size = rel(1),face=""bold""))+\r\n  xlab(""Temperature"")+\r\n  ylab(""Moto"")\r\n\r\nggplot(data=All2,aes(x=factor(TG),y= Moto.mean))+\r\n  geom_boxplot(position=position_dodge(1))+\r\n  geom_jitter(size=0.4, alpha=0.9)+\r\n  facet_grid(Stage~Region.ID)+\r\n  theme_bw( )+  \r\n  theme(axis.title = element_text(size = rel(1.5)),axis.text=element_text(size=rel(1.0),face=""bold"", colour=""black""),\r\n        strip.text=element_text(size = rel(1.3)),legend.text =element_text(size = rel(1.0),face=""bold""),\r\n        legend.title =element_text(size = rel(1),face=""bold""))+\r\n  xlab(""Tempera', 'library(ggplot2)\r\nrequire(dplyr)\r\n\r\n## Exclude Moto outliers for statistical tests.\r\n\r\nAll<-read.csv(""Moto_re_stage_3_R3.csv"",header = T)\r\n\r\n# Calculate average Moto and temperature for each life stages \r\n# (larva, early juvenile and later juvenile). \r\n# Data with missing data within each life stage were excluded\r\n\r\n# Because Age.range ""A"" corresponds 30 days interval and ""B-G"" to 15 days \r\n# intervals for JP sardine, Age.range ""A"" of JP sardine was duplicated \r\n# before averaging for weighting.\r\n\r\nJS1=subset(All, All$Age.range==""A"" & All$Region.ID==""JP"")\r\nAll=rbind(All,JS1)\r\nJS1=subset(All, All$Stage==""S1"" & All$Region.ID==""JP"")\r\nn = JS1 %>% count(Fish.ID)\r\nn2 = n$Fish.ID[n$n==3]\r\nJS1=filter(JS1, JS1$Fish.ID %in% n2)\r\nJS1=aggregate(JS1[,8:11], list(JS1$Fish.ID), mean)\r\nJS1$Stage=\'S1\'\r\nJS1$Region.ID=\'JP\'\r\n\r\n\r\nJS2=subset(All, All$Stage==""S2"" & All$Region.ID==""JP"")\r\nn = JS2 %>% count(Fish.ID)\r\nn2 = n$Fish.ID[n$n==2]\r\nJS2=filter(JS2, JS2$Fish.ID %in% n2)\r\nJS2=aggregate(JS2[,8:11], list(JS2$Fish.ID), mean)\r\nJS2$Stage=\'S2\'\r\nJS2$Region.ID=\'JP\'\r\n\r\n\r\nJS3=subset(All, All$Stage==""S3"" & All$Region.ID==""JP"")\r\nn = JS3 %>% count(Fish.ID)\r\nn2 = n$Fish.ID[n$n==2]\r\nJS3=filter(JS3, JS3$Fish.ID %in% n2)\r\nJS3=aggregate(JS3[,8:11], list(JS3$Fish.ID), mean)\r\nJS3$Stage=\'S3\'\r\nJS3$Region.ID=\'JP\'\r\n\r\n\r\nCS1=subset(All, All$Stage==""S1"" & All$Region.ID==""CA"")\r\nn = CS1 %>% count(Fish.ID)\r\nn2 = n$Fish.ID[n$n==2]\r\nCS1=filter(CS1, CS1$Fish.ID %in% n2)\r\nCS1=aggregate(CS1[,8:11], list(CS1$Fish.ID), mean)\r\nCS1$Stage=\'S1\'\r\nCS1$Region.ID=\'CA\'\r\n\r\n\r\nCS2=subset(All, All$Stage==""S2"" & All$Region.ID==""CA"")\r\nCS2=aggregate(CS2[,8:11], list(CS2$Fish.ID), mean)\r\nCS2$Stage=\'S2\'\r\nCS2$Region.ID=\'CA\'\r\n\r\n\r\nCS3=subset(All, All$Stage==""S3"" & All$Region.ID==""CA"")\r\nCS3=aggregate(CS3[,8:11], list(CS3$Fish.ID), mean)\r\nCS3$Stage=\'S3\'\r\nCS3$Region.ID=\'CA\'\r\n\r\n\r\nAll2=rbind(JS1,JS2,JS3,CS1,CS2,CS3) # Restore data\r\nAll2$TG=round(All2$Estimated.Temperature)\r\n\r\n# write.csv(All2,file=\'Moto_T_bin_re_R3.csv\') \r\n\r\n# Outliers were detected and removed for Moto analyses.\r\n\r\nboxplot(JS1$Moto.mean, plot=FALSE)$out\r\noutliers <- boxplot(JS1$Moto.mean, plot=FALSE)$out\r\nJS1<- JS1[-which(JS1$Moto.mean %in% outliers),]\r\n\r\nboxplot(JS2$Moto.mean, plot=FALSE)$out\r\noutliers <- boxplot(JS2$Moto.mean, plot=FALSE)$out\r\nJS2<- JS2[-which(JS2$Moto.mean %in% outliers),]\r\n\r\nboxplot(JS3$Moto.mean, plot=FALSE)$out\r\noutliers <- boxplot(JS3$Moto.mean, plot=FALSE)$out\r\nJS3<- JS3[-which(JS3$Moto.mean %in% outliers),]\r\n\r\nboxplot(CS1$Moto.mean, plot=FALSE)$out\r\n#outliers <- boxplot(CS1$Moto.mean, plot=FALSE)$out\r\n#CS1<- CS1[-which(CS1$Moto.mean %in% outliers),]\r\n\r\n\r\nboxplot(CS2$Moto.mean, plot=FALSE)$out\r\n#outliers <- boxplot(CS2$Moto.mean, plot=FALSE)$out\r\n#CS2<- CS2[-which(CS2$Moto.mean %in% outliers),]\r\n\r\nboxplot(CS3$Moto.mean, plot=FALSE)$out\r\noutliers <- boxplot(CS3$Moto.mean, plot=FALSE)$out\r\nCS3<- CS3[-which(CS3$Moto.mean %in% outliers),]\r\n\r\n\r\nAll3=rbind(JS1,JS2,JS3,CS1,CS2,CS3) # Restore data\r\nAll3$TG=round(All3$Estimated.Temperature)\r\n\r\nwrite.csv(All3,file=\'Moto_T_bin_re_outed_R3.csv\')', '### Test1 #######################\r\n## Test the difference in Moto between JP and CA subpopulations for\r\n## the larval (S1), early juvenile (S2) and late juvenile (S3) stages\r\n## accounting for the effect of temperature.\r\n\r\nAll<-read.csv(\'Moto_T_bin_re_outed_R3.csv\',header = T)\r\n\r\nhist(All$Moto.mean)#check distribution for determining family function in GLM\r\nshapiro.test(All$Moto.mean)#check distribution\r\n# no significant from normal distribution so using Gaussian family \r\n\r\n\r\nglm1<-glm(Moto.mean~Estimated.Temperature*(Region.ID), data=All,family=gaussian(link = ""log""))\r\n#considering reviewer\'s opinion, we use ""log"" to see if it is better to describe the trend\r\nglm2<-glm(Moto.mean~Estimated.Temperature*(Region.ID), data=All,family=gaussian(link = ""identity""))\r\nglm3<-glm(Moto.mean~Estimated.Temperature+(Region.ID), data=All,family=gaussian(link = ""identity""))\r\nglm4<-glm(Moto.mean~Estimated.Temperature*(Region.ID)*Stage, data=All,family=gaussian(link = ""identity""))\r\nglm5<-glm(Moto.mean~Estimated.Temperature*(Region.ID)+Stage, data=All,family=gaussian(link = ""identity""))\r\n\r\nAIC(glm1, glm2, glm3, glm4, glm5)# model selection glm4 is the best (Supplementary Table 7)\r\nsummary(glm4) #Supplementary Table 8\r\nsink(""glm4_summary.txt"")\r\nsummary(glm4)\r\nsink()\r\npar(mfrow=c(2,2))# model diagnostic plots (Supplementary Figure 9)\r\nplot(glm4) # all look good\r\n\r\n### Test1 Ends. ##############################################']",4,"sardines, populations, climate variability, eastern North Pacific, western North Pacific, commercial fisheries, marine predators, ocean temperature, otolith microstructure, stable isotope analyses, habitat temperature, early life growth rates, energy expenditure, metabolically optimal"
Branch-scale ember generation study at Oregon State University,"Videos and other supporting data for project used to identify the time to generation of dowels or natural samples when burned in a vertical wind tunnel. The overall goal was to identify which physical processes are most important in controlling ignition. The data was used for the publication, Effects of fuel characteristics on ember generation characteristics at branch-scales, International Journal of Wildland fire (in press).","['# Screening Study Factorial Analysis\r\n# Tyler Hudson\r\n# 11/15/17\r\n\r\nstudydata <- read.csv(file.choose())\r\nDiameter <- factor(studydata$Diameter)\r\nHeat.intensity <- factor(studydata$Heat.Intensity)\r\nCrossflow <- factor(studydata$Crossflow)\r\nSpecies <- factor(studydata$Species)\r\nMoisture.content <- factor(studydata$Moisture.content)\r\nCondition <- factor(studydata$Condition)\r\n\r\n# use this one for natural vs dowel analysis\r\nanova <- aov(Average.Frames ~ Diameter * Heat.intensity * Crossflow * Condition * Moisture.content, data = studydata)\r\n\r\n# use this one for species analysis\r\nanova <- aov(Average.Frames ~ Diameter * Heat.intensity * Crossflow * Moisture.content * Species, data = studydata)\r\n\r\nsummarytable <- summary(anova)\r\nmodel.tables(anova,""means"")\r\nresults <- TukeyHSD(anova,ordered=TRUE)\r\n\r\n\r\n#Save data to txt file, make sure you rename\r\ncapture.output(summarytable,file=""3reps.txt"")\r\n\r\n#Save data to txt file, make sure you rename\r\ncapture.output(summarytable,file=""NvsD.txt"")\r\n\r\n# species only results\r\n\r\n\r\ninteraction.plot(Diameter,Moisture.content,studydata$Average.Frames, ylab = ""Average Frames"")\r\n#gp = ggplot(data=studydata, aes(x=Species,y=Average.Frames,colour=Diameter, group=Diameter))\r\n#gp + geom_line(size=.6) + \r\n     geom_point(size=3) \r\n  \r\npng(""file.png"",1000,1000)   \r\nplotmeans(studydata$Average.Frames[Diameter==6]~Species[Diameter==6], ylim=c(100,1500),ylab=""Average Frames"")\r\nplotmeans(studydata$Average.Frames[Diameter==2]~Species[Diameter==2],add=TRUE)\r\ndev.off()\r\n\r\ndf<-with(studydata, aggregate(studydata$Average.Frame, list(Species=Species, Diameter=Diameter), mean))\r\ndf$se<-with(studydata, aggregate(studydata$Average.Frame, list(Species=Species, Diameter=Diameter), function(x) sd(x)/sqrt(10)))[,3]\r\ngp <- ggplot(df, aes(x=Species, y=x, colour=Diameter, group=Diameter))\r\ngp + geom_line(aes(linetype=factor(Diameter))) + \r\n     geom_point(aes(shape=factor(Diameter))) + \r\n     geom_errorbar(aes(ymax=x+se, ymin=x-se))\r\n']",4,"ember, generation, study, Oregon State University, videos, supporting data, project, identification, time, dowels, natural samples, burned, vertical wind tunnel, physical processes, controlling ignition, publication, fuel characteristics, branch-scales, International Journal"
Soil network analysis functions,"These functions are used to calculate metrics from network science and transport geography on two-dimensional soil macropore networks, such as those extracted using image morphology or X-ray techniques.","['#####################################################################################\r\n# calculate_network_metrics.R                                                       #\r\n#   Parses soil networks stored as text files and calculates network metrics for    #\r\n#     each main network (all nodes and links) and each disconnected subnetwork.     #\r\n#####################################################################################\r\nsource(""network_metrics_functions.R"")\r\n\r\n#################################################################\r\n# PARSING FUNCTIONS                                             #\r\n#################################################################\r\n# Main function for parsing network files, calculating metrics for main net and subnets\r\nparse_network_file <- function(net_file) {\r\n  # ID of photo\r\n  soil_net_id <- strsplit(net_file, split = ""_"")[[1]][2]\r\n  \r\n  # Read in data file - first determine how many columns each row could have\r\n  max_cols <- max(count.fields(net_file, sep = "" ""))\r\n  # Read in max columns for each row - delete the other ones later\r\n  network_data <- read.table(net_file, sep = "" "", col.names = paste0(""V"", seq_len(max_cols)), \r\n                             fill = TRUE)\r\n  # Metadata rows have number of nodes and number of links - these split up the data file\r\n  metadata_rows <- which(is.na(network_data$V4))\r\n  # First section of the data file is network membership\r\n  network_members <- as.data.frame(network_data[metadata_rows[1]:metadata_rows[2], ])\r\n  # Second section of the data file is the lengths of the links between nodes - node ID is from\r\n  #   order of the network membership list\r\n  link_lengths <- as.data.frame(network_data[metadata_rows[2]:length(network_data$V1), ])\r\n  \r\n  # Clean up network membership list - keep only real columns\r\n  network_members <- network_members[ , c(""V2"", ""V4"", ""V6"")]\r\n  # Rename columns - assumes structure of data to be network ID, x coordinate, y coordinate\r\n  names(network_members) <- c(""network_id"", ""x"", ""y"")\r\n  # Delete metadata row\r\n  network_members <- network_members[which(! is.na(network_members$x)), ]\r\n  # Add node ID\r\n  network_members$node_id <- seq_along(network_members$x)\r\n  \r\n  # Clean up link lengths list- keep only real columns\r\n  link_lengths <- link_lengths[ , c(""V2"", ""V4"", ""V6"")]\r\n  # Rename columns - assumes structure of data to be node_from ID, node_to ID, link length\r\n  names(link_lengths) <- c(""node_from"", ""node_to"", ""length"")\r\n  # Delete metadata row - will only have one column\r\n  link_lengths <- link_lengths[which(! is.na(link_lengths$node_to)), ]\r\n  # Remove duplicates, e.g link from 1,2 is the same as 2,1\r\n  link_lengths <- link_lengths[!duplicated(t(apply(link_lengths, 1, sort))), ]\r\n  \r\n  # Create igraph network from soil network structure data\r\n  net <- graph_from_data_frame(link_lengths, directed = FALSE)\r\n  # Set link lengths as edge weights so weights don\'t have to be specified as \'length\' in later algorithms\r\n  E(net)$weight <- E(net)$length \r\n  \r\n  # Calculate metrics for each subnet\r\n  subnets_data <- do.call(rbind, lapply(unique(network_members$network_id), parse_subnet, \r\n                                              network_members, link_lengths, soil_net_id))\r\n\r\n  # Calculate exponent and fit of power law of link length distribution\r\n  pl_linklength <- powerlaw_linklength_distribution(link_lengths)\r\n  \r\n  # Calculate exponent and fit of power law of spatial entropy\r\n  pl_sp_entropy <- get_spatial_entropy(net)\r\n  \r\n  # Calculate convex hull area and density (uses hull area)\r\n  chull_area <- get_chull_area(network_members)\r\n  network_density <- vcount(net)/chull_area\r\n  \r\n  # Run metrics and output CSV data for main network\r\n  net_data <- data.frame(\r\n    subnet_id = NA,\r\n    net_id = soil_net_id,\r\n    sw = is_smallworld(net),\r\n    n_nodes = vcount(net),\r\n    n_links = ecount(net),\r\n    avg_degree = get_avg_node_degree(net),\r\n    gamma_index = get_gamma_index(net),\r\n    beta_index = get_beta_index(net),\r\n    diameter = get_network_diameter(net),\r\n    cost = get_network_cost(net),\r\n    grc = global_reach_centrality(net),\r\n    chull_area = chull_area,\r\n    network_density = network_density,\r\n    linklength_powerlaw_exp = pl_linklength$power_law_exponent,\r\n    linklength_powerlaw_xmin = pl_linklength$xmin,\r\n    linklength_powerlaw_ks_stat = pl_linklength$ks_stat,\r\n    linklength_powerlaw_ks_p = pl_linklength$ks_p,\r\n    sp_entropy_powerlaw_exp = pl_sp_entropy$power_law_exponent,\r\n    sp_entropy_powerlaw_xmin = pl_sp_entropy$xmin,\r\n    sp_entropy_powerlaw_ks_stat = pl_sp_entropy$ks_stat,\r\n    sp_entropy_powerlaw_ks_p = pl_sp_entropy$ks_p\r\n  )\r\n  \r\n  # Combine main network metrics and subnet metrics to return\r\n  all_net_data <- rbind(net_data, subnets_data)\r\n  \r\n  # Update progress bar\r\n  i <<- i + 1\r\n  setTxtProgressBar(pb, i)\r\n  \r\n  return(all_net_data)\r\n  \r\n}\r\n\r\n# Function called by parse_network_file to parse each of the subnets that comprise the main network\r\nparse_subnet <- function(subnet_id, n']",4,"Soil network analysis, metrics, network science, transport geography, soil macropore networks, image morphology, X-ray techniques."
TESS Network Motivation Survey,"The TESS Network motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting light pollution: the network of around 120 hosts of the TESS photometers. Volunteers of this network accepted to host and install sensors to monitor sky brightness in order to collect data for measuring the level of light pollution in many areas of the Earth.The volunteers are very diverse: professional astronomers, amateur astronomers, light pollution fighters, astronomical outreach (museum, planetarium, dark sky association, etc.), astro-tourism actors, public administrations and others.The TESS Network Survey motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation A poster and a paper describing the study are additional resources referenced by the research object.","['setwd(""tess-motivation-survey"")\r\n\r\n#----------- Preparation \r\n\r\n# Read data extracted from Coney\r\nraw.data <- read.csv(""tess-network-results.csv"", sep ="";"")\r\nraw.data <- cbind(raw.data, tag.question = paste0(raw.data$tag,""."",raw.data$question_id))\r\n# Remove test users\r\nraw.data.filtered<-raw.data[!raw.data$user %in% c(""testCefriel"", ""testCefriel2""),]\r\n\r\n# Select unfinished survey completions\r\nraw.data.unfinished<-raw.data.filtered[raw.data.filtered$totalDuration == \'unfinished\', ]\r\n# Remove unfinished survey completions\r\nraw.data.finished<-raw.data.filtered[!raw.data.filtered$totalDuration == \'unfinished\', ]\r\nrm(raw.data.filtered)\r\n\r\n\r\n#----------- Analyse finished survey completions\r\n\r\n# Select answers to closed questions with values between 1 and 5\r\n\r\n# Find users completing the survey multiple times, keep only the most recent survey completion\r\nlibrary(dplyr)\r\nres <- raw.data.finished %>% group_by(user,session) %>% summarise(Freq=n())\r\nres$duplicated.user <- duplicated(res$user)\r\nduplicated.user<- res[res$duplicated.user == TRUE, ""user""]\r\n\r\nfor(i in duplicated.user$user){\r\n  b<-raw.data.finished[raw.data.finished$user == i, ]\r\n  b.first.question<- b[b$question_id == unique(b$question_id[b$question_id == min(b$question_id)]),]\r\n  b.first.question$date.time <- as.POSIXlt(paste(b.first.question$date, b.first.question$time), format=""%d-%m-%Y %H:%M:%S"")\r\n  # Extract the session\r\n  session.to.keep<-as.character(b.first.question$session[b.first.question$date.time == min(b.first.question$date.time)])\r\n  session.to.delete<-as.character(b.first.question$session[!b.first.question$session == session.to.keep])\r\n  # Remove the duplicated survey completion\r\n  raw.data.finished<-raw.data.finished[! (raw.data.finished$user == i & raw.data.finished$session %in% session.to.delete), ]\r\n}\r\n\r\nwrite.csv(raw.data.finished, file =  ""completed-survey-all-answers.csv"", row.names = F)\r\n\r\n# Select answers to closed questions with values between 1 and 5\r\nraw.data.finished <- raw.data.finished[raw.data.finished$minValue == \'1\' & raw.data.finished$maxValue == \'5\' ,  ]\r\n\r\nrm(i, session.to.delete, session.to.keep, res, b, b.first.question, duplicated.user, raw.data)\r\n\r\n# Pivoting table: one row for each user and one column for each question, as value the numerical value given as answer\r\nlibrary(reshape)\r\npivot.for.question<-cast(raw.data.finished, user ~ tag.question, fun.aggregate = mean)\r\nnames(pivot.for.question) <- c(""user"" , ""achievement.1"" , ""achievement.2"" , ""belongingness.1"" , ""belongingness.2"",     \r\n                               ""benevolence.1"" ,  ""benevolence.2"" , ""conformity.1"" , ""conformity.2"" ,  ""data usage.1"" , \r\n                               ""data usage.2"" ,  ""engagement.1"" , ""engagement.2"" ,   ""engagement.3"" ,   ""global motivation"",\r\n                               ""hedonism.1"" ,    ""hedonism.2"" ,   ""power.1"" , ""power.2"" , ""routine.1"" ,    \r\n                               ""routine.2"" ,     ""self-direction.1"" , ""self-direction.2"" , ""stimulation.1"" , ""stimulation.2"" , \r\n                               ""universalism.1"" ,  ""universalism.2"")\r\n\r\n# Save result as a csv\r\nwrite.csv(pivot.for.question, file =  ""completed-survey-closed-answers.csv"", row.names = F)\r\n\r\n\r\n# Correlation analysis\r\n\r\ncompleted<-read.csv(file =  ""completed-survey-closed-answers.csv"", header = T)\r\n\r\n# Average of questions for each latent variable\r\ncompleted$ach <- rowMeans(subset(completed, select = c(achievement.1, achievement.2)), na.rm = TRUE)\r\ncompleted$bel <- rowMeans(subset(completed, select = c(belongingness.1, belongingness.2)), na.rm = TRUE)\r\ncompleted$ben <- rowMeans(subset(completed, select = c(benevolence.1, benevolence.2)), na.rm = TRUE)\r\ncompleted$conf <- rowMeans(subset(completed, select = c(conformity.1, conformity.2)), na.rm = TRUE)\r\ncompleted$hed <- rowMeans(subset(completed, select = c(hedonism.1, hedonism.2)), na.rm = TRUE)\r\ncompleted$pwr <- rowMeans(subset(completed, select = c(power.1, power.2)), na.rm = TRUE)\r\ncompleted$rout <- rowMeans(subset(completed, select = c(routine.1, routine.2)), na.rm = TRUE)\r\ncompleted$self <- rowMeans(subset(completed, select = c(self.direction.1, self.direction.2)), na.rm = TRUE)\r\ncompleted$stim <- rowMeans(subset(completed, select = c(stimulation.1, stimulation.2)), na.rm = TRUE)\r\ncompleted$univ <- rowMeans(subset(completed, select = c(universalism.1, universalism.2)), na.rm = TRUE)\r\ncompleted$engag <- rowMeans(subset(completed, select = c(engagement.1, engagement.2, engagement.3)), na.rm = TRUE)\r\ncompleted$datause <- rowMeans(subset(completed, select = c(data.usage.1, data.usage.2)), na.rm = TRUE)\r\n\r\n# Correlation between global motivation and each latent variable\r\nd.cor<-completed\r\nglobal.corr <- data.frame(var=c(),corr=c(),pv=c(),sign=c(), global.motivation= c())\r\nfor(i in 28:39){\r\n  t <- cor.test(d.cor[,i], d.cor$global.motivation)\r\n  nome <- names(d.cor)[i]\r\n  corr <- t$estimate\r\n  pv <- t$p.value\r\n  sign <- ifelse(pv<0.001, ""***"", ifelse(pv<0.01, ""**"", ifelse(pv<0.05, ""*"", """")))\r\n  global.motivati']",4,"TESS Network, motivation, survey, citizen science, participation, personal enjoyment, improvement, achievement, personal relationships, environment, factors, successful design, implementation, long-term engagement, light pollution, TESS photometers, sensors, data collection, diverse"
"Sociality and tattoo skin disease among bottlenose dolphins in Shark Bay, Australia","Social behavior is an important driver of infection dynamics, though identifying the social interactions that foster infectious disease transmission is challenging. Here we examine how social behavior impacts disease transmission in Indo-Pacific bottlenose dolphins (Tursiops aduncus) using an easily identifiable skin disease and social network data. We analyzed tattoo skin disease (TSD) lesions based on photographs collected as part of a 34-year longitudinal study in relation to the sociality of T. aduncus using three metrics (degree, time spent socializing, and time in groups) and network structure, using the k-test. We show that calves with TSD in the second year of life associated more with TSD-positive individuals in the first year of life compared with calves that did not have TSD. Additionally, the network k-test showed that the social network links are epidemiologically relevant for transmission. However, degree, time spent in groups, and time spent socializing were not significantly different between infected and uninfected groups. Our findings indicate that association with infected individuals is predictive of an individual's risk for TSD and that the social association network can serve as a proxy for studying the epidemiology of skin diseases in bottlenose dolphins.","['###Get averaged degree, actual degree, infected degree\n\noptions(stringsAsFactors = FALSE)\n\nSurveyFile<-read.csv(""allsightings.csv"")\nFocalFile<-read.csv(""allfocals.csv"")\n\ncalves<-unique(FocalFile$dolphin_id)\nn<-length(calves)\n\n#format dates\nSurveyFile$date<-as.Date(SurveyFile$date)\nFocalFile$birthdate<-as.Date(FocalFile$birthdate)\n\n#calculate average degree by resampling sets of 5 surveys\nindivs<-split(SurveyFile, SurveyFile$dolphin_id)\n\nresults<-list()\n\nset.seed(2019) \n\nfor (i in 1:n){\n  \n  focal<-calves[i]\n  start<-FocalFile$birthdate[which(FocalFile$dolphin_id==focal)]\n  end<-start+365\n  surveys<-indivs[[focal]]\n  trim_surveys<-surveys[(surveys$date>=start & surveys$date<=end),]\n  observation_ids<-unique(trim_surveys$observation_id)\n  #number of surveys\n  total<-length(observation_ids)\n  \n  if(length(observation_ids)<5) {next} else{\n    \n    subsample_degree<-replicate(1000,{\n      \n      sobservation_ids<-sample(observation_ids, 5)\n      associates<-SurveyFile$dolphin_id[which(SurveyFile$observation_id %in% sobservation_ids)]\n      nassoc<-length(unique(associates[which(associates!=focal)]))\n      return(nassoc)})\n  }\n  \n  mean_degree<-mean(unlist(subsample_degree))\n  \n  #total and infected number of associates\n  associates<-SurveyFile[,c(""dolphin_id"", ""infection_status"")][which(SurveyFile$observation_id %in% observation_ids),]\n  assoc<-aggregate(infection_status~dolphin_id, data=associates, max)\n  assoc<-assoc[assoc$dolphin_id!=focal,]\n\n  ind_results<-data.frame(dolphin_id=focal, \n                          avg_degree=mean_degree,\n                          true_degree=nrow(assoc),\n                          infected_degree=sum(assoc$infection_status),\n                          total_surveys=total)\n  results[[i]]<-ind_results\n}\n\nmodeldata<-as.data.frame(do.call(""rbind"", results))\n\nmodeldata$proportion_infected<-modeldata$infected_degree/modeldata$true_degree\n\n#Merge with FocalFile\n\nmodeldata<-merge(FocalFile, modeldata, by=""dolphin_id"")\n\nwrite.csv(modeldata, ""modeldata.csv"", row.names = FALSE)\n', '###Run models on real data \n\noptions(stringsAsFactors = FALSE)\n\nmodeldata<-read.csv(""modeldata.csv"")\n\nmodeldatasexed<-modeldata[-which(is.na(modeldata$sex)),]\n\nmodelfocal<-modeldatasexed[-which(is.na(modeldatasexed$proportion_groups)),]\n\nlibrary(logistf)\n\n#effect on degree on disease\ndegree_log<-logistf(formula = tsd ~ avg_degree + sex, data = modeldatasexed, pl = TRUE,\n                    alpha = 0.05, firth = TRUE)\n\nsummary(degree_log)\n\ninfdegree_log<-logistf(formula = tsd ~ proportion_infected + sex, data = modeldatasexed, pl = TRUE,\n                       alpha = 0.05, firth = TRUE)\n\nsummary(infdegree_log)\n\nsocial_log<-logistf(formula = tsd ~ proportion_social + sex, data = modelfocal, pl = TRUE,\n                    alpha = 0.05, firth = TRUE)\n\nsummary(social_log)\n\ngroups_log<-logistf(formula = tsd ~ proportion_groups + sex, data = modelfocal, pl = TRUE,\n                    alpha = 0.05, firth = TRUE)\n\nsummary(groups_log)\n', '#Randomzing Functions\n\n#Directly from Strona et al. 2014 (Nature Communications)\n\ncurve_ball<-function(m){\n  RC=dim(m)\n  R=RC[1]\n  C=RC[2]\n  hp=list()\n  for (row in 1:dim(m)[1]) {hp[[row]]=(which(m[row,]==1))}\n  l_hp=length(hp)\n  for (rep in 1:(5*l_hp)){\n    AB=sample(1:l_hp,2)\n    a=hp[[AB[1]]]\n    b=hp[[AB[2]]]\n    ab=intersect(a,b)\n    l_ab=length(ab)\n    l_a=length(a)\n    l_b=length(b)\n    if ((l_ab %in% c(l_a,l_b))==F){\n      tot=setdiff(c(a,b),ab)\n      l_tot=length(tot)\n      tot=sample(tot, l_tot, replace = FALSE, prob = NULL)\n      L=l_a-l_ab\n      hp[[AB[1]]] = c(ab,tot[1:L])\n      hp[[AB[2]]] = c(ab,tot[(L+1):l_tot])}\n    \n  }\n  rm=matrix(0,R,C)\n  for (row in 1:R){rm[row,hp[[row]]]=1}\n  dimnames(rm)<-dimnames(m) #added retention of dimnames\n  rm\n}\n\n\nlibrary(SocGen) #devtools::install_github(""vjf2/SocGen"")\n\nmake_random<-function(sf){\n  rand1 <- list()\n  \n  for (i in unique(sf$twomonth_interval)) {\n    thesemonths <- sf[sf$twomonth_interval == i, ]\n    \n    infected <- unique(thesemonths$dolphin_id[which(thesemonths$infection_status == 1)])\n    \n    #make matrix\n    m <- dat2mat(data.frame(thesemonths$observation_id, thesemonths$dolphin_id, 1))\n    m[is.na(m)] <- 0\n    \n    m1 <- curve_ball(m)\n    \n    #convert back to original format\n    d1 <- mat2dat(m1)\n    d1$infection_status <- ifelse(d1$ID2 %in% infected, 1, 0)\n    \n    #add infection status\n    rand1[[i]] <- d1[d1$values == 1, c(1,2,4)]\n  }\n  \n  randsurv <- data.table::rbindlist(rand1)\n  \n  randsurv$date <- sf$date[match(randsurv$ID1, sf$observation_id)]\n  \n  names(randsurv)[1:2]<-c(""observation_id"", ""dolphin_id"")\n  \n  return(as.data.frame(randsurv))\n  \n}\n', '#Randomize data using curveball algorithm\n\noptions(stringsAsFactors = FALSE)\n\nsf<-read.csv(""allsightings.csv"")\nmodeldata<-read.csv(""modeldata.csv"")\nmodeldatasexed<-modeldata[-which(is.na(modeldata$sex)),]\nmodeldatasexed$birthdate<-as.Date(modeldatasexed$birthdate)\n\n#set up empty matrices\n\ncalves<-sort(modeldatasexed$dolphin_id)\nn<-length(calves)\n\nmonths<-unique(sf$twomonth_interval)\ndegree_matrix<-matrix(NA, nrow=n, ncol=1000)\ninfected_degree_matrix<-matrix(NA, nrow=n, ncol=1000)\nrownames(degree_matrix)<-calves\n\ntp<-aggregate(observation_id~twomonth_interval, data=sf, function(x) length(unique(x)))\nmean(tp$observation_id) #169 sightings per period\n\nsource(\'3aRandomizingFunctions.R\')\n\nset.seed(2019)\n\nfor (q in 1:1000) {\n  \n  randsurveys<-make_random(sf)\n  \n  #now calculate degree and infected degree for all focal individuals\n  \n  indivs<-split(randsurveys, randsurveys$dolphin_id)\n  \n  results<-list()\n  \n  for (i in 1:n){\n\n    focal<-calves[i]\n    start<-modeldatasexed$birthdate[which(modeldatasexed$dolphin_id==focal)]\n    end<-start+365\n    surveys<-indivs[[focal]]\n    trim_surveys<-surveys[(surveys$date>=start & surveys$date<=end),]\n    observation_ids<-unique(trim_surveys$observation_id)\n    \n    #total and infected number of associates\n    associates<-randsurveys[,c(""dolphin_id"", ""infection_status"")][which(randsurveys$observation_id %in% observation_ids),]\n    assoc<-aggregate(infection_status~dolphin_id, data=associates, max)\n    assoc<-assoc[assoc$dolphin_id!=focal,]\n    \n    ind_results<-data.frame(dolphin_id=focal, \n                            true_degree=nrow(assoc),\n                            infected_degree=sum(assoc$infection_status))\n    results[[i]]<-ind_results\n    }\n  \n  fakemodeldata<-as.data.frame(do.call(""rbind"", results))\n  \n  degree_matrix[,q]<-fakemodeldata$true_degree\n  infected_degree_matrix[,q]<-fakemodeldata$infected_degree\n}\n\nproportion_infected_matrix<-infected_degree_matrix/degree_matrix\n\nwrite.csv(proportion_infected_matrix, ""curveball_proportion_infected.csv"")\n', '#Compare real and random\n\noptions(stringsAsFactors = FALSE)\n\nmodeldata<-read.csv(""modeldata.csv"")\nmodeldatasexed<-modeldata[-which(is.na(modeldata$sex)),]\n\nproportion_infected_matrix<-read.csv(""curveball_proportion_infected.csv"", row.names = 1)\n\nrealnsim<-merge(modeldatasexed, proportion_infected_matrix, \n                by.x=""dolphin_id"", by.y=0,all.x=TRUE, all.y=FALSE)\n\nlibrary(logistf)\n\nflog<-logistf(formula = tsd ~ proportion_infected + sex, data = modeldatasexed, pl = TRUE,\n              alpha = 0.05, firth = TRUE)\n\nactualp<-flog$prob[2]\nactualB<-coef(flog)[2]\nactualse<-sqrt(flog$var[2,2])\n\nrandp<-list()\nrandB<-list()\nrandse<-list()\n\nfor (i in 1:1000){\n  \n  flog<-logistf(formula = tsd ~ realnsim[,i+11] + sex, data = realnsim, pl = TRUE,\n                alpha = 0.05, firth = TRUE)\n  \n  randp[[i]]<-flog$prob[2]\n  randB[[i]]<-flog$coefficients[2]\n  randse[[i]]<-sqrt(flog$var[2,2])\n}\n\n\nallrandB<-unlist(randB)\nallrandse<-unlist(randse)\nallrandz<-allrandB/allrandse\nactualz<-actualB/actualse\n\nBtile<-round(ecdf(allrandB)(actualB)*100,0)\nZtile<-round(ecdf(allrandz)(actualz)*100,0)\n\n\n\npdf(file=""supplemental_histograms.pdf"", width=9.5, height=5.9)\npar(mfrow=c(1,2), xpd=NA, mar=c(5.1, 4.1, 2.1, 2.1))\nhg<-hist(allrandB, probability = TRUE, col=""grey85"", border=NA,\n         main=NA, \n         xlab=""Coefficients generated from randomized data"", \n         ylab=NA,\n         axes=FALSE, \n         cex.lab=1.3)\n\naxis(1, cex.axis=1.3)\n\nsegments(actualB,0,actualB, max(hg$density)*1.02, col=""red"", lty=3, lwd=2)\ntext(x=actualB, y=max(hg$density)*1.05,\n     labels=expression(paste(""Observed "", beta, "" = 23rd %tile"")))\n\ny1<-density(allrandB, adjust=1.25, from=min(hg$breaks), to=max(hg$breaks))\nlines(y1$x, y1$y, lwd=2, col=""darkgrey"", lty=2)\n\n##plot 2\nhg<-hist(allrandz, probability = TRUE, col=""grey85"", border=NA,\n         main=NA, \n         xlab=""Z-values generated from randomized data"", \n         ylab=NA,\n         xlim=c(min(allrandz),max(c(allrandz, actualz))),\n         axes=FALSE, \n         cex.lab=1.3)\n\naxis(1, cex.axis=1.3)\n\nsegments(actualz,0,actualz, max(hg$density)*1.02, col=""red"", lty=3, lwd=2)\ntext(x=actualz, y=max(hg$density)*1.05, labels=""Observed z-value = 80th %tile"")\n\ny1<-density(allrandz, adjust=1.25, from=min(hg$breaks), to=max(hg$breaks))\nlines(y1$x, y1$y, lwd=2, col=""darkgrey"", lty=2)\n\ndev.off()\n\n\n\n\n\n\n']",4,"Social behavior, tattoo skin disease, bottlenose dolphins, Shark Bay, Australia, infectious disease transmission, Indo-Pacific bottlenose dolphins, Tursiops aduncus, skin disease, social network data, social interactions, sociality, degree"
Do Carefully-timed Email Messages Increase Accuracy and Precision in Citizen Scientists' Reports of Events?,"Periodic messages are a commonly used tactic for reminding citizen science program participants to take a desired action such as collecting observations. In this study, we evaluate the impact of such messages on the accuracy and precision of observations contributed to Natures Notebook, a citizen science phenology observing program.To encourage participants in Natures Notebook to log the timing of leaf-out and flowering with maximum accuracy and precision, we email observers three days prior to when the events were expected to occur based on forecast models. Unplanned interruptions to the scripts driving these email prompts allowed us to evaluate whether the messages had the intended impacts.","['# downloading lilac phenometrics data for comparing with Constant Contact message open stats\r\n# T Crimmins (theresa@usanpn.org) updated 1-6-21\r\n\r\nlibrary(rnpn)\r\nlibrary(dplyr)\r\n\r\nlilacdata <- npn_download_individual_phenometrics(\r\n                                     request_source = ""TCrimmins"",\r\n                                     years=c(""2019""),\r\n                                     species_ids = c(35,36), \r\n                                     six_leaf_layer = TRUE, \r\n                                     six_sub_model = ""lilac"",\r\n                                     )\r\n\r\n\r\n# subset to only LEAF or BLOOM \r\n# unique(lilacdata$phenophase_description)\r\n\r\nlilacbloom <- subset(lilacdata, phenophase_description==""Open flowers (lilac)"")\r\n#OR\r\nlilacleaf <- subset(lilacdata, phenophase_description==""Breaking leaf buds (lilac/honeysuckle)"")\r\n\r\n# thin out multiple first y\'s -- only one individual_id per year for a single phenophase\r\nlilacleafsub <- lilacleaf %>% group_by(individual_id,first_yes_year) %>% filter(first_yes_doy==min(as.numeric(as.character(first_yes_doy))))\r\n\r\n\r\n# load 20XX LEAF or BLOOM message open file\r\nleaf2019msgstats <- read.csv(file = ""c:/NPN/Manuscripts/Working/Springcasting_evaluation/data_files/2019_Leaf_msg_stats.csv"")\r\n\r\n# fix column name, if necessary\r\ncolnames(leaf2019msgstats)[1]<-""Station_ID""\r\n\r\n# join w/ConCon message open info \r\n###I THINK ON THIS STEP I WANT TO RETAIN ALL ROWS - even situations where folks didn\'t log obs - if what this option, keep the ""all.y = TRUE""\r\nleaf2019merge <- merge(x = lilacleafsub, y = leaf2019msgstats, by.x = ""site_id"", by.y = ""Station_ID"", all.x = TRUE, all.y = TRUE)\r\n\r\n# Look at number of rows in MERGED df - if greater than Pheno data, need to do next steps:\r\n# in 2018, had duplicates - multiple people received emails for same plants\r\n\r\n# assign codes for email open codes:\r\n# Opened = 1, Did not open = 2, Did not receive = 3 - Ensure the order is correct here!!\r\nleaf2019merge$Code2 <- c(""2"", ""3"", ""1"")[leaf2019merge$LeafCode]\r\n\r\n# generate stats on open rate & data logging rate\r\ntable <- leaf2019merge %>%\r\n  group_by(first_yes_year, LeafCode) %>%\r\n  tally()\r\n\r\n\r\n\r\n# CHECK WHAT\'s REMOVED IN NEXT STEPS.....\r\n# remove duplicates based on code\r\nleaf2019trim <- leaf2019merge %>% group_by(individual_id) %>% filter(Code2==min(Code2))\r\n\r\n# remove additional duplicates - this preps file for other analyses\r\nleaf2019trim2 <- leaf2019trim[!duplicated(leaf2019trim$individual_id), ]\r\n\r\n# save data file...fix filename here\r\nwrite.csv(leaf2019trim, ""c:/NPN/Manuscripts/Working/Springcasting_evaluation/merged_files/2019_lilac_leaf-submission_rate.csv"")\r\n\r\n\r\n\r\n']",4,"keywords: citizen science, email messages, accuracy, precision, observations, Natures Notebook, phenology, timing, reminders, interruptions."
"The Cyclostratigraphy Intercomparison Project (CIP): consistency, merits and pitfalls","Supplementary materials for Submission: Sinnesael et al., Earth-Science Reviews ""The Cyclostratigraphy Intercomparison Project (CIP): consistency, merits and pitfalls "". Earth Science Review 199, #102965, 2019. Matthias Sinnesaela, David De Vleeschouwer, Christian Zeedenc, Sietske J. Batenburg, Anne-Christine Da Silva, Niels J. de Winter, Jaume Dinars-Turell, Anna Joy Drury, Gabriele Gambacorta Frederik J. Hilgen, Linda A. Hinnov, Alexander J.L. Hudson, David B. Kemp, Margriet L. Lantink, Ji Laurin, Mingsong Li, Diederik Liebrand, Chao Ma, Stephen R. Meyers, Johannes Monkenbusch, Alessandro Montanari, Theresa Nohl,Heiko Plike, Damien Pas, Micha Ruhl, Nicolas Thibault, Maximilian Vahlenkamp, Luis Valero, Sbastien Wouters, Huaichun Wu, Philippe Claeys","['# this script is partof the manuscript \'The Cyclostratigraphy Intercomparison Project (CIP): a consistency test for cyclostratigraphy and astrochronology\' by M. Sinnesael et al. \r\n\r\n# ! Note that noise generation was not fixed, and that therefore this script deviates from the original CIP case. However, the Imbire modes is used as for the original CIP case.\r\n\r\n# load the \'astrochron\' package, which must be installed\r\nlibrary(astrochron)\r\n\r\n#set time steps\r\nDt <- 1\r\n\r\n# get orbital solution\r\nLa04 <- getLaskar()\r\n\r\n# extract obliquity and precession\r\nObl <- etp(solution = La04, tmin=0, tmax=6000, dt=Dt, eWt=0, oWt=1, pWt=0)\r\nPrec <- etp(solution = La04, tmin=0, tmax=6000, dt=Dt, eWt=0, oWt=0, pWt=1)\r\n\r\n# compile p-0.5T mimicking NH insolation\r\nPminhalfT <- Prec\r\nPminhalfT[,2] <- (-Prec[,2]+.5*Obl[,2])\r\n\r\n# scale to mimick insolation (needed for the Imbrie model)\r\nPminhalfT <- s(PminhalfT)\r\nPminhalfT[,2] <- PminhalfT[,2]*11.66+500\r\n\r\n# apply the II model as in LRo4\r\n# set parameters\r\nb1=.6\r\nb2=.3\r\nTm1=15\r\nTm2=5\r\nTc=.5\r\nDt=1\r\n\r\n\r\n# initialize the I&I model by parameter setting for the \'imbrie\' function\r\n# set b and Tm parameters\r\n# set b parameter\r\nbtpts <- matrix(NA,4,2)\r\nbtpts[1,1] <- 0\r\nbtpts[1,2] <- b1\r\nbtpts[2,1] <- 1500\r\nbtpts[2,2] <- b1\r\nbtpts[3,1] <- 3000\r\nbtpts[3,2] <- b2\r\nbtpts[4,1] <- 6000\r\nbtpts[4,2] <- b2\r\n\r\nb <- linterp(btpts,Dt, verbose=F, genplot=F)\r\n\r\n# set Tm parameter\r\nTmtpts <- matrix(NA,4,2)\r\nTmtpts[1,1] <- 0\r\nTmtpts[1,2] <- Tm1\r\nTmtpts[2,1] <- 1500\r\nTmtpts[2,2] <- Tm1\r\nTmtpts[3,1] <- 3000\r\nTmtpts[3,2] <- Tm2\r\nTmtpts[4,1] <- 6000\r\nTmtpts[4,2] <- Tm1\r\n\r\nTm <- linterp(Tmtpts,Dt, verbose=F, genplot=F)\r\n\r\n# run Imbrie model\r\nIIM<- imbrie(PminhalfT,b=b[,2],Tm=Tm[,2],output=T,genplot=1)\r\n\r\n# plot result again (can be omitted)\r\ndev.off()\r\nplot(s(IIM[1:1500,]), type=""l"", col=\'red\')\r\n\r\n# standardize model\r\nsIIM <- s(IIM)\r\n# create AR1 noise\r\n# ! random noise will differ fro run to run, therefore CIP case 2 can not be perfectly reproduced\r\nnoise <- ar1(npts=5500,dt=1,mean=0,sdev=1,rho=0.9,shuffle=F,nsim=1,genplot=F,verbose=T)\r\n# create data series from 500-1000 and 1200-1500 ka\r\nsIIM <- rbind(sIIM[500:1000,], sIIM[1200:1500,])\r\n# place on new \'depth\' scale\r\nsIIM <- (cb(1:802,-sIIM[,2]))\r\n# plot for check\r\nplot(sIIM, type=""l"", xlab=""Time in ka"", ylab= ""Imbrie & Imbrie ice model"")\r\n# plot line at time break\r\nabline(v=501, lwd=2, col=""red"")\r\n#apply ramping sedimentation rate\r\nsrII23 <- sedRamp(sIIM, srstart=1, srend=,1.5)\r\nsrII23 <- s(linterp(srII23, dt=1))\r\n# plot for check\r\nplot(srII23, type=""l"", xlab=""Depth in cm"", ylab= ""Imbrie & Imbrie ice model"", main=""Sedimentation rate ramping from 1-1.5 cm/ka"")\r\n\r\n# combine model with noise\r\nsrIInoise <- cb(srII23[,1], srII23[,2]+noise[1:1002,2]*.6)\r\n\r\n# plot for check\r\ndev.off()\r\nplot(srIInoise, type=""l"", xlab=""Depth in cm"", ylab=""Artificial proxy data"", main=""Signal2, different noise representation"")\r\nabline(v=579, lwd=2, col=""red"")\r\n\r\n# end script']",4,"Cyclostratigraphy, Intercomparison Project, consistency, merits, pitfalls, Earth-Science Reviews, Matthias Sinnesaela, David De Vleeschouwer, Christian Zeedenc, Sietske J. Batenburg,"
The gut microbiome reflects ancestry despite dietary shifts across a hybrid zone,"The microbiome is critical to an organism's phenotype, and its composition is shaped by, and a driver of, eco-evolutionary interactions. We investigated how host ancestry, habitat, and diet shape gut microbial composition in a mammalian hybrid zone that occurs across an ecotone between distinct vegetation communities. We found that habitat is the primary determinant of diet, while host genotype is the primary determinant of the gut microbiomea finding further supported by intermediate microbiome composition in first generation hybrids. Despite these distinct primary drivers, microbial richness was correlated with diet richness, and individuals that maintained higher dietary richness had greater gut microbial community stability. Both relationships were stronger in the relative dietary generalist of the two parental species. Our findings show that host ancestry interacts with dietary habits to shape the microbiome, ultimately resulting in the organismal phenotypic plasticity that host-microbial interactions allow.","['\nrm(list=ls())\n\n#####################################\n##### Workspace, libraries, and data \n####################################\n\nlibrary(phyloseq)\nlibrary(ggplot2)\nlibrary(vegan)\nlibrary(ggpubr)\nlibrary(glmmTMB)\n\n\n\n#data import - from mothur\ndata <- import_mothur(mothur_shared_file = ""OTU table.txt"",\n                      mothur_constaxonomy_file = ""taxonomy.txt"")\n\n\n#read in metadata\nmetadata <- read.csv(""metadata.csv"", header=TRUE, row.names = 1)\n\n#subset data to use only 1 sample per individual, or all the resampled inds only\nmetadata_subset <- subset(metadata, """")\n\n\n#Read in the CNVRG estimates for plants and microbes\ncnvrg_microbes <- read.csv(""CNVRG_microbe.csv"", header=TRUE)\ncnvrg_plants <- read.csv(""CNVRG_plants.csv"", header=TRUE)\n\n#a quick function to turn into matrix\nmatrix.please<-function(x) {\n  m<-as.matrix(x[,-1])\n  rownames(m)<-x[,1]\n  m\n}\n#make otu matrix of cnvrg data for phyloseq\notu_matrix <- matrix.please(cnvrg_otu) #convert OTU table to matrix\n\n###Make the constructors needed for phyloseq###\notu <- otu_table(otu_matrix, taxa_are_rows = FALSE) #this will use the cnvrg estimates\n\n\n#create phyloseq object raw mothur data - for calculations not using CNVRG estimates\nphyseq <- merge_phyloseq(data, metadata_subset) #for MOTHUR dat\n\n\n#create phyloseq object with CNVRG estimates\nphyseq1 <- merge_phyloseq(tax_table(data),otu, metadata) #for CNVRG data\n\n\n\n\n\n\n###\n###Alpha Diversity###\n###\n#estimate 16S and diet alpha diversity\n\n#16S richness\nsample_data(physeq1_mothur)$rich <- estimate_richness(physeq1_mothur, measures = ""Observed"")\n\n#diet richness\ndiet <- data.frame(sample_data(physeq1_mothur))\ndiet$diet_rich <- apply(diet[,22:67]>0,1,sum)\n\n\n\n## glmmTMB model \nmixed_mod <- glmmTMB(microbiome_diversity$Observed ~ diet_richness + Season + Habitat + Genotype, data=diet)\nsummary(mixed_mod)    \n#get marginal and conditional coefficients of determination\nMuMIn::r.squaredGLMM(mixed_mod)\n\n\ndiet_by_microbiome <- ggplot(data=diet, aes(x=diet_richness, y=microbiome_diversity$Observed, color=Genotype, shape=Habitat)) +\n  geom_point(size=3) + ylab(""Number of Microbial OTUs"") + xlab(""Number of Plants in Diet"") + theme_bw() +\n  geom_abline(aes(intercept=451.732, slope=17.951))+ \n  scale_shape_manual(values=c(17,15)) + theme(legend.title = element_text(size=16),\n                                              legend.text=element_text(size=14)) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_line(colour = ""black""))+\n  annotate(""text"", x = 15, y=1050, label = ""Conditional R^2 = 0.12"",size = unit(5, ""pt"")) +\n  annotate(""text"", x = 14.5, y=1000, label = ""P < 0.001"",size = unit(5, ""pt"")) +\n  scale_color_manual(values= c(""forestgreen"",""lightgreen"", ""black"", ""orange"", ""maroon""), \n                     labels= c(expression(paste(italic(""N. bryanti""))), expression(paste(""BC-"",italic(""bryanti""))), ""F1"",\n                               expression(paste(""BC-"",italic(""lepida""))), expression(paste(italic(""N. lepida"")))))\n\n\n\n#now just use parentals and BCs in their \'native\' habitat, and F1s. \n\ndiet_by_microbiome_pures <- ggplot(data=diet_pure_hab, aes(x=diet_richness, y=microbiome_diversity$Observed, color=Genotype)) +\n  geom_point(size=3) + ylab("""") + xlab("""") + \n  theme_bw() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_line(colour = ""black""))+\n  facet_wrap(~Habitat, nrow=1) +\n  geom_smooth(method=lm, se=FALSE) +\n  stat_cor(method=""spearman"", label.x = 1, label.y = c(1600,1675,1750,1600,1675,1750)) +\n  scale_color_manual(values= c(""forestgreen"",""lightgreen"", ""black"", ""orange"", ""maroon""), \n                     labels= c(expression(paste(italic(""N. bryanti""))), expression(paste(""BC-"",italic(""bryanti""))), ""F1"",\n                               expression(paste(""BC-"",italic(""lepida""))), expression(paste(italic(""N. lepida"")))))\n\n\n\n#Resampled individuals plots\n#get the dist. matrix data with comparisons between inds resampled\n\n\ndiet_micro_distance_plot <- species_plot_16S$data\n\n#randomly sample 1 observation per sample\ndiet_micro_distance_plot <- diet_micro_distance_plot %>% group_by(ID.1) %>% slice_sample(n=1)\n\nspecies_plot_16S <- ggplot(data = diet_micro_distance_plot, aes(x=trnL_distance, y=microbiome_distance, shape=habitat.1, color=habitat.1)) +\n  geom_point(size=4) + xlab(""Bray-Curtis Diet Distance"") + ylab(""Bray-Curtis Microbiome Distance"") +\n  #theme(legend.position = ""none"")+ #facet_wrap(~habitat.1)+\n  geom_smooth(method=lm, se=FALSE, color= ""black"") +\n  #scale_linetype_manual(values=c(""twodash"", ""solid"")) +\n  #geom_smooth(data=subset(pairwise_pure_hab, species.1==""N. lepida""), aes(group=species.1), method=""lm"") +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_line(colour = ""black""))+\n  scale_color_manual(values=c(""maroon"", ""forestgreen"")']",4,"gut microbiome, ancestry, dietary shifts, hybrid zone, phenotype, eco-evolutionary interactions, host ancestry, habitat, diet, microbial composition, mammalian hybrid zone, vegetation communities, microbiome richness, dietary richness, gut microbial community stability"
Evolutionary convergence on hummingbird pollination in Neotropical Costus provide insight into the causes of pollinator shifts,"1. The evolution of hummingbird pollination is common across angiosperm lineages throughout the Americas, presenting an opportunity to examine convergence in both traits and environments to better understand how complex phenotypes arise. We examine multiple independent shifts from bee to hummingbird pollination in the Neotropical spiral gingers (Costus) and use our data to address several common explanations for the prevalence of bee to bird pollination transitions.2. We use floral traits of species with observed pollinators to predict pollinators of unobserved species and reconstruct ancestral pollination states on a well-resolved phylogeny. We examine whether independent transitions evolve towards the same phenotypic optimum and whether shifts to hummingbird pollination are associated with high elevation or climatic niche.3. Traits predicting hummingbird pollination include small flower size, brightly-colored floral bracts, and the absence of nectar guides. We find many shifts to hummingbird pollination and no reversals, a single shared phenotypic optimum across hummingbird flowers, and no association between pollination and elevation or climatic niche. 4. Costus presents surprising findings compared to other plant clades. Hummingbird flowers are consistently smaller than bee flowers and primary flower colors are not predictive of pollinators. Moreover, hummingbird pollination shows no association with high elevation, as found in other tropical plants.","['# This R script combines the floral trait data for 17 floral traits \n# across 52 species using data from greenhouse measurements, previously \n# published studies, and photographs. It then imputes 34 missing trait \n# values, and performs factor analysis on the imputed data. It uses the \n# input files ""Costus_traits_greenhouse.csv"" and ""Costus_traits_Maas.csv""\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(corrplot)\nlibrary(Hmisc)\nlibrary(PerformanceAnalytics)\nlibrary(FactoMineR)\nlibrary(factoextra) \nlibrary(missMDA)\n\nsetwd(""~/Dropbox/Costus grant/Dena and Kathleen/FloralEvolution/Dryad_Deposit"")\n\n#We have floral trait data from Maas (and other monographs), the greenhouse, the field, \n#and a few photo sources (e.g., Dave Skinner\'s website, our own field photos). \n#First we asked, for species that overlap, do the measures correspond? \n#This analysis is at the end of the script. Based on how well the measures agreed\n#between monographs and our greenhouse data, we decided we could use both. We gave\n#precedence to our greenhouse data because we measured all the traits, whereas \n#Maas was often missing some. The one discrepancy is that Maas measured stamen and \n#labellum length from above the corolla tube, whereas we measured from the ovary.\n#To correct this, we added the length of the corolla tube to Maas\' stamen and \n#labellum measures.\n\n#first read in data from monographs, primarily authored by P.J.Maas\nMaas <- read.csv(""Costus_traits_Maas.csv"")\n#next read in date from traits measured in the greenhouse and field and on photos from field and other collections\nGreenhouse <- read.csv(""Costus_traits_greenhouse.csv"")\n\n#Maas sometimes reported max and min values for each trait. When he did, we took the midpoint. \n#When he only reported one value, we used that.\n#subset the data for just the columns we want\n\nMaas <- select(Maas, !contains(""Max""))\nMaas <- select(Maas, !contains(""Min""))\n\n#subset Maas data set getting rid of verbal descriptions of color and variables relating to extrafloral nectaries\nMaas <- Maas %>% select(Species_Maas, sp_tip_label, Syndrome, Stamen_Length_mid,\tStamen_Width_mid,\n                        Corolla_Length_mid,\tCorolla_Tube_Length_mid,\tCorolla_Lobe_Length_mid,\n                        Labellum_Length_mid,\tLabellum_Width_mid,\tAnther_Length_mid, bract_color_K,\n                        corolla_color_K,\tlab_color_K,\tyellow_labstripe,\tred_labstripe,\t\n                        stamen_color_K,\tstamen_tip_color,\tnectar_guides, Style_length_mid)\n\n#Maas did not report stamen exsertion directly, so we calculated it as the\n#stamen length minus the labellum length\n#add calculated exsertion column to Maas\nMaas <- dplyr::mutate(Maas, Stamen_exsertion_mid=Stamen_Length_mid - Labellum_Length_mid)\n\n\n#adjust his labellum and stamen columns by adding tube length (which he left out)\nMaas <- dplyr::mutate(Maas, Labellum_Length_mid = Labellum_Length_mid + Corolla_Tube_Length_mid)\nMaas <- dplyr::mutate(Maas, Stamen_Length_mid = Stamen_Length_mid + Corolla_Tube_Length_mid)\n\n#make sure factors are factors\nMaas$bract_color_K <- as.factor(Maas$bract_color_K)\nMaas$corolla_color_K <- as.factor(Maas$corolla_color_K)\nMaas$lab_color_K <- as.factor(Maas$lab_color_K)\nMaas$yellow_labstripe <- as.factor(Maas$yellow_labstripe)\nMaas$red_labstripe <- as.factor(Maas$red_labstripe)\nMaas$stamen_color_K <- as.factor(Maas$stamen_color_K)\nMaas$stamen_tip_color <- as.factor(Maas$stamen_tip_color)\nMaas$nectar_guides <- as.factor(Maas$nectar_guides)\n\n#remove rows for samples not in phylogeny\nMaas$sp_tip_label[Maas$sp_tip_label == """"] <- NA\nMaas <- filter(Maas, !is.na(sp_tip_label))\n\n\n#subset greenhouse data set getting rid of unnecessary columns\nGreenhouse <- Greenhouse %>% select(Greenhouse_ID, tip_label,\tsp_tip_label,\tSyndrome,\tCorolla_Length,\n                        Corolla_Lobe_Length,\tCorolla_Tube_Length,\tStamen_exsertion,\tLabellum_Length,\n                        Labellum_Width,\tStamen_Length,\n                        Stamen_Width,\tAnther_Length, Style_length, bract_color_K,\n                        corolla_color_K,\tlab_color_K,\tyellow_labstripe,\tred_labstripe,\t\n                        stamen_color_K,\tstamen_tip_color,\tnectar_guides)\n\n#make sure factors are factors\nGreenhouse$bract_color_K <- as.factor(Greenhouse$bract_color_K)\nGreenhouse$corolla_color_K <- as.factor(Greenhouse$corolla_color_K)\nGreenhouse$lab_color_K <- as.factor(Greenhouse$lab_color_K)\nGreenhouse$yellow_labstripe <- as.factor(Greenhouse$yellow_labstripe)\nGreenhouse$red_labstripe <- as.factor(Greenhouse$red_labstripe)\nGreenhouse$stamen_color_K <- as.factor(Greenhouse$stamen_color_K)\nGreenhouse$stamen_tip_color <- as.factor(Greenhouse$stamen_tip_color)\nGreenhouse$nectar_guides <- as.factor(Greenhouse$nectar_guides)\n\n#summarize (average or mode, depending on whether the data is continuous or categorical)\n#data within greenhouse and Maas data frames\n#first take average/mode of individuals measured multiple times\n#then take average/mode of different', '# This R script examines pollinator observation data. Specifically, \n# it tests whether the proportion of floral visits matching syndrome \n# varies by syndrome (Figure 2) using a phylogenetic ANOVA. It uses \n# the phylogenetic tree file ""chronogram_v8_2.tre"" and the data file \n# ""Pollinator_observations.csv""\n\nlibrary(dplyr)\nlibrary(geiger) #for phylANOVA\nlibrary(car) #levene\'s test\nlibrary(ggplot2)\n\nsetwd(""~/Dropbox/Costus grant/Dena and Kathleen/FloralEvolution/Dryad_Deposit"")\n\n#######\n### Prune phylogeny to match pollinator data\n#######\n\n#import chronogram\nphy <- read.nexus(""chronogram_v8_2.tre"")\nplot(phy, cex=0.4)\n\n#import species pollinator observations\nphy.data <- read.csv(""Pollinator_observations.csv"")\n\n#remove species without observations\nphy.data <- filter(phy.data, observed == ""yes"")\nphy.data$syndrome <- as.factor(phy.data$syndrome)\n\n(name.check(phy, phy.data, phy.data$tip_label) -> phyOverlap)\ndrop.tip(phy, phyOverlap$tree_not_data) -> phyComparativeTree\nplot(phyComparativeTree, cex=0.6)\n\nattach(phy.data)\n\n#######\n### plot prop visits matching synd by syndrome\n#######\n\nmycol <- c(""#1F639B"",""#ED553B"")\n\npdf(file=""Fig2.pdf"", width=5,height=4)\nprop_plot <- ggplot(phy.data, aes(x = syndrome, y = prop_matching, color = syndrome, fill = syndrome)) +\n  geom_jitter(size = log(phy.data$Total_visits_observed), show.legend = FALSE, width = 0.3) +\n  scale_color_manual(values = mycol) +\n  scale_fill_manual(values = mycol) +\n  geom_boxplot(fill = NA, show.legend = FALSE) +\n  scale_color_manual(values = mycol) +\n  ylab(""Proportion of visits matching syndrome"") +\n  ylim(0.5,1)+\n  xlab(""Pollination syndrome"")+\n  theme(legend.position = ""top"")+\n  theme_classic()\nprop_plot\ndev.off()\n\n#######\n### Does variance in proportion of visits matching syndrome differ by syndrome?\n#######\n\nleveneTest(phy.data$prop_matching ~ phy.data$syndrome)\n# Levene\'s Test for Homogeneity of Variance (center = median)\n#       Df F value  Pr(>F)  \n# group  1  6.7828 0.01502 *\n#       26                  \n# ---\n# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n#######\n### PHY ANOVA on proportion of visits matching syndrome by syndrome\n#######\n\ndat = phy.data$prop_matching\ndat = as.data.frame(dat)\nrownames(dat) = phy.data$tip_label\ndat = na.omit(dat)\ngrp = as.factor(phy.data$syndrome)\nnames(grp)=phy.data$tip_label\nt=aov.phylo(dat ~ grp, phyComparativeTree) \n# Analysis of Variance Table\n# \n# Response: dat\n# Df   Sum-Sq  Mean-Sq F-value   Pr(>F) Pr(>F) given phy\n# group      1 0.032215 0.032215  6.3412 0.018288           0.1399\n# Residuals 26 0.132085 0.005080   \n\n#not significant with phylogeny\n#significant without phylogeny\n', '# This R script uses random forest models to determine which \n# floral traits differentiate primary floral visitors (Figure 3). \n# Method follows that of Dellinger et al. 2019 and 2021. It uses \n# the floral trait data ""imputed.csv"" and the data file \n# ""Pollinator_observations.csv""\n\nlibrary(randomForest)\nrequire(caTools)\nlibrary(sciplot) #bargraph with confidence intervals\n\nsetwd(""~/Dropbox/Costus grant/Dena and Kathleen/FloralEvolution/Dryad_Deposit"")\n\ndata <- read.csv(""imputed.csv"")\n\n#pollinator observations (for sub-setting data)\npolobs <- read.csv(""Pollinator_observations.csv"")\npolobs <- polobs[,c(1,3)]\n\nsetdiff(polobs$tip_label, data$sp_tip_label)\nsetdiff(data$sp_tip_label,polobs$tip_label)\n\ndata <- merge(data, polobs, by.x=""sp_tip_label"", by.y=""tip_label"", all.x=T, all.y=F)\n\n#subset traits to remove those highly correlated (determined in previous analysis)\ndata.sub <- data[,c(1,2,5,6,7,8,11,12,13,14,15,16,17,18,19,20)]\n\n#taxa with pollinator obs\ndata.obs <- subset(data, observed==""yes"") #28 species with obs\ntraits <- data.obs[,2:19]\n\n#taxa missing pollinator obs (note that NA obs are excluded: these are african taxa)\ndata.missing <- subset(data,  is.na(observed)) #21 missing pollinator obs + 3 outgroups\ntraits.missing <- data.missing[,2:18]\n\n#get gini coef., robustness, and predicted syndromes for 100 random forests\nrobust <- c()\npredict <- c()\ngini <- c()\n\nfor (i in 1:1000) {\nrf <- randomForest(\n  as.factor(Syndrome) ~ .,\n  data=traits,\n  ntree=500,\n  mtry=4, #this is equal to sqrt(number traits), see help file\n)\nmyrobust <- data.obs$Syndrome == rf$predicted #to test robustness, examine whether a species was correctly classified in the training models\nrobust <- cbind(robust, myrobust)\nmygini <- as.character(rf$importance)\ngini <- cbind(gini, mygini)\nmysyndrome <- as.character(predict(rf, newdata=traits.missing)) #predict syndrome for missing taxa\nmypredict <- mysyndrome[1:21] == data.missing$Syndrome\npredict <- cbind(predict, mypredict)\n}\n\n# proportion of 1000 RFs where syndrome was correctly determined\nprop <- c()\nfor (i in 1:nrow(robust)) {\n  myprop <- table(robust[i,])[""TRUE""]/1000\n  prop <- c(prop, myprop)\n}\nsum.robust <- as.data.frame(cbind(data.obs$sp_tip_label,as.numeric(prop)))\ncolnames(sum.robust) <- c(""sp_tip_label"", ""prop_syn_correct"")\nsum.robust #proportion RF for taxa with pollinator obs where model correctly predicted pollinator\n\n# proportion of 1000 RFs where predicted syndrome matches apriori predictions for taxa missing pollinator observations\nprop <- c()\nfor (i in 1:nrow(predict)) {\n  myprop <- table(predict[i,])[""TRUE""]/1000 #add line here so if NA set value to 0\n  prop <- c(prop, myprop)\n}\nsum.predict <- as.data.frame(cbind(data.missing$sp_tip_label,as.numeric(prop)))\ncolnames(sum.predict) <- c(""sp_tip_label"", ""prop_syn_matched_apriori"")\nsum.predict[c(19,14,8),2] <- 0 #NA value is 0 (was NA because 0 matches with apriori 0/1000 = NA)\nsum.predict #predicted pollinators for 21 taxa missing observations\n\n# get mean and SD gini coefficients\nmean <- c()\nsd <- c()\nfor (i in 1:nrow(gini)) {\n  mymean <- mean(as.numeric(gini[i,]))\n  mysd <- sd(as.numeric(gini[i,]))\n  mean <- c(mean, mymean)\n  sd <- c(sd, mysd)\n}\nsum.gini <- as.data.frame(cbind(names(traits[1:17]),mean,sd))\ncolnames(sum.gini) <- c(""trait"", ""gini.mean"", ""gini.sd"")\nsum.gini #ranked importance of each floral trait to predicting syndrome\n\n## examine key traits by syndrome for 28 taxa with pollinator obs\ntable(data.obs[,c(""Syndrome"",""bract_color"")])\ntable(data.obs[,c(""Syndrome"",""yellow_labstripe"")])\ntable(data.obs[,c(""Syndrome"",""red_labstripe"")])\nb <- subset(data.obs, Syndrome==""bee"")\nh <- subset(data.obs, Syndrome==""hummingbird"")\nmean(b$Labellum_Length)/mean(h$Labellum_Length) #bee taxa 2.03 times size of hummer taxa on average\nmean(b$Labellum_Width)/mean(h$Labellum_Width) #bee taxa 3.27 times size of hummer taxa on average\nmean(b$Stamen_Width)/mean(h$Stamen_Width) #bee taxa 1.6 times size of hummer taxa on average\n\n#number predicted to be bee vs bird\ntmp<-merge(sum.predict,data.missing[,c(1,19)], by=""sp_tip_label"")\ntmp$Syndrome[c(19,14,8)] = c(""hummingbird"",""hummingbird"",""hummingbird"")\ntable(tmp$Syndrome)\n\n#######\n## Figure gini\n#######\n#relabel traits and order by mean gini value\nsum.gini$trait <- c(""corolla length"", ""corolla lobe length"",""corolla tube length"",""stamen exsertion"",""labellum length"",""labellum width"",""stamen length"",""stamen width"",""anther length"", ""style length"",""bract color"",""corolla color"",""labellum color"",""yellow labellum stripe"", ""red labellum stripe"",""stamen color"",""stamen tip color"")\nsum.gini <- sum.gini[order(sum.gini$gini.mean,decreasing = TRUE),]\n\npdf(file=""Fig3.pdf"", width=4,height=4) # specifications for your \npar (mfrow=c(1,1), mar=c(9,5,1,1))\nbase_r_barplot <- barplot(as.numeric(sum.gini$gini.mean),names.arg = sum.gini$trait, ylim = c(0, 3),xaxt=""none"", cex.lab=0.8,\n                          ylab=""Mean gini index"")\naxis(1, at=base_r_barplot,labels=sum.gini$trait, las=2, font=1, cex.axis=0.8, tick=F,', '# This R script compares models of evolutionary rate shifts between \n# pollination syndromes: symmetric rates (b to h = h to b), all rates \n# different, or irreversible b to h. Then, using the preferred irreversible \n# model, it performs stochastic character mapping. It uses the phylogenetic \n# tree file ""chronogram_v8_2.tre"" and the floral trait factor analysis output \n# data file ""famd_coords.csv""\n\n# UPDATED IN REVIEW: Added Bisse to simultaneously account for impacts on \n# speciation and extinction when estimating irreversibility\n\nlibrary(geiger)\nlibrary(phytools)\nlibrary(diversitree)\n\nsetwd(""~/Dropbox/Costus grant/Dena and Kathleen/FloralEvolution/Dryad_Deposit"")\n\n# chronogram\ntree <- read.nexus(""chronogram_v8_2.tre"")\nplot(tree, cex=0.4)\n\n#################\n## use syndromes from Random Forest predictions\n################\n# floral Dim characters\ndata <- read.csv(""famd_coords.csv"",row.names=1)\n\n# match tree and data\n(name.check(tree, data) -> phyOverlap)\ndrop.tip(tree, phyOverlap$tree_not_data) -> phyComparativeTree\ndata <- data[!(row.names(data)  %in% phyOverlap$data_not_tree), ]\n\n#use RF syndrome predictions\ndata$...2[33] = ""hummingbird"" #Costus_sp_nov_19168\ndata$...2[39] = ""hummingbird"" #Costus_varzearum_19252\ndata$...2[51] = ""hummingbird"" #Costus_dirzoi_98079\n\n# syndrome transition models\n#prepare data\ndat <- as.data.frame(data$...2)\nrownames(dat) <- row.names(data)\nsynd_geiger <- treedata(phyComparativeTree, dat)\n#symmetric rates\n(synd_SYM_geiger <- fitDiscrete(synd_geiger$phy, synd_geiger$data [,1],\n                                type=""discrete"", model = ""SYM""))\n#all rates\n(synd_ARD_geiger <- fitDiscrete(synd_geiger$phy, synd_geiger$data [,1],\n                                type=""discrete"", model = ""ARD""))\n# model: only B to H, but not reverse\nmodel<-matrix(c(0,0,1,0),2,2)\ncolnames(model)<-rownames(model)<-c(""bee"",""hummingbird"")\nmodel\n(fitIrr<-fitDiscrete(synd_geiger$phy, synd_geiger$data [,1],model=model))\n#aic weights\n(aicc<-setNames(\n  c(synd_SYM_geiger$opt$aicc,synd_ARD_geiger$opt$aicc,fitIrr$opt$aicc),\n  c(""ER"",""ARD"",""Irr"")))\naic.w(aicc) #Irr model best\n# ER       ARD       Irr \n# 0.3505889 0.1647360 0.4846752 \n\n# stochastic character mapping of syndrome\nfmode<-setNames(as.factor(data$...2),rownames(data))\nx <- make.simmap(phyComparativeTree, x=fmode, model= model, nsim=100)\nsummary(x)\n# 100 trees with a mapped discrete character with states:\n#  bee, hummingbird \n#\n# trees have 13.11 changes between states on average\n#\n# changes are of the following types:\n#   bee,hummingbird hummingbird,bee\n# x->y           13.11               0\n#\n# mean total time spent in each state is:\n#  bee hummingbird     total\n# raw  0.4986612   0.2781890 0.7768502\n# prop 0.6419013   0.3580987 1.0000000\n\ntable(summary(x)$count) # range changes between states\n#    0  13  14  15 \n#  100 180  18   2 \n\nobj <- summary(x)\n\npdf(file=""Fig4phylo.pdf"",width=11,height=11,paper=\'special\') \nhh<-max(nodeHeights(phyComparativeTree))*0.02 # for label offset\nplot(phyComparativeTree,no.margin=TRUE,label.offset=hh,edge.width=2)\nnodelabels(pie=obj$ace,piecol=c(""#1F639B"",""#ED553B""), cex=0.3)\ntiplabels(pie=obj$tips, piecol=c(""#1F639B"",""#ED553B""),cex=0.3)\ndev.off()\n\n###########\n#BISSE\n###########\nlibrary(tidyverse)\n\nnames(dat) <- c(""syn"")\ndat$syn \n\n#format trait vector for bisse\ndat.bisse <- dat %>% mutate(syn = case_when(syn==""bee"" ~ ""0"", syn==""hummingbird"" ~ ""1""))\ndat.bisse <- as.numeric(dat.bisse$syn)\nnames(dat.bisse) <- row.names(dat)\n\n#bisse\nlik <- make.bisse(phyComparativeTree, dat.bisse)\nlik(pars) #-211.3333\n\n#starting point for ML search\n(p <- starting.point.bisse(phyComparativeTree))\n\n#initiat ML search\nfit <- find.mle(lik, p)\nfit$lnLik #log-likelihood value\nround(coef(fit), 3) #coefficients\n\n# lambda: speciation\n# mu: extinction\n# q: transition x to y (0=bee, 1=bird)\n# lambda0 lambda1     mu0     mu1     q01     q10 \n# 73.954  46.780   0.000   0.000  28.197   0.000 \n\n#test hypothesis that transition rates are different\nlik.l <- constrain(lik, q01 ~ q10)\nfit.l <- find.mle(lik.l, p[argnames(lik.l)])\nfit.l$lnLik # 125.8509\nround(rbind(full=coef(fit), equal.q=coef(fit.l, TRUE)), 3)\nanova(fit, equal.l=fit.l) # not significantly better model\n# although with this small a tree and 6 parameters that is not at all surprising\n\n#test hypothesis that speciation rates are different\nlik.l <- constrain(lik, lambda0 ~ lambda1)\nfit.l <- find.mle(lik.l, p[argnames(lik.l)])\nfit.l$lnLik # 125.8925\nround(rbind(full=coef(fit), equal.l=coef(fit.l, TRUE)), 3)\nanova(fit, equal.l=fit.l) # not significantly better model\n# although with this small a tree and 6 parameters that is not at all surprising\n\n#because we fitting 6 parameters with only 52 taxa\n#priors needed (according to bisse tutorial) so that\n#posterior distribution is proper\nprior <- make.prior.exponential(1 / (2 * (p[1] - p[3])))\n#establish parameter update method using a short chain run\nset.seed(1)\ntmp <- mcmc(lik, fit$par, nsteps=100, prior=prior,\n            lower=0, w=rep(1, 6), print.every=0)', '# This R script tests whether pollination syndromes predict \n# floral trait factor analysis dimensionss 1 - 10 using phylogenetic \n# Anova.  It also makes a figure projecting phylogeny onto species \n# values for floral trait dimensions 1 and 2 (Figure 5). It uses the \n# phylogenetic tree file ""chronogram_v8_2.tre"" and the floral trait factor \n# analysis output data file ""famd_coords.csv""\n\nlibrary(geiger) # match tree and data\nlibrary(phytools)\nlibrary(ggplot2)\n\nsetwd(""~/Dropbox/Costus grant/Dena and Kathleen/FloralEvolution/Dryad_Deposit"")\n\n#chronogram\ntree <- read.nexus(""chronogram_v8_2.tre"")\nplot(tree, cex=0.4)\n\n#floral dim characters\ndata <- read.csv(""famd_coords.csv"",row.names=1)\n\n# match tree and data\n(name.check(tree, data) -> phyOverlap)\ndrop.tip(tree, phyOverlap$tree_not_data) -> phyComparativeTree\ndata <- data[!(data[,1]  %in% phyOverlap$data_not_tree), ]\ndata<- data[,1:11]\nnames(data) <- c(""syndrome"", ""Dim.1"", ""Dim.2"",""Dim.3"", ""Dim.4"",""Dim.5"", ""Dim.6"",""Dim.7"", ""Dim.8"",""Dim.9"", ""Dim.10"")\n\n# use random forest predicted syndromes\ndata[""Costus_dirzoi_98079"",""syndrome""]<-""hummingbird""\ndata[""Costus_sp_nov_19168"",""syndrome""]<-""hummingbird""\ndata[""Costus_varzearum_19252"",""syndrome""]<-""hummingbird""\n\n# Phy ANOVAs Dim1 by syndrome\ndat = data$Dim.1\ndat = as.data.frame(dat)\nrownames(dat) = rownames(data)\ngrp = as.factor(data$syndrome)\nnames(grp)=rownames(data)\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n# Analysis of Variance Table\n#\n# Response: dat\n# Df Sum-Sq Mean-Sq F-value     Pr(>F) Pr(>F) given phy    \n# group      1 350.98  350.98  126.11 2.8149e-15         0.000999 ***\n#  Residuals 50 139.16    2.78       \n\n# Phy ANOVAs Dim2 by syndrome\ndat = data$Dim.2\ndat = as.data.frame(dat)\nrownames(dat) = rownames(data)\ngrp = as.factor(data$syndrome)\nnames(grp)=rownames(data)\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n# Analysis of Variance Table\n#\n# Response: dat\n# Df Sum-Sq Mean-Sq F-value  Pr(>F) Pr(>F) given phy\n# group      1   7.90  7.9003  2.2991 0.13575           0.3626\n# Residuals 50 171.82  3.4363               \n\n# Phy ANOVAs Dim3 by syndrome\ndat = data$Dim.3\ndat = as.data.frame(dat)\nrownames(dat) = rownames(data)\ngrp = as.factor(data$syndrome)\nnames(grp)=rownames(data)\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n\n# Phy ANOVAs Dim4 by syndrome\ndat = data$Dim.4\ndat = as.data.frame(dat)\nrownames(dat) = rownames(data)\ngrp = as.factor(data$syndrome)\nnames(grp)=rownames(data)\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n\n# Phy ANOVAs Dim5 by syndrome\ndat = data$Dim.5\ndat = as.data.frame(dat)\nrownames(dat) = rownames(data)\ngrp = as.factor(data$syndrome)\nnames(grp)=rownames(data)\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n\n# Phy ANOVAs Dim6 by syndrome\ndat = data$Dim.6\ndat = as.data.frame(dat)\nrownames(dat) = rownames(data)\ngrp = as.factor(data$syndrome)\nnames(grp)=rownames(data)\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n\n# Phy ANOVAs Dim7 by syndrome\ndat = data$Dim.7\ndat = as.data.frame(dat)\nrownames(dat) = rownames(data)\ngrp = as.factor(data$syndrome)\nnames(grp)=rownames(data)\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n\n# Phy ANOVAs Dim8 by syndrome\ndat = data$Dim.8\ndat = as.data.frame(dat)\nrownames(dat) = rownames(data)\ngrp = as.factor(data$syndrome)\nnames(grp)=rownames(data)\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n\n# Phy ANOVAs Dim9 by syndrome\ndat = data$Dim.9\ndat = as.data.frame(dat)\nrownames(dat) = rownames(data)\ngrp = as.factor(data$syndrome)\nnames(grp)=rownames(data)\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n\n# Phy ANOVAs Dim10 by syndrome\ndat = data$Dim.10\ndat = as.data.frame(dat)\nrownames(dat) = rownames(data)\ngrp = as.factor(data$syndrome)\nnames(grp)=rownames(data)\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n\n##############\n#FIGURE 5\n##############\n\n#get colors to paint tips\ntip.col <- data[,1]\ntip.col[tip.col==""bee""] <- ""#1F639B""\ntip.col[tip.col==""hummingbird""] <- ""#ED553B""\nnames(tip.col)<- row.names(data)\ncols<-c(tip.col[phyComparativeTree$tip.label],rep(""black"",phyComparativeTree$Nnode))\nnames(cols)<-1:(length(phyComparativeTree$tip)+phyComparativeTree$Nnode)\n\n\npdf(file=""Figure5.pdf"",width=5,height=5.5,paper=\'special\') \nphylomorphospace(phyComparativeTree, data[,c(2,3)], ftype=""off"",node.by.map=TRUE,\n                 xlab=""Dim1 (33.5% PVE)"", ylab=""Dim2 (12.4 PVE)"", bty=""l"",control=list(col.node=cols),\n                 node.size=c(0,1) )\nlegend(x=-5, y=4, legend=c(""Hummingbird"",""Bee""), \n       title=""Pollination syndrome"",col=c(""black""), pch=21, pt.bg=c(""#ED553B"",""#1F639B""), cex=0.7)\n\n#add X symbol to taxa where RF syndrome was different than apriori\ndata.sub <- data[c(""Costus_dirzoi_98079"",""Costus_sp_nov_19168"",""Costus_varzearum_19252""),]\npoints(data.sub$Dim.1,data.sub$Dim.2, pch=4, cex=0.45)\n\ndev.off()\n\n\n', '# This R script detects location of evolutionary shifts in the \n# optima of floral trait factor analysis dimension 1 and 2, and \n# tests for convergent regimes, i.e., convergent optima. It uses \n# the phylogenetic tree file ""chronogram_v8_2.tre"" and the floral \n# trait factor analysis output data file ""famd_coords.csv""\n\nlibrary(geiger) # match tree and data\n#library(devtools)\n#install_github(""glmgen/genlasso"")\n#install_github(""khabbazian/l1ou"")\nlibrary(l1ou)\n\nsetwd(""~/Dropbox/Costus grant/Dena and Kathleen/FloralEvolution/Dryad_Deposit"")\n\n# chronogram\ntree <- read.nexus(""chronogram_v8_2.tre"")\nplot(tree, cex=0.4)\n\n# floral Dim characters\ndata <- read.csv(""famd_coords.csv"",row.names=1)\n\n# match tree and data\n(name.check(tree, data) -> phyOverlap)\ndrop.tip(tree, phyOverlap$tree_not_data) -> phyComparativeTree\ndata <- data[!(row.names(data)  %in% phyOverlap$data_not_tree), ]\n\n# Prep tree and data to meet requirements for estimate_shift_configuration\n#dim1\ndat <- as.data.frame(data[,2]) #dim 1 -5 (each explains >5% of variation in floral space)\nrownames(dat) <- rownames(data)\nmyadj.dim1 <- adjust_data(phyComparativeTree, dat,normalize = TRUE, quietly = FALSE)\n#dim2\ndat <- as.data.frame(data[,3]) #dim 1 -5 (each explains >5% of variation in floral space)\nrownames(dat) <- rownames(data)\nmyadj.dim2 <- adjust_data(phyComparativeTree, dat,normalize = TRUE, quietly = FALSE)\n\n#detect evolutionary shifts under OU model and get convergent regimes\n#Dim1\n(eModel <- estimate_shift_configuration(myadj.dim1$tree, myadj.dim1$Y, \n                criterion=""AICc"", alpha.upper = 100))\n(fit_conv <- estimate_convergent_regimes(eModel,criterion=""pBIC""))\npdf(""FigOUConvergenceDim1.pdf"", width=10, height=10)\nplot(fit_conv)\ndev.off()\n\n#Dim2\n(eModel <- estimate_shift_configuration(myadj.dim2$tree, myadj.dim2$Y, criterion=""AICc"", alpha.upper=500))\n(fit_conv <- estimate_convergent_regimes(eModel,criterion=""AICc""))\npdf(""FigOUConvergenceDim2.pdf"", width=10, height=10)\nplot(fit_conv)\ndev.off()\n', '# This R script tests whether there is an association between \n# pollination syndrome and species\' median elevation or climate \n# niche using phylogenetic anova.  It uses the phylogenetic tree \n# file ""chronogram_v8_2.tre"" and the data file ""species.elev.csv"" \n\nlibrary(geiger)\nlibrary(phytools) #for phylANOVA\nlibrary(car) #levene\'s test\n\nsetwd(""~/Dropbox/Costus grant/Dena and Kathleen/FloralEvolution/Dryad_Deposit"")\n\n# chronogram\nphy <- read.nexus(""chronogram_v8_2.tre"")\nplot(phy)\n\n#import species elev traits\nphy.data <- read.csv(""species.elev.csv"")\nrownames(phy.data) <- phy.data[,c(""species.phyname"")]\n\n#occurrence record stats\nocc.data <- read.csv(""occurrences.elev.csv"")\nocc.data.summary<- as.data.frame(table(occ.data$acceptedScientificName))\nnames(occ.data.summary) <- c(""acceptedScientificName"", ""N_occ"")\ntmp <- merge(phy.data, occ.data.summary, all.x=T, all.y=F, by=""acceptedScientificName"")\nsum(tmp$N_occ) #total number of occurrences used\nnrow(tmp) #number taxa\nrange(tmp$N_occ) #range per taxa\nmean(tmp$N_occ) #mean per taxa\n\n# use random forest predicted syndromes\nphy.data[""Costus_dirzoi_98079"",""syndrome""]<-""H""\nphy.data[""Costus_varzearum_19252"",""syndrome""]<-""H""\n\n(name.check(phy, phy.data) -> phyOverlap)\ndrop.tip(phy, phyOverlap$tree_not_data) -> phyComparativeTree\nplot(phyComparativeTree, cex=0.6)\n\nattach(phy.data)\n\n###\n###PHY ANOVA on elevation and climate traits\n###\n\n#ELEV\ndat = phy.data$elev_median\ndat = as.data.frame(dat)\nrownames(dat) = phy.data$species.phyname\ndat = na.omit(dat)\ngrp = as.factor(phy.data$syndrome)\nnames(grp)=phy.data$species.phyname\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n# Analysis of Variance Table\n#\n# Response: dat\n# Df  Sum-Sq Mean-Sq F-value  Pr(>F) Pr(>F) given phy\n# group      1  125105  125105 0.63968 0.42803           0.6863\n# Residuals 45 8800820  195574 \n\nhum <- subset(phy.data, syndrome==""H"")\nbee <- subset(phy.data, syndrome==""B"")\nrange(hum$elev_median)\nrange(bee$elev_median)\ntable(phy.data$syndrome)\n\n#climate PC1\ndat = phy.data$PC1\ndat = as.data.frame(dat)\nrownames(dat) = phy.data$species.phyname\ndat = na.omit(dat)\ngrp = as.factor(phy.data$syndrome)\nnames(grp)=phy.data$species.phyname\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n# Analysis of Variance Table\n#\n# Response: dat\n# Df  Sum-Sq Mean-Sq F-value  Pr(>F) Pr(>F) given phy\n# group      1  1.1281 1.12806  2.1961 0.14533           0.4825\n# Residuals 45 23.1147 0.51366       \n\n#climate PC2\ndat = phy.data$PC2\ndat = as.data.frame(dat)\nrownames(dat) = phy.data$species.phyname\ndat = na.omit(dat)\ngrp = as.factor(phy.data$syndrome)\nnames(grp)=phy.data$species.phyname\nt=aov.phylo(dat ~ grp, phyComparativeTree) #not significant\n# Analysis of Variance Table\n#\n# Response: dat\n# Df  Sum-Sq Mean-Sq F-value  Pr(>F) Pr(>F) given phy\n# group      1  0.1287 0.12870 0.29961 0.58683           0.7912\n# Residuals 45 19.3307 0.42957         \n\nleveneTest(phy.data$elev_median ~ phy.data$syndrome)\nleveneTest(phy.data$PC1 ~ phy.data$syndrome)\nleveneTest(phy.data$PC2 ~ phy.data$syndrome)\n\n####\n## PLOT\n####\n\npdf(file=""Fig6.pdf"", width=8,height=3.5)\npar(mfrow=c(1,3))\nboxplot(phy.data$elev_median ~ phy.data$syndrome, col=c(""#1F639B"",""#ED553B""),ylab=""Elevation (m)"", horizontal=F, names=c(""Orchid bee"",""Hummingbird""), varwidth=T, xlab="""", cex.lab=1.2)\nmtext(""A"", side=3, adj=0, cex=0.8)\nboxplot(phy.data$PC1 ~ phy.data$syndrome, col=c(""#1F639B"",""#ED553B""),ylab=""Climate PC1"", horizontal=F, names=c(""Orchid bee"",""Hummingbird""), varwidth=T, xlab="""", cex.lab=1.2)\nmtext(""B"", side=3, adj=0, cex=0.8)\nboxplot(phy.data$PC2 ~ phy.data$syndrome, col=c(""#1F639B"",""#ED553B""),ylab=""Climate PC2"", horizontal=F, names=c(""Orchid bee"",""Hummingbird""), varwidth=T, xlab="""", cex.lab=1.2)\nmtext(""C"", side=3, adj=0, cex=0.8)\ndev.off()\n']",4,"Evolutionary convergence, hummingbird pollination, Neotropical Costus, pollinator shifts, angiosperm lineages, convergence in traits, convergence in environments, complex phenotypes, bee pollination, multiple independent shifts, floral traits, observed"
Anticipatory plasticity: frog embryos respond to environmental cues by producing an adaptive phenotype at hatching,"Developmental plasticity can occur at any life stage, but a context in which it might be crucial is when individuals that produce specific phenotypes early in development gain a competitive advantage at a later life stage. Here we asked if pre-hatching (embryonic) exposure to a nutrient-rich resource can impact hatchling morphology in tadpoles of Mexican spadefoot toads, Spea multiplicata. Induction of a distinctive carnivore morph can occur when a tadpole eats live fairy shrimp. We investigated whether cues from fairy shrimp, detected as embryos, determine hatchling morphology in a manner allowing individuals to take advantage of this nutritious resource. We found that hatchlings with embryonic exposure to shrimp were larger and had larger jaw musclestraits that increase their ability to compete for shrimp. Thus, embryos can assess and respond to environmental cues by producing preemptive resource-use phenotypes. Such anticipatory plasticity may be an important but understudied form of developmental plasticity.","['#This is code to replicate the analyses and figures from my 2023 Egg Priming paper. Code developed\r\n#by Emily Harmon\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\nlibrary(lme4)\r\nlibrary(lmerTest)\r\nlibrary(effects)\r\nlibrary(tidyr)\r\nlibrary(ggforce)\r\n\r\n#Read in the data\r\ntads <- read.csv(""EmbryonicPlasticity.csv"")\r\ntads$Family <- factor(tads$Family)\r\ntads$Round <- factor(tads$Round)\r\ntads$Box <- factor(tads$Box)\r\ntads$OH_L <- as.numeric(tads$OH_L)\r\ntads$OH_R <- as.numeric(tads$OH_R)\r\ntads$GutLength <- as.numeric(tads$GutLength)\r\n\r\n#Does treatment impact SVL at hatching?_________________________________________\r\n\r\nsvlmod <- lmer(SVL~Treatment + Round +(1|Family/Box), data=tads)\r\nsummary(svlmod)#Round has no blocking effect\r\n\r\nsvlmod1 <-lmer(SVL~Treatment + (1|Family/Box), data=tads)\r\nsummary(svlmod1)\r\nplot(allEffects(svlmod1))\r\ndrop1(svlmod1, ddf=""Kenward-Roger"")\r\n\r\n#SVL figure\r\nsvl.summary<-group_by(tads,Treatment) %>%\r\n  dplyr::summarise(\r\n    sd=sd(SVL),\r\n    se=sd/sqrt(n()),\r\n    SVL=mean(SVL))\r\nggplot(tads, aes(x=Treatment, y=SVL))+\r\n  geom_sina(aes(color=Treatment), maxwidth=.7, alpha=.5, show.legend = F)+\r\n  geom_errorbar(aes(ymin=SVL-se, ymax=SVL+se), data=svl.summary, width=.2, color=""black"", size=1)+\r\n  geom_point(data=svl.summary, size=4, shape=18)+\r\n  theme_classic(base_size=20)+\r\n  ylab(NULL)+\r\n  scale_x_discrete(labels = NULL, breaks = NULL) + labs(x = NULL)+\r\n  scale_color_manual(values=c(""#0C7BDC"", ""#FFA60A""))\r\n\r\n\r\n#Does treatment impact Gosner stage at hatching?________________________________\r\n\r\nGosmod <- lmer(log(Gosner)~Treatment + Round + (1|Family/Box), data=tads)\r\nsummary(Gosmod) #Round has no blocking effect\r\n\r\nGosmod1 <- lmer(Gosner~Treatment + (1|Family/Box), data=tads)\r\nsummary(Gosmod1)\r\nplot(allEffects(Gosmod1))\r\ndrop1(Gosmod1, ddf=""Kenward-Roger"")\r\n\r\n#Gosner stage figure\r\ngos.summary<-group_by(tads,Treatment) %>%\r\n  dplyr::summarise(\r\n    sd=sd(Gosner),\r\n    se=sd/sqrt(n()),\r\n    Gosner=mean(Gosner))\r\nggplot(tads, aes(x=Treatment, y=Gosner))+\r\n  geom_sina(aes(color=Treatment), maxwidth=.7, alpha=.5, show.legend=F)+\r\n  geom_errorbar(aes(ymin=Gosner-se, ymax=Gosner+se), data=gos.summary, width=.2, color=""black"", size=1)+\r\n  geom_point(data=gos.summary, size=4, shape=18)+\r\n  theme_classic(base_size=20)+\r\n  ylab(NULL)+\r\n  scale_x_discrete(labels = NULL, breaks = NULL) + labs(x = NULL)+\r\n  scale_color_manual(values=c(""#0C7BDC"", ""#FFA60A""))\r\n\r\n\r\n#Does treatment impact jaw width at hatching?___________________________________\r\n\r\n#New dataset with the size-corrected OH jaw measurement\r\ntads <- mutate(tads, OH.avg = (OH_L+OH_R)/2)\r\nOHtads <- tads %>% drop_na(OH.avg)\r\nOHtads <- OHtads%>%\r\n  mutate(OH.SVL= resid(lm(log(OH.avg)~log(SVL))))\r\n\r\njawmod <- lmer(OH.SVL~Treatment + Round+(1|Family/Box), data=OHtads)\r\nsummary(jawmod)#Round has no blocking effect\r\n\r\njawmod1 <-lmer(OH.SVL ~ Treatment + (1|Family/Box), data=OHtads)\r\nsummary(jawmod1)\r\nplot(allEffects(jawmod1))\r\ndrop1(jawmod1, ddf=""Kenward-Roger"")\r\n\r\n#Jaw width figure\r\njaw.summary<-group_by(OHtads,Treatment) %>%\r\n  dplyr::summarise(\r\n    sd=sd(OH.SVL),\r\n    se=sd/sqrt(n()),\r\n    OH.SVL=mean(OH.SVL))\r\nggplot(OHtads, aes(x=Treatment, y=OH.SVL))+\r\n  geom_sina(aes(color=Treatment), maxwidth=.7, alpha=.5, show.legend=F)+\r\n  geom_errorbar(aes(ymin=OH.SVL-se, ymax=OH.SVL+se), data=jaw.summary, width=.2, color=""black"", size=1)+\r\n  geom_point(data=jaw.summary, size=4, shape=18)+\r\n  theme_classic(base_size=20)+\r\n  ylab(NULL)+\r\n  scale_x_discrete(labels = NULL, breaks = NULL) + labs(x = NULL)+\r\n  scale_color_manual(values=c(""#0C7BDC"", ""#FFA60A""))\r\n\r\n\r\n#Does treatment impact gut length at hatching?__________________________________\r\n\r\n#New dataset with the size-corrected gut length\r\ngltads <- tads %>% drop_na(GutLength)\r\ngltads <- gltads%>%\r\n  mutate(Gut.SVL= resid(lm(log(GutLength)~log(SVL))))\r\n\r\ngutmod <- lmer(Gut.SVL~Treatment + Round + (1|Family/Box), data=gltads)\r\nsummary(gutmod)#Round has no blocking effect\r\n\r\ngutmod1 <- lmer(Gut.SVL~Treatment + (1|Family/Box), data=gltads)\r\nsummary(gutmod1)\r\nplot(allEffects(gutmod1))\r\ndrop1(gutmod1, ddf=""Kenward-Roger"")\r\n\r\n#Gut length figure\r\ngut.summary<-group_by(gltads,Treatment) %>%\r\n  dplyr::summarise(\r\n    sd=sd(Gut.SVL),\r\n    se=sd/sqrt(n()),\r\n    Gut.SVL=mean(Gut.SVL))\r\nggplot(gltads, aes(x=Treatment, y=Gut.SVL))+\r\n  geom_sina(aes(color=Treatment),maxwidth=.7, alpha=.5, show.legend=F)+\r\n  geom_errorbar(aes(ymin=Gut.SVL-se, ymax=Gut.SVL+se), data=gut.summary, width=.2, color=""black"", size=1)+\r\n  geom_point(data=gut.summary, size=4, shape=18)+\r\n  theme_classic(base_size=20)+\r\n  ylab(NULL)+\r\n  scale_x_discrete(labels = NULL, breaks = NULL) + labs(x = NULL)+\r\n  scale_color_manual(values=c(""#0C7BDC"", ""#FFA60A""))\r\n']",4,"Anticipatory plasticity, frog embryos, environmental cues, adaptive phenotype, hatching, developmental plasticity, nutrient-rich resource, hatchling morphology, Mexican spadefoot toads, Spea multiplicata, carnivore morph, fairy shrimp,"
Simultaneous integration and modularity underlie the exceptional body shape diversification of characiform fishes,"Evolutionary biology has long striven to understand why some lineages diversify exceptionally while others do not. Most studies have focused on how extrinsic factors can promote differences in diversification dynamics, but a clade's intrinsic modularity and integration can also catalyze or restrict its evolution. Here, we integrate geometric morphometrics, phylogenetic comparative methods and visualizations of covariance to infer the presence of distinct modules in the body plan of Characiformes, an ecomorphologically diverse fish radiation. Strong covariances reveal a cranial module, and more subtle patterns support a statistically significant subdivision of the postcranium into anterior (precaudal) and posterior (caudal) modules. We uncover substantial covariation among cranial and postcranial landmarks, indicating body-wide evolutionary integration as lineages transition between compressiform and fusiform body shapes. A novel method of matrix subdivision reveals that within- and among-module covariation contributes substantially to the overall eigenstructure of characiform morphospace, and that both phenomena led to biologically important divergence among characiform lineages. Functional integration between the cranium and post-cranial skeleton appears to have allowed lineages to optimize the aspect ratio of their bodies for locomotion, while the capacity for independent change in the head, body and tail likely eased adaptation to diverse dietary and hydrological regimes. These results reinforce a growing consensus that modularity and integration synergize to promote diversification.","['#########################################\r\n#          Load Data\r\n#########################################\r\n\r\nrequire(geomorph)\r\nrequire(ape)\r\n\r\nsource(""Buseretal2017_customfunctions.R"") #code to take averages code from https://doi.org/10.5061/dryad.2p4k0\r\n\r\ncoords <- readland.tps(file = ""Morphological Data.TPS"", specID = ""ID"") #TPS file with landmark data\r\n\r\nclassifier <- read.csv(file = ""Classifiers.csv"") # Classifier used to take averages\r\n\r\nspecies<-read.csv(""species.csv"", row.names=1) # list of species names\r\n\r\nphy<-read.nexus(""tree_Burns et al 2019.nexus"")\r\n\r\n####################################################\r\n# Seperate procrustes alignments: Two modules\r\n####################################################\r\n\r\nY.coords<-two.d.array(coords)\r\n\r\nhd <- Y.coords[,c(1:8,29:42)] #cranial module\r\n\r\nbod<- Y.coords[,c(9:28,43:48)] # post-cranial module\r\n\r\n\r\nhd.array<-arrayspecs(hd,11,2) \r\nbod.array<-arrayspecs(bod,13,2)\r\n\r\nhd.aligned<-gpagen(hd.array) #procrustes alignment\r\nbod.aligned<-gpagen(bod.array) #procrustes alignment\r\n\r\n\r\navg.aligned.hd<-my.landmark.species.average(classifier = classifier, aligned.coords = hd.aligned$coords) #calculating average landmark position across all individuals per species\r\n\r\navg.aligned.bod<-my.landmark.species.average(classifier = classifier, aligned.coords = bod.aligned$coords) #calculating average landmark position across all individuals per species\r\n\r\n\r\n####################################################\r\n# Seperate procrustes alignments: Three modules\r\n####################################################\r\n\r\nY.coords<-two.d.array(coords)\r\n\r\nhd<-Y.coords[,c(1:8,29:42)] #cranial module\r\nabd<-Y.coords[,c(9:12,23:28,43:48)] #abdominal module\r\ntl<-Y.coords[,c(13:22)] #caudal module\r\n\r\nhd.array<-arrayspecs(hd,11,2)\r\nabd.array<-arrayspecs(abd,8,2)\r\ntl.array<-arrayspecs(tl,5,2)\r\n\r\nhd.aligned<-gpagen(hd.array)\r\nabd.aligned<-gpagen(abd.array)\r\ntl.aligned<-gpagen(tl.array)\r\n\r\navg.aligned.hd<-my.landmark.species.average(classifier = classifier, aligned.coords = hd.aligned$coords)\r\n\r\navg.aligned.abd<-my.landmark.species.average(classifier = classifier, aligned.coords = abd.aligned$coords)\r\n\r\navg.aligned.tl<-my.landmark.species.average(classifier = classifier, aligned.coords = tl.aligned$coords)\r\n\r\n###############################################################\r\n# Removing species from the Citharinoidei from the alingments\r\n################################################################\r\ncith<- c(""13_Citharinus_sp"", ""13_Distichodus_decemmaculatus"", ""13_Distichodus_fasciolatus"", ""13_Hemigrammocharax"",""13_Ichthyborus_sp"", ""13_Neolebias_trilineatus"")\r\n\r\nd<-rownames(species)\r\n\r\nintersection<-d %in% cith\r\n\r\nf<-which(intersection== TRUE)\r\n\r\ne<-c(31,42,43,51,59,77)\r\n\r\navg.aligned.abd.characoid<-avg.aligned.abd[,,-e]\r\n\r\navg.aligned.hd.characoid<-avg.aligned.hd[,,-e]\r\n\r\navg.aligned.tl.characoid<-avg.aligned.tl[,,-e]\r\n\r\navg.aligned.bod.characoid<-avg.aligned.bod[,,-e]\r\n\r\n\r\nspecies.cith<-species[-f,]\r\n\r\n\r\nphy.characoid <- drop.tip(phy, cith)\r\n\r\n################################\r\n# Specify module configuration\r\n################################\r\nland.gp2<-c(""A"", ""A"", ""A"", ""A"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""A"", ""A"", ""A"", ""A"", ""A"", \r\n\t   ""A"", ""A"", ""B"", ""B"",""B"" ) # two module hypothesis\r\n\r\nland.gp3 <- c(""A"", ""A"", ""A"", ""A"", ""B"", ""B"", ""C"", ""C"", ""C"", ""C"", ""C"", ""B"", ""B"", ""B"", ""A"", ""A"", ""A"", ""A"", \r\n            ""A"", ""A"", ""A"", ""B"", ""B"", ""B"") #three module hypothesis\r\n\r\n############################################################################################################\r\n# Testing Phylogenetic Modularity and comparing the strength of modularity between landmark configurations\r\n############################################################################################################\r\npm3<-phylo.modularity(avg.aligned.hd,land.gp3,phy=phy, iter=9999, print.progress = FALSE)\r\npm3\r\npm2<-phylo.modularity(avg.aligned,land.gp2, phy=phy, iter=9999, print.progress = FALSE)\r\npm2\r\n\r\n\r\npmodel.Z<-compare.CR(pm2,pm3, CR.null = TRUE) #compare strength of modularity between modular hypotheses\r\n\r\nsummary(pmodel.Z)\r\n\r\n##################\r\n#Characoidei only\r\n##################\r\npm3.characoid<-phylo.modularity(avg.aligned.characoid,land.gp3,phy=phy.characoid, iter=9999, print.progress = FALSE)\r\npm3.characoid\r\npm2.characoid<-phylo.modularity(avg.aligned.characoid,land.gp2, phy=phy.characoid, iter=9999, print.progress = FALSE)\r\npm2.characoid\r\n\r\n\r\npmodel.Z.characoid<-compare.CR(pm2,pm3, CR.null = TRUE) #compare strength of modularity between modular hypotheses\r\n\r\nsummary(pmodel.Z.characoid)\r\n\r\n######################################################################################################################\r\n## Phylogenetic integration tests between all pairwise comparisons of modules \r\n\r\n#We need to perform pairwise PLS analysis to determine whether all of the modules exhibit integrated evolution.\r\n####################################################################################################', '#########################################\r\n#          Load Data\r\n#########################################\r\n\r\nrequire(geomorph)\r\nrequire(ape)\r\n\r\nsource(""Buseretal2017_customfunctions.R"") #code to take averages code from https://doi.org/10.5061/dryad.2p4k0\r\n\r\ncoords <- readland.tps(file = ""Morphological Data.TPS"", specID = ""ID"") #TPS file with landmark data\r\n\r\nclassifier <- read.csv(file = ""Classifiers.csv"") # Classifier used to take averages\r\n\r\nspecies<-read.csv(""species.csv"", row.names=1) # list of species names\r\n\r\nphy<-read.nexus(""tree_Burns et al 2019.nexus"")\r\n\r\n###############################################################\r\n# Removing species from the Citharinoidei from the alingments\r\n################################################################\r\ncith<- c(""13_Citharinus_sp"", ""13_Distichodus_decemmaculatus"", ""13_Distichodus_fasciolatus"", ""13_Hemigrammocharax"",""13_Ichthyborus_sp"", ""13_Neolebias_trilineatus"")\r\n\r\nd<-rownames(species)\r\n\r\nintersection<-d %in% cith\r\n\r\nf<-which(intersection== TRUE)\r\n\r\ne<-c(31,42,43,51,59,77)\r\n\r\navg.aligned.characoid<-avg.aligned[,,-e]\r\n\r\nspecies.cith<-species[-f,]\r\n\r\nphy.characoid <- drop.tip(phy, cith)\r\n\r\n################################################################################\r\n# Specify module configuration\r\n###################################################################################\r\nland.gp2<-c(""A"", ""A"", ""A"", ""A"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""A"", ""A"", ""A"", ""A"", ""A"", \r\n\t   ""A"", ""A"", ""B"", ""B"",""B"" ) # two module hypothesis\r\n\r\nland.gp3 <- c(""A"", ""A"", ""A"", ""A"", ""B"", ""B"", ""C"", ""C"", ""C"", ""C"", ""C"", ""B"", ""B"", ""B"", ""A"", ""A"", ""A"", ""A"", \r\n            ""A"", ""A"", ""A"", ""B"", ""B"", ""B"") #three module hypothesis\r\n\r\n############################################################################################################\r\n# Testing Phylogenetic Modularity and comparing the strength of modularity between landmark configurations\r\n############################################################################################################\r\n\r\npm3<-phylo.modularity(avg.aligned,land.gp3,phy=phy, iter=9999, print.progress = FALSE)\r\npm3\r\npm2<-phylo.modularity(avg.aligned,land.gp2, phy=phy, iter=9999, print.progress = FALSE)\r\npm2\r\n\r\n\r\npmodel.Z<-compare.CR(pm2,pm3, CR.null = TRUE) #compare strength of modularity between modular hypotheses\r\n\r\nsummary(pmodel.Z)\r\n\r\n##################\r\n#Characoidei only\r\n##################\r\n\r\npm3.characoid<-phylo.modularity(avg.aligned.characoid,land.gp3,phy=phy.characoid, iter=9999, print.progress = FALSE)\r\npm3.characoid\r\npm2.characoid<-phylo.modularity(avg.aligned.characoid,land.gp2, phy=phy.characoid, iter=9999, print.progress = FALSE)\r\npm2.characoid\r\n\r\n\r\npmodel.Z.characoid<-compare.CR(pm2,pm3, CR.null = TRUE) #compare strength of modularity between modular hypotheses\r\n\r\nsummary(pmodel.Z.characoid)\r\n\r\n######################################################################################################################\r\n## Phylogenetic integration tests between all pairwise comparisons of modules \r\n\r\n#We need to perform pairwise PLS analysis to determine whether all of the modules exhibit integrated evolution.\r\n#####################################################################################################################\r\n\r\n################################################################### \r\n#Integration between cranial module and post-cranial body module\r\n###################################################################\r\n#All species\r\n#subset landmarks into modules\r\nY.avg<-two.d.array(avg.aligned)\r\nhd<-Y.avg[,c(1:8,29:42)]\r\nbod<-Y.avg[,c(9:22,23:28,43:48)]\r\n\r\n\r\nhd.array<-arrayspecs(hd,11,2)\r\nbod.array<-arrayspecs(bod,13,2)\r\n\r\n# Integration tests\r\npls.hd.bod<-phylo.integration(hd.array, bod.array, phy=phy, iter=9999, print.progress = FALSE)\r\nsummary(pls.hd.bod)\r\nplot(pls.hd.bod)\r\n\r\n#Characoidei only\r\n#subset landmarks into modules\r\nY.avg.characoid<-two.d.array(avg.aligned.characoid)\r\nhd.characoid<-Y.avg.characoid[,c(1:8,29:42)]\r\nbod.characoid<-Y.avg.characoid[,c(9:22,23:28,43:48)]\r\n\r\nhd.array.characoid<-arrayspecs(hd.characoid,11,2)\r\nbod.array.characoid<-arrayspecs(bod.characoid,13,2)\r\n\r\n# Integration tests\r\npls.hd.bod.characoid<-phylo.integration(hd.array.characoid, bod.array.characoid, phy=phy.characoid, iter=9999, print.progress = FALSE)\r\nsummary(pls.hd.bod.characoid)\r\nplot(pls.hd.bod.characoid)\r\n\r\n############################################################## \r\n#Integration between cranial, abdominal, and caudal modules\r\n################################################################\r\n#All species\r\n#subset landmarks into modules\r\nY.avg<-two.d.array(avg.aligned)\r\nhd<-Y.avg[,c(1:8,29:42)]\r\nabd<-Y.avg[,c(9:12,23:28,43:48)]\r\ntl<-Y.avg[,c(13:22)]\r\n\r\nhd.array<-arrayspecs(hd,11,2)\r\nabd.array<-arrayspecs(abd,8,2)\r\ntl.array<-arrayspecs(tl,5,2)\r\n\r\n# Integration tests\r\n\r\npls.hd.abd<-phylo.integration(hd.array, abd.array, phy=phy, iter=9999, print.progress = FALSE)\r\nsummary(pls.hd.abd)\r\nplot(pls.hd.abd)\r\n\r\npls.hd.tl<-phylo.integ']",4,"evolutionary biology, diversification, intrinsic modularity, integration, Characiformes, geometric morphometrics, phylogenetic comparative methods, covariance, cranial module, postcranium, compressiform, fusiform, morphospace, aspect ratio"
Data from: Using metabolic theory to describe temperature and thermal acclimation effects on parasitic infection,"Predicting temperature effects on species interactions can be challenging, especially for parasitism where it is difficult to experimentally separate host and parasite thermal performance curves. Prior authors proposed a possible solution based on the metabolic theory of ecology (MTE), using MTE-based equations to describe the thermal mismatch between host and parasite performance curves and account for thermal acclimation responses. Here we use published infection data, supplemented with experiments measuring metabolic responses to temperature in each species, to show that this modeling framework can successfully describe thermal acclimation effects on two different stages of infection in a tadpole-trematode system. All thermal acclimation effects on host performance manifested as changes in one key model parameter (activation energy), with measurements of host respiration generating similar MTE parameter estimates and acclimation effects compared to measurements of the host's ability to clear encysted parasites. This result suggests that metabolic parameter estimates for whole-body metabolism can sometimes be used to estimate temperature effects on host and parasite performance curves. However, we found different thermal patterns for measurements of host prevention of initial parasite encystment emphasizing potential challenges when applying MTE-based models to complex parasite-host systems with multiple distinct stages of infection.","['# Activation_energy_bootstrap_Sckrabulis_et_al_2021_AmNat.R\r\n\r\n# Bootstrapped activation energy (Ea) acclimation effects\r\n# Predictions and confidence intervals generated here were combined into a single .csv file (we make our generated dataset available as \'Activation energy bootstrap - Sckrabulis et al 2021 AmNat.csv\', but due to random sampling that data cannot be exactly reproduced)\r\n\r\n# Clearance Data (reduced to 4 columns for centered Acc effect: 1=PerfTemp, 2=Response, 3=AccTemp, 4=AccCenter) for use in bootstrap function\r\n# Use Altman et al. 2016 data here as a .csv\r\n# Reference: Altman, K. A., S. H. Paull, P. T. J. Johnson, M. N. Golembieski, J. P. Stephens, B. E. LaFonte, and T. R. Raffel. 2016. Host and parasite thermal acclimation responses depend on the stage of infection. Journal of Animal Ecology 85(4): 1014-1024. doi:10.1111/1365-2656.12510\r\n# Dryad repository doi: 10.5061/dryad.f3k8p\r\npropClearData2 <- read.csv(file.choose(), header=TRUE)\r\npropClearData2 <- subset(propClearData2, PropCleared1!=""NA"")\r\npropClearData2$AccCenter <- propClearData2$AccTemp-mean(propClearData2$AccTemp)\r\nreducedClear <- cbind(perf=propClearData2$PerfTemp, response=propClearData2$PropCleared1, acc=propClearData2$AccTemp, center=propClearData2$AccCenter)\r\n\r\n# Respiration (reduced to 4 columns for centered Acc effect: 1=PerfTemp, 2=Response, 3=AccTemp, 4=AccCenter) for use in bootstrap function\r\n# Use \'Uninfected_tadpole_respiration_Sckrabulis_et_al_2021_AmNat.csv\' in this repository\r\nrespData2 <- read.csv(file.choose(), header=TRUE)\r\nrespData2 <- subset(respData2, AccTemp!=""NA"")\r\nrespData2$AccCenter <- respData2$AccTemp-mean(respData2$AccTemp)\r\nreducedResp <- cbind(perf=respData2$PerfTemp, response=respData2$corO2.Time.Mass, acc=respData2$AccTemp, center=respData2$AccCenter)\r\n\r\n# Function to bootstrap SS model data for quadratic acclimation effects (centered) on Ea with the arguments:\r\n# data = dataset to bootstrap\r\n# iterations = number of iterations for bootstrap sampling\r\n# To = value for T0 from optimized final model\r\n# starting = vector of starting values for parameters\r\n\r\nbootEaCentered <- function(data, iterations, To, starting){\r\n\tplot(data[,1], data[,2])\r\n\t\r\n\t# Vector of x values for the entire range of AccTemp\r\n\tciX<-seq(from=min(data[,4]), to=max(data[,4]),by=.1)\r\n\r\n\t# Number of columns for predictions and for complete dataframe (3 parameters in quadratic model)\r\n\tcolPredict <- length(ciX)\r\n\tcolTotal <- colPredict+3\r\n\tn <- length(data[,1])\r\n\t\r\n\t# Empty dataframe for storing parameters and predictions\r\n\tparams <- data.frame(matrix(ncol=colTotal, nrow=iterations))\r\n\tconf_interval <- data.frame(matrix(ncol=3, nrow=length(ciX)))\r\n\tnames(params) <- c(""b"", ""m"", ""q"")\r\n\r\n\t# NLS model equation as a function\r\n\t# We replace \'k\' from all other .R files with \'boltz\' here to facilitate for() loop nomenclature\r\n\tquadSS <- function(x, Tacc, A, bE, mE, qE, Th){\r\n\t\tK <- 273.15\r\n\t\tboltz <- 8.62*10^-5\r\n\t\tT0 <- To+K\r\n\t\tEd <- 3.25\r\n\t\tEa <- function(Tacc,bE,mE,qE){\r\n\t\t\tEa <- bE+mE*(Tacc)+qE*(Tacc)^2\r\n\t\t}\r\n\t\ty <- A*exp(-Ea(Tacc,bE,mE,qE)/boltz*(1/(x+K)-1/T0))*(1+exp(Ed/boltz*(1/Th-1/(x+K))))^-1\r\n\t}\r\n\r\n\t# For loop to iterate bootstrap and predictions until X iterations are complete (accounting for convergence errors)\r\n\tfor(i in 1:iterations){\r\n\t\tskip <- FALSE\r\n\t\t\r\n\t\t# Empty dataframe and generate bootstrap data with replacement\r\n\t\tbootData <- data.frame(matrix(ncol=4, nrow=n))\r\n\t\tbootRows <- sample(1:length(data[,1]), size=length(data[,1]), replace=TRUE)\r\n\r\n\t\t# For loop to insert data into bootData for modeling in next step\r\n\t\tfor(j in 1:length(bootRows)){\r\n\t\t\tbootData[j,] <- data[bootRows[j],]\r\n\t\t}\r\n\t\tnames(bootData) <- c(""perf"", ""response"", ""acc"", ""center"")\r\n\t\tplot(bootData[,1], bootData[,2])\r\n\r\n\t\t# Wrap model fitting in error catching function to circumvent fatal convergence errors and warnings\r\n\t\tattempt <- tryCatch(\r\n\t\t\t{\r\n\t\t\t\tmodel <- nls(response ~ quadSS(x=perf, Tacc=center, A, bE, mE, qE, Th), start=starting, data=bootData, control=c(warnOnly=TRUE))\r\n\t\t\t\tsummary(model)\r\n\t\t\t},\r\n\t\t\terror = function(e){\r\n\t\t\t\tskip <<- TRUE\r\n\t\t\t}\r\n\t\t)\r\n\t\t\t\r\n\t\tif(skip == TRUE){\r\n\t\t\tparams[i,] <- ""NA""\r\n\t\t\tnext\r\n\t\t} else if(skip == FALSE){\r\n\t\t\tmodel <- nls(response ~ quadSS(x=perf, Tacc=center, A, bE, mE, qE, Th), start=starting, data=bootData, control=c(warnOnly=TRUE))\r\n\t\t\tparams[i,1] <- summary(model)$coef[2]\r\n\t\t\tparams[i,2] <- summary(model)$coef[3]\r\n\t\t\tparams[i,3] <- summary(model)$coef[4]\r\n\t\t\t\r\n\t\t\t# Predict Ea curve over entire range of ciX \r\n\t\t\tfor(k in 1:length(ciX)){\r\n\t\t\t\tparams[i,k+3] <- summary(model)$coef[2]+summary(model)$coef[3]*(ciX[k])+summary(model)$coef[4]*(ciX[k])^2\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t# Calculate confidence intervals for each new value in ciX\r\n\tfor(p in 1:colPredict){\r\n\t\tnewParams <- subset(params, params$b!=""NA"")\r\n\t\tvalues <- as.numeric(newParams[,p+3])\r\n\t\tlowCI <- sort(values)[trunc(0.025*length(newParams[,p+3]))]\r\n\t\thighCI <- sort(values)[length(newParams[,p+3])-trunc(0.025*length(newParams[,p+3]))]\r\n\t\tconf_interval[p,1] <- c', ""# Cercaria_swimming_speed_Sckrabulis_et_al_2021_AmNat.R\r\n\r\n# Cercaria swimming speed analysis \r\n\r\n#####\r\n# Libraries\r\nlibrary(nls.multstart)\r\nlibrary(nlstools)\r\nlibrary(modelr)\r\nlibrary(broom)\r\nlibrary(purrr)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(Hmisc)\r\n\r\n# Constants\r\nk <- 8.62*10^-5\r\nK <- 273.15\r\n\r\n# Use 'Cercaria_swimming_speed_Sckrabulis_et_al_2021_AmNat.csv' in this repository\r\nswimData <- read.csv(file.choose(), header=TRUE)\r\nswimData$PerfTempK <- swimData$Temperature+K\r\nswimData$logavgSpeedMM <- log(swimData$avgSpeedMM+1)\r\nplot(swimData$Temperature, swimData$avgSpeedMM)\r\n\r\n#####\r\n# Xiao et al. 2011 AIC calculation\r\n# Reference: Xiao, X., E. P. White, M. B. Hooten, and S. L. Durham. 2011. On the use of log-transformation vs. nonlinear regression for analyzing biological power laws. Ecology 92: 1887-1894. doi:10.1890/11-0538.1\r\nAIC.Xiao <- function(numParams, loglik, n){\r\n\t2*numParams-2*loglik\r\n}\r\n\r\n#####\r\n# Testing for best fit error distribution\r\n\r\n#Boltzmann-Arrhenius equation\r\nswimBA <- function(x, iTo, Ei, Toi){\r\n\tToi <- Toi+K\r\n\ty <- iTo*exp(-Ei/k*(1/(x+K)-1/Toi))\r\n}\r\n\r\n# Model with normal error distribution\r\nBAnorm <- nls(avgSpeedMM ~ swimBA(x=Temperature, iTo, Ei, Toi=19),\tstart=c(iTo=1, Ei=.6), data=swimData, control=c(warnOnly=TRUE))\r\n\r\n# Plotting function using estimates from 'BAnorm' model\r\nnormPlotBA <- function(x){\r\n\tToi <- 19+K\r\n\tiTo <- 2.6159\r\n\tEi <- 0.09557\r\n\ty <- iTo*exp(-Ei/k*(1/(x+K)-1/Toi))\r\n}\r\n\r\n# Standard deviation and log-likelihood calculations\r\nsdnorm <- sd((swimData$avgSpeedMM)-normPlotBA(swimData$Temperature))\r\nll_norm <- sum(log(dnorm(swimData$avgSpeedMM, normPlotBA(swimData$Temperature), sdnorm)))\r\n\r\n# AIC calculation\r\nnormAIC <- AIC.Xiao(numParams=2, loglik=ll_norm, n=length(swimData$Temperature))\r\nnormAIC\r\n\r\n# Model with lognormal error error distribution\r\nBAlnorm <- nls(log(avgSpeedMM+1) ~ log(swimBA(x=Temperature, iTo, Ei, Toi=19)), start=c(iTo=1, Ei=.6), data=swimData, control=c(warnOnly=TRUE))\r\n\r\n# Plotting function using estimates from 'BAlnorm' model\r\nlnormPlotBA <- function(x){\r\n\tToi <- 19+K\r\n\tiTo <- 3.3505\r\n\tEi <- .05907\r\n\ty <- log(iTo*exp(-Ei/k*(1/(x+K)-1/Toi)))\r\n}\r\n\r\n# Standard deviation and log-likelihood calculations\r\nsdlnorm <- sd((swimData$logavgSpeedMM)-lnormPlotBA(swimData$Temperature))\r\nll_lnorm <- sum(log(dlnorm(swimData$avgSpeedMM+1, lnormPlotBA(swimData$Temperature), sdlnorm)))\r\n\r\n# AIC calculation\r\nlnormAIC <- AIC.Xiao(numParams=2, loglik=ll_lnorm, n=length(swimData$Temperature))\r\nlnormAIC\r\n\r\n#__\r\n# Sharpe-Schoolfield equation\r\nswimSS <- function(x, iTo, Ei, Ehi, Thi, Toi){\r\n\tToi <- Toi+K\r\n\ty <- iTo*exp(-Ei/k*(1/(x+K)-1/Toi))*(1+exp(Ehi/k*(1/Thi-1/(x+K))))^-1\r\n}\r\n\r\n# Model with normal error distribution\r\nSSnorm <- nls(avgSpeedMM ~ swimSS(x=Temperature, iTo, Ei, Ehi, Thi, Toi=19), start=c(iTo=2, Ei=.6, Ehi=3, Thi=303), data=swimData, control=c(warnOnly=TRUE))\r\n\r\n# Plotting function using estimates from 'SSnorm' model\r\nnormPlotSS <- function(x){\r\n\tToi <- 19+K\r\n\tiTo <- 2.63454\r\n\tEi <- .52545\r\n\tEhi <- 4.24226\r\n\tThi <- 304.05194\r\n\ty <- iTo*exp(-Ei/k*(1/(x+K)-1/Toi))*(1+exp(Ehi/k*(1/Thi-1/(x+K))))^-1\r\n}\r\n\r\n# Standard deviation and log-likelihood calculations\r\nsdnorm <- sd((swimData$avgSpeedMM)-normPlotSS(swimData$Temperature))\r\nll_norm <- sum(log(dnorm(swimData$avgSpeedMM, normPlotSS(swimData$Temperature), sdnorm)))\r\n\r\n# AIC calculation\r\nnormAIC <- AIC.Xiao(numParams=4, loglik=ll_norm, n=length(SwimData$Temperature))\r\nnormAIC\r\n\r\n#Model with lognormal error distribution\r\nSSlnorm <- nls(log(avgSpeedMM+1) ~ log(swimSS(x=Temperature, iTo, Ei, Ehi, Thi, Toi=19)), start=c(iTo=2, Ei=.6, Ehi=3, Thi=303), data=swimData, control=c(warnOnly=TRUE))\r\n\r\n# Plotting function using estimates from 'SSlnorm' model\r\nlnormPlotSS <- function(x){\r\n\tToi <- 19+K\r\n\tiTo <- 3.5399\r\n\tEi <- .4073\r\n\tEhi <- 3.7302\r\n\tThi <- 304.7443\r\n\ty <- log(iTo*exp(-Ei/k*(1/(x+K)-1/Toi))*(1+exp(Ehi/k*(1/Thi-1/(x+K))))^-1)\r\n}\r\n\r\n# Standard deviation and log-likelihood calculations\r\nsdlnorm <- sd((swimData$logavgSpeedMM)-lnormPlotSS(swimData$Temperature))\r\nll_lnorm <- sum(log(dlnorm(swimData$avgSpeedMM+1, lnormPlotSS(swimData$Temperature), sdlnorm)))\r\n\r\n# AIC calculation\r\nlnormAIC <- AIC.Xiao(numParams=4, loglik=ll_lnorm, n=length(swimData$Temperature))\r\nlnormAIC\r\n\r\n#####\r\n# Using nls.multstart package to generate confidence bands (Padfield & Matheson 2018)\r\n# Reference: Padfield, D., and G. Matheson. 2018. Nls.multstart: An R package for robust non-linear regression using AIC scores. v1.0.0\r\nswimMult <- function(lniTo, Ei, Ehi, Thi, temp, Toi) {\r\n\tToi <- Toi+K\r\n\tboltzmann.term <- lniTo + log(exp(Ei/k*(1/Toi - 1/temp)))\r\n\tinactivation.term <- log(1/(1 + exp(Ehi/k*(1/Thi - 1/temp))))\r\n\treturn(boltzmann.term + inactivation.term)\r\n}\r\n\r\n# One fit\r\nfit <- nls_multstart(avgSpeedMM ~ swimMult(lniTo, Ei, Ehi, Thi, temp = PerfTempK, Toi = 19),\r\n\tdata = swimData,iter = 500,\r\n\tstart_lower = c(lniTo = -10, Ei = 0.1, Ehi = 0.2, Thi = 285),\r\n\tstart_upper = c(lniTo = 10, Ei = 2, Ehi = 5, Thi = 330),\r\n\tsupp_errors = '"", '# Metacercaria encystment analysis\r\n\r\n# Use Altman et al. 2016 data here as a .csv\r\n# Reference: Altman, K. A., S. H. Paull, P. T. J. Johnson, M. N. Golembieski, J. P. Stephens, B. E. LaFonte, and T. R. Raffel. 2016. Host and parasite thermal acclimation responses depend on the stage of infection. Journal of Animal Ecology 85(4): 1014-1024. doi:10.1111/1365-2656.12510\r\n# Dryad repository doi: 10.5061/dryad.f3k8p\r\ncystData <- read.csv(file.choose(), header=TRUE)\r\ncystData <- cystData[order(cystData$PerfTemp),]\r\ncystData$logAdjPropEncysted <- log(cystData$AdjPropEncysted)\r\ncystData$AccCenter <- cystData$AccTemp-mean(cystData$AccTemp)\r\n\r\n# Constants\r\nk<-8.62*10^-5\r\nK<-273.15\r\n\r\n#_\r\n# Test upper and lower thresholds on infectivity\r\n\r\n# Sharpe-Schoolfield infectivity functions with and without thresholding, using estimates from best fit model in \'Cercaria swimming speed.txt\'\r\n\r\n# No thresholds\r\ninfectNoThresh <- function(x, iTo){\r\n\tEi <- .40734\r\n\tEhi <- 3.73023\r\n\tThi <- 304.74427\r\n\tToi <- 19+K\r\n\ty <- iTo*exp(-(Ei/k*(1/(x+K)-1/Toi)))*(1+exp((Ehi/k*(1/Thi-1/(x+K)))))^-1\r\n}\r\n\r\n# Upper threshold (conditional programming) and lower threshold (through the use of translational constant C and conditional programming)\r\ninfectThresh <- function(x, iTo, C){\r\n\tEi <- .40734\r\n\tEhi <- 3.73023\r\n\tThi <- 304.74427\r\n\tToi <- 19+K\r\n\ty <- iTo*exp(-(Ei/k*(1/(x+K)-1/Toi)))*(1+exp((Ehi/k*(1/Thi-1/(x+K)))))^-1-C\r\n\tifelse(y>1, 1, ifelse(y<0,0,y))\r\n}\r\n\r\n# Boltzmann-Arrhenius resistance equation\r\nba <- function(x, Tor, rTo, Er){\r\n\tTor <- Tor+K\r\n\ty <- rTo*exp(-Er/k*(1/(x+K)-1/Tor))\r\n}\r\n\r\nbaModel <- nls(AdjPropEncysted ~ infectNoThresh(x=PerfTemp, iTo)-ba(x=PerfTemp, Tor=19, rTo, Er),start=c(iTo=.6, rTo=.1, Er=.6), data=cystData, control=c(warnOnly=TRUE), algorithm=""port"", lower=c(0,0,0), upper=c(10,10,10))\r\nbaUpper <- nls(AdjPropEncysted ~ infectThresh(x=PerfTemp, iTo, C=0)-ba(x=PerfTemp, Tor=19, rTo, Er), start=c(iTo=.6, rTo=.1, Er=.6), data=cystData, control=c(warnOnly=TRUE), algorithm=""port"", lower=c(0,0,0), upper=c(10,10,10))\r\nbaUpperC <- nls(AdjPropEncysted ~ infectThresh(x=PerfTemp, iTo, C)-ba(x=PerfTemp, Tor=19, rTo, Er), start=c(iTo=.6, C=0, rTo=.1, Er=.6), data=cystData, control=c(warnOnly=TRUE), algorithm=""port"", lower=c(0,-5,0,0), upper=c(10,10,10,10))\r\n\r\nAIC(baModel)\r\nAIC(baUpper)\r\nAIC(baUpperC)\r\n\r\n\r\n# Plot all threshold models\r\n\r\n# Plot parameters\r\npar(mfrow=c(3,1))\r\n\r\n# No thresholds\r\n\r\n# Interaction plot function\r\nbaPlot <- function(x){\r\n\tEi <- .40734\r\n\tEhi <- 3.73023\r\n\tThi <- 304.74427\r\n\tToi <- 19+K\r\n\tiTo <- .8450\r\n\tinfect <- iTo*exp(-(Ei/k*(1/(x+K)-1/Toi)))*(1+exp((Ehi/k*(1/Thi-1/(x+K)))))^-1\r\n\tTor <- 19+K\r\n\trTo <- (.1108)\r\n\tEr <- 1.2477\r\n\tresist <- rTo*exp(-(Er/k*(1/(x+K)-1/Tor)))\r\n\ty <- infect-resist\r\n}\r\n\r\n# Resistance plot function\r\nresistbaPlot <- function(x){\r\n\tTor <- 19+K\r\n\trTo <- (.1108)\r\n\tEr <- 1.2477\r\n\tresist <- rTo*exp(-(Er/k*(1/(x+K)-1/Tor)))\r\n}\r\n\r\n# Infectivity plot function\r\ninfectbaPlot <- function(x){\r\n\tEi <- .40734\r\n\tEhi <- 3.73023\r\n\tThi <- 304.74427\r\n\tToi <- 19+K\r\n\tiTo <- .8450\r\n\tinfect <- iTo*exp(-(Ei/k*(1/(x+K)-1/Toi)))*(1+exp((Ehi/k*(1/Thi-1/(x+K)))))^-1\r\n}\r\nplot(cystData$PerfTemp, cystData$AdjPropEncysted, xlim=c(10,40), ylim=c(0,1.5))\r\ncurve(baPlot, from=10, to=40, add=TRUE)\r\ncurve(resistbaPlot, from=10, to=40, add=TRUE, col=""green"")\r\ncurve(infectbaPlot, from=10, to=40, add=TRUE, col=""red"")\r\n\r\n# Upper threshold\r\n\r\n# Interaction plot function\r\nbaUpperPlot <- function(x){\r\n\tEi <- .40734\r\n\tEhi <- 3.73023\r\n\tThi <- 304.74427\r\n\tToi <- 19+K\r\n\tiTo <- 1.00649\r\n\tC <- 0\r\n\tinfect <- iTo*exp(-(Ei/k*(1/(x+K)-1/Toi)))*(1+exp((Ehi/k*(1/Thi-1/(x+K)))))^-1-C\r\n\tthresh <- ifelse(infect>=1, 1, infect)\r\n\tTor <- 19+K\r\n\trTo <- (.20979)\r\n\tEr <- .22307\r\n\tresist <- rTo*exp(-(Er/k*(1/(x+K)-1/Tor)))\r\n\ty <- thresh-resist\r\n}\r\n\r\n# Resistance plot function\r\nresistbaUpperPlot <- function(x){\r\n\tTor <- 19+K\r\n\trTo <- (.20979)\r\n\tEr <- .22307\r\n\tresist <- rTo*exp(-(Er/k*(1/(x+K)-1/Tor)))\r\n}\r\n\r\n# Infectivity plot function\r\ninfectbaUpperPlot <- function(x){\r\n\tEi <- .40734\r\n\tEhi <- 3.73023\r\n\tThi <- 304.74427\r\n\tToi <- 19+K\r\n\tiTo <- 1.00649\r\n\tC <- 0\r\n\tinfect <- iTo*exp(-(Ei/k*(1/(x+K)-1/Toi)))*(1+exp((Ehi/k*(1/Thi-1/(x+K)))))^-1-C\r\n\tthresh <- ifelse(infect>=1, 1, infect)\r\n}\r\nplot(cystData$PerfTemp, cystData$AdjPropEncysted, xlim=c(10,40), ylim=c(0,1.5))\r\ncurve(baUpperPlot, from=10, to=40, add=TRUE)\r\ncurve(resistbaUpperPlot, from=10, to=40, add=TRUE, col=""green"")\r\ncurve(infectbaUpperPlot, from=10, to=40, add=TRUE, col=""red"")\r\n\r\n# Upper and lower thresholds\r\n\r\n# Interaction plot function\r\nbaUpperCPlot<-function(x){\r\n\tEi <- .40734\r\n\tEhi <- 3.73023\r\n\tThi <- 304.74427\r\n\tToi <- 19+K\r\n\tiTo <- 2.58397\r\n\tC <- (1.28218)\r\n\tinfect <- iTo*exp(-(Ei/k*(1/(x+K)-1/Toi)))*(1+exp((Ehi/k*(1/Thi-1/(x+K)))))^-1-C\r\n\tthresh <- ifelse(infect>=1, 1, infect)\r\n\tTor <- 19+K\r\n\trTo <- (.17965)\r\n\tEr <- .4351\r\n\tresist <- rTo*exp(-(Er/k*(1/(x+K)-1/Tor)))\r\n\ty <- thresh-resist\r\n\tifelse(y<0, 0, y)\r\n}\r\n\r\n# Resistance plot function\r\nresistbaUpperCPlot<-function(x){\r\n\tTor<-19+K\r\n\trTo<-(', '# \'Metacercaria_persistence_Sckrabulis_et_al_2021_AmNat.R\'\r\n\r\n# Metacercaria clearance analysis\r\n\r\n# Use Altman et al. 2016 data here as a .csv\r\n# Reference: Altman, K. A., S. H. Paull, P. T. J. Johnson, M. N. Golembieski, J. P. Stephens, B. E. LaFonte, and T. R. Raffel. 2016. Host and parasite thermal acclimation responses depend on the stage of infection. Journal of Animal Ecology 85(4): 1014-1024. doi:10.1111/1365-2656.12510\r\n# Dryad repository doi: 10.5061/dryad.f3k8p\r\npropClearData <- read.csv(file.choose(), header=TRUE)\r\npropClearData$PerfTempK <- propClearData$PerfTemp+K\r\npropClearData$AccTempK <- propClearData$AccTemp+K\r\npropClearData$AccCenter <- propClearData$AccTemp-mean(propClearData$AccTemp)\r\npropClearData <- subset(propClearData, PropCleared1!=""NA"")\r\npropClearData$logPropClear <- log(propClearData$PropCleared1+1)\r\n\r\n# Constants\r\nk<-8.62*10^-5\r\nK<-273.15\r\n\r\n#_\r\n# Initial test of normal and lognormal error distribution & model type\r\n\r\n# Xiao et al. 2011 AIC calculation\r\n# Reference: Xiao, X., E. P. White, M. B. Hooten, and S. L. Durham. 2011. On the use of log-transformation vs. nonlinear regression for analyzing biological power laws. Ecology 92: 1887-1894. doi:10.1890/11-0538.1\r\nAIC.Xiao <- function(numParams, loglik, n){\r\n\t2*numParams-2*loglik\r\n}\r\n\r\n#_\r\n# Boltzmann-Arrhenius\r\n\r\n# BA equation \r\nba <- function(x, Toc, cTo, Ec){\r\n\tToc <- Toc+K\r\n\ty <- cTo*exp(-(Ec/k*(1/(x+K)-1/(Toc))))\r\n}\r\n\r\n# Normal error distribution\r\nbaModel <- nls(PropCleared1 ~ ba(x=PerfTemp, Toc=19, cTo, Ec), start=c(cTo=1, Ec=.6), data=propClearData, control=c(warnOnly=TRUE))\r\n\r\n# Plotting function using parameter estimates from \'baModel\', also used for model predictions\r\nbaPlot <- function(x){\r\n\tToc <- 19+K\r\n\tcTo <- .46322\r\n\tEc <- .43064\r\n\ty <- cTo*exp(-(Ec/k*(1/(x+K)-1/(Toc))))\r\n}\r\nplot(propClearData$PerfTemp, propClearData$PropCleared1)\r\ncurve(baPlot, from=10, to=40, add=TRUE)\r\n\r\n# Standard deviation and log-likelihood calculations\r\nsdnorm <- sd(propClearData$PropCleared1-baPlot(propClearData$PerfTemp))\r\nll_norm <- sum(log(dnorm(propClearData$PropCleared1, baPlot(propClearData$PerfTemp), sdnorm)))\r\n\r\n# AIC calculation\r\nnormAIC <- AIC.Xiao(numParams=2, loglik=ll_norm, n=length(propClearData$PerfTemp))\r\nnormAIC\r\n\r\n# Lognormal error distribution\r\nlogbaModel <- nls(logPropClear ~ log(ba(x=PerfTemp, Toc=19, cTo, Ec)), start=c(cTo=1, Ec=.6), data=propClearData, control=c(warnOnly=TRUE))\r\n\r\n# Plotting function using parameter estimates from \'logbaModel\', also used for model predictions\r\nlogbaPlot <- function(x){\r\n\tToc <- 19+K\r\n\tcTo <- 1.44366\r\n\tEc <- .14767\r\n\ty <- log(cTo*exp(-(Ec/k*(1/(x+K)-1/(Toc)))))\r\n}\r\nplot(propClearData$PerfTemp, propClearData$logPropClear)\r\ncurve(logbaPlot, from=10, to=40, add=TRUE)\r\n\r\n# Standard deviation and log-likelihood calculations\r\nsdlnorm <- sd(propClearData$logPropClear-logbaPlot(propClearData$PerfTemp))\r\nll_lnorm <- sum(log(dlnorm(propClearData$PropCleared1+1, logbaPlot(propClearData$PerfTemp), sdlnorm)))\r\n\r\n# AIC calculation\r\nlnormAIC <- AIC.Xiao(numParams=2, loglik=ll_lnorm, n=length(propClearData$PerfTemp))\r\nlnormAIC\r\n\r\n#_\r\n# Sharpe-Schoolfield\r\n\r\n# Sharpe-Schoolfield equation\r\nss <- function(x, Toc, cTo, Ec, Ehc, Thc){\r\n\tToc <- Toc+K\r\n\ty <- cTo*exp(-(Ec/k*(1/(x+K)-1/(Toc))))*(1+exp(Ehc/k*(1/Thc-1/(x+K))))^-1\r\n}\r\n\r\n# Normal error distribution\r\nssModel <- nls(PropCleared1 ~ ss(x=PerfTemp, Toc=19, cTo, Ec, Ehc=3.25, Thc), start=c(cTo=1, Ec=.6, Thc=300), data=propClearData, control=c(warnOnly=TRUE))\r\n\r\n# Plotting function using estimates from \'ssModel\', also used for model predictions\r\nssPlot <- function(x){\r\n\tToc <- 19+K\r\n\tcTo <- .46815\r\n\tEc <- .54951\r\n\tEhc <- 3.25\r\n\tThc <- 304.98628\r\n\ty<-cTo*exp(-(Ec/k*(1/(x+K)-1/(Toc))))*(1+exp(Ehc/k*(1/Thc-1/(x+K))))^-1\r\n}\r\nplot(propClearData$PerfTemp, propClearData$PropCleared1)\r\ncurve(ssPlot, from=10, to=40, add=TRUE)\r\n\r\n# Standard deviation and log-likelihood calculations\r\nsdnorm <- sd(propClearData$PropCleared1-ssPlot(propClearData$PerfTemp))\r\nll_norm <- sum(log(dnorm(propClearData$PropCleared1, ssPlot(propClearData$PerfTemp), sdnorm)))\r\n\r\n# AIC calculation\r\nnormAIC <- AIC.Xiao(numParams=3, loglik=ll_norm, n=length(propClearData$PerfTemp))\r\nnormAIC\r\n\r\n# Lognormal error distribution - convergence error\r\nlogssModel <- nls(logPropClear ~ log(ss(x=PerfTemp, Toc=19, cTo, Ec, Ehc=3.25, Thc)),\tstart=c(cTo=1, Ec=.1, Thc=300), data=propClearData, control=c(warnOnly=TRUE), trace=TRUE)\r\n\r\n#_\r\n\r\n# Model selection - BA\r\n\r\n# BA model equation with acclimation effects\r\nclearanceBA <- function(x, Tacc, Toc, bcTo, mcTo, qcTo, bEc, mEc, qEc){\r\n\tK <- 273.15\r\n\tk <- 8.62*10^-5\r\n\tToc <- Toc+K\r\n\tcTo <- function(Tacc, bcTo, mcTo, qcTo){\r\n\t\tcTo <- bcTo+mcTo*(Tacc)+qcTo*(Tacc)^2\r\n\t}\r\n\tEc <- function(Tacc, bEr, mEr, qEr){\r\n\t\tEr <- bEr+mEr*(Tacc)+qEr*(Tacc)^2\r\n\t}\r\n\ty <- cTo(Tacc,bcTo,mcTo,qcTo)*exp(-(Ec(Tacc,bEc,mEc,qEc)/k*(1/(x+K)-1/(Toc))))\r\n}\r\n\r\n# No acclimation effects\r\nnoAcc <- nls(PropCleared1 ~ clearanceBA(x=PerfTemp, Tacc=AccCenter, Toc=27.3, bcTo, mcTo=0, qcTo=0, bEc, mE', '# \'Mismatches_Sensitivity_and_To_Sckrabulis_et_al_2021_AmNat.R\'\r\n\r\n# Thermal mismatch examples, sensitivity analysis and To optimization figure\r\n\r\n#_\r\n# Figure S1: Thermal mismatch examples\r\n\r\n# Function to plot hypothetical thermal mismatches between BA infectivity and resistance (arguments p1 and p2 are vertical positions in units of the y-axis to place labelled values)\r\nmismatch.BA<-function(iTo, Toi, Ei, rTo, Tor, Er, p1, p2){\r\n\r\n\tPerfTemp<-seq(10,30,by=0.5)\r\n\tk <- 8.62*10^-5\r\n\tK <- 273.15\r\n\r\n\tToi <- Toi+K\r\n\tTor <- Tor+K\r\n\r\n\t# Equations\r\n\tinf <- iTo*exp(-(Ei/k)*(1/(PerfTemp+K)-1/Toi))\r\n\tres <- rTo*exp(-(Er/k)*(1/(PerfTemp+K)-1/Tor))\r\n\r\n\tylim <- c(min(c(0,inf,res,inf-res)), max((c(inf,res,inf-res))))\r\n\r\n\t# Plot each line and outcome\r\n\tplot(inf-res~PerfTemp, ylim=ylim, col=""white"", ylab=expression(italic(i[T]~-~r[T])), xlab=expression(paste(""Performance Temperature ("",degree,""C)"")), cex.lab=1.2)\r\n\tlines(inf-res~PerfTemp, lwd=2)\r\n\tlines(inf~PerfTemp,col=""red"", lwd=2, lty=4)\r\n\tlines(res~PerfTemp,col=""green"", lwd=2, lty=2)\r\n\tlines(rep(0,41)~PerfTemp, lty=2)\r\n\r\n\t# Positions of text labels\r\n\tpos1 <- ylim[1]+p1*(ylim[2]-ylim[1])\r\n\tpos2 <- ylim[1]+p2*(ylim[2]-ylim[1])\r\n\t\r\n\t# Text labels of parameter values\r\n\ttext(13, pos1, expression(\'i\'[T0]*\' = \'), pos=2, offset=0);\ttext(13, pos1, iTo, pos=4, offset=0)\r\n\ttext(13, pos2, expression(\'E\'[i]*\' = \'), pos=2, offset=0);\ttext(13, pos2, Ei, pos=4, offset=0)\r\n\ttext(19, pos1, expression(\'r\'[T0]*\' = \'), pos=2, offset=0);\ttext(19, pos1, rTo, pos=4, offset=0)\r\n\ttext(19, pos2, expression(\'E\'[r]*\' = \'), pos=2, offset=0);\ttext(19, pos2, Er, pos=4, offset=0)\r\n}\r\n\r\n# Plot parameters\r\npar(mfrow=c(3,2), mar=c(5,4,0.8,0.8))\r\n\r\n# A: No mismatch\r\nmismatch.BA(iTo=0.3, Toi=20, Ei=0.3, rTo=0.2, Tor=18, Er=0.4, p1=0.9, p2=0.75)\r\ntext(x=24, y=.4, expression(italic(""i""[T])), cex=1.5)\r\ntext(x=24, y=.22, expression(italic(""r""[T])), cex=1.5)\r\ntext(x=27, y=.11, expression(italic(lambda[T])), cex=1.5)\r\nmtext(LETTERS[1], side=3, adj=.02, line=-2)\r\n\r\n# B\r\nmismatch.BA(iTo=0.5, Toi=20, Ei=0.8, rTo=0.5, Tor=20, Er=0.2, p1=0.9, p2=0.75)\r\nmtext(LETTERS[2], side=3, adj=.02, line=-2)\r\n\r\n# C\r\nmismatch.BA(iTo=0.5, Toi=20, Ei=0.3, rTo=0.1, Tor=20, Er=1.2, p1=0.95, p2=0.8)\r\nmtext(LETTERS[3], side=3, adj=.02, line=-2)\r\n\r\n# D\r\nmismatch.BA(iTo=0.5, Toi=20, Ei=0.2, rTo=0.5, Tor=20, Er=0.8, p1=0.9, p2=0.75)\r\nmtext(LETTERS[4], side=3, adj=.02, line=-2)\r\n\r\n# E: Infectivity = 1\r\nmismatch.BA(iTo=1.0, Toi=20, Ei=0, rTo=0.2, Tor=20, Er=0.8, p1=0.6, p2=0.45)\r\nmtext(LETTERS[5], side=3, adj=.02, line=-3)\r\n\r\n# F: Resitance = 0\r\nmismatch.BA(iTo=0.5, Toi=20, Ei=0.8, rTo=0, Tor=20, Er=0.65, p1=0.9, p2=0.75)\r\nmtext(LETTERS[6], side=3, adj=.02, line=-2)\r\n\r\n#_\r\n# Figure S2: Sensitivity analysis\r\n\r\n# Constants\r\nK<-273.15\r\nk<-8.62*10^-5\r\n\r\n# Plotting functions\r\n\r\n# Simulated resistance equation\r\nresistPlot <- function(x, rTo, Ear, Thr){\r\n\tTor <- 19+K\r\n\tEhr <- 3.25\r\n\ty <- rTo*exp(-(Ear/k*(1/(x+K)-1/(Tor))))*(1+exp(Ehr/k*(1/Thr-1/(x+K))))^-1\r\n\tifelse(y>1,1,y)\r\n}\r\n\r\n# Infectivity curve, using estimates from the best fit model from \'Cercaria_swimming_speed_Sckrabulis_et_al_2021_AmNat.R\'\r\ninfectPlot <- function(x){\r\n\tEi <- .40734\r\n\tEhi <- 3.73023\r\n\tThi <- 304.74427\r\n\tToi <- 19+K\r\n\tiTo <- 2.6682\r\n\tC <- (-1.382)\r\n\tinfect <- iTo*exp(-(Ei/k*(1/(x+K)-1/Toi)))*(1+exp((Ehi/k*(1/Thi-1/(x+K)))))^-1+C\r\n\ty <- ifelse(infect>=1, 1, infect)\r\n\tifelse(y<0, 0, y)\r\n}\r\n\r\n# Interaction model (infectivity-resistance)\r\nfullPlot <- function(x, rTo, Ear, Thr){\r\n\tEi <- .40734\r\n\tEhi <- 3.73023\r\n\tThi <- 304.74427\r\n\tToi <- 19+K\r\n\tiTo <- 2.6682\r\n\tC <-(-1.382)\r\n\tinfect <- iTo*exp(-(Ei/k*(1/(x+K)-1/Toi)))*(1+exp((Ehi/k*(1/Thi-1/(x+K)))))^-1+C\r\n\tthresh <- ifelse(infect>=1, 1, infect)\r\n\tEhr <- 3.25\r\n\tTor <- 19+K\r\n\tresist <- rTo*exp(-(Ear/k*(1/(x+K)-1/(Tor))))*(1+exp(Ehr/k*(1/Thr-1/(x+K))))^-1\r\n\ty <- thresh-resist\r\n\tifelse(y<0, 0, y)\r\n}\r\n\r\n# Plot parameters\r\npar(mfrow=c(3,2),omi=c(.4,.4,0,.4))\r\n\r\n#A: Linear change in rTo for resistance\r\n\r\n\t# Panel parameters\r\n\tpar(mai=c(.2,.2,.2,.2))\r\n\t\r\n\t# Plot curves at incremental inreases to rTo\r\n\tcurve(resistPlot(x,rTo=.1,Ear=.65,Thr=303.15), from=10, to=40, ylim=c(0,1), lwd=2, xlab="""", ylab="""", xaxt=""n"", lty=6, col=""grey65"")\r\n\tcurve(resistPlot(x,rTo=.15,Ear=.65,Thr=303.15), from=10, to=40, lwd=2, add=TRUE, lty=5, col=""grey55"")\r\n\tcurve(resistPlot(x,rTo=.2,Ear=.65,Thr=303.15), from=10, to=40, lwd=2, add=TRUE, lty=4, col=""grey45"")\r\n\tcurve(resistPlot(x,rTo=.25,Ear=.65,Thr=303.15), from=10, to=40, lwd=2, add=TRUE, lty=3, col=""grey35"")\r\n\tcurve(resistPlot(x,rTo=.3,Ear=.65,Thr=303.15), from=10, to=40, lwd=2, add=TRUE, lty=2, col=""grey25"")\r\n\tcurve(resistPlot(x,rTo=.35,Ear=.65,Thr=303.15), from=10, to=40, lwd=2, add=TRUE, lty=1)\r\n\taxis(side=1, labels=FALSE)\r\n\tlegend(32.5, 1.05, c(\'.1\',\'.15\',\'.2\',\'.25\',\'.3\',\'.35\'), lty=c(6,5,4,3,2,1), lwd=c(2,2,2,2,2,2),\tcol=c(\'grey65\',\'grey55\',\'grey45\',\'grey35\',\'grey25\',\'black\'), bty=\'n\')\r\n\tmtext(LETTERS[1], side=3, adj=.05, line=-2, cex=1)\r\n\r\n#B: Inserting values from Panel A into interaction model\r\n\r\n\t# P', '# \'Uninfected_tadpole_respiration_Sckrabulis_et_al_2021_AmNat.R\'\r\n\r\n# Uninfected tadpole respitation analysis\r\n\r\n# Libraries\r\nlibrary(Hmisc)\r\n\r\n# Use \'Uninfected_tadpole_respiration_Sckrabulis_et_al_2021_AmNat.csv\' in this repository\r\nrespData <- read.csv(file.choose(), header=TRUE)\r\nrespData$PerfTempK <- respData$PerfTemp+K\r\nrespData$AccTempK <- respData$AccTemp+K\r\nrespData <- subset(respData, AccTemp!=""NA""&corO2.Time.Mass!=""NA"")\r\nrespData$logcorO2.Time.Mass <- log(respData$corO2.Time.Mass+1)\r\nrespData$AccCenter <- respData$AccTemp-mean(respData$AccTemp)\r\n\r\n# Constants\r\nk <- 8.62*10^-5\r\nK <- 273.15\r\n\r\n#_ Initial test of normal and lognormal error distribution and model type\r\n\r\n# Xiao et al. 2011 AIC calculation\r\n# Reference: Xiao, X., E. P. White, M. B. Hooten, and S. L. Durham. 2011. On the use of log-transformation vs. nonlinear regression for analyzing biological power laws. Ecology 92: 1887-1894. doi:10.1890/11-0538.1\r\nAIC.Xiao <- function(numParams, loglik, n){\r\n\t2*numParams-2*loglik\r\n}\r\n\r\n#_ Boltzmann-Arrhenius\r\n\r\n# BA equation\r\nba <- function(x, ToR, RTo, ER){\r\n\tToR <- ToR+K\r\n\ty <- RTo*exp(-(ER/k*(1/(x+K)-1/(ToR))))\r\n}\r\n\r\n# Normal error distribution\r\nbaNorm <- nls(corO2.Time.Mass ~ ba(x=PerfTemp, ToR=19, RTo, ER), data=respData, start=c(RTo=1, ER=.6), control=c(warnOnly=TRUE))\r\n\r\nbaNormPlot <- function(x){\r\n\tk <- 8.62*10^-5\r\n\tK <- 273.15\r\n\tRTo <- .0021595\r\n\tER <- .4631972\r\n\tToR <- 19+K\r\n\ty <- RTo*exp(-(ER/k*(1/(x+K)-1/(ToR))))\r\n}\r\n\r\n# Standard deviation and log-likelihood calculations\r\nsdnorm <- sd(respData$corO2.Time.Mass-baNormPlot(x=respData$PerfTemp))\r\nll_norm <- sum(log(dnorm(respData$corO2.Time.Mass, baNormPlot(x=respData$PerfTemp), sdnorm)))\r\n\r\n# AIC calculation\r\nnormAIC <- AIC.Xiao(numParams=2, loglik=ll_norm, n=length(respData$PerfTemp))\r\nnormAIC\r\n\r\n# Lognormal error distribution\r\nbaLognorm <- nls(logcorO2.Time.Mass ~ log(ba(x=PerfTemp, ToR=19, RTo, ER)), data=respData, start=c(RTo=1, ER=.6), control=c(warnOnly=TRUE))\r\n\r\nbaLognormPlot <- function(x){\r\n\tk <- 8.62*10^-5\r\n\tK <- 273.15\r\n\tRTo <- 1.002\r\n\tER <- .001478\r\n\tToR <- 19+K\r\n\ty <- log(RTo*exp(-(ER/k*(1/(x+K)-1/(ToR)))))\r\n}\r\n\r\n# Standard deviation and log-likelihood calculation\r\nsdlnorm <- sd(respData$logcorO2.Time.Mass-baLognormPlot(x=respData$PerfTemp))\r\nll_lnorm <- sum(log(dlnorm(respData$corO2.Time.Mass+1, baLognormPlot(x=respData$PerfTemp), sdlnorm)))\r\n\r\n# AIC calculation\r\nlnormAIC <- AIC.Xiao(numParams=2, loglik=ll_lnorm, n=length(respData$PerfTemp))\r\nlnormAIC\r\n\r\n#_ Sharpe-Schoolfield\r\n\r\n# SS equation\r\nss <- function(x, ToR, RTo, ER, EhR, ThR){\r\n\tToR <- ToR+K\r\n\ty <- RTo*exp(-(ER/k*(1/(x+K)-1/(ToR))))*(1+exp(EhR/k*(1/ThR-1/(x+K))))^-1\r\n}\r\n\r\n# Normal error distribution\r\nssNorm <- nls(corO2.Time.Mass ~ ss(x=PerfTemp, ToR=19, RTo, ER, EhR, ThR), start=c(RTo=.1, ER=.65, EhR=3, ThR=300), data=respData, control=c(warnOnly=TRUE))\r\n\r\nssNormPlot <- function(x){\r\n\tk <- 8.62*10^-5\r\n\tK <- 273.15\r\n\tRTo <- .001967\r\n\tThR <- 308.2\r\n\tER <- .6878\r\n\tToR <- 19+K\r\n\tEhR <- 3.111\r\n\ty <- RTo*exp(-(ER/k*(1/(x+K)-1/ToR)))*(1+exp(EhR/k*(1/ThR-1/(x+K))))^-1\r\n}\r\n\r\n# Standard deviation and log-likelihood calculations\r\nsdnorm <- sd((respData$corO2.Time.Mass)-(ssNormPlot(respData$PerfTemp)))\r\nll_norm <- sum(log(dnorm(respData$corO2.Time.Mass, ssNormPlot(respData$PerfTemp), sdnorm)))\r\n\r\n# AICc calculation\r\nnormAIC <- AIC.Xiao(numParams=4, loglik=ll_norm, n=length(respData$PerfTemp))\r\nnormAIC\r\n\r\n# Lognormal error distribution - convergence warning (step factor)\r\nssLognorm <- nls(logcorO2.Time.Mass ~ log(ss(x=PerfTemp, ToR=19, RTo, ER, EhR, ThR)),\tstart=c(RTo=.1, ER=.65, EhR=4, ThR=40+K), data=respData, control=c(warnOnly=TRUE))\r\n\r\nssLognormPlot<-function(x){\r\n\tRTo <- 1.002\r\n\tThR <- 309.3\r\n\tER <- .001546\r\n\tToR <- 19+K\r\n\tEhR <- 20.07\r\n\ty <- log(RTo*exp(-(ER/k*(1/(x+K)-1/ToR)))*(1+exp(EhR/k*(1/ThR-1/(x+K))))^-1)\r\n}\r\n\r\n# Standard deviation and log-likelihood calculation\r\nsdlnorm <- sd(respData$logcorO2.Time.Mass-(ssLognormPlot(respData$PerfTemp)))\r\nll_lnorm <- sum(log(dlnorm(respData$corO2.Time.Mass+1, ssLognormPlot(respData$PerfTemp), sdlnorm)))\r\n\r\n# AICc calculation\r\nlnormAIC <- AIC.Xiao(numParams=4, loglik=ll_lnorm, n=length(respData$PerfTemp))\r\nlnormAIC\r\n\r\n#_ \r\n\r\n# Model selection - BA\r\n\r\n#BA model equation with acclimation effects\r\nrespBA <- function(x, Tacc, ToR, bRTo, mRTo, qRTo, bER, mER, qER){\r\n\tK <- 273.15\r\n\tk <- 8.62*10^-5\r\n\tRTo <- function(Tacc, bRTo, mRTo, qRTo){\r\n\t\tRTo <- bRTo+mRTo*(Tacc)+qRTo*(Tacc)^2\r\n\t}\r\n\tER <- function(Tacc, bER, mER, qER){\r\n\t\tEc <- bER+mER*(Tacc)+qER*(Tacc)^2\r\n\t}\r\n\ty <- RTo(Tacc,bRTo,mRTo,qRTo)*exp(-(ER(Tacc,bER,mER,qER)/k*(1/(x+K)-1/(ToR+K))))\r\n}\r\n\r\n# No acclimation effects\r\nnoAcc <- nls(corO2.Time.Mass ~ respBA(x=PerfTemp, Tacc=AccCenter, ToR=12.95, bRTo, mRTo=0, qRTo=0, bER, mER=0, qER=0), data=respData, start=c(bRTo=.1, bER=.6))\r\nsummary(noAcc)\r\nAIC(noAcc)\r\n\r\n# Linear (l) acclimation effects\r\n# RTo\r\nlRTo <- nls(corO2.Time.Mass ~ respBA(x=PerfTemp, Tacc=AccCenter, ToR=12.95, bRTo, mRTo, qRTo=0, bER, mER=0, qER=0), data=respData, star']",4,"metabolic theory, temperature, thermal acclimation, parasitic infection, species interactions, host, parasite, performance curves, equations, published infection data, experiments, tadpole-trematode system, activation energy, respiration, encysted"
Clinging ability is related to particular aspects of foot morphology in salamanders,"The interaction between morphology, performance, and ecology has long been studied in order to explain variation in the natural world. Within arboreal salamanders, diversification in foot morphology and microhabitat use are thought to be linked by the impact of foot size and shape on clinging and climbing performance, resulting in an ability to access new habitats. We examine whether various foot shape metrics correlate with stationary cling performance and microhabitat to explicitly quantify this performance gradient across 14 species of salamander, including both arboreal and non-arboreal species. Clinging performance did not correlate with foot shape, as quantified by landmark-based geometric morphometrics, nor with microhabitat use. Mass-corrected foot centroid size and foot contact area, on the other hand, correlated positively with clinging performance on a smooth substrate. Interestingly, these foot variables correlated negatively with clinging performance on rough substrates, suggesting the use of multiple clinging mechanisms dependent upon the texture of the surface. These findings demonstrate that centroid size and foot contact area are more functionally relevant for clinging in salamanders than foot shape, suggesting that foot shape need not converge in order to achieve convergent performance. More broadly, our results provide an example of how the quantification of the performance gradient can provide the appropriate lens through which to understand the macroevolution of morphology and ecology.","['# Clinging analyses and Figures\nlibrary(geomorph)\nlibrary(ape)\n\nfootshape <- readland.tps(""footshape.tps"", specID = ""ID"")\nfull_dataset <- read.csv(""full_dataset.csv"")\nphylo <- read.tree(""phylo.tre"")\nphylo$tip.label <- stringr::str_replace_all(phylo$tip.label, ""_"", "" "") \n\nGDF <- geomorph.data.frame(foot = footshape, cling_0 = full_dataset$Max_angle_0µm, \n                           cling_2000 = full_dataset$Max_angle_2000µm,\n                           microhab_strict = as.factor(full_dataset$Strict), \n                           centroid = full_dataset$centroid_pruned....1., \n                           cent_corrected = full_dataset$centroid_pruned....1./full_dataset$fsa_mass_mean,\n                           fsa_corrected = full_dataset$fsa_mean/full_dataset$fsa_mass_mean,\n                           mass = full_dataset$fsa_mass_mean,\n                           phy = phylo)\n\n## Cling ~ microhab\n\nanov <- procD.pgls(cling_0~microhab_strict, data = GDF, phy = phy)\nsummary(anov) # z = 0.15041, p = 0.454\n\nanov <- procD.pgls(cling_2000~microhab_strict, data = GDF, phy = phy)\nsummary(anov) # z = 0.1561, p = 0.428\n\n## Landmark-based Shape analyses\n\n### smooth\ncling <- full_dataset$Max_angle_0µm\ncling <- as.numeric(cling)\nnames(cling) <- full_dataset$Species\nidentical(GDF$phy$tip.label, dimnames(GDF$foot)[[3]])\n\nresults_full <- phylo.integration(cling, GDF$foot, GDF$phy) # same math as two.b.pls but can add phylo\nresults_full\n\ncling_a <- cling[which(GDF$microhab_strict == ""A"")]\nfoot_a <- GDF$foot[,,which(GDF$microhab_strict == ""A"")]\nphy_a <- keep.tip(GDF$phy, GDF$phy$tip.label[which(GDF$microhab_strict == ""A"")])\n\nresults_a <- phylo.integration(cling_a, foot_a, phy_a) # same math as two.b.pls but can add phylo\nresults_a\n\ncling_t <- cling[which(GDF$microhab_strict == ""T"")]\nfoot_t <- GDF$foot[,,which(GDF$microhab_strict == ""T"")]\nphy_t <- keep.tip(GDF$phy, GDF$phy$tip.label[which(GDF$microhab_strict == ""T"")])\n\nresults_t <- phylo.integration(cling_t, foot_t, phy_t) # same math as two.b.pls but can add phylo\nresults_t\n\ncling_w <- cling[which(GDF$microhab_strict == ""W"")]\nfoot_w <- GDF$foot[,,which(GDF$microhab_strict == ""W"")]\nphy_w <- keep.tip(GDF$phy, GDF$phy$tip.label[which(GDF$microhab_strict == ""W"")])\n\nresults_w <- phylo.integration(cling_w, foot_w, phy_w) # same math as two.b.pls but can add phylo\nresults_w\n\nsmooth_results <- rbind(c(results_full$r.pls, results_full$Z, results_full$P.value),\n                         c(results_a$r.pls, results_a$Z, results_a$P.value),\n                         c(results_t$r.pls, results_t$Z, results_t$P.value),\n                         c(results_w$r.pls, results_w$Z, results_w$P.value))\n### rough\ncling <- full_dataset$Max_angle_2000µm\ncling <- as.numeric(cling)\nnames(cling) <- full_dataset$Species\n\nresults <- phylo.integration(cling, GDF$foot, GDF$phy) # same math as two.b.pls but can add phylo\nresults_full\n\ncling_a <- cling[which(GDF$microhab_strict == ""A"")]\nfoot_a <- GDF$foot[,,which(GDF$microhab_strict == ""A"")]\nphy_a <- keep.tip(GDF$phy, GDF$phy$tip.label[which(GDF$microhab_strict == ""A"")])\n\nresults_a <- phylo.integration(cling_a, foot_a, phy_a) # same math as two.b.pls but can add phylo\nresults_a\n\ncling_t <- cling[which(GDF$microhab_strict == ""T"")]\nfoot_t <- GDF$foot[,,which(GDF$microhab_strict == ""T"")]\nphy_t <- keep.tip(GDF$phy, GDF$phy$tip.label[which(GDF$microhab_strict == ""T"")])\n\nresults_t <- phylo.integration(cling_t, foot_t, phy_t) # same math as two.b.pls but can add phylo\nresults_t\n\ncling_w <- cling[which(GDF$microhab_strict == ""W"")]\nfoot_w <- GDF$foot[,,which(GDF$microhab_strict == ""W"")]\nphy_w <- keep.tip(GDF$phy, GDF$phy$tip.label[which(GDF$microhab_strict == ""W"")])\n\nresults_w <- phylo.integration(cling_w, foot_w, phy_w) # same math as two.b.pls but can add phylo\nresults_w\n\nrough_results <- rbind(c(results_full$r.pls, results_full$Z, results_full$P.value),\n                        c(results_a$r.pls, results_a$Z, results_a$P.value),\n                        c(results_t$r.pls, results_t$Z, results_t$P.value),\n                        c(results_w$r.pls, results_w$Z, results_w$P.value))\n\n\n### Table 2 \nall_results <- rbind(smooth_results, rough_results)\nall_results <- cbind(rep(c(""Full"", ""A"", ""T"", ""W""), 2), all_results)\nall_results <- cbind(rep(c(""Smooth"", ""Rough""), each = 4), all_results)\ncolnames(all_results) <- c(""Substrate"", ""Data Subset"", ""r-PLS"", ""Effect Size (Z)"", ""P-value"")\nall_results\n\n## Centroid Size\n            \n### smooth\nresults_full <- procD.pgls(cling_0 ~ cent_corrected*microhab_strict, data = GDF, phy = phy) # same math as two.b.pls but can add phylo\nsummary(results_full)\nresults_full$pgls.coefficients\nresults_full$aov.table # Table S1 Csize Smooth\n\nresults_full <- procD.pgls(cling_0 ~ cent_corrected*microhab_strict + mass, data = GDF, phy = phy) # same math as two.b.pls but can add phylo\nsummary(results_full)\nresults_full$pgls.coefficients\nresults_full$aov.table # Table 3 Csize Smooth\n\nmicrohab_col <- as.character(GDF$microhab_strict)\nmicrohab_col[which(microhab_col==']",4,"clinging ability, foot morphology, salamanders, performance, ecology, foot size, foot shape, microhabitat use, climbing performance, habitat access, foot shape metrics, stationary cling performance, mass-corrected foot centroid size, foot contact"
Using geometric morphometrics to determine the 'fittest' floral shape: a case study in large-flowered buzz-pollinated Melastomataceae,"PREMISEFloral shape, i.e. the relative arrangement and position of floral organs, is critical in mediating fit with pollinators and maximizing conspecific pollen transfer. This seems particularly true for functionally specialized systems. To date, however, few studies have attempted to quantify flowers as the inherently three-dimensional structures that they are, and determine the effect of intraspecific shape variation on pollen transfer. We here address this research gap using a functionally specialized system, buzz pollination, where bees extract pollen through vibrations, as a model. Our study species, Meriania hernandoi (Melastomataceae), undergoes a natural floral shape change from pseudo-campanulate corollas with more actinomorphically-arranged stamens (first day) to open corollas with more zygomorphic stamens (second day) over anthesis, providing a natural experiment to test how variation in floral shape affects male and female fitness.METHODSIn one population of M. hernandoi, we bagged 51 pre-anthetic flowers and exposed half of them to bee pollinators when they were in either stage of their shape transition. We then collected flowers, obtained 3D flower models through X-ray Computed Tomography for 3D geometric morphometrics, and counted the amount of pollen grains remaining per stamen (male fitness) and stigmatic pollen loads (female fitness). KEY RESULTSWe found significantly higher male fitness in open flowers with zygomorphic androecia than in pseudo-campanulate flowers. Female fitness did not differ among floral shapes. CONCLUSIONSThese results suggest that there is an 'optimal' shape for male fitness, while the movement of bees around the flower when buzzing the spread-out stamens results in sufficient pollen deposition regardless of floral shape.","['library(geomorph)\r\nlibrary(knitr)\r\nlibrary(car)\r\nlibrary(rgl)\r\n\r\n###############################\r\n#01) calculate and plot shape changes, disparity and space occurpation\r\n##############################\r\n\r\n#perform procrustes fitting\r\n#https://cran.r-project.org/web/packages/geomorph/vignettes/geomorph.PCA.html\r\nmerall <- read.morphologika(""all_estimated_202208.txt"")\r\nnames<-dimnames(merall)[[3]]\r\nallfit <- gpagen(merall)\r\n\r\nages <- read.csv(""ages.csv"", sep="";"")[1:52,]\r\nages$age2[ages$age==1]<-""young""\r\nages$age2[ages$age==2]<-""old""\r\nages$age2[ages$age==3]<-""old""\r\n\r\nagess<-match(ages$specimen,names)\r\ngrps<-ages$age2\r\n\r\npca1compl<-gm.prcomp(allfit$coords) #nice separation of age classes :-)\r\n\r\ncol<-ages$age2\r\ncol[which(col==""young"")]<-""slategray2""  \r\ncol[which(col==""old"")]<-""salmon1"" \r\n\r\nwrite.csv(pca1compl$x[,1:3],""PCA1_3_full.csv"")\r\nwrite.csv(allfit$Csize, ""full_size.csv"")\r\n\r\nlibrary(scales)\r\n\r\nwindows()\r\nplot(pca1compl,pch=21,cex=4,col=col,bg=alpha(col,0.6))\r\ntext(pca1compl$x[,1:2],names)\r\n\r\nsm<-summary(pca1compl)\r\n\r\n#calculate morphological disparity for young versus alt\r\ngdf1 <- geomorph.data.frame(allfit,grps=grps)\r\nDgroupmean1 <- morphol.disparity(coords~grps,groups=~grps,data=gdf1,iter=999) #md on group means\r\n\r\nwrite.csv(Dgroupmean1$Procrustes.var,paste(""Disp.csv"",sep=""""))\r\nwrite.csv(Dgroupmean1$PV.dist,paste(""pwdist.csv"",sep=""""))\r\nwrite.csv(Dgroupmean1$PV.dist.Pval,paste(""Disp_p.csv"",sep=""""))\r\n\r\n\r\n#assess shape variation\r\nmsh <- mshape(allfit$coords)\r\nplotRefToTarget(pca1compl$shapes$shapes.comp1$min, msh)\r\n\r\n#compare two extremes (of PC1)\r\nplotRefToTarget(pca1compl$shapes$shapes.comp1$min, pca1compl$shapes$shapes.comp1$max, method = ""vector"", mag = 1,webgl=TRUE)\r\n\r\n\r\n\r\npicknplot.shape(plot(pca1compl), method = ""vector"")\r\n\r\n#compare two extremes (of PC2)\r\nplotRefToTarget(pca1compl$shapes$shapes.comp2$min, pca1compl$shapes$shapes.comp2$max, method = ""vector"", mag = 2)\r\nplotRefToTarget(pca1compl$shapes$shapes.comp2$min, pca1compl$shapes$shapes.comp2$max, method = ""points"", mag = 2)\r\n\r\n#compare two extremes (of PC3)\r\nplotRefToTarget(pca1compl$shapes$shapes.comp3$min, pca1compl$shapes$shapes.comp3$max, method = ""vector"", mag = 2)\r\n\r\nplot(pca1compl$shapes$shapes.comp1$min)\r\nplot(pca1compl$shapes$shapes.comp1$max)\r\n\r\n\r\nplot(pca2compl$shapes$shapes.comp2$min)\r\nplot(pca2compl$shapes$shapes.comp2$max)\r\n\r\n\r\n####################\r\n###01 b - plot the androecium and stigma!\r\n###################\r\n\r\nstafit <- gpagen(merall[c(1:30,32),,])\r\n\r\npca2compl<-gm.prcomp(stafit$coords) #nice separation of age classes :-)\r\nwrite.csv(pca2compl$x[,1:3],""PCA_31_1_3.csv"")\r\n\r\nwindows()\r\nplot(pca2compl,col=col,pch=21,cex=4,bg=alpha(col,0.6))\r\ntext(pca2compl$x[,1:2],names)\r\n\r\nsm<-summary(pca2compl)\r\n\r\n#compare two extremes (of PC2)\r\nplotRefToTarget(pca2compl$shapes$shapes.comp1$min, pca2compl$shapes$shapes.comp1$max, method = ""vector"", mag = 2)\r\nplotRefToTarget(pca2compl$shapes$shapes.comp2$min, pca2compl$shapes$shapes.comp2$max, method = ""vector"", mag = 2)\r\n\r\n#calculate morphological disparity for young versus alt\r\ngdf2 <- geomorph.data.frame(stafit,grps=grps)\r\nDgroupmean2 <- morphol.disparity(coords~grps,groups=~grps,data=gdf2,iter=999) #md on group means\r\n\r\n\r\n\r\n################\r\n## 01c - only appendages and stigma (determine the position of the bee)\r\n#################\r\nappfit <- gpagen(merall[c(1:20,32),,])\r\n\r\npca3compl<-gm.prcomp(appfit$coords) \r\nwrite.csv(pca3compl$x[,1:3],""PCA_andr_1_3.csv"")\r\n\r\nwindows()\r\nplot(pca3compl,col=col,pch=21,cex=4,bg=alpha(col,0.6))\r\ntext(pca3compl$x[,1:2],names)\r\n\r\nsm<-summary(pca3compl)\r\n\r\n#compare two extremes (of PC2)\r\nplotRefToTarget(pca3compl$shapes$shapes.comp1$min, pca3compl$shapes$shapes.comp1$max, method = ""vector"", mag = 2)\r\nplotRefToTarget(pca3compl$shapes$shapes.comp2$min, pca3compl$shapes$shapes.comp2$max, method = ""vector"", mag = 2)\r\n\r\n\r\n#calculate morphological disparity for young versus alt\r\ngdf3 <- geomorph.data.frame(appfit,grps=grps)\r\nDgroupmean3 <- morphol.disparity(coords~grps,groups=~grps,data=gdf3,iter=999) #md on group means\r\n\r\n\r\n\r\n################\r\n## 01d - only pores and stigma (determine pollen deposition)\r\n#################\r\nporefit <- gpagen(merall[c(21:30,32),,])\r\n\r\npca4compl<-gm.prcomp(porefit$coords) \r\nwrite.csv(pca4compl$x[,1:3],""PCA_pore_1_3.csv"")\r\n\r\nwindows()\r\nplot(pca4compl,col=col,pch=21,cex=4,bg=alpha(col,0.6))\r\ntext(pca4compl$x[,1:2],names)\r\n\r\nsm<-summary(pca3compl)\r\n\r\n#compare two extremes (of PC2)\r\nplotRefToTarget(pca4compl$shapes$shapes.comp1$min, pca4compl$shapes$shapes.comp1$max, method = ""vector"", mag = 2)\r\nplotRefToTarget(pca4compl$shapes$shapes.comp2$min, pca4compl$shapes$shapes.comp2$max, method = ""vector"", mag = 2)\r\n\r\n#calculate morphological disparity for young versus alt\r\ngdf4 <- geomorph.data.frame(porefit,grps=grps)\r\nDgroupmean4 <- morphol.disparity(coords~grps,groups=~grps,data=gdf4,iter=999) #md on group means\r\n\r\n\r\n\r\n\r\n\r\n################\r\n## 01e - only corolla shape\r\n#################\r\ncorfit <- gpagen(merall[c(31,33:37),,])\r\n\r\npca5compl<-gm.prcom', '#########################################################################\r\n###   Analysis of pollen removal in relation to corolla shape   ###\r\n#########################################################################\r\n\r\nlibrary(lmerTest) #for making glms\r\nlibrary(emmeans) #for estimating marginal means\r\nlibrary(sjPlot)\r\nlibrary(RColorBrewer)\r\n\r\n###########################\r\n#1) load data\r\n###########################\r\n\r\npoll<-read.csv(""pollenremoval_fordryad.csv"")\r\n\r\nhist(poll$Pollen_no)\r\n\r\nwindows()\r\npar(mfrow=c(2,2))\r\nhist(poll$Pollen_no)\r\nboxplot(poll$Pollen_no~poll$Stamen)\r\nboxplot(poll$Pollen_no~poll$age)\r\nboxplot(poll$Pollen_no~poll$Ind)\r\n\r\nboxplot(poll$Pollen_no~poll$Stamen*poll$age)\r\n\r\n\r\n\r\n\r\n###################################################\r\n## 2) test for impact of age and stamen number on pollen removal\r\n######################\r\n##assess dataset\r\ntable(poll$age)\r\ntable(poll$Ind)\r\n\r\n\r\n#set up a model with Gaussian normal distribution (pollen counts, albeit count data, follow a normal distribution)\r\nmod0l<-lmer(Pollen_no~as.factor(age)*as.factor(Stamen)+(1|Ind),data=poll) #stamen nested in individual\r\nsummary(mod0l)\r\nmod0la<-lmer(Pollen_no~as.factor(age)+as.factor(Stamen)+(1|Ind),data=poll) #stamen nested in individual\r\nanova(mod0l,mod0la)\r\ndrop1(mod0l)\r\ndrop1(mod0la)\r\nmod0lb<-lmer(Pollen_no~as.factor(age):as.factor(Stamen)+(1|Ind),data=poll) #stamen nested in individual\r\nmod0lc<-lmer(Pollen_no~as.factor(age)+(1|Ind),data=poll) #stamen nested in individual\r\nmod0ld<-lmer(Pollen_no~as.factor(Stamen)+(1|Ind),data=poll)\r\nanova(mod0l,mod0la,mod0lb,mod0lc,mod0ld)\r\ndrop1(mod0lc)\r\ndrop1(mod0ld)\r\nsummary(mod0lc)\r\n\r\n#this is the original model, which we did not retain\r\nwindows()\r\nplot_model(mod0l,type=""pred"",terms=c(""age"",""Stamen""),colors = col,dot.size=8)\r\n\r\nwindows() #this is the model we retained\r\nplot_model(mod0lc,type=""pred"",terms=c(""age""),colors = col,dot.size=8)\r\n\r\n#calculate pairwise differences between age classes\r\npairs(emmeans(mod0lc,~as.factor(age)))\r\nplot_model(mod0lc,type=""pred"")\r\nplot_model(mod0lc)\r\nemmip(mod0lc,~as.factor(age),CI=TRUE) #this is the model output represented for all stamens collectively\r\n\r\n\r\n\r\n######\r\n# plot the raw data to visualize pollen removal across stamens and the large variances detected among stamens\r\nlibrary(ggplot2)\r\npoll$Stamen<-as.character(poll$Stamen)\r\npoll$Stamen<-factor(poll$Stamen, levels=c(""1"",""2"",""3"",""4"",""5"",""6"",""7"",""8"",""9"",""10""))\r\n\r\n\r\n#this is the plot retained for Figure 2\r\nggplot(poll, aes(x=Stamen, y=Pollen_no,fill=age)) + \r\n  geom_boxplot(position=position_dodge(1))+ scale_fill_manual(values=c(""slategray2"", ""salmon1"")) +theme_minimal()\r\n\r\n\r\n\r\n#############################\r\n# 4) assess pollen deposition\r\n#############################\r\n\r\n#only one stigma per flower, so we do not need glmms controlling for flower effects\r\ndat<-read.csv(""stigsta_fordryad.csv"")\r\n\r\nhist(dat$pollen_stigma)\r\nhist(resid(lm(dat$pollen_stigma~dat$age)))\r\n\r\nkruskal.test(dat$pollen_stigma~dat$age)\r\nwilcox.test(pollen_stigma ~ age, data=dat)\r\n\r\n\r\n\r\n################################\r\n# 5) summary values for pollen removal and deposition\r\n###############################\r\n\r\n#summarized young and old flowers\r\nwith(dat, tapply(pollen_stigma, age, mean))\r\nwith(dat, tapply(pollen_stigma, age, sd))\r\n\r\nwith(dat, tapply(pollen_stamen, age, mean))\r\nwith(dat, tapply(pollen_stamen, age, sd))\r\n', '###############################\r\n# JOINT ANALYSIS OF POLLEN REMOVAL AND POLLEN DEPOSITION\r\n###############################\r\n\r\n\r\nlibrary(readxl)\r\nlibrary(lmerTest) #for making glms\r\nlibrary(emmeans) #for estimating marginal means\r\nlibrary(sjPlot)\r\nlibrary(RColorBrewer)\r\nlibrary(ggplot2)\r\n\r\ndat<-read.csv(""stigsta_fordryad.csv"")\r\n\r\n#visualize the stigma data\r\nboxplot(dat$pollen_stigma~dat$age)\r\nhist(dat$pollen_stigma)\r\nwilcox.test(dat$pollen_stigma~dat$age)\r\n\r\n#visualize the raw data\r\nwindows()\r\nggplot(dat,aes(x=pollen_stamen,y=pollen_stigma,color=as.factor(age)))+geom_point()+geom_smooth(method=""glm"")\r\nplot(dat$pollen_stigma~dat$pollen_stamen)\r\nhist(dat$pollen_stigma)\r\n\r\n\r\ncor(dat$pollen_stigma, dat$pollen_stamen)\r\n\r\nmod1 <- glm(pollen_stigma~pollen_stamen*as.factor(age),dat = dat, family=""poisson"")\r\nmod <- glm(pollen_stigma~pollen_stamen+as.factor(age),dat = dat, family=""poisson"")\r\nmod2 <- glm(pollen_stigma~pollen_stamen,dat = dat, family=""poisson"")\r\n\r\ndrop1(mod1,test=""Chisq"") #significant interaction\r\n\r\nsummary(mod)\r\nsummary(mod1)\r\nanova(mod,mod1,mod2)\r\nAIC(mod,mod1,mod2)\r\n\r\n#plot_model(mod,type=""pred"", terms = c(""Pollen_stamen"", ""age""))\r\nlibrary(sjPlot)\r\nlibrary(ggplot2)\r\nset_theme(base = theme_classic(), #To remove the background color and the grids\r\n          #theme.font = \'serif\',   #To change the font type\r\n          axis.title.size = 2.0,  #To change axis title size\r\n          axis.textsize.x = 1.5,  #To change x axis text size\r\n          axis.textsize.y = 1.5)  #To change y axis text size\r\n\r\ncol<-as.character(dat$age)\r\ncol[which(col==""1"")]<-""slategray2""  \r\ncol[which(col==""2"")]<-""salmon1"" \r\n\r\n\r\nwindows()\r\nplot_model(mod1,type=""pred"", terms = c(""pollen_stamen"", ""age""), colors=c(""slategray2"", ""salmon1""), show.data=TRUE,dot.size=6) #+aes(linetype=col,color=col)\r\n\r\npairs(emmeans(mod1,~pollen_stamen*as.factor(age)))\r\nplot_model(mod1,type=""resid"", terms = c(""pollen_stamen"", ""age""), colors=c(""slategray2"", ""salmon1""), show.data=TRUE,dot.size=6) #+aes(linetype=col,color=col)\r\n']",4,"geometric morphometrics, floral shape, pollinators, conspecific pollen transfer, buzz pollination, Meriania hernandoi, Melastomataceae, male fitness, female fitness, X-ray Computed Tomography, intras"
Data for: Symbiotic nitrogen fixation does not stimulate soil phosphatase activity under temperate and tropical trees,"Symbiotic nitrogen (N)-fixing plants can enrich ecosystems with N, which can alter the cycling and demand for other nutrients. Researchers have hypothesized that fixed N could be used by plants and soil microbes to produce extracellular phosphatase enzymes, which release P from organic matter. Consistent with this speculation, the presence of N-fixing plants is often associated with high phosphatase activity, either in the soil or on root surfaces, although other studies have not found this association, and the connection between phosphatase and rates of N fixationthe mechanistic part of the argumentis tenuous. Here, we measured soil phosphatase activity under N-fixing trees and non-fixing trees transplanted and grown in tropical and temperate sites in the USA: two sites in Hawaii, and one each in New York and Oregon. This provides a rare example of phosphatase activity measured in a multi-site field experiment with rigorously quantified rates of N fixation. We found no difference in soil phosphatase activity under N-fixing vs. non-fixing trees nor across rates of N fixation, though we note that no sites were P limited and only one was N limited. Our results add to the literature showing no connection between N fixation rates and phosphatase activity.","['# Figure setup\r\n\r\n# Add in some columns\r\n\r\ndat$TRT <- -1\r\ndat[dat$Treatment==""LN"",]$TRT <- 0\r\ndat[dat$Treatment==""MN"",]$TRT <- 1\r\ndat[dat$Treatment==""HN"",]$TRT <- 2\r\ndat[dat$Treatment==""PHN"",]$TRT <- 3\r\n\r\nTRT <- rep(-1,nrow(dat))\r\nTRT[dat$Treatment==""LN""] <- 0\r\nTRT[dat$Treatment==""MN""] <- 1\r\nTRT[dat$Treatment==""HN""] <- 2\r\nTRT[dat$Treatment==""PHN""] <- 3\r\njTRT <- jitter(TRT)\r\n\r\nxtick <- 0:3\r\n#xlabs <- c(""L"",""M"",""H"",""H+P"")\r\n\r\n# Make sure HN is base when calling this\r\nplot_u_se <- function(xoff,ct,cl,pc,logy){\r\n\tif(logy){\r\n\t\tpoints(0+xoff,exp(ct[1,1]+ct[2,1]),col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS LN\r\n\t\tsegments(0+xoff,exp(ct[1,1]+ct[2,1]-ct[2,2]),0+xoff,exp(ct[1,1]+ct[2,1]+ct[2,2]),col=cl)\r\n\t\tpoints(1+xoff,exp(ct[1,1]+ct[3,1]),col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS MN\r\n\t\tsegments(1+xoff,exp(ct[1,1]+ct[3,1]-ct[3,2]),1+xoff,exp(ct[1,1]+ct[3,1]+ct[3,2]),col=cl)\r\n\t\tpoints(2+xoff,exp(ct[1,1]+0),col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS HN\r\n\t\tsegments(2+xoff,exp(ct[1,1]+0-ct[1,2]),2+xoff,exp(ct[1,1]+0+ct[1,2]),col=cl)\r\n\t\tpoints(3+xoff,exp(ct[1,1]+ct[4,1]),col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS PHN\r\n\t\tsegments(3+xoff,exp(ct[1,1]+ct[4,1]-ct[4,2]),3+xoff,exp(ct[1,1]+ct[4,1]+ct[4,2]),col=cl)\r\n\t}else{\r\n\t\tpoints(0+xoff,ct[1,1]+ct[2,1],col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS LN\r\n\t\tsegments(0+xoff,ct[1,1]+ct[2,1]-ct[2,2],0+xoff,ct[1,1]+ct[2,1]+ct[2,2],col=cl)\r\n\t\tpoints(1+xoff,ct[1,1]+ct[3,1],col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS MN\r\n\t\tsegments(1+xoff,ct[1,1]+ct[3,1]-ct[3,2],1+xoff,ct[1,1]+ct[3,1]+ct[3,2],col=cl)\r\n\t\tpoints(2+xoff,ct[1,1]+0,col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS HN\r\n\t\tsegments(2+xoff,ct[1,1]+0-ct[1,2],2+xoff,ct[1,1]+0+ct[1,2],col=cl)\r\n\t\tpoints(3+xoff,ct[1,1]+ct[4,1],col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS PHN\r\n\t\tsegments(3+xoff,ct[1,1]+ct[4,1]-ct[4,2],3+xoff,ct[1,1]+ct[4,1]+ct[4,2],col=cl)\r\n\t}\r\n}\r\nplot_u_se_lme <- function(xoff,ct,bm,cl,pc,logy){\r\n\tif(logy){\r\n\t\tpoints(0+xoff,exp(ct[1,1]+bm*ct[5,1]+ct[2,1]),col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS LN\r\n\t\tsegments(0+xoff,exp(ct[1,1]+bm*ct[5,1]+ct[2,1]-ct[2,2]),0+xoff,exp(ct[1,1]+bm*ct[5,1]+ct[2,1]+ct[2,2]),col=cl)\r\n\t\tpoints(1+xoff,exp(ct[1,1]+bm*ct[5,1]+ct[3,1]),col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS MN\r\n\t\tsegments(1+xoff,exp(ct[1,1]+bm*ct[5,1]+ct[3,1]-ct[3,2]),1+xoff,exp(ct[1,1]+bm*ct[5,1]+ct[3,1]+ct[3,2]),col=cl)\r\n\t\tpoints(2+xoff,exp(ct[1,1]+bm*ct[5,1]+0),col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS HN\r\n\t\tsegments(2+xoff,exp(ct[1,1]+bm*ct[5,1]+0-ct[1,2]),2+xoff,exp(ct[1,1]+bm*ct[5,1]+0+ct[1,2]),col=cl)\r\n\t\tpoints(3+xoff,exp(ct[1,1]+bm*ct[5,1]+ct[4,1]),col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS PHN\r\n\t\tsegments(3+xoff,exp(ct[1,1]+bm*ct[5,1]+ct[4,1]-ct[4,2]),3+xoff,exp(ct[1,1]+bm*ct[5,1]+ct[4,1]+ct[4,2]),col=cl)\r\n\t}else{\r\n\t\tpoints(0+xoff,ct[1,1]+bm*ct[5,1]+ct[2,1],col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS LN\r\n\t\tsegments(0+xoff,ct[1,1]+bm*ct[5,1]+ct[2,1]-ct[2,2],0+xoff,ct[1,1]+bm*ct[5,1]+ct[2,1]+ct[2,2],col=cl)\r\n\t\tpoints(1+xoff,ct[1,1]+bm*ct[5,1]+ct[3,1],col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS MN\r\n\t\tsegments(1+xoff,ct[1,1]+bm*ct[5,1]+ct[3,1]-ct[3,2],1+xoff,ct[1,1]+bm*ct[5,1]+ct[3,1]+ct[3,2],col=cl)\r\n\t\tpoints(2+xoff,ct[1,1]+bm*ct[5,1]+0,col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS HN\r\n\t\tsegments(2+xoff,ct[1,1]+bm*ct[5,1]+0-ct[1,2],2+xoff,ct[1,1]+bm*ct[5,1]+0+ct[1,2],col=cl)\r\n\t\tpoints(3+xoff,ct[1,1]+bm*ct[5,1]+ct[4,1],col=cl,pch=pc,cex=1.2) # Mean estimate for ROPS PHN\r\n\t\tsegments(3+xoff,ct[1,1]+bm*ct[5,1]+ct[4,1]-ct[4,2],3+xoff,ct[1,1]+bm*ct[5,1]+ct[4,1]+ct[4,2],col=cl)\r\n\t}\r\n}\r\nplot_siglets <- function(xoff,yv,siglets,cl){\r\n\ttext((0:3)+xoff,c(yv,yv,yv,yv),siglets,col=cl)\r\n}', '# NSF FX data analysis for the Volcano biomass data\n\n# There are a number of ways we could analyze biomass growth rates\n# for these trees. Each of these will be separate for each species.\n# - Compute absolute and relative growth rates for each time period\n# - Compute absolute and relative growth rates for each year\n# - Compute absolute and relative growth rates from t0 to each tf\n# For each of these, we want to analyze growth as a function of \n# treatment, which answers our initial scientific question:\n# Is the species limited by N and/or P at each treatment level?\n# We also want to account for the fact that growth rate varies with\n# tree size, and possibly with year.\n\n# How to handle odd datapoints? By odd datapoints, I mean ones that \n# result in negative growth or are otherwise anomalous compared to the\n# previous/expected growth rate of the tree.\n# There are a reasons why datapoints could be odd:\n# - Measurement or data input error\n# - Point loss of biomass (branches etc.) due to physical damage\n# - Persistent loss of biomass due to pest damage or other illness\n# The way we deal with odd datapoints would be different for each.\n# - Measurement or data input error should correct themselves over time, \n# although negative relative growth rates will be tricky.\n# - Point loss of biomass should be a ""restart"" of growth measurements.\n# - Persistent loss of biomass due to pest damage or other illness should\n# be removed from the analysis, as it does not address the question.\n\n# I\'m editing the ""Use_growth_0X"" columns in\n# ""HI_V_FX_Size_FoliarCNIsotope_Data.csv"" to reflect these odd datapoints\n# based on biomass trends.\n# 1 means not odd in any way.\n# 0.5 means use July 2019 if not using January 2019, but not if using January 2019.\n# 0 means sufficiently ill or damaged that it shouldn\'t be used.\n\nrm(list=ls())\nlibrary(nlme)\nlibrary(emmeans)\n\nsetwd(""/Users/duncanmenge/Documents/Academia/Grants/NSF/2014_Strategies/Data/Master datafiles/"")\n\ndat <- read.csv(""HI_V_FX_Size_FoliarCNIsotope_Data.csv"")[1:96,1:111]\n\n# Dates\n# _01: January 2016\n# _02: July 2016\n# _03: June 2017\n# _04: June 2018\n# _05: January 2019\n# _06: July 2019\n\n# Option to print out how many are in each category (uncomment)\n\n#print(dat[dat$Species==""ACKO"" & dat$Treatment==""LN"",c(22,33,44,59,76,87)])\n#print(dat[dat$Species==""ACKO"" & dat$Treatment==""MN"",c(22,33,44,59,76,87)])\n#print(dat[dat$Species==""ACKO"" & dat$Treatment==""HN"",c(22,33,44,59,76,87)])\n#print(dat[dat$Species==""ACKO"" & dat$Treatment==""PHN"",c(22,33,44,59,76,87)])\n\n#print(dat[dat$Species==""MOFA"" & dat$Treatment==""LN"",c(22,33,44,59,76,87)])\n#print(dat[dat$Species==""MOFA"" & dat$Treatment==""MN"",c(22,33,44,59,76,87)])\n#print(dat[dat$Species==""MOFA"" & dat$Treatment==""HN"",c(22,33,44,59,76,87)])\n#print(dat[dat$Species==""MOFA"" & dat$Treatment==""PHN"",c(22,33,44,59,76,87)])\n\n#print(dat[dat$Species==""DOVI"" & dat$Treatment==""LN"",c(22,33,44,59,76,87)])\n#print(dat[dat$Species==""DOVI"" & dat$Treatment==""MN"",c(22,33,44,59,76,87)])\n#print(dat[dat$Species==""DOVI"" & dat$Treatment==""HN"",c(22,33,44,59,76,87)])\n#print(dat[dat$Species==""DOVI"" & dat$Treatment==""PHN"",c(22,33,44,59,76,87)])\n\n#############################################################\n################ Calculate growth rates #####################\n#############################################################\n\n# ""AGR"" is absolute growth rate and ""RGR"" is relative growth rate. \n# Each is calculated for aboveground biomass, \n# for the following time periods: \n\n# Since some of the trees were replanted in July 2016 or January 2017,\n# I need to account for that.\n\n# Annual/semiannual increments:\n\n# AGB\n\n# July 2016 to June 2017\ndat$AGR_AGB_2016_2017 <- (dat$AGB_est_kg_03 - dat$AGB_est_kg_02)/((dat$Days_03 - dat$Days_02)/365.25)\ndat$RGR_AGB_2016_2017 <- (log(dat$AGB_est_kg_03) - log(dat$AGB_est_kg_02))/((dat$Days_03 - dat$Days_02)/365.25)\n\nreplant <- which(dat$Use_growth_02==0 & dat$Use_growth_01==1)\ndat[replant,]$AGR_AGB_2016_2017 <-\n\t(dat[replant,]$AGB_est_kg_03 - dat[replant,]$AGB_est_kg_01)/\n\t((dat[replant,]$Days_03 - dat[replant,]$Days_01)/365.25)\ndat[replant,]$RGR_AGB_2016_2017 <-\n\t(log(dat[replant,]$AGB_est_kg_03) - log(dat[replant,]$AGB_est_kg_01))/\n\t((dat[replant,]$Days_03 - dat[replant,]$Days_01)/365.25)\n\n# June 2017 to June 2018\ndat$AGR_AGB_2017_2018 <- (dat$AGB_est_kg_04 - dat$AGB_est_kg_03)/((dat$Days_04 - dat$Days_03)/365.25)\ndat$RGR_AGB_2017_2018 <- (log(dat$AGB_est_kg_04) - log(dat$AGB_est_kg_03))/((dat$Days_04 - dat$Days_03)/365.25)\n\n# June 2018 to July 2019\ndat$AGR_AGB_2018_2019 <- (dat$AGB_est_kg_06 - dat$AGB_est_kg_04)/((dat$Days_06 - dat$Days_04)/365.25)\ndat$RGR_AGB_2018_2019 <- (log(dat$AGB_est_kg_06) - log(dat$AGB_est_kg_04))/((dat$Days_06 - dat$Days_04)/365.25)\n\n# 2016 to tf increments:\n\n# AGB\n\n# January 2016 to June 2018\ndat$AGR_AGB_2016_2018 <- (dat$AGB_est_kg_04 - dat$AGB_est_kg_01)/((dat$Days_04 - dat$Days_01)/365.25)\ndat$RGR_AGB_2016_2018 <- (log(dat$AGB_est_kg_04) - log(dat$AGB_est_kg_01))/((dat$Days_04 - dat$Days_01)/365.25)\n\n# Re', '# NSF FX data analysis for the Waiakea biomass data\n\n# There are a number of ways we could analyze biomass growth rates\n# for these trees. Each of these will be separate for each species.\n# - Compute absolute and relative growth rates for each time period\n# - Compute absolute and relative growth rates for each year\n# - Compute absolute and relative growth rates from t0 to each tf\n# For each of these, we want to analyze growth as a function of \n# treatment, which answers our initial scientific question:\n# Is the species limited by N and/or P at each treatment level?\n# We also want to account for the fact that growth rate varies with\n# tree size, and possibly with year.\n\n# How to handle odd datapoints? By odd datapoints, I mean ones that \n# result in negative growth or are otherwise anomalous compared to the\n# previous/expected growth rate of the tree.\n# There are a reasons why datapoints could be odd:\n# - Measurement or data input error\n# - Point loss of biomass (branches etc.) due to physical damage\n# - Persistent loss of biomass due to pest damage or other illness\n# The way we deal with odd datapoints would be different for each.\n# - Measurement or data input error should correct themselves over time, \n# although negative relative growth rates will be tricky.\n# - Point loss of biomass should be a ""restart"" of growth measurements.\n# - Persistent loss of biomass due to pest damage or other illness should\n# be removed from the analysis, as it does not address the question.\n\n# I\'m editing the ""Use_growth_0X"" columns in\n# ""HI_W_FX_Size_FoliarCNIsotope_Data.csv"" to reflect these odd datapoints\n# based on biomass trends.\n# 1 means not odd in any way.\n# 0.5 means use July 2019 if not using January 2019, but not if using January 2019.\n# 0 means sufficiently ill or damaged that it shouldn\'t be used.\n# The ""Use_growth_0X"" columns in \n# ""HI_W_FX_Height_Diam_FoliarCNIsotope_Data.csv"" were based on diameter,\n# height, and canopy dimension data independently.\n\nrm(list=ls())\nlibrary(nlme)\nlibrary(emmeans)\n\nsetwd(""/Users/duncanmenge/Documents/Academia/Grants/NSF/2014_Strategies/Data/Master datafiles/"")\n\ndat <- read.csv(""HI_W_FX_Size_FoliarCNIsotope_Data.csv"")[1:108,1:111]\n\n# Dates\n# _01: January 2016\n# _02: July 2016\n# _03: June 2017\n# _04: June 2018\n# _05: January 2019\n# _06: July 2019\n\n# Option to print out how many are in each category (uncomment)\n\n#print(dat[dat$Species==""GLSE"" & dat$Treatment==""LN"",c(33,44,59,76,87)])\n#print(dat[dat$Species==""GLSE"" & dat$Treatment==""MN"",c(33,44,59,76,87)])\n#print(dat[dat$Species==""GLSE"" & dat$Treatment==""HN"",c(33,44,59,76,87)])\n#print(dat[dat$Species==""GLSE"" & dat$Treatment==""PHN"",c(33,44,59,76,87)])\n\n#print(dat[dat$Species==""CAEQ"" & dat$Treatment==""LN"",c(33,44,59,76,87)])\n#print(dat[dat$Species==""CAEQ"" & dat$Treatment==""MN"",c(33,44,59,76,87)])\n#print(dat[dat$Species==""CAEQ"" & dat$Treatment==""HN"",c(33,44,59,76,87)])\n#print(dat[dat$Species==""CAEQ"" & dat$Treatment==""PHN"",c(33,44,59,76,87)])\n\n#print(dat[dat$Species==""PSCA"" & dat$Treatment==""LN"",c(33,44,59,76,87)])\n#print(dat[dat$Species==""PSCA"" & dat$Treatment==""MN"",c(33,44,59,76,87)])\n#print(dat[dat$Species==""PSCA"" & dat$Treatment==""HN"",c(33,44,59,76,87)])\n#print(dat[dat$Species==""PSCA"" & dat$Treatment==""PHN"",c(33,44,59,76,87)])\n\n#############################################################\n################ Calculate growth rates #####################\n#############################################################\n\n# ""AGR"" is absolute growth rate and ""RGR"" is relative growth rate. \n# Each is calculated for aboveground biomass, \n# for the following time periods: \n\n# Annual/semiannual increments:\n\n# AGB\n\n# January 2016 to July 2016\ndat$AGR_AGB_2016_01_2016 <- (dat$AGB_est_kg_02 - dat$AGB_est_kg_01)/((dat$Days_02 - dat$Days_01)/365.25)\ndat$RGR_AGB_2016_01_2016 <- (log(dat$AGB_est_kg_02) - log(dat$AGB_est_kg_01))/((dat$Days_02 - dat$Days_01)/365.25)\n\n# July 2016 to June 2017\ndat$AGR_AGB_2016_2017 <- (dat$AGB_est_kg_03 - dat$AGB_est_kg_02)/((dat$Days_03 - dat$Days_02)/365.25)\ndat$RGR_AGB_2016_2017 <- (log(dat$AGB_est_kg_03) - log(dat$AGB_est_kg_02))/((dat$Days_03 - dat$Days_02)/365.25)\n\n# June 2017 to June 2018\ndat$AGR_AGB_2017_2018 <- (dat$AGB_est_kg_04 - dat$AGB_est_kg_03)/((dat$Days_04 - dat$Days_03)/365.25)\ndat$RGR_AGB_2017_2018 <- (log(dat$AGB_est_kg_04) - log(dat$AGB_est_kg_03))/((dat$Days_04 - dat$Days_03)/365.25)\n\n# June 2018 to July 2019\ndat$AGR_AGB_2018_2019 <- (dat$AGB_est_kg_06 - dat$AGB_est_kg_04)/((dat$Days_06 - dat$Days_04)/365.25)\ndat$RGR_AGB_2018_2019 <- (log(dat$AGB_est_kg_06) - log(dat$AGB_est_kg_04))/((dat$Days_06 - dat$Days_04)/365.25)\n\n# June 2018 to January 2019\ndat$AGR_AGB_2018_2019_01 <- (dat$AGB_est_kg_05 - dat$AGB_est_kg_04)/((dat$Days_05 - dat$Days_04)/365.25)\ndat$RGR_AGB_2018_2019_01 <- (log(dat$AGB_est_kg_05) - log(dat$AGB_est_kg_04))/((dat$Days_05 - dat$Days_04)/365.25)\n\n# January 2019 to July 2019\ndat$AGR_AGB_2019_01_2019 <- (dat$AGB_est_kg_06 - dat$AGB_est_kg_05)/((dat$Days_06 - dat$Days_05)/365.25)\ndat$RGR_AGB', '# NSF FX growth analysis for the New York biomass data\n\n# There are a number of ways we could analyze biomass growth rates\n# for these trees. Each of these will be separate for each species.\n# - Compute absolute and relative growth rates for each time period\n# - Compute absolute and relative growth rates for each year\n# - Compute absolute and relative growth rates from t0 to each tf\n# For each of these, we want to analyze growth as a function of \n# treatment, which answers our initial scientific question:\n# Is the species limited by N and/or P at each treatment level?\n# We also want to account for the fact that growth rate varies with\n# tree size, possibly individual trees, and possibly with year.\n# Also want to do these for each year: Up to 2016, 2017, 2018, 2019.\n\n# How to handle odd datapoints? By odd datapoints, I mean ones that \n# result in negative growth or are otherwise anomalous compared to the\n# previous/expected growth rate of the tree.\n# There are a reasons why datapoints could be odd:\n# - Measurement or data input error\n# - Point loss of biomass (branches etc.) due to physical damage\n# - Persistent loss of biomass due to pest damage or other illness\n# The way we deal with odd datapoints would be different for each.\n# - Measurement or data input error should correct themselves over time, \n# although negative relative growth rates will be tricky.\n# - Point loss of biomass should be a ""restart"" of growth measurements.\n# - Persistent loss of biomass due to pest damage or other illness should\n# be removed from the analysis, as it does not address the question.\n\n# I edited the ""Use_growth_0X"" columns in ""NY_FX_Height_Diam_FoliarCNIsotope_Data.csv"" and \n# ""NY_FX_Size_FoliarCNIsotope_Data.csv"" to reflect these odd datapoints\n# based on biomass trends.\n# 1 means not odd in any way.\n# 0 means sufficiently ill or damaged that it shouldn\'t be used.\n\nrm(list=ls())\n\nlibrary(lme4)\nlibrary(nlme)\nlibrary(emmeans)\n\nsetwd(""/Users/duncanmenge/Documents/Academia/Grants/NSF/2014_Strategies/Data/Master datafiles/"")\n\ndat <- read.csv(""NY_FX_Size_FoliarCNIsotope_Data.csv"")[1:66,1:166]\n\n# Option to print out how many are in each category (uncomment)\n\n#print(dat[dat$Species==""BENI"" & dat$Treatment==""LN"",c(32,64,99,123)])\n#print(dat[dat$Species==""BENI"" & dat$Treatment==""MN"",c(32,64,99,123)])\n#print(dat[dat$Species==""BENI"" & dat$Treatment==""HN"",c(32,64,99,123)])\n#print(dat[dat$Species==""BENI"" & dat$Treatment==""PHN"",c(32,64,99,123)])\n\n#print(dat[dat$Species==""ROPS"" & dat$Treatment==""LN"",c(32,64,99,123)])\n#print(dat[dat$Species==""ROPS"" & dat$Treatment==""MN"",c(32,64,99,123)])\n#print(dat[dat$Species==""ROPS"" & dat$Treatment==""HN"",c(32,64,99,123)])\n#print(dat[dat$Species==""ROPS"" & dat$Treatment==""PHN"",c(32,64,99,123)])\n\n#############################################################\n################ Calculate growth rates #####################\n#############################################################\n\n# ""AGR"" is absolute growth rate and ""RGR"" is relative growth rate. \n# Each is calculated for aboveground, belowground, and total biomass, \n# for the following time periods: \n\n# Annual increments:\n\n# Biomass\n\n# June 2015 to October 2016\ndat$AGR_Biomass_2015_2016 <- (dat$Biomass_est_kg_02 - dat$Biomass_est_kg_01)/((dat$Days_02 - dat$Days_01)/365.25)\ndat$RGR_Biomass_2015_2016 <- (log(dat$Biomass_est_kg_02) - log(dat$Biomass_est_kg_01))/((dat$Days_02 - dat$Days_01)/365.25)\n\n# October 2016 to October 2017\ndat$AGR_Biomass_2016_2017 <- (dat$Biomass_est_kg_04 - dat$Biomass_est_kg_02)/((dat$Days_04 - dat$Days_02)/365.25)\ndat$RGR_Biomass_2016_2017 <- (log(dat$Biomass_est_kg_04) - log(dat$Biomass_est_kg_02))/((dat$Days_04 - dat$Days_02)/365.25)\n\n# October 2017 to September 2018\ndat$AGR_Biomass_2017_2018 <- (dat$Biomass_est_kg_07 - dat$Biomass_est_kg_04)/((dat$Days_07 - dat$Days_04)/365.25)\ndat$RGR_Biomass_2017_2018 <- (log(dat$Biomass_est_kg_07) - log(dat$Biomass_est_kg_04))/((dat$Days_07 - dat$Days_04)/365.25)\n\n# September 2018 to October 2019\ndat$AGR_Biomass_2018_2019 <- (dat$Biomass_est_kg_08 - dat$Biomass_est_kg_07)/((dat$Days_08 - dat$Days_07)/365.25)\ndat$RGR_Biomass_2018_2019 <- (log(dat$Biomass_est_kg_08) - log(dat$Biomass_est_kg_07))/((dat$Days_08 - dat$Days_07)/365.25)\n\n# AGB\n\n# June 2015 to October 2016\ndat$AGR_AGB_2015_2016 <- (dat$AGB_est_kg_02 - dat$AGB_est_kg_01)/((dat$Days_02 - dat$Days_01)/365.25)\ndat$RGR_AGB_2015_2016 <- (log(dat$AGB_est_kg_02) - log(dat$AGB_est_kg_01))/((dat$Days_02 - dat$Days_01)/365.25)\n\n# October 2016 to October 2017\ndat$AGR_AGB_2016_2017 <- (dat$AGB_est_kg_04 - dat$AGB_est_kg_02)/((dat$Days_04 - dat$Days_02)/365.25)\ndat$RGR_AGB_2016_2017 <- (log(dat$AGB_est_kg_04) - log(dat$AGB_est_kg_02))/((dat$Days_04 - dat$Days_02)/365.25)\n\n# October 2017 to September 2018\ndat$AGR_AGB_2017_2018 <- (dat$AGB_est_kg_07 - dat$AGB_est_kg_04)/((dat$Days_07 - dat$Days_04)/365.25)\ndat$RGR_AGB_2017_2018 <- (log(dat$AGB_est_kg_07) - log(dat$AGB_est_kg_04))/((dat$Days_07 - dat$Days_04)/365.25)\n\n# September 2018 to October 2019\n', '# NSF FX growth analysis for the Oregon biomass data\n\n# There are a number of ways we could analyze biomass growth rates\n# for these trees. Each of these will be separate for each species.\n# - Compute absolute and relative growth rates for each time period\n# - Compute absolute and relative growth rates for each year\n# - Compute absolute and relative growth rates from t0 to each tf\n# For each of these, we want to analyze growth as a function of \n# treatment, which answers our initial scientific question:\n# Is the species limited by N and/or P at each treatment level?\n# We also want to account for the fact that growth rate varies with\n# tree size, possibly individual trees, and possibly with year.\n# Also want to do these for each year: Up to 2017, 2018, 2019, 2020.\n\n# How to handle odd datapoints? By odd datapoints, I mean ones that \n# result in negative growth or are otherwise anomalous compared to the\n# previous/expected growth rate of the tree.\n# There are a reasons why datapoints could be odd:\n# - Measurement or data input error\n# - Point loss of biomass (branches etc.) due to physical damage\n# - Persistent loss of biomass due to pest damage or other illness\n# The way we deal with odd datapoints would be different for each.\n# - Measurement or data input error should correct themselves over time, \n# although negative relative growth rates will be tricky.\n# - Point loss of biomass should be a ""restart"" of growth measurements.\n# - Persistent loss of biomass due to pest damage or other illness should\n# be removed from the analysis, as it does not address the question.\n\n# I edited the ""Use_growth_0X"" columns in ""OR_FX_Height_Diam_FoliarCNIsotope_Data.csv"" and \n# ""OR_FX_Size_FoliarCNIsotope_Data.csv"" to reflect these odd datapoints\n# based on biomass trends.\n# 1 means not odd in any way.\n# 0 means sufficiently ill or damaged that it shouldn\'t be used.\n# Only one tree (a MN ALRU) in OR is odd, corresponding to notes saying it\'s dead.\n\nrm(list=ls())\n\nlibrary(lme4)\nlibrary(nlme)\nlibrary(emmeans)\n\nsetwd(""/Users/duncanmenge/Documents/Academia/Grants/NSF/2014_Strategies/Data/Master datafiles/"")\n\ndat <- read.csv(""OR_FX_Size_FoliarCNIsotope_Data.csv"")[1:64,1:122]\n\n# Option to print out how many are in each category (uncomment)\n\n#print(dat[dat$Species==""PSME"" & dat$Treatment==""LN"",c(43,54,71,87)])\n#print(dat[dat$Species==""PSME"" & dat$Treatment==""MN"",c(43,54,71,87)])\n#print(dat[dat$Species==""PSME"" & dat$Treatment==""HN"",c(43,54,71,87)])\n#print(dat[dat$Species==""PSME"" & dat$Treatment==""PHN"",c(43,54,71,87)])\n\n#print(dat[dat$Species==""ALRU"" & dat$Treatment==""LN"",c(43,54,71,87)])\n#print(dat[dat$Species==""ALRU"" & dat$Treatment==""MN"",c(43,54,71,87)])\n#print(dat[dat$Species==""ALRU"" & dat$Treatment==""HN"",c(43,54,71,87)])\n#print(dat[dat$Species==""ALRU"" & dat$Treatment==""PHN"",c(43,54,71,87)])\n\n#############################################################\n################ Calculate growth rates #####################\n#############################################################\n\n# ""AGR"" is absolute growth rate and ""RGR"" is relative growth rate. \n# Each is calculated for aboveground, belowground, and total biomass, \n# for the following time periods: \n\n# Annual increments:\n\n# Biomass\n\n# June 2016 to October 2017\ndat$AGR_Biomass_2016_2017 <- (dat$Biomass_est_kg_03 - dat$Biomass_est_kg_01)/((dat$Days_03 - dat$Days_01)/365.25)\ndat$RGR_Biomass_2016_2017 <- (log(dat$Biomass_est_kg_03) - log(dat$Biomass_est_kg_01))/((dat$Days_03 - dat$Days_01)/365.25)\n\n# October 2017 to October 2018\ndat$AGR_Biomass_2017_2018 <- (dat$Biomass_est_kg_04 - dat$Biomass_est_kg_03)/((dat$Days_04 - dat$Days_03)/365.25)\ndat$RGR_Biomass_2017_2018 <- (log(dat$Biomass_est_kg_04) - log(dat$Biomass_est_kg_03))/((dat$Days_04 - dat$Days_03)/365.25)\n\n# October 2018 to September 2019\ndat$AGR_Biomass_2018_2019 <- (dat$Biomass_est_kg_05 - dat$Biomass_est_kg_04)/((dat$Days_05 - dat$Days_04)/365.25)\ndat$RGR_Biomass_2018_2019 <- (log(dat$Biomass_est_kg_05) - log(dat$Biomass_est_kg_04))/((dat$Days_05 - dat$Days_04)/365.25)\n\n# September 2019 to September 2020\ndat$AGR_Biomass_2019_2020 <- (dat$Biomass_est_kg_06 - dat$Biomass_est_kg_05)/((dat$Days_06 - dat$Days_05)/365.25)\ndat$RGR_Biomass_2019_2020 <- (log(dat$Biomass_est_kg_06) - log(dat$Biomass_est_kg_05))/((dat$Days_06 - dat$Days_05)/365.25)\n\n# AGB\n\n# June 2016 to October 2017\ndat$AGR_AGB_2016_2017 <- (dat$AGB_est_kg_03 - dat$AGB_est_kg_01)/((dat$Days_03 - dat$Days_01)/365.25)\ndat$RGR_AGB_2016_2017 <- (log(dat$AGB_est_kg_03) - log(dat$AGB_est_kg_01))/((dat$Days_03 - dat$Days_01)/365.25)\n\n# October 2017 to October 2018\ndat$AGR_AGB_2017_2018 <- (dat$AGB_est_kg_04 - dat$AGB_est_kg_03)/((dat$Days_04 - dat$Days_03)/365.25)\ndat$RGR_AGB_2017_2018 <- (log(dat$AGB_est_kg_04) - log(dat$AGB_est_kg_03))/((dat$Days_04 - dat$Days_03)/365.25)\n\n# October 2018 to September 2019\ndat$AGR_AGB_2018_2019 <- (dat$AGB_est_kg_05 - dat$AGB_est_kg_04)/((dat$Days_05 - dat$Days_04)/365.25)\ndat$RGR_AGB_2018_2019 <- (log(dat$AGB_est_kg_05) - log(dat$AGB_est_kg_0', '# NSF FX data plotting\n\n# Figure 1\n# Aboveground Biomass absolute growth rate from 2016-2018 for Pase paper\n\nrm(list=ls())\n\nsetwd(""/Users/duncanmenge/Documents/Academia/Grants/NSF/2014_Strategies/Data/Master datafiles/"")\n\nlibrary(colorBlindness)\n\n#########################################################################\n############################# Set up figure #############################\n#########################################################################\n\n#pdf(file=""Pase_Fig01.pdf"",\n#\twidth=5,height=4)\nsetEPS()\npostscript(file=""Pase_Fig01.eps"",\n           width=5,height=4)\n# omi and mai coordinates are bottom, left, top, right\npar(mfrow=c(4,4),omi=c(.4,.25,.2,.2),mai=c(0,.3,0,0))\n\n##############################################\n################## New York ##################\n##############################################\n\n# Run code to get the data to plot\nsource(""NY_growth_analysis.R"")\n# Run code for plotting specifics\nsource(""Fig_3-6_setup.R"")\n\nnonfixcol <- Blue2DarkRed12Steps[1]\nrhizcol <- Blue2DarkRed12Steps[11]\nactcol <- Blue2DarkRed12Steps[9]\n\n# Filter out growth and size metrics from dead/unhealthy trees\n\ndat[dat$Use_growth_02 == 0,]$AGR_AGB_2015_2016 <- NA\ndat[dat$Use_growth_04 == 0,]$AGR_AGB_2015_2017 <- NA\ndat[dat$Use_growth_04 == 0,]$AGR_AGB_2016_2017 <- NA\ndat[dat$Use_growth_07 == 0,]$AGR_AGB_2015_2018 <- NA\ndat[dat$Use_growth_07 == 0,]$AGR_AGB_2017_2018 <- NA\n\ndat[dat$Use_growth_01 == 0,]$AGB_est_kg_01 <- NA\ndat[dat$Use_growth_02 == 0,]$AGB_est_kg_02 <- NA\ndat[dat$Use_growth_04 == 0,]$AGB_est_kg_04 <- NA\ndat[dat$Use_growth_07 == 0,]$AGB_est_kg_07 <- NA\n\nplot(jitter(dat[dat$Species==""ROPS"" & !is.na(dat$AGR_AGB_2015_2016),]$TRT+0.1),\n\tdat[dat$Species==""ROPS"" & !is.na(dat$AGR_AGB_2015_2016),]$AGR_AGB_2015_2016,\n\tcol=rhizcol,pch=2,cex=0.8,xlim=c(-0.5,3.5),xaxt=""n"",yaxt=""n"",\n\tylim=c(0.002,2),log=""y"")\naxis(side=1, at=c(0,2), labels=c(""C"",""15""), padj=-.5,cex.axis=0.8)\naxis(side=1, at=c(1,3), labels=c(""10"",""   15+P""), padj=-.5,cex.axis=0.8)\naxis(side=2, at=c(0.01,1),labels=c(""0.01"",""1""),cex.axis=0.8)\naxis(side=2, at=c(0.1),labels=c(""0.1""),cex.axis=0.8)\naxis(side=2, at=seq(0.002,0.009,0.001),labels=FALSE,lwd.ticks = 0.25)\naxis(side=2, at=seq(0.02,0.09,0.01),labels=FALSE,lwd.ticks = 0.25)\naxis(side=2, at=seq(0.2,0.9,0.1),labels=FALSE,lwd.ticks = 0.25)\naxis(side=2, at=c(2),labels=FALSE,lwd.ticks = 0.25)\npoints(jitter(dat[dat$Species==""BENI"" & !is.na(dat$AGR_AGB_2015_2016),]$TRT-0.1),\n\tdat[dat$Species==""BENI"" & !is.na(dat$AGR_AGB_2015_2016),]$AGR_AGB_2015_2016,\n\tcol=nonfixcol,pch=1,cex=0.8)\t\nmtext(""a"",at=-0.35,padj=1.5,cex=0.8)\nmtext(""2015-2016"",at=1.5,cex=0.8)\nabline(v=c(0.5,1.5,2.5),lty=c(2,2,1),col=""gray"")\n\nplot(jitter(dat[dat$Species==""ROPS"" & !is.na(dat$AGR_AGB_2016_2017),]$TRT+0.1),\n\tdat[dat$Species==""ROPS"" & !is.na(dat$AGR_AGB_2016_2017),]$AGR_AGB_2016_2017,\n\tcol=rhizcol,pch=2,cex=0.8,xlim=c(-0.5,3.5),xaxt=""n"",yaxt=""n"",\n\tylim=c(0.005,15),log=""y"")\naxis(side=2, at=c(0.01,1,10),labels=c(""0.01"",""1"",""10""),cex.axis=0.8)\naxis(side=2, at=c(0.1),labels=c(""0.1""),cex.axis=0.8)\naxis(side=2, at=seq(0.005,0.009,0.001),labels=FALSE,lwd.ticks = 0.25)\naxis(side=2, at=seq(0.02,0.09,0.01),labels=FALSE,lwd.ticks = 0.25)\naxis(side=2, at=seq(0.2,0.9,0.1),labels=FALSE,lwd.ticks = 0.25)\naxis(side=2, at=seq(2,9,1),labels=FALSE,lwd.ticks = 0.25)\npoints(jitter(dat[dat$Species==""BENI"" & !is.na(dat$AGR_AGB_2016_2017),]$TRT-0.1),\n\tdat[dat$Species==""BENI"" & !is.na(dat$AGR_AGB_2016_2017),]$AGR_AGB_2016_2017,\n\tcol=nonfixcol,pch=1,cex=0.8)\t\nmtext(""b"",at=-0.35,padj=1.5,cex=0.8)\nmtext(""2016-2017"",at=1.5,cex=0.8)\nabline(v=c(0.5,1.5,2.5),lty=c(2,2,1),col=""gray"")\n\nplot(jitter(dat[dat$Species==""ROPS"" & !is.na(dat$AGR_AGB_2017_2018),]$TRT+0.1),\n\tdat[dat$Species==""ROPS"" & !is.na(dat$AGR_AGB_2017_2018),]$AGR_AGB_2017_2018,\n\tcol=rhizcol,pch=2,cex=0.8,xlim=c(-0.5,3.5),xaxt=""n"",yaxt=""n"",\n\tylim=c(0.01,50),log=""y"")\naxis(side=2, at=c(0.01,1,10),labels=c(""0.01"",""1"",""10""),cex.axis=0.8)\naxis(side=2, at=c(0.1),labels=c(""0.1""),cex.axis=0.8)\naxis(side=2, at=seq(0.02,0.09,0.01),labels=FALSE,lwd.ticks = 0.25)\naxis(side=2, at=seq(0.2,0.9,0.1),labels=FALSE,lwd.ticks = 0.25)\naxis(side=2, at=seq(2,9,1),labels=FALSE,lwd.ticks = 0.25)\naxis(side=2, at=seq(20,40,10),labels=FALSE,lwd.ticks = 0.25)\npoints(jitter(dat[dat$Species==""BENI"" & !is.na(dat$AGR_AGB_2017_2018),]$TRT-0.1),\n\tdat[dat$Species==""BENI"" & !is.na(dat$AGR_AGB_2017_2018),]$AGR_AGB_2017_2018,\n\tcol=nonfixcol,pch=1,cex=0.8)\t\nmtext(""c"",at=-0.35,padj=1.5,cex=0.8)\nmtext(""2017-2018"",at=1.5,cex=0.8)\nabline(v=c(0.5,1.5,2.5),lty=c(2,2,1),col=""gray"")\n\nplot(jitter(dat[dat$Species==""ROPS"" & !is.na(dat$AGR_AGB_2015_2018),]$TRT+0.1),\n\tdat[dat$Species==""ROPS"" & !is.na(dat$AGR_AGB_2015_2018),]$AGR_AGB_2015_2018,\n\tcol=""white"",pch=2,cex=0.8,xlim=c(-0.5,3.5),xaxt=""n"",yaxt=""n"",\n\tylim=c(0.03,50),log=""y"")\naxis(side=2, at=c(0.1,1,10),labels=c(""0.1"",""1"",""10""),cex.axis=0.8)\naxis(side=2, at=seq(0.2,0.9,0.1),labels=FALSE,lwd.ticks = 0.25)\naxis(side=2, at=seq(2,9,1),labels=FALSE,lwd.ticks = 0.25)\naxis(side=2, at=seq(20,40', '# NSF FX data plotting\n\n# Figure 2\n# Soil phosphatase by treatment and species\n\nrm(list=ls())\n\nsetwd(""/Users/duncanmenge/Documents/Academia/Grants/NSF/2014_Strategies/Data/Master datafiles/"")\n\nlibrary(colorBlindness)\n\n#########################################################################\n############################# Set up figure #############################\n#########################################################################\n\n#pdf(file=""Pase_Fig02.pdf"",\n#\twidth=6,height=4)\nsetEPS()\npostscript(file=""Pase_Fig02.eps"",\n           width=6,height=4)\n# omi and mai coordinates are bottom, left, top, right\npar(mfrow=c(2,3),omi=c(.4,.25,.2,.2),mai=c(.2,.3,0,0))\n\nN <- read.csv(""NY_FX_Soil_Pase.csv"")\nO <- read.csv(""OR_FX_Soil_Pase.csv"")\nW <- read.csv(""HI_W_FX_Soil_Pase.csv"")\nV <- read.csv(""HI_V_FX_Soil_Pase.csv"")\n\nN_TRT <- rep(-1,nrow(N))\nN_TRT[N$Treatment==""LN""] <- 0\nN_TRT[N$Treatment==""MN""] <- 1\nN_TRT[N$Treatment==""HN""] <- 2\nN_TRT[N$Treatment==""PHN""] <- 3\nN_jTRT <- jitter(N_TRT)\n\nO_TRT <- rep(-1,nrow(O))\nO_TRT[O$Treatment==""LN""] <- 0\nO_TRT[O$Treatment==""MN""] <- 1\nO_TRT[O$Treatment==""HN""] <- 2\nO_TRT[O$Treatment==""PHN""] <- 3\nO_jTRT <- jitter(O_TRT)\n\nW_TRT <- rep(-1,nrow(W))\nW_TRT[W$Treatment==""LN""] <- 0\nW_TRT[W$Treatment==""MN""] <- 1\nW_TRT[W$Treatment==""HN""] <- 2\nW_TRT[W$Treatment==""PHN""] <- 3\nW_jTRT <- jitter(W_TRT,amount=0.1)\n\nV_TRT <- rep(-1,nrow(V))\nV_TRT[V$Treatment==""LN""] <- 0\nV_TRT[V$Treatment==""MN""] <- 1\nV_TRT[V$Treatment==""HN""] <- 2\nV_TRT[V$Treatment==""PHN""] <- 3\nV_jTRT <- jitter(V_TRT,amount=0.1)\n\nnonfixcol <- Blue2DarkRed12Steps[1]\nrhizcol <- Blue2DarkRed12Steps[11]\nactcol <- Blue2DarkRed12Steps[9]\n\n##############################################\n################## New York ##################\n##############################################\n\nylx <- max(c(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1,]$Soil_Pase_umol_g_hr_04),na.rm=TRUE)*1.16\nplot(N_jTRT[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI""]-0.25,\n\tN[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"",]$Soil_Pase_umol_g_hr_04,\n\txlim=c(-0.5,3.5),ylim=c(0,ylx),\n\tpch=1,xaxt=""n"",col=nonfixcol,cex=1.5,cex.axis=0.8)\nabline(h=0,col=""gray"")\npoints(N_jTRT[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""ROPS""]+0.25,\n\tN[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""ROPS"",]$Soil_Pase_umol_g_hr_04,\n\tcol=rhizcol,pch=2,cex=1.5)\n\nBENI_04_LN_u <- mean(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04,na.rm=TRUE)\nBENI_04_LN_se <- sd(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04,na.rm=TRUE)/\n  sqrt(length(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04))\npoints(-0.25,BENI_04_LN_u,pch=16,col=nonfixcol,cex=2)\nsegments(-0.25,BENI_04_LN_u - BENI_04_LN_se,-0.25,BENI_04_LN_u + BENI_04_LN_se,col=nonfixcol)\nBENI_04_MN_u <- mean(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""MN"",]$Soil_Pase_umol_g_hr_04,na.rm=TRUE)\nBENI_04_MN_se <- sd(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""MN"",]$Soil_Pase_umol_g_hr_04,na.rm=TRUE)/\n  sqrt(length(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""MN"",]$Soil_Pase_umol_g_hr_04))\npoints(0.75,BENI_04_MN_u,pch=16,col=nonfixcol,cex=2)\nsegments(0.75,BENI_04_MN_u - BENI_04_MN_se,0.75,BENI_04_MN_u + BENI_04_MN_se,col=nonfixcol)\nBENI_04_HN_u <- mean(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""HN"",]$Soil_Pase_umol_g_hr_04,na.rm=TRUE)\nBENI_04_HN_se <- sd(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""HN"",]$Soil_Pase_umol_g_hr_04,na.rm=TRUE)/\n  sqrt(length(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""HN"",]$Soil_Pase_umol_g_hr_04))\npoints(1.75,BENI_04_HN_u,pch=16,col=nonfixcol,cex=2)\nsegments(1.75,BENI_04_HN_u - BENI_04_HN_se,1.75,BENI_04_HN_u + BENI_04_HN_se,col=nonfixcol)\nBENI_04_PHN_u <- mean(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""PHN"",]$Soil_Pase_umol_g_hr_04,na.rm=TRUE)\nBENI_04_PHN_se <- sd(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""PHN"",]$Soil_Pase_umol_g_hr_04,na.rm=TRUE)/\n  sqrt(length(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""PHN"",]$Soil_Pase_umol_g_hr_04))\npoints(2.75,BENI_04_PHN_u,pch=16,col=nonfixcol,cex=2)\nsegments(2.75,BENI_04_PHN_u - BENI_04_PHN_se,2.75,BENI_04_PHN_u + BENI_04_PHN_se,col=nonfixcol)\n\nROPS_04_LN_u <- mean(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04,na.rm=TRUE)\nROPS_04_LN_se <- sd(N[!is.na(N$Soil_Pase_umol_g_hr_', '# NSF FX data plotting\n\n# Figure 3\n# Soil phosphatase by treatment and species and symbiotic N fixation\n\nrm(list=ls())\n\nsetwd(""/Users/duncanmenge/Documents/Academia/Grants/NSF/2014_Strategies/Data/Master datafiles/"")\n\nlibrary(colorBlindness)\n\n#########################################################################\n############################# Set up figure #############################\n#########################################################################\n\n#pdf(file=""Pase_Fig03.pdf"",\n#\twidth=6,height=4)\nsetEPS()\npostscript(file=""Pase_Fig03.eps"",\n           width=6,height=4)\n# omi and mai coordinates are bottom, left, top, right\npar(mfrow=c(2,3),omi=c(.4,.25,.2,.2),mai=c(.2,.3,0,0))\n\nN <- read.csv(""NY_FX_Soil_Pase.csv"")\nN_SNF <- read.csv(""NY_FX_SNF.csv"")\nN <- cbind(N,N_SNF[,c(8,11,14:51)])\n\nO <- read.csv(""OR_FX_Soil_Pase.csv"")\nO_SNF <- read.csv(""OR_FX_SNF.csv"")\nO <- cbind(O,O_SNF[,c(8,11,14:44)])\n\nW <- read.csv(""HI_W_FX_Soil_Pase.csv"")\nW_SNF <- read.csv(""HI_W_FX_SNF.csv"")\nW <- cbind(W,W_SNF[,c(8,11:29)])\n\nV <- read.csv(""HI_V_FX_Soil_Pase.csv"")\nV_SNF <- read.csv(""HI_V_FX_SNF.csv"")\nV <- cbind(V,V_SNF[,c(8,11:29)])\n\nN_TRT <- rep(-1,nrow(N))\nN_TRT[N$Treatment==""LN""] <- 0\nN_TRT[N$Treatment==""MN""] <- 1\nN_TRT[N$Treatment==""HN""] <- 2\nN_TRT[N$Treatment==""PHN""] <- 3\nN_jTRT <- jitter(N_TRT)\n\nO_TRT <- rep(-1,nrow(O))\nO_TRT[O$Treatment==""LN""] <- 0\nO_TRT[O$Treatment==""MN""] <- 1\nO_TRT[O$Treatment==""HN""] <- 2\nO_TRT[O$Treatment==""PHN""] <- 3\nO_jTRT <- jitter(O_TRT)\n\nW_TRT <- rep(-1,nrow(W))\nW_TRT[W$Treatment==""LN""] <- 0\nW_TRT[W$Treatment==""MN""] <- 1\nW_TRT[W$Treatment==""HN""] <- 2\nW_TRT[W$Treatment==""PHN""] <- 3\nW_jTRT <- jitter(W_TRT,amount=0.1)\n\nV_TRT <- rep(-1,nrow(V))\nV_TRT[V$Treatment==""LN""] <- 0\nV_TRT[V$Treatment==""MN""] <- 1\nV_TRT[V$Treatment==""HN""] <- 2\nV_TRT[V$Treatment==""PHN""] <- 3\nV_jTRT <- jitter(V_TRT,amount=0.1)\n\nnonfixcol <- Blue2DarkRed12Steps[1]\nrhizcol <- Blue2DarkRed12Steps[11]\nactcol <- Blue2DarkRed12Steps[9]\n\n##############################################\n################## New York ##################\n##############################################\n\nxln <- min(c(0,N[N$Use_growth_04==1,]$Ndfa_u_04),na.rm=TRUE)-5\nxlx <- max(c(100,N[N$Use_growth_04==1,]$Ndfa_u_04),na.rm=TRUE)\nylx <- max(c(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1,]$Soil_Pase_umol_g_hr_04),na.rm=TRUE)*1.02\n\nplot(N[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""LN"",]$Ndfa_u_04,\n\tN[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04,\n\txlim=c(xln,xlx),ylim=c(0,ylx),xaxt=""n"",#yaxt=""n"",\n\tpch=2,col=rhizcol,cex=1.5,cex.axis=0.8)\npoints(N[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""MN"",]$Ndfa_u_04,\n\tN[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""MN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=3,col=rhizcol,cex=1.5)\npoints(N[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""HN"",]$Ndfa_u_04,\n\tN[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""HN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=4,col=rhizcol,cex=1.5)\npoints(N[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""PHN"",]$Ndfa_u_04,\n\tN[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""PHN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=5,col=rhizcol,cex=1.5)\nabline(v=c(0,100),col=""gray"")\nabline(h=0,col=""gray"")\npoints(rep(0,nrow(N[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""LN"",])),\n\tN[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=2,col=nonfixcol,cex=1.5)\npoints(rep(0,nrow(N[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""MN"",])),\n\tN[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""MN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=3,col=nonfixcol,cex=1.5)\npoints(rep(0,nrow(N[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""HN"",])),\n\tN[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""HN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=4,col=nonfixcol,cex=1.5)\npoints(rep(0,nrow(N[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""PHN"",])),\n\tN[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""PHN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=5,col=nonfixcol,cex=1.5)\nmtext(""a"",at=10,padj=1.5,cex=0.8)\nmtext(""New York 2017"",at=mean(c(xln,xlx)),cex=0.8)\n\nsummary(NY_FX_Pase_Ndfa_u_04_lm <- lm(Soil_Pase_umol_g_hr_04 ~ \n\tNdfa_u_04 * Treatment,data=N[N$Use_growth_04==1 & N$Species==""ROPS"",]))\nanova(NY_FX_Pase_Ndfa_u_04_lm)\n# Nothing significant\n\nylx <- max(c(N[!is.na(N$Soil_Pase_umol_g_hr_07) & N$Use_growth_07==1,]$Soil_Pase_umol_g_hr_07),na.rm=TRUE)*1.02\n\nplot(N[N$Use_growth_07==1 & N$Species==""ROPS"" & N$Treatment==""LN"",]$Ndfa_u_07,\n\tN[N$Use_growth_07==1 & N$Species==""ROPS"" & N$Treatment==""LN"",]$Soil_Pase_umol_g_hr_07,\n\txlim=c(xln,xlx),ylim=c(0,ylx),#xaxt=""n"",yaxt=""n"",\n\tpch=2,col=rhizcol,cex=1.5,cex.axis=0.8)\npoints(N[N$Use_growth_07==1 & N$Species==""ROPS"" & N$Treatment==""MN"",]$Ndfa_u_07,\n\tN[N$Use_growth_07==1 & N$Species==""ROPS"" & N$Treatment==""MN"",]$Soil_Pase_umol_g_hr_07,\n\tpch=3,col=rhizcol,cex=1.5)\npoints(N[N$Use_growth_07==1 & N$Species==""ROPS"" & N$Treatment==""HN"",]$Ndfa_u_07,\n\tN[N$Use_growth_07==1 & N$Speci', '# NSF FX data plotting\n\n# Figure 4\n# Soil phosphatase by treatment and species and symbiotic N fixation\n\nrm(list=ls())\n\nsetwd(""/Users/duncanmenge/Documents/Academia/Grants/NSF/2014_Strategies/Data/Master datafiles/"")\n\nlibrary(colorBlindness)\n\n#########################################################################\n############################# Set up figure #############################\n#########################################################################\n\n#pdf(file=""Pase_Fig04.pdf"",\n#\twidth=6,height=4)\nsetEPS()\npostscript(file=""Pase_Fig04.eps"",\n           width=6,height=4)\n# omi and mai coordinates are bottom, left, top, right\npar(mfrow=c(2,3),omi=c(.5,.25,.2,.2),mai=c(.2,.3,0.2,0))\n\nN <- read.csv(""NY_FX_Soil_Pase.csv"")\nN_SNF <- read.csv(""NY_FX_SNF.csv"")\nN <- cbind(N,N_SNF[,c(8,11,14:51)])\n\nO <- read.csv(""OR_FX_Soil_Pase.csv"")\nO_SNF <- read.csv(""OR_FX_SNF.csv"")\nO <- cbind(O,O_SNF[,c(8,11,14:44)])\n\nW <- read.csv(""HI_W_FX_Soil_Pase.csv"")\nW_SNF <- read.csv(""HI_W_FX_SNF.csv"")\nW <- cbind(W,W_SNF[,c(8,11:29)])\n\nV <- read.csv(""HI_V_FX_Soil_Pase.csv"")\nV_SNF <- read.csv(""HI_V_FX_SNF.csv"")\nV <- cbind(V,V_SNF[,c(8,11:29)])\n\nN_TRT <- rep(-1,nrow(N))\nN_TRT[N$Treatment==""LN""] <- 0\nN_TRT[N$Treatment==""MN""] <- 1\nN_TRT[N$Treatment==""HN""] <- 2\nN_TRT[N$Treatment==""PHN""] <- 3\nN_jTRT <- jitter(N_TRT)\n\nO_TRT <- rep(-1,nrow(O))\nO_TRT[O$Treatment==""LN""] <- 0\nO_TRT[O$Treatment==""MN""] <- 1\nO_TRT[O$Treatment==""HN""] <- 2\nO_TRT[O$Treatment==""PHN""] <- 3\nO_jTRT <- jitter(O_TRT)\n\nW_TRT <- rep(-1,nrow(W))\nW_TRT[W$Treatment==""LN""] <- 0\nW_TRT[W$Treatment==""MN""] <- 1\nW_TRT[W$Treatment==""HN""] <- 2\nW_TRT[W$Treatment==""PHN""] <- 3\nW_jTRT <- jitter(W_TRT,amount=0.1)\n\nV_TRT <- rep(-1,nrow(V))\nV_TRT[V$Treatment==""LN""] <- 0\nV_TRT[V$Treatment==""MN""] <- 1\nV_TRT[V$Treatment==""HN""] <- 2\nV_TRT[V$Treatment==""PHN""] <- 3\nV_jTRT <- jitter(V_TRT,amount=0.1)\n\nnonfixcol <- Blue2DarkRed12Steps[1]\nrhizcol <- Blue2DarkRed12Steps[11]\nactcol <- Blue2DarkRed12Steps[9]\n\n##############################################\n################## New York ##################\n##############################################\n\nxln <- min(c(0,N[N$Use_growth_04==1,]$Nfix_g_N_yr_u_04),na.rm=TRUE)\nxlx <- max(N[N$Use_growth_04==1,]$Nfix_g_N_yr_u_04,na.rm=TRUE)\nylx <- max(N[!is.na(N$Soil_Pase_umol_g_hr_04) & N$Use_growth_04==1,]$Soil_Pase_umol_g_hr_04,na.rm=TRUE)*1.02\n\nplot(N[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""LN"",]$Nfix_g_N_yr_u_04,\n\tN[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04,\n\txlim=c(xln,xlx),ylim=c(0,ylx),#xaxt=""n"",#yaxt=""n"",\n\tpch=2,col=rhizcol,cex=1.5,cex.axis=0.8)\npoints(N[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""MN"",]$Nfix_g_N_yr_u_04,\n\tN[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""MN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=3,col=rhizcol,cex=1.5)\npoints(N[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""HN"",]$Nfix_g_N_yr_u_04,\n\tN[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""HN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=4,col=rhizcol,cex=1.5)\npoints(N[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""PHN"",]$Nfix_g_N_yr_u_04,\n\tN[N$Use_growth_04==1 & N$Species==""ROPS"" & N$Treatment==""PHN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=5,col=rhizcol,cex=1.5)\nabline(v=0,col=""gray"")\nabline(h=0,col=""gray"")\npoints(rep(0,nrow(N[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""LN"",])),\n\tN[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=2,col=nonfixcol,cex=1.5)\npoints(rep(0,nrow(N[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""MN"",])),\n\tN[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""MN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=3,col=nonfixcol,cex=1.5)\npoints(rep(0,nrow(N[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""HN"",])),\n\tN[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""HN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=4,col=nonfixcol,cex=1.5)\npoints(rep(0,nrow(N[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""PHN"",])),\n\tN[N$Use_growth_04==1 & N$Species==""BENI"" & N$Treatment==""PHN"",]$Soil_Pase_umol_g_hr_04,\n\tpch=5,col=nonfixcol,cex=1.5)\nmtext(""a"",at=xlx/10,padj=1.5,cex=0.8)\nmtext(""New York 2017"",at=mean(c(xln,xlx)),cex=0.8)\n\nsummary(NY_FX_Pase_Nfix_g_N_yr_u_04_lm <- lm(Soil_Pase_umol_g_hr_04 ~ \n\tNfix_g_N_yr_u_04 * Treatment,data=N[N$Use_growth_04==1 & N$Species==""ROPS"",]))\nanova(NY_FX_Pase_Nfix_g_N_yr_u_04_lm)\n# Nothing significant\n\nxln <- min(c(0,N[N$Use_growth_07==1,]$Nfix_g_N_yr_u_07),na.rm=TRUE)\nxlx <- max(N[N$Use_growth_07==1,]$Nfix_g_N_yr_u_07,na.rm=TRUE)\nylx <- max(N[!is.na(N$Soil_Pase_umol_g_hr_07) & N$Use_growth_07==1,]$Soil_Pase_umol_g_hr_07,na.rm=TRUE)*1.02\n\nplot(N[N$Use_growth_07==1 & N$Species==""ROPS"" & N$Treatment==""LN"",]$Nfix_g_N_yr_u_07,\n\tN[N$Use_growth_07==1 & N$Species==""ROPS"" & N$Treatment==""LN"",]$Soil_Pase_umol_g_hr_07,\n\txlim=c(xln,xlx),ylim=c(0,ylx),#xaxt=""n"",yaxt=""n"",\n\tpch=2,col=rhizcol,cex=1.5,cex.axis=0.8)\npoints(N[N$Use_growth_07==1 & N$Species==""ROPS"" & N$Treatment==""MN"",]$Nfix_g_N_yr_u_07,\n\tN[N$Use_growth_07==1 & N$Species==', '# NSF FX data\n\n# Power analysis for Pase paper\n\nrm(list=ls())\n\nsetwd(""/Users/duncanmenge/Documents/Academia/Grants/NSF/2014_Strategies/Data/Master datafiles/"")\n\n# Read in data\n\nN <- read.csv(""NY_FX_Soil_Pase.csv"")\nO <- read.csv(""OR_FX_Soil_Pase.csv"")\nW <- read.csv(""HI_W_FX_Soil_Pase.csv"")\nV <- read.csv(""HI_V_FX_Soil_Pase.csv"")\n\n# Run stats needed for power analysis\n\nsummary(NY_FX_Pase_04_lm <- lm(Soil_Pase_umol_g_hr_04 ~ Species * Treatment,\n                               data=N[N$Use_growth_04==1,]))\nsummary(NY_FX_Pase_07_lm <- lm(Soil_Pase_umol_g_hr_07 ~ Species * Treatment,\n                               data=N[N$Use_growth_07==1,]))\nsummary(OR_FX_Pase_04_lm <- lm(Soil_Pase_umol_g_hr_04 ~ Species * Treatment,\n                               data=O[O$Use_growth_04==1,]))\nsummary(HI_W_FX_Pase_04_lm <- lm(Soil_Pase_umol_g_hr_04 ~ Species * Treatment,\n                                 data=W[W$Use_growth_04==1,]))\nsummary(HI_V_FX_Pase_04_lm <- lm(Soil_Pase_umol_g_hr_04 ~ Species * Treatment,\n                                 data=V[V$Use_growth_04==1,]))\n\n############################\n###### Power analysis ######\n############################\n\nTreat_eff <- rep(NA,21)\nTreat_P <- array(NA,dim=c(21,5,1000))\nTreat_P_mean <- array(NA,dim=c(21,5))\n\nfor(i in 1:21){\n  \n  Treat_eff[i] <- i-1\n  \n  for(j in 1:1000){\n    \n    NY_04_simdata <- N[N$Use_growth_04==1,c(6,7,10)]\n    SD_NY_04_resids <- sd(NY_FX_Pase_04_lm$residuals)\n    NY_04_simdata$Soil_Pase_umol_g_hr_04 <- \n      rnorm(nrow(NY_04_simdata),mean = 1,sd = SD_NY_04_resids)\n    NY_04_simdata[NY_04_simdata$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04 <- \n      NY_04_simdata[NY_04_simdata$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04 + Treat_eff[i]\n    summary(NY_04_simdata_Pase_lm <- lm(Soil_Pase_umol_g_hr_04 ~ Species * Treatment,\n                                        data=NY_04_simdata))\n    \n    NY_07_simdata <- N[N$Use_growth_07==1,c(6,7,13)]\n    SD_NY_07_resids <- sd(NY_FX_Pase_07_lm$residuals)\n    NY_07_simdata$Soil_Pase_umol_g_hr_07 <- \n      rnorm(nrow(NY_07_simdata),mean = 1,sd = SD_NY_07_resids)\n    NY_07_simdata[NY_07_simdata$Treatment==""LN"",]$Soil_Pase_umol_g_hr_07 <- \n      NY_07_simdata[NY_07_simdata$Treatment==""LN"",]$Soil_Pase_umol_g_hr_07 + Treat_eff[i]\n    summary(NY_07_simdata_Pase_lm <- lm(Soil_Pase_umol_g_hr_07 ~ Species * Treatment,\n                                        data=NY_07_simdata))\n    \n    OR_04_simdata <- O[O$Use_growth_04==1,c(6,7,10)]\n    SD_OR_04_resids <- sd(OR_FX_Pase_04_lm$residuals)\n    OR_04_simdata$Soil_Pase_umol_g_hr_04 <- \n      rnorm(nrow(OR_04_simdata),mean = 1,sd = SD_OR_04_resids)\n    OR_04_simdata[OR_04_simdata$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04 <- \n      OR_04_simdata[OR_04_simdata$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04 + Treat_eff[i]\n    summary(OR_04_simdata_Pase_lm <- lm(Soil_Pase_umol_g_hr_04 ~ Species * Treatment,\n                                        data=OR_04_simdata))\n    \n    HI_W_04_simdata <- W[W$Use_growth_04==1,c(7,8,11)]\n    SD_HI_W_04_resids <- sd(HI_W_FX_Pase_04_lm$residuals)\n    HI_W_04_simdata$Soil_Pase_umol_g_hr_04 <- \n      rnorm(nrow(HI_W_04_simdata),mean = 1,sd = SD_HI_W_04_resids)\n    HI_W_04_simdata[HI_W_04_simdata$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04 <- \n      HI_W_04_simdata[HI_W_04_simdata$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04 + Treat_eff[i]\n    summary(HI_W_04_simdata_Pase_lm <- lm(Soil_Pase_umol_g_hr_04 ~ Species * Treatment,\n                                          data=HI_W_04_simdata))\n    \n    HI_V_04_simdata <- V[V$Use_growth_04==1,c(7,8,11)]\n    SD_HI_V_04_resids <- sd(HI_V_FX_Pase_04_lm$residuals)\n    HI_V_04_simdata$Soil_Pase_umol_g_hr_04 <- \n      rnorm(nrow(HI_V_04_simdata),mean = 1,sd = SD_HI_V_04_resids)\n    HI_V_04_simdata[HI_V_04_simdata$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04 <- \n      HI_V_04_simdata[HI_V_04_simdata$Treatment==""LN"",]$Soil_Pase_umol_g_hr_04 + Treat_eff[i]\n    summary(HI_V_04_simdata_Pase_lm <- lm(Soil_Pase_umol_g_hr_04 ~ Species * Treatment,\n                                          data=HI_V_04_simdata))\n    \n    Treat_P[i,1,j] <- anova(NY_04_simdata_Pase_lm)[[5]][2]\n    Treat_P[i,2,j] <- anova(NY_07_simdata_Pase_lm)[[5]][2]\n    Treat_P[i,3,j] <- anova(OR_04_simdata_Pase_lm)[[5]][2]\n    Treat_P[i,4,j] <- anova(HI_W_04_simdata_Pase_lm)[[5]][2]\n    Treat_P[i,5,j] <- anova(HI_V_04_simdata_Pase_lm)[[5]][2]\n    \n  }\n  \n  Treat_P_mean[i,1] <- mean(Treat_P[i,1,])\n  Treat_P_mean[i,2] <- mean(Treat_P[i,2,])\n  Treat_P_mean[i,3] <- mean(Treat_P[i,3,])\n  Treat_P_mean[i,4] <- mean(Treat_P[i,4,])\n  Treat_P_mean[i,5] <- mean(Treat_P[i,5,])\n  \n}\n\npar(mfrow=c(1,1))\nplot(Treat_eff,Treat_P_mean[,1],xlab=""Treatment effect (umol/g/hr)"",ylab=""P value"",type=""l"",col=""red"")\npoints(Treat_eff,Treat_P_mean[,2],type=""l"",col=""orange"")\npoints(Treat_eff,Treat_P_mean[,3],type=""l"",col=""yellow"")\npoints(Treat_eff,Treat_P_mean[,4],type=""l"",col=""green"")\npoints(Treat_eff,Treat_P_mean[,5],type=""l"",col=""blue"")\nabline(h=0.05)\n\n##########################################################']",4,"symbiotic nitrogen fixation, soil phosphatase activity, temperate trees, tropical trees, extracellular phosphatase enzymes, organic matter, N-fixing plants, high phosphatase activity, multi-site field experiment, rates of N fixation,"
Iterative evolution of large-bodied hypercarnivory in canids benefits species but not clades,"Ecological specialization has costs and benefits at various scales: traits benefitting an individual may disadvantage its population, species or clade. In particular, large body size and hypercarnivory (diet over 70% meat) have evolved repeatedly in mammals; yet large hypercarnivores are thought to be trapped in a macroevolutionary ""ratchet"", marching unilaterally toward decline. Here, we weigh the impact of this specialization on extinction risk using the rich fossil record of North American canids (dogs). In two of three canid subfamilies over the past 40 million years, large-bodied hypercarnivory appears to constrain diversification at the clade level, biasing specialized lineages to extinction. However, despite shorter species durations, extinction rates of large hypercarnivores have been mostly similar to those of all other canids. Extinction was size- and carnivory-selective only at the end of the Pleistocene epoch 11,000 years ago, suggesting that large hypercarnivores were not disadvantaged at the species level before anthropogenic influence.","['################################################################################\n# From Balisi and Van Valkenburgh (2020) Communications Biology\n# CODE FOR FIGURE 2:\n## Ecomorphology of extinct and survived canid species over 17 time slices\n################################################################################\n\n\n# read in data \ndogdata = read.csv(""datasetS1_m1L-BL-DL-m1BS-mass-dur_MB_BVV.csv"", stringsAsFactors=FALSE, row.names=1)\n\n# pull m1BS and log10mass from dogdata \ndata = dogdata[, 4:5]\n# weed out taxa missing one variable or the other \ndata.comp = data[complete.cases(data), ]\n\n# convert dataframe columns into their own vectors \nlogmass = data.comp$log10mass\nnames(logmass) = rownames(data.comp)\nm1bs = data.comp$m1BS\nnames(m1bs) = rownames(data.comp)\n\n# Time slices \n\n# Boundaries (see Supplementary Table 4)\n# OREL / WHIT = 32.2 Ma \n# WHIT / EEAK = 30 \n# EEAK / LEAK = 27.9 \n# LEAK / ELAK = 23.8 \n# ELAK / LLAK = 19.5 \n# LLAK / EHMF = 18.8 \n# EHMF / LHMF = 17.5 \n# LHMF / EBAR = 15.9 \n# EBAR / LBAR = 14.8 \n# LBAR / ECLA = 12.5 \n# ECLA / LCLA = 10.75  # boundary set by myself \n# LCLA / EEHP = 9 \n# EEHP / LEHP = 7.5\n# LEHP / ELHP = 6.7\n# ELHP / LLHP = 5.9\n# LLHP / BLAN = 4.7 \n# BLAN / IRVI = 1.7 \n# IRVI / RANC = 0.45 \n# RANC / HOLO = 0.01 \n\n# read in species lists per time slice\nOREL.csv = read.csv(""01_Orellan.csv"")\nOREL = paste(OREL.csv$GENUS, sep=""_"", OREL.csv$SPECIES)\nWHIT.csv = read.csv(""02_Whitneyan.csv"")\nWHIT = paste(WHIT.csv$GENUS, sep=""_"", WHIT.csv$SPECIES)\nEEAK.csv = read.csv(""03_EEAK.csv"")\nEEAK = paste(EEAK.csv$GENUS, sep=""_"", EEAK.csv$SPECIES)\nLEAK.csv = read.csv(""04_LEAK.csv"")\nLEAK = paste(LEAK.csv$GENUS, sep=""_"", LEAK.csv$SPECIES)\nELAK.csv = read.csv(""05_ELAK.csv"")\nELAK = paste(ELAK.csv$GENUS, sep=""_"", ELAK.csv$SPECIES)\nLLAK.csv = read.csv(""06_LLAK.csv"")\nLLAK = paste(LLAK.csv$GENUS, sep=""_"", LLAK.csv$SPECIES)\nEHMF.csv = read.csv(""07_EHMF.csv"")\nEHMF = paste(EHMF.csv$GENUS, sep=""_"", EHMF.csv$SPECIES)\nLHMF.csv = read.csv(""08_LHMF.csv"")\nLHMF = paste(LHMF.csv$GENUS, sep=""_"", LHMF.csv$SPECIES)\nEBAR.csv = read.csv(""09_EBAR.csv"")\nEBAR = paste(EBAR.csv$GENUS, sep=""_"", EBAR.csv$SPECIES)\nLBAR.csv = read.csv(""10_LBAR.csv"")\nLBAR = paste(LBAR.csv$GENUS, sep=""_"", LBAR.csv$SPECIES)\nECLA.csv = read.csv(""11_ECLA.csv"")\nECLA = paste(ECLA.csv$GENUS, sep=""_"", ECLA.csv$SPECIES)\nLCLA.csv = read.csv(""12_LCLA.csv"")\nLCLA = paste(LCLA.csv$GENUS, sep=""_"", LCLA.csv$SPECIES)\nEEHP.csv = read.csv(""13_EEHP.csv"")\nEEHP = paste(EEHP.csv$GENUS, sep=""_"", EEHP.csv$SPECIES)\nLEHP.csv = read.csv(""14_LEHP.csv"")\nLEHP = paste(LEHP.csv$GENUS, sep=""_"", LEHP.csv$SPECIES)\nEHMP.csv = merge(EEHP.csv, LEHP.csv, all=TRUE)  # merging of EEHP and LEHP \nEHMP = paste(EHMP.csv$GENUS, sep=""_"", EHMP.csv$SPECIES) \nELHP.csv = read.csv(""15_ELHP.csv"")\nELHP = paste(ELHP.csv$GENUS, sep=""_"", ELHP.csv$SPECIES)\nLLHP.csv = read.csv(""16_LLHP.csv"")\nLLHP = paste(LLHP.csv$GENUS, sep=""_"", LLHP.csv$SPECIES)\nLHMP.csv = merge(ELHP.csv, LLHP.csv, all=TRUE)  # merging of ELHP and LLHP \nLHMP = paste(LHMP.csv$GENUS, sep=""_"", LHMP.csv$SPECIES)\nBLAN.csv = read.csv(""17_BLAN.csv"")\nBLAN = paste(BLAN.csv$GENUS, sep=""_"", BLAN.csv$SPECIES)\nIRVI.csv = read.csv(""18_IRVI.csv"")\nIRVI = paste(IRVI.csv$GENUS, sep=""_"", IRVI.csv$SPECIES)\nRANC.csv = read.csv(""19_RANC.csv"")\nRANC = paste(RANC.csv$GENUS, sep=""_"", RANC.csv$SPECIES)\nHOLO.csv = read.csv(""20_HOLO.csv"")\nHOLO = paste(HOLO.csv$GENUS, sep=""_"", HOLO.csv$SPECIES)\n\n\n# COLOR-CODED BY SUBFAMILY\n# also colors changed for red-blue color-blind folks\n# 05/28/2019: changed font sizes for publication\ndogcolors = c(""green"", ""red"", ""blue"")\n\npar(mfrow=c(5,4), oma=c(4,4,0,0)+0.1, mar=c(1,2,0,0)+0.1)\n\nplot(10^logmass[c(OREL, WHIT)], m1bs[c(OREL, WHIT)], type=""n"", log=""x"",\n     xlim=range(10^logmass, na.rm=T), axes=FALSE,\n     ylim=c(range(m1bs, na.rm=T)[1], range(m1bs, na.rm=T)[2]+0.002))\nrect(14.488, 0.111, 50, 0.15, col=""whitesmoke"", border=NA)\npoints(10^logmass[OREL][OREL %in% WHIT], m1bs[OREL][OREL %in% WHIT], \n       pch=c(19, 15, 17)[OREL.csv$X.SUB.FAMILY][OREL %in% WHIT],\n       col=dogcolors[OREL.csv$X.SUB.FAMILY][OREL %in% WHIT], cex=3)\npoints(10^logmass[OREL][! OREL %in% WHIT], m1bs[OREL][! OREL %in% WHIT], \n       pch=c(0, 0, 2)[OREL.csv$X.SUB.FAMILY][! OREL %in% WHIT], \n       col=dogcolors[OREL.csv$X.SUB.FAMILY][! OREL %in% WHIT], cex=3)\naxis(side=1, labels=FALSE)\naxis(side=2, labels=TRUE, cex.axis=2)\nbox(which=""plot"", bty=""l"")\ntext(""32.2 Ma"", x=10^0.4, y=0.14, cex=2.5)\n\nplot(10^logmass[c(WHIT, EEAK)], m1bs[c(WHIT, EEAK)], type=""n"", log=""x"",\n     xlim=range(10^logmass, na.rm=T), axes=FALSE,\n     ylim=c(range(m1bs, na.rm=T)[1], range(m1bs, na.rm=T)[2]+0.002)) \nrect(14.488, 0.111, 50, 0.15, col=""whitesmoke"", border=NA)\npoints(10^logmass[WHIT][WHIT %in% EEAK], m1bs[WHIT][WHIT %in% EEAK], \n       pch=c(19, 15, 17)[WHIT.csv$X.SUB.FAMILY][WHIT %in% EEAK], \n       col=dogcolors[WHIT.csv$X.SUB.FAMILY][WHIT %in% EEAK], cex=3)\npoints(10^logmass[WHIT][! WHIT %in% EEAK], m1bs[WHIT][! WHIT %in% EEAK], \n       pch=c(0, 0, 2)[WHIT.csv$X.SUB.FAMILY', '################################################################################\n# From Balisi and Van Valkenburgh (2020) Communications Biology\n# CODE FOR FIGURE 3:\n## Species richness and extinction rates for North American fossil canids, \n## both large hypercarnivores and not\n################################################################################\n\n\n## set up large multipanel plot\nlibrary(scales)\nlibrary(viridis)\ntrans = 0.25\n\n## log files and long lists of numbers below generated by PyRate analyses\n# 95% HPDs calculated using code from Biopy (https://www.cs.auckland.ac.nz/~yhel002/biopy/)\n\n\n################################################################################\n# FIGURE 3\n################################################################################\n\npar(mfrow=c(2, 1), mar=c(0, 5, 0, 0)+0.1, oma=c(4.5, 0, 0, 0), cex.lab=2, cex.axis=1.5)\n\n\n################################################################################\n# Figure 3A. Species richness\n################################################################################\n\n# read in not-large hypercarnivore data\ntbl_notLg = read.table(file=""pyrate_mcmc_logs/notLgHyper_lineageThroughTime.txt"", header=T)\ntime_notLg = -tbl_notLg$time\n\n# read in large-hypercarnivore data\ntbl_lgH = read.table(file=""pyrate_mcmc_logs/lgHyper_lineageThroughTime.txt"", header=T)\ntime_lgH = -tbl_lgH$time\n\n# set up plot with notLg dimensions because notLg more expansive\nplot(time_notLg, tbl_notLg$diversity, type=""n"", ylab= ""Number of species"",\n     xlab=""Time (Ma)"", xlim=c(-40, 0), ylim=c(0, 30), xaxt=""n"")\n# credible interval for not-large hypercarnivores:\npolygon(c(time_notLg, rev(time_notLg)), c(tbl_notLg$M_div, rev(tbl_notLg$m_div)),\n        col=alpha(viridis(4)[2], trans), border=NA)\n# credible interval for large hypercarnivores:\npolygon(c(time_lgH, rev(time_lgH)), c(tbl_lgH$M_div, rev(tbl_lgH$m_div)), \n        col=alpha(viridis(4)[4], trans), border=NA)\n# average line for not-large hypercarnivores:\nlines(time_notLg, tbl_notLg$diversity, type=""l"", lwd=3, col=viridis(4)[2])\n# average line for large hypercarnivores:\nlines(time_lgH, tbl_lgH$diversity, type=""l"", lwd=3, col=viridis(4)[4])\n\n\n################################################################################\n# Figure 3B. Extinction rates\n################################################################################\n\n# notLg times--also for extinction rates later\ntime_div_notLg = c(-0.500252842105, -1.50075852632, -2.50126421053, -3.50176989474,\n                   -4.50227557895, -5.50278126316, -6.50328694737, -7.50379263158,\n                   -8.50429831579, -9.504804, -10.5053096842, -11.5058153684,\n                   -12.5063210526, -13.5068267368, -14.5073324211, -15.5078381053,\n                   -16.5083437895, -17.5088494737, -18.5093551579, -19.5098608421,\n                   -20.5103665263, -21.5108722105, -22.5113778947, -23.5118835789,\n                   -24.5123892632, -25.5128949474, -26.5134006316, -27.5139063158,\n                   -28.514412, -29.5149176842, -30.5154233684, -31.5159290526,\n                   -32.5164347368, -33.5169404211, -34.5174461053, -35.5179517895,\n                   -36.5184574737, -37.5189631579)\n# lgHyper times--also for extinction rates later\ntime_div_lgH = c(-0.515298892857, -1.54589667857, -2.57649446429, -3.60709225,\n                 -4.63769003571, -5.66828782143, -6.69888560714, -7.72948339286,\n                 -8.76008117857, -9.79067896429, -10.82127675, -11.8518745357,\n                 -12.8824723214, -13.9130701071, -14.9436678929, -15.9742656786,\n                 -17.0048634643, -18.03546125, -19.0660590357, -20.0966568214,\n                 -21.1272546071, -22.1578523929, -23.1884501786, -24.2190479643,\n                 -25.24964575, -26.2802435357, -27.3108413214, -28.3414391071)\n\n# notLg extinction rates\nrate_ex_notLg = c(0.506162853863, 0.506162330404, 0.504951842325, 0.501630397942,\n                  0.499258733773, 0.493366025957, 0.487897839908, 0.486520596396,\n                  0.485519459278, 0.485164807836, 0.484494442885, 0.483961434901,\n                  0.483770924626, 0.481406885882, 0.479287966122, 0.479005387106,\n                  0.479417727326, 0.482303628405, 0.476923516423, 0.454080089956,\n                  0.142102427113, 0.136923935386, 0.136808966729, 0.137396699778,\n                  0.137918262916, 0.138207547985, 0.138461774707, 0.13865700271,\n                  0.138267350558, 0.135602030681, 0.134668689565, 0.134389687061,\n                  0.134272119483, 0.134259326174, 0.134157206787, 0.134059733611,\n                  0.134023480127, 0.134032416543)\nminHPD_ex_notLg = c(0.321654084389, 0.321654084389, 0.324029877092, 0.320859878429,\n                    0.319905674732, 0.31663531098, 0.330943849733, 0.33457326281,\n                    0.336369713846, 0.337031168723, 0.337678151005, 0.338368674689,\n                    0.336701681783, 0.332713207368, 0.328504581232, 0.327549600965,\n                    0.328101788976, 0.', '################################################################################\n# From Balisi and Van Valkenburgh (2020) Communications Biology\n# CODE FOR FIGURE 4:\n## Origination and extinction rates through time with credible intervals, \n## and histograms of inferred times of rate shifts for origination and extinction \n################################################################################\n\n\n## set up large multiframe plot\nlibrary(scales)\nlibrary(viridis)\ntrans = 0.1\npar(mfrow=c(4, 3), cex.lab=2, cex.axis=1.5, mar=c(1, 5.5, 0, 0)+0.1, oma=c(5.5, 0, 0, 0))\n\n# 95% HPDs calculated using code from Biopy (https://www.cs.auckland.ac.nz/~yhel002/biopy/)\n## long lists of numbers below generated by PyRate analyses\n\n\n################################################################################\n# Row 1, col 1. ALL-CANID DIVERSIFICATION RATES\n################################################################################\n\ntime_all = c(-0.500341421053, -1.50102426316, -2.50170710526, -3.50238994737,\n             -4.50307278947, -5.50375563158, -6.50443847368, -7.50512131579,\n             -8.50580415789, -9.506487, -10.5071698421, -11.5078526842, \n             -12.5085355263, -13.5092183684, -14.5099012105, -15.5105840526, \n             -16.5112668947, -17.5119497368, -18.5126325789, -19.5133154211, \n             -20.5139982632, -21.5146811053, -22.5153639474, -23.5160467895, \n             -24.5167296316, -25.5174124737, -26.5180953158, -27.5187781579, \n             -28.519461, -29.5201438421, -30.5208266842, -31.5215095263, \n             -32.5221923684, -33.5228752105, -34.5235580526, -35.5242408947, \n             -36.5249237368, -37.5256065789)\n\n# ALL-CANID ORIGINATION RATES\nrate_sp_all = c(1.10562092993, 1.10554880289, 1.00229230151, 0.495192787199,\n             0.350849293064, 0.342062622624, 0.341321289116, 0.34119330688,\n             0.341238641648, 0.340893133008, 0.340241472782, 0.340305793998,\n             0.341265389417, 0.341790826886, 0.342115172927, 0.343246255402,\n             0.345651602491, 0.345517307263, 0.345500682019, 0.347286958694,\n             0.1589566511, 0.0953385986251, 0.0941892222473, 0.0946161718242,\n             0.0953585971534, 0.0965978251524, 0.0990556635193, 0.101707131962,\n             0.106597717142, 0.245980865616, 0.463472615719, 0.464375980024,\n             0.464621174327, 0.466661605706, 0.467691714895, 0.466996321984,\n             0.466323583094, 0.46608356375)\nminHPD_sp_all = c(0.321565764194, 0.321588355032, 0.267015444001, 0.220340399073,\n               0.192710928064, 0.210281375576, 0.225848596236, 0.228619337304,\n               0.230151490188, 0.229998867838, 0.230216217254, 0.230163962923,\n               0.233675185926, 0.234380722903, 0.234400726918, 0.235039865591,\n               0.230227580178, 0.229698089875, 0.224209654569, 0.205929217319,\n               0.0307883463442, 0.0101186286286, 0.0014096155567,\n               0.00944419108755, 0.0169789542981, 0.0239606822649,\n               0.0285354314108, 0.0319252755776, 0.0353582758279,\n               0.0449626624783, 0.21060240541, 0.219614839491, 0.221137345238,\n               0.220886336365, 0.221554017778, 0.220690878893, 0.217867327599,\n               0.214002849029)\nmaxHPD_sp_all = c(1.8199229164, 1.81987414178, 1.63904258778, 1.48921244475,\n               1.07083682067, 0.480883483022, 0.462033403794, 0.457864277489,\n               0.456793214257, 0.454779379426, 0.454781575713, 0.454258202199,\n               0.454075847384, 0.453647198818, 0.454716918607, 0.461230592106,\n               0.476331533392, 0.480391202006, 0.484681577281, 0.549325636678,\n               0.420932315892, 0.284716816326, 0.232085835898, 0.235775659708,\n               0.242715939983, 0.257155898814, 0.279354541967, 0.310820985662,\n               0.379831664964, 0.569622617222, 0.765491895716, 0.734183146992,\n               0.73193265924, 0.734136415767, 0.740026276243, 0.744463115855,\n               0.748705567802, 0.75362429945)\n\n# ALL-CANID EXTINCTION RATES\nrate_ex_all = c(0.722002269951, 0.721992813356, 0.682744017708, 0.612058478209,\n                0.588537088942, 0.45133099993, 0.405397331548, 0.40081224012,\n                0.397125992543, 0.396774960844, 0.39604825817, 0.39574037713,\n                0.396270101707, 0.395766814332, 0.395223492263, 0.395872660478,\n                0.397331491451, 0.408561206636, 0.406872164145, 0.394054729005,\n                0.149030160987, 0.135811604247, 0.134847673777, 0.135068743405,\n                0.13528754967, 0.135346844786, 0.135357424579, 0.135558832938,\n                0.135292106223, 0.132595761707, 0.131809850752, 0.131622653624,\n                0.131540044149, 0.13153716737, 0.131453060593, 0.131373741703,\n                0.13134061279, 0.131361568005)\nminHPD_ex_all = c(0.336094210327, 0.336089560492, 0.331445784804, 0.316343515497,\n                  0.313708862868, 0.245370357668, 0.227290223386, 0.232654394064,\n                  0.251453022014, 0.254335268676, 0.255115', '################################################################################\n# From Balisi and Van Valkenburgh (2020) Communications Biology\n# CODE FOR SUPPLEMENTARY FIGURE 1:\n## Extinction rates through time with credible intervals, \n## and histograms of inferred times of rate shifts, \n## for large hypercarnivores and all other canids\n################################################################################\n\n\n## set up large multipanel plot\nlibrary(scales)\nlibrary(viridis)\ntrans = 0.2\npar(mfrow=c(3, 1), cex.lab=2, cex.axis=1.5, mar=c(0, 5, 0, 0)+0.1, oma=c(4.5, 0, 0, 0))\n\n# 95% HPDs calculated using code from Biopy (https://www.cs.auckland.ac.nz/~yhel002/biopy/)\n## long lists of numbers below generated by PyRate analyses\n\n\n################################################################################\n# FIGURE S1\n################################################################################\n\n# notLg times\ntime_div_notLg = c(-0.500252842105, -1.50075852632, -2.50126421053, -3.50176989474,\n                   -4.50227557895, -5.50278126316, -6.50328694737, -7.50379263158,\n                   -8.50429831579, -9.504804, -10.5053096842, -11.5058153684,\n                   -12.5063210526, -13.5068267368, -14.5073324211, -15.5078381053,\n                   -16.5083437895, -17.5088494737, -18.5093551579, -19.5098608421,\n                   -20.5103665263, -21.5108722105, -22.5113778947, -23.5118835789,\n                   -24.5123892632, -25.5128949474, -26.5134006316, -27.5139063158,\n                   -28.514412, -29.5149176842, -30.5154233684, -31.5159290526,\n                   -32.5164347368, -33.5169404211, -34.5174461053, -35.5179517895,\n                   -36.5184574737, -37.5189631579)\n\n# lgHyper times\ntime_div_lgH = c(-0.515298892857, -1.54589667857, -2.57649446429, -3.60709225,\n                 -4.63769003571, -5.66828782143, -6.69888560714, -7.72948339286,\n                 -8.76008117857, -9.79067896429, -10.82127675, -11.8518745357,\n                 -12.8824723214, -13.9130701071, -14.9436678929, -15.9742656786,\n                 -17.0048634643, -18.03546125, -19.0660590357, -20.0966568214,\n                 -21.1272546071, -22.1578523929, -23.1884501786, -24.2190479643,\n                 -25.24964575, -26.2802435357, -27.3108413214, -28.3414391071)\n\n################################################################################\n# Fig S1a. Extinction rates\n################################################################################\n\n# notLg extinction rates\nrate_ex_notLg = c(0.506162853863, 0.506162330404, 0.504951842325, 0.501630397942,\n                  0.499258733773, 0.493366025957, 0.487897839908, 0.486520596396,\n                  0.485519459278, 0.485164807836, 0.484494442885, 0.483961434901,\n                  0.483770924626, 0.481406885882, 0.479287966122, 0.479005387106,\n                  0.479417727326, 0.482303628405, 0.476923516423, 0.454080089956,\n                  0.142102427113, 0.136923935386, 0.136808966729, 0.137396699778,\n                  0.137918262916, 0.138207547985, 0.138461774707, 0.13865700271,\n                  0.138267350558, 0.135602030681, 0.134668689565, 0.134389687061,\n                  0.134272119483, 0.134259326174, 0.134157206787, 0.134059733611,\n                  0.134023480127, 0.134032416543)\nminHPD_ex_notLg = c(0.321654084389, 0.321654084389, 0.324029877092, 0.320859878429,\n                    0.319905674732, 0.31663531098, 0.330943849733, 0.33457326281,\n                    0.336369713846, 0.337031168723, 0.337678151005, 0.338368674689,\n                    0.336701681783, 0.332713207368, 0.328504581232, 0.327549600965,\n                    0.328101788976, 0.321205619495, 0.146442826477, 0.104120378396,\n                    0.0725358772269, 0.0676038679255, 0.0749260354739, 0.0786296732191,\n                    0.0811044066496, 0.0818575650187, 0.0811630527487, 0.0807747106522,\n                    0.080322103039, 0.0687542822945, 0.0564278684939, 0.0509952352228,\n                    0.0477932299587, 0.0471064884405, 0.0415183735081, 0.0382131491382,\n                    0.0363344678488, 0.0310161268574)\nmaxHPD_ex_notLg = c(1.0635228137, 1.0635228137, 1.03970063549, 0.975752083735,\n                    0.928297215724, 0.77445633317, 0.679863453881, 0.666839243216,\n                    0.659778404989, 0.656669727786, 0.653058885123, 0.651197213483,\n                    0.648882804257, 0.64579151426, 0.645758938712, 0.645758938712,\n                    0.648178995467, 0.659380777753, 0.631853356725, 0.606485902393,\n                    0.499292130725, 0.226991996741, 0.2168069619, 0.211380731587,\n                    0.209665698568, 0.209956809488, 0.210393705031, 0.212205754724,\n                    0.212667474231, 0.214552268166, 0.212669150344, 0.21011559537,\n                    0.208576442557, 0.209948492183, 0.20762458169, 0.207262750234,\n                    0.208581545717, 0.207035483075)\n\n# lgHyper extinction rates\nrate_ex_lgH = c(0.585691472435, 0.580123528764, ', '################################################################################\n# From Balisi and Van Valkenburgh (2020) Communications Biology\n# CODE FOR SUPPLEMENTARY FIGURE 2:\n## Graphical representation of Supplementary Table 2: \n## histograms illustrating weak correlations between traits or trait combinations\n## and some diversification rates\n################################################################################\n\n\n## all log files read below are generated by PyRate Covar analyses\n\n\n## set up large multi-frame plot\nlibrary(scales)\npar(mfrow=c(4, 6), mar=c(5, 5, 4, 0)+0.1, cex.lab=2, cex.axis=1.5)\ncutscol = alpha(c(""black"", ""white""), 0.5)[c(1, 2, 1)]\n\n\n# Hesperocyoninae\n\nhesp_covar_mass = read.csv(""Hesperocyoninae_Covar_mass.log"", sep=""\\t"")\n\nhesp_mass_cov_sp = hist(hesp_covar_mass$cov_sp, plot=FALSE)\ncuts = cutscol[cut(hesp_mass_cov_sp$breaks, c(-Inf, -0.6376, 2.1522, Inf))]\nplot(hesp_mass_cov_sp, col=cuts, main="""", xlab="""", xlim=c(-4, 4), \n     border=alpha(""black"", 0.5))\nabline(v=0)\n\nhesp_mass_cov_ex = hist(hesp_covar_mass$cov_ex, plot=FALSE)\ncuts = cutscol[cut(hesp_mass_cov_ex$breaks, c(-Inf, -0.6075, 2.1987, Inf))]\nplot(hesp_mass_cov_ex, col=cuts, main="""", xlab="""", ylab="""",\n     border=alpha(""black"", 0.5), xlim=c(-4, 4))\nabline(v=0)\n\nhesp_covar_m1bs = read.csv(""Hesperocyoninae_Covar_m1BS.log"", sep=""\\t"")\n\nhesp_m1bs_cov_sp = hist(hesp_covar_m1bs$cov_sp, plot=FALSE)\ncuts = cutscol[cut(hesp_m1bs_cov_sp$breaks, c(-Inf, -0.6562, 2.1738, Inf))]\nplot(hesp_m1bs_cov_sp, col=cuts, main="""", xlab="""", ylab="""",\n     border=alpha(""black"", 0.5), xlim=c(-4, 4))\nabline(v=0)\n\nhesp_m1bs_cov_ex = hist(hesp_covar_m1bs$cov_ex, plot=FALSE)\ncuts = cutscol[cut(hesp_m1bs_cov_ex$breaks, c(-Inf, -0.5773, 2.2365, Inf))]\nplot(hesp_m1bs_cov_ex, col=cuts, main="""", xlab="""", ylab="""",\n     border=alpha(""black"", 0.5), xlim=c(-3, 3))\nabline(v=0)\n\nhesp_covar_two = read.csv(""Hesperoyoninae_Covar_twoTrait.log"", sep=""\\t"")\n\nhesp_two_cov_sp = hist(hesp_covar_two$cov_sp, plot=FALSE)\ncuts = cutscol[cut(hesp_two_cov_sp$breaks, c(-Inf, -0.5936, 2.1998, Inf))]\nplot(hesp_two_cov_sp, col=cuts, main="""", xlab="""", ylab="""",\n     border=alpha(""black"", 0.5), xlim=c(-4, 4))\nabline(v=0)\n\nhesp_two_cov_ex = hist(hesp_covar_two$cov_ex, plot=FALSE)\ncuts = cutscol[cut(hesp_two_cov_ex$breaks, c(-Inf, -2.0053, 1.9019, Inf))]\nplot(hesp_two_cov_ex, col=cuts, main="""", xlab="""", ylab="""",\n     border=alpha(""black"", 0.5), xlim=c(-4, 4))\nabline(v=0)\n\n\n# Borophaginae\n\nboro_covar_mass = read.csv(""Borophaginae_Covar_mass.log"", sep=""\\t"")\n\nboro_mass_cov_sp = hist(boro_covar_mass$cov_sp, plot=FALSE)\ncuts = cutscol[cut(boro_mass_cov_sp$breaks, c(-Inf, -0.2423, 0.8444, Inf))]\nplot(boro_mass_cov_sp, col=cuts, main="""", xlab="""",\n     border=alpha(""black"", 0.5), xlim=c(-2, 2))\nabline(v=0)\n\nboro_mass_cov_ex = hist(boro_covar_mass$cov_ex, plot=FALSE)\ncuts = cutscol[cut(boro_mass_cov_ex$breaks, c(-Inf, -0.2392, 0.8526, Inf))]\nplot(boro_mass_cov_ex, col=cuts, main="""", xlab="""", ylab="""",\n     border=alpha(""black"", 0.5), xlim=c(-2, 2))\nabline(v=0)\n\nboro_covar_m1bs = read.csv(""Borophaginae_Covar_m1BS.log"", sep=""\\t"")\n\nboro_m1bs_cov_sp = hist(boro_covar_m1bs$cov_sp, plot=FALSE)\ncuts = cutscol[cut(boro_m1bs_cov_sp$breaks, c(-Inf, -0.2383, 0.8512, Inf))]\nplot(boro_m1bs_cov_sp, col=cuts, main="""", xlab="""", ylab="""",\n     border=alpha(""black"", 0.5), xlim=c(-2, 2))\nabline(v=0)\n\nboro_m1bs_cov_ex = hist(boro_covar_m1bs$cov_ex, plot=FALSE)\ncuts = cutscol[cut(boro_m1bs_cov_ex$breaks, c(-Inf, -0.2481, 0.8407, Inf))]\nplot(boro_m1bs_cov_ex, col=cuts, main="""", xlab="""", ylab="""",\n     border=alpha(""black"", 0.5), xlim=c(-2, 2))\nabline(v=0)\n\nboro_covar_two = read.csv(""Borophaginae_Covar_twoTrait.log"", sep=""\\t"")\n\nboro_two_cov_sp = hist(boro_covar_two$cov_sp, plot=FALSE)\ncuts = cutscol[cut(boro_two_cov_sp$breaks, c(-Inf, -0.1632, 0.9278, Inf))]\nplot(boro_two_cov_sp, col=cuts, main="""", xlab="""", ylab="""",\n     border=alpha(""black"", 0.5), xlim=c(-1.5, 1.5))\nabline(v=0)\n\nboro_two_cov_ex = hist(boro_covar_two$cov_ex, plot=FALSE)\ncuts = cutscol[cut(boro_two_cov_ex$breaks, c(-Inf, -1.9942, 1.9218, Inf))]\nplot(boro_two_cov_ex, col=cuts, main="""", xlab="""", ylab="""",\n     border=alpha(""black"", 0.5), xlim=c(-4, 4))\nabline(v=0)\n\n\n# Caninae\n\ncaninae_covar_mass = read.csv(""Caninae_Covar_mass.log"", sep=""\\t"")\n\ncaninae_mass_cov_sp = hist(caninae_covar_mass$cov_sp, plot=FALSE)\ncuts = cutscol[cut(caninae_mass_cov_sp$breaks, c(-Inf, -0.3396, 1.6877, Inf))]\nplot(caninae_mass_cov_sp, col=cuts, main="""", xlab="""",\n     border=alpha(""black"", 0.5), xlim=c(-3, 3))\nabline(v=0)\n\ncaninae_mass_cov_ex = hist(caninae_covar_mass$cov_ex, plot=FALSE)\ncuts = cutscol[cut(caninae_mass_cov_ex$breaks, c(-Inf, -0.3473, 1.8331, Inf))]\nplot(caninae_mass_cov_ex, col=cuts, main="""", xlab="""", ylab="""",\n     border=alpha(""black"", 0.5), xlim=c(-3, 3))\nabline(v=0)\n\ncaninae_covar_m1bs = read.csv(""Caninae_Covar_m1BS.log"", sep=""\\t"")\n\ncaninae_m1bs_cov_sp = hist(caninae_covar_m1bs$cov_sp, plot=FALSE)\ncuts = cutscol[cut(caninae_m1bs_cov_sp$breaks, c(-Inf, -0.3584, 1', '################################################################################\n# From Balisi and Van Valkenburgh (2020) Communications Biology\n# CODE FOR SUPPLEMENTARY FIGURE 3:\n## Per-subfamily diversification rates of North American fossil canids \n## superimposed on a smoothed temperature curve based on the \n## Zachos et al. (2008) global oxygen isotope record\n################################################################################\n\n\n# initialize\n\n## read in paleoclimate data from Zachos et al. (2008) Nature\ntemp = read.csv(""2008CompilationData.csv"", header=T)\n## CSV converted from http://www.es.ucsc.edu/~jzachos/Data/2008CompilationData.xls\n## linked from Zachos publications on https://websites.pmc.ucsc.edu/~jzachos/Publications.html\n\ntemp = temp[temp$age<=40, ]  # truncate data from 40 million years ago to Recent\noxygen = temp[!is.na(temp$oxygen), c(2, 4)] # extract d18O data\n\n## smooth oxygen data for visualization as a line through time\nsplO = smooth.spline(-oxygen$age, oxygen$oxygen, df=15) \nxO = cbind(-splO$x, splO$y)\ncolnames(xO) = c(""time"", ""oxygen"")\n\n## does this work? test with plot:\nplot(xO, bty=""n"", xlab=""time (Ma)"", ylab=""oxygen isotopes"", col=""gray"",\n     xlim=rev(range(xO[, 1])), ylim=rev(range(xO[, 2])))\n\n## set up large multiframe plot\nlibrary(scales)\nlibrary(viridis)\ntrans = 0.1\npar(mfrow=c(4, 1), mar=c(1, 0, 0, 0)+0.1, oma=c(3, 4, 0, 4), cex=1, \n    cex.lab=2, cex.axis=1.5)\n\n## long lists of numbers below generated by PyRate analyses\n\n\n################################################################################\n# Supplementary Figure 3a. ALL-CANID DIVERSIFICATION RATES\n################################################################################\n\ntime_all = c(-0.500341421053, -1.50102426316, -2.50170710526, -3.50238994737,\n             -4.50307278947, -5.50375563158, -6.50443847368, -7.50512131579,\n             -8.50580415789, -9.506487, -10.5071698421, -11.5078526842, \n             -12.5085355263, -13.5092183684, -14.5099012105, -15.5105840526, \n             -16.5112668947, -17.5119497368, -18.5126325789, -19.5133154211, \n             -20.5139982632, -21.5146811053, -22.5153639474, -23.5160467895, \n             -24.5167296316, -25.5174124737, -26.5180953158, -27.5187781579, \n             -28.519461, -29.5201438421, -30.5208266842, -31.5215095263, \n             -32.5221923684, -33.5228752105, -34.5235580526, -35.5242408947, \n             -36.5249237368, -37.5256065789)\n\n# ALL-CANID ORIGINATION RATES\nrate_sp_all = c(1.10562092993, 1.10554880289, 1.00229230151, 0.495192787199,\n                0.350849293064, 0.342062622624, 0.341321289116, 0.34119330688,\n                0.341238641648, 0.340893133008, 0.340241472782, 0.340305793998,\n                0.341265389417, 0.341790826886, 0.342115172927, 0.343246255402,\n                0.345651602491, 0.345517307263, 0.345500682019, 0.347286958694,\n                0.1589566511, 0.0953385986251, 0.0941892222473, 0.0946161718242,\n                0.0953585971534, 0.0965978251524, 0.0990556635193, 0.101707131962,\n                0.106597717142, 0.245980865616, 0.463472615719, 0.464375980024,\n                0.464621174327, 0.466661605706, 0.467691714895, 0.466996321984,\n                0.466323583094, 0.46608356375)\nminHPD_sp_all = c(0.321565764194, 0.321588355032, 0.267015444001, 0.220340399073,\n                  0.192710928064, 0.210281375576, 0.225848596236, 0.228619337304,\n                  0.230151490188, 0.229998867838, 0.230216217254, 0.230163962923,\n                  0.233675185926, 0.234380722903, 0.234400726918, 0.235039865591,\n                  0.230227580178, 0.229698089875, 0.224209654569, 0.205929217319,\n                  0.0307883463442, 0.0101186286286, 0.0014096155567,\n                  0.00944419108755, 0.0169789542981, 0.0239606822649,\n                  0.0285354314108, 0.0319252755776, 0.0353582758279,\n                  0.0449626624783, 0.21060240541, 0.219614839491, 0.221137345238,\n                  0.220886336365, 0.221554017778, 0.220690878893, 0.217867327599,\n                  0.214002849029)\nmaxHPD_sp_all = c(1.8199229164, 1.81987414178, 1.63904258778, 1.48921244475,\n                  1.07083682067, 0.480883483022, 0.462033403794, 0.457864277489,\n                  0.456793214257, 0.454779379426, 0.454781575713, 0.454258202199,\n                  0.454075847384, 0.453647198818, 0.454716918607, 0.461230592106,\n                  0.476331533392, 0.480391202006, 0.484681577281, 0.549325636678,\n                  0.420932315892, 0.284716816326, 0.232085835898, 0.235775659708,\n                  0.242715939983, 0.257155898814, 0.279354541967, 0.310820985662,\n                  0.379831664964, 0.569622617222, 0.765491895716, 0.734183146992,\n                  0.73193265924, 0.734136415767, 0.740026276243, 0.744463115855,\n                  0.748705567802, 0.75362429945)\n\n# ALL-CANID EXTINCTION RATES\nrate_ex_all = c(0.722002269951, 0.721992813356, 0.682744017708, 0.612058478209,\n                0.588537088942, 0.45133099993,']",4,"Iterative evolution, Large-bodied hypercarnivory, Canids, Species, Clades, Ecological specialization, Costs and benefits, Traits, Individual, Population, Body size, Diet, Meat, Mammals, Macroevolutionary """
Reproductive benefits associated with dispersal in headwater populations of Trinidadian guppies (Poecilia reticulata),"Theory suggests that the evolution of dispersal is balanced by its fitness costs and benefits, yet empirical evidence is sparse due to the difficulties of measuring dispersal and fitness in natural populations. Here, we use spatially-explicit data from a multi-generational capture-mark-recapture study of two populations of Trinidadian guppies (Poecilia reticulata) along with pedigrees to test whether there are fitness benefits correlated with dispersal. Combining these ecological and molecular datasets allows us to directly measure the relationship between movement and reproduction. Individual dispersal was measured as the total distance moved by a fish during its lifetime. We analyzed the effects of dispersal propensity and distance on a variety of reproductive metrics. We found that number of mates and number of offspring produced were positively correlated to dispersal, especially for males. Our results also reveal individual and environmental variation in dispersal, with sex, size, season, and stream acting as determining factors.","['#### Set working directory ####\n\nsetwd(""/Users/eu/Michigan State University/Fitzpatrick, Sarah - Isabela/guppy_movement_ms/data and models"")\n\n#### Loading packages ####\n\nlibrary(pscl)\nlibrary(ggplot2)\nlibrary(MASS)\nlibrary(boot)\nlibrary(MuMIn)\nlibrary(sjPlot)\nlibrary(lme4)\nlibrary(texreg)\nlibrary(glmmTMB)\nlibrary(bbmle)\nlibrary(piecewiseSEM)\nlibrary(dplyr)\nlibrary(modelsummary)\nlibrary(DHARMa)\nlibrary(gridExtra)\nlibrary(cowplot)\n\n##### 1. Load data #####\n\n# main movement and fitness data\nd = read.csv(""taylor_caigual_fitness.csv"", sep="";"")\n\n# seasonality data\nc.disp.less <- read.csv(""c.disp.subsetted.csv"", sep="";"")\nt.disp <- read.csv(""taylor.dispersed.updown.csv"", sep="";"")\nall.season <- read.csv(""both_streams_seasonal_updown.csv"", sep="";"")\nall.d <- subset(all.season, Season==""D"")\nall.w <- subset(all.season, Season==""W"")\n\n##### 2. Wrangle data #####\n\n# subset capture occasions before onset of gene flow\nd = subset(d, d$Capt.event>5 & d$Capt.event<17)\n\n##### 3. Centering and transforming #####\n\nhist(d$total_dist) # poisson\nd$total_dist_l = log(d$total_dist)\nhist(d$total_dist_l) # normal\n\nhist(d$LRS) # poisson\nd$LRS_l = log(d$LRS)\nhist(d$LRS_l) # left skewed\n\n# adding dispersal status: dispersers = TRUE \nd$disp.status = as.factor(d$total_dist>=10)\nplot(d$disp.status)\nprop.disp.status = length(d$disp.status[d$disp.status == TRUE])/length(d$disp.status)\nprop.non.disp.status = 1 - prop.disp.status\nboxplot(data=d, total_dist ~ disp.status)\n\n# make Sex = 1 or 2 (1 for F, 2 for M)\nd$Sex <- sapply(d$Sex.x, is.logical)\nd$Sex <- sapply(d$Sex.x, as.numeric)\nhead(d)\nplot(Sex ~ Sex.x, data=d) \n\nd$total_dist_10 <- d$total_dist\nd$total_dist_10[d$total_dist_10 < 10] <- NA\n\n# make it so that longevity starts from 0, if needed \n\nhist(d$Longevity, xlim=c(0,20))\nd$Longevity_adjusted <- d$Longevity - 2\nhist(d$Longevity_adjusted, xlim=c(0,20))\n\n# creating subsets of data for each sex for each stream\nd.tay = subset(d, Stream==""Taylor"")\nd.cai = subset(d, Stream==""Caigual"")\n\nd.m = subset(d, Sex.x==""M"")\nd.f = subset(d, Sex.x==""F"")\n\nd.tay.f = subset(d.tay, Sex.x==""F"")\nd.tay.m = subset(d.tay, Sex.x==""M"")\n\nd.cai.f = subset(d.cai, Sex.x==""F"")\nd.cai.m = subset(d.cai, Sex.x==""M"")\n\n# dispersers-only subsets\nd.disp = subset(d, total_dist>=10)\n\nd.disp.f = subset(d.disp, Sex.x==""F"")\nd.disp.m = subset(d.disp, Sex.x==""M"")\n\nd.non.disp = subset(d, total_dist<10)\n\nd.non.disp.f = subset(d.non.disp, Sex.x==""F"")\nd.non.disp.m = subset(d.non.disp, Sex.x==""M"")\n\nd.tay.dist <- subset(d.tay, d.tay$total_dist >= 10)\nd.cai.dist <- subset(d.cai, d.cai$total_dist >= 10)\nd.f.dist <- subset(d.f, d.f$total_dist >= 10)\nd.m.dist <- subset(d.m, d.m$total_dist >= 10)\nd.tay.rep <- subset(d.tay, d.tay$LRS >= 1)\nd.cai.rep <- subset(d.cai, d.cai$LRS >= 1)\n\n#### 4. Analyses: Total distance and status ####\n\n# status models\nhstatus.all <- glm(disp.status ~ Sex.x + Stream*hindex + Longevity, data=d, family=""binomial"")\nhstatus.m <- glm(disp.status ~Stream*hindex + Longevity + Male_SL, data=d.m, family=""binomial"")\nstatus.all <- glm(disp.status ~ Sex.x + Stream + Longevity, data=d, family=""binomial"")\nstatus.m <- glm(disp.status ~ Stream + Longevity + Male_SL, data=d.m, family=""binomial"")\n\n# distance models\nglmmTMB.dist.all <- glmmTMB(total_dist ~ Sex.x + Stream*hindex + log(Longevity),\n                            ziformula = ~ Sex.x + Stream*hindex + log(Longevity),\n                            family=nbinom2,data=d)\nglmmTMB.dist.m <- glmmTMB(total_dist ~ Male_SL + Stream*hindex + log(Longevity), \n                              ziformula = ~ Male_SL + Stream*hindex + log(Longevity),\n                              family=nbinom2,data=d.m)\nhzi.dist.all <- zeroinfl(total_dist ~ Sex.x + Stream*hindex + Longevity, data=d, dist=""negbin"")\nhzi.dist.m <- zeroinfl(total_dist ~ Male_SL + Stream*hindex + Longevity, data=d.m, dist=""negbin"")\nzi.dist.all <- zeroinfl(total_dist ~ Sex.x + Stream + Longevity, data=d, dist=""negbin"")\nzi.dist.m <- zeroinfl(total_dist ~ Male_SL + Stream + Longevity, data=d.m, dist=""negbin"")\n\n# model selection and testing for model adequacy\n\nAICctab(status.all,hstatus.all) \nAICctab(zi.dist.all,hzi.dist.all,glmmTMB.dist.all) \nAICctab(zi.dist.m,hzi.dist.m,glmmTMB.dist.m) \n\ntestDispersion(hstatus.all)\nhstatus.all.sim <- simulateResiduals(fittedModel = hstatus.all, plot = F)\nplot(hstatus.all.sim)\n\ntestDispersion(hstatus.m)\nhstatus.m.sim <- simulateResiduals(fittedModel = hstatus.m, plot = F)\nplot(hstatus.m.sim) \n\ntestDispersion(glmmTMB.dist.all)\nglmmTMB.dist.all.sim <- simulateResiduals(fittedModel = glmmTMB.dist.all, plot = F)\nplot(glmmTMB.dist.all.sim) \n\ntestDispersion(glmmTMB.dist.m)\nglmmTMB.dist.males.sim <- simulateResiduals(fittedModel = glmmTMB.dist.m, plot = F)\nplot(glmmTMB.dist.males.sim) \n\n#### 5. Analyses: Lifetime reproductive success (LRS) ####\n\n# male status models\nglmmTMB.hZINB.status.m <- glmmTMB(LRS ~ disp.status + Stream*hindex + Longevity + Male_SL, \n                                  ziformula = ~ disp.status + Stream*hindex + Longevity + Male_SL,\n           ']",4,"reproductive benefits, dispersal, headwater populations, Trinidadian guppies, Poecilia reticulata, evolution, fitness costs, empirical evidence, spatially-explicit data, capture-mark-recapture study, pedigrees, fitness benefits,"
"Replication Data for: ""Affiliation Information in DataCite Dataset Metadata: a Flemish Case Study""","The files in the data package allow to reproduce the findings presented in the publication ""Affiliation Information in DataCite Dataset Metadata: a Flemish Case Study"", Data Science Journal (2021).","['\r\n### Computational script-------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n# This script aims to harvest dataset metadata (... - 2020) related to one or more of the Flemish universities (Katholieke Universiteit Leuven, Universiteit Gent, Universiteit Hasselt, Universiteit Antwerpen), according to predetermined queries. The harvest took place the 26th of December 2020. The output of this harvesting process can be found in the file ""Data_DataCite_harvest_Flemish_universities"", available in txt- and csv-formats. \r\n\r\n\r\n## Load libraries--------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n.libPaths(""C:/R/library"") # Modify the path to where the R libraries are available. If the packages are not yet installed, please install them first.\r\n\r\nlibrary(""rdatacite"")\r\nlibrary(""dplyr"")\r\nlibrary(""stringr"")\r\nlibrary(""reshape"")\r\n\r\n\r\n## DataCite - Flemish universities---------------------------------------------------------------------------------------------------------------------------\r\n\r\n\r\n# Request data from DataCite API\r\nqueries <- c(""Vrije AND Universiteit AND Brussel"", ""https://ror.org/006e5kg04"", ""Universiteit AND Gent"", ""UGent"", ""https://ror.org/00cv9y106"", ""KU AND Leuven"", ""Katholieke AND Universiteit AND Leuven"", ""KULeuven"", ""Catholic AND University AND Leuven"", ""University AND Leuven"", ""https://ror.org/05f950310"", ""Universiteit AND Antwerpen"", ""UAntwerpen"", ""UAntwerp"", ""University AND Antwerp"", ""https://ror.org/008x57b05"", ""Universiteit AND Hasselt"", ""UHasselt"", ""https://ror.org/04nbhqj75"") # list of search items: different entities referring to the five Flemish universities\r\n\r\nresults_list <- list()\r\nfor (i in 1:length(queries)) { # loop over the search items, to obtain the data\r\n  \r\n  response<-dc_dois(query = queries[i], resource_type_id = ""dataset"", limit = 500)\r\n  data<-response[[""data""]]\r\n\r\n  results_list[[i]]<-data\r\n}\r\n\r\nresponse_Ghent_University<-dc_dois(query = ""Ghent AND University AND NOT (Global AND Biodiversity AND Information AND Facility)"", resource_type_id = ""dataset"", limit = 500, random = TRUE) # separate search to exclude hits from the Global Biodiversity Information Facility \r\nresults_list[[(length(results_list)+1)]]<-response_Ghent_University[[""data""]]\r\n\r\nresponse_Hasselt_University<-dc_dois(query = ""University AND Hasselt AND NOT (Global AND Biodiversity AND Information AND Facility)"", resource_type_id = ""dataset"", limit = 500, random = TRUE) # separate search to exclude hits from the Global Biodiversity Information Facility\r\nresults_list[[(length(results_list)+1)]]<-response_Hasselt_University[[""data""]]\r\n\r\nqueries_2 <- c(""Vrije AND Universiteit AND Brussel"", ""https://ror.org/006e5kg04"", ""Universiteit AND Gent"", ""UGent"", ""https://ror.org/00cv9y106"", ""KU AND Leuven"", ""Katholieke AND Universiteit AND Leuven"", ""KULeuven"", ""Catholic AND University AND Leuven"", ""University AND Leuven"", ""https://ror.org/05f950310"", ""Universiteit AND Antwerpen"", ""UAntwerpen"", ""UAntwerp"", ""University AND Antwerp"", ""https://ror.org/008x57b05"", ""Universiteit AND Hasselt"", ""UHasselt"", ""https://ror.org/04nbhqj75"", ""Ghent AND University AND NOT (Global AND Biodiversity AND Information AND Facility)"", ""University AND Hasselt AND NOT (Global AND Biodiversity AND Information AND Facility)"") # overview of all the queries\r\n\r\n                                                        \r\n# Transform selection of requested data into dataframe\r\nfinal_results_list<-list()\r\n\r\nfor (i in 1:length(results_list)) {\r\n\r\n  if (length(results_list[[i]]) != 0) {\r\n  \r\n  response_data <- results_list[[i]]\r\n  \r\n  response_data_URL<-select(response_data$attributes, url) # url\r\n  response_data_DOI<-select(response_data$attributes, doi) # doi\r\n  response_query<-rep(queries_2[i], length(response_data_DOI[,1])) # query\r\n  \r\n  response_descriptions<-select(response_data$attributes, descriptions)$descriptions # descriptions\r\n  response_descriptions_2<-unlist(lapply(response_descriptions, function(x) ifelse(length(x) == 0, ""no_description"", x$description)))\r\n  \r\n  response_data_publisher<-select(response_data$attributes, publisher) # publisher\r\n  response_data_client<-select(response_data$relationships, client)$client$data$id # client id\r\n  response_data_publicationYear<-select(response_data$attributes, publicationYear) # publicationYear\r\n  \r\n  response_data_relatedIdentifiers<-select(response_data$attributes, relatedIdentifiers) # identifiers of related research outputs \r\n  response_data_relatedIdentifiers_2<-unlist(lapply(response_data_relatedIdentifiers[[1]], function(x) ifelse(length(x) != 0, paste(x, collapse="" ; ""), ""no_identifier"")))\r\n  \r\n  response_data_creators<-select(response_data$attributes, creators) # data creators\r\n  response_data_creators_names<-unlist(lapply(response_data_creators[[1]], function(x) paste(x$name, collapse="" / ""))) # name data creators\r\n  \r\n  resp', '\r\n\r\n### Computational script-------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n# This script aims to harvest links to related outputs from the Scholexplorer API, on the basis of the dataset DOIs available in the file ""Data_DataCite_harvest_Flemish_universities.txt"". The harvest took place the 26th of December 2020. The output of this harvesting process can be found in the file ""Data_Scholexplorer_harvest_Flemish_universities"", available in txt- and csv-formats. \r\n\r\n\r\n## Load libraries--------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n.libPaths(""C:/R/library"") # Modify the path to where the R libraries are available. If the packages are not yet installed, please install them first.\r\n\r\nlibrary(httr)\r\nlibrary(stringr)\r\nlibrary(jsonlite)\r\nlibrary(dplyr)\r\n\r\n\r\n## Load data------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\ndata<-read.table(file=file.choose(), header=TRUE, sep=""\\t"", comment.char="""", fill=TRUE, na.strings=""character"", quote="""", row.names=NULL, stringsAsFactors = FALSE, strip.white = TRUE, encoding = ""utf-8"", blank.lines.skip = TRUE) # input the file ""Data_DataCite_harvest_Flemish_universities.txt""\r\n\r\ndata_2<-filter(data[,1:15], Inclusion == ""yes"" & Versioning_manual == ""primary"")\r\ndata<-data_2\r\n\r\nDOI_data<-as.data.frame(data[,3])\r\n\r\n\r\n## Request data from Scholexplorer API-----------------------------------------------------------------------------------------------------------------------\r\n\r\nscholix_api <- function(query) {\r\n  query_2<-str_replace_all(query, ""/"", ""%2F"")\r\n  url <- paste(""http://api.scholexplorer.openaire.eu/v2/Links?sourcePid="", query_2, sep = """")\r\n  GET(url)\r\n  }\r\n\r\nscholix_resp<-list()\r\n\r\nfor (i in 1:length(DOI_data[,1])) {\r\n  \r\n  temporary<-DOI_data[i,]\r\n  scholix_resp[[i]]<- scholix_api(temporary)\r\n  \r\n}\r\n\r\nscholix_parsed<-list()\r\n\r\nfor (i in 1:length(scholix_resp)) {\r\n  \r\n  if (scholix_resp[[i]]$status_code == 200) {\r\n  \r\n  temporary_2 <- scholix_resp[[i]]\r\n  scholix_parsed[[i]] <- content(temporary_2, ""parsed"")\r\n    \r\n  } else {\r\n    \r\n    scholix_parsed <- ""Wrong status code""\r\n    }}\r\n\r\nscholix_results<-list()\r\n\r\nfor (i in 1:length(scholix_parsed)) {\r\n\r\n  if (length(scholix_parsed[[i]]$result) == 0) {\r\n    \r\n    temporary_3 <- scholix_parsed[[i]]\r\n    scholix_results[[i]] <- ""No scholix links""\r\n    \r\n  } else {\r\n    \r\n    scholix_results[[i]]<-list()\r\n    number_of_results <- length(scholix_parsed[[i]]$result)\r\n    \r\n    for (j in 1:(length(number_of_results))) {\r\n    \r\n      DOI_of_source_dataset<-DOI_data[i,1]\r\n      \r\n      RelationshipType<-scholix_parsed[[i]]$result[[j]]$RelationshipType$Name\r\n      \r\n      RelationshipSubtype<-scholix_parsed[[i]]$result[[j]]$RelationshipType$SubType\r\n      \r\n      Type_of_research_output<-scholix_parsed[[i]]$result[[j]]$target$Type\r\n      \r\n      Persistent_identifier<-scholix_parsed[[i]]$result[[j]]$target$Identifier[[1]]$IDURL\r\n      \r\n      Results_total<-data.frame(DOI_of_source_dataset, RelationshipType, RelationshipSubtype, Type_of_research_output, Persistent_identifier)\r\n    \r\n      scholix_results[[i]][[j]]<-Results_total\r\n      \r\n    }\r\n    \r\n    next\r\n    \r\n    }}\r\n  \r\n\r\n\r\nscholix_number_links<-vector()\r\n\r\nfor (i in 1:length(scholix_results)) {\r\n  \r\n  scholix_number_links<-append(scholix_number_links, ifelse(scholix_results[[i]] == ""No scholix links"", 0, length(scholix_results[[i]])))\r\n}\r\n\r\nscholix_number_links[scholix_number_links > 1]\r\n\r\nscholix_link_to<-vector()\r\n\r\nfor (i in 1:length(scholix_results)) {\r\n  \r\n  scholix_link_to<-append(scholix_link_to, ifelse(scholix_results[[i]] == ""No scholix links"", ""No_scholix_links"", as.character(scholix_results[[i]][[1]]$Type_of_research_output)))\r\n  }\r\n\r\n\r\nscholix_DOI<-vector()\r\nfor (i in 1:length(scholix_results)) {\r\n  \r\n  scholix_DOI<-append(scholix_DOI, ifelse(scholix_results[[i]] == ""No scholix links"", ""No_DOI"", as.character(scholix_results[[i]][[1]]$Persistent_identifier)))\r\n  \r\n}\r\n\r\n\r\nscholix_table_end<-data.frame(scholix_link_to, scholix_DOI)\r\n\r\nwrite.table(scholix_table_end, file=""Scholix_info.txt"", sep = \'\\t\', row.names = FALSE, col.names = TRUE, quote=FALSE, eol=""\\n"") # create output file\r\n\r\n##-----------------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nrm(list=ls())\r\n\r\n\r\n\r\n\r\n\r\n', '\r\n### Computational script-------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n# This script aims to analyze the data in the file ""Data_DataCite_harvest_Flemish_universities.txt"". \r\n\r\n\r\n## Load libraries--------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n.libPaths(""C:/R/library"") # Modify the path to where the R libraries are available. If the packages are not yet installed, please install them first.\r\n\r\nlibrary(""ggplot2"")\r\nlibrary(""scales"")\r\nlibrary(""dplyr"")\r\n\r\n\r\n## Load data------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\ndata<-read.table(file=file.choose(), header=TRUE, sep=""\\t"", comment.char="""", fill=TRUE, na.strings=""character"", quote="""", row.names=NULL, stringsAsFactors = FALSE, strip.white = TRUE, encoding = ""utf-8"", blank.lines.skip = TRUE) # cf. the file ""Data_DataCite_harvest_Flemish_universities.txt""\r\n\r\ndata_2<-filter(data[,1:15], Inclusion == ""yes"" & Versioning_manual == ""primary"")\r\ndata<-data_2\r\n\r\n\r\n## Analysis--------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n\r\n# Univariate testing\r\n\r\n(Distribution_query<-as.data.frame(table(data$Query))[order(as.data.frame(table(data$Query))[,2], decreasing = TRUE),])\r\n(Distribution_publisher<-as.data.frame(table(data$Publisher))[order(as.data.frame(table(data$Publisher))[,2], decreasing = TRUE),])\r\n(Distribution_publicationyear<-as.data.frame(table(data$PublicationYear))[order(as.data.frame(table(data$PublicationYear))[,2], decreasing = TRUE),])\r\n(Distribution_Affiliation_field<-as.data.frame(table(data$Affiliation_information_fields))[order(as.data.frame(table(data$Affiliation_information_fields))[,2], decreasing = TRUE),])\r\n\r\ndata_2<-data # collapse certain values into broader categories\r\ndata_2$Identifiers_related_outputs[data_2$Identifiers_related_outputs!=""no_identifier""]<-""identifier""\r\ndata_2$Identifiers_data_creators[data_2$Identifiers_data_creators!=""no_identifier""]<-""identifier""\r\ndata_2$Affiliations_data_creators[data_2$Affiliations_data_creators!=""no_affiliation""]<-""affiliation""\r\n\r\n(Distribution_related_outputs<-as.data.frame(table(data_2$Identifiers_related_outputs))[order(as.data.frame(table(data_2$Identifiers_related_outputs))[,2], decreasing = TRUE),])\r\n(Distribution_identifiers_creators<-as.data.frame(table(data_2$Identifiers_data_creators))[order(as.data.frame(table(data_2$Identifiers_data_creators))[,2], decreasing = TRUE),])\r\n(Distribution_affiliations_creators<-as.data.frame(table(data_2$Affiliations_data_creators))[order(as.data.frame(table(data_2$Affiliations_data_creators))[,2], decreasing = TRUE),])\r\n\r\n\r\n# Multivariate testing\r\n\r\ntable(data_2$Identifiers_data_creators, data_2$PublicationYear)\r\ntable(data_2$Affiliation_information_fields, data_2$PublicationYear)\r\npublisher_orcid<-as.data.frame(table(data_2$Publisher, data_2$Identifiers_data_creators))\r\n\r\n\r\n## Overview of plots----------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n\r\n# Plot 1\r\nNumber_of_DOIs_per_year_and_affiliationfield<-aggregate(DOI ~ PublicationYear + Affiliation_information_fields, data_2, function(x) length(unique(x))) \r\n\r\nNumber_of_DOIs_per_year_and_affiliationfield$PublicationYear<-as.numeric(Number_of_DOIs_per_year_and_affiliationfield$PublicationYear)\r\nNumber_of_DOIs_per_year_and_affiliationfield$DOI<-as.numeric(Number_of_DOIs_per_year_and_affiliationfield$DOI)\r\nNumber_of_DOIs_per_year_and_affiliationfield$Affiliation_information_fields<-as.factor(Number_of_DOIs_per_year_and_affiliationfield$Affiliation_information_fields)\r\n\r\ndata_points<-as.data.frame(table(Number_of_DOIs_per_year_and_affiliationfield$Affiliation_information_fields))\r\naffiliation_info_major<-filter(data_points, Freq > 2)\r\n\r\nNumber_of_DOIs_per_year_and_affiliationfield<-filter(Number_of_DOIs_per_year_and_affiliationfield, Affiliation_information_fields %in% affiliation_info_major[,1])\r\n\r\nplot_1<-ggplot(Number_of_DOIs_per_year_and_affiliationfield, aes(x=PublicationYear, y= DOI, color = Affiliation_information_fields)) + theme_minimal() + geom_jitter(size=3, width = 0.1) + geom_line(size = 1, alpha = 0.7) + scale_x_continuous(breaks = scales::pretty_breaks(n = 10), limits=c(2005.75,2020.25)) + scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) + theme(axis.text.x=element_text(angle=60, hjust=1)) + xlab(""publication year of the dataset"") + ylab(""number of DOIs"") + labs(color = ""Affiliation info"") + scale_color_manual(values=c(""#e41a1c"", ""#377eb8"", ""#4daf4a"", ""#984ea3"", ""#ff7f00"", ""#A65628"", ""#999999"")) # zoom in!\r\n\r\njpeg(file = ""Number_of_DOIs_per_year_and_affiliationfield.jpeg"", width = 748, height = 634, quality = 100, res = 100)\r\nplot_1', '\r\n### Computational script-------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n# This script aims to harvest dataset DOIs from DataCite in a random fashion. Results are restricted to repositories currently used by researchers affiliated to Flemish universities. For each repository, a sample of 1000 metadata records is collected (if possible), limited to the publication years 2019 and 2020. Next, the raw data are trimmed down to samples of 450 records per repository (when possible). \r\n\r\n\r\n## Load libraries--------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n.libPaths(""C:/R/library"") # Modify the path to where the R libraries are available. If the packages are not yet installed, please install them first.\r\n\r\nlibrary(""rdatacite"")\r\nlibrary(""dplyr"")\r\nlibrary(""tidyr"")\r\nlibrary(""stringr"")\r\nlibrary(""reshape"")\r\n\r\n\r\n## DataCite - repositories used by researchers affiliated to Flemish universities----------------------------------------------------------------------------\r\n\r\ndata<-read.table(file=file.choose(), header=TRUE, sep=""\\t"", comment.char="""", fill=TRUE, na.strings=""character"", quote="""", row.names=NULL, stringsAsFactors = FALSE, strip.white = TRUE, encoding = ""utf-8"", blank.lines.skip = TRUE) # cf. the file ""Data_DataCite_harvest_Flemish_universities.txt""\r\n\r\ndata_2<-filter(data[,1:15], Inclusion == ""yes"" & Versioning_manual == ""primary"")\r\ndata<-data_2\r\n\r\nclient_id_repo<-unique(data$Client_ID) # repositories used by Flemish researchers\r\nresults_repo_list <- list()\r\n\r\nfor (j in 1:length(client_id_repo)) {\r\n\r\nresponse_repo<-dc_dois(query = ""publicationYear:2019 OR publicationYear:2020"", client_id = client_id_repo[j], resource_type_id = ""dataset"", sample_size = ""1000"", random = TRUE, limit = 1000) # sample of 1000 hits, for publication years 2019 and 2020\r\ndata_repo<-response_repo[[""data""]]\r\n\r\nresults_repo_list[[j]]<-data_repo\r\n}\r\n\r\n\r\n# Transform selection of requested data into dataframe       \r\nfinal_results_repo_list<-list()\r\n\r\nfor (i in 1:length(results_repo_list)) {\r\n  \r\n  if (length(results_repo_list[[i]]) != 0) {\r\n    \r\n    response_data_repo <- results_repo_list[[i]]\r\n    \r\n    response_data_repo_DOI<-select(response_data_repo$attributes, doi) #doi\r\n    \r\n    response_data_repo_publisher<-select(response_data_repo$attributes, publisher) #publisher\r\n    response_data_repo_client<-select(response_data_repo$relationships, client)$client$data$id # client id\r\n    response_data_repo_publicationYear<-select(response_data_repo$attributes, publicationYear) #publicationYear\r\n    \r\n    response_data_repo_relatedIdentifiers<-select(response_data_repo$attributes, relatedIdentifiers) #identifiers of related research outputs \r\n    response_data_repo_relatedIdentifiers_2<-unlist(lapply(response_data_repo_relatedIdentifiers[[1]], function(x) ifelse(length(x) != 0, paste(x, collapse="" ; ""), ""no_identifier"")))\r\n    \r\n    response_data_repo_creators<-select(response_data_repo$attributes, creators) #data creators\r\n    response_data_repo_creators_names<-unlist(lapply(response_data_repo_creators[[1]], function(x) paste(x$name, collapse="" / ""))) # name data creators\r\n    \r\n    response_data_repo_creators_nameIdentifiers<-lapply(response_data_repo_creators[[1]], function(x) ifelse(length(x$nameIdentifiers) != 0, paste(unlist(x), collapse="" ; ""), ""no_identifier""))# identifiers of data creators\r\n    response_data_repo_creators_nameIdentifiers_2<-unlist(lapply(response_data_repo_creators_nameIdentifiers, function(x) ifelse(x == ""list()"" | x == ""no_identifier"", ""no_identifier"", paste(unlist(str_extract_all(unlist(x[[1]]), ""https://orcid.org/..................."")), collapse="" ; ""))))\r\n    response_data_repo_creators_nameIdentifiers_3<-ifelse(response_data_repo_creators_nameIdentifiers_2 == """", ""no_identifier"", response_data_repo_creators_nameIdentifiers_2)\r\n    \r\n    response_data_repo_creators_affiliation<-unlist(lapply(response_data_repo_creators[[1]], function(x) paste(x$affiliation, collapse="" / ""))) # affiliation of data creators\r\n    response_data_repo_creators_affiliation_2<-unlist(lapply(response_data_repo_creators_affiliation, function(x) ifelse(grepl(""list()"", x, fixed = FALSE), ""no_affiliation"", x)))\r\n    \r\n    response_results_repo<-data.frame(response_data_repo_DOI, response_data_repo_publisher, response_data_repo_client, response_data_repo_publicationYear, response_data_repo_relatedIdentifiers_2, response_data_repo_creators_names, response_data_repo_creators_nameIdentifiers_3, response_data_repo_creators_affiliation_2) # resulting data frame with all information\r\n    \r\n    final_results_repo_list[[i]]<-response_results_repo\r\n  }}\r\n\r\n\r\nheaders_for_columns<-c(""DOI"", ""Publisher"", ""Client_ID"", ""PublicationYear"", ""Identifiers_related_outputs"", ""Names_data_creators"", ""Identifiers_data_creators"", ""Affiliations_data_creators"")\r\n\r\ntotal_results_repo_2<-lapply(', '\r\n### Computational script-------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n# This script aims to analyze the data in the file ""Data_DataCite_processed_random.txt"".\r\n\r\n\r\n## Load libraries--------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n.libPaths(""C:/R/library"") # Modify the path to where the R libraries are available. If the packages are not yet installed, please install them first.\r\n\r\nlibrary(\'ggplot2\')\r\nlibrary(\'vcd\')\r\nlibrary(\'tidyr\')\r\nlibrary(\'gmodels\')\r\nlibrary(\'pheatmap\')\r\nlibrary(\'cluster\')\r\nlibrary(\'fpc\')\r\n\r\n\r\n## Load data------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\ndata_repo_final<-read.table(file=file.choose(), header=TRUE, sep=""\\t"", comment.char="""", fill=TRUE, na.strings=""character"", quote="""", row.names=NULL, stringsAsFactors = FALSE, strip.white = TRUE, encoding = ""utf-8"", blank.lines.skip = TRUE) # cf. the file ""Data_DataCite_processed_random.txt""\r\n\r\n\r\n## Bivariate analysis----------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nTABLE_1<-table(data_repo_final$Client_ID, data_repo_final$Identifiers_data_creators, dnn = c(""Client_ID"", ""Identifiers_data_creators""))\r\nTABLE_2<-table(data_repo_final$Client_ID, data_repo_final$Affiliations_data_creators, dnn = c(""Client_ID"", ""Affiliations_data_creators""))\r\nTABLE_3<-table(data_repo_final$Client_ID, data_repo_final$Identifiers_related_outputs, dnn = c(""Client_ID"", ""Identifiers_related_outputs""))\r\n\r\nmosaicplot(TABLE_1, shade = TRUE) # mosaic plot\r\nassocstats(TABLE_1)\r\nCrossTable(TABLE_1, chisq = TRUE, resid = TRUE, sresid = TRUE, digits = 2, format=""SPSS"")\r\n\r\nmosaicplot(TABLE_2, shade = TRUE) # mosaic plot\r\nassocstats(TABLE_2)\r\nCrossTable(TABLE_2, chisq = TRUE, resid = TRUE, sresid = TRUE, digits = 2, format=""SPSS"")\r\n\r\nmosaicplot(TABLE_3, shade = TRUE, main = """") # mosaic plot\r\nassocstats(TABLE_3)\r\nCrossTable(TABLE_3, chisq = TRUE, resid = TRUE, sresid = TRUE, digits = 2, format=""SPSS"")\r\n\r\njpeg(file = ""mosaic_plot.jpeg"", width = 748, height = 634, quality = 100, res = 100)\r\nmosaicplot(TABLE_3, shade = TRUE, main = """", las = 2, xlab = ""archiving organization"", ylab = ""identifiers related outputs"") # mosaic plot\r\ndev.off() \r\n\r\n\r\n## Detailed crosstabulations---------------------------------------------------------------------------------------------------------------------------------\r\n\r\nMyTab2 <- xtabs(~ Identifiers_related_outputs + Client_ID + Affiliations_data_creators, data= data_repo_final)\r\nMyTab3 <- xtabs(~ Identifiers_data_creators + Client_ID + Affiliations_data_creators, data = data_repo_final)\r\nMyTab4 <- xtabs(~ Identifiers_data_creators + Client_ID + Affiliations_data_creators + Identifiers_related_outputs, data= data_repo_final)\r\n\r\n\r\n## Detailed cotab plots--------------------------------------------------------------------------------------------------------------------------------------\r\n\r\ncotabplot( ~ Identifiers_data_creators * Client_ID | Affiliations_data_creators, data = data_repo_final, split_vertical = FALSE, gp = shading_hcl, labeling_args = list(rot_labels = c(top = 80), offset_varnames = c(top = 3), offset_labels = c(top = 0.5)))\r\n\r\ncotabplot( ~ Identifiers_related_outputs * Client_ID | Affiliations_data_creators, data = data_repo_final, split_vertical = FALSE, gp = shading_hcl, labeling_args = list(rot_labels = c(top = 80), offset_varnames = c(top = 3), offset_labels = c(top = 0.5)))\r\n\r\n\r\n## Heatmap---------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\ncount_frame<-as.data.frame(MyTab4)\r\nunite_DF <- count_frame %>% unite(New, Identifiers_data_creators, Affiliations_data_creators, Identifiers_related_outputs, sep = ""__"")\r\nwide_DF <- unite_DF %>% spread(New, Freq)\r\nwide_DF_2 <- data.frame(wide_DF[,-1], row.names=wide_DF[,1])\r\nwide_DF_2 <- wide_DF_2 / 450 # sample size\r\nwide_DF_3 <- wide_DF_2\r\ncolnames(wide_DF_3)<-c(""yes_yes_no"", ""yes_yes_yes"", ""yes_no_no"", ""yes_no_yes"", ""no_yes_no"", ""no_yes_yes"", ""no_no_no"", ""no_no_yes"")\r\n\r\nheatmap_plot<-pheatmap(wide_DF_3, clustering_distance_rows = ""euclidean"", clustering_distance_cols = ""canberra"", clustering_method = ""ward.D2"", cutree_rows = 3, show_rownames = T, show_colnames = T, main = """", display_numbers = TRUE, angle_col = 45)\r\n\r\njpeg(file = ""heatmap.jpeg"", width = 748, height = 634, quality = 100, res = 100)\r\nheatmap_plot\r\ndev.off() \r\n\r\nwide_DF_3.dist<-dist(wide_DF_3, method = ""euclidean"")\r\nwide_DF_3.hc <- hclust(wide_DF_3.dist, method = ""ward.D2"")\r\nplot(wide_DF_3.hc, hang = -1)\r\nrect.hclust(wide_DF_3.hc, k=3)\r\ncluster.stats(wide_DF_3.dist, cutree(wide_DF_3.hc, k=3))\r\n(asw<-sapply(2:8, function(x) summary(silhouette(cutree(wide_DF_3.h']",4,"Replication data, Affiliation Information, DataCite Dataset Metadata, Flemish Case Study, findings, publication, Data Science Journal, 2021, reproducible, research."
The direct drivers of recent global anthropogenic biodiversity loss,"Data files (which need to be placed in a folder called data) and program code (run the Rmd file) for the analyses presented in Jaureguiberry, Titeux et al. (2022) Science Advances.","['one.bootstrap.replicate <- function(to.bootstrap.long = droplevels(attplot), to.bootstrap.wide = droplevels(attplot.wide),\n\tstudies = unique(attplot.wide$UI)){\n\t\n\tn.studies <- length(studies)\n\n\tr.studies <- sample(studies, length(studies), replace = TRUE)\n\tr.studyXdriver <- data.frame(UI = rep(r.studies, each=nlevels(to.bootstrap.long[,3])), IPBES.direct.driver = rep(levels(to.bootstrap.long[,3]), n.studies))\n\tlong.replicate <- merge(r.studyXdriver, to.bootstrap.long, by.x = c(""UI"", ""IPBES.direct.driver""), by.y = c(""UI"", ""IPBES.direct.driver""), all.x=TRUE, all.y=FALSE)\n\t\n\tmatching.rows <- match(r.studies, to.bootstrap.wide$UI)\n\twide.replicate <- to.bootstrap.wide[matching.rows,]\n\n\treturn(list(long = long.replicate, wide = wide.replicate))\n\t}\n\n\n', 'analyse.long.data <- function(longdata = attplot.long){\n\n\t### Relative impacts of drivers aggregated (mean or weighted mean) ACROSS the different studies included in the analysis \n\t#   --> for each indicator (option ""analysis.level"" = indicators)\n\t#   --> for each indicator and ebv class (option ""analysis.level"" = indicators.x.ebv)\n\t#   --> for each indicator and then for each EBV class (option ""analysis.level"" = ebv)\n\t#   --> across all indicators irrespective of ebv class (option ""analysis.level"" = overall)\n\t\n\tnumber.of.rows <- nrow(longdata)\n\tnumber.of.different.studies <- length(unique(longdata[,1]))\n\t\n\tif(weighted.mean.procedure==""No""){ # arithmetic mean (irrespective of number of drivers assessed or spatial coverage)\n\t  if(analysis.level==""indicators"" || analysis.level==""overall""){\n\t\tlongdata <- aggregate(Direct.driver.importance.rescaled~Indicator.name+IPBES.direct.driver,data=longdata,mean)\n\t\tif(analysis.level==""overall""){\n\t\t  longdata <- aggregate(Direct.driver.importance.rescaled~IPBES.direct.driver,data=longdata,mean)      \n\t\t}\n\t  } else {\n\t\tlongdata <- aggregate(Direct.driver.importance.rescaled~Indicator.name+EBV.class+IPBES.direct.driver,data=longdata,mean)\n\t\tif(analysis.level==""ebv""){\n\t\t  longdata <- aggregate(Direct.driver.importance.rescaled~EBV.class+IPBES.direct.driver,data=longdata,mean)      \n\t\t}\n\t  }\n\t} else { # weighted mean (importance of each study varies according to the number of drivers assessed and the spatial coverage)\n\t  longdata$Weight <- (longdata$Ndrivers.weight+longdata$Scale.weight)/2 # weight is the average of scale and ndriver weights, in this analysis\n\t  if(analysis.level==""indicators"" || analysis.level==""overall""){  \n\t\tlongdata <- ldply(split(longdata,list(longdata$Indicator.name,longdata$IPBES.direct.driver)), function(longdata) weighted.mean(longdata$Direct.driver.importance.rescaled,w=longdata$Weight))\n\t\tlongdata[,3:4] <- data.frame(do.call(\'rbind\',strsplit(as.character(longdata$.id),\'.\',fixed=TRUE)))\n\t\tlongdata <- cbind(longdata[,3:4],longdata[,2])\n\t\tcolnames(longdata) <- c(""Indicator.name"",""IPBES.direct.driver"",""Direct.driver.importance.rescaled"")\n\t\tif(analysis.level==""overall""){\n\t\t  longdata <- aggregate(Direct.driver.importance.rescaled~IPBES.direct.driver,data=longdata,mean)  # each indicator is assumed to have the same importance for the overall pattern\n\t\t}\n\t  } else {\n\t\tlongdata <- ldply(split(longdata,list(longdata$Indicator.name,longdata$EBV.class,longdata$IPBES.direct.driver)), function(longdata) weighted.mean(longdata$Direct.driver.importance.rescaled,w=longdata$Weight))\n\t\tlongdata[,3:5] <- data.frame(do.call(\'rbind\',strsplit(as.character(longdata$.id),\'.\',fixed=TRUE)))\n\t\tlongdata <- cbind(longdata[,3:5],longdata[,2])\n\t\tcolnames(longdata) <- c(""Indicator.name"",""EBV.class"",""IPBES.direct.driver"",""Direct.driver.importance.rescaled"")\n\t\tif(analysis.level==""ebv""){\n\t\t  longdata <- aggregate(Direct.driver.importance.rescaled~EBV.class+IPBES.direct.driver,data=longdata,mean)  \n\t\t}  \n\t  }\n\t}\n\n\t# Rescale again the ranking of drivers (--> sum of ranks = 1) (needed after the averaging procedure applied above)\n\n\tif(analysis.level==""indicators""){\n\t  stats.importance <- longdata %>% group_by(Indicator.name) %>% summarise(Sum.importance=sum(Direct.driver.importance.rescaled,na.rm=T))\n\t  longdata <- merge(longdata,stats.importance,by.x=c(""Indicator.name""),by.y=c(""Indicator.name""),all.x=T,all.y=T)\n\t  longdata$Direct.driver.importance.rescaled <- longdata$Direct.driver.importance.rescaled/longdata$Sum.importance\n\t  longdata <- longdata[,-ncol(longdata)]\n\t  longdata <- merge(longdata,n.studies.per.indicator,by.x=c(""Indicator.name""),by.y=c(""Indicator.name""),all.x=T,all.y=T)\n\t} else if(analysis.level==""overall""){\n\t  stats.importance <- longdata %>% group_by() %>% summarise(Sum.importance=sum(Direct.driver.importance.rescaled,na.rm=T))\n\t  stats.importance <- as.numeric(stats.importance)\n\t  longdata$Direct.driver.importance.rescaled <- longdata$Direct.driver.importance.rescaled/stats.importance\n\t} else if(analysis.level==""indicators.x.ebv""){\n\t  stats.importance <- longdata %>% group_by(Indicator.name,EBV.class) %>% summarise(Sum.importance=sum(Direct.driver.importance.rescaled,na.rm=T))\n\t  longdata <- merge(longdata,stats.importance,by.x=c(""Indicator.name"",""EBV.class""),by.y=c(""Indicator.name"",""EBV.class""),all.x=T,all.y=T)\n\t  longdata$Direct.driver.importance.rescaled <- longdata$Direct.driver.importance.rescaled/longdata$Sum.importance\n\t  longdata <- longdata[,-ncol(longdata)]\n\t  longdata <- merge(longdata,n.studies.per.indicator.x.ebv,by.x=c(""Indicator.name"",""EBV.class""),by.y=c(""Indicator.name"",""EBV.class""),all.x=T,all.y=T)\n\t} else {\n\t  stats.importance <- longdata %>% group_by(EBV.class) %>% summarise(Sum.importance=sum(Direct.driver.importance.rescaled,na.rm=T))\n\t  longdata <- merge(longdata,stats.importance,by.x=c(""EBV.class""),by.y=c(""EBV.class""),all.x=T,all.y=T)\n\t  longdata$Direct.driver.importance.rescaled <- longdata$Direct.driver.importance.rescaled/longdata$Sum.importan', 'analyse.DS.mat <- function(r_i = rel.impacts, studied.drivers = study.drivers, IR = indicator.reweights, summarise.evidence=FALSE, show.result=FALSE){\n\t#Although this could be done using more functions from EloRating, the DS.mat is instead constructed manually\n\t#The reason is that it makes weighting and different treatment of draws easier\n\t\n\trequire(EloRating)\n\n  names(r_i) <- make.names(names(r_i)) #Some of the original names are not syntactically valid\n\n\t# replace inferred relative impacts with NAs, using info from studied.drivers\n\tnot.CC.studies <- studied.drivers$Study[studied.drivers$CC == FALSE]\n\tr_i$Direct.driver.importance.rescaled.Climate.change[r_i$UI %in% not.CC.studies] <- NA\n\n\tnot.DE.studies <- studied.drivers$Study[studied.drivers$DE == FALSE]\n\tr_i$Direct.driver.importance.rescaled.Direct.exploitation[r_i$UI %in% not.DE.studies] <- NA\n\n\tnot.IAS.studies <- studied.drivers$Study[studied.drivers$IAS == FALSE]\n\tr_i$Direct.driver.importance.rescaled.Invasive.alien.species[r_i$UI %in% not.IAS.studies] <- NA\n\n\tnot.LU.studies <- studied.drivers$Study[studied.drivers$LU == FALSE]\n\tr_i$Direct.driver.importance.rescaled.Land.sea.use.change[r_i$UI %in% not.LU.studies] <- NA\n\n\tnot.PO.studies <- studied.drivers$Study[studied.drivers$PO == FALSE]\n\tr_i$Direct.driver.importance.rescaled.Pollution[r_i$UI %in% not.PO.studies] <- NA\n\n\tnot.OT.studies <- studied.drivers$Study[studied.drivers$OT == FALSE]\n\tr_i$Direct.driver.importance.rescaled.Other[r_i$UI %in% not.OT.studies] <- NA\n\n\t#Establish structure to hold the scores ready for conversion to pairgames, i.e., one row per driver within each study\n\tr_i$row.number <- c(1:nrow(r_i))\n\tr_i$unique.ref <- paste(r_i$UI, r_i$row.number, sep=""_"") #So \'study\' will still be unique in bootstraps\n\n\t# How many of the 5 big direct drivers?\n\tr_i$Nd <- as.numeric(!is.na(r_i$Direct.driver.importance.rescaled.Climate.change)) +\n\t  as.numeric(!is.na(r_i$Direct.driver.importance.rescaled.Direct.exploitation)) +\n\t  as.numeric(!is.na(r_i$Direct.driver.importance.rescaled.Invasive.alien.species)) +\n\t  as.numeric(!is.na(r_i$Direct.driver.importance.rescaled.Land.sea.use.change)) +\n\t  as.numeric(!is.na(r_i$Direct.driver.importance.rescaled.Pollution))\n\t\n\t# Calculate case weights from geographic scale and number of drivers\n  r_i$case.weight <- r_i$Scale.weight * (r_i$Nd-1) # case.weight is how much clout this row will have in the DS calculation\n  r_i$old.cw <- r_i$Scale.weight * r_i$Ndrivers.weight # case weight in pre-July-2021 version of code; not the Ndriver.weight values are NOT the triangular numbers!\n  \n  # Set up data frame in which the indicators are rows and features of the evidence about them are the columns\n\tstatus <- as.data.frame(table(r_i$Indicator.name))\n\tnames(status) <- c(""Indicator.name"", ""Comparisons"")\n\tstatus$Scale <- tapply(r_i$Scale.weight, r_i$Indicator.name, FUN=""sum"") #Scale holds the sum of the Scale.weight values for all comparisons for that indicator\n\tstatus$Ndrivers <- tapply(r_i$Nd, r_i$Indicator.name, FUN=""sum"") #Ndrivers holds the sum of the number of drivers across all the indicator\'s comparisons\n\tstatus$ndw <- tapply(r_i$Ndrivers.weight, r_i$Indicator.name, FUN=""mean"") #ndw holds the mean of the (old) Ndriver.weight values for the indicator\'s comparisons\n\tstatus$sum.case.wt <- tapply(r_i$case.weight, r_i$Indicator.name, FUN=""sum"") # sum.case.wt is the summed clout per indicator, without considering any indicator weighting\n\n\tstatus$sumscale.ir <- mean(status$Scale)/status$Scale\n\tstatus$comparison.ir <- mean(status$Comparisons)/status$Comparisons\n\tstatus$equalising.ir <- mean(status$sum.case.wt)/status$sum.case.wt\n\t\n\tif (summarise.evidence==TRUE){\n\t  print(status)\n\t  plot(status$Indicator.name, status$Comparisons, ylab=""Number of comparisons"", xlab=""Indicator name"", cex.axis=0.45,\n\t       main = paste(""Range of values: "", min(status$Comparisons), ""-"", max(status$Comparisons),\n\t                    "" (factor of "", round(max(status$Comparisons)/min(status$Comparisons),1), "")""), sep="""")\n\t  plot(status$Indicator.name, status$Scale, ylab=""Summed scale weight"", xlab=""Indicator name"", cex.axis=0.45,\n\t       main = paste(""Range of values: "", round(min(status$Scale), 3), ""-"", round(max(status$Scale), 3),\n\t                    "" (factor of "", round(max(status$Scale)/min(status$Scale),1), "")""), sep="""")\n\t  plot(status$Indicator.name, status$sum.case.wt, ylab=""Sum of (scale wt * (Nd-1)"", xlab=""Indicator name"", cex.axis=0.45,\n\t       main = paste(""Range of values: "", round(min(status$sum.case.wt), 3), ""-"", round(max(status$sum.case.wt), 3),\n\t                    "" (factor of "", round(max(status$sum.case.wt)/min(status$sum.case.wt),1), "")""), sep="""")\n\t}\n\n\t# Set indicator weights based on value of IW passed to function, or the default which is ""comparisons""\n\tif (IR == ""comparisons"") r_i$Indicator.reweight <- status$comparison.ir[match(r_i$Indicator.name, status$Indicator.name)] #Weights inversely proportional to no of comparisons\n\tif (IR == ""none"") r_i$Indicator.reweight <- 1 #', '# Function to test whether normalised David\'s scores from different subsets of the data differ significantly\n\ncompare_subsets <- function(pooled, nrands=10000, plot=TRUE, output.each = TRUE, pairwise=""Conditional"", what=""among subsets""){\n  # Test for significance of among-subset differences in normalised DS scores\n  # Whether all pairs are then compared depends on value of pairwise: ""Yes"" -> yes; ""No"" -> no; \n  #   ""Conditional"" = only if p < 0.05 & n.subsets > 2\n  \n  # 1. Calculate the observed value of the test statistic\n  pooled <- droplevels(pooled) #necessary given how loop is constructed\n  \n  for (j in 1:nlevels(pooled$source)){\n    this <- subset(pooled, source == levels(pooled$source)[j])\n    this.DS <- analyse.DS.mat(this, study.drivers, IR=indicator.reweights, summarise.evidence=FALSE, show.result=FALSE)\n    this.DS$david_scores$source <- levels(pooled$source)[j]\n    if (j == 1){\n      this.obs <- this.DS$david_scores\n    }else{\n      this.obs <- rbind(this.obs, this.DS$david_scores)\n    }\n    if (output.each == TRUE){\n      # Run the bootstrap\n      # Still need to add the code\n    }\n  }\n  m1 <- lm(normDS ~ ID, data = this.obs)\n  observed.sigma <- summary(m1)$sigma\n  \n  # 2. Do randomisations\n  null.sigma <- rep(NA, nrands)\n  shuffled <- pooled\n  for (i in 1:nrands){\n    shuffled$source <- sample(shuffled$source)\n    for (j in 1:nlevels(shuffled$source)){\n      this <- subset(shuffled, source == levels(shuffled$source)[j])\n      this.DS <- analyse.DS.mat(this, study.drivers, IR=indicator.reweights, summarise.evidence=FALSE, show.result=FALSE)\n      if (j == 1){\n        this.rand <- this.DS$david_scores\n      }else{\n        this.rand <- rbind(this.rand, this.DS$david_scores)\n      }\n    }\n    m.rand <- lm(normDS ~ ID, data = this.rand)\n    null.sigma[i] <- summary(m.rand)$sigma\n  }\n  \n  # 3. Test\n  p.value <- sum(null.sigma >= observed.sigma)/length(null.sigma)\n  if (plot==TRUE){\n    plot(density(null.sigma), main = paste(""Test for sig diff "", what, "": p = "", p.value, sep=""""), \n         xlab=""Residual standard error"",\n         xlim=c(min(c(observed.sigma, null.sigma)), max(c(observed.sigma, null.sigma))))\n    abline(v=observed.sigma, col=""red"")\n  }\n  \n  # 4. Do pairwise comparisons among levels if appropriate\n  if (pairwise == ""Conditional"" & p.value < 0.05 & nlevels(pooled$source) > 2) pairwise <- TRUE\n  if (pairwise == TRUE){\n    # Make all pairwise comparisons\n    comparisons <- NULL\n    p.values <- NULL\n    k <- nlevels(pooled$source)\n    for (i in 1:(k-1)){\n      for (j in (i+1):k){\n        relevant <- levels(pooled$source)[c(i, j)]\n        comparison <- paste(levels(pooled$source)[c(i, j)], collapse="" v "")\n        print(comparison)\n        compdata <- subset(pooled, source %in% relevant)\n        pairdiff <- compare_subsets(compdata, what=comparison, nrands=1000) #Not 10,000 here - life\'s too short\n        comparisons <- append(comparisons, comparison)\n        p.values <- append(p.values, pairdiff$p.value)\n      }\n    }\n    pairdiffs <- data.frame(comparison=comparisons, uncorrected.p=p.values)\n    pairdiffs$corrected.p <- p.adjust(pairdiffs$uncorrected.p, method=""BY"")\n    to.return <- list(matrices=this.obs, sigma=observed.sigma, p.value=p.value, null.sigma=null.sigma, pairwise=pairdiffs)\n  }else{\n    to.return <- list(matrices=this.obs, sigma=observed.sigma, p.value=p.value, null.sigma=null.sigma)\n  }\n  \n  # 4. Return results\n  return(to.return)\n}\n', '### Load packages and simple functions\n\n# The following packages are needed to run the script\n\nlibrary(reshape)\nlibrary(ggplot2)\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(Hmisc)\nlibrary(EloRating)\n\nsummarise_bootstraps <- function(bootstrap, report=TRUE){\n  sb <- data.frame(driver = c(""climate_change"", ""direct_exploitation"", ""ias"", ""land_sea_use"", ""pollution"", ""other""),\n                   mean=rep(NA, 6), lower_ci=rep(NA, 6), upper_ci=rep(NA, 6))\n  upper <- ifelse(exclude.other.drivers == ""Yes"", 5, 6)\n  for (i in 1:upper){\n    sb$mean[i] <- mean(bootstrap[[i]])\n    sb$lower_ci[i] <- quantile(bootstrap[[i]], 0.025)\n    sb$upper_ci[i] <- quantile(bootstrap[[i]], 0.975)\n    if (report==TRUE){\n      cat(paste(sb$driver[i]),"": mean = "", round(sb$mean[i],3), ""; 95% CI:"", round(sb$lower_ci[i], 3), "" - "", round(sb$upper_ci[i], 3), ""\\n"", sep="""")\n    }\n  }\n  if (report==TRUE) cat(""\\n"")\n  sb <- sb[1:upper,]\n  return(sb)\n}\n\nwrite.results.csv <- function(observed.david.score=observed.david.score, sb=sb, ps=ps, filestem){\n  sb <- sb[1:5,]\n  p.boot <- ps$uncorrected.p\n  p.adj <- ps$adjusted.p\n  \n  results <- data.frame(Driver = sort(observed.david.score$david_scores$ID),\n                        David_score = observed.david.score$david_scores$normDS[order(observed.david.score$david_scores$ID)],\n                        DS0.025 = sb$lower_ci[1:5],\n                        DS0.975 = sb$upper_ci[1:5],\n                        p_diff_CC = c(NA, p.boot[1:4]),\n                        p_diff_DE = c(p.boot[1], NA, p.boot[5:7]),\n                        p_diff_IAS = c(p.boot[c(2, 5)], NA, c(p.boot[8:9])),\n                        p_diff_LU = c(p.boot[c(3, 6, 8)], NA, p.boot[10]),\n                        p_diff_PO = c(p.boot[c(4, 7, 9, 10)], NA),\n                        padj_diff_CC = c(NA, p.adj[1:4]),\n                        padj_diff_DE = c(p.adj[1], NA, p.adj[5:7]),\n                        padj_diff_IAS = c(p.adj[c(2, 5)], NA, c(p.adj[8:9])),\n                        padj_diff_LU = c(p.adj[c(3, 6, 8)], NA, p.adj[10]),\n                        padj_diff_PO = c(p.adj[c(4, 7, 9, 10)], NA))\n  filename <- paste(""output/DSresults_"", make.names(paste(indicator.reweights, ""_"", sub("" "", """", filestem),\n                                                ""_"", Sys.time(), "".csv"", sep="""")))\n  write.csv(results, file=filename)\n  print(paste(""Results written to"", filename))\n  return(results)\n}\n\n\n', '# This block of code allows the user to control the way the data are analysed\n\n# Specify whether Other is to be included or excluded\nexclude.other.drivers <- ""Yes"" # options: ""No"", ""Yes""\n# --> ""No"": Other drivers are included in the comparisons and weights\n# --> ""Yes"": Comparisons involving Other drivers are removed so that only the ""Big 5"" are considered\n\n# Specify the hypothesis made to assign a relative impact to drivers that are not analysed in a study \ninclude.non.assessed.drivers <- ""Less"" # options: ""No"", ""Less"", ""Equal"", ""Unknown"" \n# --> ""No"": non-assessed drivers have NO impact (hypothesis 1)\n# --> ""Less"": non-assessed drivers have LESS impact than lowest-ranked assess driver (hypothesis 2)\n# --> ""Equal"": non-assessed drivers have SAME impact as lowest-ranked assessed driver (hypothesis 2bis)\n# --> ""Unknown"": non-assessed drivers have UNKNOW impact - i.e. average impact of assessed drivers (hypothesis 3)\n\n# Specify whether magnitude of impact should be used instead of ranks to estimate the relative impact of drivers for the studies where this information is available\ncombine.rank.magnitude <- ""No"" # options:  ""Yes"", ""No""\n# --> ""Yes"": use magnitude instead of rank information when magnitude is reliably estimated\n# --> ""No"": use only rank information for all studies even if magnitude is reliably estimated (we decided to go for this option)\n\n# Specify the level at which the relative impacts of drivers are to be assessed\nanalysis.level <- ""overall"" # options: ""indicators"", ""ebv"", ""indicators.x.ebv"", ""overall""\n# --> ""indicators"": assessment of relative impact of drivers at the level of each individual indicators (i.e. aggregated across studies within each indicator)\n# --> ""ebv"": assessment of relative impact of drivers at the level of each ebv class (i.e. aggregated across the individual indicators within each ebv class)\n# --> ""indicators.x.ebv"": assessment of relative impact of drivers at the level of each individual indicators  for the ebv level separately\n# --> ""overall"": assessment of relative impact of drivers at the overall level (i.e. aggregated across all indicators)\n\n# Specify whether the analysis should be restricted to a selected list of indicators \nindicator.selection <- ""No"" # options: ""Yes"", ""No""\n# --> ""No"": analysis based on all indicators included in the data files\n# --> ""Yes"": analysis based on the indicators selected below\nselected.indicator <- c(""FSS"",""IFL"",""IUC"",""LCC"",""LPI"",""LSR"",""MFC"",""MLF"",""MSA"",""NPP"",""RLI"") # acronyms of the indicators selected for the analysis\n\n# Specify whether the analysis should be excluded some indicators \nindicator.exclusion <- ""No"" # options: ""Yes"", ""No""\n# --> ""No"": analysis based on all indicators included in the data files\n# --> ""Yes"": analysis based on all indicators except those selected below\nexcluded.indicator <- c("""") # acronyms of the indicators excluded from the analysis\n\n# Specify whether the analysis should be restricted to a particular realm \nrealm.selection <- ""No"" # options: ""Yes"", ""No""\n# --> ""No"": analysis based on all realms\n# --> ""Yes"": analysis based on the realm selected below\nselected.realm <- ""Marine"" # options: ""Terrestrial"", ""Freshwater"", ""Marine"", ""All realms"" (""All realms"" = studies with assessments across the three realms)\ninclude.all.realms <- ""No"" # options: ""Yes"", ""No""\n# --> ""No"": exclude results from studies with cross-realm assessments (""All realms"")\n# --> ""Yes"": include results from studies with cross-realm assessments (""All realms"") along with studies with assessments on the selected realm\n\n# Specify whether the analysis should be restricted to a particular IPBES region \nregion.selection <- ""No"" # options: ""Yes"", ""No""\n# --> ""No"": analysis based on all IPBES regions\n# --> ""Yes"": analysis based on the IPBES region selected below\nselected.region <- ""Europe and Central Asia"" # options: ""Americas"", ""Africa"", ""Europe and Central Asia"", ""Asia-Pacific"", ""All regions"" (""All regions"" = studies with assessments across the four regions)\ninclude.all.regions <- ""No"" # options: ""Yes"", ""No""\n# --> ""No"": exclude results from studies with cross-region assessments (""All regions"")\n# --> ""Yes"": include results from studies with cross-region assessments (""All regions"") along with studies with assessments on the selected region\n\n# Specify whether the analysis should be restricted to a particular climatic domain \ndomain.selection <- ""No"" # options: ""Yes"", ""No""\n# --> ""No"": analysis based on all domains\n# --> ""Yes"": analysis based on the domain selected below\nselected.domain <- ""Polar"" # options: ""Polar"", ""Boreal"", ""Temperate"", ""Subtropical"", ""Tropical"", ""All domains"" (""All domains"" = studies with assessments across the five domains)\ninclude.all.domains <- ""No"" # options: ""Yes"", ""No""\n# --> ""Yes"": exclude results from studies with cross-domain assessments (""All domains"")\n# --> ""Yes"": include results from studies with cross-domain assessments (""All domains"") along with studies with assessments on the selected domain\n\n# Specify whether the analysis shoul', '\n### Replace NA values in data files with values from the previous row where needed\nno.replace.na <- c(17,19,20,21,22,24,25,26) # identify columns in step 4 files whose NA values should not be replaced with values from the previous row\n\natt3 <- rbind(att3.wos,att3.no.wos)\natt4 <- rbind(att4.wos,att4.no.wos)\n\n# Identify, within each study, which drivers actually have ranks\nstudy.drivers <- data.frame(Study = att3$UI)\nstudy.drivers$CC <- ifelse(att3$Assessment.of.climate.change.impact == ""Yes"", TRUE, FALSE)\nstudy.drivers$DE <- ifelse(att3$Assessment.of.resource.extraction.impact == ""Yes"", TRUE, FALSE)\nstudy.drivers$IAS <- ifelse(att3$Assessment.of.invasive.alien.species.impact == ""Yes"", TRUE, FALSE)\nstudy.drivers$LU <- ifelse(att3$Assessment.of.land.sea.use.change.impact == ""Yes"", TRUE, FALSE)\nstudy.drivers$PO <- ifelse(att3$Assessment.of.pollution.impact == ""Yes"", TRUE, FALSE)\nstudy.drivers$OT <- ifelse(att3$Assessment.of.other.driver.s..impact == ""Yes"", TRUE, FALSE)\nif (exclude.other.drivers == ""Yes"") study.drivers$OT <- FALSE # We are not considering other drivers\nsaveRDS(study.drivers, ""output/study.drivers.Rds"")\n\nlevels(att4$Taxonomic.group..2.) <- c(levels(att4$Taxonomic.group..2.),""Not applicable"")\nlevels(att4$Original.indicator.name) <- c(levels(att4$Original.indicator.name),""Not applicable"")\nlevels(att4$Relevance.of.direct.driver.magnitude.assessment) <- c(levels(att4$Relevance.of.direct.driver.magnitude.assessment),""Not applicable"")\n\nfor (i in 1:nrow(att4)) {  \n  if (!is.na(att4$Indicator.name[i])) {\n    if (is.na(att4$Original.indicator.name[i])) {\n      att4$Original.indicator.name[i] <- ""Not applicable""\n    }\n    if (is.na(att4$Taxonomic.group..1.[i])) {\n      att4$Taxonomic.group..1.[i] <- ""Not specified""\n    }\n    if (is.na(att4$Relevance.of.direct.driver.magnitude.assessment[i])) {\n      att4$Relevance.of.direct.driver.magnitude.assessment[i] <- ""Not applicable""\n    }\n}\n  if (!is.na(att4$Taxonomic.group..1.[i])) {\n    if (is.na(att4$Taxonomic.group..2.[i])) {\n      att4$Taxonomic.group..2.[i] <- ""Not applicable""\n    }\n  }\n  col <- which(is.na(att4[i,]) | att4[i,]==\'\')\n  col <- col[-which(col %in% no.replace.na)]\n  if (is.na(att4$Unique.identifier..UI.[i])){ \n    att4[i,][col] <- att4[i-1,][col]   \n  } else {\n    att4[i,][col] <- att4[i,][col]\n  }\n}\n\natt <- merge(att3,att4,by.x=c(""UI""),by.y=c(""Unique.identifier..UI.""),all.x=F,all.y=F)\natt$IPBES.direct.driver <- as.factor(att$IPBES.direct.driver) #Added 2021-06-02\n### Rename levels of some factors\n\n# Drivers\nlevels(att$IPBES.direct.driver) <- c(levels(att$IPBES.direct.driver),""Direct exploitation"") \natt$IPBES.direct.driver[att$IPBES.direct.driver==""Resource extraction""]  <- ""Direct exploitation""\nold.lvl <- levels(att$IPBES.direct.driver)\n\nif (exclude.other.drivers == TRUE){\n    # This condition and the next line added 2020-09-02 to tidy up conditional removal of Other as a direct driver; previously the \'else\' condition was unconditional\n    att$IPBES.direct.driver <- factor(att$IPBES.direct.driver,levels=sort(old.lvl[old.lvl!=""Other""],decreasing=F))\n  }else{\n    att$IPBES.direct.driver <- factor(att$IPBES.direct.driver,levels=c(sort(old.lvl[old.lvl!=""Other""],decreasing=F),""Other""))\n  }\natt$IPBES.direct.driver <- factor(att$IPBES.direct.driver,levels=levels(factor(att$IPBES.direct.driver)))\n\n# EBV classes\nlevels(att$EBV.class) <- c(levels(att$EBV.class),""Genetic\\ncomposition"",""Community\\ncomposition"",""Ecosystem\\nfunction"",""Ecosystem\\nstructure"",""Species\\npopulations"",""Species\\ntraits"") \natt$EBV.class[att$EBV.class==""Genetic composition""]  <- ""Genetic\\ncomposition""\natt$EBV.class[att$EBV.class==""Community composition""]  <- ""Community\\ncomposition""\natt$EBV.class[att$EBV.class==""Ecosystem function""]  <- ""Ecosystem\\nfunction""\natt$EBV.class[att$EBV.class==""Ecosystem structure""]  <- ""Ecosystem\\nstructure""\natt$EBV.class[att$EBV.class==""Species populations""]  <- ""Species\\npopulations""\natt$EBV.class[att$EBV.class==""Species traits""]  <- ""Species\\ntraits""\nold.lvl <- levels(att$EBV.class)\natt$EBV.class <- factor(att$EBV.class,levels=c(""Genetic\\ncomposition"",""Species\\npopulations"",""Species\\ntraits"",""Community\\ncomposition"",""Ecosystem\\nstructure"",""Ecosystem\\nfunction""))\n\n### Selection of subsets of data to be included in the analysis depending on options specified above by the user\n\n# Realm selection\nif(realm.selection==""Yes""){\n  selected.realm.vector <- att[grep(selected.realm,att$Realm.s),]\n  selected.realm.vector <- as.vector(unique(selected.realm.vector$Realm.s))\n  if(include.all.realms==""Yes""){\n    selected.realm.vector <- c(selected.realm.vector,""All realms"")\n  } else {\n    selected.realm.vector <- selected.realm.vector\n  }\n  att <- att[is.element(att$Realm.s,selected.realm.vector),]\n  print(paste(""Selected realms:"", paste(selected.realm.vector, collapse="", "")))\n}\n\n# Region selection\nif(region.selection==""Yes""){\n  selected.region.vector <- att[grep(selected.region,att$IPBES.Region.s),]\n  selected.region.vector <- as.vector(unique(selected.region.vector$IP', '\natt <- merge(att,stats.ranks,by.x=c(""Unique.identifier.level""),by.y=c(""Unique.identifier.level""),all.x=T,all.y=T)\natt <- merge(att,assessments.with.other,by.x=c(""Unique.identifier.level""),by.y=c(""Unique.identifier.level""),all.x=T,all.y=T)\n\nfor (i in 1:nrow(att)) {\n  if (is.na(att$Other.direct.driver[i])){ \n    att$Other.direct.driver[i] <- 0  \n  }\n}\n\nif (exclude.other.drivers == ""Yes""){\n  att <- subset(att, IPBES.direct.driver != ""Other"")\n}\n\natt$N.IPBES.direct.drivers <- att$N.direct.drivers\n\nfor (i in 1:nrow(att)) {\n  if(include.non.assessed.drivers==""No""){\n    att$Direct.driver.rank.reversed[i] <- att$Max.rank[i]-att$Direct.driver.rank[i]+1\n    if (is.na(att$Direct.driver.rank[i])){\n      att$Direct.driver.rank.reversed[i] <- 0 # non assessed drivers have NO impact \n    }\n  }   \n  if(include.non.assessed.drivers==""Less""){\n    if (is.na(att$Direct.driver.rank[i])){\n      att$Direct.driver.rank[i] <- att$Max.rank[i]+1 # non assessed drivers have LESS impact than any assessed driver\n    }\n    if (att$N.direct.drivers[i]==6){ \n      att$Direct.driver.rank.reversed[i] <- att$Max.rank[i]-att$Direct.driver.rank[i]+1  \n    } else { \n      att$Direct.driver.rank.reversed[i] <- att$Max.rank[i]+1-att$Direct.driver.rank[i]+1  \n    }\n  }\n  if(include.non.assessed.drivers==""Equal""){\n    if (is.na(att$Direct.driver.rank[i])){\n      att$Direct.driver.rank[i] <- att$Max.rank[i] # non assessed drivers have SAME impact as lowest-ranked assessed driver\n    }\n    att$Direct.driver.rank.reversed[i] <- att$Max.rank[i]-att$Direct.driver.rank[i]+1\n  } \n  if(include.non.assessed.drivers==""Unknown""){\n    if (is.na(att$Direct.driver.rank[i])){\n      att$Direct.driver.rank[i] <- att$Avg.rank[i] # non assessed drivers have UNKNOWN impact\n    }\n    att$Direct.driver.rank.reversed[i] <- att$Max.rank[i]-att$Direct.driver.rank[i]+1\n  } \n}  \n  \natt <- att[with(att,order(Unique.identifier.level,UI)),]\n\nfor (i in 1:nrow(att)) {  \n  col <- which(is.na(att[i,]))\n  col <- col[-which(col %in% c(62,63,64,65,67,68))]\n  att[i,][col] <- att[i-1,][col]   \n}\n\n### Rescale the ranking of drivers (--> sum of ranks = 1 for each assessment within each study)\n\nstats.ranks <- att %>% group_by(UI,Level.of.analysis) %>% summarise(Sum.rank=sum(Direct.driver.rank.reversed,na.rm=T))\natt <- merge(att,stats.ranks,by.x=c(""UI"",""Level.of.analysis""),by.y=c(""UI"",""Level.of.analysis""),all.x=T,all.y=T)\natt$Direct.driver.rank.rescaled <- att$Direct.driver.rank.reversed/att$Sum.rank\nstats.ranks.rescaled <- att %>% group_by(UI,Level.of.analysis) %>% summarise(Sum.rank.rescaled=sum(Direct.driver.rank.rescaled,na.rm=T))\natt <- merge(att,stats.ranks.rescaled,by.x=c(""UI"",""Level.of.analysis""),by.y=c(""UI"",""Level.of.analysis""),all.x=T,all.y=T)\n\n### Rescale the magnitude of drivers (-->sum of magnitudes = 1 for each assessment within each study)\n\nstats.magnitude <- att %>% group_by(UI,Level.of.analysis) %>% summarise(Sum.magnitude=sum(Direct.driver.magnitude,na.rm=T))\natt <- merge(att,stats.magnitude,by.x=c(""UI"",""Level.of.analysis""),by.y=c(""UI"",""Level.of.analysis""),all.x=T,all.y=T)\natt$Direct.driver.magnitude.rescaled <- att$Direct.driver.magnitude/att$Sum.magnitude\nstats.magnitude.rescaled <- att %>% group_by(UI,Level.of.analysis) %>% summarise(Sum.magnitude.rescaled=sum(Direct.driver.magnitude.rescaled,na.rm=T))\natt <- merge(att,stats.magnitude.rescaled,by.x=c(""UI"",""Level.of.analysis""),by.y=c(""UI"",""Level.of.analysis""),all.x=T,all.y=T)\n\n\nfor (i in 1:nrow(att)) {   \n  if (is.na(att$Direct.driver.magnitude.rescaled[i])){\n    if (att$Sum.magnitude.rescaled[i]==0){\n      att$Direct.driver.importance.rescaled[i] <- att$Direct.driver.rank.rescaled[i]\n    } else {\n      att$Direct.driver.magnitude.rescaled[i] <- 0\n      if(combine.rank.magnitude==""Yes""){\n        if(att$Relevance.of.direct.driver.magnitude.assessment[i]==""Yes""){\n          att$Direct.driver.importance.rescaled[i] <- att$Direct.driver.magnitude.rescaled[i]\n        } else {\n          att$Direct.driver.importance.rescaled[i] <- att$Direct.driver.rank.rescaled[i]\n        } \n      } else {\n        att$Direct.driver.importance.rescaled[i] <- att$Direct.driver.rank.rescaled[i]\n      }\n    }\n  } else {\n    if(combine.rank.magnitude==""Yes""){\n      if(att$Relevance.of.direct.driver.magnitude.assessment[i]==""Yes""){\n        att$Direct.driver.importance.rescaled[i] <- att$Direct.driver.magnitude.rescaled[i]\n      } else {\n        att$Direct.driver.importance.rescaled[i] <- att$Direct.driver.rank.rescaled[i]\n      } \n    } else {\n      att$Direct.driver.importance.rescaled[i] <- att$Direct.driver.rank.rescaled[i]\n    }\n  }\n}\nstats.importance.rescaled <- att %>% group_by(UI,Level.of.analysis) %>% summarise(Sum.importance.rescaled=sum(Direct.driver.importance.rescaled,na.rm=T))\natt <- merge(att,stats.importance.rescaled,by.x=c(""UI"",""Level.of.analysis""),by.y=c(""UI"",""Level.of.analysis""),all.x=T,all.y=T)\n\n### Remove redundant information among different assessments within each study (based on column ""Sublevel of analysis"")\n#   --> assessments']",4,"Keywords: anthropogenic biodiversity loss, data files, program code, Rmd file, Jaureguiberry, Titeux, Science Advances."
Stranger danger: A meta-analysis of the dear enemy hypothesis,"The dear enemy hypothesis predicts that territorial individuals will be less aggressive toward known neighbors than to strangers. This hypothesis has been well studied and there is a wealth of data demonstrating its prevelance among some taxa. However, a quantitative synthesis is needed to test the generality of the phenomenon, identify key mechanisms driving the behavior, and guide future research. Here we made a comprehensive collection of Dear Enemy data from 138 studies representing 105 species spread over eight taxonomic classes. The associated paper finds that Dear Enemy is a common phenomenon but drivers of this phenomenon are still understudied and current knowledge is unable to disentangle competing hypotheses of drivers of this behavior.","['## new phyloglmm_setup\n\nphylo.to.Z <- function(r,stand=FALSE){\n  ntip <- length(r$tip.label)\n  Zid <- Matrix(0.0,ncol=length(r$edge.length),nrow=ntip)\n  nodes <- (ntip+1):max(r$edge)\n  root <- nodes[!(nodes %in% r$edge[,2])]\n  for (i in 1:ntip){\n    cn <- i  ## current node\n    while (cn != root){\n      ce <- which(r$edge[,2]==cn)   ## find current edge\n      Zid[i,ce] <- 1   ## set Zid to 1\n      cn <- r$edge[ce,1]            ## find previous node\n    }\n  }\n  V <- vcv(r)\n  # V <- V/max(V)\n  sig <- exp(as.numeric(determinant(V)[""modulus""])/ntip)\n  # sig <- det(V)^(1/ntip)\n  Z <- t(sqrt(r$edge.length) * t(Zid))\n  if(stand){Z <- t(sqrt(r$edge.length/sig) * t(Zid))}\n  rownames(Z) <- r$tip.label\n  colnames(Z) <- 1:length(r$edge.length)\n  return(Z)                                  \n}\n\nphylo_lmm <- function(formula,data,phylo,phylonm=NULL,phyloZ=NULL,control,REML,doFit,lambhack=NULL){\n  lmod <- lFormula(formula=formula,data = data,control=control, REML=REML,phylonm=phylonm, phyloZ=phyloZ,lambhack=lambhack)\n  if(!lambhack){\n  devfun <- do.call(lme4:::mkLmerDevfun, lmod)\n  }\n  if(lambhack){\n    devfun <- do.call(mkLmerDevfun, lmod)\n  }\n  opt <- optimizeLmer(devfun,control=control$optCtrl)\n  mkMerMod(environment(devfun), opt, lmod$reTrms, fr = lmod$fr)\n}\n\nlFormula <- function (formula, data = NULL, REML = TRUE, subset, weights, \n          na.action, offset, contrasts = NULL, control = lmerControl(),phylonm,phyloZ, lambhack=FALSE,\n          ...) \n{\n  control <- control$checkControl\n  mf <- mc <- match.call()\n  ignoreArgs <- c(""start"", ""verbose"", ""devFunOnly"", ""control"")\n  l... <- list(...)\n  l... <- l...[!names(l...) %in% ignoreArgs]\n  do.call(lme4:::checkArgs, c(list(""lmer""), l...))\n  if (!is.null(list(...)[[""family""]])) {\n    mc[[1]] <- quote(lme4::glFormula)\n    if (missing(control)) \n      mc[[""control""]] <- glmerControl()\n    return(eval(mc, parent.frame()))\n  }\n  cstr <- ""check.formula.LHS""\n  lme4:::checkCtrlLevels(cstr, control[[cstr]])\n  denv <- lme4:::checkFormulaData(formula, data, checkLHS = control$check.formula.LHS == \n                             ""stop"")\n  formula <- as.formula(formula, env = denv)\n  lme4:::RHSForm(formula) <- expandDoubleVerts(lme4:::RHSForm(formula))\n  mc$formula <- formula\n  m <- match(c(""data"", ""subset"", ""weights"", ""na.action"", ""offset""), \n             names(mf), 0L)\n  mf <- mf[c(1L, m)]\n  mf$drop.unused.levels <- TRUE\n  mf[[1L]] <- quote(stats::model.frame)\n  fr.form <- subbars(formula)\n  environment(fr.form) <- environment(formula)\n  for (i in c(""weights"", ""offset"")) {\n    if (!eval(bquote(missing(x = .(i))))) \n      assign(i, get(i, parent.frame()), environment(fr.form))\n  }\n  mf$formula <- fr.form\n  fr <- eval(mf, parent.frame())\n  fr <- factorize(fr.form, fr, char.only = TRUE)\n  attr(fr, ""formula"") <- formula\n  attr(fr, ""offset"") <- mf$offset\n  n <- nrow(fr)\n  reTrms <- mkReTrms(findbars(lme4:::RHSForm(formula)), fr, phylonm, phyloZ, lambhack)\n  wmsgNlev <- lme4:::checkNlevels(reTrms$flist, n = n, control)\n  wmsgZdims <- lme4:::checkZdims(reTrms$Ztlist, n = n, control, allow.n = FALSE)\n  if (anyNA(reTrms$Zt)) {\n    stop(""NA in Z (random-effects model matrix): "", ""please use "", \n         shQuote(""na.action=\'na.omit\'""), "" or "", shQuote(""na.action=\'na.exclude\'""))\n  }\n  wmsgZrank <- lme4:::checkZrank(reTrms$Zt, n = n, control, nonSmall = 1e+06)\n  fixedform <- formula\n  lme4:::RHSForm(fixedform) <- nobars(lme4:::RHSForm(fixedform))\n  mf$formula <- fixedform\n  fixedfr <- eval(mf, parent.frame())\n  attr(attr(fr, ""terms""), ""predvars.fixed"") <- attr(attr(fixedfr, \n                                                         ""terms""), ""predvars"")\n  ranform <- formula\n  lme4:::RHSForm(ranform) <- subbars(lme4:::RHSForm(lme4:::reOnly(formula)))\n  mf$formula <- ranform\n  ranfr <- eval(mf, parent.frame())\n  attr(attr(fr, ""terms""), ""predvars.random"") <- attr(terms(ranfr), \n                                                     ""predvars"")\n  X <- model.matrix(fixedform, fr, contrasts)\n  if (is.null(rankX.chk <- control[[""check.rankX""]])) \n    rankX.chk <- eval(formals(lmerControl)[[""check.rankX""]])[[1]]\n  X <- lme4:::chkRank.drop.cols(X, kind = rankX.chk, tol = 1e-07)\n  if (is.null(scaleX.chk <- control[[""check.scaleX""]])) \n    scaleX.chk <- eval(formals(lmerControl)[[""check.scaleX""]])[[1]]\n  X <- lme4:::checkScaleX(X, kind = scaleX.chk)\n  list(fr = fr, X = X, reTrms = reTrms, REML = REML, formula = formula, phyloZ=phyloZ, \n       wmsgs = c(Nlev = wmsgNlev, Zdims = wmsgZdims, Zrank = wmsgZrank))\n}\n\n\nmkReTrms <- function(bars, fr, phylonm,phyloZ,lambhack,drop.unused.levels = TRUE){\n  if (!length(bars)) \n    stop(""No random effects terms specified in formula"", \n         call. = FALSE)\n  stopifnot(is.list(bars), vapply(bars, is.language, NA), inherits(fr, \n                                                                   ""data.frame""))\n  names(bars) <- lme4:::barnames(bars)\n  term.names <- vapply(bars, lme4:::safeDeparse, """")\n  blist <- lapply(bars, mkBlist, fr, phylonm, phyloZ, drop.unused.levels)\n  ']",4,"stranger danger, meta-analysis, dear enemy hypothesis, territorial individuals, aggression, neigbors, strangers, taxa, quantitative synthesis, behavior, mechanisms, research, data, species, taxonomic classes, common phenomenon, drivers, competing hypotheses,"
Data and Supplemental Material from: A multifunction trade-off has contrasting effects on the evolution of form and function,"Trade-offs caused by the use of an anatomical apparatus for more than one function are thought to be an important constraint on evolution. However, whether multifunctionality suppresses diversification of biomechanical systems is challenged by recent literature showing that traits more closely tied to trade-offs evolve more rapidly. We contrast the evolutionary dynamics of feeding mechanics and morphology between fishes that exclusively capture prey with suction and multifunctional species that augment this mechanism with biting behaviors to remove attached benthic prey. Diversification of feeding kinematic traits was, on average, over 13.5 times faster in suction feeders, consistent with constraint on biters due to mechanical trade-offs between biting and suction performance. Surprisingly, we found that the evolution of morphology contrasts directly with these differences in kinematic evolution, with significantly faster rates of evolution of head shape in biters. This system provides clear support for an often postulated, but rarely confirmed prediction that multifunctionality stifles functional diversification, while also illustrating the sometimes weak relationship between form and function.","['# CORN, MARTINEZ, BURRESS, and WAINWRIGHT at Systematic Biology\r\n#doi: https://doi.org/10.1093/sysbio/syaa091\r\n\r\n########################################\r\n#         LOADING IN THE DATA          #\r\n########################################\r\n\r\n#This section is code from C. Martinez, modified by K. Corn\r\n\r\n#load packages\r\nrequire(geomorph)\r\nrequire(tidyverse)\r\n\r\n#load data table\r\nspecies_table= read.csv("".../Corn_et_al_SysBio_scales.csv"", header=T) #this is the table that contains the scale info\r\nnames(species_table)\r\nspecies_table <- species_table[-c(37:42),] #take out dactylopus because it\'s wacky and screws everything up\r\nspecies_table[85,2] \r\n\r\nrequire(car) #just fixing some species whose names are spelled wrong\r\nspecies_table$species<-recode(species_table$species,""\'Paracentropodon_rubripinnis\'=\'Paracentropogon_rubripinnis\'"")\r\nspecies_table$species<-recode(species_table$species,""\'Siganus_sp_black\'=\'Siganus_uspi\'"")\r\nspecies_table$species<-recode(species_table$species,""\'Siganus_sp_white\'=\'Siganus_virgatus\'"")\r\nspecies_table$species<-recode(species_table$species,""\'Inermia_vittata\'=\'Haemulon_vittatum\'"")\r\nspecies_table$species<-recode(species_table$species,""\'Centrogenys_species\'=\'Centrogenys_vaigiensis\'"")\r\nspecies_table$species<-recode(species_table$species,""\'Chromileptes_altivelis\'=\'Cromileptes_altivelis\'"")\r\nspecies_table$species<-recode(species_table$species,""\'Pomacanthus_xanthometapon\'=\'Pomacanthus_xanthometopon\'"")\r\nspecies_table$species<-recode(species_table$species,""\'Parupeneus_cyclostoma\'=\'Parupeneus_cyclostomus\'"")\r\nspecies_table$species_specimen<-recode(species_table$species_specimen,""\'Emmelichthyops_atlanticus_18\'=\'Emmelichthyops_atlanticus_8\'"")\r\n#if these start giving you problems, replace ""recode"" with ""car::recode"". recode can conflict with dplyr recode function\r\n\r\n#make a vector w order that everything is in, then go through every object and compare and make sure that object is in the right order matching that vector\r\n\r\nspecies_table$video_id <- paste0(species_table$species_specimen,""_"",seq_along(species_table$species))\r\n\r\nnrow(species_table)\r\nn_strike= 175 #should be the same number as nrow(species_table)\r\n\r\ncounter= c(1:n_strike)\r\nstrike_labels= as.data.frame(matrix(nrow=(n_strike*10), ncol=1)) # table that complements data of 1750 aligned shapes\r\nfor(i in 1:n_strike){\r\n  num= counter[i]\r\n  strike_labels[c(1:10) + (i-1)*(10),]= rep(num,10)\r\n}\r\n\r\n#read in landmarks\r\ncoord_data= readland.tps(file="".../Corn_et_al_SysBio_LMs.TPS"",specID = ""imageID"") \r\nminus_dactylopus <- coord_data[,,-c(361:420)]\r\n#read in data table of the semilandmarks\r\nsemiland= as.matrix(read.table("".../Corn_et_al_SysBio_sliding_semi.txt"", header=T))\r\n\r\nunaligned_2d= two.d.array(minus_dactylopus) \r\n\r\n\r\n\r\nn_specimen=175\r\n\r\nscaled= as.data.frame(matrix(nrow=1750, ncol=36))\r\nfor(i in 1:n_specimen){\r\n  sc = species_table[i,4] #same as ""video labels""\r\n  shapes=  unaligned_2d[c(1:10) + (i-1)*(10),]\r\n  scaled_shape= shapes/sc\r\n  scaled[c(1:10) + (i-1)*(10),]= scaled_shape\r\n}\r\n\r\nscaled\r\nrownames(scaled) <- rownames(unaligned_2d)\r\n\r\np=18 #number of landmarks\r\nk=2 #number of lms\r\nscaled_3d= arrayspecs(scaled, p=p, k=k)\r\n\r\n#<---------------- GPA then put coords in 2D table ---------------->#\r\ny= gpagen(scaled_3d, curves=semiland, ProcD=T) # $coords and $Csize  \r\ny2= two.d.array(y$coords) \r\n\r\ntang = gm.prcomp(A=y$coords)\r\nplot(tang, axis1 = 1, axis2 = 2)\r\n\r\nn_specimen=175\r\n\r\n## table of Procrustes distances b/t successive motion points\r\nkinetic_procrustes= as.data.frame(matrix(ncol=n_specimen+1, nrow=12))\r\nfor(f in 1:n_specimen){\r\n  spec= y2[10*(f-1) +c(1:10),]\r\n  for(i in 1:9){\t\r\n    proc= sqrt(sum((spec[i,]-spec[i+1,])^2))\r\n    kinetic_procrustes[i,f+1]= proc # successive procrustes distances b/t steps\r\n    kinetic_procrustes[10,f+1]= sum(kinetic_procrustes[1:9,f+1]) # total path length\r\n    kinetic_procrustes[11,f+1]= sqrt(sum((spec[1,]-spec[10,])^2)) # procrustes first to last shapes (linear vector)\r\n    kinetic_procrustes[12,f+1]= kinetic_procrustes[11,f+1]/kinetic_procrustes[10,f+1] # ratio of linear to nonlinear  \r\n  }}\r\nkinetic_procrustes[,1]= c(1:9, ""total"", ""linear"",""ratio"")\r\ncolnames(kinetic_procrustes) <- c(""dist"",species_table$video_id)\r\n# kinetic_procrustes\t# rows are distances, columns are specimens\r\nkinetic_procrustes2 = as.data.frame(t(kinetic_procrustes[,2:(n_specimen+1)])) # rows are specimens, columns are distances\r\n\r\nkinetic_procrustes_total= as.data.frame(kinetic_procrustes2[,10])\r\nefficiency_ratio_total <- as.data.frame(kinetic_procrustes2[,12])\r\n\r\nrequire(ggplot2)\r\n\r\nl= density(kinetic_procrustes_total[,1])\r\nt=ggplot(kinetic_procrustes_total, aes(x=kinetic_procrustes_total[,1])) + geom_density(size=1.2, adjust=1.2) + labs(x=""kinesis"",y=""density"") + xlim(range(l$x)) + theme_classic(base_size=14)\r\nt\r\n\r\n\r\nindiv_kinesis <- as.vector(kinetic_procrustes2[,10])\r\n\r\n\r\n#check dfs order\r\nkin_order_check <- data.frame(species_table$species,rownames(kinetic_procrustes2))\r\n\r\n#<---------------- MAKE DATA FRAME ---------------->#\r\n\r\n\r\nd']",4,"evolution, trade-offs, multifunctionality, biomechanical systems, feeding mechanics, morphology, fishes, suction feeders, biting behaviors, prey capture, diversification, kinematic traits, constraint, mechanical trade-offs, performance, head shape,"
Competition-colonization dynamics and multimodality in diversity-disturbance relationships,"Diversity_disturbance.R  R code for diversity-disturbance relationships, including all simulation cases in the main text.Diversity_disturbance.R  R code for diversity-disturbance relationships, including all simulation cases in the main text.","['library(deSolve) # For solving differential equations\r\nlibrary(tidyverse) # For efficient data manipulation and plotting\r\nlibrary(rcartocolor) # Colorblind-friendly color palette\r\ntheme_set(theme_bw()) # Set basic plotting theme\r\ntheme_update(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\r\n\r\n\r\n# Evaluate the right-hand side of the model equations\r\n# Input:\r\n# - time: Moment to evaluate right-hand side at\r\n# - p: Vector of state variables (here: site occupancies)\r\n# - pars: A list of model parameters, with the following entries:\r\n#     $c: Vector of colonization rates\r\n#     $e: Vector of mortality rates (or single mortality, if equal for all species)\r\n#     $D: The extent of the disturbance (i.e., fraction of density removed)\r\n#     $period: Disturbance period\r\n#     $H: Competitive matrix; H[i,j] = prob. that individual of species i displaces j\r\n# Output:\r\n# - The vector of right-hand sides, coerced into a list\r\nmodelEqs <- function(time, p, pars) {\r\n  recruitment <- pars$c * p * (1 - sum(p)) # c_i * p_i * (1 - sum_j p_j)\r\n  # Mortality: e_i * p_i plus forcing term, -log(1 - D) / period, which\r\n  # ensures that p_i drops to (1 - D) * p_i over one period\r\n  mortality <- pars$e * p - (p * log(1 - pars$D) / pars$period)\r\n  # Displacement term:\r\n  displacement <- as.numeric(diag(pars$c * p) %*% (pars$H %*% p) -\r\n                               diag(p) %*% (t(pars$H) %*% (pars$c * p)))\r\n  return(list(recruitment - mortality + displacement))\r\n}\r\n\r\n\r\n# Integrate the model equations and return solution in a tidy table\r\n# Input:\r\n# - pars: A list with the following entries:\r\n#     $n: Number of species\r\n#     $D: Extent of disturbance (fraction of density removed)\r\n#     $period: Disturbance period\r\n#     $c: Vector of colonization rates\r\n#     $e: Vector of mortality rates (or single mortality, if equal for all species)\r\n#     $H: Competitive matrix; H[i,j] = prob. that individual of species i displaces j\r\n#     $tmax: Integration time\r\n# Output:\r\n# - A tidy data frame with three columns: time, species, and p (site occupancy)\r\nrunModel <- function(pars) {\r\n  ic <- rep(1 / pars$n, pars$n) # Initial conditions\r\n  tseq <- seq(0, pars$tmax, by = pars$tstep) # Sampling points in time\r\n  ode(func = modelEqs, y = ic, times = tseq, parms = pars, method = ""bdf"") %>%\r\n    as.data.frame() %>% # Convert solution to data frame (needed for next step)\r\n    as_tibble() %>% # Convert solution to tibble\r\n    pivot_longer(cols = 2:ncol(.), names_to = ""species"", values_to = ""p"") # Tidy up data\r\n}\r\n\r\n\r\n# Obtain the matrix H for n species\r\n# Input:\r\n# - n: The number of species (so H will be an n-by-n matrix)\r\n# - rseed: Random seed (for reproducibility of randomly generated matrix entries)\r\n# - u: Limit for upper-triangular entries (which go between 1-u and 1)\r\n# - l: Limit for lower-triangular entries (which go between 0 and l)\r\n# Output:\r\n# - An n-by-n matrix whose (i,j)th entry is the probability that an individual of\r\n#   species i displaces an individual of species j in competition for a site\r\nHmat <- function(n, rseed, u, l) {\r\n  set.seed(rseed) # Set random seed\r\n  H <- matrix(0, n, n) # Initialize H matrix to 0\r\n  H[upper.tri(H)] <- runif(choose(n, 2), 1-u, 1) # Upper triangular entries from [1-u, 1]\r\n  H[lower.tri(H)] <- runif(choose(n, 2), 0, l) # Lower triangular entries from [0, l]\r\n  return(H)\r\n}\r\n\r\n\r\n# Uniformly drawn colonization rates, sorted in increasing order\r\n# Input:\r\n# - rseed: Random seed (for reproducibility)\r\n# - n: The number of species (so the function returns a vector with n entries)\r\n# - clower: Lower limit for randomly drawn colonization rates\r\n# - cupper: Upper limit for randomly drawn colonization rates\r\n# Output:\r\n# - A vector with n entries, sorted in increasing order but otherwise randomly and\r\n#   independently drawn from the interval [clower, cupper].\r\nrandomColon <- function(rseed, n, clower, cupper) {\r\n  set.seed(rseed) # Set random seed\r\n  list(sort(runif(n, clower, cupper))) # Return sorted random colonization rates\r\n}\r\n\r\n\r\n# Create table of a factorial numerical experiment; each row contains the parameters\r\n# for one model run\r\n# Input:\r\n# - n: Number of species\r\n# - D: Extent of disturbance (fraction of density removed)\r\n# - period: Disturbance period\r\n# - tmax: Number of time units to integrate equations for\r\n# - tstep: Time step for ODE integration output\r\n# - clower: Lower limit for colonization rates\r\n# - cupper: Upper limit for colonization rates\r\n# - e: Vector of mortality rates (or single mortality, if equal for all species)\r\n# - rseed: Random seed (for reproducibility of randomly generated values)\r\n# - randc: TRUE if colonization rates are randomized; FALSE if they are evenly spaced\r\n# - u: Limit for upper-triangular entries of H (which will go between 1-u and 1)\r\n# - l: Limit for lower-triangular entries of H (which will go between 0 and l)\r\n# Output:\r\n# - A tibble with the parameters of a single numerical scenario in each row\r\nparamTable <- function(n = 3:6, ']",4,"competition, colonization, dynamics, multimodality, diversity, disturbance, R code, simulation cases, main text."
R and Python code for Science Translational Medicine Manuscript abl5849,"R CodeRand_Alloc_Peak_Templates.r, script for performing random allocation of subjects to Discovery or Replication sets.Regression_Models_Main.r, script for fitting Elastic net, LASSO and Ridge models on main GcGc matrix. Script also performs batch effect adjustment.Regression_Models_Air_Supply_Room_Air.r, script for fitting Elastic net, LASSO and Ridge models on GcGc matrix for air supply and room air.Graphs_by_Group.r, script for generating graphs for each disease group based on correlation matrix for 101 features. Script is also used to perform Louvain clustering on the graphs.GSVA_models.r, script for generating graph for all disease groups (healthy excluded) ,discovery and replication combined. Graph based on correlation matrix for 101 features. Script is also used to perform Louvain clustering on the graph and feature enrichment.Calculate_Scores.r, script for calculating example biomarker scores. PYTHON CodeEMBER_TDA_Main.py, script for performing TDA using Kepler Mapper on GcGc matrix.","['setwd(""~/EMBER/Graph_Experiments"")\ndd<-read.table(file=""Discovery_TDA_Features_Rows.csv"",header=TRUE,sep="","")\n\nlibrary(igraph)\nlibrary(lsa)\nlibrary(WGCNA)\nlibrary(dunn.test)\nlibrary(netcom)\nlibrary(bluster)\nd_cut<-0.5\n\n#All data Graph\ndd_ALL<-dd\ndd_c<-t(dd_ALL)\ndd_c<-data.frame(dd_c)\nnames(dd_c)<-dd_ALL$Feature\ndd_c<-dd_c[-nrow(dd_c),]\ndd_c<-apply(dd_c,2,as.numeric)\nd<-1-abs(cor(dd_c))\nd[d>d_cut]<-0\nALL_g<-graph.adjacency(d,mode=""undirected"",weighted=TRUE,diag=FALSE)\n\n\ndd_ALL<-dd\ndd_c<-t(dd_ALL)\ndd_c<-data.frame(dd_c)\nnames(dd_c)<-dd_ALL$Feature\ndd_c<-dd_c[-nrow(dd_c),]\ndd_c<-apply(dd_c,2,as.numeric)\npickSoftThreshold(dd_c,dataIsExpr = TRUE)\nd<-abs(cor(dd_c))^4\nTOM_sim<-TOMsimilarity(d)\ncolnames(TOM_sim)<-colnames(d)\nrownames(TOM_sim)<-rownames(d)\t\n#ALL_g<-graph.adjacency(d,mode=""undirected"",weighted=TRUE,diag=FALSE)\nALL_g<-graph.adjacency(TOM_sim,mode=""undirected"",weighted=TRUE,diag=FALSE)\nplot(ALL_g, edge.label=round(E(ALL_g)$weight, 3))\n\nALL_g_clust<-cluster_louvain(ALL_g,weights = edge_attr(ALL_g)$weight)\nALL_g_clust<-cluster_leading_eigen(ALL_g,weights = edge_attr(ALL_g)$weight)\nALL_g_clust<-cluster_optimal(ALL_g,weights = edge_attr(ALL_g)$weight)\nALL_g_clust<-cluster_fast_greedy(ALL_g,weights = edge_attr(ALL_g)$weight)\nplot(ALL_g_clust,ALL_g,mark.groups=NULL,main=""ALL"")\ncommunities(ALL_g_clust)\n\ndd_ALL_t<-t(dd_ALL)\ndd_ALL_t<-data.frame(dd_ALL_t)\nnames(dd_ALL_t)<-dd_ALL_t[nrow(dd_ALL_t),]\ndd_ALL_t<-dd_ALL_t[-nrow(dd_ALL_t),]\n\nGrp<-c()\nfor(i in 1:nrow(dd_ALL_t)){\nGrp[i]<-paste(gsub(substr(rownames(dd_ALL_t)[i],1,8),"""",rownames(dd_ALL_t)[i]))\n}\n\ndd_ALL_t<-data.frame(apply(dd_ALL_t,2,as.numeric))\n\neigen_features<-list()\nfor(i in 1:length(communities(ALL_g_clust))){\nif(length(communities(ALL_g_clust)[[i]]) >= 2)\t{eigen_features[[i]]<-list(prcomp(dd_ALL_t[,names(dd_ALL_t) %in% communities(ALL_g_clust)[[i]]],center=TRUE,scale.=FALSE)$x[,1],length(communities(ALL_g_clust)[[i]]))}\nif(length(communities(ALL_g_clust)[[i]]) < 2)\t{eigen_features[[i]]<-list(dd_ALL_t[,names(dd_ALL_t) %in% communities(ALL_g_clust)[[i]]],length(communities(ALL_g_clust)[[i]]))}   \n}\n\ndd_eigen_features<-NULL\nfor(i in 1:length(communities(ALL_g_clust))){\ndd_eigen_features<-cbind(dd_eigen_features,eigen_features[[i]][[1]])\n}\t\ndd_eigen_features<-data.frame(dd_eigen_features)\ndd_eigen_features$Grp<-Grp\n\np_vals<-apply(dd_eigen_features[,-ncol(dd_eigen_features)], 2, function(x) kruskal.test(x,dd_eigen_features[,ncol(dd_eigen_features)])$p.value)\nwhich(p_vals<0.05)\n\t\t\t  \nfor(i in which(p_vals<0.05)){\nprint(communities(ALL_g_clust)[[i]])\n}\nfor(i in which(p_vals<0.05)){\t\t\t  \ndunn.test(x=dd_eigen_features[,i], g= dd_eigen_features[,ncol(dd_eigen_features)], method = \'bonferroni\', altp = TRUE)\n}\n\t\t\t  \ndd_feature_red<-data.frame(dd_eigen_features[,3],dd_eigen_features$Grp)\nnames(dd_feature_red)<-c(""Eigen_Feature"",""Group"")\t\nlibrary(ggpubr)\t\t\t  \nggviolin(dd_feature_red, x = ""Group"",\n          y = c(""Eigen_Feature""),\n          combine = TRUE, \n          color = ""Group"", palette = ""jco"",\n          ylab = ""Eigen Feature Score"", \n          add = ""median_iqr"")\nlibrary(pROC)\nsc1<-roc(as.numeric(dd_eigen_features$Grp==""Healthy""),dd_eigen_features[,3])\nplot(sc1,print.auc=TRUE)\n\t\t\t  \nlibrary(TDAstats)\neigen_phom<-calculate_homology(dd_eigen_features[,-ncol(dd_eigen_features)])\t\t\t  \nplot_barcode(eigen_phom)\t\t\t  \nplot_persist(eigen_phom)\t\n\n\t\t  \n#By Group Graphs\ndd_H<-dd[,c(grep(""Healthy"",names(dd)),140)]\ndd_c<-t(dd_H)\ndd_c<-data.frame(dd_c)\nnames(dd_c)<-dd_H$Feature\ndd_c<-dd_c[-nrow(dd_c),]\ndd_c<-apply(dd_c,2,as.numeric)\nd<-1-abs(cor(dd_c))\nd[d>d_cut]<-0\nTOM_sim<-TOMsimilarity(d)\ncolnames(TOM_sim)<-colnames(d)\nrownames(TOM_sim)<-rownames(d)\t\t\t  \n#H_g<-graph.adjacency(d,mode=""undirected"",weighted=TRUE,diag=FALSE)\nH_g<-graph.adjacency(TOM_sim,mode=""undirected"",weighted=TRUE,diag=FALSE)\ncoords_H_g<-layout.fruchterman.reingold(H_g)\t\t  \n\ndd_C<-dd[,c(grep(""COPD"",names(dd)),140)]\ndd_c<-t(dd_C)\ndd_c<-data.frame(dd_c)\nnames(dd_c)<-dd_C$Feature\ndd_c<-dd_c[-nrow(dd_c),]\ndd_c<-apply(dd_c,2,as.numeric)\nd<-1-abs(cor(dd_c))\nd[d>d_cut]<-0\nTOM_sim<-TOMsimilarity(d)\ncolnames(TOM_sim)<-colnames(d)\nrownames(TOM_sim)<-rownames(d)\t\t\t  \n#C_g<-graph.adjacency(d,mode=""undirected"",weighted=TRUE,diag=FALSE)\nC_g<-graph.adjacency(TOM_sim,mode=""undirected"",weighted=TRUE,diag=FALSE)\t\t\t  \ncoords_C_g<-layout.fruchterman.reingold(C_g)\n\t\t\t  \ndd_A<-dd[,c(grep(""Asthma"",names(dd)),140)]\t  \ndd_c<-t(dd_A)\ndd_c<-data.frame(dd_c)\nnames(dd_c)<-dd_A$Feature\ndd_c<-dd_c[-nrow(dd_c),]\ndd_c<-apply(dd_c,2,as.numeric)\nd<-1-abs(cor(dd_c))\nd[d>d_cut]<-0\nTOM_sim<-TOMsimilarity(d)\ncolnames(TOM_sim)<-colnames(d)\nrownames(TOM_sim)<-rownames(d)\t\t\t  \n#A_g<-graph.adjacency(d,mode=""undirected"",weighted=TRUE,diag=FALSE)\nA_g<-graph.adjacency(TOM_sim,mode=""undirected"",weighted=TRUE,diag=FALSE)\ncoords_A_g<-layout.fruchterman.reingold(A_g)\t\t\t  \n\ndd_P<-dd[,c(grep(""Pneumonia"",names(dd)),140)]\ndd_c<-t(dd_P)\ndd_c<-data.frame(dd_c)\nnames(dd_c)<-dd_P$Feature\ndd_c<-dd_c[-nrow(dd_c),]\ndd_c<-apply(dd_c,2,as.numeric)\nd<-1-abs(cor(dd_', 'setwd(""~/EMBER/Reintegrated_Data_Nov2020"")\ndd_clinical<-read.table(file=""FINAL_DATASET.csv"",header=TRUE,sep="","")\nfor(i in 1:nrow(dd_clinical)){\nif(dd_clinical$Diagnosis[i]==1) {dd_clinical$Grp[i]<-""Healthy""}\nelse if(dd_clinical$Diagnosis[i]==2) {dd_clinical$Grp[i]<-""Acute Asthma""}\nelse if(dd_clinical$Diagnosis[i]==3) {dd_clinical$Grp[i]<-""Acute COPD""}\nelse if(dd_clinical$Diagnosis[i]==4) {dd_clinical$Grp[i]<-""HF""}\nelse if(dd_clinical$Diagnosis[i]==5) {dd_clinical$Grp[i]<-""Pneumonia""}\n}\nfor(i in 1:nrow(dd_clinical)){\t\nif(is.na(dd_clinical$V1.BNP[i])) {dd_clinical$V1.BNP[i]<-median(dd_clinical$V1.BNP,na.rm=TRUE)}\t\nif(!is.na(dd_clinical$V1.BNP[i])) {dd_clinical$V1.BNP[i]<-dd_clinical$V1.BNP[i]}\nif(is.na(dd_clinical$V1.CRP[i])) {dd_clinical$V1.CRP[i]<-median(dd_clinical$V1.CRP,na.rm=TRUE)}\t\nif(!is.na(dd_clinical$V1.CRP[i])) {dd_clinical$V1.CRP[i]<-dd_clinical$V1.CRP[i]}\t\nif(is.na(dd_clinical$V1.Eos.count[i])) {dd_clinical$V1.Eos.count[i]<-median(dd_clinical$V1.Eos.count,na.rm=TRUE)}\t\nif(!is.na(dd_clinical$V1.Eos.count[i])) {dd_clinical$V1.Eos.count[i]<-dd_clinical$V1.Eos.count[i]}\t\n}\n\t\nfor(i in 1:nrow(dd_clinical)){\t\nif(is.na(dd_clinical$V1.BNP[i])) {dd_clinical$BNP_Grp[i]<-NA}\t\nif(dd_clinical$V1.BNP[i]>=500 & !is.na(dd_clinical$V1.BNP[i])) {dd_clinical$BNP_Grp[i]<-""High""}\nif(dd_clinical$V1.BNP[i]<500 & !is.na(dd_clinical$V1.BNP[i])) {dd_clinical$BNP_Grp[i]<-""Low""}\t\nif(is.na(dd_clinical$V1.CRP[i])) {dd_clinical$CRP_Grp[i]<-NA}\t\nif(dd_clinical$V1.CRP[i]>=50 & !is.na(dd_clinical$V1.CRP[i])) {dd_clinical$CRP_Grp[i]<-""High""}\nif(dd_clinical$V1.CRP[i]<50 & !is.na(dd_clinical$V1.CRP[i])) {dd_clinical$CRP_Grp[i]<-""Low""}\nif(is.na(dd_clinical$V1.Eos.count[i])) {dd_clinical$EOS_Grp[i]<-NA}\t\nif(dd_clinical$V1.Eos.count[i]>=0.3 & !is.na(dd_clinical$V1.Eos.count[i])) {dd_clinical$EOS_Grp[i]<-""High""}\nif(dd_clinical$V1.Eos.count[i]<0.3 & !is.na(dd_clinical$V1.Eos.count[i])) {dd_clinical$EOS_Grp[i]<-""Low""}\t\t\n}\n\ndd_clinical_grps<-data.frame(dd_clinical$Sample_ID,dd_clinical$BNP_Grp,dd_clinical$CRP_Grp,dd_clinical$EOS_Grp,dd_clinical$Grp)\nnames(dd_clinical_grps)<-c(""Sample_ID"",""BNP_Grp"",""CRP_Grp"",""EOS_Grp"",""Grp"")\n\t\n\nsetwd(""~/EMBER/Graph_Experiments"")\ndd_disc<-read.table(file=""Discovery_TDA_Features_Rows.csv"",header=TRUE,sep="","")\ndd_rep<-read.table(file=""Replication_TDA_Features_Rows.csv"",header=TRUE,sep="","")\ndd_tmp<-rbind(data.frame(t(dd_disc)),data.frame(t(dd_rep)))\ndd<-t(dd_tmp)\ndd<-data.frame(dd)\ndd<-dd[,-grep(""Healthy"",names(dd))]\n\n\nlibrary(igraph)\nlibrary(lsa)\nlibrary(WGCNA)\nlibrary(dunn.test)\nlibrary(netcom)\nlibrary(bluster)\n\n\n#All data Graph\ndd_ALL<-dd\ndd_c<-t(dd_ALL)\ndd_c<-data.frame(dd_c)\nnames(dd_c)<-dd_ALL$Feature\ndd_c<-dd_c[-nrow(dd_c),]\ndd_c<-apply(dd_c,2,as.numeric)\nd<-abs(cor(dd_c))\nd_cut<-0.5\nd[d>d_cut]<-0\nALL_g<-graph.adjacency(d,mode=""undirected"",weighted=TRUE,diag=FALSE)\nplot(ALL_g, edge.label=round(E(ALL_g)$weight, 3))\nALL_g_clust<-cluster_louvain(ALL_g,weights = edge_attr(ALL_g)$weight)\nplot(ALL_g_clust,ALL_g,mark.groups=NULL,main=""ALL"")\ncommunities(ALL_g_clust)\n\n#Unsigned\ndd_ALL<-dd\ndd_c<-t(dd_ALL)\ndd_c<-data.frame(dd_c)\nnames(dd_c)<-dd_ALL$Feature\ndd_c<-dd_c[-113,]\ndd_c<-dd_c[-nrow(dd_c),]\t\ndd_c<-apply(dd_c,2,as.numeric)\npickSoftThreshold(dd_c,dataIsExpr = TRUE)\n#Beta = 5 for Disc\nbeta_pow<-5\n#Beta = 7 for Rep\n#beta_pow<-7\nd<-abs(cor(dd_c))^beta_pow\nk=as.vector(apply(d,2,sum, na.rm=T))\nscaleFreePlot(k, main=""Check scale free topology\\n"")\n#d<-abs((1+cor(dd_c))/2)^3\n#TOM_sim<-TOMsimilarity(d)\n#colnames(TOM_sim)<-colnames(d)\n#rownames(TOM_sim)<-rownames(d)\t\nALL_g<-graph.adjacency(d,mode=""undirected"",weighted=TRUE,diag=FALSE)\n#ALL_g<-graph.adjacency(TOM_sim,mode=""undirected"",weighted=TRUE,diag=FALSE)\nplot(ALL_g, edge.label=round(E(ALL_g)$weight, 3))\n\n\nALL_g_clust<-cluster_louvain(ALL_g,weights = edge_attr(ALL_g)$weight)\nplot(ALL_g_clust,ALL_g,mark.groups=NULL,main=""ALL"")\n\ntiff(filename = ""Disc_Rep_Graph_Louvain.tiff"" ,units=""in"", width=13, height=9, res=400,compression = ""lzw"")\t\nplot(ALL_g_clust,ALL_g,vertex.label="""",mark.groups=NULL,main=""Discovery and Replication, 101 Features, Graph with Louvain Clusters"")\ndev.off()\ncommunities(ALL_g_clust)\n\n\t\n\t\n\t\n#Signed\ndd_ALL<-dd\ndd_c<-t(dd_ALL)\ndd_c<-data.frame(dd_c)\nnames(dd_c)<-dd_ALL$Feature\ndd_c<-dd_c[-nrow(dd_c),]\ndd_c<-apply(dd_c,2,as.numeric)\npickSoftThreshold(dd_c,dataIsExpr = TRUE, networkType = ""signed"")\n#d<-abs(cor(dd_c))^3\nd<-abs((1+cor(dd_c))/2)^3\n#TOM_sim<-TOMsimilarity(d)\n#colnames(TOM_sim)<-colnames(d)\n#rownames(TOM_sim)<-rownames(d)\t\nALL_g<-graph.adjacency(d,mode=""undirected"",weighted=TRUE,diag=FALSE)\n#ALL_g<-graph.adjacency(TOM_sim,mode=""undirected"",weighted=TRUE,diag=FALSE)\nplot(ALL_g, edge.label=round(E(ALL_g)$weight, 3))\n\nALL_g_clust<-cluster_louvain(ALL_g,weights = edge_attr(ALL_g)$weight)\nplot(ALL_g_clust,ALL_g,mark.groups=NULL,main=""ALL"")\ncommunities(ALL_g_clust)\n\n\n\n\n\nlibrary(rlist)\nfeatureSets<-list()\nfor(i in 1:length(communities(ALL_g_clust))){\nfeatureSets<-list.append(featureSets,communities(ALL_g_clust)[[i]])\t\n}\t\n#names(featureSets)<-c(""Set1""', 'setwd(""~/EMBER_GcGc_Data_Nov_2019/Final_Clinical_Data_and_Peak_Table_April2020"")\r\ndd<-read.table(file=""Pat_ID_Group.csv"",header=TRUE,sep="","")\r\nlibrary(randomizr)\r\nZ <- complete_ra(N = nrow(dd))\r\ntable(Z)\r\ndd$Grp_Aloc<-Z\r\n\r\nblocks<-paste(dd$Grp_Aloc,dd$Diagnosis)\r\nZ <- block_ra(blocks = blocks, prob_each =  c(0.4693141, 1-0.4693141))\r\ndd$alloc_ind<-as.numeric(Z==0)\r\nfor(i in 1:nrow(dd)){\r\nif(dd$alloc_ind[i]==0){dd$Data_Set[i]<-""Non-Template""}\r\nelse if(dd$alloc_ind[i]==1){dd$Data_Set[i]<-""Template""}\r\n}\r\ntable(dd$Data_Set)\r\ntable(dd$Data_Set,dd$Final_adjudicated_diagnosis)\r\ntable(dd$Final_adjudicated_diagnosis)/277\r\nprop.table(table(dd$Data_Set,dd$Final_adjudicated_diagnosis),margin=1)\r\ndd_template<-subset(dd,dd$Data_Set==""Template"")\r\ndd_nontemplate<-subset(dd,dd$Data_Set==""Non-Template"")\r\n#intersect should be 0\r\nintersect(as.character(dd_template$Study.number),as.character(dd_nontemplate$Study.number))\r\ntable(dd_template$Final_adjudicated_diagnosis)\r\ntable(dd_nontemplate$Final_adjudicated_diagnosis)\r\n\r\nwrite.table(dd_template,file=""Subjects_for_Template_n130.csv"",row.names=FALSE,sep="","")\r\n\r\n', '#module load gcc/6.3\n#module load R/4.0.0\nlibrary(data.table)\n\n\n\nsetwd(""/lustre/ahome3/m/mr251/EMBER/Reintegrated_Data_Nov2020/RA_Air_Supply_Feb2021"")\ndd_n277<-read.table(file=""AS_peak table based on 277 template for Matt with 9 reintegrated features.csv"",header=TRUE,sep="","")\ndd_n277<-read.table(file=""RA_peak table based on 277 template for Matt with 9 reintegrated features.csv"",header=TRUE,sep="","")\n\n\n\ndd_gcgc_breath_feature<-dd_n277[,-c(1:4)]\ndd_gcgc_breath_feature<-data.frame(dd_gcgc_breath_feature)\nnon_zero<-function(x){\n(sum(as.numeric(x !=0),na.rm=TRUE)/length(x))*100\n}\n\n\nfor(i in 1:nrow(dd_n277)){\nif(as.numeric(substr(as.character(dd_n277$Date_Tray[i]),1,6)) >=170830 & as.numeric(substr(as.character(dd_n277$Date_Tray[i]),1,6)) <=171019) {dd_n277$Batch_ID[i]<-1}\nif(as.numeric(substr(as.character(dd_n277$Date_Tray[i]),1,6)) >=171109 & as.numeric(substr(as.character(dd_n277$Date_Tray[i]),1,6)) <=180328) {dd_n277$Batch_ID[i]<-2}\nif(as.numeric(substr(as.character(dd_n277$Date_Tray[i]),1,6)) >=180412 & as.numeric(substr(as.character(dd_n277$Date_Tray[i]),1,6)) <=181203) {dd_n277$Batch_ID[i]<-3}\n}\ndd_batch_effects<-read.table(file=""~/EMBER/Final_Clinical_Data_and_Peak_Table_April2020/Peak_Tables_April_2020/Peak_tables_from_templates/Batch_Effects.csv"",header=TRUE,sep="","")\ndd_clinical<-read.table(file=""~/EMBER/Reintegrated_Data_Nov2020/FINAL_DATASET.csv"",header=TRUE,sep="","")\nnames(dd_clinical)[1]<-""Study_number""\ntbl_gcgc<-setDT(dd_n277)\ntbl_clinical<-setDT(dd_clinical)\nsetkey(tbl_gcgc,Patient.ID)\nsetkey(tbl_clinical,Study_number)\ntbl_gcgc_clinical <-tbl_gcgc[tbl_clinical, nomatch=0]\ntbl_batch<-setDT(dd_batch_effects)\nsetkey(tbl_batch,Patient.ID)\n\ntbl_gcgc_clinical_batch <-tbl_gcgc_clinical[tbl_batch, nomatch=0]\ndd_gcgc_clinical_batch<-data.frame(tbl_gcgc_clinical_batch)\n\nfor(i in 1:nrow(dd_gcgc_clinical_batch)){\nif(dd_gcgc_clinical_batch$Diagnosis[i]==1) {dd_gcgc_clinical_batch$Grp[i]<-""Healthy""}\nelse if(dd_gcgc_clinical_batch$Diagnosis[i]==2) {dd_gcgc_clinical_batch$Grp[i]<-""Acute Asthma""}\nelse if(dd_gcgc_clinical_batch$Diagnosis[i]==3) {dd_gcgc_clinical_batch$Grp[i]<-""Acute COPD""}\nelse if(dd_gcgc_clinical_batch$Diagnosis[i]==4) {dd_gcgc_clinical_batch$Grp[i]<-""HF""}\nelse if(dd_gcgc_clinical_batch$Diagnosis[i]==5) {dd_gcgc_clinical_batch$Grp[i]<-""Pneumonia""}\n}\n\ngcgc_feature_counts<-apply(dd_gcgc_breath_feature,2,non_zero)\ngcgc_feature_counts<-data.frame(gcgc_feature_counts)\nnames(gcgc_feature_counts)<-""Percent_Non_Zero""\nsum(as.numeric(gcgc_feature_counts$Percent_Non_Zero) !=0,na.rm=TRUE)\n\nprop_non_zero<-function(p_r){\nsum(as.numeric(gcgc_feature_counts$Percent_Non_Zero) >= p_r,na.rm=TRUE)\n}\n\n\ndd_feature_counts<-data.frame(c(100,98,95,90,85,80,75,70,65,60,55,50),unlist(lapply(c(100,98,95,90,85,80,75,70,65,60,55,50),prop_non_zero)))\nnames(dd_feature_counts)<-c(""Present in Percentage of Samples"",""N"")\ndd_feature_counts$`Present in Percentage of Samples`<-c(""=100"","">=98"","">=95"","">=90"","">=85"","">=80"","">=75"","">=70"","">=65"","">=60"","">=55"","">=50"")\ndd_feature_counts$Percent<-(dd_feature_counts$N/ncol(dd_gcgc_breath_feature))*100\nnames(dd_feature_counts)[2:3]<-c(""Number of Features"",""Percent of Total Features"")\n\nnames(dd_n277)[3]<-""Patient_ID""\ndd_n277<-data.frame(dd_n277)\ndd_gcgc<-dd_n277[,as.character(names(dd_n277)) %in% c(""Patient_ID"",rownames(gcgc_feature_counts)[which(gcgc_feature_counts$Percent_Non_Zero >80)])]\ndd_gcgc<-data.frame(dd_gcgc[,-1])\ndd_gcgc<-apply(dd_gcgc,2,log1p)\ndd_gcgc<-data.frame(dd_gcgc)\n\n\n\nlibrary(limma)\nlibrary(mixOmics)\nlibrary(PMA)\nlibrary(sva)\nlibrary(pamr)\n#Un-processed Data\nedata<-t(dd_gcgc)\n#pheno<-data.frame(as.character(dd_n277$Patient_ID),as.factor(dd_n277$Batch_ID))\npheno<-data.frame(as.character(dd_gcgc_clinical_batch$Patient.ID),as.factor(dd_gcgc_clinical_batch$Batch_ID),as.factor(dd_gcgc_clinical_batch$Operator),as.factor(dd_gcgc_clinical_batch$Time.Collected),as.factor(dd_gcgc_clinical_batch$Time.Stored.Wet),as.factor(dd_gcgc_clinical_batch$Time.Stored.Dry),as.factor(dd_gcgc_clinical_batch$Collection.volume),as.factor(dd_gcgc_clinical_batch$Diagnosis))\nnames(pheno)<-c(""Sample_ID"",""Batch_ID"",""Operator"",""Time.Collected"",""Time.Stored.Wet"",""Time.Stored.Dry"",""Collection.volume"",""Diagnosis"")\nmod = model.matrix(~as.factor(Diagnosis), data=pheno)\nmod0 = model.matrix(~1,data=pheno)\nn.sv = num.sv(edata,mod,method=""be"")\nsvobj = sva(edata,mod,mod0,n.sv=n.sv)\npValues = f.pvalue(edata,mod,mod0)\nqValues = p.adjust(pValues,method=""BH"")\nsum(as.numeric(qValues < 0.05))\nmodSv = cbind(mod,svobj$sv)\nmod0Sv = cbind(mod0,svobj$sv)\npValuesSv = f.pvalue(edata,modSv,mod0Sv)\nqValuesSv = p.adjust(pValuesSv,method=""BH"")\nsum(as.numeric(qValuesSv < 0.05))\ndd_Sig_Features_latent<-data.frame(qValuesSv[qValuesSv<0.05])\ndd_Sig_Features_no_latent<-data.frame(qValuesSv[qValues<0.05])\n\n#Adjust for Batch Effect using ComBat\n#Parametric empirical Bayesian adjustment\n\nbatch = pheno$Batch_ID\nmodcombat = model.matrix(~1, data=pheno)\ncombat_edata = ComBat(dat=edata, batch=batch, mod=modcombat, par.prior=TRUE, prior.plots=TRUE)\n', '#module load gcc/6.3\n#module load R/4.0.0\nlibrary(data.table)\n\n\n#Main data set REINTEGRATED\nsetwd(""/lustre/ahome3/m/mr251/EMBER/Reintegrated_Data_Nov2020"")\ndd_n277<-read.table(file=""templaten277_peaktable277_vol1p4_SNR30_FILTERED_TO_278_FEATURES_with_9_FEATURES_REINTEGRATED.csv"",header=TRUE,sep="","")\n\n\n#Intitial Data Set\nsetwd(""/lustre/ahome3/m/mr251/EMBER/Final_Clinical_Data_and_Peak_Table_April2020/Peak_Tables_April_2020/Peak_tables_from_templates"")\ndd_n277<-read.table(file=""templaten277_peaktable277_vol1p4_SNR30.csv"",header=TRUE,sep="","")\n\ndd_gcgc_breath_feature<-dd_n277[,-c(1:4)]\ndd_gcgc_breath_feature<-data.frame(dd_gcgc_breath_feature)\nnon_zero<-function(x){\n(sum(as.numeric(x !=0),na.rm=TRUE)/length(x))*100\n}\n\n\nfor(i in 1:nrow(dd_n277)){\nif(as.numeric(substr(as.character(dd_n277$Date_Tray[i]),1,6)) >=170830 & as.numeric(substr(as.character(dd_n277$Date_Tray[i]),1,6)) <=171019) {dd_n277$Batch_ID[i]<-1}\nif(as.numeric(substr(as.character(dd_n277$Date_Tray[i]),1,6)) >=171109 & as.numeric(substr(as.character(dd_n277$Date_Tray[i]),1,6)) <=180328) {dd_n277$Batch_ID[i]<-2}\nif(as.numeric(substr(as.character(dd_n277$Date_Tray[i]),1,6)) >=180412 & as.numeric(substr(as.character(dd_n277$Date_Tray[i]),1,6)) <=181203) {dd_n277$Batch_ID[i]<-3}\n}\ndd_batch_effects<-read.table(file=""~/EMBER/Final_Clinical_Data_and_Peak_Table_April2020/Peak_Tables_April_2020/Peak_tables_from_templates/Batch_Effects.csv"",header=TRUE,sep="","")\ndd_clinical<-read.table(file=""~/EMBER/Reintegrated_Data_Nov2020/FINAL_DATASET.csv"",header=TRUE,sep="","")\nnames(dd_clinical)[1]<-""Study_number""\ntbl_gcgc<-setDT(dd_n277)\ntbl_clinical<-setDT(dd_clinical)\nsetkey(tbl_gcgc,Patient.ID)\nsetkey(tbl_clinical,Study_number)\ntbl_gcgc_clinical <-tbl_gcgc[tbl_clinical, nomatch=0]\ntbl_batch<-setDT(dd_batch_effects)\nsetkey(tbl_batch,Patient.ID)\n\ntbl_gcgc_clinical_batch <-tbl_gcgc_clinical[tbl_batch, nomatch=0]\ndd_gcgc_clinical_batch<-data.frame(tbl_gcgc_clinical_batch)\n\nfor(i in 1:nrow(dd_gcgc_clinical_batch)){\nif(dd_gcgc_clinical_batch$Diagnosis[i]==1) {dd_gcgc_clinical_batch$Grp[i]<-""Healthy""}\nelse if(dd_gcgc_clinical_batch$Diagnosis[i]==2) {dd_gcgc_clinical_batch$Grp[i]<-""Acute Asthma""}\nelse if(dd_gcgc_clinical_batch$Diagnosis[i]==3) {dd_gcgc_clinical_batch$Grp[i]<-""Acute COPD""}\nelse if(dd_gcgc_clinical_batch$Diagnosis[i]==4) {dd_gcgc_clinical_batch$Grp[i]<-""HF""}\nelse if(dd_gcgc_clinical_batch$Diagnosis[i]==5) {dd_gcgc_clinical_batch$Grp[i]<-""Pneumonia""}\n}\n\ngcgc_feature_counts<-apply(dd_gcgc_breath_feature,2,non_zero)\ngcgc_feature_counts<-data.frame(gcgc_feature_counts)\nnames(gcgc_feature_counts)<-""Percent_Non_Zero""\nsum(as.numeric(gcgc_feature_counts$Percent_Non_Zero) !=0,na.rm=TRUE)\n\nprop_non_zero<-function(p_r){\nsum(as.numeric(gcgc_feature_counts$Percent_Non_Zero) >= p_r,na.rm=TRUE)\n}\n\n\ndd_feature_counts<-data.frame(c(100,98,95,90,85,80,75,70,65,60,55,50),unlist(lapply(c(100,98,95,90,85,80,75,70,65,60,55,50),prop_non_zero)))\nnames(dd_feature_counts)<-c(""Present in Percentage of Samples"",""N"")\ndd_feature_counts$`Present in Percentage of Samples`<-c(""=100"","">=98"","">=95"","">=90"","">=85"","">=80"","">=75"","">=70"","">=65"","">=60"","">=55"","">=50"")\ndd_feature_counts$Percent<-(dd_feature_counts$N/ncol(dd_gcgc_breath_feature))*100\nnames(dd_feature_counts)[2:3]<-c(""Number of Features"",""Percent of Total Features"")\n\nnames(dd_n277)[3]<-""Patient_ID""\ndd_n277<-data.frame(dd_n277)\ndd_gcgc<-dd_n277[,as.character(names(dd_n277)) %in% c(""Patient_ID"",rownames(gcgc_feature_counts)[which(gcgc_feature_counts$Percent_Non_Zero >=2)])]\ndd_gcgc<-data.frame(dd_gcgc[,-1])\ndd_gcgc<-apply(dd_gcgc,2,log1p)\ndd_gcgc<-data.frame(dd_gcgc)\n\n\n\nlibrary(limma)\nlibrary(mixOmics)\nlibrary(PMA)\nlibrary(sva)\nlibrary(pamr)\n#Un-processed Data\nedata<-t(dd_gcgc)\n#pheno<-data.frame(as.character(dd_n277$Patient_ID),as.factor(dd_n277$Batch_ID))\npheno<-data.frame(as.character(dd_gcgc_clinical_batch$Patient.ID),as.factor(dd_gcgc_clinical_batch$Batch_ID),as.factor(dd_gcgc_clinical_batch$Operator),as.factor(dd_gcgc_clinical_batch$Time.Collected),as.factor(dd_gcgc_clinical_batch$Time.Stored.Wet),as.factor(dd_gcgc_clinical_batch$Time.Stored.Dry),as.factor(dd_gcgc_clinical_batch$Collection.volume),as.factor(dd_gcgc_clinical_batch$Diagnosis))\nnames(pheno)<-c(""Sample_ID"",""Batch_ID"",""Operator"",""Time.Collected"",""Time.Stored.Wet"",""Time.Stored.Dry"",""Collection.volume"",""Diagnosis"")\nmod = model.matrix(~as.factor(Diagnosis), data=pheno)\nmod0 = model.matrix(~1,data=pheno)\nn.sv = num.sv(edata,mod,method=""be"")\nsvobj = sva(edata,mod,mod0,n.sv=n.sv)\npValues = f.pvalue(edata,mod,mod0)\nqValues = p.adjust(pValues,method=""BH"")\nsum(as.numeric(qValues < 0.05))\nmodSv = cbind(mod,svobj$sv)\nmod0Sv = cbind(mod0,svobj$sv)\npValuesSv = f.pvalue(edata,modSv,mod0Sv)\nqValuesSv = p.adjust(pValuesSv,method=""BH"")\nsum(as.numeric(qValuesSv < 0.05))\ndd_Sig_Features_latent<-data.frame(qValuesSv[qValuesSv<0.05])\ndd_Sig_Features_no_latent<-data.frame(qValuesSv[qValues<0.05])\n\n#Adjust for Batch Effect using ComBat\n#Parametric empirical Bayesian adjustment\n\nbatch = pheno$Batc']",4,"R programming, Python programming, Science Translational Medicine Manuscript, abl5849R Code, random allocation of subjects, Discovery set, Replication set, regression models, Elastic net, LASSO, Ridge models, batch effect adjustment, air"
Foundations of Legal Data Science (Open Data Impact Award 2022),"AbstractFr den Datensatz 'Corpus of Decisions: International Court of Justice' wurde ich mit dem 3. Platz beim Open Data Impact Award 2022 des Stifterverbands fr die Deutsche Wissenschaft ausgezeichnet. In diesem Vortrag stelle ich mein Projekt Foundations of Legal Data Science vor.Bei dem Projekt Foundations of Legal Data Science geht es um die Schaffung der wissenschaftlichen Grundlagen fr die Analyse juristischer Daten. Dafr habe ich bislang 16 Datenstze open access verffentlicht, Tutorials geschrieben und Wissenschaftskommunikation betrieben. Im Anschluss spreche ich ber meine Motivation, die juristische Daten als Infrastruktur des Rechtstaats auffasst, nicht als Wirtschaftsgut.Im zweiten Teil des Vortrags zeige ich den Impact des Projekts auf. Alle Datenstze zusammen wurden bislang fast 11.000 mal heruntergeladen. Die wissenschaftlichen und praktischen Anwendungen sind breit und in den Folien beispielhaft aufgezhlt. Konkret diskutiere ich die Analyse der Verfahrensdauer vor dem BVerfG und die Strukturanalyse des Brgerlichen Gesetzbuchs. Der Foliensatz schliet mit einer Auswahl an Vertiefungshinweisen. KontextDiese Prsentation war Grundlage fr den von Sen Fobbe am 7. November 2022 gehaltenen Vortrag Foundations of Legal Data Science anlsslich der Verleihung des Open Data Impact Award 2022. VariantenDie Prsentation ist in zwei Varianten verffentlicht:Nur FolienFolien und Notizen (Hinweise, Literatur zur Vertiefung und Quellenangaben) PodcastWas Legal Data Science ist und warum sich alle Jurist:innen damit beschftigen sollten (Talking Legal Tech 2022) VertiefungFobbe, S. (2022). Legal Data Science verstndlich erklrt  Teil I: Was ist das?. Legal Tribune Online. https://www.lto.de/persistent/a_id/48673/Fobbe, S. (2022). Legal Data Science verstndlich erklrt  Teil II: Wie man sie nutzen kann. Legal Tribune Online. https://www.lto.de/persistent/a_id/49024/Fobbe, S. (2021). Juristische Netzwerkdaten fr Einsteiger. RECHTS|EMPIRIE. https://rechtsempirie.de/10.25527/re.2021.11/juristische-netzwerkdaten-fuer-einsteiger/Fobbe, S. (2021). Open Legal Data: Das Fundament des Rechtsstaates. VOTUM, Heft 1, 2126. https://doi.org/10.5281/zenodo.4646696Fobbe, S. (2022). Introducing Twin Corpora of Decisions for the International Court of Justice (ICJ) and the Permanent Court of International Justice (PCIJ). Journal of Empirical Legal Studies, 19(2), 491-524. https://doi.org/10.1111/jels.12313 Source CodeDer Source Code fr das Diagramm auf Folie 8 in der Programmiersprache R ist an dieser Stelle mitverffentlicht. Eine strenge Replikation ist nicht mglich weil die Werte auf Zenodo tglich aktualisiert werden, die Zahlen werden aber nicht geringer ausfallen. ber den PreistrgerSen Fobbbe ist Vlkerrechtler und Legal Data Scientist. Seine Forschungsinteressen liegen im internationalen Menschenrechtsschutz, der Friedensforschung, dem Kulturgterschutz und der maschinellen Analyse juristischer Texte mittels statistischer Methoden.Website: www.seanfobbe.de","['#\'# Load Packages\n\nlibrary(rvest)\nlibrary(ggplot2)\n\n\n\n#\'# Functions\n\n\n#\'## f.repo.content\n\n#\' @param reponame Name of Zenodo repository, e.g. ""sean-fobbe-data"".\n#\'\n#\' @return A data frame of titles and concept DOIs.\n\n\nf.repo.content <- function(reponame){\n    \n    url <- paste0(""https://zenodo.org/oai2d?verb=ListRecords&set=user-"",\n                  reponame,\n                  ""&metadataPrefix=dcat"")\n    \n\n    ## Read XML\n    xml <- read_html(url)\n\n    ## DCAT: Extract Concept DOI Link\n    nodes <- html_nodes(xml, ""isversionof"")\n    url <- html_attr(nodes, ""rdf:resource"")\n\n    ## DCAT: Extract Title\n    nodes <- html_nodes(xml, ""title"")\n    title <- html_text(nodes)\n\n    ## Create Data Frame\n    df <- data.frame(title, url)\n\n    ## Deduplicate via Concept DOIs\n    df <- df[-which(duplicated(df$url)), ]\n    rownames(df) <- 1:nrow(df)\n\n    \n    return(df)\n\n    \n}\n\n\n\n\n\n#\'## f.zenodo.stats\n\n#\' @param url URL of Zenodo entry, e.g. ""https://zenodo.org/record/7051929"". Works on DOIs returned by f.repo.content (see above).\n#\'\n#\' @return A data frame of views and downloads for the specified URL, including the date the statistics were acquired.\n\n\n\nf.zenodo.stats <- function(url){\n    \n    html <- read_html(url)\n    nodes <- html_nodes(html, ""[class=\'stats-data\']"")\n    text <- html_text(nodes, trim = TRUE)\n    \n    temp <- gsub("","", """", text)\n    temp <- as.numeric(temp)\n    view <- temp[1]\n    dl <- temp[2]\n    date <- Sys.Date()\n    \n    df <- data.frame(view,\n                     dl,\n                     date,\n                     url)\n\n    Sys.sleep(1)\n    \n    return(df)\n}\n\n\n\n\n\n\n#\'# Acquire Statistics\n\n\n\n\n#+\n#\'## Data\n\n\n#+\n#\'### Download Repository Contents\n\n#+ data-repo\ndf.fobbe.data <- f.repo.content(""sean-fobbe-data"")\n\n#\'### Download Stats\n\n#+ data-stats\nstats.data.list <- lapply(df.fobbe.data$url, f.zenodo.stats)\nstats.data <- do.call(rbind, stats.data.list)\n\n#\'### Merge Stats\n\ndf.fobbe.data.all <- merge(df.fobbe.data, stats.data)\n\n\n#\'### Add Acronym\n\ndf.fobbe.data.all$acronym <- gsub("".*\\\\((.*)\\\\).*"",\n                                  ""\\\\1"",\n                                  df.fobbe.data.all$title)\n\ndf.fobbe.data.all$acronym <- gsub(""C-DB"", ""C-DBR"", df.fobbe.data.all$acronym)\n\n\nSys.sleep(10)\n\n\n\n\n\n\n#\'## Code\n\n#+\n#\'### Download Repository Contents\n\n#+ code-repo\ndf.fobbe.code <- f.repo.content(""sean-fobbe-code"")\n\n#\'### Download Stats\n\n#+ code-stats\nstats.code.list <- lapply(df.fobbe.code$url, f.zenodo.stats)\nstats.code <- do.call(rbind, stats.code.list)\n\n#\'### Merge Stats\n\ndf.fobbe.code.all <- merge(df.fobbe.code, stats.code)\n\n\n\n#\'### Add Acronym\n\ndf.fobbe.code.all$acronym <- gsub("".*\\\\((.*)\\\\).*"",\n                                  ""\\\\1"",\n                                  df.fobbe.code.all$title)\n\n\ndf.fobbe.code.all$acronym <- gsub(""\\\\(C-DB\\\\)"", ""\\\\(C-DBR\\\\)"", df.fobbe.code.all$acronym)\n\ndf.fobbe.code.all$acronym <- gsub(""\\\\[R\\\\] Source Code und Diagramme für \'Juristische Netzwerkdaten für Einsteiger\'"",\n                                  ""Netzwerkdaten für Einsteiger"",\n                                  df.fobbe.code.all$acronym)\n\ndf.fobbe.code.all$acronym <- gsub(""\\\\[R\\\\] High-Performance Parallel Extraction of Text from PDF Documents"",\n                                  ""Parallel Text Extraction"",\n                                  df.fobbe.code.all$acronym)\n\n\n\n\n\n\n\n#\'# Visualize\n\ndir.create(""figures"")\ncaption <- ""Fobbe | DOI: 10.5281/zenodo.7277155""\n\n\n#+\n#\'## Data\n\n\nggplot(data = df.fobbe.data.all, aes(x = dl, y = reorder(acronym, dl)))+\n    geom_bar(stat = ""identity"",\n             fill = ""black"")+\n    geom_text(aes(label = dl), hjust = -0.3)+\n    annotate(""text"",\n             x = 1000,\n             y = 5.5,\n             size = 6,\n             label = paste0(""\\u03A3 = "",\n                            format(sum(df.fobbe.data.all$dl), big.mark = "".""),\n                            "" Downloads"")\n             )+\n    theme_bw()+\n    xlim(c(0, 1500))+\n    labs(\n        title = paste0(""Downloads | Fobbe Data | "",\n                       unique(df.fobbe.data.all$date)),\n        caption = caption,\n        x = ""Downloads"",\n        y = ""Data Set""\n    )+\n    theme(\n        text = element_text(size = 14),\n        plot.title = element_text(size = 14,\n                                  face = ""bold""),\n        legend.position = ""none"",\n        plot.margin = margin(10, 50, 10, 10)\n    )\n\n\nggsave(""figures/Fobbe_Download_Stats_Data.png"",\n       device = ""png"",\n       height = 6,\n       width = 9)\n\n\n\n\n\n#+\n#\'## Code\n\n\nggplot(data = df.fobbe.code.all, aes(x = dl, y = reorder(acronym, dl)))+\n    geom_bar(stat = ""identity"",\n             fill = ""black"")+\n    geom_text(aes(label = dl), hjust = -0.3)+\n    annotate(""text"",\n             x = 300,\n             y = 5.5,\n             size = 6,\n             label = paste0(""\\u03A3 = "",\n                            format(sum(df.fobbe.code.all$dl), big.mark = "".""),\n                            "" Downloads""))+\n    theme_bw()+\n    xlim(c(0, 500))+\n    labs(\n        title = paste0(""Downloads | Fobbe Code | ']",4,"Foundations of Legal Data Science, Open Data Impact Award 2022, Corpus of Decisions, International Court of Justice, open access, tutorials, wissenschaftskommunikation, Rechtsstaat, Analyse juristischer Daten, Ver"
Data from: Responses of sympatric canids to human development revealed through citizen science,"Measuring wildlife responses to anthropogenic activities often requires long-term, large-scale datasets that are difficult to collect. This is particularly true for rare or cryptic species, which includes many mammalian carnivores. Citizen science, in which members of the public participate in scientific work, can facilitate collection of large datasets while increasing public awareness of wildlife research and conservation. Hunters provide unique benefits for citizen science given their knowledge and interest in outdoor activities. We examined how anthropogenic changes to land cover impacted relative abundance of two sympatric canids, coyote (Canis latrans) and red fox (Vulpes vulpes) at a large spatial scale. In order to assess how land cover affected canids at this scale, we used citizen science data from bow hunter sighting logs collected throughout New York State, USA, during 20042017. We found that the two species had contrasting responses to development, with red foxes positively correlated and coyotes negatively correlated with the percentage of low-density development. Red foxes also responded positively to agriculture, but less so when agricultural habitat was fragmented. Agriculture provides food and denning resources for red foxes, whereas coyotes may select forested areas for denning. Though coyotes and red foxes compete in areas of sympatry, we did not find a relationship between species abundance, likely a consequence of the coarse spatial resolution used. Red foxes may be able to coexist with coyotes by altering their diets and habitat use, or by maintaining territories in small areas between coyote territories. Our study shows the value of citizen science, and particularly hunters, in collection of long-term data across large areas (i.e., the entire state of New York) that otherwise would unlikely be obtained.","['#Code to fit models from:\n#Responses of sympatric canids to human development revealed through citizen science\n#Kellner KF, Hil JE, Gantchoff MG, Kramer DW, Bailey AM, Belant JL. \n#Ecology and Evolution\n\n#Required libraries\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(car)\nlibrary(MuMIn)\nlibrary(psych)\n\n#Overdispersion test function from:\n#https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\noverdisp_fun <- function(model) {\n  rdf <- df.residual(model)\n  rp <- residuals(model,type=""pearson"")\n  Pearson.chisq <- sum(rp^2)\n  prat <- Pearson.chisq/rdf\n  pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)\n  c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)\n}\n\n#Read in raw data\ncanid_data <- read_csv(""data/canid_data.csv"")\n\n#Check correlation among covariates\ncor(as.matrix(canid_data[6:8]))\npairs.panels(canid_data[6:8])\n\n#Coyotes-----------------------------------------------------------------------\n\n#Fit model\ncoy_fit <- glmer(coyote ~ ag + urbanL + para_mn + (1|year) + (1|id) +\n             + (1|wmu) + offset(log(hours)), data=canid_data,\n             family=\'poisson\',na.action=\'na.fail\')\n#Save fit\nsaveRDS(coy_fit, ""data/coyote_model.Rds"")\n\n#Look at output (Table 1)\nsummary(coy_fit)\n\n#Check VIF\nvif(coy_fit)\n\n#Check fit\nr.squaredGLMM(coy_fit)\noverdisp_fun(coy_fit)\npchisq(deviance(coy_fit), df=df.residual(coy_fit), lower.tail=FALSE)\n\n#Red fox-----------------------------------------------------------------------\n\n#Get coyote abundance covariate\ncoy_sc <- c(scale(canid_data$coyote/canid_data$hours))\n\n#Fit model\nrf_fit <- glmer(redfox ~ ag + I(ag^2) + urbanL + para_mn +coy_sc + (1|year) + (1|id)\n             + (1|wmu) + offset(log(hours)), data=canid_data,\n             family=\'poisson\',na.action=\'na.fail\')\n#Save fit\nsaveRDS(rf_fit, ""data/redfox_model.Rds"")\n\n#Look at output (Table 2)\nsummary(rf_fit)\n\n#Check VIF\nvif(rf_fit)\n\n#Check fit\nr.squaredGLMM(rf_fit)\noverdisp_fun(rf_fit)\npchisq(deviance(rf_fit), df=df.residual(rf_fit), lower.tail=FALSE)\n']",4,"citizen science, anthropogenic activities, wildlife research, conservation, mammalian carnivores, canids, coyote, red fox, land cover, bow hunter sighting logs, New York State, USA, development, agriculture, food, denning resources"
Herbivores can benefit both plants and their pathogens through selective herbivory on diseased tissue,"Infectious disease can be a key driver of community structure, particularly when pathogens affect foundation species. Seagrasses are foundation species that form meadows along coasts worldwide, controlling sediment deposition and biogeochemical cycling while supporting a diverse community of fish and invertebrates. These plants are affected by wasting diseases that cause necrotic tissue lesions. These lesions could alter seagrass value as food, habitat, and mediators of ecosystem processes. We assess the role of a common eelgrass herbivore, Pentidotea resecata, in affecting prevalence and severity of eelgrass wasting disease lesions. We found that while herbivores vector the eelgrass wasting disease pathogen (L. zosterae) among isolated leaves, on balance they reduce disease severity by more than 50% in field-realistic settings. This was likely because herbivores strongly prefer diseased rather than healthy tissue, consuming nearly twice as much lesion area in choice trials. This preference is caused by pathogen-driven changes in the host plant; lesioned tissue requires less force to penetrate than non-lesioned tissue. Additionally, as lesions increase in size, their phenolic acid concentrations drop, which further increases the magnitude of preference for lesioned tissue. Our results suggest that herbivores could help maintain disease in this system at a high prevalence (by vectoring) but low severity (due to preferential consumption), which is consistent with our field observations of nearly 100% prevalence and low severity in a natural bed where herbivore density is high. Understanding these kinds of vector-pathogen-host interactions in marine systems will advance our predictions of future disease states beyond current understanding, which focuses primarily on the influence of environmental change on pathogen outbreaks.","['library(ggplot2)\r\nlibrary(doBy) #for summaryBy() function to get mean/sd/se/n\r\nlibrary(lme4) # mixed effects models\r\nlibrary(Hotelling)\r\n\r\n\r\n##### P. resecata feeding trial #####\r\nd <- subset(feeding_trial_FINAL)\r\nd <- as.data.frame(d)\r\n\r\nggplot(d, aes(x = Leaf_Health, y= Actual_Area_Consumed, fill = Leaf_Health)) +\r\n  geom_boxplot() +\r\n  geom_jitter(color=""black"", size=1.5, alpha=0.8) +\r\n  theme_classic() +\r\n  theme(legend.position=""none"")+\r\n  ylab(""Area Consumed (cm2)"")+ xlab(NULL) +\r\n  scale_fill_manual(values=c(""darkolivegreen3"",""darkgoldenrod4"")) +\r\n  scale_x_discrete(labels=c(""Green"",""Lesion""))+\r\n  theme(text=element_text(size=12))\r\nggsave(""Field_Area_Consumed_BOXPLOT.png"", dpi=300, units=""in"", width=3.5, height=2.5)\r\n\r\n\r\n#Consumption Stats for Actual_Area_Consumed\r\nm <- lm(Actual_Area_Consumed ~ Leaf_Health, data=d)\r\nsummary(m)\r\n\r\n\r\n#checking residuals\r\n#residuals should be greater than 0.05 \r\nshapiro.test(residuals(m1))\r\nhist(residuals(m1))\r\n\r\n##### Penetrometer Figure ########\r\n\r\nd <- penetrometer_2020\r\nd <- as.data.frame(d)\r\n\r\nggplot(d, aes(x = Leaf_Health, y = Penetrometer_Mass, fill = Leaf_Health)) +\r\n  geom_boxplot() +\r\n  geom_jitter(color=""black"", size=1.5, alpha=0.8) +\r\n  theme_classic() +\r\n  theme(legend.position=""none"")+\r\n  ylab(""Penetrometer Mass (mg)"")+ xlab(NULL) +\r\n  scale_fill_manual(values=c(""darkolivegreen3"",""darkgoldenrod4"")) +\r\n  scale_x_discrete(labels=c(""Green"",""Lesion""))+\r\n  theme(text=element_text(size=12))\r\nggsave(""Penetrometer_Field_Trial_BOXPLOT.png"", dpi=300, units=""in"", width= 3.5, height= 2.5)\r\n       \r\n\r\n#Penetrometer Stats \r\nm <- lm(Penetrometer_Mass ~ Leaf_Health, data=d)\r\nsummary(m)\r\nshapiro.test(residuals(m))\r\n\r\n\r\n##### A. valida feeding trial #####\r\n\r\nd <- valida_data\r\nd <- as.data.frame(d)\r\n\r\n##### Percent of Total Consumption #######\r\n\r\nggplot(d, aes(x = Treatment, y= Percent_Consumed, fill = Treatment)) +\r\n  geom_boxplot() +\r\n  geom_jitter(color=""black"", size=1.5, alpha=0.8) +\r\n  theme_classic() +\r\n  theme(legend.position=""none"")+\r\n  ylab(""% of Total COnsumption"")+ xlab(NULL) +\r\n  scale_fill_manual(values=c(""darkolivegreen3"",""darkgoldenrod4"")) +\r\n  scale_x_discrete(labels=c(""Green"",""Lesion""))+\r\n  theme(text=element_text(size=12))\r\nggsave(""Valida_Percent_Total_Consumed_BOXPLOT.png"", dpi=300, units=""in"", width=3.5, height=2.5)\r\n\r\n\r\nm <- lm(Percent_Consumed ~ Treatment, data=d)\r\nsummary(m)\r\n\r\n#checking residuals\r\n#residuals should be greater than 0.05 \r\nshapiro.test(residuals(m1))\r\nhist(residuals(m1))\r\n\r\n\r\n\r\n']",4,"Herbivores, plants, pathogens, selective herbivory, infectious disease, community structure, foundation species, seagrasses, meadows, coast, sediment deposition, biogeochemical cycling, fish, invertebrates, wasting diseases,"
Herbivores can benefit both plants and their pathogens through selective herbivory on diseased tissue,"Infectious disease can be a key driver of community structure, particularly when pathogens affect foundation species. Seagrasses are foundation species that form meadows along coasts worldwide, controlling sediment deposition and biogeochemical cycling while supporting a diverse community of fish and invertebrates. These plants are affected by wasting diseases that cause necrotic tissue lesions. These lesions could alter seagrass value as food, habitat, and mediators of ecosystem processes. We assess the role of a common eelgrass herbivore, Pentidotea resecata, in affecting prevalence and severity of eelgrass wasting disease lesions. We found that while herbivores vector the eelgrass wasting disease pathogen (L. zosterae) among isolated leaves, on balance they reduce disease severity by more than 50% in field-realistic settings. This was likely because herbivores strongly prefer diseased rather than healthy tissue, consuming nearly twice as much lesion area in choice trials. This preference is caused by pathogen-driven changes in the host plant; lesioned tissue requires less force to penetrate than non-lesioned tissue. Additionally, as lesions increase in size, their phenolic acid concentrations drop, which further increases the magnitude of preference for lesioned tissue. Our results suggest that herbivores could help maintain disease in this system at a high prevalence (by vectoring) but low severity (due to preferential consumption), which is consistent with our field observations of nearly 100% prevalence and low severity in a natural bed where herbivore density is high. Understanding these kinds of vector-pathogen-host interactions in marine systems will advance our predictions of future disease states beyond current understanding, which focuses primarily on the influence of environmental change on pathogen outbreaks.","['library(ggplot2)\r\nlibrary(doBy) #for summaryBy() function to get mean/sd/se/n\r\n\r\n\r\nd <- Lesions_2020\r\nd <- as.data.frame(d)\r\n\r\nfun <- function(x){c(m=mean(x), sd=sd(x),se=sd(x)/sqrt(length(x)), n=length(x))}\r\n\r\nsum <- summaryBy(Lesion_Cover ~ Day_Year, data=d, FUN=fun)\r\nsum_readable <- round(sum, digits = 1)\r\n\r\nsum_readable$year <- c(""2020"", ""2019"", ""2020"", ""2019"", ""2020"", ""2019"", ""2020"", ""2019"", ""2020"", ""2020"")\r\n\r\nggplot(sum_readable, aes(x=Day_Year, y=Lesion_Cover.m, color=year))+\r\n  geom_line(size=1.2)+\r\n  geom_errorbar(aes(ymax=Lesion_Cover.m+Lesion_Cover.se,\r\n                    ymin=Lesion_Cover.m-Lesion_Cover.se),\r\n                width=.05)+\r\n  scale_color_manual(values = c(""darkolivegreen4"", ""darkolivegreen3""))+\r\n  theme_classic()+\r\n  ylab(""Average % Lesion Cover"")+ xlab("""")+\r\n  geom_point(size=3)+\r\n  scale_x_continuous(breaks=c(175, 200, 225, 250), labels=c(""June"",""July"",""August"",""Sept""))+\r\n  theme(text=element_text(size=12))\r\nggsave(""Field_Lesions_nomean_presentation.png"", dpi=300, units=""in"", width=6, height=3)\r\n\r\n', 'library(ggplot2)\r\nlibrary(emmeans)\r\n\r\nd <- mesocosm\r\nd <- as.data.frame(d)\r\nd <- subset(d, AFTER_Lesions != 44)\r\n\r\n##### Lesion Area Change #######\r\n\r\n#Stats\r\nm <- lm(Lesion_Area_Change ~ Treatment, data=d)\r\nsummary(m)\r\nemm <- emmeans(m, ~Treatment)\r\ncontrast(emm, method = ""tukey"")\r\nshapiro.test(residuals(m))\r\n\r\nhist(residuals(m))\r\n\r\n#There is an outlier\r\ncooksd <-- cooks.distance(m)\r\n\r\nggplot(d, aes(x=Treatment, y=Lesion_Area_Change)) +\r\n  geom_boxplot(fill = ""olivedrab3"", color = ""black"") \r\n\r\n#It is Clip8, which I will go ahead and remove \r\n\r\nd <- subset(d, AFTER_Lesions != 44)\r\n#Residuals now normal. \r\n#Stats re-run with Clip8 removed from the dataset\r\n\r\n#want a box plot to see the negative change values for Bug \r\n\r\nggplot(d, aes(x = Treatment, y= Lesion_Area_Change, fill = Treatment)) +\r\n  geom_boxplot() +\r\n  geom_jitter(color=""black"", size=1.5, alpha=0.8) +\r\n  theme_classic() +\r\n  geom_hline(yintercept = 0) + \r\n  theme(legend.position=""none"")+\r\n  ylab(""Change in Total Lesion Area (cm2)"")+ xlab(NULL) +\r\n  scale_x_discrete(labels=c(""Isopod"",""Clipped"", ""Control""))+\r\n  scale_fill_manual(values=c(""lightsteelblue1"", ""lightsteelblue3"", ""lightsteelblue4"")) +\r\n  theme(text=element_text(size=10))\r\nggsave(""Lesion_Area_Mesocosms_BoxPlot.png"", dpi=300, units=""in"", width=3.5, height=2.5)\r\n\r\n', 'library(ggplot2)\r\nlibrary(doBy) #for summaryBy() function to get mean/sd/se/n\r\nlibrary(car)\r\nlibrary(emmeans)\r\n\r\nd <- micro_redo\r\nd <- as.data.frame(d)\r\nd <- subset(d, AFTER_p1 == ""Focal"")\r\n\r\n##### Hurdle Model ######\r\n\r\nd <- subset(d, Treatment != ""Clip"")\r\n\r\n# Creating a binomial variable for lesion presence\r\nd$Lesion_P <- ifelse(d$AFTER_pclean_redo> 0, ""1"",""0"")\r\nd$Lesion_P <- as.integer(d$Lesion_P)\r\n\r\n# Running binomial model on lesion presence\r\nm <- glm(Lesion_P ~ Treatment, data=d, family = binomial)\r\nsummary(m)\r\n\r\n#Running a linear model on the non-zero lesion values\r\nm <- glm(AFTER_pclean_redo ~ Treatment, data = subset(d, AFTER_pclean_redo > 0), family = Gamma)\r\nsummary(m)\r\n\r\n\r\n#### Boxplot #####\r\n\r\nggplot(d, aes(x = Treatment, y= AFTER_pclean_redo, fill = Treatment)) +\r\n  geom_boxplot() +\r\n  geom_jitter(color=""black"", size=1.5, alpha=0.8) +\r\n  theme_classic() +\r\n  geom_hline(yintercept = 0) + \r\n  theme(legend.position=""none"")+\r\n  ylab(""New Lesion Area (cm2)"")+ xlab(NULL) +\r\n  scale_x_discrete(labels=c(""Isopod"",""Clipped"", ""Control""))+\r\n  scale_fill_manual(values=c(""lightsteelblue1"", ""lightsteelblue3"", ""lightsteelblue4"")) +\r\n  theme(text=element_text(size=10))\r\nggsave(""microcosm_newlesion_boxplot_redo.png"", dpi=300, units=""in"", width=3.5, height=2.5)\r\n', 'library(lavaan)\r\nlibrary(MVN)\r\n\r\nd_full <- as.data.frame(gone_wild)\r\nd <- subset(d_full, Cup_Number != 11 & Cup_Number != 20)\r\nsubd <- d[,c(1,6,10,13,16,23,28)] #keeping wanted columns\r\n\r\n#Standardize the data (putting everything into units of standard deviation)\r\n#different variables have different units and you don\'t want that to affect the model \r\nd1 <- data.frame(lapply(2:7, function(x,...){(subd[,x]-mean(subd[,x],na.rm = T))/sd(subd[,x],na.rm = T)}))\r\n\r\n#addID column back in \r\nd2<- cbind(subd[,1],d1)\r\n#putting column names back in \r\ncolnames(d2) <- colnames(subd[1:7])\r\n\r\n#check for multivariate normality. \r\nmvn(d2, mvnTest = c(""mardia""), covariance = T)\r\n\r\n\r\n#### Original_MetaModel ####\r\nm<-\r\n  \'Percent_Total_Consumed_Lesion ~ Lesion_Penetrometer + Lesion_Phenolics + Lesion_C_N\r\nLesion_Phenolics ~ Plant_Lesion_Cover\r\nLesion_Penetrometer ~ Plant_Lesion_Cover\'\r\n\r\nfit_1 <- sem(m, data=d2[,2:7], missing = ""ml"")\r\nsummary(fit_1, standardized=T, rsquare=TRUE)\r\n\r\n#### Original_minus_C_N (attempt 1 at correcting model fit) ####\r\nm2<-\r\n  \'Percent_Total_Consumed_Lesion ~ Lesion_Penetrometer + Lesion_Phenolics\r\nLesion_Phenolics ~ Plant_Lesion_Cover\r\nLesion_Penetrometer ~ Plant_Lesion_Cover\r\nLesion_C_N ~ Lesion_C_N\'\r\n\r\nfit_2 <- sem(m2, data=d2[,2:7], missing = ""ml"")\r\nsummary(fit_2, standardized=T, rsquare=TRUE)\r\n\r\n#### Original_minus_C_N (attempt 2 at correcting model fit) ####\r\nm2<-\r\n  \'Percent_Total_Consumed_Lesion ~ Lesion_Penetrometer + Lesion_Phenolics\r\nLesion_Phenolics ~ Plant_Lesion_Cover\r\nLesion_Penetrometer ~ Plant_Lesion_Cover\'\r\n\r\nfit_2 <- sem(m2, data=d2[,2:7], missing = ""ml"")\r\nsummary(fit_2, standardized=T, rsquare=TRUE)\r\nAIC(fit_2)\r\n\r\n#### Model_Fits_minus_Penetrometer (removing first insignificant path) ####\r\nm2<-\r\n  \'Percent_Total_Consumed_Lesion ~ Lesion_Phenolics\r\nLesion_Phenolics ~ Plant_Lesion_Cover\r\nLesion_Penetrometer ~ Plant_Lesion_Cover\'\r\n\r\nfit_2 <- sem(m2, data=d2[,2:7], missing = ""ml"")\r\nsummary(fit_2, standardized=T, rsquare=TRUE)\r\nAIC(fit_2)\r\n\r\n#### Model_Fits_minus_Penetrometer (removing second insignificant path) ####\r\nm2<-\r\n  \'Percent_Total_Consumed_Lesion ~ Lesion_Phenolics\r\nLesion_Phenolics ~ Plant_Lesion_Cover\r\nLesion_Penetrometer ~ Lesion_Penetrometer\'\r\n\r\nfit_2 <- sem(m2, data=d2[,2:7], missing = ""ml"")\r\nsummary(fit_2, standardized=T, rsquare=TRUE)\r\nAIC(fit_2)\r\n\r\n#### Just Phenolics #### \r\n#lesion cover --> phenolics --> consumption\r\nmphenolics<-\r\n  \'Percent_Total_Consumed_Lesion ~ Lesion_Phenolics \r\nLesion_Phenolics ~ Plant_Lesion_Cover\'\r\n\r\nfit_phenolics <- sem(mphenolics, data=d2[,2:7], missing = ""ml"")\r\nsummary(fit_phenolics, standardized=T, rsquare=TRUE)\r\n\r\n#### Adding Clip Width ####\r\nm2<-\r\n  \'Percent_Total_Consumed_Lesion ~ Lesion_Phenolics + Lesion_Clip_Width\r\nLesion_Phenolics ~ Plant_Lesion_Cover\r\nLesion_Penetrometer ~ Lesion_Penetrometer\'\r\n\r\nfit_2 <- sem(m2, data=d2[,2:7], missing = ""ml"")\r\nsummary(fit_2, standardized=T, rsquare=TRUE)\r\nAIC(fit_2)\r\n\r\n\r\n#### Checking AIC with width in the model matrix ####\r\nm2<-\r\n  \'Percent_Total_Consumed_Lesion ~ Lesion_Phenolics\r\nLesion_Phenolics ~ Plant_Lesion_Cover\r\nLesion_Penetrometer ~ Lesion_Penetrometer\r\nLesion_Clip_Width ~ Lesion_Clip_Width\'\r\n\r\nfit_2 <- sem(m2, data=d2[,2:7], missing = ""ml"")\r\nsummary(fit_2, standardized=T, rsquare=TRUE)\r\nAIC(fit_2)\r\n']",4,"Herbivores, plants, pathogens, selective herbivory, infectious disease, community structure, foundation species, seagrasses, meadows, coasts, sediment deposition, biogeochemical cycling, fish, invertebrates, wasting diseases"
Response diversity in corals: hidden differences in bleaching mortality among cryptic Pocillopora species,"Variation among functionally similar species in their response to environmental stress buffers ecosystems from changing states. Functionally similar species may often be cryptic species representing evolutionarily distinct genetic lineages that are morphologically indistinguishable. However, the extent to which cryptic species differ in their response to stress, and could therefore provide a source of response diversity, remains unclear because they are often not identified or are assumed to be ecologically equivalent. Here, we uncover differences in the bleaching response between sympatric cryptic species of the common Indo-Pacific coral, Pocillopora. In April 2019, prolonged ocean heating occurred at Moorea, French Polynesia. 72% of pocilloporid colonies bleached after 22 days of severe heating (>8C-days) at 10 m depth on the north shore fore reef. Colony mortality ranged from 11% to 42% around the island four months after heating subsided. The majority (86%) of pocilloporids that died from bleaching belonged to a single haplotype, despite twelve haplotypes, representing at least five species, being sampled. Mitochondrial (open reading frame) sequence variation was greater between the haplotypes that experienced mortality versus haplotypes that all survived than it was between nominal species that all survived. Colonies >30 cm in diameter were identified as the haplotype experiencing the most mortality, and in 1125 colonies that were not genetically identified, bleaching and mortality increased with colony size. Mortality did not increase with colony size within the haplotype suffering the highest mortality, suggesting that size-dependent bleaching and mortality at the genus level was caused instead by differences among cryptic species. The relative abundance of haplotypes shifted between February and August, driven by declines in the same common haplotype for which mortality was estimated directly, at sites where heat accumulation was greatest, and where larger colony sizes occurred. The identification of morphologically indistinguishable species that differ in their response to thermal stress, but share a similar ecological function in terms of maintaining a coral-dominated state, has important consequences for uncovering response diversity that drives resilience, especially in systems with low or declining functional diversity.","['## R code to produce Figure 3, and the accompanying analyses presented in the text, in: \n## Burgess SC, Johnston EC, Wyatt ASJ, Leichter JJ, Edmunds PJ (in press) Response diversity in corals: hidden differences in bleaching mortality among cryptic Pocillopora species. Ecology.\n## Code written by Scott Burgess. Feb 2021. Send comments or corrections to sburgess@bio.fsu.edu\n## R version 3.6.3 (2020-02-29)\n\n\n# Load required library\nlibrary(plyr)\n\n\n# Import data\n# setwd("""")\ndat <- read.csv(""Data on Bleaching 2019.csv"")\n\n# Prepare data\ndat$Site <- as.factor(dat$Site)\ndat$Quadrat <- as.factor(dat$Quadrat)\ndat$Site.Quadrat <- interaction(dat$Site,dat$Quadrat)\ndat$BleachAny <- ifelse(dat$Bleaching==""0"",0,1)\ndat$Bleach1 <- ifelse(dat$Bleaching==""1"",1,\n\t\t\t\t\t\t\tifelse(dat$Bleaching==""0"",0,NA))\ndat$Bleach2 <- ifelse(dat$Bleaching==""2"",1,\n\t\t\t\t\t\t\tifelse(dat$Bleaching==""0"",0,NA))\ndat$Bleach3 <- ifelse(dat$Bleaching==""3"",1,\n\t\t\t\t\t\t\tifelse(dat$Bleaching==""0"",0,NA))\n\n\n# Subset data into each month\ndatmarch <- dat[dat$Month==""March"",]\ndatmay <- dat[dat$Month==""May"",]\n\n\n# Sample sizes\n# Size and bleaching were measured for 641 out of 1023 (63%) colonies assigned to a bleaching category in May 2019.\nlength(datmay[!is.na(datmay$Longest.cm) & !is.na(datmay$BleachAny),which(names(datmay)==""Longest.cm"")]) # 641\nlength(datmay$Bleaching[!is.na(datmay$Bleaching)])\t# 1023\n(641/1032) * 100 # 62.11%\n\n# Sample sizes at each site (from Figure 3 caption)\n# n=328 at Site 1\nlength(datmay[datmay$Site==""1"" &\n\t\t\t!is.na(datmay$Longest.cm) & !is.na(datmay$BleachAny),which(names(datmay)==""Longest.cm"")])\n# n=313 at Site 2\nlength(datmay[datmay$Site==""2"" &\n\t\t\t!is.na(datmay$Longest.cm) & !is.na(datmay$BleachAny),which(names(datmay)==""Longest.cm"")])\n\n\n# Prepare data for Figure 3\nfoo <- datmay[datmay$Site==""1"",which(names(datmay)==""Longest.cm"")]\nLengthvecS1 <- seq(min(foo,na.rm=T),max(foo,na.rm=T),length=100)\nfoo <- datmay[datmay$Site==""2"",which(names(datmay)==""Longest.cm"")]\nLengthvecS2 <- seq(min(foo,na.rm=T),max(foo,na.rm=T),length=100)\nl <- c(LengthvecS1, LengthvecS2)\n\nnewdat <- data.frame(Longest.cm=l,Site=as.factor(sort(rep(1:2,100))))\n\n## BleachAny in May\nm1 <- glm(BleachAny ~ Longest.cm * Site, data=datmay,family=""binomial"")\np <- predict(m1,newdat,se.fit=T)\npred.BleachAny.Length.May <- data.frame(newdat,\n\tmean=p$fit,\n\tupr = p$fit + 2*p$se.fit,\n\tlwr = p$fit - 2*p$se.fit)\n\n## Bleach1 in May\nm1 <- glm(Bleach1 ~ Longest.cm * Site, data=datmay,family=""binomial"")\np <- predict(m1,newdat,se.fit=T)\npred.Bleach1.Length.May <- data.frame(newdat,\n\tmean=p$fit,\n\tupr = p$fit + 2*p$se.fit,\n\tlwr = p$fit - 2*p$se.fit)\n\n## Bleach2 in May\nm1 <- glm(Bleach2 ~ Longest.cm * Site, data=datmay,family=""binomial"")\np <- predict(m1,newdat,se.fit=T)\npred.Bleach2.Length.May <- data.frame(newdat,\n\tmean=p$fit,\n\tupr = p$fit + 2*p$se.fit,\n\tlwr = p$fit - 2*p$se.fit)\n\n\t\n## Bleach3 in May\nm1 <- glm(Bleach3 ~ Longest.cm * Site, data=datmay,family=""binomial"")\np <- predict(m1,newdat,se.fit=T)\npred.Bleach3.Length.May <- data.frame(newdat,\n\tmean=p$fit,\n\tupr = p$fit + 2*p$se.fit,\n\tlwr = p$fit - 2*p$se.fit)\n\t\n\n# Make Figure 3  \nlabs <- c(""Partial"",""Moderate"",""Severe"",""Any"")\nx.ticks <- seq(0,45,5) \npanel.labs <- c(""a) Site 1 (10m)"",""b) Site 2 (10m)"")\npoints.cex <- 1.5\nax.cex <- 1.2\nx.ax <- seq(0,1,0.2)\ncols <- c(""grey80"",""grey60"",""grey30"",""black"")\ntcksize <- 0.05\nxmin=0\nxmax <- 45\nbrks <- seq(0,xmax,5)\n\ndev.new(width=8,height=4)\nnf <- layout(matrix(1:4,2,2,byrow=T),c(3,3),c(2,0.6,2,0.6))\n# layout.show(n=4)\npar(oma=c(2,2,1,0),mar=c(0,0,0,0))\n\n## Site 1\npar(mar=c(1,3,1,1))\nplot(c(min(x.ticks),max(x.ticks)),c(0,1),type=""n"",ylab="""",xlab="""",yaxt=""n"",xaxt=""n"",bty=""l"")\naxis(side=1,at=x.ticks,cex.axis=ax.cex)\naxis(side=2,at=x.ax,las=1,cex.axis=ax.cex)\nmtext(panel.labs[1],side=3,line=-1,cex=ax.cex,adj=0.05)\n\n# Site 1 Bleach1\nmaxsize <- max(datmay[datmay$Site==""1"" &\n\t\t\t!is.na(datmay$Longest.cm) & !is.na(datmay$Bleach1),which(names(datmay)==""Longest.cm"")])\nfoo <- pred.Bleach1.Length.May[pred.Bleach1.Length.May$Site==""1"" &\n\t\t\t\tpred.Bleach1.Length.May$Longest.cm <max(maxsize),]\nwith(foo, lines(Longest.cm,plogis(mean),lwd=2,col=cols[1]))\n\n# Site 1 Bleach2\nmaxsize <- max(datmay[datmay$Site==""1"" &\n\t\t\t!is.na(datmay$Longest.cm) & !is.na(datmay$Bleach2),which(names(datmay)==""Longest.cm"")])\nfoo <- pred.Bleach2.Length.May[pred.Bleach2.Length.May$Site==""1"" &\n\t\t\t\tpred.Bleach2.Length.May$Longest.cm <max(maxsize),]\nwith(foo, lines(Longest.cm,plogis(mean),lwd=2,col=cols[2]))\n\n# Site 1 Bleach3\nmaxsize <- max(datmay[datmay$Site==""1"" &\n\t\t\t!is.na(datmay$Longest.cm) & !is.na(datmay$Bleach3),which(names(datmay)==""Longest.cm"")])\nfoo <- pred.Bleach3.Length.May[pred.Bleach3.Length.May$Site==""1"" &\n\t\t\t\tpred.Bleach3.Length.May$Longest.cm <max(maxsize),]\nwith(foo, lines(Longest.cm,plogis(mean),lwd=2,col=cols[3]))\n\n# Site 1 BleachAny\nmaxsize <- max(datmay[datmay$Site==""1"" &\n\t\t\t!is.na(datmay$Longest.cm) & !is.na(datmay$BleachAny),which(names(datmay)==""Longest.cm"")])\nfoo <- pred.BleachAny.Length.May[pred.BleachAny.Length.May$Site==""1""', '## R code to produce Figure 4, and the accompanying analyses presented in the text, in: \n## Burgess SC, Johnston EC, Wyatt ASJ, Leichter JJ, Edmunds PJ (in press) Response diversity in corals: hidden differences in bleaching mortality among cryptic Pocillopora species. Ecology.\n## Code written by Scott Burgess. Feb 2021. Send comments or corrections to sburgess@bio.fsu.edu\n## R version 3.6.3 (2020-02-29)\n\n\nlibrary(plyr)\n\n# Import data\n# setwd("""")\ndatmortality <- read.csv(""Data on Mortality 2019.csv"")\n\n# Prepare data\ndatmortality$Site <- as.factor(datmortality$Site)\ndatmortality$Site.Sector.Quadrat <- interaction(datmortality$Site,datmortality$Sector,datmortality$Quadrat)\n\n\n# Size was measured for 1125 out of 1186 (95%) of colonies observed in Febraury 2019 and categorized as alive or dead in August 2019\nlength(datmortality[!is.na(datmortality$Mortality) & !is.na(datmortality$Longest.cm),which(names(datmortality)==""Longest.cm"")]) # 1125\n\nlength(datmortality[!is.na(datmortality$Mortality),which(names(datmortality)==""Longest.cm"")]) # 1186\n\n# What percent of colonies were >30cm at each site?\nfoo <- datmortality[!is.na(datmortality$Mortality) & !is.na(datmortality$Longest.cm),]\n(with(foo[foo$Longest.cm>30,],table(Site))/with(foo,table(Site)))*100\n\n# How many corals exhibited partial mortality?\nlength(datmortality[datmortality$Partial.colony.mortality==""1"",which(names(datmortality)==""Partial.colony.mortality"")])\nlength(datmortality[!is.na(datmortality$Mortality),which(names(datmortality)==""Partial.colony.mortality"")])\n(74/1186)*100 # 6.24%\n\n\n# Set up coral length vectors for plotting\nfoo <- datmortality[datmortality$Site==""1"",which(names(datmortality)==""Longest.cm"")]\nLengthvecS1 <- seq(min(foo,na.rm=T),max(foo,na.rm=T),length=100)\nfoo <- datmortality[datmortality$Site==""2"",which(names(datmortality)==""Longest.cm"")]\nLengthvecS2 <- seq(min(foo,na.rm=T),max(foo,na.rm=T),length=100)\nfoo <- datmortality[datmortality$Site==""3"",which(names(datmortality)==""Longest.cm"")]\nLengthvecS3 <- seq(min(foo,na.rm=T),max(foo,na.rm=T),length=100)\nfoo <- datmortality[datmortality$Site==""4"",which(names(datmortality)==""Longest.cm"")]\nLengthvecS4 <- seq(min(foo,na.rm=T),max(foo,na.rm=T),length=100)\nfoo <- datmortality[datmortality$Site==""5"",which(names(datmortality)==""Longest.cm"")]\nLengthvecS5 <- seq(min(foo,na.rm=T),max(foo,na.rm=T),length=100)\nfoo <- datmortality[datmortality$Site==""6"",which(names(datmortality)==""Longest.cm"")]\nLengthvecS6 <- seq(min(foo,na.rm=T),max(foo,na.rm=T),length=100)\n\n# Calculate number of corals recorded for mortality and size \nfoo1<-with(datmortality[!is.na(datmortality$Longest.cm),],aggregate(x= Longest.cm,by=list(Site),FUN=length))\nfoo2<-with(datmortality[!is.na(datmortality$Mortality),],aggregate(x=Mortality,by=list(Site),FUN=length))\nSampleSize <- as.data.frame(cbind(foo1,foo2$x))\nnames(SampleSize) <- c(""Site"",""n.Longest"",""n.Total"")\n\n# Set up new data frame for model predictions\nl <- c(LengthvecS1,LengthvecS2,LengthvecS3,LengthvecS4,LengthvecS5,LengthvecS6)\nnewdat <- data.frame(Longest.cm=l,Site=as.factor(sort(rep(1:6,100)))) \n\nm1m <- glm(Mortality ~ Longest.cm * Site,\n\t\t\tdata=datmortality[!is.na(datmortality$Longest.cm),],family=""binomial"")\nm2m <- glm(Mortality ~ Longest.cm + Site,\n\t\t\tdata=datmortality[!is.na(datmortality$Longest.cm),],family=""binomial"")\n\n# The relationship between coral size and mortality varied among sites.\n# X^2 = 39.498; p = 1.885e-07\nanova(m1m,m2m,test=""Chisq"")\n\nm3m <- glm(Mortality ~ Site,\n\t\t\tdata=datmortality,family=""binomial"") # this model includes corals with no size measurement\np <- predict(m1m,newdat,se.fit=T)\npred.Mortality.Length <- data.frame(newdat,\n\tmean=p$fit,\n\tupr = p$fit + 2*p$se.fit,\n\tlwr = p$fit - 2*p$se.fit)\np <- predict(m3m,data.frame(Site=c(""1"",""2"",""3"",""4"",""5"",""6"")),se.fit=T) # includes corals with no size measurement\npred.Mortality.Site <- data.frame(\n\tSite=c(""1"",""2"",""3"",""4"",""5"",""6""),\n\tmean=p$fit,\n\tupr = p$fit + 2*p$se.fit,\n\tlwr = p$fit - 2*p$se.fit)\npred.Mortality.Site$fit <- plogis(pred.Mortality.Site$mean)\npred.Mortality.Site$upr.fit <- plogis(pred.Mortality.Site$upr) \npred.Mortality.Site$lwr.fit <- plogis(pred.Mortality.Site$lwr) \npred.Mortality.Site <- cbind(pred.Mortality.Site,SampleSize)\n\n# Extract slope coefficients for each site\nlvls <- as.data.frame(matrix(c(""1"",""2"",""3"",""4"",""5"",""6"",\n\t\t\t\t\t\t\t""2"",""1"",""3"",""4"",""5"",""6"",\n\t\t\t\t\t\t\t""3"",""2"",""1"",""4"",""5"",""6"",\n\t\t\t\t\t\t\t""4"",""2"",""3"",""1"",""5"",""6"",\n\t\t\t\t\t\t\t""5"",""2"",""3"",""4"",""1"",""6"",\n\t\t\t\t\t\t\t""6"",""2"",""3"",""4"",""5"",""1""),\n\t\t\t\t\t\t\tnrow=6,ncol=6))\nslopes <- as.data.frame(matrix(NA,nrow=6,ncol=3))\nfor(i in 1:6){\n\tdatmortality$Site <- factor(datmortality$Site,levels=lvls[,i])\n\tm1 <- glm(Mortality ~ Longest.cm * Site, \n\t\tdata=datmortality[!is.na(datmortality$Longest.cm),],family=""binomial"")\n\tslopes[i,] <- round(c(exp(coef(m1)[2]),exp(confint(m1)[2,])),3)\n}\t\t\t\t\t\t\t\t\t\t\t\t\nslopes$Site <- 1:6\nnames(slopes)[1:3] <- c(""mean"",""lwr"",""upr"") \n\n\n# By August 2019, bleaching mortality was highest on the north shore (Site 1: 33% [28 – 38, 95% CI];  Site 2: 42', '## R code to produce Figure 5, and the accompanying analyses presented in the text, in: \n## Burgess SC, Johnston EC, Wyatt ASJ, Leichter JJ, Edmunds PJ (in press) Response diversity in corals: hidden differences in bleaching mortality among cryptic Pocillopora species. Ecology.\n## Code written by Scott Burgess. Feb 2021. Send comments or corrections to sburgess@bio.fsu.edu\n## R version 3.6.3 (2020-02-29)\n\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Import data\n# setwd("""")\ndat <- read.csv(""Data on Haplotypes 2019.csv"")\n\n# Prepare data\ndat$Trip.Site <- interaction(dat$Trip,dat$Site)\ndat1 <- dat[dat$Trip==""1"",] # Data from February 2019\ndat2 <- dat[dat$Trip==""2"",] # Data from August 2019\n\n\n# How many corals ID\'d genetically in Feb at 10m\nlength(dat1$Species) # 68 colonies haplotyped\nlength(dat1[!is.na(dat1$Photos_max.length.cm), which(names(dat1)==""Species"")]) # 67 colonies haplotyped and size measured\ndplyr::count(dat1,vars=Site) # Site 1 n=7; Site 2 n=30; Site 3 n=19; Site 5 n=12.\nlength(dat1[!is.na(dat1$Mortality), which(names(dat1)==""Species"")]) # 51 colonies haplotyped, size measured, and survival recorded in Aug.\n\n# How many corals ID\'d genetically in Aug at 10m\nlength(dat2$Species) # 394 colonies haplotyped\ndplyr::count(dat2,vars=Site) # Site 1 n=65; Site 2 n=69; Site 3 n=42; Site 4 n=83; Site 5 n=68; Site 6 n=67.\n\n\n# How many corals ID\'d genetically in Feb and Aug at 10m?\nlength(dat$Species) # 462 colonies haplotyped\n\n# How many haplotypes found in Feb and Aug?\ndplyr::count(dat,vars=Species.haplotype) # 12 haplotypes total\ndplyr::count(dat,vars=Species) # At least 5 nominal species. \n\nfeb01 <- with(dat1,table(Species,Mortality))\nfeb01 <- feb01[c(5,6,4,7,3,1,2),]; feb01 # 14 colonies died, 12 were haplotype 11\n\n# How many haplotypes found in Feb where size and mortality was measured?\ntmp <- dplyr::count(dat1[!is.na(dat1$Mortality),],vars=Species) \ntmp[c(5,6,4,7,3,1,2),]\nsum(tmp$n)\n\n# How many haplotypes found in Feb?\nfeb <- dplyr::count(dat1,vars=Species) # 7 species in Feb\nfeb <- feb[c(5,6,4,7,3,1,2),]\nfeb$prop <- feb$n / sum(feb$n)\n\n# How many haplotypes found in Aug?\naug <- dplyr::count(dat2,vars=Species) # 7 species in Aug\naug <- aug[c(5,6,4,7,3,1,2),]\naug <- aug[match(feb$vars,aug$vars),]\naug$prop <- aug$n / sum(aug$n)\n\n# Size and mortality measured\ndplyr::count(dat1[!is.na(dat1$Mortality),],vars=Site) # Site 1 n=6; Site 2 n=27; Site 3 n=9; Site 5 n=9.\n\n\n# Mortality rates for all haplotypes\nm <- glm(Mortality ~ Photos_max.length.cm, data=dat1,family=""binomial"")\nm0 <- glm(Mortality ~ 1, data=dat1,family=""binomial"")\nanova(m,m0,test=""Chisq"")\nslope1 <- c(as.numeric(round(coef(m)[2],2)), as.numeric(round(confint(m)[2,],2)))\nmean1 <- c(as.numeric(coef(m0)),as.numeric(confint(m0)))\nround(plogis(mean1),2) # \nround(exp(slope1),2)\nn1 <- length(dat1$Mortality[!is.na(dat1$Mortality)])\nsvec <- seq(0,max(dat1$Photos_max.length.cm,na.rm=T),0.1)\np <- predict(m,list(Photos_max.length.cm=svec),se.fit=T)\np1 <- data.frame(size=svec,\n\tmean=p$fit,\n\tupr = p$fit + 2*p$se.fit,\n\tlwr = p$fit - 2*p$se.fit)\n\n\n# Mortality rates for haplotype 11\nsp <- ""Haplotype 11""\nfoo <- dat1[dat1$Species.haplotype==sp,]\nn11 <- length(foo$Mortality[!is.na(foo$Mortality)])\nmb <- glm(Mortality ~ Photos_max.length.cm, data=foo,family=""binomial"")\nmb0 <- glm(Mortality ~ 1, data=foo,family=""binomial"")\nsummary(mb0)\n\n# Mortality did not increase with colony size for haplotype 11\nanova(mb,mb0,test=""Chisq"")\nslope11 <- c(as.numeric(round(coef(mb)[2],2)), as.numeric(round(confint(mb)[2,],2)))\nmean11 <- c(as.numeric(coef(mb0)),as.numeric(confint(mb0)))\n\n# Mortality rate for haplotype 11\nround(plogis(mean11),2)\n\nsvec <- seq(0,max(dat1$Photos_max.length.cm,na.rm=T),0.1)\np <- predict(mb,list(Photos_max.length.cm=svec),se.fit=T)\np11 <- data.frame(size=svec,\n\tmean=p$fit,\n\tupr = p$fit + 2*p$se.fit,\n\tlwr = p$fit - 2*p$se.fit)\n\n\n\n# Species names for plotting\nspecies.vec <- c(""Haplotype 1 (P. eydouxi)"",\n\t""Haplotype 1 (P. meandrina)"",\n\t""Haplotype 2 (P. cf. effusus)"",\n\t""Haplotype 3 (\'P. verrucosa\')"",\n\t""Haplotype 8a"",\n\t""Haplotype 10"",\n\t""Haplotype 11"")\n\n# Species names for indexing dataframe\nhaplotype.vec <- c(""P. eydouxi"",\n\t""P. meandrina"",\n\t""P. cf. effusus"",\n\t""P. verrucosa"",\n\t""Haplotype 8a"",\n\t""Haplotype 10"",\n\t""Haplotype 11"")\n\n# Colorblind-friendly colours\ncols <- c(""#56B4E9"", # P.e\n\t""#0072B2"", # P.m \n\t""#B2DF8A"", # P. cf. e\n\t""#E69F00"", # P.v\n\t""#CC79A7"", # 8a\n\t""#D55E00"", # 10\n\t""#009E73"" # 11\n)\n\t\nHaplotype.Colors <- as.data.frame(cbind(haplotype.vec, cols))\n\nhap.vec <- unique(dat1[!is.na(dat1$Mortality),which(names(dat1)==""Species"")])\nhap.vec <- hap.vec[c(5,3,4,7,6,1,2)]  # \n\n\n\n\n\n##### Make Figure 5 \nquartz(width=11,height=8)\nnf <- layout(matrix(1:15,5,3,byrow=T),c(3,3,3),c(4,1,1,1,1))\npar(oma=c(4,3,3,0))\n# layout.show(nf)\n\nsize.vec <- seq(0,60,10)\nprob.vec <- seq(0,1,0.2)\nxmax <- 65\nxmin <- -5\nymax=0.1\nbrks <- seq(0,xmax,by=5)\n\npar(mar=c(1,3,0,0))\nplot(c(xmin,xmax),c(0,1),type=""n"",xaxt=""n"",yaxt=""n"",ylab="""",xlab="""",bty=""l"")\nwith(p1, polygon(c(rev(size),size),c(rev(plo', '## R code to produce Figure 6, and the accompanying analyses presented in the text, in: \n## Burgess SC, Johnston EC, Wyatt ASJ, Leichter JJ, Edmunds PJ (in press) Response diversity in corals: hidden differences in bleaching mortality among cryptic Pocillopora species. Ecology.\n## Code written by Scott Burgess. Feb 2021. Send comments or corrections to sburgess@bio.fsu.edu\n## R version 3.6.3 (2020-02-29)\n\n\nlibrary(tidyr)\nlibrary(vegan)\nlibrary(BiodiversityR)\n\n# Import data\n# setwd("""")\ndat <- read.csv(""Data on Haplotypes 2019.csv"")\n\n# Prepare data\ndat$Trip.Site <- interaction(dat$Trip,dat$Site)\n\n# Function to calculate proportion of haplotypes or species in each Trip and Site\nProp.function <- function(x,ind,type){\n\tfoo <- x[x$Trip.Site==ind,which(names(x)==type)]\n\tfoo1 <- as.data.frame(table(foo))\n\tfoo2 <- as.data.frame(prop.table(table(foo)))\n\tfoo1$Prop <- foo2$Freq\n\tfoo1$Total <- rep(sum(foo1$Freq),(dim(foo1)[1]))\n\tfoo1$Trip.Site <- rep(ind,dim(foo1)[1])\n\treturn(foo1)\n}\n\nind.vec <- unique(dat$Trip.Site)\nhaplotype.vec <- unique(dat$Species)\n\ndat.prop.haplotype <- list()\nfor(i in 1:length(ind.vec)){\n\tfoo <- Prop.function(x=dat,ind=ind.vec[i],type=""Species"")\n\tdat.prop.haplotype <- rbind(dat.prop.haplotype,foo)\t\t\n}\nss <- strsplit(as.character(dat.prop.haplotype$Trip.Site),""."",fixed=T)\ndat.prop.haplotype$Trip <- sapply(ss,function(x) x[1])\ndat.prop.haplotype$Site <- sapply(ss,function(x) x[2])\nnames(dat.prop.haplotype)[1] <- ""Haplotype""\ndat.prop.haplotype$Haplotype <- factor(dat.prop.haplotype$Haplotype, levels=haplotype.vec)\n# dat.prop.haplotype[1:14,]\n\ndmulti <- spread(dat.prop.haplotype[,c(1,3,5:7)],Haplotype,Prop)\ndmulti <- dmulti[order(dmulti$Trip),]\ndmat <- dmulti[,-1:-3]\n\n# Get Bray-Curtis distance matrix\ndismat <- vegdist(dmat,method=""bray"")\n# Try PCoA\npcoa1 <- cmdscale(dismat,eig=T)\npcoa1$points[,2] <- pcoa1$points[,2]*-1\npcoa1 <- add.spec.scores(pcoa1,dmat,method=""pcoa.scores"",Rscale=T,scaling=1,multi=1)\n\n# Plot - the analysis uses ALL haplotypes, but here we\'re only plotting species scores for the common haplotypes\n# Extract Site scores (weighted sums of species scores)\nxy <- as.matrix(pcoa1$points)\n# Extract species scores\nhaps <- pcoa1$cproj\nhaps <- haps[c(5,1,4,7,6,3,2),]\nhap.names <- c(\n\t""P. eydouxi"",\n\t""P. meandrina"",\n\t""P. cf. effusus"",\n\t""P. verrucosa"",\n\t""Haplotype 8a"",\n\t""Haplotype 10"",\n\t""Haplotype 11"")\n\nhaplotype.vec <- c(""P. eydouxi"",\n\t""P. meandrina"",\n\t""P. cf. effusus"",\n\t""P. verrucosa"",\n\t""Haplotype 8a"",\n\t""Haplotype 10"",\n\t""Haplotype 11"")\n\ncols <- c(""#56B4E9"", # P.e\n\t""#0072B2"", # P.m \n\t""#B2DF8A"", # P. cf. e\n\t""#E69F00"", # P.v\n\t""#CC79A7"", # 8a\n\t""#D55E00"", # 10\n\t""#009E73"" # 11\n)\n\t\nHaplotype.Colors <- as.data.frame(cbind(haplotype.vec, cols))\n\n\nname.cols<-as.character(Haplotype.Colors[match(hap.names,Haplotype.Colors$haplotype.vec),2])\n\n# Custom dataframe sort\ndat.prop.haplotype$Haplotype <- factor(dat.prop.haplotype$Haplotype, levels=hap.names)\ndat.prop.haplotype <- dat.prop.haplotype[order(dat.prop.haplotype$Haplotype),]\n\n\n## Make Figure 6\nquartz(width=7,height=5)\n# nf <- layout(matrix(1:15,5,3,byrow=T),c(3,3,3),c(4,1,1,1,1))\npar(oma=c(1,1,1.5,1), mar=c(4,4,0,0),fig=c(0,0.7,0,1))\n\nplot(xy,xlim=c(-1,0.9),ylim=c(-0.6,0.8),xaxt=""n"",yaxt=""n"",type=""n"",cex.lab=1.2,ylab="""",xlab="""")\nabline(v=0,h=0,lty=2,col=""grey"")\ntext(haps, hap.names,cex=1,col=name.cols)\n# Add location of each Site \npchs <- c(0,1,5,2,15,19,23,8,17,4)\npoints(xy,pch=pchs,cex=2,col=adjustcolor(""black"",alpha.f=0.6),bg=adjustcolor(""black"",alpha.f=0.6),lwd=2)\naxis(side=1,at=seq(-1,1,0.2),cex.axis=1.2)\naxis(side=2,at=seq(-1,1,0.2),las=2,cex.axis=1.2)\nmtext(""PCoA 1"",side=1,line=2.5,cex=1.2)\nmtext(""PCoA 2"",side=2,line=3.5,cex=1.2)\nlegend(-0.9,0.8,legend=c(1,2,3,4,5,6),bty=""n"")\npoints(rep(-0.85,4),c(0.72,0.64,0.56,0.4),pch=pchs[1:4],cex=1.2,col=adjustcolor(""black"",alpha.f=0.6),bg=adjustcolor(""black"",alpha.f=0.6),lwd=2)\npoints(rep(-0.6,6),c(0.72,0.64,0.56,0.48,0.4,0.32),pch=pchs[5:10],cex=1.2,col=adjustcolor(""black"",alpha.f=0.6),bg=adjustcolor(""black"",alpha.f=0.6),lwd=2)\ntext(-0.71,0.8,paste(""Feb"",""Site"",""Aug""))\n\nmtext(""a)"",side=3,adj=0,cex=1.5)\nmtext(""b)"",side=3,adj=1.1,cex=1.5)\n\npar(mar=c(0,4,1,0),fig=c(0.7,1,0.6,1),new=T)\nfoo1 <- dat.prop.haplotype[dat.prop.haplotype$Trip.Site==""1.1"",which(names(dat.prop.haplotype)==""Prop"")]\nfoo1names <- dat.prop.haplotype[dat.prop.haplotype$Trip.Site==""1.1"",which(names(dat.prop.haplotype)==""Haplotype"")]\npie.col<-as.character(Haplotype.Colors[match(foo1names,Haplotype.Colors$haplotype.vec),2])\nbarplot(as.matrix(foo1),xlim=c(0,10),las=1,cex.axis=1,col=pie.col,width=1.5)\naxis(side=1,at=0.8,line=-1,lty=0,label=""Feb"",hadj=0.6)\nmtext(side=3,""Site 1"",adj=0.1)\nmtext(side=2,""Proportion of colonies"",line=2.5,adj=70)\n\nfoo1 <- dat.prop.haplotype[dat.prop.haplotype$Trip.Site==""2.1"",which(names(dat.prop.haplotype)==""Prop"")]\nfoo1names <- dat.prop.haplotype[dat.prop.haplotype$Trip.Site==""2.1"",which(names(dat.prop.haplotype)==""Haplotype"")]\npie.col<-as.character(Haplotype.Colors[match(foo1names,Haplotype.Colors$haplotype.vec),2]']",4,"Response diversity, corals, Pocillopora, bleaching mortality, cryptic species, environmental stress, genetic lineages, morphologically indistinguishable, sympatric species, Indo-Pacific coral, ocean heating, French Polynesia, hap"
Code and data for: Nitrogen-induced hysteresis in grassland biodiversity: A theoretical test of litter-mediated mechanisms,"The global rise in anthropogenic reactive nitrogen (N) and the negative impacts of N deposition on terrestrial plant diversity are well-documented. The R* theory of resource competition predicts reversible decreases in plant diversity in response to N loading. However, empirical evidence for the reversibility of N-induced biodiversity loss is mixed. In a long-term N-enrichment experiment in Minnesota, a low-diversity state that emerged during N addition has persisted for decades after additions ceased. Hypothesized mechanisms preventing recovery of biodiversity include nutrient recycling, insufficient external seed supply, and litter inhibition of plant growth. Here we present an ODE model that unifies these mechanisms, produces bistability at intermediate N inputs, and qualitatively matches the observed hysteresis at Cedar Creek. Key features of the model, including native species' growth advantage in low-N conditions and limitation by litter accumulation, generalize from Cedar Creek to North American grasslands. Our results suggest that effective biodiversity restoration in these systems may require management beyond reducing N inputs, such as burning, grazing, haying, and seed additions. By coupling resource competition with an additional inter-specific inhibitory process, the model also illustrates a general mechanism for bistability and hysteresis that may occur in multiple ecosystem types.","['rm(list=ls())\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\nlibrary(dplyr)\nlibrary(ggplot2)\n\n#################################\n#### Initial Data Processing ####\n#################################\n\n#import biomass data\nbiomass <- read.csv(""e141biomassAboveBelowLitterRoot.csv"")\n\n#shorten variable names\nbiomass <- biomass %>% rename(""above"" =""AbovegroundTotal.Biomass..g.m.2."",\n                              ""below""=""Total.root.biomass.0.20..g.m.2."",\n                              ""total""=""Total.Biomass"",\n                              ""litter""=""Miscellaneous.Litter..g.m.2."")\n\n#restrict to ambient CO2 treatment\nambCO2 <- biomass %>% filter(CO2.Treatment == ""Camb"")\n\n#restrict to pairs of years with a spring burn followed by no burn \n# (these turn out to be even followed by odd years)\n#ASSUMPTIONS:\n## the spring burn resets litter levels to near zero\n## August measurements of aboveground biomass give an upper estimate of litter production that year\n## June measurements of litter the following year give a lower estimate of litter production\n\nambCO2burn <- ambCO2 %>% filter((year == ""2003"" & Season == ""August"") | \n                                  (year == ""2004"" & Season == ""June"") | \n                                  (year == ""2005"" & Season == ""August"") | \n                                  (year == ""2006"" & Season == ""June"") |\n                                  (year == ""2007"" & Season == ""August"") | \n                                  (year == ""2008"" & Season == ""June"") |\n                                  (year == ""2009"" & Season == ""August"") |\n                                  (year == ""2010"" & Season == ""June""))\n#############################\n### Exotic Species Group ####\n#############################\n\n#generate ""exotics"" sub-dataset from monospecies plots of Elymus (Agropyron) repens and Poa pratensis\nexotics <- ambCO2burn %>% filter(monospecies == ""Agropyron repens"" | monospecies == ""Poa pratensis"")\n\n#join plot data from adjacent years (Aug 2003 with June 2004, etc.)\nexotics_Aug <- exotics %>% filter(Season == ""August"") %>% select(year, Season, Plot, monospecies, above, below, total) %>% mutate(augYear=year)\nexotics_Jun <- exotics %>% filter(Season == ""June"") %>% select(year, Season, Plot, litter) %>% mutate(augYear=year-1)\nexotics <- full_join(exotics_Aug, exotics_Jun, by=c(""augYear"", ""Plot""))\n\n#calculate upper and lower estimates of the parameter m\nexotics <- exotics %>% mutate (mUpper = above / total) #August aboveground biomass per total biomass\nexotics <- exotics %>% mutate (mLower = litter / total) #following June litter per previous year total biomass\n#note: root biomass was estimated from samples 20cm deep, making ""total"" a likely underestimate and inflating mUpper, mLower.\n\n#Based on the following boxplots, we select the value m=0.2 yr^(-1) \n#as reasonable for the exotic litter production parameter\nggplot(data = exotics, aes(y = mUpper, x = monospecies)) + geom_boxplot() \nggplot(data = exotics, aes(y = mLower, x = monospecies)) + geom_boxplot()\n\n#############################\n### Native Species Group ####\n#############################\n\n#generate ""natives"" sub-dataset from monospecies plots of Schizachyrium scoparium and 16-species plots\n#(the 16-species plots may contain some non-native species, but feature biodiversity, a linked attribute in our model)\nnatives <- ambCO2burn %>% filter(monospecies == ""Schizachyrium scoparium"" | CountOfSpecies == ""16"")\n\n#join plot data from adjacent years (Aug 2003 with June 2004, etc.)\nnatives_Aug <- natives %>% filter(Season == ""August"") %>% select(year, Season, Plot, monospecies, above, below, total) %>% mutate(augYear=year)\nnatives_Jun <- natives %>% filter(Season == ""June"") %>% select(year, Season, Plot, litter) %>% mutate(augYear=year-1)\nnatives <- full_join(natives_Aug, natives_Jun, by=c(""augYear"", ""Plot""))\n\n#calculate upper and lower estimates of the parameter m\nnatives <- natives %>% mutate (mUpper = above / total) #August aboveground biomass per total biomass\nnatives <- natives %>% mutate (mLower = litter / total) #following June litter per previous year total biomass\n#note: root biomass was estimated from samples 20cm deep, making ""total"" a likely underestimate and inflating mUpper, mLower.\n\n#Based on the following boxplots, we select the value m=0.2 yr^(-1) \n#as a slightly low but still plausible value for the native litter production parameter\nggplot(data = natives, aes(y = mUpper, x = monospecies)) + geom_boxplot() \nggplot(data = natives, aes(y = mLower, x = monospecies)) + geom_boxplot()\n', 'rm(list=ls())\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\nlibrary(dplyr)\nlibrary(ggplot2)\n\n#############################\n###Nitrogen Concentrations###\n#############################\n\n#import soil nitrogen concentration data\nsoilConc <- read.csv(""nohe141clean.csv"")\n\n#rename nitrate/nitrite and ammonium variables\nsoilConc <- soilConc %>% rename(""NOX"" =""NO2.NO3.mg.kg."",\n                        ""NH4""=""NH4.mg.kg."")\n\n#Note there are two observations with missing values in the NOX column\n#any(is.na(soilConc)) \n#which(is.na(soilConc$NOX))\n\n#Restrict to ambient CO2 and N treatment and to zero-species plots\nAmb0 <- soilConc %>% filter(CO2.Treatment == ""Camb"" & Nitrogen.Treatment == ""Namb"" & CountOfSpecies == 0)\n\n#Take mean concentrations of nitrate/nitrite (NOX) and ammonium (NH4) by depth, excluding observations with measurement of 0\nNOXmean020 <- mean(Amb0$NOX[Amb0$NOX>0 & Amb0$Depth == ""0-20""], na.rm = TRUE)\nNOXmean4060 <- mean(Amb0$NOX[Amb0$NOX>0 & Amb0$Depth == ""40-60""], na.rm = TRUE)\nNH4mean020 <- mean(Amb0$NH4[Amb0$NH4>0 & Amb0$Depth == ""0-20""], na.rm = TRUE)\nNH4mean4060 <- mean(Amb0$NH4[Amb0$NH4>0 & Amb0$Depth == ""40-60""], na.rm = TRUE)\n\n#Interpolate and extrapolate soil concentrations linearly by depth in increments of 20cm until negative values of reached\n#NOX (nitrate and nitrite)\ndnox <- (NOXmean4060-NOXmean020)/2 #the increment dnox is half the difference between measured 0-20cm and 40-60cm values\nnox <- NOXmean020 #initialize at top layer\nNOXconcList <- c(nox) #stores nitrate concentrations (mg / kg soil) by depth\nwhile (nox > -dnox){\n  nox <- nox + dnox\n  NOXconcList[(length(NOXconcList)+1)] <- nox\n}\n\n#NH4 (ammonium)\ndnh4 <- (NH4mean4060-NH4mean020)/2 #the increment da is half the difference between measured 0-20cm and 40-60cm values\nnh4 <- NH4mean020 #initialize at top layer\nNH4concList <- c(nh4) #stores ammonium concentrations (mg / kg soil) by depth\nwhile (nh4 > -dnh4){\n  nh4 <- nh4 + dnh4\n  NH4concList[(length(NH4concList)+1)] <- nh4\n}\n\n#Add two entries of zero to give NH4concList the same length as NOXconcList\nNH4concList[(length(NH4concList)+1)] <- 0\nNH4concList[(length(NH4concList)+1)] <- 0\n\n#convert from molecular to elemental nitrogen (N) concentrations\n##conversion ratios\nNOX_to_N_high <- (14.007)/(14.007+2*15.999) #NO2\nNOX_to_N_low <- (14.007)/(14.007+3*15.999) #NO3\nNOX_to_N_avg <- 0.5*(NOX_to_N_high + NOX_to_N_low) #average NOX (simple assumption)\nNH4_to_N <- (14.007)/(14.007+4*1.0078) #NH4\n##elemental nitrogen concentrations (mg/kg)\nNconcList <- NOX_to_N_avg * NOXconcList + NH4_to_N * NH4concList\n\n#############################\n######### Soil Mass #########\n#############################\n\n#import soil bulk density data\nsoilDens <- read.csv(""bde141clean.csv"")\n\n#rename density variable (units are g/cm^3)\nsoilDens <- soilDens %>% rename(""Dens"" =""Bulk.Density..g.cm.3."")\n\n#Restrict to ambient CO2 and N treatment and to zero-species plots\nAmb0_D <- soilDens%>% filter(CO2.Treatment == ""Camb"" & Nitrogen.Treatment == ""Namb"" & CountOfSpecies == 0)\n\n#visualize trend in density measurements by depth\n#boxplot(Amb0_D$Dens ~ Amb0_D$Depth)\n\n#take mean density across plots and depths (g/cm^3)\nmeanD <-mean(Amb0_D$Dens)\n\n#calculate volume of one square meter by 20 cm in cm^3\nlayerVol <- 100 * 100 * 20\n\n#convert soil bulk density to soil mass\nlayerSoilMass <- meanD * layerVol #units of *(g/cm^3)*(cm^3)=g\n\n#convert soil mass from units of g to kg\nlayerSoilMass <- 0.001 * layerSoilMass\n\n#############################\n####### Nitrogen Mass #######\n#############################\n\n#compute N mass in each layer as \nlayerNmass <- NconcList * layerSoilMass #units of (mg/kg)*(kg) = mg\n\n#sum across masses to estimate elemental N mass (in grams) below one square meter of surface\nNmass <- sum(layerNmass)\n\n#convert from mg N to g N\nNmass <- 0.001 * Nmass\n\n#Nmass is grams of elemental nitrogen below one square meter of surface.\n#Code returns ~0.633 g N per square meter.\nprint(Nmass)', 'rm(list=ls())\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n#################################\n#### Initial Data Processing ####\n#################################\n\n#import biomass data\nbiomass <- read.csv(""e141biomassAboveBelowLitterRoot.csv"")\n\n#shorten variable names\nbiomass <- biomass %>% rename(""total""=""Total.Biomass"")\n\n#restrict to ambient CO2 treatment and remove rows with N/A values in the ingrowth column\nambCO2 <- biomass %>% filter(CO2.Treatment == ""Camb"")\n\n#############################\n### Exotic Species Group ####\n#############################\n\n#generate ""exotics"" sub-dataset from monospecies plots of Elymus (Agropyron) repens and Poa pratensis\nexotics <- ambCO2 %>% filter(monospecies == ""Agropyron repens"" | monospecies == ""Poa pratensis"")\n\n#The following boxplots illustrate order of magnitude of biomass density (g/m^2) for exotics Elymus (Agropyron) repens and Poa pratensis\nggplot(data = exotics, aes(y = total, x = monospecies)) + geom_boxplot() \n\n#############################\n### Native Species Group ####\n#############################\n\n#generate ""natives"" sub-dataset from monospecies plots of Schizachyrium scoparium and 16-species plots\n#(the 16-species plots may contain some non-native species, but feature biodiversity, a linked attribute in our model)\nnatives <- ambCO2 %>% filter(monospecies == ""Schizachyrium scoparium"" | CountOfSpecies == ""16"")\n\n#The following boxplots illustrate order of magnitude of biomass density (g/m^2) for 16 species plots (no label) and native Schizachyrium scoparium\nggplot(data = natives, aes(y = total, x = monospecies)) + geom_boxplot() ', 'rm(list=ls())\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n#################################\n#### Initial Data Processing ####\n#################################\n\n#import biomass data\nbiomass <- read.csv(""e141biomassAboveBelowLitterRoot.csv"")\n\n#shorten variable names\nbiomass <- biomass %>% rename(""ingrowth""=""Annual.Total.Root.Ingrowth..g.m.2."",\n                              ""total""=""Total.Biomass"",)\n\n#restrict to ambient CO2 treatment and remove rows with N/A values in the ingrowth column\nambCO2 <- biomass %>% filter(CO2.Treatment == ""Camb"")\nambCO2 <- ambCO2 %>% drop_na(ingrowth)\n\n#############################\n### Exotic Species Group ####\n#############################\n\n#generate ""exotics"" sub-dataset from monospecies plots of Elymus (Agropyron) repens and Poa pratensis\nexotics <- ambCO2 %>% filter(monospecies == ""Agropyron repens"" | monospecies == ""Poa pratensis"")\n\n#estimate u_e \n#use root ingrowth as proxy for root senesence, assuming the two are in balance\n#measurements of both root ingrowth and root mass are in the top 0-20cm of soil \nexotics <- exotics %>% mutate (ue = ingrowth / total) #divide yearly root ingrowth by total plant biomass, yielding value with units of of g g^(-1) yr^ = yr^(-1)\n\n#Based on the following boxplots, we select the value ue=0.2 yr^(-1) \n#as reasonable for the exotic belowground tissue senesence parameter\nggplot(data = exotics, aes(y = ue, x = monospecies)) + geom_boxplot() \n\n\n#############################\n### Native Species Group ####\n#############################\n\n#generate ""natives"" sub-dataset from monospecies plots of Schizachyrium scoparium and 16-species plots\n#(the 16-species plots may contain some non-native species, but feature biodiversity, a linked attribute in our model)\nnatives <- ambCO2 %>% filter(monospecies == ""Schizachyrium scoparium"" | CountOfSpecies == ""16"")\n\n#estimate u_n \n#use root ingrowth as proxy for root senesence, assuming the two are in balance\n#measurements of both root ingrowth and root mass are in the top 0-20cm of soil \nnatives <- natives %>% mutate (un = ingrowth / total) #divide yearly root ingrowth by total plant biomass, yielding value with units of of g g^(-1) yr^ = yr^(-1)\n\n#Based on the following boxplots, we select the value un=0.2 yr^(-1) \n#as reasonable for the native belowground tissue senesence parameter\nggplot(data = natives, aes(y = un, x = monospecies)) + geom_boxplot() \n\n\n']",4,"Keywords: Code, Data, Nitrogen-induced hysteresis, Grassland biodiversity, Theoretical test, Litter-mediated mechanisms, Anthropogenic reactive nitrogen, N deposition, R* theory, Resource competition, Empirical evidence, Biodiversity loss"
The hidden legacy of megafaunal extinction: loss of functional diversity and resilience over the late Quaternary at Hall's Cave,"This dataset contains trait data and R code used in the analysis for the paper ""Hedberg, C.P., Lyons S.K., & Smith F.A. (2021). THe Hidden Legacy of megafaunal extinction: loss of functional diversity and resilience over the Late Quaternary at Hall's Cave. Global Ecology and Biogeography. https://doi.org/10.1111/geb.13428""We collected data for eight functional traits (mass, diet, arboreality, cursoriality, soil disturbance, group size, activity period, migration habit) that collectively describe a species' ecological role and influence on ecosystem processes. With these data, we investigated changes in functional diversity and redundancy of a local mammal community over time at Hall's Cave, a site in Central Texas with a continuous record from 21,000 years ago to the present. Additionally, we included several common introduced and domestic species to the modern community to test whether they restore some lost ecological function. We found that declines in functional diversity were greater than expected given the decrease in species richness, implying lost taxa contributed higher than average distinct ecological function. Functional distances between remaining species increased through time leading to lowered functional redundancy in younger communities. However, recently introduced taxa increased functional diversity to levels similar to the Holocene and partially restored functional space occupied by Late Pleistocene fauna. Our local-scale analysis demonstrates how prolonged biodiversity erosion not only leads to functionally depauperate communities, but critically lowers ecological resilience to future disturbance.","['##########################################################################################################################\r\n#R code used in The hidden legacy of megafaunal extinction:  Loss of functional diversity and ecosystem resilience over \r\n#the Late Quaternary at Hall\'s Cave by Carson P. Hedberg, S. Kathleen Lyons, and Felisa A. Smith\r\n#All R code written by Carson P. Hedberg, chedberg@unm.edu\r\n\r\n##########################################################################################################################\r\n\r\n\r\n#Install required packages\r\ninstall.packages(""pacman"", repos=""https://cloud.r-project.org"")\r\npacman::p_load(dplyr,\r\n               ggplot2,\r\n               ggtree,\r\n               FD,\r\n               picante,\r\n               labdsv,\r\n               hypervolume,\r\n               BAT,\r\n               ade4,\r\n               foreach,\r\n               doParallel,\r\n               readr,\r\n               reshape2,\r\n               plyr,\r\n               cluster,\r\n               forcats,\r\n               funrar,\r\n               update= F)\r\n\r\n\r\n\r\n\r\n###########################################################################################################################\r\n###Data cleaning and Set-up###\r\n\r\n#upload Input data from Dryad : LINK\r\nAll_Taxa <- read_csv(""Input data/All_Taxa.csv"")\r\nPA_Chart <- read_csv(""Input data/PA_Chart.csv"")\r\n\r\n#rename raw data \r\nAllcom<- as.data.frame(All_Taxa)\r\n#set species names as rownames \r\nrow.names(Allcom)<- (Allcom$Binomial)\r\nAllcom<- Allcom[order(Allcom$Binomial),]\r\n\r\n#Set rownames in presence absence chart as time bin names\r\nPA_Chart<- as.data.frame(PA_Chart)\r\nrow.names(PA_Chart)<- PA_Chart$Time_bin\r\nPA_Chart$Time_bin<- NULL\r\n#Add dummy row that includes all taxa\r\nPA_Chart[7,]<- rep(1, dim(PA_Chart)[2])\r\nrownames(PA_Chart)[7]<- ""All""\r\n\r\n#Isolate traits and assign variable types\r\ntraits.all<- Allcom[, c(9:17, 19:21)]\r\ntraits.all$Running<- as.ordered(traits.all$Running)\r\ntraits.all$Arboreality<- as.ordered(traits.all$Arboreality)\r\ntraits.all$Soil_Disturbance<- as.ordered(traits.all$Soil_Disturbance)\r\ntraits.all$Activity_Period<- as.factor(traits.all$Activity_Period)\r\ntraits.all$Migratory<- as.numeric(traits.all$Migratory)\r\n\r\n#create weights vector for traits\r\n#body size = 1\r\n#collective diet = 1\r\n#other collective ecological traits = 1\r\nweights<- c(1, 1/5,1/5,1/5,1/5,1/5, 1/6, 1/6, 1/6, 1/6, 1/6, 1/6)\r\n\r\n\r\n###########################################################################################################################\r\n\r\n\r\n\r\n###########################################################################################################################\r\n###Functional Diversity Metrics###\r\n\r\n\r\n#Run function for assessing the quality of functional space from 2 to N number of dimensions fora given trait matrix. \r\n#Developed by Eva Maire and Sebastien Villager (https://doi.org/10.1111/geb.12299), R code for function found in supplementary \r\n#information\r\n#Testing with a maximum of 12 dimensions because there are 12 separate trait values for each species\r\nQF<-quality_funct_space(traits.all, nbdim=12, metric=""Gower"", plot=""quality_funct_space"")\r\nQF$meanSD\r\nQF$details_funct_space$alg_best_tree\r\n\r\n\r\n#create Gower\'s distance matrix from functional traits matrix\r\ndistALL <- gowdis(traits.all, weights, ord=""podani"")\r\n\r\n\r\n#Run principle coordinate analysis on Gower\'s distance matrix\r\npcoALL <- pco(distALL, k= 12)\r\npcopointsALL <- pcoALL$points\r\n\r\n#caclulate variation explained by first 4 eigenvectors \r\ntoteigALL <- sum(pcoALL$eig[pcoALL$eig>0])\r\neig1ALL <-round((pcoALL$eig[1]/toteigALL)*100,digits=0) #42\r\neig2ALL <-round((pcoALL$eig[2]/toteigALL)*100,digits=0) #14\r\neig3ALL<- round((pcoALL$eig[3]/toteigALL)*100,digits=0) #8\r\neig4ALL<- round((pcoALL$eig[4]/toteigALL)*100,digits=0) #7\r\n\r\n#calculate how traits load onto each PCoA axis\r\nefG.all <- envfit(pcopointsALL, traits.all, permu=999)\r\nefG.all\r\n\r\n\r\n#Make empty matrix to house all FD metrics\r\nFD.metrics<- as.data.frame(matrix(data= NA, nrow= 7, ncol= 6, dimnames= list(c(""TP"", ""EH"", ""MH"", \r\n              ""LH"", ""Hist"", ""Mod"", ""All""),  c(""FRic"", ""FDis"", ""FDiv"", ""FEve"", ""FVol"", ""SpeciesRichness""))))\r\n\r\n\r\n#check to make sure taxa are in the same order \r\nrownames(traits.all)== colnames(PA_Chart)\r\n#if not all TRUE, run code below to put both in alphebetical order\r\n#traits.all<- traits.all[order(rownames(traits.all)),]\r\n#PA_Chart<- PA_Chart[,order(colnames(PA_Chart))]\r\n\r\n\r\n#calculate FRic, FDis, FDiv, FEve, and community-wide means (CWM\'s) using dbFD function from FD package\r\n#calculates Gower\'s distance among taxa based on trait matix and specified weights, then performs PCoA on distance matrix \r\n#to construct functional space\r\n#ord= ""podani"" and corr = ""calliez"" specify methods for treatment of ordinal varibles and negative eigenvectors, respectivley\r\n#stand.FRic determines whether FRic values should be standardized by FRic value when all species are included\r\n#m = x designates number of PCoA axes to use for construction of fu']",4,"megafaunal extinction, functional diversity, resilience, late Quaternary, Hall's Cave, trait data, R code, ecological role, ecosystem processes, mammal community, Central Texas, continuous record, introduced species, domestic species, species richness,"
Tempo and mode in karyotype evolution revealed by a probabilistic model incorporating both chromosome number and morphology,"Karyotype, including the chromosome and arm numbers, is a fundamental genetic characteristic of all organisms and has long been used as a species-diagnostic character. Additionally, karyotype evolution plays an important role in divergent adaptation and speciation. Centric fusion and fission change chromosome numbers, whereas the intra-chromosomal movement of the centromere, such as pericentric inversion, changes arm numbers. A probabilistic model simultaneously incorporating both chromosome and arm numbers has not been established. Here, we built a probabilistic model of karyotype evolution based on the ""karyograph"", which treats karyotype evolution as a walk on the two-dimensional space representing the chromosome and arm numbers. This model enables analysis of the stationary distribution with a stable karyotype for any given parameter. After evaluating their performance using simulated data, we applied our model to two large taxonomic groups of fish, Eurypterygii and series Otophysi, to perform maximum likelihood estimation of the transition rates and reconstruct the evolutionary history of karyotypes. The two taxa significantly differed in the evolution of arm number. The inclusion of speciation and extinction rates demonstrated possibly high extinction rates in species with karyotypes other than the most typical karyotype in both groups. Finally, we made a model including polyploidization rates and applied it to a small plant group. Thus, the use of this probabilistic model can contribute to a better understanding of tempo and mode in karyotype evolution and its possible role in speciation and extinction.","###### Library used in the study ######

# For MLE
library(diversitree)

# For subsequent data analysis
library(expm)
library(stats4)
library(dplyr)
library(tidyr)
library(Matrix)
library(ggplot2)
library(colorRamps)
library(ggpubr)
library(extrafont)

###### Function for change between karyotype state vs. (arm no., chr no.) ######


#(chr number, arm number) -> state number
#this is (y,x) but not (x,y).
scal<-function(d,j){d*(d+1)/2+j-d}

#state number -> c(chr number, arm number)
chr_arm_cal<-function(s){
  d<- 1
  while(s-d*(d+1)/2 > d){
    d<- d+1
  }
  return(c(d,s-d*(d-1)/2))
}

#state vector -> matrix with two columns of chr number and arm number of each state
chr_arm_vec<-function(svec){
  chr_arm_mat<-c()  
  for(i in 1:length(svec)){
    chr_arm_mat<-rbind(chr_arm_mat,chr_arm_cal(svec[i]))
  }
  return(chr_arm_mat)
}

#state number -> c(arm number,chr number)
arm_chr_cal<-function(s){
  d<- 1
  while(s-d*(d+1)/2 > d){
    d<- d+1
  }
  return(c(s-d*(d-1)/2,d))
}

#state number -> karyotype ""(x,y)""
karyotype_cal<-function(s){
  d<- 1
  while(s-d*(d+1)/2 > d){
    d<- d+1
  }
  return(paste(""("",s-d*(d-1)/2,"","",d,"")"",sep=""""))
}

## Function, chr_arm_list
### vector of states -> list with [[1]] chr no. vector and [[2]] arm no. vector

chr_arm_list<-function(state){
  maxstate<-max(state)
  chr<-rep(0,length=length(state))
  arm<-rep(0,length=length(state))
  s<-1
  d<-1
  while(s <= maxstate){
    for(j in d:(2*d)){
      idents<-which(state==s)
      if(length(idents)>0){
        chr[idents]<-rep(d,length=length(idents))
        arm[idents]<-rep(j,length=length(idents))
      }
      s<-s+1
    }
    d<-d+1
  }
  return(list(chr,arm))
}


###### Function, q_generation_2020 ######
### making Q-matrix from the 4 parameters of karytype evolution
### chrmax: maximum chromosome number

q_generation_2020<-function(k1,k2,k3,k4,chrmax){
  stanum<-(chrmax+1)*(chrmax+2)/2-1
  q<-matrix(rep(0),nrow=stanum,ncol=stanum)
  
  #        k2
  #        ^
  #        |
  # k4 <---+---> k3
  #        |
  #        v
  #        k1
  #
  
  #transition rates from states with limited directions of transition
  #in the beginning are defined one by one.
  
  q[1,2]<- k3
  q[1,1]<- -k3
  q[2,1]<- k4
  q[2,3]<- k2
  q[2,2]<- -k2 -k4
  q[3,2]<- k1 ##2020
  q[3,4]<- 2*k3
  q[3,3]<- -k1-2*k3 ##2020
  q[4,3]<- k4
  q[4,5]<- k3
  q[4,6]<- k2
  q[4,4]<- -k4-k3-k2
  q[5,4]<- 2*k4
  q[5,7]<- 2*k2
  q[5,5]<- -2*k4-2*k2
  
  for(d in 3:(chrmax-1)){
    #d, chromosome number
    
    #d*(d+1)/2 state with no metacentric (no fission, no M-A transition)
    q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
    q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1/2 ##2020
    q[d*(d+1)/2,d*(d+1)/2]<- (-d*k3 -(d*(d-1)*k1/2)) ##2020
    
    for(j in (d+1):(2*d-2)){
      #j, arm number
      #loop for the states with 4 directions
      
      #k,state number
      #up, upper state
      #down, lower state
      #anum, the number of acrocentric
      #mnum, the number of metacentric
      
      k<-d*(d+1)/2+j-d
      up<-(d+1)*(d+2)/2+j-d-1
      down<-d*(d-1)/2+j-d+1
      anum<-2*d-j
      mnum<-j-d
      q[k,k+1]<-anum*k3
      q[k,k-1]<-mnum*k4
      q[k,down]<-anum*(anum-1)*k1/2 ##2020
      q[k,up]<-mnum*k2
      q[k,k]<- -q[k,k+1]-q[k,k-1]-q[k,down]-q[k,up]
    }
    
    #states with 1 acrocentric (no fusion)
    k<-d*(d+1)/2+2*d-1-d
    up<-(d+1)*(d+2)/2+2*d-1-d-1
    q[k,k+1]<-k3
    q[k,k-1]<-(d-1)*k4
    q[k,up]<-(d-1)*k2
    q[k,k]<- -q[k,k+1]-q[k,k-1]-q[k,up]
    
    #states with no acrocentric (no fusion, no A-M transition)
    k<-d*(d+1)/2+2*d-d
    up<-(d+1)*(d+2)/2+2*d-d-1
    q[k,k-1]<-d*k4
    q[k,up]<-d*k2
    q[k,k]<- -q[k,k-1]-q[k,up]
    
  }
  
  d<-chrmax
  
  #state in the chrmax (no fission)
  #no metacentric case (no fission, no M-A transition)
  q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
  q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1/2 ##2020
  q[d*(d+1)/2,d*(d+1)/2]<- -d*k3-d*(d-1)*k1/2 ##2020
  
  for(j in (d+1):(2*d-2)){
    #3 directions
    k<-d*(d+1)/2+j-d
    down<-d*(d-1)/2+j-d+1
    anum<-2*d-j
    mnum<-j-d
    q[k,k+1]<-anum*k3
    q[k,k-1]<-mnum*k4
    q[k,down]<-anum*(anum-1)*k1/2 ##2020
    q[k,k]<- -q[k,k+1]-q[k,k-1]-q[k,down]
  }
  
  #one acrocentric case (no fission, no fusion)
  k<-d*(d+1)/2+2*d-1-d
  q[k,k+1]<-k3
  q[k,k-1]<-(d-1)*k4
  q[k,k]<- -q[k,k+1]-q[k,k-1]
  
  #no acrocentric case (no fission, no fusion, no A-M transition)
  k<-d*(d+1)/2+2*d-d
  q[k,k-1]<-d*k4
  q[k,k]<- -q[k,k-1]
  
  return(q)
}



###### Function for Mkn and MuSSE ######

## function, tcal
### Aquisition of number of transition rate 
### snum, total number of states
### is, initial state
### ts, transited state
### The no. of transition is
### snum x snum - snum (diagonals)
### right entries of diagonals have shifted index

tcal<-function(snum,is,ts){
  if(is<ts){
    (is-1)*(snum-1)+ts-1
  }else{
    (is-1)*(snum-1)+ts
  }
}


## function, qwrite
### Writing transition rate q like q0405
### The order of states are important.
### If you use few states, the (1,1) state are expressed as ""q01"".
### But if many, q001 or more longer.

qwrite<-function(ord,istate,tstate){
  ord<-as.character(sprintf(""%02d"",ord))
  qexp<-paste(""q%"",ord,""d%"",ord,""d"",sep="""")
  sprintf(qexp,istate,tstate)
}


## For Mk-n, function get_cons_and_target_ec
### Mk-n, 4 parameter
### Constraints of the model are expressed as formulae by this function.
### This function also output target.i indicating index of target parameters.
### q003002,q002003,q001002 and q002001 are free parameters here,
### which corresponding to k1, k2, k3 and k4, respectively.
### The constraints are made with these 4 parameters.
### chrmax, upper limit of chromosome number (>=4)

get_cons_and_target_ec<-function(chrmax){
  
  #state number
  snum<-(chrmax+1)*(chrmax+2)/2-1
  
  #In diversitree, the transition is written as q001003 to show transition from state1 to state3 for example.
  #The digit of the state number is depending on the total number of the state
  #ord and ord2 shows this digit number.
  ord<-as.integer(log10(snum))+1
  ord2<-as.character(sprintf(""%02d"",ord))
  
  #formula, that represents ""q%03d%03d~%d*%s"" for example.
  #this is used in sprintf
  formula<-paste(""q%"",ord2,""d%"",ord2,""d~%d*%s"",sep="""")
  
  #kdel, that gives the transition rate corresponding to 4 parameters
  kdel<-c(qwrite(ord,3,2),qwrite(ord,2,3),qwrite(ord,1,2),qwrite(ord,2,1))
  
  #con_add, express and add constraint formulae to the given cons vector
  #is, initial state
  #ts, transited state
  #kcoef, coefficient multiplied to kx
  #knum, the index of parameters in the sense, kx
  con_add<-function(cons,is,ts,kcoef,knum){
    return(c(cons,list(sprintf(formula,is,ts,kcoef,kdel[knum]))))
  }
  
  #the states of chromosome number(y) = 1 or 2, which have limited directions
  cons<-list()
  cons<-con_add(cons,3,4,2,3) #q[3,4]<- 2*k3
  cons<-con_add(cons,4,3,1,4) #q[4,3]<- k4
  cons<-con_add(cons,4,5,1,3) #q[4,5]<- k3
  cons<-con_add(cons,4,6,1,2) #q[4,6]<- k2
  cons<-con_add(cons,5,4,2,4) #q[5,4]<- 2*k4
  cons<-con_add(cons,5,7,2,2) #q[5,7]<- 2*k2
  target.i<-c(tcal(snum,3,4),tcal(snum,4,3),tcal(snum,4,5),tcal(snum,4,6),tcal(snum,5,4),tcal(snum,5,7))
  
  #the states of chromosome number(y) = 3 to (limit-1)
  for(d in 3:(chrmax-1)){
    
    # the states of chromosome number(y) = arm number(x), which have two directions
    s<-scal(d,d)
    down<-scal(d-1,d)
    cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1
    cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
    target.i<-c(target.i,tcal(snum,s,down),tcal(snum,s,s+1))
    
    # the states of y < x < 2*y, which have four directions
    for(j in (d+1):(2*d-2)){
      s<-scal(d,j)
      up<-scal(d+1,j)
      down<-scal(d-1,j)
      anum<-2*d-j
      mnum<-j-d
      cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
      cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
      cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
      cons<-con_add(cons,s,up,mnum,2)#q[k,up]<-mnum*k2
      target.i<-c(target.i,tcal(snum,s,down),tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up))
    }
    
    # the state of x = 2*y-1, which have three directions
    s<-scal(d,2*d-1)
    up<-scal(d+1,2*d-1)
    cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
    cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
    cons<-con_add(cons,s,up,d-1,2)#q[k,up]<-(d-1)*k2
    target.i<-c(target.i,tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up))
    
    # the state of x = 2*y, which have two directions
    s<-scal(d,2*d)
    up<-scal(d+1,2*d-1)
    cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
    cons<-con_add(cons,s,up,d,2)#q[k,up]<-d*k2
    target.i<-c(target.i,tcal(snum,s,s-1),tcal(snum,s,up))
    
  }
  
  # the states of y = limit
  d<-chrmax
  
  # the state of y = x = limit, which has two directions
  s<-scal(d,d)
  down<-scal(d-1,d)
  cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1
  cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
  target.i<-c(target.i,tcal(snum,s,down),tcal(snum,s,s+1))
  
  # the states of y = limit < x < 2y, which have three directions
  for(j in (d+1):(2*d-2)){
    s<-scal(d,j)
    down<-scal(d-1,j)
    anum<-2*d-j
    mnum<-j-d
    cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
    cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
    cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
    target.i<-c(target.i,tcal(snum,s,down),tcal(snum,s,s-1),tcal(snum,s,s+1))
  }

  # the state of x = 2y-1 = 2*limit-1, which has two directions
  s<-scal(d,2*d-1)
  cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
  cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
  target.i<-c(target.i,tcal(snum,s,s-1),tcal(snum,s,s+1))
  
  # the state of x = 2y = 2*limit, which has one direction
  s<-scal(d,2*d)
  cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
  target.i<-c(target.i,tcal(snum,s,s-1))
  
  return(list(cons,target.i))
}


## For MuSSE, Function, get_cons_and_target_musse_null_ec
### M0 model, 6 parameters
### Constraints of the model are expressed as formulae by this function.
### This function also output target.i indicating index of target parameters.
### lambda and mu express speciation and extinction rates.
### all labda or all mu are assumed as constant as lambda001 or mu001, respectively.
### q003002,q002003,q001002 and q002001 are free parameters,
### which corresponding to k1, k2, k3 and k4, respectively.
### chrmax, upper limit of chromosome number

get_cons_and_target_musse_null_ec<-function(chrmax){

  # state number
  snum<-(chrmax+1)*(chrmax+2)/2-1
  
  #In diversitree, the transition is written as q001003 to show transition from state1 to state3 for example.
  #The speciation and extinction rate are also expressed like lambda001 and mu001 for example.
  #The digit of the state number is depending on the total number of the state
  #ord and ord2 shows this digit number.
  ord<-as.integer(log10(snum))+1
  ord2<-as.character(sprintf(""%02d"",ord))
  
  #state 1 is given as the representative state for speciation and extinction rate of all states
  all_state<-2:snum
  #This formula represents ""lambda%03~lambdad%03"" for example.
  formula<-paste(""lambda%"",ord2,""d~lambda%"",ord2,""d"",sep="""")
  #all constraint formulae for lambda are put in the vector
  cons<-as.list(sprintf(formula,all_state,1))
  
  #This formula represents ""mu%03~mu%03"" for example.
  formula<-paste(""mu%"",ord2,""d~mu%"",ord2,""d"",sep="""")
  #all constraint formulae for mu are put in the vector
  cons<-c(cons,as.list(sprintf(formula,all_state,1)))
  
  #target.i for lambda and mu
  target.i<-1:(snum*2)
  target.i<-target.i[-c(1,(snum+1))]
  
  #formula, that represents ""q%03d%03d~%d*%s"" for example.
  #this is used in sprintf
  formula<-paste(""q%"",ord2,""d%"",ord2,""d~%d*%s"",sep="""")
  
  #kdel, that gives the transition rate corresponding to 4 parameters
  kdel<-c(qwrite(ord,3,2),qwrite(ord,2,3),qwrite(ord,1,2),qwrite(ord,2,1))
  
  #con_add, express and add constraint formulae to the given cons vector
  #is, initial state
  #ts, transited state
  #kcoef, coefficient multiplied to kx
  #knum, the index of parameters in the sense, kx
  con_add<-function(cons,is,ts,kcoef,knum){
    return(c(cons,list(sprintf(formula,is,ts,kcoef,kdel[knum]))))
  }
  
  #states of y<= 2
  cons<-con_add(cons,3,4,2,3) #q[3,4]<- 2*k3
  cons<-con_add(cons,4,3,1,4) #q[4,3]<- k4
  cons<-con_add(cons,4,5,1,3) #q[4,5]<- k3
  cons<-con_add(cons,4,6,1,2) #q[4,6]<- k2
  cons<-con_add(cons,5,4,2,4) #q[5,4]<- 2*k4
  cons<-con_add(cons,5,7,2,2) #q[5,7]<- 2*k2
  
  #targets has the same numbering as target.i in Mk-n. The number is shifted later.
  targets<-c(tcal(snum,3,4),tcal(snum,4,3),tcal(snum,4,5),tcal(snum,4,6),tcal(snum,5,4),tcal(snum,5,7))

  #states of 2<y<limit
  for(d in 3:(chrmax-1)){
    
    #states of y = x, which have two directions
    s<-scal(d,d)
    down<-scal(d-1,d)
    cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1
    cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
    targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s+1))
    
    #states of y<x<2y, which have four direcction
    for(j in (d+1):(2*d-2)){
      s<-scal(d,j)
      up<-scal(d+1,j)
      down<-scal(d-1,j)
      anum<-2*d-j
      mnum<-j-d
      cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
      cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
      cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
      cons<-con_add(cons,s,up,mnum,2)#q[k,up]<-mnum*k2
      targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up))
    }
    
    #states of x=2y-1, which have three directions
    s<-scal(d,2*d-1)
    up<-scal(d+1,2*d-1)
    cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
    cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
    cons<-con_add(cons,s,up,d-1,2)#q[k,up]<-(d-1)*k2
    targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up))
    
    #states of x=2y, which have two directions
    s<-scal(d,2*d)
    up<-scal(d+1,2*d-1)
    cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
    cons<-con_add(cons,s,up,d,2)#q[k,up]<-d*k2
    targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,up))
    
  }
  
  #states of y=limit
  d<-chrmax
  
  #x=y, two directions
  s<-scal(d,d)
  down<-scal(d-1,d)
  cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1
  cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
  targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s+1))
  
  #y<x<2y, three directions
  for(j in (d+1):(2*d-2)){
    s<-scal(d,j)
    down<-scal(d-1,j)
    anum<-2*d-j
    mnum<-j-d
    cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
    cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
    cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
    targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s-1),tcal(snum,s,s+1))
  }
  
  #x=2y-1, two directions
  s<-scal(d,2*d-1)
  cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
  cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
  targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,s+1))
  
  #x=2y, one direction
  s<-scal(d,2*d)
  cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
  targets<-c(targets,tcal(snum,s,s-1))
  
  #target.i is combined with targets (those number are shifted)
  target.i<-c(target.i,(targets+((snum)*2)))
  
  return(list(cons,target.i))
}


## For MuSSE, Function, get_cons_and_target_musse_null_Ki1_ec
### M1 model, 5 parameters
### For null hypothesis with k3 = k4.
### One constraint is added q002001 ~ q001002 to get_cons_and_target_musse_null_ec.
### chrmax, upper limit of chromosome number

get_cons_and_target_musse_null_Ki1_ec<-function(chrmax){
  
  snum<-(chrmax+1)*(chrmax+2)/2-1
  all_state<-2:snum
  ord<-as.integer(log10(snum))+1
  ord2<-as.character(sprintf(""%02d"",ord))
  
  formula<-paste(""lambda%"",ord2,""d~lambda%"",ord2,""d"",sep="""")
  cons<-as.list(sprintf(formula,all_state,1))
  formula<-paste(""mu%"",ord2,""d~mu%"",ord2,""d"",sep="""")
  cons<-c(cons,as.list(sprintf(formula,all_state,1)))
  target.i<-1:(snum*2)
  target.i<-target.i[-c(1,(snum+1))]
  
  formula<-paste(""q%"",ord2,""d%"",ord2,""d~%d*%s"",sep="""")
  
  #for 5par k4 is set as the same transition as k3
  kdel<-c(qwrite(ord,3,2),qwrite(ord,2,3),qwrite(ord,1,2),qwrite(ord,1,2)) #changed for 5 par
  con_add<-function(cons,is,ts,kcoef,knum){
    return(c(cons,list(sprintf(formula,is,ts,kcoef,kdel[knum]))))
  }
  
  cons<-con_add(cons,2,1,1,3) #q[2,1]<- q[1,2]=k3 newly added for 5 par
  cons<-con_add(cons,3,4,2,3) #q[3,4]<- 2*k3
  cons<-con_add(cons,4,3,1,4) #q[4,3]<- k4
  cons<-con_add(cons,4,5,1,3) #q[4,5]<- k3
  cons<-con_add(cons,4,6,1,2) #q[4,6]<- k2
  cons<-con_add(cons,5,4,2,4) #q[5,4]<- 2*k4
  cons<-con_add(cons,5,7,2,2) #q[5,7]<- 2*k2
  targets<-c(tcal(snum,2,1),tcal(snum,3,4),tcal(snum,4,3),tcal(snum,4,5),tcal(snum,4,6),tcal(snum,5,4),tcal(snum,5,7)) #changed for 5 par
  for(d in 3:(chrmax-1)){
    
    s<-scal(d,d)
    down<-scal(d-1,d)
    cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)/2*k1
    cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
    targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s+1))
    
    for(j in (d+1):(2*d-2)){
      s<-scal(d,j)
      up<-scal(d+1,j)
      down<-scal(d-1,j)
      anum<-2*d-j
      mnum<-j-d
      cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
      cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
      cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
      cons<-con_add(cons,s,up,mnum,2)#q[k,up]<-mnum*k2
      targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up))
    }
    
    s<-scal(d,2*d-1)
    up<-scal(d+1,2*d-1)
    cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
    cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
    cons<-con_add(cons,s,up,d-1,2)#q[k,up]<-(d-1)*k2
    targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up))
    
    s<-scal(d,2*d)
    up<-scal(d+1,2*d-1)
    cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
    cons<-con_add(cons,s,up,d,2)#q[k,up]<-d*k2
    targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,up))
    
  }
  
  d<-chrmax
  
  s<-scal(d,d)
  down<-scal(d-1,d)
  cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1
  cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
  targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s+1))
  
  for(j in (d+1):(2*d-2)){
    s<-scal(d,j)
    down<-scal(d-1,j)
    anum<-2*d-j
    mnum<-j-d
    cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
    cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
    cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
    targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s-1),tcal(snum,s,s+1))
  }
  
  s<-scal(d,2*d-1)
  cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
  cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
  targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,s+1))
  
  s<-scal(d,2*d)
  cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
  targets<-c(targets,tcal(snum,s,s-1))
  target.i<-c(target.i,(targets+((snum)*2)))
  
  return(list(cons,target.i))
}


## For MuSSE, Function, get_cons_and_target_musse_testSE_ec
### M2 model, 8 parameters
### Almost same as get_cons_and_target_musse_null_ec.
### But two speciation and two extinction rates were given.
### sp_state specifies states have different rates from the other states.
### This function is specific to the test to know two groups of states have different lambda and mu.
### chrmax, upper limit of chromosome number
### sp_state is the state numbers for different speciation rate (calculated with scal)

get_cons_and_target_musse_testSE_ec<-function(chrmax,sp_state){
  
  snum<-(chrmax+1)*(chrmax+2)/2-1
  
  #other_state is defined as the number of targetted other state from sp_state
  other_state<-1:snum
  other_state<-other_state[-sp_state]
  
  #repstate represents the representative state for the free parameter of the other states.
  repstate<-other_state[1]
  other_state<-other_state[-1]
  
  ord<-as.integer(log10(snum))+1
  ord2<-as.character(sprintf(""%02d"",ord))
  formula<-paste(""lambda%"",ord2,""d~lambda%"",ord2,""d"",sep="""")
  cons<-as.list(sprintf(formula,other_state,repstate))
  formula<-paste(""mu%"",ord2,""d~mu%"",ord2,""d"",sep="""")
  cons<-c(cons,as.list(sprintf(formula,other_state,repstate)))
  
  #when sp_state has more than one state 
  #the constraints within sp_state are made.
  if(length(sp_state)>1){
    f_sp_state<-sp_state[1]
    l_sp_state<-sp_state[-1]
    formula<-paste(""lambda%"",ord2,""d~lambda%"",ord2,""d"",sep="""")
    cons<-append(cons,as.list(sprintf(formula,l_sp_state,f_sp_state)),l_sp_state-3)
    formula<-paste(""mu%"",ord2,""d~mu%"",ord2,""d"",sep="""")
    cons<-append(cons,as.list(sprintf(formula,l_sp_state,f_sp_state)),snum+l_sp_state-5)
  }
  
  target.i<-1:(snum*2)
  target.i<-target.i[-c(repstate,sp_state[1],(snum+repstate),(snum+sp_state[1]))]
  
  formula<-paste(""q%"",ord2,""d%"",ord2,""d~%d*%s"",sep="""")
  kdel<-c(qwrite(ord,3,2),qwrite(ord,2,3),qwrite(ord,1,2),qwrite(ord,2,1))
  con_add<-function(cons,is,ts,kcoef,knum){
    return(c(cons,list(sprintf(formula,is,ts,kcoef,kdel[knum]))))
  }
  
  cons<-con_add(cons,3,4,2,3) #q[3,4]<- 2*k3
  cons<-con_add(cons,4,3,1,4) #q[4,3]<- k4
  cons<-con_add(cons,4,5,1,3) #q[4,5]<- k3
  cons<-con_add(cons,4,6,1,2) #q[4,6]<- k2
  cons<-con_add(cons,5,4,2,4) #q[5,4]<- 2*k4
  cons<-con_add(cons,5,7,2,2) #q[5,7]<- 2*k2
  targets<-c(tcal(snum,3,4),tcal(snum,4,3),tcal(snum,4,5),tcal(snum,4,6),tcal(snum,5,4),tcal(snum,5,7))
  for(d in 3:(chrmax-1)){
    
    s<-scal(d,d)
    down<-scal(d-1,d)
    cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1
    cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
    targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s+1))
    
    for(j in (d+1):(2*d-2)){
      s<-scal(d,j)
      up<-scal(d+1,j)
      down<-scal(d-1,j)
      anum<-2*d-j
      mnum<-j-d
      cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
      cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
      cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
      cons<-con_add(cons,s,up,mnum,2)#q[k,up]<-mnum*k2
      targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up))
    }
    
    s<-scal(d,2*d-1)
    up<-scal(d+1,2*d-1)
    cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
    cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
    cons<-con_add(cons,s,up,d-1,2)#q[k,up]<-(d-1)*k2
    targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up))
    
    s<-scal(d,2*d)
    up<-scal(d+1,2*d-1)
    cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
    cons<-con_add(cons,s,up,d,2)#q[k,up]<-d*k2
    targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,up))
    
  }
  
  d<-chrmax
  
  s<-scal(d,d)
  down<-scal(d-1,d)
  cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1
  cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
  targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s+1))
  
  for(j in (d+1):(2*d-2)){
    s<-scal(d,j)
    down<-scal(d-1,j)
    anum<-2*d-j
    mnum<-j-d
    cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
    cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
    cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
    targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s-1),tcal(snum,s,s+1))
    
  }
  
  s<-scal(d,2*d-1)
  cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
  cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
  targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,s+1))
  
  s<-scal(d,2*d)
  cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
  targets<-c(targets,tcal(snum,s,s-1))
  
  ## The target.i from snum*2+1 are for the transition rate.
  
  target.i<-c(target.i,(targets+((snum)*2)))
  
  return(list(cons,target.i))
}


## Function, get_cons_and_target_musse_testSE_Ki1_ec
### M3 model, 7 parameters
### The constraint, Ki=1 is given to get_cons_and_target_musse_testSE_ec
### chrmax, upper limit of chromosome number
### sp_state is the state numbers for different speciation rate (calculated with scal)

get_cons_and_target_musse_testSE_Ki1_ec<-function(chrmax,sp_state){
  
  snum<-(chrmax+1)*(chrmax+2)/2-1
  other_state<-1:snum
  other_state<-other_state[-sp_state]
  repstate<-other_state[1]
  ord<-as.integer(log10(snum))+1
  ord2<-as.character(sprintf(""%02d"",ord))
  
  formula<-paste(""lambda%"",ord2,""d~lambda%"",ord2,""d"",sep="""")
  cons<-as.list(sprintf(formula,other_state,repstate))
  formula<-paste(""mu%"",ord2,""d~mu%"",ord2,""d"",sep="""")
  cons<-c(cons,as.list(sprintf(formula,other_state,repstate)))
  
  if(length(sp_state)>1){
    f_sp_state<-sp_state[1]
    l_sp_state<-sp_state[-1]
    formula<-paste(""lambda%"",ord2,""d~lambda%"",ord2,""d"",sep="""")
    cons<-append(cons,as.list(sprintf(formula,l_sp_state,f_sp_state)),l_sp_state-3)
    formula<-paste(""mu%"",ord2,""d~mu%"",ord2,""d"",sep="""")
    cons<-append(cons,as.list(sprintf(formula,l_sp_state,f_sp_state)),snum+l_sp_state-5)
  }

  target.i<-1:(snum*2)
  target.i<-target.i[-c(repstate,sp_state[1],(snum+repstate),(snum+sp_state[1]))]
  
  formula<-paste(""q%"",ord2,""d%"",ord2,""d~%d*%s"",sep="""")
  
  #k4 is set as the same transition as k3
  kdel<-c(qwrite(ord,3,2),qwrite(ord,2,3),qwrite(ord,1,2),qwrite(ord,1,2)) #for Ki=1
  con_add<-function(cons,is,ts,kcoef,knum){
    return(c(cons,list(sprintf(formula,is,ts,kcoef,kdel[knum]))))
  }
  
  cons<-con_add(cons,2,1,1,3) #for Ki=1
  cons<-con_add(cons,3,4,2,3) #q[3,4]<- 2*k3
  cons<-con_add(cons,4,3,1,4) #q[4,3]<- k4
  cons<-con_add(cons,4,5,1,3) #q[4,5]<- k3
  cons<-con_add(cons,4,6,1,2) #q[4,6]<- k2
  cons<-con_add(cons,5,4,2,4) #q[5,4]<- 2*k4
  cons<-con_add(cons,5,7,2,2) #q[5,7]<- 2*k2
  targets<-c(tcal(snum,2,1),tcal(snum,3,4),tcal(snum,4,3),tcal(snum,4,5),tcal(snum,4,6),tcal(snum,5,4),tcal(snum,5,7)) #for Ki=1
  for(d in 3:(chrmax-1)){
    
    s<-scal(d,d)
    down<-scal(d-1,d)
    cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1
    cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
    targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s+1))
    
    for(j in (d+1):(2*d-2)){
      s<-scal(d,j)
      up<-scal(d+1,j)
      down<-scal(d-1,j)
      anum<-2*d-j
      mnum<-j-d
      cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
      cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
      cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
      cons<-con_add(cons,s,up,mnum,2)#q[k,up]<-mnum*k2
      targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up))
    }
    
    s<-scal(d,2*d-1)
    up<-scal(d+1,2*d-1)
    cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
    cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
    cons<-con_add(cons,s,up,d-1,2)#q[k,up]<-(d-1)*k2
    targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up))
    
    s<-scal(d,2*d)
    up<-scal(d+1,2*d-1)
    cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
    cons<-con_add(cons,s,up,d,2)#q[k,up]<-d*k2
    targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,up))
    
  }
  
  d<-chrmax
  
  s<-scal(d,d)
  down<-scal(d-1,d)
  cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1
  cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
  targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s+1))
  
  for(j in (d+1):(2*d-2)){
    s<-scal(d,j)
    down<-scal(d-1,j)
    anum<-2*d-j
    mnum<-j-d
    cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
    cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
    cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
    targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s-1),tcal(snum,s,s+1))
    
  }
  
  s<-scal(d,2*d-1)
  cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
  cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
  targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,s+1))
  
  s<-scal(d,2*d)
  cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
  targets<-c(targets,tcal(snum,s,s-1))
  
  ## The target.i from snum*2+1 are for the transition rate.
  
  target.i<-c(target.i,(targets+((snum)*2)))
  
  return(list(cons,target.i))
}


## Function, get_cons_and_target_musse_polyp_ec
### M4 model, 7 parameters
### Almost same as get_cons_and_target_musse_null_ec 
### but a new transition for polyploidization was given.
### Only states with half chromosome number of the limit or less
### have this transition as doubling chromosome and arm number.
### The new transition, k5, is given as q001003.(i.e.(1,1)=>(2,2))
### chrmax, upper limit of chromosome number which should be more than 3.

get_cons_and_target_musse_polyp_ec<-function(chrmax){
  
  snum<-(chrmax+1)*(chrmax+2)/2-1
  all_state<-2:snum
  ord<-as.integer(log10(snum))+1
  ord2<-as.character(sprintf(""%02d"",ord))
  
  formula<-paste(""lambda%"",ord2,""d~lambda%"",ord2,""d"",sep="""")
  cons<-as.list(sprintf(formula,all_state,1))
  formula<-paste(""mu%"",ord2,""d~mu%"",ord2,""d"",sep="""")
  cons<-c(cons,as.list(sprintf(formula,all_state,1)))
  target.i<-1:(snum*2)
  target.i<-target.i[-c(1,(snum+1))]
  
  formula<-paste(""q%"",ord2,""d%"",ord2,""d~%d*%s"",sep="""")
  
  #k5 is given for the new transition for polyploidization as q001003
  kdel<-c(qwrite(ord,3,2),qwrite(ord,2,3),qwrite(ord,1,2),qwrite(ord,2,1),qwrite(ord,1,3))
  con_add<-function(cons,is,ts,kcoef,knum){
    return(c(cons,list(sprintf(formula,is,ts,kcoef,kdel[knum]))))
  }
  
  cons<-con_add(cons,2,5,1,5) #q[2,5]<- 1*k5
  cons<-con_add(cons,3,4,2,3) #q[3,4]<- 2*k3
  cons<-con_add(cons,3,10,1,5) #q[3,10]<- 1*k5
  cons<-con_add(cons,4,3,1,4) #q[4,3]<- k4
  cons<-con_add(cons,4,5,1,3) #q[4,5]<- k3
  cons<-con_add(cons,4,6,1,2) #q[4,6]<- k2
  cons<-con_add(cons,4,12,1,5) #q[4,12]<- 1*k5
  cons<-con_add(cons,5,4,2,4) #q[5,4]<- 2*k4
  cons<-con_add(cons,5,7,2,2) #q[5,7]<- 2*k2
  cons<-con_add(cons,5,14,1,5) #q[5,14]<- 1*k5
  targets<-c(tcal(snum,2,5),tcal(snum,3,4),tcal(snum,3,10),tcal(snum,4,3),
             tcal(snum,4,5),tcal(snum,4,6),tcal(snum,4,12),tcal(snum,5,4),
             tcal(snum,5,7),tcal(snum,5,14))
  half<-trunc(chrmax/2)
  if(half>=3){

    #chromosome number with 5 transitions (i.e. polyploidization happens)
    for(d in 3:half){
      
      s<-scal(d,d)
      down<-scal(d-1,d)
      poly<-scal(2*d,2*d)
      cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1
      cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
      cons<-con_add(cons,s,poly,1,5) #q[d*(d+1)/2,2*d*(2*d+1)/2]<-1*k5
      targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s+1),tcal(snum,s,poly))
      
      for(j in (d+1):(2*d-2)){
        s<-scal(d,j)
        up<-scal(d+1,j)
        down<-scal(d-1,j)
        poly<-scal(2*d,2*j)
        anum<-2*d-j
        mnum<-j-d
        cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
        cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
        cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
        cons<-con_add(cons,s,up,mnum,2)#q[k,up]<-mnum*k2
        cons<-con_add(cons,s,poly,1,5)
        targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s-1),
                   tcal(snum,s,s+1),tcal(snum,s,up),tcal(snum,s,poly))
      }
      
      s<-scal(d,2*d-1)
      up<-scal(d+1,2*d-1)
      poly<-scal(2*d,4*d-2)
      cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
      cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
      cons<-con_add(cons,s,up,d-1,2)#q[k,up]<-(d-1)*k2
      cons<-con_add(cons,s,poly,1,5)
      targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up),tcal(snum,s,poly))
      
      s<-scal(d,2*d)
      up<-scal(d+1,2*d-1)
      poly<-scal(2*d,4*d)
      cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
      cons<-con_add(cons,s,up,d,2)#q[k,up]<-d*k2
      cons<-con_add(cons,s,poly,1,5)
      targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,up),tcal(snum,s,poly))
    }
    nextchr<-half+1
  }else{
    nextchr<-3
  }
  
  #chromosome number with 4 or less transitions
  for(d in nextchr:(chrmax-1)){
    
    s<-scal(d,d)
    down<-scal(d-1,d)
    cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1
    cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
    targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s+1))
    
    for(j in (d+1):(2*d-2)){
      s<-scal(d,j)
      up<-scal(d+1,j)
      down<-scal(d-1,j)
      anum<-2*d-j
      mnum<-j-d
      cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
      cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
      cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
      cons<-con_add(cons,s,up,mnum,2)#q[k,up]<-mnum*k2
      targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up))
    }
    
    s<-scal(d,2*d-1)
    up<-scal(d+1,2*d-1)
    cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
    cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
    cons<-con_add(cons,s,up,d-1,2)#q[k,up]<-(d-1)*k2
    targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,s+1),tcal(snum,s,up))
    
    s<-scal(d,2*d)
    up<-scal(d+1,2*d-1)
    cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
    cons<-con_add(cons,s,up,d,2)#q[k,up]<-d*k2
    targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,up))
    
  }
  
  #chromosome number with 3 or less transitions
  d<-chrmax
  
  s<-scal(d,d)
  down<-scal(d-1,d)
  cons<-con_add(cons,s,down,d*(d-1)/2,1)#q[d*(d+1)/2,d*(d-1)/2+1]<-d*(d-1)*k1
  cons<-con_add(cons,s,s+1,d,3) #q[d*(d+1)/2,d*(d+1)/2+1]<-d*k3
  targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s+1))
  
  for(j in (d+1):(2*d-2)){
    s<-scal(d,j)
    down<-scal(d-1,j)
    anum<-2*d-j
    mnum<-j-d
    cons<-con_add(cons,s,down,anum*(anum-1)/2,1)#q[k,down]<-anum*(anum-1)*k1
    cons<-con_add(cons,s,s-1,mnum,4) #q[k,k-1]<-mnum*k4
    cons<-con_add(cons,s,s+1,anum,3) #q[k,k+1]<-anum*k3
    targets<-c(targets,tcal(snum,s,down),tcal(snum,s,s-1),tcal(snum,s,s+1))
  }
  
  s<-scal(d,2*d-1)
  cons<-con_add(cons,s,s-1,d-1,4) #q[k,k-1]<-(d-1)*k4
  cons<-con_add(cons,s,s+1,1,3) #q[k,k+1]<-k3
  targets<-c(targets,tcal(snum,s,s-1),tcal(snum,s,s+1))
  
  s<-scal(d,2*d)
  cons<-con_add(cons,s,s-1,d,4) #q[k,k-1]<-d*k4
  targets<-c(targets,tcal(snum,s,s-1))
  target.i<-c(target.i,(targets+((snum)*2)))
  
  return(list(cons,target.i))
}


## For constrain.kt (bellow), function get_exp
### Returns selected formulae from the list in lapply

get_exp<-function(form,ind){
  return(as.formula(form)[[ind]])
}

## Function, constrain.kt
### This function replace ""constrain{diversitree}"" function that produced errors in our analysis.
### You need to specify index of free parameters and target parameters as free.i and targt.i.
### free.i can be determined using tcal (see the example of running scripts)
### target.i is given by function of get_cons_and_target_~
### The other transition than free.i and target.i were set as 0.

constrain.kt <-function(f,formulae=NULL,free.i,target.i,names=argnames(f)){
  
  final<-names[free.i]
  rels<-lapply(formulae,get_exp,3)
  names(rels)<-as.character(lapply(formulae,get_exp,2))
  target.names<-names[target.i]
  target.i <- target.i[match(names(rels),target.names)]
  pars.out=rep(0,length=length(names))
  names(pars.out)<-names #really needed?
  
  g<- function(pars, ...,pars.only=FALSE){
    pars.out[free.i]<-pars
    e<-structure(as.list(pars),names=final)
    pars.out[target.i]<-unlist(lapply(rels,eval,e))
    if(pars.only)
      pars.out
    else
      f(pars.out, ...)
  }
  class (g) <- c(""constrained"",class(f))
  attr(g, ""argnames"")<-final
  attr(g, ""formulae"")<-formulae #substitution of formulae attr is not precise 
  attr(g, ""func"") <- f
  g
}
",4,"karyotype evolution, chromosome number, arm numbers, centric fusion, fission, pericentric inversion, probabilistic model, karyograph, taxonomic groups, Eurypterygii, Otophysi, maximum likelihood estimation"
"Data to accompany dissertation: Geographic, Cultural, and Ecological Correlations with Indigenous Language Vitality in North America","Text files:readme_general.txt contains a brief description of files included.readme_modeldata.txt contains a metadata description of model_data.csv.readme_languageslandNorthAmerica.txt contains a metadata description of Languages_land_NorthAmerica.csv.readme_languagerevitalizationdatabase.txt contains a metadata description of Language_revitalization_database.csv.CSV files:Languages_land_NorthAmerica.csv is a version of the Languages of Government-Recognized Native Land Areas in the Continental United States database. It includes data from the US Census 2017 TIGER/Line AIANNH shapefile with one row per Native land area and additional columns for associated information that was coded and calculated for this dissertation as discussed in Section 3.3.1.Language_revitalization_database.csv is the Language Revitalization Database. It contains the master language list used for this dissertation and columns created while coding data for the language revitalization variable, as discussed in Section 3.3.2.model_data.csv contains data for all variables used in the analysis and is the .csv file needed to run LanguageVitalityModels.R.R scripts:LanguageVitalityModels.R is the R script for the main part of the dissertation analysis.","['## Modeling Language Vitality using Social and Environmental Predictors\n## Kirsten Helgeson with Dr. Bradley Rentz and Dr. Gary Holton\n## Created using R 3.5.0, R 3.6.1, and R 4.1.3 starting in November 2018, adapted through April 2022\n\n# Load packages\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Set working directory\nsetwd(""~/Desktop/Supplementary Materials"")\n\ndata.clean <- read.csv (""model_data.csv"")\n\noptions(scipen = 999)  # This turns off scientific notation.\n\n#### Part 1: Setting up data ####\n\n## 1a. Language vitality ##\n\n# Make language status levels into ordinal data\ndata.clean$Vitality_rank <- factor(data.clean$Vitality_rank, levels=c(""0"",""1"",""2"",""3"",""4"",""5"",""6"",""7""), ordered=T)\nplot(data.clean$Vitality_rank)\nsummary(data.clean$Vitality_rank)\n\n## 1b. Subsistence strategy ##\n\n# Set subsistence strategy to nominal/categorical (rather than interval) data\ndata.clean$Subsistence_cat <- factor(data.clean$Subsistence_cat, levels=c(""Agriculture"",""Hunting"",""Gathering"",""Fishing"",""Mixed""), ordered=F) \nplot(data.clean$Subsistence_cat)\nsummary(data.clean$Subsistence_cat)\n\n## 1c. Land area ##\n\nhist(data.clean$Area_sum, breaks = 50, main = ""Land Area at Time of European Contact"", xlab = ""Area"", ylab = ""Frequency"")\nsummary(data.clean$Area_sum)\n\n## Square root of land area\ndata.clean$Area_sum_sqrt <- sqrt(data.clean$Area_sum)\nhist(data.clean$Area_sum_sqrt, breaks = 50, main = ""Land Area at Time of European Contact"", xlab = ""Area (sqrt)"", ylab = ""Frequency"")\n\n## Log of land area\ndata.clean$Area_sum_log <- log(data.clean$Area_sum)\nhist(data.clean$Area_sum_log, breaks = 50, main = ""Land Area at Time of European Contact"", xlab = ""Log area"", ylab = ""Frequency"")\n\n## 1d. Elevation range ##\n\nhist(data.clean$Elevation_range, breaks = 50, main = ""Elevation range"", xlab = ""Elevation (meters)"", ylab = ""Frequency"")\nsummary(data.clean$Elevation_range)\n\n## Square root of elevation range\ndata.clean$Elevation_range_sqrt <- sqrt(data.clean$Elevation_range)\nhist(data.clean$Elevation_range_sqrt, breaks = 50, main = ""Elevation range"", xlab = ""Square root of elevation (meters)"", ylab = ""Frequency"")\n\n## Log of elevation range\ndata.clean$Elevation_range_log <- log(data.clean$Elevation_range)\nhist(data.clean$Elevation_range_log, breaks = 50, main = ""Elevation range"", xlab = ""Log Elevation"", ylab = ""Frequency"")\n\n## 1e. Displacement ##\n\n# Set historical displacement to categorical and reference level to ""none.""\ndata.clean$Displacement_cat <- factor(data.clean$Displacement_cat, levels=c(""none"",""outside"",""inside"",""both""), ordered=F) # Change to ""ordered=T"" for ordinal data.\nsummary(data.clean$Displacement_cat)\nplot(data.clean$Displacement_cat)\n\n## 1f. Highways ##\n\n## Highway density\n\n# Explore data\nsummary(data.clean$Hwy_density)\nhist(data.clean$Hwy_density, breaks = 50, main = ""Highway density"", xlab = ""Density"", ylab = ""Frequency"")\n\n# Scaled highway density\ndata.clean$Hwy_density_scaled <- scale(data.clean$Hwy_density, center=F, scale=T)\nhist(data.clean$Hwy_density_scaled, breaks = 30)\n\n# Square root of highway density\ndata.clean$Hwy_density_sqrt <- sqrt(data.clean$Hwy_density)\nhist(data.clean$Hwy_density_sqrt, breaks = 50, main = ""Highway density"", xlab = ""Square root of density"", ylab = ""Frequency"")\nsummary(data.clean$Hwy_density_sqrt)\n\n# Log of highway density\ndata.clean$Hwy_density_log <- log(data.clean$Hwy_density)\nhist(data.clean$Hwy_density_log, breaks = 50, main = ""Highway density"", xlab = ""Log of density"", ylab = ""Frequency"")\n## The problem with log transforming is that most highway density values are zero, so there are many values with infinitely negative values on the log transformation scale.\n\n# Adding a small number\ndata.clean$Hwy_density_new <- (data.clean$Hwy_density+1)\ndata.clean$Hwy_density_new_log <- log(data.clean$Hwy_density_new)\nhist(data.clean$Hwy_density_new_log, breaks = 100)\n\ndata.clean$Hwy_density_newd <- (data.clean$Hwy_density+0.000001)\ndata.clean$Hwy_density_newd_log <- log(data.clean$Hwy_density_newd)\nhist(data.clean$Hwy_density_newd_log, breaks = 10)\n\n## Highway presence\nsummary(data.clean$Highway)\nplot(data.clean$Highway)\n\n## 1g. Revitalization ##\n\n# Set revitalization to ordinal, ""none"" as reference level.\ndata.clean$Revit_cat <- factor(data.clean$Revit_cat, levels=c(""none"", ""revitalization"", ""education"", ""immersion""), ordered=F) # Change to ""ordered=T"" for ordinal data.\nsummary(data.clean$Revit_cat)\nplot(data.clean$Revit_cat)\n\n# Define variables in R.\nVitality <- data.clean$Vitality_rank\nArea <- data.clean$Area_sum_log\nElevation <- data.clean$Elevation_range_sqrt\nSubsistence <- data.clean$Subsistence_cat\nDisplacement <- data.clean$Displacement_cat\nHighway <- data.clean$Highway\nRevitalization <- data.clean$Revit_cat\n\n#### Part 2: Running models ####\n\nlibrary(ordinal)\nlibrary(sure) # For model diagnostics and checking model fit\nlibrary(sjPlot) # For visualizing results\n\n#### Model 5AESDR ####\n\nModel5AESDR <- clm(Vitality~Area+Elevation+Subsistence+Displacement+Revitalization, data=data.clean)\nsummary(Model5AESDR)\n\nplot_m']",4,"Geographic correlations, cultural correlations, ecological correlations, indigenous language vitality, North America, data, metadata, CSV files, TIGER/Line AIANNH shapefile, US Census 2017, master language list, language revitalization variable,"