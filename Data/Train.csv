Paper,description,content,sentence,label
Integrating omics datasets with the OmicsPLS package,"Background: With the exponential growth in available biomedical data, there is a need for data integration methods that can extract information about relationships between the data sets. However, these data sets might have very different characteristics. For interpretable results, data-specific variation needs to be quantified. For this task, Two-way Orthogonal Partial Least Squares (O2PLS) has been proposed. To facilitate application and development of the methodology, free and open-source software is required. However, this is not the case with O2PLS.Results: We introduce OmicsPLS, an open-source implementation of the O2PLS method in R. It can handle both low- and high-dimensional datasets efficiently. Generic methods for inspecting and visualizing results are implemented. Both a standard and faster alternative cross-validation methods are available to determine the number of components. A simulation study shows good performance of OmicsPLS compared to alternatives, in terms of accuracy and CPU runtime. We demonstrate OmicsPLS by integrating genetic and glycomic data.Conclusions: We propose the OmicsPLS R package: a free and open-source implementation of O2PLS for statistical data integration. OmicsPLS is available at https://cran.r-project.org/package=OmicsPLS and can be installed in R via install.packages(""OmicsPLS"").","['set.seed(218627L)\r\nlibrary(OmicsPLS)\r\nlibrary(r.jive)\r\nlibrary(parallel)\r\nlibrary(tidyverse)\r\nlibrary(magrittr)\r\n\r\ncoverW <- function(W0, W){\r\n  W0 <- W0[,apply(crossprod(W0,W)^2,2,which.max)]\r\n  W0 <- W0 %*% sign(crossprod(W0,W)*diag(1,ncol(W)))\r\n  W0\r\n}\r\n\r\nK = 2*4\r\ncl <- makePSOCKcluster(names = rep(\'localhost\',detectCores()), outfile = \'tmp.txt\')\r\nclusterEvalQ(cl, {library(OmicsPLS);library(r.jive); library(tidyverse);library(magrittr)})\r\nclusterExport(cl, varlist = c(""coverW""))\r\n\r\nresults <- parLapply(cl = cl, X = 1:K, fun = function(dummy_i) {\r\n  ## Choose parameters\r\n  p <- 1e2\r\n  q <- 1e2\r\n  N <- 500\r\n  n <- 2\r\n  nx <- 3\r\n  ny <- 1\r\n\r\n  X_loadings <- lapply(1:(n+nx), function(i) rnorm(p))\r\n  Y_loadings <- lapply(1:(n+ny), function(i) rnorm(q))\r\n  X_loadings <- orth(Reduce(cbind, X_loadings)) %*% diag(rep(c(1,10),c(n,nx)))\r\n  # X_loadings[,1:n] %<>% orth\r\n  # X_loadings[,-(1:n)] %<>% orth\r\n  Y_loadings <- orth(Reduce(cbind, Y_loadings)) %*% diag(rep(c(1,10),c(n,ny)))\r\n  # Y_loadings[,1:n] %<>% orth\r\n  # Y_loadings[,-(1:n)] %<>% orth\r\n  \r\n  ## Simulate scores\r\n  X_scores <- matrix(rnorm(N*(n+nx)), N, n+nx)\r\n  B_mat <- diag(1, n)\r\n  Y_jointsc <- X_scores[,1:n] %*% B_mat #*10\r\n  # Y_jointsc <- Y_jointsc + matrix(rnorm(N*n, sd=ssq(Y_jointsc)/N/n/10),N,n)\r\n  Y_scores <- cbind(Y_jointsc, matrix(rnorm(N*(ny)), N, ny))\r\n  \r\n  ## Construct data\r\n  X_sim <- X_scores %*% t(X_loadings)\r\n  Y_sim <- Y_scores %*% t(Y_loadings)\r\n  X_sim <- X_sim + matrix(rnorm(N*p, sd = ssq(X_sim)/N/p/10), N, p)\r\n  Y_sim <- Y_sim + matrix(rnorm(N*q, sd = ssq(X_sim)/N/q/10), N, q)\r\n  X_sim <- scale(X_sim, scale = F)\r\n  Y_sim <- scale(Y_sim, scale = F)\r\n#  X_sim <- X_sim / sqrt(ssq(X_sim)*(N*p+N*q)) * 10 \r\n#  Y_sim <- X_sim / sqrt(ssq(Y_sim)*(N*p+N*q)) * 1\r\n  \r\n  ## O2PLS fit\r\n  time_O2PLS <- \r\n    system.time(\r\n      fit_O2PLS <- o2m(X_sim, Y_sim, \r\n                       n, nx, ny, \r\n                       stripped = TRUE)\r\n    )\r\n  fit_O2PLS$W. %>% coverW(X_loadings[,1:n])%>% crossprod(X_loadings[,1:n])\r\n  ## JIVE fit\r\n  print(time_JIVE <- \r\n    system.time(\r\n      fit_JIVE <- jive(data = list(X=t(X_sim),Y=t(Y_sim)), \r\n                       rankJ = n, rankA = c(nx,ny), \r\n                       method = ""given"", scale = FALSE,\r\n                       showProgress = TRUE, maxiter = 500)\r\n    ))\r\n  # svd(fit_JIVE$joint[[1]],nu=n,nv=0)$u[,1:n] %>% coverW(X_loadings[,1:n])%>% crossprod(X_loadings[,1:n])\r\n  ## Compare loadings\r\n  X_loadings_O2PLS <- cbind(fit_O2PLS$W. %>% coverW(X_loadings[,1:n]), orth(fit_O2PLS$P_Yosc.) %>% coverW(X_loadings[,n+1:nx]))\r\n  X_loadings_JIVE <- cbind(svd(fit_JIVE$joint[[1]],nu=n,nv=0)$u[,1:n]  %>% coverW(X_loadings[,1:n]), \r\n                           svd(fit_JIVE$indiv[[1]],nu=nx,nv=0)$u[,1:nx]  %>% coverW(X_loadings[,n+1:nx]))\r\n  \r\n  Y_loadings_O2PLS <- cbind(fit_O2PLS$C.  %>% coverW(Y_loadings[,1:n]), fit_O2PLS$P_Xosc.)\r\n  Y_loadings_JIVE <- cbind(svd(fit_JIVE$joint[[2]],nu=n,nv=0)$u[,1:n]  %>% coverW(Y_loadings[,1:n]), \r\n                           svd(fit_JIVE$indiv[[2]],nu=ny,nv=0)$u[,1:ny] )\r\n  X_test_stat <- list(\r\n    J_O2PLS = sapply(1:n, function(i) abs(crossprod(X_loadings[,i], orth(X_loadings_O2PLS[,i])))),\r\n    J_JIVE = sapply(1:n, function(i) abs(crossprod(X_loadings[,i], orth(X_loadings_JIVE[,i])))),\r\n    S_O2PLS = sapply(1:nx, function(i) abs(crossprod(X_loadings[,n+i], orth(X_loadings_O2PLS[,n+i])))),\r\n    S_JIVE = sapply(1:nx, function(i) abs(crossprod(X_loadings[,n+i], orth(X_loadings_JIVE[,n+i]))))\r\n  )\r\n  Y_test_stat <- list(\r\n    J_O2PLS = sapply(1:n, function(i) abs(crossprod(Y_loadings[,i], orth(Y_loadings_O2PLS[,i])))),\r\n    J_JIVE = sapply(1:n, function(i) abs(crossprod(Y_loadings[,i], orth(Y_loadings_JIVE[,i])))),\r\n    S_O2PLS = sapply(1:ny, function(i) abs(crossprod(Y_loadings[,n+i], orth(Y_loadings_O2PLS[,n+i])))),\r\n    S_JIVE = sapply(1:ny, function(i) abs(crossprod(Y_loadings[,n+i], orth(Y_loadings_JIVE[,n+i]))))\r\n  )\r\n  \r\n  return(list(X = X_test_stat, \r\n              Y = Y_test_stat, \r\n              time = c(time_O2PLS = time_O2PLS[3], time_JIVE = time_JIVE[3])))\r\n})\r\n\r\nlibrary(ggplot2)\r\nlibrary(reshape2)\r\n# results2 concatenates X and Y\r\n# results3 contains differences\r\n# results4 contains means per component\r\nresults2 <- lapply(results, function(e) c(e$X,e$Y))\r\nresults3 <- data.frame(X.joint = t(sapply(results2, function(e) e[[1]] - e[[2]])))\r\nresults3 <- cbind(results3, X.spec = t(sapply(results2, function(e) e[[3]] - e[[4]])))\r\nresults3 <- cbind(results3, Y.joint = t(sapply(results2, function(e) e[[5]] - e[[6]])))\r\nresults3 <- cbind(results3, Y.spec = (sapply(results2, function(e) e[[7]] - e[[8]])))\r\n#ggplot(data = melt(results2)) + geom_boxplot(aes(x = variable, y = value, col = variable)) + theme_bw()\r\n\r\nresults4 <- data.frame(OmicsPLS_Xjoint = sapply(results2, function(e) mean(e[[1]])))\r\nresults4 <- cbind(results4, R.JIVE_Xjoint = sapply(results2, function(e) mean(e[[2]])))\r\nresults4 <- cbind(results4, OmicsPLS_Xspec = sapply(results2, function(e) mean(e[[3]])))\r\nresults4 <- cbind(results4, R.JIVE_Xspec = sapply(results2, function(e) mean(e[[4]])))\r\nresults4 <- cbind(results4, OmicsPLS_Yjoint = sapply(results2, function(e) mean(e[[5]])))\r\nresults4 <- cbind(results4, R.JIVE_Yjoint = sapply(results2, function(e) mean(e[[6]])))\r\nresults4 <- cbind(results4, OmicsPLS_Yspec = sapply(results2, function(e) mean(e[[7]])))\r\nresults4 <- cbind(results4, R.JIVE_Yspec = sapply(results2, function(e) mean(e[[8]])))\r\nresults4 <- melt(results4)\r\nresults4$Software <- ifelse(grepl(""OmicsPLS"",results4$var), ""OmicsPLS"", ""R.JIVE"")\r\nggplot(data = results4) + \r\n  geom_boxplot(aes(x = sapply(strsplit(as.character(variable),\'_\'), function(e) e[2]), y = value, col = Software), size = 1.75) + \r\n  xlab(NULL) + ylab(""Inner product"") + ggtitle(""Average inner product with true loading vectors"") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(face=\'bold\', hjust=0.5), legend.title=element_text(face=\'bold\')) + \r\n  theme(text = element_text(size = 25))\r\n\r\nsapply(sort(names(table(results4[,1]))), function(e) results4 %>% filter(variable == e) %>% (function(x) c(Median=median(x$val),MAD=mad(x$val))))\r\napply(sapply(results, function(e) e$time), 1, function(x) c(Median=median(x),MAD=mad(x)))\r\n\r\n']","Integrating omics datasets with the OmicsPLS package Background: With the exponential growth in available biomedical data, there is a need for data integration methods that can extract information about relationships between the data sets. However, these data sets might have very different characteristics. For interpretable results, data-specific variation needs to be quantified. For this task, Two-way Orthogonal Partial Least Squares (O2PLS) has been proposed. To facilitate application and development of the methodology, free and open-source software is required. However, this is not the case with O2PLS.Results: We introduce OmicsPLS, an open-source implementation of the O2PLS method in R. It can handle both low- and high-dimensional datasets efficiently. Generic methods for inspecting and visualizing results are implemented. Both a standard and faster alternative cross-validation methods are available to determine the number of components. A simulation study shows good performance of OmicsPLS compared to alternatives, in terms of accuracy and CPU runtime. We demonstrate OmicsPLS by integrating genetic and glycomic data.Conclusions: We propose the OmicsPLS R package: a free and open-source implementation of O2PLS for statistical data integration. OmicsPLS is available at https://cran.r-project.org/package=OmicsPLS and can be installed in R via install.packages(""OmicsPLS"").",0
Guide for library design and bias correction for large-scale transcriptome studies using highly multiplexed RNAseq methods,"Background: Standard RNAseq methods using bulk RNA and recent single-cell RNAseq methods use DNA barcodes to identify samples and cells, and the barcoded cDNAs are pooled into a library pool before high throughput sequencing. In cases of single-cell and low-input RNAseq methods, the library is further amplified by PCR after the pooling. Preparation of hundreds or more samples for a large study often requires multiple library pools. However, sometimes correlation between expression profiles among the libraries is low and batch effect biases make integration of data between library pools difficult.Results: We investigated 166 technical replicates in 14 RNAseq libraries made using the STRT method. The patterns of the library biases differed by genes, and uneven library yields were associated with library biases. The former bias was corrected using the NBGLM-LBC algorithm, which we present in the current study. The latter bias could not be corrected directly, but could be solved by omitting libraries with particularly low yields. A simulation experiment suggested that the library bias correction using NBGLM-LBC requires a consistent sample layout. The NBGLM-LBC correction method was applied to an expression profile for a cohort study of childhood acute respiratory illness, and the library biases were resolved.Conclusions: The R source code for the library bias correction named NBGLM-LBC is available at https://shka.github.io/NBGLM-LBC and https://shka.bitbucket.io/NBGLM-LBC. This method is applicable to correct the library biases in various studies that use highly multiplexed sequencing-based profiling methods with a consistent sample layout with samples to be compared (e.g., ""cases"" and ""controls"") equally distributed in each library.","['if (!file.exists(\'out\')) dir.create(\'out\')\n\n## Before running this script, (i) place LBC-additionalFile2.txt and\n## LBC-additionalFile3.txt to the working directory, and (ii) install\n## preprocessCore, DESeq2 and factoextra packages.\n\n\n## Preparation of a raw read count matrix before correction and\n## normalization (reads.beas.pre).\nreads.beas.pre <-\n    as.matrix(read.table(""LBC-additionalFile3.txt"",\n                         header=T, sep=""\\t"", quote=\'\', row.names=1))\n\n## Preparation of depths (depths.beas) and libraries (libraries.beas)\n## of the BEAS samples.\nsamples.all <- read.table(""LBC-additionalFile2.txt"",\n                          header=T, sep=""\\t"", quote=\'\', row.names=1)\nsamples.beas <- samples.all[which(samples.all$PROJECT == \'BEAS\'), ]\ndepths.beas <- samples.beas[colnames(reads.beas.pre), \'TOTAL_READS\']\nlibraries.beas <- factor(samples.beas[colnames(reads.beas.pre), \'LIBRARY\'])\n\n## A raw read count matrix after the library bias correction by\n## NBGLM-LBC (reads.beas.post).\nsource(""https://shka.github.io/NBGLM-LBC.R"")\nreads.beas.post <-\n    library_bias_correction(reads.beas.pre, depths.beas, libraries.beas)\n\n\n## Plots between depths and raw read counts of CCDC85B and RRM2 genes,\n## before and after the library bias correction (Fig. 2a).\n\ncols.solarized <- c(\'#dc322f\', \'#859900\', \'#268bd2\', \'#b58900\', \'#cb4b16\',\n                    \'#6c71c4\', \'#d33682\', \'#2aa198\', \'#002b36\')\n\nplot_depths_and_counts <-\n    function(gene, reads,\n             depths.log=log(depths.beas), libraries=libraries.beas,\n             cols=cols.solarized, pch=1) {\n        fit <- glm.nb(y ~ x + l, link=log, \n                      data=data.frame(y = as.numeric(reads[gene, ]),\n                                      x = depths.log,\n                                      l = libraries))\n        tmp <- data.frame(x = rep(seq(from = min(depths.log),\n                                      to = max(depths.log),\n                                      length.out = 100), nlevels(libraries)),\n                          l = factor(rep(levels(libraries), each = 100)))\n        tmp <- cbind(tmp, predict(fit, tmp, type=\'link\', se.fit=T))\n        tmp <- within(tmp, {\n            x.org <- exp(x)\n            y <- exp(fit)\n            y.lower <- exp(fit - 1.96*se.fit)\n            y.upper <- exp(fit + 1.96*se.fit)\n        })\n        par(mar=c(2.75, 2.75, .5, .5), mgp=c(1.75, .75, 0))\n        plot(exp(depths.log),\n             reads[gene, ],\n             col=cols[as.numeric(libraries)], pch=pch,\n             xlab=\'\', ylab=\'\', log=\'xy\')\n        for(i in 1:nlevels(libraries)) {\n            l <- levels(libraries)[i]\n            tmp.l <- which(tmp$l == l)\n            tmp.x <- tmp[tmp.l, \'x.org\']\n            tmp.c <- cols[i]\n            lines(tmp.x, tmp[tmp.l, \'y\'], col=tmp.c)\n            polygon(c(tmp.x, rev(tmp.x)),\n                    c(tmp[tmp.l, \'y.upper\'], rev(tmp[tmp.l, \'y.lower\'])),\n                    border=NA,\n                    col=do.call(\'rgb\',\n                                as.list(c(col2rgb(tmp.c)[, 1]/255,\n                                          alpha=.125))))\n        }\n    }\n\npdf(""out/depths_and_counts-beas-CCDC85B-before.pdf"",\n    width=1.3, height=1.3, pointsize=6)\nplot_depths_and_counts(\'CCDC85B\', reads.beas.pre)\ndev.off()\n\npdf(""out/depths_and_counts-beas-CCDC85B-after.pdf"",\n    width=1.3, height=1.3, pointsize=6)\nplot_depths_and_counts(\'CCDC85B\', reads.beas.post)\ndev.off()\n\npdf(""out/depths_and_counts-beas-RRM2-before.pdf"",\n    width=1.3, height=1.3, pointsize=6)\nplot_depths_and_counts(\'RRM2\', reads.beas.pre)\ndev.off()\n\npdf(""out/depths_and_counts-beas-RRM2-after.pdf"",\n    width=1.3, height=1.3, pointsize=6)\nplot_depths_and_counts(\'RRM2\', reads.beas.post)\ndev.off()\n\n\n## While the corrected matrix can be applied to some methods, which\n## use raw read counts (ex. differential expression tests by DESeq,\n## edgeR, SAMseq/SAMstrt etc), normalization of the corrected read\n## counts is required before many down-stream analysis (ex. PCA,\n## WGCNA, t-SNE etc). Here this script demonstrates PCA on the\n## normalized counts by DESeq2\'s VST method, with and without the\n## library bias correction.\n\n## Normalization and the methods\n\n## NOTE: The read count matrix must contain rows beginning ""RNA_SPIKE_""\n## as spike-ins.\nspikein_normalization <- function(reads) {\n    reads.spikes <-\n        colSums(reads[which(substr(rownames(reads), 1, 10) == \'RNA_SPIKE_\'), ])\n    reads/rep(reads.spikes, each=nrow(reads))\n}\n\nlibrary(preprocessCore)\nquantile_normalization <- function(reads) {\n    nreads.quantile <- normalize.quantiles(log2(reads+1))\n    colnames(nreads.quantile) <- colnames(reads)\n    rownames(nreads.quantile) <- rownames(reads)\n    nreads.quantile\n}\n\nrpm_normalization <- function(reads) {\n    reads.sizes <- colSums(reads)\n    reads/rep(reads.sizes, each=nrow(reads))*1000000\n}\n\nlibrary(DESeq2)\nvst_normalization <- function(reads) DESeq2::vst(reads)\n\nlogged_spikein_normalization <- function(reads) {\n    tmp <- spikein_normalization(reads)\n    log2(tmp + min(tmp[which(tmp > 0)]) / 2)\n}\n\nlogged_rpm_normalization <- function(reads) {\n    tmp <- rpm_normalization(reads)\n    log2(tmp + min(tmp[which(tmp > 0)]) / 2)\n}\n\nnreads.beas.pre.spikein.log <- logged_spikein_normalization(reads.beas.pre)\nnreads.beas.post.spikein.log <- logged_spikein_normalization(reads.beas.post)\n\nnreads.beas.pre.rpm.log <- logged_rpm_normalization(reads.beas.pre)\nnreads.beas.post.rpm.log <- logged_rpm_normalization(reads.beas.post)\n\nnreads.beas.pre.quantile.log <- quantile_normalization(reads.beas.pre)\nnreads.beas.post.quantile.log <- quantile_normalization(reads.beas.post)\n\nnreads.beas.pre.vst.log <- vst_normalization(reads.beas.pre)\nnreads.beas.post.vst.log <- vst_normalization(reads.beas.post)\n\n\n## PCA\n\nlibrary(factoextra)\nlibrary(ggplot2)\n\nplot_pca <- function(nreads.log, habillage,\n                     palette=cols.solarized, addEllipses=T,\n                     points=1, col.ind=\'black\') {\n    pca <- prcomp(t(nreads.log))\n    fviz_pca_ind(pca, axes=c(1, 2), label=\'none\', pointsize=1,\n                 palette=palette, habillage=habillage, addEllipses=addEllipses,\n                 points=points, col.ind=col.ind) +\n        theme(axis.title=element_text(size=6),\n                     axis.text.x=element_text(size=6),\n                     axis.text.y=element_text(size=6),\n                     plot.margin=unit(c(-10, 5, 5, 5), \'pt\'),\n                     legend.justification=c(0, 0),\n                     legend.position=c(1, 0),\n                     legend.title=element_text(size=6),\n                     legend.text=element_text(size=6)) + ggtitle(\'\')\n}\n\npdf(""out/pca-beas-before.pdf"", width=1.69, height=1.69, pointsize=6)\nplot_pca(nreads.beas.pre.vst.log, libraries.beas)  ## Fig.1b\ndev.off()\n\npdf(""out/pca-beas-after.pdf"", width=1.69, height=1.69, pointsize=6)\nplot_pca(nreads.beas.post.vst.log, libraries.beas) ## Fig.2c\ndev.off()\n\n\n## Simulation when there are both case and control samples.\n\ntests.up <- sample(nrow(reads.beas.pre), nrow(reads.beas.pre)*.1)\ntests.dw <- sample(nrow(reads.beas.pre), nrow(reads.beas.pre)*.1)\n\nlength(tests.up)\nlength(tests.dw)\n\n#### Test1 :: Consistent layout - all libraries have similar number of\n#### case and control samples.\n\nreads.test1.pre <- reads.beas.pre\ntypes.test1 <-\n    factor(ifelse(is.element(substr(colnames(reads.test1.pre), 5, 7),\n                             c(\'a.F\', \'b.F\', \'c.E\', \'d.E\',\n                               \'e.F\', \'f.F\', \'g.E\', \'h.E\')) |\n                  is.element(substr(colnames(reads.test1.pre), 5, 8),\n                             c(\'c.F1\',\n                               \'d.F1\', \'d.F2\', \'d.F3\',\n                               \'g.F1\',\n                               \'h.F1\', \'h.F2\')),\n                  \'case\', \'control\'), levels=c(\'control\', \'case\'))\ncases.test1 <- which(types.test1 == \'case\')\nreads.test1.pre[tests.up, cases.test1] <-\n    reads.test1.pre[tests.up, cases.test1] * 2\nreads.test1.pre[tests.dw, cases.test1] <-\n    round(reads.test1.pre[tests.dw, cases.test1] / 2)\nnreads.test1.pre.vst.log <- vst_normalization(reads.test1.pre)\n\ntable(data.frame(library=libraries.beas, type=types.test1))\n\npdf(""out/pca-test1-before.pdf"", width=1.69, height=1.69, pointsize=6)\nplot_pca(nreads.test1.pre.vst.log, \'none\', addEllipses=F, col.ind=\'white\') +\n    scale_color_manual(values=cols.solarized) +\n    geom_point(aes(colour=libraries.beas, shape=types.test1))\ndev.off()\n\nreads.test1.post <-\n    library_bias_correction(reads.test1.pre, depths.beas, libraries.beas)\nnreads.test1.post.vst.log <- vst_normalization(reads.test1.post)\n\npdf(""out/pca-test1-after.pdf"", width=1.69, height=1.69, pointsize=6)\nplot_pca(nreads.test1.post.vst.log, \'none\', addEllipses=F, col.ind=\'white\') +\n    scale_color_manual(values=cols.solarized) +\n    geom_point(aes(colour=libraries.beas, shape=types.test1))\ndev.off()\n\n#### Test2 :: Inconsistent layout - 4 libraries have case samples\n#### only, and the other libraries have control samples only.\n\nreads.test2.pre <- reads.beas.pre\ntypes.test2 <- factor(ifelse(is.element(substr(colnames(reads.test2.pre), 5, 5),\n                                        c(\'a\', \'b\', \'c\', \'d\')),\n                             \'case\', \'control\'), levels=c(\'control\', \'case\'))\ncases.test2 <- which(types.test2 == \'case\')\nreads.test2.pre[tests.up, cases.test2] <-\n    reads.test2.pre[tests.up, cases.test2] * 2\nreads.test2.pre[tests.dw, cases.test2] <-\n    round(reads.test2.pre[tests.dw, cases.test2] / 2)\nnreads.test2.pre.vst.log <- vst_normalization(reads.test2.pre)\n\ntable(data.frame(library=libraries.beas, type=types.test2))\n\npdf(""out/pca-test2-before.pdf"", width=1.69, height=1.69, pointsize=6)\nplot_pca(nreads.test2.pre.vst.log, \'none\', addEllipses=F, col.ind=\'white\') +\n    scale_color_manual(values=cols.solarized) +\n    geom_point(aes(colour=libraries.beas, shape=types.test2))\ndev.off()\n\nreads.test2.post <-\n    library_bias_correction(reads.test2.pre, depths.beas, libraries.beas)\nnreads.test2.post.vst.log <- vst_normalization(reads.test2.post)\n\npdf(""out/pca-test2-after.pdf"", width=1.69, height=1.69, pointsize=6)\nplot_pca(nreads.test2.post.vst.log, \'none\', addEllipses=F, col.ind=\'white\') +\n    scale_color_manual(values=cols.solarized) +\n    geom_point(aes(colour=libraries.beas, shape=types.test2))\ndev.off()\n']","Guide for library design and bias correction for large-scale transcriptome studies using highly multiplexed RNAseq methods Background: Standard RNAseq methods using bulk RNA and recent single-cell RNAseq methods use DNA barcodes to identify samples and cells, and the barcoded cDNAs are pooled into a library pool before high throughput sequencing. In cases of single-cell and low-input RNAseq methods, the library is further amplified by PCR after the pooling. Preparation of hundreds or more samples for a large study often requires multiple library pools. However, sometimes correlation between expression profiles among the libraries is low and batch effect biases make integration of data between library pools difficult.Results: We investigated 166 technical replicates in 14 RNAseq libraries made using the STRT method. The patterns of the library biases differed by genes, and uneven library yields were associated with library biases. The former bias was corrected using the NBGLM-LBC algorithm, which we present in the current study. The latter bias could not be corrected directly, but could be solved by omitting libraries with particularly low yields. A simulation experiment suggested that the library bias correction using NBGLM-LBC requires a consistent sample layout. The NBGLM-LBC correction method was applied to an expression profile for a cohort study of childhood acute respiratory illness, and the library biases were resolved.Conclusions: The R source code for the library bias correction named NBGLM-LBC is available at https://shka.github.io/NBGLM-LBC and https://shka.bitbucket.io/NBGLM-LBC. This method is applicable to correct the library biases in various studies that use highly multiplexed sequencing-based profiling methods with a consistent sample layout with samples to be compared (e.g., ""cases"" and ""controls"") equally distributed in each library.",0
GOexpress: an R/Bioconductor package for the identification and visualisation of robust gene ontology signatures through supervised learning of gene expression data,"Background: Identification of gene expression profiles that differentiate experimental groups is critical for discovery and analysis of key molecular pathways and also for selection of robust diagnostic or prognostic biomarkers. While integration of differential expression statistics has been used to refine gene set enrichment analyses, such approaches are typically limited to single gene lists resulting from simple two-group comparisons or time-series analyses. In contrast, functional class scoring and machine learning approaches provide powerful alternative methods to leverage molecular measurements for pathway analyses, and to compare continuous and multi-level categorical factors.Results: We introduce GOexpress, a software package for scoring and summarising the capacity of gene ontology features to simultaneously classify samples from multiple experimental groups. GOexpress integrates normalised gene expression data (e.g., from microarray and RNA-seq experiments) and phenotypic information of individual samples with gene ontology annotations to derive a ranking of genes and gene ontology terms using a supervised learning approach. The default random forest algorithm allows interactions between all experimental factors, and competitive scoring of expressed genes to evaluate their relative importance in classifying predefined groups of samples.Conclusions: GOexpress enables rapid identification and visualisation of ontology-related gene panels that robustly classify groups of samples and supports both categorical (e.g., infection status, treatment) and continuous (e.g., time-series, drug concentrations) experimental factors. The use of standard Bioconductor extension packages and publicly available gene ontology annotations facilitates straightforward integration of GOexpress within existing computational biology pipelines.","['# External packages required to run the code below\r\n\r\nlibrary(Biobase)\r\nlibrary(GOexpress)\r\nlibrary(biomaRt)\r\nlibrary(ggplot2)\r\nlibrary(RColorBrewer)\r\nlibrary(tools)\r\n\r\n# Move to the folder containing the ""MDM.eSet.rds"" file\r\n#setwd(\'./path_to_file\')\r\n\r\n# Load the ExpressionSet in the appropriate variable name for the code below\r\nMDM.eSet = readRDS(file = \'MDM.eSet.rds\')\r\n\r\n# Download biomart annotations from Ensembl 77 (for microarray!) ----------\r\n\r\n# View the available datasets in the Ensembl release 77 archive\r\nlistMarts(host = \'oct2014.archive.ensembl.org\')\r\n\r\n# Prepare the connection to the Bos taurus Ensembl annotation dataset\r\nensembl77 = useMart(\r\n  host = \'oct2014.archive.ensembl.org\',\r\n  biomart = \'ENSEMBL_MART_ENSEMBL\',dataset = \'btaurus_gene_ensembl\')\r\n\r\n# View the available gene annotations\r\nlistAttributes(mart = ensembl77, page = \'feature_page\')\r\n\r\n# Download the gene annotations\r\nallgenes = getBM(\r\n  attributes = c(\'affy_bovine\', \'external_gene_name\', \'description\'),\r\n  mart = ensembl77)\r\n# Rename the microarray-specific header to the header supported by GOexpress\r\ncolnames(allgenes)[1] = \'gene_id\'\r\n\r\n# Download the gene ontology annotations\r\nallGO = getBM(\r\n  attributes = c(\'go_id\', \'name_1006\', \'namespace_1003\'),\r\n  mart = ensembl77)\r\n\r\n# Download the table mapping genes to gene ontologies\r\nGO_genes = getBM(attributes = c(\'affy_bovine\', \'go_id\'), mart = ensembl77)\r\n# Rename the microarray-specific header to the header supported by GOexpress\r\ncolnames(GO_genes)[1] = \'gene_id\'\r\n# Extra step for sanity check:\r\n# some gene ontologies have no annotated genes\r\n# some genes have no annotated gene ontologies\r\n# Remove those blank fields\r\nsum(GO_genes$go_id == \'\') # number of empty GO identifiers in the query\r\nGO_genes = GO_genes[GO_genes$go_id != \'\',]\r\nsum(GO_genes$gene_id == \'\') # number of empty GO identifiers in the query\r\nGO_genes = GO_genes[GO_genes$gene_id != \'\',]\r\n\r\n# Save the annotations in local files\r\nsave(allgenes, file=\'allgenes.rdata\')\r\nsave(allGO, file=\'allGO.rdata\')\r\nsave(GO_genes, file=\'GO_genes.rdata\')\r\n\r\n# GOexpress (Infection) ---------------------------------------------------\r\n\r\n# Set the random seed to allow reproducible results\r\nset.seed(4598)\r\n\r\n# Run the GO_analyse function with desired parameters and local annotations\r\nGOx.Infection = GO_analyse(\r\n  eSet = MDM.eSet, f = \'Infection\',\r\n  subset = list(Time=c(\'2HR\',\'6HR\',\'24HR\')),\r\n  GO_genes = GO_genes, all_GO = allGO, all_genes = allgenes)\r\n\r\n# Save the result of the analysis to a R session file\r\nsave(GOx.Infection, file=\'GOx.Infection.rda\')\r\n# Compress the file to save space\r\nresaveRdaFiles(\'GOx.Infection.rda\')\r\n\r\n# Look at the top-ranked genes prior to filtering\r\nhead(GOx.Infection$genes, n=20)\r\n# Export the entire unfiltered gene scoring table to a TAB-delimited file\r\nwrite.table(\r\n  x = GOx.Infection$genes, file = \'GOx.Infection.genes.txt\',\r\n  sep = \'\\t\')\r\n\r\n# Expression plot and profile ---------------------------------------------\r\n\r\n# Expressin profile of some top-ranking genes (prior to filtering)\r\nexpression_profiles_symbol(\r\n  gene_symbol = \'CCL5\', result = GOx.Infection, eSet = MDM.eSet,\r\n  x_var = \'Hours.post.infection\', seriesF = \'Animal.Infection\', line.size = 3,\r\n  ylab = expression(\'log\'[2]*\' intensity\'),\r\n  xlab = \'Hours post-infection\')\r\nexpression_plot_symbol(\r\n  gene_symbol = \'CCL5\', result = GOx.Infection, eSet = MDM.eSet,\r\n  x_var = \'Hours post-infection\',\r\n  ylab = expression(\'log\'[2]*\' intensity\'))\r\n\r\n# Pvalue ------------------------------------------------------------------\r\n\r\n# Compute and append p-values to the gene ontology scoring table\r\n# Duration ~ 30 min\r\nGOx.Infection.pval = pValue_GO(\r\n  GOx.Infection, N = 1000,\r\n  ranked.by = \'Rank\', rank.by = \'p.val\')\r\n\r\n# Export the entire unfiltered table to a TAB-delimited file\r\nwrite.table(\r\n  x = GOx.Infection.pval$GO, file = \'GOx.Infection.pValue.GO.txt\',\r\n  sep = \'\\t\', row.names = F)\r\n\r\n# Filtering of ontologies -------------------------------------------------\r\n\r\n# Filtering for all ontologies with >= 15 genes and P-value <= 0.05\r\nGOx.infection.filtered = subset_scores(\r\n  result = GOx.Infection.pval, total=15, p.val = 0.05)\r\n\r\nhead(GOx.infection.filtered$GO, n=20)\r\n\r\nwrite.table(\r\n  x = GOx.infection.filtered$GO,\r\n  file = \'GOx.Infection.filtered.GO.txt\', sep = \'\\t\', row.names = F)\r\n\r\n# List the top genes in the filtered data \r\n# Genes without annotations were discarded, leaving gaps in the apparent ranks\r\nhead(GOx.infection.filtered$genes, n=20)\r\n\r\n# Heatmap chemokines ------------------------------------------------------\r\n\r\n# Expression heatmap of genes annotated with chemokine activity\r\nheatmap_GO(\r\n  go_id = \'GO:0008009\', result = GOx.infection.filtered, eSet = MDM.eSet,\r\n  cexRow = 0.9, labRow = ""Infection.Time"",\r\n  margins = c(12, 7))\r\n\r\n# Table_genes for chemokine activity --------------------------------------\r\n\r\nwrite.table(\r\n  x = table_genes(\r\n    go_id = \'GO:0008009\', result = GOx.infection.filtered, data.only = TRUE),\r\n  file = \'table-genes_chemokines_dataset.txt\', sep = \'\\t\')\r\n']","GOexpress: an R/Bioconductor package for the identification and visualisation of robust gene ontology signatures through supervised learning of gene expression data Background: Identification of gene expression profiles that differentiate experimental groups is critical for discovery and analysis of key molecular pathways and also for selection of robust diagnostic or prognostic biomarkers. While integration of differential expression statistics has been used to refine gene set enrichment analyses, such approaches are typically limited to single gene lists resulting from simple two-group comparisons or time-series analyses. In contrast, functional class scoring and machine learning approaches provide powerful alternative methods to leverage molecular measurements for pathway analyses, and to compare continuous and multi-level categorical factors.Results: We introduce GOexpress, a software package for scoring and summarising the capacity of gene ontology features to simultaneously classify samples from multiple experimental groups. GOexpress integrates normalised gene expression data (e.g., from microarray and RNA-seq experiments) and phenotypic information of individual samples with gene ontology annotations to derive a ranking of genes and gene ontology terms using a supervised learning approach. The default random forest algorithm allows interactions between all experimental factors, and competitive scoring of expressed genes to evaluate their relative importance in classifying predefined groups of samples.Conclusions: GOexpress enables rapid identification and visualisation of ontology-related gene panels that robustly classify groups of samples and supports both categorical (e.g., infection status, treatment) and continuous (e.g., time-series, drug concentrations) experimental factors. The use of standard Bioconductor extension packages and publicly available gene ontology annotations facilitates straightforward integration of GOexpress within existing computational biology pipelines.",0
Data from: Data reuse and the open data citation advantage,"Background: Attribution to the original contributor upon reuse of published data is important both as a reward for data creators and to document the provenance of research findings. Previous studies have found that papers with publicly available datasets receive a higher number of citations than similar studies without available data. However, few previous analyses have had the statistical power to control for the many variables known to predict citation rate, which has led to uncertain estimates of the ""citation benefit"". Furthermore, little is known about patterns in data reuse over time and across datasets. Method and Results: Here, we look at citation rates while controlling for many known citation predictors, and investigate the variability of data reuse. In a multivariate regression on 10,555 studies that created gene expression microarray data, we found that studies that made data available in a public repository received 9% (95% confidence interval: 5% to 13%) more citations than similar studies for which the data was not made available. Date of publication, journal impact factor, open access status, number of authors, first and last author publication history, corresponding author country, institution citation history, and study topic were included as covariates. The citation benefit varied with date of dataset deposition: a citation benefit was most clear for papers published in 2004 and 2005, at about 30%. Authors published most papers using their own datasets within two years of their first publication on the dataset, whereas data reuse papers published by third-party investigators continued to accumulate for at least six years. To study patterns of data reuse directly, we compiled 9,724 instances of third party data reuse via mention of GEO or ArrayExpress accession numbers in the full text of papers. The level of third-party data use was high: for 100 datasets deposited in year 0, we estimated that 40 papers in PubMed reused a dataset by year 2, 100 by year 4, and more than 150 data reuse papers had been published by year 5. Data reuse was distributed across a broad base of datasets: a very conservative estimate found that 20% of the datasets deposited between 2003 and 2007 had been reused at least once by third parties. Conclusion: After accounting for other factors affecting citation rate, we find a robust citation benefit from open data, although a smaller one than previously reported. We conclude there is a direct effect of third-party data reuse that persists for years beyond the time when researchers have published most of the papers reusing their own data. Other factors that may also contribute to the citation benefit are considered.We further conclude that, at least for gene expression microarray data, a substantial fraction of archived datasets are reused, and that the intensity of dataset reuse has been steadily increasing since 2003.","['\n#### HELPER FUNCTIONS\n\n# tile multiple ggplots in one figure\n# usage: p1 = ggplot(..); p2=ggplot(..); multiplot(p1, p2)\n# From http://wiki.stdout.org/rcookbook/Graphs/Multiple%20graphs%20on%20one%20page%20(ggplot2)/\n\nmultiplot <- function(..., plotlist=NULL, cols) {\n    require(grid)\n\n    # Make a list from the ... arguments and plotlist\n    plots <- c(list(...), plotlist)\n\n    numPlots = length(plots)\n\n    # Make the panel\n    plotCols = cols                          # Number of columns of plots\n    plotRows = ceiling(numPlots/plotCols) # Number of rows needed, calculated from # of cols\n\n    # Set up the page\n    grid.newpage()\n    pushViewport(viewport(layout = grid.layout(plotRows, plotCols)))\n    vplayout <- function(x, y)\n        viewport(layout.pos.row = x, layout.pos.col = y)\n\n    # Make each plot, in the correct location\n    for (i in 1:numPlots) {\n        curRow = ceiling(i/plotCols)\n        curCol = (i-1) %% plotCols + 1\n        print(plots[[i]], vp = vplayout(curRow, curCol ))\n    }\n\n}\n\n#### Some of my variables are extracted by looking to see if MEDLINE records have a MeSH term.\n# This is fine, but sometimes MEDLINE records are incomplete, they are in process or aren\'t on the list\n# of journals indexed by MeSH indexers.  This means a lack of relevant MeSH term does not mean the MeSH\n# term does not apply.  Thus, must replace these 0s with NAs to indicate the data is indeed missing\nmedline.values = function(raw_values, medline_status) {\n    values = raw_values\n    values[medline_status!=""indexed for MEDLINE""] = NA\n    return(values)\n}\n\n\n# Based on hetcore.data.frame from polycor\n# Modified slightly to call different correlation function for continuous correlations\n# and print out updates\n""hetcor.modified"" <-\nfunction(data, ML=FALSE, std.err=TRUE, use=c(""complete.obs"", ""pairwise.complete.obs""),\n  bins=4, pd=TRUE, ...){\n  se.r <- function(r, n){\n    rho <- r*(1 + (1 - r^2)/(2*(n - 3))) # approx. unbiased estimator\n    v <- (((1 - rho^2)^2)/(n + 6))*(1 + (14 + 11*rho^2)/(2*(n + 6)))\n    sqrt(v)\n    }\n  use <- match.arg(use)\n  if (class(data) != ""data.frame"") stop(""argument must be a data frame."")\n  if (use == ""complete.obs"") data <- na.omit(data)\n  p <- length(data)\n  if (p < 2) stop(""fewer than 2 variables."")\n  R <- matrix(1, p, p)\n  Type <- matrix("""", p, p)\n  SE <- matrix(0, p, p)\n  N <- matrix(0, p, p)\n  Test <- matrix(0, p, p)\n  diag(N) <- if (use == ""complete.obs"") nrow(data)\n             else sapply(data, function(x) sum(!is.na(x)))\n  for (i in 2:p) {\n#    print(i)\n    for (j in 1:(i-1)){\n      x <- data[[i]]\n      y <- data[[j]]\n      if (inherits(x, c(""numeric"", ""integer"")) && inherits(y, c(""numeric"", ""integer""))) {\n#         r <- cor(x, y, use=""complete.obs"")\n         r <- rcorr(x, y, type=""pearson"")$r[1,2]\n#         Type[i, j] <- Type[j, i] <- ""Pearson""\n         Type[i, j] <- Type[j, i] <- ""rcorr Pearson""\n         R[i, j] <- R[j, i] <- r\n         if (std.err) {\n           n <- sum(complete.cases(x, y))\n           SE[i, j] <- SE[j, i] <- se.r(r, n)\n           N[i, j] <- N[j, i] <- n\n           Test[i, j] <- pchisq(chisq(x, y, r, bins=bins), bins^2 - 2, lower.tail=FALSE)\n           }\n         }\n      else if (inherits(x, ""factor"") && inherits(y, ""factor"")) {\n         Type[i, j] <- Type[j, i] <- ""Polychoric""\n         result <- polychor(x, y, ML=ML, std.err=std.err)\n         if (std.err){\n           n <- sum(complete.cases(x, y))\n           R[i, j] <- R[j, i] <- result$rho\n           SE[i, j] <- SE[j, i] <- sqrt(result$var[1,1])\n           N[i, j] <- N[j, i] <- n\n           Test[i, j] <- if (result$df > 0)\n                pchisq(result$chisq, result$df, lower.tail=FALSE)\n                else NA\n           }\n         else R[i, j] <- R[j, i] <- result\n         }\n       else {\n         if (inherits(x, ""factor"") && inherits(y, c(""numeric"", ""integer"")))\n           result <- polyserial(y, x, ML=ML, std.err=std.err, bins=bins)\n         else if (inherits(x, c(""numeric"", ""integer"")) && inherits(y, ""factor""))\n           result <- polyserial(x, y, ML=ML, std.err=std.err, bins=bins)\n         else {\n             stop(""columns must be numeric or factors."")\n             }\n         Type[i, j] <- Type[j, i] <- ""Polyserial""\n         if (std.err){\n           n <- sum(complete.cases(x, y))\n           R[i, j] <- R[j, i] <- result$rho\n           SE[i, j] <- SE[j, i] <- sqrt(result$var[1,1])\n           N[i, j] <- N[j, i] <- n\n           Test[i, j] <- pchisq(result$chisq, result$df, lower.tail=FALSE)\n           }\n         else R[i, j] <- R[j, i] <- result\n         }\n       }\n     }\n   if (pd) {\n       if (min(eigen(R, only.values=TRUE)$values) < 0){\n            cor <- nearcor(R)\n            if (!cor$converged) stop(""attempt to make correlation matrix positive-definite failed"")\n            warning(""the correlation matrix has been adjusted to make it positive-definite"")\n            R <- cor$cor\n        }\n    }\n   rownames(R) <- colnames(R) <- names(data)\n   result <- list(correlations=R, type=Type, NA.method=use, ML=ML)\n   if (std.err) {\n     rownames(SE) <- colnames(SE) <- names(data)\n     rownames(N) <- colnames(N) <- names(N)\n     rownames(Test) <- colnames(Test) <- names(data)\n     result$std.errors <- SE\n     result$n <- if (use == ""complete.obs"") n else N\n     result$tests <- Test\n     }\n   class(result) <- ""hetcor""\n   result\n   }\n\n\n', '\nrequire(plyr)\n\n# transform for count data\n# using sqrt with minimum value of 1, as per advice at\n# http://www.webcitation.org/query?url=http%3A%2F%2Fpareonline.net%2Fgetvn.asp%3Fv%3D8%26n%3D6&date=2010-02-11\ntr = function(x) return(sqrt(1 + x))\n\nlog.tr = function(x) return(log(1 + x))\n\nundo.tr = function(y) return(y^2 - 1)\n\nundo.log.tr = function(y) return(exp(y) - 1)\n\nget.dat.nums = function (dat.raw) {\n    dat.nums = colwise(as.numeric)(dat.raw)  ##<<details this produces warnings\n    return(dat.nums)\n}\n\n\npreprocess.raw.data = function\n### Do preprocessing on raw data\n### change variables to ordered factors, integers, etc as appropriate\n### return cleaned data as a data frame\n(\n    dat.raw ##<< data frame containing raw data\n)\n{\n    names(dat.raw) = gsub(""_"", ""."", names(dat.raw))\n    \n    ####  CONVERT STRINGS TO NUMBERS \n    dat.nums = get.dat.nums(dat.raw)  ##<<details this produces warnings\n    \n    #library(psych)\n    #described.dat.nums = psych::describe(dat.nums)\n    #write.table(described.dat.nums,""described_dat_nums.txt"",append=F,quote=F,sep=""\\t"",row.names=T)\n\n    #### START TO BUILD STATS DATASET\n    dat = data.frame(pmid = as.numeric(dat.nums$pmid))\n\n\n    # skip issn\n    # skip essn\n    # My naming convention is that ""ago"" means number of years between 2010 and the original variable\n    dat$pubmed.year.published = dat.nums$pubmed.year.published\n    dat$years.ago.tr = tr(2010 - dat.nums$pubmed.year.published)\n    dat$pubmed.date.in.pubmed = dat.nums$pubmed.date.in.pubmed\n    \n    dat$num.authors.tr = tr(dat.nums$pubmed.number.authors)\n\n    ####### FIRST AUTHOR VARIABLES\n\n    # skip first_author_first_name\n    dat$first.author.female                 = ordered(dat.nums$author.first.author.female)\n    dat$first.author.female[which(pmax(dat.nums$author.first.author.male, dat.nums$author.first.author.female) <= 0)] = NA\n\n    # Removing male because so highly correlated with female\n    #dat$first.author.male                   = ordered(dat.nums$author.first.author.male)\n    dat$first.author.gender.not.found       = ordered(dat.nums$author.first.author.gender.not.found)\n    dat$first.author.num.prev.pubs.tr       = tr(dat.nums$first.author.num.prev.pubs)\n    dat$first.author.num.prev.pmc.cites.tr  = tr(dat.nums$first.author.num.prev.pmc.cites)\n\n    dat$first.author.year.first.pub.ago.tr  = tr( 2010 - dat.nums$first.author.year.first.pub)\n    # NAs are either because there was no cluster, or the cluster included no prior papers\n    # (unfortunately, I didn\'t collect an indication of which was true, but in theory there should\n    # be a cluster for all papers published before 2009 Author-ity data was run, so I will \n    # assume it is the latter, that there we no prior papers.)  \n    # I will code no prior papers as zero years since the prior paper\n    dat$first.author.year.first.pub.ago.tr[is.na(dat.nums$first.author.year.first.pub)] = tr(0)\n\n    dat$first.author.num.prev.microarray.creations.tr = tr(dat.nums$first.author.num.prev.microarray.creations)\n\n    # set to NA if the number of publications is unrealistically big, because this probably means that\n    # Author-ity combined two or more authors together.  Better to set to NA and deal with bias than introduce\n    # such large numbers into our data\n    first.author.thresh.cluster.size = quantile(dat$first.author.num.prev.pubs.tr, .98, na.rm=TRUE)\n    first.author.unrealistic.pub.clusters = which(dat$first.author.num.prev.pubs.tr > first.author.thresh.cluster.size)\n    dat$first.author.num.prev.pubs.tr[first.author.unrealistic.pub.clusters] = NA\n    dat$first.author.num.prev.pmc.cites.tr[first.author.unrealistic.pub.clusters] = NA\n    dat$first.author.year.first.pub.ago.tr[first.author.unrealistic.pub.clusters] = NA\n    dat$first.author.num.prev.microarray.creations.tr[first.author.unrealistic.pub.clusters] = NA\n\n\n    ####### LAST AUTHOR VARIABLES\n\n    # skip last_author_first_name\n    dat$last.author.female                 = ordered(dat.nums$last.first.author.female)\n    dat$last.author.female[which(pmax(dat.nums$last.first.author.male, dat.nums$last.first.author.female) <= 0)] = NA\n\n    # Removing male because so highly correlated with female\n    #dat$last.author.male                   = ordered(dat.nums$last.first.author.male)\n    dat$last.author.gender.not.found       = ordered(dat.nums$last.first.author.gender.not.found)\n    dat$last.author.num.prev.pubs.tr        = tr(dat.nums$last.author.num.prev.pubs)\n    dat$last.author.num.prev.pmc.cites.tr   = tr(dat.nums$last.author.num.prev.pmc.cites)\n\n    dat$last.author.year.first.pub.ago.tr   = tr(2010 - dat.nums$last.author.year.first.pub)\n    # See comment on first author above for treatment of NAs for this variable\n    dat$last.author.year.first.pub.ago.tr[is.na(dat.nums$last.author.year.first.pub)] = tr(0)\n\n    dat$last.author.num.prev.microarray.creations.tr = tr(dat.nums$last.author.num.prev.microarray.creations)\n\n    # set to NA if the number of publications is unrealistically big, because this probably means that\n    # Author-ity combined two or more authors together.  Better to set to NA and deal with bias than introduce\n    # such large numbers into our data\n    last.author.thresh.cluster.size = quantile(dat$last.author.num.prev.pubs.tr, .98, na.rm=TRUE)\n    last.author.unrealistic.pub.clusters = which(dat$last.author.num.prev.pubs.tr >= last.author.thresh.cluster.size)\n    dat$last.author.num.prev.pubs.tr[last.author.unrealistic.pub.clusters] = NA\n    dat$last.author.num.prev.pmc.cites.tr[last.author.unrealistic.pub.clusters] = NA\n    dat$last.author.year.first.pub.ago.tr[last.author.unrealistic.pub.clusters] = NA\n    dat$last.author.num.prev.microarray.creations.tr[last.author.unrealistic.pub.clusters] = NA\n\n    ####  SOME FUNDING VARIABLES AND STUDY VARIABLES\n    # skip address\n    # skip institution, use instition_clean instead below\n    # skip country, use country_clean instead below\n    # skip grant_numbers, use lots of other related variables below\n    dat$num.grant.numbers.tr = tr(dat.nums$num.grant.numbers)\n\n    # skip is_review because all 0s, as it should be\n    # need to use pubmed.medline.status to figure out what should really be NA because of incomplete mesh terms\n    dat$pubmed.is.humans    = ordered(medline.values(dat.nums$pubmed.is.humans, dat.raw$pubmed.medline.status))\n    dat$pubmed.is.animals   = ordered(medline.values(dat.nums$pubmed.is.animals, dat.raw$pubmed.medline.status))\n    dat$pubmed.is.mice      = ordered(medline.values(dat.nums$pubmed.is.mice, dat.raw$pubmed.medline.status))\n    dat$pubmed.is.fungi     = ordered(medline.values(dat.nums$pubmed.is.fungi, dat.raw$pubmed.medline.status))\n    dat$pubmed.is.bacteria  = ordered(medline.values(dat.nums$pubmed.is.bacteria, dat.raw$pubmed.medline.status))\n    dat$pubmed.is.plants    = ordered(medline.values(dat.nums$pubmed.is.plants, dat.raw$pubmed.medline.status))\n    dat$pubmed.is.viruses   = ordered(medline.values(dat.nums$pubmed.is.viruses, dat.raw$pubmed.medline.status))\n    dat$pubmed.is.cultured.cells    = ordered(medline.values(dat.nums$pubmed.is.cultured.cells, dat.raw$pubmed.medline.status))\n    # is cancer appears to depend not entirely on MEDLINE completeness\n    dat$pubmed.is.cancer        = ordered(dat.nums$pubmed.is.cancer)\n    # Open access filter, and some others below, are by journal not article, so doesn\'t depend on MEDLINE completion status\n    dat$pubmed.is.open.access   = ordered(dat.nums$pubmed.is.open.access)\n    dat$pubmed.is.effectiveness = ordered(medline.values(dat.nums$pubmed.is.effectiveness, dat.raw$pubmed.medline.status))\n    dat$pubmed.is.diagnosis     = ordered(medline.values(dat.nums$pubmed.is.diagnosis, dat.raw$pubmed.medline.status))\n    dat$pubmed.is.prognosis     = ordered(medline.values(dat.nums$pubmed.is.prognosis, dat.raw$pubmed.medline.status))\n    dat$pubmed.is.core.clinical.journal = ordered(dat.nums$pubmed.is.core.clinical.journal)\n    # skip is_clinical_trial because not enough\n    # skip is_randomized_clinical_trial because not enough\n    # skip is_meta_analysis because none (yay!)\n    dat$pubmed.is.comparative.study = ordered(medline.values(dat.nums$pubmed.is.comparative.study, dat.raw$pubmed.medline.status))\n    # skip multicenter because not enough (surprising?)\n    # skip validation because not enough (surprising?)\n    # skip funded_stimulus because all 0s\n    # external funding seems set sometimes even for 1s, so doesn\'t depend on MEDLINE completely\n    # commenting out external in favour of a combined nih below\n    #dat$pubmed.is.funded.nih.extramural   = ordered(dat.nums$pubmed.is.funded.nih.extramural)\n    dat$pubmed.is.funded.nih.intramural   = ordered(dat.nums$pubmed.is.funded.nih.intramural)\n    # Combining extramural, intramural, and whether the PMID had a link to funding in the NIH data sources\n    dat$pubmed.is.funded.nih            = ordered(pmax(dat.nums$pubmed.is.funded.nih.extramural, dat.nums$pubmed.is.funded.nih.intramural, !is.na(dat.nums$num.grants)))\n    dat$pubmed.is.funded.non.us.govt    = ordered(dat.nums$pubmed.is.funded.non.us.govt)\n    # Sharing in PDB and Swissprot very small, so will combine\n    dat$pubmed.is.shared.other          = ordered(pmax(dat.nums$pubmed.in.genbank, dat.nums$pubmed.in.pdb, dat.nums$pubmed.in.swissprot))\n    # geo.reuse is tiny but I want to use it anyway!\n    dat$pubmed.is.geo.reuse             = ordered(dat.nums$is.geo.reuse)\n    dat$pubmed.num.cites.from.pmc.tr    = tr(dat.nums$pubmed.number.times.cited.in.pmc)\n    dat$pubmed.num.cites.from.pmc.per.year = dat.nums$pubmed.number.times.cited.in.pmc/(2010 - dat.nums$pubmed.year.published)\n    # Comment these out for now, because they look wrong\n    #dat$found.by.highwire               = ordered(dat.nums$found.by.highwire)\n    #dat$found.by.scirus                 = ordered(dat.nums$found.by.highwire)\n    #dat$found.by.googlescholar          = ordered(dat.nums$found.by.highwire)\n    #dat$found.by.pmc                    = ordered(dat.nums$portal.pmids.found.by.pmc)\n\n    ##### DEPENDENT VARIABLES\n    dat$dataset.in.geo                  = ordered(dat.nums$pubmed.in.geo)\n    # commenting this out in favour of a combined metric below\n    #dat$dataset.in.arrayexpress         = ordered(dat.nums$in.arrayexpress)\n    dat$dataset.in.geo.or.ae            = ordered(dat.nums$in.ae.or.geo)\n\n    ###### INSTITUTION\n    dat$country.australia              = ordered(ifelse(""Australia"" == dat.raw$country.clean, 1, 0))\n    dat$country.canada                 = ordered(ifelse(""Canada"" == dat.raw$country.clean, 1, 0))\n    dat$country.china                  = ordered(ifelse(""China"" == dat.raw$country.clean, 1, 0))\n    dat$country.france                 = ordered(ifelse(""France"" == dat.raw$country.clean, 1, 0))\n    dat$country.germany                = ordered(ifelse(""Germany"" == dat.raw$country.clean, 1, 0))\n    dat$country.japan                  = ordered(ifelse(""Japan"" == dat.raw$country.clean, 1, 0))\n    dat$country.korea                  = ordered(ifelse(""Korea"" == dat.raw$country.clean, 1, 0))\n    dat$country.uk                     = ordered(ifelse(""UK"" == dat.raw$country.clean, 1, 0))\n    dat$country.usa                    = ordered(ifelse(""USA"" == dat.raw$country.clean, 1, 0))\n    dat$institution.is.medical         = ordered(dat.nums$institution.hospital.or.medlcal)\n    dat$institution.rank               = dat.nums$institution.rank\n    dat$institution.is.higher.ed       = ordered(ifelse(""Higher educ."" == dat.raw$institution.sector, 1, 0))\n    dat$institution.is.higher.ed[which(is.na(dat$institution.rank))] = NA\n    dat$institution.is.govnt           = ordered(ifelse(""Government"" == dat.raw$institution.sector, 1, 0))\n    dat$institution.is.govnt[which(is.na(dat$institution.rank))] = NA\n    dat$institution.research.output.tr = tr(dat.nums$institution.output)\n    dat$institution.international.collaboration = dat.nums$international.collaboration\n    dat$institution.mean.norm.impact.factor     = dat.nums$institution.norm.sjr\n    dat$institution.mean.norm.citation.score    = dat.nums$institution.norm.citation.score\n\n    # Include variables for the three most common institutions\n    #top.institutions = names(sort(table(dat.institutions$institutions.factor), decreasing=TRUE)[0:25])\n    #display.institutions = c(top.institutions)\n    dat$institution.harvard            = ordered(ifelse(""Harvard University"" == dat.raw$institution.clean, 1, 0))\n    dat$institution.nci                = ordered(ifelse(""National Cancer Institute"" == dat.raw$institution.clean, 1, 0))\n    dat$institution.stanford           = ordered(ifelse(""Stanford University"" == dat.raw$institution.clean, 1, 0))\n\n    ### JOURNAL VARIABLES\n    # skip total cites 2008 because it could be due to so many things, not very informative\n    #dat$journal.cites.2008.tr       = tr(dat.nums$journal.2008.cites)\n    dat$journal.impact.factor.tr   = tr(dat.nums$journal.impact.factor)\n\n    dat$journal.impact.factor.log   = log.tr(dat.nums$journal.impact.factor)\n\n    # PLoS One is a big part of our sample, and doesn\'t have an official impact factor yet.\n    # Important, because it is relatively low, so better to give it our estimate instead of leave missing\n    # Estimate of 3 comes from http://pbeltrao.blogspot.com/2009/04/guestimating-plos-one-impact-factor.html\n    dat$journal.impact.factor.log[which(dat.raw$pubmed.journal== ""PLoS One"")] = log.tr(3)\n\n    dat$journal.5yr.impact.factor.log = log.tr(dat.nums$journal.5yr.impact.factor)\n    dat$journal.immediacy.index.log = log.tr(dat.nums$journal.immediacy.index)\n    dat$journal.num.articles.2008.tr = tr(dat.nums$journal.num.articles.2008)\n    dat$journal.cited.halflife      = dat.nums$journal.cited.halflife\n\n    # Add a variable saying how many microarray creating (as defined by being in this set)\n    # papers the journal has published\n    table.journal.counts = table(dat.raw$pubmed.journal)\n    dat$journal.microarray.creating.count.tr = tr(as.numeric(table.journal.counts[dat.raw$pubmed.journal]))\n\n    #### MORE FUNDING VARIABLES\n    # set the missing values to 0 or tr(0), as appropriate\n\n    # skip pmid:1 as duplicate column\n    dat$num.grants.via.nih.tr   = tr(dat.nums$num.grants)\n    dat$num.grants.via.nih.tr[is.na(dat$num.grants.via.nih.tr)] = tr(0)\n    dat$max.grant.duration.tr   = tr(dat.nums$longest.num.years)\n    dat$max.grant.duration.tr[is.na(dat$max.grant.duration.tr)] = tr(0)\n\n    dat$nih.first.year.ago.tr   = tr(2010 - dat.nums$first.year)\n    dat$nih.first.year.ago.tr[is.na(dat$nih.first.year.ago.tr)] = NA\n    dat$nih.last.year.ago.tr    = tr(2010 - dat.nums$last.year)\n    dat$nih.last.year.ago.tr[is.na(dat$nih.last.year.ago.tr)] = NA\n    dat$nih.cumulative.years.tr = tr(dat.nums$cumulative.years)\n    dat$nih.cumulative.years.tr[is.na(dat$nih.cumulative.years.tr)] = tr(0)\n\n    dat$nih.sum.avg.dollars.tr  = tr(dat.nums$sum.avg.dollars)\n    dat$nih.sum.avg.dollars.tr[is.na(dat$nih.sum.avg.dollars.tr)] = tr(0)\n    dat$nih.sum.sum.dollars.tr  = tr(dat.nums$sum.sum.dollars)\n    dat$nih.sum.sum.dollars.tr[is.na(dat$nih.sum.sum.dollars.tr)] = tr(0)\n    dat$nih.max.max.dollars.tr  = tr(dat.nums$max.max.dollars)\n    dat$nih.max.max.dollars.tr[is.na(dat$nih.max.max.dollars.tr)] = tr(0)\n\n\n    # skip\n    # skip grant_activity_codes list, because captured below\n    dat$has.R01.funding = ordered(dat.nums$has.R01.funding)\n    dat$has.R01.funding[is.na(dat$has.R01.funding)] = 0\n    #dat$has.T32.funding = ordered(dat.nums$has.T32.funding)\n    #dat$has.T32.funding[is.na(dat$has.T32.funding)] = 0\n    # other funding types dropped because don\'t occurr often enough\n\n    # look for patterns here\n    dat$has.P.funding = ordered(ifelse(grepl(""P"", dat.raw$grant.activity.codes), 1, 0))\n    dat$has.R.funding = ordered(ifelse(grepl(""R"", dat.raw$grant.activity.codes), 1, 0))\n    dat$has.T.funding = ordered(ifelse(grepl(""T"", dat.raw$grant.activity.codes), 1, 0))\n    dat$has.U.funding = ordered(ifelse(grepl(""U"", dat.raw$grant.activity.codes), 1, 0))\n    dat$has.K.funding = ordered(ifelse(grepl(""K"", dat.raw$grant.activity.codes), 1, 0))\n\n    dat$dataset.in.geo.or.ae.int = dat.nums$in.ae.or.geo\n\n    dat$nCitedBy = dat.raw$nCitedBy\n    dat$nCitedBy.log = log.tr(dat$nCitedBy)\n    #dim(dat)\n\n    #save(dat, file=""dat.Rdata"")\n    return(dat)\n}\n\n']","Data from: Data reuse and the open data citation advantage Background: Attribution to the original contributor upon reuse of published data is important both as a reward for data creators and to document the provenance of research findings. Previous studies have found that papers with publicly available datasets receive a higher number of citations than similar studies without available data. However, few previous analyses have had the statistical power to control for the many variables known to predict citation rate, which has led to uncertain estimates of the ""citation benefit"". Furthermore, little is known about patterns in data reuse over time and across datasets. Method and Results: Here, we look at citation rates while controlling for many known citation predictors, and investigate the variability of data reuse. In a multivariate regression on 10,555 studies that created gene expression microarray data, we found that studies that made data available in a public repository received 9% (95% confidence interval: 5% to 13%) more citations than similar studies for which the data was not made available. Date of publication, journal impact factor, open access status, number of authors, first and last author publication history, corresponding author country, institution citation history, and study topic were included as covariates. The citation benefit varied with date of dataset deposition: a citation benefit was most clear for papers published in 2004 and 2005, at about 30%. Authors published most papers using their own datasets within two years of their first publication on the dataset, whereas data reuse papers published by third-party investigators continued to accumulate for at least six years. To study patterns of data reuse directly, we compiled 9,724 instances of third party data reuse via mention of GEO or ArrayExpress accession numbers in the full text of papers. The level of third-party data use was high: for 100 datasets deposited in year 0, we estimated that 40 papers in PubMed reused a dataset by year 2, 100 by year 4, and more than 150 data reuse papers had been published by year 5. Data reuse was distributed across a broad base of datasets: a very conservative estimate found that 20% of the datasets deposited between 2003 and 2007 had been reused at least once by third parties. Conclusion: After accounting for other factors affecting citation rate, we find a robust citation benefit from open data, although a smaller one than previously reported. We conclude there is a direct effect of third-party data reuse that persists for years beyond the time when researchers have published most of the papers reusing their own data. Other factors that may also contribute to the citation benefit are considered.We further conclude that, at least for gene expression microarray data, a substantial fraction of archived datasets are reused, and that the intensity of dataset reuse has been steadily increasing since 2003.",0
KEJP_2020_splatPop,"This dataset contains code and analysis scripts to support the manuscript ""splatPop: simulating population scale single-cell RNA sequencing data"" by Christina B. Azodi, Luke Zappia, Alicia Oshlack, Davis J. McCarthy in Genome Biology, December 2021.This project demonstrates use cases for splatPop software, an extension of the splat model implemented in Splatter, that allows for the simulation of population-scale single-cell RNA-sequencing data.The splatPop functions are available in the splatter package (v1.18.1+), available in Bioconductor.Key features:The simulation of known eQTL effects that give the simulated data realistic population structure.The ability to make these eQTL effects global, cell-group specific, or cohort specific.The simulation of differential gene expression effects between cell-groups or between cohorts within the population.The most convenient way to access this material is via the website associated with it: https://biocellgen-public.svi.edu.au/KEJP_2020_splatPop/. The full repository from which these files derive is available here: https://gitlab.svi.edu.au/biocellgen-public/KEJP_2020_splatPop. This version of the dataset corresponds to the state of the repository at the time of final correction of proofs for the manuscript. This repository will not be updated further, while the GitLab repository at the link above may be updated in the future.",,"KEJP_2020_splatPop This dataset contains code and analysis scripts to support the manuscript ""splatPop: simulating population scale single-cell RNA sequencing data"" by Christina B. Azodi, Luke Zappia, Alicia Oshlack, Davis J. McCarthy in Genome Biology, December 2021.This project demonstrates use cases for splatPop software, an extension of the splat model implemented in Splatter, that allows for the simulation of population-scale single-cell RNA-sequencing data.The splatPop functions are available in the splatter package (v1.18.1+), available in Bioconductor.Key features:The simulation of known eQTL effects that give the simulated data realistic population structure.The ability to make these eQTL effects global, cell-group specific, or cohort specific.The simulation of differential gene expression effects between cell-groups or between cohorts within the population.The most convenient way to access this material is via the website associated with it: https://biocellgen-public.svi.edu.au/KEJP_2020_splatPop/. The full repository from which these files derive is available here: https://gitlab.svi.edu.au/biocellgen-public/KEJP_2020_splatPop. This version of the dataset corresponds to the state of the repository at the time of final correction of proofs for the manuscript. This repository will not be updated further, while the GitLab repository at the link above may be updated in the future.",0
"Liftover of DGRP D.melanogaster genotypes to reference genome assembly v6.0, with QC graph","Output are vcf and plink-format genotype files. Also provided are the run code in bash and R, the logs, summary statistics, and a graph showing how the positions of SNPs have changed.","['###\nlogname = ""log_r_check_liftover.txt""\nmylog = file(logname, open = ""wt"")\nsink(mylog, append = TRUE, type = ""message"")\nSys.time() ; sessionInfo()\n\n## DESCRIPTION\n## Compares variant positions after \'liftover\' conversion to a new genome assembly.\n## For information on D.melanogaster genome assembly version 6 see:\n## Hoskins et al 2015 The Release 6 reference sequence of the D.melanogaster genome\n## http://genome.cshlp.org/content/early/2015/01/14/gr.185579.114\n\n\nlibrary(tidyverse)  ## Data formatting, and fancy plots.\nlibrary(cowplot)  ## Arranging and saving fancy plots.\nlibrary(dtplyr)  ## dplyr-data.table compatibility\nlibrary(data.table)  ## Loading big data.\nlibrary(car)  ## Recoding character variables.\n\n\nsetwd(""~/Documents/liftover/"")\n\n## Custom function for loading data, and repairing chromosome names.\nreadMap = function(dataFile){\n\tx = fread(dataFile, verbose = TRUE, sep = ""\\t"") %>%\n\t\tmutate(V1 = car::recode(V1, ""23 = \'X\'; \'chr2L\' = \'2L\';\'chr2R\' = \'2R\';\'chr3L\' = \'3L\';\'chr3R\' = \'3R\'""))\n\treturn(x)}\n\n\n## Load the data\ndfo = readMap(""dgrp2.vcf.bim"")\ndfn = readMap(""dgrp2_dm6_dbSNP.vcf.bim"")\n\n## Assign column names\ncolnames(dfo) = c(""chrom_o"", ""varID"", ""cM_o"", ""BP_o"", ""A1_o"", ""A2_o"")\ncolnames(dfn) = c(""chrom_n"", ""varID"", ""cM_n"", ""BP_n"", ""A1_n"", ""A2_n"")\n\n\nhead(dfo)\nhead(dfn)\n\n\n## Join the old and new data, by varID, having separated-out the dbSNP IDs in the new data.\ndf = full_join(dfo, dfn %>%\n\t\tseparate(varID, c(""varID"", ""dbSNP_ID""), sep = "";"", remove = TRUE), by = ""varID"") %>%\n\tmutate(dist_moved = (BP_n - BP_o)/1e+06) %>%\n\tmutate(bp_n_mb = BP_n / 1e+06) %>%\n\tarrange(chrom_o, BP_o)\nhead(df)\n\n\n## Genome plot of distance moved.\nsave_plot(""figure_dgrp_liftover_results.png"",\n\tggplot(df,\t\taes(BP_o/1e+06, BP_n/1e+06)) +\n\t\tgeom_point(size = .5, colour = ""red4"", shape = 2, stroke = .1) +\n\t\tfacet_grid(.~chrom_o, space = ""free"", scales = ""free"") +\n\t\tscale_x_continuous(name = ""Genomic position (Mb), dm5.9 assembly"", expand = c(0,1), limits = c(-0.01, NA),\n\t\t\tbreaks = seq(0,40,5), labels = c(0, """", 10, """", 20, """", 30, """", 40)) +\n\tscale_y_continuous(name = ""Genomic position (Mb), dm6 assembly"", expand = c(0,1), limits = c(-0.01, NA),\n\t\t\tbreaks = seq(0,40,5), labels = c(0, """", 10, """", 20, """", 30, """", 40)) +\n\t\ttheme_bw(base_size = 7) +\n\t\ttheme(\n\t\t\taxis.line.x = element_line(size = .2, colour = ""black""),\n\t\t\taxis.line.y = element_line(size = .2, colour = ""black""),\n\t\t\tpanel.border = element_rect(colour = ""grey80""),\n\t\t\tpanel.grid.minor = element_line(size = .1),\n\t\t\tpanel.grid.major = element_line(size = .2),\n\t\t\tstrip.background = element_rect(colour = ""grey80"", fill = ""grey90"")),\n\tbase_height = 2, base_width = 6)\n\n\n## End stuff.\nls(all.names = TRUE)\nrm(list = ls())\nsessionInfo() ; Sys.time()\nprint(""William P. Gilks, wpgilks@gmail.com, unemployed 2017 https://sites.google.com/view/williamgilks/home"")\nprint(""End of script"")\nsink() ; unlink(logname)\n###\n']","Liftover of DGRP D.melanogaster genotypes to reference genome assembly v6.0, with QC graph Output are vcf and plink-format genotype files. Also provided are the run code in bash and R, the logs, summary statistics, and a graph showing how the positions of SNPs have changed.",0
Correlating gut microbial membership to brown bear health metrics,"Data (both phyloseq-R object from imported qiime2 artifacts, and demultiplexed EMP-paired end sequences from Argonne National laboratory) and R code for Trujillo et al. 2021","['####### Supplemental File 4 Script for Statistical Analysis in R #######\n\n#ST 10/2021\n\npackageVersion(qiime2R)\n\nif (!requireNamespace(""BiocManager"", quietly = TRUE))\n  install.packages(""BiocManager"")\n\nBiocManager::install(""microbiome"")\nlibrary(microbiome)\nlibrary(""BiocManager"")\n## data analysis\ninstall.packages(""remotes"")\nremotes::install_github(""jbisanz/qiime2R"")\nlibrary(""remotes"")\nlibrary(qiime2R) # import data\nlibrary(phyloseq) # also the basis of data object. Data analysis and visualization\nlibrary(vegan) # some utility tools\nlibrary(data.table) # alternative to data.frame\nlibrary(dplyr) # data handling\nlibrary(tidyverse)\nlibrary(DT) ## interactive tables\nlibrary(ggpubr) ## plotting \nlibrary(ggplot2)\ndevtools::install_github(""leffj/mctoolsr"")\nlibrary(mctoolsr)\nlibrary(picante) ## faith\'s PD\nlibrary(see)\nlibrary(Rmisc)## graphing\nlibrary(SRS)\nlibrary(cowplot)\nlibrary(shiny)\nlibrary(glue)\nsetwd(""/Users/sarahtrujillo/Documents/CH2/R/"")\n\n#### Import & create phyloseq dataframe with qiime2R and QIIME2 artifacts #####\n## Following Tutorial: Integrating QIIME2 and R for data visualization and analysis using qiime2R by J. Bisanz\n## you will need\n# 1.) Metafile.tsv will need to have the second row removed and the # infront of SampleID removed for it to read okay\n# 2.) taxonomy.qza\n# 3.) cleantableblahblah-rm.qza\n# 4.) filterrooted.qza\n\n## import artifacts & metadata file\nmetadata1<-read_tsv(""brownbearmeta.tsv"")\nmetadata2<-read_tsv(""metadata.tsv"")\nhead(metadata2)\nmetadata<-full_join(metadata1, metadata2)\nhead(metadata)\nSVs<-read_qza(""clean-brownbear-table-unassigned_Unknown_Arch-rm.qza"")\nhead(SVs)\ntaxonomy<-read_qza(""brownbear-taxonomy_renamed.qza"")\ntaxtable<-taxonomy$data %>% as_tibble() %>% separate(Taxon, sep="";"", c(""Domain"", ""Phylum"", ""Class"", ""Order"", ""Family"", ""Genus"", ""Species"")) #convert the table into a tabular split version\ntree<-read_qza(""filter-rooted-brownbear-tree.qza"")\n\nmetadata$`Body Fat`<- cut(metadata$`Body Fat (%)`,3)\nmetadata$NetBodyMass<-cut(metadata$NetBodyMasskgs,3)\nmetadata$`Fat Mass`<-cut(metadata$`Fat Mass (kg)`,3)\nmetadata$`Lean Mass`<-cut(metadata$`Lean Mass (kg)`,3)\n\n## Create the phyloseq object\nphy_obj<-phyloseq(\n  otu_table(SVs$data, taxa_are_rows = TRUE), \n  phy_tree(tree$data), \n  tax_table(as.data.frame(taxtable) %>% dplyr::select(-Confidence) %>% column_to_rownames(""Feature.ID"") %>% as.matrix()), #moving the taxonomy to the way phyloseq wants it\n  sample_data(metadata %>% as.data.frame() %>% column_to_rownames(""SampleID"")))\n\n\n## view data table\ndatatable(tax_table(phy_obj))\n\n##### Clean Taxonomy table #####\n## Rename NAs to last known group\ntax.clean <- data.frame(tax_table(phy_obj))\nfor (i in 1:7){ tax.clean[,i] <- as.character(tax.clean[,i])}\ntax.clean[is.na(tax.clean)] <- """"\n\nfor (i in 1:nrow(tax.clean)){\n  if (tax.clean[i,2] == """"){\n    kingdom <- paste(""Kingdom_"", tax.clean[i,1], sep = """")\n    tax.clean[i, 2:7] <- kingdom\n  } else if (tax.clean[i,3] == """"){\n    phylum <- paste(""Phylum_"", tax.clean[i,2], sep = """")\n    tax.clean[i, 3:7] <- phylum\n  } else if (tax.clean[i,4] == """"){\n    class <- paste(""Class_"", tax.clean[i,3], sep = """")\n    tax.clean[i, 4:7] <- class\n  } else if (tax.clean[i,5] == """"){\n    order <- paste(""Order_"", tax.clean[i,4], sep = """")\n    tax.clean[i, 5:7] <- order\n  } else if (tax.clean[i,6] == """"){\n    family <- paste(""Family_"", tax.clean[i,5], sep = """")\n    tax.clean[i, 6:7] <- family\n  } else if (tax.clean[i,7] == """"){\n    tax.clean$Species[i] <- paste(""Genus"",tax.clean$Genus[i], sep = ""_"")\n  }\n}\n## import new taxonomy table\ntax_table(phy_obj) <- as.matrix(tax.clean)\n\ndatatable(tax_table(phy_obj))\n\n###### Rename uncultured\ntax.clean2 <- data.frame(tax_table(phy_obj))\n\nfor (i in 1:7){ tax.clean2[,i] <- as.character(tax.clean2[,i])}\nfor (i in 1:nrow(tax.clean2)){\n  if (tax.clean2[i,2] == ""uncultured""){\n    kingdom <- paste(""Kingdom_"", tax.clean2[i,1], sep = """")\n    tax.clean2[i, 2:7] <- kingdom\n  } else if (tax.clean2[i,3] == ""uncultured""){\n    phylum <- paste(""Phylum_"", tax.clean2[i,2], sep = """")\n    tax.clean2[i, 3:7] <- phylum\n  } else if (tax.clean2[i,4] == ""uncultured""){\n    class <- paste(""Class_"", tax.clean2[i,3], sep = """")\n    tax.clean2[i, 4:7] <- class\n  } else if (tax.clean2[i,5] == ""uncultured""){\n    order <- paste(""Order_"", tax.clean2[i,4], sep = """")\n    tax.clean2[i, 5:7] <- order\n  } else if (tax.clean2[i,6] == ""uncultured""){\n    family <- paste(""Family_"", tax.clean2[i,5], sep = """")\n    tax.clean2[i, 6:7] <- family\n  } else if (tax.clean2[i,7] == """"){\n    tax.clean2$Species[i] <- paste(""Genus"",tax.clean.bear$Genus[i], sep = ""_"")\n  }\n}\n\n## import new taxonomy table\ntax_table(phy_obj) <- as.matrix(tax.clean2)\n\ndatatable(tax_table(phy_obj))\n\nsaveRDS(phy_obj, ""~/physeq.rds"")\n\nphy_obj<- readRDS(""physeq.rds"")\n\n#SRS\n\nset.seed(9242)\notu<-as.data.frame(otu_table(phy_obj))\nSRS.shiny.app(otu)\n\n#normalize\nnew_otu<-as.matrix(SRS(otu, Cmin=4087, set_seed = T, seed=9242), rownames=T)\n# lost 4 samples\nrownames(new_otu)<-rownames(otu)\nsrs_obj<-phyloseq(otu_table(new_otu, taxa_are_rows = T),\n                  phy_tree(phy_obj), \n                  tax_table(phy_obj),\n                  sample_data(phy_obj))\n\nsummary(sample_sums(srs_obj))\nsaveRDS(srs_obj, ""~/physeq_srs.rds"")\nsrs_obj<- readRDS(""physeq_srs.rds"")\n                                \n\n####  Alpha Diversity ####\n##  pull metadata from physeq object\nsam.meta <- meta(srs_obj)\nsam.meta\n\n## Add the rownames as a new colum for easy integration later.\nsam.meta$sam_name <- rownames(sam.meta)\n\n##### Non-phylogenetic diversities: Shannon ####\n## calculated with microbiome package\ndiv_shan<- microbiome::alpha(srs_obj, index = ""shannon"")\n\n## Add the rownames to diversity table\ndiv_shan$sam_name <- rownames(div_shan)\ndiv_shan$sam_name<-factor(div_shan$sam_name)\nwrite.csv(div_shan,\'shannon_diversity.csv\')\ndiv_shan<- read.csv(\'shannon_diversity.csv\')\n\n##### Non-phylogenetic diversities: Simpson ####\n## calculated with microbiome package\ndiv_sim<- microbiome::alpha(srs_obj, index=""diversity_inverse_simpson"") \n\n## Add the rownames to diversity table\ndiv_sim$sam_name <- rownames(div_sim)\nas.factor(""sam_name"")\n\n##### Phylogenetic diversity: Faith\'s PD #####\n#Phylogenetic diversity is calculated using the picante package.\n\n## pull ASV table\nphyb.rar.asvtab <- as.data.frame(srs_obj@otu_table)\n\n## pull tree\nphyb.rar.tree <- srs_obj@phy_tree\n\n##check if the tree is rooted or not \n\nsrs_obj@phy_tree\n###rooted so we are good to go\n\n## Getting the data ready\ndiv_pd <- pd(t(phyb.rar.asvtab), phyb.rar.tree,include.root=T) \n# t(ou_table) transposes the table for use in picante and the\n#tree file comes from the first code  we used to read tree\n#file (see making a phyloseq object section)\n\n## Add the rownames to diversity table\ndiv_pd$sam_name <- rownames(div_pd)\n\n##merge all of the alphas into one file\nmerged_table2<-merge(div_pd,div_shan, by = ""sam_name"", all=T)\nmerged_table3<-merge(merged_table2,sam.meta, by = ""sam_name"", all=T)\nalpha_table <- merge(merged_table3,div_sim, by = ""sam_name"", all=T)\n\ndatatable(alpha_table)\n\n#Visualization\nlibrary(scico)\n\nalpha_table$Body.Fat2<-cut(alpha_table$Body.Fat....,3, labels = c(""Below Median"", ""Median"", ""Above Median""))\nalpha_table$NetBodyMass2<- cut(alpha_table$NetBodyMasskgs, 3, labels = c(""Below Median"", ""Median"", ""Above Median""))\nalpha_table$Lean.Mass2<- cut(alpha_table$Lean.Mass..kg., 3,labels = c(""Below Median"", ""Median"", ""Above Median""))\nalpha_table$Fat.Mass2<- cut(alpha_table$Fat.Mass..kg., 3, labels = c(""Below Median"", ""Median"", ""Above Median""))\n\nalpha_table$Park<-factor(alpha_table$Park, levels=c( ""KATM"", ""LACL"", ""GAAR""))\nalpha_table$Body.Fat2<-factor(alpha_table$Body.Fat2, levels=c( ""Below Median"", ""Median"", ""Above Median""))\nalpha_table$NetBodyMass2<-factor(alpha_table$NetBodyMass2, levels=c( ""Below Median"", ""Median"", ""Above Median""))\nalpha_table$Lean.Mass2<-factor(alpha_table$Lean.Mass2, levels=c( ""Below Median"", ""Median"", ""Above Median""))\nalpha_table$Fat.Mass2<-factor(alpha_table$Fat.Mass2, levels=c( ""Below Median"", ""Median"", ""Above Median""))\n\nalpha_table<-alpha_table[complete.cases(alpha_table$Body.Fat2),]\nalpha_table<-alpha_table[complete.cases(alpha_table$NetBodyMass2),]\nalpha_table<-alpha_table[complete.cases(alpha_table$Lean.Mass2),]\nalpha_table<-alpha_table[complete.cases(alpha_table$Fat.Mass2),]\n\nbodyfat_pd<-ggplot(data = alpha_table, aes(x=Body.Fat2, y=PD))+\n  geom_boxplot()+\n  ylab(""Faith\'s PD"")+\n  xlab(""% Body Fat"")+\n  theme_bw()+\n  theme(axis.text.x = element_text(size=8, color=""black""),axis.text.y = element_text(size=8, color=""black""),axis.ticks = element_blank(),panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")+\n  scale_fill_manual(values=c(""#FDB4B2"", ""#A58B2D"",""#185364""))\nbodyfat_pd\n\nnetmass_pd<-ggplot(data = alpha_table, aes(x=NetBodyMass2, y=PD))+\n  geom_boxplot()+\n  ylab(""Faith\'s PD"")+\n  xlab(""Net Mass"")+\n  theme_bw()+\n  theme(axis.text.x = element_text(size=8, color=""black""),axis.text.y = element_text(size=8, color=""black""),axis.ticks = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")\nnetmass_pd\n\nleanmass_pd<-ggplot(data = alpha_table, aes(x=Lean.Mass2, y=PD))+\n  geom_boxplot()+\n  ylab(""Faith\'s PD"")+\n  xlab(""Lean Mass"")+\n  theme_bw()+\n  theme(axis.text.x = element_text(size=8, color=""black""),axis.text.y = element_text(size=8, color=""black""),axis.ticks = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")\nleanmass_pd\n\nfatmass_PD<-ggplot(data = alpha_table, aes(x=Fat.Mass2, y=PD))+\n  geom_boxplot()+\n  ylab(""Faith\'s PD"")+\n  xlab(""Fat Mass"")+\n  theme_bw()+\n  theme(axis.text.x = element_text(size=8, color=""black""),axis.text.y = element_text(size=8, color=""black""),legend.position = ""none"",panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\nggplot(data = alpha_table, aes(x=Body.Fat2, y=diversity_shannon))+\n  geom_boxplot()+\n  ylab("""")+\n  xlab("" "")+\n  theme_bw()+\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")\n\nggplot(data = alpha_table, aes(x=Body.Fat2, y=diversity_inverse_simpson))+\n  geom_boxplot()+\n  ylab(""Inverse Simpson"")+\n  xlab("" "")+\n  ylim(0,100)+\n  theme_bw()+\n  theme(legend.position = ""none"",panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\nggplot(data = alpha_table, aes(x=NetBodyMass2, y=diversity_shannon))+\n  geom_boxplot()+\n  ylab("""")+\n  xlab("" "")+\n  theme_bw()+\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")\n\nggplot(data = alpha_table, aes(x=NetBodyMass2, y=diversity_inverse_simpson))+\n  geom_boxplot()+\n  ylab(""Inverse Simpson"")+\n  xlab("" "")+\n  ylim(0,100)+\n  theme_bw()+\n  theme(legend.position = ""none"",panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\nggplot(data = alpha_table, aes(x=Lean.Mass2, y=diversity_shannon))+\n  geom_boxplot()+\n  ylab("""")+\n  xlab("" "")+\n  theme_bw()+\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")\n\nggplot(data = alpha_table, aes(x=Lean.Mass2, y=diversity_inverse_simpson))+\n  geom_boxplot()+\n  ylab(""Inverse Simpson"")+\n  xlab("" "")+\n  ylim(0,100)+\n  theme_bw()+\n  theme(legend.position = ""none"",panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\nggplot(data = alpha_table, aes(x=Fat.Mass2, y=diversity_shannon))+\n  geom_boxplot()+\n  ylab("""")+\n  xlab("" "")+\n  theme_bw()+\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")\n\nggplot(data = alpha_table, aes(x=Fat.Mass2, y=diversity_inverse_simpson))+\n  geom_boxplot()+\n  ylab(""Inverse Simpson"")+\n  xlab("" "")+\n  ylim(0,100)+\n  theme_bw()+\n  theme(legend.position = ""none"",panel.grid.major = element_blank(), panel.grid.minor = element_blank()) \n\nsummarySE(alpha_table, measurevar = ""PD"", groupvars =c(""Body.Fat2""))\nsummarySE(alpha_table, measurevar = ""PD"", groupvars =c(""NetBodyMass2""))\nsummarySE(alpha_table, measurevar = ""PD"", groupvars =c(""Lean.Mass2""))\nsummarySE(alpha_table, measurevar = ""PD"", groupvars =c(""Fat.Mass2""))\nsummarySE(alpha_table, measurevar = ""diversity_inverse_simpson"", groupvars=c(""Body.Fat2""))\nsummarySE(alpha_table, measurevar = ""diversity_inverse_simpson"",groupvars =c(""NetBodyMass2""))\nsummarySE(alpha_table, measurevar = ""diversity_inverse_simpson"", groupvars=c(""Lean.Mass2""))\nsummarySE(alpha_table, measurevar = ""diversity_inverse_simpson"", groupvars=c(""Fat.Mass2""))\nsummarySE(alpha_table, measurevar = ""diversity_shannon"", groupvars =c(""Body.Fat2""))\nsummarySE(alpha_table, measurevar = ""diversity_shannon"", groupvars =c(""NetBodyMass2""))\nsummarySE(alpha_table, measurevar = ""diversity_shannon"", groupvars =c(""Lean.Mass2""))\nsummarySE(alpha_table, measurevar = ""diversity_shannon"", groupvars =c(""Fat.Mass2""))\n\n#####alpha diversity significant differences####\n\nlibrary(car)\nlibrary(RVAideMemoire)\n\nbyf.hist(data=alpha_table, PD~Body.Fat2, density = TRUE, sep = FALSE) #plot for every comparison\n\nbyf.shapiro(data=alpha_table, log(PD)~Body.Fat2)#pass\nleveneTest(data=alpha_table, log(PD)~Body.Fat2)#pass\nbyf.shapiro(data=alpha_table, log(PD)~NetBodyMass2)#not enough above median samples\nbyf.shapiro(data=alpha_table, log(PD)~Lean.Mass2)#pass\nleveneTest(data=alpha_table, log(PD)~Lean.Mass2)#pass\nbyf.shapiro(data=alpha_table, log(PD)~Fat.Mass2)#pass\nleveneTest(data=alpha_table, log(PD)~Fat.Mass2)#pass\n\nbyf.shapiro(data=alpha_table, log(diversity_shannon)~Body.Fat2)#pass\nleveneTest(data=alpha_table, log(diversity_shannon)~Body.Fat2)#pass\nbyf.shapiro(data=alpha_table, log(diversity_shannon)~NetBodyMass2)#not enough above median samples\nbyf.shapiro(data=alpha_table, log(diversity_shannon)~Lean.Mass2)#pass\nleveneTest(data=alpha_table, log(diversity_shannon)~Lean.Mass2)#pass\nbyf.shapiro(data=alpha_table, log(diversity_shannon)~Fat.Mass2)#pass\nleveneTest(data=alpha_table, log(diversity_shannon)~Fat.Mass2)#pass\n\nbyf.shapiro(data=alpha_table, log(diversity_inverse_simpson)~Body.Fat2)#pass\nleveneTest(data=alpha_table, log(diversity_inverse_simpson)~Body.Fat2)#pass\nbyf.shapiro(data=alpha_table, log(diversity_inverse_simpson)~NetBodyMass2)#not enough above median samples\nbyf.shapiro(data=alpha_table, log(diversity_inverse_simpson)~Lean.Mass2)#below median not normal\nleveneTest(data=alpha_table, log(diversity_inverse_simpson)~Lean.Mass2)#pass\nbyf.shapiro(data=alpha_table, log(diversity_inverse_simpson)~Fat.Mass2)#below median not normal\nleveneTest(data=alpha_table, log(diversity_inverse_simpson)~Fat.Mass2)#pass\n\nmod1<-aov(data=alpha_table, PD~Body.Fat2)\nsummary(mod1)\nplot(mod1)\nmod2<-aov(data=alpha_table, diversity_shannon~Body.Fat2)\nsummary(mod2)\nwith(alpha_table, par(mfrow=c(2,2)))\nplot(mod2)\nmod3<-aov(data=alpha_table, diversity_inverse_simpson~Body.Fat2)\nsummary(mod3)\nwith(alpha_table, par(mfrow=c(2,2)))\nplot(mod3)\n\nmod1b<-aov(data=alpha_table, PD~NetBodyMass2)\nsummary(mod1b)\nplot(mod1b)\nmod2b<-aov(data=alpha_table, diversity_shannon~NetBodyMass2)\nsummary(mod2b)\nwith(alpha_table, par(mfrow=c(2,2)))\nplot(mod2b)\nmod3b<-aov(data=alpha_table, diversity_inverse_simpson~NetBodyMass2)\nsummary(mod3b)\nwith(alpha_table, par(mfrow=c(2,2)))\nplot(mod3b)\n\nmod4<-aov(data=alpha_table, PD~Lean.Mass2)\nsummary(mod4)\nplot(mod4)\nmod5<-aov(data=alpha_table, diversity_shannon~Lean.Mass2)\nsummary(mod5)\nwith(alpha_table, par(mfrow=c(2,2)))\nplot(mod5)\nmod6<-aov(data=alpha_table, diversity_inverse_simpson~Lean.Mass2)\nsummary(mod6)\nwith(alpha_table, par(mfrow=c(2,2)))\nplot(mod6)\n\nmod7<-aov(data=alpha_table, PD~Fat.Mass2)\nsummary(mod7)\nplot(mod7)\nmod8<-aov(data=alpha_table, diversity_shannon~Fat.Mass2)\nsummary(mod8)\nwith(alpha_table, par(mfrow=c(2,2)))\nplot(mod8)\nmod9<-aov(data=alpha_table, diversity_inverse_simpson~Fat.Mass2)\nsummary(mod9)\nwith(alpha_table, par(mfrow=c(2,2)))\nplot(mod9)\n\n#####Alpha diversity correlation####\n\nggplot(alpha_table, aes(x=PD, y=Body.Fat....))+\n  geom_point() #run for every cor.test\n\ncor.test(alpha_table$PD, alpha_table$Body.Fat...., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(alpha_table$PD, alpha_table$NetBodyMasskgs, method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(alpha_table$PD, alpha_table$Lean.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(alpha_table$PD, alpha_table$Fat.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\n\ncor.test(alpha_table$diversity_shannon, alpha_table$Body.Fat...., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(alpha_table$diversity_shannon, alpha_table$NetBodyMasskgs, method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(alpha_table$diversity_shannon, alpha_table$Lean.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(alpha_table$diversity_shannon, alpha_table$Fat.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\n\ncor.test(alpha_table$diversity_inverse_simpson, alpha_table$Body.Fat...., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(alpha_table$diversity_inverse_simpson, alpha_table$NetBodyMasskgs, method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(alpha_table$diversity_inverse_simpson, alpha_table$Lean.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(alpha_table$diversity_inverse_simpson, alpha_table$Fat.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\n\n#### Community composition  ######\n## filter\n# Remove taxa not seen more than 5 times in at least 20% of the samples\n## relative abundance\npseq.rel <- microbiome::transform(srs_obj, ""compositional"")\n\n## merge to phylum rank\nphlyum <- tax_glom(pseq.rel, taxrank = ""Phylum"")\nntaxa(phlyum)\n#44\n## melt\nphylum_melt<- psmelt(phlyum)\n\nunique(phylum_melt$Phylum)\n#44\n## get summary statistics phyla \n\nsummarySE(phylum_melt, measurevar = ""Abundance"", groupvars= ""Phylum"")\n\np_abund<-summarySE(phylum_melt, measurevar = ""Abundance"", groupvars =c(""Phylum"",""Body.Fat""))\np_abund%>%\n  group_by(Phylum,Body.Fat) %>%\n  dplyr::summarise(sum(Abundance))%>%\n  View()\np_abund$Abundance[p_abund$Abundance==0] <- NA\np_abund<-p_abund[complete.cases(p_abund$Abundance),]\np_abund<- p_abund %>% \n  mutate_if(is.numeric, round, digits = 5)\np_abund<-p_abund[complete.cases(p_abund$Body.Fat),]\n\np_abund_netmass<-summarySE(phylum_melt, measurevar = ""Abundance"", groupvars =c(""Phylum"", ""NetBodyMass""))\np_abund_netmass%>%\n  group_by(Phylum,NetBodyMass) %>%\n  dplyr::summarise(sum(Abundance))%>%\n  View()\np_abund_netmass$Abundance[p_abund_netmass$Abundance==0] <- NA\np_abund_netmass<-p_abund_netmass[complete.cases(p_abund_netmass$Abundance),]\np_abund_netmass<- p_abund_netmass %>% \n  mutate_if(is.numeric, round, digits = 5)\np_abund_netmass<-p_abund_netmass[complete.cases(p_abund_netmass$NetBodyMass),]\n\np_abund_lean<-summarySE(phylum_melt, measurevar = ""Abundance"", groupvars =c(""Phylum"", ""Lean.Mass""))\nsummarySE(phylum_melt, measurevar = ""Abundance"", groupvars =c(""Firmicutes""))\np_abund_lean$Abundance[p_abund_lean$Abundance==0] <- NA\np_abund_lean<-p_abund_lean[complete.cases(p_abund_lean$Abundance),]\np_abund_lean<- p_abund_lean %>% \n  mutate_if(is.numeric, round, digits = 5)\np_abund_lean<-p_abund_lean[complete.cases(p_abund_lean$Lean.Mass),]\n\np_abund_fat<-summarySE(phylum_melt, measurevar = ""Abundance"", groupvars =c(""Phylum"", ""Fat.Mass""))\np_abund_fat%>%\n  group_by(Phylum,Fat.Mass) %>%\n  dplyr::summarise(sum(Abundance))%>%\n  View()\np_abund_fat$Abundance[p_abund_fat$Abundance==0] <- NA\np_abund_fat<-p_abund_fat[complete.cases(p_abund_fat$Abundance),]\np_abund_fat<- p_abund_fat %>% \n  mutate_if(is.numeric, round, digits = 5)\np_abund_fat<-p_abund_fat[complete.cases(p_abund_fat$Fat.Mass),]\n\n## genus \n## merge to Genus rank\nrel_genus <- tax_glom(pseq.rel, taxrank = ""Genus"")\ngenus_melt<- psmelt(rel_genus)\nntaxa(rel_genus)\n\ngenus<-summarySE(genus_melt, measurevar = ""Abundance"", groupvars = c(""Genus"", ""Phylum""))\ngenus$Abundance[genus$Abundance==0] <- NA\ngenus<-genus[complete.cases(genus$Abundance),]\ngenus<- genus %>% \n  mutate_if(is.numeric, round, digits = 5)\ngenus$Genus[genus$Abundance <= 0.01] <- ""Minor""\n\ng_abund<-summarySE(genus_melt, measurevar = ""Abundance"", groupvars =c(""Genus"", ""Body.Fat""))\n\n##remove 0 abundance\ng_abund$Abundance[g_abund$Abundance==0] <- NA\ng_abund<-g_abund[complete.cases(g_abund$Abundance),]\ng_abund<- g_abund %>% \n  mutate_if(is.numeric, round, digits = 5)\ng_abund<-g_abund[complete.cases(g_abund$Body.Fat),]\ng_abund%>%\n  group_by(Genus,Body.Fat) %>%\n  dplyr::summarise(sum(Abundance))%>%\n  View()\n\ng_abund_netmass<-summarySE(genus_melt, measurevar = ""Abundance"", groupvars =c(""Genus"", ""NetBodyMass""))\ng_abund_netmass$Abundance[g_abund_netmass$Abundance==0] <- NA\ng_abund_netmass<-g_abund_netmass[complete.cases(g_abund_netmass$Abundance),]\ng_abund_netmass<- g_abund_netmass %>% \n  mutate_if(is.numeric, round, digits = 5)\ng_abund_netmass<-g_abund_netmass[complete.cases(g_abund_netmass$NetBodyMass),]\ng_abund_netmass%>%\n  group_by(Genus,NetBodyMass) %>%\n  dplyr::summarise(sum(Abundance))%>%\n  View()\n\ng_abund_lean<-summarySE(genus_melt, measurevar = ""Abundance"", groupvars =c(""Genus"", ""Lean.Mass""))\ng_abund_lean$Abundance[g_abund_lean$Abundance==0] <- NA\ng_abund_lean<-g_abund_lean[complete.cases(g_abund_lean$Abundance),]\ng_abund_lean<- g_abund_lean %>% \n  mutate_if(is.numeric, round, digits = 5)\ng_abund_lean<-g_abund_lean[complete.cases(g_abund_lean$Lean.Mass),]\ng_abund_lean%>%\n  group_by(Genus,Lean.Mass) %>%\n  dplyr::summarise(sum(Abundance))%>%\n  View()\n\ng_abund_fat<-summarySE(genus_melt, measurevar = ""Abundance"", groupvars =c(""Genus"", ""Fat.Mass""))\ng_abund_fat$Abundance[g_abund_fat$Abundance==0] <- NA\ng_abund_fat<-g_abund_fat[complete.cases(g_abund_fat$Abundance),]\ng_abund_fat<- g_abund_fat %>% \n  mutate_if(is.numeric, round, digits = 5)\ng_abund_fat<-g_abund_fat[complete.cases(g_abund_fat$Fat.Mass),]\ng_abund_fat%>%\n  group_by(Genus,Fat.Mass) %>%\n  dplyr::summarise(sum(Abundance))%>%\n  View()\n\nwrite.csv(g_abund,\'gabund.csv\')\n\n#####Major phyla significant differences ####\n\nsupplement1a<-summarySE(P_kruskal, measurevar = ""Abundance"", groupvars = c(""Phylum"", ""Body.Fat""))\nsupplement1b<-summarySE(P_kruskal, measurevar = ""Abundance"", groupvars = c(""Phylum"", ""NetBodyMass""))\nsupplement1c<-summarySE(P_kruskal, measurevar = ""Abundance"", groupvars = c(""Phylum"", ""Lean.Mass""))\nsupplement1d<-summarySE(P_kruskal, measurevar = ""Abundance"", groupvars = c(""Phylum"", ""Fat.Mass""))\n\nP_kruskal<-phylum_melt %>%\n  select(""Abundance"", ""Phylum"", ""Body.Fat"", ""NetBodyMass"", ""Lean.Mass"", ""Fat.Mass"")\nP_kruskal$Abundance[P_kruskal$Abundance==0] <- NA\nP_kruskal<-P_kruskal[complete.cases(P_kruskal$Abundance),]\nP_kruskal<- P_kruskal%>% \n  mutate_if(is.numeric, round, digits = 5)\n\nactino_kw<- P_kruskal%>%\n  filter(Phylum == ""Actinobacteria"")\nbacteroidetes_kw<- P_kruskal%>%\n  filter(Phylum == ""Bacteroidetes"")\nepsilon_kw<- P_kruskal%>%\n  filter(Phylum == ""Epsilonbacteraeota"")\nfirmicutes_kw<- P_kruskal%>%\n  filter(Phylum == ""Firmicutes"")\nproteo_kw<- P_kruskal%>%\n  filter(Phylum == ""Proteobacteria"")\ntenericutes_kw<- P_kruskal%>%\n  filter(Phylum == ""Tenericutes"")\n\nbyf.hist(data=actino_kw, Abundance~Body.Fat, density = TRUE, sep = FALSE) #plot for every comparison\n\nbyf.shapiro(data=actino_kw, log(Abundance)~Body.Fat)#pass\nleveneTest(data=actino_kw, log(Abundance)~Body.Fat)#pass\nbyf.shapiro(data=actino_kw, log(Abundance)~NetBodyMass)#not enough above median samples\nbyf.shapiro(data=actino_kw, log(Abundance)~Lean.Mass)#pass\nleveneTest(data=actino_kw, log(Abundance)~Lean.Mass)#pass\nbyf.shapiro(data=actino_kw, log(Abundance)~Fat.Mass)#pass\nleveneTest(data=actino_kw, log(Abundance)~Fat.Mass)#pass\n\nbyf.shapiro(data=bacteroidetes_kw, log(Abundance)~Body.Fat)#pass\nleveneTest(data=bacteroidetes_kw, log(Abundance)~Body.Fat)#pass\nbyf.shapiro(data=bacteroidetes_kw, log(Abundance)~NetBodyMass)#pass\nleveneTest(data=bacteroidetes_kw, log(Abundance)~NetBodyMass)#not enough above median samples\nleveneTest(data=bacteroidetes_kw, log(Abundance)~Lean.Mass)#pass\nbyf.shapiro(data=bacteroidetes_kw, log(Abundance)~Fat.Mass)#pass\nleveneTest(data=bacteroidetes_kw, log(Abundance)~Fat.Mass)#pass\n\nbyf.shapiro(data=epsilon_kw, log(Abundance)~Body.Fat)#pass\nleveneTest(data=epsilon_kw, log(Abundance)~Body.Fat)#pass\nbyf.shapiro(data=epsilon_kw, log(Abundance)~NetBodyMass)#pass\nleveneTest(data=epsilon_kw, log(Abundance)~NetBodyMass)#not enough above median samples\nleveneTest(data=epsilon_kw, log(Abundance)~Lean.Mass)#pass\nbyf.shapiro(data=epsilon_kw, log(Abundance)~Fat.Mass)#pass\nleveneTest(data=epsilon_kw, log(Abundance)~Fat.Mass) #pass\n\nbyf.shapiro(data=firmicutes_kw, log(Abundance)~Body.Fat)#fail\nleveneTest(data=firmicutes_kw, log(Abundance)~Body.Fat)#pass\nbyf.shapiro(data=firmicutes_kw, log(Abundance)~NetBodyMass)#not enough above median samples\nbyf.shapiro(data=firmicutes_kw, log(Abundance)~Lean.Mass)#fail\nleveneTest(data=firmicutes_kw, log(Abundance)~Lean.Mass)#fail\nbyf.shapiro(data=firmicutes_kw, log(Abundance)~Fat.Mass)#fail\nleveneTest(data=firmicutes_kw, log(Abundance)~Fat.Mass)#pass\n\nbyf.shapiro(data=proteo_kw, log(Abundance)~Body.Fat)#fail\nleveneTest(data=proteo_kw, log(Abundance)~Body.Fat)#pass\nbyf.shapiro(data=proteo_kw, log(Abundance)~NetBodyMass)#not enough above median samples\nbyf.shapiro(data=proteo_kw, Abundance~Lean.Mass)#fail\nleveneTest(data=proteo_kw, log(Abundance)~Lean.Mass)#pass\nbyf.shapiro(data=proteo_kw, Abundance~Fat.Mass)#fail\nleveneTest(data=proteo_kw, log(Abundance)~Fat.Mass)#pass\n\nbyf.shapiro(data=tenericutes_kw, log(Abundance)~Body.Fat)#not enough above median samples\nbyf.shapiro(data=tenericutes_kw, log(Abundance)~NetBodyMass)#not enough above median samples\nbyf.shapiro(data=tenericutes_kw, log(Abundance)~Lean.Mass)#pass\nleveneTest(data=tenericutes_kw, log(Abundance)~Lean.Mass)#pass\nbyf.shapiro(data=tenericutes_kw, log(Abundance)~Fat.Mass)#not enough above median samples\n\nmod10<-aov(data=actino_kw, Abundance~Body.Fat)\nsummary(mod10)\nplot(mod10)\nmod11<-aov(data=actino_kw, Abundance~NetBodyMass)\nsummary(mod11)\nwith(actino_kw, par(mfrow=c(2,2)))\nplot(mod11)\nmod12<-aov(data=actino_kw, Abundance~Lean.Mass)\nsummary(mod12)\nwith(actino_kw, par(mfrow=c(2,2)))\nplot(mod12)\nmod13<-aov(data=actino_kw, Abundance~Fat.Mass)\nsummary(mod13)\nwith(actino_kw, par(mfrow=c(2,2)))\nplot(mod13)\n\nmod14<-aov(data=bacteroidetes_kw, Abundance~Body.Fat)\nsummary(mod14)\nwith(bacteroidetes_kw, par(mfrow=c(2,2)))\nplot(mod14)\nmod15<-aov(data=bacteroidetes_kw, Abundance~NetBodyMass)\nsummary(mod15)\nwith(bacteroidetes_kw, par(mfrow=c(2,2)))\nplot(mod15)\nmod16<-aov(data=bacteroidetes_kw, Abundance~Lean.Mass)\nsummary(mod16)\nwith(bacteroidetes_kw, par(mfrow=c(2,2)))\nplot(mod16)\nmod17<-aov(data=bacteroidetes_kw, Abundance~Fat.Mass)\nsummary(mod17)\nwith(bacteroidetes_kw, par(mfrow=c(2,2)))\nplot(mod17)\n\nmod18<-aov(data=epsilon_kw, Abundance~Body.Fat)\nsummary(mod18)\nwith(epsilon_kw, par(mfrow=c(2,2)))\nplot(mod18)\nmod19<-aov(data=epsilon_kw, Abundance~NetBodyMass)\nsummary(mod19)\nwith(epsilon_kw, par(mfrow=c(2,2)))\nplot(mod19)\nmod20<-aov(data=epsilon_kw, Abundance~Lean.Mass)\nsummary(mod20)\nwith(epsilon_kw, par(mfrow=c(2,2)))\nplot(mod20)\nmod21<-aov(data=epsilon_kw, Abundance~Fat.Mass)\nsummary(mod21)\nwith(epsilon_kw, par(mfrow=c(2,2)))\nplot(mod21)\n\nmod22<-aov(data=tenericutes_kw, Abundance~Body.Fat)\nsummary(mod22)\nwith(tenericutes_kw, par(mfrow=c(2,2)))\nplot(mod22)\nmod23<-aov(data=tenericutes_kw, Abundance~NetBodyMass)\nsummary(mod23)\nwith(tenericutes_kw, par(mfrow=c(2,2)))\nplot(mod23)\nmod24<-aov(data=tenericutes_kw, Abundance~Lean.Mass)\nsummary(mod24)\nwith(tenericutes_kw, par(mfrow=c(2,2)))\nplot(mod24)\nmod25<-aov(data=tenericutes_kw, Abundance~Fat.Mass)\nsummary(mod25)\nwith(tenericutes_kw, par(mfrow=c(2,2)))\nplot(mod25)\n\nkruskal.test(data=firmicutes_kw, Abundance~Body.Fat)\n#Kruskal-Wallis chi-squared =0.489, df = 2, p-value = 0.7831\nkruskal.test(data=firmicutes_kw, Abundance~NetBodyMass)\n#Kruskal-Wallis chi-squared = 1.6216, df = 2, p-value = 0.4445\nkruskal.test(data=firmicutes_kw, Abundance~Lean.Mass)\n#Kruskal-Wallis chi-squared = 3.4136, df = 2, p-value = 0.1814\nkruskal.test(data=firmicutes_kw, Abundance~Fat.Mass)\n#Kruskal-Wallis chi-squared =1.2877, df = 2, p-value = 0.5253\nkruskal.test(data=proteo_kw, Abundance~Body.Fat)\n#Kruskal-Wallis chi-squared =0.064987, df = 2, p-value = 0.968\nkruskal.test(data=proteo_kw, Abundance~NetBodyMass)\n#Kruskal-Wallis chi-squared =0.81357, df = 2, p-value = 0.6658\nkruskal.test(data=proteo_kw, Abundance~Lean.Mass)\n#Kruskal-Wallis chi-squared = 2.3159, df = 2, p-value = 0.3141\nkruskal.test(data=proteo_kw, Abundance~Fat.Mass)\n#Kruskal-Wallis chi-squared =0.18392, df = 2, p-value = 0.9121\n\n#####Major phyla correlation#####\n\nP_correlation<-phylum_melt %>%\n  select( ""Abundance"",""Phylum"", ""Body.Fat...."", ""NetBodyMasskgs"", ""Lean.Mass..kg."", ""Fat.Mass..kg."")\nP_correlation$Abundance[P_correlation$Abundance==0] <- NA\nP_correlation<-P_correlation[complete.cases(P_correlation$Abundance),]\nP_correlation<- P_correlation%>% \n  mutate_if(is.numeric, round, digits = 5)\n\nactino<- P_correlation%>%\n  filter(Phylum == ""Actinobacteria"")\nbacteroidetes<- P_correlation%>%\n  filter(Phylum == ""Bacteroidetes"")\nepsilon<- P_correlation%>%\n  filter(Phylum == ""Epsilonbacteraeota"")\nfirmicutes<- P_correlation%>%\n  filter(Phylum == ""Firmicutes"")\nproteo<- P_correlation%>%\n  filter(Phylum == ""Proteobacteria"")\ntenericutes<- P_correlation%>%\n  filter(Phylum == ""Tenericutes"")\n\nggplot(actino, aes(x=Abundance, y=Body.Fat....))+\n  geom_point() #run for every cor.test\n\ncor.test(actino$Abundance, actino$Body.Fat...., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(actino$Abundance, actino$NetBodyMasskgs, method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(actino$Abundance, actino$Lean.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(actino$Abundance, actino$Fat.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\n\ncor.test(bacteroidetes$Abundance, bacteroidetes$Body.Fat...., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(bacteroidetes$Abundance, bacteroidetes$NetBodyMasskgs, method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(bacteroidetes$Abundance, bacteroidetes$Lean.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(bacteroidetes$Abundance, bacteroidetes$Fat.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\n\ncor.test(epsilon$Abundance, epsilon$Body.Fat...., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(epsilon$Abundance, epsilon$NetBodyMasskgs, method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(epsilon$Abundance, epsilon$Lean.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(epsilon$Abundance, epsilon$Fat.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\n                            \ncor.test(firmicutes$Abundance, firmicutes$Body.Fat...., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(firmicutes$Abundance, firmicutes$NetBodyMasskgs, method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(firmicutes$Abundance, firmicutes$Lean.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(firmicutes$Abundance, firmicutes$Fat.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\n\ncor.test(proteo$Abundance, proteo$Body.Fat...., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(proteo$Abundance, proteo$NetBodyMasskgs, method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(proteo$Abundance, proteo$Lean.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(proteo$Abundance, proteo$Fat.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\n\ncor.test(tenericutes$Abundance, tenericutes$Body.Fat...., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(tenericutes$Abundance, tenericutes$NetBodyMasskgs, method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(tenericutes$Abundance, tenericutes$Lean.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\ncor.test(tenericutes$Abundance, tenericutes$Fat.Mass..kg., method=""spearman"", use=""complete.obs"", exact=FALSE)\n\n#### Community composition  Visualization ####\n\n#rename phyla with < 1% abundance\np_abund$Phylum[p_abund$Abundance <= 0.01] <- ""Minor""\nunique(p_abund$Phylum)\n\nwrite_csv(p_abund, ""p_abund.csv"")\n## phyla order\np_abund$Phylum <- factor(p_abund$Phylum, levels = c( ""Minor""     ,           ""Bacteroidetes""  ,   ""Actinobacteria""  ,  ""Epsilonbacteraeota"",\n                                                        ""Tenericutes""      ,   ""Proteobacteria""  , ""Firmicutes""      ))\nspatial_plot <- ggplot(data=p_abund, aes(x=Body.Fat, y=Abundance, fill=Phylum, width=.5))\n\ninstall.packages(""ggthemes"")\nlibrary(ggthemes)\n\n#%body fat\np1<-spatial_plot + geom_bar(aes(),stat=""identity"", position=""stack"", width =.9) +\n  scale_color_scico_d(palette = ""batlow"",\n                      aesthetics = ""fill"")+\n  theme_classic()+scale_y_continuous(position = ""left"", expand = c(0,0))+\n  theme(legend.position=""bottom"", legend.key.size = unit(0.2, ""cm""),\n        legend.text = element_text(color=""black"", size=8),legend.title.align=0,\n        legend.key.width=unit(0.2,\'cm\'),legend.spacing.x = unit(.2, \'cm\'),\n        legend.spacing.y=unit(.2, \'cm\'),legend.title = element_blank(),\n        axis.title=element_text(size=10),\n        axis.text.x =element_text(color=""black"", size = 8),axis.ticks.x =element_blank(),\n        axis.text.y =element_text(color=""black"", size = 8 ),\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  xlab(""% Body Fat"")+ ylab(""Phyla relative abundance"")+ \n  scale_x_discrete(labels = c(\'Below\\n\',\'Median\\n\',\'Above\\n\'))\np1 \n\n#Net Mass\n\np_abund_netmass$Phylum[p_abund_netmass$Abundance <= 0.01] <- ""Minor""\np_abund_netmass$Phylum <- factor(p_abund_netmass$Phylum, levels = c( ""Minor"",  ""Spirochaetes"",     ""Chloroflexi"" ,    ""Cyanobacteria""   ,    ""Acidobacteria""  ,        ""Bacteroidetes"",     \n ""Chlamydiae""  ,    ""Planctomycetes"",  ""Dependentiae""   ,   ""Actinobacteria"" ,\n""Epsilonbacteraeota""      ,     ""Verrucomicrobia"",    ""Fusobacteria"",""Tenericutes"", ""Proteobacteria"", ""Firmicutes"" ))\n\nspatial_plot2 <- ggplot(data=p_abund_netmass, aes(x=NetBodyMass, y=Abundance, fill=Phylum, width=.5))\n\n\np2<-spatial_plot2 + geom_bar(aes(),stat=""identity"", position=""stack"", width =.9) +\n  scale_color_scico_d(palette = ""batlow"",\n                      aesthetics = ""fill"")+\n  theme_classic()+\n  theme(legend.position=""bottom"",legend.key.size = unit(0.2, ""cm""),\n        legend.text = element_text(color=""black"", size=8),legend.title.align=0,\n        legend.key.width=unit(0.2,\'cm\'),legend.spacing.x = unit(.2, \'cm\'),\n        legend.spacing.y=unit(.2, \'cm\'),legend.title = element_blank(),\n        panel.background = element_blank(), axis.text.x =element_text(color=""black"", size=8 ),axis.ticks=element_blank(),axis.text.y = element_blank(),\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + \n  xlab(""Net Body Mass (kg)"")+ ylab("""")+ scale_y_continuous(position = ""left"", expand = c(0,0)) +\n  scale_x_discrete(labels = c(\'Below\\n\',\'Median\\n\',\'Above\\n\'))\np2\n\n\n#Fat Mass\n\np_abund_fat$Phylum[p_abund_fat$Abundance <= 0.01] <- ""Minor""\np_abund_fat$Phylum <- factor(p_abund_fat$Phylum, levels = c( ""Minor""     ,           ""Bacteroidetes""  ,   ""Actinobacteria""  ,  ""Epsilonbacteraeota"",   ""Fusobacteria"" ,\n                                                     ""Tenericutes""      ,   ""Proteobacteria""  , ""Firmicutes""      ))\n\nspatial_plot3 <- ggplot(data=p_abund_fat, aes(x=Fat.Mass, y=Abundance, fill=Phylum, width=.5))\n\np3<-spatial_plot3 + geom_bar(aes(),stat=""identity"", position=""stack"", width =.9) +\n  scale_color_scico_d(palette = ""batlow"",\n                      aesthetics = ""fill"")+\n  theme_classic()+\n  theme(legend.position=""bottom"", legend.key.size = unit(0.2, ""cm""),\n        legend.text = element_text(color=""black"", size=8),legend.title.align=0,\n        legend.key.width=unit(0.2,\'cm\'),legend.spacing.x = unit(.2, \'cm\'),\n        legend.spacing.y=unit(.2, \'cm\'),legend.title = element_blank(),\n        axis.ticks.x =element_blank(), axis.text.x =element_text(color=""black"", size=8 ),\n        axis.ticks.y = element_blank(), axis.text.y = element_blank(),\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + \n  xlab(""Fat Mass (kg)"")+ ylab("""")+ guides(fill=guide_legend(nrow = 2)) +scale_y_continuous(position = ""left"", expand = c(0,0))+\n  scale_x_discrete(labels = c(\'Below\\n\',\'Median\\n\',\'Above\\n\'))\np3\n\n#Lean Mass\np_abund_lean$Phylum[p_abund_lean$Abundance <= 0.01] <- ""Minor""\np_abund_lean$Phylum <- factor(p_abund_lean$Phylum, levels = c( ""Minor""     ,   ""Fusobacteria"" ,       ""Bacteroidetes""  ,   ""Actinobacteria""  ,  ""Epsilonbacteraeota"",   \n                                                             ""Tenericutes""      ,   ""Proteobacteria""  , ""Firmicutes""      ))\n\nspatial_plot4 <- ggplot(data=p_abund_lean, aes(x=Lean.Mass, y=Abundance, fill=Phylum, width=.5))\n\np4<-spatial_plot4 + geom_bar(aes(),stat=""identity"", position=""stack"", width =.9) +\n  scale_color_scico_d(palette = ""batlow"",\n                      aesthetics = ""fill"")+\n  theme_classic()+\n  theme(legend.position=""bottom"", legend.key.size = unit(0.2, ""cm""),\n        legend.text = element_text(color=""black"", size=8),legend.title.align=0,\n        legend.key.width=unit(0.2,\'cm\'),legend.spacing.x = unit(.2, \'cm\'),\n        legend.spacing.y=unit(.2, \'cm\'),legend.title = element_blank(),\n        axis.ticks.x =element_blank(), axis.text.x =element_text(color=""black"", size=8 ),\n        axis.text.y = element_text(color = ""black""),\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + \n  xlab(""Lean Mass (kg)"")+ ylab(""Phyla relative abundance"")+ guides(fill=guide_legend(nrow = 2)) +scale_y_continuous(position = ""left"", expand = c(0,0))+\n  scale_x_discrete(labels = c(\'Below\\n\',\'Median\\n\',\'Above\\n\'))\np4\n\nggdraw() +\n  draw_plot(p1, x = 0, y = .5, width = .5, height = .5) +\n  draw_plot(p2, x = .5, y = .5, width = .5, height = .5) +\n  draw_plot(p3, x = .5, y = 0, width = .5, height = .5) +\n  draw_plot(p4, x = 0, y = 0, width = .5, height = 0.5) +\n  draw_plot_label(label = c(""A"", ""B"", ""C"", ""D""), size = 8,\n                  x = c(0, 0.5, 0, 0.5), y = c(1, 1, 0.5, 0.5))\n\n## genus\n\ng_abund$Genus[g_abund$Abundance <= 0.01] <- ""Minor""\nwrite_csv(g_abund, ""g_abund.csv"")\nunique(g_abund$Genus)\n### put in order you want \ng_abund$Genus <- factor(g_abund$Genus, levels = c(""Minor"", ""Actinobacillus"",  ""Bacteroides"",                            \n""Cellulosilyticum"", ""Clostridium sensu stricto 1"" ,""Edwardsiella"",       ""Escherichia-Shigella"" ,""Family_Enterobacteriaceae""  ,  ""Family_Peptostreptococcaceae"",                   \n""Helicobacter"" , ""Mycoplasma"" , ""Order_Lactobacillales"",  ""Paenalcaligenes"" , ""Pseudomonas"", ""Romboutsia"", ""Streptococcus"",  ""Terrisporobacter"" ,\n""Turicibacter"", ""Ureaplasma"" ,""Allorhizobium-Neorhizobium-Pararhizobium-Rhizobium"", ""Lactobacillus"",)) \nunique(g_abund_netmass$Genus)\ng_abund_netmass$Genus <- factor(g_abund_netmass$Genus, levels = c( ""Minor"",  ""Actinobacillus"",""Bacteroides"", ""Cellulosilyticum"",""Clostridium sensu stricto 1"",  \n""Edwardsiella"", ""Escherichia-Shigella"", ""Family_Enterobacteriaceae"", ""Family_Peptostreptococcaceae"", ""Helicobacter"",  ""Mycoplasma"",\n""Order_Lactobacillales"", ""Paenalcaligenes"", ""Pseudomonas"", ""Romboutsia"", ""Streptococcus"", ""Terrisporobacter"", ""Turicibacter"", ""Ureaplasma"", ""Family_Microbacteriaceae"",  ))\n\n#% body fat\nspatial_plot5 <- ggplot(data=g_abund, aes(x=Body.Fat, y=Abundance, fill=Genus, width=.5))\ng1<-spatial_plot5 + geom_bar(aes(),stat=""identity"", position=""stack"", width =.9) +\n  theme_classic()+\n  scale_color_scico_d(palette = ""batlow"",\n                      aesthetics = ""fill"")+\n  theme(legend.position=""none"",legend.key.size = unit(0.2, ""cm""),\n        legend.text = element_text(size=8),\n        legend.key.width=unit(0.2,\'cm\'),legend.spacing.x = unit(0.2, \'cm\'),legend.title.align=0,\n        legend.spacing.y=unit(0, \'cm\'),legend.title = element_blank(),\n        axis.text =element_text(color=""black"", size=8),axis.ticks.x =element_blank(),  axis.title=element_text(color=""black"", size = 10 ),\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  guides(fill=guide_legend(ncol=1))+\n  xlab(""% Body Fat"")+ ylab(""Genera relative abundance"")+\n  scale_y_continuous(position = ""left"", expand = c(0,0), limits=c(0, 1)) +\n  scale_x_discrete(labels = c(\'Below\\n\',\'Median\\n\',\'Above\\n\'))\ng1\n\n #Net Mass\n g_abund_netmass$Genus[g_abund_netmass$Abundance <= 0.01] <- ""Minor""\n \n spatial_plot6 <- ggplot(data=g_abund_netmass, aes(x=NetBodyMass, y=Abundance, fill=Genus, width=.5))\n g2<-spatial_plot6 + geom_bar(aes(),stat=""identity"", position=""stack"", width =.9) +\n   theme_classic()+scale_y_continuous(position = ""left"", expand = c(0,0), limits=c(0, 1))+\n   scale_color_scico_d(palette = ""batlow"",\n                       aesthetics = ""fill"")+\n   theme(legend.position=""none"",legend.key.size = unit(0.2, ""cm""),\n         legend.text = element_text(size=8),\n         legend.key.width=unit(0.2,\'cm\'),legend.spacing.x = unit(.2, \'cm\'),legend.title.align=0,\n         legend.spacing.y=unit(0, \'cm\'),legend.title = element_blank(), axis.text.x =element_text(color=""black"", size = 8),\n         axis.ticks=element_blank(), axis.text.y =element_blank(),\n         panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + \n   guides(fill=guide_legend(ncol=1))+\n   xlab(""Net Body Mass (kg)"")+ ylab("""")+\n scale_x_discrete(labels = c(\'Bellow\\n\',\'Median\\n\',\'Above\\n\'))\n g2\n\n #Lean mass \n g_abund_lean$Genus[g_abund_lean$Abundance <= 0.01] <- ""Minor""\n unique(g_abund_lean$Genus)\n g_abund_lean$Genus <- factor(g_abund_lean$Genus, levels = c( ""Minor"", ""Actinobacillus"", ""Bacteroides"", ""Cellulosilyticum"", ""Clostridium sensu stricto 1"", ""Edwardsiella"",\n""Escherichia-Shigella"", ""Family_Enterobacteriaceae"", ""Family_Peptostreptococcaceae"", ""Helicobacter"", ""Mycoplasma"", \n""Order_Lactobacillales"", ""Paenalcaligenes"", ""Romboutsia"", ""Streptococcus"", ""Terrisporobacter"", ""Turicibacter"", ""Ureaplasma"", ""Family_Microbacteriaceae"",""Fusobacterium"")) \n \n spatial_plot7 <- ggplot(data=g_abund_lean, aes(x=Lean.Mass, y=Abundance, fill=Genus, width=.5))\n g3<-spatial_plot7 + geom_bar(aes(),stat=""identity"", position=""stack"", width =.9) +\n   theme_classic()+scale_y_continuous(position = ""left"", expand = c(0,0), limits=c(0, 1))+\n   scale_color_scico_d(palette = ""batlow"",\n                       aesthetics = ""fill"")+\n   theme(legend.position=""none"",legend.key.size = unit(0.2, ""cm""),\n         legend.text = element_text(size=8),\n         legend.key.width=unit(0.2,\'cm\'),legend.spacing.x = unit(.2, \'cm\'),legend.title.align=0,\n         legend.spacing.y=unit(0, \'cm\'),legend.title = element_blank(),  axis.text =element_text(color=""black"", size = 8),\n         axis.text.y =element_blank(),axis.ticks.x =element_blank(),axis.ticks.y =element_blank(),\n         panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + \n   guides(fill=guide_legend(ncol=1))+\n   xlab(""Lean Mass (kg)"")+ ylab("""")+\n   scale_x_discrete(labels = c(\'Below\\n\',\'Median\\n\',\'Above\\n\'))\n g3\n\n #Fat Mass\n g_abund_fat$Genus[g_abund_fat$Abundance <= 0.01] <- ""Minor""\n unique(g_abund_fat$Genus)\n g_abund_fat$Genus <- factor(g_abund_fat$Genus, levels = c(""Minor"", ""Actinobacillus"",  ""Cellulosilyticum"", ""Clostridium sensu stricto 1"" ,                     \n""Edwardsiella"", ""Escherichia-Shigella"",""Family_Enterobacteriaceae"",  ""Family_Peptostreptococcaceae"", ""Helicobacter"", ""Mycoplasma"",                                       \n ""Order_Lactobacillales"", ""Romboutsia"",""Streptococcus"", ""Terrisporobacter"", ""Turicibacter"", ""Ureaplasma"", \n""Ursidibacter"", ""Allorhizobium-Neorhizobium-Pararhizobium-Rhizobium"" ,""Bibersteinia"",""Family_Pasteurellaceae"",""Lactobacillus"", ""Order_Bacteroidales"",""Pseudomonas"")) \n \n \n spatial_plot8 <- ggplot(data=g_abund_fat, aes(x=Fat.Mass, y=Abundance, fill=Genus, width=.5))\n g4<-spatial_plot8 + geom_bar(aes(),stat=""identity"", position=""stack"", width =.9) +\n   theme_classic()+scale_y_continuous(position = ""left"", expand = c(0,0), limits=c(0, 1))+\n   scale_color_scico_d(palette = ""batlow"",\n                       aesthetics = ""fill"")+\n   theme(legend.position=""none"",legend.key.size = unit(0.2, ""cm""),\n         legend.text = element_text(size=8),\n         legend.key.width=unit(0.2,\'cm\'),legend.spacing.x = unit(.2, \'cm\'),legend.title.align=0,\n         legend.spacing.y=unit(0, \'cm\'),legend.title = element_blank(),  axis.text =element_text(color=""black"", size = 8),\n         axis.text.y =element_blank(),axis.ticks.x =element_blank(),axis.ticks.y =element_blank(),\n         panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + \n   guides(fill=guide_legend(ncol=1))+\n   xlab(""Fat Mass (kg)"")+ ylab("""")+\n   scale_x_discrete(labels = c(\'Below\\n\',\'Median\\n\',\'Above\\n\'))\n g4\n \n write_csv(genus_melt, ""genus.csv"")\n \n ggdraw() +\n   draw_plot(g1, x = 0, y = .5, width = .5, height = .5) +\n   draw_plot(g2, x = .5, y = .5, width = .5, height = .5) +\n   draw_plot(g4, x = .5, y = 0, width = .5, height = .5) +\n   draw_plot(g3, x = 0, y = 0, width = .5, height = 0.5) +\n   draw_plot_label(label = c(""A"", ""B"", ""C"", ""D""), size = 8,\n                   x = c(0, 0.5, 0, 0.5), y = c(1, 1, 0.5, 0.5))\n']","Correlating gut microbial membership to brown bear health metrics Data (both phyloseq-R object from imported qiime2 artifacts, and demultiplexed EMP-paired end sequences from Argonne National laboratory) and R code for Trujillo et al. 2021",0
"The Bacteria Genome Pipeline (BAGEP): An automated, scalable workflow for bacteria genomes with Snakemake.","We present an automated and scalable pipeline called BAGEP that performs quality control on FASTQ paired end files, maps them to a reference genome of choice, constructs a phylogenetic tree from core genome alignments and an interactive Short Nucleotide Polymorphism (SNP) visualiser across core genomes in the dataset. The objective is to create an easy to use pipeline from existing bioinformatics tools that can be deployed on a personal computer with an interactive SNP visualization. BAGEP is freely available on Github: https://github.com/idolawoye/BAGEP under the MIT licence.","['library(\'vcfR\', verbose = FALSE)\nlibrary(\'heatmaply\', verbose = FALSE)\nlibrary(\'argparse\', verbose = FALSE)\n\nparser <- ArgumentParser(description=\'VCF visualisation script\')\nparser$add_argument(""vcf"", type=\'character\', action=""store"", help=""Input VCF file"")\nparser$print_help()\nargs <- commandArgs(trailingOnly = TRUE)\n\nif (length(args)>1) {\n  stop(""Only one input must be supplied: VCF file"", call.=FALSE)\n} else if (length(args)==1) {\n  noquote("". . . Checking input files"")\n}\nvcf_file <- args[[1]]\nvcf <- read.vcfR(vcf_file, verbose = FALSE)\n\nchrom <- create.chromR(name = \'vcf_heatmap\', vcf = vcf, seq = NULL, ann = NULL)\n\ngenotype <- extract.gt(chrom, element = ""GT"", as.numeric = TRUE)\nrownames(genotype) <- chrom@var.info$POS\ngenotype[!rowSums(!is.finite(genotype)), verbose = FALSE]\ngenotype[!is.finite(genotype)] <- 0\n\nheatmaply(genotype, colors = viridis(n = 256, alpha = 1, begin = 0, end = 1, option = \'viridis\')\n          , limits = c(0,2), xlab = \'Genomes\', ylab = \'SNP position\',\n          k_row = 1, k_col = 3, column_text_angle = 90, width = 500, height = 900,\n          label_names = c(""position"", ""sample"", ""SNP""), file = \'heatmap_output.html\')\nnoquote(""DONE!"")\nnoquote(\'Thank you for using vcfR heatmapper\')\n']","The Bacteria Genome Pipeline (BAGEP): An automated, scalable workflow for bacteria genomes with Snakemake. We present an automated and scalable pipeline called BAGEP that performs quality control on FASTQ paired end files, maps them to a reference genome of choice, constructs a phylogenetic tree from core genome alignments and an interactive Short Nucleotide Polymorphism (SNP) visualiser across core genomes in the dataset. The objective is to create an easy to use pipeline from existing bioinformatics tools that can be deployed on a personal computer with an interactive SNP visualization. BAGEP is freely available on Github: https://github.com/idolawoye/BAGEP under the MIT licence.",0
Data from: Public sharing of research datasets: a pilot study of associations,"The public sharing of primary research datasets potentially benefits the research community but is not yet common practice. In this pilot study, we analyzed whether data sharing frequency was associated with funder and publisher requirements, journal impact factor, or investigator experience and impact. Across 397 recent biomedical microarray studies, we found investigators were more likely to publicly share their raw dataset when their study was published in a high-impact journal and when the first or last authors had high levels of career experience and impact. We estimate the USA's National Institutes of Health (NIH) data sharing policy applied to 19% of the studies in our cohort; being subject to the NIH data sharing plan requirement was not found to correlate with increased data sharing behavior in multivariate logistic regression analysis. Studies published in journals that required a database submission accession number as a condition of publication were more likely to share their data, but this trend was not statistically significant. These early results will inform our ongoing larger analysis, and hopefully contribute to the development of more effective data sharing initiatives. **Earlier version presented at ASIS&T and ISSI Pre-Conference: Symposium on Informetrics and Scientometrics 2009**","['#install.packages(\'Hmisc\')\n#install.packages(\'Design\')\n#install.packages(\'tree\')\n#install.packages(\'tree\')\n\nlibrary(Hmisc,T)\nlibrary(Design,T)\nlibrary(tree)\n\n#### READ DATA\nsetwd(""."")\ndat.raw = read.csv(""Piwowar_Metrics2009_rawdata.csv"", header=TRUE, sep="","")\ndim(dat.raw)\nnames(dat.raw) = gsub(""_"", ""."", names(dat.raw))\nnames(dat.raw)\n\n\n### SET UP DERIVED VARIABLES\nfirst.career.length = 2008-dat.raw[""first.first.year""]\nnames(first.career.length) = c(""first.career.length"")\nlast.career.length = 2008-dat.raw[""last.first.year""]\nnames(last.career.length) = c(""last.career.length"")\n\ndat.orig = cbind(dat.raw, first.career.length, last.career.length)\ndat.orig$policy.strength = ordered(dat.orig$policy.strength)\n\n\n#### Compute principal components of author experience\n### FIRST AUTHOR\npc.first = princomp(scale( cbind(log(1+dat.orig$first.hindex), \n\t\t\t\t\t\t\t\tlog(1+dat.orig$first.aindex), \n\t\t\t\t\t\t\t\tdat.orig$first.career.length)))\nsummary(pc.first)\n# NOTE THE NEGATIVE!  This is so that\n# higher hindexes etc correlate with higher author experience scores\nfirst.author.exp = - pc.first$scores[,1]  \npc.first$loadings\n#biplot(pc.first)\nfirst.author.exp.freq = ecdf(first.author.exp)(first.author.exp)\n\n### LAST AUTHOR\npc.last = princomp(scale( cbind(log(1+dat.orig$last.hindex), \n\t\t\t\t\t\t\t\tlog(1+dat.orig$last.aindex), \n\t\t\t\t\t\t\t\tdat.orig$last.career.length)))\nsummary(pc.last)\n# NOTE THE NEGATIVE!  This is so that\n# higher hindexes etc correlate with higher author experience scores\nlast.author.exp = - pc.last$scores[,1]\npc.last$loadings\n#biplot(pc.last)\nlast.author.exp.freq = ecdf(last.author.exp)(last.author.exp)\n\n#### OVERVIEW TABLE\n\nv.orig = names(dat.orig)[3:length(dat.orig)]\nv = c(""is.data.shared"", ""policy.strength"", ""impact.factor"", ""is.usa.address"", \n""is.nih.funded"", ""any.nih.data.sharing.applies"", ""sum.of.max.award.for.each.grant"", \n""any.direct.cost.over.500k"", ""any.new.or.renewed.since.2003"", ""num.authors"",\n""first.author.exp"", ""last.author.exp"", \n""first.career.length"", ""last.career.length""\n)\ndat.extra = cbind(dat.orig, first.author.exp, last.author.exp)\ndat = dat.extra[,v]\t\t\t\t\t\t\ndd = datadist(dat)\noptions(datadist=\'dd\')\noptions(digits=2)\n\ndescribe(dat.orig[,v.orig], listunique=0) \nsummary(dat.orig[,v.orig]) \n\t\n##### FIGURE 1  (mean line was added manually later)\n\ncuteq = function(X, n) cut(X,quantile(X,(0:n)/n),include.lowest=TRUE) \n\t\ndots = NULL\ndots$is.data.shared = dat.orig$is.data.shared\ndots$impact.factor = cut(dat.orig$impact.factor, c(0,6,8,15,50)) \ndots$journal.policy.strength = dat.orig$policy.strength\ndots$num.authors = cuteq(dat.orig$num.authors,3)\ndots$FIRST.author.hindex = cuteq(dat.orig$first.hindex,3) \ndots$FIRST.author.aindex = cuteq(dat.orig$first.aindex,3) \ndots$FIRST.author.career.length = cuteq(dat.orig$first.career.length,3) \ndots$LAST.author.hindex = cuteq(dat.orig$last.hindex,3) \ndots$LAST.author.aindex = cuteq(dat.orig$last.aindex,3)\ndots$LAST.author.career.length = cuteq(dat.orig$last.career.length,3) \ndots$is.usa.address = dat.orig$is.usa.address\ndots$is.nih.funded = dat.orig$is.nih.funded\ndots$nih.funds = cut(dat.orig$sum.of.max.award.for.each.grant/1000, c(-1, 1, 750, 2000, 500000))\ndots$nih.requires.data.sharing.plan = dat.orig$any.nih.data.sharing.applies\ndots$is.nih.grant.number.missing = is.na(dots$nih.requires.data.sharing.plan)\n\ns = summary(is.data.shared ~ ., dat=dots)\ns\n\n\ntiff(""figure1_no_mean_line.tiff"", bg=""white"", width=880, height=1200)\nplot(s)\ntitle(""Proportion of studies with shared datasets"")\ndev.off()\n\n\n### DO IMPUTATION\n\ndo.imputation = function(column) {\n\tdat.orig$temp = na.include(column)\n\tmytree = tree(temp ~ policy.strength + num.authors + \n\t\tis.usa.address + \n\t\tis.nih.funded + \n\t\tfirst.hindex + first.aindex + first.career.length + \n\t\tfirst.num.papers + first.total.pmc.citations + \n\t\tlast.hindex + last.aindex + last.career.length + \n\t\tlast.num.papers + last.total.pmc.citations +\n\t\tlog(impact.factor),\n\t\tdat=dat.orig, \n\t\tcontrol=tree.control(nobs=502, mincut=20) \n\t\t)\n\tsummary(mytree)\n\tplot(mytree); text(mytree)\n\tresponse = impute(column, \n\t\t\tpredict(mytree, dat.orig)[is.na(column)])\n\tresponse = round(response)\n\tprint(response)\n\tresponse\n}\n\npar(mfrow=c(1,2))\ndat$any.nih.data.sharing.applies.imputed = \n\tdo.imputation(dat.orig$any.nih.data.sharing.applies)\ndat$sum.of.max.award.for.each.grant.imputed = \n\tdo.imputation(dat.orig$sum.of.max.award.for.each.grant)\n\n#######  MULTIVARIATE REGRESSION\n\npar(mfrow=c(1,1))\ndat.all = cbind(abs(dots$is.nih.grant.number.missing), dat.extra, dat[15:18])\n\ndat$log.award = log(1+sum.of.max.award.for.each.grant.imputed)\n\ndd = datadist(dat)\noptions(datadist=\'dd\')\noptions(digits=2)\n\t\nf = lrm(formula = is.data.shared ~ ordered(policy.strength) + \n\tis.usa.address*is.nih.funded + \n\tany.nih.data.sharing.applies.imputed + \n\trcs(first.author.exp,4) + rcs(last.author.exp, 4) +\n\trcs(log(1+sum.of.max.award.for.each.grant.imputed), 4) + \n\trcs(log(impact.factor), 4),\n\tdat=dat.all, x=T, y=T\n\t)\nanova(f)\nf\npar(mfrow=c(4,5))\nresid(f, \'partial\', pl=TRUE)\nresid(f, \'gof\')\n\n#### TABLE 1\nanova(f)\n\n\n#####  MODEL FOR ODDS RATIO ANALYSIS\n\nf2 = lrm(formula = is.data.shared ~ ordered(policy.strength) + \n\tis.usa.address + \n\tany.nih.data.sharing.applies.imputed + \n\trcs(first.author.exp,4) + rcs(last.author.exp, 4) +\n\trcs(log(1+sum.of.max.award.for.each.grant.imputed), 4) + \n\trcs(log(impact.factor), 4),\n\tdat=dat, x=T, y=T\n\t)\nf2\nanova(f2)\npar(mfrow=c(4,5))\nresid(f2, \'partial\', pl=TRUE)\nresid(f2, \'gof\')\n\n# The p-value is large (>.3 in that example) indicating no significant lack of fit.\n# from http://www.unc.edu/courses/2006spring/ecol/145/001/docs/solutions/final.htm\nresiduals.lrm(f2,type=\'gof\')\n\n\n####  FIGURE 2\nattach(dat)\n\ntiff(""figure2.tiff"", bg=""white"", width=880)\n\nsumm = summary(f2, \n\tsum.of.max.award.for.each.grant.imputed=c(1,750000),\n\tpolicy.strength=0,\n\timpact.factor=c(5,15), \n\tfirst.author.exp=quantile(first.author.exp, c(0.25,0.75)),\n\tlast.author.exp=quantile(last.author.exp, c(0.25,0.75))\n\t)\nsumm\nplot(summ, log=T)\n\ndev.off()\n\n### FIGURE 3\n\ntiff(""figure3.tiff"", bg=""white"", width=880)\n\ncutHML = function(X) {\n\tn=3\n    cuts = cut(X,quantile(X,(0:n)/n),include.lowest=TRUE, labels=FALSE)\n\tifelse(cuts==1, \'low\', ifelse(cuts==2, \'med\', \'high\'))\n\t}\n\npar(mfrow=c(1,2))  \ngroup = factor(cutHML(first.author.exp))\ngrouplty=NULL\ngrouplty[levels(group)] = c(1:3)\nlabels = c(""high"", ""med"", ""low"")\nplsmo(log(impact.factor), is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\txlim=c(1,4), ylim=c(0,1), label.curves=F,\n\tylab=""Probability of shared data"", xlab= ""Log of impact factor"")\nlegend(""topleft"", labels, lty=grouplty[labels], bty=""n"")\ntitle(""IF by First author experience"")\n\ngroup = factor(cutHML(last.author.exp))\ngrouplty=NULL\ngrouplty[levels(group)] = c(1:3)\nlabels = c(""high"", ""med"", ""low"")\nplsmo(log(impact.factor), is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\txlim=c(1,4), ylim=c(0,1), label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab=""Log of impact factor"")\nlegend(""topleft"", labels, lty=grouplty[labels], bty=""n"")\t\ntitle(""IF by Last author experience"")\t\n\ndev.off()\n\n\n#### FIGURE 4\n\ntiff(""figure4.tiff"", bg=""white"", width=880)\n\npar(mfrow=c(1,3))  \ngroup = policy.strength\nlevels(group) = c(""None"", ""Weak"", ""Strong"")  #### Hrm, not very robust!\ngrouplabels = c(""Strong"", ""Weak"", ""None"")\ngrouplty=NULL\ngrouplty[grouplabels] = c(3:1)\n\n\nplsmo(log(impact.factor), is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F,\n\txlim=c(1,4), ylim=c(0,1), trim=0, label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab= ""Log of impact factor"")\nlegend(""topleft"", grouplabels, lty=grouplty[levels(group)], bty=""n"")\t\ntitle(""IF by Journal policy strength"")\t\t\t\nplsmo(first.author.exp, is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\tylim=c(0,1), label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab= ""First author experience"")\nlegend(""topleft"", grouplabels, lty=grouplty[levels(group)], bty=""n"")\t\t\ntitle(""First author by Journal policy"")\t\t\t\t\nplsmo(last.author.exp, is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\tylim=c(0,1), label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab= ""Last author experience"")\nlegend(""topleft"", grouplabels, lty=grouplty[levels(group)], bty=""n"")\t\t\t\ntitle(""Last author by Journal policy"")\n\ndev.off()\n\n##### FIGURE 5\n\ntiff(""figure5.tiff"", bg=""white"", width=880)\n\npar(mfrow=c(1,3))  \ngroup = factor(any.nih.data.sharing.applies)\n#### Hrm, this isn\'t a very robust ordering of the legend.\n## It is accurate for this version of the data, but may not be accurate if the data are resorted.\n## What is a better way to do this?\nlevels(group) = c(""Not required"", ""NIH sharing plan required"")  \ngrouplabels = c(""NIH sharing plan required"", ""Not required"")\ngrouplty=NULL\ngrouplty[grouplabels] = c(2:1)\n\nplsmo(log(impact.factor), is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\txlim=c(1,4), ylim=c(0,1), trim=0, label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab= ""Log of impact factor"")\nlegend(""topleft"", grouplabels, lty=grouplty[levels(group)], bty=""n"")\t\t\t\ntitle(""IF by NIH policy"")\t\nplsmo(first.author.exp, is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\tylim=c(0,1), label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab= ""First author experience"")\nlegend(""topleft"", grouplabels, lty=grouplty[levels(group)], bty=""n"")\t\t\t\ntitle(""First author by NIH policy"")\t\nplsmo(last.author.exp, is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\tylim=c(0,1), label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab= ""Last author experience"")\nlegend(""topleft"", grouplabels, lty=grouplty[levels(group)], bty=""n"")\t\t\t\ntitle(""Last author by NIH policy"")\t\n\ndev.off()\n']","Data from: Public sharing of research datasets: a pilot study of associations The public sharing of primary research datasets potentially benefits the research community but is not yet common practice. In this pilot study, we analyzed whether data sharing frequency was associated with funder and publisher requirements, journal impact factor, or investigator experience and impact. Across 397 recent biomedical microarray studies, we found investigators were more likely to publicly share their raw dataset when their study was published in a high-impact journal and when the first or last authors had high levels of career experience and impact. We estimate the USA's National Institutes of Health (NIH) data sharing policy applied to 19% of the studies in our cohort; being subject to the NIH data sharing plan requirement was not found to correlate with increased data sharing behavior in multivariate logistic regression analysis. Studies published in journals that required a database submission accession number as a condition of publication were more likely to share their data, but this trend was not statistically significant. These early results will inform our ongoing larger analysis, and hopefully contribute to the development of more effective data sharing initiatives. **Earlier version presented at ASIS&T and ISSI Pre-Conference: Symposium on Informetrics and Scientometrics 2009**",0
Intrinsic and extrinsic factors influence on an omnivore's gut microbiome,"Data (both phyloseq-R object from imported qiime2 artifacts, and demultiplexed EMP-paired end sequences from Argonne National laboratory) and R code for Trujillo et al. 2021",,"Intrinsic and extrinsic factors influence on an omnivore's gut microbiome Data (both phyloseq-R object from imported qiime2 artifacts, and demultiplexed EMP-paired end sequences from Argonne National laboratory) and R code for Trujillo et al. 2021",0
Source code to produce the PSI plots,"<h1 id=""source-code-to-produce-the-psi-plots"">Source code to produce the PSI plots</h1><p>The scripts were used to generate the PSI plots to appear in Radke, M and Badillo-Lisakowski, V et al &quot;Therapeutic inhibition of RBM20 improves diastolic function in a murine heart failure model and human engineered heart tissue&quot;.deposited for reproducibility purposes.</p><p>This repository was updated on 27.01.2022 to include the <code>Snakefile</code> and <code>compute_PSI.pl</code>.</p><p>The workflow is based on Snakemake, see <code>Snakefile</code> for more information.Workflow was called with:</p><pre><code class=""lang-bash"">snakemake -<span class=""ruby"">-configfile config.yml</span> -<span class=""ruby"">-printshellcmds</span> -<span class=""ruby"">-cluster <span class=""hljs-string"">""sbatch</span></span> -<span class=""ruby""><span class=""hljs-string"">-mem {cluster.mem}</span></span> -<span class=""ruby""><span class=""hljs-string"">-out {cluster.out}</span></span> -<span class=""ruby""><span class=""hljs-string"">-error {cluster.out}</span></span> -<span class=""ruby""><span class=""hljs-string"">c {cluster.c}</span></span> -<span class=""ruby""><span class=""hljs-string"">-parsable ""</span></span> -<span class=""ruby"">-cluster-config cluster.yaml</span> -<span class=""ruby"">-jobs <span class=""hljs-number"">10</span></span></code></pre><h2 id=""dependencies-"">Dependencies:</h2><ul><li>stringtie/1.3.5</li><li>cufflinks/2.2.1</li><li>python2</li><li>R/3.5.1</li><li>perl</li><li>bedtools</li></ul><p>Dependencies were handled with Environment Modules on the Snakefile.</p><h2 id=""workflow-steps-"">Workflow steps:</h2><ol><li><p>Guided transcriptome assembly with StringTie:</p></li><li><p>Parameters:</p><ul><li>min_isoform_fraction: 0.2</li><li>min_anchor_length: 8</li><li>min_junction_coverage: 3</li><li>reference: /biodb/genomes/mus_musculus/GRCm38_85/GRCm38.85.gtfThe modified GRCm38 84 annotation accopanies this repository. The version of Ensembl has been used to study Titin and match the exons to domains.</li></ul></li><li><p>Merge annotation with cuffmerge:</p></li><li><p>Parameters:</p><ul><li>reference: /biodb/genomes/mus_musculus/GRCm38_85/GRCm38.85.gtf</li><li>min_isoform_fraction: 0.2</li></ul></li><li><p>Find &#39;containers&#39;, loci with that contains alternative splicing events, and calculate PSI from STAR <code>SJ.out.tab</code> intron counts with compute_PSI.plPSI is define by the ratio between the number of reads including an exon and total reads for that event (reads including + reads excluding).</p></li><li><p>Produce the plots using PSI_plot_aso.R script:One plot per gene, only for gene with two or more exons, in addiiton:</p></li><li>Removde duplicated exons</li><li>Merged overlapping exons</li></ol><p>The accompanying test data reproduces figure 4.b of the paper:<code>Rscript testdata/PSI_plot_aso.R testdata/output/ testdata/ldb3.gtf testdata/</code></p><p>The subset of the data was prepared with:</p><pre><code class=""lang-{fish}""><span class=""hljs-built_in"">set</span> gtf <span class=""hljs-string"">""/beegfs/prj/Mitch_Gotthardt/Thiago/psi_plots/Mus_musculus.GRCm38.84_cleaned.gtf""</span>grep <span class=""hljs-string"">'gene_name ""Ldb3"";'</span> <span class=""hljs-variable"">$gtf</span> | sort -k1,1 -k4,4n | bedtools merge <span class=""hljs-_"">-d</span> 1000000 <span class=""hljs-_"">-s</span> -c 5,6,7 -o first &gt; testdata/ldb3.bedintersectBed -b testdata/ldb3.bed <span class=""hljs-_"">-a</span> <span class=""hljs-variable"">$gtf</span> -wa <span class=""hljs-_"">-s</span> &gt; testdata/ldb3.gtf<span class=""hljs-keyword"">for</span> f <span class=""hljs-keyword"">in</span> stringtie_guided/*/psi.txt; mkdir testdata/(basename (dirname <span class=""hljs-variable"">$f</span>)); intersectBed -b testdata/ldb3.bed <span class=""hljs-_"">-a</span> <span class=""hljs-variable"">$f</span> -wa <span class=""hljs-_"">-s</span> &gt; testdata/(basename (dirname <span class=""hljs-variable"">$f</span>))/psi.txt;end</code></pre>","['suppressPackageStartupMessages({\n  library(stringr)\n  library(readr)\n  library(dplyr)\n  library(rtracklayer)\n  library(tidyr)\n})\n\nargs <- commandArgs(TRUE)\nout_path <- args[1]\nmerged_gtf <- args[2]\npsi_base_path <- args[3]\n\ndir.create(out_path, showWarnings = FALSE)\n\ncolors <-\n  c(\n    ""black"",\n    ""darksalmon"",\n    ""darkred"",\n    ""lightblue"",\n    ""blue"",\n    ""#E6AB02""\n  )\n\nplot_psi <- function(exons) {\n  tx_name <- names(exons)\n  exons <- exons[[1]]\n  exons <- reduce(exons)\n  message(tx_name, "" "", length(exons), "" exons\\n"")\n  hits <- mergeByOverlaps(exons, gr2, type = ""equal"")\n  exons <- GRanges(hits$exons)\n  mcols(exons) <- hits[, 3:ncol(hits)]\n  if (names(which.max(table(strand(exons)))) == ""-"") {\n    exons <- sort(exons, decreasing = T)\n  }\n  Func.plot(exons, tx_name)\n}\n\nselect_longest_transcript <- function(exons) {\n  if (length(unique(exons$gene_name)) > 1) {\n    stop(""Multiple genes in exons table. Please pass a single gene."")\n  }\n\n  tmp <- exons %>%\n    as.data.frame() %>%\n    group_by(transcript_name) %>%\n    summarise(len = sum(width))\n\n  tmp[which.max(tmp$len), ][[""transcript_name""]]\n}\n\nselect_largest_transcript <- function(exons) {\n  if (length(unique(exons$gene_name)) > 1) {\n    stop(""Multiple genes in exons table. Please pass a single gene."")\n  }\n\n  tmp <- exons %>%\n    mcols() %>%\n    as.data.frame() %>%\n    group_by(transcript_name) %>%\n    summarise(n_exons = n_distinct(exon_id))\n\n  tmp[which.max(tmp$n_exons), ][[""transcript_name""]]\n}\n\n\nFunc.plot <-\n  function(SetOfExons, tx_name) {\n    runOfIDX <- numeric()\n\n    if (as.character(SetOfExons[1]@strand) == ""-"") {\n      runOfIDX <- length(SetOfExons):1\n    } else {\n      runOfIDX <- 1:length(SetOfExons)\n    }\n\n    df <- as.data.frame(SetOfExons)\n\n    tmp.exonNum <- nrow(df)\n\n    tmp.factor <- 5\n    tmp.intStart <- df[1, ""start""]\n    tmp.intLength <- df[tmp.exonNum, ""end""] - tmp.intStart\n\n    tmp.scaleFactor <- tmp.exonNum * tmp.factor / tmp.intLength\n\n    out <- str_glue(""{out_path}{tx_name}.pdf"") # file.path(out_path, tx_name, \'.pdf\')\n    pdf(\n      out,\n      height = 10,\n      width = min(max(10, floor(tmp.exonNum / 10)), 40)\n    )\n\n    plot(\n      0,\n      0,\n      col = ""white"",\n      xlim = c(0, (tmp.exonNum + 10) * tmp.factor),\n      ylim = c(0, 3000),\n      xlab = tx_name,\n      ylab = c(""""),\n      axes = 0\n    )\n    lines(\n      c(0, tmp.exonNum * tmp.factor),\n      c(2900, 2900),\n      lwd = 2,\n      col = ""grey""\n    )\n    for (i in 1:tmp.exonNum) {\n      rect(floor((df[i, ""start""] - tmp.intStart) * tmp.scaleFactor),\n        2880,\n        floor((df[i, ""end""] - tmp.intStart) * tmp.scaleFactor),\n        2920,\n        col = ""grey""\n      )\n\n      lines(c(tmp.factor * i, floor((\n        df[i, ""start""] - tmp.intStart\n      ) * tmp.scaleFactor)),\n      c(2000, 2880),\n      lwd = 0.5,\n      col = ""grey""\n      )\n\n      lines(c(tmp.factor * i, tmp.factor * i),\n        c(2000, 200),\n        lwd = 0.5,\n        col = ""grey""\n      )\n\n      if (i == 1) {\n        text(tmp.factor * i,\n          200,\n          paste0(""E"", runOfIDX[i]),\n          col = ""red"",\n          cex = 0.8\n        )\n      } else {\n        if (tmp.exonNum > 5 &&\n          i %% 5 == 0) {\n          text(tmp.factor * i,\n            200,\n            paste0(""E"", runOfIDX[i]),\n            col = ""red"",\n            cex = 0.8\n          )\n        }\n      }\n    }\n\n    text(1, 2000, ""100"", cex = 0.8, col = ""red"")\n\n    text(1, 1100, ""50"", cex = 0.8, col = ""red"")\n\n    text(-4,\n      1100,\n      ""PSI"",\n      cex = 0.8,\n      col = ""red"",\n      srt = 90\n    )\n\n    text(1, 200, ""0"", cex = 0.8, col = ""red"")\n\n    lines(\n      1:tmp.exonNum * tmp.factor,\n      1800 * values(SetOfExons)$""WT.PBS"" + 200,\n      lwd = 1.5,\n      col = colors[1]\n    )\n    lines(\n      1:tmp.exonNum * tmp.factor,\n      1800 * values(SetOfExons)$""WT.ASO"" + 200,\n      lwd = 1.5,\n      col = colors[2]\n    )\n    lines(\n      1:tmp.exonNum * tmp.factor,\n      1800 * values(SetOfExons)$""KO.PBS"" + 200,\n      lwd = 1.5,\n      col = colors[3]\n    )\n    lines(\n      1:tmp.exonNum * tmp.factor,\n      1800 * values(SetOfExons)$""KO.ASO"" + 200,\n      lwd = 1.5,\n      col = colors[4]\n    )\n\n    legend(\n      ""right"",\n      legend = c(\n        ""WT.PBS"",\n        ""WT.ASO"",\n        ""KO.PBS"",\n        ""KO.ASO""\n      ),\n      fill = colors[1:5]\n    )\n    lines(c(0, 1000) * tmp.scaleFactor, c(3000, 3000), col = ""red"") # 1kb scale\n    text(\n      x = 5,\n      y = 2950,\n      ""1kb"",\n      col = ""red"",\n      cex = 0.5\n    )\n\n    dev.off()\n  }\n\nmessage(""Loading data"")\ngtf <- rtracklayer::import(merged_gtf)\n\nfnames <- Sys.glob(""stringtie_guided/*/psi.txt"")\ncond_names <- stringr::str_split(fnames, ""/"", simplify = T)\ncond_names <- cond_names[, ncol(cond_names) - 1]\nmessage(cond_names)\n\ndata <-\n  lapply(\n    fnames,\n    read_tsv,\n    col_names = c(\n      ""chr"",\n      ""start"",\n      ""end"",\n      ""score"",\n      ""name"",\n      ""strand"",\n      ""a"",\n      ""b"",\n      ""c"",\n      ""psi""\n    ),\n    col_types = c(chr = ""c"", strand = ""c"", psi = ""d"")\n  )\nmessage(""Processing data"")\nnames(data) <- cond_names\ndata <- bind_rows(data, .id = ""sample"")\ndata$sample <- str_split(data$sample, ""_"", simplify = T)[, 1]\n\ngr2 <- data %>%\n  dplyr::select(sample, chr, start, end, strand, psi) %>%\n  group_by(sample, chr, start, end, strand) %>%\n  summarise(mean_psi = mean(psi)) %>%\n  spread(key = sample, value = mean_psi) %>%\n  ungroup()\n\nexons <- gr2 %>%\n  str_glue_data(""{chr}:{start}-{end}:{strand}"")\n\nexons2genes <- setNames(gtf$gene_name, as.character(gtf))\n\ngr2[""gene_name""] <- exons2genes[exons]\n\ngr2 <- GRanges(data.frame(gr2))\ngtf <- gtf[!is.na(gtf$gene_name), ]\n\nexons <- subset(gtf, type == ""exon"" & transcript_biotype == ""protein_coding"")\n\ne <- split(exons, exons$transcript_name)\n\ne <- e[lapply(e, length) >= 2]\n\nmessage(""Starting to plot "")\nlapply(seq_along(e), function(i) try(plot_psi(e[i])))\n']","Source code to produce the PSI plots <h1 id=""source-code-to-produce-the-psi-plots"">Source code to produce the PSI plots</h1><p>The scripts were used to generate the PSI plots to appear in Radke, M and Badillo-Lisakowski, V et al &quot;Therapeutic inhibition of RBM20 improves diastolic function in a murine heart failure model and human engineered heart tissue&quot;.deposited for reproducibility purposes.</p><p>This repository was updated on 27.01.2022 to include the <code>Snakefile</code> and <code>compute_PSI.pl</code>.</p><p>The workflow is based on Snakemake, see <code>Snakefile</code> for more information.Workflow was called with:</p><pre><code class=""lang-bash"">snakemake -<span class=""ruby"">-configfile config.yml</span> -<span class=""ruby"">-printshellcmds</span> -<span class=""ruby"">-cluster <span class=""hljs-string"">""sbatch</span></span> -<span class=""ruby""><span class=""hljs-string"">-mem {cluster.mem}</span></span> -<span class=""ruby""><span class=""hljs-string"">-out {cluster.out}</span></span> -<span class=""ruby""><span class=""hljs-string"">-error {cluster.out}</span></span> -<span class=""ruby""><span class=""hljs-string"">c {cluster.c}</span></span> -<span class=""ruby""><span class=""hljs-string"">-parsable ""</span></span> -<span class=""ruby"">-cluster-config cluster.yaml</span> -<span class=""ruby"">-jobs <span class=""hljs-number"">10</span></span></code></pre><h2 id=""dependencies-"">Dependencies:</h2><ul><li>stringtie/1.3.5</li><li>cufflinks/2.2.1</li><li>python2</li><li>R/3.5.1</li><li>perl</li><li>bedtools</li></ul><p>Dependencies were handled with Environment Modules on the Snakefile.</p><h2 id=""workflow-steps-"">Workflow steps:</h2><ol><li><p>Guided transcriptome assembly with StringTie:</p></li><li><p>Parameters:</p><ul><li>min_isoform_fraction: 0.2</li><li>min_anchor_length: 8</li><li>min_junction_coverage: 3</li><li>reference: /biodb/genomes/mus_musculus/GRCm38_85/GRCm38.85.gtfThe modified GRCm38 84 annotation accopanies this repository. The version of Ensembl has been used to study Titin and match the exons to domains.</li></ul></li><li><p>Merge annotation with cuffmerge:</p></li><li><p>Parameters:</p><ul><li>reference: /biodb/genomes/mus_musculus/GRCm38_85/GRCm38.85.gtf</li><li>min_isoform_fraction: 0.2</li></ul></li><li><p>Find &#39;containers&#39;, loci with that contains alternative splicing events, and calculate PSI from STAR <code>SJ.out.tab</code> intron counts with compute_PSI.plPSI is define by the ratio between the number of reads including an exon and total reads for that event (reads including + reads excluding).</p></li><li><p>Produce the plots using PSI_plot_aso.R script:One plot per gene, only for gene with two or more exons, in addiiton:</p></li><li>Removde duplicated exons</li><li>Merged overlapping exons</li></ol><p>The accompanying test data reproduces figure 4.b of the paper:<code>Rscript testdata/PSI_plot_aso.R testdata/output/ testdata/ldb3.gtf testdata/</code></p><p>The subset of the data was prepared with:</p><pre><code class=""lang-{fish}""><span class=""hljs-built_in"">set</span> gtf <span class=""hljs-string"">""/beegfs/prj/Mitch_Gotthardt/Thiago/psi_plots/Mus_musculus.GRCm38.84_cleaned.gtf""</span>grep <span class=""hljs-string"">'gene_name ""Ldb3"";'</span> <span class=""hljs-variable"">$gtf</span> | sort -k1,1 -k4,4n | bedtools merge <span class=""hljs-_"">-d</span> 1000000 <span class=""hljs-_"">-s</span> -c 5,6,7 -o first &gt; testdata/ldb3.bedintersectBed -b testdata/ldb3.bed <span class=""hljs-_"">-a</span> <span class=""hljs-variable"">$gtf</span> -wa <span class=""hljs-_"">-s</span> &gt; testdata/ldb3.gtf<span class=""hljs-keyword"">for</span> f <span class=""hljs-keyword"">in</span> stringtie_guided/*/psi.txt; mkdir testdata/(basename (dirname <span class=""hljs-variable"">$f</span>)); intersectBed -b testdata/ldb3.bed <span class=""hljs-_"">-a</span> <span class=""hljs-variable"">$f</span> -wa <span class=""hljs-_"">-s</span> &gt; testdata/(basename (dirname <span class=""hljs-variable"">$f</span>))/psi.txt;end</code></pre>",0
Bioinformatics Code: Dysfunctional ERG signaling drives pulmonary vascular aging and persistent fibrosis,Bioinformatics processing code for Epigenomics and RNA analysis and integation.,"['library(plyr)\nlibrary(readxl)\n\n# DiffBind\nATAC_SY_BY <- read.delim(""ATAC_DIFF/DBA_Sham-YOUNG_vs_Bleo-YOUNG.diffbind.p1f0.all.sigsites.annotated.tsv"")\n# bleo young base\n\nATAC_SY_SO <- read.delim(""ATAC_DIFF/DBA_Sham-YOUNG_vs_Sham-OLD.diffbind.p1f0.all.sigsites.annotated.tsv"")\n# sham old base\n\nATAC_SY_BO <- read.delim(""ATAC_DIFF/DBA_Sham_YOUNG_vs_Bleo_OLD.diffbind.p1f0.all.sigsites.annotated.tsv"")\n# bleo old base\n\n# RNA\nRNA_SY_BY <- read.delim(""RNA_DGE/raw.results/young.bleo.vs.sham.tsv"")\n# sham young base\n\nRNA_SY_SO <- read_xlsx(""sham.old.vs.young NEW ANALYSIS.xlsx"",sheet = ""sham.old.vs.young"")\n# sham young base\n\nRNA_SY_BO <- read.delim(""RNA_DGE/../old.bleo.vs.young.sham/results/old.bleo.vs.young.sham.tsv"")\n# sham young base\n\n\n\nATAC_SY_BY <- ATAC_SY_BY[,c(""Chr"",""Start"",""End"",""Fold"",""p.value"",""FDR"",""Annotation"",""Detailed.Annotation"",""Distance.to.TSS"",""Gene.Name"")]\nATAC_SY_SO <- ATAC_SY_SO[,c(""Chr"",""Start"",""End"",""Fold"",""p.value"",""FDR"",""Annotation"",""Detailed.Annotation"",""Distance.to.TSS"",""Gene.Name"")]\nATAC_SY_BO <- ATAC_SY_BO[,c(""Chr"",""Start"",""End"",""Fold"",""p.value"",""FDR"",""Annotation"",""Detailed.Annotation"",""Distance.to.TSS"",""Gene.Name"")]\n\nATAC_SY_BY <- ATAC_SY_BY[which(abs(ATAC_SY_BY$Distance.to.TSS) < 1001),]\nATAC_SY_SO <- ATAC_SY_SO[which(abs(ATAC_SY_SO$Distance.to.TSS) < 1001),]\nATAC_SY_BO <- ATAC_SY_BO[which(abs(ATAC_SY_BO$Distance.to.TSS) < 1001),]\n\ncolnames(ATAC_SY_BY)[10] <- ""GeneName""\ncolnames(ATAC_SY_SO)[10] <- ""GeneName""\ncolnames(ATAC_SY_BO)[10] <- ""GeneName""\n\nRNA_SY_BY <- RNA_SY_BY[,c(""GeneId"",""GeneName"",""log2FoldChange"",""pvalue"",""padj"")]\nRNA_SY_SO <- RNA_SY_SO[,c(""GeneId"",""GeneName"",""log2FoldChange"",""pvalue"",""padj"")]\nRNA_SY_BO <- RNA_SY_BO[,c(""GeneId"",""GeneName"",""log2FoldChange"",""pvalue"",""padj"")]\n\n\nINT_SY_BY <- join(ATAC_SY_BY,RNA_SY_BY)\nINT_SY_SO <- join(ATAC_SY_SO,RNA_SY_SO)\nINT_SY_BO <- join(ATAC_SY_BO,RNA_SY_BO)\n\n\nINT_SY_BY$Fold <- INT_SY_BY$Fold * -1\nINT_SY_SO$Fold <- INT_SY_SO$Fold * -1\nINT_SY_BO$Fold <- INT_SY_BO$Fold * -1\n\nINT_SY_BY <- INT_SY_BY[!is.na(INT_SY_BY$GeneId),]\nINT_SY_SO <- INT_SY_SO[!is.na(INT_SY_SO$GeneId),]\nINT_SY_BO <- INT_SY_BO[!is.na(INT_SY_BO$GeneId),]\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(ggpubr)\n\nlm_eqn <- function(df){\n  m <- lm(log2FoldChange ~ Fold, df);\n  eq <- substitute(italic(y) == a + b %.% italic(x)*"",""~~italic(r)^2~""=""~r2, \n                   list(a = format(unname(coef(m)[1]), digits = 2),\n                        b = format(unname(coef(m)[2]), digits = 2),\n                        r2 = format(summary(m)$r.squared, digits = 3)))\n  as.character(as.expression(eq));\n}\n\nlm_eqn(INT_SY_BO)\n\nintegration_plotter <- function(x,y){\n  int_df <- x\n  z <- gsub("" "",""_"",y)\n  p1 <- ggplot(int_df,aes(x=Fold,y=log2FoldChange)) + geom_point(aes(alpha=0.1),show.legend = F) +\n     xlab(""ATAC Fold Change"") + ylab(""RNA Fold Change"") + ggtitle(paste0(y,"" Unfiltered"")) +\n    geom_vline(xintercept = 0, color=""red"",linetype=""dashed"") + geom_hline(yintercept = 0, color =""red"",linetype=""dashed"") + ylim(c(-6,6)) + xlim(c(-3,3)) +\n    geom_smooth(method = ""lm"", se=FALSE, color=""black"", formula = y ~ x) + geom_text(x = -2, y = 6, label = lm_eqn(int_df), parse = TRUE)\n  \n  # write.table(int_df,paste0(""OutputTables/"",z,""_All_int.tsv""),sep = ""\\t"",quote = F,row.names = F,col.names = T)\n  \n  atac_filt <- int_df[which(abs(int_df$Fold) >= 0.5),]\n  atac_filt <- atac_filt[which(abs(atac_filt$p.value) < 1e-5),]\n  p2 <- ggplot(atac_filt,aes(x=Fold,y=log2FoldChange)) + geom_point(aes(alpha=0.1),show.legend = F) +\n    xlab(""ATAC Fold Change"") + ylab(""RNA Fold Change"") + ggtitle(paste0(y,"" ATAC Filtered"")) +\n    geom_vline(xintercept = 0, color=""red"",linetype=""dashed"") + geom_hline(yintercept = 0, color =""red"",linetype=""dashed"") + ylim(c(-6,6)) + xlim(c(-3,3)) +\n    geom_smooth(method = ""lm"", se=FALSE, color=""black"", formula = y ~ x) + geom_text(x = -2, y = 6, label = lm_eqn(atac_filt), parse = TRUE)\n  \n  # write.table(atac_filt,paste0(""OutputTables/"",z,""_ATAC_F05_p1e5.tsv""),sep = ""\\t"",quote = F,row.names = F,col.names = T)\n  \n  atac_p <- int_df[which(abs(int_df$p.value) < 1e-5),]\n  p2.5 <- ggplot(atac_p,aes(x=Fold,y=log2FoldChange)) + geom_point(aes(alpha=0.1),show.legend = F) +\n    xlab(""ATAC Fold Change"") + ylab(""RNA Fold Change"") + ggtitle(paste0(y,"" ATAC p1e-5"")) +\n    geom_vline(xintercept = 0, color=""red"",linetype=""dashed"") + geom_hline(yintercept = 0, color =""red"",linetype=""dashed"") + ylim(c(-6,6)) + xlim(c(-3,3)) +\n    geom_smooth(method = ""lm"", se=FALSE, color=""black"", formula = y ~ x) + geom_text(x = -2, y = 6, label = lm_eqn(atac_p), parse = TRUE)\n  \n  # write.table(atac_p,paste0(""OutputTables/"",z,""_ATAC_p1e5.tsv""),sep = ""\\t"",quote = F,row.names = F,col.names = T)\n  \n  rna_filt <- int_df[which(abs(int_df$log2FoldChange) >= 0.5),]\n  rna_filt <- rna_filt[which(abs(rna_filt$padj) < 0.05),]\n  p3 <- ggplot(rna_filt,aes(x=Fold,y=log2FoldChange)) + geom_point(aes(alpha=0.1),show.legend = F) +\n    xlab(""ATAC Fold Change"") + ylab(""RNA Fold Change"") + ggtitle(paste0(y,"" RNA Filtered"")) +\n    geom_vline(xintercept = 0, color=""red"",linetype=""dashed"") + geom_hline(yintercept = 0, color =""red"",linetype=""dashed"") + ylim(c(-6,6)) + xlim(c(-3,3)) +\n    geom_smooth(method = ""lm"", se=FALSE, color=""black"", formula = y ~ x) + geom_text(x = -2, y = 6, label = lm_eqn(rna_filt), parse = TRUE)\n  \n  write.table(rna_filt,paste0(""OutputTables/"",z,""_RNA_F05_padj05.tsv""),sep = ""\\t"",quote = F,row.names = F,col.names = T)\n  \n  rna_p <- int_df[which(abs(int_df$padj) < 0.05),]\n  p3.5 <- ggplot(rna_p,aes(x=Fold,y=log2FoldChange)) + geom_point(aes(alpha=0.1),show.legend = F) +\n    xlab(""ATAC Fold Change"") + ylab(""RNA Fold Change"") + ggtitle(paste0(y,"" RNA padj0.05"")) +\n    geom_vline(xintercept = 0, color=""red"",linetype=""dashed"") + geom_hline(yintercept = 0, color =""red"",linetype=""dashed"") + ylim(c(-6,6)) + xlim(c(-3,3)) +\n    geom_smooth(method = ""lm"", se=FALSE, color=""black"", formula = y ~ x) + geom_text(x = -2, y = 6, label = lm_eqn(rna_p), parse = TRUE)\n  \n  # write.table(rna_filt,paste0(""OutputTables/"",z,""_RNA_padj05.tsv""),sep = ""\\t"",quote = F,row.names = F,col.names = T)\n  \n  rna_medium <- rna_filt[which(abs(rna_filt$p.value) < 0.05),]\n  p6 <- ggplot(rna_medium,aes(x=Fold,y=log2FoldChange)) + geom_point(aes(alpha=0.1),show.legend = F) +\n    xlab(""ATAC Fold Change"") + ylab(""RNA Fold Change"") + ggtitle(paste0(y,"" RNA & ATAC Filtered"")) +\n    geom_vline(xintercept = 0, color=""red"",linetype=""dashed"") + geom_hline(yintercept = 0, color =""red"",linetype=""dashed"") + ylim(c(-6,6)) + xlim(c(-3,3)) +\n    geom_smooth(method = ""lm"", se=FALSE, color=""black"", formula = y ~ x) + geom_text(x = -2, y = 6, label = lm_eqn(rna_medium), parse = TRUE)\n\n  \n  both_filt <- atac_filt[which(abs(atac_filt$log2FoldChange) >= 0.5),]\n  both_filt <- both_filt[which(abs(both_filt$padj) < 0.05),]\n  p4 <- ggplot(both_filt,aes(x=Fold,y=log2FoldChange)) + geom_point(aes(alpha=0.1),show.legend = F) +\n    xlab(""ATAC Fold Change"") + ylab(""RNA Fold Change"") + ggtitle(paste0(y,"" RNA & ATAC Filtered"")) +\n    geom_vline(xintercept = 0, color=""red"",linetype=""dashed"") + geom_hline(yintercept = 0, color =""red"",linetype=""dashed"") + ylim(c(-6,6)) + xlim(c(-3,3)) +\n    geom_smooth(method = ""lm"", se=FALSE, color=""black"", formula = y ~ x) + geom_text(x = -2, y = 6, label = lm_eqn(both_filt), parse = TRUE)\n  \n  # write.table(both_filt,paste0(""OutputTables/"",z,""_ATAC_F05_p1e5_RNA_F05_padj05.tsv""),sep = ""\\t"",quote = F,row.names = F,col.names = T)\n  \n  both_p <- atac_p[which(abs(atac_p$log2FoldChange) >= 0.5),]\n  both_p <- both_p[which(abs(both_p$padj) < 0.05),]\n  \n  p4.5 <- ggplot(both_p,aes(x=Fold,y=log2FoldChange)) + geom_point(aes(alpha=0.1),show.legend = F) +\n    xlab(""ATAC Fold Change"") + ylab(""RNA Fold Change"") + ggtitle(paste0(y,"" RNA & ATAC padj0.05 p1e-5"")) +\n    geom_vline(xintercept = 0, color=""red"",linetype=""dashed"") + geom_hline(yintercept = 0, color =""red"",linetype=""dashed"") + ylim(c(-6,6)) + xlim(c(-3,3)) +\n    geom_smooth(method = ""lm"", se=FALSE, color=""black"", formula = y ~ x) + geom_text(x = -2, y = 6, label = lm_eqn(both_p), parse = TRUE)\n  \n  # write.table(both_filt,paste0(""OutputTables/"",z,""_ATAC_p1e5_RNA_padj05.tsv""),sep = ""\\t"",quote = F,row.names = F,col.names = T)\n  \n  p6\n  # grid.arrange(p1,p2.5,p2,p3.5,p3,p4.5,p4, nrow=3)\n  \n}\n# debug(integration_plotter)\nintegration_plotter(INT_SY_BY,""Sham Young vs Bleo Young"")\nintegration_plotter(INT_SY_SO,""Sham Young vs Sham Old"")\nintegration_plotter(INT_SY_BO,""Sham Young vs Bleo Old"")\n\n\n\n\n\n\n', 'library(plyr)\nlibrary(dplyr)\n\n# inputFile <- ""diffbind/DBA_Sham-YOUNG_vs_Bleo-YOUNG.diffbind.p1f0.all.sigsites.annotated.tsv""\n# prefix <- ""SY_BY""\n\nargs <- commandArgs(T)\ninputFile <- args[1]\nprefix <- args[2]\n\ninput <- read.delim(inputFile)\n\nnonSig <- input[which(input$p.value >= 0.05),]\nSig <- input[which(input$p.value < 0.05),]\n\nsigUp <- Sig[which(Sig$Fold > 0),]\nsigDown <- Sig[which(Sig$Fold < 0),]\n\nflat <- Sig[which(Sig$Fold == 0),]\n\nUp_In_Con <- sigUp\nDown_In_Con <- sigDown\nnonsignificant <- bind_rows(nonSig,flat)\n\n\nUp_In_Con <- Up_In_Con[,c(1:3)]\nUp_In_Con$Peak <- paste0(""peak_"",seq(1:nrow(Up_In_Con)))\nUp_In_Con$Score <- "".""\nUp_In_Con$Strand <- "".""\n\n\n\nDown_In_Con <- Down_In_Con[,c(1:3)]\nDown_In_Con$Peak <- paste0(""peak_"",seq(1:nrow(Down_In_Con)))\nDown_In_Con$Score <- "".""\nDown_In_Con$Strand <- "".""\n\n\nnonsignificant <- nonsignificant[,c(1:3)]\nnonsignificant$Peak <- paste0(""peak_"",seq(1:nrow(nonsignificant)))\nnonsignificant$Score <- "".""\nnonsignificant$Strand <- "".""\n\n\nwrite.table(Up_In_Con,paste0(prefix,""_UpInCon.bed""),sep = ""\\t"",quote = F,row.names = F,col.names = F)\nwrite.table(Down_In_Con,paste0(prefix,""_DownInCon.bed""),sep = ""\\t"",quote = F,row.names = F,col.names = F)\nwrite.table(nonsignificant,paste0(prefix,""_NS.bed""),sep = ""\\t"",quote = F,row.names = F,col.names = F)\n', 'library(plyr)\noptions(stringsAsFactors = F)\nargs <- commandArgs(T)\ninputFile <- args[1]\n\ndiff_bind_to_homer <- function(x){\n  input <- read.csv(x)\n  strand <- rep(""+"",nrow(input))\n  peak_id <- paste0(""peak_"",seq(1,nrow(input)))\n  intermediate <- input\n  intermediate$peak_id <- peak_id\n  intermediate$strand <- strand\n  output <- intermediate[,c(10,1:3,11,4:9)]\n  output_name <- gsub("".csv"","".h.csv"",x)\n  write.table(output,output_name,quote = F, sep = ""\\t"",row.names = F,col.names = T)\n  return(output)\n}\n# debug(diff_bind_to_homer)\nhomer_bed <- diff_bind_to_homer(inputFile)\noutputFile <- gsub("".csv"","".h.csv"",inputFile)\nannOF <- gsub("".h.csv"","".h.ann.csv"",outputFile)\nsystem(paste0(""/usr/local/biotools/homer/4.10/bin/annotatePeaks.pl "",outputFile,"" mm10 > "",annOF))\n\nbed_joiner <- function(x,y){\n  annotated_bed <- read.delim(y)\n  output <- gsub(""h.ann.csv"",""annotated.tsv"",y)\n  colnames(annotated_bed)[1:5] <- colnames(x)[1:5]\n  joined_bed <- join(x,annotated_bed)\n  joined_bed <- joined_bed[,-1]\n  write.table(joined_bed,output,quote = F, sep = ""\\t"",row.names = F,col.names = T)\n  #return(joined_bed)\n}\n\nbed_joiner(homer_bed,annOF)\nsystem(paste0(""rm "",outputFile,"" "",annOF))\n', '# (""Circos"")\noptions(stringsAsFactors = F)\nlibrary(circlize)\nlibrary(dplyr)\n\narchiveDir <- ""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/""\n\n\n# test_bed <- read.delim(paste0(archiveDir,""12__GioL-ATAC-7.FCH5CK7BBXY_L6_R1_ICTCTCTAC.fastq.gz.PE_macs2_peaks.bed""),header = F,skip = 1)\n# test_bed <- test_bed[,c(1:3,5)]\n# colnames(test_bed) <- c(""chr"",""start"",""stop"",""value"")\n\n### young sham \n# 21 22 23 24 25\nys1 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/21__GioL-ATAC-15.FCH5CK7BBXY_L8_R1_ITGGATCTG.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nys2 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/22__GioL-ATAC-16.FCH5CK7BBXY_L8_R1_ICCGTTTGT.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nys3 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/23__GioL-ATAC-17.FCH5CK7BBXY_L8_R1_ITGCTGGGT.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nys4 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/24__GioL-ATAC-18.FCH5CK7BBXY_L8_R1_IGAGGGGTT.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nys5 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/25__GioL-ATAC-19.FCH5CK7BBXY_L8_R1_IAGGTTGGG.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nyoung_sham <- bind_rows(ys1,ys2,ys3,ys4,ys5)\nyoung_sham <- young_sham[,c(1:3,5)]\ncolnames(young_sham) <- c(""chr"",""start"",""stop"",""value"")\n\n### young bleo\n# 17 18 19 20\nyb1 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/17__GioL-ATAC-11.FCH5CK7BBXY_L7_R1_IAAGAGGCA.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nyb2 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/18__GioL-ATAC-12.FCH5CK7BBXY_L7_R1_IGTAGAGGA.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nyb3 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/19__GioL-ATAC-13.FCH5CK7BBXY_L7_R1_IGTCGTGAT.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nyb4 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/20__GioL-ATAC-14.FCH5CK7BBXY_L8_R1_IACCACTGT.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nyoung_bleo <- bind_rows(yb1,yb2,yb3,yb4)\nyoung_bleo <- young_bleo[,c(1:3,5)]\ncolnames(young_bleo) <- c(""chr"",""start"",""stop"",""value"")\n\n### aged sham\n# 12 13 14 15\nas1 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/12__GioL-ATAC-7.FCH5CK7BBXY_L6_R1_ICTCTCTAC.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nas2 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/13__GioL-ATAC-8.FCH5CK7BBXY_L7_R1_ICAGAGAGG.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nas3 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/14__GioL-ATAC-9.FCH5CK7BBXY_L7_R1_IGCTACGCT.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nas4 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/15__GioL-ATAC-10.FCH5CK7BBXY_L7_R1_ICGAGGCTG.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\naged_sham <- bind_rows(as1,as2,as3,as4)\naged_sham <- aged_sham[,c(1:3,5)]\ncolnames(aged_sham) <- c(""chr"",""start"",""stop"",""value"")\n\n### aged bleo\n# 1 2 3 4 5\nab1 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/1__GioL-ATAC-1.FCH5CK7BBXY_L6_R1_ITAAGGCGA.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nab2 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/2__GioL-ATAC-2.FCH5CK7BBXY_L6_R1_ICGTACTAG.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nab3 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/3__GioL-ATAC-3.FCH5CK7BBXY_L6_R1_IAGGCAGAA.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nab4 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/4__GioL-ATAC-4.FCH5CK7BBXY_L6_R1_ITCCTGAGC.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\nab5 <- read.delim(""/research/bsi/archive/PI/Ligresti_Giovanni_m151312/chipseq/190424_K00203_0269_BH5CK7BBXY/atac_macs/delivery/macs2out/5__GioL-ATAC-5.FCH5CK7BBXY_L6_R1_IGGACTCCT.fastq.gz.PE_macs2_peaks.bed"",\n                  header = F, skip = 1)\n\naged_bleo <- bind_rows(ab1,ab2,ab3,ab4,ab5)\naged_bleo <- aged_bleo[,c(1:3,5)]\ncolnames(aged_bleo) <- c(""chr"",""start"",""stop"",""value"")\n\n####\nrm(ab1,ab2,ab3,ab4,ab5,as1,as2,as3,as4,yb1,yb2,yb3,yb4,ys1,ys2,ys3,ys4,ys5)\n\n# bed_list <- list(aged_bleo,aged_sham,young_bleo,young_sham)\n####\ncircos.initializeWithIdeogram(species = ""mm10"")\ncircos.genomicDensity(young_sham, col = c(""red""), count_by = ""number"", track.height = 0.15)\ncircos.genomicDensity(young_bleo, col = c(""blue""), count_by = ""number"", track.height = 0.15)\ncircos.genomicDensity(aged_sham, col = c(""orange""), count_by = ""number"", track.height = 0.15)\ncircos.genomicDensity(aged_bleo, col = c(""green""), count_by = ""number"", track.height = 0.15)\n\n\ncircos.initializeWithIdeogram(species = ""mm10"")\ncircos.genomicDensity(young_sham, col = c(""red""),window.size = 5e5, count_by = ""number"", track.height = 0.1)\ncircos.genomicDensity(young_bleo, col = c(""blue""),window.size = 5e5, count_by = ""number"", track.height = 0.1)\ncircos.genomicDensity(aged_sham, col = c(""orange""),window.size = 5e5, count_by = ""number"", track.height = 0.1)\ncircos.genomicDensity(aged_bleo, col = c(""green""),window.size = 5e5, count_by = ""number"", track.height = 0.1)\n\n\n#####################\nsetwd(""../PCA"")\nlibrary(DiffBind)  \n\n\n# setwd(""/research/bsi/projects/staff_analysis/m210841/PI/Caporarello_Diffbind/PCA/"")\ninput.sheet <- ""/research/bsi/projects/staff_analysis/m210841/PI/Caporarello_Diffbind/all_samp.csv""\ntest <- read.csv(input.sheet)\ntest$bamReads <- paste0(""/research/bsi/projects/staff_analysis/m210841/PI/Caporarello_Diffbind/PCA/"",test$bamReads)\nentity_raw <- dba(sampleSheet=test, minOverlap =1)\nentity_cnt <- dba.count(entity_raw, minOverlap=1)  \n\n\n###correlation based on affinity matrix\n  # sampleClusterPDF <- paste(""ALL_SAMP"", "".diffbind.cnt_sampleCluster.pdf"", sep = """")\n  # pdf(sampleClusterPDF)\n  # plot(entity_cnt)\n  # dev.off()\n\n\n##global PCA\n  dbfPCAPDF <- paste(""ALL_SAMP"", "".diffbind.cnt_PCAPlot.pdf"",  sep = """")\n  pdf(dbfPCAPDF)\n  dba.plotPCA(entity_cnt,vColors = c(""green"",""orange"",""blue"",""red""),dotSize = 3)\n  dev.off()\n\n\n\n\n###############\n  #FDR < 0.05\n  # abs FC +1\nlibrary(ggplot2)\nlibrary(gridExtra)\nsetwd(""../Combined_DiffBind/"")\ncomparison1 <- read.csv(""DBA_Sham-YOUNG_vs_Bleo-YOUNG.diffbind.p1f0.all.sigsites.csv"")\n# comparison1 <- comparison1[which(comparison1$Conc >= 5),]\na <- ggplot(comparison1, aes(x=Conc,y=Fold*-1)) + geom_point(colour=""blue"") + geom_smooth(se = F,colour=""black"") + ggtitle(""Young Sham vs Young Bleo Mice"") + xlab(""Average ATAC Signal"") + ylab(""Fold Change in Bleo over Sham"")\nb <- ggplot(comparison1, aes(y=Fold*-1)) + geom_density(color=""red"") + xlab(""Density"") + ylab("""")\nc <- grid.arrange(a,b, ncol=2, widths=c(0.7,0.3))\nggsave(""YoungSham_YoungBleo_UNF.png"",c)\n\ncomparison1 <- comparison1[which(comparison1$FDR < 0.05),]\ncomparison1 <- comparison1[which(abs(comparison1$Fold) >= 1),]\na <- ggplot(comparison1, aes(x=Conc,y=Fold*-1)) + geom_point(colour=""blue"") + geom_smooth(se = F,colour=""black"") + ggtitle(""Young Sham vs Young Bleo Mice"") + xlab(""Average ATAC Signal"") + ylab(""Fold Change in Bleo over Sham"")\nb <- ggplot(comparison1, aes(y=Fold*-1)) + geom_density(color=""red"") + xlab(""Density"") + ylab("""")\nc <- grid.arrange(a,b, ncol=2, widths=c(0.7,0.3))\nggsave(""YoungSham_YoungBleo_FILT.png"",c)\n\n#####################\ncomparison1 <- read.csv(""DBA_Sham-YOUNG_vs_Sham-OLD.diffbind.p1f0.all.sigsites.csv"")\na <- ggplot(comparison1, aes(x=Conc,y=Fold*-1)) + geom_point(colour=""blue"") + geom_smooth(se = F,colour=""black"") + ggtitle(""Young Sham vs Aged Sham Mice"") + xlab(""Average ATAC Signal"") + ylab(""Fold Change in Aged over Young"")\nb <- ggplot(comparison1, aes(y=Fold*-1)) + geom_density(color=""red"") + xlab(""Density"") + ylab("""")\nc <- grid.arrange(a,b, ncol=2, widths=c(0.7,0.3))\nggsave(""YoungSham_AgedSham_UNF.png"",c)\n\n\ncomparison1 <- comparison1[which(comparison1$FDR < 0.05),]\ncomparison1 <- comparison1[which(abs(comparison1$Fold) >= 1),]\na <- ggplot(comparison1, aes(x=Conc,y=Fold*-1)) + geom_point(colour=""blue"") + geom_smooth(se = F,colour=""black"") + ggtitle(""Young Sham vs Aged Sham Mice"") + xlab(""Average ATAC Signal"") + ylab(""Fold Change in Aged over Young"")\nb <- ggplot(comparison1, aes(y=Fold*-1)) + geom_density(color=""red"") + xlab(""Density"") + ylab("""")\nc <- grid.arrange(a,b, ncol=2, widths=c(0.7,0.3))\nggsave(""YoungSham_AgedSham_FILT.png"",c)\n\n#################\n\ncomparison1 <- read.csv(""DBA_Sham_YOUNG_vs_Bleo_OLD.diffbind.p1f0.all.sigsites.csv"")\na <- ggplot(comparison1, aes(x=Conc,y=Fold*-1)) + geom_point(colour=""blue"") + geom_smooth(se = F,colour=""black"") + ggtitle(""Young Sham vs Aged Bleo Mice"") + xlab(""Average ATAC Signal"") + ylab(""Fold Change in Bleo over Sham"")\nb <- ggplot(comparison1, aes(y=Fold*-1)) + geom_density(color=""red"") + xlab(""Density"") + ylab("""")\nc <- grid.arrange(a,b, ncol=2, widths=c(0.7,0.3))\nggsave(""YoungSham_AgedBleo_UNF.png"",c)\n\n\ncomparison1 <- comparison1[which(comparison1$FDR < 0.05),]\ncomparison1 <- comparison1[which(abs(comparison1$Fold) >= 1),]\na <- ggplot(comparison1, aes(x=Conc,y=Fold*-1)) + geom_point(colour=""blue"") + geom_smooth(se = F,colour=""black"") + ggtitle(""Young Sham vs Aged Bleo Mice"") + xlab(""Average ATAC Signal"") + ylab(""Fold Change in Bleo over Sham"")\nb <- ggplot(comparison1, aes(y=Fold*-1)) + geom_density(color=""red"") + xlab(""Density"") + ylab("""")\nc <- grid.arrange(a,b, ncol=2, widths=c(0.7,0.3))\nggsave(""YoungSham_AgedBleo_FILT.png"",c)\n\n\npheatmap::pheatmap()', '# sampleID <- ""OutputTables/MostRelevant/MotifAnalysis2/Sham_Young_vs_Bleo_Old_RNA_F05_padj05""\nlm_eqn <- function(df){\n  m <- lm(log2FoldChange ~ Fold, df);\n  eq <- substitute(italic(y) == a + b %.% italic(x)*"",""~~italic(r)^2~""=""~r2, \n                   list(a = format(unname(coef(m)[1]), digits = 2),\n                        b = format(unname(coef(m)[2]), digits = 2),\n                        r2 = format(summary(m)$r.squared, digits = 3)))\n  as.character(as.expression(eq));\n}\nquadrant_merger <- function(x,y){\n  q1 <- paste0(x,"".Q1_erg_motif.out.tsv"")\n  q1 <- read.delim(q1)\n  q2 <- paste0(x,"".Q2_erg_motif.out.tsv"")\n  q2 <- read.delim(q2)\n  q3 <- paste0(x,"".Q3_erg_motif.out.tsv"")\n  q3 <- read.delim(q3)\n  q4 <- paste0(x,"".Q4_erg_motif.out.tsv"")\n  q4 <- read.delim(q4)  \n  \n  q1$target <- F\n  q2$target <- F\n  q3$target <- F\n  q4$target <- F\n  \n  q1$quadrant <- ""q1""\n  q2$quadrant <- ""q2""\n  q3$quadrant <- ""q3""\n  q4$quadrant <- ""q4""\n  \n  q2$target[which(!is.na(q2$Motif.Name))] <- T\n  q3$target[which(!is.na(q3$Motif.Name))] <- T\n  \n  int_df <- bind_rows(q1,q2,q3,q4)\n  rna_filt <- int_df[which(abs(int_df$log2FoldChange) >= 0.5),]\n  rna_filt <- rna_filt[which(abs(rna_filt$padj) < 0.05),]\n  p1 <- ggplot(rna_filt,aes(x=Fold,y=log2FoldChange,color=target)) + geom_point(aes(alpha=0.1),show.legend = F) +\n    xlab(""ATAC Fold Change"") + ylab(""RNA Fold Change"") + ggtitle(paste0(y,"" RNA padj 0.05 Fold 0.5"")) +\n    geom_vline(xintercept = 0, color=""red"",linetype=""dashed"") + geom_hline(yintercept = 0, color =""red"",linetype=""dashed"") + ylim(c(-6,6)) + xlim(c(-3,3)) +\n    geom_smooth(method = ""lm"", se=FALSE, color=""black"", formula = y ~ x) + geom_text(x = -2, y = 6, label = lm_eqn(rna_filt), parse = TRUE)\n  p1 <- p1 + theme(legend.position = ""none"") + scale_color_manual(values=c(""black"", ""red""))\n  \n  write.table(rna_filt,paste0(x,"".Q2-Q3T.motif.tsv""),sep = ""\\t"",col.names = T,row.names = F,quote = F)\n  \n  rna_filt_05 <- rna_filt[which(rna_filt$p.value < 0.05),]\n  p2 <- ggplot(rna_filt_05,aes(x=Fold,y=log2FoldChange,color=target)) + geom_point(aes(alpha=0.1),show.legend = F) +\n    xlab(""ATAC Fold Change"") + ylab(""RNA Fold Change"") + ggtitle(paste0(y,"" RNA padj 0.05 Fold 0.5 ATAC 0.05"")) +\n    geom_vline(xintercept = 0, color=""red"",linetype=""dashed"") + geom_hline(yintercept = 0, color =""red"",linetype=""dashed"") + ylim(c(-6,6)) + xlim(c(-3,3)) +\n    geom_smooth(method = ""lm"", se=FALSE, color=""black"", formula = y ~ x) + geom_text(x = -2, y = 6, label = lm_eqn(rna_filt_05), parse = TRUE)\n  p2 <- p2 + theme(legend.position = ""none"") + scale_color_manual(values=c(""black"", ""red""))\n  write.table(rna_filt_05,paste0(x,"".Q2-Q3T.filt.motif.tsv""),sep = ""\\t"",col.names = T,row.names = F,quote = F)\n  \n  q1$target[which(!is.na(q1$Motif.Name))] <- T\n  q4$target[which(!is.na(q4$Motif.Name))] <- T\n  \n  int_df <- bind_rows(q1,q2,q3,q4)\n  rna_filt <- int_df[which(abs(int_df$log2FoldChange) >= 0.5),]\n  rna_filt <- rna_filt[which(abs(rna_filt$padj) < 0.05),]\n  p3 <- ggplot(rna_filt,aes(x=Fold,y=log2FoldChange,color=target)) + geom_point(aes(alpha=0.1),show.legend = F) +\n    xlab(""ATAC Fold Change"") + ylab(""RNA Fold Change"") + ggtitle(paste0(y,"" RNA padj 0.05 Fold 0.5"")) +\n    geom_vline(xintercept = 0, color=""red"",linetype=""dashed"") + geom_hline(yintercept = 0, color =""red"",linetype=""dashed"") + ylim(c(-6,6)) + xlim(c(-3,3)) +\n    geom_smooth(method = ""lm"", se=FALSE, color=""black"", formula = y ~ x) + geom_text(x = -2, y = 6, label = lm_eqn(rna_filt), parse = TRUE)\n  p3 <- p3 + theme(legend.position = ""none"") + scale_color_manual(values=c(""black"", ""red""))\n  write.table(rna_filt,paste0(x,"".Q1-Q4T.motif.tsv""),sep = ""\\t"",col.names = T,row.names = F,quote = F)\n\n  rna_filt_05 <- rna_filt[which(rna_filt$p.value < 0.05),]\n  p4 <- ggplot(rna_filt_05,aes(x=Fold,y=log2FoldChange,color=target)) + geom_point(aes(alpha=0.1),show.legend = F) +\n    xlab(""ATAC Fold Change"") + ylab(""RNA Fold Change"") + ggtitle(paste0(y,"" RNA padj 0.05 Fold 0.5 ATAC 0.05"")) +\n    geom_vline(xintercept = 0, color=""red"",linetype=""dashed"") + geom_hline(yintercept = 0, color =""red"",linetype=""dashed"") + ylim(c(-6,6)) + xlim(c(-3,3)) +\n    geom_smooth(method = ""lm"", se=FALSE, color=""black"", formula = y ~ x) + geom_text(x = -2, y = 6, label = lm_eqn(rna_filt_05), parse = TRUE)\n  p4 <- p4 + theme(legend.position = ""none"") + scale_color_manual(values=c(""black"", ""red""))\n  write.table(rna_filt_05,paste0(x,"".Q1-Q4T.filt.motif.tsv""),sep = ""\\t"",col.names = T,row.names = F,quote = F)\n  \n  print(table(rna_filt[,c(""quadrant"",""target"")]))\n  \n  print(table(rna_filt_05[,c(""quadrant"",""target"")]))\n  \n  grid.arrange(p1,p2,p3,p4, nrow=2)\n}\n\n\n\n# debug(quadrant_merger)\n# quadrant_merger(""OutputTables/MostRelevant/MotifAnalysis2/Sham_Young_vs_Bleo_Old_RNA_F05_padj05"",""Sham Young vs Bleo Old"")\nquadrant_merger(""OutputTables/MostRelevant/MotifAnalysis3/Sham_Young_vs_Sham_Old_RNA_F05_padj05"",""Sham Young vs Sham Old"")\n# quadrant_merger(""OutputTables/MostRelevant/MotifAnalysis2/Sham_Young_vs_Bleo_Young_RNA_F05_padj05"",""Sham Young vs Bleo Young"")\n', 'library(plyr)\n\nfileDir=""/research/bsi/projects/staff_analysis/m210841/PI/Capo_RNA_Int/OutputTables/MostRelevant/""\nmotifDir=""/research/bsi/projects/staff_analysis/m210841/PI/Capo_RNA_Int/OutputTables/MostRelevant/MotifAnalysis3/""\n\nargs <- commandArgs(T)\n# inputFile=""Sham_Young_vs_Bleo_Old_RNA_F05_padj05.Q2.homer""\n\ninputFile=args[1]\ninputFile <- gsub("".homer"","".tsv"",inputFile)\n\nfullFile <- paste0(fileDir,inputFile)\nfullData <- read.delim(fullFile,header = F)\nfullData <- fullData[which(fullData$V1 != ""Chr""),]\n\ncolnames <- ""Chr\tStart\tEnd\tFold\tp.value\tFDR\tAnnotation\tDetailed.Annotation\tDistance.to.TSS\tGeneName\tGeneId\tlog2FoldChange\tpvalue\tpadj""\ncolnames <- unlist(strsplit(colnames,""\\t""))\ncolnames(fullData) <- colnames\n\nmotifFile <- paste0(motifDir,gsub("".tsv"",""_erg_motif.tsv"",inputFile))\nmotifData <- read.delim(motifFile,header = T)\nmotif_coord <- data.frame(t(sapply(motifData$PositionID,function(x) unlist(strsplit(x,""_""))[2:4])))\ncolnames(motif_coord) <- c(""Chr"",""Start"",""End"")\nmotifOut <- data.frame(motifData,motif_coord)\n\nfinalOut <- join(fullData,motifOut)\n\nwrite.table(finalOut,gsub(""_erg_motif.tsv"",""_erg_motif.out.tsv"",motifFile),sep = ""\\t"",quote = F,col.names = T,row.names = F)\n', '#!/usr/bin/env Rscript\n#\n# Program: ngs.plot.r\n# Purpose: Plot sequencing coverages at different genomic regions.\n#          Allow overlaying various coverages with gene lists.\n#\n# -- by Li Shen, MSSM\n# Created:      Nov 2011.\n#\n\n# library(compiler)\n# enableJIT(3)\n\nngsplot.version <- \'2.63\'\n# Program environment variable.\nprogpath <- ""/research/bsi/tools/biotools/ngsplot/2.63/ngsplot""\nif(progpath == """") {\n  stop(""Set environment variable NGSPLOT before run the program. See README \nfor details.\\n"")\n}\n\nsource(file.path(progpath, \'lib\', \'parse.args.r\'))\nsource(file.path(progpath, \'lib\', \'genedb.r\'))\nsource(file.path(progpath, \'lib\', \'plotlib.r\'))\nsource(file.path(progpath, \'lib\', \'coverage.r\'))\n\n# Deal with command line arguments.\ncmd.help <- function(){\n  cat(""\\nVisit https://github.com/shenlab-sinai/ngsplot/wiki/ProgramArguments101 for details\\n"")\n  cat(paste(""Version:"", ngsplot.version, sep="" ""))\n  cat(""\\nUsage: ngs.plot.r -G genome -R region -C [cov|config]file\\n"")\n  cat(""                  -O name [Options]\\n"")\n  cat(""\\n## Mandatory parameters:\\n"")\n  cat(""  -G   Genome name. Use ngsplotdb.py list to show available genomes.\\n"")\n  cat(""  -R   Genomic regions to plot: tss, tes, genebody, exon, cgi, enhancer, dhs or bed\\n"")\n  cat(""  -C   Indexed bam file or a configuration file for multiplot\\n"")\n  cat(""  -O   Name for output: multiple files will be generated\\n"")\n  cat(""## Optional parameters related to configuration file:\\n"")\n  cat(""  -E   Gene list to subset regions OR bed file for custom region\\n"")\n  cat(""  -T   Image title\\n"")\n  EchoCoverageArgs()\n  EchoPlotArgs()\n  cat(""\\n"")\n}\n\n\n###########################################################################\n#################### Deal with program input arguments ####################\nargs <- commandArgs(T)\n# args <- unlist(strsplit(\'-G mm10 -R tss -C K27M_no_stim_3_GATCAG_L006_R1_001Aligned.out.srt.rmdup.bam -O test -Debug 1\', \' \'))\nargs <-c( ""-G"", ""mm10"", ""-R"", ""bed"", ""-C"", ""config_heatmap.txt"", ""-O"", ""All_Peaks"")\n# Input argument parser.\nargs.tbl <- parseArgs(args, c(\'-G\', \'-C\', \'-R\', \'-O\'))\nif(is.null(args.tbl)){\n  cmd.help()\n  stop(\'Error in parsing command line arguments. Stop.\\n\')\n}\ngenome <- args.tbl[\'-G\']\nreg2plot <- args.tbl[\'-R\']\noname <- args.tbl[\'-O\']\n\ncat(""Configuring variables..."")\n# Load tables of database: default.tbl, dbfile.tbl\ndefault.tbl <- read.delim(file.path(progpath, \'database\', \'default.tbl\'))\ndbfile.tbl <- read.delim(file.path(progpath, \'database\', \'dbfile.tbl\'))\nCheckRegionAllowed(reg2plot, default.tbl)\n\n# Setup variables from arguments.\ncov.args <- CoverageVars(args.tbl, reg2plot)\ncov.args$cores.number <- 1\nattach(cov.args)\nplot.args <- PlotVars(args.tbl)\nattach(plot.args)\n\n# Configuration: coverage-genelist-title relationships.\nctg.tbl <- ConfigTbl(args.tbl, fraglen)\n\n# Setup plot-related coordinates and variables.\nplotvar.list <- SetupPlotCoord(args.tbl, ctg.tbl, default.tbl, dbfile.tbl, \n                               progpath, genome, reg2plot, inttag, flanksize, \n                               samprate, galaxy)\nattach(plotvar.list)\n\n# Setup data points for plot.\npts.list <- SetPtsSpline(pint, lgint)\npts <- pts.list$pts  # data points for avg. profile and standard errors.\nm.pts <- pts.list$m.pts  # middle data points. For pint, m.pts=1.\nf.pts <- pts.list$f.pts  # flanking region data points.\n\n# Setup matrix for avg. profiles.\nregcovMat <- CreatePlotMat(pts, ctg.tbl)\n# Setup matrix for standard errors.\nconfiMat <- CreateConfiMat(se, pts, ctg.tbl)\n\n# Genomic enrichment for all profiles in the config. Use this for heatmaps.\nenrichList <- vector(\'list\', nrow(ctg.tbl))\ncat(""Done\\n"")\n\n# Load required libraries.\ncat(""Loading R libraries"")\nif(!suppressMessages(require(ShortRead, warn.conflicts=F))) {\n  source(""http://bioconductor.org/biocLite.R"")\n  biocLite(ShortRead)\n  if(!suppressMessages(require(ShortRead, warn.conflicts=F))) {\n    stop(\'Loading package ShortRead failed!\')\n  }\n}\ncat(\'.\')\nif(!suppressMessages(require(BSgenome, warn.conflicts=F))) {\n  source(""http://bioconductor.org/biocLite.R"")\n  biocLite(BSgenome)\n  if(!suppressMessages(require(BSgenome, warn.conflicts=F))) {\n    stop(\'Loading package BSgenome failed!\')\n  }\n}\ncat(\'.\')\nif(!suppressMessages(require(doMC, warn.conflicts=F))) {\n  install.packages(\'doMC\')\n  if(!suppressMessages(require(doMC, warn.conflicts=F))) {\n    stop(\'Loading package doMC failed!\')\n  }\n}\n# Register doMC with CPU number.\nif(cores.number == 0){\n  registerDoMC()\n} else {\n  registerDoMC(cores.number)\n}\ncat(\'.\')\nif(!suppressMessages(require(caTools, warn.conflicts=F))) {\n  install.packages(\'caTools\')\n  if(!suppressMessages(require(caTools, warn.conflicts=F))) {\n    stop(\'Loading package caTools failed!\')\n  }\n}\ncat(\'.\')\nif(!suppressMessages(require(utils, warn.conflicts=F))) {\n  install.packages(\'utils\')\n  if(!suppressMessages(require(utils, warn.conflicts=F))) {\n    stop(\'Loading package utils failed!\')\n  }\n}\ncat(\'.\')\ncat(""Done\\n"")\n\n#######################################################################\n# Here start to extract coverages for all genomic regions and calculate \n# data for plotting.\n\ncat(""Analyze bam files and calculate coverage"")\n# Extract bam file names from configuration and determine if bam-pair is used.\nbfl.res <- bamFileList(ctg.tbl)\nbam.pair <- bfl.res$bbp  # boolean for bam-pair.\nbam.list <- bfl.res$bam.list  # bam file list.\nCheckHMColorConfig(hm.color, bam.pair)\n\n# Determine if bowtie is used to generate the bam files.\n# Index bam files if not done yet.\nv.map.bowtie <- headerIndexBam(bam.list)\n\n# Retrieve chromosome names for each bam file.\nsn.list <- seqnamesBam(bam.list)\n\n# Calculate library size from bam files for normalization.\nv.lib.size <- libSizeBam(bam.list)\n\nv.low.cutoff <- vector(""integer"", nrow(ctg.tbl))  # low count cutoffs.\n# Process the config file row by row.\n# browser()\nfor(r in 1:nrow(ctg.tbl)) {  # r: index of plots/profiles.\n  \n  reg <- ctg.tbl$glist[r]  # retrieve gene list names.\n  # Create coordinate chunk indices.\n  chkidx.list <- chunkIndex(nrow(coord.list[[reg]]), gcs)\n  \n  # Do coverage for each bam file.\n  bam.files <- unlist(strsplit(ctg.tbl$cov[r], \':\'))\n  \n  # Obtain fraglen for each bam file.\n  fraglens <- as.integer(unlist(strsplit(ctg.tbl$fraglen[r], \':\')))\n  \n  # Obtain bam file basic info.\n  libsize <- v.lib.size[bam.files[1]]\n  v.low.cutoff[r] <- low.count / libsize * 1e6\n  result.pseudo.rpm <- 1e6 / libsize\n  sn.inbam <- sn.list[[bam.files[1]]]\n  # chr.tag <- chrTag(sn.inbam)\n  chr.tag <- NA  # do NOT modify the chromosome names.\n  is.bowtie <- v.map.bowtie[bam.files[1]]\n  # if(class(chr.tag) == \'character\') {\n  #     stop(sprintf(""Read %s error: %s"", bam.files[1], chr.tag))\n  # }\n  # browser()\n  # Rprof(""Rprof_covBamExons2.out"", append=T)\n  result.matrix <- covMatrix(debug, chkidx.list, coord.list[[reg]], rnaseq.gb, \n                             exonmodel, libsize, TRUE, chr.tag, pint, \n                             reg2plot, flanksize, flankfactor, m.pts, f.pts, \n                             bufsize, cov.algo, bam.files[1], sn.inbam, \n                             fraglens[1], map.qual, is.bowtie, \n                             strand.spec=strand.spec)\n  # Rprof(NULL)\n  if(bam.pair) {  # calculate background.\n    fraglen2 <- ifelse(length(fraglens) > 1, fraglens[2], fraglens[1])\n    libsize <- v.lib.size[bam.files[2]]\n    bkg.pseudo.rpm <- 1e6 / libsize\n    sn.inbam <- sn.list[[bam.files[2]]]\n    # chr.tag <- chrTag(sn.inbam)\n    chr.tag <- NA\n    is.bowtie <- v.map.bowtie[bam.files[2]]\n    # if(class(chr.tag) == \'character\') {\n    #     stop(sprintf(""Read %s error: %s"", bam.files[2], chr.tag))\n    # }\n    bkg.matrix <- covMatrix(debug, chkidx.list, coord.list[[reg]], rnaseq.gb, \n                            exonmodel, libsize, TRUE, chr.tag, pint, \n                            reg2plot, flanksize, flankfactor, m.pts, f.pts, \n                            bufsize, cov.algo, bam.files[2], sn.inbam, \n                            fraglen2, map.qual, is.bowtie, \n                            strand.spec=strand.spec)\n    # browser()\n    result.matrix <- log2((result.matrix + result.pseudo.rpm) / \n                            (bkg.matrix + bkg.pseudo.rpm))\n  }\n  \n  # Calculate SEM if needed. Shut off SEM in single gene case.\n  if(nrow(result.matrix) > 1 && se){\n    confiMat[, r] <- apply(result.matrix, 2, function(x) CalcSem(x, robust))\n  }\n  \n  # Book-keep this matrix for heatmap.\n  enrichList[[r]] <- result.matrix\n  \n  # Return avg. profile.\n  regcovMat[, r] <- apply(result.matrix, 2, function(x) mean(x, trim=robust, \n                                                             na.rm=T))\n}\n# browser()\ncat(""Done\\n"")\n\n########################################\n# Add row names to heatmap data.\nfor(i in 1:length(enrichList)) {\n  reg <- ctg.tbl$glist[i]  # gene list name.\n  rownames(enrichList[[i]]) <- paste(coord.list[[reg]]$gname, \n                                     coord.list[[reg]]$tid, sep=\':\')\n}\n# Some basic parameters.\nxticks <- genXticks(reg2plot, pint, lgint, pts, flanksize, flankfactor, Labs)\nunit.width <- 4\nng.list <- sapply(enrichList, nrow)  # number of genes per heatmap.\n\n# Create image file and plot data into it.\nif(!fi_tag){\n  cat(""Plotting figures..."")\n  #### Average profile plot. ####\n  if(galaxy) {\n    out.plot <- avgname\n  } else {\n    out.plot <- paste(oname, \'.avgprof.pdf\', sep=\'\')\n  }\n  pdf(out.plot, width=plot.width, height=plot.height, pointsize=font.size)\n  plotmat(regcovMat, ctg.tbl$title, ctg.tbl$color, bam.pair, xticks, pts, \n          m.pts, f.pts, pint, shade.alp, confiMat, mw, prof.misc)\n  out.dev <- dev.off()\n  \n  #### Heatmap. ####\n  # Setup output device.\n  hd <- SetupHeatmapDevice(reg.list, uniq.reg, ng.list, pts, font.size, \n                           unit.width, rr)\n  reg.hei <- hd$reg.hei  # list of image heights for unique regions.\n  hm.width <- hd$hm.width  # image width.\n  hm.height <- hd$hm.height # image height.\n  lay.mat <- hd$lay.mat  # matrix for heatmap layout.\n  heatmap.mar <- hd$heatmap.mar # heatmap margins in inches.\n  \n  if(galaxy) {\n    out.hm <- heatmapname\n  } else {\n    out.hm <- paste(oname, \'.heatmap.pdf\', sep=\'\')\n  }\n  pdf(out.hm, width=hm.width, height=hm.height, pointsize=font.size)\n  par(mai=heatmap.mar)\n  layout(lay.mat, heights=reg.hei)\n  \n  # Do heatmap plotting.\n  go.list <- plotheat(reg.list, uniq.reg, enrichList, v.low.cutoff, go.algo, \n                      go.paras, ctg.tbl$title, bam.pair, xticks, flood.frac, \n                      do.plot=T, hm.color=hm.color, color.distr=color.distr, \n                      color.scale=color.scale)\n  out.dev <- dev.off()\n  cat(""Done\\n"")\n} else {\n  go.list <- plotheat(reg.list, uniq.reg, enrichList, v.low.cutoff, go.algo, \n                      go.paras, ctg.tbl$title, bam.pair, xticks, flood.frac, \n                      do.plot=F, hm.color=hm.color, color.distr=color.distr, \n                      color.scale=color.scale)\n}\n\n# Save plotting data.\nif(galaxy==1) { oname1=""data"" }\ncat(""Saving results..."")\nif(galaxy==1) {\n  dir.create(oname1, showWarnings=F)\n} else {\n  dir.create(oname, showWarnings=F)\n}\n# Average profiles.\nif(galaxy==1){\n  out.prof <- file.path(oname1, \'avgprof.txt\')\n}else{\n  out.prof <- file.path(oname, \'avgprof.txt\')\n}\nwrite.table(regcovMat, file=out.prof, row.names=F, sep=""\\t"", quote=F)\n\n# Standard errors of mean.\nif(!is.null(confiMat)){\n  if(galaxy==1){\n    out.confi <- file.path(oname1, \'sem.txt\')\n  }else{\n    out.confi <- file.path(oname, \'sem.txt\')\n  }\n  write.table(confiMat, file=out.confi, row.names=F, sep=""\\t"", quote=F)\n}\n\n# Heatmap density values.\nfor(i in 1:length(enrichList)) {\n  reg <- ctg.tbl$glist[i]  # gene list name.\n  if(galaxy==1){\n    out.heat <- file.path(oname1, paste(\'hm\', i, \'.txt\', sep=\'\'))\n  }else{\n    out.heat <- file.path(oname, paste(\'hm\', i, \'.txt\', sep=\'\'))\n  }\n  write.table(cbind(coord.list[[reg]][, c(\'gid\', \'gname\', \'tid\', \'strand\')], \n                    enrichList[[i]]), \n              file=out.heat, row.names=F, sep=""\\t"", quote=F)\n}\n\n# Avg. profile R data.\nif(galaxy==1){\n  prof.dat <- file.path(oname1, \'avgprof.RData\')\n}else{\n  prof.dat <- file.path(oname, \'avgprof.RData\')\n}\nsave(plot.width, plot.height, regcovMat, ctg.tbl, bam.pair, xticks, pts, \n     m.pts, f.pts, pint, shade.alp, confiMat, mw, prof.misc, se, v.lib.size, \n     font.size,\n     file=prof.dat)\n\n# Heatmap R data.\nif(galaxy==1) {\n  heat.dat <- file.path(oname1, \'heatmap.RData\')\n} else {\n  heat.dat <- file.path(oname, \'heatmap.RData\')\n}\nsave(reg.list, uniq.reg, ng.list, pts, enrichList, v.low.cutoff, go.algo, \n     ctg.tbl, bam.pair, xticks, flood.frac, hm.color, unit.width, rr, \n     go.list, color.scale, v.lib.size, font.size, go.paras, low.count,\n     color.distr, \n     file=heat.dat)\ncat(""Done\\n"")\n\n# Wrap results up.\ncat(""Wrapping results up..."")\ncur.dir <- getwd()\nif(galaxy==1){\n  out.dir <- dirname(oname1)\n  out.zip <- basename(oname1)\n}else{\n  out.dir <- dirname(oname)\n  out.zip <- basename(oname)\n}\nsetwd(out.dir)\nif(!zip(paste(out.zip, \'.zip\', sep=\'\'), out.zip, extras=\'-q\')) {\n  if(unlink(oname, recursive=T)) {\n    warning(sprintf(""Unable to delete intermediate result folder: %s"", \n                    oname))\n  }\n}\nsetwd(cur.dir)\ncat(""Done\\n"")\ncat(""All done. Cheers!\\n"")\n\n']",Bioinformatics Code: Dysfunctional ERG signaling drives pulmonary vascular aging and persistent fibrosis Bioinformatics processing code for Epigenomics and RNA analysis and integation.,0
Data from: Inferring the age of a fixed beneficial allele,"Estimating the age and strength of beneficial alleles is central to understanding how adaptation proceeds in response to changing environmental conditions. Several haplotype-based estimators exist for inferring the age of segregating beneficial mutations. Here, we develop an approximate Bayesian based approach that rather estimates these parameters for fixed beneficial mutations in single populations. We integrate a range of existing diversity, site frequency spectrum, haplotype and linkage disequilibrium based summary statistics. We show that for strong selective sweeps on de novo mutations the method can estimate allele age and selection strength even in non-equilibrium demographic scenarios. We extend our approach to models of selection on standing variation, and co-infer the frequency at which selection began to act upon the mutation. Finally, we apply our method to estimate the age and selection strength of a previously identified mutation underpinning cryptic color adaptation in a wild deer mouse population, and compare our findings with previously published estimates as well as with geological data pertaining to the presumed shift in selective pressure.","['#######################################################################\n# Joint inference of s and T assuming an equilibrium population\n# Example script for mouse coat color\n\n#There are four sections to this script\n# 1) Run the simulations for ABC inference\n# A file containing 500,000 simulations for this example can be read in at the end of part 1\n# 2) Calculate PLS components from the statistics\n# 3) Run simulations of 100 pseudo observables to establish how well the method works across different orders of magnitude for s and T\n# 4) Apply the method to the mouse data\n\n#Ensure that msms and msstats are installed\n#Set the working directory, and ensure that the necessary text files are located in the same directory\n#setwd(""~/Downloads"")\n\n\n#install and open the required R packages\nlibrary(""pls""); library (""abc""); library(""MASS"")\n\n#1) Run the simulations for ABC inference\n#Identify the parameters for input to the simulations to be run in msms (please refer to msms manual)\n#Here all parameters are from Linnen et al. 2013.  A sequence length of 40kb either side of the serine deletion was used. After filtering for genotype quality a sample size of 48 was obtained.\n#the length of the sequence\nL=80000\n#the effective population size\nNe=53080\n#the sample size \nsamplesize=48\n#the number of replicates (here we use one replicate per draw of s and T)\nnrep=1\n#the position of the selected mutation halfway along the sequence\nSp=0.5\n#Condition on theta, rho and the number of segregating sites\n#theta\nmu=3.67*10^-8\ntheta=4*Ne*mu*L\n#theta=623.3715\n#rho\nr= 0.62*10^-8\nrho=4*Ne*r*L\n#rho=105.3107\n#number of segregating sites\nS=842\n\n  \n#set the filename \nrun=1  # different run numbers can be used to generate parallel sets of simulations\nfilename <- paste(\'mice\',run,sep=\'\')\n\n#run the simulations\nnsim <-10000 # set the number of simulations.  Alternatively, there is a file with 500,000 simulations to be read in at the end of part 1.\n\nmsstats=c()\nfor (i in 1:nsim)  {\n  a <- runif(1, -4, -0.5) # draw the selection coefficient s from a log uniform prior log10(T)~U(-4;-0.5)\n  selec_coef=10^a\n  alpha_homo=2*Ne*selec_coef  #set alpha for the homozygote and heterozygote genotypes using the selection coefficient\n  alpha_hetero=(alpha_homo)/2 \n  b <- runif(1,-4,-0.5)  #draw T from from a log uniform prior log10(T)~U(-4;-0.5)\n  T<-10^b\n  # write these parameters to a file for input to the msms command line and run the msms simulation\n  write.table(data.frame(""alpha_homo""=format(alpha_homo,scientific=F),""alpha_hetero""=format(alpha_hetero,scientific=F),""T""=format(T,scientific=F)), file=paste(\'Inputs_\', filename,\'.txt\',sep=\'\'), quote=F,row.name=F, col.name=F)\n  String <-system(paste(""msms 48 1 -N 53080 -t 623.3715 -r 105.3107 -s 842 -SAA tbs -SAa tbs -Sp 0.5 -SF tbs -SFC < Inputs_"",filename,"".txt"",sep=\'\'), intern=TRUE)\n  #write the SNP output from the msms simulation to a file\n  write (String, file=paste(""string_"",filename,"".txt"",sep=\'\'), sep =""\\t"")\n  #calculate statistics from the msms output using mssstats and write these to a file\n  system (paste(""cat string_"",filename,"".txt | msstats >  msstats_"",filename,"".txt"",sep=\'\'))\n  stats <-read.table(file=paste(""msstats_"",filename,"".txt"",sep=\'\'), header = TRUE)\n  #combine the calculated statistics with the values of s and T and record these in the object msstatstotal \n  stats2 <- cbind(selec_coef,T,stats)\n  msstats <- rbind(msstats,stats2)\n}\nwrite.table (msstats, file=paste(""msstats"",filename,"".txt"",sep=\'\'), sep =""\\t"")\n\n#Here we read in a file containing 500,000 previously run simulations\nmsstats<-read.table(file=""equilibrium_mouse_sims.txt"", header=TRUE) # because of the large size of the file this may take a few minutes\n\n#2) Calculate PLS components from the statistics \n\n#check that all statistics have been calculated correctly \nrow.has.na <- apply(msstats, 1, function(x){any(is.na(x))})\nsum(row.has.na) #ideally this should be 0 so that the log priors are uniformly sampled. \nmsstats <- msstats[!row.has.na,]\n\n#calculate PLS loadings based on a subset of the first 10,000 simulations (adapted from Wegmann et al. 2010)\n#The PLS calculation takes a few minutes.\na<- msstats[1:10000,]\nstat<-a[,4:23]; param<-a[,1:2]; #the number of segregating sites S is excluded from the calculation as we have conditioned on S in the msms simulations\n#remove invariant statistics\nstat<-stat[,-13]\nstat<-stat[,-3]\n\n#standardize the parameters and the statistics\nmymeanparam <- c()\nmysdparam <- c()\nfor(i in 1:length(param)){\n  mymeanparam <- c(mymeanparam, mean(param[,i])); mysdparam <- c(mysdparam, sd(param[,i]));\n  param[,i]<-(param[,i]-mean(param[,i]))/sd(param[,i]);}\nmyMax<-c(); myMin<-c(); lambda<-c(); myGM<-c();\nfor(i in 1:length(stat)){\n  myMax<-c(myMax, max(stat[,i])); myMin<-c(myMin, min(stat[,i]));\n  stat[,i]<-1+(stat[,i] -myMin[i])/(myMax[i]-myMin[ i]);\n}\n# apply Box-Cox transformation to normalize the statistics prior to PLS calculation\nfor(i in 1:length(stat)){\n  d<-cbind(stat[,i], param);\n  mylm<-lm(as.formula(d), data=d);\n  myboxcox<-boxcox(mylm, lambda=seq(-20,100,1/10), interp=T, eps=1/50);\n  lambda<-c(lambda, myboxcox$x[myboxcox$y==max(myboxcox$y)]);\n  myGM<-c(myGM, mean(exp(log(stat[,i]))));\n}\n#standardize the Box-Coxed statistics\nmyBCMeans<-c(); myBCSDs<-c();\nfor(i in 1:length(stat)){\n  stat[,i]<-(stat[,i] ^lambda[i] - 1)/(lambda[i]*myGM[i] ^(lambda[i]-1));\n  myBCSDs<-c(myBCSDs, sd(stat[,i]));\n  myBCMeans<-c(myBCMeans, mean(stat[,i]));\n  stat[,i]<-(stat[,i] -myBCMeans[i])/myBCSDs[i];\n}\n#Calculate the PLS components\nmyPlsr<-plsr(as.matrix(param)~as.matrix(stat), scale=F, validation=\'LOO\');\n#write the PLS information to a file\nmyPlsrDataFrame<-data.frame(comp1=myPlsr$loadings[,1]);\nfor(i in 2:10){myPlsrDataFrame<-cbind(myPlsrDataFrame, myPlsr$loadings[,i])}\nwrite.table(cbind(names(stat), myMax, myMin, lambda, myGM, myBCMeans, myBCSDs,myPlsrDataFrame), file=""plssummary_eqm.txt"", sep=""\\t"", col.names=F,row.names=F, quote=F);\n# the reduction in RMSE for each parameter from using PLS can be visualised\nplot(RMSEP(myPlsr));\n# the component loadings can be generated using\n#loadings(myPlsr)\n\n#the required PLS information for the next step - the ABC inference - can be recovered from the summary file as follows:\nsummary <- read.table(""plssummary_mice_eqm.txt"", header=FALSE)\nmyMax <- summary[,2]\nmyMin <- summary[,3]\nlambda <- summary[,4]\nmyGM <- summary[,5]\nmyBCMeans <- summary[,6]\nmyBCSDs <- summary[,7]\nmyPlsrDataFrame <- summary[,8:length(summary)]\n\n\n#convert the statistics from the simulations into PLS components\nb<-msstats\nscores<-b[,4:23]; param<-b[,1:2];\n#remove invariant statistics (same steps as for PLS calculation)\nscores <- scores[,-13]\nscores <- scores[,-3]\n#standardize the statistics \nfor(i in 1:length(scores)){\n  scores[,i]<- 1+(scores[,i] -myMin[i])/(myMax[i]-myMin[ i]);\n}\n#apply the Box Cox transformation and normalize \nfor(i in 1:length(scores)){\n  scores[,i]<-(scores[,i] ^lambda[i] - 1)/(lambda[i]*myGM[i] ^(lambda[i]-1));\n  scores[,i]<-(scores[,i] -myBCMeans[i])/myBCSDs[i];\n}\n#convert into PLS components (here the top 7 components are used)\nscores <- (as.matrix(scores))%*%(as.matrix(myPlsrDataFrame[,1:7]))\n\n#3) Run simulations of 100 pseudo observables to establish how well the method works across different orders of magnitude for s and T\n#E.g. true values of s=0.1, T=0.01\ns_true=0.1\nalpha_homo=2*s_true*Ne\nalpha_homo\nalpha_hetero=alpha_homo/2\nalpha_hetero\nT_true=0.01\n\n#run the inference for the pseudo-observables and calculate relative bias and RMSE\n#input the values of s_true and T_true into the msms command line \nztot=matrix(0,nrow=100,ncol=100)\ns_box=c()\nT_box=c()\nfor (i in 1:100) {\n  #run the pseudo observable simulation and convert calculated statistics into scores applying the same steps as above\n  system(""msms 48 1 -N 53080 -t 623.3715 -r 105.3107 -s 842 -SAA 10616 -SAa 5308 -Sp 0.5 -SF 0.01 -SFC | msstats >test.txt"")\n  test<-read.table(""test.txt"", header = TRUE)\n  test <- test[,-1]\n  test <- test[,-13]\n  test <- test[,-3]\n  for(i in 1:length(test)){\n    test[,i]<- 1+(test[,i] -myMin[i])/(myMax[i]-myMin[ i]);\n  }\n  for(i in 1:length(test)){\n    test[,i]<-(test[,i] ^lambda[i] - 1)/(lambda[i]*myGM[i] ^(lambda[i]-1));\n    test[,i]<-(test[,i] -myBCMeans[i])/myBCSDs[i];\n  }\n  newtest <- (as.matrix(test))%*%(as.matrix(myPlsrDataFrame[,1:7]))\n  #run the ABC inference using a tolerance of 0.005 and rejection ABC\n  sim<-abc(target = c(newtest[1,c(1,2,3,4,5,6,7)]), param=param[,c(1,2)], sumstat=scores[,c(1,2,3,4,5,6,7)], method =""rejection"", tol=0.005, transf=""none"")\n  post_s=(sim$unadj.values[,1])\n  post_T=(sim$unadj.values[,2])\n  #calculate the joint posterior density and record it for the cumulative joint density plots\n  z <- kde2d(log10(post_s),log10(post_T),lims=c(-4,0,-4,0),n=100)\n  ztot=ztot+z$z\n  maxindex=which(z$z==max(z$z),arr.ind=T)\n  #calculate point estimates of s and T for each pseudo-observable simulation and record these\n  s_pred <- z$x[maxindex[1]]\n  T_pred <- z$y[maxindex[2]]\n  s_box=c(s_box,s_pred)\n  T_box=c(T_box,T_pred)\n}\n\n#Display the cumulative joint density plots from the simulations\npar(mfrow=c(1,1))\nimage(z$x,z$y,ztot,xlab=""log(s)"",ylab=""log(T)"", main=""Cumulative joint distribution"", cex.main=1, cex.lab=1)\npoints(x=log10(s_true),y=log10(T_true),pch=4, cex=2)\n\n#Calculate relative bias and RMSE\nrelbias_s=(mean((10^s_box)-s_true))/s_true\nRMSE_s=sqrt((sum((10^s_box)-s_true)^2)/100)\nrelbias_T=(mean((10^T_box)-T_true))/T_true\nRMSE_T=sqrt((sum((10^T_box)-T_true)^2)/100)\nrelbias_s\nRMSE_s\nrelbias_T\nRMSE_T\n\n#4) Apply the method to the mouse data \n\n#calculate statistics from the SNP mouse data\nsystem ((""cat mousedata80kb.txt | msstats > msstatstest.txt""), intern=TRUE)\n\ntest <-read.table(""msstatstest.txt"", header = TRUE)\n#remove S and invariant statistics and convert the statistics using the PLS loadings (same steps as above)\ntest <- test[,-1]\ntest <- test[,-13]\ntest <- test[,-3]\nfor(i in 1:length(test)){\n  test[,i]<- 1+(test[,i] -myMin[i])/(myMax[i]-myMin[ i]);\n}\nfor(i in 1:length(test)){\n  test[,i]<-(test[,i] ^lambda[i] - 1)/(lambda[i]*myGM[i] ^(lambda[i]-1));\n  test[,i]<-(test[,i] -myBCMeans[i])/myBCSDs[i];\n}\nnewtest <- (as.matrix(test))%*%(as.matrix(myPlsrDataFrame[,1:7]))\n\n# Do the ABC inference\nsim<-abc(target = c(newtest[1,c(1,2,3,4,5,6,7)]), param=param, sumstat=scores[,c(1,2,3,4,5,6,7)], method =""rejection"", tol=0.005, transf=""none"")\npost_s=(sim$unadj.values[,1])\npost_T=(sim$unadj.values[,2])\nz <- kde2d(log10(post_s),log10(post_T),lims=c(-4,0,-4,0),n=100)\nmaxindex=which(z$z==max(z$z),arr.ind=T)\ns_pred <- z$x[maxindex[1]]\nT_pred <- z$y[maxindex[2]]\n\n#Display the joint posterior density plot with a point estimate from the mode (indicated by a black cross)\npar(mfrow=c(1,1))\nimage(z,xlab=""log(s)"",ylab=""log(T)"",cex.main=1.5, cex.lab=1.5, main=""P_maniculatus"")\npoints(x=s_pred,y=T_pred, pch=4, cex=2)\n\n#Point estimates for s and T\nEst_s<-10^(s_pred)\nEst_T<-10^(T_pred)\nEst_s\nEst_T\n\n#Generate 95% confidence intervals\nquantile(post_s,probs=c(0.025,0.975))\nquantile(post_T,probs=c(0.025,0.975))\n\n#Display histograms of the marginal posterior density\npar(mfrow=c(1,2))\nhist(log10(post_s), xlim=c(-4,0), main=""Predicted s"", xlab=""log(s)"", breaks=20)\nabline(v=s_pred,b=0, col=""red"", cex=100)\nhist(log10(post_T),xlim=c(-4,0),main=""Predicted T"", xlab=""log(T)"", breaks=20)\nabline(v=T_pred,b=0, col=""red"", cex=100)\n\n\n', '#######################################################################\n# Joint inference of s and T under non-equilibrium demography\n# Example script for mouse coat color\n\n#There are four sections to this script\n# 1) Run the simulations for ABC inference\n# A file containing 500,000 simulations for this example can be read in at the end of part 1\n# 2) Calculate PLS components from the statistics\n# 3) Run simulations of 100 pseudo-observables to establish how well the method works across different orders of magnitude for s and T\n# 4) Apply the method to the mouse data\n\n#Ensure that msms and msstats are installed\n#Set the working directory, and ensure that the required python and text files are located in the same directory\n#setwd(""~/Downloads"")\n\n#install and open the required R packages\nlibrary(""pls""); library (""abc""); library(""MASS"")\n\n#set the filename \nrun=1  # different run numbers can be used to generate parallel sets of simulations\nfilename <- paste(\'mice\',run,sep=\'\')\n\n#create the python files for the script\nsystem(paste(""cp pythonscript1.py "", filename,""1.py"", sep=\'\'))\nsystem(paste(""cp pythonscript2.py "", filename,""2.py"", sep=\'\'))\n\n#1) Run the simulations for ABC inference\n#Identify the parameters for input to the simulations to be run in msms (please refer to msms manual)\n#here all parameters are from Linnen et al. 2013.  A sequence length of 40kb either side of the serine deletion was used. After filtering for genotype quality a sample size of 48 was obtained.\n#the length of the sequence\nL=80000\n#the effective population size\nNe=53080\n#the sample size \nsamplesize=48\n#the number of replicates (here we use one replicate per draw of s and T)\nnrep=1\n#the position of the selected mutation halfway along the sequence\nSp=0.5\n#the starting frequency assuming a model of de novo mutation\nfs=1/Ne\n#Condition on theta, rho and the number of segregating sites\n#theta\nmu=3.67*10^-8\ntheta=4*Ne*mu*L\n#theta=623.3715\n#rho\nr= 0.62*10^-8\nrho=4*Ne*r*L\n#rho=105.3107\n#number of segregating sites\nS=842\n\n#Incorporate the previously inferred demographic scenario into the msms simulations.  Please refer to the msms manual and to PopPlanner (http://www.mabs.at/ewing/msms/popplanner.shtml) for guidance on parameterization.\n#Here we simulate the demographic scenario inferred in Linnen et al. 2013 of a bottleck 2,900 years ago which reduced the population to 0.4% of its original size,\n#followed by an exponential recovery to 65% of its original size, with an assumed generation time of 6 months.\n#The command that we include is ""-I 1 48 -eg 0 1 186.5443 -en 0.0273 1 1.5385"" and we use the -SI switch (rather than the -SF switch) to set the time since the start of selection Ts\n#and the starting frequency of the selected mutation (a de novo mutation model is assumed)\n#use the -oTrace switch to calculate the frequency of the selected mutation in the population, to ensure it is above 99% at the present time (the time of sampling)\n#use the -SFC switch to ensure that the selected mutation is not lost to drift in simulations \n\n#run the simulations\nnsim <-10000 # set the number of simulations.  Alternatively, there is a file with 500,000 simulations to be read in at the end of part 1.\n\nmsstats=c()\nfrequency <- c()\nfor (i in 1:nsim)  {\n  a <- runif(1, -4, -0.5) # draw the selection coefficient s from a log uniform prior log10(T)~U(-4;-0.5)\n  selec_coef=10^a\n  alpha_homo=2*Ne*selec_coef  #set alpha for the homozygote and heterozygote genotypes using the selection coefficient\n  alpha_hetero=(alpha_homo)/2 \n  Tsoj<-(2*(log(2*Ne))/selec_coef)/(4*Ne) # calculate the sojourn time for a mutation of strength s in equilibrium populations to set the limits for the prior for Ts, assuming fixation\n  lowerlimit <- log10(Tsoj)  # set the lower limit for the log uniform prior for Ts\n  upperlimit<-log10(0.3+Tsoj) # set the upper limit for the log uniform prior for Ts\n  b <- runif(1,lowerlimit,upperlimit) #draw Ts from from a log uniform prior \n  Ts<-10^b\n  # write these parameters to a file for input to the msms command line\n  write.table(data.frame(""Ts""=format(Ts,scientific=F),""alpha_homo""=format(alpha_homo,scientific=F),""alpha_hetero""=format(alpha_hetero,scientific=F)),row.names=FALSE, col.names=FALSE, quote=FALSE, file=paste(\'Inputs_\', filename,\'.txt\',sep=\'\'))\n  count<-0\n  count1<-0\n  repeat  {\n    #run the msms simulation, incorporating the demographic scenario and the -oTrace swith to follow the trajectory of the selected mutation in the population to check that the frequency is above 99% at the present time\n    String <-system(paste(""msms 48 1 -N 53080 -t 623.3715 -r 105.3107 -s 842 -I 1 48 -eg 0 1 186.5443 -en 0.0273 1 1.5385 -Sp 0.5 -SI tbs 1 0.000019 -SFC -SAA tbs -SAa tbs -oTrace < Inputs_"",filename,"".txt"",sep=\'\'), intern=TRUE)\n    #write the SNP and frequency tracing output from the msms simulation to a file\n    write (String, file=paste("""",filename,"".txt"",sep=\'\'), sep =""\\t"")\n    #separate the frequency tracing output from the standard msms output and calculate the frequency of the selected mutation in the population\n    system(paste(""python "",filename,""1.py "",filename,"".txt output"",filename,""1.txt"", sep=\'\'))\n    freq=read.table(file=paste(""output"",filename,""1.txt"",sep=\'\'),sep=\'\')\n    freq <- (tail(freq,1)[[3]])\n    #print(freq) # to read the frequency being generated through the simulations\n    count1<-count1 + 1\n    #record the draws of s and T and resulting frequency of the selected muation\n    freqtot <- cbind(freq, selec_coef, Ts)\n    frequency <- rbind(frequency, freqtot)\n    # if the final frequency is above 99% assume the mutation has fixed and calculate statistics from the msms output using mssstats \n    if (freq > 0.99) {\n      count=count+1\n      system(paste(""python "",filename,""2.py "",filename,"".txt output"",filename,""2.txt"", sep=\'\'))\n      system (paste(""cat output"", filename,""2.txt | msstats >  msstats_"",filename,"".txt"",sep=\'\'))\n      # write the statistics to a file and move to next random draw of input variables\n      stats <-read.table(file=paste(""msstats_"",filename,"".txt"",sep=\'\'), header = TRUE)\n      stats2 <- cbind(selec_coef,Ts,Tsoj,stats)\n    }\n    if (count > 0) break\n    # move to the next random draw of Ts and s from the priors if a frequency greater than 99% is not obtained in 100 simulations (to stop the simulations getting stuck; because the prior for Ts is set after taking into account Tsoj, this is not expected to happen)\n    #(all draws of parameters are recorded in the object frequency to keep a record of how well the priors are sampled)\n    if (count1 > 99) break\n  }\n  msstats <- rbind(msstats,stats2)\n}\n#write the statistics from the simulations with the corresponding values of s, Ts and Tsoj in a file\nwrite.table (msstats, file=paste(""msstats"",filename,"".txt"",sep=\'\'), sep =""\\t"")\n#keep a record of the frequencies generated for each random draw of s and Ts \nwrite.table (frequency, file=paste(""frequency"",filename,"".txt"",sep=\'\'), sep =""\\t"")\n\n#Here we read in a file with the results of 500,000 previously run simulations\nmsstats<-read.table(file=""non_equilibrium_mouse_sims.txt"", header=TRUE) # because of the large size of the file, this may take a few minutes.\n\n#2) Calculate PLS components for the statistics \n\n#check that all statistics have been calculated correctly \nrow.has.na <- apply(msstats, 1, function(x){any(is.na(x))})\nsum(row.has.na) #ideally this should be 0 so that the log priors are uniformly sampled. \nmsstats <- msstats[!row.has.na,]\n\n#calculate PLS loadings based on a subset of the first 10,000 simulations (script adapted from Wegmann et al. 2010).  \n#The PLS calculation takes a few minutes.\na<- msstats[1:10000,]\nstat<-a[,5:24]; param<-a[,1:2]; #the number of segregating sites S is excluded from the calculation as we have conditioned on S in the msms simulations\n#remove invariant statistics\nstat<-stat[,-13]\nstat<-stat[,-3]\n\n#standardize the parameters and the statistics\nmymeanparam <- c()\nmysdparam <- c()\nfor(i in 1:length(param)){\n  mymeanparam <- c(mymeanparam, mean(param[,i])); mysdparam <- c(mysdparam, sd(param[,i]));\n  param[,i]<-(param[,i]-mean(param[,i]))/sd(param[,i]);}\nmyMax<-c(); myMin<-c(); lambda<-c(); myGM<-c();\nfor(i in 1:length(stat)){\n  myMax<-c(myMax, max(stat[,i])); myMin<-c(myMin, min(stat[,i]));\n  stat[,i]<-1+(stat[,i] -myMin[i])/(myMax[i]-myMin[ i]);\n}\n# apply Box-Cox transformation to normalize the statistics prior to PLS calculation\nfor(i in 1:length(stat)){\n  d<-cbind(stat[,i], param);\n  mylm<-lm(as.formula(d), data=d);\n  myboxcox<-boxcox(mylm, lambda=seq(-20,100,1/10), interp=T, eps=1/50);\n  lambda<-c(lambda, myboxcox$x[myboxcox$y==max(myboxcox$y)]);\n  myGM<-c(myGM, mean(exp(log(stat[,i]))));\n}\n#standardize the Box-Coxed statistics\nmyBCMeans<-c(); myBCSDs<-c();\nfor(i in 1:length(stat)){\n  stat[,i]<-(stat[,i] ^lambda[i] - 1)/(lambda[i]*myGM[i] ^(lambda[i]-1));\n  myBCSDs<-c(myBCSDs, sd(stat[,i]));\n  myBCMeans<-c(myBCMeans, mean(stat[,i]));\n  stat[,i]<-(stat[,i] -myBCMeans[i])/myBCSDs[i];\n}\n#Calculate the PLS components\nmyPlsr<-plsr(as.matrix(param)~as.matrix(stat), scale=F, validation=\'LOO\');\n#write the PLS information to a file\nmyPlsrDataFrame<-data.frame(comp1=myPlsr$loadings[,1]);\nfor(i in 2:10){myPlsrDataFrame<-cbind(myPlsrDataFrame, myPlsr$loadings[,i])}\nwrite.table(cbind(names(stat), myMax, myMin, lambda, myGM, myBCMeans, myBCSDs,myPlsrDataFrame), file=""plssummary_non_eqm.txt"", sep=""\\t"", col.names=F,row.names=F, quote=F);\n# the reduction in RMSE for each parameter from using PLS can be visualised\nplot(RMSEP(myPlsr));\n# the component loadings can be generated using\n#loadings(myPlsr)\n\n#the required PLS information for the mice example that is required for the next step - the ABC inference - can be recovered from the summary file as follows:\n#summary <- read.table(""plssummary_mice_non_eqm.txt"", header=FALSE)\n#myMax <- summary[,2]\n#myMin <- summary[,3]\n#lambda <- summary[,4]\n#myGM <- summary[,5]\n#myBCMeans <- summary[,6]\n#myBCSDs <- summary[,7]\n#myPlsrDataFrame <- summary[,8:length(summary)]\n\n\n#convert the statistics from the simulations into PLS components\nb<-msstats\nscores<-b[,5:24]; param<-b[,1:2];\n#remove invariant statistics (same steps as for PLS calculation)\nscores <- scores[,-13]\nscores <- scores[,-3]\n#standardize the statistics \nfor(i in 1:length(scores)){\n  scores[,i]<- 1+(scores[,i] -myMin[i])/(myMax[i]-myMin[ i]);\n}\n#apply the Box Cox transformation and normalize \nfor(i in 1:length(scores)){\n  scores[,i]<-(scores[,i] ^lambda[i] - 1)/(lambda[i]*myGM[i] ^(lambda[i]-1));\n  scores[,i]<-(scores[,i] -myBCMeans[i])/myBCSDs[i];\n}\n#convert into PLS components (here the top 7 components are used)\nscores <- (as.matrix(scores))%*%(as.matrix(myPlsrDataFrame[,1:7])) \n\n#3) Run simulations of 100 pseudo-observables to establish how well the method works across different orders of magnitude for s and T\n#Note that depending on the demographic scenario, the method\'s power may vary across the parameter space.  \n#For example, sweeps older than a bottleneck may be inferred as neutral, or sweeps occuring at the time of the bottleneck may be inferred as neutral\n#Therefore, simulations should be used to establish how well parameters can be inferred for any given demographic scenario prior to applying the method to the data (section 4)\n#See Ormond et al. 2015 for a discussion of how well the method works for the demographic model inferred in Linnen et al. 2013\n\n#E.g. true pseudo-observable values of s=0.1, T=0.001\ns_true=0.1\nalpha_homo=2*s_true*Ne\nalpha_homo\nalpha_hetero=alpha_homo/2\nalpha_hetero\n#calculate sojourn time Tsoj and Ts_true if T_true=0.001 (where T is the time since fixation) for input to the msms pseudo-observable command line\nT_true=0.001\nTsoj<-(2*(log(2*Ne))/s_true)/(4*Ne)  #calculate the sojourn time Tsoj to allow sufficient time for the selected mutation to fix\nTsoj\nTs_true<-T_true+Tsoj \nTs_true \n\n#run the inference for the pseudo-observables and calculate relative bias and RMSE\n#input the values of s_true and T_true into the msms command line \n#the frequency f of the selected mutation is printed to the screen.  The pseudo-observable simulation is only retained if f is higher than 99%.\n#if frequencies below 99% are being consistently generated, stop the pseudo-observable simulation and choose an older Ts to enable sufficient time for the selected mutation to fix \nztot=matrix(0,nrow=100,ncol=100)\ns_box=c()\nTs_box=c()\nfor (i in 1:10)  {\n  count<-0 \n  count1<-0\n  repeat  {\n    #run the pseudo-observable simulation \n    String <-system(paste(""msms 48 1 -N 53080 -t 623.3715 -r 105.3107 -s 842 -I 1 48 -eg 0 1 186.5443 -en 0.0273 1 1.5385 -SAA 10616 -SAa 5308 -Sp 0.5 -SI 0.002 1 0.000019 -SFC -oTrace""), intern=TRUE)\n    write (String, file=paste("""",filename,"".txt"",sep=\'\'), sep =""\\t"")\n    #separate the frequency tracing output from the msms standard output and check that the frequency is above 99% at the present time\n    system(paste(""python "",filename,""1.py "",filename,"".txt output"",filename,""1.txt"", sep=\'\'))\n    freq=read.table(file=paste(""output"",filename,""1.txt"",sep=\'\'),sep=\'\')\n    freq <- (tail(freq,1)[[3]]) \n    print(freq)\n    count1<-count1 + 1\n    # if the final frequency is above 99% calculate the statistics and convert these into scores applying the same steps as above\n    if (freq > 0.99) { \n      count=count+1\n      system(paste(""python "",filename,""2.py "",filename,"".txt output"",filename,""2.txt"", sep=\'\'))\n      system (paste(""cat output"", filename,""2.txt | msstats >  msstats_"",filename,"".txt"",sep=\'\'))\n      test<-read.table(file=paste(""msstats_"", filename, "".txt"", sep=\'\'),header = TRUE)\n      test <- test[,-1]\n      test <- test[,-13]\n      test <- test[,-3]\n      for(i in 1:length(test)){\n        test[,i]<- 1+(test[,i] -myMin[i])/(myMax[i]-myMin[ i]);\n      }\n      for(i in 1:length(test)){\n        test[,i]<-(test[,i] ^lambda[i] - 1)/(lambda[i]*myGM[i] ^(lambda[i]-1));\n        test[,i]<-(test[,i] -myBCMeans[i])/myBCSDs[i];\n      }\n      newtest <- (as.matrix(test))%*%(as.matrix(myPlsrDataFrame[,1:7]))\n      #run the ABC inference using a tolerance of 0.005 and rejection ABC\n      sim<-abc(target = c(newtest[1,c(1,2,3,4,5,6,7)]), param=param[,c(1,2)], sumstat=scores[,c(1,2,3,4,5,6,7)], method =""rejection"", tol=0.01, transf=""none"")\n      post_s=(sim$unadj.values[,1])\n      post_T=(sim$unadj.values[,2])\n      #calculate the joint posterior density and record it for the cumulative joint density plots\n      z <- kde2d(log10(post_s),log10(post_T),lims=c(-4,0,-4,0.7),n=100)\n      ztot=ztot+z$z\n      maxindex=which(z$z==max(z$z),arr.ind=T)\n      #calculate point estimates of s and T for each pseudo-observable simulation from the joint posterior density and record these\n      s_pred <- z$x[maxindex[1]]\n      Ts_pred <- z$y[maxindex[2]]\n      s_box=c(s_box,s_pred)\n      Ts_box=c(Ts_box,Ts_pred)\n    }\n    if (count > 0) break\n    if (count1 > 99) break\n  }  \n}\n\n#Display the cumulative joint density plots from the simulations\npar(mfrow=c(1,1))\nimage(z$x,z$y,ztot,xlab=""log(s)"",ylab=""log(Ts)"", main=""Cumulative joint distribution"", cex.main=1, cex.lab=1)\npoints(x=log10(s_true),y=log10(Ts_true),pch=4, cex=2)\n\n#Calculate relative bias and RMSE\nrelbias_s=(mean((10^s_box)-s_true))/s_true\nRMSE_s=sqrt((sum((10^s_box)-s_true)^2)/100)\nrelbias_Ts=(mean((10^Ts_box)-Ts_true))/Ts_true\nRMSE_Ts=sqrt((sum((10^Ts_box)-Ts_true)^2)/100)\nrelbias_s\nRMSE_s\nrelbias_Ts\nRMSE_Ts\n\n#4) Apply the method to the mouse data \n\n#calculate statistics from the SNP mouse data\nsystem ((""cat mousedata80kb.txt | msstats > msstatstest.txt""), intern=TRUE)\n\ntest <-read.table(""msstatstest.txt"", header = TRUE)\n#remove S and invariant statistics and convert the statistics using the PLS loadings (same steps as above)\ntest <- test[,-1]\ntest <- test[,-13]\ntest <- test[,-3]\nfor(i in 1:length(test)){\n  test[,i]<- 1+(test[,i] -myMin[i])/(myMax[i]-myMin[ i]);\n}\nfor(i in 1:length(test)){\n  test[,i]<-(test[,i] ^lambda[i] - 1)/(lambda[i]*myGM[i] ^(lambda[i]-1));\n  test[,i]<-(test[,i] -myBCMeans[i])/myBCSDs[i];\n}\nnewtest <- (as.matrix(test))%*%(as.matrix(myPlsrDataFrame[,1:7]))\n\n# run the ABC inference\nsim<-abc(target = c(newtest[1,c(1,2,3,4,5,6,7)]), param=param, sumstat=scores[,c(1,2,3,4,5,6,7)], method =""rejection"", tol=0.005, transf=""none"")\npost_s=(sim$unadj.values[,1])\npost_Ts=(sim$unadj.values[,2])\nz <- kde2d(log10(post_s),log10(post_Ts),lims=c(-4,0,-4,0.7),n=100)\nmaxindex=which(z$z==max(z$z),arr.ind=T)\ns_pred <- z$x[maxindex[1]]\nTs_pred <- z$y[maxindex[2]]\n\n#Display the joint posterior density plot with a point estimate from the mode (indicated by a black cross)\nimage(z,xlab=""log(s)"",ylab=""log(Ts)"",cex.main=1, cex.lab=1, main=""P_maniculatus"")\npoints(x=s_pred,y=Ts_pred, pch=4, cex=2)\n\n#Point estimates for s and Ts\nEst_s<-10^(s_pred)\nEst_Ts<-10^(Ts_pred)\nEst_s\nEst_Ts\n\n#Generate 95% confidence intervals\nquantile(post_s,probs=c(0.025,0.975))\nquantile(post_Ts,probs=c(0.025,0.975))\n\n#Display histograms of the marginal posterior distribution\npar(mfrow=c(1,2))\nhist(log10(post_s), xlim=c(-4,0), main=""Predicted s"", xlab=""log(s)"", breaks=20)\nabline(v=s_pred,b=0, col=""red"", cex=100)\nhist(log10(post_Ts),xlim=c(-4,0.7),main=""Predicted Ts"", xlab=""log(Ts)"", breaks=20)\nabline(v=Ts_pred,b=0, col=""red"", cex=100)\n\n\n']","Data from: Inferring the age of a fixed beneficial allele Estimating the age and strength of beneficial alleles is central to understanding how adaptation proceeds in response to changing environmental conditions. Several haplotype-based estimators exist for inferring the age of segregating beneficial mutations. Here, we develop an approximate Bayesian based approach that rather estimates these parameters for fixed beneficial mutations in single populations. We integrate a range of existing diversity, site frequency spectrum, haplotype and linkage disequilibrium based summary statistics. We show that for strong selective sweeps on de novo mutations the method can estimate allele age and selection strength even in non-equilibrium demographic scenarios. We extend our approach to models of selection on standing variation, and co-infer the frequency at which selection began to act upon the mutation. Finally, we apply our method to estimate the age and selection strength of a previously identified mutation underpinning cryptic color adaptation in a wild deer mouse population, and compare our findings with previously published estimates as well as with geological data pertaining to the presumed shift in selective pressure.",0
Hierarchical deployment of Tbx3 dictates the identity of hypothalamic KNDy neurons to control puberty onset,"The database is the detailed codes of bioinformatics analysis in the article which DOI number is ""10.1126/sciadv.abq2987"".","['library(Seurat)\r\nlibrary(patchwork)\r\nlibrary(ggplot2)\r\nlibrary(cowplot)\r\nlibrary(BiocParallel)\r\nlibrary(dplyr)\r\nlibrary(stringr)\r\nlibrary(Matrix)\r\nlibrary(scales)\r\nlibrary(RCurl)\r\n\r\nc1.data <- Read10X(data.dir = ""D:/personal_user/Shyr Shyang/04 TT 10X sequence/cellranger_result/C1/filtered_feature_bc_matrix"")\r\nc1 <- CreateSeuratObject(counts = c1.data, project = ""C1"", min.cells = 3, min.features = 200)\r\nc1\r\n\r\nc2.data <- Read10X(data.dir = ""D:/personal_user/Shyr Shyang/04 TT 10X sequence/cellranger_result/C2/filtered_feature_bc_matrix"")\r\nc2 <- CreateSeuratObject(counts = c2.data, project = ""C2"", min.cells = 3, min.features = 200)\r\nc2\r\n\r\nk1.data <- Read10X(data.dir = ""D:/personal_user/Shyr Shyang/04 TT 10X sequence/cellranger_result/K1/filtered_feature_bc_matrix"")\r\nk1 <- CreateSeuratObject(counts = k1.data, project = ""k1"", min.cells = 3, min.features = 200)\r\nk1\r\n\r\nk2.data <- Read10X(data.dir = ""D:/personal_user/Shyr Shyang/04 TT 10X sequence/cellranger_result/K2/filtered_feature_bc_matrix"")\r\nk2 <- CreateSeuratObject(counts = k2.data, project = ""k2"", min.cells = 3, min.features = 200)\r\nk2\r\n\r\nmerged_seurat <- merge(x = c1, y= list(c2, k1, k2), add.cell.ids = c(""c1"", ""c2"", ""k1"", ""k2""))\r\n\r\n# Check that the merged object has the appropriate sample-specific prefixes\r\nhead(merged_seurat@meta.data)\r\ntail(merged_seurat@meta.data)\r\n\r\n\r\nmerged_seurat$log10GenesPerUMI <- log10(merged_seurat$nFeature_RNA)/log10(merged_seurat$nCount_RNA)\r\n\r\nmerged_seurat$mitoRatio <- PercentageFeatureSet(object = merged_seurat, pattern = ""^mt-"")\r\nmerged_seurat$mitoRatio <- merged_seurat@meta.data$mitoRatio / 100\r\n\r\n\r\n# Create metadata dataframe\r\nmetadata <- merged_seurat@meta.data\r\n\r\n\r\n# Add cell IDs to metadata\r\nmetadata$cells <- rownames(metadata)\r\n\r\n# Rename columns\r\nmetadata <- metadata %>%\r\n  dplyr::rename(seq_folder = orig.ident,\r\n                nUMI = nCount_RNA,\r\n                nGene = nFeature_RNA)\r\n\r\n# Create sample column\r\nmetadata$sample <- NA\r\nmetadata$sample[which(str_detect(metadata$cells, ""^c1_""))] <- ""ctrl1""\r\nmetadata$sample[which(str_detect(metadata$cells, ""^c2_""))] <- ""ctrl2""\r\nmetadata$sample[which(str_detect(metadata$cells, ""^k1_""))] <- ""cKO1""\r\nmetadata$sample[which(str_detect(metadata$cells, ""^k2_""))] <- ""cKO2""\r\n\r\nmetadata$type <- NA\r\nmetadata$type[which(str_detect(metadata$cells, ""^c1_""))] <- ""Ctrl""\r\nmetadata$type[which(str_detect(metadata$cells, ""^c2_""))] <- ""Ctrl""\r\nmetadata$type[which(str_detect(metadata$cells, ""^k1_""))] <- ""CKO""\r\nmetadata$type[which(str_detect(metadata$cells, ""^k2_""))] <- ""CKO""\r\n\r\nmerged_seurat@meta.data <- metadata\r\n\r\n####################################################################### \r\nsaveRDS(merged_seurat, file=""D:/personal_user/Shyr Shyang/04 TT 10X sequence/v3.1 20210430/01 merged_seurat.rds"")\r\n\r\n\r\n# Visualize the number of cell counts per sample\r\nmetadata %>%\r\n  ggplot(aes(x=sample, fill=sample)) +\r\n  geom_bar() +\r\n  theme_classic() +\r\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\r\n  theme(plot.title = element_text(hjust=0.5, face=""bold"")) +\r\n  ggtitle(""NCells"")\r\n\r\n\r\n# Visualize the number UMIs/transcripts per cell\r\nmetadata %>%\r\n  ggplot(aes(color=sample, x=nUMI, fill= sample)) +\r\n  geom_density(alpha = 0.2) +\r\n  scale_x_log10() +\r\n  theme_classic() +\r\n  ylab(""log10 Cell density"") +\r\n  geom_vline(xintercept = 500)\r\n\r\n\r\n# Visualize the distribution of genes detected per cell via histogram\r\nmetadata %>%\r\n  ggplot(aes(color=sample, x=nGene, fill= sample)) +\r\n  geom_density(alpha = 0.2) +\r\n  theme_classic() +\r\n  scale_x_log10() +\r\n  geom_vline(xintercept = 300)\r\n\r\n\r\n# Visualize the distribution of genes detected per cell via boxplot\r\nmetadata %>%\r\n  ggplot(aes(x=sample, y=log10(nGene), fill=sample)) +\r\n  geom_boxplot() +\r\n  theme_classic() +\r\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\r\n  theme(plot.title = element_text(hjust=0.5, face=""bold"")) +\r\n  ggtitle(""NCells vs NGenes"")\r\n\r\n\r\n# Visualize the correlation between genes detected and number of UMIs and determine whether strong presence of cells with low numbers of genes/UMIs\r\nmetadata %>%\r\n  ggplot(aes(x=nUMI, y=nGene, color=mitoRatio)) +\r\n  geom_point() +\r\n  scale_colour_gradient(low = ""gray90"", high = ""black"") +\r\n  stat_smooth(method=lm) +\r\n  scale_x_log10() +\r\n  scale_y_log10() +\r\n  theme_classic() +\r\n  geom_vline(xintercept = 500) +\r\n  geom_hline(yintercept = 600) +\r\n  facet_wrap(~sample)\r\n\r\n\r\n# Visualize the distribution of mitochondrial gene expression detected per cell\r\nmetadata %>%\r\n  ggplot(aes(color=sample, x=mitoRatio, fill=sample)) +\r\n  geom_density(alpha = 0.2) +\r\n  scale_x_log10() +\r\n  theme_classic() +\r\n  geom_vline(xintercept = 0.08)\r\n\r\n\r\n# Visualize the overall complexity of the gene expression by visualizing the genes detected per UMI\r\nmetadata %>%\r\n  ggplot(aes(x=log10GenesPerUMI, color = sample, fill=sample)) +\r\n  geom_density(alpha = 0.2) +\r\n  theme_classic() +\r\n  geom_vline(xintercept = 0.8)\r\n\r\n\r\nmerged_seurat1 <- merged_seurat\r\n\r\n\r\n# Filter out low quality reads using selected thresholds - these will change with experiment\r\nfiltered_seurat <- subset(x = merged_seurat1,\r\n                          subset= (nUMI >= 500) &\r\n                            (nGene >= 600) &\r\n                            (log10GenesPerUMI > 0.80) &\r\n                            (mitoRatio < 0.10))\r\n\r\n# Output a logical vector for every gene on whether the more than zero counts per cell\r\n# Extract counts\r\ncounts <- GetAssayData(object = filtered_seurat, slot = ""counts"")\r\n\r\n# Output a logical vector for every gene on whether the more than zero counts per cell\r\nnonzero <- counts > 0\r\n\r\n# Sums all TRUE values and returns TRUE if more than 10 TRUE values per gene\r\nkeep_genes <- Matrix::rowSums(nonzero) >= 10\r\n\r\n# Only keeping those genes expressed in more than 10 cells\r\nfiltered_counts <- counts[keep_genes, ]\r\n\r\n# Reassign to filtered Seurat object\r\nfiltered_seurat <- CreateSeuratObject(filtered_counts, meta.data = filtered_seurat@meta.data)\r\n\r\n\r\n####################################################################### Create .RData object to load at any time\r\nsaveRDS(filtered_seurat, file=""D:/personal_user/Shyr Shyang/04 TT 10X sequence/v3.1 20210430/02 seurat_filtered.rds"")\r\nfiltered_seurat <- readRDS(""D:/personal_user/Shyr Shyang/04 TT 10X sequence/v3.1 20210430/02 seurat_filtered.rds"")\r\n\r\n#Normalize the counts\r\nseurat_phase <- NormalizeData(filtered_seurat)\r\n\r\n\r\n\r\n# Load cell cycle markers\r\nload( ""E:/scRNA-seq Nkx2.1-Cre Tbx3 floxed mice/raw data from ZH Chen_scRNA-seq_Nkx2.1-Cre; Tbx3 floxed mice/cellranger_result/integrate/mouse_cell_circle_gene.Rdata"")\r\n# Score cells for cell cycle\r\nseurat_phase1 <- CellCycleScoring(seurat_phase,\r\n                                 g2m.features = g2m.genes,\r\n                                 s.features = s.genes)\r\n\r\n# View cell cycle scores and phases assigned to cells\r\nView(seurat_phase1@meta.data)\r\n\r\n\r\n# Identify the most variable genes\r\nseurat_phase1 <- FindVariableFeatures(seurat_phase1,\r\n                                     selection.method = ""vst"",\r\n                                     nfeatures = 2000,\r\n                                     verbose = FALSE)\r\n\r\n# Scale the counts\r\nseurat_phase1 <- ScaleData(seurat_phase1)\r\n\r\n\r\n# Perform PCA\r\nseurat_phase1 <- RunPCA(seurat_phase1)\r\n\r\n\r\n# Plot the PCA colored by cell cycle phase\r\nDimPlot(seurat_phase1,\r\n        reduction = ""pca"",\r\n        group.by= ""Phase"",\r\n        split.by = ""Phase"")\r\n\r\n\r\n# Split seurat object by condition to perform cell cycle scoring and SCT on all samples\r\n####R version 4.0.3, Seurat version 4.0.1####\r\nsplit_seurat <- SplitObject(filtered_seurat, split.by = ""sample"")\r\n\r\nsplit_seurat <- split_seurat[c(""ctrl1"", ""ctrl2"", ""cKO1"", ""cKO2"")]\r\n\r\n\r\n\r\n\r\nsplit_seurat1 <- split_seurat\r\n\r\n  split_seurat1$ctrl1 <- NormalizeData(split_seurat1$ctrl1, verbose = TRUE)\r\n  split_seurat1$ctrl1 <- CellCycleScoring(split_seurat1$ctrl1, g2m.features=g2m.genes, s.features=s.genes)\r\n  split_seurat1$ctrl1 <- SCTransform(split_seurat1$ctrl1, vars.to.regress = c(""mitoRatio""))\r\n\r\n\r\n\r\n\r\nfor (i in 1:length(split_seurat)) {\r\n  split_seurat[[i]] <- NormalizeData(split_seurat[[i]], verbose = TRUE)\r\n  split_seurat[[i]] <- CellCycleScoring(split_seurat[[i]], g2m.features=g2m.genes, s.features=s.genes)\r\n  split_seurat[[i]] <- SCTransform(split_seurat[[i]], vars.to.regress = c(""mitoRatio""))\r\n}\r\n\r\n\r\n# Select the most variable features to use for integration\r\ninteg_features <- SelectIntegrationFeatures(object.list = split_seurat,\r\n                                            nfeatures = 3000)\r\n\r\n# Prepare the SCT list object for integration\r\nsplit_seurat <- PrepSCTIntegration(object.list = split_seurat,\r\n                                   anchor.features = integ_features)\r\n\r\n\r\n# Find best buddies - can take a while to run\r\ninteg_anchors <- FindIntegrationAnchors(object.list = split_seurat,\r\n                                        normalization.method = ""SCT"",\r\n                                        anchor.features = integ_features)\r\n\r\n\r\n# Integrate across conditions\r\nseurat_integrated <- IntegrateData(anchorset = integ_anchors,\r\n                                   normalization.method = ""SCT"")\r\n\r\n####################################################################### Save integrated seurat object\r\nsaveRDS(seurat_integrated, ""D:/personal user/XShi/Nkx2.1-Cre; Tbx3 floxed mice_10X genomics/v2.2/integrated_seurat.rds"")\r\n####R version 4.0.3, Seurat version 4.0.1####\r\n\r\n\r\nseurat_integrated <- readRDS(""D:/personal_user/Shyr Shyang/04 TT 10X sequence/v3.1 20210430/03 integrated_seurat.rds"")\r\nDefaultAssay(seurat_integrated) <- ""RNA""\r\n# Run PCA\r\nseurat_integrated <- RunPCA(object = seurat_integrated)\r\n\r\n\r\n# Plot PCA\r\nPCAPlot(seurat_integrated, split.by = ""sample"")\r\n\r\n\r\n# Run UMAP\r\nseurat_integrated <- RunUMAP(seurat_integrated,\r\n                             dims = 1:20,\r\n                             reduction = ""pca"",\r\n                             min.dist = 0.4)\r\n\r\n\r\n# Plot UMAP\r\nDimPlot(seurat_integrated, label=T)\r\n\r\n\r\n# Explore heatmap of PCs\r\nDimHeatmap(seurat_integrated, \r\n           dims = 1:9, \r\n           cells = 500, \r\n           balanced = TRUE)\r\n\r\n\r\n# Printing out the most variable genes driving PCs\r\nprint(x = seurat_integrated[[""pca""]], \r\n      dims = 1:10, \r\n      nfeatures = 5)\r\n\r\n\r\n# Plot the elbow plot\r\nElbowPlot(object = seurat_integrated, \r\n          ndims = 40)\r\n\r\n\r\n# Determine the K-nearest neighbor graph\r\nseurat_integrated <- FindNeighbors(object = seurat_integrated, \r\n                                   dims = 1:40)\r\n\r\n# Determine the clusters for various resolutions                                \r\nseurat_integrated <- FindClusters(object = seurat_integrated,\r\n                                  resolution = c(1.0))\r\n\r\n\r\nseurat_integrated <- RunUMAP(seurat_integrated, \r\n                  reduction = ""pca"", \r\n                  dims = 1:40, min.dist = 2)\r\n# Plot the UMAP\r\nDimPlot(seurat_integrated,\r\n        reduction = ""umap"",\r\n        label = TRUE, pt.size = 1)\r\n\r\n\r\nn_cells <- FetchData(seurat_integrated, \r\n                     vars = c(""ident"", ""orig.ident"")) %>%\r\n  dplyr::count(ident, orig.ident) %>%\r\n  tidyr::spread(ident, n)\r\n\r\n# View table\r\nView(n_cells)\r\n\r\n\r\n# UMAP of cells in each cluster by sample\r\nDimPlot(seurat_integrated, \r\n        label = TRUE, \r\n        split.by = ""sample"")  + NoLegend()\r\n\r\n\r\n# Explore whether clusters segregate by cell cycle phase\r\nDimPlot(seurat_integrated,\r\n        label = TRUE, \r\n        split.by = ""Phase"")  + NoLegend()\r\n\r\n\r\n# Determine metrics to plot present in seurat_integrated@meta.data\r\nmetrics <-  c(""nUMI"", ""nGene"", ""S.Score"", ""G2M.Score"", ""mitoRatio"")\r\n\r\nFeaturePlot(seurat_integrated, \r\n            reduction = ""umap"", \r\n            features = metrics,\r\n            pt.size = 0.4, \r\n            sort.cell = TRUE,\r\n            min.cutoff = \'q10\',\r\n            label = TRUE)\r\n\r\n\r\n# Defining the information in the seurat object of interest\r\ncolumns <- c(paste0(""PC_"", 1:16),\r\n             ""ident"",\r\n             ""UMAP_1"", ""UMAP_2"")\r\n\r\n# Extracting this data from the seurat object\r\npc_data <- FetchData(seurat_integrated, \r\n                     vars = columns)\r\n\r\n\r\n# Extract the UMAP coordinates for the first 10 cells\r\nseurat_integrated@reductions$umap@cell.embeddings[1:10, 1:2]\r\n#The FetchData() function just allows us to extract the data more easily\r\n\r\n\r\n# Adding cluster label to center of cluster on UMAP\r\numap_label <- FetchData(seurat_integrated, \r\n                        vars = c(""ident"", ""UMAP_1"", ""UMAP_2""))  %>%\r\n  group_by(ident) %>%\r\n  summarise(x=mean(UMAP_1), y=mean(UMAP_2))\r\n\r\n\r\n# Examine PCA results \r\nprint(seurat_integrated[[""pca""]], dims = 1:5, nfeatures = 5)\r\n\r\n\r\nDimPlot(object = seurat_integrated, \r\n        reduction = ""umap"", \r\n        label = TRUE, pt.size = 1) + NoLegend()\r\n\r\n\r\n# Select the RNA counts slot to be the default assay\r\nDefaultAssay(seurat_integrated) <- ""RNA""\r\n\r\n# Normalize RNA data for visualization purposes\r\nseurat_integrated <- NormalizeData(seurat_integrated, verbose = FALSE)\r\n\r\n\r\n####FindConservedMarkers(seurat_integrated, ident.1 = 16, grouping.var = ""sample"", only.pos = TRUE, min.diff.pct = 0.25, min.pct = 0.25, logfc.threshold = 0.25)\r\n\r\n\r\nseurat_integrated.markers <- FindAllMarkers(object =seurat_integrated, only.pos =TRUE,min.pct = 0.25, logfc.threshold = 0.3)\r\n\r\n\r\nFeaturePlot(seurat_integrated, \r\n            reduction = ""umap"", \r\n            features = c(""Stmn2"", ""Gap43"", ""Stmn3"", ""Tubb3""), \r\n            sort.cell = TRUE,\r\n            min.cutoff = \'q10\', \r\n            label = TRUE)\r\n\r\n\r\nDotPlot(object =tap, features = c(""Crym"", ""Mia"", ""Rax"", ""Gpr50"", ""Slc32a1"", ""Slc17a6"", ""Gap43"", ""Stmn2"", ""Tubb3"", ""Epcam"", ""Pitx1"", ""Pou1f1"", ""Slc1a3"", \r\n                                  ""Aqp4"", ""Fgfr3"", ""Olig1"", ""Pdgfra"", ""Sox10"", ""Sytl4"", ""Chga"", ""Scg2"", ""Cldn5"", ""Ctla2a"", \r\n                                  ""Esam"", ""Csf1r"", ""Cx3cr1"", ""Aif1""), dot.scale = 5) +RotatedAxis() +SeuratAxes()\r\n\r\nseurat_integrated_ <- seurat_integrated\r\nneu <- subset(seurat_integrated_1, idents= c(""15"", ""24"", ""28"", ""33""))\r\nneu1 <- subset(seurat_integrated_1, idents= c(""15"", ""24"", ""28""))\r\n\r\n\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1, ""15"" = ""NEU"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""24"" = ""NEU"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""28"" = ""NEU"")\r\n\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""5"" = ""Tan/EpC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""7"" = ""Tan/EpC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""20"" = ""Tan/EpC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""32"" = ""Tan/EpC"")\r\n\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""4"" = ""MIC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""10"" = ""MIC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""27"" = ""MIC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""29"" = ""MIC"")\r\n\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""8"" = ""EC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""21"" = ""EC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""25"" = ""EC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""35"" = ""EC"")\r\n\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""31"" = ""PEC"")\r\n\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""0"" = ""AS"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""1"" = ""AS"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""3"" = ""AS"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""6"" = ""AS"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""11"" = ""AS"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""13"" = ""AS"")\r\nseurat_integrated_2.named <- RenameIdents(object = seurat_integrated_1.named, ""26"" = ""AS"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""36"" = ""AS"")\r\n\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""2"" = ""OPC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""9"" = ""OPC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""12"" = ""OPC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""14"" = ""OPC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""16"" = ""OPC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""17"" = ""OPC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""18"" = ""OPC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""23"" = ""OPC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""26"" = ""OPC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""22"" = ""OPC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""30"" = ""OPC"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""33"" = ""OPC"")\r\n\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""19"" = ""Unrecognized"")\r\nseurat_integrated_1.named <- RenameIdents(object = seurat_integrated_1.named, ""34"" = ""Unrecognized"")\r\n\r\n####min.dist = 2####\r\nseurat_integrated_1.named <- RunUMAP(seurat_integrated_1.named, \r\n                             reduction = ""pca"", \r\n                             dims = 1:40, min.dist = 1, n.neighbors = 4)\r\n\r\nDimPlot(seurat_integrated_1.named,\r\n        reduction = ""umap"",\r\n        label = T, pt.size = 1.2)\r\n\r\nmy_levels <- c(""Unrecognized"", ""MIC"", ""EC"", ""OPC"", ""AS"", ""PEC"", ""NEU"", ""Tan/EpC"")\r\nfactor(Idents(seurat_integrated_1.named), levels= my_levels)\r\nIdents(seurat_integrated_1.named) <- factor(Idents(seurat_integrated_1.named), levels= my_levels)\r\nDotPlot(object =seurat_integrated_1.named, features = c(""Crym"", ""Mia"", ""Rax"", ""Gap43"", ""Stmn2"", ""Tubb3"", ""Epcam"", ""Pitx1"", ""Scg2"", ""Slc1a3"", \r\n                                                        ""Aqp4"", ""Fgfr3"", ""Olig1"", ""Pdgfra"", ""Sox10"", ""Cldn5"", ""Ctla2a"", \r\n                                                        ""Esam"", ""Csf1r"", ""Cx3cr1"", ""Aif1"", ""Dcn"", ""Lum"", ""Ogn""), dot.scale = 5) +RotatedAxis() +SeuratAxes()+NoLegend()\r\n\r\nFeaturePlot(seurat_integrated_1.named, \r\n            reduction = ""umap"", \r\n            features = c(""Tubb3""), \r\n            sort.cell = TRUE,\r\n            min.cutoff = \'q10\', \r\n            label = T,\r\n            pt.size = 1.2)\r\n\r\n\r\nsaveRDS(seurat_integrated_1.named, ""D:/personal user/XShi/03 20210225_BGI_Nkx2.1-Tbx3 mice nuclei-seq/20210502 v3.1/20210508_seurat_integrated_1.named.rds"")\r\n\r\nseurat_integrated_1.named <- readRDS(""D:/personal user/XShi/03 20210225_BGI_Nkx2.1-Tbx3 mice nuclei-seq/20210502 v3.1/20210508_seurat_integrated_1.named.rds"")', 'library(Seurat)\r\nlibrary(patchwork)\r\nlibrary(ggplot2)\r\nlibrary(cowplot)\r\nlibrary(scatterpie)\r\nlibrary(BiocParallel)\r\nlibrary(dplyr)\r\nlibrary(stringr)\r\nlibrary(Matrix)\r\nlibrary(scales)\r\nlibrary(RCurl)\r\n\r\n## total cells analysis\r\n# import data\r\ndata_dir <- ""D:/personal_user/Shyr Shyang/00 Tbx3-CreER; Ai14 lineage tracing_single cell-seq/v2.0/total cells/""\r\nlist.files(data_dir) # Should show barcodes.tsv, genes.tsv, and matrix.mtx\r\ntbx3P12_matrix <- Read10X(data.dir = data_dir)\r\ntbx3P12 <- CreateSeuratObject(counts = tbx3P12_matrix,min.cells = 3)\r\ntbx3P12\r\n# QC\r\ntbx3P12[[""percent.mito""]] <- PercentageFeatureSet(tbx3P12, pattern = ""^mt-"")\r\nVlnPlot(object=tbx3P12, features = c(""nFeature_RNA"", ""nCount_RNA"", ""percent.mito"",""percent.mt""), ncol=4)\r\nFeatureScatter(object=tbx3P12, feature1=""nCount_RNA"", feature2=""percent.mito"")\r\nFeatureScatter(object=tbx3P12, feature1=""nCount_RNA"", feature2=""nFeature_RNA"")\r\ntap12 <- subset(x=tbx3P12, subset= nFeature_RNA > 800 & nCount_RNA<30000 & nFeature_RNA<6000 & percent.mito < 8)\r\ntap13 <- subset(x=tbx3P12, subset= nFeature_RNA > 800 & nCount_RNA<30000 & nFeature_RNA<6000 & percent.mito < 10)\r\ntap12\r\nVlnPlot(object=tap12, features = c(""nFeature_RNA"", ""nCount_RNA"", ""percent.mito""), ncol=4)\r\n# Normalizing the data and Identification of highly variable features (feature selection)\r\ntap12 <- NormalizeData(object = tap12, normalization.method = ""LogNormalize"",scale.factor = 10000)\r\ntap12 <- FindVariableFeatures(object = tap12)\r\nVariableFeaturePlot(object = tap12)\r\nlength(VariableFeatures(tap12))\r\n\r\n# Scaling the data\r\ntap12 <- ScaleData(tap12,features = rownames(tap12))\r\ntap12 <- RunPCA(object = tap12,features = VariableFeatures(object = tap12), genes.print = 10)\r\n\r\nElbowPlot(object = tap12,ndims = 50)\r\n\r\n# Clust""Sstr2 Slc4a2 Sctr Krt7 Kcnn2 Itgb4 Ggt6 Cftr Cckbr Aqp4 Aqp1 Pigr Ggt1 Jag1 Gpbar1 Ggt7 Krt19 Onecut2 Alpl Hnf1b Alb Agr2 Muc3 Tff3 Tff1 Soer the cells-TSNE and UMAP\r\n\r\ntap <- tap12\r\ntap <- FindNeighbors(object = tap12, dims = 1:40)\r\ntap <- FindClusters(object = tap,  resolution = 0.18)\r\ntap <- RunUMAP(object = tap, dims = 1:5, min.dist = 0.5)\r\nDimPlot(object = tap,label=TRUE, label.size = 6, pt.size=1.5)\r\nDimPlot(object = tap,label=F, label.size = 6, pt.size=5)+NoLegend()\r\n####tap <- subset(x=tap, ident= \'7\',invert=TRUE )\r\n####tap\r\n####saveRDS(tap,file = ""Tbx3.remove.doublets.9333.rds"")\r\n\r\ntap.markers <- FindAllMarkers(object =tap, only.pos =TRUE,min.pct = 0.25, logfc.threshold = 0.25)\r\nDotPlot(object =tap, features = c(""Crym"", ""Mia"", ""Rax"", ""Gap43"", ""Stmn2"", ""Tubb3"", ""Epcam"", ""Pitx1"", ""Pou1f1"", ""Slc1a3"", \r\n                                  ""Aqp4"", ""Fgfr3"", ""Olig1"", ""Pdgfra"", ""Sox10"", ""Sytl4"", ""Chga"", ""Scg2"", ""Cldn5"", ""Ctla2a"", \r\n                                  ""Esam"", ""Csf1r"", ""Cx3cr1"", ""Aif1""), dot.scale = 5) +RotatedAxis() +SeuratAxes()\r\n\r\ntap <- RenameIdents(object = tap,\'0\'=""TC/EpC"",\'4\'=""TC/EpC"",\'1\'=""NEU"",\'3\'=\'NEU\',\'6\'=""NEU"",\'8\'=""NEU"", \'9\'=""NEU"", \'12\'=""NEU"")\r\ntap <- RenameIdents(object = tap,\'2\'=""AS"",\'5\'=""PEC"",\'10\'=""PEC"",\'13\'=\'PEC\',\'14\'=""PEC"")\r\ntap <- RenameIdents(object = tap,\'7\'=""OPC"",\'15\'=""EC"",\'17\'=""MIC"")\r\nsaveRDS(tap,file = ""Tbx3_original_9501.rds"")\r\n\r\n\r\ntap11 <- subset(x=tap, ident= \'11\')\r\ntap16 <- subset(x=tap, ident= \'16\')\r\ntap <- subset(x=tap, ident= \'16\',invert=TRUE)\r\ntap <- subset(x=tap, ident= \'11\',invert=TRUE)\r\nsaveRDS(tap,file = ""Tbx3_removed_9224.rds"")\r\n\r\n\r\nsetwd(""D:/personal_user/Shyr Shyang/00 Tbx3-CreER; Ai14 lineage tracing_single cell-seq/v2.0/total cells"")\r\ntap <- readRDS(file = ""Tbx3_removed_9224.rds"")\r\n\r\n\r\nFeaturePlot(object=tap, features=c(""Tubb3""),label.size = 20,pt.size = 4,coord.fixed =1,reduction = \'umap\')+NoLegend()+coord_fixed(ratio = 1.2)\r\nDimPlot(object = tap,label=F, pt.size=2)+NoLegend()\r\n\r\n\r\n\r\n\r\n## neurons analysis\r\nsetwd(""D:/personal_user/Shyr Shyang/00 Tbx3-CreER; Ai14 lineage tracing_single cell-seq/v2.0/neurons"")\r\n\r\ntap.neu <- subset(x=tap, ident=\'NEU\')\r\ntap.neu <- FindVariableFeatures(object = tap.neu)\r\nlength(VariableFeatures(tap.neu))\r\ntap.neu <- ScaleData(tap.neu)\r\ntap.neu <- RunPCA(object = tap.neu,features = VariableFeatures(object = tap.neu), genes.print = 10)\r\nElbowPlot(object = tap.neu,ndims = 50)\r\ntap.neu <- FindNeighbors(object = tap.neu, dims = 1:40)\r\n#tap.neu <- FindClusters(object = tap.neu, resolution = 0.12, algorithm=4)\r\ntap.neu <- FindClusters(object = tap.neu, resolution = 0.2)\r\ntap.neu <- RunUMAP(object = tap.neu, dims = 1:40, min.dist = 0.35)\r\n# tap.neu <- RunTSNE(object = tap.neu, dims = 1:40, perplexity=30)\r\nDimPlot(object = tap.neu, label=T, label.size = 3, pt.size=1.5, reduction = ""umap"")\r\nDimPlot(object = tap.neu, label=F, label.size = 3, pt.size=4, reduction = ""umap"")+NoLegend()\r\nlevels(tap.neu)\r\n\r\ntap.neu.markers <- FindAllMarkers(object =tap.neu, only.pos =TRUE,min.pct = 0.25, logfc.threshold = 0.3)\r\nDotPlot(object =tap.neu, features = c(""Gap43"", ""Stmn2"", ""Tubb3"", ""Dlx1"", ""Nfix"", ""Otp"", ""Agrp"", ""Sst"", ""Snhg11"",\r\n                                      ""Meg3"", ""Pomc"", ""Foxb1"", ""Adcyap1"", ""Crabp1"", ""Isl1"", ""Nhlh2"", ""Tac2"", ""Kiss1"",\r\n                                      ""Fam19a4"", ""Pdyn"", ""Ghrh"", ""Gsx1"", ""Th"", ""Slc32a1"", ""Slc17a6""), dot.scale = 5) +RotatedAxis() +SeuratAxes()\r\n\r\nsaveRDS(tap.neu,file = ""Tbx3_original neuron samples_3460.rds"")\r\n\r\n##Cluster 9 is the mixture of tancytes and astrocytes\r\n##Cluster 6 is not neurons\r\ntap.neu <- subset(x=tap.neu, ident= \'9\',invert=TRUE)\r\ntap.neu <- subset(x=tap.neu, ident= \'6\',invert=TRUE)\r\ntap.neu <- RenameIdents(object = tap.neu,\'1\'=""Agrp/Sst"",\'0\'=""Dlx1/Nfix"",\'2\'=""Foxb1/Adcyap1"",\'3\'=\'Crabp1/Isl1\', \'4\'=""Tac2/Nhlh2"", \'5\'=""Fam19a4/Pdyn"", \'6\'=""Pomc/Vgf"", \'7\'=""Ghrh/Gsx1"", \'8\'=""Gpr88/Mafb"")\r\n\r\n\r\nsaveRDS(tap.neu,file = ""Tbx3_neuron_removed samples_3061.rds"")\r\n\r\n\r\n## TC/EpC analysis\r\nTC <- subset(x=tap, ident=\'TC/EpC\')\r\ntap.tc <- TC\r\nsaveRDS(TC, file = ""TC and EpC_3449_original.rds"")\r\nsetwd(""D:/personal_user/Shyr Shyang/00 Tbx3-CreER; Ai14 lineage tracing_single cell-seq/v2.0/tanycytes and EpCs"")\r\n\r\ntap.tc <- FindVariableFeatures(object = tap.tc)\r\nlength(VariableFeatures(tap.tc))\r\ntap.tc <- ScaleData(tap.tc)\r\ntap.tc <- RunPCA(object = tap.tc,features = VariableFeatures(object = tap.tc), genes.print = 10)\r\nElbowPlot(object = tap.tc, ndims = 50)\r\ntap.tc <- FindNeighbors(object = tap.tc, dims = 1:40)\r\ntap.tc <- FindClusters(object = tap.tc, resolution = 5)\r\ntap.tc <- RunUMAP(object = tap.tc, dims = 1:40, min.dist = 0.35)\r\nDimPlot(object = tap.tc, label=T, label.size = 3, pt.size=1.5, reduction = ""umap"")\r\n\r\ntap.tc.markers <- FindAllMarkers(object =tap.tc, only.pos =TRUE,min.pct = 0.25, logfc.threshold = 0.3)\r\n\r\nDotPlot(object =tap.tc, features = c(""Crym"", ""Mia"", ""Rax"", ""Gpr50"", ""Gap43"", ""Stmn2"", ""Tubb3"", ""Epcam"", ""Pitx1"", ""Pou1f1"", ""Slc1a3"", \r\n                                     ""Aqp4"", ""Fgfr3"", ""Olig1"", ""Pdgfra"", ""Sox10"", ""Sytl4"", ""Chga"", ""Scg2"", ""Cldn5"", ""Ctla2a"", \r\n                                     ""Esam"", ""Csf1r"", ""Cx3cr1"", ""Aif1""), dot.scale = 5) +RotatedAxis() +SeuratAxes()\r\n\r\nDotPlot(object =tap.tc, features = c(""Hdc"", ""Tm4sf1"", ""Cd59a"", ""Mafb"", ""S100a6"", ""Pdzph1"", ""Miat"", ""Slit2"", ""Dlk1"", ""Col25a1"", ""Fndc3c1"", \r\n                                     ""Scn7a"", ""Col13a1"", ""Igfbp4""), dot.scale = 5) +RotatedAxis() +SeuratAxes()\r\n\r\ntap.tc <- subset(x=tap.tc, ident= \'32\',invert=TRUE)\r\n\r\n\r\ntap.tc <- FindVariableFeatures(object = tap.tc)\r\nlength(VariableFeatures(tap.tc))\r\ntap.tc <- ScaleData(tap.tc)\r\ntap.tc <- RunPCA(object = tap.tc,features = VariableFeatures(object = tap.tc), genes.print = 10)\r\nElbowPlot(object = tap.tc, ndims = 50)\r\ntap.tc <- FindNeighbors(object = tap.tc, dims = 1:40)\r\ntap.tc <- FindClusters(object = tap.tc, resolution = 5)\r\ntap.tc <- RunUMAP(object = tap.tc, dims = 1:40, min.dist = 0.35)\r\nDimPlot(object = tap.tc, label=T, label.size = 3, pt.size=1.5, reduction = ""umap"")\r\n\r\ntap.tc.markers <- FindAllMarkers(object =tap.tc, only.pos =TRUE,min.pct = 0.25, logfc.threshold = 0.3)\r\n\r\nDotPlot(object =tap.tc, features = c(""Crym"", ""Mia"", ""Rax"", ""Gpr50"", ""Gap43"", ""Stmn2"", ""Tubb3"", ""Epcam"", ""Pitx1"", ""Pou1f1"", ""Slc1a3"", \r\n                                     ""Aqp4"", ""Fgfr3"", ""Olig1"", ""Pdgfra"", ""Sox10"", ""Sytl4"", ""Chga"", ""Scg2"", ""Cldn5"", ""Ctla2a"", \r\n                                     ""Esam"", ""Csf1r"", ""Cx3cr1"", ""Aif1""), dot.scale = 5) +RotatedAxis() +SeuratAxes()\r\n\r\nDotPlot(object =tap.tc, features = c(""Hdc"", ""Tm4sf1"", ""Cd59a"", ""Mafb"", ""S100a6"", ""Pdzph1"", ""Miat"", ""Slit2"", ""Dlk1"", ""Col25a1"", ""Fndc3c1"", \r\n                                     ""Scn7a"", ""Col13a1"", ""Igfbp4""), dot.scale = 5) +RotatedAxis() +SeuratAxes()\r\n\r\n\r\ntap.tc_renamed <- RenameIdents(object = tap.tc, \'10\'=""2"", \'31\'=""2"")\r\ntap.tc_renamed <- RenameIdents(object = tap.tc_renamed, \'4\'=""1"", \'15\'=""1"", \'19\'=""1"", \'26\'=""1"", \'1\'=""1"", \'22\'=""1"", \'24\'=""1"")\r\ntap.tc_renamed <- RenameIdents(object = tap.tc_renamed, \'6\'=""EpC"")\r\ntap.tc_renamed <- RenameIdents(object = tap.tc_renamed, \'28\'=""1"", \'29\'=""1"", \'14\'=""1"", \'3\'=""1"", \'11\'=""1"", \'25\'=""1"")\r\ntap.tc_renamed <- RenameIdents(object = tap.tc_renamed, \'18\'=""1"", \'23\'=""1"")\r\ntap.tc_renamed <- RenameIdents(object = tap.tc_renamed, \'0\'=""2"", \'2\'=""2"", \'5\'=""2"", \'7\'=""2"", \'8\'=""2"", \'9\'=""2"", \'12\'=""2"", \'13\'=""2"", \'16\'=""2"", \'17\'=""2"", \'20\'=""2"", \'21\'=""2"", \'27\'=""2"", \'30\'=""2"")\r\n\r\nDimPlot(object = tap.tc_renamed, label=T, label.size = 3, pt.size=1.5, reduction = ""umap"")\r\nDotPlot(object =tap.tc_renamed, features = c(""Hdc"", ""Tm4sf1"", ""Cd59a"", ""Mafb"", ""S100a6"", ""Pdzph1"", ""Miat"", ""Slit2"", ""Dlk1"", ""Col25a1"", ""Fndc3c1"", \r\n                                     ""Scn7a"", ""Col13a1"", ""Igfbp4""), dot.scale = 5) +RotatedAxis() +SeuratAxes()\r\n\r\ntap.tc_renamed <- RunUMAP(object = tap.tc_renamed, dims = 1:40, min.dist = 1.5)\r\nDimPlot(object = tap.tc_renamed, label=F, label.size = 3, pt.size=1.5, reduction = ""umap"") + NoLegend()\r\n\r\n\r\ntap.tc_renamed1 <- tap.tc_renamed\r\nmy_levels1 <- c(""2"", ""1"", ""2"", ""1"", ""EpC"")\r\nIdents(tap.tc_renamed1) <- factor(Idents(tap.tc_renamed1), levels= my_levels1)\r\nDotPlot(object =tap.tc_renamed1, features = c(""Igfbp4"", ""Col13a1"", ""Scn7a"", ""Fndc3c1"", ""Col25a1"",\r\n                                              ""Dlk1"", ""Slit2"", ""Miat"", ""Pdzph1"", ""S100a6"", ""Mafb"",\r\n                                              ""Cd59a"", ""Tm4sf1"", ""Hdc""), dot.scale = 5) +RotatedAxis() +SeuratAxes() +NoLegend()\r\n\r\nsaveRDS(tap.tc_renamed,file = ""TC and EpC_3439_renamed.rds"")']","Hierarchical deployment of Tbx3 dictates the identity of hypothalamic KNDy neurons to control puberty onset The database is the detailed codes of bioinformatics analysis in the article which DOI number is ""10.1126/sciadv.abq2987"".",0
RBPed,RBPed is a bioinformatics pipeline to identify differential RNA editing events between eCLIP-seq and RNA-seq. For usage and more details can be found at : https://github.com/YangLabProject/RBPed,"['##for calculating fisher test pvalue and delta editing levels\n\n\n#data is the editing levels for specific editing site from eCLIP and RNA-seq dataset\n\n\nPvalue=list()\ni=1\nwhile(i<=dim(data)[1]){\ndata2=data[i,]\n#V7 is number of base I in eclip, V8 is total base covered this site in ECLIP, V11 is number of base I in RNA-seq ,V10 is total base covered this site in RNA-seq.\nPvalue[i]=fisher.test(matrix(c(data2$V7,data2$V11,data2$V8,data2$V10),nrow=2,ncol=2))$p.value\ni=i+1\n}\n\ndelta=data$V7/data$V8-data$V11/data$V10\nm2=cbind(data,unlist(Pvalue),delta,eclipEL=data$V7/data$V8,RNAEL=data$V11/data$V10)\n\n#m2 is the results for potential binding preference.']",RBPed RBPed is a bioinformatics pipeline to identify differential RNA editing events between eCLIP-seq and RNA-seq. For usage and more details can be found at : https://github.com/YangLabProject/RBPed,0
Program Supplement to Genomic Analysis of Human Population Structure,"Over the course of my PhD project, Genomic Analysis of Human Population Structure, I wrote over sixty short programs (or scripts) to supplement my research, including a program that can carry out a bootstrap sub-sampling procedure on genetic data. It is expected that the scripts created for this research will be of use to other people carrying out similar work, and they have been written with the expectation that the scripts will be adapted by other people in the future.Most commonly, the purpose of these scripts is to convert data from one file format to another -- data is often received in different formats from different researchers, and will usually need to be converted into another format in order to work with a particular program. Scripts have also been written for producing many of the figures seen in my thesis, as well as simple summary statistics. Where more complex calculations (in an external program) have beenrequired to be done repeatedly, a script has been created to automate that process.Documentation for programs developed over the course of this research project is found in the file 'programs.pdf'. This Zenodo repository also contains the source code for those programs.The chapters are arranged by programming language (Perl, R, shell), and contain a summary of the types of programs written in that language, followed by more verbose information about the programs that were written.Note: these scripts and programs are in the state they were when my PhD thesis was published. Some have been updated since, and have been added to my bioinformatics scripts repository. If there's an issue, check there first (and please tell me even if the problem is fixed, so I can make a note here of the known problems).",,"Program Supplement to Genomic Analysis of Human Population Structure Over the course of my PhD project, Genomic Analysis of Human Population Structure, I wrote over sixty short programs (or scripts) to supplement my research, including a program that can carry out a bootstrap sub-sampling procedure on genetic data. It is expected that the scripts created for this research will be of use to other people carrying out similar work, and they have been written with the expectation that the scripts will be adapted by other people in the future.Most commonly, the purpose of these scripts is to convert data from one file format to another -- data is often received in different formats from different researchers, and will usually need to be converted into another format in order to work with a particular program. Scripts have also been written for producing many of the figures seen in my thesis, as well as simple summary statistics. Where more complex calculations (in an external program) have beenrequired to be done repeatedly, a script has been created to automate that process.Documentation for programs developed over the course of this research project is found in the file 'programs.pdf'. This Zenodo repository also contains the source code for those programs.The chapters are arranged by programming language (Perl, R, shell), and contain a summary of the types of programs written in that language, followed by more verbose information about the programs that were written.Note: these scripts and programs are in the state they were when my PhD thesis was published. Some have been updated since, and have been added to my bioinformatics scripts repository. If there's an issue, check there first (and please tell me even if the problem is fixed, so I can make a note here of the known problems).",0
Comparison of re-called Albacore and Flappie sequences from E. coli K-12 MG1655,"I used the first few (545) original Fast5 files from Nick Loman's ultra-long read E. coli K-12 MG1655 R9.4 sequencing run. See the blog post here. Direct link to the complete fast5 dataset here.Program versions:* Albacore 2.1.10* Flappie 1.0.0-0048dfdThe example alignment was carried out using seaview (1:4.6.1.2-2), and visualised using spiralign from my bioinfscripts repository (see source code in this archive): $ spiralign.r -size 2000x2000 -noalign -noborder -loops 12.75 -outfmt png -type nucl -title ""Flappie vs Albacore\n(Ecoli_MG1655)"" aligned_all_ddea.faFlappie was distributed across multiple processing threads using GNU parallel: $ ls Ecoli_MinKNOW_1.4_RAD002_Sambrook/0/nanopore2_20170301_FNFAF09967_MN17024_mux_scan_170301_MG1655_PC_RAD002_76964_ch* | parallel --group -j 10 -L 1 ~/install/flappie/flappie | gzip > called_flappie_Ecoli_MinKNOW_1.4_RAD002_Sambrook.fq.gz Tange (2011): GNU Parallel - The Command-Line Power Tool, ;login: The USENIX Magazine, February 2011:42-47.","['#!/usr/bin/Rscript\n\nsetwd(""~/bioinf/presentations/2017-Sep-03"");\n\ntype <- ""aa"";\n\nif(type == ""nucl""){\n    ## Nucleotides\n    library(msa);\n    input.seqs <- readDNAStringSet(""notUSCO_EOG091H04CB.tran.fa"");\n    names(input.seqs) <- sub("" .*$"","""",names(input.seqs));\n    msa.df <- data.frame(t(as.matrix(msa(input.seqs))), stringsAsFactors=FALSE);\n    colnames(msa.df) <- 1:ncol(msa.df);\n\n    efg.cols <- c(""G"" = ""gold"", ""C"" = ""blue"", ""A"" = ""darkgreen"",\n                  ""T"" = ""red"", ""-"" = ""grey20"");\n\n    png(""msa.png"", width=1024, height=1024, pointsize=24);\n    par(mar=c(0.5,0.5,0.5,0.5), bg=""black"");\n    loops <- 5;\n    lstt <- 3;\n    lend <- loops+lstt;\n    ## integrate(2*pi*r,r=lstt..x)\n    ## => pi(x-(lstt))\n    dTot <- pi*((lstt + loops)^2 - (lstt)^2); ## total ""distance"" travelled\n    ## s = pi(x-(lstt))\n    ## => s/pi = x - (lstt)\n    ## => x = sqrt((lstt) + s/pi)\n    msa.df$s <- seq(0,dTot, length.out=nrow(msa.df)); ## distance at each pos\n    msa.df$r <- sqrt(lstt^2 + msa.df$s/pi); ## path radius at each pos\n    msa.df$theta <- msa.df$r * 2*pi; ## traversed angle at each pos\n    msa.df$deg <- (msa.df$theta / (2*pi)) * 360;\n    msa.df$x <- msa.df$r * cos(msa.df$theta);\n    msa.df$y <- msa.df$r * sin(msa.df$theta);\n    plot(NA,xlim=c(-lend,lend), ylim=c(-lend,lend), ann=FALSE, axes=FALSE);\n    pcex <- 0.45;\n    for(p in seq(1,nrow(msa.df))){\n        pr <- msa.df$r[p];\n        pt <- msa.df$theta[p];\n        text(x=-(pr-0.25)*cos(pt), y=(pr-0.25)*sin(pt), labels="""",\n             srt=-msa.df$deg[p],\n             cex=pcex*0.9, col=efg.cols[msa.df[p,1]]);\n        text(x=-pr*cos(pt), y=pr*sin(pt), labels="""",\n             srt=-msa.df$deg[p],\n             cex=pcex, col=efg.cols[msa.df[p,2]]);\n        text(x=-(pr+0.25)*cos(pt), y=(pr+0.25)*sin(pt), labels="""",\n             srt=-msa.df$deg[p],\n             cex=pcex*1.05, col=efg.cols[msa.df[p,3]]);\n    }\n    text(0,0.5, expression(italic(Nippostrongylus)), col=""white"");\n    text(0,0, expression(italic(brasiliensis)), col=""white"");\n    text(0,-0.5, ""Fructose-1,6-bisphosphatase"", col=""white"", cex=0.75);\n    invisible(dev.off());\n}\n\n## Amino Acids\nlibrary(msa);\ninput.seqs <- readAAStringSet(""notUSCO_EOG091H04CB.prot.fa"");\nnames(input.seqs) <- sub("" .*$"","""",names(input.seqs));\nmsa.df <- data.frame(t(as.matrix(msa(input.seqs, order=""input""))),\n                     stringsAsFactors=FALSE);\nmsa.df <- msa.df[nrow(msa.df):1,]\n\nrasmol.cols <- c(""D"" = ""#E60A0A"", ""E"" = ""#E60A0A"",\n                 ""C"" = ""#E6E600"", ""M"" = ""#E6E600"",\n                 ""K"" = ""#145AFF"", ""R"" = ""#145AFF"",\n                 ""S"" = ""#FA9600"", ""T"" = ""#FA9600"",\n                 ""F"" = ""#3232AA"", ""Y"" = ""#3232AA"",\n                 ""N"" = ""#00DCDC"", ""Q"" = ""#00DCDC"",\n                 ""G"" = ""#EBEBEB"",\n                 ""L"" = ""#0F820F"", ""V"" = ""#0F820F"", ""I"" = ""#0F820F"",\n                 ""A"" = ""#C8C8C8"",\n                 ""W"" = ""#B45AB4"",\n                 ""H"" = ""#8282D2"",\n                 ""P"" = ""#DC9682"",\n                 ""-"" = ""grey20"", ""X"" = ""grey20"");\nrasmol.cats <- tapply(names(rasmol.cols),rasmol.cols,paste,collapse="","");\n\npng(""msa_aa.png"", width=1024, height=1024, pointsize=24);\npar(mar=c(0.5,0.5,0.5,0.5), bg=""black"");\nloops <- 2.75;\nlstt <- 3;\nlend <- loops+lstt;\n## integrate(2*pi*r,r=lstt..x)\n## => pi(x-(lstt))\ndTot <- pi*((lstt + loops)^2 - (lstt)^2); ## total ""distance"" travelled\n## s = pi(x-(lstt))\n## => s/pi = x - (lstt)\n## => x = sqrt((lstt) + s/pi)\nmsa.df$s <- seq(0,dTot, length.out=nrow(msa.df)); ## distance at each pos\nmsa.df$r <- sqrt(lstt^2 + msa.df$s/pi); ## path radius at each pos\nmsa.df$theta <- msa.df$r * 2*pi; ## traversed angle at each pos\nmsa.df$deg <- (msa.df$theta / (2*pi)) * 360;\nmsa.df$x <- msa.df$r * cos(msa.df$theta);\nmsa.df$y <- msa.df$r * sin(msa.df$theta);\nplot(NA,xlim=c(-lend,lend), ylim=c(-lend,lend), ann=FALSE, axes=FALSE);\npcex <- 0.8;\nfor(p in seq(1,nrow(msa.df))){\n    pr <- msa.df$r[p];\n    pt <- msa.df$theta[p];\n    pym <- length(input.seqs);\n    pyr <- seq(-(pym-2)/(pym-1),(pym-2)/(pym-1), length.out=pym)/2;\n    for(py in 1:length(input.seqs)){\n        text(x=-(pr+pyr[py])*cos(pt), y=(pr+pyr[py])*sin(pt), labels="""",\n             srt=-msa.df$deg[p],\n             cex=pcex, col=rasmol.cols[msa.df[p,py]]);\n    }\n}\nfor(py in 1:length(input.seqs)){\n    text(x=-(max(msa.df$r)+pyr[py])*cos(max(msa.df$theta)),\n         y=(max(msa.df$r)+pyr[py])*sin(max(msa.df$theta))-0.02,\n         labels=names(input.seqs)[py],\n         srt=0, pos=2,\n         cex=0.6, col=""white"");\n}\ntext(0,0.5, expression(italic(Nippostrongylus)), col=""white"");\ntext(0,0, expression(italic(brasiliensis)), col=""white"");\ntext(0,-0.5, ""Fructose-1,6-bisphosphatase"", col=""white"", cex=0.75);\nlegend(""topleft"", fill=names(rasmol.cats), text.col=""white"",\n    legend=rasmol.cats, ncol=2, cex=0.8);\ninvisible(dev.off());\n']","Comparison of re-called Albacore and Flappie sequences from E. coli K-12 MG1655 I used the first few (545) original Fast5 files from Nick Loman's ultra-long read E. coli K-12 MG1655 R9.4 sequencing run. See the blog post here. Direct link to the complete fast5 dataset here.Program versions:* Albacore 2.1.10* Flappie 1.0.0-0048dfdThe example alignment was carried out using seaview (1:4.6.1.2-2), and visualised using spiralign from my bioinfscripts repository (see source code in this archive): $ spiralign.r -size 2000x2000 -noalign -noborder -loops 12.75 -outfmt png -type nucl -title ""Flappie vs Albacore\n(Ecoli_MG1655)"" aligned_all_ddea.faFlappie was distributed across multiple processing threads using GNU parallel: $ ls Ecoli_MinKNOW_1.4_RAD002_Sambrook/0/nanopore2_20170301_FNFAF09967_MN17024_mux_scan_170301_MG1655_PC_RAD002_76964_ch* | parallel --group -j 10 -L 1 ~/install/flappie/flappie | gzip > called_flappie_Ecoli_MinKNOW_1.4_RAD002_Sambrook.fq.gz Tange (2011): GNU Parallel - The Command-Line Power Tool, ;login: The USENIX Magazine, February 2011:42-47.",0
Supporting data: Reporting phenotypes in model organisms when considering body size as a potential confounder.,"This directory contains the data and associated scripts used to generate the figures in the manuscript ""Reporting phenotypes in model organisms when considering body size as a potential confounder."" submitted to the Journal of Biomedical Semantics","['# Generation of figures within  manuscript\r\n################################################################################\r\n\r\n######################Figure 2\r\nDlg4=read.csv(""Dlg4.csv"")\r\nlibrary(plyr)\r\ncountDataPoints<-function(dataset, depVariable){\r\n\tprint(sum(is.finite(dataset[ , depVariable])))\r\n}\r\n\r\nIndex = ddply(.data=Dlg4, .variables=c(""Genotype"", ""Gender""), .fun=countDataPoints, depVariable=""Lean.Mass"")\t\t\r\n\r\npng(""Dlg4_LM_Male.png"")\r\nscatterplot(data=Dlg4, Lean.Mass~Weight|Genotype, subset= Dlg4$Gender==""Male"", grid=FALSE, legend.plot=FALSE, cex.axis=1.4, cex.lab=1.6, lwd=3, smooth=FALSE, xlab=""Body weight (g)"", ylab=""Lean mass (g)"")\r\nlegend(23,29, # places a legend at the appropriate place \r\n\t\tc(""Wildtype"",""Dlg4""), # puts text in the legend\r\n\t\t\r\n\t\tlty=c(1,1), # gives the legend appropriate symbols (lines)\r\n\t\t\r\n\t\t,col=c(""black"",""red""),bty=""n"", lwd=3, cex=1.6)\r\ndev.off()\r\n\r\npng(""Dlg4_LM_Female.png"")\r\nscatterplot(data=Dlg4, Lean.Mass~Weight|Genotype, subset= Dlg4$Gender==""Female"", grid=FALSE, legend.plot=FALSE, cex.axis=1.4, cex.lab=1.6, lwd=3, smooth=FALSE, xlab=""Body weight (g)"", ylab=""Lean mass (g)"")\r\nlegend(15,24, # places a legend at the appropriate place \r\n\t\tc(""Wildtype"",""Dlg4""), # puts text in the legend\r\n\t\t\r\n\t\tlty=c(1,1), # gives the legend appropriate symbols (lines)\r\n\t\t\r\n\t\t,col=c(""black"",""red""),bty=""n"", lwd=3, cex=1.6)\r\n\r\ndev.off()\r\n\r\n\r\nlibrary(phenStat)\r\nPhenObject=PhenList(Dlg4, testGenotype=""Dlg4/Dlg4"", dataset.colname.sex=""Gender"", dataset.clean=TRUE,dataset.colname.batch=""Assay.Date"", dataset.colname.genotype=""Genotype"", dataset.colname.weight=""Weight"", dataset.values.male=""Male"", \t\tdataset.values.female=""Female"") \r\nModelfeatures=testDataset(PhenObject, depVariable=""Lean.Mass"", equation=""withWeight"", transformValues=FALSE)  \r\nsummaryOutput(Modelfeatures)\r\nModelfeatures=testDataset(PhenObject, depVariable=""Lean.Mass"", equation=""withoutWeight"", transformValues=FALSE)  \r\nsummaryOutput(Modelfeatures)\r\n\r\n\r\n######################Figure 3\r\n\r\ndf1=read.csv(""Akt2dataset.csv"")\r\nsetwd(""Y:\\\\Natasha\\\\Ontology issue with BW\\\\Figures"")\r\n\r\nlibrary(plyr)\r\ncountDataPoints<-function(dataset, depVariable){\r\n\tprint(sum(is.finite(dataset[ , depVariable])))\r\n}\r\n\r\nIndex = ddply(.data=df1, .variables=c(""Genotype"", ""Gender""), .fun=countDataPoints, depVariable=""Lean.Mass"")\t\t\r\n\r\n\r\npng(""Akt2_LM_Male.png"")\r\nscatterplot(data=df1, Lean.Mass~Weight|Genotype, subset= Dlg4$Gender==""Male"", grid=FALSE, legend.plot=FALSE, cex.axis=1.4, cex.lab=1.6, lwd=3, smooth=FALSE, xlab=""Body weight (g)"", ylab=""Lean mass (g)"")\r\nlegend(15,28, # places a legend at the appropriate place \r\n\t\tc(""Wildtype"",""Akt2""), # puts text in the legend\r\n\t\t\r\n\t\tlty=c(1,1), # gives the legend appropriate symbols (lines)\r\n\t\t\r\n\t\t,col=c(""black"",""red""),bty=""n"", lwd=3, cex=1.6)\r\ndev.off()\r\n\r\npng(""Akt2_LM_Female.png"")\r\nscatterplot(data=df1, Lean.Mass~Weight|Genotype, subset= Dlg4$Gender==""Female"", grid=FALSE, legend.plot=FALSE, cex.axis=1.4, cex.lab=1.6, lwd=3, smooth=FALSE, xlab=""Body weight (g)"", ylab=""Lean mass (g)"")\r\nlegend(17,26, # places a legend at the appropriate place \r\n\t\tc(""Wildtype"",""Akt2""), # puts text in the legend\r\n\t\t\r\n\t\tlty=c(1,1), # gives the legend appropriate symbols (lines)\r\n\t\t\r\n\t\t,col=c(""black"",""red""),bty=""n"", lwd=3, cex=1.6)\r\ndev.off()\r\n\r\n\r\nlibrary(phenStat)\r\nPhenObject=PhenList(df1, testGenotype=""Akt2/Akt2"", dataset.colname.sex=""Gender"", dataset.clean=TRUE,dataset.colname.batch=""Assay.Date"", dataset.colname.genotype=""Genotype"", dataset.colname.weight=""Weight"", dataset.values.male=""Male"", \t\tdataset.values.female=""Female"") \r\nModelfeatures=testDataset(PhenObject, depVariable=""Lean.Mass"", equation=""withWeight"", transformValues=FALSE)  \r\nsummaryOutput(Modelfeatures)\r\nModelfeatures=testDataset(PhenObject, depVariable=""Lean.Mass"", equation=""withoutWeight"", transformValues=FALSE)  \r\nsummaryOutput(Modelfeatures)\r\n\r\n\r\n######################Figure 4\r\n# Question 2: How often weight included in the models  (Figure 4)?\r\n#PhenStat completes a model optimisation process.  If weight is not significant in explaining the variation (F test, p<0.05) then weight is not included in the final model.\r\nFinalDf=read.csv(""MERGED_withANDwithoutWeight.csv"")\r\n\r\nWEIGHTinclusion<-function(dataset){\r\n\tnumberDatasets=sum(is.finite(dataset[ , ""GENOTYPE_CONTRIBUTION.y""]))\r\n\tnumberIncluded= sum(dataset[ , ""WEIGHT_PVALUE.y""]<=0.05, na.rm = TRUE)\r\n\tPercentIncluded=(numberIncluded/numberDatasets)*100\r\n\toutput=c(numberDatasets, numberIncluded,PercentIncluded )\r\n\tnames(output)=c(""numberDatasets"", ""numberIncluded"",""PercentIncluded"")\r\n\treturn(output)\r\n}\r\nlibrary(plyr)\r\nIndex = ddply(.data=FinalDf, .variables=c(""PARAMETER.y""), .fun=WEIGHTinclusion)\t\t\r\n\r\npdf(""Figure4_RoleOfWeight.pdf"")\r\nhist(Index$PercentIncluded, main ="""", xlab=""Modelling that included weight (%)"", ylab=""Number of variables"", cex.lab=1.4)\r\ndev.off()\r\n\r\n#######################Figure 5\r\n#Load statistical analysis output from data being processed by  A1 (equation=""withoutWeight"") and then A2 (equation=""withWeight"").\r\n#The columns are those contains in the vector output from PhenStat version 2.0.1. The descriptors of the columns are described in the associated PhenStat User Guide. \r\n#columns dervied from A1 are appended with .x and the columns dervied from A2 are appended with .y\r\n\r\nFinalDf=read.csv(""MERGED_withANDwithoutWeight.csv"")\r\n\r\n#Question 1: How often does including weight change the number of significant calls (Figure 5)?\r\n#This output was used to construct the Venn diagram in figure 5\r\n\r\nConsistencyCalls<-function(df, Sig_withoutWeight, Sig_withWeight){\r\n\tSigWithoutWeight=df[ ,Sig_withoutWeight]<=0.0001\r\n\tSigWithWeight=df[ ,Sig_withWeight]<=0.0001\r\n\tif(SigWithoutWeight==TRUE && SigWithWeight==TRUE){\r\n\t\tCallConsistency=""Both Significant""\r\n\t}else if (SigWithoutWeight==FALSE && SigWithWeight==TRUE){\r\n\t\tCallConsistency=""Significant withWeight Only""\r\n\t}else if (SigWithoutWeight==TRUE && SigWithWeight==FALSE){\r\n\t\tCallConsistency=""Significant withoutWeight Only""\r\n\t}else{\r\n\t\tCallConsistency=""no call""\t\r\n\t}\r\n\treturn(CallConsistency)\r\n}\r\n\r\nlibrary(plyr)\r\n\r\nIndex1= ddply(.data=FinalDf, .variables=c(""id""), .fun=ConsistencyCalls, Sig_withoutWeight=""GENOTYPE_CONTRIBUTION.x"", Sig_withWeight=""GENOTYPE_CONTRIBUTION.y"")\t\t\r\nhead(Index1)\r\ntable(Index1$V1)\r\n\r\n\r\n']","Supporting data: Reporting phenotypes in model organisms when considering body size as a potential confounder. This directory contains the data and associated scripts used to generate the figures in the manuscript ""Reporting phenotypes in model organisms when considering body size as a potential confounder."" submitted to the Journal of Biomedical Semantics",0
Supplementary Datasets for dadasnake workflow,"This dataset contains configuration and results files for the proof-of-principle of the dadasnake pipeline. Includes dadasnake output and tables with the composition of ground-truth data or mock-communities.dadasnake is a user-friendly, one-command Snakemake pipeline that wraps the pre-processing of sequencing reads and the delineation of exact sequence variants by using the favorably benchmarked and widely-used DADA2 algorithm with a taxonomic classification and the post-processing of the resultant tables, including hand-off in standard formats. The suitability of the provided default configurations is demonstrated using mock-community data from bacteria and archaea, as well as fungi. By use of Snakemake, dadasnake makes efficient use of high-performance computing infrastructures. Easy user configuration guarantees flexibility of all steps, including the processing of data from multiple sequencing platforms. dadasnake facilitates easy installation via conda environments. dadasnake is available at https://github.com/a-h-b/dadasnake .","['# preparation on cluster:\n# setwd(""../EMP"")\n# qa <- list.files(pattern="".*_.*-.*.txt"")\n# ql <- read.fwf(qa[1],stringsAsFactors=F,widths=c(13,400),header=F,comment.char=""="")\n# ql$V1 <- gsub("" *$"","""",gsub(""^ *"","""",ql$V1))\n# ql$V2 <- gsub("" *$"","""",gsub(""^ *"","""",ql$V2))\n# ql3 <- matrix(ql$V2,ncol=51,nrow=nrow(ql)/51,byrow=T)\n# colnames(ql3) <- ql$V1[1:51]\n# for(i in 2:length(qa)){\n#   ql <- read.fwf(qa[i],stringsAsFactors=F,widths=c(13,400),header=F,comment.char=""="")\n#   ql$V1 <- gsub("" *$"","""",gsub(""^ *"","""",ql$V1))\n#   ql$V2 <- gsub("" *$"","""",gsub(""^ *"","""",ql$V2))\n#   ql3 <- rbind(ql3,matrix(ql$V2,ncol=51,nrow=nrow(ql)/51,byrow=T))\n# }\n# ql3 <- as.data.frame(ql3,stringsAsFactors = F)\n# ql3s <- ql3[order(as.POSIXlt(ql3$start_time,format = ""%m/%d/%Y %H:%M:%OS"")),]\n# \n# setwd(""../Ukulinga"")\n# ql <- read.fwf(""4_jobs.txt"",stringsAsFactors=F,widths=c(13,400),header=F,comment.char=""="")\n# ql$V1 <- gsub("" *$"","""",gsub(""^ *"","""",ql$V1))\n# ql$V2 <- gsub("" *$"","""",gsub(""^ *"","""",ql$V2))\n# ql <- ql[1:(nrow(ql)-2),]\n# ql3 <- matrix(ql$V2,ncol=51,nrow=nrow(ql)/51,byrow=T)\n# colnames(ql3) <- ql$V1[1:51]\n# qd <- as.data.frame(ql3,stringsAsFactors=F)\n# qds <- qd[order(as.POSIXlt(qd$start_time,format = ""%m/%d/%Y %H:%M:%OS"")),]\n# saveRDS(qds,""Ukulinga_4.jobMatrix.RDS"")\n# \n# ql <- read.fwf(""1_job.txt"",stringsAsFactors=F,widths=c(13,400),header=F,comment.char=""="")\n# ql$V1 <- gsub("" *$"","""",gsub(""^ *"","""",ql$V1))\n# ql$V2 <- gsub("" *$"","""",gsub(""^ *"","""",ql$V2))\n# ql <- ql[1:(nrow(ql)-2),]\n# ql3 <- matrix(ql$V2,ncol=51,nrow=nrow(ql)/51,byrow=T)\n# colnames(ql3) <- ql$V1[1:51]\n# qd <- as.data.frame(ql3,stringsAsFactors=F)\n# qds <- qd[order(as.POSIXlt(qd$start_time,format = ""%m/%d/%Y %H:%M:%OS"")),]\n# saveRDS(qds,""Ukulinga_1.jobMatrix.RDS"")\n# \n# setwd(""../NutNet"")\n# ql <- read.fwf(""4_jobs.txt"",stringsAsFactors=F,widths=c(13,400),header=F,comment.char=""="")\n# ql$V1 <- gsub("" *$"","""",gsub(""^ *"","""",ql$V1))\n# ql$V2 <- gsub("" *$"","""",gsub(""^ *"","""",ql$V2))\n# ql <- ql[1:(nrow(ql)-2),]\n# ql3 <- matrix(ql$V2,ncol=51,nrow=nrow(ql)/51,byrow=T)\n# colnames(ql3) <- ql$V1[1:51]\n# qd <- as.data.frame(ql3,stringsAsFactors=F)\n# qds <- qd[order(as.POSIXlt(qd$start_time,format = ""%m/%d/%Y %H:%M:%OS"")),]\n# saveRDS(qds,""NutNet_4.jobMatrix.RDS"")\n# ql <- read.fwf(""normal_jobs.txt"",stringsAsFactors=F,widths=c(13,400),header=F,comment.char=""="")\n# ql$V1 <- gsub("" *$"","""",gsub(""^ *"","""",ql$V1))\n# ql$V2 <- gsub("" *$"","""",gsub(""^ *"","""",ql$V2))\n# ql <- ql[1:(nrow(ql)-2),]\n# ql3 <- matrix(ql$V2,ncol=51,nrow=nrow(ql)/51,byrow=T)\n# colnames(ql3) <- ql$V1[1:51]\n# qd <- as.data.frame(ql3,stringsAsFactors=F)\n# qds <- qd[order(as.POSIXlt(qd$start_time,format = ""%m/%d/%Y %H:%M:%OS"")),]\n# saveRDS(qds,""NutNet_50.jobMatrix.RDS"")\n\n\n#### local workspace:\n\n# 1 core - small data set\nqd1 <- readRDS(""Ukulinga_1.jobMatrix.RDS"")\nqd1$efficiency <- as.numeric(qd1$cpu) / (as.numeric(qd1$slots) * as.numeric(qd1$ru_wallclock))\nweighted.mean(qd1$efficiency,as.numeric(qd1$ru_wallclock)) # 93%\nsum(as.numeric(qd1$ru_wallclock))/60\nqd1$col <- sapply(gsub("".[[:digit:]]+.sh"","""",\n                       gsub(""snakejob."","""",qd1$jobname)),\n                  function(x) colorRampPalette(rainbow(9))(length(unique(gsub("".[[:digit:]]+.sh"","""",\n                                                                              gsub(""snakejob."","""",qd1$jobname)))))[which(unique(gsub("".[[:digit:]]+.sh"","""",\n                                                                                                                                     gsub(""snakejob."","""",qd1$jobname)))==x)])\n\nqtimes1 <- data.frame(""action""=rep(c(""sub"",""start"",""end""),each=nrow(qd1))[order(as.POSIXlt(c(qd1$qsub_time,qd1$start_time,qd1$end_time),\n                                                                                           format = ""%m/%d/%Y %H:%M:%OS""))],\n                      ""slots""=as.numeric(rep(qd1$slots,3)[order(as.POSIXlt(c(qd1$qsub_time,qd1$start_time,qd1$end_time),\n                                                                           format = ""%m/%d/%Y %H:%M:%OS""))]),\n                      ""job""=rep(qd1$jobname,3)[order(as.POSIXlt(c(qd1$qsub_time,qd1$start_time,qd1$end_time),\n                                                                format = ""%m/%d/%Y %H:%M:%OS""))],\n                      ""time""=sort(as.POSIXlt(c(qd1$qsub_time,qd1$start_time,qd1$end_time),\n                                             format = ""%m/%d/%Y %H:%M:%OS"")),\n                      stringsAsFactors = F)\nqtimes1$number <- 0\nfor(i in 1:nrow(qtimes1)){\n  if(qtimes1$action[i]==""start""){\n    if(i==1) qtimes1$number[i] <- 0+qtimes1$slots[i] else qtimes1$number[i] <- qtimes1$number[i-1]+qtimes1$slots[i]\n  }else if(qtimes1$action[i]==""end""){\n    if(i==1) qtimes1$number[i] <- 0-qtimes1$slots[i] else qtimes1$number[i] <- qtimes1$number[i-1]-qtimes1$slots[i]\n  }else if(qtimes1$action[i]==""sub""){\n    if(i==1) qtimes1$number[i] <-0 else qtimes1$number[i] <- qtimes1$number[i-1]\n  }\n  if(qtimes1$number[i]<0) stop(""something\'s wrong"")\n}\nif(qtimes1$number[nrow(qtimes1)]!=0) warning(""no end"")\nmax(qtimes1$number)\n\nqtimes1$abstime <- as.numeric(as.POSIXlt(qtimes1$time))\n\nqd1$abstime_start <- sapply(qd1$jobname,\n                            function(x) qtimes1$abstime[qtimes1$action==""start""&qtimes1$job==x])\nqd1$abstime_end <- sapply(qd1$jobname,\n                          function(x) qtimes1$abstime[qtimes1$action==""end""&qtimes1$job==x])\n\n\n# 4 cores, small dataset\nqd4 <- readRDS(""Ukulinga_4.jobMatrix.RDS"")\nqd4$efficiency <- as.numeric(qd4$cpu) / (as.numeric(qd4$slots) * as.numeric(qd4$ru_wallclock))\nweighted.mean(qd4$efficiency,as.numeric(qd4$ru_wallclock)) # 93%\nsum(as.numeric(qd4$ru_wallclock))/60\n\nqtimes4 <- data.frame(""action""=rep(c(""sub"",""start"",""end""),each=nrow(qd4))[order(as.POSIXlt(c(qd4$qsub_time,qd4$start_time,qd4$end_time),\n                                                                                           format = ""%m/%d/%Y %H:%M:%OS""))],\n                      ""slots""=as.numeric(rep(qd4$slots,3)[order(as.POSIXlt(c(qd4$qsub_time,qd4$start_time,qd4$end_time),\n                                                                           format = ""%m/%d/%Y %H:%M:%OS""))]),\n                      ""job""=rep(qd4$jobname,3)[order(as.POSIXlt(c(qd4$qsub_time,qd4$start_time,qd4$end_time),\n                                                                format = ""%m/%d/%Y %H:%M:%OS""))],\n                      ""time""=sort(as.POSIXlt(c(qd4$qsub_time,qd4$start_time,qd4$end_time),\n                                             format = ""%m/%d/%Y %H:%M:%OS"")),\n                      stringsAsFactors = F)\nqtimes4$number <- 0\nfor(i in 1:nrow(qtimes4)){\n  if(qtimes4$action[i]==""start""){\n    if(i==1) qtimes4$number[i] <- 0+qtimes4$slots[i] else qtimes4$number[i] <- qtimes4$number[i-1]+qtimes4$slots[i]\n  }else if(qtimes4$action[i]==""end""){\n    if(i==1) qtimes4$number[i] <- 0-qtimes4$slots[i] else qtimes4$number[i] <- qtimes4$number[i-1]-qtimes4$slots[i]\n  }else if(qtimes4$action[i]==""sub""){\n    if(i==1) qtimes4$number[i] <-0 else qtimes4$number[i] <- qtimes4$number[i-1]\n  }\n  if(qtimes4$number[i]<0) stop(""something\'s wrong"")\n}\nif(qtimes4$number[nrow(qtimes4)]!=0) warning(""no end"")\nmax(qtimes4$number)\n\nqtimes4$abstime <- as.numeric(as.POSIXlt(qtimes4$time))\n\n# remove long waiting times\nfor(i in nrow(qtimes4):277){\n  if(qtimes4$action[i]==""sub""){\n    offset <- qtimes4$abstime[qtimes4$action==""start""&qtimes4$job==qtimes4$job[i]]-qtimes4$abstime[i]\n    qtimes4$abstime[1:i] <- qtimes4$abstime[1:i]+offset\n  }\n}\noffset <- qtimes4$abstime[qtimes4$action==""start""&qtimes4$job==qtimes4$job[275]]-qtimes4$abstime[274]-5\nqtimes4$abstime[1:274] <- qtimes4$abstime[1:274]+offset\n\nqd4$abstime_start <- sapply(qd4$jobname,\n                            function(x) qtimes4$abstime[qtimes4$action==""start""&qtimes4$job==x])\nqd4$abstime_end <- sapply(qd4$jobname,\n                          function(x) qtimes4$abstime[qtimes4$action==""end""&qtimes4$job==x])\n\n## 4 cores, medium data set\niqd4 <- readRDS(""NutNet_4.jobMatrix.RDS"")\niqd4$efficiency <- as.numeric(iqd4$cpu) / (as.numeric(iqd4$slots) * as.numeric(iqd4$ru_wallclock))\nweighted.mean(iqd4$efficiency,as.numeric(iqd4$ru_wallclock)) # 83%\nsum(as.numeric(iqd4$ru_wallclock))/60\nsum(as.numeric(iqd4$ru_wallclock))/3600\n\niqtimes4 <- data.frame(""action""=rep(c(""sub"",""start"",""end""),each=nrow(iqd4))[order(as.POSIXlt(c(iqd4$qsub_time,iqd4$start_time,iqd4$end_time),\n                                                                                             format = ""%m/%d/%Y %H:%M:%OS""))],\n                       ""slots""=as.numeric(rep(iqd4$slots,3)[order(as.POSIXlt(c(iqd4$qsub_time,iqd4$start_time,iqd4$end_time),\n                                                                             format = ""%m/%d/%Y %H:%M:%OS""))]),\n                       ""job""=rep(iqd4$jobname,3)[order(as.POSIXlt(c(iqd4$qsub_time,iqd4$start_time,iqd4$end_time),\n                                                                  format = ""%m/%d/%Y %H:%M:%OS""))],\n                       ""time""=sort(as.POSIXlt(c(iqd4$qsub_time,iqd4$start_time,iqd4$end_time),\n                                              format = ""%m/%d/%Y %H:%M:%OS"")),\n                       stringsAsFactors = F)\niqtimes4$number <- 0\nfor(i in 1:nrow(iqtimes4)){\n  if(iqtimes4$action[i]==""start""){\n    if(i==1) iqtimes4$number[i] <- 0+iqtimes4$slots[i] else iqtimes4$number[i] <- iqtimes4$number[i-1]+iqtimes4$slots[i]\n  }else if(iqtimes4$action[i]==""end""){\n    if(i==1) iqtimes4$number[i] <- 0-iqtimes4$slots[i] else iqtimes4$number[i] <- iqtimes4$number[i-1]-iqtimes4$slots[i]\n  }else if(iqtimes4$action[i]==""sub""){\n    if(i==1) iqtimes4$number[i] <-0 else iqtimes4$number[i] <- iqtimes4$number[i-1]\n  }\n  if(iqtimes4$number[i]<0) stop(""something\'s wrong"")\n}\nif(iqtimes4$number[nrow(iqtimes4)]!=0) warning(""no end"")\nmax(iqtimes4$number)\n\niqtimes4$abstime <- as.numeric(as.POSIXlt(iqtimes4$time))\n\n#remove long waiting times\noffset <- iqtimes4$abstime[iqtimes4$action==""start""&iqtimes4$job==iqtimes4$job[2438]]-iqtimes4$abstime[2437]-5\niqtimes4$abstime[1:2437] <- iqtimes4$abstime[1:2437]+offset\n\niqd4$abstime_start <- sapply(iqd4$jobname,\n                             function(x) iqtimes4$abstime[iqtimes4$action==""start""&iqtimes4$job==x])\niqd4$abstime_end <- sapply(iqd4$jobname,\n                           function(x) iqtimes4$abstime[iqtimes4$action==""end""&iqtimes4$job==x])\n\n\n## 50 cores, medium data set - uses only 15 at a time, because the queue is always full\niqd50 <- readRDS(""NutNet_50.jobMatrix.RDS"")\n\niqtimes50 <- data.frame(""action""=rep(c(""sub"",""start"",""end""),each=nrow(iqd50))[order(as.POSIXlt(c(iqd50$qsub_time,iqd50$start_time,iqd50$end_time),\n                                                                                               format = ""%m/%d/%Y %H:%M:%OS""))],\n                        ""slots""=as.numeric(rep(iqd50$slots,3)[order(as.POSIXlt(c(iqd50$qsub_time,iqd50$start_time,iqd50$end_time),\n                                                                               format = ""%m/%d/%Y %H:%M:%OS""))]),\n                        ""job""=rep(iqd50$jobname,3)[order(as.POSIXlt(c(iqd50$qsub_time,iqd50$start_time,iqd50$end_time),\n                                                                    format = ""%m/%d/%Y %H:%M:%OS""))],\n                        ""time""=sort(as.POSIXlt(c(iqd50$qsub_time,iqd50$start_time,iqd50$end_time),\n                                               format = ""%m/%d/%Y %H:%M:%OS"")),\n                        stringsAsFactors = F)\niqtimes50$number <- 0\nfor(i in 1:nrow(iqtimes50)){\n  if(iqtimes50$action[i]==""start""){\n    if(i==1) iqtimes50$number[i] <- 0+iqtimes50$slots[i] else iqtimes50$number[i] <- iqtimes50$number[i-1]+iqtimes50$slots[i]\n  }else if(iqtimes50$action[i]==""end""){\n    if(i==1) iqtimes50$number[i] <- 0-iqtimes50$slots[i] else iqtimes50$number[i] <- iqtimes50$number[i-1]-iqtimes50$slots[i]\n  }else if(iqtimes50$action[i]==""sub""){\n    if(i==1) iqtimes50$number[i] <-0 else iqtimes50$number[i] <- iqtimes50$number[i-1]\n  }\n  if(iqtimes50$number[i]<0) stop(""something\'s wrong"")\n}\nif(iqtimes50$number[nrow(iqtimes50)]!=0) warning(""no end"")\nmax(iqtimes50$number)\n\niqtimes50$abstime <- as.numeric(as.POSIXlt(iqtimes50$time))\n\n#remove long waiting times\noffset <- iqtimes50$abstime[iqtimes50$action==""start""&iqtimes50$job==iqtimes50$job[2438]]-iqtimes50$abstime[2437]-5\niqtimes50$abstime[1:2437] <- iqtimes50$abstime[1:2437]+offset\n\niqd50$abstime_start <- sapply(iqd50$jobname,\n                              function(x) iqtimes50$abstime[iqtimes50$action==""start""&iqtimes50$job==x])\niqd50$abstime_end <- sapply(iqd50$jobname,\n                            function(x) iqtimes50$abstime[iqtimes50$action==""end""&iqtimes50$job==x])\n\n\n### plotting preparation of colours and legend\ncols <- data.frame(""job""=unique(gsub("".[[:digit:]]+.sh"","""",\n                                     gsub(""snakejob."","""",c(iqd4$jobname,qd4$jobname)))),\n                   \n                   ""col""=c(colorRampPalette(c(""white"",rainbow(9)[1]))(10)[7],#copy_fwd\n                           rainbow(9)[2],#dada_filter\n                           ""grey20"",#input_numbers\n                           ""grey30"",#primer_numbers \n                           colorRampPalette(c(""white"",rainbow(24)[19]))(10)[2],#dada_qc1\n                           rainbow(9)[3],#dada_errors                \n                           colorRampPalette(c(""white"",rainbow(24)[19]))(10)[4],#dada_qc_filtered            \n                           ""grey45"",#filter_numbers              \n                           colorRampPalette(c(""white"",rainbow(9)[5]))(10)[c(5,10)],#dada_mergeReadPairs,#dada_mergeSamples           \n                           ""grey60"",#merged_numbers              \n                           rainbow(24)[13],#dada_mergeruns             \n                           ""grey75"",#nochime_numbers \n                           colorRampPalette(c(""white"",rainbow(9)[7]))(10)[c(5,7,10)],#ITSx,mothur_taxonomy_postITSx,#taxonomy_to_OTUtab\n                           colorRampPalette(c(""white"",rainbow(24)[19]))(10)[8],#biom_handoff_tax  \n                           rainbow(24)[14],#filtering_table \n                           ""grey90"",#table_filter_numbers     \n                           colorRampPalette(c(""white"",rainbow(24)[16]))(10)[4],#guilds_Filter\n                           colorRampPalette(c(""white"",rainbow(24)[19]))(10)[10],#phyloseq_handoff_postFilter\n                           colorRampPalette(c(""white"",rainbow(24)[19]))(10)[6],#rarefaction_curve_Filter\n                           colorRampPalette(c(""white"",rainbow(9)[1]))(10)[10],#cut_primer_both\n                           colorRampPalette(c(""white"",rainbow(9)[7]))(10)[7],#mothur_taxonomy\n                           colorRampPalette(c(""white"",rainbow(24)[16]))(10)[7],#prepare_blastn              \n                           colorRampPalette(c(""white"",rainbow(24)[16]))(10)[9],#blastn       \n                           colorRampPalette(c(""white"",rainbow(9)[9]))(10)[c(7,10)]#multiAlign_Filter,#treeing_Filter_fasttreeMP   \n                   ),\n                   ""order""=c(1,3,23,24,           \n                             20,4,21, 25,            \n                             5, 6, 26,  7,             \n                             27,           8,9,11,        \n                             18, 14, 28,  15,             \n                             19, 22,   2,10,            \n                             12,13,16,17 ),\n                   ""niceNames""=c(""copy preprocessed reads"",""DADA2: quality filter"",""numbers: input"",""numbers: primers"",\n                                 ""input reads QC plot"",""DADA2: error models"",""filtered reads QC plot"",""numbers: filtered"",\n                                 ""DADA2: ASVs from reads pairs"",""DADA2: ASV tables per run"",""numbers: DADA2"",\n                                 ""DADA2: merge runs + chimera removal"",""numbers: non-chimeric"",""ITSx"",\n                                 ""taxonomy: mothur"",""taxonomy: add to table"",""handoff: biom"",""filter ASV table"",\n                                 ""numbers: filtered table"",""FunGuild"",""handoff: phyloseq"",""rarefaction curves"",\n                                 ""cut primers"",""taxonomy: mothur"",\n                                 ""prepare blastn"", ""blastn"", ""multiple alignment"", ""fasttreeMP""),\n                   stringsAsFactors = F)\n\nqd1$nicecol <- sapply(gsub("".[[:digit:]]+.sh"","""",\n                           gsub(""snakejob."","""",qd1$jobname)),\n                      function(x) cols$col[cols$job==x])\nqd4$nicecol <- sapply(gsub("".[[:digit:]]+.sh"","""",\n                           gsub(""snakejob."","""",qd4$jobname)),\n                      function(x) cols$col[cols$job==x])\niqd4$nicecol <- sapply(gsub("".[[:digit:]]+.sh"","""",\n                            gsub(""snakejob."","""",iqd4$jobname)),\n                       function(x) cols$col[cols$job==x])\niqd50$nicecol <- sapply(gsub("".[[:digit:]]+.sh"","""",\n                             gsub(""snakejob."","""",iqd50$jobname)),\n                        function(x) cols$col[cols$job==x])\n\n#plot\n\npdf(""timeflow_both.pdf"",width=17/2.54,height=7/2.54,pointsize=9)\nlayout(matrix(1:5,nrow=1,ncol=5,byrow = T),widths = c(0.74*c(4/36,7/36,7/36,18/36),0.26))\nlatest <- max(qd1$abstime_end)+1\nplotoff <- 240*60-(latest-min(qd1$abstime_start))\npar(mar=c(1.5,3.1,1.5,1),tcl=-0.3,mgp=c(1.8,0.5,0))\nplot(1,type=""n"",xlim=c(0,max(qtimes1$number)),\n     ylim=c(0,240*60),xaxs=""i"",\n     las=1,axes=F,ann=F,yaxs=""i"")\naxis(2,at=seq((240-160)*60,240*60,by=1200),labels=seq(160,0,by=-20),las=1,lwd=0.5)\nmtext(""minutes"",2,1.8,adj = (480-160)/480,cex=7/9)\n#mtext(""cores"",1,-5,cex=6/8)\npos <- rep(qd1$abstime_start[1]-1,1)\nfor(i in 1:nrow(qd1)){\n  newpos <- min(which(pos<qd1$abstime_start[i]))\n  polygon(c(0.1,-0.1,-0.1,0.1)+\n            c(newpos-1,\n              as.numeric(qd1$slots[i])+newpos-1,\n              as.numeric(qd1$slots[i])+newpos-1,\n              newpos-1),\n          plotoff+rep(c(latest-qd1$abstime_start[i],\n                        latest-qd1$abstime_end[i]),each=2),\n          border=NA, col=qd1$nicecol[i])\n  pos[newpos] <- qd1$abstime_end[i]\n}\n\nlatest <- max(qd4$abstime_end)+1\nplotoff <- 240*60-(latest-min(qd4$abstime_start))\npar(mar=c(1.5,3.2,1.5,1),tcl=-0.3,mgp=c(1.8,0.5,0))\nplot(1,type=""n"",xlim=c(0,4),\n     ylim=c(0,240*60),xaxs=""i"",\n     las=1,axes=F,ann=F,yaxs=""i"")\naxis(2,at=seq((240-100)*60,240*60,by=1200),labels=seq(100,0,by=-20),las=1,lwd=0.5)\nmtext(""minutes"",2,1.8,adj = (480-100)/480,cex=7/9)\n#mtext(""cores"",1,-25.5)\npos <- rep(qd4$abstime_start[1]-1,4)\nfor(i in 1:nrow(qd4)){\n  newpos <- min(which(pos<qd4$abstime_start[i]))\n  polygon(c(0.1,-0.1,-0.1,0.1)+\n            c(newpos-1,\n              as.numeric(qd4$slots[i])+newpos-1,\n              as.numeric(qd4$slots[i])+newpos-1,\n              newpos-1),\n          plotoff+rep(c(latest-qd4$abstime_start[i],\n                        latest-qd4$abstime_end[i]),each=2),\n          border=NA, col=qd4$nicecol[i])\n  pos[newpos] <- qd4$abstime_end[i]\n}\n\nlatest <- max(iqd4$abstime_end)+1\nplotoff <- 240*60-(latest-min(iqd4$abstime_start))\npar(mar=c(1.5,3,1.5,1),tcl=-0.3,mgp=c(1.8,0.5,0))\nplot(1,type=""n"",xlim=c(0,4),\n     ylim=c(plotoff,240*60),xaxs=""i"",\n     las=1,axes=F,ann=F,yaxs=""i"")\naxis(2,at=seq(0,240*60,by=1200),labels=seq(240,0,by=-20),las=1,lwd=0.5)\nmtext(""minutes"",2,1.8,adj = 0.5,cex=7/9)\n#mtext(""cores"",1,0.3)\npos <- rep(iqd4$abstime_start[1]-1,4)\nfor(i in 1:nrow(iqd4)){\n  newpos <- min(which(pos<iqd4$abstime_start[i]))\n  polygon(c(0.1,-0.1,-0.1,0.1)+\n            c(newpos-1,\n              as.numeric(iqd4$slots[i])+newpos-1,\n              as.numeric(iqd4$slots[i])+newpos-1,\n              newpos-1),\n          plotoff+rep(c(latest-iqd4$abstime_start[i],\n                        latest-iqd4$abstime_end[i]),each=2),\n          border=NA, col=iqd4$nicecol[i])\n  pos[newpos] <- iqd4$abstime_end[i]\n}\n\nlatest <- max(iqd50$abstime_end)+1\nplotoff <- 240*60-(latest-min(iqd50$abstime_start))\npar(mar=c(1.5,3,1.5,1),tcl=-0.3,mgp=c(1.8,0.5,0))\nplot(1,type=""n"",xlim=c(0,max(iqtimes50$number)),\n     ylim=c(0,240*60),xaxs=""i"",\n     las=1,axes=F,ann=F,yaxs=""i"")\naxis(2,at=seq((240-120)*60,240*60,by=1200),labels=seq(120,0,by=-20),las=1,lwd=0.5)\nmtext(""minutes"",2,1.8,adj = 0.75,cex=7/9)\n#mtext(""cores"",1,-4.5)\npos <- rep(iqd50$abstime_start[1]-1,max(iqtimes50$number))\nfor(i in 1:nrow(iqd50)){\n  newpos <- min(which(pos<iqd50$abstime_start[i]))\n  polygon(c(0.1,-0.1,-0.1,0.1)+\n            c(newpos-1,\n              as.numeric(iqd50$slots[i])+newpos-1,\n              as.numeric(iqd50$slots[i])+newpos-1,\n              newpos-1),\n          plotoff+rep(c(latest-iqd50$abstime_start[i],\n                        latest-iqd50$abstime_end[i]),each=2),\n          border=NA, col=iqd50$nicecol[i])\n  pos[newpos] <- iqd50$abstime_end[i]\n}\n\npar(mar=c(0,0,0,0))\nplot(1,type=""n"",las=1,axes=F,ann=F)\nlegend(""topleft"",fill=unique(cols$col[order(cols$order)]),ncol=1,\n       border=NA,legend=unique(cols$niceNames[order(cols$order)]),\n       bty=""n"")\n\ndev.off()\n\n', '# load libraries\nlibrary(data.table)\nlibrary(vegan)\n\n\nwd <- ""../mock_datasets""\nsetwd(wd)\n\n# list dadasnake results for fungi and bacteria\nfuns <- gsub(""fungi."","""",gsub("".all.seqTab.tax.RDS"","""",list.files(pattern=""fungi.*.all.seqTab.tax"")))\nbacs <- gsub(""bac."","""",gsub("".all.seqTab.tax.RDS"","""",list.files(pattern=""bac.*.all.seqTab.tax"")))\n\n# tidy data.frame of metadata\nfundf <- data.frame(""allOTUs_file""=paste0(""fungi."",funs,"".all.seqTab.tax.RDS""),\n                    ""filteredOTUs_file""=paste0(""fungi."",funs,"".filtered.seqTab.RDS""),\n                    ""seqNums_file""=paste0(""fungi."",funs,"".finalNumbers_perSample.tsv""),\n                    ""ID""=funs,\n                    ""reads""=as.numeric(sub(""k"","""",funs))*1000^as.numeric(grepl(""k"",funs)),\n                    stringsAsFactors = F)\nbacdf <- data.frame(""allOTUs_file""=paste0(""bac."",bacs,"".all.seqTab.tax.RDS""),\n                    ""filteredOTUs_file""=paste0(""bac."",bacs,"".filtered.seqTab.RDS""),\n                    ""seqNums_file""=paste0(""bac."",bacs,"".finalNumbers_perSample.tsv""),\n                    ""ID""=bacs,\n                    ""reads""=as.numeric(sub(""k"","""",bacs))*1000^as.numeric(grepl(""k"",bacs)),\n                    stringsAsFactors = F)\n\n# load dadasnake results\nfunA <- lapply(1:nrow(fundf),\n                    function(x) readRDS(fundf$allOTUs_file[x]))\nbacA <- lapply(1:nrow(bacdf),\n               function(x) readRDS(bacdf$allOTUs_file[x]))\n\n# number of OTUs\nfundf$allOTUs <- sapply(1:nrow(fundf),function(x) nrow(funA[[x]]))\nbacdf$allOTUs <- sapply(1:nrow(bacdf),function(x) nrow(bacA[[x]]))\n\n# number of filtered OTUs - load and count\nfunF <- lapply(1:nrow(fundf),\n               function(x) readRDS(fundf$filteredOTUs_file[x]))\nbacF <- lapply(1:nrow(bacdf),\n               function(x) readRDS(bacdf$filteredOTUs_file[x]))\nfundf$filteredOTUs <- sapply(1:nrow(fundf),function(x) nrow(funF[[x]]))\nbacdf$filteredOTUs <- sapply(1:nrow(bacdf),function(x) nrow(bacF[[x]]))\n\n# number of reads - add to metadata table\nfundf <- data.frame(fundf,\n                    t(sapply(1:nrow(fundf), function(x){\n                      a <- read.delim(fundf$seqNums_file[x],stringsAsFactors=F)[,-c(1:5)]\n                    })),\n                    stringsAsFactors = F)\nbacdf <- data.frame(bacdf,\n                    t(sapply(1:nrow(bacdf), function(x){\n                      a <- read.delim(bacdf$seqNums_file[x],stringsAsFactors=F)[,-c(1:5)]\n                    })),\n                    stringsAsFactors = F)\n\n# OTU table fungi\nfunA2 <- rbindlist(lapply(1:nrow(fundf),\n                          function(x) data.frame(""depth""=fundf$reads[x],\n                                                 readRDS(fundf$allOTUs_file[x]),\n                                                 stringsAsFactors = F)))\nfunOTU <- tapply(funA2$Mock,list(funA2$Row.names,funA2$depth),sum)\nfunOTU[is.na(funOTU)] <- 0\nfunTax <- t(sapply(rownames(funOTU),\n                   function(x) {\n                     y <- as.data.frame(funA2)[funA2$Row.names==x,grep(""mothur"",colnames(funA2))]\n                     apply(y,2,\n                           function(z){\n                             if(length(which(z!=""""))>0) paste(unique(z[z!=""""]),sep=""|"",collapse=""|"") else """"\n                           })\n                     }\n                   ))\n\n# OTU table bacteria\nbacA2 <- rbindlist(lapply(1:nrow(bacdf),\n                          function(x) data.frame(""depth""=bacdf$reads[x],\n                                                 readRDS(bacdf$allOTUs_file[x]),\n                                                 stringsAsFactors = F)))\nbacOTU <- tapply(bacA2$mockbac,list(bacA2$Row.names,bacA2$depth),sum)\nbacOTU[is.na(bacOTU)] <- 0\nbacTax <- t(sapply(rownames(bacOTU),\n                   function(x) {\n                     y <- as.data.frame(bacA2)[bacA2$Row.names==x,grep(""mothur"",colnames(bacA2))]\n                     apply(y,2,\n                           function(z){\n                             if(length(which(z!=""""))>0) paste(unique(z[z!=""""]),sep=""|"",collapse=""|"") else """"\n                           })\n                   }\n))\n# some adjustments to names\nbacTax[95,8] <- ""Hungateiclostridium_thermocellum_ATCC_27405""\nbacTax[95,7] <- ""Hungateiclostridium""\nbacTax[95,6] <- ""Hungateiclostridiaceae""\nbacTax[95,5] <- """"\nbacTax[47:48,7] <- ""Nostoc""\n\n# check number of reads at the different steps - fungi\nplot(fundf$reads,fundf$reads_primers_fwd,log=""xy"",pch=16,col=rainbow(5)[1],xlim=c(40,50000),ylim=c(40,50000),\n     las=1)\npoints(fundf$reads,fundf$reads_filtered_fwd,pch=16,col=rainbow(5)[3])\npoints(fundf$reads,fundf$reads_merged,pch=16,col=rainbow(5)[4])\npoints(fundf$reads,fundf$reads_chimera_checked,pch=16,cex=0.5,col=rainbow(5)[5])\nabline(a=0,b=1,lty=3)\nplot(fundf$reads_chimera_checked,fundf$allOTUs,pch=16,ylim=c(0,22))\nabline(h=19,lty=3)\nrarecurve(t(funOTU),label = F)\nestimateR(t(funOTU))\n\n# check number of reads at the different steps - bacteria\nplot(bacdf$reads,bacdf$reads_primers_fwd,log=""xy"",pch=16,col=rainbow(5)[1],xlim=c(20,16e6),ylim=c(20,16e6),\n     las=1)\npoints(bacdf$reads,bacdf$reads_filtered_fwd,pch=16,col=rainbow(5)[3])\npoints(bacdf$reads,bacdf$reads_merged,pch=16,col=rainbow(5)[4])\npoints(bacdf$reads,bacdf$reads_chimera_checked,pch=16,cex=0.5,col=rainbow(5)[5])\nabline(a=0,b=1,lty=3)\nplot(bacdf$reads_chimera_checked,bacdf$allOTUs,pch=16,ylim=c(0,100))\nabline(h=59,lty=3)\nrarecurve(t(bacOTU),label = F)\nestimateR(t(bacOTU))\n\n# format ground truth datasets\nbacGT <- read.delim(""../Damore_GT.txt"",header=F,stringsAsFactors = F)[,-3]\nfor(i in 1:ncol(bacGT)) bacGT[,i] <- gsub("" +$"","""",gsub(""^ +"","""",bacGT[,i]))\nbacGT$V6 <- as.numeric(bacGT$V6)\ncolnames(bacGT) <- c(""ID"",""strain"",""Phylum"",""Class"",""Prop"")\nwrite.table(bacGT,""../Damore_GT_formated.txt"",sep=""\\t"",quote = F,row.names = F)\nfunGT <- read.delim(""../Fungi_GT.txt"",stringsAsFactors = F,header=F,strip.white = T,sep=""\\t"")\ncolnames(funGT) <- c(""ID"",""Phylum"",""Class"",""Order"",""Family"",""Genus"",""Species"")\nwrite.table(data.frame(funOTU,funTax,stringsAsFactors = F),\n            ""../Fungi_ID.tsv"",sep=""\\t"",quote = F,col.names = NA)\n\n# check overlaps\nsetdiff(funGT$Species,gsub(""_"","" "",funTax[,8]))\n# ""Penicillium expansum""     ""Claviceps purpurea""       ""Naganishia albida""        ""Rhizomucor miehei""       \n# ""Mortierella verticillata"" ""Fusarium graminearum""     ""Fusarium verticillioides"" ""Saccharomyces cerevisiae""\nsetdiff(funGT$Genus,funTax[,7])\n#  ""Rhizomucor""        \nsetdiff(funGT$Phylum,funTax[,3])\n# ""Mucoromycota""\nsetdiff(funGT$Class,funTax[,4])\n# ""Mucoromycetes""\nsetdiff(funGT$Order,funTax[,5])\n#  ""Mucorales""\nsetdiff(funGT$Family,funTax[,6])\n# ""Lichtheimiaceae""\nsetdiff(gsub(""_"","" "",funTax[,8]),funGT$Species)\n# """"                            ""Naganishia adeliensis""       ""Mortierella humilis""         ""Penicillium aurantiogriseum""\n# ""Fusarium ramigenum""          ""Gibberella pulicaris""        ""Claviceps sp""  \nsetdiff(funTax[,7],funGT$Genus)\n#  ""Gibberella""   #Gibberella confused with a Fusarium\nintersect(funTax[,7],funGT$Genus)\nsetdiff(funTax[,4],funGT$Class)\n# \nsetdiff(funTax[,3],funGT$Phylum)\n# \nsetdiff(funTax[,5],funGT$Order)\n#  \nsetdiff(funTax[,6],funGT$Family)\n# \n\n# recognised diversity:\n# Aspergillus: 2 correct\n# Fusarium: 3 correct, but one misclassified\n\n#inflated diversity:\n# Rhizophagus: 5 -> 1 in reality\n\nbothGTOTU_fun <- merge(data.frame(funOTU,funTax,stringsAsFactors = F)[funTax[,7] %in% \n                                                                    names(table(funTax[funTax[,7] %in% \n                                                                                         intersect(funTax[,7],\n                                                                                                   names(table(funGT$Genus))[table(funGT$Genus)==1]),\n                                                                                       7]))[table(funTax[funTax[,7] %in% \n                                                                                                           intersect(funTax[,7],\n                                                                                                                     names(table(funGT$Genus))[table(funGT$Genus)==1]),\n                                                                                                         7])==1],\n                                                                  c(1:9,16)],\n                   funGT,by.x=10,by.y=""Genus"")\nplot(bothGTOTU_fun$X40000)\nsd(bothGTOTU_fun$X40000)/mean(bothGTOTU_fun$X40000) \n# 90%\nplot(sort(unlist(fundf$reads_chimera_checked)),\n     100*sapply(2:10,function(x) sd(bothGTOTU_fun[,x])/mean(bothGTOTU_fun[,x])),\n     log=""x"",las=1,xlab=""processed reads"",ylab=""Coefficient of variation"")\n\n# plots\npdf(""fungi_ASVvsDepth.pdf"",width=5.5/2.54,height=5/2.54,pointsize=8)\npar(mar=c(2.8,3,0.5,0.5),mgp=c(1.6,0.5,0),tcl=-0.3)\nplot(fundf$reads_chimera_checked,\n     sapply(fundf$reads,function(x){\n       length(which(funOTU[,which(colnames(funOTU)==x)]>0))\n     })\n     ,pch=16,ylim=c(0,23),\n     ylab=""observed ASVs"",xlab=""processed reads"",las=1,\n     xaxt = ""n"",\n     cex.axis=6/8)\neaxis(1,cex.axis=6/8,use.expr = T,at=c(0,5e3,1e4,2e4,3e4))\nabline(h=19,lty=3,col=""grey30"")\ndev.off()\n\npdf(""fungi_rarecurve.pdf"",width=5.5/2.54,height=5/2.54,pointsize=8)\npar(mar=c(2.8,3,0.5,0.5),mgp=c(1.6,0.5,0),tcl=-0.3)\nrarecurve(t(funOTU),label = F,las=1,xaxt=""n"",ylab = ""observed ASVs"",cex.axis=6/8)\neaxis(1,cex.axis=6/8,use.expr = T,at=c(0,5e3,1e4,2e4,3e4))\ndev.off()\n\npdf(""fungi_CVvsDepth.pdf"",width=5.5/2.54,height=5/2.54,pointsize=8)\npar(mar=c(2.8,3,0.5,0.5),mgp=c(1.6,0.5,0),tcl=-0.3)\nplot(sort(unlist(fundf$reads_chimera_checked)),\n     100*sapply(2:10,function(x) sd(bothGTOTU_fun[,x])/mean(bothGTOTU_fun[,x])),\n     las=1,xlab=""processed reads"",ylab=""Coefficient of variation"",xaxt=""n"",cex.axis=6/8,\n     pch=16)\neaxis(1,cex.axis=6/8,use.expr = T,at=c(0,5e3,1e4,2e4,3e4))\nabline(h=19,lty=3,col=""grey30"")\ndev.off()\n\n# format ground truth data set - bacteria\nbacGT <- read.delim(""../Damore_GT.txt"",header=F,stringsAsFactors = F)[,-3]\nfor(i in 1:ncol(bacGT)) bacGT[,i] <- gsub("" +$"","""",gsub(""^ +"","""",bacGT[,i]))\nbacGT$V6 <- as.numeric(bacGT$V6)\ncolnames(bacGT) <- c(""ID"",""strain"",""Phylum"",""Class"",""Prop"")\nwrite.table(bacGT,""../Damore_GT_formated.txt"",sep=""\\t"",quote = F,row.names = F)\nbacGT <- read.delim(""../Damore_GT_formated.txt"",stringsAsFactors = F)\n# adjust nomeclature\nbacGT$Genus <- sapply(bacGT$Spec.corr,function(x)unlist(strsplit(x,split="" ""))[1])\nbacGT$Genus[bacGT$Genus==""Paraburkholderia""] <- ""Burkholderia-Caballeronia-Paraburkholderia""\nbacGT[nrow(bacGT)+1,] <- c(""PUT"","""","""","""","""","""",""Proteobacteria"",""Gammaproteobacteria"",\n                           ""Dickeya solani"",""Morganellaceae"",""Enterobacterales"",""Dickeya"")\nbacGT$Prop <- as.numeric(bacGT$Prop)\nwrite.table(bacGT,""../Damore_GT_formated_cleanedNames.tsv"",sep=""\\t"",quote = F,row.names = F)\nwrite.table(data.frame(bacOTU,bacTax,stringsAsFactors = F),\n            ""../Bacteria_ID.tsv"",sep=""\\t"",quote = F,col.names = NA)\n# check overlaps\nsetdiff(bacGT$Genus,bacTax[,7])\n# ""Pelodictyon""      ""Ignicoccus""    ""Pyrococcus""  ""Rhodospirillum""   ""Ruegeria""        \nintersect(bacGT$Genus,bacTax[,7])\n# 40\nsetdiff(bacGT$Phylum.corr,bacTax[,3])\n#none\nsetdiff(bacGT$Class.corr,bacTax[,4])\n# ""Thermococci""\nsetdiff(bacGT$Order,bacTax[,5])\n# ""Desulfurococcales"" ""Thermococcales""    ""Rhodospirillales"" \nsetdiff(bacGT$Family,bacTax[,6])\n#""Ignicoccaceae""     ""Thermococcaceae""   ""Rhodospirillaceae""\nsetdiff(bacTax[,7],bacGT$Genus)\n# """"                               ""Pseudomonas""                    ""Enhydrobacter""                 \n# ""Acinetobacter""                  ""Methylobacterium-Methylorubrum"" ""Brevundimonas""                 \n# ""Phyllobacterium""                ""Brucella""                       ""Mesorhizobium""                 \n# ""Stenotrophomonas""               ""Flavobacterium""                 ""Pedobacter""                    \n# ""Sphingomonas""                                     \n# ""Klebsiella""                     ""Escherichia-Shigella""           ""Lutibacter""                    \n# ""Ralstonia""                      ""Massilia""                      \n# ""Rhodococcus""                    ""Corynebacterium""                ""Streptococcus""                 \n# ""Hydrogenobacter""                ""Staphylococcus""                 ""Lactobacillus""  \nsetdiff(bacTax[,4],bacGT$Class.corr)\n# Polyangia\nsetdiff(bacTax[,3],bacGT$Phylum.corr)\n# ""Myxococcota""\nsetdiff(bacTax[,5],bacGT$Order)\n#  ""mle1-27""                             ""Pseudomonadales""                    \n# ""Rhizobiales""                         ""Caulobacterales""                    \n# ""Xanthomonadales""                     ""Flavobacteriales""                   \n# ""Sphingobacteriales""                  ""Peptostreptococcales-Tissierellales""\n# ""Corynebacteriales""                   ""Staphylococcales""     \nsetdiff(bacTax[,6],bacGT$Family)\n# [1] """"                    ""Pseudomonadaceae""    ""Moraxellaceae""       ""Beijerinckiaceae""    ""Caulobacteraceae""   \n# [6] ""Rhizobiaceae""        ""Xanthomonadaceae""    ""Flavobacteriaceae""   ""Sphingobacteriaceae"" ""Pectobacteriaceae""  \n# [11] ""Enterobacteriaceae""  ""Peptoniphilus""       ""Oxalobacteraceae""    ""Nocardiaceae""        ""Corynebacteriaceae"" \n# [16] ""Streptococcaceae""    ""Aquificaceae""        ""Staphylococcaceae""   ""Lactobacillaceae""    \n#View(cbind(bacOTU,bacTax)[which(bacTax[,6] %in% setdiff(bacTax[,6],bacGT$Family)),])\n\n# check similarity between ASVs - bacteria\nlibrary(DECIPHER)\nlibrary(Biostrings)\nbacSeq <- DNAStringSet(rownames(bacTax))\nnames(bacSeq) <- paste0(""cbOTU"",sprintf(""%03s"",1:nrow(bacTax)))\nwriteXStringSet(bacSeq,""../bac_combined_OTUs.fasta"")\nbacMSA <- readDNAStringSet(""../bac_combined_OTUs.mafft.fasta"")\nd <- DECIPHER::DistanceMatrix(bacMSA)\n\nView(cbind(apply(d[bacTax[,6] %in% setdiff(bacTax[,6],bacGT$Family),\n                   !bacTax[,6] %in% setdiff(bacTax[,6],bacGT$Family)],1,\n                 function(x)min(x[x>0])),\n           cbind(bacOTU,bacTax)[which(names(bacSeq) %in% \n                                  names(apply(d[bacTax[,6] %in% setdiff(bacTax[,6],bacGT$Family),\n                                          !bacTax[,6] %in% setdiff(bacTax[,6],bacGT$Family)],\n                                        1,\n                                        function(x) min(x[x>0])))),]))\n# 2 Flavobacter (Flavobacterium, Salter and others)\n# In molecular biology, Pedobacter has also been identified as a contaminant of DNA extraction kit reagents and ultra-pure water systems, which may lead to its erroneous appearance in microbiota or metagenomic datasets.\n# Peptoniphilus?\n# a Myxococcota\n# Stenotrophomonas (Salter and others)\n# Hydrogenobacter?\n\n# Dickeya from the other Dickeya that is likely in the data set \n# => 31 OTUs that are likely contaminants + 1 duplication\n# 5 OTUs in ground truth dataset that were never found\n# 62 OTUs representing the other 54 OTUs \n# genera with more than 1 OTU:\n# Bacteroides: 4 -> 2 in reality (diversity for B tio in Vetrovsky and Baldrian 2013)\n# Chlorobium: 5 (4 per sample) -> 3 in reality ?no genome info\n# Chloroflexus: 2 -> 1 in reality  ?no genome info\n# Fusobacterium: 3 -> 1 in reality \n# Nostoc: 2 -> 1 in reality (2 divergent reads at 16000k)\n# Thermoanaerobacter: 2 -> 1 in reality (diversity for B tio in Vetrovsky and Baldrian 2013)\n# Treponema: 3 -> 2 in reality\n\n# recognised diversity:\n# Desulfovibrio: 2 correct\n# Methanococcus: 2 correct\n# Pyrobaculum: 2 correct - no genome info\n# Shewanella: 2 correct\n# Sulfurihydrogenibium: 2 correct\n# Caldicellulosiruptor: 3 -> 2 in reality (also only detects 2, but with different sequences in different samples)\n\n\n#unrecognised diversity:\n# Thermotoga: 2 -> 3 in reality\n# Salinispora: 1 -> 2 in reality\n# Sulfitobacter: 1 -> 2 in reality\n\nsapply(names(table(bacTax[bacTax[,7] %in% intersect(bacTax[,7],bacGT$Genus),7]))[table(bacTax[bacTax[,7] %in% intersect(bacTax[,7],bacGT$Genus),7])>1],\n       function(x) d[bacTax[,7]==x,\n                     bacTax[,7]==x])\nView(cbind(bacOTU,bacTax)[bacTax[,7] %in% names(table(bacTax[bacTax[,7] %in% intersect(bacTax[,7],bacGT$Genus),7]))[table(bacTax[bacTax[,7] %in% intersect(bacTax[,7],bacGT$Genus),7])>1],])\n\nplot(bacdf$reads_chimera_checked,\n     sapply(bacdf$reads,function(x){\n       length(which(bacOTU[bacTax[,7] %in% intersect(bacTax[,7],c(bacGT$Genus,""Insertae_Sedis""))|\n                             bacTax[,6] ==""Rhodobacteraceae"",\n              which(colnames(bacOTU)==x)]>0))\n     })\n     ,pch=16,ylim=c(0,60),\n     ylab=""observed OTUs"",xlab=""processed reads"",las=1)\nabline(h=59,lty=3,col=""grey"")\nabline(h=55,lty=3,col=""red"")\n\nplot(bacdf$reads_chimera_checked,\n     sapply(bacdf$reads,function(x){\n       length(which(bacOTU[bacTax[,7] %in% setdiff(bacTax[,7],c(bacGT$Genus,""Insertae_Sedis""))&\n                             !bacTax[,6] ==""Rhodobacteraceae"",\n                           which(colnames(bacOTU)==x)]>0))\n     })\n     ,pch=16,\n     ylab=""observed OTUs"",xlab=""processed reads"",las=1)\n\n# plot results for bacteria\nlibrary(sfsmisc)\npdf(""bacteria_ASVvsDepth.pdf"",width=5.5/2.54,height=5/2.54,pointsize=8)\npar(mar=c(2.8,3,0.5,0.5),mgp=c(1.6,0.5,0),tcl=-0.3)\nplot(bacdf$reads_chimera_checked,\n     sapply(bacdf$reads,function(x){\n       length(which(bacOTU[bacTax[,7] %in% intersect(bacTax[,7],c(bacGT$Genus,""Insertae_Sedis""))|\n                             bacTax[,6] ==""Rhodobacteraceae"",\n                           which(colnames(bacOTU)==x)]>0))\n     })\n     ,pch=16,ylim=c(0,60),\n     ylab=""observed ASVs"",xlab=""processed reads"",las=1,xaxt = ""n"",cex.axis=6/8,type=""n"")\neaxis(1,cex.axis=6/8,at = c(0,1e5,5e5,1e6),lwd=0.5,at.small = c(2e5,3e5,4e5,6:9*1e5))\nabline(h=59,lty=3,col=""grey30"")\n\npoints(bacdf$reads_chimera_checked,\n     sapply(bacdf$reads,function(x){\n       length(which(bacOTU[bacTax[,7] %in% setdiff(bacTax[,7],c(bacGT$Genus,""Insertae_Sedis""))&\n                             !bacTax[,6] ==""Rhodobacteraceae"",\n                           which(colnames(bacOTU)==x)]>0))\n     })\n     ,pch=16,col=""grey80"")\npoints(bacdf$reads_chimera_checked,\n       sapply(bacdf$reads,function(x){\n         length(which(bacOTU[bacTax[,7] %in% intersect(bacTax[,7],c(bacGT$Genus,""Insertae_Sedis""))|\n                               bacTax[,6] ==""Rhodobacteraceae"",\n                             which(colnames(bacOTU)==x)]>0))\n       })\n       ,pch=16)\ndev.off()\n\n\npdf(""bacteria_rarecurve.pdf"",width=5.5/2.54,height=5/2.54,pointsize=8)\npar(mar=c(2.8,3,0.5,0.5),mgp=c(1.6,0.5,0),tcl=-0.3)\nrarecurve(t(bacOTU[bacTax[,7] %in% intersect(bacTax[,7],c(bacGT$Genus,""Insertae_Sedis""))|\n                     bacTax[,6] ==""Rhodobacteraceae"",]),label = F,las=1,xaxt=""n"",\n          ylab = ""observed ASVs"",cex.axis=6/8)\neaxis(1,cex.axis=6/8,at = c(0,1e5,5e5,1e6),lwd=0.5,at.small = c(2e5,3e5,4e5,6:9*1e5))\ndev.off()\n\n\nbothGTOTU <- merge(data.frame(bacOTU,bacTax,stringsAsFactors = F)[bacTax[,7] %in% \n                                  names(table(bacTax[bacTax[,7] %in% \n                                                       intersect(bacTax[,7],\n                                                                 names(table(bacGT$Genus))[table(bacGT$Genus)==1]),\n                                                     7]))[table(bacTax[bacTax[,7] %in% \n                                                                         intersect(bacTax[,7],\n                                                                                   names(table(bacGT$Genus))[table(bacGT$Genus)==1]),\n                                                                       7])==1],\n                                c(1:14,21)],\n                   bacGT,by.x=15,by.y=""Genus"")\nplot(bothGTOTU$Prop,bothGTOTU$X1600000)\ncor.test(bothGTOTU$Prop,bothGTOTU$X1600000)\n# no correlation whatsoever\nplot(sort(unlist(bacdf$reads_chimera_checked)),\n  sapply(2:15,function(x) cor.test(bothGTOTU$Prop,bothGTOTU[,x])$estimate),\n  log=""x"",las=1,xlab=""processed reads"",ylab=""Pearson\'s r"")\nplot(sort(unlist(bacdf$reads_chimera_checked)),\n     sapply(2:15,function(x) cor.test(bothGTOTU$Prop,bothGTOTU[,x],method=""spearman"")$estimate),\n     log=""x"",las=1,xlab=""processed reads"",ylab=""Spearman\'s rho"")    \n\n\npdf(""bacteria_props.pdf"",width=5.5/2.54,height=5/2.54,pointsize=8)\npar(mar=c(2.8,3,0.5,0.5),mgp=c(1.6,0.5,0),tcl=-0.3)\nplot(bothGTOTU$Prop,100*bothGTOTU$X1600000/sum(bothGTOTU$X1600000),\n     las=1,xlab=""% of mock community"",ylab=""% reads"",\n     #xaxt=""n"",\n     cex.axis=6/8,\n     pch=16)\n#eaxis(1,cex.axis=6/8,use.expr = T,at=c(0,5e3,1e4,2e4,3e4))\ndev.off()\n']","Supplementary Datasets for dadasnake workflow This dataset contains configuration and results files for the proof-of-principle of the dadasnake pipeline. Includes dadasnake output and tables with the composition of ground-truth data or mock-communities.dadasnake is a user-friendly, one-command Snakemake pipeline that wraps the pre-processing of sequencing reads and the delineation of exact sequence variants by using the favorably benchmarked and widely-used DADA2 algorithm with a taxonomic classification and the post-processing of the resultant tables, including hand-off in standard formats. The suitability of the provided default configurations is demonstrated using mock-community data from bacteria and archaea, as well as fungi. By use of Snakemake, dadasnake makes efficient use of high-performance computing infrastructures. Easy user configuration guarantees flexibility of all steps, including the processing of data from multiple sequencing platforms. dadasnake facilitates easy installation via conda environments. dadasnake is available at https://github.com/a-h-b/dadasnake .",0
Epigenetic Timer of Cancer-2,"epiTOC2.R is an R-script that aims to estimate the mitotic age of a sample profiled genome-wide at the DNA methylation level. It is based on a mitotic clock model where DNA methylation errors occuring during cell-division accrue in the adult stem cell pool of a tissue, allowing tracking of the number of stem-cell divisions. epiTOC2 not only estimates the cumulative number of stem-cell divisions, but also allows the intrinsic rate of stem-cell division per stem-cell per year to be estimated if the chronological age of the sample(s) is known. To run epiTOC2.R, see details within the .R file. Running epiTOC2 requires uploading of a data object, dataETOC2.Rd, as shown in the .R script. This data object contains information regarding the specific CpGs that make up epiTOC2.","['#### epiTOC2.R\n\n#### Author: Andrew E Teschendorff (a.teschendorff@ucl.ac.uk)\n#### Date: 8th Apr.2019\n#### Copyright 2019 Andrew Teschendorff\n#### Copyright permission: epiTOC2 is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License version-3 as published by the Free Software  Foundation. epiTOC2 is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details (http://ww.gnu.org/licenses/).\n\n\n#### DESCRIPTION\n#### An R-function to estimate the cumulative number of stem-cell divisions in a sample using the epiTOC2 model. Only required input argument is a DNA methylation data matrix, normalized with BMIQ or another type-2 probe correction method. If the ages of the samples are provided and assuming all samples are from the same tissue-type, function also returns the estimated intrinsic rate of stem-cell division for each sample as well as a median estimate for the tissue. The function returns mitotic age estimates for two different epiTOC2 models, which differ in the ground-state methylation values for the epiTOC2 CpGs. In the full model, we use the estimated methylation ground state values, whereas in the simplified model we assume that these are all zero. Reason for returning the values estimated using the simplified model is because it could happen that some methylation beta-values in the data matrix are lower than the ground-state values, which in principle is not allowed. If this is the case, then more reliable estimates are provided by using the simplified model. The functions alerts the user to this, if it detects lots of beta-values less than ground-state values. Finally, the function also returns the pcgtAge-score and HypoClock score for each column (sample) in the input data matrix using the previous epiTOC model and solo-WCGWs, respectively.\n\n#### REQUIRED OBJECTS:\n#### dataETOC2.Rd: this object file will be loaded and must reside in the working directory. It is a list with the 3 elements. The first element is a matrix with rows labeling the 163 epiTOC2 CpGs, and columns labeling the estimated de-novo and ground-state methylation parameters. The 2nd element is a vector of CpG identifiers for the 385 epiTOC CpGs. The 3rd element is a vector of CpG identifiers for the 678 solo-WCGWs which are constitutively methylated in fetal tissue. \n\n#### INPUT:\n#### data.m: DNAm data beta-valued matrix with rownames labeling CpGs and columns labeling samples. All samples should be from the same tissue-type. \n#### ages.v: Optional argument representing the chronological ages or surrogates thereof of the samples. Vector must be of same length as the number of samples.\n\n\n\n#### OUPTUT:\n#### A list containing the following entries\n#### tnsc: the estimated cumulative number of stem-cell divisions per stem-cell per year and per sample using the full epiTOC2 model.\n#### tnsc2: the estimated cumulative number of stem-cell divisions per stem-cell per year and per sample using an approximation of epiTOC2 which assumes all epiTOC2 CpGs have beta-values exactly 0 in the fetal stage.\n#### irS: this is returned only if the ages are provided, and gives the estimated average lifetime intrinsic rate of stem-cell division per sample, as derived from epiTOC2\n#### irS2: as irS, but for the approximation.\n#### irT: the median estimate over all irS values, yielding a median estimate for the intrinsic rate of stem-cell division for the tissue.\n#### irT2: as irT, but for the approximation.\n#### pcgtAge: this is the mitotic-score obtained using our previous epiTOC model.\n#### hypoSC: the HypoClock score over the 678 solo-WCGWs\n\nepiTOC2 <- function(data.m,ages.v=NULL){\n    load(""dataETOC2.Rd""); ## this loads the CpG information\n    cpgETOC.v <- dataETOC2.l[[2]];\n    estETOC2.m <- dataETOC2.l[[1]];\n    soloCpG.v <- dataETOC2.l[[3]];\n    ### do epiTOC\n    common.v <- intersect(rownames(data.m),cpgETOC.v);\n    print(paste(""Number of represented epiTOC CpGs (max=385)="",length(common.v),sep=""""));\n    map.idx <- match(common.v,rownames(data.m));\n    pcgtAge.v <- colMeans(data.m[map.idx,],na.rm=TRUE);\n    ### do epiTOC2\n    map.idx <- match(rownames(estETOC2.m),rownames(data.m));\n    rep.idx <- which(is.na(map.idx)==FALSE);\n    print(paste(""Number of represented epiTOC2 CpGs (max=163)="",length(rep.idx),sep=""""))\n    tmp.m <- data.m[map.idx[rep.idx],];\n    TNSC.v <- 2*colMeans(diag(1/(estETOC2.m[rep.idx,1]*(1-estETOC2.m[rep.idx,2]))) %*% (tmp.m - estETOC2.m[rep.idx,2]),na.rm=TRUE);\n    TNSC2.v <- 2*colMeans(diag(1/estETOC2.m[rep.idx,1]) %*% tmp.m,na.rm=TRUE);\n    ### do HypoClock\n    common.v <- intersect(rownames(data.m),soloCpG.v);\n    print(paste(""Number of represented solo-WCGWs (max=678)="",length(common.v),sep=""""));\n    map.idx <- match(common.v,rownames(data.m));\n    hypoSC.v <- colMeans(data.m[map.idx,],na.rm=TRUE);\n\n    estIR.v <- NULL; estIR2.v <- NULL;\n    estIR <- NULL;  estIR2 <- NULL;\n    if(!is.null(ages.v)){\n      estIR.v <- TNSC.v/ages.v;\n      estIR <- median(estIR.v,na.rm=TRUE);\n      estIR2.v <- TNSC2.v/ages.v;\n      estIR2 <- median(estIR2.v,na.rm=TRUE);\n    }\n\n    \n    return(list(tnsc=TNSC.v,tnsc2=TNSC2.v,irS=estIR.v,irS2=estIR2.v,irT=estIR,irT2=estIR2,pcgtAge=pcgtAge.v,hypoSC=hypoSC.v));\n}\n\n']","Epigenetic Timer of Cancer-2 epiTOC2.R is an R-script that aims to estimate the mitotic age of a sample profiled genome-wide at the DNA methylation level. It is based on a mitotic clock model where DNA methylation errors occuring during cell-division accrue in the adult stem cell pool of a tissue, allowing tracking of the number of stem-cell divisions. epiTOC2 not only estimates the cumulative number of stem-cell divisions, but also allows the intrinsic rate of stem-cell division per stem-cell per year to be estimated if the chronological age of the sample(s) is known. To run epiTOC2.R, see details within the .R file. Running epiTOC2 requires uploading of a data object, dataETOC2.Rd, as shown in the .R script. This data object contains information regarding the specific CpGs that make up epiTOC2.",0
"Data from: eDNA metabarcoding of log hollow sediments and soils highlights the importance of substrate type, frequency of sampling and animal size, for vertebrate species detection","Fauna monitoring often relies on visual monitoring techniques such as camera trappings, which have biases leading to underestimates of vertebrate species diversity. Environmental DNA (eDNA) has emerged as a new source of biodiversity data that may improve biomonitoring; however, eDNA based assessments of species richness remain relatively untested in terrestrial environments. We investigated the suitability of fallen log hollow sediment as a source of vertebrate eDNA, across two sites in south-western Australia - one with a Mediterranean climate and the other semi-arid. We compared two different approaches (camera trapping and eDNA metabarcoding) for monitoring of vertebrate species, and investigated the effect of other factors (frequency of species, timing of visits, frequency of sampling, body size) on vertebrate species detectability. Metabarcoding of hollow sediments resulted in the detection of higher species richness in comparison Hollow sediment detected higher species richness (29 taxa: six birds, three reptiles and 20 mammals) to metabarcoding of soil at the entrance of the hollow (13 taxa: three birds, two reptiles and eight mammals). We detected 31 taxa in total with eDNA metabarcoding and 47 with camera traps, with 14 taxa detected by both (12 mammals and two birds). By comparing camera trap data with eDNA read abundance, we were able to detect vertebrates through eDNA metabarcoding that had visited the area up to two months prior to sample collection. Larger animals were more likely to be detected, and so were vertebrates that were identified multiple times in the camera traps. These findings demonstrate the importance of substrate selection, frequency of sampling, and animal size, on eDNA based monitoring. Future eDNA experimental design should consider all these factors as they affect detection of target taxa.","['BiocManager::install(""phyloseq"")\r\nBiocManager::install(""IRanges"")\r\n\r\nlibrary(phyloseq)\r\nlibrary(ggplot2)\r\nlibrary(vegan)\r\n\r\notu_mat<-read.csv(""ethan_otu_table2.csv"", row.names = 1)\r\n#stay the same\r\ntax<-read.csv(""ethanhollow_edna_species7.csv"", row.names = 1)\r\ntax\r\ntax_mat<-as.matrix(tax)\r\n#done\r\nsample_df<-read.csv(""ethanhollow_samplelist.csv"", row.names = 1)\r\n#done\r\n\r\n\r\n##Make phyloseq object\r\notu<-otu_table(otu_mat, taxa_are_rows = T)\r\ntax<-tax_table(tax_mat)\r\nsamples<-sample_data(sample_df)\r\n\r\nholl<-phyloseq(otu, tax, samples)\r\nholl\r\nsample_data(holl)\r\ntax_table(holl)\r\n\r\n#Check ASVs found in extraction control\r\nextracts<-subset_samples(holl, Type ==""Extract"")\r\ntaxa_sums(extracts)<2\r\nkeeptaxa<-taxa_sums(extracts)<2\r\nholl2<-prune_taxa(keeptaxa, holl)\r\nholl2<-subset_samples(holl2, Type != ""Extract"")\r\nholl2\r\n#These ASVs are human and will be removed in the next step anyway\r\n\r\n#remove low abundance samples\r\n#Check rarefaction curve\r\n#rarecurve(t(otu_table(holl)), step=50, cex=0.5)\r\n#rarecurve(t(otu_table(holl2)), step=50, cex=0.5, xlim=c(0, 10000))\r\nsums<-as.data.frame(sample_sums(holl))\r\nholl2<-subset_samples(holl, sample_sums(holl)>642)\r\n\r\n#rarecurve(t(otu_table(holl2)), step=50, cex=0.5, xlim=c(0, 10000))\r\n#rarecurve(t(otu_table(holl2)), step=50, cex=0.5)\r\n\r\n\r\n#Filtering by Relative abundance within samples, taxa making up less than 0.5% of the sample have counts reverted to 0\r\n#You can change the level of filtering here, or just remove singletons later if you think this is too strict\r\nholl2.df<-psotu2veg(holl2)\r\n\r\nholl2.df<-t(holl2.df)\r\nholl2.df=data.frame(holl2.df)\r\ncolnames(holl2.df)\r\nholl2.df2=holl2.df\r\nfor (i in 1:ncol(holl2.df)){\r\n  \r\n  holl2.df[,i]\r\n  temp=holl2.df[,i]\r\n  which(temp<sum(holl2.df[,i])*0.005)\r\n  temp[which(temp<sum(holl2.df[,i])*0.005)] =0\r\n  holl2.df[,i]=temp\r\n  \r\n}\r\n\r\n\r\n##Make phyloseq object again\r\notu2<-otu_table(holl2.df, taxa_are_rows = T)\r\n\r\nholl2.fil<-phyloseq(otu2, tax_table(holl2), sample_data(holl2))\r\nsums2<-as.data.frame(sample_sums(holl2.fil))\r\nholl2\r\ntax_table(holl2.fil)\r\n\r\n#Select only Chordata ASVs\r\nholl3<-subset_taxa(holl2.fil, phylum==""Chordata"")\r\nholl3\r\n\r\n#Remove contamination ASVs\r\nholl3<-subset_taxa(holl3, Contamination==""N"")\r\nholl3\r\nsums3<-as.data.frame(sample_sums(holl3))\r\n#Also remove A LOT of sequences\r\nsum(sums3)#5,616,468\r\nsum(sums)#15,849,075\r\n\r\n\r\n\r\n#Remove the now extract samples\r\nholl4<-subset_samples(holl3, Type != ""Extract"")\r\n\r\n\r\n################################################################\r\n# convert the sample_data() within a phyloseq object to a vegan compatible data object\r\npssd2veg <- function(physeq) {\r\n  sd <- sample_data(physeq)\r\n  return(as(sd,""data.frame""))\r\n}\r\n\r\n# convert the otu_table() within a phyloseq object to a vegan compatible data object\r\npsotu2veg <- function(physeq) {\r\n  OTU <- otu_table(physeq)\r\n  if (taxa_are_rows(OTU)) {\r\n    OTU <- t(OTU)\r\n  }\r\n  return(as(OTU, ""matrix""))\r\n}\r\n####################################################################\r\n\r\n#########Aggregate taxa\r\nholl5<-tax_glom(holl4, taxrank=""final_name"")\r\nholl5\r\ntax_table(holl5)\r\ntaxa_names(holl5)<-tax_table(holl5)[,9]\r\ntaxa_names(holl5)\r\n?taxa_names\r\n#attempting to duplicate multiple taxa after transposing holl2\r\n#At this point holl4 is the filtered frame with ASVs, and holl5 is with the taxa aggregated\r\n\r\n###########################################\r\n#Transformations\r\nlibrary(microbiome)\r\nholl5.log<-transform(holl5, ""log"")\r\nholl5.pa<-transform(holl5, ""pa"")\r\nholl4.log<-transform(holl4, ""log"")\r\nholl4.pa<-transform(holl4, ""pa"")\r\n###########################################\r\n\r\n#Load camera data\r\ncam.otu<-read.csv(""ethan_camera_table2.csv"", row.names=1)\r\ncam.otu #picumnus present\r\ncam.env<-read.csv(""ethan_camera_map.csv"", row.names=1)\r\ncam.env\r\ncam.tax<-read.csv(""cam_tax_filled3.csv"", row.names = 1)\r\ncam.tax\r\ncam.tax_mat<-as.matrix(cam.tax)\r\n\r\n##Make phyloseq object\r\ncam.otu<-otu_table(cam.otu, taxa_are_rows = T)\r\ncam.tax<-tax_table(cam.tax_mat)\r\ncam.samples<-sample_data(cam.env)\r\n\r\ncam<-phyloseq(cam.otu, cam.tax, cam.samples)\r\ncam\r\ntax_table(cam)\r\ncam.pa<-transform(cam, ""pa"")\r\ncam.tax\r\ncam.otu\r\ncam.samples\r\n']","Data from: eDNA metabarcoding of log hollow sediments and soils highlights the importance of substrate type, frequency of sampling and animal size, for vertebrate species detection Fauna monitoring often relies on visual monitoring techniques such as camera trappings, which have biases leading to underestimates of vertebrate species diversity. Environmental DNA (eDNA) has emerged as a new source of biodiversity data that may improve biomonitoring; however, eDNA based assessments of species richness remain relatively untested in terrestrial environments. We investigated the suitability of fallen log hollow sediment as a source of vertebrate eDNA, across two sites in south-western Australia - one with a Mediterranean climate and the other semi-arid. We compared two different approaches (camera trapping and eDNA metabarcoding) for monitoring of vertebrate species, and investigated the effect of other factors (frequency of species, timing of visits, frequency of sampling, body size) on vertebrate species detectability. Metabarcoding of hollow sediments resulted in the detection of higher species richness in comparison Hollow sediment detected higher species richness (29 taxa: six birds, three reptiles and 20 mammals) to metabarcoding of soil at the entrance of the hollow (13 taxa: three birds, two reptiles and eight mammals). We detected 31 taxa in total with eDNA metabarcoding and 47 with camera traps, with 14 taxa detected by both (12 mammals and two birds). By comparing camera trap data with eDNA read abundance, we were able to detect vertebrates through eDNA metabarcoding that had visited the area up to two months prior to sample collection. Larger animals were more likely to be detected, and so were vertebrates that were identified multiple times in the camera traps. These findings demonstrate the importance of substrate selection, frequency of sampling, and animal size, on eDNA based monitoring. Future eDNA experimental design should consider all these factors as they affect detection of target taxa.",0
Disease-associated Oligodendrocyte Responses Across Neurodegenerative Diseases,"Oligodendrocyte dysfunction has been implicated in the pathogenesis of neurodegenerative diseases, so understanding their activation states would shed new light on disease processes. We identify three distinct activation states of oligodendrocytes from single-cell RNA-seq of mouse models of Alzheimers Disease (AD) and Multiple Sclerosis (MS): DA1 (disease-associated1, associated with immunogenic genes), DA2 (disease-associated2, associated with genes influencing survival) and IFN (associated with interferon response genes). Spatial analysis of DAOs in the cuprizone model show that DA1 and DA2 establish outside of the lesion area during demyelination and DA1 repopulate the lesion during remyelination. Independent meta-analysis of human single-nuclei RNA-seq datasets reveal that the transcriptional responses of MS oligodendrocytes share features with mouse models. In contrast, the oligodendrocyte activation signatures observed in human AD is largely distinct from those observed in mice. This catalog (http://research-pub.gene.com/OligoLandscape/) of oligodendrocyte activation states will be important for understanding disease progression and developing novel therapeutic interventions.","['library(Seurat)\nsource(\'~/CustomFunctions.R\')\n\n\n\nrun_core_integration <- function (out_dir = NULL){\n  dir <- \'/gstore/data/omni/neuroscience/Oligodendrocytes/seurat_objects/mouse/\' #replace with local working dir\n  load(paste0(dir, \'ngs2330_oligos.RObj\')) #download from GEO \n  load(paste0(dir, \'ngs3111_oligos.RObj\')) #download from GEO \n  load(paste0(dir, \'ngs2722_oligos.RObj\')) #download from GEO \n  load(paste0(dir, \'ps2app_oligos.RObj\')) #download from GEO \n  load(paste0(dir, \'ngs3033_oligos.RObj\')) #download from GEO \n  \n  \n  #ngs2330 and ngs3111 and ps2app have significant batch effects so, we will be integrating each batch as a separate study\n  ngs2330_oligos_list <- SplitObject(ngs2330_oligos, split.by = \'batch\')\n  ngs3111_oligos_list <- SplitObject(ngs3111_oligos, split.by = \'batch\')\n  ps2app_oligos_list <- SplitObject(ps2app_oligos, split.by = \'batch\')\n  print(\'Loaded all datasets and split by batch\')\n  \n  all.oligo.list <- c(ngs2722_oligos, ngs3033_oligos, ngs3111_oligos_list, ngs2330_oligos_list, ps2app_oligos_list)\n  \n  for (i in 1:length(all.oligo.list)) {\n    all.oligo.list[[i]] <- NormalizeData(all.oligo.list[[i]], verbose = FALSE)\n    all.oligo.list[[i]] <- FindVariableFeatures(all.oligo.list[[i]], selection.method = ""vst"", nfeatures = 2500, verbose = FALSE)\n  }\n  \n  print(\'Finished normalizing and finding variable features\')\n  reference.list <- all.oligo.list\n  oligo.anchors <- FindIntegrationAnchors(object.list = reference.list, dims = 1:30)\n  save(oligo.anchors, file = paste0(out_dir, \'all_oligo_anchors5_30.RData\'))\n  \n  oligos_all <- IntegrateData(anchorset = oligo.anchors, dims = 1:30)\n  save(oligos_all, file = paste0(out_dir, \'mouse/oligos_all.RObj\'))\n  \n  library(ggplot2)\n  library(cowplot)\n  library(patchwork)\n  # switch to integrated assay. The variable features of this assay are automatically\n  # set during IntegrateData\n  DefaultAssay(oligos_all) <- ""integrated""\n  \n  plotQC(oligos_all)\n  \n  # Run the standard workflow for visualization and clustering\n  oligos_all <- ScaleData(oligos_all, verbose = FALSE)\n  oligos_all <- RunPCA(oligos_all, npcs = 20, verbose = FALSE)\n  oligos_all <- RunTSNE(oligos_all, npcs = 20, verbose = FALSE)\n  oligos_all <- RunUMAP(oligos_all, reduction = ""pca"", dims = 1:20)\n  oligos_all <- FindNeighbors(oligos_all,reduction = \'pca\', dims = 1:20) \n  oligos_all <- FindClusters(oligos_all, resolution = 0.5)\n  oligos_all <- FindClusters(oligos_all, resolution = 1.0)\n  oligos_all <- FindClusters(oligos_all, resolution = 1.25)\n  save(oligos_all, file = paste0(out_dir, \'oligos_5all_30.RObj\'))\n  DefaultAssay(oligos_all) <- \'RNA\'\n  oligo.integrated.markers <- FindAllMarkers(oligos_all, logfc.threshold = 0.7, only.pos = T)\n  \n  write.csv(oligo.integrated.markers, file = paste0(out_dir, \'interim markers.csv\')) #for prelim cluster interpretations\n  save(oligos_all, file = paste0(out_dir, \'oligos_all.RObj\'))\n  return(oligos_all)\n}\n\noligos_all <- run_core_integration(out_dir = \'/gstore/data/omni/neuroscience/oligodendrocytes/website/\') \n\ninterpret_core_integration <- function(){\n  source(\'~/CustomFunctions.R\')\n  load(\'/gstore/data/omni/neuroscience/oligodendrocytes/website/oligos_final_pub.RObj\') #load from local dir\n  load(\'~/FinalMouseOligoGSL.RData\')\n  DefaultAssay(oligos_all) <- \'RNA\'\n  for (i in 1: length(baselineOL_markers)){\n    oligos_all <- calculate_gene_set_score(oligos_all, \n                                           features = mouse_oligo_subtypes[[i]], \n                                           module.name = names(mouse_oligo_subtypes[i]))\n  }\n  \n  order_plot = c(""Non-Tg_young"", ""Non-Tg_old"", ""PS2APP_young"", ""PS2APP_old"",\n                 ""NonTg/TREM2KO"", ""P301L"", ""PS2APP/P301L"",  ""PS2APP/P301L/TREM2KO"",\n                 ""TauP301S Negative"", ""TauP301S Positive"", \n                 ""Baseline"",  ""Cuprizone_4w"", ""Cuprizone_4+3w"",\n                 ""NL"",""5dpl"",""14dpl"", ""28dpl"")\n  \n  oligos_all <- SetIdent(oligos_all, value = \'integrated_snn_res.1\')\n  oligos_all <- calculate_cluster_composition(seurat.obj = oligos_all, \n                                              treatment = \'treatment\', \n                                              cluster_idents = \'integrated_snn_res.1\', \n                                              samples = \'sampleID\', \n                                              order = order_plot, \n                                              order_facets = 1:23, \n                                              dataset = \'dataset\')\n  calculate_da_statistics(oligos_all)\n  #nominate broad clusters based on gene sets: \n  oligo_markers <- FindAllMarkers(oligos_all, logfc.threshold = 0.7, assay = \'RNA\', only.pos = T)\n  oligos_all <- SetIdent(oligos_all, value = \'integrated_snn_res.1.5\')\n  oligos_all <- BuildClusterTree(oligos_all)\n  \n  oligos_all <- RenameIdents(oligos_all, \n                                    \'0\' = \'MOL5/6\',\n                                   \'1\' = \'OPC\',\n                                   \'2\' = \'MOL5/6\', \n                                   \'3\' = \'MOL5/6\',\n                                   \'4\' = \'MOL5/6\', \n                                   \'5\' = \'MOL5/6\',\n                                   \'6\' = \'MOL5/6\',\n                                   \'7\' = \'MOL2\',\n                                   \'8\' = \'MOL5/6\',\n                                   \'9\' = \'MOL5/6\',\n                                   \'10\' = \'MOL5/6\',\n                                   \'11\' = \'MOL5/6\',\n                                   \'12\' = \'MFOL\',\n                                   \'13\' = \'MOL2\',\n                                   \'14\' = \'OPC\',\n                                   \'15\' = \'MOL5/6\',\n                                   \'16\' = \'MOL1\',\n                                   \'17\' = \'OPC\',\n                                   \'18\' = \'MFOL\',\n                                   \'19\' = \'OPC\',\n                                   \'20\' = \'MOL5/6\', \n                                   \'21\' = \'COP\',\n                                   \'22\' = \'MOL1\',\n                                   \'23\' = \'NFOL\',\n                                   \'24\' = \'MFOL\',\n                                   \'25\' = \'OPC\')\n  oligos_all$broad_cluster <- Idents(oligos_all)\n  \n  #split NFOL into Subclusters \n  oligos_all <- SetIdent(oligos_all, value = \'broad_cluster\')\n  oligos_all <- FindSubCluster(oligos_all, cluster = \'NFOL\', resolution = 0.05, subcluster.name = \'broad2\', graph.name = \'integrated_nn\')\n  oligos_all <- SetIdent(oligos_all, value = \'broad2\')\n  oligos_all <- RenameIdents(oligos_all, \n                             \'NFOL_0\' = \'NFOL1\', \n                             \'NFOL_1\' = \'NFOL2\')\n  oligos_all$broad_cluster <- Idents(oligos_all)\n\n  \n  #define disease associated OL clusters\n  oligos_all <- SetIdent(oligos_all, value = \'integrated_snn_res.1\')\n  oligos_all <- RenameIdents(oligos_all, \n                             \'0\' = \'MOL5/6\',\n                             \'1\' = \'OPC\',\n                             \'2\' = \'MOL5/6\', \n                             \'3\' = \'MOL5/6\',\n                             \'4\' = \'tMOL5/6\', \n                             \'5\' = \'MOL5/6_DA2\',\n                             \'6\' = \'MOL5/6_DA1\',\n                             \'7\' = \'MOL2\',\n                             \'8\' = \'MOL5/6_DA1\',\n                             \'9\' = \'MOL5/6_DA1\',\n                             \'10\' = \'MOL5/6\',\n                             \'11\' = \'MOL5/6\',\n                             \'12\' = \'MFOL\',\n                             \'13\' = \'MOL2_DA1\',\n                             \'14\' = \'OPC\',\n                             \'15\' = \'MOL_IFN\',\n                             \'16\' = \'MOL1\',\n                             \'17\' = \'DA2-like OPC\',\n                             \'18\' = \'MFOL\',\n                             \'19\' = \'cycOPC\',\n                             \'20\' = \'MOL1\',\n                             \'22\' = \'MOL1\',\n                             \'23\' = \'OPC\',\n                             \'24\' = \'MFOL\',\n                             \'25\' = \'OPC\')\n  oligos_all$finalInterps <- Idents(oligos_all)\n  \n  #split IFN into sub-clsusters MOl2_IFN and MOL5/6_IFN\n  oligos_all <- SetIdent(oligos_all, value = \'finalInterps\')\n  oligos_all <- FindSubCluster(oligos_all, cluster = \'IFN\', resolution = 0.025, graph.name = \'integrated_nn\', subcluster.name = \'finalInterp2\')\n  oligos_all <- SetIdent(oligos_all, value = \'finalInterp2\')\n  oligos_all <- RenameIdents(oligos_all, \n                             \'IFN_0\' = \'MOL5/6_IFN\', \n                             \'IFN_1\' = \'MOL5/6_IFN\', \n                             \'IFN_2\' = \'MOL2_IFN\')\n  oligos_all$broad_cluster <- Idents(oligos_all)\n  save(oligos_all, file = \'~/oligos_all.RObj\')\n  \n}\n\n\ndir <- \'/gstore/data/omni/neuroscience/oligodendrocytes/seurat_objects/\' # where the object is saved post prelim integration\nload(paste0(dir, \'oligos_all.RObj\'))\ninterpret_core_integration()\n\n\n#set up pseudobulks\noligos_all <- SetIdent(oligos_all, value = \'finalInterps\')\noligo_psb<- setup_se_psb(seurat.obj=oligos_all, treatment_field=\'treatment\', celltype_field=\'finalInterps\', sample_field = \'sampleID\', study_field = \'dataset\')\nsaveHDF5SummarizedExperiment(oligo_psb, dir = out_dir, replace = T)\n', '\n############################\n## Custom Functions\n## Shristi Pandey\n###########################\n\n##\' Given a seurat object containing just the counts mat, runs a standard seurat pipeline without data integration\n##\'\n##\' @param seurat.obj Seurat object containing the count matrix \n##\' @param filter whether or not to filter UMI, gene \n##\' @param vars.to.regress which variable to regress out, for instance batch, percent MT\n##\' @param umi.low.threshold Minimum UMI per cell\n##\' @param umi.high.threshold Max UMI per cell\n##\' @param gene.high.threshold Min gene per cell\n##\' @param gene.low.threshold Max gene per cell \n##\' @param gene.low.threshold Max gene per cell \n##\' @param npcs Number of PCs to use for downstream TSNE, UMAP and Clustering. Check elbow plot for decision\n##\' @return seurat object containing PCA, UMAP, TSNE and clsutering results\n##\' @author Shristi Pandey\n##\'\n\n\nrun_seurat <- function(seurat.obj = NULL, filter = TRUE, vars.to.regress = NULL, umi.low.threshold = NULL, umi.high.threshold = NULL, gene.high.threshold = NULL, gene.low.threshold = NULL, npcs = NULL){\n  if (filter){\n    seurat.obj <- subset(seurat.obj, subset = nCount_RNA > umi.low.threshold & nCount_RNA < umi.high.threshold)\n    seurat.obj <- subset(seurat.obj, subset = nFeature_RNA > gene.low.threshold &nFeature_RNA < gene.high.threshold)\n  }\n  seurat.obj <- NormalizeData(seurat.obj, normalization.method = ""LogNormalize"", scale.factor = 10000)\n  seurat.obj <- FindVariableFeatures(seurat.obj, selection.method = ""vst"", nfeatures = 2000)\n  all.genes <- rownames(seurat.obj)\n  seurat.obj <- ScaleData(seurat.obj, features = all.genes, vars.to.regress = vars.to.regress)\n  seurat.obj <- RunPCA(seurat.obj, features = VariableFeatures(object = seurat.obj))\n  print(ElbowPlot(seurat.obj))\n  seurat.obj <- RunTSNE(seurat.obj, dims = 1:npcs)\n  seurat.obj <- FindNeighbors(seurat.obj, dims = 1:npcs)\n  seurat.obj <- FindClusters(seurat.obj, resolution = 0.5)\n  seurat.obj <- FindClusters(seurat.obj, resolution = 1)\n  seurat.obj <- FindClusters(seurat.obj, resolution = 2)\n  seurat.obj <- RunUMAP(seurat.obj, dims= 1:npcs)\n  return(seurat.obj)\n}\n\n\n##\' Given a set of metadata parameters, computes all the plots of these metadata\n##\'\n##\' @param seurat.obj Seurat Object\n##\' @param features_list list of meta data slots\n##\' @author Shristi Pandey\n##\' @return Seurat Object with the gene set calculated and stored in a metadata with the module.name\n##\'\n\nplotQC <- function(seurat.obj = NULL, discrete_meta = NULL, continous_meta = NULL){\n  library(ggpubr)\n  library(ggplot2)\n  library(gridExtra)\n  clusters <- DimPlot(seurat.obj, label = T) + NoAxes()\n  a <- FeaturePlot(seurat.obj, \'nCount_RNA\' ) + NoAxes()\n  b <- FeaturePlot(seurat.obj, \'nFeature_RNA\') + NoAxes()\n  c <- FeaturePlot(seurat.obj, \'percent.mt\') + NoAxes()\n  plots <- list(clusters, a, b, c)\n  if (!is.null(continous_meta)){\n    for (meta in continous_meta){\n      plot <- FeaturePlot(seurat.obj, meta) + NoAxes()\n      plots <- c(plots, list(plot))\n    }\n  }\n  for (elem in discrete_meta){\n    seurat.obj <- SetIdent(seurat.obj, value = elem)\n    plot <- DimPlot(seurat.obj) + NoAxes() + ggtitle(elem)\n    plots <- c(plots, list(plot))\n  }\n  #print(\'Plotting\', length(plots), \'meta data slots\')\n  ggarrange(plotlist = plots)\n}\n\n##\' Given a set of genes, computes the average gene set score and adds to metadata in a seurat object\n##\'\n##\' @param seurat.obj Seurat Object\n##\' @param features Vector of features for which to calculate gene set score\n##\' @param module.name String representing the metadata column name to attach to the Seurat Object\n##\' @author Shristi Pandey\n##\' @return Seurat Object with the gene set calculated and stored in a metadata with the module.name\n##\'\ncalculate_gene_set_score <- function(seurat.obj, features = NULL, module.name = NULL){\n  if (is.null(features)){\n    stop(\'No features provided\')\n  }\n  \n  if (is.null(module.name)){\n    stop(\'Please provide module name used to store the gene set score as metadata\')\n  }\n  log2_norm_umi <- GetAssayData(seurat.obj)\n  log2_norm_umi_selected <- log2_norm_umi[rownames(log2_norm_umi) %in% features, ]\n  avg_module_score <- colMeans(as.array(log2_norm_umi_selected))\n  seurat.obj <- AddMetaData(seurat.obj, col.name = module.name, metadata = avg_module_score)\n  return (seurat.obj)\n  \n}\n\n\n\n\n\n##\' Create a pseudoBulk from a single-cell expression matrix\n##\'\n##\' @param n Matrix of expression values, genes x cells\n##\' @param clusters Vector or factor along the cells giving cluster memberships.\n##\' Must have length(clusters) == ncol(n). If a factor then unused levels will be\n##\' dropped; if not a factor then will be converted to one.\n##\' @param rowAggregator Default rowSum. Or some other function that accepts a matrix\n##\' and returns a numeric vector along the rows giving some ""bulk summary"" of the expression value.\n##\' @return Matrix of features x clusters\n##\' @author Adapted from Brad Friedman\n##\' @export\n##\'\npseudoBulk <- function(seurat.obj, clusters, rowAggregator = rowSums)  {\n  DefaultAssay(seurat.obj) <- \'RNA\'\n  n <- as.matrix(GetAssayData(seurat.obj, slot = \'counts\'))\n  stopifnot(length(clusters) == ncol(n))\n  \n  ## drop unused levels if it IS a factor, otherwise make it a factor\n  clusters <- factor(clusters)\n  \n  names(clusterLevels) <- clusterLevels <- levels(clusters)\n  \n  psb <- sapply(clusterLevels, function(cl)  {\n    #.richMessage(""... collapsing "", cl, caller = ""pseudoBulk"")\n    i <- clusters == cl\n    vec <- rowAggregator(n[,i, drop = FALSE])\n    stopifnot(names(vec) == rownames(n))  ## double check we are preserving rownames\n    vec\n  })\n  \n  return(psb)\n}\n\n##\' Given a seurat object, returns a set up summarized object to run pseudobulk with voom\n##\'\n##\' @param seurat.obj Seurat Object\n##\' @param treatment string representing the genotype/timepoint or other treatment type\n##\' @param samples String representing the sample metadata\n##\' @param clusters String representing the cluster field in the seurat object\n##\' @param study_field String representing the study field if the object represents an integration of multiple studies\n##\' @author Shristi Pandey\n##\' @return Summarized experiment object with appropriate\n##\'\n\nsetup_se_psb <- function(seurat.obj, treatment_field=NULL, sample_field = \'sampleID\', celltype_field = NULL, study_field = \'orig.ident\'){\n  library(dplyr)\n  DefaultAssay(seurat.obj) <- \'RNA\'  #important when you are performing experiment with integrated object\n  treatment = unlist(seurat.obj[[treatment_field]])\n  samples = unlist(seurat.obj[[sample_field]])\n  if (!is.null(celltype_field)){\n    celltype = unlist(seurat.obj[[celltype_field]])\n  }\n  if (is.null(celltype_field)){\n    pseudobulkID <- paste0(treatment,\'_\',samples)\n  } else {\n    pseudobulkID <- paste0(treatment,\'_\',samples, \'_\', celltype)\n  }\n  \n  seurat.obj$pseudobulkID <- pseudobulkID\n  psb_clusters <- factor(seurat.obj$pseudobulkID)\n  psb <- pseudoBulk(seurat.obj, psb_clusters, rowAggregator = rowSums)\n  \n  if (is.null(celltype_field)) {\n    col_data <- data.frame(seurat.obj$pseudobulkID, seurat.obj[[treatment_field]], seurat.obj[[sample_field]], seurat.obj[[study_field]])\n    colnames(col_data) <- c(\'pseudobulkID\', \'treatment\', \'batch\', \'study\')\n  } else{\n    col_data <- data.frame(seurat.obj$pseudobulkID, seurat.obj[[treatment_field]], seurat.obj[[sample_field]], seurat.obj[[celltype_field]], seurat.obj[[study_field]])\n    colnames(col_data) <- c(\'pseudobulkID\', \'treatment\', \'batch\', \'celltype\', \'study\')\n  }\n  col_data <- unique(col_data)\n  rownames(col_data) <- col_data$pseudobulkID\n  col_data <- col_data[colnames(psb),]\n  \n  if(!all_equal(rownames(col_data), colnames(psb))) {\n    stop(\'Rownames of coldata do not match with the colnames of the assay matrix. Check\')\n  }\n  \n  norm_counts <- cpm(psb, log=FALSE)\n  norm_counts <- log2(norm_counts+1)\n  se <- SummarizedExperiment(assays = list(counts = psb, normalized = norm_counts), colData = col_data)\n  return (se)\n}\n\n\n##\' Given treatment factors and cluster idents of a study, graphs what fraction of cells of each treatment factor\n##\' are in which clusters, segregated by animals in the form of a barplot with geom_points\n##\'\n##\' @param treatment String representing the metadata that represents treatment type\n##\' @param cluster_idents String representing the meta data that represents cluster information\n##\' @param samples String representing the sampleID field in the metadata\n##\' @param order Order in which to plot the treatment types\n##\' @param order_facets order of the facets for each clusters\n##\' @return ggplot Object representing the strip plot\n##\' @author Shristi Pandey\n##\'\ncalculate_cluster_composition <- function(seurat.obj, treatment = NULL, cluster_idents = NULL, samples = NULL, order = NULL, order_facets = NULL, dataset = NULL){\n  library(ggplot2)\n  library(tibble)\n  clusters <- as.data.frame(seurat.obj[[cluster_idents]])\n  treatments <- as.data.frame(seurat.obj[[treatment]])\n  animals <- as.data.frame(seurat.obj[[samples]])\n  study <- as.data.frame(seurat.obj[[dataset]])\n  \n  df<- cbind(clusters, treatments, animals, study)\n  colnames(df) <- c(""clusters"", ""Treatment"", ""animals"", \'study\')\n  \n  count <- table(df$clusters, df$animals)\n  pct <- sweep(count, 2, colSums(count), ""/"")*100\n  \n  df1 <- df[!duplicated(df$animals), ]\n  rownames(df1) <- df1$animals\n  df1$clusters <- NULL\n  \n  df2 <- data.frame(pct)\n  colnames(df2) <- c(\'clusters\', \'animals\', \'Percent\')\n  df3 <- merge(df2, df1, by = \'animals\')\n  \n  df3$Treatment <- factor(df3$Treatment, levels = order)\n  df3$clusters <- factor(df3$clusters, levels = order_facets)\n  \n  My_Theme = theme(\n    axis.title.x = element_text(size = 15, color = \'black\'),\n    axis.text.x = element_blank(),\n    axis.title.y = element_text(size = 15),\n    axis.text.y = element_text(size = 20),\n    axis.title=element_text(size = 25),\n    legend.text = element_text(size = 18),\n    legend.position = \'bottom\' ,\n    plot.title = element_text(hjust=0.5, size = 14),\n    strip.text =  element_text(size = 10)\n  )\n  #strip.text.x = element_text(size = 20), axis.text.y=element_text(size=20), axis.title=element_text(size = 25)\n  \n  p <- ggplot(df3, aes_string(as.name(\'Treatment\'), \'Percent\', color = as.name(\'Treatment\'), fill = as.name(\'Treatment\'))) +\n    geom_point(position = position_jitterdodge(), size =3)+\n    geom_boxplot(alpha = 0.8)+\n    ggtitle(\'What percent of cells within each genotypes are in each Cluster?\') +\n    theme_bw() + facet_grid(.~ factor(clusters))+\n    My_Theme\n  return (p)\n  \n  \n}\n\n\n\n##\' Given an integrated seurat object, calculates DA statistics for cell type abundances.\n##\'\n##\' @param seurat.obj Seurat object containing the count matrix \n##\' @return DA statistics and fold changes\n##\' @author Shristi Pandey\n##\'\n\ncalculate_da_statistics <- function(seurat.obj=oligos){\n  abundances <- table(seurat.obj$finalInterps, seurat.obj$sampleID)\n  abundances <- unclass(abundances)\n  \n  library(edgeR)\n  ph <- seurat.obj@meta.data\n  extra.info <- ph[match(colnames(abundances), ph$sampleID),]\n  y.ab <- DGEList(abundances, samples=extra.info)\n  keep <- filterByExpr(y.ab, group=y.ab$samples$treatment, min.count = 200)\n  summary(keep)\n  \n  #set up design matrix \n  y.ab$samples$treatment <- gsub("" "", """", y.ab$samples$treatment)\n  y.ab$samples$treatment <- factor(y.ab$samples$treatment,\n                                   levels = c(""NonTgy"", ""NonTgo"", ""PS2APPy"", ""PS2APPo"", \n                                              ""TauP301SNegative"", ""TauP301SPositive"", \n                                              ""Nontg"", ""TauPS2app"", ""P301L"",""TauPS2appTrem2"", \n                                              ""Baseline"",""CuprizoneRec"", ""Cuprizone"",  \n                                              ""NL"", ""5dpl"", ""14dpl"", ""28dpl""))\n  design <- model.matrix(~0 + treatment, y.ab$samples)\n  \n  \n  contrasts <- makeContrasts(ps2app_control_old = treatmentNonTgy - treatmentPS2APPy,\n                             ps2app_control_young = treatmentNonTgo - treatmentPS2APPo,\n                             p301s_control = treatmentTauP301SNegative - treatmentTauP301SPositive, \n                             taups2app_control = treatmentNontg - treatmentTauPS2app, \n                             p301L_control = treatmentNontg - treatmentP301L,\n                             taups2apptrem2_control = treatmentNontg - treatmentTauPS2appTrem2, \n                             baseline_4w = treatmentBaseline - treatmentCuprizone, \n                             C4w_4plus3w = treatmentCuprizone - treatmentCuprizoneRec, \n                             Cbaseline_4plus3w = treatmentBaseline - treatmentCuprizoneRec, \n                             NL_5dp = treatmentNL - treatment5dpl,\n                             NLdp_14dp = treatmentNL - treatment14dpl,\n                             fivedp_14dp = treatment5dpl - treatment14dpl,\n                             fourteendp_28dp = treatment14dpl - treatment28dpl,\n                             NL_28dp = treatmentNL - treatment28dpl,\n                             \n                             levels = design)\n  \n  \n  \n  y.ab <- estimateDisp(y.ab, design = design, trend=""none"")\n  fit.ab <- glmQLFit(y.ab, design, robust=TRUE, abundance.trend=FALSE)\n  \n  \n  y.res <- list(NULL)\n  for(i in 1:ncol(contrasts)){\n    qlf <- glmQLFTest(fit.ab, contrast = contrasts[, i])\n    comparison <- gsub(""^.*?timepoint"", """", topTags(qlf)$comparison)\n    comparison <- gsub("" .*timepoint"",""_v_"", comparison)\n    res <- topTags(qlf)\n    \n    y.res[[i]] <- res\n    names(y.res)[i] <- comparison\n  }\n  \n  y.res.sig <- lapply(y.res, function(x){\n    comp <- gsub(""^.*?group"", """", x$comparison)\n    comp <- gsub("" .*group"",""_v_"", comp)\n    x$table$contrast <- comp\n    x <- x$table[x$table$FDR < 0.05,]\n    x$population <- rownames(x)\n    return(x)\n  })\n  return(y.res.sig)\n}\n\n\n\n\n']","Disease-associated Oligodendrocyte Responses Across Neurodegenerative Diseases Oligodendrocyte dysfunction has been implicated in the pathogenesis of neurodegenerative diseases, so understanding their activation states would shed new light on disease processes. We identify three distinct activation states of oligodendrocytes from single-cell RNA-seq of mouse models of Alzheimers Disease (AD) and Multiple Sclerosis (MS): DA1 (disease-associated1, associated with immunogenic genes), DA2 (disease-associated2, associated with genes influencing survival) and IFN (associated with interferon response genes). Spatial analysis of DAOs in the cuprizone model show that DA1 and DA2 establish outside of the lesion area during demyelination and DA1 repopulate the lesion during remyelination. Independent meta-analysis of human single-nuclei RNA-seq datasets reveal that the transcriptional responses of MS oligodendrocytes share features with mouse models. In contrast, the oligodendrocyte activation signatures observed in human AD is largely distinct from those observed in mice. This catalog (http://research-pub.gene.com/OligoLandscape/) of oligodendrocyte activation states will be important for understanding disease progression and developing novel therapeutic interventions.",0
Data from: Network-based biostratigraphy for the late Permian to mid-Triassic Beaufort Group (Karoo Supergroup) in South Africa enhances biozone applicability and stratigraphic correlation,"The Permo-Triassic vertebrate assemblage zones (AZs) of South Africa's Karoo Basin are a standard for local and global correlations. However, temporal, geographical, and methodological limitations challenge the AZs reliability. We analyze a unique fossil dataset comprising 1408 occurrences of 115 species grouped into 19 stratigraphic bin intervals from the Cistecephalus, Daptocephalus, Lystrosaurus declivis, and Cynognathus AZs. Using network science tools we compare six frameworks: Broom, Rubidge, Viglietti, Member, Formation, including a framework suggesting diachroneity of the Daptocephalus/Lystrosaurus AZ boundary (Gastaldo). Our results demonstrate that historical frameworks (Broom, Rubidge) still identify the Karoo AZs. No scheme supports the Cistecephalus AZ, and it likely comprises two discrete communities. The Lystrosaurus declivis AZ is traced across all frameworks, despite many shared species with the underlying Daptocephalus AZ, suggesting the extinction event across this interval is not a statistical artifact. A community shift at the upper Katberg to lower Burgersdorp formations may indicate a depositional hiatus, which has important implications for regional correlations and Mesozoic ecosystem evolution. The Gastaldo model still identifies a Lystrosaurus and Daptocephalus AZ community shift, does not significantly improve recent AZ models (Viglietti), and highlights important issues with some AZ studies. Localized bed-scale lithostratigraphy (sandstone datums), and singleton fossils cannot be used to reject the patterns shown by hundreds of fossils, and regional chronostratigraphic markers of the Karoo foreland basin. Meter-level occurrence data suggest that 2050 m sampling intervals capture Karoo AZs, unifying the use of meter-level placements of singleton fossils to delineate biozone boundaries and make regional correlations.","['#Karoo Assemblage Zone test\r\n\r\n#Each section is repeated to create a node and edge list for each AZ community. \r\n#At the end are optional steps to plt different route, directed, undirected or weighted networks.\r\n#set your own working directory and file location before starting.\r\n# the working directory and file location for Viglietti is as follows: (""C:/Users/pviglietti/Dropbox/Field Museum/2019 postdoc/Publications/Bipartite Network Analysis/Assemblage zone test"")\r\n\r\n#********************************\r\n#Network 1: Rubidge\r\n#********************************\r\n\r\nAZ_Rubidge95<-read.csv(""C:/Users/pviglietti/Dropbox/Field Museum/2019 postdoc/Publications/Bipartite Network Analysis/Assemblage zone test/AZ_Rubidge95.csv"")\r\n\r\nlibrary(tidyverse)\r\n\r\n\r\n#Optional step: creating our node objects. Our nodes are stratigraphic interval (bin) and species (taxa).\r\n\r\nBin95 <- distinct(AZ_Rubidge95, Bin_Interval)\r\n\r\nTaxa95 <- distinct(AZ_Rubidge95, Taxa)\r\n\r\n#Step 2: Combine node objects to create node list\r\n\r\n# Need to give both columns in the object were given the same ""label"" so they can be joined in a node list\r\n# Note the 2nd step above can be skipped as you can name the columns of interest ""label"" directly from main dataset\r\n\r\nTaxa95 <- AZ_Rubidge95 %>%\r\n  distinct(Taxa) %>%\r\n  rename(label = Taxa)\r\n\r\nBin95 <- AZ_Rubidge95 %>%\r\n  distinct(Bin_Interval) %>%\r\n  rename(label = Bin_Interval)\r\n\r\n#Step 3 Create Node list\r\n# Now that both columns of interest were given the same ""label"" they could be joined in a node list\r\n\r\nnodes.AZ_Rubidge95 <- full_join(Taxa95,Bin95, by = ""label"")\r\n\r\n#We now have an object called nodes with is our node list for the network analysis.\r\n# Now give give each node a unique id by adding another column that will give it a unique number.\r\nnodes.Rubidge95 <- nodes.AZ_Rubidge95 %>% rowid_to_column(""id"")\r\n\r\n# Step 4 create Edge list\r\n#Now we need a ""weight"" column which is basically a taxon abundance count for each bin.\r\n\r\nper_route.Rubidge95 <- AZ_Rubidge95 %>%  \r\n  group_by(Taxa, Bin_Interval) %>%\r\n  summarise(weight = n()) %>% \r\n  ungroup()\r\n\r\n#Repeat the above steps for making edge list of the other communities by substituting other community dataset.\r\n#The column names are the same for each dataset.\r\n#How to export edge list CSV file\r\nwrite.csv(per_route.Rubidge95, ""edgelist_Rubidge95.csv"")\r\n\r\n\r\n#*********************************\r\n#Network 2: Member\r\n#*********************************\r\n\r\nAZ_Member<-read.csv(""C:/Users/pviglietti/Dropbox/Field Museum/2019 postdoc/Publications/Bipartite Network Analysis/Assemblage zone test/AZ_Member.csv"")\r\n\r\n#Step 1: Combine node objects to create node list\r\n\r\nTaxa_M <- AZ_Member %>%\r\n  distinct(Taxa) %>%\r\n  rename(label = Taxa)\r\n\r\nBin_M <- AZ_Member %>%\r\n  distinct(Bin_Interval) %>%\r\n  rename(label = Bin_Interval)\r\n\r\n#Step 3 Create Node list\r\n\r\nnodes.AZ_Member <- full_join(Taxa_M,Bin_M, by = ""label"")\r\n\r\nnodes.Member <- nodes.AZ_Member %>% rowid_to_column(""id"")\r\n\r\n# Step 4 create Edge list\r\n\r\nper_route.Member <- AZ_Member %>%  \r\n  group_by(Taxa, Bin_Interval) %>%\r\n  summarise(weight = n()) %>% \r\n  ungroup()\r\n\r\nwrite.csv(per_route.Member, ""edgelist_Member.csv"")\r\n\r\n#*********************************\r\n#Network 3: Viglietti et al 2021\r\n#*********************************\r\n\r\nAZ_Viglietti21<-read.csv(""C:/Users/pviglietti/Dropbox/Field Museum/2019 postdoc/Publications/Bipartite Network Analysis/Assemblage zone test/AZ_Viglietti21.csv"")\r\n\r\n#Step 1: Combine node objects to create node list\r\n\r\nTaxa21 <- AZ_Viglietti21 %>%\r\n  distinct(Taxa) %>%\r\n  rename(label = Taxa)\r\n\r\nBin21 <- AZ_Viglietti21 %>%\r\n  distinct(Bin_Interval) %>%\r\n  rename(label = Bin_Interval)\r\n\r\n#Step 2 Create Node list\r\n\r\nnodes.AZ_Viglietti21 <- full_join(Taxa21,Bin21, by = ""label"")\r\n\r\nnodes.Viglietti21 <- nodes.AZ_Viglietti21 %>% rowid_to_column(""id"")\r\n\r\n# Step 3 create Edge list\r\n\r\nper_route.Viglietti21 <- AZ_Viglietti21 %>%  \r\n  group_by(Taxa, Bin_Interval) %>%\r\n  summarise(weight = n()) %>% \r\n  ungroup()\r\n\r\nwrite.csv(per_route.Viglietti21, ""edgelist_Viglietti21.csv"")\r\n\r\n#****************************\r\n#Network 4: Formation\r\n#****************************\r\n\r\nAZ_Formation<-read.csv(""C:/Users/pviglietti/Dropbox/Field Museum/2019 postdoc/Publications/Bipartite Network Analysis/Assemblage zone test/AZ_Formation.csv"")\r\n\r\n#Step 1: Combine node objects to create node list\r\n\r\nTaxaFm <- AZ_Formation %>%\r\n  distinct(Taxa) %>%\r\n  rename(label = Taxa)\r\n\r\nBinFm <- AZ_Formation %>%\r\n  distinct(Bin_Interval) %>%\r\n  rename(label = Bin_Interval)\r\n\r\n#Step 2 Create Node list\r\n\r\nnodes.AZ_Formation <- full_join(TaxaFm,BinFm, by = ""label"")\r\n\r\nnodes.Formation <- nodes.AZ_Formation %>% rowid_to_column(""id"")\r\n\r\n# Step 3 create Edge list\r\n\r\nper_route.Formation <- AZ_Formation %>%  \r\n  group_by(Taxa, Bin_Interval) %>%\r\n  summarise(weight = n()) %>% \r\n  ungroup()\r\n\r\nwrite.csv(per_route.Formation, ""edgelist_Formation.csv"")\r\n\r\n#****************************\r\n#Network 5: Broom (1906)\r\n#****************************\r\n\r\nAZ_Broom06<-read.csv(""C:/Users/pviglietti/Dropbox/Field Museum/2019 postdoc/Publications/Bipartite Network Analysis/Assemblage zone test/AZ_Broom06.csv"")\r\n\r\n#Step 1: Combine node objects to create node list\r\n\r\nTaxaBroom06 <- AZ_Broom06 %>%\r\n  distinct(Taxa) %>%\r\n  rename(label = Taxa)\r\n\r\nBinBroom06 <- AZ_Broom06 %>%\r\n  distinct(Bin_Interval) %>%\r\n  rename(label = Bin_Interval)\r\n\r\n#Step 2 Create Node list\r\n\r\nnodes.AZ_Broom06 <- full_join(TaxaBroom06,BinBroom06, by = ""label"")\r\n\r\nnodes.Broom06 <- nodes.AZ_Broom06 %>% rowid_to_column(""id"")\r\n\r\n# Step 3 create Edge list\r\n\r\nper_route.Broom06 <- AZ_Broom06 %>%  \r\n  group_by(Taxa, Bin_Interval) %>%\r\n  summarise(weight = n()) %>% \r\n  ungroup()\r\n\r\nwrite.csv(per_route.Broom06, ""edgelist_Broom06.csv"")\r\n\r\n#****************************\r\n#Network 6: Gastaldo (2021)\r\n#****************************\r\n\r\nAZ_gastaldo<-read.csv(""C:/Users/pviglietti/Dropbox/Field Museum/2019 postdoc/Publications/Bipartite Network Analysis/Assemblage zone test/AZ_gastaldo.csv"")\r\n\r\n#Step 1: Combine node objects to create node list\r\n\r\nTaxagastaldo <- AZ_gastaldo %>%\r\n  distinct(Taxa) %>%\r\n  rename(label = Taxa)\r\n\r\nBingastaldo <- AZ_gastaldo %>%\r\n  distinct(Bin_Interval) %>%\r\n  rename(label = Bin_Interval)\r\n\r\n#Step 2 Create Node list\r\n\r\nnodes.AZ_gastaldo <- full_join(Taxagastaldo,Bingastaldo, by = ""label"")\r\n\r\nnodes.gastaldo <- nodes.AZ_gastaldo %>% rowid_to_column(""id"")\r\n\r\n# Step 3 create Edge list\r\n\r\nper_route.gastaldo <- AZ_gastaldo %>%  \r\n  group_by(Taxa, Bin_Interval) %>%\r\n  summarise(weight = n()) %>% \r\n  ungroup()\r\n\r\nwrite.csv(per_route.gastaldo, ""edgelist_gastaldo.csv"")\r\n\r\n# *****************************************\r\n#Optional steps for making network plots***\r\n#******************************************\r\n\r\n#Currently ""source"" and ""destination"" columns contain labels rather than ids.\r\n#Now we link ids assigned in the nodes to each location in the source and destination.\r\n#This is accomplished by another join function that links our S (""source"") and F (""destination"") object to node ids.\r\n\r\n#*******************************************\r\n#Network 1: Rubidge\r\n#*******************************************\r\n\r\nedges.AZ_Rubidge95 <- per_route.Rubidge95 %>% \r\n  left_join(nodes.Rubidge95, by = c(""Taxa"" = ""label"")) %>% \r\n  rename(from = id)\r\n\r\nedges.Rubidge95 <- edges.AZ_Rubidge95 %>% \r\n  left_join(nodes.Rubidge95, by = c(""Bin_Interval"" = ""label"")) %>% \r\n  rename(to = id)\r\n\r\n#when we did the previous analysis the per route object data frame was on the left of the columns\r\n#Now we redorder so that the ""to"" and ""from"" lists are on the left of the data frame\r\n#our ""source"" and ""destination"" columns are also removed as the ""to"" and ""from"" serve this purpose (numerically).\r\nedges.Rubidge95_final <- select(edges.Rubidge95, from, to, weight)\r\nedges.Rubidge95_final\r\n\r\n\r\n#Step 6\r\n#Repeat steps for different bin intervals\r\n\r\n#Step 7 plotting routes network\r\n\r\n#Making a network object\r\n\r\nlibrary(network)\r\n\r\nroutes_Rubidge95 <- network(edges.Rubidge95, vertex.attr = nodes.Rubidge95, matrix.type = ""edgelist"", ignore.eval = FALSE)\r\n#can view the type of class the routes network object is by using the class() function.\r\n\r\nclass(routes_Rubidge95)\r\nsummary(routes_Rubidge95)\r\n\r\n#We can now plot a rudimentary network graph\r\nplot(routes_Rubidge95, vertex.cex = 3)\r\n\r\n#Graph cleanup and igraph.Ths step removes routes_Rubidge95 object so don\'t do step if you want to do more plots.\r\n\r\ndetach(package:network)\r\nrm(routes_Rubidge95)\r\n\r\n#Install package igraph\r\n\r\nlibrary(igraph)\r\n\r\nroutes_Rubidge95 <- graph_from_data_frame(d = edges, vertices = nodes, directed = TRUE)\r\n\r\nplot(routes_Rubidge95, edge.arrow.size = 0.2)\r\n\r\nplot(routes_Rubidge95, layout = layout_with_graphopt, edge.arrow.size = 0.2)\r\n\r\n# Load Tidygraph and ggraph\r\n#Always start by loading necessary packages.\r\nlibrary(tidygraph)\r\nlibrary(ggraph)\r\n# Going to create a tbl_graph using Tidygraph which uses an edge and node tibble.\r\n# tbl_graphs are essentially igraph objects.\r\nroutes_tidy <- tbl_graph(nodes = nodes.Rubidge95, edges = edges.Rubidge95, directed = TRUE)\r\n\r\nroutes_tidy\r\n\r\n#Plot new graph with ggraph\r\n\r\nggraph(routes_tidy) + geom_edge_link() + geom_node_point() + theme_graph()\r\n# We can also show the weight of the edges and make graph more informative.\r\n\r\nggraph(routes_tidy, layout = ""graphopt"") + \r\n  geom_node_point() +\r\n  geom_edge_link(aes(width = weight), alpha = 0.8) + \r\n  scale_edge_width(range = c(0.2, 2)) +\r\n  geom_node_text(aes(label = label), repel = TRUE) +\r\n  labs(edge_width = ""Letters"") +\r\n  theme_graph()\r\n\r\n#Arc graphs\r\n#Indicate directionality of the edges\r\n\r\nggraph(routes_igraph, layout = ""linear"") + \r\n  geom_edge_arc(aes(width = weight), alpha = 0.8) + \r\n  scale_edge_width(range = c(0.2, 2)) +\r\n  geom_node_text(aes(label = label)) +\r\n  labs(edge_width = ""Letters"") +\r\n  theme_graph()\r\n\r\n#Interactive network graphs using visNetwork and networkD3\r\n\r\nlibrary(visNetwork)\r\nlibrary(networkD3)\r\n\r\n#visNetwork function uses a nodes list and edges list to create an interactive graph. \r\n#The nodes list must include an ""id"" column, and the edge list must have ""from"" and ""to"" columns. \r\n\r\nvisNetwork(nodes.Rubidge95, edges.Rubidge95)\r\n\r\n#The mutate() function allows us to create a graph with variable edge widths.\r\n\r\nedges <- mutate(edges.Rubidge95, width = weight/5 + 1)\r\n\r\nvisNetwork(nodes.Rubidge95, edges) %>% \r\n  visIgraphLayout(layout = ""layout_with_fr"") %>% \r\n  visEdges(arrows = ""middle"")\r\n\r\n#NetworkD3\r\n\r\n#Some preparation required to present data in networkD3 graph.\r\n#The edge and node list requires that the IDs be a series of numeric integers that begin with 0. \r\n#Currently, the node IDs for our data begin with 1, and so we have to do a bit of data manipulation.\r\n#Once again, this can be done with the mutate() function. \r\n#The goal is to recreate the current columns, while subtracting 1 from each ID. \r\n#making new node and edge objects that are manipulated to start ids at 0 rather than 1.\r\n\r\nnodes_d3 <- mutate(nodes.Rubidge95, id = id - 1)\r\nedges_d3 <- mutate(edges, from = from - 1, to = to - 1)\r\n\r\n#It is now possible to plot a networkD3 graph.\r\n\r\nforceNetwork(Links=edges_d3, Nodes=nodes_d3, Source=""from"", Target=""to"", \r\n             NodeID=""label"", Group=""id"", Value=""weight"", \r\n             opacity= 1, fontSize= 16, zoom= TRUE)\r\n\r\n#Making a Sankey diagram\r\n#Good for when there are not too many nodes...\r\n#uses the sankeyNetwork function which takes the same arguments as forceNetwork.\r\nsankeyNetwork(Links = edges_d3, Nodes = nodes_d3, Source = ""from"", Target = ""to"", \r\n              NodeID = ""label"", Value = ""weight"", fontSize = 16, unit = ""Letter(s)"")\r\n\r\n\r\n\r\n#*********************************\r\n#Network 2: Member\r\n#*********************************\r\n\r\nedges.AZ_Member <- per_route.Member %>% \r\n  left_join(nodes.Member, by = c(""Taxa"" = ""label"")) %>% \r\n  rename(from = id)\r\n\r\nedges.Member <- edges.AZ_Member %>% \r\n  left_join(nodes.Member, by = c(""Bin_Interval"" = ""label"")) %>% \r\n  rename(to = id)\r\n\r\n#when we did the previous analysis the per route object data frame was on the left of the columns\r\n#Now we redorder so that the ""to"" and ""from"" lists are on the left of the data frame\r\n#our ""source"" and ""destination"" columns are also removed as the ""to"" and ""from"" serve this purpose (numerically).\r\nedges.Member_final <- select(edges.Member, from, to, weight)\r\nedges.Member_final\r\n\r\n\r\n#Step 6\r\n#Repeat steps for different bin intervals\r\n\r\n#Step 7 plotting routes network\r\n\r\n#Making a network object\r\n\r\nlibrary(network)\r\n\r\nroutes_Member <- network(edges.Member, vertex.attr = nodes.Member, matrix.type = ""edgelist"", ignore.eval = FALSE)\r\n#can view the type of class the routes network object is by using the class() function.\r\n\r\nclass(routes_Member)\r\nsummary(routes_Member)\r\n\r\n#We can now plot a rudimentary network graph\r\nplot(routes_Member, vertex.cex = 3)\r\n\r\n#Graph cleanup and igraph.Ths step removes routes_Rubidge95 object so don\'t do step if you want to do more plots.\r\n\r\ndetach(package:network)\r\nrm(routes_Member)\r\n\r\n#Install package igraph\r\n\r\nlibrary(igraph)\r\n\r\nroutes_Member <- graph_from_data_frame(d = edges, vertices = nodes, directed = TRUE)\r\n\r\nplot(routes_Member, edge.arrow.size = 0.2)\r\n\r\nplot(routes_Member, layout = layout_with_graphopt, edge.arrow.size = 0.2)\r\n\r\n# Load Tidygraph and ggraph\r\n#Always start by loading necessary packages.\r\nlibrary(tidygraph)\r\nlibrary(ggraph)\r\n# Going to create a tbl_graph using Tidygraph which uses an edge and node tibble.\r\n# tbl_graphs are essentially igraph objects.\r\nroutes_tidy <- tbl_graph(nodes = nodes.Member, edges = edges.Member, directed = TRUE)\r\n\r\nroutes_tidy\r\n\r\n#Plot new graph with ggraph\r\n\r\nggraph(routes_tidy) + geom_edge_link() + geom_node_point() + theme_graph()\r\n# We can also show the weight of the edges and make graph more informative.\r\n\r\nggraph(routes_tidy, layout = ""graphopt"") + \r\n  geom_node_point() +\r\n  geom_edge_link(aes(width = weight), alpha = 0.8) + \r\n  scale_edge_width(range = c(0.2, 2)) +\r\n  geom_node_text(aes(label = label), repel = TRUE) +\r\n  labs(edge_width = ""Letters"") +\r\n  theme_graph()\r\n\r\n#Arc graphs\r\n#Indicate directionality of the edges\r\n\r\nggraph(routes_igraph, layout = ""linear"") + \r\n  geom_edge_arc(aes(width = weight), alpha = 0.8) + \r\n  scale_edge_width(range = c(0.2, 2)) +\r\n  geom_node_text(aes(label = label)) +\r\n  labs(edge_width = ""Letters"") +\r\n  theme_graph()\r\n\r\n#Interactive network graphs using visNetwork and networkD3\r\n\r\nlibrary(visNetwork)\r\nlibrary(networkD3)\r\n\r\n#visNetwork function uses a nodes list and edges list to create an interactive graph. \r\n#The nodes list must include an ""id"" column, and the edge list must have ""from"" and ""to"" columns. \r\n\r\nvisNetwork(nodes.Member, edges.Member)\r\n\r\n#The mutate() function allows us to create a graph with variable edge widths.\r\n\r\nedges <- mutate(edges.Member, width = weight/5 + 1)\r\n\r\nvisNetwork(nodes.Member, edges) %>% \r\n  visIgraphLayout(layout = ""layout_with_fr"") %>% \r\n  visEdges(arrows = ""middle"")\r\n\r\n#NetworkD3\r\n\r\n#Some preparation required to present data in networkD3 graph.\r\n#The edge and node list requires that the IDs be a series of numeric integers that begin with 0. \r\n#Currently, the node IDs for our data begin with 1, and so we have to do a bit of data manipulation.\r\n#Once again, this can be done with the mutate() function. \r\n#The goal is to recreate the current columns, while subtracting 1 from each ID. \r\n#making new node and edge objects that are manipulated to start ids at 0 rather than 1.\r\n\r\nnodes_d3 <- mutate(nodes.Member, id = id - 1)\r\nedges_d3 <- mutate(edges, from = from - 1, to = to - 1)\r\n\r\n#It is now possible to plot a networkD3 graph.\r\n\r\nforceNetwork(Links=edges_d3, Nodes=nodes_d3, Source=""from"", Target=""to"", \r\n             NodeID=""label"", Group=""id"", Value=""weight"", \r\n             opacity= 1, fontSize= 16, zoom= TRUE)\r\n\r\n#Making a Sankey diagram\r\n#Good for when there are not too many nodes...\r\n#uses the sankeyNetwork function which takes the same arguments as forceNetwork.\r\nsankeyNetwork(Links = edges_d3, Nodes = nodes_d3, Source = ""from"", Target = ""to"", \r\n              NodeID = ""label"", Value = ""weight"", fontSize = 16, unit = ""Letter(s)"")\r\n\r\n\r\n#*********************************\r\n#Network 3: Viglietti\r\n#*********************************\r\n\r\nedges.AZ_Viglietti21 <- per_route.Viglietti21 %>% \r\n  left_join(nodes.Viglietti20, by = c(""Taxa"" = ""label"")) %>% \r\n  rename(from = id)\r\n\r\nedges.Viglietti21 <- edges.AZ_Viglietti20 %>% \r\n  left_join(nodes.Viglietti20, by = c(""Bin_Interval"" = ""label"")) %>% \r\n  rename(to = id)\r\n\r\n#when we did the previous analysis the per route object data frame was on the left of the columns\r\n#Now we redorder so that the ""to"" and ""from"" lists are on the left of the data frame\r\n#our ""source"" and ""destination"" columns are also removed as the ""to"" and ""from"" serve this purpose (numerically).\r\nedges.Viglietti21_final <- select(edges.Viglietti21, from, to, weight)\r\nedges.Viglietti21_final\r\n\r\n\r\n#Step 6\r\n#Repeat steps for different bin intervals\r\n\r\n#Step 7 plotting routes network\r\n\r\n#Making a network object\r\n\r\nlibrary(network)\r\n\r\nroutes_Viglietti21 <- network(edges.Viglietti21, vertex.attr = nodes.Viglietti21, matrix.type = ""edgelist"", ignore.eval = FALSE)\r\n#can view the type of class the routes network object is by using the class() function.\r\n\r\nclass(routes_Viglietti21)\r\nsummary(routes_Viglietti21)\r\n\r\n#We can now plot a rudimentary network graph\r\nplot(routes_Viglietti21, vertex.cex = 3)\r\n\r\n#Graph cleanup and igraph.Ths step removes routes_Rubidge95 object so don\'t do step if you want to do more plots.\r\n\r\ndetach(package:network)\r\nrm(routes_Viglietti21)\r\n\r\n#Install package igraph\r\n\r\nlibrary(igraph)\r\n\r\nroutes_Viglietti21 <- graph_from_data_frame(d = edges, vertices = nodes, directed = TRUE)\r\n\r\nplot(routes_Viglietti21, edge.arrow.size = 0.2)\r\n\r\nplot(routes_Viglietti21, layout = layout_with_graphopt, edge.arrow.size = 0.2)\r\n\r\n# Load Tidygraph and ggraph\r\n#Always start by loading necessary packages.\r\nlibrary(tidygraph)\r\nlibrary(ggraph)\r\n# Going to create a tbl_graph using Tidygraph which uses an edge and node tibble.\r\n# tbl_graphs are essentially igraph objects.\r\nroutes_tidy <- tbl_graph(nodes = nodes.Viglietti21, edges = edges.Viglietti21, directed = TRUE)\r\n\r\nroutes_tidy\r\n\r\n#Plot new graph with ggraph\r\n\r\nggraph(routes_tidy) + geom_edge_link() + geom_node_point() + theme_graph()\r\n# We can also show the weight of the edges and make graph more informative.\r\n\r\nggraph(routes_tidy, layout = ""graphopt"") + \r\n  geom_node_point() +\r\n  geom_edge_link(aes(width = weight), alpha = 0.8) + \r\n  scale_edge_width(range = c(0.2, 2)) +\r\n  geom_node_text(aes(label = label), repel = TRUE) +\r\n  labs(edge_width = ""Letters"") +\r\n  theme_graph()\r\n\r\n#Arc graphs\r\n#Indicate directionality of the edges\r\n\r\nggraph(routes_igraph, layout = ""linear"") + \r\n  geom_edge_arc(aes(width = weight), alpha = 0.8) + \r\n  scale_edge_width(range = c(0.2, 2)) +\r\n  geom_node_text(aes(label = label)) +\r\n  labs(edge_width = ""Letters"") +\r\n  theme_graph()\r\n\r\n#Interactive network graphs using visNetwork and networkD3\r\n\r\nlibrary(visNetwork)\r\nlibrary(networkD3)\r\n\r\n#visNetwork function uses a nodes list and edges list to create an interactive graph. \r\n#The nodes list must include an ""id"" column, and the edge list must have ""from"" and ""to"" columns. \r\n\r\nvisNetwork(nodes.Viglietti21, edges.Viglietti21)\r\n\r\n#The mutate() function allows us to create a graph with variable edge widths.\r\n\r\nedges <- mutate(edges.Viglietti21, width = weight/5 + 1)\r\n\r\nvisNetwork(nodes.Viglietti20, edges) %>% \r\n  visIgraphLayout(layout = ""layout_with_fr"") %>% \r\n  visEdges(arrows = ""middle"")\r\n\r\n#NetworkD3\r\n\r\n#Some preparation required to present data in networkD3 graph.\r\n#The edge and node list requires that the IDs be a series of numeric integers that begin with 0. \r\n#Currently, the node IDs for our data begin with 1, and so we have to do a bit of data manipulation.\r\n#Once again, this can be done with the mutate() function. \r\n#The goal is to recreate the current columns, while subtracting 1 from each ID. \r\n#making new node and edge objects that are manipulated to start ids at 0 rather than 1.\r\n\r\nnodes_d3 <- mutate(nodes.Viglietti21, id = id - 1)\r\nedges_d3 <- mutate(edges, from = from - 1, to = to - 1)\r\n\r\n#It is now possible to plot a networkD3 graph.\r\n\r\nforceNetwork(Links=edges_d3, Nodes=nodes_d3, Source=""from"", Target=""to"", \r\n             NodeID=""label"", Group=""id"", Value=""weight"", \r\n             opacity= 1, fontSize= 16, zoom= TRUE)\r\n\r\n#Making a Sankey diagram\r\n#Good for when there are not too many nodes...\r\n#uses the sankeyNetwork function which takes the same arguments as forceNetwork.\r\nsankeyNetwork(Links = edges_d3, Nodes = nodes_d3, Source = ""from"", Target = ""to"", \r\n              NodeID = ""label"", Value = ""weight"", fontSize = 16, unit = ""Letter(s)"")\r\n\r\n#*********************************\r\n#Network 4: Formation\r\n#*********************************\r\n\r\nedges.AZ_Formation <- per_route.Formation %>% \r\n  left_join(nodes.Formation, by = c(""Taxa"" = ""label"")) %>% \r\n  rename(from = id)\r\n\r\nedges.Formation <- edges.AZ_Formation %>% \r\n  left_join(nodes.Formation, by = c(""Bin_Interval"" = ""label"")) %>% \r\n  rename(to = id)\r\n\r\n#when we did the previous analysis the per route object data frame was on the left of the columns\r\n#Now we redorder so that the ""to"" and ""from"" lists are on the left of the data frame\r\n#our ""source"" and ""destination"" columns are also removed as the ""to"" and ""from"" serve this purpose (numerically).\r\nedges.Formation_final <- select(edges.Formation, from, to, weight)\r\nedges.Formation_final\r\n\r\n\r\n#Step 6\r\n#Repeat steps for different bin intervals\r\n\r\n#Step 7 plotting routes network\r\n\r\n#Making a network object\r\n\r\nlibrary(network)\r\n\r\nroutes_Formation <- network(edges.Formation, vertex.attr = nodes.Formation, matrix.type = ""edgelist"", ignore.eval = FALSE)\r\n#can view the type of class the routes network object is by using the class() function.\r\n\r\nclass(routes_Formation)\r\nsummary(routes_Formation)\r\n\r\n#We can now plot a rudimentary network graph\r\nplot(routes_Formation, vertex.cex = 3)\r\n\r\n#Graph cleanup and igraph.Ths step removes routes_Formation object so don\'t do step if you want to do more plots.\r\n\r\ndetach(package:network)\r\nrm(routes_Formation)\r\n\r\n#Install package igraph\r\n\r\nlibrary(igraph)\r\n\r\nroutes_Formation <- graph_from_data_frame(d = edges, vertices = nodes, directed = TRUE)\r\n\r\nplot(routes_Formation, edge.arrow.size = 0.2)\r\n\r\nplot(routes_Formation, layout = layout_with_graphopt, edge.arrow.size = 0.2)\r\n\r\n# Load Tidygraph and ggraph\r\n#Always start by loading necessary packages.\r\nlibrary(tidygraph)\r\nlibrary(ggraph)\r\n# Going to create a tbl_graph using Tidygraph which uses an edge and node tibble.\r\n# tbl_graphs are essentially igraph objects.\r\nroutes_tidy <- tbl_graph(nodes = nodes.Formation, edges = edges.Viglietti20, directed = TRUE)\r\n\r\nroutes_tidy\r\n\r\n#Plot new graph with ggraph\r\n\r\nggraph(routes_tidy) + geom_edge_link() + geom_node_point() + theme_graph()\r\n# We can also show the weight of the edges and make graph more informative.\r\n\r\nggraph(routes_tidy, layout = ""graphopt"") + \r\n  geom_node_point() +\r\n  geom_edge_link(aes(width = weight), alpha = 0.8) + \r\n  scale_edge_width(range = c(0.2, 2)) +\r\n  geom_node_text(aes(label = label), repel = TRUE) +\r\n  labs(edge_width = ""Letters"") +\r\n  theme_graph()\r\n\r\n#Arc graphs\r\n#Indicate directionality of the edges\r\n\r\nggraph(routes_igraph, layout = ""linear"") + \r\n  geom_edge_arc(aes(width = weight), alpha = 0.8) + \r\n  scale_edge_width(range = c(0.2, 2)) +\r\n  geom_node_text(aes(label = label)) +\r\n  labs(edge_width = ""Letters"") +\r\n  theme_graph()\r\n\r\n#Interactive network graphs using visNetwork and networkD3\r\n\r\nlibrary(visNetwork)\r\nlibrary(networkD3)\r\n\r\n#visNetwork function uses a nodes list and edges list to create an interactive graph. \r\n#The nodes list must include an ""id"" column, and the edge list must have ""from"" and ""to"" columns. \r\n\r\nvisNetwork(nodes.Formation, edges.Formation)\r\n\r\n#The mutate() function allows us to create a graph with variable edge widths.\r\n\r\nedges <- mutate(edges.Formation, width = weight/5 + 1)\r\n\r\nvisNetwork(nodes.Formation, edges) %>% \r\n  visIgraphLayout(layout = ""layout_with_fr"") %>% \r\n  visEdges(arrows = ""middle"")\r\n\r\n#NetworkD3\r\n\r\n#Some preparation required to present data in networkD3 graph.\r\n#The edge and node list requires that the IDs be a series of numeric integers that begin with 0. \r\n#Currently, the node IDs for our data begin with 1, and so we have to do a bit of data manipulation.\r\n#Once again, this can be done with the mutate() function. \r\n#The goal is to recreate the current columns, while subtracting 1 from each ID. \r\n#making new node and edge objects that are manipulated to start ids at 0 rather than 1.\r\n\r\nnodes_d3 <- mutate(nodes.Formation, id = id - 1)\r\nedges_d3 <- mutate(edges, from = from - 1, to = to - 1)\r\n\r\n#It is now possible to plot a networkD3 graph.\r\n\r\nforceNetwork(Links=edges_d3, Nodes=nodes_d3, Source=""from"", Target=""to"", \r\n             NodeID=""label"", Group=""id"", Value=""weight"", \r\n             opacity= 1, fontSize= 16, zoom= TRUE)\r\n\r\n#Making a Sankey diagram\r\n#Good for when there are not too many nodes...\r\n#uses the sankeyNetwork function which takes the same arguments as forceNetwork.\r\nsankeyNetwork(Links = edges_d3, Nodes = nodes_d3, Source = ""from"", Target = ""to"", \r\n              NodeID = ""label"", Value = ""weight"", fontSize = 16, unit = ""Letter(s)"")\r\n\r\n']","Data from: Network-based biostratigraphy for the late Permian to mid-Triassic Beaufort Group (Karoo Supergroup) in South Africa enhances biozone applicability and stratigraphic correlation The Permo-Triassic vertebrate assemblage zones (AZs) of South Africa's Karoo Basin are a standard for local and global correlations. However, temporal, geographical, and methodological limitations challenge the AZs reliability. We analyze a unique fossil dataset comprising 1408 occurrences of 115 species grouped into 19 stratigraphic bin intervals from the Cistecephalus, Daptocephalus, Lystrosaurus declivis, and Cynognathus AZs. Using network science tools we compare six frameworks: Broom, Rubidge, Viglietti, Member, Formation, including a framework suggesting diachroneity of the Daptocephalus/Lystrosaurus AZ boundary (Gastaldo). Our results demonstrate that historical frameworks (Broom, Rubidge) still identify the Karoo AZs. No scheme supports the Cistecephalus AZ, and it likely comprises two discrete communities. The Lystrosaurus declivis AZ is traced across all frameworks, despite many shared species with the underlying Daptocephalus AZ, suggesting the extinction event across this interval is not a statistical artifact. A community shift at the upper Katberg to lower Burgersdorp formations may indicate a depositional hiatus, which has important implications for regional correlations and Mesozoic ecosystem evolution. The Gastaldo model still identifies a Lystrosaurus and Daptocephalus AZ community shift, does not significantly improve recent AZ models (Viglietti), and highlights important issues with some AZ studies. Localized bed-scale lithostratigraphy (sandstone datums), and singleton fossils cannot be used to reject the patterns shown by hundreds of fossils, and regional chronostratigraphic markers of the Karoo foreland basin. Meter-level occurrence data suggest that 2050 m sampling intervals capture Karoo AZs, unifying the use of meter-level placements of singleton fossils to delineate biozone boundaries and make regional correlations.",0
Data from: Predicted input of uncultured fungal symbionts to a lichen symbiosis from metagenome-assembled genomes,"Basidiomycete yeasts have recently been reported as stably associated secondary fungal symbionts (SFSs) of many lichens, but their role in the symbiosis remains unknown. Attempts to sequence their genomes have been hampered both by the inability to culture them and their low abundance in the lichen thallus alongside two dominant eukaryotes (an ascomycete fungus and chlorophyte alga). Using the lichen Alectoria sarmentosa, we selectively dissolved the cortex layer in which SFSs are embedded to enrich yeast cell abundance and sequenced DNA from the resulting slurries as well as bulk lichen thallus. In addition to yielding a near-complete genome of the filamentous ascomycete using both methods, metagenomes from cortex slurries yielded a 36- to 84-fold increase in coverage and near-complete genomes for two basidiomycete species, members of the classes Cystobasidiomycetes and Tremellomycetes. The ascomycete possesses the largest gene repertoire of the three. It is enriched in proteases often associated with pathogenicity and harbours the majority of predicted secondary metabolite clusters. The basidiomycete genomes possess 35% fewer predicted genes than the ascomycete and have reduced secretomes even compared to close relatives, while exhibiting signs of nutrient limitation and scavenging. Furthermore, both basidiomycetes are enriched in genes coding for enzymes producing secreted acidic polysaccharides, representing a potential contribution to the shared extracellular matrix. All three fungi retain genes involved in dimorphic switching, despite the ascomycete not being known to possess a yeast stage. The basidiomycete genomes are an important new resource for exploration of lifestyle and function in fungal-fungal interactions in lichen symbioses.","['#Script for generating GC vs coverage plots. Takes a fasta file, a binning file provided by CONCOCT, and a coverage file (see coverage.sh for how coverage files are generated)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(plotly)\nlibrary(Biostrings)\n\n###generating GC to coverage plot\n#function to get a contig average coverage\ngetCov<-function(coverage_file){\n  colnames(coverage_file) <- c(""contig"", ""position"", ""coverage"") #name columns\n  cov.contig <- as.data.frame(tapply(coverage_file$coverage, coverage_file$contig, mean,na.rm=T)) #calculate avg coverage per contig\n  colnames(cov.contig)<-c(\'coverage\') #rename column\n  return(cov.contig)\n}\n\ngetGCcov<-function(fasta,coverage_file,...){\n  #process fasta\n  contig_names<-names(fasta) #get contig names\n  nucl_freq<-alphabetFrequency(fasta)\n  nucl_freq<-as.data.frame(nucl_freq) %>% mutate(gc=100*(G+C+S)/(A+T+W+G+C+S)) #get gc content\n  dataset1<-data.frame(contig_names,nucl_freq$gc)\n  row.names(dataset1)<-dataset1$contig_names\n  #process coverage file\n  dataset2<-getCov(coverage_file)\n  #combine\n  final_dataset<-merge(dataset1,dataset2,by = ""row.names"")\n  return(final_dataset)\n}\n\n#read files  \ncov<-read.delim(\'path/to/coverage/file\',header=F)\nfasta<-readDNAStringSet(\'path/to/fasta/file\')\nbinning<-read.csv2(\'path/to/concoct/file\',header=F)\ncolnames(binning)<-c(\'contig_names\',""bin"")\nbinning$bin<-paste(\'bin\',binning$bin,sep=\'\')\n\n#get the data\ngc_dataset1<-getGCcov(fasta,cov) #get GC and coverage data\ngc_dataset2<-left_join(gc_dataset1[2:4],binning) #add info on the bin assignment\ngc_dataset2$bin<-factor(gc_dataset2$bin,levels=c(""bin0"",\'bin1\',""bin2"",\'bin3\',""bin4"",""bin5"",""bin6"",""bin7"",""bin8"",\n                                                 ""bin10"",""bin11"",""bin12"",""bin13"",""bin14"",""bin15"",""bin16"",""bin17"", ""bin18"",""bin19"",""bin20"",""bin21"",""bin22"",\n                                                 ""bin23"",""bin24"",""bin25"",""bin26"",""bin27"",\'bin9\',""unbinned""))\n\nwrite.table(gc_dataset2,""gc_plot.txt"",sep=\'\\t\',quote = F,row.names = F)\n\n#make a graph\ngc_dataset2<-read.delim(""gc_plot.txt"",as.is =T)\ncolnames(gc_dataset2)<-c(""contig"",""nucl_freq.gc"",""coverage"",""bin"")\ngc_dataset2[is.na(gc_dataset2$bin),4]<-\'unbinned\'\ngc_dataset2$bin<-as.factor(gc_dataset2$bin)\n\npalette<-c(\'#47306a\',\'#6cd445\',\'#a94de4\',\'#cddc32\',\'#592ea8\',\'#64d57c\',\'#c656c2\',\'#aec04f\',\'#6574dc\',\'#dd9c2b\',\'#908acf\',\n           \'#a7b696\',\'#d84472\',\'#5fbb8c\',\'#c74694\',\'#779c4f\',\'#652a42\',\'#cab176\',\'#6c9abe\',\'#b25038\',\'#a1a7b2\',\'#b27c36\',\'#c789b3\',\'#534a25\',\n           \'#b35055\',\'#373e3c\',\'#bb8a82\',\'#db482b\',\'#61c2bc\')\ngc_plot<-plot_ly(data=gc_dataset2,x=~nucl_freq.gc,y=~coverage,color=~bin,colors = palette,marker=list(size=4)) %>%add_markers()%>%  \n  layout( xaxis = list(title = \'GC%\'), \n          yaxis = list(title = \'Coverage\',type=""log""))\n\nhtmlwidgets::saveWidget(as.widget(gc_plot), ""gc_plot.html"", selfcontained = FALSE) \n\n#calculate median coverage per bin\ncoverage_table<-gc_dataset2 %>% group_by(bin) %>% summarise(median=median(coverage),medianGC=median(nucl_freq.gc))\nwrite.table(coverage_table,""bin_coverage.txt"",sep=\'\\t\',quote = F,row.names = F,col.names=T)\n\n', 'library(dplyr)\nlibrary (stringr)\nlibrary (seqRFLP)\nlibrary(Biostrings)\n\n#load the fun annotate table\ngenome_ann<-read.delim(\'/file/with/funannotate/annotation/tables\',na.strings=\'\')\n\n#filter gene models labelled as secreted by SignalP\ngenome_signalp<-genome_ann[!is.na(genome_ann$Secreted),c(1,20)]\n\n#write them as fasta\ngenome_signalp_faa<-dataframe2fas(genome_signalp,file=\'genome_signalp.faa\')\n\n\n\n#take the fasta file, analyze it with the TNHMM web server. The lines below process the output file produced by TMHMM\ngenome_tmhmm<-read.delim(\'genome_tmhmm.txt\',sep=\'\\t\',header=F)\ngenome_tmhmm_readable<-genome_tmhmm %>% separate(V2, into=c(\'t1\',\'len\'),sep = ""="",extra=""merge"") %>% separate(V3, into=c(\'t2\',\'ExpAA\'),sep = ""="",extra=""merge"") %>%\n  separate(V4, into=c(\'t3\',\'First60\'),sep = ""="",extra=""merge"") %>%   separate(V5, into=c(\'t4\',\'PredHel\'),sep = ""="",extra=""merge"") %>%\n  separate(V6, into=c(\'t5\',\'Topology\'),sep = ""="",extra=""merge"") %>% select(-c(t1,t2,t3,t4,t5))\nwrite.table(genome_tmhmm_readable,""genome_tmhmm_readable.txt"",sep=\'\\t\',quote = F,row.names = F)\n\ngenome_signalp2<-data.frame(genome_signalp$GeneID)\ncolnames(genome_signalp2)<-\'V1\'\ngenome_secreted<-left_join(genome_signalp2,genome_tmhmm_readable)\ngenome_secreted$PredHel<-as.numeric(genome_secreted$PredHel)\ngenome_secreted$len<-as.numeric(genome_secreted$len)\ngenome_secreted$ExpAA<-as.numeric(genome_secreted$ExpAA)\ngenome_secreted$First60<-as.numeric(genome_secreted$First60)\n\ngenome_secreted <- mutate(genome_secreted,delta=ExpAA-First60) \ngenome_secreted <-genome_secreted[genome_secreted$delta<18 & genome_secreted$PredHel<2,]\nwrite.table(genome_secreted,""genome_secreted_signalp_tmhmm.txt"",sep=\'\\t\',quote = F,row.names = F)\n\ngenome_secreted_signalp_tmhmm_fa<-genome_ann[genome_ann$GeneID %in% genome_secreted$V1,c(1,20)]\ngenome_secreted_signalp_tmhmm_fa<-dataframe2fas(genome_secreted_signalp_tmhmm_fa,file=\'genome_secreted_signalp_tmhmm.faa\')\ngenome_secreted_signalp_tmhmm_fa<-readAAStringSet(\'genome_secreted_signalp_tmhmm.faa\')\nwriteXStringSet(genome_secreted_signalp_tmhmm_fa,\'genome_secreted_signalp_tmhmm_batch1.faa\')\n\n\n\n#take the resulting file and analyze it with the WolfPSORT web server. The lines below process the output file produced by WolfPSORT\ngenome_wolf<-as.character(read.delim(\'genome_wolf_output.txt\',header = F,as.is=T))\ngenome_wolf<-as.data.frame(unlist(strsplit(genome_wolf,\'clgrpredictedgene_\'))[-1])\ncolnames(genome_wolf)<-\'V1\'\ngenome_wolf2<-separate(genome_wolf,\'V1\',into=c(\'ID\',\'pred\'),extra=\'merge\')\ngenome_wolf2$ID<-paste(\'clgrpredictedgene_\',genome_wolf2$ID,sep=\'\')\n\npredictions<-c(\'plas\', \'extr\', \'cyto_nucl\', \'nucl\', \'cyto\', \'pero\', \'mito\', \'E.R.\', \'vacu\', \'golg\',\'cyto_mito\',\'cysk\')\n\nfor (p in predictions){\n  regex<-paste("".*?"",p,\':([0-9]+).*$\',sep=\'\')\n  vector<-str_extract(genome_wolf2$pred,regex)\n  vector[!is.na(vector)]<-sub(regex, ""\\\\1"", vector[!is.na(vector)])\n  genome_wolf2<-cbind(genome_wolf2,as.numeric(as.character(vector)))\n  colnames(genome_wolf2)[ncol(genome_wolf2)]<-p}\n\ngenome_wolf2<- genome_wolf2 %>% mutate(X = extr/rowSums(.[3:14],na.rm = T))\ngenome_wolf2_filter<-genome_wolf2[genome_wolf2$X>0.6 & !is.na(genome_wolf2$X),1]\nwrite.table(genome_wolf2_filter,""genome_secreted_signalp_tmhmm_wolf.txt"",sep=\'\\t\',quote = F,row.names = F,col.names=F)\n\n\n\n#write FINAL secretome fastas\ngenome_wolf2_fa<-genome_ann[genome_ann$GeneID %in% genome_wolf2_filter,c(1,20),]\ngenome_wolf2_fa<-dataframe2fas(genome_wolf2_fa,file=\'genome_secreted_signalp_tmhmm_wolf.faa\')\n\n#make a summary for the content of secretome\ngenome_wolf2_filter<-read.delim(""genome_secreted_signalp_tmhmm_wolf.txt"",header=F)[,1]\n\ngenome_secretome_summary<-data.frame()\ngenome_secretome_summary[1,1]<-length(genome_wolf2_filter)\ngenome_secretome_summary[1,2]<-length(genome_wolf2_filter)/nrow(genome_ann)\ngenome_secretome_summary[1,3]<-nrow(genome_ann[genome_ann$GeneID %in% genome_wolf2_filter & !is.na(genome_ann$CAZyme),])\ngenome_secretome_summary[1,4]<-nrow(genome_ann[genome_ann$GeneID %in% genome_wolf2_filter & !is.na(genome_ann$CAZyme),])/length(genome_wolf2_filter)\ngenome_secretome_summary[1,5]<-nrow(genome_ann[genome_ann$GeneID %in% genome_wolf2_filter & !is.na(genome_ann$Protease),])\ngenome_secretome_summary[1,6]<-nrow(genome_ann[genome_ann$GeneID %in% genome_wolf2_filter & !is.na(genome_ann$Protease),])/length(genome_wolf2_filter)\ngenome_effector<-read.delim(\'genome_effectorp.txt\',sep=\'\\t\',header=T)\ngenome_secretome_summary[1,7]<- nrow(genome_effector[genome_effector$Prediction==\'Effector\',]) \ngenome_secretome_summary[1,8]<- nrow(genome_ann[genome_ann$GeneID %in% genome_wolf2_filter & (genome_ann$Product!=\'hypothetical protein\' | !is.na(genome_ann$PFAM)) & is.na(genome_ann$CAZyme) & is.na(genome_ann$Protease),])\n\ncolnames(genome_secretome_summary)<-c(\'secretome\',\'genome_in_secretome\',\'cazy\',\'secretome_in_cazy\',\'merops\',\'secretome_in_merops\',\'effectors\',\'annotated_rest\')\nwrite.table(genome_secretome_summary,""genome_secretome_summary.txt"",sep=\'\\t\',quote = F,row.names = F)\n']","Data from: Predicted input of uncultured fungal symbionts to a lichen symbiosis from metagenome-assembled genomes Basidiomycete yeasts have recently been reported as stably associated secondary fungal symbionts (SFSs) of many lichens, but their role in the symbiosis remains unknown. Attempts to sequence their genomes have been hampered both by the inability to culture them and their low abundance in the lichen thallus alongside two dominant eukaryotes (an ascomycete fungus and chlorophyte alga). Using the lichen Alectoria sarmentosa, we selectively dissolved the cortex layer in which SFSs are embedded to enrich yeast cell abundance and sequenced DNA from the resulting slurries as well as bulk lichen thallus. In addition to yielding a near-complete genome of the filamentous ascomycete using both methods, metagenomes from cortex slurries yielded a 36- to 84-fold increase in coverage and near-complete genomes for two basidiomycete species, members of the classes Cystobasidiomycetes and Tremellomycetes. The ascomycete possesses the largest gene repertoire of the three. It is enriched in proteases often associated with pathogenicity and harbours the majority of predicted secondary metabolite clusters. The basidiomycete genomes possess 35% fewer predicted genes than the ascomycete and have reduced secretomes even compared to close relatives, while exhibiting signs of nutrient limitation and scavenging. Furthermore, both basidiomycetes are enriched in genes coding for enzymes producing secreted acidic polysaccharides, representing a potential contribution to the shared extracellular matrix. All three fungi retain genes involved in dimorphic switching, despite the ascomycete not being known to possess a yeast stage. The basidiomycete genomes are an important new resource for exploration of lifestyle and function in fungal-fungal interactions in lichen symbioses.",0
VGEA: A Snakemake Pipeline for Virus Genome Assembly from Next Generation Sequencing Data,"We describe VGEA (Viral Genomes Easily Assembled), a snakemake workflow for advanced assembly of viral genomes from NGS data. VGEA enables users to split bam or fastq files into forward and reverse reads, carry out de novo assembly of forward and reverse reads to generate contigs, pre-process reads for quality and contamination, and map reads to a reference tailored to the sample using corrected contigs supplemented by the users choice of reference sequences. VGEA is freely available on GitHub at: https://github.com/pauloluniyi/VGEA under the GNU General Public License.","['#!/usr/bin/env Rscript\n\n# Thanks to Matthew Hall, the code\'s original author.\n\nlist.of.packages <- c(""ggplot2"", ""reshape"", ""grid"", ""gridExtra"", ""gtable"", ""scales"", ""argparse"", ""assertthat"")\nnew.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,""Package""])]\nif(length(new.packages)){\n  cat(""Missing dependencies; replacing the path below appropriately, run\\nsudo [path to your shiver code]/tools/AlignmentPlotting_PackageInstall.R\\nthen try again.\\n"")\n  quit(save=""no"", status=1)\n}\n\n\nsuppressMessages(require(ggplot2, quietly=TRUE, warn.conflicts=FALSE))\nsuppressMessages(require(reshape, quietly=TRUE, warn.conflicts=FALSE))\nsuppressMessages(require(grid, quietly=TRUE, warn.conflicts=FALSE))\nsuppressMessages(require(gridExtra, quietly=TRUE, warn.conflicts=FALSE))\nsuppressMessages(require(gtable, quietly=TRUE, warn.conflicts=FALSE))\nsuppressMessages(require(scales, quietly=TRUE, warn.conflicts=FALSE))\nsuppressMessages(require(argparse, quietly=TRUE, warn.conflicts=FALSE))\nsuppressMessages(require(assertthat, quietly=TRUE, warn.conflicts=FALSE))\n\n\n\n# Miseq\ngene.height <- 3.2\nother.seq.height = 3.5\ngene.font.size <- 2.7\nv.loop.font.size <- 2.5\nplot.relative.heights <- c(2,4,6.5)\nplot.width <- 13\nplot.total.height <- 3.8\nlegend.position <- ""none""\n\n# Hiseq\n#gene.height <- 3.2\n#other.seq.height <- 3.6\n#gene.font.size <- 2.7\n#v.loop.font.size <- 2.5\n#plot.relative.heights <- c(2,5,6.5)\n#plot.width <- 13\n#plot.total.height <- 5\n#legend.position <- ""none""\n\n\narg_parser = ArgumentParser(description=paste(""Use this script to produce a"",\n""plot showing an alignment of sequences, genes relative to those sequences,"",\n""and the coverage obtained by mapping to two of those sequences (the alignment"",\n""may contain additional sequences with those two). NOTE: I am not"",\n""able to offer support for this script as I am for all of the other code in"",\n""shiver. It was written, not by me, with the intention of one-off use for the"",\n""figures in the shiver paper. In my limited experience of R code I find it"",\n""produces incomprehensible error messages; I wish you luck debugging this code"",\n""if it doesn\'t work for you. Also note that there are lots of plotting"",\n""parameters - sizes, placements, colours etc. - you can modify these by"",\n""opening this file and modifying the code.""))\n\narg_parser$add_argument(""coverageFile"",\nhelp=paste(""A csv file of the format output by shiver\'s"",\n""tools/AlignBaseFreqFiles.py script with the option --coverage-only. i.e. the"",\n""first column should be position in the alignment,"",\n""and the fourth and fifth columns should be coverage with respect"",\n""to the two references. (The first line of the csv should be the field"",\n""names).""))  \narg_parser$add_argument(""coloursFile"", help=""A csv file produced by running shiver\'s tools/ConvertAlnToColourCodes.py on your alignment."")  \narg_parser$add_argument(""GeneCoordsFile"", help=\'A csv file with the following fields: ""ReadingFrame,GeneName,StartPos,EndPos"". The start and end positions should be in the coordinates of your alignment. An optional extra field ""OnTop"" should contain binary values ""yes"" or ""no"", specifying whether that gene should be displayed on top of another gene at the same position (we use yes values for putting loop regions of the env gene on top of env itself). If your alignment contains HXB2, you could create this file by running a command like this (assuming your shiver code lives in ~/shiver/): echo ""ReadingFrame,GeneName,StartPos,EndPos"" > MyGeneCoordsFile.csv; while read line; do read name RF seq <<< $(echo $line); start=$(~/shiver/tools/FindSubSeqsInAlignment.py ""$i"" B.FR.83.HXB2_LAI_IIIB_BRU.K03455 --start ""$seq"" --alignment-coords); end=$(~/shiver/tools/FindSubSeqsInAlignment.py ""$i"" B.FR.83.HXB2_LAI_IIIB_BRU.K03455 --end ""$seq"" --alignment-coords); echo ""$RF,$name,$start,$end""; done < ~/shiver/info/HXB2_GeneName_ReadingFrame_Sequence.txt >> MyGeneCoordsFile.csv\')  \narg_parser$add_argument(""outputFile"", help=\'(Will be in pdf format.)\')  \n\nargs <- arg_parser$parse_args()\n\ncov.file.name <- args$coverageFile\ncol.file.name <- args$coloursFile\ngen.file.name <- args$GeneCoordsFile\nout.file.name <- args$outputFile\n\nAlignPlots <- function(...) {\n  LegendWidth <- function(x) x$grobs[[8]]$grobs[[1]]$widths[[4]]\n  \n  plots.grobs <- lapply(list(...), ggplotGrob)\n  \n  max.widths <- do.call(unit.pmax, lapply(plots.grobs, ""[["", ""widths""))\n  plots.grobs.eq.widths <- lapply(plots.grobs, function(x) {\n    x$widths <- max.widths\n    x\n  })\n  \n  legends.widths <- lapply(plots.grobs, LegendWidth)\n  max.legends.width <- do.call(max, legends.widths)\n  plots.grobs.eq.widths.aligned <- lapply(plots.grobs.eq.widths, function(x) {\n    if (is.gtable(x$grobs[[8]])) {\n      x$grobs[[8]] <- gtable_add_cols(x$grobs[[8]],\n                                      unit(abs(diff(c(LegendWidth(x),\n                                                      max.legends.width))),\n                                           ""mm""))\n    }\n    x\n  })\n  \n  plots.grobs.eq.widths.aligned\n}\n\nprocess.string <- function(string){\n  anything.exists <- F\n  \n  gs <- gregexpr(""g*"", string)\n  lengths <- attr(gs[[1]], ""match.length"")\n  nonzero.lengths <- which(lengths!=0)\n  if(length(nonzero.lengths)>0){\n    starts <- gs[[1]][nonzero.lengths]\n    display.starts <- starts - 0.5\n    ends <- starts + lengths[nonzero.lengths] - 1\n    display.ends <- ends + 0.5\n    out.g <- data.frame(start = starts, end = ends, display.start = display.starts, display.end = display.ends, char = ""g"")\n    \n    anything.exists <- T\n    out <- out.g\n  }\n  \n  ds <- gregexpr(""d*"", string)\n  lengths <- attr(ds[[1]], ""match.length"")\n  nonzero.lengths <- which(lengths!=0)\n  if(length(nonzero.lengths)>0){\n    starts <- ds[[1]][nonzero.lengths]\n    display.starts <- starts - 0.5\n    ends <- starts + lengths[nonzero.lengths] - 1\n    display.ends <- ends + 0.5\n    out.d <- data.frame(start = starts, end = ends, display.start = display.starts, display.end = display.ends, char = ""d"")\n    \n    if(anything.exists){\n      out <- rbind(out, out.d)\n    } else {\n      anything.exists <- T\n      out <- out.d\n    }\n  }\n  \n  bs <- gregexpr(""b*"", string)\n  lengths <- attr(bs[[1]], ""match.length"")\n  nonzero.lengths <- which(lengths!=0)\n  if(length(nonzero.lengths)>0){\n    starts <- bs[[1]][nonzero.lengths]\n    display.starts <- starts - 0.5\n    ends <- starts + lengths[nonzero.lengths] - 1\n    display.ends <- ends + 0.5\n    out.b <- data.frame(start = starts, end = ends, display.start = display.starts, display.end = display.ends, char = ""b"")\n    \n    if(anything.exists){\n      out <- rbind(out, out.b)\n    } else {\n      anything.exists <- T\n      out <- out.b\n    }\n  }\n  \n  return(out)\n}\n\nread.csv.and.check.col.names <- function(csv.file, all.required.col.names,\n                                         discard.unwanted.cols=FALSE,\n                                         check.names=TRUE,\n                                         stringsAsFactors=FALSE) {\n  \n  # Read the file, check all required columns present\n  assert_that(file.exists(csv.file))\n  data.frame <- read.table(csv.file, sep="","", header=T, check.names=check.names,\n  stringsAsFactors=stringsAsFactors)\n  col.names <- colnames(data.frame)\n  for (expected.col.name in all.required.col.names) {\n    if (! expected.col.name %in% col.names) {\n      stop(""Expected column name "", expected.col.name, "" missing from "",\n           csv.file)\n    }\n  }\n  \n  if (discard.unwanted.cols) {\n    data.frame <- data.frame[all.required.col.names]\n  }\n  \n  return(data.frame)\n}\n\n\ncov.data <- read.table(cov.file.name, sep="","", header=T, stringsAsFactors = F)\ncov.data <- cov.data[,c(1,4,5)]\ncomp.factors <- c(colnames(cov.data)[c(3,2)])\ncov.data <- melt(cov.data, id = 1)\ncov.data$variable <- factor(cov.data$variable, comp.factors)\n\npos.data <- read.table(col.file.name, sep="","", header=F, stringsAsFactors = F)\ncolnames(pos.data) <- c(""Name"", ""Sequence"")\npos.data$Name <- factor(pos.data$Name, rev(pos.data$Name))\n\nline.list <- lapply(pos.data$Sequence, process.string)\n\nfirst <- T\n\nfor(sequence.no in seq(1, nrow(pos.data))){\n  temp <- line.list[[sequence.no]]\n  temp$phantom.pos <- sequence.no\n  temp$sequence.name <- pos.data$Name[sequence.no]\n  if(first){\n    line.df <- temp\n    first <- F\n  } else {\n    line.df <- rbind(line.df, temp)\n  }\n  \n}\n\nann.data <- read.csv.and.check.col.names(gen.file.name, c(\'ReadingFrame\',\'GeneName\',\'StartPos\',\'EndPos\'))\nann.data$display.start <- ann.data$StartPos-0.5\nann.data$display.end <- ann.data$EndPos+0.5\nann.data <- ann.data[order(ann.data$GeneName),]\n\n\nstack.genes <- ""OnTop"" %in% colnames(ann.data)\nif (stack.genes) {\n  ann.data.main <- ann.data[which(ann.data[,5]==""no""),]\n  ann.data.extra <- ann.data[which(ann.data[,5]==""yes""),]\n} else {\n  ann.data.main <- ann.data\n}\n\ngraph.cov <- ggplot(data=cov.data)\n\ngraph.cov <- graph.cov + \n  geom_line(aes(x=Alignment.position, y=value, colour=variable, alpha=0.5)) +\n  ylab(""Coverage"") +\n  xlab(""Alignment position"") +\n  scale_x_continuous(limits=c(min(cov.data$Alignment.position)-1, max(cov.data$Alignment.position)+1), expand=c(0,100)) +\n  scale_y_log10(limits = c(1,NA), labels = comma) +\n  scale_colour_manual(values=c(""red"", ""blue"")) +\n  theme_bw() +\n  #theme(legend.title = element_blank()) +\n  theme(plot.margin = unit(c(0,0.5,0.5,0.5), ""cm""),\n  legend.position=legend.position)\n  #legend.text = element_text(size=10))\n\ngraph.ann <- ggplot()\n\ngraph.ann <- graph.ann + \n  geom_segment(data = ann.data.main, aes(y=ReadingFrame, yend = ReadingFrame, x = display.start, xend=display.end, colour=GeneName), size=gene.height) +\n  geom_text(data = ann.data.main, aes(label=GeneName, y=ReadingFrame, x=display.start), hjust=""left"", nudge_x=10, size=gene.font.size) +\n  theme_bw() +\n  scale_x_continuous(limits=c(min(cov.data$Alignment.position)-1, max(cov.data$Alignment.position)+1), expand=c(0,100)) +\n  scale_y_continuous(limits=c(0, max(ann.data$ReadingFrame)+1)) +\n  scale_color_discrete(h = c(0, 720) + 15, direction = -1) +\n  theme(axis.text=element_blank(),\n        axis.ticks=element_blank(),\n        axis.title=element_blank(),\n        legend.position=""none"",\n        panel.background=element_blank(),\n        panel.grid.major.y=element_blank(),\n        panel.grid.minor.y=element_blank(),\n        plot.background=element_blank(),\n        plot.margin=unit(c(0.5,0.5,0,0.5), ""cm""),\n        panel.margin=unit(c(0.5,0.5,0,0.5), ""cm""))\n\nif (stack.genes) {\ngraph.ann <- graph.ann +\n  geom_segment(data = ann.data.extra, aes(y=ReadingFrame, yend = ReadingFrame, x = display.start, xend=display.end), size=gene.height, alpha=0.5, colour=""grey10"") +\n  geom_text(data = ann.data.extra, aes(label=GeneName, y=ReadingFrame, x=display.start), hjust=""left"", nudge_x=10, size=v.loop.font.size, colour=""white"") \n}\n\ngraph.pos <- ggplot(data = line.df)\n\ngraph.pos <- graph.pos +\n  geom_segment(aes(y=sequence.name, yend = sequence.name, x=display.start, xend = display.end, colour=char, size=char)) +\n  theme_bw() +\n  scale_size_manual(values=c(other.seq.height,0.5,other.seq.height)) +\n  scale_color_manual(values=c(""grey85"", ""black"", ""black"")) +\n  scale_x_continuous(limits=c(min(cov.data$Alignment.position)-1, max(cov.data$Alignment.position)+1), expand=c(0,100)) +\n  theme(legend.position=""none"", \n        axis.ticks.y=element_blank(), \n        axis.title.y=element_blank(),\n        axis.title.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.text.x=element_blank(),\n        panel.grid.major.y=element_blank(),\n        panel.grid.minor.y=element_blank(), \n        plot.margin=unit(c(0,0.5,0,0.5), ""cm""),\n        panel.margin=unit(c(0,0.5,0,0.5), ""cm""))\n\nplots1 <- AlignPlots(graph.ann, graph.pos, graph.cov)\nplots1$ncol <- 1\nplots1$heights <- unit(plot.relative.heights, ""null"")\n\n# Append \'.pdf\' to the output file name if not already there.\nif (! grepl(\'.pdf$\', out.file.name)) out.file.name <- paste0(out.file.name, \'.pdf\')\n\npdf(file=out.file.name, width=plot.width, height=plot.total.height)\ndo.call(grid.arrange, plots1)\ndev.off()\n', '#!/usr/bin/env Rscript\n\nlist.of.packages <- c(""ggplot2"", ""reshape"", ""grid"", ""gridExtra"", ""gtable"", ""scales"", ""argparse"", ""assertthat"")\nnew.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,""Package""])]\nif(length(new.packages)) install.packages(new.packages, dependencies = T, repos=""http://cran.ma.imperial.ac.uk/"")\n']","VGEA: A Snakemake Pipeline for Virus Genome Assembly from Next Generation Sequencing Data We describe VGEA (Viral Genomes Easily Assembled), a snakemake workflow for advanced assembly of viral genomes from NGS data. VGEA enables users to split bam or fastq files into forward and reverse reads, carry out de novo assembly of forward and reverse reads to generate contigs, pre-process reads for quality and contamination, and map reads to a reference tailored to the sample using corrected contigs supplemented by the users choice of reference sequences. VGEA is freely available on GitHub at: https://github.com/pauloluniyi/VGEA under the GNU General Public License.",0
A digital light microscopic method for diatom surveys using embedded acid-cleaned samples,"This folder contains the supplementary data from the article submitted to MDPI Water in which we compare diatom identification with traditional light microscopy and digital microscopy that can be use as training data for the deep learning algorithms.The data contain:#R ScriptsStructured results.R #contains the comparison of the original inventories and biotic indices with ANOVA, species diversity analyses, ANOSIM, NMDS and HeatmapStructured results_reduced.R #similar setup as before only with a reduced dataset joining diatom complexes togetherTime comparison.R #comparison of times used for diatom identification with digital and traditional light microscopyANOVA pairwise.R #Contains the pairwise comparisons in plot form#Datasets##Structured resultsDiatomInv SlScMic_totalBioindex SlScMic_total.txt##Structured results reducedOMNIDIA.txt##ANOVA pairwiseDiatomInv SlScMic_summ.txt##TimeTime comparison.txtCopy of Time comparison.txtCopy of Copy of Time comparison_with per sample.txt#Images##Contains subfolders of triplicate slides and labelled diatom valves 6 samplesMenne_May2020_M1_Flowing.zip #contains triplicates of samples of the Menne River source (Karst) with low flowMenne_May2020_M2_Flowing.zip #contains triplicats of samples of the unnamed River tributary (Karst) with low flowMenne_May2020_M3_Flowing.zip #contains triplicates of samples of the Menne River after tributary (Karst) with mid-flowMenne_May2020_M4_Pool.zip #contains triplicates of samples of the pool of the Menne River (Karst) in mid-low reachMenne_May2020_M5_Dry.zip #contains triplicates of the dry lower reach of the Menne River (Karst)Menne_May2020_M6_Pool.zip #contains triplicates of samples of the pool of the Menne River (Karst) in the mid reach",,"A digital light microscopic method for diatom surveys using embedded acid-cleaned samples This folder contains the supplementary data from the article submitted to MDPI Water in which we compare diatom identification with traditional light microscopy and digital microscopy that can be use as training data for the deep learning algorithms.The data contain:#R ScriptsStructured results.R #contains the comparison of the original inventories and biotic indices with ANOVA, species diversity analyses, ANOSIM, NMDS and HeatmapStructured results_reduced.R #similar setup as before only with a reduced dataset joining diatom complexes togetherTime comparison.R #comparison of times used for diatom identification with digital and traditional light microscopyANOVA pairwise.R #Contains the pairwise comparisons in plot form#Datasets##Structured resultsDiatomInv SlScMic_totalBioindex SlScMic_total.txt##Structured results reducedOMNIDIA.txt##ANOVA pairwiseDiatomInv SlScMic_summ.txt##TimeTime comparison.txtCopy of Time comparison.txtCopy of Copy of Time comparison_with per sample.txt#Images##Contains subfolders of triplicate slides and labelled diatom valves 6 samplesMenne_May2020_M1_Flowing.zip #contains triplicates of samples of the Menne River source (Karst) with low flowMenne_May2020_M2_Flowing.zip #contains triplicats of samples of the unnamed River tributary (Karst) with low flowMenne_May2020_M3_Flowing.zip #contains triplicates of samples of the Menne River after tributary (Karst) with mid-flowMenne_May2020_M4_Pool.zip #contains triplicates of samples of the pool of the Menne River (Karst) in mid-low reachMenne_May2020_M5_Dry.zip #contains triplicates of the dry lower reach of the Menne River (Karst)Menne_May2020_M6_Pool.zip #contains triplicates of samples of the pool of the Menne River (Karst) in the mid reach",0
Prediction of evolutionary constraint by genomic annotations improves functional prioritization of genomic variants in maize,"Software and scripts used for the manuscript ""Prediction of evolutionary constraint by genomic annotations improves functional prioritization of genomic variants in maize"" (Guillaume P. Ramstein, Edward S. Buckler; 2022).The pipeline introduced in the manuscript (PICNC: Prediction of mutation Impact by Calibrated Nucleotide Conservation) predicts phylogenetic nucleotide conservation for nonsynonymous mutations in maize, by genomic annotations derived from bioinformatics and protein representation by UniRep (Alley et al. 2018, Alley et al. 2019).Supporting data: https://www.doi.org/10.25739/hybz-2957Scripts and additional information: https://bitbucket.org/bucklerlab/snpconstraintprediction/src/master/",,"Prediction of evolutionary constraint by genomic annotations improves functional prioritization of genomic variants in maize Software and scripts used for the manuscript ""Prediction of evolutionary constraint by genomic annotations improves functional prioritization of genomic variants in maize"" (Guillaume P. Ramstein, Edward S. Buckler; 2022).The pipeline introduced in the manuscript (PICNC: Prediction of mutation Impact by Calibrated Nucleotide Conservation) predicts phylogenetic nucleotide conservation for nonsynonymous mutations in maize, by genomic annotations derived from bioinformatics and protein representation by UniRep (Alley et al. 2018, Alley et al. 2019).Supporting data: https://www.doi.org/10.25739/hybz-2957Scripts and additional information: https://bitbucket.org/bucklerlab/snpconstraintprediction/src/master/",0
Polish is quantitatively different on quartzite flakes used on different worked materials [R analysis],"This upload includes the following files related to the R analysis:- Raw data as a CSV table (processing-quartzite-final.csv), i.e. results from the ConfoMap analysis (see https://doi.org/10.5281/zenodo.3979116)- RStudio project (Quantification quartzite final.Rproj)- R scripts as R Markdown files (*.Rmd)- R scripts knitted to HTML files (*.html)- An R script (RStudioVersion.R) to write the used version of RStudio to a text file (RStudioVersion.txt)- Output from script #1: processing-quartzite-final.Rbin and processing-quartzite-final.xlsx- Output from script #2: processing-quartzite-final_summary-stats.xlsx- Output from script #3: all plots as PDF files.Note that for running the scripts, the raw data files (processing-quartzite-final.csv, .Rbin and .xlsx) should be stored in a ""Data"" folder within the working directory.Output (processing-quartzite-final_summary-stats.xlsx and PDF plots) were saved into ""Summary-stats"" and ""Plots"" folders, respectively.Zenodo does not allow sub-folders, so this folder structure had to be removed.Instructions to download all files at once are given here: https://doi.org/10.5281/zenodo.4011952","['#written by IC, 08.04.2020\r\n\r\n#Output current version of RStudio to a text file for reporting purposes.\r\n\r\nvers <- as.character(RStudio.Version()$version)\r\nwriteLines(c(vers, ""\\n""), ""RStudioVersion.txt"")\r\n']","Polish is quantitatively different on quartzite flakes used on different worked materials [R analysis] This upload includes the following files related to the R analysis:- Raw data as a CSV table (processing-quartzite-final.csv), i.e. results from the ConfoMap analysis (see https://doi.org/10.5281/zenodo.3979116)- RStudio project (Quantification quartzite final.Rproj)- R scripts as R Markdown files (*.Rmd)- R scripts knitted to HTML files (*.html)- An R script (RStudioVersion.R) to write the used version of RStudio to a text file (RStudioVersion.txt)- Output from script #1: processing-quartzite-final.Rbin and processing-quartzite-final.xlsx- Output from script #2: processing-quartzite-final_summary-stats.xlsx- Output from script #3: all plots as PDF files.Note that for running the scripts, the raw data files (processing-quartzite-final.csv, .Rbin and .xlsx) should be stored in a ""Data"" folder within the working directory.Output (processing-quartzite-final_summary-stats.xlsx and PDF plots) were saved into ""Summary-stats"" and ""Plots"" folders, respectively.Zenodo does not allow sub-folders, so this folder structure had to be removed.Instructions to download all files at once are given here: https://doi.org/10.5281/zenodo.4011952",0
Analysis code for GSE186453 and GSE186452,"Analysis code related to NCBI GEO datasets GSE186453 (RNA-seq) and GSE186452 (ChIP-seq). For RNA-seq analysis, custom functions leveraging the limma/voom and fgsea packages were used. The custom functions were designed for compatibility with the Roche/Genentech bioinformatics portal, and will not operate correctly outside of this environment. (Per the journal's request, we have deposited the analysis code as is.) However, comparable analyses can be performed using the publically-available parent packages listed above. For ChIP-seq analysis, all code used publically-available R packages, including DiffBind, DESeq2, and profileplyr.Associated with: J. Liang et al., Giredestrant counters a progesterone hypersensitivity program driven by estrogen receptor mutations in breast cancer. Science Translational Medicine 14, eabo5959 (2022)",,"Analysis code for GSE186453 and GSE186452 Analysis code related to NCBI GEO datasets GSE186453 (RNA-seq) and GSE186452 (ChIP-seq). For RNA-seq analysis, custom functions leveraging the limma/voom and fgsea packages were used. The custom functions were designed for compatibility with the Roche/Genentech bioinformatics portal, and will not operate correctly outside of this environment. (Per the journal's request, we have deposited the analysis code as is.) However, comparable analyses can be performed using the publically-available parent packages listed above. For ChIP-seq analysis, all code used publically-available R packages, including DiffBind, DESeq2, and profileplyr.Associated with: J. Liang et al., Giredestrant counters a progesterone hypersensitivity program driven by estrogen receptor mutations in breast cancer. Science Translational Medicine 14, eabo5959 (2022)",0
Morphological phylogenetics evaluated using novel evolutionary simulations,"Supplementary data from the following publication:Keating, J.N., Sansom, R.S., Sutton, M.D., Knight, Chris G., and Garwood, R.J. Morphological phylogenetics evaluated using novel evolutionary simulations. Systematic Biology. This comprises: exemplar outputs from both the TREvoSim and MBL2017 software; the R functions employed herein for node-based distance measures; the scripts used for analysis for this study; output codes for the current study for use with the TREvoSim logging system, and the modified source code for the USPR software used herein.USPR was coded by Chris Whidden and Frederick Matsen, and published under a GNU liscense. The software is described in the following publication:Whidden C., Matsen F. 2018. Calculating the Unrooted Subtree Prune-and-Regraft Distance. IEEE/ACM Transactions on Computational Biology and Bioinformatics. 16(3):898911.","['install.packages(""phangorn"")\ninstall.packages(""phytools"")\ninstall.packages(""phylobase"")\nlibrary(phangorn)\nlibrary(phytools)\nlibrary(phylobase)\n\n\n\n###################################################################################################\n\n#Function to compute the branch length distance and binary character difference for each pair of taxa within a tree\n#tree = an object of class phylo\n#morph = a binary character matrix of class phydat\n\ncoherence <- function(tree, morph){\n\tchar_n <- length(morph[[1]])\n\tTN <- Ntip(tree)-1\n\tlistOne <- tree$tip.label[c(1:TN)]\n\ttaxon_distances <- data.frame(Taxon_A = 0, Taxon_B = 0, seq_distance = 0, BL_distance = 0)\n\tfor(x in 1:length(listOne)){\n\t\tlistTwo <-  tree$tip.label[c((x+1):length( tree$tip.label))]\n\t\tfor(i in 1:length(listTwo)){\n\t\t\tseq_diff <- as.numeric(unlist( morph[(listOne[x])])) - as.numeric(unlist( morph[(listTwo[i])]))\n\t\t\tseq_distance <- length(seq_diff) - sum(seq_diff == 0)\n\t\t\tMRC_ancestor <- MRCA( tree, c(listOne[x], listTwo[i]))\n\t\t\tBL_distance <- (nodeheight( tree, match(listOne[[x]],  tree$tip.label)) - nodeheight( tree, MRC_ancestor)) + (nodeheight( tree, match(listTwo[[i]],  tree$tip.label)) - nodeheight( tree, MRC_ancestor))\n\t\t\tnewrow <- data.frame(Taxon_A = listOne[x], Taxon_B = listTwo[i], seq_distance = seq_distance, BL_distance = BL_distance)\n\t\t\ttaxon_distances <- rbind(taxon_distances, newrow)}}\t\t\n\ttaxon_distances <- taxon_distances[-c(1), ]\n\treturn(taxon_distances)}\n\t\n###################################################################################################\n\t\n#example script to demonstarte compution of raw and adjusted morphological coherence\n\t#create a random tree\n\t\ttree1 <- rtree(30)\n\t#create a random binary character matrix comprising 200 characters\n\t\tmorph.m <- matrix(0, Ntip(tree1), 200)\n\t\trownames(morph.m) <- tree1$tip.label\n\t\tfor(a in 1:Ntip(tree1)){\n\t\t\tfor(b in 1:200){\n\t\t\t\tmorph.m[[a, b]] <- sample(0:1, 1)\n\t\t\t}\n\t\t}\n\t\tphydat_m <- as.phyDat(morph.m, type=""USER"", levels = c(0, 1))\n\t#compute taxon pair distances\n\t\tdistances <- coherence(tree1, phydat_m)\n\t#compute Spearmans Rank correlation (raw morphological coherence)\n\t\tcor(distances$seq_distance, distances$BL_distance, method = ""spearman"")\n\t#strip taxon pairs with long branches\n\t\tdistances2 <- subset(distances, BL_distance <= (max(distances$BL_distance))/2, select = )\n\t#compute Spearmans Rank correlation (adjusted morphological coherence)\n\t\tcor(distances2$seq_distance, distances2$BL_distance, method = ""spearman"")\n\t\t\n###################################################################################################\n\n#Function to compute stemminess of a tree (Fiala & Sokal, 1985)\n\n#tree = an object of class phylo\n\nstemminess <- function(tree){\n\tNodes <- Nnode(tree)\n\tTips <- Ntip(tree)\n\tEdges <- tree$edge.length\n\ttree2 <- phylo4(tree)\n\tsim_df <- head(tree2, (Tips+Nodes+1))\n\tstemminess <- data.frame(node = 0, values = 0)\n\tfor(i in (Tips + 2):(Tips + Nodes)){\n\t\tdec <- descendants(tree2, i, ""all"")\n\t\tvalues <- sim_df$edge.length[[i]]\n\t\tfor(j in dec){\n\t\t\tvalues <- c(values, sim_df$edge.length[[j]])\n\t\t\t}\n\t\tnewrow <- data.frame(node = i, values = values[[1]]/sum(values))\n\t\tstemminess <- rbind(stemminess, newrow)}\n\tstemminess <- stemminess[-c(1), ] \n\tmean_s <- mean(stemminess$values)\t\n\treturn(mean_s)}\n\t\t\n###################################################################################################\n\n#function to calculate the number of bipartitions shared between 2 unrooted trees. Symmetric metric. For unrooted trees, shared nodes = shared bibartitions + 1. \n\nSB <- function(tree1, tree2){\n\ttree1 <- unroot(tree1)\n\ttree2 <- unroot(tree2)\n\tRF <- RF.dist(tree1, tree2)\n\tpart_all <- (Nnode(tree1) -1) + (Nnode(tree2) -1)\n\treturn((part_all - RF)/2)}\n\n###################################################################################################\n\n#function to calculate the number of bipartitions found in unrooted tree 1 that are not found in unrooted tree 2. Asymmetric metric. For unrooted trees, unique nodes = unique bibartitions. \n   \nUB <- function(tree1, tree2){\n\ttree1 <- unroot(tree1)\n\ttree2 <- unroot(tree2)\n\tRF <- RF.dist(tree1, tree2)\n\tpart1 <- (Nnode(tree1) -1)\n\tpart2 <- (Nnode(tree2) -1)\n\tpart_all <- (Nnode(tree1) -1) + (Nnode(tree2) -1)\n\tSB <- ((part_all - RF)/2)\n\treturn(part1 - SB)}\n']","Morphological phylogenetics evaluated using novel evolutionary simulations Supplementary data from the following publication:Keating, J.N., Sansom, R.S., Sutton, M.D., Knight, Chris G., and Garwood, R.J. Morphological phylogenetics evaluated using novel evolutionary simulations. Systematic Biology. This comprises: exemplar outputs from both the TREvoSim and MBL2017 software; the R functions employed herein for node-based distance measures; the scripts used for analysis for this study; output codes for the current study for use with the TREvoSim logging system, and the modified source code for the USPR software used herein.USPR was coded by Chris Whidden and Frederick Matsen, and published under a GNU liscense. The software is described in the following publication:Whidden C., Matsen F. 2018. Calculating the Unrooted Subtree Prune-and-Regraft Distance. IEEE/ACM Transactions on Computational Biology and Bioinformatics. 16(3):898911.",0
Data from: Genetic signatures of ecological diversity along an urbanization gradient,"Despite decades of work in environmental science and ecology, estimating human influences on ecosystems remains challenging. This is partly due to complex chains of causation among ecosystem elements, exacerbated by the difficulty of collecting biological data at sufficient spatial, temporal, and taxonomic scales. Here, we demonstrate the utility of environmental DNA (eDNA) for quantifying associations between human land use and changes in an adjacent ecosystem. We analyze metazoan eDNA sequences from water sampled in nearshore marine eelgrass communities and assess the relationship between these ecological communities and the degree of urbanization in the surrounding watershed. Counter to conventional wisdom, we find strongly increasing richness and decreasing beta diversity with greater urbanization, and similar trends in the diversity of life histories with urbanization. We also find evidence that urbanization influences nearshore communities at local (hundreds of meters) rather than regional (tens of km) scales. Given that different survey methods sample different components of an ecosystem, we then discuss the advantages of eDNAwhich we use here to detect hundreds of taxa simultaneouslyas a complement to traditional ecological sampling, particularly in the context of broad ecological assessments where exhaustive manual sampling is impractical. Genetic data are a powerful means of uncovering human-ecosystem interactions that might otherwise remain hidden; nevertheless, no sampling method reveals the whole of a biological community.",,"Data from: Genetic signatures of ecological diversity along an urbanization gradient Despite decades of work in environmental science and ecology, estimating human influences on ecosystems remains challenging. This is partly due to complex chains of causation among ecosystem elements, exacerbated by the difficulty of collecting biological data at sufficient spatial, temporal, and taxonomic scales. Here, we demonstrate the utility of environmental DNA (eDNA) for quantifying associations between human land use and changes in an adjacent ecosystem. We analyze metazoan eDNA sequences from water sampled in nearshore marine eelgrass communities and assess the relationship between these ecological communities and the degree of urbanization in the surrounding watershed. Counter to conventional wisdom, we find strongly increasing richness and decreasing beta diversity with greater urbanization, and similar trends in the diversity of life histories with urbanization. We also find evidence that urbanization influences nearshore communities at local (hundreds of meters) rather than regional (tens of km) scales. Given that different survey methods sample different components of an ecosystem, we then discuss the advantages of eDNAwhich we use here to detect hundreds of taxa simultaneouslyas a complement to traditional ecological sampling, particularly in the context of broad ecological assessments where exhaustive manual sampling is impractical. Genetic data are a powerful means of uncovering human-ecosystem interactions that might otherwise remain hidden; nevertheless, no sampling method reveals the whole of a biological community.",0
Towards establishing extracellular vesicle-associated RNAs as biomarkers for HER2+ breast cancer,"Extracellular vesicles (EVs) are emerging as key players in breast cancer progression and hold immense promise as cancer biomarkers. However, difficulties in obtaining sufficient quantities of EVs for the identification of potential biomarkers hampers progress in this area. To circumvent this obstacle, we cultured BT-474 breast cancer cells in a two-chambered bioreactor with CDM-HD serum replacement to significantly improve the yield of cancer cell-associated EVs and eliminate bovine EV contamination. Cancer-relevant mRNAs BIRC5 (Survivin) and YBX1, as well as long-noncoding RNAs HOTAIR, ZFAS1, and AGAP2-AS1 were detected in BT-474 EVs by quantitative RT-PCR. Bioinformatics meta-analyses showed that BIRC5 and HOTAIR RNAs were substantially upregulated in breast tumours compared to non-tumour breast tissue, warranting further studies to explore their usefulness as biomarkers in patient EV samples. We envision this effective procedure for obtaining large amounts of cancer-specific EVs will accelerate discovery of EV-associated RNA biomarkers for cancers including HER2+ breast cancer.","['\r\n#>> Installing and loading required packages ----------------------------------------------------\r\n\r\ninstall.packages(""pacman"")\r\nlibrary(pacman)\r\np_load(ggplot2, readxl, plyr, reshape2, viridis, effsize, ggsci)\r\n\r\n#>> Loading and pre-processing the EV dataset ----------------------------------------------------\r\n\r\ndownload.file(""https://datadryad.org/stash/downloads/file_stream/763633"", ""EV.xlsx"", mode = ""wb"")\r\nEV <-  as.data.frame(read_excel(""EV.xlsx""))\r\nEV[,2:4] <- lapply(EV[,2:4], as.factor)\r\nEV <- EV[,c(1:7,11:12,8:10,13:14)]\r\n\r\nEV.2C <- melt(EV, id.vars=names(EV)[2:4], measure.vars = names(EV)[5:12],\r\n              variable.name = ""Gene"", value.name = ""Expr"")\r\nEV.2C$Type <- EV.2C$Gene\r\nlevels(EV.2C$Type) <- rep(c(""Protein coding"",""lncRNA""), c(5,3))\r\nlevels(EV.2C$Gene)[8] <- ""AGAP2-\\nAS1""\r\n\r\n#>> Rendering Figure 2B ---------------------------------------------------------\r\n\r\nSIZE = 10\r\n\r\nggplot(ddply(melt(EV, id.vars=names(EV)[2:4], measure.vars = names(EV)[5:12],\r\n                  variable.name = ""Gene"", value.name = ""Expr""),\r\n                  .(Gene, X_study, cat2), plyr::summarize, Expr = median(Expr)), aes(x = X_study, y = cat2)) +\r\n  \r\n  geom_point(shape=16, aes(colour = Expr, size = Expr^2)) +\r\n  \r\n  labs(x = """",y = """", title = """") +\r\n  facet_wrap(.~Gene, nrow = 1, scales = ""fixed"") +\r\n  \r\n  scale_x_discrete(labels = c(""non-tumour"",""tumour""), expand = c(0.9,0)) +\r\n  scale_color_viridis_c(begin = 0, end = 1, direction = -1, option = ""B"", alpha = 0.9, na.value = ""white"",\r\n                        limits = c(0,18.7), breaks=c(0,11.6,18.7), labels=c(0,11.6,18.7)) +\r\n  scale_size(name="""", guide=""legend"", range = c(1,5.5),\r\n             limits = c(0,18.7)^2, breaks=c(0,11.6,18.7)^2, labels=c(0,12,19))+\r\n  guides(size = guide_legend(title.position=""left"",title.hjust = 0.5,order = 1, label.position = ""bottom""),\r\n         color = guide_colorbar(barwidth = 2.4, title = NULL, label = FALSE)) +\r\n  \r\n  theme(text=element_text(size=SIZE, color = ""grey10"", face = ""plain""),\r\n      axis.text.x=element_text(size=SIZE*0.65, color = ""grey20"", angle=45,hjust=1,vjust=1),\r\n      axis.ticks=element_line(colour=""grey25"",size=0.05)) +\r\n  theme(panel.grid = element_line(color=""grey90"",size=0.1))+\r\n  theme(legend.key.size=unit(0.5,""lines""),legend.position = c(-0.17,1.05), legend.direction = ""horizontal"",\r\n        legend.box = ""vertical"", legend.box.just = ""left"",\r\n        legend.background=element_blank(),\r\n        legend.key=element_blank(),legend.text=element_text(size=SIZE*0.7, face=""plain""),\r\n        legend.spacing = unit(0,""mm""), legend.margin = margin(0,0,0,0,""mm""))+\r\n  theme(strip.background=element_rect(fill= ""grey60""),\r\n        strip.text.x=element_text(color=""white"",face=""bold"",size=SIZE*0.8,angle=90,vjust=0.5,hjust=0))+\r\n  theme(panel.spacing = unit(0.1,""lines""),panel.background = element_rect(fill=NA),\r\n        panel.border=element_rect(fill=NA,color=NA))\r\n\r\n  ggsave(file = paste0(Sys.Date(),""_Figure_2B.pdf""), dpi=300, scale = 1,\r\n         width=8, height=15, units=""cm"")\r\n\r\n#>> Rendering Figure 2C and calculating the Hedges g effect sizes  -----------------------------------------\r\n  \r\n# Defining the split violin plot visualisation\r\n  \r\nGeomSplitViolin <- ggproto(""GeomSplitViolin"", GeomViolin, \r\n                             draw_group = function(self, data, ..., draw_quantiles = NULL) {\r\n                               data <- transform(data, xminv = x - violinwidth * (x - xmin), xmaxv = x + violinwidth * (xmax - x))\r\n                               grp <- data[1, ""group""]\r\n                               newdata <- plyr::arrange(transform(data, x = if (grp %% 2 == 1) xminv else xmaxv), if (grp %% 2 == 1) y else -y)\r\n                               newdata <- rbind(newdata[1, ], newdata, newdata[nrow(newdata), ], newdata[1, ])\r\n                               newdata[c(1, nrow(newdata) - 1, nrow(newdata)), ""x""] <- round(newdata[1, ""x""])\r\n                               \r\n                               if (length(draw_quantiles) > 0 & !scales::zero_range(range(data$y))) {\r\n                                 stopifnot(all(draw_quantiles >= 0), all(draw_quantiles <=\r\n                                                                           1))\r\n                                 quantiles <- ggplot2:::create_quantile_segment_frame(data, draw_quantiles)\r\n                                 aesthetics <- data[rep(1, nrow(quantiles)), setdiff(names(data), c(""x"", ""y"")), drop = FALSE]\r\n                                 aesthetics$alpha <- rep(1, nrow(quantiles))\r\n                                 both <- cbind(quantiles, aesthetics)\r\n                                 quantile_grob <- GeomPath$draw_panel(both, ...)\r\n                                 ggplot2:::ggname(""geom_split_violin"", grid::grobTree(GeomPolygon$draw_panel(newdata, ...), quantile_grob))\r\n                               }\r\n                               else {\r\n                                 ggplot2:::ggname(""geom_split_violin"", GeomPolygon$draw_panel(newdata, ...))\r\n                               }\r\n                             })\r\n  \r\ngeom_split_violin <- function(mapping = NULL, data = NULL, stat = ""ydensity"", position = ""identity"", ..., \r\n                                draw_quantiles = NULL, trim = TRUE, scale = ""area"", na.rm = FALSE, \r\n                                show.legend = NA, inherit.aes = TRUE) {\r\n    layer(data = data, mapping = mapping, stat = stat, geom = GeomSplitViolin, \r\n          position = position, show.legend = show.legend, inherit.aes = inherit.aes, \r\n          params = list(trim = trim, scale = scale, draw_quantiles = draw_quantiles, na.rm = na.rm, ...))\r\n}\r\n\r\n# Plotting\r\n\r\nSIZE = 9\r\n\r\nggplot(subset(EV.2C, cat2 == ""Breast""), aes(x= Gene, y=Expr, colour = X_study, fill = X_study)) +\r\n  theme_minimal(base_size = 14) +\r\n  \r\n  geom_split_violin(trim = FALSE, scale = ""width"", adjust = 0.5, size = 0.1,  width = 1) +\r\n  stat_summary(fun = mean, geom = ""point"", size = 2, shape = 23, stroke = 0.5, alpha = 1,\r\n               show.legend = FALSE, fill = ""white"",\r\n               position = position_dodge(width = 0)) +\r\n  \r\n  facet_grid(.~Type, scales = ""free_x"", space = ""free"") +\r\n  \r\n  scale_fill_jco(name = ""Breast tissue: "", alpha = 0.3, labels = c(""non-tumour (n=178) "",""tumour (n=1099)"")) +\r\n  scale_color_jco(guide = FALSE, alpha = 0.9) +\r\n  scale_y_continuous(position = ""left"", expand = c(0,0.5), minor_breaks = NULL) +\r\n  scale_x_discrete(position = ""top"")+\r\n  \r\n  guides(fill = guide_legend(override.aes = list(color = adjustcolor(get_palette(""jco"",2), alpha.f = 0.8),\r\n                                                 fill = adjustcolor(get_palette(""jco"",2), alpha.f = 0.5)),\r\n                             title.position = ""left"", title.vjust = 0.5)) +\r\n  \r\n  theme(strip.background=element_rect(fill= ""grey90"", color = ""white"", size = 1),\r\n        strip.text.x=element_text(color=""black"",face=""bold"",size=SIZE,angle=0,vjust=0.5,hjust=0.5)) +\r\n  theme(panel.spacing = unit(0,""lines""),panel.background = element_rect(fill=NA, color = NA),\r\n        panel.border=element_rect(fill=NA,color=NA)) +\r\n  theme(axis.ticks=element_line(colour=""grey35"",size=0.1), axis.ticks.length = unit(3,""pt""),\r\n        panel.grid.major.x = element_line(color=""grey80"",size=0.2, linetype = 2), \r\n        text = element_text(colour=""grey35"", size = 12),\r\n        panel.grid.major.y = element_blank(),\r\n        legend.title = element_text(size = 12), axis.title.y = element_text(size = 12),\r\n        axis.text.x = element_text(face = ""plain"", vjust = 0, hjust = 0.5, size = 10, color = ""black""),\r\n        legend.position = ""bottom"", legend.direction = ""horizontal"", legend.key.size=unit(1,""lines"")) +\r\n  labs(x = NULL, y = expression(""log""[2]*"" normalised RNA count""), title = """") +\r\n  coord_cartesian(ylim = c(-3,20), clip = ""off"") +\r\n  \r\n# Hedges g effect size calculation and plot labeling  \r\n  \r\n  geom_label(data = setNames(cbind(ddply(subset(EV.2C, cat2 == ""Breast""),.(Type, Gene),\r\n                                         function(z) {cohen.d(Expr~X_study, data = z,\r\n                                                              hedges.correction = TRUE, paired=FALSE)}[[""estimate""]]*-1), value = -3.4),\r\n                             c(""Type"", ""Gene"",""lab"",""Expr"")), aes(label = sprintf(""%0.2f"", round(lab, digits = 2)),\r\n                                                                  x=Gene,y=Expr),\r\n             colour = ""grey50"", inherit.aes = F, size = 3.5, label.size = unit(0,""lines""), label.padding = unit(2,""pt""),\r\n             parse = T) +\r\n  geom_text(data = subset(EV.2C, cat2 == ""Breast"" & Gene %in% c(""EPCAM"") )[1,],\r\n            x = 0.2, y=-3.2, label = ""Hedges g:"", color = ""grey 50"", size = 3.5)\r\n\r\n# Saving the plot\r\n\r\nggsave(file = paste0(Sys.Date(),""_Figure_2C.pdf""), dpi=300, scale = 1,\r\n       width=16, height=10, units=""cm"")\r\n']","Towards establishing extracellular vesicle-associated RNAs as biomarkers for HER2+ breast cancer Extracellular vesicles (EVs) are emerging as key players in breast cancer progression and hold immense promise as cancer biomarkers. However, difficulties in obtaining sufficient quantities of EVs for the identification of potential biomarkers hampers progress in this area. To circumvent this obstacle, we cultured BT-474 breast cancer cells in a two-chambered bioreactor with CDM-HD serum replacement to significantly improve the yield of cancer cell-associated EVs and eliminate bovine EV contamination. Cancer-relevant mRNAs BIRC5 (Survivin) and YBX1, as well as long-noncoding RNAs HOTAIR, ZFAS1, and AGAP2-AS1 were detected in BT-474 EVs by quantitative RT-PCR. Bioinformatics meta-analyses showed that BIRC5 and HOTAIR RNAs were substantially upregulated in breast tumours compared to non-tumour breast tissue, warranting further studies to explore their usefulness as biomarkers in patient EV samples. We envision this effective procedure for obtaining large amounts of cancer-specific EVs will accelerate discovery of EV-associated RNA biomarkers for cancers including HER2+ breast cancer.",0
Long-term experimental drought alters floral scent and pollinator visits in a Mediterranean plant community despite overall limited impacts on plant phenotype and reproduction,"Pollinators are declining globally, with climate change implicated as an important driver. Climate change can induce phenological shifts and reduce floral resources for pollinators, but little is known about its effects on floral attractiveness and how this might cascade to affect pollinators, pollination functions and plant fitness. We used an in situ long-term drought experiment to investigate multiple impacts of reduced precipitation in a natural Mediterranean shrubland, a habitat where climate change is predicted to increase the frequency and intensity of droughts. Focusing on three insect-pollinated plant species that provide abundant rewards and support a diversity of pollinators (Cistus albidus, Salvia rosmarinus and Thymus vulgaris), we investigated the effects of drought on a suite of floral traits including nectar production and floral scent. We also measured the impact of reduced rainfall on pollinator visits, fruit set and germination in S. rosmarinus and C. albidus. Drought altered floral emissions of all three plant species qualitatively, and reduced nectar production in T. vulgaris only. Apis mellifera and Bombus gr. terrestris visited more flowers in control plots than drought plots, while small wild bees visited more flowers in drought plots than control plots. Pollinator species richness did not differ significantly between treatments. Fruit set and seed set in S. rosmarinus and C. albidus did not differ significantly between control and drought plots, but seeds from drought plots had slower germination for S. rosmarinus and marginally lower germination success in C. albidus. Synthesis. Overall, we found limited but consistent impacts of a moderate experimental drought on floral phenotype, plant reproduction and pollinator visits. Increased aridity under climate change is predicted to be stronger than the level assessed in the present study. Drought impacts will likely be stronger and this could profoundly affect the structure and functioning of plant-pollinator networks in Mediterranean ecosystems. --",,"Long-term experimental drought alters floral scent and pollinator visits in a Mediterranean plant community despite overall limited impacts on plant phenotype and reproduction Pollinators are declining globally, with climate change implicated as an important driver. Climate change can induce phenological shifts and reduce floral resources for pollinators, but little is known about its effects on floral attractiveness and how this might cascade to affect pollinators, pollination functions and plant fitness. We used an in situ long-term drought experiment to investigate multiple impacts of reduced precipitation in a natural Mediterranean shrubland, a habitat where climate change is predicted to increase the frequency and intensity of droughts. Focusing on three insect-pollinated plant species that provide abundant rewards and support a diversity of pollinators (Cistus albidus, Salvia rosmarinus and Thymus vulgaris), we investigated the effects of drought on a suite of floral traits including nectar production and floral scent. We also measured the impact of reduced rainfall on pollinator visits, fruit set and germination in S. rosmarinus and C. albidus. Drought altered floral emissions of all three plant species qualitatively, and reduced nectar production in T. vulgaris only. Apis mellifera and Bombus gr. terrestris visited more flowers in control plots than drought plots, while small wild bees visited more flowers in drought plots than control plots. Pollinator species richness did not differ significantly between treatments. Fruit set and seed set in S. rosmarinus and C. albidus did not differ significantly between control and drought plots, but seeds from drought plots had slower germination for S. rosmarinus and marginally lower germination success in C. albidus. Synthesis. Overall, we found limited but consistent impacts of a moderate experimental drought on floral phenotype, plant reproduction and pollinator visits. Increased aridity under climate change is predicted to be stronger than the level assessed in the present study. Drought impacts will likely be stronger and this could profoundly affect the structure and functioning of plant-pollinator networks in Mediterranean ecosystems. --",0
Fungal OTUs during dead wood succession of aspen,"During decomposition of organic matter, microbial communities may follow different successional trajectories depending on the initial environment and colonizers. The timing and order of the assembly history can lead to divergent communities through priority effects. We explored how assembly history and substrate quality affected fungal dead wood communities and decomposition, 1.5 and 4.5 years after tree felling. In addition, we investigated the effect of invertebrate exclusion during the first two summers. For aspen (Populus tremula) logs, we measured initial bark and wood resource quality, and surveyed the fungal communities by DNA metabarcoding at different time points during succession. We found that a gradient in fungal community composition was related to resource quality and we discuss how this may reflect tolerance-dominance trade-offs in fungal life history strategies. As with previous studies, the initial amount of bark tannins was negatively correlated with wood decomposition rate over 4.5 years. The latent fungal community explained variation in community composition after 1.5, but not after 4.5 years, of succession. Although the assembly history of latent fungi may cause alternate trajectories in successional communities, our results indicate that the communities may easily converge with the arrival of secondary colonizers. We also identified a strong invertebrate-induced priority effect of fungal communities, even after 4.5 years of succession, thereby adding crucial knowledge to the importance of invertebrates in affecting fungal community development. By measuring and manipulating aspects of assembly history and resource quality that have rarely been studied, we expand our understanding of the complexity of fungal community dynamics.","['\r\n# Invertebrate-induced priority effects and secondary metabolites control fungal community composition in dead wood\r\n\r\n# Figures and analyses\r\n\r\n## READ LIBRARIES AND DATA ####\r\nlibrary(vegan)\r\nlibrary(ggplot2)\r\nlibrary(openxlsx)\r\nlibrary(lme4)\r\nlibrary(lmerTest)\r\nlibrary(phyloseq)\r\nlibrary(MuMIn)\r\nlibrary(sjPlot)\r\n\r\nenv<-read.table(""rarefied_yboth_sample_data.csv"",row.names=1,header=T,sep="","")\r\notu.both<-read.table(""rarefied_yboth_otu_table.csv"",row.names=1,header=T,sep="","")\r\ntaxa.both<-otu.both[,427:442]\r\notu.both.2<-otu.both[,1:426] ### without taxonomy\r\notu.both.2<-otu.both.2[,c(1:23,25:140,142:426)] #L026, L161\r\notu.both.3<-as.data.frame(t(otu.both.2))\r\ndist.both<-vegdist(otu.both.3, method=""bray"") # b-c distance matrix\r\ngnmds.both<-read.table(""nmds_yboth_axes.csv"",sep="","",header=T,row.names=1)\r\npaired<-c(""#FFFF99"",""#FFFF99"",""#B15928"",""#B15928"",""#CAB2D6"",""#CAB2D6"",""#6A3D9A"",""#6A3D9A"")\r\ntreat.pair<-c(""#B15928"",""#FFFF99"",""#CAB2D6"",""#6A3D9A"")\r\ngnmdsy5<-gnmds.both[1:211,]\r\ngnmdsy2<-gnmds.both[212:424,]\r\nMDS1.5<-gnmdsy5$MDS1\r\nMDS2.5<-gnmdsy5$MDS2\r\nMDS3.5<-gnmdsy5$MDS3\r\nMDS4.5<-gnmdsy5$MDS4\r\nMDS1.2<-gnmdsy2$MDS1\r\nMDS2.2<-gnmdsy2$MDS2\r\nMDS3.2<-gnmdsy2$MDS3\r\nMDS4.2<-gnmdsy2$MDS4\r\nenv2<-env[rownames(env) %in% rownames(otu.both.3),]\r\nrownames(env2)<-sort(rownames(env2))\r\ncbind(rownames(env2),rownames(otu.both.3)) # check if they\'re similar\r\nenv2$Treatment<-as.factor(env2$Treatment)\r\nenv2$treat_year<-as.factor(env2$treat_year)\r\nenv2$Site<-as.factor(env2$Site)\r\nenv2$Tree_ID<-as.factor(env2$Tree_ID)\r\nselect2ax<-env2[,c(""zDCA2"",""Wood_C"",""b_sum_flavonoids"")]\r\nfit_select2<-envfit(scores(gnmds.both,display=""sites"",choices=c(1,2),origin=TRUE)[,1:2],select2ax,999)\r\n\r\n#\r\notu<-read.xlsx(""exclusion_phyloseq.xlsx"",sheet=""otu_table""\r\n               ,colNames=T,rowNames=T)\r\notu.mat<-as.matrix(otu)\r\ntaxa<-read.xlsx(""exclusion_phyloseq.xlsx"",sheet=""tax_table""\r\n                ,colNames=T,rowNames=T)\r\ntaxa.mat<-as.matrix(taxa)\r\nmeta<-read.xlsx(""exclusion_phyloseq.xlsx"",sheet=""meta_table""\r\n                ,colNames=T,rowNames=T)\r\notu2<-otu_table(otu.mat,taxa_are_rows=T)\r\ntaxa2<-tax_table(taxa.mat)\r\nmeta2<-sample_data(meta)\r\np.exc<-phyloseq(otu2,taxa2,meta2) # 560 samples, 1287 otus\r\n## remove neg samples\r\np.exc3<-subset_samples(p.exc,Section != ""neg"") # 553 samples\r\np.exc4<-subset_samples(p.exc3,Section != ""pcr_neg"") # 550 samples\r\n# remove L61 reps from year 5. results in 545 samples\r\np.exc5<-subset_samples(p.exc4,old_index!=""S063"" & old_index!=""S072"" & \r\n                         old_index!=""S245"" & old_index!=""S100"" & \r\n                         old_index!=""S115"")\r\n# remove tech reps from both years. results in 514 samples\r\np.exc6<-subset_samples(p.exc5,old_index!=""S429"" & \r\n                         old_index!=""S359"" & old_index!=""S353"" &old_index!=""S491""&old_index!=""S436"" &old_index!=""S378"" & \r\n                         old_index!=""S530""& old_index!=""S533""&old_index!=""S584""&old_index!=""S535"" & \r\n                         old_index!=""S358""&old_index!=""S360""&old_index!=""S490"" & old_index!=""S488""& old_index!=""S532""&\r\n                         old_index!=""S431"" & old_index!=""S380"" & old_index!=""S489""&old_index!=""S261""&\r\n                         old_index!=""S262""&old_index!=""S260""&old_index!=""S265""&old_index!=""S267""&\r\n                         old_index!=""S045""& old_index!=""S266""&old_index!=""S190"" &old_index!=""S067""&\r\n                         old_index!=""S068""&old_index!=""S192""&old_index!=""S203""&old_index!=""S091"")\r\np.exc7<-prune_taxa(taxa_sums(p.exc6) >= 10,p.exc6) # removed 356 taxa\r\np.exc8<-subset_taxa(p.exc7,!Kingdom %in% NA) #annotated only - 832 taxa\r\n# remove samples from fresh wood (latent community)\r\ny.both<-subset_samples(p.exc8,year!=""fresh"") \r\n# remove non-replicated samples\r\ny.both2<-subset_samples(y.both,old_index!=""S323"" & old_index!=""S361"" & old_index!=""S467"" &\r\n                          old_index!=""S469"" & old_index!=""S583"" & old_index!=""S578""&old_index!=""S571"" & \r\n                          old_index!=""S576"" & old_index!=""S387"" & old_index!=""S317""&old_index!=""S343"" & old_index!=""S345"" & \r\n                          old_index!=""S453"" & old_index!=""S526"" & old_index!=""S373""&old_index!=""S368"" & \r\n                          old_index!=""S456"" & old_index!=""S013"" & old_index!=""S171"") ##442 samples\r\n#remove y2 equivalent of the y5 removed at subsampling depth 14407\r\ny.both3<-subset_samples(y.both2,old_index!=""S318"" & old_index!=""S335"" & old_index!=""S539"" &\r\n                          old_index!=""S390"" & old_index!=""S586"" & old_index!=""S443""&old_index!=""S342"" &\r\n                          old_index!=""S585"") #433 samples\r\ny.both4<-prune_taxa(taxa_sums(y.both3) > 0,y.both3) # 788 otus\r\n#quantile(colSums(otu_table(p.exc8))) # min y2=14407\r\ny_rare_both<-rarefy_even_depth(y.both4,sample.size=14407,rngseed=2)\r\nexc.alpha.ri<-plot_richness(y_rare_both,x=""sample"",color=""Treatment"",\r\n                            measures=c(""Observed""),nrow=1)\r\nsample_data(y_rare_both)$richness<-exc.alpha.ri$data$value\r\n\r\ny2rared<-subset_samples(y_rare_both,year==""year2"")\r\nprune_taxa(taxa_sums(y2rared) > 0,y2rared) # 537 otus\r\ny2df<-data.frame(sample_data(y2rared))\r\ny2df2<-y2df[order(y2df$Sample_ID),]\r\ny2df2$Tree_ID<-as.factor(y2df2$Tree_ID)\r\n\r\ny5rared<-subset_samples(y_rare_both,year==""year5"")\r\nprune_taxa(taxa_sums(y5rared) > 0,y5rared) # 422 otus\r\ny5df<-data.frame(sample_data(y5rared))\r\ny5df2<-y5df[order(y5df$Sample_ID),]\r\n\r\n## RUN GNMDS ####\r\n# Note that the configuration used in the article cannot be reproduced, \r\n# but can be found as ""nmds_yboth_axes.csv"" and were made with the following settings\r\n# the resulting nmds had stress levels of 0.135 \r\nmds.both<-NULL\r\nfor(i in 1:1000)\r\n{mds.both[[i]]<-monoMDS(dist.both, matrix(c(runif(dim(otu.both.3)[1]*k)),nrow=dim(otu.both.3)[1]), \r\n                        k=4, model = ""global"", maxit=500, smin = 1e-7, sfgrmin = 1e-7)}\r\n\r\nmds.both.stress<-unlist(lapply(mds.both,function(v){v[[22]]})) \r\nordered<-order(mds.both.stress)\r\nmds.both.best<-postMDS(mds.both[[ordered[1]]],dist.both, pc = TRUE, halfchange = TRUE, threshold = 0.8) #0.1354878\r\n\r\n## FIGURE 1 ####\r\nplot(gnmds.both$MDS1,gnmds.both$MDS2,type=""n"",xlab=""gNMDS1"",ylab=""gNMDS2"",xlim=c(-1,1),ylim=c(-1,1))\r\npoints(MDS1.5,MDS2.5,cex=0.5,pch=16,col=""grey75"")\r\npoints(MDS1.2,MDS2.2,cex=0.5,pch=16,col=""grey90"")\r\nordibar(gnmds.both[,c(1,2)],env2$treat_year,kind = ""sd"",col=paired)\r\nordiellipse(gnmds.both[,c(1,2)],env2$treat_year,kind = ""se"", draw = ""polygon"",col=paired)\r\nlegend(""topleft"",legend=c(""Cage"",""Cage control"",""Control"",""Positive control""),col=treat.pair,pch=16,cex=0.8,bty=""n"",\r\n       y.intersp=1.45,x.intersp=0.4)\r\nlegend(""topleft"",legend=c(""YEAR 5"",""YEAR 2""),col=c(""grey90"",""grey75""),bty=""n"",\r\n       cex=0.8,y.intersp=0.7,x.intersp=11.8,horiz=T)\r\nplot(fit_select2,add=T,\r\n     labels=c(paste(""             "",""     Initial community (2)""),\r\n              ""Wood carbon"",\r\n              ""Bark flavonoids""),\r\n     col=""black"", cex=0.8)\r\n\r\n\r\n## TABLE 1 AND 2 - CCA MODEL SELECTION ####\r\n# for example with a year 2 dataset\r\nenv.y2<-read.table(""rarefied_both_subset_y2_sample_data.csv"",row.names=1,header=T,sep="","")\r\nenv.y5<-read.table(""rarefied_both_subset_y5_sample_data.csv"",row.names=1,header=T,sep="","")\r\notu_y2<-read.table(""rarefied_both_subset_y2.csv"",row.names=1,header=T,sep="","")\r\notu_y2<-otu_y2[,1:213]\r\notu_y2<-as.data.frame(t(otu_y2))\r\notu_y5<-read.table(""rarefied_both_subset_y5.csv"",row.names=1,header=T,sep="","")\r\notu_y5<-otu_y5[,1:213]\r\n\r\ny2CCA1.1<-cca(otu_y2~Wood_C,data=env.y2)\r\npermutest(y2CCA1.1,permutations=999) #\r\ny2CCA1.2<-cca(otu_y2~Wood_N,data=env.y2)\r\npermutest(y2CCA1.2,permutations=999) #\r\ny2CCA1.3<-cca(otu_y2~Site,data=env.y2)\r\npermutest(y2CCA1.3,permutations=999) #\r\ny2CCA1.4<-cca(otu_y2~Landscape,data=env.y2)\r\npermutest(y2CCA1.4,permutations=999) # LANDSCAPE is chosen\r\ny2CCA1.5<-cca(otu_y2~Section,data=env.y2)\r\npermutest(y2CCA1.5,permutations=999)#\r\ny2CCA1.6<-cca(otu_y2~Treatment,data=env.y2)\r\npermutest(y2CCA1.6,permutations=999)#\r\ny2CCA1.8<-cca(otu_y2~b_sum_phenolic_acid,data=env.y2)\r\npermutest(y2CCA1.8,permutations=999)#\r\ny2CCA1.9<-cca(otu_y2~b_sum_flavonoids,data=env.y2)\r\npermutest(y2CCA1.9,permutations=999)#\r\ny2CCA1.10<-cca(otu_y2~Bark_sol_CT,data=env.y2)\r\npermutest(y2CCA1.10,permutations=999)#\r\ny2CCA1.11<-cca(otu_y2~Bark_insol_CT,data=env.y2)\r\npermutest(y2CCA1.11,permutations=999)#\r\ny2CCA1.12<-cca(otu_y2~w_sum_phenolic_acids,data=env.y2)\r\npermutest(y2CCA1.12,permutations=999)#\r\ny2CCA1.13<-cca(otu_y2~w_sum_flavonoids,data=env.y2)\r\npermutest(y2CCA1.13,permutations=999)#\r\ny2CCA1.14<-cca(otu_y2~w_sum_salicylates,data=env.y2)\r\npermutest(y2CCA1.14,permutations=999)#\r\ny2CCA1.15<-cca(otu_y2~zDCA1,data=env.y2)\r\npermutest(y2CCA1.15,permutations=999)#\r\ny2CCA1.16<-cca(otu_y2~zDCA2,data=env.y2)\r\npermutest(y2CCA1.16,permutations=999)#\r\n# forward selection with p value as selection criterion. when p values were equal between models, F value was criterion\r\n# when a variable was selected, it was set as a conditional variable in the next round\r\n# e.g. in this example, Landscape was selected:\r\ny2CCA2.1<-cca(otu_y2~Wood_C+Condition(Landscape),data=env.y2)\r\npermutest(y2CCA2.1,permutations=999) #\r\n# etc...\r\n# we continue adding conditioning variables until no extra additions are significant (p<0.05)\r\n\r\n## FIGURE 2 - LMM analyses ####\r\n# the same forward selection procedures (and with the same variables) were used for all LMMs, namely\r\n# richness year 2, richness year 5, wood density year 2, wood density year 5,\r\n# gNMDS1, gNMDS2, gNMDS3 and gNMDS4\r\ny2.rich.0<-lmer(richness~1+(1|Site),data=y2df2)\r\ny2.rich.1.1<-lmer(richness~Treatment+(1|Site),data=y2df2)\r\ny2.rich.1.2<-lmer(richness~Section+(1|Site),data=y2df2)\r\ny2.rich.1.3<-lmer(richness~Landscape+(1|Site),data=y2df2)\r\ny2.rich.1.4<-lmer(richness~b_sum_flavonoids+(1|Site),data=y2df2)\r\ny2.rich.1.5<-lmer(richness~b_sum_phenolic_acid+(1|Site),data=y2df2)\r\ny2.rich.1.6<-lmer(richness~Bark_sol_CT+(1|Site),data=y2df2)\r\ny2.rich.1.7<-lmer(richness~Bark_insol_CT+(1|Site),data=y2df2)\r\ny2.rich.1.8<-lmer(richness~w_sum_salicylates+(1|Site),data=y2df2)\r\ny2.rich.1.9<-lmer(richness~w_sum_flavonoids+(1|Site),data=y2df2)\r\ny2.rich.1.10<-lmer(richness~Wood_C+(1|Site),data=y2df2)\r\ny2.rich.1.11<-lmer(richness~Wood_N+(1|Site),data=y2df2)\r\ny2.rich.1.12<-lmer(richness~zDCA1+(1|Site),data=y2df2)\r\ny2.rich.1.13<-lmer(richness~zDCA2+(1|Site),data=y2df2)\r\ny2.rich.full<-lmer(richness~Treatment+Section+Landscape+\r\n                     Bark_sol_CT+Bark_insol_CT+b_sum_phenolic_acid+b_sum_flavonoids+\r\n                     w_sum_salicylates+w_sum_phenolic_acids+w_sum_flavonoids+zDCA1+zDCA2+\r\n                     zDensity+Wood_C+Wood_N+Diameter+(1|Site),data=y2df2)\r\nAICc(y2.rich.0,y2.rich.1.1,y2.rich.1.2,y2.rich.1.3,y2.rich.1.4,y2.rich.1.5,\r\n     y2.rich.1.6,y2.rich.1.7,y2.rich.1.8,y2.rich.1.9,y2.rich.1.10,\r\n     y2.rich.1.11,y2.rich.1.12,y2.rich.1.13) # 1.4, b_sum_flavonoids!\r\n\r\ny2.rich.2.1<-lmer(richness~Treatment+b_sum_flavonoids+(1|Site),data=y2df2)\r\n# etc...\r\n\r\n# ended up with these models\r\ny2.rich.best<-lmer(richness~Treatment+Section+Landscape+\r\n                     zDCA1+zDCA2+Wood_C+Wood_N+\r\n                     w_sum_phenolic_acids+w_sum_salicylates+w_sum_flavonoids+\r\n                     b_sum_phenolic_acid+b_sum_flavonoids+Bark_sol_CT+Bark_insol_CT+\r\n                     (1|Site),data=y2df2)\r\ny5.rich.best<-lmer(richness~w_sum_salicylates+Treatment+\r\n                     Wood_C+w_sum_phenolic_acids+b_sum_flavonoids+(1|Site),data=y5df2)\r\n\r\n# FIGURE 2\r\nplot_models(y2.rich.best,y5.rich.best,m.labels=c(""Year 2"",""Year 5""),\r\n            title="""",wrap.labels=100,colors=c(""black"",""#a3a3a3""),\r\n            axis.labels=c(""Bark insol. tannins"",""Bark sol. tannins"",""Bark flavonoids"",""Bark phenolic acids"",\r\n                          ""Wood flavonoids"",""Wood salicylates"",""Wood phenolic acids"",\r\n                          ""Wood nitrogen"",""Wood carbon"",""Initial fungal community (2)"",\r\n                          ""Initial fungal community (1)"",""Landscape (Nordmarka)"",""Log section (mid)"",""(OH control)"",\r\n                          ""(control)"",""Invertebrate exclusion (cage control)"",""Intercept""),\r\n            show.intercept=T)+labs(y=""Fungal OTU richness"")+\r\n  theme(strip.text.x=element_blank(),panel.background=element_blank(),\r\n        panel.grid.major = element_blank(),panel.grid.minor = element_blank(),\r\n        axis.text.y=element_text(size=12,color=""black"",family=""serif"",lineheight=0.7,margin=margin(l=20)),\r\n        axis.title.x=element_text(size=12,color=""black"",family=""serif""),\r\n        legend.position=c(0.7,0.75),legend.text=element_text(size=12,color=""black"",family=""serif""),\r\n        axis.ticks.y=element_blank(),axis.ticks.x=element_blank(),\r\n        legend.title=element_blank(),legend.key=element_rect(fill=""white""))+\r\n  geom_hline(yintercept=0.8,alpha=0.3) #\r\n\r\n']","Fungal OTUs during dead wood succession of aspen During decomposition of organic matter, microbial communities may follow different successional trajectories depending on the initial environment and colonizers. The timing and order of the assembly history can lead to divergent communities through priority effects. We explored how assembly history and substrate quality affected fungal dead wood communities and decomposition, 1.5 and 4.5 years after tree felling. In addition, we investigated the effect of invertebrate exclusion during the first two summers. For aspen (Populus tremula) logs, we measured initial bark and wood resource quality, and surveyed the fungal communities by DNA metabarcoding at different time points during succession. We found that a gradient in fungal community composition was related to resource quality and we discuss how this may reflect tolerance-dominance trade-offs in fungal life history strategies. As with previous studies, the initial amount of bark tannins was negatively correlated with wood decomposition rate over 4.5 years. The latent fungal community explained variation in community composition after 1.5, but not after 4.5 years, of succession. Although the assembly history of latent fungi may cause alternate trajectories in successional communities, our results indicate that the communities may easily converge with the arrival of secondary colonizers. We also identified a strong invertebrate-induced priority effect of fungal communities, even after 4.5 years of succession, thereby adding crucial knowledge to the importance of invertebrates in affecting fungal community development. By measuring and manipulating aspects of assembly history and resource quality that have rarely been studied, we expand our understanding of the complexity of fungal community dynamics.",0
Data and code for: Parthenogenesis is self-destructive for scaled reptiles,"Parthenogenesis is rare in nature. With 39 described true parthenogens, scaled reptiles (Squamata) are the only vertebrates that evolved this reproductive strategy. Parthenogenesis is ecologically advantageous in the short-term, but the young age and rarity of parthenogenetic species indicate it is less advantageous in the long-term. This suggests parthenogenesis is self-destructive: it arises often but is lost due to increased extinction rates, high rates of reversal or both. However, this role of parthenogenesis as a self-destructive trait remains unknown. We used a phylogeny of Squamata (5,388 species), tree metrics, null simulations and macroevolutionary scenarios of trait diversification to address the factors that best explain the rarity of parthenogenetic species. We show that parthenogenesis can be considered as self-destructive, with high extinction rates mainly responsible for its rarity in nature. Since these parthenogenetic species occur, this trait should be ecologically relevant in the short-term.",,"Data and code for: Parthenogenesis is self-destructive for scaled reptiles Parthenogenesis is rare in nature. With 39 described true parthenogens, scaled reptiles (Squamata) are the only vertebrates that evolved this reproductive strategy. Parthenogenesis is ecologically advantageous in the short-term, but the young age and rarity of parthenogenetic species indicate it is less advantageous in the long-term. This suggests parthenogenesis is self-destructive: it arises often but is lost due to increased extinction rates, high rates of reversal or both. However, this role of parthenogenesis as a self-destructive trait remains unknown. We used a phylogeny of Squamata (5,388 species), tree metrics, null simulations and macroevolutionary scenarios of trait diversification to address the factors that best explain the rarity of parthenogenetic species. We show that parthenogenesis can be considered as self-destructive, with high extinction rates mainly responsible for its rarity in nature. Since these parthenogenetic species occur, this trait should be ecologically relevant in the short-term.",0
Data for LSA analysis,"This code is supplementary to the paper ""Kinetic modulation of bacterial hydrolases by microbial community structure in coastal waters"" by Abad et al. It contains the following files: 0) README.txt: This README file.1) Complex saturation kinetics modelling.R: It contains the code developed for the determination of the kinetic parameters of the extracellular enzymatic activities by fitting the hydrolysis rates to four different kinetic models of increasing complexity using a non-linear least squares regression.2) LSA functions.R: It contains the functions implemented in R to perform the Local Similarity analysis.Our specific modifications related to the function LocalSimilarity3 are indicated by the comment ""#New: modified function"".3) LSA script.R: It is an updated version of the code developed by Ruan et al (2006) that has been used to perform the Local Similarity analysis in our study. The comment ""#New: modified function indicates our specific modifications within the original code.The following modifications were added to the original code: The calculation of the linear interpolation of missing values (NAs) using the R package zoo (Zeleis et al 2021).The calculation of the q-values by using the R package qvalue (Storey et al 2022).NOTE: it is important to set the working directory in the same folder where all the provided files are stored.","['#--------------------------------------------------------------------------------------\r\n#\r\n#  July 2022\r\n#  Code associated with publication:\r\n#  ""Kinetic modulation of bacterial hydrolases by microbial community structure in coastal waters"" \r\n#   Authors: N. Abad, A. Uranga, B. Ayo, J.M. Arrieta,Z. Baa, I. Artolozaga, I. Aza, J. Iriberri, Santos J. Gonzlez-Roj and M. Unanue\r\n#\r\n#\r\n#  Creative Commons Licence: Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)\r\n#--------------------------------------------------------------------------------------\r\n\r\n# July 2022\r\n# Script by N. Abad\r\n\r\n#################### NOTES before starting the analysis #####################\r\n\r\n# 1) It is required to previously install ""nlstools"" and ""AICcmodavg"" packages. Run the following\r\n# code line if needed:\r\n\r\ninstall.packages(c(""nlstools"",""AICcmodavg""))\r\n\r\n# 2) Input data is a file with n-rows (samples) and 2-columns: column 1 with header ""S"" corresponds to\r\n# the different concentrations of substrate used in the experiment and column 2 with header ""V"" corresponds \r\n# to the hydrolysis rates.\r\n\r\n#################################################\r\n\r\n## 1_Load the file, omit the rows where there is no data (NA) and plot Hydrolysis rates vs Substrate concentration.\r\n\r\nrm(list=ls())\r\ndev.off()\r\n\r\nlibrary(nlstools)\r\nlibrary(AICcmodavg)\r\n\r\nfile<-file.choose()\r\nsetwd<-dirname(file)\r\nsetwd(setwd)\r\ndata<-read.delim(file)\r\ndata.with.na<-data\r\ndata<-na.omit(data.with.na)\r\n\r\n# set the units in brackets to the enzymatic experiment\r\nplot(data,xlab=""Substrate concentration (M)"",ylab=""Hydrolysis rate (nMh-1)"",\r\nylim=c(min(data$V),max(data$V)),xlim=c(min(data$S),max(data$S)))\r\n\r\n#################################################\r\n\r\n## 2_Perform the fit of different models of increasing complexity to the dataset\r\n\r\n# MODEL 1: the simplest, is derived from MM kinetics assuming that all \r\n# of the data collected are in the region of first order kinetics (S<<Km)\r\n# Parameters Km and Vmax cannot be estimated separately under such circumstances; \r\n# ratio Vmax/Km trated as one parameter can be determined reliably\r\n\r\n### The code that is commented allows a simulation to better estimate \r\n### the starting values of model 1\r\n\r\n#formula<-as.formula(V~(S/Tt))\r\n#preview(formula, data=data, start= list(Tt=5)) \r\n\r\nmodel1 <- nls(V~(S/Tt), data=data,\r\n            start =list(Tt = 2),\r\n            alg = ""port"", trace = TRUE,\r\n            control=list(maxiter=200, tol=1e-15))\r\n\r\nRSS1 <- sum(residuals(model1)^2)\r\nTSS1 <- sum((data$V- mean(data$V))^2)\r\nsummary(model1)\r\nprint(paste(""R-squared:"",format(1 - (RSS1/TSS1),digits=2)))\r\nRSS1\r\n\r\nabline(0,1/coef(model1)[""Tt""], col=""grey"", lty=2,lwd=2) \r\n\r\n#####################################################\r\n\r\n# MODEL 2: the Henri-Michaelis-Menten equation\r\n\r\n# set the units in brackets to the enzymatic experiment\r\nplot(data,xlab=""Substrate concentration (M)"",ylab=""Hydrolysis rate (nMh-1)"",\r\nylim=c(min(data$V),max(data$V)),\r\nxlim=c(min(data$S),max(data$S)))\r\n\r\nmodel2 <- nls(V~(Vmax*S)/(Km+S), data=data,\r\n            start =list( Km = 10 , Vmax = max(data$V,na.rm=TRUE)),\r\n            lower =list( Km = 1 , Vmax = min(data$V,na.rm=TRUE)),\r\n            upper =list( Km = max(data$S,na.rm=TRUE),Vmax= 1000),\r\n            alg = ""port"", trace = TRUE,\r\n            control=list(maxiter=200, tol=1e-15))\r\n\r\nRSS2 <- sum(residuals(model2)^2)\r\nTSS2 <- sum((data$V- mean(data$V))^2)\r\nsummary(model2)\r\nprint(paste(""R-squared:  "",format(1 - (RSS2/TSS2),digits=2)))\r\nRSS2\r\n            \r\nif(substr(model2[1],1,5)!=""Error"")\r\n\t{\r\n    \tKm<-coef(model2)[""Km""] \r\n   \t\tVmax<-coef(model2)[""Vmax""]\r\n\t\t  smin<-min(data$S)   \r\n\t\t  smax<-max(data$S)\r\n\t\t  ds<-1\r\n\t\t  s <- seq(smin,smax,ds) \r\n\t\t  n <- length(s) \r\n\t\t  N <- min(data$V) \r\n\t\t  for (j in 1:n) \r\n\t\t\t{ \r\n\t\t\t  N[j] <- (Vmax*s[j])/(Km+s[j]) \r\n      \t\t\t}\r\n\t\t  lines(s,N, col=""red"") \r\n \t\t}\r\n\r\n## Choose the model best representing experimental data on the basis of the corrected Akaike\'s \r\n## Information Criteria (AICc). \r\n\r\n## NOTE: is important to maintain the order in which the code below is written because the models \r\n## are in  hierarchical order.\r\n\r\nanova(model1,model2)\r\nAIC(model1,model2) \r\n\r\nAICc(model1)\r\nAICc(model2) \r\n\r\n##########################################\r\n\r\n# MODEL 3: assumes a system of two independent enzymes whose \r\n# kinetics are described by models 1 and 2\r\n\r\nplot(data,xlab=""Substrate concentration (M)"",ylab=""Hydrolysis rate (nMh-1)"",\r\n     ylim=c(min(data$V),max(data$V)),\r\n     xlim=c(min(data$S),max(data$S)))\r\n\r\n### The code that is commented allows a simulation to better estimate \r\n### the starting values of model 3. \r\n\r\n#formula<-as.formula(V~(S<= ""Value"")*((Vmax*S)/(Km+S)) + ((S> ""Value"")* (S/Tt)))\r\n#preview(formula, data=data, start= list(Km= 1, Vmax= 7, Tt=3)) \r\n\r\n# NOTE: in the code line V~(S<= ""Value"")*((Vmax*S)/(Km+S)) + ((S> ""Value"")* (S/Tt),\r\n# the ""Value"" item has to be substituted according to the concentration of substrates\r\n# used in the enzymatic assay\r\n\r\nmodel3 <- nls(V~(S<= 25)*((Vmax*S)/(Km+S)) + ((S> 25)* (S/Tt)), \r\n\tdata=data, start =list( Km = 1 , Vmax = 0.5, Tt= 50),\r\n\talg = ""port"", trace = TRUE,\r\n\tcontrol=list(maxiter=200, tol=1e-15))\r\n\r\nRSS3 <- sum(residuals(model3)^2)\r\nTSS3 <- sum(( data$V- mean(data$V))^2)\r\nsummary(model3)\r\nprint(paste(""R-squared:  "",format(1 - (RSS3/TSS3),digits=2)))\r\nRSS3\r\n\r\nif(substr(model3[1],1,5)!=""Error"")\r\n{\r\nmodel.fit<-as.list(coef(model3))\r\nKm<-coef(model3)[""Km""] \r\nVmax<-coef(model3)[""Vmax""]\r\nTt<-coef(model3)[""Tt""]\r\nsmin <- min(data$S)\r\nsmax <- max(data$S)\r\nds <- 1 \r\ns <- seq(smin,smax,ds) \r\nn <- length(s) \r\nN1 <- min(data$V) \r\nN2<-  min(data$V)\r\n\r\nfor (j in 1:n) \r\n{ \r\n  N1[j] <- (Vmax*s[j])/(Km+s[j])\r\n  N2[j] <- (s[j])/(Tt)\r\n}\r\n\r\nlines(s[1:n],N1[1:n], col=""blue"", lwd=2)\r\nlines(s[1:n],N2[1:n],col=""darkblue"", lwd=2)\r\n\r\n}\r\n\r\n## Choose the model best representing experimental data on the basis of the corrected Akaike\'s \r\n## Information Criteria (AICc). \r\n\r\n## NOTE: is important to maintain the order in which the code below is written because the models \r\n## are in  hierarchical order.\r\n\r\n## Depending on the result obtained in the previous AICc test, replace ""modelX"" by ""model1"" or ""model 2""\r\nanova(modelX,model3)\r\nAIC(modelX,model3) \r\n\r\nAICc(modelX)\r\nAICc(model3) \r\n\r\n#############################################################\r\n\r\n# MODEL 4: the most complex model assumes a system of two independent \r\n# enzymes whose kinetics follow the Henri-Michaelis-Menten \r\n# equation (high- and low-affinity)\r\n\r\nplot(data,xlab=""Substrate concentration (M)"",ylab=""Hydrolysis rate (nMh-1)"",\r\n     \tylim=c(min(data$V),max(data$V)),\r\n     \txlim=c(min(data$S),max(data$S)))\r\n\r\nmodel4 <- nls(V~(VmaxL*S)/(KmL+S)+(VmaxH*S)/(KmH+S),data=data, \r\n              start =list(KmH=1, VmaxH= 0.4,\r\n                          KmL=max(data$S,na.rm=TRUE),\r\n                          VmaxL=max(data$V,na.rm=TRUE)),\r\n              alg = ""port"", trace = TRUE,control=list(maxiter=200, tol=1e-10))\r\n\r\nRSS4 <- sum(residuals(model4)^2)\r\nTSS4 <- sum(( data$V- mean(data$V))^2)\r\nsummary(model4)\r\nprint(paste(""R-squared:  "",format(1 - (RSS4/TSS4),digits=2)))\r\nRSS4\r\n\r\nif(substr(model4[1],1,5)!=""Error"")\r\n    \t{\r\nmodel.fit<-as.list(coef(model4))\r\nKmH<-coef(model4)[""KmH""] \r\nVmaxH<-coef(model4)[""VmaxH""]\r\nKmL<-coef(model4)[""KmL""]\r\nVmaxL<-coef(model4)[""VmaxL""]\r\nsmin <- min(data$S)\r\nsmax <- max(data$S)\r\nds <- 1 \r\ns <- seq(smin,smax,ds) \r\nn <- length(s) \r\nN1 <- min(data$V) \r\nN2<-  min(data$V)\r\nfor (j in 1:n) \r\n{ \r\n  N1[j] <- (VmaxH*s[j])/(KmH+s[j])\r\n  N2[j] <- (VmaxL*s[j])/(KmL+s[j])\r\n}\r\n\r\nlines(s[1:n],N1[1:n], col=""darkmagenta"", lwd=2)\r\nlines(s[1:n],N2[1:n],col=""darkorchid1"", lwd=2)\r\n}\r\n\r\n## Choose the model best representing experimental data on the basis of the corrected Akaike\'s \r\n## Information Criteria (AICc). \r\n\r\n## NOTE: is important to maintain the order in which the code below is written because the models \r\n## are in  hierarchical order.\r\n\r\n## Depending on the result obtained in the previous AICc test (a.k.a. modelX vs model3), replace ""modelY"" \r\n\r\nanova(modelY,model4)\r\nAIC(modelY,model4) \r\nAICc(modelY) \r\nAICc(model4)\r\n\r\n#################### THE END ####################', '#--------------------------------------------------------------------------------------\r\n#\r\n#  July 2022\r\n#  Code associated with publication:\r\n#  ""Kinetic modulation of bacterial hydrolases by microbial community structure in coastal waters"" \r\n#   Authors: N. Abad, A. Uranga, B. Ayo, J.M. Arrieta,Z. Baa, I. Artolozaga, I. Aza, J. Iriberri, Santos J. Gonzlez-Roj and M. Unanue\r\n#\r\n#\r\n#  Creative Commons Licence: Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)\r\n#--------------------------------------------------------------------------------------\r\n\r\n### Adapted from Ruan et al (2006): doi:10.1093/bioinformatics/btl417\r\n\r\n### The modified module from the original source is displayed below. The explanations of the original code \r\n### have been retained for the sake of better comprehension. Specific modifications related to a function are \r\n### indicated by the comment ""#New: modified function"".\r\n\r\n\r\n################################\r\n# LocalSimilarityAnalysis.R  #\r\n################################\r\n#\r\n# The following functions are implemented:\r\n#\r\n# . normalTransform <- function(rawMatrix)\r\n#\r\n#\t. tempChangeSeq(dataX)\r\n#\r\n#\t. colSum2One(dataX)\r\n#\r\n#\t. normalization(x, dims=1, na.rm=FALSE)\r\n#\r\n#\t. LocalSimilarity3(lsTS1, lsTS2, maxDelay=1, numTimePoints=1, scale=F)  #New: modified function\r\n#\r\n#\t. SigTestMaxDelay(data1, N=dim(data1)[[2]], delay=0, permu=1000)\r\n#\r\n#\t. sigTestingCor1(lsTS1, lsTS2, method=""pearson"", permu=1000)\r\n#\r\n#\t. sigTestingCor2(dataX, method=""pearson"", permu=1000)\r\n#\r\n#\t. PlotOtuPair2(idx=15, ls.res=ls.res.all, ylab=""rel. abund.(normalized)"", data=dataXe)\r\n#\r\n#\t. PlotOtuPair3(idx=15, ls.res=ls.res.all, ylab=""rel. abund."", data=dataTe)\r\n#\r\n#\t. CovaryOtuSif(tmp5) \r\n#\r\n#\r\n# Last updated: Apr 20, 2006, Quansong Ruan.  @All rights reserved.\r\n#############################################################################\r\n\r\n################################\r\n# normalTransform(rawMatrix): \r\n# \r\n# For a rawMatrix[pxq] matrix, with each column as an OTU, and row as time points,\r\n# this function perform a rank normal score transformation [Ker-chau Li, PNAS 2002]\r\n# \r\n# Output: rankScoreMatrix: a normal score matrix of the same size as rawMatrix\r\n############################### \r\n\r\nnormalTransform <- function(rawMatrix)\r\n{\r\n\t\r\ndimRM <- dim(rawMatrix);\r\nrowNum <- dimRM[[1]];\r\ncolNum <- dimRM[[2]];\r\nrankScoreMatrix <- rawMatrix;\r\n\r\nfor(i in 1:colNum)\r\n{\r\n\trk <- rank(rawMatrix[,i]);\r\n\trankScoreMatrix[,i] <- qnorm(rk/(rowNum+1));\r\n}\r\n\r\nrankScoreMatrix\r\n}\r\n\r\n###################################################\r\n# temporalChangeSeq(dataX):\r\n#\r\n# This function calculate the temporal change sequence for each time series\r\n#\r\n# Arguments:\r\n# =========\r\n# \tdataX: a M by N non-negative data matrix. each row in dataX is a time series\r\n#\r\n# Return:\r\n# ======\r\n#    a M by (N-1) data matrix containing the temporal change sequences of \r\n# the time series in dataX.\r\n#\r\n# Note:\r\n# =====\r\n#\r\n#\r\n#  The temp change between a[i] and a[i+1] is defined as follows:\r\n# \r\n#     Ta[i]  = 0, if a[i+1]=a[i]=0\r\n#\r\n#\t     = (a[i+1]-a[i]) / (a[i+1]+a[i]), otherwise\r\n#\r\n#\r\n# Mar 24, 2006\r\n###################################################\r\ntempChangeSeq <- function(dataX)\r\n{\r\n\tdata <- dataX\r\n\tdata <- as.matrix(data)\r\n\tdim1 <- dim(data)\r\n\t\r\n\tdt1 <- data[,2:dim1[[2]]] - data[,1:(dim1[[2]]-1)]\r\n\tdt2 <- data[,2:dim1[[2]]] + data[,1:(dim1[[2]]-1)]\r\n\r\n\r\n\t# trick: if dt2[i,j]=0, then dt1[i,j] must be 0 as well\r\n\t# by definition, Ta[i,j] should be 0. so for efficient \r\n\t# operation, if dt2[i,j]=0, set dt2[i,j] <- 108 (any \r\n\t# non-zero number does not change the result)\r\n\t# \r\n\tdt2[dt2<=0] <- 108\r\n\r\n\treturn (dt1 / dt2) # entry-wise division\r\n}\r\n\r\n\r\n\r\n\r\n\r\n####################################################\r\n# colSum2One(dataX): \r\n#\r\n#    This function re-calculate the percentage for each OTU at the same time spot\r\n#\r\n#################################################### \r\ncolSum2One <- function(dataX)\r\n{\r\n\r\n\ttmp <- dataX\r\n\tdim1 <- dim(tmp)\r\n\tcsum1 <- colSums(tmp)\r\n\tfor(i in 1:dim1[[2]])\r\n\t{\r\n\t\ttmp[,i] <- tmp[,i]/csum1[i]*100\t\r\n\t}\r\n\t\r\n\treturn(tmp)\r\n}\r\n\r\n\r\n#########################################################################\r\n# normalization <- function(x, dims=1, na.rm=FALSE)\r\n###########################\r\n#\r\n#\tThis function performs normalization of given data (vector, array, \r\n# matrix, data frame)\r\n# Arguments: \r\n# =========\t\r\n#\t    x: an array of two or more dimensions, containing numeric, \r\n#\t       complex, integer or logical values, or a numeric data frame.\r\n#\r\n#\t dims: which dimension is to normalize over. 1-row, 2-column\r\n#\r\n#\tna.rm: remove NA\'s if TRUE, ignore NA\'s oterwise\r\n#\r\n# RETURN:\r\n#    Row(column) normalized data of the same size as x\r\n#   \r\n##########################################################################\r\nnormalization <- function(x, dims=2, na.rm=FALSE)\r\n{\r\n\r\n tmpX <- x; \r\n dim1 <- dim(tmpX)\r\n\r\n if(is.null(dim1)) \t# integer or  1-dim array (vector)\r\n {\r\n      tmpX <- (tmpX - mean(tmpX, na.rm=na.rm)) / sd(tmpX, na.rm=na.rm)\r\n }\r\n else   \t\t# at least two dimensions\r\n {\r\n   if(dims==1)   tmpX <- t(tmpX);\r\n   \r\n   dim1 <- dim(tmpX)\r\n\r\n   Tmp.m <- colMeans(tmpX, na.rm=na.rm) \r\n   Tmp.sd <- apply(tmpX, 2, sd, na.rm=na.rm)    # standard deviation of each column\r\n                  #sd(as.matrix(tmpX), na.rm=na.rm) \r\n\r\n   vect1 <- t(t(rep(1,dim1[[1]])));\r\n   tmpX <- (tmpX - vect1 %*% Tmp.m) / (vect1 %*% Tmp.sd)\r\n   \r\n   if(dims==1) tmpX <- t(tmpX);\r\n\r\n }\r\n\r\n   tmpX\r\n} # normalization <- function(x, dims=2, na.rm=FALSE)\r\n\r\n# a<- matrix(1:10, nrow=2,ncol=5)\r\n# normalization(a, dim=2)\r\n# normalization(a[1,])\r\n# normalization(a[,1], dim=2)\r\n\r\n\r\n##################################################################\r\n# LocalSimilarity3<- function(lsTS1, lsTS2, maxDelay=1, numTimePoints=1, scale=F) \r\n#\r\n#  This function computes the local similarity score for two sequences.\r\n#\r\n# INPUT:\r\n# ======\r\n#\r\n#\tlsTS1, lsTS2\t: sequences to copute LS score\r\n#\tmaxDelay\t: maximum time shift allowed in computing LS score.\r\n#\tnumTimePoints\t: length of the sequences for computing LS score\r\n#\tscale\t\t: If TRUE, perform normalization first; False, otherwise.\r\n#\r\n# RETURN:\r\n# ======\r\n# \r\n#  A five element vector contains: c(scoreMax, (startX-length), (startY-length), length, PosOrNeg)\r\n#\r\nLocalSimilarity3<- function(lsTS1, lsTS2, maxDelay=1, numTimePoints=1, scale=F) \r\n{\r\n\r\nif(!is.array(lsTS1))\r\n{\r\n\tls.TS1x <- t(lsTS1)\r\n\tls.TS2x <- t(lsTS2)\r\n}\r\n\r\nls.TS1 <- ls.TS1x\r\nls.TS2 <- ls.TS2x\r\n\r\n\r\n\t\r\nif(scale==T)\r\n{\r\n\tlsTS1 <- (lsTS1-mean(lsTS1))/sd(lsTS1)  # Splus: stdev(lsTS1)\r\n\tlsTS2 <- (lsTS2-mean(lsTS2))/sd(lsTS2)  # Splus: stdev(lsTS2)\t\r\n}\r\n\r\n\r\nscoreMatrixPos <- matrix(0, numTimePoints+1,numTimePoints+1);\r\nscoreMatrixNeg <- matrix(0, numTimePoints+1, numTimePoints+1);\r\n\r\nscoreMax <- 0.;\r\nPosOrNeg <- 0.;\r\nstartX <- 0; # start of sub seq in dt1 from back to front\r\nstartY <- 0; # start of sub seq in dt2 from back to front\r\n\r\nThresh2 <- 0.000001\r\n\r\nfor(i in 2:(numTimePoints+1))\r\n{\r\n\tfor(j in 2:(numTimePoints+1))\r\n\t{\r\n\t\tif(abs(i-j) > maxDelay)\r\n\t\t\tnext;\r\n\t\t\r\n\t\tscoreMatrixPos[i,j] <- scoreMatrixPos[i-1,j-1] + lsTS1[i-1] * lsTS2[j-1];\r\n\t\tif(scoreMatrixPos[i,j] < 0)\r\n\t\t\tscoreMatrixPos[i,j] <- 0;\r\n\t\t\t\r\n\t\tscoreMatrixNeg[i,j] <- scoreMatrixNeg[i-1,j-1] - lsTS1[i-1] * lsTS2[j-1];\r\n\t\tif(scoreMatrixNeg[i,j] < 0)\r\n\t\t\tscoreMatrixNeg[i,j] <- 0;\r\n\t\t\r\n\t\tif(scoreMatrixPos[i,j] > scoreMax)\r\n\t\t{\r\n\t\t\tscoreMax <- scoreMatrixPos[i,j];\r\n\t\t\tstartX <- i;\r\n\t\t\tstartY <- j;\r\n\t\t\tPosOrNeg <- 1;\t\r\n\t\t}\t\r\n\t\t\r\n\t\tif(scoreMatrixNeg[i,j] > scoreMax)\r\n\t\t{\r\n\t\t\tscoreMax <- scoreMatrixNeg[i,j];\r\n\t\t\tstartX <- i;\r\n\t\t\tstartY <- j;\r\n\t\t\tPosOrNeg <- 0;\t\r\n\t\t}\r\n\t\t\r\n\t}\r\n}\r\n\r\nif(PosOrNeg == 1)\r\n{\r\n\tfor(i in 1:numTimePoints)\r\n    \tif(scoreMatrixPos[startX-i,startY-i]<=Thresh2)\r\n\t\t{    \t\tbreak;\r\n       }\r\n}\r\nelse {\r\n\tfor(i in 1:numTimePoints)\r\n       if(scoreMatrixNeg[startX-i, startY-i]<=Thresh2)\r\n\t\t{     \t\tbreak;\r\n       }\r\n\r\n}\r\nlength = i;\r\n\t\t\t\t\r\nreturn(c(scoreMax, (startX-length), (startY-length), length, PosOrNeg));\r\n}\r\n\r\n\r\n######################\r\n# compute the significance level of the LS score\r\n#\r\n# LocalSimilarity3<- function(lsTS1, lsTS2, maxDelay=1, numTimePoints=1, scale=F) \r\n######################\r\nsigTesting3 <- function(stOtu1, stOtu2, numPermu=10, maxDelay=1, numTimePoints=1, scale=F, indexNai)\r\n{\r\n\tscoreArray <- rep(0.0, numPermu+1);\r\n\t\r\n\tscoreMax1 <- LocalSimilarity3Nai(stOtu1, stOtu2, maxDelay, numTimePoints=length(stOtu1), scale)[indexNai]; #New: modified function\r\n\tscoreArray[1] <- scoreMax1;\r\n\thighScoreCnt <- 1;\r\n\t\r\n\tfor(idx in 1:(numPermu-1)){\r\n\t\tdtt1 <- stOtu1[sample(numTimePoints)];\r\n\t\tdtt2 <- stOtu2[sample(numTimePoints)];\r\n\t\tscoreTmp <- LocalSimilarity3Nai(dtt1, dtt2, maxDelay, numTimePoints=length(dtt1), scale)[indexNai]; #New: modified function\r\n\r\n\t\tscoreArray[idx+1] <- scoreTmp;\r\n\t\thighScoreCnt <- highScoreCnt + (scoreTmp >= scoreMax1);\r\n\t}\r\n\t\r\n\tpValue <- 1.0 * highScoreCnt / numPermu;\r\n\t\r\n\treturn(pValue);\r\n\t#scoreArray[numPermu+1] <- pValue;\r\n\t#return(scoreArray)\r\n}\r\n\r\n#ls01x15 <-sigTesting3(data1[,1], data1[,15], maxDelay=1, numTimePoints=length(data1[,1]), scale=T, 10)\r\n#ls01x16 <- sigTesting3(data1[,1], data1[,16], maxDelay=1, numTimePoints=length(data1[,1]), scale=T, 100)\r\n#ls01x16 <- sigTesting3(data1[,1], data1[,16], maxDelay=1, numTimePoints=length(data1[,1]), scale=T, 10)\r\n\r\n\r\n####################################################################\r\n#SigTestMaxDelay <- function(data1, N=dim(data1)[[2]], delay=0, permu=1000)\r\n#\r\n# \r\n# This function computes the p-value matrix for the corresponding LS score matrix\r\n# computed by LocalSimilarity3() above.\r\n#\r\n# Arguments: \r\n# ==========\r\n#\t    data1: a M by N data matrix/data frame. NOTE: should be at least two columns\r\n#\r\n#\t delay:  maximal time shift allowed between two subsequences.\r\n#\r\n#\tpermu: = number of permutations for compuating the p-value of the LS score\r\n#\r\n# RETURN:\r\n# =======\r\n#    \tlsMatrixN: A N x 10 matrix: c(""rIdx"", ""cIdx"", ""LSscore"", ""startR"", ""startC"", ""length"", \r\n#\t\t\t\t\t""PorN"", ""pValue"", ""cor"", ""corpVal""), saved in file \r\n#\tpaste(""lsMatrixN.R.MaxDelay"", delay, "".ran"", sample(10000:999999,1), "".txt"", sep="""")\r\n#\r\n# NOTE: \r\n# =====\r\n#\tdataX should have at least two columns\t\r\n#\r\n#\r\n#########################################################################\r\nSigTestMaxDelay <- function(data1, N=dim(data1)[[2]], delay=0, permu=1000)\r\n{\r\n\r\ndim1 <- dim(data1)\r\n\r\nif(is.null(dim1))\r\n  return (0)\r\n\r\n# lsMatrix to store pairwise results\r\nlsMatrixN <- matrix(0, N*N/2, 2+5+1+2);\r\n\r\nrIdx <- 0;\r\nfor(i in 1:(N-1))\r\n{\r\n\tfor(j in (i+1):N)\r\n\t{\r\n\t\tlsTmp <- LocalSimilarity3(data1[,i], data1[,j], maxDelay=delay, numTimePoints=length(data1[,1]), scale=F);\r\n\t\t\r\n\t\trIdx <- rIdx + 1;\r\n\t\tlsMatrixN[rIdx, 1] <- i;\t\t\t\t# \tfirst seq index\r\n\t\tlsMatrixN[rIdx, 2] <- j;\t\t\t\t#  \tsecond seq index\r\n\t\tlsMatrixN[rIdx, 3] <- lsTmp[1]; \t\t\t# \tLS score\r\n\t\tlsMatrixN[rIdx, 4] <- lsTmp[2]; \t\t\t# \tstartX\r\n\t\tlsMatrixN[rIdx, 5] <- lsTmp[3];\t\t\t\t# \tstartY\r\n\t\tlsMatrixN[rIdx, 6] <- lsTmp[4]; \t\t\t#\tlength\r\n\t\tlsMatrixN[rIdx, 7] <- lsTmp[5];\t\t\t\t#\tPosOrNeg\r\n\t\tlsMatrixN[rIdx, 8] <- sigTesting3(data1[,i], data1[,j], numPermu=permu, maxDelay=delay, numTimePoints=length(data1[,1]), scale=F)\r\n\r\n\t\tcorTmp <- cor(data1[,i], data1[,j]);\r\n\t\tlsMatrixN[rIdx,  9] <- corTmp;\r\n\t\tlsMatrixN[rIdx, 10] <- 0.5 + sign(corTmp) * (0.5 - pt(corTmp * sqrt((dim1[[1]]-1)/(1-corTmp^2)), df=(dim1[[1]]-1)) );\r\n\r\n\t}\r\n}\r\n\r\n# \'normalize\' the LS scores\r\nlsMatrixN[,3] <- lsMatrixN[,3]/dim1[[1]]\r\n\r\ndimnames(lsMatrixN)[[2]] <- c(""rIdx"", ""cIdx"", ""LSscore"", ""startR"", ""startC"", ""length"", ""PorN"", ""pValue"", ""cor"", ""corpVal"")\r\n\r\n# remove the empty rows\r\nlsMatrixN <- lsMatrixN[lsMatrixN[,1]>=1,]\r\n\r\nfileName <- paste(""lsMatrixN.R.MaxDelay"", delay, "".ran"", sample(10000:999999,1), "".txt"", sep="""")\r\nwrite.table(lsMatrixN, paste(fileName, sep=""""))\r\n\r\ndim(lsMatrixN)\r\n\r\n############## To read data:\r\n# lsMatrixN.rd <- read.table(paste(RWorkPath, fileName, sep=""""), header=TRUE)\r\n# lsMatrixN.rd[1:3,]\r\n# lsMatrixN[1:3,]\r\n\r\n} # end of SigTestMaxDelay() ...\r\n\r\n\r\n#SigTestMaxDelay(data1=dataXe[,1:10], N=dim(dataXe[,1:10])[[2]],delay=1)\r\n\r\n#SigTestMaxDelay(data1=DataSpots, N=dim(DataSpots)[[2]],delay=1)\r\n#SigTestMaxDelay(data1=DataSpots, N=dim(DataSpots)[[2]],delay=0)\r\n\r\n\r\n\r\n######################################\r\n# PlotOtuPair2:\r\n#\r\n# . This function is used to plot the significant OTU pairs.\r\n#\r\n# Input:\r\n# ======\r\n# . idx: the index number of the pair to be plotted in ls.res\r\n# . data: a 35 by 72 matrix\r\n# . ls.res: a N by 11 result matrix, a sublist of ls.res.all\r\n#\r\n####################################\r\nPlotOtuPair2<- function(idx=1,\r\n\t\t\tdata = dataXe, \r\n\t\t\tls.res = ls.res.all, \r\n\t\t\txdisp=0,\r\n\t\t\txcor= xdisp+0.25*dim(data)[[1]],\r\n\t\t\tydisp=0,\r\n\t\t\tycor=ydisp + 0.95*max(data[,c(ls.res[idx,1],ls.res[idx,2])]),\r\n\t\t\txlab=""time point"",\r\n\t\t\tylab= ""rel. abund.""\r\n\t\t\t)\r\n{\r\n\r\ni <- ls.res[idx,1]\r\nj <- ls.res[idx,2]\r\n\r\ndimnm <- dimnames(data)[[2]]\r\ndimnm <- c(substr(dimnm[1:58], start=2, stop=5), dimnm[59:72])\r\notuNames.dpbin.all <- paste(""otu "", 1:58, "": "", dimnm[1:58], sep="""")\r\notuNames.dpbin.all <- c(otuNames.dpbin.all, paste(""env "", 59:72, "": "", dimnm[59:72], sep=""""))\r\n\r\n\r\nylim1 <- 1.05*min(data[,c(ls.res[idx,1],ls.res[idx,2])])\r\nylim2 <- 1.05*max(data[,c(ls.res[idx,1],ls.res[idx,2])])\r\n\r\nplot(data[,j],  type=""l"", lty=2, lwd=0.5, ylim=c(ylim1, ylim2), xlab=xlab, ylab=ylab, cex.lab=1.8, cex.axis=1.5)\r\nlines(data[,i], type=""l"", lty=1, lwd=0.5, col=1)\r\n\r\nix <- (ls.res[idx,5]):(ls.res[idx,5]+ls.res[idx,6]-1)\r\nlines(ix, data[ix,j], type=""l"", lty=2, lwd=3., col=1)\r\n\r\nix <- (ls.res[idx,4]):(ls.res[idx,4]+ls.res[idx,6]-1)\r\nlines(ix, data[ix,i], type=""l"", lwd=3., col=1)\r\n\r\n\r\n# start of match points\r\nstartX <- c(ls.res[idx,4], ls.res[idx,5])\r\nstartY <- c(data[startX[1], i], data[startX[2],j])\r\npoints(startX, startY,pch=\'O\', cex=1.2)\r\n\r\n# end of match points\r\nstartX <- c(ls.res[idx,4], ls.res[idx,5])+ls.res[idx,6]-1\r\nstartY <- c(data[startX[1], i], data[startX[2],j])\r\npoints(startX, startY,pch=20, cex=2.5)\r\n\r\nlegend(xcor, ycor, legend=paste(otuNames.dpbin.all[c(i,j)], sep=""""), \r\n\tlty=c(1:2), lwd=2, cex=1.2)\r\n\r\n#text(xi,yi,paste(i, sep=""""), col=1, cex=1.6)\r\n#text(xj,yj,paste(j,sep=""""), col=1, cex=1.6)\r\ngrid()\r\n\r\n}\r\n\r\n#PlotOtuPair2(idx=15, ls.res=ls.res.all, ylab=""rel. abund.(normalized)"", data=dataXe)\r\n#PlotOtuPair2(idx=15, ls.res=ls.res.all, ylab=""rel. abund."", data=dataTe)\r\n\r\n\r\n################################################\r\n# PlotOtuPair3():\r\n#\r\n# This function is modified from PlotOtuPair2()\r\n#\r\n# This function plots the aligned view of the OTU pairs with\r\n# significant local similarity scores with some time delay\r\n# only the matching subsequences are ploted.\r\n#\r\n####################\r\nPlotOtuPair3<- function(idx=1,\r\n\t\t\tdata = dataTe, \r\n\t\t\tls.res = ls.res.all, \r\n\t\t\txlab=""time point"",\r\n\t\t\tylab= ""rel. abund.""\r\n\t\t\t)\r\n{\r\ni <- ls.res[idx,1]\r\nj <- ls.res[idx,2]\r\n\r\ndimnm <- dimnames(data)[[2]]\r\ndimnm <- c(substr(dimnm[1:58], start=2, stop=5), dimnm[59:72])\r\notuNames.dpbin.all <- paste(""otu "", 1:58, "": "", dimnm[1:58], sep="""")\r\notuNames.dpbin.all <- c(otuNames.dpbin.all, paste(""env "", 59:72, "": "", dimnm[59:72], sep=""""))\r\n\r\n\r\ndt1 <- data[(ls.res[idx,4]):(ls.res[idx,4]+ls.res[idx,6]-1),ls.res[idx,1]]\r\ndt2 <- data[(ls.res[idx,5]):(ls.res[idx,5]+ls.res[idx,6]-1),ls.res[idx,2]]\r\n\r\nylim1 <- 1.05*min(dt1,dt2)\r\nylim2 <- 1.05*max(dt1,dt2)\r\n\r\nplot(dt2,  type=""l"", lty=2, lwd=2.5, ylim=c(ylim1, ylim2), xlab=xlab, ylab=ylab, cex.lab=1.8, cex.axis=1.5)\r\nlines(dt1, type=""l"", lty=1, lwd=2.5, col=1)\r\n\r\nxdisp=0\r\nxcor= xdisp+0.15*length(dt1)\r\nycor=0.99*max(dt1,dt2)\r\nlegend(xcor, ycor, legend=paste(otuNames.dpbin.all[c(i,j)], sep=""""), \r\n\tlty=c(1:2), lwd=2, cex=1.2)\r\n\r\n#text(xi,yi,paste(i, sep=""""), col=1, cex=1.6)\r\n#text(xj,yj,paste(j,sep=""""), col=1, cex=1.6)\r\ngrid()\r\n\r\n}\r\n\r\n#PlotOtuPair3(idx=15, ls.res=ls.res.all, ylab=""rel. abund.(normalized)"", data=dataXe)\r\n#PlotOtuPair3(idx=15, ls.res=ls.res.all, ylab=""rel. abund."", data=dataTe)\r\n#PlotOtuPair2(idx=15, ls.res=ls.res.all, ylab=""rel. abund.(normalized)"", data=dataXe)\r\n\r\n\r\n############################################################\r\n# CovaryOtuSif(data): \r\n#\r\n# . This function create SIF file from LSA result. SIF file is needed \r\n# by Cytoscape (software) to generate Co-varying graphs\r\n#\r\n# Input:\r\n#\r\n# . data: a N by 11 matrix, a subset of ls.res.all\r\n#\r\n# Output:\r\n#\r\n# . lsla.covary.Notu.ranxxxxx.sif: a file needed by Cytoscape \r\n#               to draw covarying graph\r\n#\r\n##########################################  \r\nCovaryOtuSif <- function(data, otu.names=otuNames.dpbin.all) {\r\n\r\ntmp <- data\r\ndim1 <- dim(tmp)\r\ncovary <- matrix(0, ncol=3, nrow=dim1[[1]])\r\n#covary[1,2] <- ""empty"" \r\n\r\nfor(idx in 1:dim1[[1]])\r\n{\r\n\tcovary[idx,1] <- otu.names[tmp[idx,1]]\r\n\tcovary[idx,3] <- otu.names[tmp[idx,2]]\r\n\r\n\tif (tmp[idx,4]==tmp[idx,5]) { # no time delay, undirected\r\n\t\tif(tmp[idx,7] >= 1 )\r\n\t\t{\r\n\t\t\tcovary[idx,2] <- ""pu "" #1 # pu: positive, undirected\r\n\t\t} else {\r\n\t\t\tcovary[idx,2] <- ""nu "" #2 # nu: negative, undirected\r\n\t\t}\r\n\t} else if (tmp[idx,4] > tmp[idx,5])  { # otu2 is ahead of otu1, arrow points to otu1\r\n\t\tif(tmp[idx,7] >= 1 )\r\n\t\t{\r\n\t\t\tcovary[idx,2] <- ""pdl"" #3 # pdl: pos, directed to the left\r\n\t\t} else {\r\n\t\t\tcovary[idx,2] <- ""ndl"" #4 # ndl: neg, directed to the left\r\n\t\t}\r\n\t} else {  # otu1 is ahead of otu2, arrow points to otu2\r\n\t\tif(tmp[idx,7] >= 1 )\r\n\t\t{\r\n\t\t\tcovary[idx,2] <- ""pdr"" #5 # pdr: pos, directed to the right\r\n\t\t} else {\r\n\t\t\tcovary[idx,2] <- ""ndr"" #6 # ndr: neg, directed to the right\r\n\t\t}\r\n\t}\r\n}\r\n\r\n#covary\r\n\r\nfileName <- paste(""lsla.covary."", dim(covary)[[1]], ""all.ran"", sample(10000,1),"".sif"", sep="""")\r\nwrite.table(covary, file=fileName, quote=FALSE, col.names=FALSE, row.names=FALSE)\r\n\r\n} # CovaryOtuSif <- function(data)\r\n\r\n#CovaryOtuSif(tmp5)\r\n\r\n#############################################################\r\n# CovaryOtuSif2 <- function(data, otu.names=otuNames.dpbin.all)\r\n#\r\n# This function is modified from the one above. It creates SIF file and \r\n# also gives the LS score for each pair.\r\n##########################################  \r\nCovaryOtuSif2 <- function(data, otu.names=otuNames.dpbin.all) {\r\n\r\ntmp <- data\r\ndim1 <- dim(tmp)\r\ncovary <- matrix(0, ncol=5, nrow=dim1[[1]])\r\n#covary[1,2] <- ""empty"" \r\n\r\nfor(idx in 1:dim1[[1]])\r\n{\r\n\tcovary[idx,1] <- otu.names[tmp[idx,1]]\r\n\tcovary[idx,3] <- otu.names[tmp[idx,2]]\r\n\tcovary[idx,4] <- round(tmp[idx,3], digits=4)\r\n\tcovary[idx,5] <- tmp[idx,4]-tmp[idx,5] # time shift\r\n\r\n\tif (tmp[idx,4]==tmp[idx,5]) { # no time delay, undirected\r\n\t\tif(tmp[idx,7] >= 1 )\r\n\t\t{\r\n\t\t\tcovary[idx,2] <- ""pu "" #1 # pu: positive, undirected\r\n\t\t} else {\r\n\t\t\tcovary[idx,2] <- ""nu "" #2 # nu: negative, undirected\r\n\t\t}\r\n\t} else if (tmp[idx,4] > tmp[idx,5])  { # otu2 is ahead of otu1, arrow points to otu1\r\n\t\tif(tmp[idx,7] >= 1 )\r\n\t\t{\r\n\t\t\tcovary[idx,2] <- ""pdl"" #3 # pdl: pos, directed to the left\r\n\t\t} else {\r\n\t\t\tcovary[idx,2] <- ""ndl"" #4 # ndl: neg, directed to the left\r\n\t\t}\r\n\t} else {  # otu1 is ahead of otu2, arrow points to otu2\r\n\t\tif(tmp[idx,7] >= 1 )\r\n\t\t{\r\n\t\t\tcovary[idx,2] <- ""pdr"" #5 # pdr: pos, directed to the right\r\n\t\t} else {\r\n\t\t\tcovary[idx,2] <- ""ndr"" #6 # ndr: neg, directed to the right\r\n\t\t}\r\n\t}\r\n}\r\n\r\n#covary\r\n\r\nfileName <- paste(""lsla.covary."", dim(covary)[[1]], ""all.edge.ran"", sample(10000,1),"".sif"", sep="""")\r\nwrite.table(covary, file=fileName, quote=FALSE, col.names=FALSE, row.names=FALSE)\r\n\r\n} # CovaryOtuSif2 <- function(data)\r\n\r\n#CovaryOtuSif2(tmp5)\r\n\r\n\r\n\r\n', '#--------------------------------------------------------------------------------------\r\n#\r\n#  July 2022\r\n#  Code associated with publication:\r\n#  ""Kinetic modulation of bacterial hydrolases by microbial community structure in coastal waters"" \r\n#   Authors: N. Abad, A. Uranga, B. Ayo, J.M. Arrieta,Z. Baa, I. Artolozaga, I. Aza, J. Iriberri, Santos J. Gonzlez-Roj and M. Unanue\r\n#\r\n#\r\n#  Creative Commons Licence: Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)\r\n#--------------------------------------------------------------------------------------\r\n\r\n### Adapted from Ruan et al (2006): doi:10.1093/bioinformatics/btl417\r\n\r\n### The modified module from the original source is displayed below. The explanations of the original code \r\n### have been retained for the sake of better comprehension. Specific modifications related to a function are \r\n### indicated by the comment ""#New: modified function"".\r\n\r\n\r\n### References of the modified code \r\n\r\n# Storey JD, Bass AJ, Dabney A, Robinson D (2022). \r\n#     qvalue: Q-value estimation for false discovery rate control. \r\n#     R package version 2.28.0, http://github.com/jdstorey/qvalue.\r\n\r\n# Zeileis, A. & Grothendieck, G. (2005). \r\n#     zoo: S3 Infrastructure for Regular and Irregular Time Series. \r\n#     Journal of Statistical Software, 14(6), 1-27. doi:10.18637/jss.v014.i06\r\n\r\n\r\n#--------------------------------------------------------------------------------------\r\n\r\n######################################################################\r\n\r\n# LSAscript.R:\r\n\r\n# This script Computes LSA scores and PCC scores for the \r\n# ARISA time series as well as the env factors\r\n\r\n############## 0. setup   ############################################\r\n\r\n# 0_RAW DATA FRAME: \r\n\r\n# ""m x n"" (rowsxcols) data matrix, indicate missing values (NA)\r\n\r\n## NOTE: set working directory in the same folder where all files provided \r\n## by Ruan et al 2006 package are stored.\r\n\r\n\r\nsetwd(""C:\\\\Users\\\\...\\\\code\\\\"") #New: change me\r\n\r\nsource(""LSAfunctions.R"")\r\n\r\ndatos<-read.table(file.choose(),header=TRUE, sep=""\\t"", na.strings= """",row.names = 1)\r\ndim (datos)\r\nstr (datos)\r\nhead(datos)\r\n\r\n#New: Linear interpolation of missing values (NAs) (Zeleis et al 2021)\r\n\r\nlibrary(zoo) \r\n\r\nidx <- colSums(!is.na(datos)) > 1\r\nidx\r\ndatos[ , idx] <- na.approx(datos[ , idx])\r\ndataTe<-datos\r\ndataTe\r\nstr(dataTe)\r\n\r\n\r\n############## 1. Preparing data  ##############################################\r\n\r\n# normalization: this is not necessary for analysis but rather in the result plotting \r\n# since what matters in the next step is their ranks\r\n\r\ndataXe <- normalization(dataTe)\r\ndataXe <- as.data.frame(dataXe)\r\ndataXe.nt <- normalTransform(dataXe)\r\n\r\n\r\n############# 2. Computing Local Similarity scores ####################\r\n\r\nNumPermu <- 1000\r\n\r\nLocalSimilarity3Nai<- function(lsTS1, lsTS2, maxDelay=1, numTimePoints=1, scale=F)  #New: modified function\r\n  \r\n{\r\n  \r\n  if(!is.array(lsTS1))\r\n  {\r\n    ls.TS1x <- t(lsTS1)\r\n    ls.TS2x <- t(lsTS2)\r\n  }\r\n  \r\n  ls.TS1 <- ls.TS1x\r\n  ls.TS2 <- ls.TS2x\r\n  \r\n  \r\n  if(scale==T)\r\n  {\r\n    lsTS1 <- (lsTS1-mean(lsTS1))/sd(lsTS1)  # Splus: stdev(lsTS1)\r\n    lsTS2 <- (lsTS2-mean(lsTS2))/sd(lsTS2)  # Splus: stdev(lsTS2)\t\r\n  }\r\n  \r\n  scoreMatrixPos <- matrix(0, numTimePoints+1,numTimePoints+1);\r\n  scoreMatrixNeg <- matrix(0, numTimePoints+1,numTimePoints+1);\r\n  \r\n  scoreMax <- 0.;\r\n  PosOrNeg <- 0.;\r\n  startX <- 0; # start of sub seq in dt1 from back to front\r\n  startY <- 0; # start of sub seq in dt2 from back to front\r\n  \r\n  Thresh2 <- 0.000001\r\n  Tabla=rep(NA, 5)\r\n  TablaFinal=rep(NA, 5)\r\n  \r\n  for(i in 2:(numTimePoints+1)){\r\n    for(j in 2:(numTimePoints+1)){\r\n      if(abs(i-j) > maxDelay)\r\n        next;\r\n      \r\n      scoreMatrixPos[i,j] <- scoreMatrixPos[i-1,j-1] + lsTS1[i-1] * lsTS2[j-1];\r\n      if(scoreMatrixPos[i,j] < 0)\r\n        scoreMatrixPos[i,j] <- 0;\r\n      \r\n      scoreMatrixNeg[i,j] <- scoreMatrixNeg[i-1,j-1] - lsTS1[i-1] * lsTS2[j-1];\r\n      if(scoreMatrixNeg[i,j] < 0)\r\n        scoreMatrixNeg[i,j] <- 0;\r\n      \r\n      if(scoreMatrixPos[i,j] > 0)\r\n      {\r\n        scoreMax <- scoreMatrixPos[i,j];\r\n        startX <- i;\r\n        startY <- j;\r\n        PosOrNeg <- 1;\t\r\n      }\t\r\n      \r\n      if(scoreMatrixNeg[i,j] > 0)\r\n      {\r\n        scoreMax <- scoreMatrixNeg[i,j];\r\n        startX <- i;\r\n        startY <- j;\r\n        PosOrNeg <- 0;\t\r\n      }\r\n      \r\n      if(PosOrNeg == 1) {\r\n        for(k in 1:numTimePoints)\r\n          if(scoreMatrixPos[startX-k,startY-k]<=Thresh2){    \t\tbreak;\r\n          }\r\n      } else {\r\n        for(k in 1:numTimePoints)\r\n          if(scoreMatrixNeg[startX-k, startY-k]<=Thresh2)\r\n          {     \t\tbreak;\r\n          }\r\n        \r\n      }\r\n      length = k;\r\n      \r\n   \r\n      Exp=c(scoreMax, (startX-length), (startY-length), length, PosOrNeg)\r\n      Tabla=rbind(Tabla,Exp)\r\n      \r\n    }\r\n    \r\n    if(!is.null(dim(Tabla))){\r\n      Tabla=Tabla[-1,]\r\n    }\r\n    \r\n    TablaFinal=rbind(TablaFinal,Tabla)\r\n    Tabla=rep(NA, 5)\r\n  }\r\n  \r\n  TablaFinal=TablaFinal[-1,]\r\n  \r\n  \r\n  return(TablaFinal);\r\n}\r\n\r\nLocalSimilarity3Nailong<- function(lsTS1, lsTS2, maxDelay=1, numTimePoints=1, scale=F)  #New: modified function\r\n\r\n  {\r\n  \r\n  if(!is.array(lsTS1))\r\n  {\r\n    ls.TS1x <- t(lsTS1)\r\n    ls.TS2x <- t(lsTS2)\r\n  }\r\n  \r\n  ls.TS1 <- ls.TS1x\r\n  ls.TS2 <- ls.TS2x\r\n  \r\n  \r\n  if(scale==T)\r\n  {\r\n    lsTS1 <- (lsTS1-mean(lsTS1))/sd(lsTS1)  # Splus: stdev(lsTS1)\r\n    lsTS2 <- (lsTS2-mean(lsTS2))/sd(lsTS2)  # Splus: stdev(lsTS2)\t\r\n  }\r\n  \r\n  \r\n  scoreMatrixPos <- matrix(0, numTimePoints+1,numTimePoints+1);\r\n  scoreMatrixNeg <- matrix(0, numTimePoints+1,numTimePoints+1);\r\n  \r\n  scoreMax <- 0.;\r\n  PosOrNeg <- 0.;\r\n  startX <- 0; # start of sub seq in dt1 from back to front\r\n  startY <- 0; # start of sub seq in dt2 from back to front\r\n  \r\n  Thresh2 <- 0.000001\r\n  Tabla=rep(NA, 7)\r\n  TablaFinal=rep(NA, 7)\r\n  \r\n  for(i in 2:(numTimePoints+1)){\r\n    for(j in 2:(numTimePoints+1)){\r\n      if(abs(i-j) > maxDelay)\r\n        next;\r\n      \r\n      scoreMatrixPos[i,j] <- scoreMatrixPos[i-1,j-1] + lsTS1[i-1] * lsTS2[j-1];\r\n      if(scoreMatrixPos[i,j] < 0)\r\n        scoreMatrixPos[i,j] <- 0;\r\n      \r\n      scoreMatrixNeg[i,j] <- scoreMatrixNeg[i-1,j-1] - lsTS1[i-1] * lsTS2[j-1];\r\n      if(scoreMatrixNeg[i,j] < 0)\r\n        scoreMatrixNeg[i,j] <- 0;\r\n      \r\n      if(scoreMatrixPos[i,j] > 0)\r\n      {\r\n        scoreMax <- scoreMatrixPos[i,j];\r\n        startX <- i;\r\n        startY <- j;\r\n        PosOrNeg <- 1;\t\r\n      }\t\r\n      \r\n      if(scoreMatrixNeg[i,j] > 0)\r\n      {\r\n        scoreMax <- scoreMatrixNeg[i,j];\r\n        startX <- i;\r\n        startY <- j;\r\n        PosOrNeg <- 0;\t\r\n      }\r\n      \r\n      if(PosOrNeg == 1) {\r\n        for(k in 1:numTimePoints)\r\n          if(scoreMatrixPos[startX-k,startY-k]<=Thresh2){    \t\tbreak;\r\n          }\r\n      } else {\r\n        for(k in 1:numTimePoints)\r\n          if(scoreMatrixNeg[startX-k, startY-k]<=Thresh2)\r\n          {     \t\tbreak;\r\n          }\r\n        \r\n      }\r\n      length = k;\r\n      \r\n\r\n      if (PosOrNeg==1) {\r\n        TS1 <-lsTS1[(startX-length):( (startX-length)+(length-1) )] \r\n        TS2 <-lsTS2[(startY-length):( (startY-length)+(length-1) )] \r\n      } else {\r\n        TS1 <-lsTS1[(startX-length):( (startX-length)+(length-1) )] \r\n        TS2 <-rev(lsTS2[(startY-length):( (startY-length)+(length-1) )] )\r\n      }\r\n      corTmp <- cor(TS1, TS2)\r\n      \r\n      #calculate the pvalue\r\n      if( is.na(corTmp) ){\r\n        CorPval <- NA\r\n      } else {\r\n        CorPval <- 0.5 + sign(corTmp) * (0.5 - pt(corTmp * sqrt((dim1[[1]]-1)/(1-corTmp^2)), df=(dim1[[1]]-1)) )\r\n      }\r\n      Exp=c(scoreMax, (startX-length), (startY-length), length, PosOrNeg, corTmp, CorPval)\r\n      Tabla=rbind(Tabla,Exp)\r\n      \r\n    }\r\n    \r\n    if(!is.null(dim(Tabla))){\r\n      Tabla=Tabla[-1,]\r\n    }\r\n    \r\n    TablaFinal=rbind(TablaFinal,Tabla)\r\n    Tabla=rep(NA, 7)\r\n  }\r\n  \r\n  TablaFinal=TablaFinal[-1,]\r\n  \r\n\r\n  \r\n  return(TablaFinal);\r\n}\r\n\r\n\r\ndata1=dataXe.nt\r\nN=dim(data1)[[2]]\r\ndelay=3   #New: change me according to the established criteria\r\npermu=NumPermu\r\n\r\n\r\ndim1 <- dim(data1)\r\n\r\nif(is.null(dim1))\r\n  return (0)\r\n\r\nrIdx <- 0;\r\nSuperTabla <- rep(NA,10)\r\nfor(i in 1:(N-1)){\r\n  for(j in (i+1):N){\r\n    vectorPvalue <- 0\r\n    lsTmp <- LocalSimilarity3Nailong(lsTS1 = data1[,i], lsTS2 = data1[,j], maxDelay=delay, numTimePoints=length(data1[,1]), scale=F); #New: modified function\r\n    \r\n\r\n    nrepeat=length(lsTmp[,1])\t  # \tfirst seq index\r\n    column1=rep(i,nrepeat)      #  \tfirst seq index\r\n    column2=rep(j,nrepeat)      #  \tsecond seq index\r\n    \r\n    for( ncount in 1:nrepeat){\r\n      pvalue <- sigTesting3(stOtu1 = data1[,i], stOtu2 = data1[,j], numPermu=permu, maxDelay=delay, numTimePoints=length(data1[,1]), scale=F,indexNai = ncount) #New: modified function\r\n      vectorPvalue <- cbind(vectorPvalue, pvalue)\r\n    }\r\n    column10=vectorPvalue[-1]\r\n    \r\n    Exp=cbind(column1,column2,lsTmp,column10)\r\n    \r\n    SuperTabla=rbind(SuperTabla,Exp)\r\n  }\r\n}\r\n\r\nSuperTabla=SuperTabla[-1,]\r\n\r\n\r\n# \'normalize\' the LS scores\r\nSuperTabla[,3] <- SuperTabla[,3]/dim1[[1]]\r\n\r\ndimnames(SuperTabla)[[2]] <- c(""rIdx"", ""cIdx"", ""LSscore"", ""startR"", ""startC"", ""length"", ""PorN"",  ""cor"", ""corpVal"",""pValue"")\r\n\r\n# remove the empty rows\r\nSuperTabla <- SuperTabla[SuperTabla[,1]>=1,]\r\n\r\nfileName <- paste(""SuperTabla.AllDelays"", delay, \r\n                  "".ran"", sample(10000:999999,1), "".txt"", sep="""") \r\n\r\nwrite.table(SuperTabla, paste(fileName, sep=""""))\r\n\r\ndim(SuperTabla)\r\n\r\n\r\n# reading LS scores\r\nls.res.all <- SuperTabla\r\n  \r\ndim1 <- dim(ls.res.all) # n rows x 10\r\nls.res.all[1:3,]\r\n\r\ndimnames2 <- dimnames(ls.res.all)[[2]]\r\n\r\ntmp <- ls.res.all[,c(1:10, 10)]  # extend the result data frame to hold q-values\r\ntmp[,11] <- 0\r\n\r\ndimnames(tmp)[[2]] <- c(dimnames2, ""q-val"")\r\n\r\n\r\n##### 3.Computing q-values by QVALUE (Storey et al 2022, http://github.com/jdstorey/qvalue) ########\r\n\r\nlibrary(""devtools"")\r\n\r\nlibrary(qvalue)  # need to install it in R first\r\n\r\np <- ls.res.all[,8]\r\nqobj <- qvalue(p)\r\n\r\nplot(qobj)\r\nhist(qobj)\r\nrownames(ls.res.all)<-NULL\r\nls.res.all <-as.data.frame(ls.res.all)\r\nls.res.all[,11] <- qobj$qvalues\r\nhead(ls.res.all)\r\n\r\nls.res.all2<-round(ls.res.all,4) \r\nls.res.all2\r\n\r\nhead(ls.res.all2)\r\n\r\n# Note: This need to be run only once \r\n\r\nwrite.table(ls.res.all2, file=paste(""Set Path"", ""ls.res.all.txt"", sep="""")) # change me\r\n\r\n####data ready:\r\n# ls.res.all: an N by 11 matrix, with LSA and PCC score and their significance\r\n# ls.dataTe.txt: data without normalization\r\n# ls.dataXe.txt: normalized data\r\n# ls.dataXe.nt.txt: data with Normal score transformation\r\n\r\n\r\n############# 4. Analyzing results ###########################################\r\n\r\nls.res.all <- ls.res.all2\r\n\r\ntmp <- ls.res.all\r\ntmp\r\nD<-(tmp[,4])-(tmp[,5])\r\nD\r\ntmp[,12] <- D\r\nhead(tmp)\r\n\r\n## Filtering of the correlations\r\n\r\ntmp <- tmp[tmp[,1]<=58,]\r\ntmp <- tmp[tmp[,2]>58,]\r\ndim(tmp) # n rows x 12\r\n\r\ntmp1 <- tmp[tmp[,8]<=0.05,] # Filtering correlations with LS score p-value <0.05\r\ndim(tmp1) \r\n\r\ntmp2 <- tmp1[tmp1[,11]<=0.05,] # Filtering correlations with Q-value <0.05\r\ndim(tmp2)\r\n \r\ntmp3 <- tmp2[tmp2[,12]<1,] # Filtering correlations with Delay <1\r\ndim(tmp3)\r\n\r\ntmp4 <- tmp3[tmp3[,12]>-1,] # Filtering correlations with Delay >-1\r\ndim(tmp4)\r\n\r\ntmp5 <- tmp4[tmp4[,6]>16,] # Filtering correlations with length >16\r\ndim(tmp5)\r\nhead(tmp5)\r\n\r\ntmp6 <- tmp5[tmp5[,3]>0.10,] # Filtering correlations with LS score value > 0.10\r\ndim(tmp6)\r\nhead(tmp6)\r\n\r\n# Matrix ordination and naming rIdx, cIdx, lenght y Delay\r\n\r\ntmp6ord<-tmp6[order(tmp6[,1], tmp6[,2],tmp6[,6],tmp6[,12],decreasing=TRUE),]\r\nhead(tmp6ord)\r\n\r\nwrite.csv(tmp6ord, file=paste(""Set Path"", ""Set Name.csv"", sep="",""))\r\n\r\n\r\n#Variable names\r\n\r\nfor (i in 1:58){\r\n  tmp6ord$rIdx[tmp6ord$rIdx==i]<- colnames(dataTe)[i]\r\n  tmp6ord$cIdx[tmp6ord$cIdx==i]<- colnames(dataTe)[i]\r\n}\r\n\r\nhead(tmp6ord)\r\n\r\nwrite.csv(tmp6ord, file=paste(""Set Path"", ""Set Name.csv"", sep="","")) #change me\r\n\r\n']","Data for LSA analysis This code is supplementary to the paper ""Kinetic modulation of bacterial hydrolases by microbial community structure in coastal waters"" by Abad et al. It contains the following files: 0) README.txt: This README file.1) Complex saturation kinetics modelling.R: It contains the code developed for the determination of the kinetic parameters of the extracellular enzymatic activities by fitting the hydrolysis rates to four different kinetic models of increasing complexity using a non-linear least squares regression.2) LSA functions.R: It contains the functions implemented in R to perform the Local Similarity analysis.Our specific modifications related to the function LocalSimilarity3 are indicated by the comment ""#New: modified function"".3) LSA script.R: It is an updated version of the code developed by Ruan et al (2006) that has been used to perform the Local Similarity analysis in our study. The comment ""#New: modified function indicates our specific modifications within the original code.The following modifications were added to the original code: The calculation of the linear interpolation of missing values (NAs) using the R package zoo (Zeleis et al 2021).The calculation of the q-values by using the R package qvalue (Storey et al 2022).NOTE: it is important to set the working directory in the same folder where all the provided files are stored.",0
In-depth comparative analysis of Tritrichomonas foetus transcriptomics reveals novel genes linked with adaptation to feline host,"T. foetus is a protozoa flagellated and has been described as a parasite of cattle, cats and pigs. In spite of its widespread prevalence, feline T. foetus has received considerably less attention than venereal tritrichomonosis and little is known about molecular mechanisms that participate in feline host infection. In this work, we integrated available T. foetus transcriptomics information by bioinformatics and we explored the differences at transcript level with the aim to analyze the pathogenesis and adaptation processes, particularly for the feline isolate. In this sense, our analysis revealed higher abundance levels of predicted virulence factors, such as proteases and surface antigens, in feline isolate. On the basis of a comparative and expression analysis of T. foetus genes, we proposed putative virulence factors that could be involved in feline infection. Moreover, we observed a great abundance of predicted transcription factors of the family of Myb proteins (MYB) and, by a promoter analysis, we revealed that MYB related proteins could participate in the regulation of gene transcription in T. foetus. We concluded that this integrated approach will be a useful resource for future studies about the host-parasite interaction and thus, for identification of new targets of study for improving diagnosis and therapeutic intervention of the feline tritrichomonosis.","['#In-depth comparative analysis of Tritrichomonas foetus transcriptomics reveals novel genes linked with adaptation to feline host\n#author: Andrs Mariano Alonso (amalonso@intech.gov.ar)\n\nmatrix<-as.matrix(read.table(""matrixfpkm.csv"",sep=""\\t"",header = T,row.names = 1))\n\nlibrary(clusterSim)\nmd<-dist(matrix,method = ""euclidean"")\nmin_nc=200\nmax_nc=600\npasos= 5  \nres <- array(0, c(length(seq(min_nc,max_nc,pasos)), 2))\nres[,1] <- seq(min_nc,max_nc,pasos)\nclusters <- NULL\n\nfor (nc in 1:length(res[,1])){\n  hc <- hclust(md, method=""average"")\n  cl2 <- cutree(hc, k=res[nc,1])\n  res[nc, 2]<- DB <- index.DB(matrix, cl2, centrotypes=""centroids"")$DB\n  clusters <- rbind(clusters, cl2)\n}\n\nprint(paste(""min DB for"",res[which.min(res[,2]),][1],""clusters="",min(res[,2])))\n#print(clusters[which.min(res[,2]),])\nwrite.table(res,file=""DB_res.csv"",sep="";"",dec="","",row.names=TRUE,col.names=FALSE)\n\n\n####plot dbi\n\nplot(res, type=""p"", pch=0, xlab=""Number of clusters"", ylab=""DB"", xaxt=""n"")\naxis(1, c(min_nc:max_nc))\n\ndbi<-read.table(""DB_res.csv"",sep="";"")\n\n\nggplot(dbi, aes(x = V2,y=V3,))+geom_point(alpha=0.3) +geom_point(data=dbi[58,],size=3,aes(x=V2,y=V3), color=\'red\')+theme_minimal(base_line_size = NULL,base_family = ""Helvetica"",base_size = 15\n)+labs(x=""N clusters"", y =""DBI"")\n\n', '#In-depth comparative analysis of Tritrichomonas foetus transcriptomics reveals novel genes linked with adaptation to feline host\n#author: Andr�s Mariano Alonso (amalonso@intech.gov.ar)\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(plot3D)\ngenes.fpkm<-read.csv(""matrixfpkm.csv"",row.names = 1 ,header = TRUE) #input is a matrix of samples vs transcripts abundance data (fpkm)\npca.genes<-prcomp(genes.fpkm)\nout_r<-as.data.frame(pca.genes$rotation)\noutpercentage <- round(pca.genes$sdev / sum(pca.genes$sdev) * 100, 2) ## porcentaje de la varianza\noutpercentage <- paste( colnames(out_r), ""("", paste( as.character(outpercentage), ""%"", "")"", sep="""") )\nout_r$feature <- row.names(out_r)\n\n\n#######################\np1<-ggplot(out_r,aes(x=PC2,y=PC3,label=feature,color=feature ))\np1<-p1 + geom_label(aes(fill = factor(feature)), colour = ""white"", fontface = ""bold"")\np1<-p1+xlab(outpercentage[2]) + ylab(outpercentage[3])\n\n#########################################\np2<-ggplot(out_r,aes(x=PC1,y=PC3,label=feature,color=feature ))\np2<-p2 + geom_label(aes(fill = factor(feature)), colour = ""white"", fontface = ""bold"")\np2<-p2 +xlab(outpercentage[1]) + ylab(outpercentage[3])\n######################################\np3<-ggplot(out_r,aes(x=PC1,y=PC2,label= feature,color=feature))\np3<-p3 + geom_point(size=8)#+ geom_text(color=""black"")\np3<-p3+xlab(outpercentage[1]) + ylab(outpercentage[2])#+ theme(legend.position = ""none"")\n############################plot3d\np4<-scatter3D(out_r$PC1,out_r$PC2,out_r$PC3,bty = ""g"", type=""h"",\n              pch = 20, cex = 6, ticktype = ""detailed"" \n              , colkey = FALSE,col = c(""#C07402"",""#78BECB"",""#64D59D""),\n              xlab =outpercentage[1],\n              ylab=outpercentage[2],\n              zlab=outpercentage[3],\n              xlim=c(-0.5,1.5),\n              ylim=c(-1.5,1),\n              zlim=c(-2,0.5))\n\n\ntext3D(out_r$PC1,out_r$PC2,out_r$PC3, labels = row.names(out_r),add = TRUE, colkey = FALSE, cex = 1)\nrow.names(out_r)<-c(""PIG30/1"",""G10/1"",""BP-4"")\n\n##################################\n', '#In-depth comparative analysis of Tritrichomonas foetus transcriptomics reveals novel genes linked with adaptation to feline host\n#author: Andrs Mariano Alonso (amalonso@intech.gov.ar)\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(dplyr)\nlibrary(ggpubr)\nlibrary(data.table)\n\n\n############################### fc calculation vs median of pig vs bovine isolates\n\nmatrix<-read.table(""/home/andres/Escritorio/review/code/matrixfpkm.csv"",sep = ""\\t"",row.names = 1, header = T)#input is a matrix of samples vs transcrpit fpkm values#\nnew<-as.data.frame(cbind(matrix[,(""feline"")],rowMeans(matrix[,c(""porcine"",""bovine"")])))  \ncolnames(new)<-c(""feline"",""median_pb"")\n\nd<-c()\ncolx<-""feline""\ncoly<-""median_pb""\nfor(i in 1:length(new$feline)){\n  \n  new.fold<-log2(new[i,colx]/new[i,coly])\n  d<-c(d,new.fold)\n  \n  \n}\n\nnew$fc<-d\n\n##########################################scatterplots of virulence factors\nsetwd(""ids/"") ##input is a directory that contain files with extension *.ids. \n#Files contains gene id for each annotated transcript\nfi<-list.files(pattern = ""*.ids"")\nmyfiles = lapply(fi,header = F,read.delim) \n\nbsp<-new[myfiles[[1]]$V1,] \ntetras<-new[myfiles[[9]]$V1,] \npmp<-new[myfiles[[6]]$V1,] \n\ncalp<-new[myfiles[[2]]$V1,]\npap<-new[myfiles[[5]]$V1,]\ngp<-new[myfiles[[3]]$V1,]\nserine<-new[myfiles[[7]]$V1,]\nsub<-new[myfiles[[8]]$V1,]\n\nmyb<-new[myfiles[[4]]$V1,] \n\n####scatters\n\nproteases<-ggplot(log2(new),aes(x=median_pb,y=feline)) + \n  geom_point(shape=46, color=""gray"")+\n  xlab(""Average"")+ylab(""G10/1"") + \n  theme_classic() + geom_abline(linetype=2) +\n  geom_point(data = log2(calp %>% subset(fc > 4)) %>% subset(feline>7),aes(x=median_pb,y=feline,col=""calp""))+\n  geom_point(data = log2(pap %>% subset(fc > 4)) %>% subset(feline>7),aes(x=median_pb,y=feline,col=""pap""))+\n  geom_point(data = log2(gp %>% subset(fc > 4)) %>% subset(feline>7),aes(x=median_pb,y=feline,col=""gp""))+\n  geom_point(data = log2(sub %>% subset(fc > 4)) %>% subset(feline>7),aes(x=median_pb,y=feline,col=""sub""))+\n  scale_color_manual(values=c(""#69b3a2"", ""purple"", ""red"",""darkgreen""))\n\n\nproteases\n\n##########################################surface ag\nsup<-ggplot(log2(new),aes(x=median_pb,y=feline)) + \n  geom_point(shape=46, color=""gray"")+\n  xlab(""Average"")+ylab(""G10/1"") + \n  theme_classic() + geom_abline(linetype=2) +\n  geom_point(data = log2(tetras %>% subset(fc > 4))%>% subset(feline>7),aes(x=median_pb,y=feline,col=""tetraspanin""))+\n  geom_point(data = log2(bsp %>% subset(fc > 4))%>% subset(feline>7),aes(x=median_pb,y=feline,col=""BspA""))+\n  scale_color_manual(values=c(""brown"",""blue""))\n\nsup\n\n\n##############################mybs\nmibs<-ggplot(log2(new),aes(x=median_pb,y=feline)) + \n  geom_point(shape=46, color=""gray"")+\n  xlab(""Average"")+ylab(""G10/1"") + \n  theme_classic() + geom_abline(linetype=2) +\n  geom_point(data = log2(myb %>% subset(fc > 4)) %>% subset(feline>7),aes(x=median_pb,y=feline,col=""MYBs""))+ scale_color_brewer(palette = ""PuOr"")+\n  geom_point(data = log2(myb %>% subset(fc < -4)) %>% subset(median_pb>7),aes(x=median_pb,y=feline,col=""MYBs""))+ scale_color_brewer(palette = ""PuOr"")\n\nmibs\n\n##########################################\n#correaltion matrix and heatmap_plot\nlibrary(ggcorrplot)\nag<-as.vector(unlist(myfiles)) \nmm<-cbind(matrix[ag,c(1,2,3)],new[ag,""median_pb""])\nshapiro.test(mm$median)\n\ncolnames(mm)[4]<-""median""\n\ncor.mat<-cor(mm,method = ""spearman"") #calculo matriz de correlacin\n\nggcorrplot(cor.mat,hc.order = T,type = ""upper"",lab = T,   ggtheme = ggplot2::theme_gray,colors = c(""#6D9EC1"", ""white"", ""#E46726""))\n\n#############################']","In-depth comparative analysis of Tritrichomonas foetus transcriptomics reveals novel genes linked with adaptation to feline host T. foetus is a protozoa flagellated and has been described as a parasite of cattle, cats and pigs. In spite of its widespread prevalence, feline T. foetus has received considerably less attention than venereal tritrichomonosis and little is known about molecular mechanisms that participate in feline host infection. In this work, we integrated available T. foetus transcriptomics information by bioinformatics and we explored the differences at transcript level with the aim to analyze the pathogenesis and adaptation processes, particularly for the feline isolate. In this sense, our analysis revealed higher abundance levels of predicted virulence factors, such as proteases and surface antigens, in feline isolate. On the basis of a comparative and expression analysis of T. foetus genes, we proposed putative virulence factors that could be involved in feline infection. Moreover, we observed a great abundance of predicted transcription factors of the family of Myb proteins (MYB) and, by a promoter analysis, we revealed that MYB related proteins could participate in the regulation of gene transcription in T. foetus. We concluded that this integrated approach will be a useful resource for future studies about the host-parasite interaction and thus, for identification of new targets of study for improving diagnosis and therapeutic intervention of the feline tritrichomonosis.",0
Data from msGBS: A new high-throughput approach to quantify the relative species abundance in root samples of multi-species plant communities,"Plant interactions are as important belowground as aboveground. Belowground plant interactions are however inherently difficult to quantify, as roots of different species are difficult to disentangle. Although for a couple of decades molecular techniques have been successfully applied to quantify root abundance, root identification and quantification in multi-species plant communities remains particularly challenging.Here we present a novel methodology, multi-species Genotyping By Sequencing (msGBS), as a next step to tackle this challenge. First, a multi-species meta-reference database containing thousands of gDNA clusters per species is created from GBS derived High Throughput Sequencing (HTS) reads. Second, GBS derived HTS reads from multi-species root samples are mapped to this meta-reference which, after a filter procedure to increase the taxonomic resolution, allows the parallel quantification of multiple species. The msGBS signal of 111 mock-mixture root samples, with up to 8 plant species per sample, was used to calculate the within-species abundance. Optional subsequent calibration yielded the across-species abundance. The within- and across-species abundances highly correlated (R2 range 0.72-0.94 and 0.85-0.98, respectively) to the biomass-based species abundance. Compared to a qPCR based method which was previously used to analyze the same set of samples, msGBS provided similar results. Additional data on 11 congener species groups within 105 natural field root samples showed high taxonomic resolution of the method. msGBS is highly scalable in terms of sensitivity and species numbers within samples, which is a major advantage compared to the qPCR method and advances our tools to reveal hidden belowground interactions.This dataset belongs to the article ""msGBS: A new high-throughput approach to quantify the relative species abundance in root samples of multi-species plant communities"". msGBS is a technique that uses Genotyping By Sequencing on mixed plant species root samples which, after a filtering step to increase the taxonomic resolution and calibration, is able to estimate plant species abundances. The article uses data of two different experiment:the Jena field survay (13 plant species) andthe Dutch field survay (120 plant species).","['rm(list=ls())\n\nsetwd(""/Users/NielsWagemaker/Dropbox/SGBS_GBS_epiGBS_NIELS/R/noSTD_and_STD_vs_weighed_2019"")\nlibrary(ggplot2)\nlibrary(scales)\n\n#pool1\ndata<-read.csv(""NoSTD_STD_weighed_C_p1.txt"",stringsAsFactors = FALSE,h=T,sep=""\\t"",dec="","")\n#pool2\ndata2<-read.csv(""NoSTD_STD_weighed_NC_p1.txt"",stringsAsFactors = FALSE,h=T,sep=""\\t"",dec="","")\n\n\ndata$Species <- as.factor(data$Species)\ndata$Calibration <- as.factor(data$Calibration)\ny <- data$msGBS <- as.numeric(data$msGBS)\nx <- data$Weighed <- as.numeric(data$Weighed)\n\ndata2$Species <- as.factor(data2$Species)\ndata2$Calibration <- as.factor(data2$Calibration)\ny2 <- data2$msGBS <- as.numeric(data2$msGBS)\nx2 <- data2$Weighed <- as.numeric(data2$Weighed)\n\nlibrary(png)\nlibrary(grid)\nlibrary(gridExtra)\n\nimg <- readPNG(""test_P1_calibrated.png"")\nimg2 <- readPNG(""test_P1_non_calibrated.png"")\ng <- rasterGrob(img, interpolate=TRUE)\ng2 <- rasterGrob(img2, interpolate=TRUE)\n\nformula = y~x\n\n\n###CALIBRATED\nplot2 <- ggplot(data, aes(x=Weighed, y=msGBS, shape = Species, colour= Species, group =Species, linetype=Species))+\n  geom_point(size = 2)+\n  geom_smooth(method=""lm"", formula=formula, colour=""black"", se=FALSE, size=0.3)+\n  scale_shape_manual(values = c(1,3,5,7,9,10,11,13)) +\n  scale_y_continuous(limits=c(0,0.95))+\n  scale_x_continuous(limits=c(0,0.52))+\n  annotate(""text"",x=0.6,y=0.6,label=""slope =1"")+\n  annotation_custom(g,xmin=0.0,xmax=0.25,ymin=0.55,ymax=0.95)+\n  theme_bw(base_size=16)+\n  geom_abline(intercept=0, slope=1, colour = ""red"", linetype=""dashed"")+\n  ggtitle(""B Calibrated msGBS Pool 1"")+labs(x = \'Biomass-based species proportions\', y =\'Across-species abundance\')+\n  theme(axis.text=element_text(size=12), legend.position = ""none"")\n\ny <- data2$msGBS <- as.numeric(data2$msGBS)\nx <- data2$Weighed <- as.numeric(data2$Weighed)\nformula = y~x\n\nplot1 <- ggplot(data2, aes(x=Weighed, y=msGBS, shape = Species, colour= Species, group =Species, linetype=Species))+\n  geom_point(size = 2)+\n  geom_smooth(method=""lm"", formula= formula, colour=""black"", se=FALSE, size=0.3)+\n  scale_shape_manual(values = c(1,3,5,7,9,10,11,13)) +\n  scale_y_continuous(limits=c(0,0.95))+\n  scale_x_continuous(limits=c(0,0.52))+\n  annotate(""text"",x=0.6,y=0.6,label=""slope =1"")+\n  annotation_custom(g2,xmin=0.0,xmax=0.25,ymin=0.55,ymax=0.95)+\n  theme_bw(base_size=16)+\n  geom_abline(intercept=0, slope=1, colour = ""red"", linetype=""dashed"")+\n  ggtitle(""A Non-calibrated msGBS Pool 1"")+labs(x = \'Biomass-based species proportions\', y=\'Within-species abundance\')+\n  theme(axis.text=element_text(size=12), legend.title=element_text(size=12), legend.text=element_text(size=12,face = ""italic""), legend.position=""right"", legend.background = element_rect(fill = ""gray""), legend.key.size = unit(1, ""cm""), legend.key.width = unit(1.5,""cm""))\n  \n\ng_legend<-function(a.gplot){\n  tmp <- ggplot_gtable(ggplot_build(a.gplot))\n  leg <- which(sapply(tmp$grobs, function(x) x$name) == ""guide-box"")\n  legend <- tmp$grobs[[leg]]\n  return(legend)}\n\nmylegend <- g_legend(plot1)\n\nplot1 <- plot1 + theme(legend.position=""none"")\n\nplot3 <- grid.arrange(plot1, plot2, mylegend, ncol=3, widths=c(2,2,1))\n\nggsave(""Figure_4AB_ALT_Calibrated_and_non_calibrated_vs_weighed_results_of_Pool_1_BW"", plot = plot3, width=13, height=5, dpi=200)\n\n', 'rm(list=ls())\n\nsetwd(""/Users/NielsWagemaker/Dropbox/SGBS_GBS_epiGBS_NIELS/R/noSTD_and_STD_vs_weighed_2019"")\nlibrary(ggplot2)\nlibrary(scales)\n\n#pool2 - C\ndata<-read.csv(""NoSTD_STD_weighed_C_p2.txt"",stringsAsFactors = FALSE,h=T,sep=""\\t"",dec="","")\n#pool2 - NC\ndata2<-read.csv(""NoSTD_STD_weighed_NC_p2.txt"",stringsAsFactors = FALSE,h=T,sep=""\\t"",dec="","")\n\n\ndata$Species <- as.factor(data$Species)\ndata$Calibration <- as.factor(data$Calibration)\ny <- data$msGBS <- as.numeric(data$msGBS)\nx <- data$Weighed <- as.numeric(data$Weighed)\n\ndata2$Species <- as.factor(data2$Species)\ndata2$Calibration <- as.factor(data2$Calibration)\ny2 <- data2$msGBS <- as.numeric(data2$msGBS)\nx2 <- data2$Weighed <- as.numeric(data2$Weighed)\n\nlibrary(png)\nlibrary(grid)\nlibrary(gridExtra)\n\nimg <- readPNG(""test_P2_calibrated.png"")\nimg2 <- readPNG(""test_P2_non_calibrated.png"")\ng <- rasterGrob(img, interpolate=TRUE)\ng2 <- rasterGrob(img2, interpolate=TRUE)\n\nformula = y~x\n\n\n###CALIBRATED\nplot2 <- ggplot(data, aes(x=Weighed, y=msGBS, shape = Species, colour= Species, group =Species, linetype=Species))+\n  geom_point(size = 2)+\n  geom_smooth(method=""lm"", formula=formula, colour=""black"", se=FALSE, size=0.3)+\n  scale_shape_manual(values = c(1,3,5,7,9,10,11,13)) +\n  scale_y_continuous(limits=c(0,0.95))+\n  scale_x_continuous(limits=c(0,0.52))+\n  annotate(""text"",x=0.6,y=0.6,label=""slope =1"")+\n  annotation_custom(g,xmin=0.0,xmax=0.25,ymin=0.55,ymax=0.95)+\n  theme_bw(base_size=16)+\n  geom_abline(intercept=0, slope=1, colour = ""red"", linetype=""dashed"")+\n  ggtitle(""D Calibrated msGBS Pool 2"")+labs(x = \'Biomass-based species proportions\', y =\'Across-species abundance\')+\n  theme(axis.text=element_text(size=12), legend.position = ""none"")\n  \ny <- data2$msGBS <- as.numeric(data2$msGBS)\nx <- data2$Weighed <- as.numeric(data2$Weighed)\nformula = y~x\n\nplot1 <- ggplot(data2, aes(x=Weighed, y=msGBS, shape = Species, colour= Species, group =Species, linetype=Species))+\n  geom_point(size = 2)+\n  geom_smooth(method=""lm"", formula= formula, colour=""black"", se=FALSE, size=0.3)+\n  scale_shape_manual(values = c(1,3,5,7,9,10,11,13)) +\n  scale_y_continuous(limits=c(0,0.95))+\n  scale_x_continuous(limits=c(0,0.52))+\n  annotate(""text"",x=0.6,y=0.6,label=""slope =1"")+\n  annotation_custom(g2,xmin=0.0,xmax=0.25,ymin=0.55,ymax=0.95)+\n  theme_bw(base_size=16)+\n  geom_abline(intercept=0, slope=1, colour = ""red"", linetype=""dashed"")+\n  ggtitle(""C Non-calibrated msGBS Pool 2"")+labs(x = \'Biomass-based species proportions\', y=\'Within-species abundance\')+\n  theme(axis.text=element_text(size=12), legend.title=element_text(size=12), legend.text=element_text(size=12,face = ""italic""), legend.position=""right"", legend.background = element_rect(fill = ""gray""), legend.key.size = unit(1, ""cm""), legend.key.width = unit(1.5,""cm""))\n\ng_legend<-function(a.gplot){\n  tmp <- ggplot_gtable(ggplot_build(a.gplot))\n  leg <- which(sapply(tmp$grobs, function(x) x$name) == ""guide-box"")\n  legend <- tmp$grobs[[leg]]\n  return(legend)}\n\nmylegend <- g_legend(plot1)\n\nplot1 <- plot1 + theme(legend.position=""none"")\n\nplot3 <- grid.arrange(plot1, plot2, mylegend, ncol=3, widths=c(2,2,1))\n\nggsave(""Figure_4CD_ALT_Calibrated_and_non_calibrated_vs_weighed_results_of_Pool_2.png"", plot = plot3, width=13, height=5, dpi=200)\n\n', 'rm(list=ls())\n\n\nsetwd(""/Users/NielsWagemaker/Dropbox/Documents/PROJECTEN/HANS/SGBS_barcoding/article_2018/DATA/DATA/FEB_2019_READS_COMBINED_NO_locus_reallocation/POOL1_8_15_1000/"")\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(skimr)\n\n#library(""devtools"", lib.loc=""/Library/Frameworks/R.framework/Versions/3.3/Resources/library"")\n#library(ggpmisc)\n#library(gridExtra)\n\ndata<-read.csv(""R_input_MRT2019_pool1.txt"",stringsAsFactors = FALSE,h=T,sep=""\\t"",dec="","")\n#View(data)\n#skim(data)\n\nSpecies <- as.factor(data$Species)\nQPCR <- as.numeric(data$QPCR)\nSGBS <- as.numeric(data$SGBS)\nWeighed <- as.numeric(data$Weighed)\n\n#Create Model and calculate linear - Pool 1 - QPCR\nFr <- subset(data, Species == ""Festuca rubra"")\nFr_Weighed <- as.numeric(Fr$Weighed)\nFr_QPCR <- as.numeric(Fr$QPCR)\n\nCj <- subset(data, Species == ""Centaurea jacea"")\nCj_Weighed <- as.numeric(Cj$Weighed)\nCj_QPCR <- as.numeric(Cj$QPCR)\n\nHp <- subset(data, Species == ""Helictotrichon pubescens"")\nHp_Weighed <- as.numeric(Hp$Weighed)\nHp_QPCR <- as.numeric(Hp$QPCR)\n\nPp <- subset(data, Species == ""Poa pratensis"")\nPp_Weighed <- as.numeric(Pp$Weighed)\nPp_QPCR <- as.numeric(Pp$QPCR)\n\nKa <- subset(data, Species == ""Knautia arvensis"")\nKa_Weighed <- as.numeric(Ka$Weighed)\nKa_QPCR <- as.numeric(Ka$QPCR)\n\nLv <- subset(data, Species == ""Leucanthemum vulgare"")\nLv_Weighed <- as.numeric(Lv$Weighed)\nLv_QPCR <- as.numeric(Lv$QPCR)\n\nPl <- subset(data, Species == ""Plantago lanceolate"")\nPl_Weighed <- as.numeric(Pl$Weighed)\nPl_QPCR <- as.numeric(Pl$QPCR)\n\nPh <- subset(data, Species == ""Phleum pratense"")\nPh_Weighed <- as.numeric(Ph$Weighed)\nPh_QPCR <- as.numeric(Ph$QPCR)\n\n#Note on model : First y then x ###\n\nmodel_QPCR_Fr = lm(Fr_QPCR~Fr_Weighed)\nsummary(model_QPCR_Fr)\nconfint(model_QPCR_Fr,level = 0.95)\n\nmodel_QPCR_Cj = lm(Cj_QPCR~Cj_Weighed)\nsummary(model_QPCR_Cj)\nconfint(model_QPCR_Cj,level = 0.95)\n\nmodel_QPCR_Hp = lm(Hp_QPCR~Hp_QPCR)\nsummary(model_QPCR_Hp)\nconfint(model_QPCR_Hp,level = 0.95)\n\nmodel_QPCR_Ka = lm(Ka_QPCR~Ka_Weighed)\nsummary(model_QPCR_Ka)\nconfint(model_QPCR_Ka,level = 0.95)\n\nmodel_QPCR_Lv = lm(Lv_QPCR~Lv_Weighed)\nsummary(model_QPCR_Lv)\nconfint(model_QPCR_Lv,level = 0.95)\n\nmodel_QPCR_Pl = lm(Pl_QPCR~Pl_Weighed)\nsummary(model_QPCR_Pl)\nconfint(model_QPCR_Pl,level = 0.95)\n\nmodel_QPCR_Ph = lm(Ph_QPCR~Ph_Weighed)\nsummary(model_QPCR_Ph)\nconfint(model_QPCR_Ph,level = 0.95)\n\n#Create Model and calculate linear - Pool 1 -msGBS\nFr <- subset(data, Species == ""Festuca rubra"")\nFr_SGBS <- as.numeric(Fr$SGBS)\n\nCj <- subset(data, Species == ""Centaurea jacea"")\nCj_SGBS <- as.numeric(Cj$SGBS)\n\nHp <- subset(data, Species == ""Helictotrichon pubescens"")\nHp_SGBS <- as.numeric(Hp$SGBS)\n\nPp <- subset(data, Species == ""Poa pratensis"")\nPp_SGBS <- as.numeric(Pp$SGBS)\n\nKa <- subset(data, Species == ""Knautia arvensis"")\nKa_SGBS <- as.numeric(Ka$SGBS)\n\nLv <- subset(data, Species == ""Leucanthemum vulgare"")\nLv_SGBS <- as.numeric(Lv$SGBS)\n\nPl <- subset(data, Species == ""Plantago lanceolate"")\nPl_SGBS <- as.numeric(Pl$SGBS)\n\nPh <- subset(data, Species == ""Phleum pratense"")\nPh_SGBS <- as.numeric(Ph$SGBS)\n\n#Note on model : First y then x ###\nmodel_SGBS_Fr = lm(Fr_SGBS~Fr_Weighed)\nsummary(model_SGBS_Fr)\nconfint(model_SGBS_Fr,level = 0.95)\n\nmodel_SGBS_Cj = lm(Cj_SGBS~Cj_Weighed)\nsummary(model_SGBS_Cj)\nconfint(model_SGBS_Cj,level = 0.95)\n\nmodel_SGBS_Hp = lm(Hp_SGBS~Hp_Weighed)\nsummary(model_SGBS_Hp)\nconfint(model_SGBS_Hp,level = 0.95)\n\nmodel_SGBS_Pp = lm(Pp_SGBS~Pp_Weighed)\nsummary(model_SGBS_Pp)\nconfint(model_SGBS_Pp,level = 0.95)\n\nmodel_SGBS_Ka = lm(Ka_SGBS~Ka_Weighed)\nsummary(model_SGBS_Ka)\nconfint(model_SGBS_Ka,level = 0.95)\n\nmodel_SGBS_Lv = lm(Lv_SGBS~Lv_Weighed)\nsummary(model_SGBS_Lv)\nconfint(model_SGBS_Lv,level = 0.95)\n\nmodel_SGBS_Pl = lm(Pl_SGBS~Pl_Weighed)\nsummary(model_SGBS_Pl)\nconfint(model_SGBS_Pl,level = 0.95)\n\nmodel_SGBS_Ph = lm(Ph_SGBS~Ph_Weighed)\nsummary(model_SGBS_Ph)\nconfint(model_SGBS_Ph,level = 0.95)\n\n#Note on model : First y then x msGBS vs QPCR ###\n\nmodel = lm(Fr_SGBS~Fr_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Cj_SGBS~Cj_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Hp_SGBS~Hp_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Pp_SGBS~Pp_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Ka_SGBS~Ka_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Lv_SGBS~Lv_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Pl_SGBS~Pl_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Ph_SGBS~Ph_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\n# Weighed vs QPCR (blue) and msGSB (red) - \n\nsingle_species <- unique(Species)\nsingle_species\n#Species <- unique(Species)\nannotate_QPCR <- data.frame(Species = single_species, xxQ=c(0.15,0.15,0.15,0.15,0.15,0.15,0.15,0.15), yyQ=c(0.57,0.57,0.57,0.57,0.57,0.57,0.57,0.57), kleurQPCR =c(""blue"",""blue"",""blue"",""blue"",""blue"",""blue"",""blue"",""blue""), labQ= c(""QPCR R2=0.97"",""QPCR R2=0.97"",""QPCR R2=0.98"",""QPCR R2=0.94"",""QPCR R2=0.98"",""QPCR R2=0.99"",""QPCR R2=0.97"",""QPCR R2=0.98""))\nannotate_msGSB <- data.frame(Species = single_species, xxM=c(0.15,0.15,0.15,0.15,0.15,0.15,0.15,0.15), yyM=c(0.62,0.62,0.62,0.62,0.62,0.62,0.62,0.62), kleurmsGBS =c(""red"",""red"",""red"",""red"",""red"",""red"",""red"",""red""), labM = c(""msGBS R2=0.98"",""msGBS R2=0.95"",""msGBS R2=0.95"",""msGBS R2=0.97"",""msGBS R2=0.97"",""msGBS R2=0.98"",""msGBS R2=0.97"",""msGBS R2=0.98""))\n\nannotate_msGBS_vs_QPCR_stat <- data.frame(Species = single_species, xxS=c(0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56), yyS=c(0.55,0.55,0.55,0.55,0.55,0.55,0.55,0.55), labS= c(""ns"",""**"",""ns"",""***"",""ns"",""ns"",""**"",""*""))\n\naspect_ratio <-2.0\n\n#CHANGE STATISTICAL TESTING OF DIFFERENCE OF SLOPES\n\nggplot(data, group = Species) + labs(x = \'Biomass-based species proportions\', y =\'Across-species abundance\')+\n  geom_jitter(aes(Weighed, QPCR, group = Species), color=""blue"", size=0.6)+\n  geom_smooth(aes(Weighed, QPCR, group = Species), color=""blue"", method=""lm"", se=TRUE, size=0.6, linetype=""dashed"")+\n  geom_jitter(aes(Weighed, SGBS, group = Species), color=""red"", size=0.6)+\n  geom_smooth(aes(Weighed, SGBS, group = Species), color=""red"", method=""lm"", se=TRUE, size=0.6)+\n  geom_text(data = annotate_QPCR, mapping = aes(x=xxQ,y=yyQ,label=labQ, color= kleurQPCR),size=7)+\n  geom_text(data = annotate_msGSB, mapping = aes(x=xxM,y=yyM,label=labM, color= kleurmsGBS),size=7)+\n  geom_text(data = annotate_msGBS_vs_QPCR_stat, mapping = aes(x=xxS,y=yyS,label=labS),size=9)+\n  facet_wrap(~ Species,ncol=4, scales = ""free_x"")+\n  theme_bw()+\n  theme(legend.position=""none"", strip.text = element_text(size=20, face = ""bold.italic""),axis.text=element_text(size=20, face=""bold""), axis.title=element_text(size=20,face=""bold""), plot.title = element_text(size = 20, face = ""bold""))+\n  expand_limits(y=0.66,x=0.6)+\n  ggtitle(""A    Species pool 1"")+\n  scale_colour_identity() +\n  ggsave(""TEST_All_species_Weighed_vs_QPCR_vs_msGBS_MRT2019_POOL1_NEW_annotate.png"", width=10 * aspect_ratio, height=10, dpi=400)\n\n', 'rm(list=ls())\n\nsetwd(""/Users/NielsWagemaker/Dropbox/Documents/PROJECTEN/HANS/SGBS_barcoding/article_2018/DATA/DATA/FEB_2019_READS_COMBINED_NO_locus_reallocation/POOL2_8_15_1000/"")\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(skimr)\n\n#library(""devtools"", lib.loc=""/Library/Frameworks/R.framework/Versions/3.3/Resources/library"")\n#library(ggpmisc)\n#library(gridExtra)\n\ndata<-read.csv(""R_input_MRT2019_POOL2.txt"",stringsAsFactors = FALSE,h=T,sep=""\\t"",dec="","")\n#View(data)\n#skim(data)\n\nSpecies <- as.factor(data$Species)\nQPCR <- as.numeric(data$QPCR)\nSGBS <- as.numeric(data$SGBS)\nWeighed <- as.numeric(data$Weighed)\n\n#Create Model and calculate linear - Pool 1 - QPCR\nFr <- subset(data, Species == ""Festuca rubra"")\nFr_Weighed <- as.numeric(Fr$Weighed)\nFr_QPCR <- as.numeric(Fr$QPCR)\n\nCj <- subset(data, Species == ""Centaurea jacea"")\nCj_Weighed <- as.numeric(Cj$Weighed)\nCj_QPCR <- as.numeric(Cj$QPCR)\n\nHp <- subset(data, Species == ""Helictotrichon pubescens"")\nHp_Weighed <- as.numeric(Hp$Weighed)\nHp_QPCR <- as.numeric(Hp$QPCR)\n\nPp <- subset(data, Species == ""Poa pratensis"")\nPp_Weighed <- as.numeric(Pp$Weighed)\nPp_QPCR <- as.numeric(Pp$QPCR)\n\nKa <- subset(data, Species == ""Knautia arvensis"")\nKa_Weighed <- as.numeric(Ka$Weighed)\nKa_QPCR <- as.numeric(Ka$QPCR)\n\nLv <- subset(data, Species == ""Leucanthemum vulgare"")\nLv_Weighed <- as.numeric(Lv$Weighed)\nLv_QPCR <- as.numeric(Lv$QPCR)\n\nPl <- subset(data, Species == ""Plantago lanceolate"")\nPl_Weighed <- as.numeric(Pl$Weighed)\nPl_QPCR <- as.numeric(Pl$QPCR)\n\nPh <- subset(data, Species == ""Phleum pratense"")\nPh_Weighed <- as.numeric(Ph$Weighed)\nPh_QPCR <- as.numeric(Ph$QPCR)\n\n#Note on model : First y then x ###\n\nmodel_QPCR_Fr = lm(Fr_QPCR~Fr_Weighed)\nsummary(model_QPCR_Fr)\nconfint(model_QPCR_Fr,level = 0.95)\n\nmodel_QPCR_Cj = lm(Cj_QPCR~Cj_Weighed)\nsummary(model_QPCR_Cj)\nconfint(model_QPCR_Cj,level = 0.95)\n\nmodel_QPCR_Hp = lm(Hp_QPCR~Hp_QPCR)\nsummary(model_QPCR_Hp)\nconfint(model_QPCR_Hp,level = 0.95)\n\nmodel_QPCR_Ka = lm(Ka_QPCR~Ka_Weighed)\nsummary(model_QPCR_Ka)\nconfint(model_QPCR_Ka,level = 0.95)\n\nmodel_QPCR_Lv = lm(Lv_QPCR~Lv_Weighed)\nsummary(model_QPCR_Lv)\nconfint(model_QPCR_Lv,level = 0.95)\n\nmodel_QPCR_Pl = lm(Pl_QPCR~Pl_Weighed)\nsummary(model_QPCR_Pl)\nconfint(model_QPCR_Pl,level = 0.95)\n\nmodel_QPCR_Ph = lm(Ph_QPCR~Ph_Weighed)\nsummary(model_QPCR_Ph)\nconfint(model_QPCR_Ph,level = 0.95)\n\n#Create Model and calculate linear - Pool 1 -msGBS\nFr <- subset(data, Species == ""Festuca rubra"")\nFr_SGBS <- as.numeric(Fr$SGBS)\n\nCj <- subset(data, Species == ""Centaurea jacea"")\nCj_SGBS <- as.numeric(Cj$SGBS)\n\nHp <- subset(data, Species == ""Helictotrichon pubescens"")\nHp_SGBS <- as.numeric(Hp$SGBS)\n\nPp <- subset(data, Species == ""Poa pratensis"")\nPp_SGBS <- as.numeric(Pp$SGBS)\n\nKa <- subset(data, Species == ""Knautia arvensis"")\nKa_SGBS <- as.numeric(Ka$SGBS)\n\nLv <- subset(data, Species == ""Leucanthemum vulgare"")\nLv_SGBS <- as.numeric(Lv$SGBS)\n\nPl <- subset(data, Species == ""Plantago lanceolate"")\nPl_SGBS <- as.numeric(Pl$SGBS)\n\nPh <- subset(data, Species == ""Phleum pratense"")\nPh_SGBS <- as.numeric(Ph$SGBS)\n\n#Note on model : First y then x ###\nmodel_SGBS_Fr = lm(Fr_SGBS~Fr_Weighed)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel_SGBS_Cj = lm(Cj_SGBS~Cj_Weighed)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel_SGBS_Hp = lm(Hp_SGBS~Hp_Weighed)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel_SGBS_Pp = lm(Pp_SGBS~Pp_Weighed)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel_SGBS_Ka = lm(Ka_SGBS~Ka_Weighed)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel_SGBS_Lv = lm(Lv_SGBS~Lv_Weighed)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel_SGBS_Pl = lm(Pl_SGBS~Pl_Weighed)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel_SGBS_Ph = lm(Ph_SGBS~Ph_Weighed)\nsummary(model)\nconfint(model,level = 0.95)\n\n#Note on model : First y then x msGBS vs QPCR ###\n\nmodel = lm(Fr_SGBS~Fr_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Cj_SGBS~Cj_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Hp_SGBS~Hp_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Pp_SGBS~Pp_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Ka_SGBS~Ka_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Lv_SGBS~Lv_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Pl_SGBS~Pl_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\nmodel = lm(Ph_SGBS~Ph_QPCR)\nsummary(model)\nconfint(model,level = 0.95)\n\n# Weighed vs QPCR (blue) and msGSB (red) - \n\nsingle_species <- unique(Species)\nsingle_species\n#Species <- unique(Species)\nannotate_QPCR <- data.frame(Species = single_species, xxQ=c(0.15,0.15,0.15,0.15,0.15,0.15,0.15,0.15), yyQ=c(0.57,0.57,0.57,0.57,0.57,0.57,0.57,0.57), kleurQPCR =c(""blue"",""blue"",""blue"",""blue"",""blue"",""blue"",""blue"",""blue""), labQ= c(""QPCR R2=0.98"",""QPCR R2=0.94"",""QPCR R2=0.98"",""QPCR R2=0.99"",""QPCR R2=0.98"",""QPCR R2=0.94"",""QPCR R2=0.97"",""QPCR R2=0.96""))\nannotate_msGSB <- data.frame(Species = single_species, xxM=c(0.15,0.15,0.15,0.15,0.15,0.15,0.15,0.15), yyM=c(0.62,0.62,0.62,0.62,0.62,0.62,0.62,0.62), kleurmsGBS =c(""red"",""red"",""red"",""red"",""red"",""red"",""red"",""red""), labM = c(""msGBS R2=0.97"",""msGBS R2=0.91"",""msGBS R2=0.97"",""msGBS R2=0.95"",""msGBS R2=0.97"",""msGBS R2=0.95"",""msGBS R2=0.85"",""msGBS R2=0.96""))\n\nannotate_msGBS_vs_QPCR_stat <- data.frame(Species = single_species, xxS=c(0.56,0.56,0.56,0.56,0.56,0.56,0.56,0.56), yyS=c(0.55,0.55,0.55,0.55,0.55,0.55,0.55,0.55), labS= c(""**"",""***"",""ns"",""ns"",""ns"",""*"",""***"",""*""))\n\naspect_ratio <-2.0\n\n#CHANGE STATISTICAL TESTING OF DIFFERENCE OF SLOPES\n\nggplot(data, group = Species) + labs(x = \'Biomass-based species proportions\', y =\'Across-species abundance\')+\n  geom_jitter(aes(Weighed, QPCR, group = Species), color=""blue"", size=0.6)+\n  geom_smooth(aes(Weighed, QPCR, group = Species), color=""blue"", method=""lm"", se=TRUE, size=0.6, linetype=""dashed"")+\n  geom_jitter(aes(Weighed, SGBS, group = Species), color=""red"", size=0.6)+\n  geom_smooth(aes(Weighed, SGBS, group = Species), color=""red"", method=""lm"", se=TRUE, size=0.6)+\n  geom_text(data = annotate_QPCR, mapping = aes(x=xxQ,y=yyQ,label=labQ, color= kleurQPCR),size=7)+\n  geom_text(data = annotate_msGSB, mapping = aes(x=xxM,y=yyM,label=labM, color= kleurmsGBS),size=7)+\n  geom_text(data = annotate_msGBS_vs_QPCR_stat, mapping = aes(x=xxS,y=yyS,label=labS),size=9)+\n  facet_wrap(~ Species,ncol=4, scales = ""free_x"")+\n  theme_bw()+\n  theme(legend.position=""none"", strip.text = element_text(size=20, face = ""bold.italic""),axis.text=element_text(size=20, face=""bold""), axis.title=element_text(size=20,face=""bold""), plot.title = element_text(size = 20, face = ""bold""))+\n  expand_limits(y=0.66,x=0.6)+\n  ggtitle(""A    Species pool 2"")+\n  scale_colour_identity() +\n  ggsave(""TEST_All_species_Weighed_vs_QPCR_vs_msGBS_MRT2019_POOL2_NEW_annotate.png"", width=10 * aspect_ratio, height=10, dpi=200)\n\n\n', '### QPCR vs msGBS Statistics ###\n\nrm(list=ls())\n\nsetwd(""/Users/NielsWagemaker/Dropbox/Documents/PROJECTEN/HANS/SGBS_barcoding/article_2018/DATA/final_output_Article/"")\n\ndata<-read.csv(""R_input_MEI_2020_Fig_3_P2_stat.txt"",stringsAsFactors = FALSE,h=T,sep=""\\t"",dec=""."")\n\nSpecies <- as.factor(data$Species)\nWeighed <- as.numeric(data$Weighed)\nESP <- as.numeric(data$ESP)\nmethod <- as.factor(data$method)\n\n### Festuca rubra ###\n\n\nAo <- subset(data,Species == ""Anthoxanthum odoratum"")\nDg <- subset(data,Species == ""Dactylis glomerata"")\nGp <- subset(data,Species == ""Geranium pratense"")\nHl <- subset(data,Species == ""Holcus lanatus"")\nRa <- subset(data,Species == ""Ranunculus acris "")\nLv <- subset(data, Species == ""Leucanthemum vulgare"")\nPl <- subset(data, Species == ""Plantago lanceolata"")\nPh <- subset(data, Species == ""Phleum pratense "")\n\nAo_Weighed <- as.numeric(Ao$Weighed)\nDg_Weighed <- as.numeric(Dg$Weighed)\nGp_Weighed <- as.numeric(Gp$Weighed)\nHl_Weighed <- as.numeric(Hl$Weighed)\nRa_Weighed <- as.numeric(Ra$Weighed)\nLv_Weighed <- as.numeric(Lv$Weighed)\nPl_Weighed <- as.numeric(Pl$Weighed)\nPh_Weighed <- as.numeric(Ph$Weighed)\n\nAo_method <- as.factor(Ao$method)\nDg_method <- as.factor(Dg$method)\nGp_method <- as.factor(Gp$method)\nHl_method <- as.factor(Hl$method)\nRa_method <- as.factor(Ra$method)\nLv_method <- as.factor(Lv$method)\nPl_method <- as.factor(Pl$method)\nPh_method <- as.factor(Ph$method)\n\n\nAo_ESP <- as.numeric(Ao$ESP)\nDg_ESP <- as.numeric(Dg$ESP)\nGp_ESP <- as.numeric(Gp$ESP)\nHl_ESP <- as.numeric(Hl$ESP)\nRa_ESP <- as.numeric(Ra$ESP)\nLv_ESP <- as.numeric(Lv$ESP)\nPl_ESP <- as.numeric(Pl$ESP)\nPh_ESP <- as.numeric(Ph$ESP)\n\nmodel_Ao = lm(Ao_ESP~Ao_method*Ao_Weighed)\nmodel_Dg = lm(Dg_ESP~Dg_method*Dg_Weighed)\nmodel_Gp = lm(Gp_ESP~Gp_method*Gp_Weighed)\nmodel_Hl = lm(Hl_ESP~Hl_method*Hl_Weighed)\nmodel_Ra = lm(Ra_ESP~Ra_method*Ra_Weighed)\nmodel_Lv = lm(Lv_ESP~Lv_method*Lv_Weighed)\nmodel_Pl = lm(Pl_ESP~Pl_method*Pl_Weighed)\nmodel_Ph = lm(Ph_ESP~Ph_method*Ph_Weighed)\n\n\nsummary(model_Ao)\nsummary(model_Dg)\nsummary(model_Gp)\nsummary(model_Hl)\nsummary(model_Ra)\nsummary(model_Lv)\nsummary(model_Pl)\nsummary(model_Ph)\n\n### andere manier) ###\nlibrary(car)\nAnova(model_Fr, type=3)\n', '### QPCR vs msGBS Statistics ###\n\nrm(list=ls())\n\nsetwd(""/Users/NielsWagemaker/Dropbox/Documents/PROJECTEN/HANS/SGBS_barcoding/article_2018/DATA/final_output_Article/FEB_2019_READS_COMBINED_NO_locus_reallocation/POOL2/statistics"")\n\ndata<-read.csv(""R_input_MRT2019_POOL2_stat.txt"",stringsAsFactors = FALSE,h=T,sep=""\\t"",dec="","")\n\nSpecies <- as.factor(data$Species)\nWeighed <- as.numeric(data$Weighed)\nESP <- as.numeric(data$ESP)\nmethod <- as.factor(data$method)\n\n### Festuca rubra ###\n\n\nLv <- subset(data, Species == ""Leucanthemum vulgare"")\nAo <- subset(data, Species == ""Anthoxanthum odoratum"")\nDg <- subset(data, Species == ""Dactylis glomerata"")\nGp <- subset(data, Species == ""Geranium pratense"")\nHp <- subset(data, Species == ""Holcus lanatus"")\nPp <- subset(data, Species == ""Phleum pratense "")\nPl <- subset(data, Species == ""Plantago lanceolata"")\nRa <- subset(data, Species == ""Ranunculus acris "")\n\nLv_Weighed <- as.numeric(Lv$Weighed)\nAo_Weighed <- as.numeric(Ao$Weighed)\nDg_Weighed <- as.numeric(Dg$Weighed)\nGp_Weighed <- as.numeric(Gp$Weighed)\nHp_Weighed <- as.numeric(Hp$Weighed)\nPp_Weighed <- as.numeric(Pp$Weighed)\nPl_Weighed <- as.numeric(Pl$Weighed)\nRa_Weighed <- as.numeric(Ra$Weighed)\n\nLv_method <- as.factor(Lv$method)\nAo_method <- as.factor(Ao$method)\nDg_method <- as.factor(Dg$method)\nGp_method <- as.factor(Gp$method)\nHp_method <- as.factor(Hp$method)\nPp_method <- as.factor(Pp$method)\nPl_method <- as.factor(Pl$method)\nRa_method <- as.factor(Ra$method)\n\n\nLv_ESP <- as.numeric(Lv$ESP)\nAo_ESP <- as.numeric(Ao$ESP)\nDg_ESP <- as.numeric(Dg$ESP)\nGp_ESP <- as.numeric(Gp$ESP)\nHp_ESP <- as.numeric(Hp$ESP)\nPp_ESP <- as.numeric(Pp$ESP)\nPl_ESP <- as.numeric(Pl$ESP)\nRa_ESP <- as.numeric(Ra$ESP)\n\nmodel_Lv = lm(Lv_ESP~Lv_method*Lv_Weighed)\nmodel_Ao = lm(Ao_ESP~Ao_method*Ao_Weighed)\nmodel_Dg = lm(Dg_ESP~Dg_method*Dg_Weighed)\nmodel_Gp = lm(Gp_ESP~Gp_method*Gp_Weighed)\nmodel_Hp = lm(Hp_ESP~Hp_method*Hp_Weighed)\nmodel_Pp = lm(Pp_ESP~Pp_method*Pp_Weighed)\nmodel_Pl = lm(Pl_ESP~Pl_method*Pl_Weighed)\nmodel_Ra = lm(Ra_ESP~Ra_method*Ra_Weighed)\n\ns <- summary(model_Lv)\ns\ncapture.output(s, file = ""summary_Lv.txt"")\ns <- summary(model_Ao)\ns\ncapture.output(s, file = ""summary_Ao.txt"")\ns <- summary(model_Dg)\ns\ncapture.output(s, file = ""summary_Dg.txt"")\ns <- summary(model_Gp)\ns\ncapture.output(s, file = ""summary_Gp.txt"")\ns <- summary(model_Hp)\ns\ncapture.output(s, file = ""summary_Hp.txt"")\ns <- summary(model_Pp)\ncapture.output(s, file = ""summary_Pp.txt"")\ns <- summary(model_Pl)\ns\ncapture.output(s, file = ""summary_Pl.txt"")\ns <- summary(model_Ra)\ns\ncapture.output(s, file = ""summary_Ra.txt"")\n\n', '\nrm(list=ls())\n\nsetwd(""/Users/NielsWagemaker/Dropbox/SGBS_GBS_epiGBS_NIELS/R/slope_vs_cal_F/"")\nlibrary(ggplot2)\nlibrary(scales)\n#library(""devtools"", lib.loc=""/Library/Frameworks/R.framework/Versions/3.3/Resources/library"")\n#library(ggpmisc)\n#library(gridExtra)\n\n#pool1 and 2\ndata<-read.csv(""slope_vs_cal_F.txt"",stringsAsFactors = FALSE,h=T,sep=""\\t"",dec="","")\n#pool2\n#data<-read.csv(""NoSTD_STD_wrap_p2.txt"",stringsAsFactors = FALSE,h=T,sep=""\\t"",dec="","")\n#View(data)\n\ndata$Species <- as.factor(data$Species)\nx <-data$Calibration_key <- as.numeric(data$Calibration_key)\ny <- data$Slope <- as.numeric(data$Slope)\ndata$Pool <- as.factor(data$Pool)\n\n\n\n#color\nggplot(data, aes(x=Calibration_key, y=Slope, shape= Species, group = Species, colour = Pool))+\n  geom_point(size=3)+\n  #scale_color_manual(values=c(""red"",""blue""))\n  scale_shape_manual(values = c(1,2,3,4,5,6,7,8,9,10,11,12,13))+\n  scale_color_manual(values = c(""red"",""blue""))+\n  geom_smooth(method=""lm"", formula = y~x, aes(colour=""red"",linetype=""dashed""), se=FALSE, size=0.5)+\n  scale_y_continuous(limits=c(0,1.8))+\n  scale_x_continuous(limits=c(0,0.5))+\n  #annotate(""text"",x=0.75,y=0.75,label=""slope =1"")+\n  #annotate(""text"",x=0.35,y=1.35,label=""P2 - R2=0.80"", angle = -47, colour =""red"")+\n  #annotate(""text"",x=0.45,y=1.10,label=""P1 - R2=0.18"", angle = -33, colour =""red"")+\n  geom_line(colour=""gray"",linetype=""dashed"")+\n  theme_bw(base_size=18)+\n  #geom_abline(intercept=0, slope=1, colour = ""red"", linetype=""dashed"")+\n  ggtitle("""")+labs(x = \'Calibration key\', y =\'Slope\')\ntheme(axis.text=element_text(size=12), plot.title = element_text(size = 12, face = ""bold""),legend.title=element_text(size=14), \n      legend.text=element_text(size=12), legend.position=""right"")\nggsave(""Calibration_key_vs_Slope_color.png"", width=10, height=8, dpi=200)\nplot\n\n# variant\n\nggplot(data, aes(x=Calibration_key, y=Slope, shape= Pool, group = Species, colour = Species))+\n  geom_point(size=2)+\n  #geom_smooth(aes(group=Pool,colour=""red""),linetype=""dashed"",method=""lm"", se=FALSE, size=0.5)+\n  scale_y_continuous(limits=c(0,1.8))+\n  scale_x_continuous(limits=c(0,0.5))+\n  geom_smooth(method=""lm"", formula = formula, aes(colour=""red"",linetype=""dashed""), se=FALSE, size=0.5)+\n  #annotate(""text"",x=0.75,y=0.75,label=""slope =1"")+\n  #annotate(""text"",x=0.35,y=1.35,label=""R2=0.80"", angle = -47, colour =""red"")+\n  #annotate(""text"",x=0.45,y=1.10,label=""R2=0.18"", angle = -33, colour =""red"")+\n  geom_line()+\n  theme_bw(base_size=18)+\n  #geom_abline(intercept=0, slope=1, colour = ""red"", linetype=""dashed"")+\n  ggtitle("""")+labs(x = \'Calibration key\', y =\'Slope\')\ntheme(axis.text=element_text(size=12), plot.title = element_text(size = 12, face = ""bold""),legend.title=element_text(size=14), \n      legend.text=element_text(size=12), legend.position=""right"")\nggsave(""Calibration_key_vs_Slope_variant.png"", width=10, height=8, dpi=200)\nplot\n\nformula = y~x\n\n#for pool 2 - BW - in artikel\nggplot(data, aes(x=Calibration_key, y=Slope, shape= Species, group = Species, colour = Pool))+\n  geom_point(size = 4)+\n  geom_smooth(method=""lm"", formula = y~x, se=FALSE, size=0.5)+\n  scale_shape_manual(values = c(1,2,3,4,5,6,7,8,9,10,11,12,13)) +\n  scale_colour_manual(values = c(""red"",""blue"")) +\n  scale_y_continuous(limits=c(0,1.8))+\n  scale_x_continuous(limits=c(0,0.5))+\n  #annotate(""text"",x=0.75,y=0.8,label=""slope =1"")+\n  #facet_wrap(~ Species, ncol=4)+\n  theme_bw(base_size=18)+\n  geom_line(color=""black"")+\n  #geom_abline(intercept=0, slope=1, colour = ""red"", linetype=""dashed"")+\n  ggtitle("""")+labs(x = \'Calibration key\', y =\'Slope\')+\n  theme(axis.text=element_text(size=12), plot.title = element_text(size = 12, face = ""bold""),legend.title=element_text(size=14), \n        legend.text=element_text(size=12), legend.position=""right"")+\n  #geom_rect(xmin=0.1, xmax=0.2, ymin=0.1, ymax=0.2, aes(color=""black"",fill=""blue""))+\nggsave(""Calibration_key_vs_Slope_variant_BW.png"", width=10, height=8, dpi=200)\nplot\n\n\n', '### standaard BOXplot ###\n\nrm(list=ls())\n\nsetwd(""/Users/NielsWagemaker/Dropbox/SGBS_GBS_epiGBS_NIELS/R/standaard_boxplot/"")\nlibrary(ggplot2)\nlibrary(scales)\n#library(""devtools"", lib.loc=""/Library/Frameworks/R.framework/Versions/3.3/Resources/library"")\n#library(ggpmisc)\n#library(gridExtra)\n\n#pool1 and 2\ndata<-read.csv(""Standaard_boxplot_pool_2.txt"",stringsAsFactors = TRUE,header = TRUE,sep=""\\t"",dec=""."")\n\nSpecies <- as.factor(data$species)\nrel_std <- as.numeric(data$res_std)\n\nggplot(data=data, aes(y=as.numeric(data$res_std),x=data$species))+\n  geom_boxplot()+\n  ggtitle("""")+labs(x = \'Species\', y =""\'Calibration\' sample relative read mapping number"")+\n  theme(axis.text=element_text(size=14,face=""italic""), axis.title=element_text(size=16), plot.title = element_text(size = 14, face = ""bold""),legend.title=element_text(size=14), \n        legend.text=element_text(size=14), legend.position=""right"")+\n  ggsave(""/Users/NielsWagemaker/Dropbox/Documents/PROJECTEN/HANS/SGBS_barcoding/article_2018/DATA/FIGURES/boxplot_STD_pool2.png"", width=10, height=8, dpi=200)\nplot\n\n']","Data from msGBS: A new high-throughput approach to quantify the relative species abundance in root samples of multi-species plant communities Plant interactions are as important belowground as aboveground. Belowground plant interactions are however inherently difficult to quantify, as roots of different species are difficult to disentangle. Although for a couple of decades molecular techniques have been successfully applied to quantify root abundance, root identification and quantification in multi-species plant communities remains particularly challenging.Here we present a novel methodology, multi-species Genotyping By Sequencing (msGBS), as a next step to tackle this challenge. First, a multi-species meta-reference database containing thousands of gDNA clusters per species is created from GBS derived High Throughput Sequencing (HTS) reads. Second, GBS derived HTS reads from multi-species root samples are mapped to this meta-reference which, after a filter procedure to increase the taxonomic resolution, allows the parallel quantification of multiple species. The msGBS signal of 111 mock-mixture root samples, with up to 8 plant species per sample, was used to calculate the within-species abundance. Optional subsequent calibration yielded the across-species abundance. The within- and across-species abundances highly correlated (R2 range 0.72-0.94 and 0.85-0.98, respectively) to the biomass-based species abundance. Compared to a qPCR based method which was previously used to analyze the same set of samples, msGBS provided similar results. Additional data on 11 congener species groups within 105 natural field root samples showed high taxonomic resolution of the method. msGBS is highly scalable in terms of sensitivity and species numbers within samples, which is a major advantage compared to the qPCR method and advances our tools to reveal hidden belowground interactions.This dataset belongs to the article ""msGBS: A new high-throughput approach to quantify the relative species abundance in root samples of multi-species plant communities"". msGBS is a technique that uses Genotyping By Sequencing on mixed plant species root samples which, after a filtering step to increase the taxonomic resolution and calibration, is able to estimate plant species abundances. The article uses data of two different experiment:the Jena field survay (13 plant species) andthe Dutch field survay (120 plant species).",0
Data from: Fungal endophyteinfected leaf litter alters instream microbial communities and negatively influences aquatic fungal sporulation,"Endophytes are ubiquitous plantassociated microbes and although they have the potential to alter the decomposition of infected leaf litter, this has not been wellstudied. The endophyte Rhytisma punctatum infects the leaves of Acer macrophyllum (bigleaf maple), causing the appearance of black 'tar spots' that persist in senesced leaves. Other foliar fungi also cause visible damage in healthy tissues of this host plant system including an unidentified bullseyeshaped lesion, common in western Washington. Using three treatments of endophyte infection status in leaf tissue (R. punctatuminfected, bullseyeinfected, lesionfree), leaf litter discs were submerged in a thirdorder temperate stream using mesh litter bags and harvested periodically over two months to determine the effects of litter treatment and incubation time on litter mass loss, fungal sporulation, and microbial community colonization. Litter containing symptomatic endophyte infections (Rhytisma or bullseye) had reduced sporulation of aquatic hyphomycetes, but decomposed significantly faster than lesionfree or bullseyeinfected litter. Using ampliconbased sequencing, we found a significant difference in bacterial communities colonizing Rhytismainfected and bullseyeinfected leaf litter, a significant difference in fungal communities colonizing Rhytismainfected leaf litter compared to the two other treatments, and a change in both community structure and relative abundances of bacterial and fungal taxa throughout the study period. Indicator Species Analysis clarified the drivers of these community shifts at the genus level. Our results show that endophyteassociated, instream sporulation and microbial community effects are observable within one species of leaf litter.","['############################################################################################\r\n#decomposition\r\ndf <- read.csv(""rhytisma_AFDM_spor_data.csv"")\r\nhead(df)\r\n\r\nstr(df)\r\n\r\nmodel <- lm(ln_AFDM_remaining ~ Treatment * Harvest_days, data=df)\r\nmodel\r\n\r\nsummary(model)\r\n\r\nanova(model)\r\n\r\nR <- subset(df, df$Treatment == ""InR"")\r\nB <- subset(df, df$Treatment == ""InB"")\r\nU <- subset(df, df$Treatment == ""CtrlU"")\r\n\r\nlmR <- lm(R$ln_AFDM_remaining ~ R$Harvest_days)\r\nlmR\r\n\r\nlmB <- lm(B$ln_AFDM_remaining ~ B$Harvest_days)\r\nlmB\r\n\r\nlmU <- lm(U$ln_AFDM_remaining ~ U$Harvest_days)\r\nlmU\r\n\r\nplot(AFDM_remaining ~ Harvest_days, data=df, xlab=""Harvest (days)"",ylab=""% AFDM Remaining"",type=\'n\')\r\n\r\npoints(R$AFDM_remaining~R$Harvest_days, pch=15,col=""black"")\r\npoints(U$AFDM_remaining~U$Harvest_days, pch=2,col=""black"")\r\npoints(B$AFDM_remaining~B$Harvest_days, pch=16,col=""darkgrey"")\r\n\r\nlmB\r\nlmR\r\nlmU\r\n\r\nabline(lm(B$AFDM_remaining ~ B$Harvest_days), lty=2, col=""darkgrey"")\r\nabline(lm(R$AFDM_remaining ~ R$Harvest_days), lty=1,col=""black"")\r\nabline(lm(U$AFDM_remaining ~ U$Harvest_days),lty=3,col=""black"")\r\nlegend(""topright"", c(""Rhytisma-infected"",""bullseye-infected"",""uninfected""), lty=c(1,2,3), pch=c(15,16,2), col=c(""black"",""darkgrey"",""black""))\r\n\r\ndev.off()\r\n############################################################################################\r\n\r\n#sporulation rate\r\ndfsum\r\n948.96425+618.58957 #sporulation_rate+se\r\n38.99219+13.62726\r\n63.52872+42.67612\r\n\r\nlimits <- aes(ymax = dfsum$sporulation_rate + dfsum$se, ymin = dfsum$sporulation_rate)\r\np <- ggplot(data = dfsum, aes(x =Treatment, y =sporulation_rate, fill = Treatment))\r\np\r\nPgraph <- p + geom_bar(stat = ""identity"",position = position_dodge(0.9),\r\n                       color=""#000000"")  + geom_errorbar(limits, position = position_dodge(0.9), \r\n                                                         width = 0.25) + labs(x = ""Tissue infection type"", \r\n                                                                              y = ""Sporulation rate (conidia/mg AFDM * day)"") + \r\n  ggtitle("""") + scale_fill_manual(name = ""Treatment"",\r\n                                  values=c(""grey"", ""black"",""white"")) + theme(legend.position=""none"",panel.grid.major=element_blank(),panel.grid.minor=element_blank(),\r\n                                                                             panel.background=element_rect(fill=""white""),\r\n                                                                             panel.border=element_rect(fill=NA,color=""black""),axis.text.x = element_text (color=""black""),\r\n                                                                             axis.text.y = element_text (color=""black"")) + annotate(""text"",x=3,y=1600,label=""B"") + \r\n  annotate(""text"",x=1,y=85,label=""A"")+annotate(""text"",x=2,y=140,label=""A"")\r\nPgraph\r\n############################################################################################\r\n\r\n#community analyses and figures were completed using PC-ORD\r\n############################################################################################\r\n\r\n#supplementary phyloseq figures\r\nlibrary(phyloseq)\r\nlibrary(ggplot2)\r\nfungi <- read.csv(""fungi.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\ntaxa <- read.csv(""taxa.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\nenv <- read.csv(""env.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\n\r\nfungi <- as.matrix(fungi)\r\ntaxa <- as.matrix(taxa)\r\n#env should be df not matrix\r\n\r\nOTU = otu_table(fungi, taxa_are_rows = TRUE)\r\nTAX = tax_table(taxa)\r\n\r\nphyseq = phyloseq(OTU, TAX)\r\n\r\nenvdata = sample_data(env)\r\nsample_names(physeq) #make sure names match\r\nsample_names(envdata) #make sure names match\r\n\r\nmerged = merge_phyloseq(physeq, envdata)\r\n\r\n#Convert to relative abundance and remove low abundance taxa (<0.1%)\r\nrelabund = transform_sample_counts(merged, function(x) x / sum (x))\r\nno_low_taxa = filter_taxa(relabund, function(x) mean (x) > 1e-3, TRUE)\r\n\r\nno_low_taxa_fungi <- otu_table(no_low_taxa, taxa_are_rows = FALSE)\r\nwrite.csv(no_low_taxa_fungi, file = \'no_low_taxa_fungi.csv\')\r\n\r\nrelabund_fungi <- otu_table(relabund, taxa_are_rows = FALSE)\r\nwrite.csv(relabund_fungi, file = \'relabund_fungi2.csv\')\r\n\r\n#per Brett\'s suggestion, removing any OTUs not resolved past order, which will remove 27 #OTUS:\r\n#removes rows 2, 8 ,28, 44, and 46\r\n#remaining taxa saved as no_low_taxa_fungi-resolved.csv in the usual dir\r\n\r\nfungi <- read.csv(""fungi-resolved2.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\ntaxa <- read.csv(""taxa_fungi-resolved.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\nenv <- read.csv(""env.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\n\r\nfungi <- as.matrix(fungi)\r\ntaxa <- as.matrix(taxa)\r\n#env should be df not matrix\r\n\r\nOTU = otu_table(fungi, taxa_are_rows = TRUE)\r\nTAX = tax_table(taxa)\r\n\r\nphyseq = phyloseq(OTU, TAX)\r\n\r\nenvdata = sample_data(env)\r\nsample_names(physeq) #make sure names match\r\nsample_names(envdata) #make sure names match\r\n\r\nmerged = merge_phyloseq(physeq, envdata)\r\n\r\n#Convert to relative abundance and remove low abundance taxa (<0.1%)\r\nrelabund = transform_sample_counts(merged, function(x) x / sum (x))\r\nno_low_taxa = filter_taxa(relabund, function(x) mean (x) > 1e-3, TRUE)\r\n\r\notus <- otu_table(no_low_taxa, taxa_are_rows = FALSE)\r\nwrite.csv(otus, file = \'fungi-resolved_OTUs2.csv\')\r\n\r\nfungi <- read.csv(""fungi-resolved_OTUs2.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\ntaxa <- read.csv(""taxa_fungi-resolved.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\nenv <- read.csv(""env.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\n\r\nfungi <- as.matrix(fungi)\r\ntaxa <- as.matrix(taxa)\r\n#env should be df not matrix\r\n\r\nOTU = otu_table(fungi, taxa_are_rows = TRUE)\r\nTAX = tax_table(taxa)\r\n\r\nphyseq = phyloseq(OTU, TAX)\r\n\r\nenvdata = sample_data(env)\r\nsample_names(physeq) #make sure names match\r\nsample_names(envdata) #make sure names match\r\n\r\nmerged = merge_phyloseq(physeq, envdata)\r\nrelabund = transform_sample_counts(merged, function(x) x / sum (x))\r\n\r\notus <- otu_table(relabund, taxa_are_rows = FALSE)\r\nwrite.csv(otus, file = \'fungi-resolved_rel2.csv\')\r\n#this was renamed to a .xlsx for calculations\r\n#new sheet with just values given same name as fungi-resolved_rel2_final.csv to\r\n#avoid confusion\r\n\r\n\r\n#after relativizing the relative abundance matrix (2 punches for A0, so divide all counts in #each column by 2, etc. so that the rel abund on y=1)\r\nfungi <- read.csv(""fungi-resolved_rel2_final.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\ntaxa <- read.csv(""taxa_fungi-resolved_OTUs.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\nenv <- read.csv(""env.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\nfungi <- as.matrix(fungi)\r\ntaxa <- as.matrix(taxa)\r\n\r\nlevels(env$Treatment) <- c(""bullseye-infected"",""Rhytisma-infected"",""lesion-free"")\r\n\r\nenv$Harvest <- as.factor(env$Harvest)\r\n#env should be df not matrix\r\n\r\nOTU = otu_table(fungi, taxa_are_rows = TRUE)\r\nTAX = tax_table(taxa)\r\n\r\nphyseq = phyloseq(OTU, TAX)\r\nenvdata = sample_data(env)\r\nsample_names(physeq) #make sure names match\r\nsample_names(envdata) #make sure names match\r\nmerged = merge_phyloseq(physeq, envdata)\r\n#rolled 43-color palette w/ soft k-means on IWantHue\r\nf <- plot_bar(merged, ""Harvest"", fill = ""Family"", facet_grid = ~Treatment) + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position=""bottom"") + scale_fill_manual(values=c(""#dc375f"", ""#9d4e55"",\r\n                                                                                                                                                                                                                               ""#d96e76"",""#b63931"",""#e86a55"", ""#d75623"", ""#dd9a78"", ""#9c5b2f"", ""#dd9653"",\r\n                                                                                                                                                                                                                               ""#df9529"", ""#9e7423"", ""#c6ae31"", ""#766f2f"", ""#bcb365"", ""#899332"", ""#a8bd33"",\r\n                                                                                                                                                                                                                               ""#567531"", ""#6eb729"", ""#86be59"", ""#538d30"", ""#339e36"", ""#53cb5b"", ""#79b779"",\r\n                                                                                                                                                                                                                               ""#2f7c52"", ""#38c481"", ""#56bca0"", ""#3abec8"", ""#53a4d6"", ""#5b8cdf"", ""#5765a5"",\r\n                                                                                                                                                                                                                               ""#6666d4"", ""#b098dc"", ""#8555a3"", ""#a44ec9"", ""#cf7fe1"", ""#ac3d96"", ""#9e5d8d"",\r\n                                                                                                                                                                                                                               ""#df51b7"", ""#e080bf"", ""#d06792"", ""#e0458b"", ""#a13462"", ""#e796b0"")) +geom_bar(colour=""transparent"", stat=""identity"")\r\nf\r\nggsave(f, filename = ""fig_fun.jpeg"", dpi = 300)\r\n\r\n#for bacteria\r\nlibrary(phyloseq)\r\nbac <- read.csv(""bac-resolved_OTUs_rel.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\ntaxa <- read.csv(""taxa_bac_resolved2.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\nenv <- read.csv(""env_b.csv"", header=TRUE, row.names=1, stringsAsFactors=TRUE)\r\nlevels(env$Treatment) <- c(""bullseye-infected"",""Rhytisma-infected"",""lesion-free"")\r\n\r\nbac <- as.matrix(bac)\r\ntaxa <- as.matrix(taxa)\r\nenv$Harvest <- as.factor(env$Harvest)\r\n#env should be df not matrix\r\n\r\nOTU = otu_table(bac, taxa_are_rows = TRUE)\r\nTAX = tax_table(taxa)\r\n\r\nphyseq = phyloseq(OTU, TAX)\r\nenvdata = sample_data(env)\r\nsample_names(physeq) #make sure names match\r\nsample_names(envdata) #make sure names match\r\nmerged = merge_phyloseq(physeq, envdata)\r\n\r\n\r\n#now for family as the fill\r\nf <- plot_bar(merged, ""Harvest"", fill = ""Family"", facet_grid = ~Treatment) + theme_bw() + theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank(), legend.position=""bottom"") + scale_fill_manual(values=c(""#df3666"",""#b04e57"",""#e3897a"",""#d14832"",""#a0542d"",""#d97c3f"",""#dd9a35"",\r\nf\r\nggsave(f, filename = ""fig_bac.jpeg"", dpi = 300)']","Data from: Fungal endophyteinfected leaf litter alters instream microbial communities and negatively influences aquatic fungal sporulation Endophytes are ubiquitous plantassociated microbes and although they have the potential to alter the decomposition of infected leaf litter, this has not been wellstudied. The endophyte Rhytisma punctatum infects the leaves of Acer macrophyllum (bigleaf maple), causing the appearance of black 'tar spots' that persist in senesced leaves. Other foliar fungi also cause visible damage in healthy tissues of this host plant system including an unidentified bullseyeshaped lesion, common in western Washington. Using three treatments of endophyte infection status in leaf tissue (R. punctatuminfected, bullseyeinfected, lesionfree), leaf litter discs were submerged in a thirdorder temperate stream using mesh litter bags and harvested periodically over two months to determine the effects of litter treatment and incubation time on litter mass loss, fungal sporulation, and microbial community colonization. Litter containing symptomatic endophyte infections (Rhytisma or bullseye) had reduced sporulation of aquatic hyphomycetes, but decomposed significantly faster than lesionfree or bullseyeinfected litter. Using ampliconbased sequencing, we found a significant difference in bacterial communities colonizing Rhytismainfected and bullseyeinfected leaf litter, a significant difference in fungal communities colonizing Rhytismainfected leaf litter compared to the two other treatments, and a change in both community structure and relative abundances of bacterial and fungal taxa throughout the study period. Indicator Species Analysis clarified the drivers of these community shifts at the genus level. Our results show that endophyteassociated, instream sporulation and microbial community effects are observable within one species of leaf litter.",0
Early sexual dimorphism in the developing gut microbiome of northern elephant seals,"The gut microbiome is an integral part of a species' ecology, but we know little about how host characteristics impact its development in wild populations. Here, we explored the role of such intrinsic factors in shaping the gut microbiome of northern elephant seals during a critical developmental window of six weeks after weaning, when the pups stay ashore without feeding. We found substantial sex-differences in the early-life gut microbiome, even though males and females could not yet be distinguished morphologically. Sex and age both explained around 15% of the variation in gut microbial beta diversity, while microbial communities sampled from the same individual showed high levels of similarity across time, explaining another 40% of the variation. Only a small proportion of the variation in beta diversity was explained by health status, assessed by full blood counts, but clinically healthy individuals had a greater microbial alpha diversity than their clinically abnormal peers. Across the post-weaning period, the northern elephant seal gut microbiome was highly dynamic. We found evidence for several colonisation and extinction events as well as a decline in Bacteroides and an increase in Prevotella, a pattern that has previously been associated with the transition from nursing to solid food. Lastly, we show that genetic relatedness was correlated with gut microbiome similarity in males but not females, again reflecting early sex-differences. Our study represents a naturally diet-controlled and longitudinal investigation of how intrinsic factors shape the early gut microbiome in a species with extreme sex differences in morphology and life history.",,"Early sexual dimorphism in the developing gut microbiome of northern elephant seals The gut microbiome is an integral part of a species' ecology, but we know little about how host characteristics impact its development in wild populations. Here, we explored the role of such intrinsic factors in shaping the gut microbiome of northern elephant seals during a critical developmental window of six weeks after weaning, when the pups stay ashore without feeding. We found substantial sex-differences in the early-life gut microbiome, even though males and females could not yet be distinguished morphologically. Sex and age both explained around 15% of the variation in gut microbial beta diversity, while microbial communities sampled from the same individual showed high levels of similarity across time, explaining another 40% of the variation. Only a small proportion of the variation in beta diversity was explained by health status, assessed by full blood counts, but clinically healthy individuals had a greater microbial alpha diversity than their clinically abnormal peers. Across the post-weaning period, the northern elephant seal gut microbiome was highly dynamic. We found evidence for several colonisation and extinction events as well as a decline in Bacteroides and an increase in Prevotella, a pattern that has previously been associated with the transition from nursing to solid food. Lastly, we show that genetic relatedness was correlated with gut microbiome similarity in males but not females, again reflecting early sex-differences. Our study represents a naturally diet-controlled and longitudinal investigation of how intrinsic factors shape the early gut microbiome in a species with extreme sex differences in morphology and life history.",0
"Supplementary material 1 from: Leidenberger S, Bostrm S, Wayland MT (2020) Host records and geographical distribution of Corynosoma magdaleni, C. semerme and C. strumosum (Acanthocephala: Polymorphidae). Biodiversity Data Journal 8: e50500. https://doi.org/10.3897/BDJ.8.e50500",R script for generating geographical distribution maps.,"['# Script used to create distribution maps\n\n# Load required packages (install if necessary)\nif(!require(devtools)){\n        install.packages(""devtools"")\n        require(devtools)\n}\n\nif(!require(meowR)){\n        install.github(""meowR"", ""jebyrnes"")\n        require(meowR)\n}\n\n# Define function for creating distribution maps\nmakeDistributionMap <- function(ecoregions, filename){\n        if(file.exists(filename)){\n                stop(paste(""File exists: "", filename, sep=""""))\n        }\n        fillCol <- rep(1, length(ecoregions))\n        distData <- as.data.frame(cbind(ecoregions, fillCol))\n        distributionMap <- makeMEOWmap(newdata=distData,\n                                       fillColName=""fillCol"",\n                                       regionColName=""ecoregions"",\n                                       add.worldmap = T,\n                                       fillPal=""#377EB8"", \n                                       pathCol=""#377EB8"")\n        ggsave(filename, width=200, height=100, units=""mm"")\n}\n\n# Get a vector of all ecoregions, so that we can check the names in our vectors of ecoregions for each species.\ndata(""regions.df"")\nallEcoregions <- unique(regions.df$ECOREGION)\n\n# Corynosoma magdaleni\nmagdaleniEcoregions <- c(""Northern Grand Banks - Southern Labrador"",\n        ""Northern Labrador"",\n        ""Baffin Bay - Davis Strait"",\n        ""Hudson Complex"",\n        ""Lancaster Sound"",\n        ""Gulf of St. Lawrence - Eastern Scotian Shelf"",\n        ""Southern Grand Banks - South Newfoundland"",\n        ""Scotian Shelf"",\n        ""Gulf of Maine/Bay of Fundy"",\n        ""Baltic Sea"",\n        ""North Sea"")\n        \nsetdiff(magdaleniEcoregions, allEcoregions)\n\nmakeDistributionMap(magdaleniEcoregions, ""magdaleni_map.png"")\n\n# Corynosoma semerme\nsemermeEcoregions <- c(""Aleutian Islands"",\n                       ""Baltic Sea"",\n                       ""Beaufort Sea - continental coast and shelf"",\n                       ""Beaufort-Amundsen-Viscount Melville-Queen Maud"",\n                       ""Celtic Seas"",\n                       ""Chukchi Sea"",\n                       ""East Greenland Shelf"",\n                       ""East Siberian Sea"",\n                       ""Eastern Bering Sea"",\n                       ""Gulf of Alaska"",\n                       ""Gulf of St. Lawrence - Eastern Scotian Shelf"",\n                       ""High Arctic Archipelago"",\n                       ""Kara Sea"",\n                       ""Lancaster Sound"",\n                       ""Laptev Sea"",\n                       ""North American Pacific Fijordland"",\n                       ""North and East Barents Sea"",\n                       ""North and East Iceland"",\n                       ""North Greenland"",\n                       ""North Sea"",\n                       ""Northern Grand Banks - Southern Labrador"",\n                       ""Northern Labrador"",\n                       ""Northern Norway and Finnmark"",\n                       ""Scotian Shelf"",\n                       ""Sea of Okhotsk"",\n                       ""Southern Grand Banks - South Newfoundland"",\n                       ""Southern Norway"",\n                       ""West Greenland Shelf"",\n                       ""White Sea"")\n\nsetdiff(semermeEcoregions, allEcoregions)\n\nmakeDistributionMap(semermeEcoregions, ""semerme_map.png"")\n\n\n# Corynosoma strumosum\nstrumosumEcoregions <- c(""Adriatic Sea"",\n                         ""Aegean Sea"",\n                         ""Alboran Sea"",\n                         ""Aleutian Islands"",\n                         ""Baltic Sea"",\n                         ""Beaufort Sea - continental coast and shelf"",\n                         #""Caspian Sea"",\n                         ""Celtic Seas"",\n                         ""Central Kuroshio Current"",\n                         ""Central New Zealand"",\n                         ""Chukchi Sea"",\n                         ""Cortezian"",\n                         ""East China Sea"",\n                         ""East Greenland Shelf"",\n                         ""East Siberian Sea"",\n                         ""Eastern Bering Sea"",\n                         ""Faroe Plateau"",\n                         ""Gulf of Alaska"",\n                         ""Gulf of St. Lawrence - Eastern Scotian Shelf"",\n                         ""Hudson Complex"",\n                         ""Ionian Sea"",\n                         ""Kamchatka Shelf and Coast"",\n                         ""Kara Sea"",\n                         ""Laptev Sea"",\n                         ""Magdalena Transition"",\n                         ""North American Pacific Fijordland"",\n                         ""North and East Barents Sea"",\n                         ""North and East Iceland"",\n                         ""North Greenland"",\n                         ""North Sea"",\n                         ""Northeastern Honshu"",\n                         ""Northeastern New Zealand"",\n                         ""Northern California"",\n                         ""Northern Grand Banks - Southern Labrador"",\n                         ""Northern Labrador"",\n                         ""Oregon, Washington, Vancouver Coast and Shelf"",\n                         ""Oyashio Current"",\n                         ""Puget Trough/Georgia Basin"",\n                         ""Scotian Shelf"",\n                         ""Sea of Japan/East Sea"",\n                         ""Sea of Okhotsk"",\n                         ""Southern Grand Banks - South Newfoundland"",\n                         ""South and West Iceland"",\n                         ""South European Atlantic Shelf"",\n                         ""South New Zealand"",\n                         ""Southern California Bight"",\n                         ""Southern Grand Banks - South Newfoundland"",\n                         ""Southern Norway"",\n                         ""West Greenland Shelf"",\n                         ""Western Mediterranean"",\n                         ""White Sea"",\n                         ""Yellow Sea"")\n\nsetdiff(strumosumEcoregions, allEcoregions)\n\nmakeDistributionMap(strumosumEcoregions, ""strumosum_map.png"")\n\n\n']","Supplementary material 1 from: Leidenberger S, Bostrm S, Wayland MT (2020) Host records and geographical distribution of Corynosoma magdaleni, C. semerme and C. strumosum (Acanthocephala: Polymorphidae). Biodiversity Data Journal 8: e50500. https://doi.org/10.3897/BDJ.8.e50500 R script for generating geographical distribution maps.",0
Analyzing indoor mycobiomes through a large-scale citizen science study in Norway,"In the built environment, fungi can cause important deterioration of building materials and adverse health effects for the occupants. Increased knowledge about indoor mycobiomes from different regions of the world, and their main environmental determinants, will enable improved indoor air quality management and identification of health risks. This is the first citizen science study about indoor mycobiomes at a large geographical scale in Europe, including 271 houses from Norway and 807 dust samples from three house compartments: outside of the building, living room and bathroom. The fungal community composition determined by DNA metabarcoding was clearly different between indoor and outdoor samples, but there were no significant differences between the two indoor compartments. The 32 selected variables, related to the outdoor environment, building features and occupant characteristics, accounted for 15% of the overall variation in community composition, with the house compartment as the key factor (7.6%). Next, the climate was the main driver of the dust mycobiomes (4.2%), while building and occupant variables had significant but minor influences (1.4% and 1.1%, respectively). The house-dust mycobiomes were dominated by ascomycetes (70%) with Capnodiales and Eurotiales as the most abundant orders. Compared to the outdoor samples, the indoor mycobiomes showed higher species richness, which is probably due to the mixture of fungi from outdoor and indoor sources. The main indoor indicator fungi belonged to two ecological groups with allergenic potential: xerophilic molds and skin-associated yeasts. Our results suggest that citizen science is a successful approach for unraveling the built microbiome at large geographical scales.",,"Analyzing indoor mycobiomes through a large-scale citizen science study in Norway In the built environment, fungi can cause important deterioration of building materials and adverse health effects for the occupants. Increased knowledge about indoor mycobiomes from different regions of the world, and their main environmental determinants, will enable improved indoor air quality management and identification of health risks. This is the first citizen science study about indoor mycobiomes at a large geographical scale in Europe, including 271 houses from Norway and 807 dust samples from three house compartments: outside of the building, living room and bathroom. The fungal community composition determined by DNA metabarcoding was clearly different between indoor and outdoor samples, but there were no significant differences between the two indoor compartments. The 32 selected variables, related to the outdoor environment, building features and occupant characteristics, accounted for 15% of the overall variation in community composition, with the house compartment as the key factor (7.6%). Next, the climate was the main driver of the dust mycobiomes (4.2%), while building and occupant variables had significant but minor influences (1.4% and 1.1%, respectively). The house-dust mycobiomes were dominated by ascomycetes (70%) with Capnodiales and Eurotiales as the most abundant orders. Compared to the outdoor samples, the indoor mycobiomes showed higher species richness, which is probably due to the mixture of fungi from outdoor and indoor sources. The main indoor indicator fungi belonged to two ecological groups with allergenic potential: xerophilic molds and skin-associated yeasts. Our results suggest that citizen science is a successful approach for unraveling the built microbiome at large geographical scales.",0
Predicted occurrence probability for ticks in Great Britain (2014 to 2021) at 1 km spatial resolution,"The dataset contains predictions of occurrence probability for ticks in Great Britain (2014 to 2021) at 1 km spatial resolution + all covariate layers used for modeling. Over seven million electronic health records (EHRs), among which 11,741 EHRs reported tick attachment, were used to evaluate climate, environmental and animal host factors affecting the risk of tick attachment in cats and dogs in Great Britain (GB). The tick presence/absence EHRs for dogs and cats were further overlaid with spatiotemporal time-series of climatic, vegetation, human influence, hydrological and terrain variables (slope, wetness index) to produce a spatiotemporal regression matrix; an Ensemble Machine Learning framework was used to fine-tune hyperparameters for Random Forest (classif.ranger), Gradient boosting (classif.xgboost) and GLM-net (classif.glmnet) algorithms, which were then used to produce a final ensemble meta-learner that predicts the probability of occurrence of ticks across GB with monthly intervals.gb1km_covariates.zip contains ALL covariate layers as GeoTIFFs (time-series) used for modeling ticks dynamics;data_1km_2014_M01.rds = contains all covariates for January 2014 prepared as SpatialGridDataFrame (R data object);Codes of files indicate e.g.:""monthly.tick.prob_savsnet.mar_p_1km_s_2014_2021"" = monthly occurrence probability for January based on the training data from 2014 to 2021;""monthly.tick.prob_savsnet.oct_md_1km_s_20211001_20211031"" = monthly prediction (model) error derived as the standard deviation from multiple base learners;The dataset is described in detail in the following publication:Arsevska, E., Hengl, T., Singelton, D. et al. (2023?) Risk factors for tick attachment in companion animals in Great Britain: a spatiotemporal analysis covering 20142021. Submitted to Parasites & Vectors (in review).Acknowledgements: We are grateful to data providers in veterinary practice (VetSolutions, Teleos, CVS, and other practitioners). We are grateful to the INRAE MIGALE bioinformatics facility (MIGALE, INRAE, 2020. Migale Bioinformatics Facility, doi: 10.15454/1.5572390655343293E12) for providing computing resources. We are also grateful forthe help and support provided by SAVSNET team members Bethaney Brant, Susan Bolan and Steven Smyth.This study was funded mainly by a grant from the Biotechnology and Biological Sciences Research Council,BB/NO19547/1 and British Small Animal Veterinary Association (BSAVA). The research was partly funded by the National Institute for Health Research Health Protection Research Unit (NIHR HPRU) in Emerging and Zoonotic Infections at the University of Liverpool in partnership with Public Health England (PHE) and Liverpool School of Tropical Medicine (LSTM). This work has been partially funded by the Monitoring outbreak events for disease surveillance in a data science context"" (MOOD) project from the European Unions Horizon 2020 research and innovation program under grant agreement No. 874850 (https://mood-h2020.eu/). The views expressed are those of the authors and not necessarily those of the NHS, the NIHR, the Department of Health or Public Health England.","['## Spacetime modeling ticks\n## tom.hengl@opengeohub.org & elena.arsevska@cirad.fr\n\nlibrary(raster)\nlibrary(terra)\nlibrary(rgdal)\n#library(landmap)\nlibrary(data.table)\nlibrary(vroom)\nlibrary(openair)\nlibrary(lubridate)\n#load("".RData"")\nsource(""SPM_functions.R"")\n\n## gridded data ----\n## 1km raster:\ngb1km = raster(""./data/Layers1km/cats_1km_GB.tif"")\ngb1km\ncrs = ""+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs""\n\n## import ALL consultations ----\nticks = readRDS.gz(""/mnt/landmark/sdm_ticks_gb/data/SAVSNET/cons_sdm_gb_anim.rds"")\nticks = as.data.frame(cbind(ticks, sf::st_coordinates(ticks)))\ndim(ticks)\n## 7518709      20\nhead(ticks)\nsummary(as.factor(ticks$tick))\n#        0       1 \n## 7506968   11741\n## 0.15%\n## Zero inflated variable. Only 0.15% are detected ticks\n## convert to cumulative months since 2000:\nticks$cmonth = 12*(year(ymd(paste0(ticks$year_month, ""-01""))) - year(ymd(""2000-01-01""))) + month(ymd(paste0(ticks$year_month, ""-01"")))\n#hist(ticks$cmonth)\n## after month 198 there is still some data but less and less\nsummary(ticks$Date)\n#Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n#""2014-03-27"" ""2017-10-10"" ""2019-03-26"" ""2019-03-20"" ""2020-11-13"" ""2022-11-01""\n## local coords\nticks$olc_id = olctools::encode_olc(ticks$Y, ticks$X, 11)\nlength(levels(as.factor(ticks$olc_id)))\n## 609,412\n## spatial blocking 5.5km\n## https://en.wikipedia.org/wiki/Open_Location_Code#Specification\nticks$ID = olctools::encode_olc(ticks$Y, ticks$X, 6)\nlength(levels(as.factor(ticks$ID)))\n## 8314\nticks.xy = ticks[!duplicated(ticks$olc_id),c(""olc_id"",""X"",""Y"")]\n#dim(ticks.xy)\nticks.xy = ticks.xy[!is.na(ticks.xy$X),]\ncoordinates(ticks.xy) <- ~ X + Y\nproj4string(ticks.xy) <- ""EPSG:4326""\nticks.pr = spTransform(ticks.xy, CRS(crs))\n## plot(ticks.pr)\nticks$eastng = plyr::join(ticks[""olc_id""], as.data.frame(ticks.pr[""olc_id""]))$X\nticks$nrthng = plyr::join(ticks[""olc_id""], as.data.frame(ticks.pr[""olc_id""]))$Y\n## duplicate ID check:\nsummary(duplicated(ticks$cons_ID))\nhead(ticks)\n\n## Export for plotting:\n## 3971\nxyt.pnts = ticks[ticks$tick==1,]\nxyt.pnts = xyt.pnts[!is.na(xyt.pnts$X),]\ncoordinates(xyt.pnts) <- ~ X + Y\nproj4string(xyt.pnts) <- ""EPSG:4326""\n#plot(xyt.pnts)\nunlink(""./data/SAVSNET/cat_dog_tick_cons_raw.gpkg"")\nwriteOGR(xyt.pnts[""species""], ""./data/SAVSNET/cat_dog_tick_cons_raw.gpkg"", ""cat_dog_tick_cons_raw"", ""GPKG"")\nsummary(as.factor(xyt.pnts$year_month))\n## Copy density of dogs and cats:\n#ticks$dogs_dens = terra::extract(terra::rast(""/mnt/landmark/sdm_ticks_gb/data/Layers1km/dogs_1km_GB.tif""), terra::vect(as.matrix(ticks[,c(""eastng"", ""nrthng"")]), crs=crs))[,2]\n#ticks$cats_dens = terra::extract(terra::rast(""/mnt/landmark/sdm_ticks_gb/data/Layers1km/cats_1km_GB.tif""), terra::vect(as.matrix(ticks[,c(""eastng"", ""nrthng"")]), crs=crs))[,2]\n#ticks$deer_dens = terra::extract(terra::rast(""/mnt/landmark/sdm_ticks_gb/gb1km/static/deer.prob_osgbirall3_p_1km_s_20180101_20201231_gb_epsg.27700_v20230207.tif""), terra::vect(as.matrix(ticks[,c(""eastng"", ""nrthng"")]), crs=crs))[,2]\n#ticks$tick_dens = ifelse(ticks$species==""cat"", ticks$tick_count/ticks$cats_dens, ticks$tick_count/ticks$dogs_dens) * 1e3\n#hist(log1p(ticks$tick_dens))\nticks$row.id = 1:nrow(ticks)\n\n## static tif overlay ----\ngc()\nstat.tifs = list.files(""/mnt/landmark/sdm_ticks_gb/gb1km/static"", glob2rx(""*.tif$""), full.names = TRUE)\n## 87\n## RAM issues!\nov.stat = parallel::mclapply(stat.tifs, function(i){terra::extract(terra::rast(i), terra::vect(as.matrix(ticks[,c(""eastng"", ""nrthng"")]), crs=crs))}, mc.cores=10)\nov.stat = dplyr::bind_cols(lapply(ov.stat, function(i){i[,2]}))\nnames(ov.stat) = tools::file_path_sans_ext(basename(stat.tifs))\nov.stat$row.id = 1:nrow(ticks)\nsave.image()\n\n## st overlay ----\n#mc.cores = parallel::detectCores()\ngc()\ntif.lst = list.files(""./gb1km/2020"", glob2rx(""*.tif""), full.names = TRUE)\n## 104\nlst.tifs = expand.grid(tif.lst, 2014:2021, stringsAsFactors = FALSE)\nlst.tifs$file = sapply(1:nrow(lst.tifs), function(i){gsub(""2020"", paste(lst.tifs$Var2[i]), paste(lst.tifs$Var1[i]))})\n## 832\n#lst.tifs$begin.tif = sapply(paste(lst.tifs$file), function(i){stripym(i, type=""begin"")})\n#lst.tifs$end.tif = sapply(paste(lst.tifs$file), function(i){stripym(i, type=""end"")})\nlst.tifs$begin.tif = lubridate::ymd(sapply(basename(lst.tifs$file), function(i){strsplit(i, ""_"")[[1]][6]}))\nsummary(lst.tifs$begin.tif)\n## 02-29 for some years does not exist!\nlst.tifs$end.tif = lubridate::ymd(sapply(basename(lst.tifs$file), function(i){strsplit(i, ""_"")[[1]][7]}))\nlst.tifs$file[is.na(lst.tifs$end.tif)] = gsub(""0229"", ""0228"", lst.tifs$file[is.na(lst.tifs$end.tif)])\nlst.tifs$end.tif = lubridate::ymd(sapply(basename(lst.tifs$file), function(i){strsplit(i, ""_"")[[1]][7]}))\nsummary(lst.tifs$end.tif)\nlst.tifs$year.tifs = paste(substr(lst.tifs$begin.tif, 1, 4))\nall(file.exists(lst.tifs$file))\n## variable names\nvar.name.lst = sapply(basename(lst.tifs$file), function(i){paste(strsplit(i, ""_"")[[1]][c(1:3)], collapse = ""_"")})\nm.lst = c(""jan"",""feb"",""mar"",""apr"",""may"",""jun"",""jul"",""aug"",""sep"",""oct"",""nov"",""dec"")\nfor(j in m.lst){ var.name.lst[grep(j, var.name.lst)] = gsub(j, """", var.name.lst[grep(j, var.name.lst)]) }\nsummary(as.factor(var.name.lst))\nvar.nms = levels(as.factor(var.name.lst))\ncovs.lst = data.frame(tif=basename(lst.tifs$file), year=lst.tifs$year.tifs, begin=lst.tifs$begin.tif, end=lst.tifs$end.tif, var.name=var.name.lst)\n#View(covs.lst)\nwrite.csv(covs.lst, ""list_tifs_dates.csv"", row.names = FALSE)\nsaveRDS(covs.lst, ""covs.lst.rds"")\n\n## 27 variables\nov.pnts <- parallel::mclapply(1:length(lst.tifs$file), function(i){ \n  extract_st(tif=lst.tifs$file[i], as.data.frame(ticks[,c(""eastng"",""nrthng"",""Date"")]), \n             date=""Date"", crs = crs,        \n             date.tif.begin = lst.tifs$begin.tif[i], \n             date.tif.end = lst.tifs$end.tif[i],\n             coords=c(""eastng"",""nrthng""), variable.name = var.name.lst[i]) }, \n  mc.cores=20)\n## takes 2mins\nnames(ov.pnts) = var.name.lst\nov.pnts = ov.pnts[!sapply(ov.pnts, is.null)]\n## 818 geotifs\n## bind based on variable name\nov.tifs = parallel::mclapply(var.nms, function(i){do.call(rbind, ov.pnts[names(ov.pnts) %in% i])}, mc.cores=length(var.nms))\n## takes 10+ minutes!\nov.tifs = plyr::join_all(ov.tifs, by=""row.id"", type=""full"", match=""first"")\n#str(ov.tifs)\nticks.rm = plyr::join_all(list(ticks, ov.stat, ov.tifs))\ndim(ticks.rm)\n## 6755886     136\ngc()\nsave.image()\n\n## fit models ----\n## 2 hrs to fit all models\n## 0/1 model\nlibrary(mlr)\n#library(h2o)\nsource(""SPM_functions.R"")\nlibrary(dplyr)\n#SL.library = c(""regr.ranger"", ""regr.h2o.deeplearning"", ""regr.cvglmnet"")\nticks.rm$cosmonth = cos((month(ticks.rm$Date)/12)*(2*pi))\nsummary(as.factor(ticks.rm$tick))\nticks.rm$tick_f = as.factor(ticks.rm$tick)\n#summary(as.factor(ticks.rm$species))\nticks.rm$species_n = ifelse(ticks.rm$species==""dog"", 1, ifelse(ticks.rm$species==""cat"", 0, NA))\nsummary(as.factor(ticks.rm$species_n))\n# 0       1    NA\'s \n# 2180815 5712267  536209 \nquantile(ticks.rm$cats_dens, 0.025, na.rm=TRUE)\nquantile(ticks.rm$dogs_dens, 0.025, na.rm=TRUE)\nquantile(ticks.rm$deer_dens, 0.025, na.rm=TRUE)\nsummary(ticks.rm$dogs_dens)\n## 170 NA\'s\n#prT.vars = c(var.nms, tools::file_path_sans_ext(basename(stat.tifs)), ""cosmonth"", ""cats_dens"", ""dogs_dens"", ""species_n"")\nprT.vars = c(var.nms, tools::file_path_sans_ext(basename(stat.tifs)), ""cosmonth"")\n## strange patterns in nighttime sd maps, probably not a good idea to use for mapping?\nprT.vars = prT.vars[-grep(""mod11a2.nighttime_sd"", prT.vars)]\n## artifacts in CHELSA bioclim layers!\nprT.vars = prT.vars[-grep(""bioclim8"", prT.vars)]\nprT.vars = prT.vars[-grep(""bioclim9"", prT.vars)]\n#prT.vars = prT.vars[-grep(""lcv_global.seasonal.s1_earth.big.data"", prT.vars)]\ntvT = ""tick_f""\nprT.vars[which(!prT.vars %in% names(ticks.rm))]\n## use all occurrences for model fine-tuning\nfs.rm0 = rbind(ticks.rm %>% sample_n(5e4), ticks.rm[ticks.rm$tick_f==""1"",])\n#fs.rm0 = fs.rm0[!duplicated(fs.rm0$cons_ID),]\nsummary(fs.rm0$tick_f)\nfm = as.formula(paste0(tvT, "" ~ "", paste(prT.vars, collapse = ""+"")))\nlength(all.vars(fm)[-1])\n## 111\n#fm\n#ticks.rm = ticks.rm[!duplicated(ticks.rm$cons_ID),]\n#dim(ticks.rm)\nna.lst = sapply(all.vars(fm), function(i){sum(is.na(ticks.rm[,i]))})\nsummary(na.lst>5e5)\n## none with missing values\nall.vars(fm)[which(na.lst>1.1e6)]\n## many points fall out because there is no data for years >2020\n#summary(as.factor(ticks$year_month))\nsaveRDS.gz(ticks.rm, ""ticks.rm.rds"")\n#ticks.rm = readRDS.gz(""ticks.rm.rds"")\n\n## subset covs ----\no.mtry = randomForestSRC::rfsrc(fm, as.data.frame(fs.rm0), ntree = 85, mtry=105, importance = TRUE)\no.mtry\n## (OOB) Misclassification rate: 0.1943111\nsaveRDS.gz(as.data.frame(fs.rm0), ""sample_ticks.rm.rds"")\n## variable selection\n#vs.sel <- randomForestSRC::var.select(fm, as.data.frame(fs.rm0), mtry=105, nodesize=1, ntree = 85)\n#str(vs.sel$topvars)\n## all variables stay selected\n## next operation takes 1hr computing + 200GB RAM\ntnd.ml = tune_learners(data = as.data.frame(fs.rm0), formula = fm, blocking = factor(fs.rm0$ID))\n## [Tune] Result: mtry=4 : logloss.test.mean=0.4319283,logloss.test.sd=0.0041423\n## Result: nrounds=10; max_depth=5; eta=0.5; subsample=1; min_child_weight=10; colsample_bytree=0.75 : logloss.test.mean=0.4144562,logloss.test.sd=0.0057818\neml.fs = train_sp_eml(data = as.data.frame(ticks.rm), tune_result = tnd.ml, blocking = as.factor(ticks.rm$ID))\n#eml.fs = readRDS.gz(""./output/mlr_eml.rds"")\nsummary(eml.fs$learner.model$super.model$learner.model)\n#calculate McFadden\'s R-squared for model\nwith(summary(eml.fs$learner.model$super.model$learner.model), 1 - deviance/null.deviance)\n## R-square 0.14\n## ?? training points\n\n## partial dependence plots\n#openair::scatterPlot(ticks.rm, x = ""clm_CHELSA_pr_m"", y = ""tick"", method = ""hexbin"", col = ""increment"",  log.y = TRUE, xlab=""Precipitation"", ylab=""tick occurrence"")\n\nlibrary(ggplot2)\nxl <- as.data.frame(mlr::getFeatureImportance(eml.fs[[""learner.model""]][[""base.models""]][[1]])$res)\nxl$relative_importance = 100*xl$importance/sum(xl$importance)\nxl = xl[order(xl$relative_importance, decreasing = T),]\nxl$variable = paste0(c(1:nrow(xl)), "". "", xl$variable)\nggplot(data = xl[1:30,], aes(x = reorder(variable, relative_importance), y = relative_importance)) +\n  geom_bar(fill = ""steelblue"",\n           stat = ""identity"") +\n  coord_flip() +\n  labs(title = ""Variable importance"",\n       x = NULL,\n       y = NULL) +\n  theme_bw() + theme(text = element_text(size=10))\n\n## make RDS files ----\n#x = list.files(""./gb1km"", pattern=glob2rx(""*.rds$""), full.names = TRUE, recursive = TRUE)\n#unlink(x)\nyrs.lst = c(""static"", 2014:2021)\nmnt.lst = c(""01"", ""02"", ""03"", ""04"", ""05"", ""06"", ""07"", ""08"", ""09"", ""10"", ""11"", ""12"")\nfor(m.i in mnt.lst){\n  x = parallel::mclapply(yrs.lst, function(i){make_rds(year=i, month=m.i, covs.lst=covs.lst)}, mc.cores = length(yrs.lst)) \n}\n\n## predictions ----\nmask1km = rgdal::readGDAL(""./gb1km/static/elevation_glo90.copernicus_m_1km_s_20000101_20201231_gb_epsg.27700_v20230207.tif"")\nmask1km$gb1km = rgdal::readGDAL(""cats_1km_GB.tif"")$band1\n#summary(mask1km$gb1km)\n#eml.fs = readRDS.gz(""./output/mlr_eml.rds"")\nin.dir = ""/mnt/landmark/sdm_ticks_gb/gb1km/""\nstat.rds <- paste0(in.dir, ""static/data_1km_static.rds"")\n\n## binomial regression\nfor(year in 2014:2021){\n  for(month in mnt.lst){\n    out.tif = paste0(""./predP/monthly.tick.prob_savsnet."", tolower(format(as.Date(paste0(year, ""-"", month, ""-01"")), ""%b"")), ""_p_1km_s_"", year, month, ""01_"", gsub(""-"", """", lubridate::ymd(paste0(year, ""-"", month, ""-01"")) + months(1) -1 ), ""_gb_epsg.27700_v20230207.tif"")\n    if(!file.exists(out.tif)){\n      temp.rds <- paste0(in.dir, year, ""/data_1km_"", year, ""_M"", month, "".rds"")\n      gb1km = cbind(readRDS.gz(stat.rds), readRDS.gz(temp.rds))\n      gb1km$cosmonth = cos((as.numeric(month)/12)*(2*pi))\n      ## low 2.5% quantile density of cats (we are looking for background occurrence)\n      gb1km@data[,grep(""dogs.density"", names(gb1km))] = 5\n      gb1km@data[,grep(""cats.density"", names(gb1km))] = 5\n      #which(!eml.fs$features %in% names(gb1km))\n      #sel.comp = complete.cases(gb1km@data[,m.ticks$features])\n      sel.comp = complete.cases(gb1km@data[,eml.fs$features]) & !is.na(mask1km$gb1km)\n      #summary(sel.comp)\n      pred = predict(eml.fs, newdata=gb1km@data[sel.comp, eml.fs$features])\n      pred.sd = as.matrix(as.data.frame(mlr::getStackedBaseLearnerPredictions(eml.fs, newdata=gb1km@data[sel.comp, eml.fs$features])))\n      pred.sd = sqrt(matrixStats::rowSds(pred.sd, na.rm=TRUE)^2)\n      mask1km$pred = NULL; mask1km$pred.sd = NULL\n      mask1km@data[sel.comp, ""pred""] = pred$data$prob.1 * 100\n      mask1km@data[sel.comp, ""pred.sd""] = pred.sd * 100\n      rgdal::writeGDAL(mask1km[""pred""], out.tif, type=""Byte"", mvFlag=255, options=""COMPRESS=DEFLATE"")\n      rgdal::writeGDAL(mask1km[""pred.sd""], gsub(""_p_"", ""_md_"", out.tif), type=""Byte"", mvFlag=255, options=""COMPRESS=DEFLATE"")\n    }\n  }\n}\n\n## LTM averages ----\nfor(i in m.lst){\n  lst = list.files(""./predP"", pattern=glob2rx(paste0(""monthly.tick.prob_savsnet."", i, ""_p_1km_s_*_*_gb_epsg.27700_*.tif"")), full.names = TRUE)\n  out.tif = paste0(""./predP/monthly.tick.prob_savsnet."", i, ""_p_1km_s_2014_2021_gb_epsg.27700_v20230207.tif"")\n  lst.grd = as(raster::stack(lst), ""SpatialGridDataFrame"")\n  lst.grd$mean = rowMeans(lst.grd@data, na.rm = TRUE)\n  rgdal::writeGDAL(lst.grd[""mean""], out.tif, type=""Byte"", mvFlag=255, options=""COMPRESS=DEFLATE"")\n}\n\n## KML ----\nlibrary(plotKML)\nlibrary(spacetime)\nsetwd(""./kml/"")\nsmp.st <- STIDF(SpatialPoints(xyt.pnts@coords, proj4string = CRS(""EPSG:4326"")), time = xyt.pnts$Date, data = xyt.pnts@data)\n# write to a KML file:\nshape <- ""http://maps.google.com/mapfiles/kml/pal2/icon18.png""\nkml(smp.st, dtime = 24*3600, colour = species, shape = shape, labels = smp.st@data$species, kmz=TRUE)\n\nplot_layer = function(year, month, soc.leg, z.lim){\n  x = readGDAL(paste0(""/mnt/landmark/sdm_ticks_gb/predP/tick.prob.dog_m_"", year, ""_"", month, "".tif""))\n  names(x) = ""ticks.prob""\n  plotKML::kml_layer(x, colour_scale = soc.leg, z.lim=z.lim,\n                     png.width = gridparameters(x)[1,""cells.dim""]*2, \n                     png.height = gridparameters(x)[2,""cells.dim""]*2,\n                     raster_name = paste0(""pred_1km_ticks_dog_"", year, ""_"", month, "".png""), plot.legend = FALSE,\n                     TimeSpan.begin = as.Date(paste0(year, ""-"", month, ""-01"")), TimeSpan.end = as.Date(paste0(year, ""-"", as.numeric(month)+1, ""-01"")))\n}\nsoc.leg <- rev(viridis::magma(20))\nz.lim = c(0, 100)\nYearMonth = expand.grid(Year=2014:2021, Month=mnt.lst)\nYearMonth$Months = paste(YearMonth$Year, YearMonth$Month, sep=""-"")\nplotKML::kml_open(paste0(""pred_1km_ticks_dog.kml""))\nx = lapply(YearMonth$Months, function(i){plot_layer(year=strsplit(i, ""-"")[[1]][1], month=strsplit(i, ""-"")[[1]][2], soc.leg=rev(viridis::magma(20)), z.lim=c(0, 100))})\nplotKML::kml_close(""pred_1km_ticks_dog.kml"")\n', '## Functions\n\nstripym = function(i, type=""begin""){\n  require(lubridate)\n  if(length(grep(""CHELSA_pr"", basename(i)))>0){\n    ym = paste0(paste(strsplit(basename(i), ""_"")[[1]][4:3], collapse = ""-""), ""-01"")\n    f = ""month""\n  }\n  if(length(grep(""TerraClimate_"", basename(i)))>0){\n    ym = paste0(paste(gsub(""M"", """", strsplit(gsub("".tif"", """", basename(i)), ""_"")[[1]][3:4]), collapse = ""-""), ""-01"")\n    f = ""month""\n  }\n  if(length(grep(glob2rx(""clm_lst_mod11a2.nighttime_*_1km_s0..0cm_*..*_v1.2.tif""), basename(i)))>0){\n    ym = gsub(""\\\\."", ""-"", substr(strsplit(basename(i), ""_"")[[1]][7], 1, 10))\n    f = ""month""\n  }\n  if(length(grep(glob2rx(""clm_lst_mod11a2.*_*_1km_s0..0cm_????_v1.2.tif""), basename(i)))>0){\n    ym = paste0(strsplit(basename(i), ""_"")[[1]][7], ""-01-01"")\n    f = ""year""\n  }\n  if(length(grep(""lcv_human.footprint"", basename(i)))>0){\n    ym = paste0(strsplit(basename(i), ""_"")[[1]][6], ""-01-01"")\n    f = ""year""\n  }\n  if(length(grep(""globalcropland"", basename(i)))>0){\n    ym = paste0(strsplit(basename(i), ""_"")[[1]][7], ""-01-01"")\n    f = ""year""\n  }\n  if(length(grep(""lcv_landcover"", basename(i)))>0){\n    ym = paste0(strsplit(basename(i), ""_"")[[1]][7], ""-01-01"")\n    f = ""year""\n  }\n  if(length(grep(""tropospheric.n02_sentinel5p"", basename(i)))>0){\n    ym = paste0(substr(strsplit(basename(i), ""_"")[[1]][7], 1, 4), ""-01-01"")\n    f = ""year""\n  }\n  if(length(grep(""vis.nightlight_vcmslcfg.npp"", basename(i)))>0){\n    ym = paste0(substr(strsplit(basename(i), ""_"")[[1]][7], 1, 4), ""-01-01"")\n    f = ""year""\n  }\n  if(length(grep(""annual.evi_mod13q1.v061"", basename(i)))>0){\n    ym = paste0(substr(strsplit(basename(i), ""_"")[[1]][7], 1, 4), ""-01-01"")\n    f = ""year""\n  }\n  if(length(grep(glob2rx(""monthly.evi_mod13q1.v061.*_p50_1km_s_*.tif""), basename(i)))>0){\n    ym = paste0(paste0(substr(strsplit(basename(i), ""_"")[[1]][6], 1, 4), ""-"", substr(strsplit(basename(i), ""_"")[[1]][6], 5, 6)), ""-01"")\n    f = ""month""\n  }\n  if(type==""end"" & f==""month""){ \n    ym <- ymd(ym) %m+% months(1)\n  }\n  if(type==""end"" & f==""year""){ \n    ym <- ymd(ym) %m+% months(12)\n  }\n  paste(ym)\n}\n\n## Hyperparameter optimization for component learners of the ensemble model\ntune_learners <- function(data, formula, blocking, out.dir=""output/"", predict.type = ""prob"", SL.library=c(""classif.ranger"",""classif.xgboost"",""classif.glmnet""), parallel=""multicore"", rdesc = mlr::makeResampleDesc(""CV"", iters = 5L) , xg.skip = FALSE, num.trees = 85){\n  tv <- all.vars(formula)[1]\n  r.sel <- stats::complete.cases(data[,all.vars(formula)])\n  df.s = data[which(r.sel),all.vars(formula)]\n  l = sapply(df.s[,-c(1)], sd, na.rm = TRUE)\n  if(length(which(l == 0))>0){\n    x = which(l==0)+1\n    message(paste0(""The following covariates were removed (sd(x) = 0): "", paste(colnames(df.s)[x], collapse = "", ""), ""...""), immediate. = TRUE)\n    df.s = df.s[,-x]\n    pr.vars = colnames(df.s[-1])\n    formula <- stats::as.formula(paste(tv, "" ~"", paste(pr.vars, collapse=""+"")))\n  }\n  rm(l)\n  gc()\n  ## Set hyperparameter space\n  \n  ## ranger\n  min.mtry = round(sqrt(length(all.vars(formula)))/3)\n  max.mtry = ifelse(length(all.vars(formula))>10, length(all.vars(formula))-4, length(all.vars(formula))-1)\n  discrete_ps <- ParamHelpers::makeParamSet(ParamHelpers::makeDiscreteParam(""mtry"", values = unique(round(seq(min.mtry, max.mtry, length.out = 10)))))\n  ## xgboost\n  xg_model_Params = ParamHelpers::makeParamSet(\n    ParamHelpers::makeDiscreteParam(""nrounds"", value=c(10)),\n    ParamHelpers::makeDiscreteParam(""max_depth"", values=c(5,10)),\n    ParamHelpers::makeDiscreteParam(""eta"", values=seq(0.5,1, by = 0.25)),\n    ParamHelpers::makeDiscreteParam(""subsample"", value=c(1)),\n    ParamHelpers::makeDiscreteParam(""min_child_weight"", values=c(5,10)),\n    ParamHelpers::makeDiscreteParam(""colsample_bytree"", values=seq(0.5,1, by = 0.25))\n  )\n  \n  ## mlr parameters\n  ctrl = mlr::makeTuneControlGrid()\n  message(paste0(""Using learners: "", paste(SL.library, collapse = "", ""), ""...""), immediate. = TRUE)\n  tsk <- mlr::makeClassifTask(data = df.s, \n                              target = tv, \n                              positive = ""1"",\n                              blocking = blocking[which(r.sel)])\n  \n  out.rf = paste0(out.dir, ""mlr_rf_model.rds"")\n  if(!file.exists(out.rf)){\n    ## fine-tune mtry\n    message(""Running tuneParams for ranger... "", immediate. = TRUE)\n    parallelMap::parallelStartSocket(parallel::detectCores())\n    resR.lst <- mlr::tuneParams(mlr::makeLearner(""classif.ranger"", \n                                                 num.threads = round(parallel::detectCores()/length(discrete_ps$pars$mtry$values)), \n                                                 num.trees=num.trees,\n                                                 predict.type = predict.type), \n                                task = tsk, \n                                resampling = rdesc, \n                                par.set = discrete_ps, \n                                control = ctrl,\n                                measures = list(logloss, setAggregation(logloss, test.sd)))\n    if(resR.lst$x$mtry >= length(all.vars(formula))){\n      lrn.rf = mlr::makeLearner(""classif.ranger"", num.threads = parallel::detectCores(), num.trees=num.trees, importance=""impurity"", predict.type = predict.type)\n    } else {\n      lrn.rf = mlr::makeLearner(""classif.ranger"", num.threads = parallel::detectCores(), mtry=resR.lst$x$mtry, num.trees=num.trees, importance=""impurity"", predict.type = predict.type)\n    }\n    var.mod1 <- mlr::train(lrn.rf, task = tsk)\n    parallelMap::parallelStop()\n    saveRDS(var.mod1, out.rf)\n    gc()\n  } else {\n    var.mod1 = readRDS(out.rf)\n  }\n  out.x = paste0(out.dir, ""mlr_xgb_model.rds"")\n  if(!file.exists(out.x)){\n    ## fine-tune xgboost\n    lrn.xg = mlr::makeLearner(""classif.xgboost"", par.vals = list(objective =\'multi:softprob\'))\n    message(""Running tuneParams for xgboost... "", immediate. = TRUE)\n    parallelMap::parallelStartSocket(parallel::detectCores())\n    resX.lst = mlr::tuneParams(mlr::makeLearner(""classif.xgboost"", \n                                                predict.type = predict.type), \n                               task = tsk, resampling = rdesc, \n                               par.set = xg_model_Params, \n                               control = ctrl,\n                               measures = list(logloss, setAggregation(logloss, test.sd)))\n    lrn.xg = mlr::setHyperPars(lrn.xg, par.vals = resX.lst$x)\n    var.mod2 <- mlr::train(lrn.xg, task = tsk)\n    parallelMap::parallelStop()\n    saveRDS(var.mod2, out.x)\n    gc()\n  } else {\n    var.mod2 = readRDS(out.x)\n  }\n  out.glm = paste0(out.dir, ""mlr_glm_model.rds"")\n  if(!file.exists(out.glm)){\n    parallelMap::parallelStartSocket(parallel::detectCores())\n    var.mod3 = mlr::train(mlr::makeLearner(""classif.glmnet"", predict.type = predict.type), task = tsk)\n    saveRDS(var.mod3, out.glm)\n    parallelMap::parallelStop()\n  } else {\n    var.mod3 = readRDS(out.glm)\n  }  \n  finetuning = list(var.mod1, var.mod2, var.mod3, formula)\n  saveRDS(finetuning, paste0(out.dir, ""mlr_finetune.rds""))\n  return(finetuning)\n}\n\n## Train spacetime model for predicting species occurrences\ntrain_sp_eml <- function(data, tune_result, blocking, out.dir=""output/"", predict.type = ""prob"", SL.library=c(""classif.ranger"",""classif.xgboost"",""classif.glmnet""), super.learner = ""classif.logreg"", parallel=""multicore"", num.trees = 85, xyn = c(""easting"", ""northing""), method = ""stack.cv""){\n  \n  tv <- all.vars(tune_result[[4]][[2]])\n  r.sel <- stats::complete.cases(data[,all.vars(tune_result[[4]])])\n  df.s = data[which(r.sel),all.vars(tune_result[[4]])]\n  l = sapply(df.s[,-c(1)], sd, na.rm = TRUE)\n  blocking = blocking[which(r.sel)]\n  \n  ## get models\n  var.mod1 = tune_result[[1]]\n  var.mod2 = tune_result[[2]]\n  var.mod3 = tune_result[[3]]\n  \n  out.eml = paste0(out.dir, ""mlr_eml.rds"")\n  if(!file.exists(out.eml)){\n    ## fit the mlr model:\n    mlr::configureMlr()\n    if(parallel==""multicore""){\n      parallelMap::parallelStartSocket(parallel::detectCores())\n    }\n    message(paste0(""Using learners: "", paste(SL.library, collapse = "", ""), ""...""), immediate. = TRUE)\n    lrns <- lapply(SL.library, mlr::makeLearner)\n    message(""Fitting a spatial learner using \'mlr::makeClassifTask\'..."", immediate. = TRUE)\n    tsk <- mlr::makeClassifTask(data = df.s, \n                                target = tv, \n                                positive = ""1"",\n                                blocking = blocking)\n    if(any(SL.library %in% ""classif.xgboost"")){\n      lrns[[which(SL.library %in% ""classif.xgboost"")]] = mlr::makeLearner(""classif.xgboost"", par.vals = list(objective =\'multi:softprob\'))\n    }\n    \n    lrns <- lapply(lrns, mlr::setPredictType, ""prob"")\n    lrns[[1]] = mlr::setHyperPars(lrns[[1]], par.vals = mlr::getHyperPars(var.mod1$learner))\n    lrns[[2]] = mlr::setHyperPars(lrns[[2]], par.vals = mlr::getHyperPars(var.mod2$learner))\n    lrns[[3]] = mlr::setHyperPars(lrns[[3]], par.vals = list(s = min(var.mod3$learner.model$lambda)))\n    gc()\n    init.m <- mlr::makeStackedLearner(base.learners = lrns, \n                                      predict.type = predict.type, \n                                      method = method, \n                                      super.learner = super.learner, \n                                      resampling=mlr::makeResampleDesc(method = ""CV"", blocking.cv=TRUE))\n    m <- mlr::train(init.m, tsk)\n    if(parallel==""multicore""){\n      parallelMap::parallelStop()\n    }\n    saveRDS(m, out.eml)\n  } else {\n    m = readRDS.gz(out.eml)\n  }\n  return(m)\n}\n\n## spacetime overlay\nextract_st <- function(tif, df, date, date.tif.begin, date.tif.end, coords=c(""x"",""y""), crs, format.date=""%Y-%m-%d"", variable.name){\n  if(any(!coords %in% colnames(df))){\n    stop(paste(""Coordinate columns"", coords, ""could not be found""))\n  }\n  if(is.character(date) & length(date)==1 & date %in% colnames(df)){\n    date = as.Date(df[,date], format=format.date, origin=""1970-01-01"")\n  } else {\n    stop(paste(""Column name"", date, ""could not be found in the dataframe""))\n  }\n  if(missing(date.tif.end)){\n    date.tif.end = date.tif.begin\n  }\n  sel <- date <= as.Date(date.tif.end, format=format.date, origin=""1970-01-01"") & date >= as.Date(date.tif.begin, format=format.date, origin=""1970-01-01"")\n  if(sum(sel)>0){\n    pnts = as.matrix(df[sel, coords])\n    attr(pnts, ""dimnames"")[[2]] = c(""x"",""y"")\n    df.v = terra::vect(pnts, crs=crs)\n    if(file.exists(tif)){\n      ov = terra::extract(terra::rast(tif), df.v)\n    } else {\n      ov = matrix(nrow=length(df.v), ncol=2)\n    }\n    ov = as.data.frame(ov)\n    if(missing(variable.name)){\n      variable.name = tools::file_path_sans_ext(basename(tif))\n    }\n    names(ov) = c(""ID"", variable.name)\n    ov$row.id = which(sel)\n    ov$ID = NULL\n    return(ov)\n  }\n}\n\ntest_tif <- function(tif){\n  res <- try( x <- raster::raster(tif, values=FALSE), silent = TRUE)\n  if(class(res) == ""try-error""){\n    x = tif\n  } else {\n    x = NULL\n  }\n  return(x)\n}\n\n#require(plyr)\nplyrChunks <- function(d, n){\n  is <- seq(from = 1, to = length(d), by = ceiling(n))\n  if(tail(is, 1) != length(d)) {\n    is <- c(is, length(d))\n  }\n  chunks <- plyr::llply(head(seq_along(is), -1),\n                  function(i){\n                    start <-  is[i];\n                    end <- is[i+1]-1;\n                    d[start:end]})\n  lc <- length(chunks)\n  td <- tail(d, 1)\n  chunks[[lc]] <- c(chunks[[lc]], td)\n  return(chunks)\n}\n\n## Model fine-tuning wrapper\n## by default run spatial CV\ntrain_eml <- function(t.var, pr.var, X, out.dir=""./modelsT/"", SL.library = c(""regr.ranger"", ""regr.xgboost"", ""regr.cubist""), discrete_ps = makeParamSet(makeDiscreteParam(""mtry"", values = seq(5,45,by=5))), ctrl = mlr::makeTuneControlGrid(), rdesc = mlr::makeResampleDesc(""CV"", iters = 2L), outer = mlr::makeResampleDesc(""CV"", iters = 2L), inner = mlr::makeResampleDesc(""Holdout""), ctrlF = mlr::makeFeatSelControlRandom(maxit = 20), xg.model_Params, hzn_depth=FALSE, out.m.rds, save.rm=TRUE, rf.feature=TRUE, xg.size=2e4){\n  require(mlr)\n  ## ""regr.glmboost"",\n  ## https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n  if(missing(xg.model_Params)){\n    xg.model_Params <- makeParamSet(\n      #nrounds=50; max_depth=4; eta=0.2; subsample=1; min_child_weight=4; colsample_bytree=0.6\n      makeDiscreteParam(""nrounds"", value=c(10,20,50)),\n      makeDiscreteParam(""max_depth"", value=c(2,4,6)),\n      makeDiscreteParam(""eta"", value=c(0.2,0.3,0.4)),\n      makeDiscreteParam(""subsample"", value=c(1)),\n      makeDiscreteParam(""min_child_weight"", value=c(1,2)),\n      makeDiscreteParam(""colsample_bytree"", value=c(0.6))\n    )\n  }\n  ## regression matrix\n  rm.x = X[,c(t.var, pr.var)]\n  r.sel = complete.cases(rm.x)\n  rm.x = rm.x[r.sel,]\n  ## remove columns without enough variation:\n  c2.sd = sapply(rm.x, function(i){var(i, na.rm=TRUE)})\n  rm.pr = which(c2.sd < 0.01)\n  if(length(rm.pr)>0){\n    rm.x = rm.x[,-rm.pr]\n  }\n  if(save.rm==TRUE){ saveRDS.gz(rm.x, paste0(out.dir, ""rm."", t.var, "".rds"")) }\n  parallelMap::parallelStartSocket(parallel::detectCores())\n  tsk0 <- mlr::makeRegrTask(data = rm.x, target = t.var, coordinates = X[r.sel,c(""eastng"", ""nrthng"")], blocking = as.factor(X$ID[r.sel]))\n  out.t.mrf = paste0(out.dir, ""t.mrf."", t.var, "".rds"")\n  if(!file.exists(out.t.mrf)){\n    ## fine-tune mtry\n    resR.lst = tuneParams(mlr::makeLearner(""regr.ranger"", num.threads = round(parallel::detectCores()/length(discrete_ps$pars$mtry$values)), num.trees=85), task = tsk0, resampling = rdesc, par.set = discrete_ps, control = ctrl)\n    ## feature selection\n    lrn.rf = mlr::makeLearner(""regr.ranger"", num.threads = parallel::detectCores(), mtry=resR.lst$x$mtry, num.trees=85, importance=""impurity"")\n    if(rf.feature==TRUE){\n      lrn1 = mlr::makeFeatSelWrapper(lrn.rf, resampling = inner, control = ctrlF, show.info=TRUE)\n    } else {\n      lrn1 = lrn.rf\n    }\n    var.mod1 = mlr::train(lrn1, task = tsk0)\n    saveRDS.gz(var.mod1, out.t.mrf)\n  } else {\n    var.mod1 = readRDS.gz(out.t.mrf)\n  }\n  if(rf.feature==TRUE & any(class(var.mod1)==""FeatSelModel"")){\n    var.sfeats1 = mlr::getFeatSelResult(var.mod1)\n  } else {\n    var.sfeats1 = data.frame(x=pr.var[which(pr.var %in% names(rm.x))])\n  }\n  out.t.xgb = paste0(out.dir, ""t.xgb."",t.var,"".rds"")\n  if(!file.exists(out.t.xgb)){\n    ## fine-tune xgboost\n    sel.xg = sample.int(nrow(rm.x), size=xg.size)\n    tsk0s <- mlr::makeRegrTask(data = rm.x[sel.xg,], target = t.var, blocking = as.factor(X$ID[r.sel][sel.xg])) ## coordinates = X[r.sel,c(""X"", ""Y"")][sel.xg,]\n    resX.lst = mlr::tuneParams(mlr::makeLearner(""regr.xgboost""), task = tsk0s, resampling = rdesc, par.set = xg.model_Params, control = ctrl)\n    lrn.xg = mlr::makeLearner(""regr.xgboost"", par.vals = list(objective =\'reg:squarederror\'))\n    lrn.xg = mlr::setHyperPars(lrn.xg, par.vals = resX.lst$x)\n    lrn2 = mlr::makeFeatSelWrapper(lrn.xg, resampling = inner, control = ctrlF, show.info=TRUE)\n    var.mod2 = mlr::train(lrn2, task = tsk0s)\n    saveRDS.gz(var.mod2, out.t.xgb)\n  } else {\n    var.mod2 = readRDS.gz(out.t.xgb)\n  }\n  var.sfeats2 = mlr::getFeatSelResult(var.mod2)\n  ## new shorter formula\n  pr.x = union(var.sfeats1$x, var.sfeats2$x)\n  if(any(!pr.x %in% pr.var)){\n    pr.x = pr.x[-which(!pr.x %in% pr.var)]\n  }\n  ## we add depth otherwise not a 3D model\n  if(hzn_depth==TRUE){\n    formulaString.y = as.formula(paste(t.var, \' ~ \', paste(c(""hzn_depth"", pr.x), collapse=""+"")))\n  } else {\n    formulaString.y = as.formula(paste(t.var, \' ~ \', paste(pr.x, collapse=""+"")))\n  }\n  ## save formula:\n  #save.rds(formulaString.y, paste0(out.dir, ""fm.m_"", t.var,"".rds""))\n  ## remove all covariates without enough variation\n  #s.lst = sapply(all.vars(formulaString.y), function(i){sd(rm.x[,i], na.rm=TRUE)})\n  #all.vars(formulaString.y)[which(s.lst<5)]\n  ## final EML model\n  if(missing(out.m.rds)){ out.m.rds <- paste0(out.dir, ""eml.m_"", t.var,"".rds"") }\n  if(!file.exists(out.m.rds)){\n    tskF <- mlr::makeRegrTask(data = rm.x[,all.vars(formulaString.y)], target = t.var, blocking = as.factor(X$ID[r.sel])) ## coordinates = X[r.sel,c(""X"", ""Y"")],\n    var.mod1 <- readRDS.gz(paste0(out.dir, ""t.mrf."", t.var, "".rds""))\n    var.mod2 <- readRDS.gz(paste0(out.dir, ""t.xgb."", t.var, "".rds""))\n    lrn.rf = mlr::setHyperPars(mlr::makeLearner(SL.library[1]), par.vals = getHyperPars(var.mod1$learner))\n    lrn.xg = mlr::setHyperPars(mlr::makeLearner(SL.library[2]), par.vals = getHyperPars(var.mod2$learner))\n    lrnsE <- list(lrn.rf, lrn.xg, mlr::makeLearner(SL.library[3])) #, mlr::makeLearner(SL.library[4]))\n    init.m <- mlr::makeStackedLearner(base.learners = lrnsE, predict.type = ""response"", method = ""stack.cv"", super.learner = ""regr.lm"", resampling=makeResampleDesc(method = ""CV"", blocking.cv=TRUE))\n    t.m <- mlr::train(init.m, tskF)\n    #t.m$learner.model$super.model$learner.model\n    saveRDS.gz(t.m, out.m.rds)\n  }\n  parallelMap::parallelStop()\n}\n\ncat_eml = function(t.var, out.dir=""./modelsT/"", n.max=50){\n  r.file = paste0(out.dir, ""resultsFit_"", t.var, "".txt"")\n  if(!file.exists(r.file)){\n    out.m.rds = paste0(out.dir, ""eml.m_"", t.var,"".rds"")\n    t.m = readRDS.gz(out.m.rds)\n    x.s = summary(t.m$learner.model$super.model$learner.model)\n    cat(""Results of ensemble model fitting \'ranger\', \'xgboost\', ...:\\n"", file=r.file)\n    cat(""\\n"", file=r.file, append=TRUE)\n    cat(paste(""Variable:"", t.var, ""\\n""), file=r.file, append=TRUE)\n    cat(paste(""R-square:"", round(x.s$adj.r.squared, 3), ""\\n""), file=r.file, append=TRUE)\n    cat(paste(""Fitted values sd:"", signif(sd(t.m$learner.model$super.model$learner.model$fitted.values), 3), ""\\n""), file=r.file, append=TRUE)\n    cat(paste(""RMSE:"", signif(sqrt(sum(t.m$learner.model$super.model$learner.model$residuals^2) / t.m$learner.model$super.model$learner.model$df.residual), 3), ""\\n\\n""), file=r.file, append=TRUE)\n    sink(file=r.file, append=TRUE, type=""output"")\n    cat(""EML model summary:"", file=r.file, append=TRUE)\n    print(x.s)\n    #cat(""\\n"", file=r.file, append=TRUE)\n    #cat(""Meta learner model:"", file=r.file, append=TRUE)\n    #print(t.m$learner.model$super.model$learner.model)\n    cat(""\\n"", file=r.file, append=TRUE)\n    imp.rds = paste0(out.dir, ""t.mrf."", t.var, "".rds"")\n    if(file.exists(imp.rds)){\n      var.mod1 <- readRDS.gz(imp.rds)\n      cat(""Variable importance:\\n"", file=r.file, append=TRUE)\n      xl <- as.data.frame(mlr::getFeatureImportance(var.mod1)$res)\n      write.csv(xl[order(xl$importance, decreasing=TRUE),], paste0(out.dir, ""rf_varImportance_"", t.var, "".csv""))\n      print(xl[order(xl$importance, decreasing=TRUE),][1:n.max,])\n    }\n    sink()\n  }\n}\n\ncat_eml = function(t.var, out.dir=""./modelsT/"", n.max=50){\n  r.file = paste0(out.dir, ""resultsFit_"", t.var, "".txt"")\n  if(!file.exists(r.file)){\n    out.m.rds = paste0(out.dir, ""eml.m_"", t.var,"".rds"")\n    t.m = readRDS.gz(out.m.rds)\n    x.s = summary(t.m$learner.model$super.model$learner.model)\n    cat(""Results of ensemble model fitting \'ranger\', \'xgboost\', ...:\\n"", file=r.file)\n    cat(""\\n"", file=r.file, append=TRUE)\n    cat(paste(""Variable:"", t.var, ""\\n""), file=r.file, append=TRUE)\n    cat(paste(""R-square:"", round(x.s$adj.r.squared, 3), ""\\n""), file=r.file, append=TRUE)\n    cat(paste(""Fitted values sd:"", signif(sd(t.m$learner.model$super.model$learner.model$fitted.values), 3), ""\\n""), file=r.file, append=TRUE)\n    cat(paste(""RMSE:"", signif(sqrt(sum(t.m$learner.model$super.model$learner.model$residuals^2) / t.m$learner.model$super.model$learner.model$df.residual), 3), ""\\n\\n""), file=r.file, append=TRUE)\n    sink(file=r.file, append=TRUE, type=""output"")\n    cat(""EML model summary:"", file=r.file, append=TRUE)\n    print(x.s)\n    #cat(""\\n"", file=r.file, append=TRUE)\n    #cat(""Meta learner model:"", file=r.file, append=TRUE)\n    #print(t.m$learner.model$super.model$learner.model)\n    cat(""\\n"", file=r.file, append=TRUE)\n    imp.rds = paste0(out.dir, ""t.mrf."", t.var, "".rds"")\n    if(file.exists(imp.rds)){\n      var.mod1 <- readRDS.gz(imp.rds)\n      cat(""Variable importance:\\n"", file=r.file, append=TRUE)\n      xl <- as.data.frame(mlr::getFeatureImportance(var.mod1)$res)\n      write.csv(xl[order(xl$importance, decreasing=TRUE),], paste0(out.dir, ""rf_varImportance_"", t.var, "".csv""))\n      print(xl[order(xl$importance, decreasing=TRUE),][1:n.max,])\n    }\n    sink()\n  }\n}\n\n## confidence limits based on RMSE:\npfunL <- function(x,y, ...){\n  panel.hexbinplot(x,y, ...)\n  panel.abline(0,1,lty=1,lw=2,col=""black"")\n  panel.abline(0+m$Summary$logRMSE,1,lty=3,lw=2,col=""black"")\n  panel.abline(0-m$Summary$logRMSE,1,lty=3,lw=2,col=""black"")\n}\n\npfun <- function(x,y, ...){\n  panel.hexbinplot(x,y, ...)\n  panel.abline(0,1,lty=1,lw=2,col=""black"")\n}\n\nplot_hexbin <- function(varn, breaks, main, meas, pred, colorcut=c(0,0.01,0.03,0.07,0.15,0.25,0.5,0.75,1), pal=c(""#FFFF60FF"", ""#FFF609FF"", ""#FFDE21FF"", ""#FFC639FF"", ""#FFAE51FF"", ""#FF9669FF"", ""#FF7D82FF"", ""#FF659AFF"", ""#FF4DB2FF"", ""#DA35CAFF"", ""#B41DE2FF"", ""#8F05FAFF"", ""#6900FFFF"", ""#4300FFFF"", ""#1D00FFFF"", ""#0000F4FF""), in.file, log.plot, out.file){\n  #require(""hexbin""); require(""plotKML""); require(""latticeExtra"")\n  if(missing(out.file)){ out.file = paste0(""./outputs/plot_CV_"", varn, "".png"") }\n  if(!file.exists(out.file)){\n    if(missing(pred)){\n      m <- readRDS.gz(in.file)\n      #pred <- t.m$learner.model$super.model$learner.model$fitted.values\n      pred <- m$predictions\n    }\n    if(missing(meas)){\n      meas <- t.m$learner.model$super.model$learner.model$model[,1]\n    }\n    if(log.plot==TRUE){\n      R.squared = yardstick::ccc(data.frame(pred, meas), truth=""meas"", estimate=""pred"")\n      #pred <- 10^pred-1\n      pred = expm1(pred)\n      #meas <- 10^meas-1\n      meas = expm1(meas)\n      d.meas <- min(meas, na.rm=TRUE)\n    } else {\n      d.meas <- min(meas, na.rm=TRUE)\n      R.squared = yardstick::ccc(data.frame(pred, meas), truth=""meas"", estimate=""pred"")\n    }\n    main.txt = paste0(main, ""  (CCC: "", signif(R.squared$.estimate, 3), "")"")\n    png(file = out.file, res = 150, width=850, height=850, type=""cairo"")\n    if(log.plot==TRUE){\n      pred <- pred+ifelse(d.meas==0, 1, d.meas)\n      meas <- meas+ifelse(d.meas==0, 1, d.meas)\n      lim <- range(breaks)+ifelse(d.meas==0, 1, d.meas)\n      meas <- ifelse(meas<lim[1], lim[1], ifelse(meas>lim[2], lim[2], meas))\n      plt <- hexbinplot(meas~pred, colramp=colorRampPalette(pal), main=main.txt, ylab=""measured"", xlab=""predicted"", type=""g"", lwd=1, lcex=8, inner=.2, cex.labels=.8, scales=list(x = list(log = 2, equispaced.log = FALSE), y = list(log = 2, equispaced.log = FALSE)), asp=1, xbins=30, ybins=30, xlim=lim, ylim=lim, panel=pfun, colorcut=colorcut)\n    } else {\n      lim <- range(breaks)\n      meas <- ifelse(meas<lim[1], lim[1], ifelse(meas>lim[2], lim[2], meas))\n      plt <- hexbinplot(meas~pred, colramp=colorRampPalette(pal), main=main.txt, ylab=""measured"", xlab=""predicted"", type=""g"", lwd=1, lcex=8, inner=.2, cex.labels=.8, xlim=lim, ylim=lim, asp=1, xbins=30, ybins=30, panel=pfun, colorcut=colorcut)\n    }\n    print(plt)\n    dev.off()\n  }\n}\n\nsaveRDS.gz <- function(object,file,threads=parallel::detectCores()) {\n  con <- pipe(paste0(""pigz -p"",threads,"" > "",file),""wb"")\n  saveRDS(object, file = con)\n  close(con)\n}\n\nreadRDS.gz <- function(file,threads=parallel::detectCores()) {\n  con <- pipe(paste0(""pigz -d -c -p"",threads,"" "",file))\n  object <- readRDS(file = con)\n  close(con)\n  return(object)\n}\n\nhor2xyd <- function(x, U=""UHDICM"", L=""LHDICM"", treshold.T=15){\n  x$DEPTH <- x[,U] + (x[,L] - x[,U])/2\n  x$THICK <- x[,L] - x[,U]\n  sel <- x$THICK < treshold.T\n  ## begin and end of the horizon:\n  x1 <- x[!sel,]; x1$DEPTH = x1[,L]\n  x2 <- x[!sel,]; x2$DEPTH = x1[,U]\n  y <- do.call(rbind, list(x, x1, x2))\n  return(y)\n}\n\nmake_mask = function(id, tile.tbl, out.rds, mask=\'/mnt/landmark/land1km/layers1km/static/lcv_landmask_esacci.lc.l4_c_1km_s0..0cm_2000..2015_v1.0.tif\', in.dir=""/mnt/landmark/land1km/tt/""){\n  if(missing(out.rds)){\n    out.rds = paste0(in.dir, ""T"", id, ""/data_mask.rds"")\n  }\n  if(!file.exists(out.rds)){\n    g1km = rgdal::readGDAL(mask, silent = TRUE,\n                           offset=unlist(tile.tbl[id,c(""offset.y"",""offset.x"")]),\n                           region.dim=unlist(tile.tbl[id,c(""region.dim.y"",""region.dim.x"")]),\n                           output.dim=unlist(tile.tbl[id,c(""region.dim.y"",""region.dim.x"")]))\n    ## remove water bodies:\n    g1km$band1 = ifelse(g1km$band1==2, NA, g1km$band1)\n    g1km = as(g1km, ""SpatialPixelsDataFrame"")\n    names(g1km) = ""mask""\n    saveRDS(g1km, out.rds)\n  }\n}\n\n## period = c(""static"", 2000:2019)\nmake_rds = function(year, month=""01"", covs.lst, in.dir=""/mnt/landmark/sdm_ticks_gb/gb1km/""){\n  out.rds <- paste0(in.dir, year, ""/data_1km_"", year, ifelse(year==""static"", """", paste0(""_M"", month)), "".rds"")\n  if(!file.exists(out.rds)){\n    tifs = list.files(paste0(in.dir, year), glob2rx(""*.tif$""), full.names = TRUE)\n    if(!year==""static""){\n      begin.tif = format(lubridate::ymd(sapply(basename(tifs), function(i){strsplit(i, ""_"")[[1]][6]})), ""%m"")\n      tifs = tifs[begin.tif==month | !grepl(""monthly"", basename(tifs))]\n      vn.lst = plyr::join(data.frame(tif=basename(tifs)), covs.lst, match=""first"")$var.name\n    } else {\n      vn.lst = tools::file_path_sans_ext(basename(tifs))\n    }\n    g1km = raster::stack(tifs)\n    g1km = as(g1km, ""SpatialGridDataFrame"")\n    names(g1km) = make.names(vn.lst)\n    saveRDS.gz(g1km, out.rds)\n  }\n}\n\nfilter_na = function(m){\n  if(any(is.na(m@data[,1]))){\n    r = raster::raster(m)\n    ## first using proximity filter:\n    rf = raster::focal(r, w=matrix(1,15,15), fun=mean, na.rm=TRUE, NAonly=TRUE)\n    if(class(m)==""SpatialPixelsDataFrame""){\n      repn = as(rf, ""SpatialGridDataFrame"")@data[m@grid.index,1]\n    } else {\n      repn = as(rf, ""SpatialGridDataFrame"")@data[,1]\n    }\n    ## second using dominant value:\n    repn = ifelse(is.na(repn), quantile(repn, probs=.5, na.rm=TRUE), repn)\n    return(repn)\n  } else {\n    return(m@data[,1])\n  }\n}\n\npred_lst <- function(tvar, id, yl, t.m, eml.cf, in.dir=""./tt/""){\n  if(missing(t.m)){\n    t.m <- readRDS.gz(paste0(""./modelsT/eml.m_"", tvar, "".rds""))\n    m.train = t.m$learner.model$super.model$learner.model$model\n    m.terms = all.vars(t.m$learner.model$super.model$learner.model$terms)\n    eml.MSE0 = matrixStats::rowSds(as.matrix(m.train[,m.terms[-1]]), na.rm=TRUE)^2\n    eml.MSE = deviance(t.m$learner.model$super.model$learner.model)/df.residual(t.m$learner.model$super.model$learner.model)\n    ## correction factor:\n    eml.cf = eml.MSE/mean(eml.MSE0, na.rm = TRUE)\n  }\n  for(year in yl){\n    pred_mlr(tvar=tvar, id=id, year=year, t.m, eml.cf=eml.cf, in.dir=in.dir)\n  }\n}\n\ntest_rds <- function(id, year, in.dir=""/mnt/landmark/land1km/tt/""){\n  g1km = cbind(readRDS(paste0(in.dir, ""T"", id, ""/data_static.rds"")),\n                   readRDS(paste0(in.dir, ""T"",  id, ""/data_"", year, "".rds"")))\n  sel.mis = sapply(g1km@data, function(x){sum(is.na(x))})\n  if(any(sel.mis>0)){\n    for(i in which(sel.mis>0)){\n      x = ifelse(sum(is.na(g1km@data[,i]))==nrow(g1km), TRUE, FALSE)\n      return(x)\n    }\n  }\n  rm(g1km)\n  return(x <- NA)\n}\n\npred_mlr <- function(tvar, id, year, t.m, eml.cf=1, in.dir=""/mnt/landmark/land1km/tt/"", out.dir=""/mnt/landmark/land1km/tt/"", multiplier=10, min.md=1, s3=FALSE, depths = c(0, 30, 60, 100)){\n  out.tif = paste0(out.dir, ""T"", id, ""/"", tvar, ""_M_"", depths, ""cm_"", year, ""_1km_T"", id, "".tif"")\n  if(any(!file.exists(out.tif))){\n    retry::wait_until( memuse::Sys.meminfo()[[2]]@size > 10 , timeout = 3600)\n    if(s3==TRUE){\n      g1km = cbind( minio.s3::s3readRDS(bucket = \'tmp\', object = file.path(paste0(\'land1km/tt/T\', id, \'/data_static.rds\')), use_https = FALSE),\n                    minio.s3::s3readRDS(bucket = \'tmp\', object = file.path(paste0(\'land1km/tt/T\', id, \'/data_\', year, \'.rds\')), use_https = FALSE))\n    } else {\n      g1km = cbind(readRDS(paste0(in.dir, ""T"", id, ""/data_static.rds"")),\n                   readRDS(paste0(in.dir, ""T"",  id, ""/data_"", year, "".rds"")))\n    }\n    g1km$hzn_depth = 0\n    g1kx = g1km[""hzn_depth""]\n    #x.lst = sapply(names(g1km@data), function(i){sum(is.na(g1km@data[,i]))})\n    #str(g1km@data[,names(g1km@data)[which(x.lst>10000)]])\n    x = t.m$features[which(!t.m$features %in% names(g1km))]\n    g1km = do.call(data.frame, lapply(g1km@data[,t.m$features], function(x) replace(x, is.infinite(x) | is.nan(x), NA)))\n    #gc()\n    cc = complete.cases(g1km)\n    g1kx = g1kx[cc,""hzn_depth""]\n    if(sum(cc)>0){\n      retry::wait_until( memuse::Sys.meminfo()[[2]]@size > 6 , timeout = 3600)\n      if(tvar==""clay_tot_psa"" | tvar==""sand_tot_psa""){\n        multiplier = 1\n        #g1kx$pred = expm1(pred$data$response)\n      }\n      if(tvar==""db_od""){\n        multiplier = 100\n        #g1kx$pred = expm1(pred$data$response)\n      }\n      for(d in 1:length(depths)){\n        g1km$hzn_depth = depths[d]\n        pred = predict(t.m, newdata=g1km[cc,])\n        out.p <- as.matrix(as.data.frame(mlr::getStackedBaseLearnerPredictions(t.m, newdata=g1km[cc,]))) * multiplier\n        g1kx$model.error <- sqrt(matrixStats::rowSds(out.p, na.rm=TRUE)^2 * eml.cf)\n        #rm(out.p)\n        #gc()\n        g1kx$model.error <- ifelse(g1kx$model.error<min.md, min.md, g1kx$model.error)\n        g1kx$pred = ifelse(pred$data$response < 0, 0, pred$data$response)*multiplier\n        rgdal::writeGDAL(g1kx[""pred""], out.tif[d], type=""Byte"", mvFlag=255, options=""COMPRESS=DEFLATE"")\n        rgdal::writeGDAL(g1kx[""model.error""], gsub(""_M_"", ""_md_"", out.tif[d]), type=""Byte"", mvFlag=255, options=""COMPRESS=DEFLATE"")\n      }\n    }\n  }\n}\n\npred_mlr_tile <- function(id, year, in.dir=""/mnt/landmark/land1km/tt/"", out.dir=""/mnt/landmark/land1km/tt/"", multiplier=10, min.md=1, s3=TRUE, depths=c(0, 30, 60, 100), tv=c(""log.oc"", ""ph_h2o"", ""db_od"", ""log.n_tot""), mc.cores=parallel::detectCores()){\n  x = sapply(tv, function(i){paste0(out.dir, id, ""/"", i, ""_M_"", depths, ""cm_"", year, ""_1km_T"", id, "".tif"")})\n  if(any(!file.exists(x))){\n    retry::wait_until( memuse::Sys.meminfo()[[2]]@size > 10 , timeout = 3600)\n    if(s3==TRUE){\n      g1km = cbind( minio.s3::s3readRDS(bucket = \'tmp\', object = file.path(\'tree-species/1km\', id , paste0(\'tile_\', id, \'_1km_static.rds\')), use_https = FALSE),\n                    minio.s3::s3readRDS(bucket = \'tmp\', object = file.path(\'tree-species/1km\', id, paste0(\'tile_\', id, \'_1km_\', year, \'.rds\')), use_https = FALSE))\n    } else {\n      g1km = cbind(readRDS.gz(paste0(in.dir, ""T"", id, ""/data_static.rds"")),\n                   readRDS.gz(paste0(in.dir, ""T"",  id, ""/data_"", year, "".rds"")))\n    }\n    g1km$hzn_depth = 0\n    g1kx = g1km[""hzn_depth""]\n    #x.lst = sapply(names(g1km@data), function(i){sum(is.na(g1km@data[,i]))})\n    #str(g1km@data[,names(g1km@data)[which(x.lst>10000)]])\n    #x = t.m$features[which(!t.m$features %in% names(g1km))]\n    for(tvar in tv){\n      out.tif = paste0(out.dir, id, ""/"", tvar, ""_M_"", depths, ""cm_"", year, ""_1km_T"", id, "".tif"")\n      if(any(!file.exists(out.tif))){\n        t.m <- readRDS.gz(paste0(""./modelsT/eml.m_"", tvar, "".rds""))\n        m.train = t.m$learner.model$super.model$learner.model$model\n        m.terms = all.vars(t.m$learner.model$super.model$learner.model$terms)\n        eml.MSE0 = matrixStats::rowSds(as.matrix(m.train[,m.terms[-1]]), na.rm=TRUE)^2\n        eml.MSE = deviance(t.m$learner.model$super.model$learner.model)/df.residual(t.m$learner.model$super.model$learner.model)\n        ## correction factor:\n        eml.cf = eml.MSE/mean(eml.MSE0, na.rm = TRUE)\n        newdata = do.call(data.frame, lapply(g1km@data[,t.m$features], function(x){ replace(x, is.infinite(x) | is.nan(x), NA) }))\n        #gc()\n        cc = complete.cases(newdata)\n        g30.pix = g1kx[cc,""hzn_depth""]\n        if(sum(cc)>0){\n          itr = plyrChunks(which(cc), n=round(length(cc)/mc.cores))\n          #registerDoMC(cores=parallel::detectCores())\n          for(d in 1:length(depths)){\n            newdata$hzn_depth = depths[d]\n            if(tvar==""clay_tot_psa"" | tvar==""sand_tot_psa""){\n              multiplier = 1\n              #g1kx$pred = expm1(pred$data$response)\n            }\n            if(tvar==""db_od""){\n              multiplier = 100\n              #g1kx$pred = expm1(pred$data$response)\n            }\n            #system.time( pred <- foreach(i=itr, .combine=c, .packages=c(""mlr""), .export=c(""t.m""))  %dopar% { predict(t.m, newdata=g1km[i,])$data$response } )\n            pred <- unlist(parallel::mclapply(itr, function(i){ predict(t.m, newdata=newdata[i,])$data$response }, mc.cores = mc.cores))\n            #out.p <- foreach(i=itr, .combine=rbind, .packages=c(""mlr""), .export=c(""t.m"")) %dopar% { as.matrix(as.data.frame(mlr::getStackedBaseLearnerPredictions(t.m, newdata=g1km[i,])))  }\n            out.p <- do.call(rbind, parallel::mclapply(itr, function(i){ as.matrix(as.data.frame(mlr::getStackedBaseLearnerPredictions(t.m, newdata=newdata[i,]))) }, mc.cores = mc.cores))\n            g30.pix$model.error <- sqrt(matrixStats::rowSds(out.p * multiplier, na.rm=TRUE)^2 * eml.cf)\n            g30.pix$model.error <- ifelse(g30.pix$model.error<min.md, min.md, g30.pix$model.error)\n            g1km.pix$pred = ifelse(pred < 0, 0, pred)*multiplier\n            rgdal::writeGDAL(g1km.pix[""pred""], out.tif[d], type=""Byte"", mvFlag=255, options=""COMPRESS=DEFLATE"")\n            rgdal::writeGDAL(g1km.pix[""model.error""], gsub(""_M_"", ""_md_"", out.tif[d]), type=""Byte"", mvFlag=255, options=""COMPRESS=DEFLATE"")\n          }\n          #parallel::stopCluster(cl)\n        }\n      }\n    }\n  }\n}\n']","Predicted occurrence probability for ticks in Great Britain (2014 to 2021) at 1 km spatial resolution The dataset contains predictions of occurrence probability for ticks in Great Britain (2014 to 2021) at 1 km spatial resolution + all covariate layers used for modeling. Over seven million electronic health records (EHRs), among which 11,741 EHRs reported tick attachment, were used to evaluate climate, environmental and animal host factors affecting the risk of tick attachment in cats and dogs in Great Britain (GB). The tick presence/absence EHRs for dogs and cats were further overlaid with spatiotemporal time-series of climatic, vegetation, human influence, hydrological and terrain variables (slope, wetness index) to produce a spatiotemporal regression matrix; an Ensemble Machine Learning framework was used to fine-tune hyperparameters for Random Forest (classif.ranger), Gradient boosting (classif.xgboost) and GLM-net (classif.glmnet) algorithms, which were then used to produce a final ensemble meta-learner that predicts the probability of occurrence of ticks across GB with monthly intervals.gb1km_covariates.zip contains ALL covariate layers as GeoTIFFs (time-series) used for modeling ticks dynamics;data_1km_2014_M01.rds = contains all covariates for January 2014 prepared as SpatialGridDataFrame (R data object);Codes of files indicate e.g.:""monthly.tick.prob_savsnet.mar_p_1km_s_2014_2021"" = monthly occurrence probability for January based on the training data from 2014 to 2021;""monthly.tick.prob_savsnet.oct_md_1km_s_20211001_20211031"" = monthly prediction (model) error derived as the standard deviation from multiple base learners;The dataset is described in detail in the following publication:Arsevska, E., Hengl, T., Singelton, D. et al. (2023?) Risk factors for tick attachment in companion animals in Great Britain: a spatiotemporal analysis covering 20142021. Submitted to Parasites & Vectors (in review).Acknowledgements: We are grateful to data providers in veterinary practice (VetSolutions, Teleos, CVS, and other practitioners). We are grateful to the INRAE MIGALE bioinformatics facility (MIGALE, INRAE, 2020. Migale Bioinformatics Facility, doi: 10.15454/1.5572390655343293E12) for providing computing resources. We are also grateful forthe help and support provided by SAVSNET team members Bethaney Brant, Susan Bolan and Steven Smyth.This study was funded mainly by a grant from the Biotechnology and Biological Sciences Research Council,BB/NO19547/1 and British Small Animal Veterinary Association (BSAVA). The research was partly funded by the National Institute for Health Research Health Protection Research Unit (NIHR HPRU) in Emerging and Zoonotic Infections at the University of Liverpool in partnership with Public Health England (PHE) and Liverpool School of Tropical Medicine (LSTM). This work has been partially funded by the Monitoring outbreak events for disease surveillance in a data science context"" (MOOD) project from the European Unions Horizon 2020 research and innovation program under grant agreement No. 874850 (https://mood-h2020.eu/). The views expressed are those of the authors and not necessarily those of the NHS, the NIHR, the Department of Health or Public Health England.",0
Data from: Effects of sampling effort on biodiversity patterns estimated from environmental DNA metabarcoding surveys,"Environmental DNA (eDNA) metabarcoding can greatly enhance our understanding of global biodiversity and our ability to detect rare or cryptic species. However, sampling effort must be considered when interpreting results from these surveys. We explored how sampling effort influenced biodiversity patterns and nonindigenous species (NIS) detection in an eDNA metabarcoding survey of four commercial ports. Overall, we captured sequences from 18 metazoan phyla with minimal differences in taxonomic coverage between 18 S and COI primer sets. While community dissimilarity patterns were consistent across primers and sampling effort, richness patterns were not, suggesting that richness estimates are extremely sensitive to primer choice and sampling effort. The survey detected 64 potential NIS, with COI identifying more known NIS from port checklists but 18 S identifying more operational taxonomic units shared between three or more ports that represent un-recorded potential NIS. Overall, we conclude that eDNA metabarcoding surveys can reveal global similarity patterns among ports across a broad array of taxa and can also detect potential NIS in these key habitats. However, richness estimates and species assignments require caution. Based on results of this study, we make several recommendations for port eDNA sampling design and suggest several areas for future research.",,"Data from: Effects of sampling effort on biodiversity patterns estimated from environmental DNA metabarcoding surveys Environmental DNA (eDNA) metabarcoding can greatly enhance our understanding of global biodiversity and our ability to detect rare or cryptic species. However, sampling effort must be considered when interpreting results from these surveys. We explored how sampling effort influenced biodiversity patterns and nonindigenous species (NIS) detection in an eDNA metabarcoding survey of four commercial ports. Overall, we captured sequences from 18 metazoan phyla with minimal differences in taxonomic coverage between 18 S and COI primer sets. While community dissimilarity patterns were consistent across primers and sampling effort, richness patterns were not, suggesting that richness estimates are extremely sensitive to primer choice and sampling effort. The survey detected 64 potential NIS, with COI identifying more known NIS from port checklists but 18 S identifying more operational taxonomic units shared between three or more ports that represent un-recorded potential NIS. Overall, we conclude that eDNA metabarcoding surveys can reveal global similarity patterns among ports across a broad array of taxa and can also detect potential NIS in these key habitats. However, richness estimates and species assignments require caution. Based on results of this study, we make several recommendations for port eDNA sampling design and suggest several areas for future research.",0
ConstellationMap (v1.1),"ConstellationMap is a GenePattern module (http://www.genepattern.org/) that helps leverage the full power of a gene set enrichment analysis by identifying commonalities between high-scoring gene sets and mapping their relationships. ConstellationMap visualizes in an interactive web application the enrichment profile similarity and gene member overlap of gene sets, which are positively or negatively enriched in relation to a phenotype in a two-class or continuous-class comparison. It uses provided enrichment scores to estimate normalized mutual information (NMI) scores and project top scoring sets onto a circular plot with the following features:(1) Gene sets are represented as nodes with radial distance to the center reflecting positive or negative enrichment in the phenotype, whichever direction is specified. Higher associations for the correlation direction are plotted closer to the center.(2) Member gene overlaps between sets are represented as edges connecting nodes with thickness proportional to degree of overlap.(3) The angular distance between nodes is relatively proportional to the similarity of their enrichment profiles, i.e. more similar enrichment patterns have a smaller angular distance.Web interactive features allow export of selected overlapping gene symbols for annotation in DAVID, MSigDB, and GeneMania. Click on the output file Visualizer.html to open the interactive ConstellationMap plot from the Jobs Tab.Use mouse click on the web visualization to select a node or an edge. Use shift+click, or click-drag to select a group of features.Selection displays gene set names, number of genes in the set, number of sets used to compute overlap, and overlapping member gene symbols.Adjust number of nodes in highlighted area for which to display the maximum number of overlapping gene symbols.In addition to the web application, ConstellationMap outputs two static plots: (1) a positive red to negative blue heat map of per sample enrichment of the gene sets ranked by NMI scores as well as Area Under the Curve (AUC) and t-test metrics along with corresponding p-values, and (2) a constellation map marking the phenotype of interest in the center in red, concentric contour arcs marking association to the phenotype (as measured by NMI), nodes as numbered open circles, overlap as green lines, and a key of the numbered gene sets.",,"ConstellationMap (v1.1) ConstellationMap is a GenePattern module (http://www.genepattern.org/) that helps leverage the full power of a gene set enrichment analysis by identifying commonalities between high-scoring gene sets and mapping their relationships. ConstellationMap visualizes in an interactive web application the enrichment profile similarity and gene member overlap of gene sets, which are positively or negatively enriched in relation to a phenotype in a two-class or continuous-class comparison. It uses provided enrichment scores to estimate normalized mutual information (NMI) scores and project top scoring sets onto a circular plot with the following features:(1) Gene sets are represented as nodes with radial distance to the center reflecting positive or negative enrichment in the phenotype, whichever direction is specified. Higher associations for the correlation direction are plotted closer to the center.(2) Member gene overlaps between sets are represented as edges connecting nodes with thickness proportional to degree of overlap.(3) The angular distance between nodes is relatively proportional to the similarity of their enrichment profiles, i.e. more similar enrichment patterns have a smaller angular distance.Web interactive features allow export of selected overlapping gene symbols for annotation in DAVID, MSigDB, and GeneMania. Click on the output file Visualizer.html to open the interactive ConstellationMap plot from the Jobs Tab.Use mouse click on the web visualization to select a node or an edge. Use shift+click, or click-drag to select a group of features.Selection displays gene set names, number of genes in the set, number of sets used to compute overlap, and overlapping member gene symbols.Adjust number of nodes in highlighted area for which to display the maximum number of overlapping gene symbols.In addition to the web application, ConstellationMap outputs two static plots: (1) a positive red to negative blue heat map of per sample enrichment of the gene sets ranked by NMI scores as well as Area Under the Curve (AUC) and t-test metrics along with corresponding p-values, and (2) a constellation map marking the phenotype of interest in the center in red, concentric contour arcs marking association to the phenotype (as measured by NMI), nodes as numbered open circles, overlap as green lines, and a key of the numbered gene sets.",0
Data from: Quantifying (non)parallelism of microbial community change using multivariate vector analysis,"Parallel evolution of phenotypic traits is regarded as strong evidence for natural selection and has been studied extensively in a variety of taxa. However, we have limited knowledge of whether parallel evolution of host organisms is accompanied by parallel changes of their associated microbial communities (i.e., microbiotas), which are crucial for their hosts' ecology and evolution. Determining the extent of microbiota parallelism in nature can improve our ability to identify the factors that are associated with (putatively adaptive) shifts in microbial communities. While it has been emphasized that (non)parallel evolution is better considered as a quantitative continuum rather than a binary phenomenon, quantitative approaches have rarely been used to study microbiota parallelism. We advocate using multivariate vector analysis (i.e., phenotypic change vector analysis) to quantify direction and magnitude of microbiota changes and discuss the applicability of this approach for studying parallelism. We exemplify its use by reanalyzing gut microbiota data from multiple fish species that exhibit parallel shifts in trophic ecology. This approach provides an analytical framework for quantitative comparisons across host lineages, thereby providing the potential to advance our capacity to predict microbiota changes. Hence, we encourage the development and application of quantitative measures, such as multivariate vector analysis, to better understand the role of microbiota dynamics during their hosts' adaptive evolution, particularly in settings of parallel evolution.",,"Data from: Quantifying (non)parallelism of microbial community change using multivariate vector analysis Parallel evolution of phenotypic traits is regarded as strong evidence for natural selection and has been studied extensively in a variety of taxa. However, we have limited knowledge of whether parallel evolution of host organisms is accompanied by parallel changes of their associated microbial communities (i.e., microbiotas), which are crucial for their hosts' ecology and evolution. Determining the extent of microbiota parallelism in nature can improve our ability to identify the factors that are associated with (putatively adaptive) shifts in microbial communities. While it has been emphasized that (non)parallel evolution is better considered as a quantitative continuum rather than a binary phenomenon, quantitative approaches have rarely been used to study microbiota parallelism. We advocate using multivariate vector analysis (i.e., phenotypic change vector analysis) to quantify direction and magnitude of microbiota changes and discuss the applicability of this approach for studying parallelism. We exemplify its use by reanalyzing gut microbiota data from multiple fish species that exhibit parallel shifts in trophic ecology. This approach provides an analytical framework for quantitative comparisons across host lineages, thereby providing the potential to advance our capacity to predict microbiota changes. Hence, we encourage the development and application of quantitative measures, such as multivariate vector analysis, to better understand the role of microbiota dynamics during their hosts' adaptive evolution, particularly in settings of parallel evolution.",0
The limits of demographic buffering in coping with environmental variation,"Animal populations have developed multiple strategies to deal with environmental change. Among them, the demographic buffering strategy consists in constraining the temporal variation of the vital rate(s) that most affect(s) the overall performance of the population. Tortoises are known to buffer their temporal variation in adult survival, which typically has the highest contribution to the population growth rate , at the expense of a high variability on reproductive rates, which contribute far less to . To identify the effects of projected increases in droughts in its natural habitat, we use field data collected across 15 locations of Testudo graeca in Southeast Spain over a decade. We analyse the effects of environmental variables on reproduction rates. In addition, we couple the demographic and environmental data to parameterise an integral projection model to simulate the effects of different scenarios of drought recurrence on  under different degrees of intensity in the survival-reproduction trade-off. We find that droughts negatively affect the probability of laying eggs; however, the overall effects on  under the current drought recurrence (one/decade) are negligible when survival is constant (independent of the reduction of reproduction by drought events) and when survival increased as a trade-off with the reduction of reproduction rates, with a threshold to population viability at three or more droughts/decade. Additionally, we show that, although some species may buffer current environmental regimes by carefully orchestrating how their vital rates vary through time, a demographic buffering strategy is insufficient to ensure population viability in extreme regimes. Our findings support the hypothesis that the demographic buffering strategy has a limit of effectiveness when adverse conditions occur frequently. Our methodological approach provides a framework for ecologists to determine how effective the management of environmental drivers can be for demographically buffering populations, and which scenarios may not provide long-term population persistence.","['####### Load Packages\r\n\r\n\r\nlibrary(popdemo)\r\nlibrary(fields)\r\nlibrary(lme4)\r\nlibrary(popbio)\r\nlibrary(IPMpack)\r\nlibrary(truncnorm)\r\n\r\n###### Load data  \r\n\r\nparams<- read.table(""params.txt"", sep=""\\t"", header=T)\r\n\r\n\r\n# 1. Modelling the life history\r\n# Each of the functions below represents one or more of the vital rates. These functions are used to build the IPM and use output (the coefficients) from the regressions developed previously. \r\n# These functions represent the modeler\'s decision about how to decompose the life cycle. We model (1) survival, (2) growth, (3) reproduction.\r\n## Vital rate functions\r\n# 1. Probability of surviving\r\ns.x=function(x,params) {\r\n  u=exp(params$surv.int+params$surv.slope*x)\r\n  return(u/(1+u))\r\n}\r\n# 2. Growth function\r\ng.yx=function(xp,x,params) { \r\n  mean=params$growth.int+params$growth.slope*x\r\n  sd=params$growth.sd\t\t\r\n  dnorm(xp,mean=mean,sd=sd)\r\n}\r\n# 3. Reproduction (probability of reproduction (fec1) described by intercept, the effect of rainfall and the linear and cuadritic relation with size; fec 2 is number of clutches per year;\r\n####### fec 3 is the number of eggs per clutch related to size; fec 4 is the probability of hatching; fec 5 is the probability of surviving to the one year)  \r\nf.yx=function(xp,x,params, Rain) { \t\t\r\n  exp(params$fec.int + params$fec.prec*Rain + params$fec.slope*x + params$fec.slope2*x^2)/(1+exp(params$fec.int+params$fec.prec*Rain+params$fec.slope*x + params$fec.slope2*x^2))*\r\n    params$fec2* exp(params$fec3.int+params$fec3.slope*x)* params$fec4 * params$fec5*\r\n    dnorm(xp,mean=params$recruit.size.mean,sd=params$recruit.size.sd) * 0.5\r\n}\r\n\r\n\r\n# 2. Make a kernel and initial population size\r\n# a. Boundary points b, mesh points y and step size h\r\n# Integration limits - these limits span the range of sizes observed in the data set, and then some.\r\nmin.size= 27\r\nmax.size= 180\r\n# number of cells in the discretized kernel\r\nn=(max.size-min.size)*2\r\n# boundary points (the edges of the cells defining the kernel)\r\nb=min.size+c(0:n)*(max.size-min.size)/n \r\n# mesh points (midpoints of the cells)\r\ny=0.5*(b[1:n]+b[2:(n+1)])\r\n# width of the cells\r\nh=y[2]-y[1]\r\n\r\n\r\n# b. Make component kernels\r\n# The function outer() evaluates the kernel at all pairwise combinations of the two vectors y and y. For the numerical integration, we\'re estimating the area of a rectangle under the curve. The heights of the rectangles are given by the outer function and the width of the rectangles is h. \r\nG=h*outer(y,y,g.yx,params=params) # growth sub-kernel \r\nS=s.x(y,params=params) # survival sub-kernel\r\nPkernel=G # placeholder; we\'re about to redefine P on the next line\r\nfor(i in 1:n) Pkernel[,i]=G[,i]*S[i]  # growth/survival sub-kernel\r\nFkernel=h*outer(y,y,f.yx,params=params, Rain = 104) # reproduction kernel for normal years (precipitation = 104 mm)\r\nK=Pkernel+Fkernel #full kernel\r\n\r\n##c. Estimate stable age distribution to estimate the initial population size distribution\r\nw.eigen=Re(eigen(K)$vectors[,1])\r\nstable.dist=w.eigen/sum(w.eigen) \r\n## Create initial population distribution with 1000 individuals\r\nstable.dist*1000->vec1\r\n\r\n# 3. Simulations and predictions\r\n##Prepare data for simulations 100 simulations with 100 yeras\r\ntime = 100\r\nsimulation = 100\r\npd = 0.1  ###### probability of drought, 0.1 correspond to 1 drought per 10 years\r\npn = 1-pd  #### probability of year with normal precipitation.\r\n\r\n##### create document for the results\r\nresults<-matrix(0,nrow =time*(simulation+1), ncol = 3)\r\nresults1<-matrix(0, nrow =simulation+1, ncol = 3)\r\n\r\n## Simulations\r\nfor (Z in 0:simulation) {\r\n  vec<-vec1\r\n  sample(c(61, 104), replace = TRUE, size = 100, prob = c(pd, pn))->states   #### 61 mm in the value of drought year and 104 mm the value of precipitation for normal years.\r\n  m <- matrix(ncol=time, nrow=length(vec))\r\n  m[,1]<-vec\r\n  for (Q in 2:time) {\r\n    states[Q]->pr\r\n    Fkernel=h*outer(y,y,f.yx,params=params, Rain = pr) # reproduction kernel\r\n    K=Pkernel+Fkernel #full kernel\r\n    #######Simulate population\r\n    m[,Q] <- K%*%m[,Q-1]\r\n    \r\n    ########Save results\r\n    results[Q-1+time*Z,1]<-Q-1\r\n    results[Q-1+time*Z,2]<-pr\r\n    results[Q-1+time*Z,3]<-sum(m[,Q])\r\n  }\r\n  results1[Z,1]<-Z\r\n  results1[Z,2]<-time\r\n  results1[Z,3]<-sum(m[,Q])\r\n}\r\n\r\nwrite.csv(results, ""resultst.csv"") ### results of the population size per year and per simulation\r\nwrite.csv(results1, ""results1.csv"") ### final population size after each simulation', '####### Load Packages\r\n\r\n\r\nlibrary(popdemo)\r\nlibrary(fields)\r\nlibrary(lme4)\r\nlibrary(popbio)\r\nlibrary(IPMpack)\r\nlibrary(truncnorm)\r\n\r\n###### Load data\r\nparams<- read.table(""params.txt"", sep=""\\t"", header=T)\r\nparams1<- read.table(""params_high_surv.csv"", header=T) ### this information for scenario with trade-off. Ror positive correlation we would use ""params_low_surv.csv""\r\n##1. Modelling the life history\r\n# Each of the functions below represents one or more of the vital rates. These functions are used to build the IPM and use output (the coefficients) from the regressions developed previously. \r\n# These functions represent the modeler\'s decision about how to decompose the life cycle. We model (1) survival, (2) growth, (3) reproduction.\r\n## Vital rate functions\r\n# 1. Probability of surviving\r\ns.x=function(x,params) {\r\n  u=exp(params$surv.int+params$surv.slope*x)\r\n  return(u/(1+u))\r\n}\r\n# 2. Growth function\r\ng.yx=function(xp,x,params) { \r\n  mean=params$growth.int+params$growth.slope*x\r\n  sd=params$growth.sd\t\t\r\n  dnorm(xp,mean=mean,sd=sd)\r\n}\r\n# 3. Reproduction (probability of reproduction (fec1) described by intercept, the effect of rainfall and the linear and cuadritic relation with size; fec 2 is number of clutches per year;\r\n####### fec 3 is the number of eggs per clutch related to size; fec 4 is the probability of hatching; fec 5 is the probability of surviving to the one year)  \r\nf.yx=function(xp,x,params, Rain) { \t\t\r\n  exp(params$fec.int + params$fec.prec*Rain + params$fec.slope*x + params$fec.slope2*x^2)/(1+exp(params$fec.int+params$fec.prec*Rain+params$fec.slope*x + params$fec.slope2*x^2))*\r\n    params$fec2* exp(params$fec3.int+params$fec3.slope*x)* params$fec4 * params$fec5*\r\n    dnorm(xp,mean=params$recruit.size.mean,sd=params$recruit.size.sd) * 0.5\r\n}\r\n\r\n# 2. Make a kernel and initial population size\r\n# a. Boundary points b, mesh points y and step size h\r\n# Integration limits - these limits span the range of sizes observed in the data set, and then some.\r\nmin.size= 27\r\nmax.size= 180\r\n# number of cells in the discretized kernel\r\nn=(max.size-min.size)*2\r\n# boundary points (the edges of the cells defining the kernel)\r\nb=min.size+c(0:n)*(max.size-min.size)/n \r\n# mesh points (midpoints of the cells)\r\ny=0.5*(b[1:n]+b[2:(n+1)])\r\n# width of the cells\r\nh=y[2]-y[1]\r\n\r\n\r\n# b. Make component kernels\r\n# The function outer() evaluates the kernel at all pairwise combinations of the two vectors y and y. For the numerical integration, we\'re estimating the area of a rectangle under the curve. The heights of the rectangles are given by the outer function and the width of the rectangles is h. \r\nG=h*outer(y,y,g.yx,params=params) # growth sub-kernel \r\nS=s.x(y,params=params) # survival sub-kernel\r\nPkernel=G # placeholder; we\'re about to redefine P on the next line\r\nfor(i in 1:n) Pkernel[,i]=G[,i]*S[i]  # growth/survival sub-kernel\r\nFkernel=h*outer(y,y,f.yx,params=params, Rain = 104) # reproduction kernel for normal years (precipitation = 104 mm)\r\nK=Pkernel+Fkernel #full kernel\r\n\r\n\r\n##c. Estimate stable age distribution to estimate the initial population size distribution\r\nw.eigen=Re(eigen(K)$vectors[,1])\r\nstable.dist=w.eigen/sum(w.eigen) \r\n## Create initial population distribution with 1000 individuals\r\nstable.dist*1000->vec1\r\n\r\n##d. kernel con different survival for scenario trade-off or positive correlation\r\nG1=h*outer(y,y,g.yx,params=params1) # growth sub-kernel (which does  not yet have survival in it)\r\nS1=s.x(y,params=params1) # survival affected by drought\r\nPkernel1=G1 \r\nfor(i in 1:n) Pkernel1[,i]=G1[,i]*S1[i]  # growth/survival sub-kernel\r\n\r\n\r\n# 3. Simulations and predictions\r\n##Prepare data for simulations 100 simulations with 100 yeras\r\ntime = 100\r\nsimulation = 100\r\npd = 0.1  ###### probability of drought, 0.1 correspond to 1 drought per 10 years\r\npn = 1-pd  #### probability of year with normal precipitation.\r\n\r\n##### create document for the results\r\nresults<-matrix(0,nrow =time*(simulation+1), ncol = 3)\r\nresults1<-matrix(0, nrow =simulation+1, ncol = 3)\r\n\r\n## Simulations\r\nfor (Z in 0:simulation) {\r\n  vec<-vec1\r\n  sample(c(61, 104), replace = TRUE, size = 100, prob = c(pd, pn))->states   #### 60 mm in the value of drought year and 104 mm the value of precipitation for normal years.\r\n  m <- matrix(ncol=time, nrow=length(vec))\r\n  m[,1]<-vec\r\n  for (Q in 2:time) {\r\n    states[Q]->pr\r\n    Fkernel=h*outer(y,y,f.yx,params=params, Rain = pr) # reproduction kernel\r\n    if (pr == 104) {    ###### when precipitation is normal 104 mm, select the Pkernel (with no effect of drought)\r\n      K=Pkernel+Fkernel #full kernel\r\n    }else{              ####### when there is a drought 61 mm, select the Pkernel1 (with the effect of drought in survival)\r\n      K=Pkernel1+Fkernel\r\n    }\r\n    #######Simulate population\r\n    m[,Q] <- K%*%m[,Q-1]\r\n    \r\n    ########Save results\r\n    results[Q-1+time*Z,1]<-Q-1\r\n    results[Q-1+time*Z,2]<-pr\r\n    results[Q-1+time*Z,3]<-sum(m[,Q])\r\n\r\n    \r\n  }\r\n  results1[Z,1]<-Z\r\n  results1[Z,2]<-time\r\n  results1[Z,3]<-sum(m[,Q])\r\n}\r\n\r\nwrite.csv(results, ""resultst.csv"") ### results of the population size per year and per simulation\r\nwrite.csv(results1, ""results1.csv"") ### final population size after each simulation']","The limits of demographic buffering in coping with environmental variation Animal populations have developed multiple strategies to deal with environmental change. Among them, the demographic buffering strategy consists in constraining the temporal variation of the vital rate(s) that most affect(s) the overall performance of the population. Tortoises are known to buffer their temporal variation in adult survival, which typically has the highest contribution to the population growth rate , at the expense of a high variability on reproductive rates, which contribute far less to . To identify the effects of projected increases in droughts in its natural habitat, we use field data collected across 15 locations of Testudo graeca in Southeast Spain over a decade. We analyse the effects of environmental variables on reproduction rates. In addition, we couple the demographic and environmental data to parameterise an integral projection model to simulate the effects of different scenarios of drought recurrence on  under different degrees of intensity in the survival-reproduction trade-off. We find that droughts negatively affect the probability of laying eggs; however, the overall effects on  under the current drought recurrence (one/decade) are negligible when survival is constant (independent of the reduction of reproduction by drought events) and when survival increased as a trade-off with the reduction of reproduction rates, with a threshold to population viability at three or more droughts/decade. Additionally, we show that, although some species may buffer current environmental regimes by carefully orchestrating how their vital rates vary through time, a demographic buffering strategy is insufficient to ensure population viability in extreme regimes. Our findings support the hypothesis that the demographic buffering strategy has a limit of effectiveness when adverse conditions occur frequently. Our methodological approach provides a framework for ecologists to determine how effective the management of environmental drivers can be for demographically buffering populations, and which scenarios may not provide long-term population persistence.",0
NanoString Differential Expression Analysis with NanoStringDiff package,Script for Nanostring analysis based on NanoStringDiff package (NanoStringAnalysis_NanoStringDiff_v5_general.R) allowing for multi-group analysis and for plotting results along with gene enrichment analysis (NanoStringAnalysis_eval_graph_GSEA_general_v6.R). Non-script files are example data for showing inputs and outputs.,"['## Extracting Signficant Results, Plotting Results, and Gene Rnrichment Analysis====\r\n\r\n##Script updated 2021-Sept-09\r\n\r\n#Libraries----\r\nlibrary(\'NanoStringDiff\')\r\nlibrary(\'tictoc\')\r\nlibrary(\'devtools\')\r\nlibrary(\'gdata\')\r\nlibrary(\'Biobase\')\r\nlibrary(\'openxlsx\')\r\nlibrary(\'stringr\')\r\nlibrary(""plyr"")\r\nlibrary(\'stats\')\r\nlibrary(\'EnvStats\')\r\nlibrary(\'HybridMTest\')\r\nlibrary(\'purrr\')\r\nlibrary(\'tidyr\')\r\nlibrary(\'factoextra\')\r\nlibrary(\'cluster\')\r\nlibrary(\'ggplot2\')\r\nlibrary(\'grid\')\r\nlibrary(\'gridExtra\')\r\nlibrary(\'ggrepel\')\r\nlibrary(\'pheatmap\')\r\nlibrary(\'RColorBrewer\')\r\nlibrary(\'reshape\')\r\nlibrary(\'dplyr\')\r\nlibrary(\'viridis\')\r\nlibrary(\'limma\')\r\nlibrary(\'ReactomePA\')\r\nlibrary(\'msigdbr\')\r\nlibrary(\'scales\')\r\nlibrary(\'biomaRt\')\r\nlibrary(\'pathview\')\r\nlibrary(\'gage\')\r\nlibrary(\'gageData\')\r\nlibrary(""AnnotationDbi"")\r\nlibrary(""org.Mm.eg.db"")\r\nlibrary(\'remotes\')\r\nlibrary(\'RankAggreg\')\r\n\r\n# rm(list = ls()) #Clear variables if needed\r\ngc()\r\n\r\n#Rdata File Input----\r\nfile.select = choose.files(default = """", caption = ""Select file"",\r\n                           multi = FALSE)\r\nfile.path = dirname(file.select)\r\nfile.name = tools::file_path_sans_ext(basename(file.select))  #file name  \r\nfile.type = tools::file_ext(file.select)\r\nfile.type = paste0(""."",file.type)\r\n\r\n#????Select control group and group labels with index based on group.ID\r\ngroup.ID = c(\'W\',\'D\',\'E\') #sample group IDs, example of 3 group IDs, make sure matches letters from differential expression analysis code\r\ncontrol = group.ID[2] \r\ngroup.name = c(\'Wildtype\', \'Deficient\',\'Engrafted\') #label for groups in plot\r\ngroup.color = c(\'#00BA38\', \'#F94040\',\'#619CFF\') %>% as.character #color of groups in plots\r\ngroup.label = c(\'Deficient:Other groups\',\'Deficient:Wild-type\',\'Deficient:Engraftment\',\'Engraftment:Wild-type\')\r\n#????\r\n\r\n#Load results from normalization and DE analysis ----\r\nsetwd(file.path)\r\nfile = paste0(file.path,""\\\\"",file.name,file.type)\r\nload(file)\r\nraw.endo = raw[str_detect(string = raw[,\'Code.Class\'],pattern =  \'Endogenous|Housekeeping\'),]\r\nraw.info = raw.endo[,c(\'Name\',\'Accession\')]\r\nNSD.result = \r\n  results.k.multi #???? Select variable for analysis\r\n\r\n#Check for Correct Gene Names ----\r\nname.check = raw.endo$Name\r\nname.corrected = alias2SymbolTable(alias = name.check,species = \'Mm\') %>% data.frame\r\ncolnames(name.corrected) = \'SYMBOL\'\r\nraw.endo$Name = name.corrected$SYMBOL\r\n\r\nname.check = NSD.result$table\r\nname.corrected = alias2SymbolTable(alias = name.check %>% rownames,species = \'Mm\') %>% data.frame\r\ncolnames(name.corrected) = \'SYMBOL\'\r\nrownames(NSD.result$table) = name.corrected$SYMBOL  \r\n\r\nname.check = Nanostring.norm.exprs %>% rownames\r\nname.check = alias2SymbolTable(alias = name.check,species = \'Mm\')\r\nrownames(Nanostring.norm.exprs) = name.check\r\n\r\n#Create save folder for results\r\nresults.folder = paste0(\'Results\')\r\nfile.path = paste0(file.path,\'\\\\\',results.folder)\r\nif(dir.exists(file.path)==0){\r\n  dir.create(results.folder)\r\n}\r\nsetwd(file.path)\r\n\r\n#Gene Descriptions/Output Data ----\r\nlibrary(mygene)\r\nraw.endo$entrez = mapIds(org.Mm.eg.db,keys = raw.endo$Name, \r\n                         column=\'ENTREZID\',keytype=\'SYMBOL\',multiVals = \'first\')\r\ngene.info = mygene::getGenes(raw.endo$entrez,fields = \'name\')\r\nraw.endo$Description = gene.info$name\r\noutfile = createWorkbook()\r\noutfile %>% addWorksheet(\'raw_endo\')\r\noutfile %>% writeData(sheet = \'raw_endo\', raw.endo, rowNames = FALSE)\r\nsaveWorkbook(outfile,paste0(file.path,\'\\\\\',\'raw_endo.xlsx\'),overwrite = TRUE)\r\n\r\n#???? specify order of groups and samples in plots\r\ngroup.order = c(17:24,9:16,1:8)\r\n#????\r\n\r\n#Export results with FDR/q-value < 0.05 and fold change > abs(1.5) for quick check ----\r\npvalue = subset(NSD.result$table, NSD.result$table$pvalue < 0.05\r\n                & abs(NSD.result$table$logFC) > log(2)\r\n) \r\nqvalue = subset(NSD.result$table, NSD.result$table$qvalue < 0.05\r\n                & abs(NSD.result$table$logFC) > log(2)\r\n) \r\nnrow(qvalue)\r\nall = NSD.result$table\r\n\r\ngroup = colnames(Nanostring.norm.exprs)\r\nfor(i in 1:length(group.ID)){\r\n  group.count = group %>%\r\n    grep(pattern = group.ID[i], ignore.case = TRUE, value = FALSE)\r\n  group[group.count] = group.ID[i]\r\n}\r\n\r\n#Single DE result plotting\r\n#PCA on genes with option for plotting based on highest coefficient of variation ---- \r\ndat.norm = log2(Nanostring.norm.exprs) \r\ndat.cv = apply(dat.norm,1,FUN = function (x) sd(x)/mean(x))\r\ncutoff = 0.5\r\ndat.var = dat.norm[rank(dat.cv)/length(dat.cv) > 1 - cutoff,] %>% t\r\ndat.var = dat.var[,which(apply(dat.var,2,var)!=0)]\r\ndat.PCA = prcomp(dat.var, center = TRUE, scale = TRUE)\r\n\r\ndat.percent = dat.PCA$sdev^2/sum(dat.PCA$sdev^2)\r\ngroup.PCA = group[group.order]\r\nplot.PCA = dat.PCA$x %>% as.data.frame %>% ggplot(aes(x = PC1, y = PC2)) + \r\n  geom_point(aes(shape = group.PCA, colour = group.PCA),size = 7) +\r\n  theme_bw(base_size=24) + \r\n  labs(x=paste0(""PC1: "",round(dat.percent[1]*100,1),""%""),\r\n       y=paste0(""PC2: "",round(dat.percent[2]*100,1),""%"")) +\r\n  theme_minimal() + \r\n  geom_vline(xintercept=0, linetype=""dotted"") + geom_hline(yintercept=0, linetype=""dotted"") +\r\n  geom_text_repel(aes(label = colnames(dat.norm)), size = 7) +\r\n  theme(legend.position = \'none\', \r\n        axis.text=element_text(size=16),axis.title=element_text(size=30))\r\n\r\n#Output of PCA related plots\r\ntiff(paste0(file.name,\'_PCA.tiff\'), units = \'cm\', height = 30, width = 40, compression = \'lzw\', res = 300)\r\nplot(plot.PCA)\r\ndev.off()\r\n\r\n#Heatmap of normalized values ----\r\nhm.value = dat.norm[c(rownames(qvalue)),]\r\nhm.value = hm.value[,group.order]\r\nmy.colors = colorRampPalette(rev(brewer.pal(n = 5, name =""RdBu"")))(100)\r\n\r\ngroup.organize = colnames(hm.value) %>% data.frame\r\nrownames(group.organize) = colnames(hm.value)\r\ncolnames(group.organize) = \'Phenotype\'\r\ngroup.organize[which(group[group.order] == \'W\'),] = group.name[1]\r\ngroup.organize[which(group[group.order] == \'D\'),] = group.name[2]\r\ngroup.organize[which(group[group.order] == \'E\'),] = group.name[3]\r\ngroup.organize$Phenotype = factor(group.organize$Phenotype, levels = group.name)\r\npheatmap(hm.value, density.info=\'none\',trace=\'none\', scale = \'row\',\r\n         dendrogram = ""none"", cluster_cols = FALSE,cluster_rows = TRUE,\r\n         fontsize = 10,\r\n         cellwidth = 20,\r\n         cellheight = 10,\r\n         border_color = NA,\r\n         show_colnames = FALSE,\r\n         # cutree_rows = 3,\r\n         color = my.colors,\r\n         angle_col = 45,\r\n         annotation_col = group.organize,\r\n         annotation_colors = list(Phenotype = c(Wildtype = group.color[1],\r\n                                                Deficient = group.color[2],\r\n                                                Engrafted = group.color[3]\r\n         )),\r\n         filename = paste0(file.name,\'_heatmap.tiff\')\r\n)\r\n\r\n#Extract results for plotting of significant results: ----\r\n#https://www.r-bloggers.com/tutorial-rna-seq-differential-expression-pathway-analysis-with-sailfish-deseq2-gage-and-pathview/\r\n#Prep data for GSA analysis \r\nresults = NSD.result$table$logFC %>% data.frame\r\nresults = results %>% cbind(NSD.result$table$pvalue %>% data.frame)\r\nresults = results %>% cbind(NSD.result$table$qvalue %>% data.frame)\r\nrownames(results) = NSD.result$table %>% rownames\r\nresults = results %>% cbind(mapIds(org.Mm.eg.db,keys = rownames(results), \r\n                                   column=\'REFSEQ\',keytype=\'SYMBOL\',multiVals = \'first\'))\r\nresults = results %>% cbind(mapIds(org.Mm.eg.db,keys = rownames(results), \r\n                                   column=\'ENTREZID\',keytype=\'SYMBOL\',multiVals = \'first\'))\r\nresults = results %>% cbind(mapIds(org.Mm.eg.db,keys = rownames(results), \r\n                                   column=\'GO\',keytype=\'SYMBOL\',multiVals = \'first\'))\r\nresults = results %>% cbind(mapIds(org.Mm.eg.db,keys = rownames(results), \r\n                                   column=\'GENENAME\',keytype=\'SYMBOL\',multiVals = \'first\'))\r\ncolnames(results) = c(\'logFC\',\'pvalue\',\'qvalue\',\'refseq\',\'entrez\',\'go\',\'name\')\r\nresults = results[which(!is.na(results[\'entrez\'])),]\r\nif(min(results$pvalue)==0){\r\n  results[results$pvalue==0,\'pvalue\'] = min(results[results$pvalue>0,\'pvalue\'])\r\n  results[results$qvalue==0,\'qvalue\'] = min(results[results$qvalue>0,\'qvalue\'])\r\n}\r\ngene.list.total = raw[which(raw$Code.Class == \'Endogenous\'),]\r\ngene.list.total = gene.list.total$Name\r\ngene.list.total = alias2SymbolTable(alias = gene.list.total,species = \'Mm\') %>% data.frame\r\ncolnames(gene.list.total) = \'name\'\r\ngene.list.total = gene.list.total %>% cbind(mapIds(org.Mm.eg.db,keys = gene.list.total$name, \r\n                                                   column=\'ENTREZID\',keytype=\'SYMBOL\',multiVals = \'first\'))\r\ncolnames(gene.list.total) = c(\'name\',\'entrez\')\r\ngene.list.total = gene.list.total[!is.na(gene.list.total$entrez),]\r\n\r\n#Volcano plot based on ranking of genes----\r\nlibrary(\'EnhancedVolcano\')\r\nvolcano = results[,\'logFC\',drop = FALSE]\r\nvolcano =  volcano %>% cbind.data.frame(results[,\'qvalue\'])\r\ncolnames(volcano) = c(\'logFC\',\'qvalue\')\r\n# volcano[which(volcano[,\'qvalue\'] ==0),] = 0.1   #ensure no zero/NA value on log scale \r\nplot.lim.y = -log10(results[rownames(qvalue),\'qvalue\']) %>% max %>% ceiling\r\nplot.lim.x = c(round(min(volcano$logFC), digits = 0)-0.5,round(max(volcano$logFC), digits = 0)+0.5)\r\nvolcano.rank = volcano[abs(volcano$logFC) >= log2(2) & volcano$qvalue < 0.05,]\r\nrank.list = rownames(volcano.rank)[order(volcano.rank$qvalue,decreasing = FALSE)] %>% t\r\nrank.list = rbind(rank.list\r\n                  ,rownames(volcano.rank)[order(abs(volcano.rank$logFC),decreasing = TRUE)] %>%t)\r\nrownames(rank.list) = c(\'qvalue\',\'logcFC\')\r\nrank.num = ncol(rank.list)\r\nif(rank.num > 25){ #???? Select number of genes in volcano plot to label\r\n  rank.num = 25\r\n}\r\ngc()\r\nrank.order = RankAggreg::RankAggreg(x = rank.list,k = rank.num,method = \'CE\')\r\nrank.labels = rank.order$top.list\r\ngc()\r\n# rank.labels = rownames(volcano.rank)\r\noptions(ggrepel.max.overlaps = 50)\r\nplot.volcano = EnhancedVolcano(volcano,lab = rownames(volcano), x = \'logFC\', y = \'qvalue\',\r\n                               pCutoff = 0.05, \r\n                               FCcutoff = log2(2), \r\n                               hline = c(0.05,0.01,0.001,0.0001),\r\n                               hlineType = c(\'solid\', \'longdash\', \'dotdash\', \'dotted\'),\r\n                               gridlines.major = FALSE,\r\n                               gridlines.minor = FALSE,\r\n                               xlim = plot.lim.x,\r\n                               ylim = c(0,plot.lim.y),\r\n                               ylab = bquote(~-Log[10]~italic(Q)),\r\n                               title = \r\n                                 \'\'                             \r\n                                 # paste(group.name[2],""versus"",\r\n                                       # group.name[1]\r\n                                       # \'other groups\'\r\n                                 ,\r\n                               subtitle = """",\r\n                               titleLabSize = 16,\r\n                               legendLabels = c(""NS"",""Log2 fold change"",""qvalue"",""qvalue & Log2 fold change""),\r\n                               cutoffLineType = \'blank\',cutoffLineCol = \'black\',\r\n                               pointSize = 4,\r\n                               labSize = 8,\r\n                               # shape = c(0,1,2,5), colAlpha = 1,\r\n                               drawConnectors = TRUE,\r\n                               boxedLabels = TRUE,\r\n                               selectLab = rank.labels,\r\n                               widthConnectors = 0.2, \r\n                               colConnectors = \'grey10\',\r\n                               legendPosition = \'bottom\',\r\n                               legendLabSize = 24,\r\n                               legendIconSize = 10\r\n) + theme(axis.title = element_text(size = 25),\r\n  axis.text.x= element_text(size = 25),\r\n  axis.text.y=element_text(size = 25)\r\n  )\r\ntiff(paste0(file.path,\'\\\\\',file.name,\'_volcano.tiff\'), units = \'cm\', height = 25, width = 45, compression = \'lzw\', res = 300)\r\nplot(plot.volcano)\r\ndev.off()\r\n\r\n#===============================================================================================\r\n#????Multi-file and gene enrichment analysis and comparsons, edit based on DE variables to compare\r\nresults.all = list()\r\nresults.all[[1]] = results.k.multi$table #???? edit variable name\r\nname.check = results.all[[1]] %>% rownames\r\nname.check = alias2SymbolTable(alias = name.check,species = \'Mm\')\r\nrownames(results.all[[1]]) = name.check\r\nresults.all[[1]] = results.all[[1]] %>% cbind.data.frame(mapIds(org.Mm.eg.db,keys = rownames(results.all[[1]]),\r\n                                                                column=\'ENTREZID\',keytype=\'SYMBOL\',multiVals = \'first\'))\r\nresults.all[[1]] = results.all[[1]] %>% cbind.data.frame(rep(group.label[1],nrow(results.all[[1]])))\r\ncolnames(results.all[[1]]) = c(\'logFC\', \'lr\', \'pvalue\',\'qvalue\',\'entrez\',\'compare\')\r\n\r\nresults.all[[2]] = results.ktow$table #???? edit variable name\r\nname.check = results.all[[2]] %>% rownames\r\nname.check = alias2SymbolTable(alias = name.check,species = \'Mm\')\r\nrownames(results.all[[2]]) = name.check\r\nresults.all[[2]] = results.all[[2]] %>% cbind.data.frame(mapIds(org.Mm.eg.db,keys = rownames(results.all[[2]]),\r\n                                                                column=\'ENTREZID\',keytype=\'SYMBOL\',multiVals = \'first\'))\r\nresults.all[[2]] = results.all[[2]] %>% cbind.data.frame(rep(group.label[2],nrow(results.all[[2]])))\r\ncolnames(results.all[[2]]) = c(\'logFC\', \'lr\', \'pvalue\',\'qvalue\',\'entrez\',\'compare\')\r\n\r\nresults.all[[3]] = results.ktor$table #???? edit variable name\r\nname.check = results.all[[3]] %>% rownames\r\nname.check = alias2SymbolTable(alias = name.check,species = \'Mm\')\r\nrownames(results.all[[3]]) = name.check\r\nresults.all[[3]] = results.all[[3]] %>% cbind.data.frame(mapIds(org.Mm.eg.db,keys = rownames(results.all[[3]]),\r\n                                                                column=\'ENTREZID\',keytype=\'SYMBOL\',multiVals = \'first\'))\r\nresults.all[[3]] = results.all[[3]] %>% cbind.data.frame(rep(group.label[3],nrow(results.all[[3]])))\r\ncolnames(results.all[[3]]) = c(\'logFC\', \'lr\', \'pvalue\',\'qvalue\',\'entrez\',\'compare\')\r\n\r\nresults.all[[4]] = results.rtow$table #???? edit variable name\r\nname.check = results.all[[4]] %>% rownames\r\nname.check = alias2SymbolTable(alias = name.check,species = \'Mm\')\r\nrownames(results.all[[4]]) = name.check\r\nresults.all[[4]] = results.all[[4]] %>% cbind.data.frame(mapIds(org.Mm.eg.db,keys = rownames(results.all[[4]]),\r\n                                                                column=\'ENTREZID\',keytype=\'SYMBOL\',multiVals = \'first\'))\r\nresults.all[[4]] = results.all[[4]] %>% cbind.data.frame(rep(group.label[4],nrow(results.all[[4]])))\r\ncolnames(results.all[[4]]) = c(\'logFC\', \'lr\', \'pvalue\',\'qvalue\',\'entrez\',\'compare\')\r\n\r\ngc()\r\nfor(j in 1:length(results.all)){\r\n  if(min(results.all[[j]]$pvalue)==0){\r\n    results.all[[j]][results.all[[j]]$pvalue==0,\'pvalue\'] = min(results.all[[j]][results.all[[j]]$pvalue>0,\'pvalue\'])\r\n    results.all[[j]][results.all[[j]]$qvalue==0,\'qvalue\'] = min(results.all[[j]][results.all[[j]]$qvalue>0,\'qvalue\'])\r\n  }\r\n  results.all[[j]]$SYMBOL = rownames(results.all[[j]])\r\n  rownames(results.all[[j]]) = NULL\r\n}\r\n\r\nsaveRDS(object = results.all,file = paste0(file.path,\'\\\\DE_multi.rds\'))\r\ngc()\r\n\r\nresults.all = readRDS(file = paste0(file.path,\'\\\\DE_multi.rds\'))\r\n\r\n#Save DE output to Excel ----\r\nresults.combo = do.call(what = rbind,args = results.all)\r\noutfile = createWorkbook()\r\noutfile %>% addWorksheet(\'DE\')\r\noutfile %>% writeData(sheet = \'DE\', results.combo, rowNames = TRUE)\r\n\r\n#Collect significant q-value and make bubbleplot ----\r\nresults.combo = do.call(what = rbind,args = results.all[2:4])\r\nresults.combo$\'-log10(qvalue)\' = results.combo$qvalue %>% log10 %>% \'*\'(-1)\r\nresults.all.q = results.combo[which(results.combo$qvalue < 0.05 \r\n                                    & abs(results.combo$logFC) >= log2(2)\r\n),]\r\ndesc.count = results.all.q$SYMBOL %>% table\r\ndesc.count = desc.count[desc.count %>% order(decreasing = FALSE)] %>% data.frame\r\ncolnames(desc.count) = c(\'SYMBOL\',\'Freq\')\r\nsig.list.multi = desc.count[which(desc.count$Freq > 1),""SYMBOL""]\r\nsig.list.sing = desc.count[which(desc.count$Freq == 1),""SYMBOL""]\r\n\r\n#Multi: Bubbleplot ----\r\nresults.bubble = results.all.q[str_detect(paste(pattern = sig.list.sing,collapse = \'|\'),\r\n                                          results.all.q$SYMBOL),]\r\nresults.bubble$SYMBOL = factor(results.bubble$SYMBOL,sort(unique(results.bubble$SYMBOL),decreasing = TRUE))\r\nresults.bubble$compare = factor(results.bubble$compare,group.label[2:4])\r\nbubbleplot.DE = ggplot(data = results.bubble, aes(y = SYMBOL, x = compare)) +\r\n  geom_point(aes(size = -log10(qvalue), fill = logFC)\r\n             , alpha = 0.75, shape = 21) +\r\n  theme(legend.key=element_blank(),\r\n        # legend.position=""bottom"",\r\n        axis.text.x = element_text(colour = ""black"", face=""bold"", size = 13, angle =45\r\n                                   , vjust = 1\r\n                                   , hjust = 1)\r\n        ,panel.background = element_blank(),\r\n        axis.text.y = element_text(size = 13, face=""bold"",)\r\n  ) +\r\n  scale_fill_gradient2(low = ""blue"",\r\n                       mid = ""white"",\r\n                       high = ""red"",\r\n                       space = ""Lab"",\r\n                       na.value = ""grey50"",\r\n                       guide = ""colourbar"",\r\n                       limits = c(-2,2),\r\n                       breaks = c(-2,-1,0,1,2),\r\n                       oob=squish,\r\n                       labels = c(\'< -2\',-1,0,1,\'> 2\')\r\n  ) +\r\n  scale_size_continuous(limits = c(0, max(results.bubble$`-log10(qvalue)`) %>% ceiling), breaks = c(1.3,2,3,4)\r\n                        ,labels = c(\'1\',\'2\',\'3\',\'4\'),name = bquote(~-Log[10]~italic(Q))) +\r\n  labs(x= """", y = """")\r\ntiff(paste0(file.path,\'\\\\DE_bubble_sing.tiff\'), units = \'cm\'\r\n     , height = 40, width = 8, compression = \'lzw\', res = 300)\r\nplot(bubbleplot.DE)\r\ndev.off()\r\n\r\nresults.bubble = results.all.q[str_detect(paste(pattern = sig.list.multi,collapse = \'|\'),\r\n                                          results.all.q$SYMBOL),]\r\nresults.bubble$SYMBOL = factor(results.bubble$SYMBOL,sort(unique(results.bubble$SYMBOL),decreasing = TRUE))\r\nresults.bubble$compare = factor(results.bubble$compare,group.label[2:4])\r\nbubbleplot.DE = ggplot(data = results.bubble, aes(y = SYMBOL, x = compare)) +\r\n  geom_point(aes(size = -log10(qvalue), fill = logFC)\r\n             , alpha = 0.75, shape = 21) +\r\n  theme(legend.key=element_blank(),\r\n        # legend.position=""bottom"",\r\n        axis.text.x = element_text(colour = ""black"", size = 13, face=""bold"", angle =45\r\n                                   , vjust = 1\r\n                                   , hjust = 1)\r\n        ,panel.background = element_blank(),\r\n        axis.text.y = element_text(size = 13, face=""bold"",)\r\n  ) +\r\n  scale_fill_gradient2(low = ""blue"",\r\n                       mid = ""white"",\r\n                       high = ""red"",\r\n                       space = ""Lab"",\r\n                       na.value = ""grey50"",\r\n                       guide = ""colourbar"",\r\n                       limits = c(-2,2),\r\n                       breaks = c(-2,-1,0,1,2),\r\n                       oob=squish,\r\n                       labels = c(\'< -2\',-1,0,1,\'> 2\')\r\n  ) +\r\n  scale_size_continuous(limits = c(0, max(results.bubble$`-log10(qvalue)`) %>% ceiling), breaks = c(1.3,2,3,4)\r\n                        ,labels = c(\'1\',\'2\',\'3\',\'4\'),name = bquote(~-Log[10]~italic(Q))) +\r\n  labs(x= """", y = """")\r\ntiff(paste0(file.path,\'\\\\DE_bubble_multi.tiff\'), units = \'cm\'\r\n     , height = 30, width = 8, compression = \'lzw\', res = 300)\r\nplot(bubbleplot.DE)\r\ndev.off()\r\n\r\n#Enrichment Analysis ----\r\n#Examples of plotting results\r\n#https://yulab-smu.github.io/clusterProfiler-book/chapter12.html\r\nlibrary(\'clusterProfiler\')\r\nlibrary(\'enrichplot\')\r\nlibrary(\'GOSemSim\')\r\n\r\n#Obtain disease associated KEGG and other database information\r\nkegg.set = kegg.gsets(species = \'mmu\',id.type = \'kegg\')\r\nkegg.set.ind = names(kegg.set$kg.sets)\r\nkegg.set.ind = substr(kegg.set.ind, 1,8)\r\nkegg.set.dise = kegg.set$kg.sets[kegg.set$dise.idx]\r\ndis.kegg = names(kegg.set.dise)\r\ndis.kegg = substr(dis.kegg, 1,8)\r\nkegg.set.sigmet = kegg.set$kg.sets[kegg.set$sigmet.idx]\r\nsigmet.kegg = names(kegg.set.sigmet)\r\nsigmet.kegg = substr(sigmet.kegg, 1,8)\r\n\r\nhallmark = msigdbr(species = ""Mus musculus"", category = ""H"") %>% dplyr::select(gs_name,entrez_gene)\r\n\r\ngc()\r\nenrich = list()\r\nfor(j in 1:length(results.all)){\r\n  gene.list = results.all[[j]]$logFC\r\n  names(gene.list) = results.all[[j]]$entrez\r\n  gene.list = sort(gene.list,decreasing = TRUE)\r\n  \r\n  temp = gseKEGG(geneList = gene.list\r\n                 ,organism = \'mmu\'\r\n                 ,pvalueCutoff = 0.05\r\n                 ,pAdjustMethod = \r\n                   \'BH\'\r\n                   # \'none\'\r\n  )\r\n  \r\n  keggres.gse.disind = match(dis.kegg,temp@result$ID,nomatch = 0)\r\n  if(sum(keggres.gse.disind)>0){\r\n    temp = slice(temp,-keggres.gse.disind)\r\n  }\r\n  temp = setReadable(x = temp, OrgDb = org.Mm.eg.db, keyType=""ENTREZID"")\r\n  temp = temp %>% data.frame\r\n  if(nrow(temp) > 0 & length(enrich) < j){\r\n    temp$Description = paste0(\'K_\',temp$Description)\r\n    enrich[[j]] = temp\r\n    } else if(nrow(temp) > 0 & length(enrich) == j){\r\n      temp$Description = paste0(\'K_\',temp$Description)\r\n      enrich[[j]] = rbind(enrich[[j]],temp)\r\n    }\r\n  rm(temp)\r\n  gc()\r\n  \r\n  temp = gseGO(geneList = gene.list\r\n               ,OrgDb= org.Mm.eg.db\r\n               ,ont = ""BP""\r\n               ,pvalueCutoff = 0.05\r\n               ,pAdjustMethod = \r\n                 \'BH\'\r\n                 # \'none\'\r\n               ,nPermSimple = 10000\r\n  )\r\n  drop.index = str_detect(pattern = \'none\'\r\n                          ,string = temp@result$Description)\r\n  if(sum(drop.index %>% as.integer)>0){\r\n    temp = slice(temp,-which(drop.index == TRUE))\r\n  }\r\n  temp = simplify(temp, cutoff=0.7, by=""pvalue"", select_fun=min)\r\n  temp = setReadable(x = temp, OrgDb = org.Mm.eg.db, keyType=""ENTREZID"")\r\n  temp = temp %>% data.frame\r\n  \r\n  if(nrow(temp) > 0 & length(enrich) < j){\r\n    temp$Description = paste0(\'G_\',temp$Description)\r\n    enrich[[j]] = temp\r\n  } else if(nrow(temp) > 0 & length(enrich) == j){\r\n    temp$Description = paste0(\'G_\',temp$Description)\r\n    enrich[[j]] = rbind(enrich[[j]],temp)\r\n  }\r\n  rm(temp)\r\n  gc()\r\n  \r\n  temp = gsePathway(geneList = gene.list\r\n                    ,organism = \'mouse\'\r\n                    ,pvalueCutoff = 0.05\r\n                    ,pAdjustMethod =\r\n                      \'BH\'\r\n                      # \'none\'\r\n  )\r\n  temp = setReadable(x = temp, OrgDb = org.Mm.eg.db, keyType=""ENTREZID"")\r\n  temp = temp %>% data.frame\r\n  \r\n  if(nrow(temp) > 0 & length(enrich) < j){\r\n    temp$Description = paste0(\'R_\',temp$Description)\r\n    enrich[[j]] = temp\r\n  } else if(nrow(temp) > 0 & length(enrich) == j){\r\n    temp$Description = paste0(\'R_\',temp$Description)\r\n    enrich[[j]] = rbind(enrich[[j]],temp)\r\n  }\r\n  rm(temp)\r\n  gc()\r\n  \r\n  temp = GSEA(geneList = gene.list\r\n              ,TERM2GENE = hallmark\r\n              ,pvalueCutoff = 0.05\r\n              ,pAdjustMethod = \'BH\'\r\n  )\r\n  temp = setReadable(x = temp, OrgDb = org.Mm.eg.db, keyType=""ENTREZID"")\r\n  temp = temp %>% data.frame\r\n  if(nrow(temp) > 0 & length(enrich) < j){\r\n    temp$Description = gsub(pattern = \'HALLMARK\',replacement = \'H_\',x = temp$Description,ignore.case = TRUE)\r\n    enrich[[j]] = temp\r\n  } else if(nrow(temp) > 0 & length(enrich) == j){\r\n    temp$Description = gsub(pattern = \'HALLMARK\',replacement = \'H_\',x = temp$Description,ignore.case = TRUE)\r\n    enrich[[j]] = rbind(enrich[[j]],temp)\r\n  }\r\n  rm(temp)\r\n  gc()\r\n  \r\n  groupcol = rep(x = group.label[j],nrow(enrich[[j]])) %>% data.frame\r\n  colnames(groupcol) = \'Group\'\r\n  enrich[[j]] = enrich[[j]] %>% cbind.data.frame(groupcol)\r\n}\r\nsaveRDS(object = enrich,file = paste0(file.path,\'\\\\enrich_multi.rds\'))\r\n\r\n#Save all enrichment results to Excel file ----\r\nenrich = readRDS(file = paste0(file.path,\'\\\\enrich_multi.rds\'))\r\nenrich.all = do.call(what = rbind,enrich)\r\nenrich.all = enrich.all[-which(\r\n  str_count(string = enrich.all$core_enrichment,pattern = \'/\') < 2),]\r\noutfile %>% addWorksheet(\'GSEA\')\r\noutfile %>% writeData(sheet = \'GSEA\', enrich.all, rowNames = TRUE)\r\nsaveWorkbook(outfile,paste0(file.path,\'/\',file.name,\'.xlsx\'),overwrite = TRUE)\r\n\r\n#Heatplot per group ----\r\nenrich = readRDS(file = paste0(file.path,\'\\\\enrich_multi.rds\'))\r\nenrich.all = do.call(what = rbind,enrich)\r\nenrich.all = enrich.all[-which(\r\n  str_count(string = enrich.all$core_enrichment,pattern = \'/\') < 2),]\r\n\r\nresults.all = readRDS(file = paste0(file.path,\'\\\\DE_multi.rds\'))\r\nresults.combo = do.call(what = rbind,args = results.all)\r\n\r\nenrich.heatplot = list()\r\nfor(i in 1:(group.label %>% length)){\r\n  enrich.group = enrich.all[which(enrich.all$Group ==group.label[i]),]  \r\n  enrich.genelist = strsplit(enrich.group$core_enrichment,\'/\')\r\n  enrich.genelist.order = unlist(enrich.genelist) %>% table\r\n  enrich.genelist.order = enrich.genelist.order[enrich.genelist.order %>% order(decreasing = TRUE)] %>% data.frame\r\n  enrich.genelist.order = enrich.genelist.order$. %>% data.frame\r\n  enrich.heatplot[[i]] = \r\n  colnames(enrich.genelist.order) = ""SYMBOL""\r\n  enrich.heatplot[[i]] = matrix(nrow = nrow(enrich.genelist.order),\r\n                                ncol = length(enrich.genelist)) %>% data.frame\r\n  rownames(enrich.heatplot[[i]]) = enrich.genelist.order$SYMBOL\r\n  colnames(enrich.heatplot[[i]]) = enrich.group$Description\r\n  for(j in 1:length(enrich.genelist)){\r\n    temp = str_detect(\r\n      string = enrich.genelist.order$SYMBOL,\r\n      pattern = paste(enrich.genelist[j] %>% unlist,collapse = \'|\')) %>% data.frame\r\n    enrich.heatplot[[i]][,j] = temp\r\n  }\r\n  \r\n  results.group = results.combo[which(results.combo$compare == group.label[i]),]\r\n  results.group = join(x = enrich.genelist.order,\r\n                        y = results.group,\r\n                        by = \'SYMBOL\')\r\n  rownames(results.group) = results.group$SYMBOL\r\n  results.group = results.group[,""logFC"",drop = FALSE]\r\n  for(j in 1:ncol(enrich.heatplot[[i]])){\r\n    enrich.heatplot[[i]][,j] = (enrich.heatplot[[i]][,j]%>% as.numeric)*(results.group$logFC %>% as.numeric)  \r\n  }\r\n  \r\n  # enrich.heatplot[[i]][enrich.heatplot[[i]]==0] = \r\n  \r\n  my.colors = colorRampPalette(rev(brewer.pal(n = 5, name =""RdBu"")))(100)\r\n  pheatmap(enrich.heatplot[[i]], density.info=\'none\',trace=\'none\', scale = \'none\',\r\n           dendrogram = ""none"", cluster_cols = TRUE,cluster_rows = FALSE,\r\n           fontsize = 5,\r\n           cellwidth = 10,\r\n           cellheight = 5,\r\n           border_color = NA,\r\n           show_colnames = TRUE,\r\n           color = my.colors,\r\n           # cutree_rows = 3,\r\n           # color = my.colors,\r\n           angle_col = 45,\r\n           filename = paste0(file.path,\'\\\\\',file.name,\'_heatplot_\',i,\'.tiff\')\r\n  )\r\n}\r\n\r\n#Bubbleplot Enrichment ----\r\ngc()\r\nenrich = readRDS(file = paste0(file.path,\'\\\\enrich_multi.rds\'))\r\nenrich.all = do.call(what = rbind,enrich)\r\nenrich.all = enrich.all[-which(\r\n  str_count(string = enrich.all$core_enrichment,pattern = \'/\') < 2),]\r\nenrich.all$stats = enrich.all$qvalue %>% log10 %>% \'*\'(-1)\r\n# enrich.all$Description = factor(enrich.all$Description,sort(unique(enrich.all$Description),decreasing = FALSE))\r\nenrich.all$Group = factor(enrich.all$Group,group.label)\r\ndesc.count = enrich.all$Description %>% table\r\ndesc.count = desc.count[desc.count %>% order(decreasing = FALSE)] %>% data.frame\r\ncolnames(desc.count) = c(\'Description\',\'Freq\')\r\nfor(i in 1:max(desc.count$Freq)){\r\n  temp = desc.count[which(desc.count$Freq == i),]\r\n  temp= temp[temp$Description %>% order(decreasing = TRUE),]\r\n  desc.count[which(desc.count$Freq == i),] = temp\r\n}\r\n\r\n# desc.count = desc.count[desc.count$Freq > 1,]\r\ndesc.count = desc.count[desc.count$Freq == 1,]\r\nenrich.all = enrich.all[enrich.all$Description %in% desc.count$Description,]\r\nenrich.all$Description = factor(enrich.all$Description,desc.count$Description)\r\n\r\ntiff(paste0(file.path,\'\\\\enrich_bubble.tiff\'), units = \'cm\', \r\n     height = 17, width = 26, compression = \'lzw\', res = 300)\r\nggplot(data = enrich.all, aes(y = Description, x = Group)) +\r\n  geom_point(aes(size = stats, fill = NES)\r\n             , alpha = 0.75, shape = 21) +\r\n  theme(legend.key=element_blank(),\r\n        # legend.position=""bottom"",\r\n        axis.text.x = element_text(colour = ""black"", face = \'bold\',size = 12, angle =45\r\n                                   , vjust = 1\r\n                                   , hjust = 1)\r\n        ,panel.background = element_blank(),\r\n        axis.text.y = element_text(size = 12,face = \'bold\')\r\n  ) +\r\n  scale_fill_gradient2(low = ""blue"",\r\n                       mid = ""white"",\r\n                       high = ""red"",\r\n                       space = ""Lab"",\r\n                       na.value = ""grey50"",\r\n                       guide = ""colourbar"",\r\n                       limits = c(-2,2),\r\n                       breaks = c(-2,-1,0,1,2),\r\n                       oob=squish,\r\n                       labels = c(\'< -2\',-1,0,1,\'> 2\')\r\n  ) +\r\n  scale_size_continuous(limits = c(0, max(enrich.all$stats) %>% ceiling), breaks = c(1.3,2,3,4)\r\n                        ,labels = c(\'1\',\'2\',\'3\',\'4\'),name = bquote(~-Log[10]~italic(Q))) +\r\n  labs(x= """", y = """")\r\ndev.off()\r\ngc()', '###NanoString Differential Expression Analysis with NanoStringDiff package \r\n##Instructions\r\n#Update script at section between ""????"" symbol in comments for specific experimental design\r\n#Format raw data file as .csv file for input with ""Code Class"", ""Name"", ""Accession"" columns then sample counts\r\n#For samples per group, have unique group letter ID per sample (ie C = ctrl samples: C1, C2, C3)\r\n\r\n##Instructions on generalized data analysis followed by \r\n#https://www.bioconductor.org/packages/release/bioc/vignettes/NanoStringDiff/inst/doc/NanoStringDiff.pdf\r\n#or uncomment and run: \r\n  # directory <- system.file(""doc"", package=""NanoStringDiff"", mustWork=TRUE)\r\n  # path<-paste(directory,""NanoStringDiff.pdf"",sep=""/"")\r\n  # shell.exec(path)\r\n#https://doi.org/10.1093/nar/gkw677\r\n\r\n##Script updated 2021-Sept-09\r\n\r\n#Load relate libraries\r\nlibrary(\'Biobase\')\r\nlibrary(\'NanoStringDiff\')\r\nlibrary(\'tidyr\')\r\nlibrary(\'tictoc\')\r\n\r\n# rm(list = ls()) #Clear variables if needed\r\ngc()\r\ntic(\'Total\') #Run timer\r\n\r\n#CSV File Input and Experimental Design----\r\nfile.select = choose.files(default = """", caption = ""Select file"",\r\n                         multi = FALSE)\r\nfile.path = dirname(file.select)\r\nfile.name = tools::file_path_sans_ext(basename(file.select))  #file name  \r\nfile.type = tools::file_ext(file.select)\r\nfile.type = paste0(""."",file.type)\r\n\r\n#????Input unique letter group ID for samples????====\r\ngroup.ID = c(\'W\',\'D\',\'E\') #sample group IDs, example of 3 group IDs\r\n#????\r\n\r\n#Import data files\r\nsetwd(file.path)\r\nfile = paste0(file.path,""\\\\"",file.name,file.type)\r\nraw = read.csv(file, header = TRUE) %>% data.frame\r\nsample.ID = colnames(raw)[4:ncol(raw)]\r\n\r\n#Distribute samples amongst groups based on unique group letter ID\r\ngroup = vector(mode = \'character\', length = length(sample.ID))\r\nfor(i in 1:length(group.ID)){\r\n  group.index = string = sample.ID %>%  \r\n    grep(pattern = group.ID[i], ignore.case = TRUE, value = FALSE)\r\n  group[group.index] = group.ID[i]\r\n}\r\n\r\n#Determine genes below threshold of negative controls based on average signal and 1 standard deviation and create new csv file with excluded data\r\nneg.mean = raw[which(raw[,\'Code.Class\'] == \'Negative\'),][,sample.ID] %>% as.matrix %>% mean\r\nneg.sd = raw[which(raw[,\'Code.Class\'] == \'Negative\'),][,sample.ID] %>% as.matrix %>% sd\r\nneg.threshold = neg.mean + neg.sd\r\ndata.threshold = raw[which(raw[,\'Code.Class\'] == \'Endogenous\'),][,sample.ID] %>% rowMeans %>% data.frame\r\nrownames(data.threshold) = raw[which(raw[,\'Code.Class\'] == \'Endogenous\'),][,\'Name\']\r\nprint(paste(\'Genes below negative threshold to be excluded\',length(which(data.threshold < neg.threshold))))\r\nrownames(data.threshold)[which(data.threshold < neg.threshold)]\r\nraw.exclude = raw[-c(which(data.threshold < neg.threshold)),]\r\n\r\nfile.name.exclude = paste0(file.name,\'_exclude\')\r\nfile = paste0(file.path,""\\\\"",file.name.exclude,file.type)\r\nwrite.csv(raw.exclude,file, row.names = FALSE)\r\n\r\n#Save into function specific class for NanostringDiff, can assess output with pData() and exprs()\r\ndesigns = data.frame(group)\r\nNanoStringData = createNanoStringSetFromCsv(file,header=TRUE, designs)\r\n\r\n#Design matrix for comparisons\r\ndesign.full = model.matrix(~0+group,pData(NanoStringData))  #Design of samples to experiment groups\r\ncolnames(design.full) = gsub(pattern = \'group\',\'\',colnames(design.full))\r\n\r\n#Normalization based on package function, https://doi.org/10.1093/nar/gkw677\r\nNanoStringData.norm=estNormalizationFactors(NanoStringData)\r\n\r\n#Normalized data for plotting\r\n#Instructions obtained from http://supportupgrade.bioconductor.org/p/109776/\r\nc = positiveFactor(NanoStringData.norm)\r\nd = housekeepingFactor(NanoStringData.norm)\r\nk =  c*d\r\nlamda_i = negativeFactor(NanoStringData.norm)\r\nY = exprs(NanoStringData.norm)\r\nY_n = sweep(Y,2,lamda_i,FUN = \'-\')\r\nY_nph = sweep(Y_n,2,k, FUN = \'/\')\r\nY_nph[Y_nph <= 0] = 0.1\r\nNanostring.norm.exprs = Y_nph\r\n\r\n#???? Write comparisons based on NanostringDiff vignette, examples below for comparisons between three groups====\r\ntic(\'Pairwise\') #Timer based on each pairwise comparison\r\n#Pairwise Comparison----\r\nresults.ktor = glm.LRT(NanoStringData.norm,design.full,contrast=c(1,-1,0))\r\nresults.ktow = glm.LRT(NanoStringData.norm,design.full,contrast=c(1,0,-1))\r\nresults.rtow = glm.LRT(NanoStringData.norm,design.full,contrast=c(0,1,-1))\r\ntoc()\r\n\r\ntic(\'Multigroup\') #Timer based on each multi-group comparison\r\nresults.k.multi = glm.LRT(NanoStringData.norm,design.full,contrast=c(1,-1/2,-1/2))\r\nresults.w.multi = glm.LRT(NanoStringData.norm,design.full,contrast=c(-1/2,-1/2,1))\r\nresults.r.multi = glm.LRT(NanoStringData.norm,design.full,contrast=c(-1/2,1,-1/2))\r\ntoc()\r\n#????\r\n\r\n#????Save output from DE, edit variable specific names====\r\nsave(NanoStringData,NanoStringData.norm,\r\n     results.k.multi,results.w.multi,results.r.multi,results.ktor, results.ktow, results.rtow #save differential input, change for variable to save\r\n     , design.full, Nanostring.norm.exprs, raw, raw.exclude, group, group.ID, \r\n     file = \'NanostringDE.Rdata\')\r\n\r\nsaveRDS(results.k.multi #???? change for variable to save\r\n        ,\'DEresults.rds\') \r\n\r\nsaveRDS(Nanostring.norm.exprs #???? change for variable to save\r\n        ,""normexp.rds"") \r\n\r\nwrite.table(results.k.multi$table #???? change for variable to save\r\n  , \'DEresults.txt\'\r\n  , sep=""\\t""\r\n  ,row.names = TRUE\r\n  ,col.names = TRUE\r\n  ,quote = FALSE\r\n)\r\n\r\ntoc()\r\n#????']",NanoString Differential Expression Analysis with NanoStringDiff package Script for Nanostring analysis based on NanoStringDiff package (NanoStringAnalysis_NanoStringDiff_v5_general.R) allowing for multi-group analysis and for plotting results along with gene enrichment analysis (NanoStringAnalysis_eval_graph_GSEA_general_v6.R). Non-script files are example data for showing inputs and outputs.,0
Out of Asia? Expansion of Eurasian Lyme borreliosis causing genospecies display unique evolutionary trajectories,"Vector-borne pathogens exist in obligate transmission cycles between vector and reservoir host species. Host and vector shifts can lead to geographic expansion of infectious agents and the emergence of new diseases in susceptible individuals. Three bacterial genospecies (Borrelia afzelii, Borrelia bavariensis, and Borrelia garinii) predominantly utilize two distinct tick species as vectors in Asia (Ixodes persulcatus) and Europe (Ixodes ricinus). Through these vectors, the bacteria can infect various vertebrate groups (e.g. rodents, birds) including humans where they cause Lyme borreliosis, the most common vector-borne disease in the Northern hemisphere. Yet, how and in which order the three Borrelia genospecies colonized each continent remains unclear including the evolutionary consequences of this geographic expansion. Here, by reconstructing the evolutionary history of 142 Eurasian isolates, we find evidence that the ancestors of each of the three genospecies likely have an Asian origin. Even so, each genospecies studied displayed a unique sub-structuring and evolutionary response to the colonization of Europe. The pattern of allele sharing between continents is consistent with the dispersal rate of the respective vertebrate hosts, supporting the concept that adaptation of Borrelia genospecies to the host is important for pathogen dispersal. Our results highlight that Eurasian Lyme borreliosis agents are all capable of geographic expansion with host association influencing their dispersal; further displaying the importance of host and vector association to the geographic expansion of vector-borne pathogens and potentially conditioning their capacity as emergent pathogens.","['#Analysis of chromosome Samples#\r\n#Robert E. Rollins - 08.12.20#\r\nrm(list=ls())\r\nsetwd(""E:/PhD/HostVectorAdaptation/Manuscripts/JapaneseSamplesEvolution/population_genetics/alignments"")\r\nlibrary(""ape"")\r\nlibrary(""adegenet"")\r\nlibrary(""pegas"")\r\nlibrary(""lme4"")\r\nlibrary(""poppr"")\r\n\r\n##########################################################################################################################################################################\r\n####AFZELII#####\r\n#Load in afzelii alignments (chromosome first, modify then for plasmids)#\r\nsetwd(""E:/PhD/HostVectorAdaptation/Manuscripts/JapaneseSamplesEvolution/population_genetics/alignments/afzelii"")\r\n\r\n####CHROMOSOME#####\r\n#all afzelii samples#\r\nall_afz_chrom<-read.dna(""allchrom_NB_RER_shortname_afz_aligned.fasta"", format = ""fasta"")\r\nall_afz_chrom_pop<-c(rep(""Asia"", 20), rep(""Europe"", 16))\r\nall_afz_chrom_genid<-  DNAbin2genind(all_afz_chrom, pop=all_afz_chrom_pop,exp.char=c(""a"",""t"",""g"",""c""), polyThres=1/100)\r\nafz_hier<-data.frame(Contient=all_afz_chrom_pop, \r\n                     Region=c(rep(""ASA"", 4),rep(""NAG"", 16), rep(""Germany"", 14), ""Austria"", ""Sweden""))\r\nstrata(all_afz_chrom_genid)<- afz_hier\r\nnameStrata(all_afz_chrom_genid) <- ~Continent/Region\r\n\r\n#All EU samples\r\nEU_afz_chrom<-read.dna(""EUchrom_NB_RER_shortname_afz_aligned.fasta"", format = ""fasta"")\r\n\r\n#All JA/Asian samples#\r\nAsia_afz_chrom<-read.dna(""justJA_chrom_allafz_aligned_160720.fasta"", format = ""fasta"")\r\nAsia_afz_chrom_pop <- c(rep(""ASA"",4), rep(""NAG"", 16))\r\nAsia_afz_chrom_genid<-DNAbin2genind(Asia_afz_chrom, pop=Asia_afz_chrom_pop,exp.char=c(""a"",""t"",""g"",""c""), polyThres=1/100)\r\nAsia_afz_hier<-data.frame(Country=c(rep(""Japan"", 20)),\r\n                          Region=Asia_afz_chrom_pop)\r\nstrata(Asia_afz_chrom_genid)<- Asia_afz_hier\r\nnameStrata(Asia_afz_chrom_genid)<- ~Country/Region\r\n\r\n#Nucleotide Diversity per Group (chrom)#\r\nnd_chrom_all_afz <- nuc.div(all_afz_chrom)\r\nnd_chrom_asia_afz <- nuc.div(Asia_afz_chrom)\r\nnd_chrom_EU_afz <- nuc.div(EU_afz_chrom)\r\n\r\n\r\n#Tajimas D per group (chrom)#\r\ntd_chrom_all_afz <- tajima.test(all_afz_chrom)\r\ntd_chrom_asia_afz <- tajima.test(Asia_afz_chrom)\r\ntd_chrom_EU_afz <- tajima.test(EU_afz_chrom)\r\nstats_afz_chrom<-data.frame(dataset = c(""JA afz"", ""EU afz""),\r\n                            n = c(length(labels(Asia_afz_chrom)),length(labels(EU_afz_chrom))),\r\n                            pi = c(nd_chrom_asia_afz,nd_chrom_EU_afz),\r\n                            tajimas_d =c(td_chrom_asia_afz$D,td_chrom_EU_afz$D))\r\nwrite.table(stats_afz_chrom, file = ""pi_TD_afz_chrom.txt"", sep = "" "", col.names = TRUE)\r\n\r\n#Calcuate AMOVA on all samples#\r\nafz_chrom_amova<-poppr.amova(all_afz_chrom_genid, hier =  ~Continent/Region)\r\nwrite.table(afz_chrom_amova$results, file = ""afz_chrom_all_amova.txt"")\r\nwrite.table(afz_chrom_amova$componentsofcovariance,file = ""afz_chrom_all_amova.txt"", append = TRUE)\r\nwrite.table(afz_chrom_amova$statphi, file = ""afz_chrom_all_amova.txt"", append = TRUE)\r\n\r\n\r\n###############################################################################################################################################################################\r\n####GARINII#####\r\n#Load in garinii alignments (chromosome first, modify then for plasmids)#\r\nsetwd(""E:/PhD/HostVectorAdaptation/Manuscripts/JapaneseSamplesEvolution/population_genetics/alignments/garinii"")\r\n\r\n####CHROMOSOME#####\r\n#all_gar_chrom#\r\nall_gar_chrom<-read.dna(""allchrom_NB_RER_shortname_gar_aligned.fasta"", format = ""fasta"")\r\nall_gar_hier<-read.csv(""garinii_hier.csv"", sep = "";"")\r\nall_gar_chrom_genind<-DNAbin2genind(all_gar_chrom, pop=all_gar_hier$Continent,exp.char=c(""a"",""t"",""g"",""c""), polyThres=1/100)\r\nstrata(all_gar_chrom_genind)<-all_gar_hier\r\nnameStrata(all_gar_chrom_genind)<- ~Continent/Region\r\n\r\n#All EU samples\r\nEU_gar_chrom<-read.dna(""EUchrom_NB_RER_shortname_gar_aligned.fasta"", format = ""fasta"")\r\n\r\n#All Asian samples#\r\nAsia_gar_chrom<-read.dna(""Asiachrom_NB_RER_shortname_gar_aligned.fasta"", format = ""fasta"")\r\n\r\n\r\n#Nucleotide Diversity per Group (chrom)#\r\nnd_chrom_asia_gar <- nuc.div(Asia_gar_chrom)\r\nnd_chrom_EU_gar <- nuc.div(EU_gar_chrom)\r\n#Tajimas D per group (chrom)#\r\ntd_chrom_asia_gar <- tajima.test(Asia_gar_chrom)\r\ntd_chrom_EU_gar <- tajima.test(EU_gar_chrom)\r\nstats_gar_chrom<-data.frame(dataset = c(""asia gar"", ""EU gar""),\r\n                            n = c(length(labels(Asia_gar_chrom)),length(labels(EU_gar_chrom))),\r\n                            pi = c(nd_chrom_asia_gar,nd_chrom_EU_gar),\r\n                            tajimas_d =c(td_chrom_asia_gar$D,td_chrom_EU_gar$D))\r\nwrite.table(stats_gar_chrom, file = ""pi_TD_gar_chrom.txt"", sep = "" "", col.names = TRUE)\r\n\r\n#Calcuate AMOVA on all samples#\r\ngar_chrom_amova<-poppr.amova(all_gar_chrom_genind, hier =  ~Continent/Region)\r\nwrite.table(gar_chrom_amova$results, file = ""gar_chrom_all_amova.txt"")\r\nwrite.table(gar_chrom_amova$componentsofcovariance,file = ""gar_chrom_all_amova.txt"", append = TRUE)\r\nwrite.table(gar_chrom_amova$statphi, file = ""gar_chrom_all_amova.txt"", append = TRUE)\r\n\r\n#Calcualte Fst and other stats for all Asia (JA) and all garelii samples#\r\nhierfstat_gar_all_chrom<-basic.stats(all_gar_chrom_hierfstat, diploid = FALSE)\r\ndput(hierfstat_gar_all_chrom$overall, file = ""hierfstat_gar_all_chrom.txt"")\r\n\r\nhierfstat_gar_JAGE_chrom<-basic.stats(JAGE_gar_chrom_hierfstat, diploid = FALSE)\r\ndput(hierfstat_gar_JAGE_chrom$overall, file = ""hierfstat_gar_JAGE_chrom.txt"")\r\n\r\n#############################################################################################################################################################################\r\n####BAVARIENSIS#####\r\n#Load in bavariensis alignments (chromosome first, modify then for plasmids)#\r\nsetwd(""E:/PhD/HostVectorAdaptation/Manuscripts/JapaneseSamplesEvolution/population_genetics/alignments/bavariensis"")\r\n\r\n####CHROMOSOME#####\r\n#all bavariensis samples#\r\nall_bav_chrom<-read.dna(""allchrom_NB_RER_shortname_bav_aligned.fasta"", format = ""fasta"")\r\nall_bav_chrom_pop<-read.csv(""bavariensis_all_hier.csv"", sep = "";"")\r\nall_bav_chrom_genind<-  DNAbin2genind(all_bav_chrom, pop=all_bav_chrom_pop$Continent,exp.char=c(""a"",""t"",""g"",""c""), polyThres=1/100)\r\nstrata(all_bav_chrom_genind)<- all_bav_chrom_pop\r\nnameStrata(all_bav_chrom_genind) <- ~Continent/Region\r\n\r\n#All EU samples\r\nEU_bav_chrom<-read.dna(""EUchrom_NB_RER_shortname_bav_aligned.fasta"", format = ""fasta"")\r\n\r\n#All Asian samples#\r\nAsia_bav_chrom<-read.dna(""Asiachrom_NB_RER_shortname_bav_aligned.fasta"", format = ""fasta"")\r\n\r\n#All JA samples#\r\nJA_bav_chrom<-read.dna(""justJA_chrom_allbav_aligned_160720.fasta"", format = ""fasta"")\r\nJA_bav_chrom_pop <- c(rep(""ASA"",6), rep(""NAG"", 7))\r\nJA_bav_chrom_genind<-DNAbin2genind(JA_bav_chrom, pop=JA_bav_chrom_pop,exp.char=c(""a"",""t"",""g"",""c""), polyThres=1/100)\r\nJA_bav_hier<-data.frame(Country=c(rep(""Japan"", 13)),\r\n                        Region=JA_bav_chrom_pop)\r\nstrata(JA_bav_chrom_genind)<- JA_bav_hier\r\nnameStrata(JA_bav_chrom_genind)<- ~Country/Region\r\n\r\n#Nucleotide Diversity per Group (chrom)#\r\nnd_chrom_asia_bav <- nuc.div(Asia_bav_chrom)\r\nnd_chrom_EU_bav <- nuc.div(EU_bav_chrom)\r\n\r\n#Tajimas D per group (chrom)#\r\ntd_chrom_asia_bav <- tajima.test(Asia_bav_chrom)\r\ntd_chrom_EU_bav <- tajima.test(EU_bav_chrom)\r\n\r\nstats_bav_chrom<-data.frame(dataset = c(""Asia bav"", ""EU bav""),\r\n                            n = c(length(labels(Asia_bav_chrom)),length(labels(EU_bav_chrom))),\r\n                            pi = c(nd_chrom_asia_bav,nd_chrom_EU_bav),\r\n                            tajimas_d =c(td_chrom_asia_bav$D,td_chrom_EU_bav$D))\r\nwrite.table(stats_bav_chrom, file = ""pi_TD_bav_chrom.txt"", sep = "" "", col.names = TRUE)\r\n\r\n#Calcuate AMOVA on all samples#\r\nbav_chrom_amova<-poppr.amova(all_bav_chrom_genind, hier =  ~Continent/Region)\r\nwrite.table(bav_chrom_amova$results, file = ""bav_chrom_all_amova.txt"")\r\nwrite.table(bav_chrom_amova$componentsofcovariance,file = ""bav_chrom_all_amova.txt"", append = TRUE)\r\nwrite.table(bav_chrom_amova$statphi, file = ""bav_chrom_all_amova.txt"", append = TRUE)\r\n\r\n#Calcualte Fst and other stats for all Asia (JA) and all bavelii samples#\r\nhierfstat_bav_all_chrom<-basic.stats(all_bav_chrom_hierfstat, diploid = FALSE)\r\ndput(hierfstat_bav_all_chrom$overall, file = ""hierfstat_bav_all_chrom.txt"")\r\n\r\nhierfstat_bav_JA_chrom<-basic.stats(JA_bav_chrom_hierfstat, diploid = FALSE)\r\ndput(hierfstat_bav_JA_chrom$overall, file = ""hierfstat_bav_JA_chrom.txt"")\r\n\r\n##Calcuating Fst and Dxy for aligned chromosomes coming from Borrelia afzelii, Borrelia garinii, and Borrelia bavariensis##\r\n#Written by: Robert E. Rollins on 08.12.20#\r\n#Updated by: Robert E. Rollins on 26.05.21#\r\n\r\n#Load PopGenome Librar#\r\nlibrary(""PopGenome"")\r\n\r\n#afzelii#\r\nsetwd(""FST_DXY/"")\r\nafz_all_popgen<-readData(path = ""/afzelii"", #add in full file name to folder contiaing the allchrom_afz aligned fasta file#\r\n                         format = ""fasta"",\r\n                         include.unknown = TRUE)\r\nafzelii_info <- read.delim(""afzelii_pops.txt"", sep = ""\\t"")\r\npopulations<- split(afzelii_info$ind, afzelii_info$pop)\r\nafz_all_popgen<-set.populations(afz_all_popgen, populations, diploid = F)\r\nafz_all_popgen<-diversity.stats(afz_all_popgen, pi = TRUE)\r\nafz_all_popgen<-neutrality.stats(afz_all_popgen)\r\nafz_all_popgen<-F_ST.stats(afz_all_popgen, mode = ""nucleotide"")\r\nafz_all_popgen<-F_ST.stats(afz_all_popgen, mode = ""haplotype"")\r\nfst_afz_all <- t(afz_all_popgen@nuc.F_ST.pairwise)\r\nnd_afz_all <- afz_all_popgen@nuc.diversity.within/afz_all_popgen@n.sites\r\ndxy_afz_all <-get.diversity(afz_all_popgen, between = T)[[2]]/afz_all_popgen@n.sites\r\ndxy_afz_all\r\n\r\n#bavariensis#\r\nbav_all_popgen<-readData(path = ""/bavariensis"", #add in full file name to folder contiaing the allchrom_bav aligned fasta file#\r\n                         format = ""fasta"",\r\n                         include.unknown = TRUE)\r\nbavariensis_info <- read.delim(""bavariensis_pops.txt"", sep = ""\\t"")\r\npopulations<- split(bavariensis_info$ind, bavariensis_info$pop)\r\nbav_all_popgen<-set.populations(bav_all_popgen, populations, diploid = F)\r\nbav_all_popgen<-diversity.stats(bav_all_popgen, pi = TRUE)\r\nbav_all_popgen<-F_ST.stats(bav_all_popgen, mode = ""nucleotide"")\r\nbav_all_popgen<-F_ST.stats(bav_all_popgen, mode = ""haplotype"")\r\nbav_all_popgen<-neutrality.stats(bav_all_popgen)\r\nfst_bav_all <- t(bav_all_popgen@nuc.F_ST.pairwise)\r\nnd_bav_all <- bav_all_popgen@nuc.diversity.within/bav_all_popgen@n.sites\r\ndxy_bav_all <-get.diversity(bav_all_popgen, between = T)[[2]]/bav_all_popgen@n.sites\r\ndxy_bav_all\r\n\r\n#garinii#\r\ngar_all_popgen<-readData(path = ""/garinii"", #add in full file name to folder contiaing the allchrom_gar aligned fasta file#\r\n                         format = ""fasta"",\r\n                         include.unknown = TRUE)\r\ngarinii_info <- read.delim(""garinii_pops.txt"", sep = ""\\t"")\r\npopulations<- split(garinii_info$ind, garinii_info$pop)\r\ngar_all_popgen<-set.populations(gar_all_popgen, populations, diploid = F)\r\ngar_all_popgen@populations\r\ngar_all_popgen<-diversity.stats(gar_all_popgen, pi = TRUE)\r\ngar_all_popgen<-F_ST.stats(gar_all_popgen, mode = ""nucleotide"")\r\ngar_all_popgen<-F_ST.stats(gar_all_popgen, mode = ""haplotype"")\r\nfst_gar_all <- t(gar_all_popgen@nuc.F_ST.pairwise)\r\ndxy_gar_all <-get.diversity(gar_all_popgen, between = T)[[2]]/gar_all_popgen@n.sites\r\ndxy_gar_all\r\n', '#Analysis of PFam+ Results for Japanese isolates manuscript#\r\n#Robert E.Rollins - 30.11.20#\r\n\r\n#set working directory to .csv location#\r\nsetwd(""E:/PhD/HostVectorAdaptation/Manuscripts/JapaneseSamplesEvolution/partitioning_genes"")\r\n\r\n#load necessary libraries for analysis#\r\nlibrary(""lme4"")\r\nlibrary(""ggplot2"")\r\nlibrary(""arm"")\r\ndispersion_glmer<- function(mod)\r\n{ n <- length(resid(mod))\r\nreturn( sqrt( sum(c(resid(mod), mod@u)^2) / n ) )}\r\nlibrary(""magrittr"")\r\nlibrary(""dplyr"")\r\nlibrary(""ggpubr"")\r\nlibrary(""factoextra"")\r\nlibrary(""purrr"")\r\nlibrary(""ggalt"")\r\n\r\n#load data frame of PFam presence absence for all strains#\r\npfamall<- read.csv(""Pfam_Analysis_forR.csv"", sep = "";"")\r\nstr(pfamall)\r\npfam_fil<- pfamall[is.na(pfamall$Souce) == F,]\r\nhist(pfam_fil$Pfam)\r\n\r\npfamall_mod<-lmer(Pfam ~ 1 + \r\n                     Geo + \r\n                     Souce +\r\n                     (1|Species),\r\n                   data = pfam_fil)\r\n\r\n\r\nsummary(pfamall_mod)\r\npar(mfrow = c(2,2))\r\nhist(resid(pfamall_mod), main = ""Distribution of Residuals"")\r\nacf(resid(pfamall_mod), main = ""ACF Plot"")\r\nscatter.smooth(fitted(pfamall_mod),resid(pfamall_mod), main = ""Tukey-Ascombe Plot"",\r\n               xlab = ""Fitted Values"",\r\n               ylab = ""Residual Values"")\r\nqqnorm(resid(pfamall_mod))\r\nqqline(resid(pfamall_mod))\r\n\r\npfamall_mod_sim<-sim(pfamall_mod, n.sim = 5000)\r\n\r\napply(pfamall_mod_sim@fixef, 2, mean) \r\napply(pfamall_mod_sim@fixef, 2, quantile, prob=c(0.025, 0.975))\r\nquantile(pfamall_mod_sim@sigma, prob=c(0.025, 0.5, 0.975))\r\n\r\n#t-tests between individual species#\r\nttest_bav<-t.test(pfam_fil[pfam_fil$Species== ""Borrelia bavariensis"" & pfam_fil$Geo == ""Asia"",]$Pfam,\r\n                  pfam_fil[pfam_fil$Species== ""Borrelia bavariensis"" & pfam_fil$Geo == ""Europe"",]$Pfam,\r\n                  alternative = c(""two.sided""),\r\n                  paired = FALSE)\r\n\r\nttest_bav\r\n\r\nttest_gar<-t.test(pfam_fil[pfam_fil$Species== ""Borrelia garinii"" & pfam_fil$Geo == ""Asia"",]$Pfam,\r\n                  pfam_fil[pfam_fil$Species== ""Borrelia garinii"" & pfam_fil$Geo == ""Europe"",]$Pfam,\r\n                  alternative = c(""two.sided""),\r\n                  paired = FALSE)\r\n\r\nttest_gar\r\n\r\nttest_afz<-t.test(pfam_fil[pfam_fil$Species== ""Borrelia afzelii"" & pfam_fil$Geo == ""Asia"",]$Pfam,\r\n                  pfam_fil[pfam_fil$Species== ""Borrelia afzelii"" & pfam_fil$Geo == ""Europe"",]$Pfam,\r\n                  alternative = c(""two.sided""),\r\n                  paired = FALSE)\r\nttest_afz\r\n\r\nttest_gartoafz<-t.test(pfam_fil[pfam_fil$Species== ""Borrelia garinii"",]$Pfam,\r\n               pfam_fil[pfam_fil$Species== ""Borrelia afzelii"",]$Pfam,\r\n               alternative = c(""two.sided""),\r\n               paired = FALSE)\r\np.adjust(ttest_gartoafz$p.value, ""bonferroni"")\r\nttest_gartoafz$p.value\r\n\r\nttest_gartobav<-t.test(pfam_fil[pfam_fil$Species== ""Borrelia garinii"" & pfam_fil$Geo == ""Europe"",]$Pfam,\r\n                       pfam_fil[pfam_fil$Species== ""Borrelia bavariensis"" & pfam_fil$Geo == ""Europe"",]$Pfam,\r\n                       alternative = c(""two.sided""),\r\n                       paired = FALSE)\r\n\r\np.adjust(ttest_gartobav$p.value, ""bonferroni"")\r\nttest_gartobav$p.value\r\n\r\n\r\n#MDS analysis for binary presence/absence of plasmids in each species#\r\nall_plasmids<-read.csv(""pfams_binary_all.csv"", sep = "";"", row.names = 1)\r\nset.seed(123)\r\n\r\n#calculate classic MDS, with clustering (kmeans)#\r\n#all\r\npar(mfrow = c(1,1))\r\nmds_all <- all_plasmids[,4:32] %>% dist() %>% cmdscale() %>% as_tibble()\r\ncolnames(mds_all) <- c(""Dimension_1"", ""Dimension_2"")\r\n\r\n# Plot mds_all\r\nggscatter(mds_all, x = ""Dimension_1"", y = ""Dimension_2"", \r\n          label = rownames(all_plasmids),\r\n          size = 1,\r\n          repel = TRUE)\r\n\r\n# function to compute total within-cluster sum of square \r\nwss_all <- function(k) {\r\n  kmeans(mds_all, k, nstart = 10 )$tot.withinss\r\n}\r\n\r\n# Compute and plot wss for k = 1 to k = 15\r\nk.values <- 1:10\r\n\r\n# extract wss for 2-15 clusters\r\nwss_all_values <- map_dbl(k.values, wss_all)\r\n\r\nplot(k.values, wss_all_values,\r\n     type=""b"", pch = 19, frame = FALSE, \r\n     xlab=""Number of clusters K"",\r\n     ylab=""Total within-clusters sum of squares"")\r\n\r\n\r\n\r\nclust <- kmeans(mds_all, 3)$cluster %>%\r\n  as.factor()\r\nmds_all <- mds_all %>%\r\n  mutate(groups = clust) %>%\r\n  mutate(names = rownames(all_plasmids))%>%\r\n  mutate(Origin = all_plasmids$Origin)%>%\r\n  mutate(Species = all_plasmids$Species)%>%\r\n  mutate(Source = all_plasmids$Souce)\r\nmds_all <- mds_all[is.na(mds_all$Source) == F,]\r\n\r\n#Plot with encircling Asian Samples#\r\nggplot(mds_all, aes(x= Dimension_1, y= Dimension_2)) + \r\n  geom_encircle(aes(fill=Species), data = mds_all[mds_all$Origin == ""Asia"",], s_shape = 1, expand = 0,\r\n                alpha = 0.4, color = ""black"", show.legend = FALSE)+\r\n  geom_encircle(aes(fill=Species), s_shape = 1, expand = 0,\r\n                alpha = 0.1, color = ""black"", show.legend = FALSE)+\r\n  geom_point(aes(shape = Species), cex =3, position = ""jitter"")+\r\n   scale_fill_manual(values = c(""azure4"", ""azure4"", ""azure4""))+\r\n  scale_shape_manual(values = c(15, 16, 17))+\r\n  ylab(""Dimension 2"") +\r\n  xlab(""Dimension 1"") +\r\n  theme(panel.grid.major = element_blank(), \r\n        panel.grid.minor = element_blank(),\r\n        panel.background = element_blank(),\r\n        axis.line.y = element_line(colour = ""black"", arrow = arrow(angle = 45, length = unit(5, ""pt""))),\r\n        axis.line.x = element_line(colour = ""black"", arrow = arrow(angle = 45, length = unit(5, ""pt""))),\r\n        text = element_text(size = 20),\r\n        axis.text.x = element_text(size = 15),\r\n        axis.text.y = element_text(size = 15),\r\n        axis.title.y = element_text(vjust = 3),\r\n        axis.title.x = element_text(vjust = -3),\r\n        plot.margin = margin(t = 30, r =30, b =30, l = 40, unit = ""pt""))\r\n\r\n#Plot with encircling European Samples#\r\nggplot(mds_all, aes(x= Dimension_1, y= Dimension_2)) + \r\n  geom_encircle(aes(fill=Species), data = mds_all[mds_all$Origin == ""Europe"",], s_shape = 1, expand = 0,\r\n                alpha = 0.4, color = ""black"", show.legend = FALSE)+\r\n  geom_encircle(aes(fill=Species), s_shape = 1, expand = 0,\r\n                alpha = 0.1, color = ""black"", show.legend = FALSE)+\r\n  geom_point(aes(shape = Species), cex =3, position = ""jitter"")+\r\n  scale_fill_manual(values = c(""azure4"", ""azure4"", ""azure4""))+\r\n  scale_shape_manual(values = c(15, 16, 17))+\r\n  ylab(""Dimension 2"") +\r\n  xlab(""Dimension 1"") +\r\n  theme(panel.grid.major = element_blank(), \r\n        panel.grid.minor = element_blank(),\r\n        panel.background = element_blank(),\r\n        axis.line.y = element_line(colour = ""black"", arrow = arrow(angle = 45, length = unit(5, ""pt""))),\r\n        axis.line.x = element_line(colour = ""black"", arrow = arrow(angle = 45, length = unit(5, ""pt""))),\r\n        text = element_text(size = 20),\r\n        axis.text.x = element_text(size = 15),\r\n        axis.text.y = element_text(size = 15),\r\n        axis.title.y = element_text(vjust = 3),\r\n        axis.title.x = element_text(vjust = -3),\r\n        plot.margin = margin(t = 30, r =30, b =30, l = 40, unit = ""pt""))\r\n\r\n']","Out of Asia? Expansion of Eurasian Lyme borreliosis causing genospecies display unique evolutionary trajectories Vector-borne pathogens exist in obligate transmission cycles between vector and reservoir host species. Host and vector shifts can lead to geographic expansion of infectious agents and the emergence of new diseases in susceptible individuals. Three bacterial genospecies (Borrelia afzelii, Borrelia bavariensis, and Borrelia garinii) predominantly utilize two distinct tick species as vectors in Asia (Ixodes persulcatus) and Europe (Ixodes ricinus). Through these vectors, the bacteria can infect various vertebrate groups (e.g. rodents, birds) including humans where they cause Lyme borreliosis, the most common vector-borne disease in the Northern hemisphere. Yet, how and in which order the three Borrelia genospecies colonized each continent remains unclear including the evolutionary consequences of this geographic expansion. Here, by reconstructing the evolutionary history of 142 Eurasian isolates, we find evidence that the ancestors of each of the three genospecies likely have an Asian origin. Even so, each genospecies studied displayed a unique sub-structuring and evolutionary response to the colonization of Europe. The pattern of allele sharing between continents is consistent with the dispersal rate of the respective vertebrate hosts, supporting the concept that adaptation of Borrelia genospecies to the host is important for pathogen dispersal. Our results highlight that Eurasian Lyme borreliosis agents are all capable of geographic expansion with host association influencing their dispersal; further displaying the importance of host and vector association to the geographic expansion of vector-borne pathogens and potentially conditioning their capacity as emergent pathogens.",0
"Supplementary material 1 from: Dimitrova M, Senderov VE, Georgiev T, Zhelezov G, Penev L (2021) Infrastructure and Population of the OpenBiodiv Biodiversity Knowledge Graph. Biodiversity Data Journal 9: e67671. https://doi.org/10.3897/BDJ.9.e67671",Comparison between GraphDB identifier look-ups and local MongoDB identifier look-ups,"['library(rdf4r)\r\nlibrary(ropenbio)\r\nlibrary(tictoc)\r\n\r\n#configuring graphdb connection and mongo connection\r\nconfiguration = yaml::yaml.load_file(""/home/backend/OpenBiodiv/local/deployment.yml"")\r\n\r\nobkms = basic_triplestore_access(\r\n  server_url = configuration$server_url,\r\n  repository = configuration$repository,\r\n  user = configuration$user,\r\n  password = configuration$password\r\n)\r\naccess_options = obkms\r\n\r\ngeneral_collection = mongolite::mongo(collection = ""new_collection"", db = ""test"")\r\n\r\n#performing graphdb lookup\r\n#set query\r\ngraph_query = \'PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> select ?s where {     ?s rdfs:label ""M Dimitrova"" }\'\r\n#lookup looped 1000 times\r\ntic(""graphdb"")\r\nfor (n in 1:1000){\r\ngraph_results = rdf4r::submit_sparql(query=graph_query, access_options = obkms)\r\n}\r\ntoc()\r\n#performing mongodb lookup\r\n#set query\r\ntype = ""author""\r\nvalue = ""M Dimitrova""\r\nhash = ropenbio::set_values_to_sha256(type, value)\r\nmongo_query = sprintf(""{\\""hash\\"": \\""%s\\""}"", hash)\r\n#lookup looped 1000 times\r\ntic(""mongodb"")\r\nfor (n in 1:1000){\r\ngeneral_collection$find(mongo_query)\r\n}\r\ntoc()\r\n\r\n']","Supplementary material 1 from: Dimitrova M, Senderov VE, Georgiev T, Zhelezov G, Penev L (2021) Infrastructure and Population of the OpenBiodiv Biodiversity Knowledge Graph. Biodiversity Data Journal 9: e67671. https://doi.org/10.3897/BDJ.9.e67671 Comparison between GraphDB identifier look-ups and local MongoDB identifier look-ups",0
Bioinformatic pipeline from: Increasing confidence for discerning species and population compositions from metabarcoding assays of environmental samples: case studies of fishes in the Laurentian Great Lakes and Wabash River,"Community composition data are essential for conservation management, facilitating identification of rare native and invasive species, along with abundant ones. However, traditional capture-based morphological surveys require considerable taxonomic expertise, are time consuming and expensive, can kill rare taxa and damage habitats, and often are prone to false negatives. Alternatively, metabarcode assays can be used to assess the genetic identity and compositions of entire communities from environmental samples, comprising a more sensitive, less damaging, and relatively time- and cost-efficient approach. However, there is a trade-off between the stringency of bioinformatic filtering needed to remove false positives and the potential for false negatives. The present investigation thus evaluated use of four mitochondrial (mt) DNA metabarcode assays and a customized bioinformatic pipeline to increase confidence in species identifications by removing false positives, while achieving high detection probability. Positive controls were used to calculate sequencing error, and results that fell below those cutoff values were removed, unless found with multiple assays. The performance of this approach was tested to discern and identify North American freshwater fishes using lab experiments (mock communities and aquarium experiments) and processing of a bulk ichthyoplankton sample. The method then was applied to field environmental (e)DNA water samples taken concomitant with electrofishing surveys and morphological identifications. This protocol detected 100% of species present in concomitant electrofishing surveys in the Wabash River and an additional 21 that were absent from traditional sampling. Using single 1 L water samples collected from just four locations, the metabarcoding assays discerned 73% of the total fish species that were discerned in comparison to four months of an extensive electrofishing river survey in the Maumee River, along with an additional nine species. In both rivers, total fish species diversity was best resolved when all four metabarcode assays were used together, which identified 35 additional species missed by electrofishing. Ecological distinction and diversity levels among the fish communities also were better resolved with the metabarcode assays than with morphological sampling and identifications, especially with the combined assays. At the population-level, metabarcode analyses targeting the invasive round goby Neogobius melanostomus and the silver carp Hypophthalmichthys molitrix identified all population haplotype variants found using Sanger sequencing of morphologically sampled fish, along with additional intra-specific diversity, meriting further investigation. Overall findings demonstrated that the use of multiple metabarcode assays and custom bioinformatics that filter potential error from true positive detections improves confidence in evaluating biodiversity.","['#BiocManager::install(""dada2"")\nlibrary(dada2); packageVersion(""dada2"")\n\npath <- ""~""#/Volumes/matty/MiSeqData/GMCFastqs/GMCFastqsTrimmedFastqs/"" # CHANGE ME to the directory containing the fastq files after unzipping.\nlist.files(path)\n# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq\nfnFs <- sort(list.files(path, pattern=""_R1_001.fastq"", full.names = TRUE))\nfnRs <- sort(list.files(path, pattern=""_R2_001.fastq"", full.names = TRUE))\n# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq\nsample.names <- sapply(strsplit(basename(fnFs), ""_""), `[`, 1)\nsample.names\nplotQualityProfile(fnFs[1:10])\nplotQualityProfile(fnRs[1:10])\n# Place filtered files in filtered/ subdirectory\nfiltFs <- file.path(path, ""filtered"", paste0(sample.names, ""_F_filt.fastq.gz""))\nfiltRs <- file.path(path, ""filtered"", paste0(sample.names, ""_R_filt.fastq.gz""))\n# Change trunclen to length based on plotQualityProfile\nout <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(150,150),\n                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,\n                     compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE\nout\n\n#Stop and check that a high proportion of reads passed filtering. If not, change settings.\n\nerrF <- learnErrors(filtFs, multithread=TRUE)\nerrR <- learnErrors(filtRs, multithread=TRUE)\nplotErrors(errF, nominalQ=TRUE)\nplotErrors(errR, nominalQ=TRUE)\nderepFs <- derepFastq(filtFs, verbose=TRUE)\nderepRs <- derepFastq(filtRs, verbose=TRUE)\n# Name the derep-class objects by the sample names\nnames(derepFs) <- sample.names\nnames(derepRs) <- sample.names\ndadaFs <- dada(derepFs, err=errF, multithread=TRUE)\ndadaRs <- dada(derepRs, err=errR, multithread=TRUE)\n# dadaFs[[1]]\nmergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)\n# Inspect the merger data.frame from the first sample\nhead(mergers[[1]])\nseqtab <- makeSequenceTable(mergers)\ndim(seqtab)\n# Inspect distribution of sequence lengths\ntable(nchar(getSequences(seqtab)))\nseqtab.nochim <- removeBimeraDenovo(seqtab, method=""consensus"", multithread=TRUE, verbose=TRUE)\ndim(seqtab.nochim)\nsum(seqtab.nochim)/sum(seqtab)\ntable(nchar(getSequences(seqtab.nochim)))\ngetN <- function(x) sum(getUniques(x))\n#Multiple samples\ntrack <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))\n# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)\n# track <- cbind(out, getN(dadaFs), getN(dadaRs), getN(mergers), rowSums(seqtab.nochim))\ncolnames(track) <- c(""input"", ""filtered"", ""denoisedF"", ""denoisedR"", ""merged"", ""nonchim"")\nrownames(track) <- sample.names\nhead(track)\ntrack\n\n\n#Write Seq Tab to File\nwrite.table(track, ""/Volumes/matty/MiSeqData/GMCFastqs/track.txt"", append = FALSE, sep = \'\\t\')\nwrite.table(seqtab.nochim, ""/Volumes/matty/MiSeqData/GMCFastqs/SeqTab.txt"", append = FALSE, sep = ""\\t"")\n\n#to inspect fastqs\nfn <- ""~""#/Volumes/matty/MiSeqData/Run190303Methods-2/GFL/GFLTrimmedFastqs2/fBKZ12_R2_001.fastq""\nfoo <- readLines(fn)\nseqlens <- sapply(foo[seq(2,length(foo),4)], nchar)\nquallens <- sapply(foo[seq(4,length(foo),4)], nchar)\ntable(seqlens); table(quallens)\nquallens != seqlens\n']","Bioinformatic pipeline from: Increasing confidence for discerning species and population compositions from metabarcoding assays of environmental samples: case studies of fishes in the Laurentian Great Lakes and Wabash River Community composition data are essential for conservation management, facilitating identification of rare native and invasive species, along with abundant ones. However, traditional capture-based morphological surveys require considerable taxonomic expertise, are time consuming and expensive, can kill rare taxa and damage habitats, and often are prone to false negatives. Alternatively, metabarcode assays can be used to assess the genetic identity and compositions of entire communities from environmental samples, comprising a more sensitive, less damaging, and relatively time- and cost-efficient approach. However, there is a trade-off between the stringency of bioinformatic filtering needed to remove false positives and the potential for false negatives. The present investigation thus evaluated use of four mitochondrial (mt) DNA metabarcode assays and a customized bioinformatic pipeline to increase confidence in species identifications by removing false positives, while achieving high detection probability. Positive controls were used to calculate sequencing error, and results that fell below those cutoff values were removed, unless found with multiple assays. The performance of this approach was tested to discern and identify North American freshwater fishes using lab experiments (mock communities and aquarium experiments) and processing of a bulk ichthyoplankton sample. The method then was applied to field environmental (e)DNA water samples taken concomitant with electrofishing surveys and morphological identifications. This protocol detected 100% of species present in concomitant electrofishing surveys in the Wabash River and an additional 21 that were absent from traditional sampling. Using single 1 L water samples collected from just four locations, the metabarcoding assays discerned 73% of the total fish species that were discerned in comparison to four months of an extensive electrofishing river survey in the Maumee River, along with an additional nine species. In both rivers, total fish species diversity was best resolved when all four metabarcode assays were used together, which identified 35 additional species missed by electrofishing. Ecological distinction and diversity levels among the fish communities also were better resolved with the metabarcode assays than with morphological sampling and identifications, especially with the combined assays. At the population-level, metabarcode analyses targeting the invasive round goby Neogobius melanostomus and the silver carp Hypophthalmichthys molitrix identified all population haplotype variants found using Sanger sequencing of morphologically sampled fish, along with additional intra-specific diversity, meriting further investigation. Overall findings demonstrated that the use of multiple metabarcode assays and custom bioinformatics that filter potential error from true positive detections improves confidence in evaluating biodiversity.",0
"Van Dijk et al. (2021), A meta-analysis of projected global food demand and population at risk of hunger for the period 20102050, data and scripts","This repository contains all data and R scripts to reproduce the figures in Van Dijk et al. (2021), A meta-analysis of global food demand and population at risk of hunger projections for the period 2010-2050, Nature Food. More specifically, it includes two databases: (1) A database with standardized information to describe the characteristics of 57 studies that were identified by the systematic literature review and (2) The Global Food Security Projections Database v1.0.1 with harmonized projections for three global food security indicators: food consumption in kcal per capita and total kcal, and population at risk of hunger. The database also includes projections for total global population that are required to derive the global food security indicators.The two scripts (nf_figures.r and nf_meta_regression.r) can be used to reproduce the figures and tables in the main paper and the supplementary information. Please start with the first script, which sources the second script. This is the first version of the Global Food Projections Database. We expect to update the data, including additional studies and variables in the future. For issues and suggestions, please contact michiel.vandijk@wur.nl.",,"Van Dijk et al. (2021), A meta-analysis of projected global food demand and population at risk of hunger for the period 20102050, data and scripts This repository contains all data and R scripts to reproduce the figures in Van Dijk et al. (2021), A meta-analysis of global food demand and population at risk of hunger projections for the period 2010-2050, Nature Food. More specifically, it includes two databases: (1) A database with standardized information to describe the characteristics of 57 studies that were identified by the systematic literature review and (2) The Global Food Security Projections Database v1.0.1 with harmonized projections for three global food security indicators: food consumption in kcal per capita and total kcal, and population at risk of hunger. The database also includes projections for total global population that are required to derive the global food security indicators.The two scripts (nf_figures.r and nf_meta_regression.r) can be used to reproduce the figures and tables in the main paper and the supplementary information. Please start with the first script, which sources the second script. This is the first version of the Global Food Projections Database. We expect to update the data, including additional studies and variables in the future. For issues and suggestions, please contact michiel.vandijk@wur.nl.",1
Data from: Variability in commercial demand for tree saplings affects the probability of introducing exotic forest diseases,"1.Several devastating forest pathogens are suspected or known to have entered the UK through imported planting material. The nursery industry is a key business of the tree trade network. Variability in demand for trees makes it difficult for nursery owners to predict how many trees to produce in their nursery. When in any given year, the demand for trees is larger than the production, nursery owners buy trees from foreign sources to match market demand. These imports may introduce exotic diseases.2.We have developed a model of the dynamics of plant production linked to an economic model to quantify the effect of demand variability on the risk of introducing an exotic disease.3.We find that-when the cost of producing a tree in a UK nursery is considerably smaller than the cost of importing a tree (in the example presented, less than half the importing cost), the risk of introducing an exotic disease is hardly affected by an increase in demand variability.-when the cost of producing a tree in the nursery is smaller than, but not very different from the cost of importing a tree, the risk of importing exotic diseases increases with increasing demand variability.4.Synthesis and implications. Our results suggest that a balanced management of demand variability and costs can reduce the risk of importing an exotic forest disease according to the management strategy adopted.","['\r\n########################################################################################################################################################\r\n# This program describes the dynamics of a tree nursery selling trees that undergo 4 growth stages before commercialisation. The trees grow from seeds #\r\n# to medium trees. Then it calculates the value of several economic indicators dependent on demand variability over time.                                       #\r\n########################################################################################################################################################\r\n\r\nYear <- numeric()\r\nTotalYears <- 44 #Total number of years the simulations runs over\r\n\r\n# create arrays called for economic indicators\r\n\r\nseed<-array(NA,c(TotalYears+1,4))            # number of seeds planted each year\r\ndemand <- array(NA,c(TotalYears+1,4))        # demand of trees of each stage every year \r\npopulation<-array(NA,c(TotalYears+1,4))      # age structured tree grown at each year where N(t+1)=L*N(t)\r\nproduction<-array(NA, c(TotalYears+1,4))     # trees produced minus sales per year \r\npop3Sdemand<-array(NA, c(TotalYears+1,4))    # demand at stage 3\r\npop4Sdemand<-array(NA, c(TotalYears+1,4))    # demand at stage 4\r\nimports <- array(NA,c(TotalYears+1,4))       # imports of trees of each stage per year\r\ncost <- array(NA,c(TotalYears+1,4))          # total costs per year\r\nprofit <- array(NA,c(TotalYears+1,4))        # total gross margin per year\r\ndiseaseProb <- array(NA,c(TotalYears+1,4))   # probability of importing a disease\r\n\r\nmean.demandS3 <- 2860  \r\nmean.demandS4 <- 715 \r\nseed.pop <- 3600  #number of seed planted each year\r\n\r\np<-0.0001 # we assume one in 10000 of foreign trees has a disease\r\n# demand.var <-50   #demand variability\r\n\r\n# Forestry commission newly planted oak trees from 1976-2017 (estimate in thousand trees)\r\n\r\ndemand.varS3 <- 0.8*c(0,0,0,450,450,390,480,600,600,570,690,750,810,1050,1440,1980,1950,3090,3510,3450,4410,5010,4320,3720,4020,3870,3960,4380,5070,3870,3720,3660,3870,3330,3690,3210,2790,2550,3300,3780,3690,4470,4410,2190,2010)\r\ndemand.varS4 <- 0.2*c(0,0,0,450,450,390,480,600,600,570,690,750,810,1050,1440,1980,1950,3090,3510,3450,4410,5010,4320,3720,4020,3870,3960,4380,5070,3870,3720,3660,3870,3330,3690,3210,2790,2550,3300,3780,3690,4470,4410,2190,2010)\r\n\r\n# Initial values\r\n\r\nseed[1,] <- c(seed.pop,0,0,0)             \r\ndemand[1,] <- c(0,0,0,0) \r\npopulation[1,] <- c(seed.pop,0,0,0) \r\npop3Sdemand[1,] <- c(0,0,0,0) \r\npop4Sdemand[1,] <- c(0,0,0,0) \r\nimports[1,] <- pmax(0,demand[1,]-population[1,])\r\nproduction[1,] <- c(seed.pop,0,0,0)\r\ncost[1,] <- rbind(0.035,0,0,0)*(seed[1,])+rbind(0.0,0.0,0.045,0.0)*(production[1,]) + rbind(0,0,0.12,0.15)*(pmax(0,imports[1,]))+ rbind(0,0,0.05,0.1)*(pmax(0,thrown[1,]))\r\npop_sale[1, ]<-c(0,0,0,0)\r\nprofit[1,] <- rbind(0,0,0.2,0.25)*demand[1,] - cost[1,]\r\ndiseaseProb[1,] <- (1-((1-p)^{imports[1,]})) \r\n\r\nqij<-1 #percentage of trees moving to the next stage*survival percentage  \r\nqii<-0 #percentage of trees staying in the same stage\r\nq44<-0 #determines percentage kept after a sale\r\n\r\nLefkovitch <- matrix(c(qii,0,0,0, 0.9,qii,0,0, 0,0.8,qii,0, 0,0,0.25,q44), nrow = 4, ncol = 4, byrow = TRUE) #0.25 of 0.8 is equivalent to 0.2 of 1\r\n\r\nfor(Year in 1:TotalYears){\r\n  \r\n  seed[Year+1,]<- c(seed.pop,0,0,0) #number of seed planted each year is constant\r\n  \r\n  #variability in the demand is introduced through a uniform or normal distribution with random variability in each stage (can be modified to other distributions)\r\n  #demand[Year+1,] <- c(0,0,0,(max(0,runif(1,mean.demandS4-demand.var,mean.demandS4+demand.var))),0) #uniform distribution\r\n  #demand[Year+1,] <- c(0,0,0,(max(0,rnorm(1,mean.demandS4,(0.75*demand.var))))) #normal distribution\r\n  \r\n  pop3Sdemand[Year+1,]<-c(0,0,demand.varS3[Year+1],0)  #demand from stage 3\r\n  pop4Sdemand[Year+1,]<-c(0,0,0,demand.varS4[Year+1])  #demand from stage 4\r\n  \r\n  demand[Year+1,] <-c(0,0,pop3Sdemand[Year+1,3],pop4Sdemand[Year+1,4])\r\n  population[Year+1,] <- pmax(0,(Lefkovitch)%*%(population[Year,]) + seed[Year+1,]) \r\n  production[Year+1,]<-pmax(0,population[Year+1,]-demand[Year+1,])  \r\n  imports[Year+1,]<-pmax(0, demand[Year+1,]-population[Year+1,])\r\n  diseaseProb[Year+1,] <- 1-((1-p)^{imports[Year+1,]})\r\n  cost[Year+1,] <- rbind(0.035,0,0,0)*(seed[Year+1,])+rbind(0.0,0.0,0.065,0.0)*(population[Year+1,]) + rbind(0,0,0.15,0.2)*(pmax(0,imports[Year+1,]))#+ rbind(0,0,0.04,0.075)*(pmax(0,thrown[Year,]))\r\n  profit[Year+1,] = rbind(0,0,0.4,0.45)*demand[Year+1,] - cost[Year+1,]\r\n\r\n}\r\n\r\n\r\n\r\n', '######################################################################################################################\r\n# This program describes the dynamics of an economic model for a tree nursery with trees moving along 4 stages, from #\r\n# seeds to medium trees. It calculates the average gross margin and the probability of introducing a forest disease  #\r\n# dependent on planting rate and demand variability of trees in stage 4, assuming sales in stage 4 only. This can    #\r\n# be modified to include Stage 3. Here we use data of newly planted oak trees from the Forestry Commission as an     #\r\n# indicator for tree planting rates and approximate costs of produced and imported saplings from nursery growers.    #                                                                         #\r\n######################################################################################################################\r\n\r\n### Initial conditions ###\r\n\r\nYear <- numeric()\r\nTotalYears <- 45 # Total number of years the simulation runs over\r\n\r\nn <- numeric()\r\nTotalRuns <- 100 # Number of runs to obtain average dynamics\r\n\r\nM<- numeric()\r\nMseq = seq(400,1500,10) # Range of planting rates (thousand trees)\r\n\r\nd <- numeric()\r\ndemand.var <- seq(0,500,5) # Range of demand variability (thousand trees)\r\n\r\nk <- numeric()\r\nTotalK <- 0.15 #production costs\r\n\r\np <- 0.0001\r\n\r\nStages <- 4\r\n\r\n### create arrays for system dynamics ###\r\n\r\n### Gross margin ###\r\nn_ProfitMean <- array(NA,c(TotalYears+1,Stages)) # Gross margin mean of each seed planting rate for all the runs i.e. dataset of TotalYears*Stages\r\nn_StageMean <- array(NA,c(length(Mseq),Stages))  # Gross margin mean of each stage after n runs i.e. vector of length(Stages) \r\nM_StageMean <- array(NA,c(length(Mseq),Stages))  # Gross margin mean of each stage after n runs for all planting rates i.e. a dataset of length(Mseq)*Stages\r\n\r\n### Costs ###\r\nn_CostMean <- array(NA,c(TotalYears+1,Stages)) \r\nn_StageCostMean <- array(NA,c(length(Mseq),Stages)) \r\nM_StageCostMean <- array(NA,c(length(Mseq),Stages)) \r\n\r\n### Imports ###\r\nn_ImportsMean <- array(NA,c(TotalYears+1,Stages)) \t \r\nn_StageImportsMean <- array(NA,c(length(Mseq),Stages)) \r\nM_StageImportsMean <- array(NA,c(length(Mseq),Stages)) \r\n\r\n### Probabilities of importing a disease ###\r\nn_Prob <- array(NA,c(TotalYears+1,Stages)) \t\t\r\nn_StageProbMean <- array(NA,c(length(Mseq),Stages)) \r\nM_Prob<- array(NA,c(length(Mseq),Stages))\t\t\r\n\r\n### Final calculated values ###\r\nAllStageProfit <- array(NA,c(length(Mseq),Stages)) # Gross margin sum of means of all stages after n runs for each planting rate and fixed demand variability\r\nAllStageCost <- array(NA,c(length(Mseq),Stages)) \r\nAllStageImports <- array(NA,c(length(Mseq),Stages)) \r\nAllStageProbs <- array(NA,c(length(Mseq),Stages))  \r\n\r\n### Create lists of variables for each planting rate and demand variability value ###\r\nProfit_var <- numeric() \r\nCost_var <- numeric()\r\nImports_var <- numeric()\r\nProb_var <- numeric()\t \r\n\r\n### create arrays of tree grownt and economic factors ###\r\nseed <- array(NA,c(TotalYears+1,Stages,TotalRuns))         # number of seeds planted for a set of runs over time. A vector of (N,0,...,0)\r\ndemand <- array(NA,c(TotalYears+1,Stages,TotalRuns))       # demand of trees of each class every year.\r\npopulation <- array(NA,c(TotalYears+1,Stages,TotalRuns))   # population is the age structured population at each year so that N(t+1)=L*N(t)\r\nproduction<-array(NA,c(TotalYears+1,Stages,TotalRuns))     # production is the age structured population at each year undergoing management\r\n#pop3Sdemand <- array(NA,c(TotalYears+1,Stages,TotalRuns))  # variable demand of trees in stage3 every year\r\npop4Sdemand <- array(NA, c(TotalYears+1,Stages,TotalRuns)) # variable demand of trees in stage4 every year\r\nimports <- array(NA,c(TotalYears+1,Stages,TotalRuns))      # imports of trees of each class every year\r\ncost <- array(NA,c(TotalYears+1,Stages,TotalRuns))         # total costs (production, imports and stock holding) over time\r\nprofit <- array(NA,c(TotalYears+1,Stages,TotalRuns))       # total profit over time   \r\npop_sale <- array(NA,c(TotalYears+1,Stages,TotalRuns))     # trees left after a sale\r\ndiseaseProb <- array(NA,c(TotalYears+1,Stages,TotalRuns))  # probability of importing a disease \r\n\r\nmax_profit <- array(NA,c(length(demand.var),Stages))   #maximum profit for each demand variability value over all planting rates\r\nopt_planting <- array(NA,c(length(demand.var),Stages)) #to which planting rate the maximum profit belongs\r\n\r\n\r\n### Leftkovitch tree growth model: age structured matrix ###\r\n\r\nT_BS <- 1.0\r\nT_SL <- 1.0\r\nT_LM <- 1.0\r\nT_MM <- 1.0\r\n\r\nA_B <- 1.0\r\nA_S <- 1.0\r\nA_L <- 1.0\r\nA_M <- 1.0\r\n\r\nTransition <- matrix(c(0,0,0,0, T_BS,0,0,0, 0,T_SL,0,0, 0,0,T_LM,0), nrow = 4, ncol = 4, byrow = TRUE)\r\nNoTransition <- matrix(c(1-T_BS,0,0,0, 0,1-T_SL,0,0, 0,0,1-T_LM,0, 0,0,0,1-T_MM), nrow = 4, ncol = 4, byrow = TRUE)\r\nSurvival <-matrix(c(A_B,0,0,0, 0,A_S,0,0, 0,0,A_L,0, 0,0,0,A_M),nrow = 4, ncol = 4, byrow = TRUE)\r\nLefkovitch <-(Transition+NoTransition)%*%Survival\r\nLefk_Trans <-(Transition)%*%Survival\r\nLefk_NoTrans <-(NoTransition)%*%Survival \r\n\r\n### mean demands ###\r\n\r\n#mean.demandS3 <- 4200  # mean demand stage4/year\r\nmean.demandS4 <- 1000   # mean demand stage4/year\r\nseed.pop <- 1000        # number of planted seeds/year\r\n\r\n################################################## Running ################################################\r\n\r\n#Runs for demand variability sequence\r\nfor(d in 1:length(demand.var)){\r\n  \r\n  #Runs for different planting rates\r\n  for(M in 1:length(Mseq)){\r\n    \r\n    #Runs economic variables n times to obtain a mean economics dynamics\r\n    for(n in 1:TotalRuns){\r\n      \r\n      #Runs economic variables over a total number years\r\n      for(Year in 1:TotalYears){\r\n        \r\n        \r\n        ### initial values\r\n        seed[1,,n] <- Mseq[M]*c(1,0,0,0)        \r\n        demand[1,,n] <- c(0,0,0,mean.demandS4)\r\n        population[1,,n] <- Mseq[M]*c(1,0,0,0) \r\n        pop_sale[1,,n] <- c(0,0,0,0)\r\n        production[1,,n]<-Mseq[M]*c(1,0,0,0)\r\n        pop4Sdemand[1,,n] <- c(0,0,0,mean.demandS4)\r\n        #pop3Sdemand[1,,n] <- c(0,0,mean.demandS3,0)\r\n        imports[1,,n] <- c(0,0,0,mean.demandS4)\r\n        cost[1,,n] <- 0.5*(rbind(0.015,0,0,0)*(seed[1,,n])+rbind(0.005,0.02,0.035,0.0)*(production[1,,n])) + rbind(0,0,0,0.15)*(pmax(0,imports[1,,n]))\r\n        profit[1,,n] <-rbind(0,0,0,0.25)*demand[1,,n] - cost[1,,n]  \r\n        \r\n        ### number of seed planted over time is constant for each set of runs. It increases with the planting rate M after each set of runs\r\n        seed[Year+1,,n] <- Mseq[M]*c(1,0,0,0)\r\n        \r\n        ### variability in the demand is introduced through a uniform distribution (or normal) with random variability in the demand stage ###\r\n        ### Sales can happen in stages 3 and 4 or only in stage 4 ###\r\n        \r\n        #pop3Sdemand[Year+1,,n] <- c(0,0,0,(max(0, runif(1,mean.demandS3-demand.var[d],mean.demandS3+demand.var[d]))))\r\n        #pop4Sdemand[Year+1,,n] <- c(0,0,0,(max(0, rnorm(1,mean.demandS4,(0.75*demand.var[d])))))\r\n        \r\n        pop4Sdemand[Year+1,,n] <- c(0,0,0,(max(0, runif(1,mean.demandS4-demand.var[d],mean.demandS4+demand.var[d]))))\r\n        #pop4Sdemand[Year+1,,n] <- c(0,0,0,(max(0, rnorm(1,mean.demandS4,(0.75*demand.var[d])))))\r\n        \r\n        #demand[Year+1,,n] <- c(0,0,pop3Sdemand[Year+1,3,n],pop4Sdemand[Year+1,4,n])                            \r\n        demand[Year+1,,n] <- c(0,0,0,pop4Sdemand[Year+1,4,n])\r\n        \r\n        ### Population increases each year in the different stages by planting seeds and transitioning stages.\r\n        ### It changes according to the survival of trees in each stage and the probability of trees in each stage moving to the next stage\r\n        population[Year+1,,n] <- pmax(0,(Lefkovitch)%*%(population[Year,,n]) + seed[Year+1,,n]) #population produced before sales\r\n        pop_sale[Year+1,,n] <- pmax(0,pmax(0,(Lefkovitch)%*%(pop_sale[Year,,n])) + seed[Year+1,,n]-demand[Year+1,,n]) #population left after sale\r\n        \r\n        ### Production are the plants produced over time in each stage\r\n        production[Year+1,,n] <- pmax(0,pmax(0,(Lefk_Trans)%*%production[Year,,n]+Lefk_NoTrans%*%pop_sale[Year,,n]+seed[Year+1,,n])-(Lefk_Trans)%*%demand[Year,,n])\r\n        imports[Year+1,,n] <- pmax(0, demand[Year+1,,n]-production[Year+1,,n])\r\n        cost[Year+1,,n] <- 0.5*(rbind(0.015,0,0,0)*(seed[Year+1,,n])+rbind(0.005,0.02,0.035,0.0)*(production[Year+1,,n])) + rbind(0,0,0,0.15)*(pmax(0,imports[Year+1,,n]))\r\n        profit[Year+1,,n] <- rbind(0,0,0,0.25)*demand[Year+1,,n] - cost[Year+1,,n]\r\n        diseaseProb[Year+1,,n] <- 1-((1-p)^{imports[Year+1,,n]})\r\n        \r\n      }\r\n      \r\n      n_ProfitMean <- apply(profit,c(1,2),mean) \r\n      n_StageMean <- colMeans(n_ProfitMean) \r\n      \r\n      n_CostMean <- apply(cost,c(1,2),mean)\r\n      n_StageCostMean <- colMeans(n_CostMean)\r\n      \r\n      n_ImportsMean <- apply(imports[5:TotalYears,,],c(1,2),mean)\r\n      n_StageImportsMean <- colMeans(n_ImportsMean)\r\n      \r\n      n_Prob <- apply(diseaseProb[5:TotalYears,,],c(1,2),mean)\r\n      n_StageProbMean <- colMeans(n_Prob)\r\n      \r\n    }\r\n    \r\n    M_StageMean[M,] <- n_StageMean  \r\n    M_StageCostMean[M, ] <- n_StageCostMean\r\n    M_StageImportsMean[M,] <- n_StageImportsMean \r\n    M_Prob[M,] <- n_StageProbMean \r\n    \r\n  }\r\n  \r\n  AllStageProfit = rowSums(M_StageMean)\r\n  Profit_var = c(Profit_var, AllStageProfit)\r\n  \r\n  \r\n  AllStageCost = rowSums(M_StageCostMean)\r\n  Cost_var = c(Cost_var, AllStageCost)\r\n  \r\n  AllStageImports =rowSums(M_StageImportsMean)\r\n  Imports_var = c(Imports_var,AllStageImports)\r\n  \r\n  AllStageProbs = rowSums(M_Prob)\r\n  Prob_var = c(Prob_var,AllStageProbs)\r\n  \r\n  max_profit[d,]<-apply(M_StageMean, MARGIN=c(2), max)\r\n  opt_planting[d,]<-apply(M_StageMean,MARGIN=c(2),which.max)\r\n  \r\n}\r\n\r\n############################ Differential of profit respect to planting rate #########################\r\n\r\nPROFIT <- matrix(Profit_var,ncol=length(demand.var),nrow=length(Mseq),dimnames=list(Mseq,demand.var))\r\nrow.names<-demand.var\r\ncol.names<-Mseq\r\n\r\nCOST <- matrix(Cost_var,ncol=length(demand.var),nrow=length(Mseq),dimnames=list(Mseq,demand.var))\r\nrow.names<-demand.var\r\ncol.names<-Mseq\r\n\r\nPROFIT_df <- data.frame(x = rep(seq_len(ncol(PROFIT)), each = nrow(PROFIT)),\r\n                        y = rep(seq_len(nrow(PROFIT)), times = ncol(PROFIT)),\r\n                        z = c(PROFIT))\r\n\r\nCOST_df <- data.frame(x = rep(seq_len(ncol(PROFIT)), each = nrow(PROFIT)),\r\n                        y = rep(seq_len(nrow(PROFIT)), times = ncol(PROFIT)),\r\n                        z = c(COST))\r\n\r\n#smooth function out so we can take the differential of profit with respect to planting rate\r\n\r\nrequire(""mgcv"")\r\n\r\n#mod <- gam(z ~ te(x, y), data = PROFIT_df)\r\nmod <- gam(z ~ te(x, y, k=c(25,50)), data = PROFIT_df)\r\nm2 <- matrix(fitted(mod), ncol = length(demand.var))\r\nrequire(""lattice"")\r\nwireframe(m2)\r\na<-apply(m2, 2, diff)\r\n']","Data from: Variability in commercial demand for tree saplings affects the probability of introducing exotic forest diseases 1.Several devastating forest pathogens are suspected or known to have entered the UK through imported planting material. The nursery industry is a key business of the tree trade network. Variability in demand for trees makes it difficult for nursery owners to predict how many trees to produce in their nursery. When in any given year, the demand for trees is larger than the production, nursery owners buy trees from foreign sources to match market demand. These imports may introduce exotic diseases.2.We have developed a model of the dynamics of plant production linked to an economic model to quantify the effect of demand variability on the risk of introducing an exotic disease.3.We find that-when the cost of producing a tree in a UK nursery is considerably smaller than the cost of importing a tree (in the example presented, less than half the importing cost), the risk of introducing an exotic disease is hardly affected by an increase in demand variability.-when the cost of producing a tree in the nursery is smaller than, but not very different from the cost of importing a tree, the risk of importing exotic diseases increases with increasing demand variability.4.Synthesis and implications. Our results suggest that a balanced management of demand variability and costs can reduce the risk of importing an exotic forest disease according to the management strategy adopted.",1
Depth and latitudinal diversity gradients in seamount benthic communities of the South Atlantic,"AimAlthough latitudinal and bathymetric species diversity gradients in the deep sea have been identified and investigated, studies have rarely considered these gradients across seamount and oceanic island ecosystems. This study aimed to identify whether the current understanding of latitudinal and bathymetric gradients in -diversity (species richness) apply to South Atlantic seamount ecosystems, as well as ascertaining whether identifiable trends were present in seamount -diversity along a bathymetric gradient.LocationThe South Atlantic Ocean.Time period2013-2019Major taxa studiedBenthic communities.MethodsDrop camera images from 39 transects, collected between 250 m and 950 m, were used to characterise species richness from within the Exclusive Economic Zones of Ascension Island, Saint Helena and Tristan da Cunha, spanning 8S to 40S. We subsequently applied linear modelling to test possible environmental drivers across latitudinal and bathymetric ranges (see transect metadata for variables). An Analysis of Similarity was employed to investigate the beta-diversity gradient, and the level of species turnover with depth.ResultsSurface primary productivity and substrate hardness both had significant positive effects on species richness, and there was significantly higher species richness at temperate latitudes. No significant relationship between species richness and depth was detected, but there was a significant species turnover with depth.Main conclusionsSeamounts and oceanic islands do not conform to established depth-diversity relationships within the depth range studied. However, despite their isolation and small sizes, seamounts and oceanic islands in the South Atlantic appear to follow latitudinal patterns of deep-sea species richness established for specific taxonomic groups in different ecosystems.","['############### EXPLORATORY ANALYSIS: CONFOUNDING VARIABLES ############### \r\nlibrary(car)\r\nlibrary(ggplot2)\r\nstandardsubsample<-read.csv(""TransectData.csv"", head=TRUE, sep="","")\r\n\r\n# Hardness/OI ANOVA\r\nshapiro.test(standardsubsample$avghardness) # p = 0.135 = assumption upheld\r\nleveneTest(standardsubsample$avghardness ~ standardsubsample$location) # p = 0.3294 = assumption upheld, proceed with ANOVA\r\nhardaov<-aov(standardsubsample$avghardness ~ standardsubsample$location)\r\nsummary(hardaov) # p = 0.09 = no sig diff in avg hardness sampled at each OI\r\n\r\n# Hardness boxplot\r\nhardnessboxplotdata<-standardsubsample\r\nnames(hardnessboxplotdata)[6] <- ""Location""\r\nnames(hardnessboxplotdata)[4] <- ""Avg.Hardness""\r\nhardbox<-ggplot(hardnessboxplotdata, aes(x=Location, y=Avg.Hardness, color=Location)) +\r\n  geom_boxplot() +\r\n  scale_color_brewer(palette = ""Dark2"") +\r\n  theme_bw() +\r\n  theme(axis.title.x = element_blank(),\r\n        axis.title.y = element_text(size=12, colour = ""black""),\r\n        axis.text.x = element_text(size=12, colour = ""black""),\r\n        legend.position = ""none"")\r\nhardbox\r\n\r\n# Depths/OI ANOVA\r\nshapiro.test(standardsubsample$Depth) # p = 0.3734 = assumption upheld\r\nleveneTest(standardsubsample$Depth ~ standardsubsample$location) # p = 0.9253 = assumption upheld, proceed with ANOVA\r\ndepthaov<-aov(standardsubsample$Depth ~ standardsubsample$location)\r\nsummary(depthaov) # p = 0.388 = no sig diff in depth sampled at each location\r\n\r\n# Depth boxplot\r\ndepthboxplotdata<-standardsubsample\r\ncolnames(depthboxplotdata)\r\nnames(depthboxplotdata)[6] <- ""Location""\r\nnames(depthboxplotdata)[22] <- ""Depth""\r\ndepthbox<-ggplot(depthboxplotdata, aes(x=Location, y=Depth, color=Location)) +\r\n  geom_boxplot() +\r\n  scale_color_brewer(palette = ""Dark2"") +\r\n  theme_bw() +\r\n  theme(axis.title.x = element_blank(),\r\n        axis.title.y = element_text(size=12, colour = ""black""),\r\n        axis.text.x = element_text(size=12, colour = ""black""),\r\n        legend.position = ""none"")\r\ndepthbox\r\n\r\n# two-way ANOVA test\r\ndepthhardnessaov<-aov(standardsubsample$avghardness * standardsubsample$Depth ~ standardsubsample$location)\r\nsummary(depthhardnessaov) # same non-sig results\r\n\r\n# check the correlation between depth and avg. hardness to show there is no bias to certain substrates at certain depths\r\ncor.test(standardsubsample$Depth, standardsubsample$avghardness, method = ""pearson"") # CorrCoeff = -0.16, therefore weak, p = 0.3319 therefore non-sig\r\n\r\n# Depth vs. hardness scatterplot\r\ndepthscatter<-standardsubsample\r\ncolnames(depthscatter)\r\nnames(depthscatter)[6] <- ""Location""\r\nnames(depthscatter)[22] <- ""Depth""\r\nnames(depthscatter)[4] <- ""Avg.Hardness""\r\ndepthhardscat<-ggplot(depthscatter, aes(x=Avg.Hardness, y=Depth, shape=Location, color=Location)) +  \r\n  geom_point(size = 3) + \r\n  scale_color_brewer(palette = ""Dark2"") +\r\n  geom_smooth(method=lm, se=FALSE, fullrange=TRUE, size=1) +\r\n  theme_bw() +\r\n  theme(axis.title.x = element_text(size=12, colour = ""black""),\r\n        axis.title.y = element_text(size=12, colour = ""black""),\r\n        axis.text.x = element_text(size=12, colour = ""black""),\r\n        legend.position = ""bottom"",\r\n        legend.title = element_blank(),\r\n        legend.text = element_text(size=14, color=""black""))\r\ndepthhardscat\r\n\r\n# calculate the R2 values for each OI line\r\nasc<-subset(standardsubsample, location ==""Ascension"")\r\nsh<-subset(standardsubsample, location ==""SaintHelena"")\r\ntdc<-subset(standardsubsample, location ==""Tristan"")\r\ncor.test(asc$Depth, asc$avghardness, method = ""pearson"") # -0.072, p>0.05\r\ncor.test(sh$Depth, sh$avghardness, method = ""pearson"") # -0.412, p>0.05\r\ncor.test(tdc$Depth, tdc$avghardness, method = ""pearson"") # -0.164, p>0.05\r\n\r\n\r\nshapiro.test(standardsubsample$TotalSpecies) # p = 0.1258 = assumption upheld\r\nleveneTest(standardsubsample$TotalSpecies ~ standardsubsample$location) # p = 0.2327 = assumption upheld, proceed with ANOVA\r\nSRaov<-aov(standardsubsample$TotalSpecies ~ standardsubsample$location)\r\nsummary(SRaov) # p = 1.05e-08 = strongly sig diff in SR recorded for each location\r\nTukeyHSD(SRaov) # TdC-Asc p<0.01, TdC-SH p<0.01, SH-Asc p>0.05\r\n\r\n# SR boxplot\r\nSRboxplotdata<-standardsubsample\r\nnames(SRboxplotdata)[6] <- ""Location""\r\nnames(SRboxplotdata)[8] <- ""SpeciesRichness""\r\nggplot(SRboxplotdata, aes(x=Location, y=SpeciesRichness, color=Location)) +\r\n  geom_boxplot() +\r\n  scale_color_brewer(palette = ""Dark2"") +\r\n  theme_bw() +\r\n  theme(axis.title.x = element_blank(),\r\n        axis.title.y = element_text(size=12, colour = ""black""),\r\n        axis.text.x = element_text(size=12, colour = ""black""),\r\n        legend.position = ""none"")\r\n\r\ncor.test(standardsubsample$Depth, standardsubsample$TotalSpecies, method=""pearson"") # CorrCoeff = 0.155, p >0.05 for all OIs combined\r\ncor.test(asc$Depth, asc$TotalSpecies, method=""pearson"") # CorrCoeff = 0.273, p > 0.05\r\ncor.test(sh$Depth, sh$TotalSpecies, method=""pearson"") # CorrCoeff = -0.347, p > 0.05\r\ncor.test(tdc$Depth, tdc$TotalSpecies, method=""pearson"") # CorrCoeff = 0.129, p > 0.05\r\n\r\n\r\n# make grid for supp. mat\r\nlibrary(gridExtra)\r\ngrid.arrange(hardbox, depthbox, depthhardscat, nrow=3, ncol=1)\r\n\r\n############### PRE-SELECTION OF GLM VARIABLES: CORRELATION ANALYSIS ############### \r\nlibrary(car)\r\nlibrary(ggplot2)\r\nlibrary(mgcv)\r\nstandardsubsample<-read.csv(""TransectData.csv"", head=TRUE, sep="","")\r\ncolnames(standardsubsample)\r\n\r\ncorrelationdata <-subset(standardsubsample, select=c(""lat"", ""avghardness"", ""lutz_poc_w"", ""Temp"", ""Depth"", ""SURF_PP"", ""Rugosity"", ""Curvature"", ""Slope"", ""FBPI"", ""BBPI""))\r\ncorrmatrix<-cor(correlationdata)\r\nprint(corrmatrix) # [temperature and depth] and  [lat and surfPP] are both > 0.7\r\n\r\nlatgam<-gam(TotalSpecies ~ s(lat), data = standardsubsample)\r\nsummary(latgam) # 66.1% explained, p<0.01\r\nplot.gam(latgam)\r\n\r\nppgam<-gam(TotalSpecies ~ s(SURF_PP), data = standardsubsample)\r\nsummary(ppgam) # 80.2% explained, p<0.01\r\nplot.gam(ppgam)\r\n\r\ntempgam<-gam(TotalSpecies ~ s(Temp), data = standardsubsample) \r\nsummary(tempgam) # explains 28.1%, p>0.05\r\nplot.gam(tempgam)\r\n\r\ndepthgam<-gam(TotalSpecies ~ s(Depth), data = standardsubsample) \r\nsummary(depthgam) # explains 3.21%, p>0.05\r\nplot.gam(depthgam)\r\n\r\n###############  LM  ###############\r\nlm2<-lm(TotalSpecies ~ SURF_PP + Temp + lutz_poc_w + avghardness + Rugosity + Slope + BBPI + FBPI + Curvature, data=standardsubsample)\r\nsummary(lm2) \r\npar(mfrow=c(2,2))\r\nplot(lm2)\r\n\r\n###############  3D PLOT ###############\r\nlibrary(plotly)\r\nstandardsubsample<-read.csv(""TransectData.csv"", head=TRUE, sep="","")\r\n\r\nAscension<-subset(standardsubsample, location == ""Ascension"")\r\nStHelena<-subset(standardsubsample, location == ""SaintHelena"")\r\nTristan<-subset(standardsubsample, location == ""Tristan"")\r\nstandardsubsample$location <- as.factor(standardsubsample$location)\r\nlibrary(tidyverse)\r\n\r\nl <- list(\r\n  font = list(\r\n    family = ""sans-serif"",\r\n    size = 16,\r\n    color = ""#000""\r\n  )\r\n)\r\n\r\nfig<-plot_ly(standardsubsample,\r\n        x= ~lat, \r\n        y= ~TotalSpecies, \r\n        z= ~Depth,\r\n        color = ~location, \r\n        colors = c(""#d7301f"", ""#fc8d59"", ""#fdcc8a""))\r\n\r\nfig<-fig %>% add_markers()\r\nfig<- fig %>% layout(scene=list(xaxis=list(title= \'Latitude\'),\r\n                          yaxis=list(title= \'Species Richness\'),\r\n                          zaxis=list(title= \'Depth (m)\')))\r\nfig<- fig%>% layout(legend = list(orientation = \'h\'))\r\nfig<- fig%>% layout(legend = l)\r\n\r\nfig\r\n\r\n', '############################################# ASCENSION IMPORTING  ################################################\r\nlibrary(mgcv)\r\nlibrary(tidyverse)\r\nlibrary(neuralnet)\r\nlibrary(GGally)\r\nlibrary(randomForest)\r\n\r\n# get data\r\ndata<-read.csv(""asc13-18.csv"") #this is CTD data from AMT cruises that are relevant and JR16-NG and JR864 bottonm temperatures from the relevant CTDs linked to the SUCS datasheets\r\nplot(data$Temperature, data$Depth, ylim = range(data$Depth), \r\n     ylab=""depth"", xlab=""temp"", main=""Ascension depth profile based on CTD data"")\r\ncolnames(data) [4] <-c(""Latitude"")\r\ncolnames(data) [5] <- c(""Longitude"") \r\ncolnames(data) [6] <- c(""Depth"")\r\ncolnames(data) [7] <- c(""Temperature"") \r\n\r\n# split data so we can test how good our predictions are\r\nTrain <- sample_frac(tbl = data , replace = FALSE, size = 0.80)\r\nTest <- anti_join(data ,Train)\r\n\r\n# bring in bathy points\r\nnewX <- read.table(""AllAsc_25m_WGS84.txt"", header=TRUE, sep="","")\r\ncolnames(newX) [3] <- c(""Depth"")\r\nnewX <- subset(newX, Depth>-1500) # makes a new dataframe called newdata which only includes rows where the depth value is over 10\r\ncolnames(newX) [4] <- c(""Longitude"") # rename column to Depth to work with predict model\r\ncolnames(newX) [5] <- c(""Latitude"") # rename column to Depth to work with predict model\r\n\r\n############################################# ASCENSION TEST SIGNIFICANCE VARIABLES INDIVIDUALLY ################################################\r\ngamdepth<-gam(Temperature~s(Depth), data=Train)\r\nsummary(gamdepth) # 97.4% deve. exp\r\n\r\ngamlong<-gam(Temperature~s(Longitude), data=Train)\r\nsummary(gamlong) # 31.9% dev. exp\r\n\r\ngamlat<-gam(Temperature~s(Latitude), data=Train)\r\nsummary(gamlat) # more important than longitude w/ 35.1% dev. exp\r\n\r\n############################################# ASCENSION GAM 1b s(Depth, k=7) ################################################\r\n\r\ngam1b<-gam(Temperature~s(Depth, k=7), data=Train)\r\nsummary(gam1b) # GCV = 1.1785, Dev. exp, 97.4%, DF = 9\r\nplot.gam(gam1b) # confidence intervals are pretty tight\r\npredict(gam1b,newdata = Test %>% select(-Temperature)) -> preds1b\r\n# plot the redictions vs. actual data\r\nas_tibble( Test,.name_repair = ""universal"") %>% mutate(predictions =  preds1b) %>% \r\n  ggplot(aes(x= Temperature , y = Depth )) +\r\n  geom_point(col = ""red4"") +\r\n  geom_line(aes(x= predictions), col = ""blue"") +\r\n  ggtitle(""Comparison of Observed (Red) and Predicted (Blue) Temperature\r\n                                      for Ascension Island"")\r\nas_tibble( Test,.name_repair = ""universal"") %>% mutate(predictions =  preds1b) %>% select(Temperature, predictions) %>% cor()\r\n#Predict temperature onto your bathymetry data using the model you made\r\nnewY<- predict(gam1b, newdata=newX)\r\naddthese1b <- data.frame(newX, newY)\r\nggplot(aes(x=newY, y = Depth), data = addthese1b) + geom_point() + ggtitle(""GAM1b preds. temp profile"")\r\n\r\n############################################# SAINT HELENA IMPORTING ################################################\r\n\r\ndata<-read.delim(""JR17004-DY100_SHCTDCASTS_1500mlimit.txt"", sep = "","", head=TRUE)\r\ncolnames(data) [2] <- c(""Depth"") # rename column to Depth to work with predict model\r\ncolnames(data) [3] <- c(""Temperature"") # rename column to Depth to work with predict model\r\ncolnames(data) [4] <- c(""Latitude"") # rename column to Depth to work with predict model\r\ncolnames(data) [5] <- c(""Longitude"") # rename column to Depth to work with predict model\r\n\r\nplot(data$Temperature, data$Depth, ylim = rev(range(data$Depth)), ylab=""depth"", xlab=""temp"", main=""JR17004-DY100 CTDs"")\r\n\r\n# split data\r\nTrain <- sample_frac(tbl = data , replace = FALSE, size = 0.80)\r\nTest <- anti_join(data ,Train)\r\n\r\n# bring in bathy points\r\nnewX <- read.table(""AllSH_25m_WGS84.txt"", header=TRUE, sep="","")\r\nnames(newX)[3] <- ""Depth""\r\nnewX <- subset(newX, Depth>-1500)\r\nnames(newX)[4] <- ""Longitude""\r\nnames(newX)[5] <- ""Latitude""\r\n\r\n############################################# SAINT HELENA TEST SIGNIFICANCE VARIABLES INDIVIDUALLY ################################################\r\ngamdepth<-gam(Temperature~s(Depth), data=Train)\r\nsummary(gamdepth)\r\n\r\ngamlong<-gam(Temperature~s(Longitude), data=Train)\r\nsummary(gamlong) \r\n\r\ngamlat<-gam(Temperature~s(Latitude), data=Train)\r\nsummary(gamlat) \r\n\r\n############################################# SAINT HELENA GAM 1 s(Depth) ################################################\r\n\r\ngam1<-gam(Temperature~s(Depth), data=Train)\r\nsummary(gam1) # GCV = 1.1785, Dev. exp, 97.4%, DF = 9\r\nplot.gam(gam1) # confidence intervals are pretty tight\r\npredict(gam1,newdata = Test %>% select(-Temperature)) -> preds\r\n# plot the redictions vs. actual data\r\nas_tibble( Test,.name_repair = ""universal"") %>% mutate(predictions =  preds) %>% \r\n  ggplot(aes(x= Temperature , y = Depth )) +\r\n  geom_point(col = ""red4"") +\r\n  geom_line(aes(x= predictions), col = ""blue"") +\r\n  ggtitle(""Comparison of Observed (Red) and Predicted (Blue) Temperature\r\n                                      for Saint Helena"")\r\nas_tibble( Test,.name_repair = ""universal"") %>% mutate(predictions =  preds) %>% select(Temperature, predictions) %>% cor()\r\n#Predict temperature onto your bathymetry data using the model you made\r\nnewY<- predict(gam1, newdata=newX)\r\naddthese <- data.frame(newX, newY)\r\nggplot(aes(x=newY, y = Depth), data = addthese) + geom_point() + ggtitle(""GAM1 preds. temp profile"")\r\n\r\n\r\n############################################# TRISTAN IMPORTING  ################################################\r\n\r\nmodeldata<-read.csv(""JR287-17004-DY100_TDCCTDCASTS_1500mlimit.csv"") # this is your temp with depth readings that your model is based on\r\ncolnames(modeldata)\r\nmodeldata$gear <- NULL # there\'s a whole load you don\'t need\r\nlibrary(""tibble"")\r\nmodeldata <- as_data_frame(modeldata)\r\nmodeldata2 <- modeldata[, c(6,7,8,9,10)]\r\nmodeldata <-modeldata2\r\nnames(modeldata)[5] <- ""Temperature""\r\nnames(modeldata)[4] <- ""Depth""\r\n\r\nplot(modeldata$Temperature, modeldata$Depth, main=""All CTD Data"")\r\n\r\n# split data\r\nTrain <- sample_frac(tbl = modeldata , replace = FALSE, size = 0.80)\r\nTest <- anti_join(modeldata ,Train)\r\n\r\n# bring in bathy points for the whole area you want to create the model for\r\nnewX <- read.table(""AllTdC_25m_WGS84.txt"", header=TRUE, sep="","")\r\nnames(newX)[3] <- ""Depth""\r\nnewX <- subset(newX, Depth>-1500)\r\nnames(newX)[4] <- ""Longitude""\r\nnames(newX)[5] <- ""Latitude""\r\n\r\n############################################# TRISTAN TEST SIGNIFICANCE VARIABLES INDIVIDUALLY ################################################\r\ngamdepth<-gam(Temperature~s(Depth), data=Train)\r\nsummary(gamdepth)\r\n\r\ngamlong<-gam(Temperature~s(Longitude), data=Train)\r\nsummary(gamlong) # 8.19% dev. exp\r\n\r\ngamlat<-gam(Temperature~s(Latitude), data=Train)\r\nsummary(gamlat) # more important than Longitude w/ 16.5% dev. exp\r\n\r\n############################################# TRISTAN GAM 1 s(Depth) ################################################\r\n\r\ngam1<-gam(Temperature~s(Depth), data=Train)\r\nsummary(gam1) # GCV = 0.50484, Dev. exp, 97.6%, DF = 9\r\n\r\nplot.gam(gam1) # confidence intervals are virtually plotting on the GAM line!\r\npredict(gam1,newdata = Test %>% select(-Temperature)) -> preds1\r\n# plot the predictions vs. actual data\r\nas_tibble( Test,.name_repair = ""universal"") %>% mutate(predictions =  preds1) %>% \r\n  ggplot(aes(x= Temperature , y = Depth )) +\r\n  geom_point(col = ""red4"") +\r\n  geom_line(aes(x= predictions), col = ""blue"") +\r\n  ggtitle(""Comparison of Observed (Red) and Predicted (Blue) Temperature\r\n                                for Tristan da Cunha"")\r\nas_tibble( Test,.name_repair = ""universal"") %>% mutate(predictions =  preds1) %>% select(Temperature, predictions) %>% cor()\r\n#Predict temperature onto your bathymetry data using the model you made\r\nnewY<- predict(gam1, newdata=newX)\r\naddthese1 <- data.frame(newX, newY)\r\nggplot(aes(x=newY, y = Depth), data = addthese1) + geom_point() + ggtitle(""GAM1 predicted temp profile"")\r\n']","Depth and latitudinal diversity gradients in seamount benthic communities of the South Atlantic AimAlthough latitudinal and bathymetric species diversity gradients in the deep sea have been identified and investigated, studies have rarely considered these gradients across seamount and oceanic island ecosystems. This study aimed to identify whether the current understanding of latitudinal and bathymetric gradients in -diversity (species richness) apply to South Atlantic seamount ecosystems, as well as ascertaining whether identifiable trends were present in seamount -diversity along a bathymetric gradient.LocationThe South Atlantic Ocean.Time period2013-2019Major taxa studiedBenthic communities.MethodsDrop camera images from 39 transects, collected between 250 m and 950 m, were used to characterise species richness from within the Exclusive Economic Zones of Ascension Island, Saint Helena and Tristan da Cunha, spanning 8S to 40S. We subsequently applied linear modelling to test possible environmental drivers across latitudinal and bathymetric ranges (see transect metadata for variables). An Analysis of Similarity was employed to investigate the beta-diversity gradient, and the level of species turnover with depth.ResultsSurface primary productivity and substrate hardness both had significant positive effects on species richness, and there was significantly higher species richness at temperate latitudes. No significant relationship between species richness and depth was detected, but there was a significant species turnover with depth.Main conclusionsSeamounts and oceanic islands do not conform to established depth-diversity relationships within the depth range studied. However, despite their isolation and small sizes, seamounts and oceanic islands in the South Atlantic appear to follow latitudinal patterns of deep-sea species richness established for specific taxonomic groups in different ecosystems.",1
Four economic principles of just sustainability transition,"Equitable income distribution is desirable for moral, economic, and social reasons. Recent studies, however, indicate that improved income allocation will result in increased environmental impacts due to our socio-economic system's current settings. Therefore, we explored the key aspects of a system that can more evenly reallocate natural and economic resources while reducing negative environmental impacts. We found that the capital is extremely important as a means of material flows and stocks. Thus, effective policy interventions should target mechanisms at this very market. Based on a comprehensive literature review and statistical analyses at various levels, we proposed a four-step policy framework that includes reducing and targeted savings, reshaping governments' spatial decisions and role in the housing market, and changing the rates of depreciation in income tax legislation used globally.","['install.packages(""stringr"")                                \r\nlibrary(""stringr"")\r\nlibrary(plyr) \r\nlibrary(dplyr)\r\n\r\nPop <- read.csv(file = ""e:/Dombi/MA/MODEL/golden_rule/Zhi_data/Pop_WB.csv"", header = T, sep = "";"",dec = "","")\r\n\r\nPop <- rename(Pop,c(""Country.Name"" = ""Country""))\r\n\r\nStock_Civ_aggr <- as.data.frame(read.csv(file = ""c:/Users/user/Documents/Stock_Civ""))\r\nStock_Res_aggr <- as.data.frame(read.csv(file = ""c:/Users/user/Documents/Stock_Res""))\r\nStock_NonR_aggr <- as.data.frame(read.csv(file = ""c:/Users/user/Documents/Stock_NonR""))\r\nTotal_Stock_aggr <- as.data.frame(read.csv(file = ""c:/Users/user/Documents/Stock_Tot""))\r\n\r\nStock_Civ_aggr <- select(Stock_Civ_aggr, -X)\r\nStock_Res_aggr <- select(Stock_Res_aggr, -X)\r\nStock_NonR_aggr <- select(Stock_NonR_aggr, -X)\r\nTotal_Stock_aggr <- select(Total_Stock_aggr, -X)\r\n\r\nTotal_Stock <- subset(Total_Stock_aggr,str_sub(Total_Stock_aggr$Stock_NonR_C15.X, -3, -1) == ""50%"")\r\nStock_Res <- subset(Stock_Res_aggr,str_sub(Stock_Res_aggr$Stock_Res_C15.X, -3, -1) == ""50%"")\r\nStock_NonR <- subset(Stock_NonR_aggr,str_sub(Stock_NonR_aggr$Stock_NonR_C15.X, -3, -1) == ""50%"")\r\nStock_Civ <- subset(Stock_Civ_aggr,str_sub(Stock_Civ_aggr$Stock_Civ_C15.X, -3, -1) == ""50%"")\r\n\r\n\r\nNames <- read.csv(file = ""e:/Dombi/MA/MODEL/golden_rule/Zhi_data/c_codes_mod.csv"", header = T, sep = "";"")\r\nTotal_Stock$`Stock_NonR_C15$X` <- Names$Country\r\nStock_Res$`Stock_Res_C15$X` <- Names$Country\r\nStock_NonR$`Stock_NonR_C15$X` <- Names$Country\r\nStock_Civ$`Stock_Civ_C15$X` <- Names$Country\r\n\r\n\r\nTotal_Stock <- rename(Total_Stock,c(""Stock_NonR_C15$X"" = ""Country""))\r\nStock_Res <- rename(Stock_Res,c(""Stock_Res_C15$X"" = ""Country""))\r\nStock_NonR <- rename(Stock_NonR,c(""Stock_NonR_C15$X"" = ""Country""))\r\nStock_Civ <- rename(Stock_Civ,c(""Stock_Civ_C15$X"" = ""Country""))\r\n\r\n\r\n\r\n\r\nTotal_Stock_w_pop <- merge(Total_Stock,Pop, by =""Country"")\r\nStock_Res_w_pop <- merge(Stock_Res,Pop, by=""Country"")\r\nStock_NonR_w_pop <- merge(Stock_NonR,Pop, by=""Country"")\r\nStock_Civ_w_pop <- merge(Stock_Civ,Pop, by=""Country"")\r\n\r\n\r\n\r\nTotal_Stock_pc <- Total_Stock_w_pop[,31:87]/Total_Stock_w_pop[,91:147]\r\nTotal_Stock_pc <- cbind(Total_Stock_w_pop$Country,Total_Stock_pc)\r\ncolnames(Total_Stock_pc) <- c(""Country"",c(1960:2016))\r\n\r\nStock_Res_pc <- Stock_Res_w_pop[,31:87]/Stock_Res_w_pop[,91:147]\r\nStock_Res_pc <- cbind(Stock_Res_w_pop$Country,Stock_Res_pc)\r\ncolnames(Stock_Res_pc) <- c(""Country"",c(1960:2016))\r\n\r\nStock_NonR_pc <- Stock_NonR_w_pop[,31:87]/Stock_NonR_w_pop[,91:147]\r\nStock_NonR_pc <- cbind(Stock_NonR_w_pop$Country,Stock_NonR_pc)\r\ncolnames(Stock_NonR_pc) <- c(""Country"",c(1960:2016))\r\n\r\nStock_Civ_pc <- Stock_Civ_w_pop[,31:87]/Stock_Civ_w_pop[,91:147]\r\nStock_Civ_pc <- cbind(Stock_Civ_w_pop$Country,Stock_Civ_pc)\r\ncolnames(Stock_Civ_pc) <- c(""Country"",c(1960:2016))\r\n\r\n######stock per cap to rGDP\r\n\r\nrGDP <- read.csv(file = ""e:/Dombi/MA/MODEL/income inequality/rGDP_WB.csv"", header = T,sep = "";"", dec = "","")\r\nrGDP_pc <- read.csv(file = ""e:/Dombi/MA/MODEL/income inequality/rGDP_per_capita_WB.csv"", header = T,sep = "";"", dec = "","")\r\nrGDP_pc <- rename(rGDP_pc,c(""Country.Name"" = ""Country""))\r\n\r\n\r\nGDP_stock <- merge(Total_Stock_pc,rGDP_pc, by= ""Country"")\r\nGDP_res_stock <- merge(Stock_Res_pc,rGDP_pc, by= ""Country"")\r\nGDP_NonR_stock <- merge(Stock_NonR_pc,rGDP_pc, by= ""Country"")\r\nGDP_Civ_stock <- merge(Stock_Civ_pc,rGDP_pc, by= ""Country"")\r\n\r\nIncome_1 <- read.csv(file = ""e:/Dombi/MA/MODEL/income inequality/income_1_WID.csv"", header = T, sep = "";"", dec = ""."")\r\n\r\nIneq_stock <- merge(Total_Stock_pc,Income_1, by= ""Country"")\r\nIneq_Res_stock <- merge(Stock_Res_pc,Income_1, by= ""Country"")\r\nIneq_NonR_stock <- merge(Stock_NonR_pc,Income_1, by= ""Country"")\r\nIneq_Civ_stock <- merge(Stock_Civ_pc,Income_1, by= ""Country"")\r\n\r\nIneq_GDP_Stock <- merge(Ineq_stock,rGDP_pc, by = ""Country"")\r\nIneq_GDP_Stock_Res <- merge(Ineq_Res_stock,rGDP_pc, by = ""Country"")\r\nIneq_GDP_Stock_NonR <- merge(Ineq_NonR_stock,rGDP_pc, by = ""Country"")\r\nIneq_GDP_Stock_Civ <- merge(Ineq_Civ_stock,rGDP_pc, by = ""Country"")\r\n\r\nR <- read.csv(file = ""e:/Dombi/MA/MODEL/income inequality/pwt100_irr.csv"", header = T, sep = "";"", dec = "","")\r\n\r\nIneq_GDP_Stock_r <- merge(Ineq_GDP_Stock,R, by= ""Country"")\r\nIneq_GDP_Stock_r_Res <- merge(Ineq_GDP_Stock_Res,R, by= ""Country"")\r\nIneq_GDP_Stock_r_NonR <- merge(Ineq_GDP_Stock_NonR,R, by= ""Country"")\r\nIneq_GDP_Stock_r_Civ <- merge(Ineq_GDP_Stock_Civ,R, by= ""Country"")\r\n\r\n\r\n\r\n#####array for GDP grpwth rate g\r\nlibrary(ggplot2)\r\n\r\nG <- array(0,dim=c(121,56))\r\nG_g <- array(0,dim=c(1,56))\r\n\r\n#looped for all the countries,\r\n# ineq - gdp - MS pathway\r\n# together with r and g\r\n\r\n\r\nfor (i in 1:121){\r\n  Data <- cbind(t(Ineq_GDP_Stock_r[i,148:204]),t(Ineq_GDP_Stock_r[i,88:144]))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""GDP"",""Ineq"")\r\n  \r\n  A <- ggplot(data = Data, aes(x=GDP,y=Ineq))+geom_path(lineend = ""butt"") +labs(\r\n    x=""GDP per capita"", y=""Share of income in upper 1%s"", title =  Ineq_GDP_Stock_r$Country[i]) + geom_point() + geom_text(\r\n      label=rownames(Data), nudge_x = 0.00035, nudge_y = 0.00035, check_overlap = T)\r\n  \r\n  Data <- cbind(t(Ineq_GDP_Stock_r[i,2:58]),t(Ineq_GDP_Stock_r[i,88:144]))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""MS"",""Ineq"")\r\n  \r\n  B <- ggplot(data = Data, aes(x=MS,y=Ineq))+geom_path(lineend = ""butt"") +labs(\r\n    x=""Material stock (concrete) per capita"", y=""Share of income in upper 1%s"") + geom_point() + geom_text(\r\n      label=rownames(Data), nudge_x = 0.00035, nudge_y = 0.00035, check_overlap = T)\r\n  \r\n  \r\n  Data <- cbind(t(Ineq_GDP_Stock_r[i,2:58]),t(Ineq_GDP_Stock_r[i,148:204]))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""MS"",""GDP"")\r\n  \r\n  C <- ggplot(data = Data, aes(x=MS,y=GDP))+geom_path(lineend = ""butt"") +labs(\r\n    x=""Material stock (concrete) per capita"", y=""GDP per capita"") + geom_point() + geom_text(\r\n      label=rownames(Data), nudge_x = 0.00035, nudge_y = 0.00035, check_overlap = T)\r\n  \r\n  G_part <- Ineq_GDP_Stock_r[i,148:204]\r\n  \r\n  for (j in 1:56){\r\n    G_g[j] <- (G_part[,j+1]/G_part[,j])-1\r\n    \r\n  }\r\n  \r\n  G[i,] <- G_g\r\n  \r\n  Data <- cbind(t(Ineq_GDP_Stock_r[i,206:261]),(G[i,]),c(1961:2016))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""r"",""g"",""years"")\r\n  Data <- reshape2::melt(Data, id.var=\'years\')\r\n  \r\n  D <- ggplot(Data, aes(x=years,y=value, col=variable)) + geom_line()\r\n  \r\n  #D <- ggplot(data = Data, aes(x=years))+geom_line(aes(y=g),color= ""red"") + geom_line(\r\n   # aes(y=r),color= ""blue"") +labs(x="""", y=""%"") \r\n  \r\n  \r\n  myplot <- ggarrange(A,B,C, D,  ncol = 2, nrow = 2, labels = c(""""))\r\n  ggsave(myplot,filename = paste0(Ineq_GDP_Stock_r$Country[i],""_with_r_g"", "".png""),width=15, height=15)\r\n\r\n  \r\n}\r\n\r\n\r\n\r\n\r\n##########\r\n# two more plots: r to ineq, r-g to ineq\r\n\r\n\r\nfor (i in 1:121){\r\n  Data <- cbind(t(Ineq_GDP_Stock_r[i,148:204]),t(Ineq_GDP_Stock_r[i,88:144]))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""GDP"",""Ineq"")\r\n  \r\n  A <- ggplot(data = Data, aes(x=GDP,y=Ineq))+geom_path(lineend = ""butt"") +labs(\r\n    x=""GDP per capita"", y=""Share of income in upper 1%s"", title =  Ineq_GDP_Stock_r$Country[i]) + geom_point() + geom_text(\r\n      label=rownames(Data), nudge_x = 0.00035, nudge_y = 0.00035, check_overlap = T)\r\n  \r\n  Data <- cbind(t(Ineq_GDP_Stock_r[i,2:58]),t(Ineq_GDP_Stock_r[i,88:144]))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""MS"",""Ineq"")\r\n  \r\n  B <- ggplot(data = Data, aes(x=MS,y=Ineq))+geom_path(lineend = ""butt"") +labs(\r\n    x=""Material stock (concrete) per capita"", y=""Share of income in upper 1%s"") + geom_point() + geom_text(\r\n      label=rownames(Data), nudge_x = 0.00035, nudge_y = 0.00035, check_overlap = T)\r\n  \r\n  \r\n  Data <- cbind(t(Ineq_GDP_Stock_r[i,2:58]),t(Ineq_GDP_Stock_r[i,148:204]))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""MS"",""GDP"")\r\n  \r\n  C <- ggplot(data = Data, aes(x=MS,y=GDP))+geom_path(lineend = ""butt"") +labs(\r\n    x=""Material stock (concrete) per capita"", y=""GDP per capita"") + geom_point() + geom_text(\r\n      label=rownames(Data), nudge_x = 0.00035, nudge_y = 0.00035, check_overlap = T)\r\n  \r\n  G_part <- Ineq_GDP_Stock_r[i,148:204]\r\n  \r\n  for (j in 1:56){\r\n    G_g[j] <- (G_part[,j+1]/G_part[,j])-1\r\n    \r\n  }\r\n  \r\n  G[i,] <- G_g\r\n  \r\n  Data <- cbind(t(Ineq_GDP_Stock_r[i,206:261]),(G[i,]),c(1961:2016))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""r"",""g"",""years"")\r\n  Data <- reshape2::melt(Data, id.var=\'years\')\r\n  \r\n  D <- ggplot(Data, aes(x=years,y=value, col=variable)) + geom_line()\r\n  \r\n  #D <- ggplot(data = Data, aes(x=years))+geom_line(aes(y=g),color= ""red"") + geom_line(\r\n  # aes(y=r),color= ""blue"") +labs(x="""", y=""%"") \r\n  \r\n  \r\n  Data <- cbind(t(Ineq_GDP_Stock_r[i,206:261]),t(Ineq_GDP_Stock_r[i,89:144]))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""r"",""Ineq"")\r\n\r\n  E <- ggplot(data = Data, aes(x=r,y=Ineq))+geom_path(lineend = ""butt"") +labs(\r\n    x=""IRR"", y=""Share of income in upper 1%s"") + geom_point() + geom_text(\r\n      label=rownames(Data), nudge_x = 0.00035, nudge_y = 0.00035, check_overlap = T)\r\n  \r\n  Data <- cbind(t(Ineq_GDP_Stock_r[i,206:261]-G[i,]),t(Ineq_GDP_Stock_r[i,89:144]))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""dif"",""Ineq"")\r\n  \r\n  Ef <- ggplot(data = Data, aes(x=dif,y=Ineq))+geom_path(lineend = ""butt"") +labs(\r\n    x=""IRR-rGDP growth rate"", y=""Share of income in upper 1%s"") + geom_point() + geom_text(\r\n      label=rownames(Data), nudge_x = 0.00035, nudge_y = 0.00035, check_overlap = T)\r\n  \r\n  myplot <- ggarrange(A,B,C, D, E,Ef, ncol = 3, nrow = 2, labels = c(""""))\r\n  \r\n  ggsave(myplot,filename = paste0(Ineq_GDP_Stock_r$Country[i],""_with_r_g_country_pthways_in_ineq"", "".png""),width=18, height=12)\r\n  \r\n}\r\n\r\n\r\n\r\nS <- read.csv(file = ""e:/Dombi/MA/MODEL/income inequality/s_WB.csv"", header = T, sep = "";"", dec = "","")\r\nS <- as.data.frame(S)\r\n\r\n\r\n\r\nIneq_GDP_Stock_r_s <- merge(Ineq_GDP_Stock_r,S, by= ""Country"")\r\nIneq_GDP_Stock_r_s_Res <- merge(Ineq_GDP_Stock_r_Res,S, by= ""Country"")\r\nIneq_GDP_Stock_r_s_NonR <- merge(Ineq_GDP_Stock_r_NonR,S, by= ""Country"")\r\nIneq_GDP_Stock_r_s_Civ <- merge(Ineq_GDP_Stock_r_Civ,S, by= ""Country"")\r\n\r\n\r\n\r\n\r\n#county pathways to s, using MS dinamics\r\n\r\n\r\ndMS <- array(0,dim=c(121,56))\r\ndMS_g <- array(0,dim=c(1,56))\r\n\r\n\r\n\r\n\r\nfor (i in 1:121){\r\n  Data <- cbind(t(Ineq_GDP_Stock_r_s[i,205:261]),t(Ineq_GDP_Stock_r_s[i,265:321]))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""r"",""s"")\r\n  \r\n  A <- ggplot(data = Data, aes(x=r,y=s))+geom_path(lineend = ""butt"") +labs(\r\n    x=""IRR"", y=""savings rate, % of GDP"", title =  Ineq_GDP_Stock_r_s$Country[i]) + geom_point() + geom_text(\r\n      label=rownames(Data), nudge_x = 0.00035, nudge_y = 0.00035, check_overlap = T)\r\n  \r\n  \r\n  Data <- cbind(t(Ineq_GDP_Stock_r_s[i,265:321]),t(Ineq_GDP_Stock_r_s[i,88:144]))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""s"",""Ineq"")\r\n  \r\n  B <- ggplot(data = Data, aes(x=s,y=Ineq))+geom_path(lineend = ""butt"") +labs(\r\n    x=""savings rate, % of GDP"", y=""Share of income in upper 1%s"") + geom_point() + geom_text(\r\n      label=rownames(Data), nudge_x = 0.00035, nudge_y = 0.00035, check_overlap = T)\r\n  \r\n  \r\n  \r\n  Data <- cbind(t(Ineq_GDP_Stock_r_s[i,206:261]-G[i,]),t(Ineq_GDP_Stock_r_s[i,266:321]))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""dif"",""s"")\r\n  \r\n  C <- ggplot(data = Data, aes(x=dif,y=s))+geom_path(lineend = ""butt"") +labs(\r\n    x=""IRR-rGDP growth rate"", y=""savings rate, % of GDP"") + geom_point() + geom_text(\r\n      label=rownames(Data), nudge_x = 0.00035, nudge_y = 0.00035, check_overlap = T)\r\n  \r\n  \r\n  \r\n  dMS_part <- Ineq_GDP_Stock_r[i,2:58]\r\n  \r\n  for (j in 1:56){\r\n    dMS_g[j] <- (dMS_part[,j+1]/dMS_part[,j])-1\r\n    \r\n  }\r\n  \r\n  dMS[i,] <- dMS_g\r\n  \r\n  Data <- cbind(dMS[i,],t(Ineq_GDP_Stock_r_s[i,266:321]))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""dMS"",""s"")\r\n  \r\n  D <- ggplot(data = Data, aes(x=dMS,y=s))+geom_path(lineend = ""butt"") +labs(\r\n    x=""growth rate of material stock (concrete) per capita"", y=""savings rate, % of GDP"") + geom_point() + geom_text(\r\n      label=rownames(Data), nudge_x = 0.00035, nudge_y = 0.00035, check_overlap = T)\r\n  \r\n  \r\n  \r\n  \r\n  myplot <- ggarrange(A,B,C, D, ncol = 2, nrow = 2, labels = c(""""))\r\n  \r\n  ggsave(myplot,filename = paste0(Ineq_GDP_Stock_r_s$Country[i],""_with_r_g_s_country_pthways_in_ineq"", "".png""),width=15, height=15)\r\n  \r\n}\r\n\r\n\r\n#############################\r\n##############################\r\n#GINI calculation of MS\r\n\r\n\r\n#here, consider 1980 like in regression\r\n\r\n\r\n\r\nlibrary(ineq)\r\n\r\nfor (i in 22:58){\r\n  \r\n  print(ineq(Total_Stock_pc[,i],type = ""Gini"",na.rm = T))\r\n  \r\n  \r\n}\r\n\r\n\r\n\r\n\r\nfor (i in 22:58){\r\n  \r\n  print(ineq(Stock_Res_pc[,i],type = ""Gini"",na.rm = T))\r\n  \r\n  \r\n}\r\n\r\n\r\n\r\nfor (i in 22:58){\r\n  \r\n  print(ineq(Stock_NonR_pc[,i],type = ""Gini"",na.rm = T))\r\n  \r\n  \r\n}\r\n\r\n\r\n\r\nfor (i in 22:58){\r\n  \r\n  print(ineq(Stock_Civ_pc[,i],type = ""Gini"",na.rm = T))\r\n  \r\n  \r\n}\r\n\r\n\r\n\r\n\r\n\r\n#################################\r\n#######################\r\n#regression model to reveal wheather the MS is affected by inequality, or in the opposite\r\n\r\n#construct GDP/MS, as a dependent variable (USD/t)\r\n\r\nIneq_GDP_Stock_r[Ineq_GDP_Stock_r==0]<-NA\r\nGDP_per_MS <- Ineq_GDP_Stock_r[,148:204]/(Ineq_GDP_Stock_r[,2:58]*1000)\r\n\r\nGDP_per_MS <- cbind(Ineq_GDP_Stock_r$Country,GDP_per_MS)\r\ncolnames(GDP_per_MS)<- c(""Country"",c(1960:2016))\r\n#GDP_per_MS[is.infinite(GDP_per_MS)]<-NA\r\n\r\n\r\nIneq_GDP_Stock_r_Res[Ineq_GDP_Stock_r_Res==0]<-NA\r\nGDP_per_MS_Res <- Ineq_GDP_Stock_r_Res[,148:204]/(Ineq_GDP_Stock_r_Res[,2:58]*1000)\r\n\r\nGDP_per_MS_Res <- cbind(Ineq_GDP_Stock_r_Res$Country,GDP_per_MS_Res)\r\ncolnames(GDP_per_MS_Res)<- c(""Country"",c(1960:2016))\r\n#GDP_per_MS[is.infinite(GDP_per_MS)]<-NA\r\n\r\n\r\nIneq_GDP_Stock_r_NonR[Ineq_GDP_Stock_r_NonR==0]<-NA\r\nGDP_per_MS_NonR <- Ineq_GDP_Stock_r_NonR[,148:204]/(Ineq_GDP_Stock_r_NonR[,2:58]*1000)\r\n\r\nGDP_per_MS_NonR <- cbind(Ineq_GDP_Stock_r_NonR$Country,GDP_per_MS_NonR)\r\ncolnames(GDP_per_MS_NonR)<- c(""Country"",c(1960:2016))\r\n#GDP_per_MS[is.infinite(GDP_per_MS)]<-NA\r\n\r\n\r\nIneq_GDP_Stock_r_Civ[Ineq_GDP_Stock_r_Civ==0]<-NA\r\nGDP_per_MS_Civ <- Ineq_GDP_Stock_r_Civ[,148:204]/(Ineq_GDP_Stock_r_Civ[,2:58]*1000)\r\n\r\nGDP_per_MS_Civ <- cbind(Ineq_GDP_Stock_r_Civ$Country,GDP_per_MS_Civ)\r\ncolnames(GDP_per_MS_Civ)<- c(""Country"",c(1960:2016))\r\n#GDP_per_MS[is.infinite(GDP_per_MS)]<-NA\r\n\r\n\r\n\r\n\r\n\r\nfor (i in 1:121){\r\n  Data <- cbind(t(GDP_per_MS[i,2:58]),c(1960:2016))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""GDP_MS"",""years"")\r\n  \r\n  A <- ggplot(Data) + geom_point(aes(x=years,y=GDP_MS)) +labs(\r\n    x="""", y=""GDP per MS"", title =  GDP_per_MS$Country[i])\r\n  \r\n  Data <- cbind(t(GDP_per_MS_Res[i,2:58]),c(1960:2016))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""GDP_MS_Res"",""years"")\r\n  \r\n  B <- ggplot(Data) + geom_line(aes(x=years,y=GDP_MS_Res)) +labs(\r\n    x="""", y=""GDP per MS, Residential"")\r\n  \r\n  Data <- cbind(t(GDP_per_MS_NonR[i,2:58]),c(1960:2016))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""GDP_MS_NonR"",""years"")\r\n  \r\n  C <- ggplot(Data) + geom_line(aes(x=years,y=GDP_MS_NonR)) +labs(\r\n    x="""", y=""GDP per MS, Non-residential"") \r\n  \r\n  Data <- cbind(t(GDP_per_MS_Civ[i,2:58]),c(1960:2016))\r\n  Data <- as.data.frame(Data)\r\n  colnames(Data)<- c(""GDP_MS_Civ"",""years"")\r\n  \r\n  D <- ggplot(Data) + geom_line(aes(x=years,y=GDP_MS_Civ)) +labs(\r\n    x="""", y=""GDP per MS, Civil engineering"") \r\n  \r\n  myplot <- ggarrange(A,B,C, D,  ncol = 2, nrow = 2, labels = c(""""))\r\n  ggsave(myplot,filename = paste0(GDP_per_MS$Country[i],""GDP_per_MS"", "".png""),width=15, height=15)\r\n  \r\n  \r\n}\r\n\r\n#in one figure, years after 1980 exclusively\r\nlibrary(""reshape2"")  \r\nDat <- melt(GDP_per_MS[,c(1,22:58)])\r\n\r\n##some outliers\r\nggplot(Dat) + geom_point(aes(x=value,y = variable, color = Country)) +labs(\r\n  x=""GDP per MS"", y="""") +geom_path(aes(x=value,y = variable, color = Country,lineend = ""butt""))\r\n\r\n\r\n\r\n\r\nggplot(Dat) + geom_point(aes(x=value,y = variable, color = Country)) +labs(\r\n  x=""GDP per MS"", y="""") +geom_path(aes(x=value,y = variable, color = Country,lineend = ""butt"")) + coord_cartesian(xlim = c(0,5000))\r\n\r\n\r\n\r\nggplot(Dat) + geom_point(aes(x=value,y = variable, color = Country)) +labs(\r\n  x=""GDP per MS"", y="""") +geom_path(aes(x=value,y = variable, color = Country,lineend = ""butt"")) + coord_cartesian(xlim = c(0,10000))\r\n\r\n\r\n\r\n\r\n\r\nlibrary(knitr)\r\nlibrary(broom)\r\n\r\nlibrary(plm)\r\nlibrary(AER)\r\nlibrary(reshape2)\r\n\r\nlibrary(sjPlot)\r\nlibrary(sjmisc)\r\nlibrary(ggplot2)\r\nlibrary(ggeffects)\r\nlibrary(effects)\r\n\r\n#establishing the long form \r\n\r\nS_to_panel <- t(Ineq_GDP_Stock_r_s[,285:321])\r\ncolnames(S_to_panel) <- Ineq_GDP_Stock_r_s$Country\r\nS_to_panel <- melt(S_to_panel)\r\n\r\nGDP_per_MS_to_panel <- t(GDP_per_MS[,22:58])\r\ncolnames(GDP_per_MS_to_panel) <- Ineq_GDP_Stock_r_s$Country\r\nGDP_per_MS_to_panel <- melt(GDP_per_MS_to_panel)   #na.rm is an option!\r\n\r\nPanel_s_long <- cbind(GDP_per_MS_to_panel,S_to_panel$value)\r\ncolnames(Panel_s_long) <- c(""year"",""Country"",""MSeff"",""s"")\r\n\r\nPanel_s_long <- Panel_s_long[c(""Country"",""year"",""MSeff"",""s"")]\r\nPanel_s_long <- as.data.frame(Panel_s_long)\r\n\r\n#Panel_s_long <- pdata.frame(Panel_s_long, index=c(""Country"", ""year""))\r\n\r\npdim(Panel_s_long)\r\n#, ""Country""\r\n\r\n\r\nIneq_GDP_Stock_r_s_m <- Ineq_GDP_Stock_r_s[-c(46),]\r\nGDP_per_MS_m <- GDP_per_MS[-c(46),]\r\nG_m <- G[-c(46),]\r\ndMS <- dMS[-c(46),]\r\n\r\nS_to_panel <- t(Ineq_GDP_Stock_r_s_m[,285:321])\r\ncolnames(S_to_panel) <- Ineq_GDP_Stock_r_s_m$Country\r\nS_to_panel <- melt(S_to_panel)\r\n\r\nlagS_to_panel <- t(Ineq_GDP_Stock_r_s_m[,284:320])\r\ncolnames(lagS_to_panel) <- Ineq_GDP_Stock_r_s_m$Country\r\nlagS_to_panel <- melt(lagS_to_panel)\r\n\r\n\r\nIneq_to_panel <- t(Ineq_GDP_Stock_r_s_m[,108:144]) \r\ncolnames(Ineq_to_panel) <- Ineq_GDP_Stock_r_s_m$Country\r\nIneq_to_panel <- melt(Ineq_to_panel)\r\n\r\nRg_to_panel <- t(Ineq_GDP_Stock_r_s_m[,225:261])-(t(G_m[,20:56])) \r\ncolnames(Rg_to_panel) <- Ineq_GDP_Stock_r_s_m$Country\r\nRg_to_panel <- melt(Rg_to_panel)\r\n\r\n\r\nlagRg_to_panel <- t(Ineq_GDP_Stock_r_s_m[,224:260])-(t(G_m[,19:55])) \r\ncolnames(lagRg_to_panel) <- Ineq_GDP_Stock_r_s_m$Country\r\nlagRg_to_panel <- melt(lagRg_to_panel)\r\n\r\nG_to_panel <- t(G_m[,20:56])\r\ncolnames(G_to_panel) <- Ineq_GDP_Stock_r_s_m$Country\r\nG_to_panel <- melt(G_to_panel)\r\n\r\nGDP_to_panel <- t(Ineq_GDP_Stock_r_s_m[,168:204])\r\ncolnames(GDP_to_panel) <- Ineq_GDP_Stock_r_s_m$Country\r\nGDP_to_panel <- melt(GDP_to_panel)\r\n\r\nMS_to_panel <- t(Ineq_GDP_Stock_r_s_m[,22:58])\r\ncolnames(MS_to_panel) <- Ineq_GDP_Stock_r_s_m$Country\r\nMS_to_panel <- melt(MS_to_panel)\r\n\r\n\r\nIrr_to_panel <- t(Ineq_GDP_Stock_r_s_m[,225:261])\r\ncolnames(Irr_to_panel) <- Ineq_GDP_Stock_r_s_m$Country\r\nIrr_to_panel <- melt(Irr_to_panel)\r\n\r\nlagIrr_to_panel <- t(Ineq_GDP_Stock_r_s_m[,224:260])\r\ncolnames(lagIrr_to_panel) <- Ineq_GDP_Stock_r_s_m$Country\r\nlagIrr_to_panel <- melt(lagIrr_to_panel)\r\n\r\n\r\nGDP_per_MS_to_panel <- t(GDP_per_MS_m[,22:58])\r\ncolnames(GDP_per_MS_to_panel) <- Ineq_GDP_Stock_r_s_m$Country\r\nGDP_per_MS_to_panel <- melt(GDP_per_MS_to_panel)   #na.rm is an option!\r\n\r\ndMS_to_panel <- t(dMS[,20:56])\r\ncolnames(dMS_to_panel) <- Ineq_GDP_Stock_r_s_m$Country\r\ndMS_to_panel <- melt(dMS_to_panel)\r\n\r\n\r\n\r\nPanel_s_long <- cbind(GDP_per_MS_to_panel,S_to_panel$value,lagS_to_panel$value,Ineq_to_panel$value,Rg_to_panel$value,lagRg_to_panel$value,G_to_panel$value,GDP_to_panel$value,MS_to_panel$value*1000000,Irr_to_panel$value,lagIrr_to_panel$value,dMS_to_panel$value)\r\ncolnames(Panel_s_long) <- c(""year"",""Country"",""MSeff"",""s"",""lagS"",""Ineq"",""Rg"",""lagRg"",""g"",""GDP"",""MS"",""Irr"",""lagIrr"",""dMS"")\r\n\r\nPanel_s_long <- Panel_s_long[c(""Country"",""year"",""MSeff"",""s"",""lagS"",""Ineq"",""Rg"",""lagRg"",""g"",""GDP"",""MS"",""Irr"",""lagIrr"",""dMS"")]\r\nPanel_s_long <- as.data.frame(Panel_s_long)\r\n\r\n#Panel_s_long <- pdata.frame(Panel_s_long, index=c(""Country"", ""year""))\r\n\r\npdim(Panel_s_long)\r\n\r\n##############\r\n#to display data at the same level, \r\n\r\nPanel_s_long$Ineq <- Panel_s_long$Ineq*100\r\nPanel_s_long$Rg <- Panel_s_long$Rg*100\r\nPanel_s_long$g <- Panel_s_long$g*100\r\nPanel_s_long$LogGDP <- log(Panel_s_long$GDP)\r\nPanel_s_long$MS[(Panel_s_long$MS==0)] <- NA\r\nPanel_s_long$LogMSP <- log(Panel_s_long$MS)\r\nPanel_s_long$s3 <- Panel_s_long$s^3\r\nPanel_s_long$se <- exp(Panel_s_long$s)\r\nPanel_s_long$Irr <- Panel_s_long$Irr*100\r\nPanel_s_long$dMS[(Panel_s_long$dMS==0)] <- NA\r\nPanel_s_long$LogdMS <- log(Panel_s_long$dMS)\r\nPanel_s_long$lagRg <- Panel_s_long$lagRg*100\r\nPanel_s_long$lagS <- Panel_s_long$lagS*100\r\nPanel_s_long$lagIrr <- Panel_s_long$lagIrr*100\r\n\r\n\r\n#############\r\n#to models with GDP separatelly, normalization is required for the data\r\n\r\nPanel_s_long$Ineq<-(Panel_s_long$Ineq-min(Panel_s_long$Ineq))/(max(Panel_s_long$Ineq)-min(Panel_s_long$Ineq))\r\nPanel_s_long$s<-(Panel_s_long$s-min(Panel_s_long$s,na.rm = T))/(max(Panel_s_long$s,na.rm = T)-min(Panel_s_long$s,na.rm = T))\r\nPanel_s_long$Rg<-(Panel_s_long$Rg-min(Panel_s_long$Rg,na.rm = T))/(max(Panel_s_long$Rg,na.rm = T)-min(Panel_s_long$Rg,na.rm = T))\r\nPanel_s_long$g<-(Panel_s_long$g-min(Panel_s_long$g,na.rm = T))/(max(Panel_s_long$g,na.rm = T)-min(Panel_s_long$g,na.rm = T))\r\nPanel_s_long$GDP<-(Panel_s_long$GDP-min(Panel_s_long$GDP,na.rm = T))/(max(Panel_s_long$GDP,na.rm = T)-min(Panel_s_long$GDP,na.rm = T))\r\nPanel_s_long$LogGDP<-(Panel_s_long$LogGDP-min(Panel_s_long$LogGDP,na.rm = T))/(max(Panel_s_long$LogGDP,na.rm = T)-min(Panel_s_long$LogGDP,na.rm = T))\r\nPanel_s_long$MS<-(Panel_s_long$MS-min(Panel_s_long$MS,na.rm = T))/(max(Panel_s_long$MS,na.rm = T)-min(Panel_s_long$MS,na.rm = T))\r\nPanel_s_long$LogMSP<-(Panel_s_long$LogMSP-min(Panel_s_long$LogMSP,na.rm = T))/(max(Panel_s_long$LogMSP,na.rm = T)-min(Panel_s_long$LogMSP,na.rm = T))\r\nPanel_s_long$s3<-(Panel_s_long$s3-min(Panel_s_long$s3,na.rm = T))/(max(Panel_s_long$s3,na.rm = T)-min(Panel_s_long$s3,na.rm = T))\r\nPanel_s_long$MSeff<-(Panel_s_long$MSeff-min(Panel_s_long$MSeff,na.rm = T))/(max(Panel_s_long$MSeff,na.rm = T)-min(Panel_s_long$MSeff,na.rm = T))\r\nPanel_s_long$se<-(Panel_s_long$se-min(Panel_s_long$se,na.rm = T))/(max(Panel_s_long$se,na.rm = T)-min(Panel_s_long$se,na.rm = T))\r\nPanel_s_long$Irr<-(Panel_s_long$Irr-min(Panel_s_long$Irr,na.rm = T))/(max(Panel_s_long$Irr,na.rm = T)-min(Panel_s_long$Irr,na.rm = T))\r\nPanel_s_long$dMS<-(Panel_s_long$dMS-min(Panel_s_long$dMS,na.rm = T))/(max(Panel_s_long$dMS,na.rm = T)-min(Panel_s_long$dMS,na.rm = T))\r\nPanel_s_long$LogdMS<-(Panel_s_long$LogdMS-min(Panel_s_long$LogdMS,na.rm = T))/(max(Panel_s_long$LogdMS,na.rm = T)-min(Panel_s_long$LogdMS,na.rm = T))\r\nPanel_s_long$lagS<-(Panel_s_long$lagS-min(Panel_s_long$lagS,na.rm = T))/(max(Panel_s_long$lagS,na.rm = T)-min(Panel_s_long$lagS,na.rm = T))\r\nPanel_s_long$lagRg<-(Panel_s_long$lagRg-min(Panel_s_long$lagRg,na.rm = T))/(max(Panel_s_long$lagRg,na.rm = T)-min(Panel_s_long$lagRg,na.rm = T))\r\nPanel_s_long$lagIrr<-(Panel_s_long$lagIrr-min(Panel_s_long$lagIrr,na.rm = T))/(max(Panel_s_long$lagIrr,na.rm = T)-min(Panel_s_long$lagIrr,na.rm = T))\r\n\r\n\r\n\r\n\r\n\r\n\r\n#####################################\r\n############################\r\n#######################\r\n#dMS\r\n#one year lag\r\n#without ineq, s, r-g\r\n\r\n\r\n\r\nJ2.pooled <- plm(LogdMS~lagS + lagRg + LogGDP, model=""pooling"", data=Panel_s_long)\r\nsummary(J2.pooled)\r\n\r\n\r\nJ2.fixed <- plm(LogdMS~lagS + lagRg + LogGDP, model=""within"", data=Panel_s_long)\r\nsummary(J2.fixed)\r\n\r\n\r\nkable(tidy(pFtest(J2.fixed, J2.pooled)), caption=\r\n        ""Fixed effects test: Ho:\'No fixed effects\'"")\r\n\r\n\r\nJ2.random <- plm(LogdMS~lagS + lagRg + LogGDP, random.method=""swar"", model=""random"",data=Panel_s_long)\r\nsummary(J2.random)\r\n\r\n\r\nplot_model(J2.fixed, type = ""slope"",show.data = T)\r\n\r\n####test endogeinty!!!\r\n\r\n#Hausmann-test: test if \r\n\r\nkable(tidy(phtest(J2.fixed, J2.random)), caption=\r\n        ""Hausman endogeneity test for the random effects wage model"")\r\n\r\n\r\n\r\n\r\n\r\n\r\n']","Four economic principles of just sustainability transition Equitable income distribution is desirable for moral, economic, and social reasons. Recent studies, however, indicate that improved income allocation will result in increased environmental impacts due to our socio-economic system's current settings. Therefore, we explored the key aspects of a system that can more evenly reallocate natural and economic resources while reducing negative environmental impacts. We found that the capital is extremely important as a means of material flows and stocks. Thus, effective policy interventions should target mechanisms at this very market. Based on a comprehensive literature review and statistical analyses at various levels, we proposed a four-step policy framework that includes reducing and targeted savings, reshaping governments' spatial decisions and role in the housing market, and changing the rates of depreciation in income tax legislation used globally.",1
"Data and statistical code for ""Reconciling biodiversity with timber production and revenue via an intensive forest management experiment""","AbstractUnderstanding how land-management intensification shapes the relationships between biodiversity, yield and economic benefit is critical for managing natural resources. Yet, manipulative experiments that test how herbicides affect these relationships are scarce, particularly in forest ecosystems where considerable time lags exist between harvest revenue and initial investments. We assessed these relationships by combining 7 years of biodiversity surveys (>800 taxa) and forecasts of timber yield and economic return from a replicated, large-scale experiment that manipulated herbicide application intensity in operational timber plantations. Herbicides reduced species richness across trophic groups (-18%), but responses by higher-level trophic groups were more variable (038% reduction) than plant responses (-40%). Financial discounting, a conventional economic method to standardize past and future cashflows, strongly modified biodiversity-revenue relationships caused by management intensity. Despite a projected 28% timber yield gain with herbicides, biodiversity-revenue tradeoffs were muted when opportunity costs were high (i.e., economic discount rates 7%). Although herbicides can drive biodiversity-yield tradeoffs, under certain conditions, financial discounting provides opportunities to reconcile biodiversity conservation with revenue.",,"Data and statistical code for ""Reconciling biodiversity with timber production and revenue via an intensive forest management experiment"" AbstractUnderstanding how land-management intensification shapes the relationships between biodiversity, yield and economic benefit is critical for managing natural resources. Yet, manipulative experiments that test how herbicides affect these relationships are scarce, particularly in forest ecosystems where considerable time lags exist between harvest revenue and initial investments. We assessed these relationships by combining 7 years of biodiversity surveys (>800 taxa) and forecasts of timber yield and economic return from a replicated, large-scale experiment that manipulated herbicide application intensity in operational timber plantations. Herbicides reduced species richness across trophic groups (-18%), but responses by higher-level trophic groups were more variable (038% reduction) than plant responses (-40%). Financial discounting, a conventional economic method to standardize past and future cashflows, strongly modified biodiversity-revenue relationships caused by management intensity. Despite a projected 28% timber yield gain with herbicides, biodiversity-revenue tradeoffs were muted when opportunity costs were high (i.e., economic discount rates 7%). Although herbicides can drive biodiversity-yield tradeoffs, under certain conditions, financial discounting provides opportunities to reconcile biodiversity conservation with revenue.",1
"Data from: Climatic suitability, isolation by distance and river resistance explain genetic variation in a Brazilian whiptail lizard","Spatial patterns of genetic variation can help understand how environmental factors either permit or restrict gene flow and create opportunities for regional adaptations. Organisms from harsh environments such as the Brazilian semiarid Caatinga biome may reveal how severe climate conditions may affect patterns of genetic variation. Herein we combine information from mitochondrial DNA with physical and environmental features to study the association between different aspects of the Caatinga landscape and spatial genetic variation in the whiptail lizard Ameivula ocellifera. We investigated which of the climatic, environmental, geographical and/or historical components best predict: (1) the spatial distribution of genetic diversity, and (2) the genetic differentiation among populations. We found that genetic variation in A. ocellifera has been influenced mainly by temperature variability, which modulates connectivity among populations. Past climate conditions were important for shaping current genetic diversity, suggesting a time lag in genetic responses. Population structure in A. ocellifera was best explained by both isolation by distance and isolation by resistance (main rivers). Our findings indicate that both physical and climatic features are important for explaining the observed patterns of genetic variation across the xeric Caatinga biome.","['# 1. Load gdm package\nlibrary(gdm)\n\n\n# 2. Defining localities coordinates\ndata <- read.csv(""Coordenates_LongLat.csv"") # load table containing species, localities and coordinates (longitude and latitude)\nenvTab <-data[,c(2:4)] # create envTab object containing names of localities and coordinates (longitude and latitude)\nenvTab\n\n\n# 3. Defining the response variable (FST) \ngdmDissim_b <- read.table(""fst.txt"") # load FST matrix\nloc <- envTab[,1] # create vector containing names of localities\ngdmDissim <- cbind(loc, gdmDissim_b) # join names of localities and first column of FST matrix\ngdmDissim \n\n\n# 4. Defining the predictor variables\n# 4.1. Current niche\ncurrent<- read.table(""preditors/current.txt"")\ncurrent<-cbind(loc, current)\n# 4.2. LGM niche\npast<- read.table(""preditors/LGM.txt"")\npast<-cbind(loc, past)\n# 4.3. Rivers\nriver<- read.table(""preditors/rivers.txt"")\nriver<-cbind(loc, river)\n# 4.4. Roughness\nrug <- read.table(""preditors/roughness.txt"")\nrug<-cbind(loc, rug)\n# 4.5. Slope\nslope <- read.table(""preditors/slope.txt"")\nslope<-cbind(loc, slope)\n\n\n# 5. Using formatsitepair fuction to create input data\n# This function takes input biological (response) and environmental, geographic, and other predictor variables and builds a site-pair table required for fitting \n# a Generalized Dissimilarity Model using the gdm function. NOTE: x-y coordinates of sites MUST be present in either the biological or the environmental data.\n# The input biological data can be in one of four formats.\n# I will use the third one: site-site biological distance (dissimilarity) matrix\n# Usage: formatsitepair(bioData, bioFormat, dist=""bray"", abundance=FALSE, siteColumn=NULL, XColumn,YColumn, sppColumn=NULL, abundColumn=NULL, sppFilter=0, predData, \n#                     distPreds=NULL,weightType=""equal"", custWeights=NULL, samples=NULL) \n# Arguments:\n# bioData: The input biological (response) data table, in one of the four formats.\n# bioFormat: An integer code specifying the format of bioData. Acceptable values are 1, 2, 3, or 4.\n# siteColumn: The name of the column in either the biological or environmental data table containing site codes/names. \n# XColumn: The name of the column containing x-coordinates of sample sites.\n# YColumn: The name of the column containing y-coordinates of sample sites. \n# sppColumn: Only used if bioFormat = 2 (x, y, species list). The name of the column containing unique identifiers for each species.\n# predData: The environmental predictor data. Accepts either a site-predictor matrix or a raster stack.\n# distPreds: A list of distance matrices to be used as predictors, either in combination with, or as a substitute for, predData. For example, pairwise dissimilarity \n# for one biological group (e.g., trees) can be used as a predictor for another group (e.g., ferns). \n# Each distance matrix must have as the first column the names of the sites (therefore the matrix will not be square). \n# The column of site names should have the same name as the siteColumn argument. \n# Site IDs are required here to ensure correct ordering of sites in the construction of the site-pair table.\n\nspTable <- formatsitepair(gdmDissim, 3, XColumn=""long"", YColumn=""lat"", sppColumn=""species"", \n                          siteColumn=""loc"", predData=envTab,distPreds=list(as.matrix(current),as.matrix(past),as.matrix(river),as.matrix(rug),as.matrix(slope)))\nspTable \n\n\n# 6. GDM analysis\n# ?gdm \n# gdm is used to fit a generalized dissimilarity model to tabular site-pair data formatted as follows using the formatsitepair function: \n# Response, Weights, X0, Y0, X1, Y1, Pred1SiteA, Pred2SiteA, ..., PredNSiteA, Pred1SiteB, Pred2SiteB, ..., PredNSiteB. \n# The first column (Response) must be any ratio based dissimilarity (distance) measure between Site A and Site B. \n# The second column defines any weighting to be applied during fitting of the model (1.0 = default).\n# The third and fourth columns, X0 and Y0, represent the spatial coordinates of the first site in the site pair (Site A). \n# The fifth and sixth columns, X1 and Y1, represent the coordinates of the second site (Site B). \n# Note that the first six columns are REQUIRED, even if you do not intend to use geographic distance as a predictor\n# The next N*2 columns contain values for N predictors for Site A, followed by values for the same N predictors for Site B.\n# Usage: gdm(data, geo=FALSE, splines=NULL, knots=NULL)\n# Arguments:\n# data: A data frame containing the site pairs to be used to fit the GDM (typically obtained using the formatsitepair function). \n# If geo is FALSE (default), then the X0, Y0, X1 and Y1 data columns must still be included, but are ignored in fitting the model.\n# geo: Set to TRUE if geographic distance between sites is to be included as a model term. Set to FALSE if geographic distance is to be omitted from the model.\n# splines: An optional vector of the number of I-spline basis functions to be used for each predictor in fitting the model.\n# If this vector is not provided (splines=NULL), then a default of 3 basis functions is used for all predictors.\n# gdm returns a gdm model object (a list). The function summary.gdm can be used to obtain or print a synopsis of the results. \n\ngdm.1 <- gdm(spTable, geo=T) # run the model considering geographic coordinates\nsummary(gdm.1)\ngdm.1$explained # The percentage of null deviance explained by the fitted GDM model. \ngdm.1$predictors # A list of the names of the predictors that were used to fit the model.\n\ngdm.2 <- gdm(spTable, geo=F) # run the model disregarding geographic coordinates\nsummary(gdm.2) \ngdm.2$explained # The percentage of null deviance explained by the fitted GDM model. \ngdm.2$predictors # A list of the names of the predictors that were used to fit the model.\n\n\n# 7. Viewing results\n#?plot.gdm  \nplot.gdm(gdm.1,plot.color=""gray"",plot.linewidth=2) # plot GDM results graphics\n# Save PDF\npdf(file= paste(""predictors.pdf""), width=12, height=10,pointsize=20,title=""predictors"")\nplot(gdm.1, plot.layout=c(2,2),plot.color=""gray"",plot.linewidth=3)\ndev.off()\n\n\n# 8. Selection of predictive variables with gdm.varImp fuction\n#?gdm.varImp\n# This function uses matrix permutation to perform variable significance testing and to estimate variable importance in a generalized dissimilarity model. \n# The function can run in parallel on multicore machines to reduce computation time (recommended).\n# Usage: gdm.varImp(spTable, geo, splines = NULL, knots = NULL,fullModelOnly = FALSE, nPerm = 100, parallel = FALSE, cores = 2)\n# Arguments:\n# spTable: A site-pair table, same as used to fit a gdm\n# geo: Similar to the gdm geo argument. The only difference is that the geo argument does not have a default in this function.\n# splines: Same as the gdm splines argument.\n# knots: Same as the gdm knots argument.\n# fullModelOnly: Set to TRUE to test only the full variable set. Set to false to estimate variable importance and significance using matrix permutation and backward \n# elimination. Default is FALSE.\n# nPerm: Number of permutations to use to estimate p-values. Default is 100.\n# parallel: Whether or not to run the matrix permutations and model fitting in parallel. Parallel processing is accomplished using a foreach loop and it is highly \n# recommended when the nPerms argument is hundreds or more. When is argument is set to FALSE, the processes are completed using lapply.\n# color: When the parallel argument is set to TRUE, the number of cores to be registered for the foreach loop. \n# Must be <= the number of cores in the machine running the function.\n\nmodeltest <- gdm.varImp(spTable, geo=T, splines = NULL, knots = NULL, fullModelOnly = FALSE, nPerm = 500, parallel = FALSE, cores=2)\nmodeltest \noptions(OutDec = ""."")\nwrite.csv(modeltest[[1]], ""results_1.csv"")\nwrite.csv(modeltest[[2]], ""results_2.csv"")\nwrite.csv(modeltest[[3]], ""results_3.csv"")\n\n\n# 9. Plotting\nexSplines <- isplineExtract(gdm.1)\nstr(exSplines)\n\npar(""mar"")\npar(mar=c(1,1,1,1))\nplot_current <- plot(exSplines$x[,""matrix_1""],exSplines$y[,""matrix_1""],type=""solid"", lwd=3, xlab=""Current niche connectivity"", ylab=""Genetic distance"",ylim=c(0,0.4),cex.lab=1.7,cex.axis=1.4,mgp=c(2.5,1,0))\nrug(spTable$s2.matrix_1, ticksize = 0.03, side = 1, lwd = 0.5, col = par(""fg""))\n\nplot_LGM <- plot(exSplines$x[,""matrix_2""],exSplines$y[,""matrix_2""],type=""solid"", lwd=3, xlab=""LGM niche connectivity"", ylab=""Genetic distance"",ylim=c(0,0.4),cex.lab=1.7,cex.axis=1.4,mgp=c(2.5,1,0))\nrug(spTable$s2.matrix_1, ticksize = 0.03, side = 1, lwd = 0.5, col = par(""fg""))\n\nplot_river <- plot(exSplines$x[,""matrix_3""],exSplines$y[,""matrix_3""],type=""solid"", lwd=3, xlab=""River resistance"", ylab=""Genetic distance"",ylim=c(0,0.4))\nrug(spTable$s2.matrix_1, ticksize = 0.03, side = 1, lwd = 0.5, col = par(""fg""))\n\nplot_rugo <- plot(exSplines$x[,""matrix_4""],exSplines$y[,""matrix_4""],type=""solid"", lwd=3, xlab=""Roughness resistance"", ylab=""Genetic distance"",ylim=c(0,0.4),cex.lab=1.7,cex.axis=1.4,mgp=c(2.5,1,0))\nrug(spTable$s2.matrix_4, ticksize = 0.03, side = 1, lwd = 0.5, col = par(""fg""))\n\nplot_slope <- plot(exSplines$x[,""matrix_5""],exSplines$y[,""matrix_5""],type=""solid"", lwd=3, xlab=""Slope resistance "", ylab=""Genetic distance"",ylim=c(0,0.4),cex.lab=1.7,cex.axis=1.4,mgp=c(2.5,1,0))\nrug(spTable$s2.matrix_5, ticksize = 0.03, side = 1, lwd = 0.5, col = par(""fg""))\n\nplot.gdm(gdm.1, plot.layout = c(1,1), plot.color = ""blue"", plot.linewidth = 3,include.rug = TRUE, rug.sitepair = spTable)\n\n']","Data from: Climatic suitability, isolation by distance and river resistance explain genetic variation in a Brazilian whiptail lizard Spatial patterns of genetic variation can help understand how environmental factors either permit or restrict gene flow and create opportunities for regional adaptations. Organisms from harsh environments such as the Brazilian semiarid Caatinga biome may reveal how severe climate conditions may affect patterns of genetic variation. Herein we combine information from mitochondrial DNA with physical and environmental features to study the association between different aspects of the Caatinga landscape and spatial genetic variation in the whiptail lizard Ameivula ocellifera. We investigated which of the climatic, environmental, geographical and/or historical components best predict: (1) the spatial distribution of genetic diversity, and (2) the genetic differentiation among populations. We found that genetic variation in A. ocellifera has been influenced mainly by temperature variability, which modulates connectivity among populations. Past climate conditions were important for shaping current genetic diversity, suggesting a time lag in genetic responses. Population structure in A. ocellifera was best explained by both isolation by distance and isolation by resistance (main rivers). Our findings indicate that both physical and climatic features are important for explaining the observed patterns of genetic variation across the xeric Caatinga biome.",1
Types of planning systems and effects on construction material volumes: An explanatory analysis in Europe,"The primary aim of this study is to assess the effect of the spatial governance and planning on the utilization of construction materials, based on a panel of 35 European countries between 1995 and 2017. Additionally, the influence of core socio-economic characteristics, namely the GDP per capita, urbanization, and economic structure is assessed as well.https://doi.org/10.1016/j.landusepol.2021.105682","['Spat <- read.table(file = ""e:/Dombi/MA/MODEL/land_use_regulation/spatial_gov_and_MS.csv"", sep = "";"", dec = "","", header = T)\r\nPop <- read.table(file = ""e:/Dombi/MA/MODEL/land_use_regulation/pop_WB.csv"", sep = "";"", dec = "","", header =T)\r\nServ <- read.table(file = ""e:/Dombi/MA/MODEL/land_use_regulation/share_of_services_WB.csv"", sep = "";"", dec = "","", header = T)\r\nConstr <- read.table(file = ""e:/Dombi/MA/MODEL/land_use_regulation/DMC_constr.csv"", sep = "";"", dec = "","", header = T)\r\nUrb <- read.table(file = ""e:/Dombi/MA/MODEL/land_use_regulation/urban_population.csv"", sep = "";"", dec = "","", header = T)\r\nGDP <- read.table(file = ""e:/Dombi/MA/MODEL/land_use_regulation/rGDP_per_capita_WB.csv"", sep = "";"", dec = "","", header = T)\r\n\r\n\r\n\r\ncolnames(Pop) <- c(""Country"",c(1995:2017))\r\ncolnames(Serv) <- c(""Country"",c(1995:2017))\r\ncolnames(Constr) <- c(""Country"",c(1995:2017))\r\ncolnames(Urb) <- c(""Country"",c(1995:2017))\r\ncolnames(GDP) <- c(""Country"",c(1995:2017))\r\n\r\n\r\n#per capita DMC construction materials, t/cap\r\nConstr_cap <- Constr[,2:24]/Pop[,2:24]\r\nConstr_cap <- cbind(Constr[,1],Constr_cap)\r\ncolnames(Constr_cap) <- c(""Country"",c(1995:2017))\r\n\r\n\r\n#plotting\r\n\r\n\r\nplot(Spat$sdm,Constr_cap$`1997`)\r\nplot(Spat$sp,Constr_cap$`2017`)\r\nplot(Spat$spgovt,Constr_cap$`2017`)\r\n\r\n\r\n\r\n\r\npar(mfrow = c(5,5))\r\nfor (i in 2:24) {\r\n    plot(Spat$sdm,Constr_cap[,i],title(c(i+1993)))\r\n  }\r\n\r\n\r\n\r\npar(mfrow = c(5,5))\r\nfor (i in 2:24) {\r\n    plot(Spat$sp,Constr_cap[,i],title(c(i+1993)))\r\n  }\r\n\r\n\r\npar(mfrow = c(5,5))\r\nfor (i in 2:24) {\r\n    plot(Spat$spgovt,Constr_cap[,i],title(c(i+1993)))\r\n }\r\n\r\n\r\n\r\npar(mfrow = c(5,5))\r\nfor (i in 2:24) {\r\n  plot(GDP[,i],Constr_cap[,i],title(c(i+1993)))\r\n}\r\n\r\n\r\n\r\n#correltion\r\n##############\r\n#2 spatial planning dimension with DMC constr\r\n\r\nfor (i in 2:24) {\r\n    print(cor.test(Spat$sdm,Constr_cap[,i],summary = T))\r\n  }\r\n\r\n\r\n\r\nfor (i in 2:24) {\r\n    print(cor.test(Spat$sp,Constr_cap[,i],summary = T))\r\n  }\r\n\r\n\r\nfor (i in 2:24) {\r\n    print(summary(glm(Spat$spgovt~Constr_cap[,i],family = binomial)))\r\n  }\r\n\r\n#5 categories with 2 dimensions\r\nsummary(glm(Spat$spgovt~Spat$sp,family = binomial))\r\nsummary(glm(Spat$spgovt~Spat$sdm,family = binomial))\r\n\r\n\r\nfor (i in 2:24) {\r\n    print(summary(lm(Constr_cap[,i]~Spat$sp + GDP[,i])))\r\n  }\r\n\r\n\r\nfor (i in 2:24) {\r\n    print(summary(lm(Constr_cap[,i]~Spat$sp + Spat$sdm + GDP[,i])))\r\n  }\r\n\r\n\r\nfor (i in 2:24) {\r\n    print(cor.test(Serv[,i],Constr_cap[,i],summary = T))\r\n  }\r\n\r\n#it\'s a match!\r\n\r\nfor (i in 2:24) {\r\n    print(summary(lm(Constr_cap[,i]~Spat$sp + Spat$sdm + GDP[,i] + Serv[,i])))\r\n  }\r\n\r\n##########\r\n#population\r\n\r\nfor (i in 2:24) {\r\n    print(cor.test(Pop[,i],Constr_cap[,i],summary = T))\r\n  }\r\n\r\n#NOT AT ALL!\r\n\r\n\r\n#urbanization\r\n\r\nfor (i in 2:24) {\r\n    print(cor.test(Urb[,i],Constr_cap[,i],summary = T))\r\n  }\r\n#it\'s a match!\r\n\r\nfor (i in 2:24) {\r\n    print(summary(lm(Constr_cap[,i]~Spat$sp + Spat$sdm + GDP[,i] + Urb[,i])))\r\n  }\r\n\r\n\r\n\r\n##########################\r\n########################\r\n#panel analysis\r\n\r\n##########x\r\n\r\nlibrary(plm)\r\nlibrary(AER)\r\n\r\n#establishing the long form \r\n\r\nCountry <- Pop$Country\r\n\r\nall.equal(Spat$Country,Country)\r\nall.equal(Serv$Country,Country)\r\nall.equal(Constr_cap$Country,Country)\r\nall.equal(Urb$Country,Country)\r\nall.equal(GDP$Country,Country)\r\n\r\n\r\nSpat_panel <- array(0,dim=c(805,13))\r\n\r\n#colnames(Spat_panel) <- c(""Country"",""year"",""sdm"",""sp"",""spgovt"",""Pop"",""Serv"",""Constr_cap"",""Urb"",""GDP"")\r\ncolnames(Spat_panel) <- c(""Country"",""year"",""sdm"",""sp"",""spgovt"",""Pop"",""Serv"", ""Constr_cap"",""Urb"",""GDP"",""logGDP"",""logConstr"",""fact_spgovt"")\r\n\r\n\r\nfor (i in 1:35){\r\n  \r\n  \r\n  Spat_panel[((i*23-22):(i*23)),1] <- rep(Country[i],23)\r\n  Spat_panel[((i*23-22):(i*23)),2] <- c(1995:2017)\r\n  Spat_panel[((i*23-22):(i*23)),3] <- rep(Spat$sdm[i])\r\n  Spat_panel[((i*23-22):(i*23)),4] <- rep(Spat$sp[i])\r\n  Spat_panel[((i*23-22):(i*23)),5] <- rep(Spat$spgovt[i])\r\n  Spat_panel[((i*23-22):(i*23)),6] <- t(Pop[i,2:24])\r\n  Spat_panel[((i*23-22):(i*23)),7] <- t(Serv[i,2:24])\r\n  Spat_panel[((i*23-22):(i*23)),8] <- t(Constr_cap[i,2:24])\r\n  Spat_panel[((i*23-22):(i*23)),9] <- t(Urb[i,2:24])\r\n  Spat_panel[((i*23-22):(i*23)),10] <- t(GDP[i,2:24])\r\n  \r\n}\r\n\r\n\r\nSpat_panel <- as.data.frame(Spat_panel)\r\n\r\n\r\nSpat_panel[,11] <- log(Spat_panel$GDP)\r\nSpat_panel[,12] <- log(Spat_panel$Constr_cap)\r\nSpat_panel[,13] <- as.factor(Spat_panel$spgovt)\r\n\r\n\r\n\r\n\r\n##plot spatial characteristics agianst the variables in all over the data ponts\r\n\r\nplot(Spat_panel$sdm,Spat_panel$Constr_cap)\r\nplot(Spat_panel$sdm,log(Spat_panel$Constr_cap))\r\n\r\nplot(Spat_panel$sp,Spat_panel$Constr_cap)\r\nplot(Spat_panel$spgovt,Spat_panel$Constr_cap)\r\n\r\n\r\n#with Constr and GDP\r\npar(mfrow = c(2,3))\r\nboxplot(Spat_panel$Constr_cap~Spat_panel$sdm, ylab = ""DMC construction minerals per capita"",xlab = ""spatial developmenet model (state-led/market-led)"", cex.lab = 1.3)\r\nboxplot(Spat_panel$Constr_cap~Spat_panel$sp, ylab = ""DMC construction minerals per capita"",xlab = ""spatial planning (conformative/performative)"", cex.lab = 1.3)\r\nboxplot(Spat_panel$Constr_cap~Spat_panel$spgovt, ylab = ""DMC construction minerals per capita"",xlab = ""spatial governance and planning system (SGPS)"", cex.lab = 1.3)\r\nboxplot(Spat_panel$GDP~Spat_panel$sdm, ylab = ""GDP per capita"",xlab = ""spatial developmenet model (state-led/market-led)"", cex.lab = 1.3)\r\nboxplot(Spat_panel$GDP~Spat_panel$sp, ylab = ""GDP per capita"",xlab = ""spatial planning (conformative/performative)"", cex.lab = 1.3)\r\nboxplot(Spat_panel$GDP~Spat_panel$spgovt, ylab = ""GDP per capita"",xlab = ""spatial governance and planning system (SGPS)"", cex.lab = 1.3)\r\n\r\n\r\npar(mfrow = c(1,2))\r\nboxplot(Spat_panel$Constr_cap, cex.lab = 1.3)\r\nboxplot(Spat_panel$GDP, cex.lab = 1.3)\r\n\r\n\r\n\r\nlibrary(knitr)\r\nlibrary(broom)\r\n\r\n\r\n\r\n\r\n#####################\r\n# are any log or exp effects present?\r\n# make some plots!\r\n\r\n#####serv\r\n\r\npar(mfrow = c(5,5))\r\n\r\nfor (i in 2:24) {\r\n    plot(Serv[,i],Constr_cap[,i],title(c(i+1993)))\r\n  }\r\n\r\n\r\nplot(Spat_panel$Serv,Spat_panel$Constr_cap)\r\nplot(Spat_panel$Serv,log(Spat_panel$Constr_cap))\r\n\r\n\r\npar(mfrow = c(5,5))\r\nfor (i in 2:24) {\r\n  plot(log(Serv[,i]),Constr_cap[,i],title(c(i+1993)))\r\n}\r\n\r\n\r\nfor (i in 2:24) {\r\n  print(cor.test(Serv[,i],Constr_cap[,i],summary = T))\r\n}\r\n\r\n\r\nfor (i in 2:24) {\r\n  print(cor.test(Serv[,i]^2,Constr_cap[,i],summary = T))\r\n}\r\n\r\n\r\nfor (i in 2:24) {\r\n    print(cor.test(log(Serv[,i]),Constr_cap[,i],summary = T))\r\n}\r\n\r\n\r\n\r\n#######Urb\r\n\r\npar(mfrow = c(5,5))\r\nfor (i in 2:24) {\r\n    plot(Urb[,i],Constr_cap[,i],title(c(i+1993)))\r\n  }\r\n\r\n\r\nplot(Spat_panel$Urb,Spat_panel$Constr_cap)\r\nplot(Spat_panel$Urb,log(Spat_panel$Constr_cap))\r\n\r\n\r\npar(mfrow = c(5,5))\r\nfor (i in 2:24) {\r\n    plot(Urb[,i],Constr_cap[,i],title(c(i+1993)))\r\n  }\r\n\r\npar(mfrow = c(5,5))\r\nfor (i in 2:24) {\r\n    plot(Urb[,i],Constr_cap[,i],title(c(i+1993)))\r\n }\r\n\r\n\r\n\r\n\r\npar(mfrow = c(5,5))\r\nfor (i in 2:24) {\r\n    plot(GDP[,i],Constr_cap[,i],title(c(i+1993)))\r\n}\r\n\r\n\r\n\r\npar(mfrow = c(5,5))\r\nfor (i in 2:24) {\r\n  plot(log(GDP[,i]),Constr_cap[,i],title(c(i+1993)))\r\n  }\r\n\r\n\r\npar(mfrow = c(5,5))\r\nfor (i in 2:24) {\r\n    plot(log(GDP[,i]),log(Constr_cap[,i]),title(c(i+1993)))\r\n  }\r\n\r\n\r\n\r\nplot(Spat_panel$GDP,Spat_panel$Constr_cap)\r\nplot(log(Spat_panel$GDP),Spat_panel$Constr_cap)\r\n\r\nplot(log(Spat_panel$GDP),log(Spat_panel$Constr_cap))\r\n\r\n\r\n\r\n#interaction effect with log(Constr)\r\n#############################################\r\n\r\n\r\nlibrary(sjPlot)\r\nlibrary(sjmisc)\r\nlibrary(ggplot2)\r\nlibrary(ggeffects)\r\nlibrary(effects)\r\n\r\n\r\nGAT <- lm(logConstr~logGDP*Serv + as.factor(spgovt) + Serv + Urb + logGDP, data=Spat_panel)\r\nsummary(GAT)\r\n\r\nGAT_urb <- lm(logConstr~logGDP*Urb + as.factor(spgovt) + Serv + Urb + logGDP, data=Spat_panel)\r\nsummary(GAT_urb)\r\n\r\nGAT_spat <- lm(logConstr~logGDP*fact_spgovt + fact_spgovt + Serv + Urb + logGDP, data=Spat_panel)\r\nsummary(GAT_spat)\r\n\r\nGAT_urb_serv <- lm(logConstr~Serv*Urb + as.factor(spgovt) + Serv + Urb + logGDP, data=Spat_panel)\r\nsummary(GAT_urb_serv)\r\n\r\nGAT_spat_serv <- lm(logConstr~Serv*fact_spgovt + fact_spgovt + Serv + Urb + logGDP, data=Spat_panel)\r\nsummary(GAT_spat_serv)\r\n\r\nGAT_spat_urb <- lm(logConstr~Urb*fact_spgovt + fact_spgovt + Serv + Urb + logGDP, data=Spat_panel)\r\nsummary(GAT_spat_urb)\r\n\r\nSpat.random.joint <- plm(logConstr~logGDP*Serv + as.factor(spgovt) + Serv + Urb + logGDP, random.method=""swar"",\r\n                         model=""random"",data=Spat_panel)\r\nsummary(Spat.random.joint)\r\n\r\nP1 <- plot_model(GAT, type = ""int"",show.data = T)\r\nP2 <- plot_model(GAT_urb, type = ""int"",show.data = T)\r\nplot_model(GAT_spat, type = ""int"",show.data = T) #this is not important, hence it is used as endogenous in the model\r\nP3 <- plot_model(GAT_urb_serv, type = ""int"",show.data = T)\r\nplot_model(GAT_spat_serv, type = ""int"",show.data = T)#this is not important, hence it is used as endogenous in the model\r\nplot_model(GAT_spat_urb, type = ""int"",show.data = T)#this is not important, hence it is used as endogenous in the model\r\n\r\n\r\n\r\n\r\n#############\r\n#maybe interaction terms are connected with the IVs! Let\'s test it\r\n\r\nGAT_IV.1 <- lm(logConstr~logGDP*sp + spgovt + Serv + Urb + logGDP, data=Spat_panel)\r\nsummary(GAT_IV.1)\r\n\r\nP4 <- plot_model(GAT_IV.1, type = ""int"",show.data = T)\r\n\r\n#no interactions\r\n\r\n\r\nGAT_IV.2 <- lm(logConstr~logGDP*sdm + spgovt + Serv + Urb + logGDP, data=Spat_panel)\r\nsummary(GAT_IV.2)\r\nP5 <- plot_model(GAT_IV.2, type = ""int"",show.data = T)\r\n\r\n#strong interactions\r\n\r\n\r\n\r\n\r\nGAT_IV.4 <- lm(logConstr~sp*Urb + spgovt + Serv + Urb + logGDP, data=Spat_panel)\r\nsummary(GAT_IV.4)\r\n\r\nP6 <- plot_model(GAT_IV.4, type = ""int"",show.data = T)\r\n#strong interactions\r\n\r\n\r\nGAT_IV.5 <- lm(logConstr~sdm*Urb + spgovt + Serv + Urb + logGDP, data=Spat_panel)\r\nsummary(GAT_IV.5)\r\n\r\nP7 <- plot_model(GAT_IV.5, type = ""int"",show.data = T)\r\n#no interactions\r\n\r\n\r\nGAT_IV.6 <- lm(logConstr~sp*Serv + spgovt + Serv + Urb + logGDP, data=Spat_panel)\r\nsummary(GAT_IV.6)\r\n\r\nP8 <- plot_model(GAT_IV.6, type = ""int"",show.data = T)\r\n#strong interactions\r\n\r\n\r\nGAT_IV.7 <- lm(logConstr~sdm*Serv + spgovt + Serv + Urb + logGDP, data=Spat_panel)\r\nsummary(GAT_IV.7)\r\n\r\nP9 <- plot_model(GAT_IV.7, type = ""int"",show.data = T)\r\n#some interactions\r\n\r\n\r\nplot_grid(list(P1,P2,P3,P4,P5,P6,P7,P8,P9), margin = c(0.5, 0.5,0.5,0.5), tags = NULL)\r\n\r\n\r\n\r\n\r\n\r\n############x\r\n###################x log-log \r\n\r\n\r\nSpat.pooled.triangle <- plm(log(Constr_cap)~as.factor(spgovt) + Serv + Urb + log(GDP) + Serv*log(GDP)*Urb, \r\n                            model=""pooling"", data=Spat_panel)\r\nsummary(Spat.pooled.triangle)\r\n\r\n\r\n#fixed effects model\r\n\r\nSpat.fixed.triangle <- plm(log(Constr_cap)~as.factor(spgovt) + Serv + Urb + log(GDP) + Serv*log(GDP)*Urb, \r\n                           model=""within"", data=Spat_panel)\r\nsummary(Spat.fixed.triangle)\r\n\r\n\r\n#test if there are fixed effects\r\n\r\nkable(tidy(pFtest(Spat.fixed.triangle, Spat.pooled.triangle)), caption=\r\n        ""Fixed effects test: Ho:\'No fixed effects\'"")\r\n\r\n#the null hypothesis of no fixed effects is rejected.\r\n\r\n\r\n#the random effects model\r\n\r\nSpat.random.triangle <- plm(log(Constr_cap)~as.factor(spgovt) + Serv + Urb + log(GDP) + Serv*log(GDP)*Urb, random.method=""swar"",\r\n                            model=""random"",data=Spat_panel)\r\nsummary(Spat.random.triangle)\r\n\r\n#Hausmann-test\r\n\r\nkable(tidy(phtest(Spat.fixed.triangle, Spat.random.triangle)), caption=\r\n        ""Hausman endogeneity test for the random effects wage model"")\r\n\r\n\r\n\r\nHT.test.triangle <- ivreg(log(Constr_cap)~as.factor(spgovt) + Serv + Urb + log(GDP) + Serv*log(GDP)*Urb\r\n                          | sdm*log(GDP) +  sdm + sp + sp*Serv + sdm*Serv + sp*Urb + Serv + Urb + log(GDP) + Serv*log(GDP)*Urb, data=Spat_panel)\r\nsummary(HT.test.triangle, diagnostics=TRUE)\r\n\r\n\r\nSpat.ht.joint.triangle.2 <- plm(log(Constr_cap)~as.factor(spgovt) + Serv + Urb + log(GDP) + Serv*log(GDP)*Urb\r\n                                | sdm*log(GDP) +  sdm + sp + sp*Serv + sdm*Serv + sp*Urb + Serv + Urb + log(GDP) + Serv*log(GDP)*Urb, \r\n                                random.method = ""ht"", model = ""random"",inst.method = ""baltagi"", data=Spat_panel)\r\nsummary(Spat.ht.joint.triangle.2)\r\n\r\n#\r\n\r\n\r\n\r\nB <- as.data.frame(t(coef(Spat.ht.joint.triangle.2)))\r\n\r\n\r\nServices <- c(40,50,60,70)  \r\nUrban <- c(40,50,60,70)  \r\nWealth <- c(15000,25000,35000,45000)\r\n\r\n\r\n##############\r\n\r\n#calculation the marginal effects with Y\r\n# devide with 10 the Serv and Urb!!! https://www.aptech.com/blog/marginal-effects-of-linear-models-with-data-transformations/ \r\n\r\n\r\n\r\n#################################  \r\n\r\n#Conformative systems\r\n\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y2_on_Serv <- (B$\'(Intercept)\'+ B$Serv*Services[i] + B$Urb*mean(Spat_panel$Urb) + B$`log(GDP)`*mean(Spat_panel$logGDP) \r\n                 + B$`Serv:log(GDP)`*Services[i]*mean(Spat_panel$logGDP) + B$`Serv:Urb`*Services[i]*mean(Spat_panel$Urb) \r\n                 + B$`Urb:log(GDP)`*mean(Spat_panel$Urb)*mean(Spat_panel$logGDP) + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$Urb)*mean(Spat_panel$logGDP)*Services[i])\r\n  \r\n  print(exp(Y2_on_Serv)/10)\r\n  \r\n}\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y2_on_GDP <- (B$\'(Intercept)\' + B$Serv*mean(Spat_panel$Serv) + B$Urb*mean(Spat_panel$Urb) \r\n                + B$`log(GDP)`*log(Wealth[i]) + B$`Serv:log(GDP)`*mean(Spat_panel$Serv)*log(Wealth[i])\r\n                + B$`Serv:Urb`*mean(Spat_panel$Serv)*mean(Spat_panel$Urb) + B$`Urb:log(GDP)`*log(Wealth[i])*mean(Spat_panel$Urb)\r\n                + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$Serv)*log(Wealth[i])*mean(Spat_panel$Urb))\r\n  \r\n  print(Y2_on_GDP/log(10000))\r\n  \r\n} \r\n\r\n\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y2_on_Urb <- (B$\'(Intercept)\' + B$Serv*mean(Spat_panel$Serv) + B$Urb*Urban[i] + B$`log(GDP)`*mean(Spat_panel$logGDP) \r\n                + B$`Serv:log(GDP)`*mean(Spat_panel$logGDP)*mean(Spat_panel$Serv) + B$`Serv:Urb`*Urban[i]*mean(Spat_panel$Serv) \r\n                + B$`Urb:log(GDP)`*mean(Spat_panel$logGDP)*Urban[i] + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$logGDP)*Urban[i]*mean(Spat_panel$Serv))\r\n  \r\n  print(exp(Y2_on_Urb)/10)\r\n  \r\n}\r\n\r\n\r\n\r\n#Market-led neo-performative systems\r\n\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y2_on_Serv <- (B$\'(Intercept)\' + B$`as.factor(spgovt)2` + B$Serv*Services[i] + B$Urb*mean(Spat_panel$Urb) + B$`log(GDP)`*mean(Spat_panel$logGDP) \r\n                 + B$`Serv:log(GDP)`*Services[i]*mean(Spat_panel$logGDP) + B$`Serv:Urb`*Services[i]*mean(Spat_panel$Urb) \r\n                 + B$`Urb:log(GDP)`*mean(Spat_panel$Urb)*mean(Spat_panel$logGDP) + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$Urb)*mean(Spat_panel$logGDP)*Services[i])\r\n  \r\n  print(exp(Y2_on_Serv)/10)\r\n  \r\n}\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y2_on_GDP <- (B$\'(Intercept)\' + B$`as.factor(spgovt)2` + B$Serv*mean(Spat_panel$Serv) + B$Urb*mean(Spat_panel$Urb) \r\n                + B$`log(GDP)`*log(Wealth[i]) + B$`Serv:log(GDP)`*mean(Spat_panel$Serv)*log(Wealth[i])\r\n                + B$`Serv:Urb`*mean(Spat_panel$Serv)*mean(Spat_panel$Urb) + B$`Urb:log(GDP)`*log(Wealth[i])*mean(Spat_panel$Urb)\r\n                + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$Serv)*log(Wealth[i])*mean(Spat_panel$Urb))\r\n  \r\n  print(Y2_on_GDP/log(10000))\r\n  \r\n} \r\n\r\n\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y2_on_Urb <- (B$\'(Intercept)\' + B$`as.factor(spgovt)2` + B$Serv*mean(Spat_panel$Serv) + B$Urb*Urban[i] + B$`log(GDP)`*mean(Spat_panel$logGDP) \r\n                + B$`Serv:log(GDP)`*mean(Spat_panel$logGDP)*mean(Spat_panel$Serv) + B$`Serv:Urb`*Urban[i]*mean(Spat_panel$Serv) \r\n                + B$`Urb:log(GDP)`*mean(Spat_panel$logGDP)*Urban[i] + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$logGDP)*Urban[i]*mean(Spat_panel$Serv))\r\n  \r\n  print(exp(Y2_on_Urb)/10)\r\n  \r\n}\r\n\r\n\r\n\r\n#Misled performative systems\r\n\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y3_on_Serv <- (B$\'(Intercept)\' + B$`as.factor(spgovt)3` + B$Serv*Services[i] + B$Urb*mean(Spat_panel$Urb) + B$`log(GDP)`*mean(Spat_panel$logGDP) \r\n                 + B$`Serv:log(GDP)`*Services[i]*mean(Spat_panel$logGDP) + B$`Serv:Urb`*Services[i]*mean(Spat_panel$Urb) \r\n                 + B$`Urb:log(GDP)`*mean(Spat_panel$Urb)*mean(Spat_panel$logGDP) + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$Urb)*mean(Spat_panel$logGDP)*Services[i])\r\n  \r\n  print(exp(Y3_on_Serv)/10)\r\n  \r\n}\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y3_on_GDP <- (B$\'(Intercept)\' + B$`as.factor(spgovt)3` + B$Serv*mean(Spat_panel$Serv) + B$Urb*mean(Spat_panel$Urb) \r\n                + B$`log(GDP)`*log(Wealth[i]) + B$`Serv:log(GDP)`*mean(Spat_panel$Serv)*log(Wealth[i])\r\n                + B$`Serv:Urb`*mean(Spat_panel$Serv)*mean(Spat_panel$Urb) + B$`Urb:log(GDP)`*log(Wealth[i])*mean(Spat_panel$Urb)\r\n                + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$Serv)*log(Wealth[i])*mean(Spat_panel$Urb))\r\n  \r\n  print(Y3_on_GDP/log(10000))\r\n  \r\n} \r\n\r\n\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y3_on_Urb <- (B$\'(Intercept)\' + B$`as.factor(spgovt)3` + B$Serv*mean(Spat_panel$Serv) + B$Urb*Urban[i] + B$`log(GDP)`*mean(Spat_panel$logGDP) \r\n                + B$`Serv:log(GDP)`*mean(Spat_panel$logGDP)*mean(Spat_panel$Serv) + B$`Serv:Urb`*Urban[i]*mean(Spat_panel$Serv) \r\n                + B$`Urb:log(GDP)`*mean(Spat_panel$logGDP)*Urban[i] + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$logGDP)*Urban[i]*mean(Spat_panel$Serv))\r\n  \r\n  print(exp(Y3_on_Urb)/10)\r\n  \r\n}\r\n\r\n\r\n\r\n#Proto-conformative systems\r\n\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y4_on_Serv <- (B$\'(Intercept)\' + B$`as.factor(spgovt)4` + B$Serv*Services[i] + B$Urb*mean(Spat_panel$Urb) + B$`log(GDP)`*mean(Spat_panel$logGDP) \r\n                 + B$`Serv:log(GDP)`*Services[i]*mean(Spat_panel$logGDP) + B$`Serv:Urb`*Services[i]*mean(Spat_panel$Urb) \r\n                 + B$`Urb:log(GDP)`*mean(Spat_panel$Urb)*mean(Spat_panel$logGDP) + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$Urb)*mean(Spat_panel$logGDP)*Services[i])\r\n  \r\n  print(exp(Y4_on_Serv)/10)\r\n  \r\n}\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y4_on_GDP <- (B$\'(Intercept)\' + B$`as.factor(spgovt)4` + B$Serv*mean(Spat_panel$Serv) + B$Urb*mean(Spat_panel$Urb) \r\n                + B$`log(GDP)`*log(Wealth[i]) + B$`Serv:log(GDP)`*mean(Spat_panel$Serv)*log(Wealth[i])\r\n                + B$`Serv:Urb`*mean(Spat_panel$Serv)*mean(Spat_panel$Urb) + B$`Urb:log(GDP)`*log(Wealth[i])*mean(Spat_panel$Urb)\r\n                + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$Serv)*log(Wealth[i])*mean(Spat_panel$Urb))\r\n  \r\n  print(Y4_on_GDP/log(10000))\r\n  \r\n} \r\n\r\n\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y4_on_Urb <- (B$\'(Intercept)\' + B$`as.factor(spgovt)4` + B$Serv*mean(Spat_panel$Serv) + B$Urb*Urban[i] + B$`log(GDP)`*mean(Spat_panel$logGDP) \r\n                + B$`Serv:log(GDP)`*mean(Spat_panel$logGDP)*mean(Spat_panel$Serv) + B$`Serv:Urb`*Urban[i]*mean(Spat_panel$Serv) \r\n                + B$`Urb:log(GDP)`*mean(Spat_panel$logGDP)*Urban[i] + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$logGDP)*Urban[i]*mean(Spat_panel$Serv))\r\n  \r\n  print(exp(Y4_on_Urb)/10)\r\n  \r\n}\r\n\r\n\r\n\r\n\r\n#State-led systems\r\n\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y5_on_Serv <- (B$\'(Intercept)\' + B$`as.factor(spgovt)5` + B$Serv*Services[i] + B$Urb*mean(Spat_panel$Urb) + B$`log(GDP)`*mean(Spat_panel$logGDP) \r\n                     + B$`Serv:log(GDP)`*Services[i]*mean(Spat_panel$logGDP) + B$`Serv:Urb`*Services[i]*mean(Spat_panel$Urb) \r\n                     + B$`Urb:log(GDP)`*mean(Spat_panel$Urb)*mean(Spat_panel$logGDP) + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$Urb)*mean(Spat_panel$logGDP)*Services[i])\r\n  \r\n  print(exp(Y5_on_Serv)/10)\r\n  \r\n}\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y5_on_GDP <- (B$\'(Intercept)\' + B$`as.factor(spgovt)5` + B$Serv*mean(Spat_panel$Serv) + B$Urb*mean(Spat_panel$Urb) \r\n                    + B$`log(GDP)`*log(Wealth[i]) + B$`Serv:log(GDP)`*mean(Spat_panel$Serv)*log(Wealth[i])\r\n                    + B$`Serv:Urb`*mean(Spat_panel$Serv)*mean(Spat_panel$Urb) + B$`Urb:log(GDP)`*log(Wealth[i])*mean(Spat_panel$Urb)\r\n                    + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$Serv)*log(Wealth[i])*mean(Spat_panel$Urb))\r\n  \r\n  print(Y5_on_GDP/log(10000))\r\n  \r\n} \r\n\r\n\r\n\r\nfor (i in 1:4){\r\n  \r\n  Y5_on_Urb <- (B$\'(Intercept)\' + B$`as.factor(spgovt)5` + B$Serv*mean(Spat_panel$Serv) + B$Urb*Urban[i] + B$`log(GDP)`*mean(Spat_panel$logGDP) \r\n                     + B$`Serv:log(GDP)`*mean(Spat_panel$logGDP)*mean(Spat_panel$Serv) + B$`Serv:Urb`*Urban[i]*mean(Spat_panel$Serv) \r\n                     + B$`Urb:log(GDP)`*mean(Spat_panel$logGDP)*Urban[i] + B$`Serv:Urb:log(GDP)`*mean(Spat_panel$logGDP)*Urban[i]*mean(Spat_panel$Serv))\r\n  \r\n  print(exp(Y5_on_Urb)/10)\r\n  \r\n}\r\n\r\n\r\n\r\n#figure 4 with waste\r\n\r\nMet <- read.table(""e:/Dombi/waste_strategy/for_plot_CD_min_stock.csv"", header = TRUE, row.names = 1, sep = "";"", dec = "","")\r\nMet <- as.data.frame(Met)\r\n\r\nmyColors <- ifelse(is.na(Met$floor), ""grey"", ifelse(Met$floor>= 45, ""red"", ifelse(Met$floor<=40 ,""blue"",""black""))) \r\n\r\n#floor area indicated\r\npar(mfrow = c(1,3), mar = c(4,4,1,1))\r\nplot(Met$cd16~Met$min16, col = myColors, xlab=""Construction minerals, kg"",ylab=""Construction and demolition waste, kg"")\r\nwith(Met,text(Met$cd16~Met$min16,labels = row.names(Met), col = myColors, cex = 1.2, pos = 3))\r\nplot(Met$gdp16~Met$min16, col = myColors, xlab=""Construction minerals, kg"",ylab=""GDP, EUR"")\r\nwith(Met,text(Met$gdp16~Met$min16,labels = row.names(Met), col = myColors, cex = 1.2, pos = 3))\r\nplot(Met$gdp16~Met$cd16, col = myColors, xlab=""Construction and demolition waste, kg"",ylab=""GDP, EUR"")\r\nwith(Met,text(Met$gdp16~Met$cd16,labels = row.names(Met), col = myColors, cex = 1.2, pos = 3))\r\n\r\n']","Types of planning systems and effects on construction material volumes: An explanatory analysis in Europe The primary aim of this study is to assess the effect of the spatial governance and planning on the utilization of construction materials, based on a panel of 35 European countries between 1995 and 2017. Additionally, the influence of core socio-economic characteristics, namely the GDP per capita, urbanization, and economic structure is assessed as well.https://doi.org/10.1016/j.landusepol.2021.105682",1
Scripts generating maps and boxplots for extracted propositions from the French Great National Debate (Grand Dbat National),"This floder provides scripts to build maps and draw boxplots, to represent extracted propositions from the French Great National Debate (Grand Dbat National) :- Dicogeo.R is a script which build the socio-economic descriptions of municipalities, based on several other files,- gnration_map_grand_dbat.py is a script which build maps to represent extracted propositions from the French Great National Debate (Grand Dbat National),- graphiques.R is a script which generates bowplots crossing extracted propositions and socio-economic features.","['## Grand Dbat National\r\n## Nomenclatures gographiques des communes\r\n\r\nlistpack <- c(""sf"",""data.table"",""tidyverse"",""readxl"",""openxlsx"")\r\n\r\nneedpack <- setdiff(listpack, rownames(installed.packages()))\r\nif(length(needpack)) {\r\n  install.packages(needpack)\r\n}\r\nlibrary(easypackages)\r\nlibraries(listpack)\r\n\r\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\r\n\r\n#Chargement des aires d\'attraction des villes - Table\r\n\r\nurl <-""https://www.insee.fr/fr/statistiques/fichier/4803954/AAV2020_au_01-01-2020.zip""\r\n\r\ntemp <- tempfile()\r\ntemp2 <- tempfile()\r\n\r\ndownload.file(url, temp)\r\nunzip(zipfile = temp, exdir = temp2)\r\n\r\ndAAV2020 <- data.frame(read_excel(file.path(temp2, ""AAV2020_au_01-01-2020_v1.xlsx""),sheet = \'Composition_communale\',skip=5))\r\ndAAV2020 <- dAAV2020 %>%\r\n    dplyr::select(DEPCOM=CODGEO,LIBGEO,AAV2020,LIBAAV2020,CATEAAV2020,DEP,REG)\r\n\r\n#Chargement des bassins de vie\r\n\r\n#url<-""https://www.insee.fr/fr/statistiques/fichier/2115016/BV2012_au_01-01-2020.zip""\r\nurl<-""https://www.insee.fr/fr/statistiques/fichier/2115016/BV2012_au_01-01-2020_v1.zip""\r\n\r\ntemp <- tempfile()\r\ntemp2 <- tempfile()\r\n\r\ndownload.file(url, temp)\r\nunzip(zipfile = temp, exdir = temp2)\r\n\r\ndBV2012 <- data.frame(read_excel(file.path(temp2, ""BV2012_au_01-01-2020_v1.xlsx""),sheet = \'Composition_communale\',skip=5))\r\ndBV2012 <- dBV2012 %>%\r\n  dplyr::select(DEPCOM=CODGEO,LIBGEO,BV2012,LIBBV2012,DEP,REG)\r\n\r\nurl <- ""https://www.insee.fr/fr/statistiques/fichier/4515941/base-ccc-serie-historique-2017.zip""\r\n\r\ntemp <- tempfile()\r\ntemp2 <- tempfile()\r\n\r\ndownload.file(url, temp)\r\nunzip(zipfile = temp, exdir = temp2)\r\n\r\nfadr<-file.path(temp2,""base-cc-serie-historique-2017.CSV"")\r\ninseehist <-data.frame(read_delim(fadr,delim="";"",col_names=TRUE))\r\n\r\ninseehist <- inseehist %>%\r\n  dplyr::select(DEPCOM=CODGEO,P17_POP,D75_POP,SUPERF,P17_LOG,P17_RP,P17_RSECOCC,P17_LOGVAC,P17_PMEN)\r\n  \r\n\r\nfadr<-file.path(temp2,""meta_base-cc-serie-historique-2017.CSV"")\r\ninseehist_meta <-data.frame(read_delim(fadr,delim="";"",col_names=TRUE))\r\n\r\ninseehist_meta <-inseehist_meta[1:63,]\r\ninseehist_meta <- inseehist_meta %>%\r\n  filter(COD_VAR %in% c(""CODGEO"",""P17_POP"",""D75_POP"",""SUPERF"",""P17_LOG"",""P17_RP"",""P17_RSECOCC"",""P17_LOGVAC"",""P17_PMEN""))\r\n\r\nCentralites <- read_excel(""E:/users/_owncloud/inra/Centralites/Livrables/Rapport final/web/202009_Data_EtudesCentralits_INRAE_ANCT.xlsx"",sheet = \'Table\')\r\nCentralites <- Centralites %>%\r\n  dplyr::select(DEPCOM=DC,EPCI,\r\n         DC_UU,\r\n         GCD,\r\n         P_NP5CLA,\r\n         Tag_Centralite,\r\n         DC_ARC4,\r\n         HC_ARC4,\r\n         DC_REPORT_ARC4,\r\n         HC_REPORT_ARC4,\r\n         DC_ARC3P,\r\n         HC_ARC3P,\r\n         DC_REPORT_ARC3P,\r\n         HC_REPORT_ARC3P,\r\n         DC_ARC2P,\r\n         HC_ARC2P,\r\n         DC_REPORT_ARC2P,\r\n         HC_REPORT_ARC2P,\r\n         DC_ARC1P,\r\n         HC_ARC1P,\r\n         DC_REPORT_ARC1P,\r\n         HC_REPORT_ARC1P,\r\n         DC_UU_AD4,\r\n         DC_UU_AD3P,\r\n         DC_UU_AD2P,\r\n         DC_UU_AD1P,\r\n         P16_POP,\r\n         P06_POP,\r\n         P16_LOG,\r\n         P06_LOG,\r\n         P16_RP,\r\n         P06_RP,\r\n         P16_RSECOCC,\r\n         P06_RSECOCC,\r\n         P16_LOGVAC,\r\n         P06_LOGVAC,\r\n         P16_EMPLT,\r\n         P06_EMPLT,\r\n         P16_CHOM1564,\r\n         P16_INACT1564,\r\n         NIV_EQUIP_2017,\r\n  )\r\n\r\ndata_cadrage <- merge(dAAV2020,dBV2012)\r\ndata_cadrage <- merge(data_cadrage,inseehist)\r\ndata_cadrage <- merge(data_cadrage,Centralites)\r\ndata_cadrage$ARRMUN <- substr(data_cadrage$DEPCOM,1,3)\r\n\r\n#Chargement des codes postaux\r\n\r\nurl <- ""https://www.data.gouv.fr/en/datasets/r/554590ab-ae62-40ac-8353-ee75162c05ee""\r\ndCP2015<-data.frame(read_delim(url,delim="";"", col_names=TRUE,col_types = ""ccccc"")) #Table officielle 2015\r\nnames(dCP2015)<-c(""DEPCOM"",""LIBCOM"",""CP"",""L5"",""ACHEMNT"")\r\n\r\nGPS<-data.frame(read_delim(url,delim="";"", col_names=TRUE)) #Table officielle 2015\r\nnames(GPS)<-c(""DEPCOM"",""LIBCOM"",""CP"",""L5"",""ACHEMNT"",""GPS"")\r\nGPS<-GPS %>%\r\n  select(GPS)\r\n\r\nGPS.split<-strsplit(GPS$GPS,split="","")\r\ntmp<-do.call(rbind,GPS.split)\r\n\r\ndCP2015<-data.frame(dCP2015,tmp)\r\nnames(dCP2015)<-c(""DEPCOM"",""LIBCOM"",""CP"",""L5"",""ACHEMNT"",""latitude"",""longitude"")\r\ndCP2015<-dCP2015 %>%\r\n  select(DEPCOM, LIBCOM, CP, latitude, longitude)\r\n\r\ndCP2015 <- dCP2015 %>% distinct(DEPCOM, CP, .keep_all = TRUE)\r\n\r\nurl <- ""https://www.data.gouv.fr/en/datasets/r/455b9d9f-df83-4310-9acc-d3e3e54c2110""\r\ndCPNEW<-data.frame(read_delim(url,delim="";"",col_names=TRUE)) #Communes nouvelles\r\ndCPNEW<-dCPNEW %>%\r\n  select(Code.INSEE.Commune.Nouvelle, Nom.Commune.Nouvelle.Sige,Adresse.2016...Code.INSEE)\r\nnames(dCPNEW)<-c(""DEPCOM"",""LIBCOM"",""CP"")\r\ndCPNEW<-filter(dCPNEW, !is.na(CP))\r\ndCPNEW<-distinct(dCPNEW, CP,.keep_all = TRUE)\r\n\r\n### Association aux codes postaux\r\n\r\nall <- full_join(dCP2015,data_cadrage, by=""DEPCOM"")\r\nall <- all[order(all$DEPCOM),]\r\n\r\n\r\nIndex<-(all$ARRMUN==""132"")\r\nall$DEPCOM[Index] <- ""13055""\r\n\r\nIndex<-(all$ARRMUN==""751"")\r\nall$DEPCOM[Index] <- ""75056""\r\n\r\nIndex<-(all$ARRMUN==""693"")\r\nall$DEPCOM[Index] <- ""69123""\r\n\r\nrm(Index)\r\n\r\nall <- all %>% distinct(CP, DEPCOM, .keep_all = TRUE)\r\n\r\nto <- (all$DEPCOM==""16351"")\r\ndCP<-filter(all, DEPCOM==""16233"")\r\n\r\nall$LIBCOM[to]<-dCP$LIBCOM\r\nall$CP[to]<-dCP$CP\r\nall$latitude[to]<-dCP$latitude\r\nall$longitude[to]<-dCP$longitude\r\n\r\nto <- (all$DEPCOM==""53239"")\r\ndCP<-filter(all, DEPCOM==""53249"")\r\n\r\nall$LIBCOM[to]<-dCP$LIBCOM\r\nall$CP[to]<-dCP$CP\r\nall$latitude[to]<-dCP$latitude\r\nall$longitude[to]<-dCP$longitude\r\n\r\nto <- (all$DEPCOM==""53274"")\r\ndCP<-filter(all, DEPCOM==""53249"")\r\n\r\nall$LIBCOM[to]<-dCP$LIBCOM\r\nall$CP[to]<-dCP$CP\r\nall$latitude[to]<-dCP$latitude\r\nall$longitude[to]<-dCP$longitude\r\n\r\nwrite.csv(all, ""data_cadrage.csv"")\r\n\r\ncp_aav2020 <- all %>% \r\n  filter(!is.na(CATEAAV2020)) %>% \r\n  group_by(CP, CATEAAV2020) %>% \r\n  summarise(SPMUN=sum(P17_POP), ) %>% \r\n  spread(key=""CATEAAV2020"",\r\n         value=SPMUN,fill=0)\r\nnames(cp_aav2020)<-c(""CP"",""POPAAV11"",""POPAAV12"",""POPAAV13"",""POPAAV20"",""POPAAV30"")\r\ncp_aav2020 <- data.frame(cp_aav2020)\r\ncp_aav2020 <- cp_aav2020 %>%\r\n  mutate(PMUN = select(., AAV11:AAV30) %>% rowSums(na.rm = TRUE))\r\n\r\ncp_aav2020pct <- all %>% \r\n  filter(!is.na(CATEAAV2020)) %>% \r\n  group_by(CP, CATEAAV2020) %>%\r\n  summarise(SPMUN=sum(P17_POP), ) %>% \r\n  mutate(pct=100*SPMUN/sum(SPMUN))%>%\r\n  subset(select=c(""CP"",""CATEAAV2020"",""pct"")) %>%   #drop sum pop\r\n  spread(key=""CATEAAV2020"", value=pct,fill=0)\r\nnames(cp_aav2020)<-c(""CP"",""PCTAAV11"",""PCTAAV12"",""PCTAAV13"",""PCTAAV20"",""PCTAAV30"")\r\n\r\ncp_aav2020<-merge(cp_aav2020,cp_aav2020pct)\r\nwrite.csv(cp_aav2020, ""cp_aav2020.csv"")\r\n\r\ncp_maxpop <- all %>%\r\n  select(DEPCOM,CP,P17_POP)\r\n\r\ncp_maxpop <- arrange(cp_maxpop,CP,desc(P17_POP))\r\nwrite.csv(cp_maxpop, ""cp_maxpop.csv"")\r\n\r\nurl <- ""https://www.insee.fr/fr/statistiques/fichier/4505239/ODD_RDS.zip""\r\ntemp <- tempfile()\r\ntemp3 <- tempfile()\r\n\r\ndownload.file(url, temp)\r\nunzip(zipfile = temp, exdir = temp3)\r\n\r\nfadr<-file.path(temp3,""ODD_COM.rds"")\r\ndata_itdd<-readRDS(fadr)\r\n\r\nurl<-""https://www.insee.fr/fr/statistiques/fichier/4505239/dictionnaire_indicateurs_territoriaux_developpement_durable.xlsx""\r\n\r\ndata_itdd_meta <- data.frame(read.xlsx(url),sheet = \'dictionnaire_indicateurs_territ\')\r\ndata_itdd_meta <- data_itdd_meta %>%\r\n  drop_na(Fichier.COM)\r\n\r\nrm(fadr,listpack,needpack,temp,temp2,temp3,url)\r\n\r\nwrite.csv(data_itdd, ""data_itdd.csv"")\r\nwrite.csv(data_itdd_meta, ""data_itdd_meta.csv"")\r\n\r\n\r\n', 'listpack <- c(""ggplot2"",""cowplot"",""tidyverse"")\nneedpack <- setdiff(listpack, rownames(installed.packages()))\nif(length(needpack)) {\n  install.packages(needpack)\n}\nlibrary(easypackages)\nlibraries(listpack)\n\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\n\n\n\ndonnees <- read.csv(""/media/lucile/DATADRIVE0/Documents/Recherche/GDN/RIG/data-1655384176388.csv"", dec="","", quote = ""\\"""") %>%\n  dplyr::select(all_CP,all_CATEAAV2020,all_P17_POP,all_SUPERF,all_GCD,all_P_NP5CLA,\n                all_HC_ARC4,all_HC_ARC3P,all_HC_ARC2P,all_HC_ARC1P,\n                nb_contrib, nb_contrib_avec_trains, nb_contrib_avec_pistes_cyclables,\n                all_P16_CHOM1564)\ndonnees %>%  dplyr::mutate(all_DENS17=all_P17_POP/all_SUPERF)\n\ncp_donnees <- donnees %>% \n  group_by(all_CP) %>%\n  summarise(sum_P17_POP=sum(all_P17_POP),\n            sum_SUPERF=sum(all_SUPERF),\n            sum_P16_CHOM1564 = sum(all_P16_CHOM1564),\n            mean_HC_ARC4=mean(all_HC_ARC4),\n            mean_HC_ARC3p=mean(all_HC_ARC3P),\n            mean_HC_ARC2p=mean(all_HC_ARC2P),\n            mean_HC_ARC1p=mean(all_HC_ARC1P),\n            nb_contrib=mean(nb_contrib),\n            nb_contrib_avec_trains=mean(nb_contrib_avec_trains),\n            nb_contrib_avec_pistes_cyclables=mean(nb_contrib_avec_pistes_cyclables)\n            ) %>% \n  mutate(pct_trains=100*nb_contrib_avec_trains/nb_contrib,\n         pct_pistes_cyclables=100*nb_contrib_avec_pistes_cyclables/nb_contrib)%>%\n  dplyr::mutate(DENS17=log(sum_P17_POP/sum_SUPERF)) %>%\n  dplyr::mutate(TAUXCHOM = sum_P16_CHOM1564/sum_P17_POP)\n\nbox <- function(xvar,xlab,xpng) {\n\n  x1 <- cp_donnees[,xvar]\n  x2 <- cp_donnees[cp_donnees$nb_contrib!=0,xvar]\n  x3 <- cp_donnees[cp_donnees$nb_contrib_avec_trains!=0,xvar]\n  x4 <- cp_donnees[cp_donnees$nb_contrib_avec_pistes_cyclables!=0,xvar]\n  \n  f1 <- rep(c(\'Ensemble des zones postales\'),length(x1))\n  f2 <- rep(c(\'Zones postales avec contributeurs\'),length(x2))\n  f3 <- rep(c(\'Zones postales avec demande de trains\'),length(x3))\n  f4 <- rep(c(\'Zones postales avec demande de pistes cyclables\'),length(x4))\n  \n  t1 <- data.frame(x1,f1)\n  names(t1)<-c(\'var\',\'group\')\n  t2 <- data.frame(x2,f2)\n  names(t2)<-c(\'var\',\'group\')\n  t3 <- data.frame(x3,f3)\n  names(t3)<-c(\'var\',\'group\')\n  t4 <- data.frame(x4,f4)\n  names(t4)<-c(\'var\',\'group\')\n  tab <- rbind(t1,t2,t3,t4)\n  \n  png(xpng, width = 600)\n  p <- ggplot(tab, aes(group, var)) + \n    geom_boxplot() + \n    stat_summary(fun=mean, geom=""point"", shape=18, size=1.5, color=""red"", fill=""grey"") +\n    coord_flip() +\n    xlab("""") + ylab(xlab) \n  dev.off()\n  print(p)\n}\n\n\nbox(xvar=""mean_HC_ARC4"",\n    xlab=""Temps de trajet vers le centre majeur le plus proche (minutes)"",\n    xpng=""/home/lucile/Tlchargements/BOXPLOT_HC_ARC4.png"")\n\nbox(xvar=""mean_HC_ARC3p"",\n    xlab=""Temps de trajet vers le centre structurant le plus proche (minutes)"",\n    xpng=""BOXPLOT_HC_ARC3P.png"")\n\nbox(xvar=""mean_HC_ARC2p"",\n    xlab=""Temps de trajet vers le centre intermediaire le plus proche (minutes)"",\n    xpng=""BOXPLOT_HC_ARC2P.png"")\n\nbox(xvar=""mean_HC_ARC1p"",\n    xlab=""Temps de trajet vers le centre local le plus proche (minutes)"",\n    xpng=""BOXPLOT_HC_ARC1P.png"")\n\nbox(xvar=""DENS17"",\n    xlab=""Densit de population en 2017 (chelle log)"",\n    xpng=""BOXPLOT_DENS17.png"")\n\nbox(xvar=""TAUXCHOM"",\n    xlab=""Taux de chmage en 2016"",\n    xpng=""BOXPLOT_DENS17.png"")\n']","Scripts generating maps and boxplots for extracted propositions from the French Great National Debate (Grand Dbat National) This floder provides scripts to build maps and draw boxplots, to represent extracted propositions from the French Great National Debate (Grand Dbat National) :- Dicogeo.R is a script which build the socio-economic descriptions of municipalities, based on several other files,- gnration_map_grand_dbat.py is a script which build maps to represent extracted propositions from the French Great National Debate (Grand Dbat National),- graphiques.R is a script which generates bowplots crossing extracted propositions and socio-economic features.",1
Modeling the economic and environmental impacts of land scarcity under deep uncertainty,"Code and input files for the Dolan et al. 2021 paper ""Modeling the economic and environmental impacts of land scarcity under deep uncertainty""","['# carbon cdf\r\n\r\n\r\nluce<-readRDS(""query_data/LUC-emissions.rds"")\r\ncarbon <-readRDS(""query_data/carbon.rds"")\r\n\r\n\r\n\r\nluce %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,year)%>%\r\n  summarize(diff=sum(diff,na.rm=T),Base_Value=sum(Base_Value,na.rm=T)) ->luce1\r\n\r\n# tonnes to Mt\r\n\r\n\r\ncarbon %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,year)%>%\r\n  summarize(diff=sum(diff,na.rm=T),Base_Value=sum(Base_Value,na.rm=T)) ->c1 \r\n\r\n\r\nc1 %>% mutate(SSP=substr(SSP,4,4)) %>%\r\n  left_join(luce1,by=c(""SSP"",""Socioeconomics"",""agLU"",""RCP"",""ESM"",""CropModel"",""Policy"",""year""))  %>% \r\n  mutate(diff=diff.x+diff.y,percdiff=(diff/(Base_Value.x+Base_Value.y)*100))-> allc1\r\n\r\nluce1$Source=""LUC""\r\n\r\nc1$Source=""FFI""\r\n\r\nallc1$Source=""All""\r\n\r\n\r\nrbind(c1,luce1,allc1)->all\r\n\r\nall %>% rename(Mt=diff,Constraint=Policy)->all\r\n\r\n\r\ncol_vector=c(""purple"",""red"",""blue"")\r\n\r\nggplot(all,aes(Mt,col=Source,linetype=Constraint))+stat_ecdf(geom=""step"",size=1)+theme_bw()+\r\n  ylab(""f(\\u0394 MtC)"")+xlab(""\\u0394 MtC"")+scale_color_manual(values = col_vector)+\r\n  geom_vline(xintercept =0,col=""black"")+\r\n  theme(axis.text=element_text(size=10),axis.title = element_text(size=14))\r\n\r\nggsave(\'carbon_ecdf.png\',dpi=300)\r\n\r\n\r\n', 'land<-readRDS(\'query_data/land-allocation.rds\')\r\n\r\nland %>% group_by(crop,year,Policy,region) %>% summarize(diff=mean(diff,na.rm=T)) %>%\r\n  group_by(crop,year,Policy) %>% summarize(diff=sum(diff))->landmn\r\n\r\n\r\n\r\n\r\n\r\nland1<-landmn %>% mutate(`Land Type`=ifelse(grepl(""Protected"",crop),""Protected"",\r\n                                     ifelse(grepl(""Unmanaged"",crop),""Unmanaged"",\r\n                                            ifelse(grepl(""OtherArableLand|PalmFruit|Corn|Rice|FiberCrop|FodderGrass|FodderHerb|MiscCrop|OilCrop|OtherGrain|Wheat|RootTuber|SugarCrop"",crop),""Crop"",\r\n                                                   ifelse(grepl(""biomass"",crop),""Biomass"",\r\n                                                          ifelse(grepl(""Tundra|Rock|Shrub"",crop),""Shrubland/Tundra/Desert"",crop))))))\r\n\r\nland1 %>% group_by(`Land Type`,year,Policy) %>% summarize(diff=sum(diff))->land1\r\n\r\nland2<-land1 %>% filter(Policy==""Baseline""|Policy==""LandConsSecGen""|Policy==""LandConsAllBio"")%>%\r\n          mutate(Policy=case_when(\r\n                 Policy==""Baseline""~""Land Conservation"",\r\n                 Policy==""LandConsSecGen""~""Second Generation Biofuels"",\r\n                 Policy==""LandConsAllBio""~""All Biofuels"",\r\n                TRUE ~ as.character(Policy)\r\n  ))\r\n\r\nggplot(land2,aes(year,diff,fill=`Land Type`))+geom_area()+facet_wrap(~Policy,ncol=1,scales=""free"")+theme_bw()+\r\n  scale_fill_brewer(palette=""Paired"")+ylab(""Land Use Change (thousand square kilometers)"")+\r\n  theme(axis.title=element_text(size=12))+scale_x_continuous(expand=c(0,0))\r\n\r\n\r\nggsave(""figures/landusechange.png"",dpi=300)\r\n\r\n', 'library(dplyr)\r\nlibrary(ggplot2)\r\n\r\n# get percent price increases and join with GDP data\r\n\r\nprice<-readRDS(\'ag-prices.rds\')\r\n\r\n\r\ngdp<-read.csv(\'query_data/gdp.csv\')\r\n\r\n\r\nprice %>% mutate(percdiff=(diff/Base_Value)*100)->price\r\n\r\n\r\n\r\ngdp %>% mutate(value=value*2.09)->gdp\r\nprice %>% left_join(gdp,by=c(""region"",""year"",""SSP""))->dat\r\n\r\n\r\ndat %>% filter(year==2100)->dat\r\n\r\n\r\ndat %>% filter(region==""Brazil""|region==""USA""|region==""Pakistan""|region==""India""|region==""Africa_Eastern"")->reg\r\n\r\n\r\ncrops=c(""Corn"",""FiberCrop"",""MiscCrop"",""OilCrop"",""OtherGrain"",""PalmFruit"",\r\n        ""Rice"",""RootTuber"",""SugarCrop"",""Wheat"")\r\n\r\n# only look at food crops. average across all dimensions but those plotted\r\n\r\nreg %>% filter(sector %in% crops) %>%\r\n  group_by(SSP,region,Policy) %>% summarize(value=mean(value,na.rm=T),\r\n                                                 percdiff=mean(percdiff,na.rm=T)) %>%\r\n  mutate(SSP=substr(SSP,4,4),value=value/1e6)->polmean\r\n\r\n\r\nggplot(polmean,aes(value,percdiff,col=region,shape=SSP))+theme_bw()+\r\n  geom_point(size=2.5)+ylab(""% Change in Price"")+xlab(""GDP (trillion 2021 USD)"")+\r\n  facet_wrap(~factor(Policy,levels=c(""Baseline"",""All Biofuels"",""Second Gen. Biofuels"",""Land Conservation"",\r\n                                     ""All Biofuels and Land Conservation"",""Second Gen. Biofuels and Land Conservation"")),\r\n             scales=""free"",labeller =label_wrap_gen(width = 25, multi_line = TRUE))+\r\n  scale_color_brewer(palette=""Dark2"")+\r\n  theme(axis.text.x=element_text(angle=90),axis.title=element_text(size=14),\r\n        legend.position = c(.85,.22),legend.box=""horizontal"")\r\n\r\n\r\nggsave(""price_gdp.png"",dpi=300)\r\n\r\n####################################################################################################################################################\r\n############################################## SI Price GDP plot ###################################################################################\r\n####################################################################################################################################################\r\n\r\n# get protected land change ratio\r\n\r\n# land conservation land allocation\r\n\r\nland90<-readRDS(\'query_data/LandCons_land-allocation.rds\')\r\n\r\n\r\n# baseline land allocation\r\n\r\nbase<-readRDS(\'query_data/Baseline_land-allocation.rds\')\r\n\r\ngroup=names(land90)[1:9]\r\n\r\nland90 %>% group_by(Units,SSP,RCP,CropModel,ESM,Socioeconomics,agLU,region,year)%>%summarise(value=sum(value,na.rm=T))->all90\r\n\r\n# find protected fraction for conservation\r\n\r\nland90 %>% mutate(Protected=ifelse(grepl(""Protected"",crop),TRUE,FALSE)) %>% filter(Protected==TRUE) %>% \r\n  group_by(Units,SSP,RCP,CropModel,ESM,Socioeconomics,agLU,region,year) %>% summarize(value=sum(value,na.rm=T)) -> p90\r\n\r\n\r\np90 %>% left_join(all90,by=group) %>% \r\n  mutate(fracProtected=value.x/value.y)->frac90\r\n\r\n# find protected fraction for base\r\n\r\nbase %>% group_by(Units,SSP,RCP,CropModel,ESM,Socioeconomics,agLU,region,year)%>%summarise(value=sum(value,na.rm=T))->allbase\r\n\r\n\r\nbase %>% mutate(Protected=ifelse(grepl(""Protected"",crop),TRUE,FALSE)) %>% filter(Protected==TRUE) %>% \r\n  group_by(Units,SSP,RCP,CropModel,ESM,Socioeconomics,agLU,region,year) %>% summarize(value=sum(value,na.rm=T)) -> pbase\r\n\r\n\r\npbase %>% left_join(allbase,by=group) %>% \r\n  mutate(fracProtected=value.x/value.y)->fracbase\r\n\r\n\r\nfrac90 %>% left_join(fracbase,by=group) %>% mutate(diff=fracProtected.x-fracProtected.y) ->land\r\n\r\nland %>% mutate(percLandChange=(diff)*100)->land\r\n\r\nland %>% group_by(region,SSP) %>% summarize(diff=mean(percLandChange,na.rm=T))->landmean\r\n\r\npolmean %>% left_join(landmean,by=c(""SSP"",""region"")) %>% mutate(NormalizedPercentPriceChange=percdiff/diff)->landdat\r\n\r\nlanddat %>% mutate(value=value/1e6)->landdat\r\nlanddat %>% filter(Policy == ""Land Conservation"" | Policy==""All Biofuels and Land Conservation""|\r\n                     Policy == ""Second Gen. Biofuels and Land Conservation"")->landdat\r\n\r\nggplot(landdat,aes(value,NormalizedPercentPriceChange,col=region,shape=SSP))+theme_bw()+\r\n  geom_point(size=2.5)+ylab(""% Change in Price per % Change in Protected Land"")+xlab(""GDP (trillion 2021 USD)"")+\r\n  facet_wrap(~factor(Policy,levels=c(""Baseline"",""All Biofuels"",""Second Gen. Biofuels"",""Land Conservation"",""All Biofuels and Land Conservation"",""Second Gen. Biofuels and Land Conservation"")),scales=""free"",labeller =label_wrap_gen(width = 25, multi_line = TRUE),ncol=1)+\r\n  scale_color_brewer(palette=""Dark2"")+\r\n  theme(axis.text.x=element_text(angle=90),axis.title=element_text(size=14),\r\n        legend.box=""horizontal"")\r\n\r\nggsave(""price_gdp_normalized_byland_SI.png"",dpi=300)\r\n', 'library(dplyr)\r\nlibrary(cowplot)\r\n\r\n\r\nbasin<-read.csv(\'mapping/gcam_basin_ids.csv\')\r\n\r\nglu<-read.csv(\'mapping/iso_GCAM_regID.csv\',skip=5)\r\n\r\n\r\nglu %>% rename(ISO=iso) %>% mutate(ISO=toupper(ISO)) %>%\r\n  left_join(basin,by=""ISO"") %>% select(GCAM_basin_ID,GLU_name,GCAM_region_ID)->comb\r\n\r\n\r\n# baseline land allocation\r\n\r\nbase<-readRDS(\'query_data/Baseline_land-allocation_db_ssp2_4p5_gepic_gfdl-esm2m.rds\')\r\nbase %>% tidyr::separate(landleaf,into=c(""name"",""crop"",""landregion"",""mgmt"",""water""),sep="","")%>%\r\n  tidyr::separate(name,into=c(""crop2"",""GLU_name"",""irr"",""mgmt""),sep=""_"") ->base #%>%\r\n\r\n\r\nbase %>% left_join(comb,by=""GLU_name"")->baseland\r\n\r\nbaseland %>% filter(year==2020) %>% group_by(GCAM_basin_ID,GCAM_region_ID) %>%\r\n  summarise(value=sum(value,na.rm=T))->all\r\n\r\n\r\nbaseland %>% filter(year==2020) %>%group_by(GCAM_basin_ID,GCAM_region_ID) %>%\r\n  mutate(Protected=ifelse(grepl(""Protected"",crop),TRUE,FALSE)) %>%\r\n  filter(Protected==TRUE) %>%\r\n  summarize(value=sum(value,na.rm=T)) -> p\r\n\r\np %>% left_join(all,by=c(""GCAM_basin_ID"",""GCAM_region_ID"")) %>%\r\n  mutate(fracProtected=value.x/value.y)->frac\r\n\r\nlibrary(gcammaptools)\r\nlibrary(gridExtra)\r\nlibrary(rgdal)\r\nlibrary(viridis)\r\nlibrary(ggplot2)\r\n\r\n##################################################\r\n# read in basin shapefile (base)\r\n\r\nshp<-readOGR(\'mapping/shp\',\'reg_glu_boundaries_moirai_combined_3p1_0p5arcmin\')\r\nshp_df<-fortify(shp)\r\n\r\nas.numeric(shp_df$id)->shp_df$id\r\n\r\nshp_df %>% mutate(id=id+1)->shp_df\r\n\r\nxl<-readxl::read_excel(\'mapping/glu_reg_TableToExcel.xls\')\r\n\r\nxl %>% mutate(id=FID+1)->xl\r\n\r\nshp_df %>% left_join(xl,by=""id"") %>% rename(GCAM_basin_ID=glu_id,GCAM_region_ID=reg_id)%>%\r\n  left_join(frac,by=c(""GCAM_region_ID"",""GCAM_basin_ID""))->shp_j\r\n\r\n\r\nP1<-ggplot()+ theme_bw()+theme(axis.title= element_text(size=14),axis.text = element_text(size=12))+ylab(\'Latitude\')+xlab(\'Longitude\')+\r\n  geom_polygon(data=shp_j,aes(x=long,y=lat,group=group,fill=fracProtected))+scale_fill_viridis(limits=c(0,1))\r\n\r\nP1$labels$fill<-""Fraction Protected Land""\r\n\r\n### master\r\n\r\nmaster<-readRDS(\'query_data/LandCons_land-allocation_db_ssp2_4p5_gepic_gfdl-esm2m.rds\')\r\n\r\nmaster %>% tidyr::separate(landleaf,into=c(""name"",""crop"",""landregion"",""mgmt"",""water""),sep="","")%>%\r\n  tidyr::separate(name,into=c(""crop2"",""GLU_name"",""irr"",""mgmt""),sep=""_"") ->master #%>%\r\n\r\n\r\nmaster %>% left_join(comb,by=""GLU_name"")->masterland\r\n\r\nmasterland %>% filter(year==2020) %>% group_by(GCAM_basin_ID,GCAM_region_ID) %>%\r\n  summarise(value=sum(value,na.rm=T))->all1\r\n\r\n\r\nmasterland %>% filter(year==2020) %>%group_by(GCAM_basin_ID,GCAM_region_ID) %>%\r\n  mutate(Protected=ifelse(grepl(""Protected"",crop),TRUE,FALSE)) %>%\r\n  filter(Protected==TRUE) %>%\r\n  summarize(value=sum(value,na.rm=T)) -> p1\r\n\r\np1 %>% left_join(all1,by=c(""GCAM_basin_ID"",""GCAM_region_ID"")) %>%\r\n  mutate(fracProtected=value.x/value.y)->frac1\r\n\r\nshp_df %>% left_join(xl,by=""id"") %>% rename(GCAM_basin_ID=glu_id,GCAM_region_ID=reg_id)%>%\r\n  left_join(frac1,by=c(""GCAM_region_ID"",""GCAM_basin_ID""))->shp_j1\r\n\r\nP2<-ggplot()+ theme_bw()+theme(axis.title= element_text(size=14),axis.text = element_text(size=12))+ylab(\'Latitude\')+xlab(\'Longitude\')+\r\n  geom_polygon(data=shp_j1,aes(x=long,y=lat,group=group,fill=fracProtected))+scale_fill_viridis(limits=c(0,1))\r\n\r\nP2$labels$fill<-""Fraction Protected Land""\r\n\r\n##########################\r\n\r\nyear=seq(2005,2100,5)\r\n\r\n# mandate\r\n\r\nbio<-c(0.0,\t0.0,\t0.0,\t31,\t64,\t80,\t98,\t106,\t116,\t122,\t130,\t138,\t146,\t154,\t162,\t170,\t178,\t186,\t194,\t202)\r\n\r\n\r\n# queried from the GCAM model interface (purpose grown, residue, municipal solid waste)\r\n\r\npg<-c(0.0,\t2,4,5.43,\t21.46,\t35.56,\t50.98,\t55.50,\t61.73,\t65.56,\t71.01,\t77.33,\t83.95,\t90.75,\t97.75,\t104.88,\t112.32,\t119.88,\t127.44,\t135.35)\r\nres<-c(15.3,18.8,22,20.5,37.8,39.3,41.4,44.5,47.8,49.6,51.8,53.1,54.1,54.9,55.6,56,56.2,56.3,56.4,56.2)\r\nmsw<-c(3,3.4,3.8,4.1,4.7,5.1,5.6,6,6.4,6.8,7.1,7.5,7.9,8.3,8.7,9,9.4,8.8,10.1,10.5)\r\n\r\ncbind(year,bio,pg,res,msw)->biomass\r\nbiomass=as.data.frame(biomass)\r\n\r\nbiomass %>% tidyr::gather(Source,value,bio,pg,res,msw)->bio1\r\nbio1$Policy=""Second Generation""\r\n\r\n# queried from the GCAM model interface (purpose grown, residue, municipal solid waste)\r\n\r\npg<-c(0.0,2,4,5.43,\t20.26,33.49,47.56,50.56,54.41,57.06,60.8,66,71.3,77.3,83.3,88.94,94.9,101.2,107.58,114.4)\r\nres<-c(15.37,18.82,22.03,20.48,36.98,38.1,40.12,42.75,46.2,47.47,50.34,52.26,54,55.1,56,56.85,57.35,57.7,58,57.95)\r\nmsw<-c(2.97,\t3.40,\t3.8,\t4.1,\t4.68,\t5.15,\t5.58,\t6,\t6.4,\t6.78,\t7.15,\t7.53,\t7.9,\t8.3,\t8.7,\t9,\t9.4,\t9.76,\t10.1,\t10.5)\r\n\r\n\r\ncbind(year,bio,pg,res,msw)->biomass2\r\nbiomass2=as.data.frame(biomass2)\r\n\r\nbiomass2 %>% tidyr::gather(Source,value,bio,pg,res,msw)->bio2\r\nbio2$Policy=""All""\r\n\r\nrbind(bio1,bio2)->all\r\n\r\n\r\nP3<-ggplot(all,aes(year,value,col=Source,linetype=Policy))+geom_line(size=1)+\r\n  scale_color_viridis(discrete=TRUE,labels=c(""Mandate"",""MSW"",""Purpose Grown"",""Residue""))+\r\n  theme_bw()+ylab(""Biomass Production (EJ)"")+theme(axis.title= element_text(size=12),axis.text = element_text(size=12))\r\n\r\n\r\nplot_grid(P1,P2,P3,labels=""auto"",ncol=1)\r\nggsave(""methods.png"",dpi=300)\r\n\r\n', '######################## single metric plots ###############################################\r\n\r\n# Carbon\r\n\r\n# LUC emissions\r\n\r\nluce<-readRDS(\'query_data/LUC-emissions.rds\')\r\n\r\nluce %>% select(-Base_Value) -> luce_pol\r\n\r\nluce %>% select(-Policy_Value,-diff) -> luce_base\r\n\r\nluce_pol %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,year,Units)%>%\r\n  summarize(diff=sum(diff,na.rm=T))->lp\r\n\r\nluce_base %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,year,Units)%>%\r\n  summarize(diff=sum(Base_Value,na.rm=T)) %>% mutate(Policy=""Baseline"") -> lb\r\n\r\nlp %>% full_join(lb,by=names(lb)) ->l\r\n\r\n\r\n# FFI emissions\r\n\r\ncarbon<-readRDS(\'query_data/carbon.rds\')\r\n\r\n\r\n\r\ncarbon %>% select(-Base_Value) -> carbon_pol\r\n\r\ncarbon %>% select(-Policy_Value,-diff) -> carbon_base\r\n\r\ncarbon_pol %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,year,Units)%>%\r\n  summarize(diff=sum(diff))->cp\r\n\r\ncarbon_base %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,year,Units)%>%\r\n  summarize(diff=sum(Base_Value)) %>% mutate(Policy=""Baseline"") -> cb\r\n\r\ncp %>% full_join(cb,by=names(cb)) ->c\r\n\r\n\r\n\r\n\r\nc %>% mutate(SSP=substr(SSP,4,4)) %>%\r\n  left_join(l,by=c(""SSP"",""Socioeconomics"",""agLU"",""RCP"",""ESM"",""CropModel"",""Policy"",""year"")) %>%\r\n  mutate(emissions=diff.x+(diff.y)) -> allc1\r\n\r\n\r\nallc1 %>% tidyr::unite(scenario,SSP,Socioeconomics,agLU,RCP,ESM,CropModel,sep=""_"")%>%\r\n  mutate(SSP=substr(scenario,1,1))->all2\r\n\r\n\r\n\r\n\r\n\r\nggplot(all2)+\r\n  geom_smooth(stat = \'summary\', alpha = 0.2, aes(x=year,y=emissions,fill = SSP, color = SSP),\r\n              fun.data = median_hilow, fun.args = list(conf.int = 1)) + \r\n  facet_wrap(~factor(Policy,levels=c(""Baseline"",""All Biofuels"",""Second Gen. Biofuels"",""Land Conservation"",""All Biofuels and Land Conservation"",""Second Gen. Biofuels and Land Conservation"")),scales=""free"") +\r\n  theme_bw()+scale_x_continuous(expand=c(0,0))+scale_color_viridis(option=""C"",discrete=TRUE)+\r\n  scale_fill_viridis(discrete = TRUE,option=""C"")+\r\n  ylab(bquote(""Baseline and Change in Carbon Emissions (MTC)""))+theme(axis.title=element_text(size=16))\r\n\r\nggsave(""carbon.png"",dpi=300)\r\n\r\n##########################################################################\r\n\r\n\r\n# water\r\n\r\nwater<-readRDS(\'query_data/water.rds\')\r\n\r\nwater %>% select(-Base_Value) -> water_pol\r\n\r\nwater %>% select(-Policy_Value,-diff) -> water_base\r\n\r\nwater_pol %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,year,Units)%>%\r\n  summarize(diff=sum(diff))->wp\r\n\r\nwater_base %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,year,Units)%>%\r\n  summarize(diff=sum(Base_Value)) %>% mutate(Policy=""Baseline"") -> wb\r\n\r\nwp %>% full_join(wb,by=names(wb)) ->w\r\n\r\n\r\n\r\n\r\nggplot(w)+\r\n  geom_smooth(stat = \'summary\', alpha = 0.2, aes(x=year,y=diff,fill = SSP, color = SSP),\r\n              fun.data = median_hilow, fun.args = list(conf.int = 1)) + \r\n  facet_wrap(~factor(Policy,levels=c(""Baseline"",""All Biofuels"",""Second Gen. Biofuels"",""Land Conservation"",""All Biofuels and Land Conservation"",""Second Gen. Biofuels and Land Conservation"")),scales=""free"") +\r\n  theme_bw()+scale_x_continuous(expand=c(0,0))+scale_color_viridis(discrete=TRUE,option=""C"")+\r\n  scale_fill_viridis(discrete = TRUE,option=""C"")+\r\n  ylab(bquote(""Baseline and Change in Water Withdrawals ""~(km^3)))+theme(axis.title=element_text(size=16))\r\n       \r\nggsave(""water.png"",dpi=300)       \r\n\r\n################################################################################################################\r\n# ag prices\r\n\r\n\r\nprice<-readRDS(\'query_data/ag-prices.rds\')\r\n\r\ncrops=c(""Corn"",""FiberCrop"",""MiscCrop"",""OilCrop"",""OtherGrain"",""PalmFruit"",""Rice"",""RootTuber"",""SugarCrop"",""Wheat"")\r\n\r\nprice %>% filter(sector %in% crops)->price\r\n\r\nprice %>% mutate(perc=(diff/Base_Value)*100)->price\r\n\r\n\r\nprice %>% select(-Base_Value) -> price_pol\r\n\r\nprice %>% select(-Policy_Value,-diff) -> price_base\r\n\r\nprice_pol %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,year,Units)%>%\r\n  summarize(perc=mean(perc))->pp\r\n\r\nprice_base %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,year,Units)%>%\r\n  summarize(perc=mean(Base_Value)) %>% mutate(Policy=""Baseline"") -> pb\r\n\r\npp %>% full_join(pb,by=names(pb)) ->p\r\n\r\n\r\n\r\n\r\nggplot(p)+\r\n  geom_smooth(stat = \'summary\', alpha = 0.2, aes(x=year,y=perc,fill = SSP, color = SSP),\r\n              fun.data = median_hilow, fun.args = list(conf.int = 1)) + \r\n  #stat_summary(geom=""ribbon"",fun.data=""mean_cl_boot"",fun.args=list(conf.int=0.1),aes(x=year,y=perc,fill=SSP),alpha=0.5)+\r\n  facet_wrap(~factor(Policy,levels=c(""Baseline"",""All Biofuels"",""Second Gen. Biofuels"",""Land Conservation"",""All Biofuels and Land Conservation"",""Second Gen. Biofuels and Land Conservation"")),scales=""free"") +\r\n  theme_bw()+scale_x_continuous(expand=c(0,0))+scale_fill_viridis(discrete = TRUE,option = ""C"")+\r\n  scale_color_viridis(discrete = TRUE,option = ""C"")+\r\n  ylab(bquote(""Baseline and Change in Crop Prices (%)""))+theme(axis.title=element_text(size=16))\r\n\r\nggsave(""agprices_c.png"",dpi=300)\r\n\r\n##################################################################################################################\r\n# production\r\n\r\nprod<-readRDS(\'query_data/ag-production.rds\')\r\n\r\ncrops=c(""Corn"",""FiberCrop"",""MiscCrop"",""OilCrop"",""OtherGrain"",""PalmFruit"",""Rice"",""RootTuber"",""SugarCrop"",""Wheat"")\r\n\r\nprod %>% filter(sector %in% crops)->prod\r\n\r\nprod %>% select(-Base_Value) -> prod_pol\r\n\r\nprod %>% select(-Policy_Value,-diff) -> prod_base\r\n\r\nprod_pol %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,year,Units)%>%\r\n  summarize(diff=sum(diff))->ppr\r\n\r\nprod_base %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,year,Units)%>%\r\n  summarize(diff=sum(Base_Value)) %>% mutate(Policy=""Baseline"") -> pbr\r\n\r\nppr %>% full_join(pbr,by=names(pbr)) ->pr\r\n\r\n\r\n\r\nggplot(pr)+\r\n  geom_smooth(stat = \'summary\', alpha = 0.2, aes(x=year,y=diff,fill = SSP, color = SSP),\r\n              fun.data = median_hilow, fun.args = list(conf.int = 1)) + \r\n  #stat_summary(geom=""ribbon"",fun.data=""mean_cl_boot"",fun.args=list(conf.int=0.95),aes(x=year,y=diff,fill=SSP),alpha=0.5)+\r\n  #geom_line(aes(year,mean,col=SSP))+\r\n  facet_wrap(~factor(Policy,levels=c(""Baseline"",""All Biofuels"",""Second Gen. Biofuels"",""Land Conservation"",""All Biofuels and Land Conservation"",""Second Gen. Biofuels and Land Conservation"")),scales=""free"") +\r\n  theme_bw()+scale_x_continuous(expand=c(0,0))+scale_fill_viridis(discrete = TRUE,option = ""C"")+\r\n  scale_color_viridis(discrete = TRUE,option = ""C"")+\r\n  ylab(bquote(""Baseline and Change in Crop Production (Mt)""))+theme(axis.title=element_text(size=16))\r\n\r\nggsave(""ag_production.png"",dpi=300)\r\n', '# tradeoff plot\r\nlibrary(GGally)\r\nlibrary(ggplot2)\r\nlibrary(viridis)\r\nlibrary(dplyr)\r\n\r\n\r\n# read in metric dat and summarize by scenario\r\n\r\nprice<-readRDS(\'query_data/ag-prices.rds\')\r\n\r\n\r\n\r\nprice %>% dplyr::mutate(perc=(diff/Base_Value)*100) %>% dplyr::group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,region)%>%\r\n  dplyr::summarise(diff=mean(perc,na.rm=T)) -> price1\r\n\r\nprice1$Metric=""Crop Price""\r\n\r\n\r\n\r\nprod<-readRDS(\'query_data/ag-production.rds\')\r\n\r\n\r\nprod %>% dplyr::mutate(perc=(diff/Base_Value)*100) %>%group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,region)%>%\r\n  summarize(diff=mean(perc,na.rm=T)) -> prod1\r\n\r\n\r\nprod1$Metric=""Crop Production""\r\n\r\n\r\nwater<-readRDS(\'query_data/water.rds\')\r\n\r\nwater %>% dplyr::mutate(perc=(diff/Base_Value)*100)%>%group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,region)%>%\r\n  summarize(diff=mean(perc,na.rm=T)) -> water1\r\n\r\n\r\n\r\nwater1$Metric=""Water Withdrawals""\r\n\r\n\r\nluce<-readRDS(\'query_data/LUC-emissions.rds\')\r\n\r\n\r\nluce %>%group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,region)%>%\r\n  summarize(diff=sum(diff,na.rm=T),Base_Value=sum(Base_Value,na.rm=T)) -> luce1\r\n\r\n\r\nc<-readRDS(\'query_data/carbon.rds\')\r\n\r\n\r\n\r\nc %>%group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,region)%>%\r\n  summarize(diff=sum(diff,na.rm=T),Base_Value=sum(Base_Value,na.rm=T)) -> c1\r\n\r\n\r\n\r\nc1 %>% mutate(SSP=substr(SSP,4,4)) %>%\r\n  left_join(luce1,by=c(""SSP"",""Socioeconomics"",""agLU"",""RCP"",""ESM"",""CropModel"",""Policy"",""region"")) %>%\r\n  mutate(diff=((diff.x+diff.y)/(Base_Value.x+Base_Value.y))*100) -> allc1\r\n\r\nallc1$Metric=""Carbon Emissions""\r\n\r\nna.omit(allc1)->allc1\r\n\r\n### bind everything\r\n\r\nprice1 %>% mutate(SSP=substr(SSP,4,4)) %>% select(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,diff,Metric,region) %>% mutate(diff=diff*-1)->price1\r\nprod1 %>% mutate(SSP=substr(SSP,4,4))%>% select(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,diff,Metric,region) -> prod1\r\nwater1 %>% mutate(SSP=substr(SSP,4,4))%>% select(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,diff,Metric,region)%>%\r\n  mutate(diff=-diff)->water1\r\nallc1 %>% select(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,diff,Metric,region) %>%\r\n  mutate(diff=-diff)-> allc1\r\n\r\nrbind(price1,prod1,water1,allc1) -> env1\r\n\r\n\r\n# filter out specific regions to plot\r\n\r\nenv1 %>% filter(region==""Brazil""|region==""USA""|region==""Pakistan""|region==""India""|region==""Africa_Eastern"")->env2\r\n\r\n# Average all dimensions but plotted ones\r\n\r\nenv2 %>% group_by(SSP,Policy,Metric,region) %>% summarize(diff=mean(diff,na.rm=T))->pl\r\n\r\n# get data into format needed by ggparcoord\r\n\r\n\r\n\r\npl %>% mutate(diff=sign(diff)*log10(abs(diff)+1))%>% tidyr::spread(region,diff)->pltest\r\n\r\n\r\npltest %>% tidyr::gather(region,diff,Africa_Eastern:USA)->pltest2\r\npltest2 %>% tidyr::spread(Metric,diff)-> pl2\r\npl2 %>% ungroup() ->pl2\r\n\r\npl2$SSP=as.factor(pl2$SSP)\r\n\r\npl2 %>% filter(Policy !=""Baseline"")->pl3\r\n\r\npl3 %>% rename(Water=`Water Withdrawals`,Carbon=`Carbon Emissions`,Price=`Crop Price`,Production=`Crop Production`)->pl3\r\n\r\n# plot\r\n\r\np2<-ggparcoord(pl3,mapping=aes(linetype=as.factor(SSP)),columns = 4:7,groupColumn = ""region"", scale=""globalminmax"",order=c(5,4,6,7))+theme_bw()+\r\n  scale_x_discrete(expand=c(0,0))+ \r\n  facet_wrap(~factor(Policy,levels=c(""All Biofuels"",""All Biofuels and Land Conservation"",""Second Gen. Biofuels"",""Second Gen. Biofuels and Land Conservation"",""Land Conservation"")),ncol=2)+   # Adjust alpha argument\r\n  guides(color = guide_legend(override.aes = list(alpha = 1)))+\r\n  ylab(""Log Modulus of % Change"")+xlab(""Metric"")+labs(linetype=""SSP"")+\r\n  scale_color_brewer(palette = ""Dark2"")+\r\n  guides(colour = guide_legend(ncol = 2),linetype = guide_legend(ncol =2))+\r\n  theme(axis.title=element_text(size=18),axis.text.y=element_text(size=12),axis.text.x=element_text(hjust=0.4,vjust=.5,size=12,angle=90),legend.position = c(.76,0.075),legend.box=""horizontal"")\r\np2$data %>% mutate(Policy=plyr::mapvalues(Policy,1:5,unique(pl3$Policy)))->p2$data\r\n\r\np2\r\n\r\n\r\nggsave(""tradeoffs.png"",p2,dpi=300)\r\n', '# variance explained plot\r\n\r\n\r\nlibrary(viridis)\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(broom)\r\n\r\n# read in metric dat and summarize by scenario\r\n\r\nprice<-readRDS(\'query_data/ag-prices.rds\')\r\n\r\nprice %>% mutate(perc=diff/Base_Value) %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,region)%>%\r\n  summarize(perc=mean(perc,na.rm=T)) -> price1\r\n\r\nprice1$diff=price1$perc\r\nprice1$Metric=""Crop Price""\r\n\r\nprod<-readRDS(\'query_data/ag-production.rds\')\r\n\r\nprod %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,region)%>%\r\n  summarize(diff=sum(diff,na.rm=T)) -> prod1\r\n\r\n\r\nprod1$Metric=""Crop Production""\r\n\r\nwater<-readRDS(\'query_data/water.rds\')\r\n\r\nwater %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,region)%>%\r\n  summarize(diff=sum(diff,na.rm=T)) -> water1\r\n\r\n\r\n\r\nwater1$Metric=""Water Withdrawals""\r\n\r\n\r\nluce<-readRDS(\'query_data/LUC-emissions.rds\')\r\n\r\nluce %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,region)%>%\r\n  summarize(diff=sum(diff,na.rm=T)) -> luce1\r\n\r\n\r\nc<-readRDS(\'query_data/carbon.rds\')\r\n\r\n\r\nc %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,region)%>%\r\n  summarize(diff=sum(diff,na.rm=T)) -> c1\r\n\r\n\r\n\r\nc1 %>% mutate(SSP=substr(SSP,4,4),diff=diff*1e6) %>%\r\n  left_join(luce1,by=c(""SSP"",""Socioeconomics"",""agLU"",""RCP"",""ESM"",""CropModel"",""Policy"",""region"")) %>%\r\n  mutate(diff=diff.x+diff.y) -> allc1\r\n\r\nallc1$Metric=""Carbon Emissions""\r\n\r\n\r\n### bind everything\r\n\r\nprice1 %>% mutate(SSP=substr(SSP,4,4)) %>% select(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,diff,Metric,region) %>% mutate(diff=diff*-1)->price1\r\nprod1 %>% mutate(SSP=substr(SSP,4,4))%>% select(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,diff,Metric,region) -> prod1\r\nwater1 %>% mutate(SSP=substr(SSP,4,4))%>% select(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,diff,Metric,region)%>%\r\n  mutate(diff=-diff)->water1\r\nallc1 %>% select(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,diff,Metric,region) %>%\r\n  mutate(diff=-diff)-> allc1\r\n\r\nrbind(price1,prod1,water1,allc1) -> env1\r\n\r\n\r\n\r\n\r\nenv1 %>% group_by(SSP,Socioeconomics,agLU,RCP,ESM,CropModel,Policy,Metric) %>%\r\n  summarize(diff=sum(diff,na.rm=T))->sum\r\n\r\nunique(sum$Policy)->pol\r\nunique(sum$Metric)->met\r\n\r\nP=c()\r\n\r\nfor (p in pol){\r\n  for (m in met){\r\n    \r\n    sum %>% filter(Policy==p,Metric==m)->dat\r\n    dat$agLU = paste0(dat$agLU, \'_ag\')\r\n    dat$Socioeconomics = paste0(dat$Socioeconomics, \'_econ\')\r\n    dat$SSP = paste0(dat$SSP, \'_NoAgEcon\')\r\n    \r\n    anova<-aov(diff~SSP+Socioeconomics+agLU+RCP+ESM+CropModel,data=dat)\r\n    \r\n    broom::tidy(anova)  %>% mutate(varExp=sumsq/sum(sumsq))->varexp\r\n    \r\n    varexp$Metric=m\r\n    varexp$Constraint=p\r\n    \r\n    P=rbind(P,varexp)\r\n  }\r\n}\r\n\r\n\r\nP1=c()\r\n\r\nfor (m in met){\r\n  \r\n  sum %>% filter(Metric==m)->dat\r\n  \r\n  dat$agLU = paste0(dat$agLU, \'_ag\')\r\n  dat$Socioeconomics = paste0(dat$Socioeconomics, \'_econ\')\r\n  dat$SSP = paste0(dat$SSP, \'_NoAgEcon\')\r\n  anova<-aov(diff~SSP+Socioeconomics+agLU+RCP+ESM+CropModel+Policy,data=dat)\r\n  \r\n  broom::tidy(anova)  %>% mutate(varExp=sumsq/sum(sumsq))->varexp1\r\n  \r\n  varexp1$Metric=m\r\n  varexp1$Constraint=""All""\r\n  \r\n  P1=rbind(P1,varexp1)\r\n  \r\n}\r\n\r\nrbind(P,P1)->all\r\n\r\nall$Variable=all$term\r\n\r\nall %>% mutate(Variable=ifelse(Variable==""Policy"",""Constraint"",Variable))->all\r\n\r\nggplot(all,aes(Metric,varExp,fill=Variable))+geom_col(stat=""identity"")+theme_bw()+\r\n  facet_wrap(~factor(Constraint,levels=c(""All"",""All Biofuels"",""Second Gen. Biofuels"",""Land Conservation"",""All Biofuels and Land Conservation"",""Second Gen. Biofuels and Land Conservation"")),labeller =label_wrap_gen(width = 25, multi_line = TRUE))+\r\n  #scale_fill_viridis(discrete=TRUE,option=""B"")+\r\n  scale_fill_brewer(palette = ""Accent"")+\r\n  ylab(""Variance Explained"")+\r\n  theme(legend.text=element_text(size=10),legend.title = element_text(size=14),axis.text=element_text(size=10),axis.title = element_text(size=16),strip.text.x = element_text(size = 10),axis.text.x=element_text(angle = 90))\r\n\r\nggsave(""varexplained_anova.png"",dpi=300)\r\n', '# water scarcity plot\r\n\r\n\r\nwater<-readRDS(""query_data/water.rds"")\r\n\r\nrun<-readRDS(""query_data/Baseline_runoff.rds"")\r\n\r\nrun %>% filter(value !=0)-> run\r\n\r\nwater %>% left_join(run,by=c(""SSP"",""Socioeconomics"",""agLU"",""RCP"",""CropModel"",""ESM"",""region"",""year""))%>%\r\n  mutate(scarcity=diff/value) ->scarcity\r\n\r\nscarcity %>% filter(Policy==""Land Conservation""|Policy==""All Biofuels""|Policy==""Second Gen. Biofuels"")->scarcity\r\n\r\n\r\n\r\nggplot(scarcity,aes(region,scarcity))+geom_boxplot(outlier.shape=NA)+theme_bw()+\r\n  theme(axis.text.x=element_text(angle=90),axis.title=element_text(size=14))+\r\n  ylab(expression(Delta*WSI))+facet_wrap(~Policy,ncol=1)+coord_cartesian(ylim=c(-.25,.45))\r\n\r\nggsave(""wsi.png"",dpi=300)\r\n']","Modeling the economic and environmental impacts of land scarcity under deep uncertainty Code and input files for the Dolan et al. 2021 paper ""Modeling the economic and environmental impacts of land scarcity under deep uncertainty""",1
Interference in the shared-stroop task: a comparison of self- and other-monitoring,"Co-acting participants represent and integrate each other's actions, even when they are not required to monitor one another. However, monitoring the actions of a partner is an important component of successful interactions, and particularly of linguistic interactions. Moreover, monitoring others may rely on similar mechanisms to those that are involved in self-monitoring. In order to investigate the effect of monitoring on shared linguistic representations, we combined a monitoring task with the shared Stroop task. In the shared Stroop task, one participant named the colour of words in one colour (e.g., red) while ignoring stimuli in the other colour (e.g., green); the other participant either named the colour of words in the other colour or did not respond. Crucially, participants either had to provide feedback about the correctness of their partner's response (Experiment 3) or did not (Experiment 2). The results showed that interference was greater when both participants responded than when they did not, but only when partners provided feedback. We argue that feedback increased joint task interference because in order to monitor their partner, participants had to represent their target utterance, and this representation interfered with self-monitoring of their own utterance.","['#Analyses run with R4.1.0\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(afex)\nlibrary(psych)\nlibrary(emmeans)\nlibrary(effectsize)\n\n#Experiment 1 Analysis\nExp1_Stroop <- read.table(""Exp1_with_filter.csv"", head=TRUE, sep="","")\nstr(Exp1_Stroop)\nExp1_Stroop$Participant <- factor(Exp1_Stroop$Participant)\nExp1_Stroop$Congruency <- factor(Exp1_Stroop$Congruency)\n#Descriptive Statistics\n#mean and sd for RT Trimmed\ndescribeBy(Exp1_Stroop$RTTrimmed, Exp1_Stroop$Congruency)\n#count of Errors **note the number of errors does not include those removed by trimming**\ngroup_by(Exp1_Stroop, Congruency) %>% \n  dplyr::summarise(sumNA = sum(is.na(RT))) #this shows errors before RTs excluded\n\n#RT Analysis\nExp1 <-group_by(Exp1_Stroop, Participant, Congruency) %>%\n  dplyr::summarise(n = n(),\n                   mean = mean(RTTrimmed, na.rm = TRUE),\n                   sd = sd(RTTrimmed, na.rm = TRUE)\n  )\nExp1.Comp <- t.test(mean ~ Congruency, data = Exp1, paired = TRUE, var.equal = TRUE)\nExp1.Comp\ncohens_d(mean ~ Congruency, data = Exp1, paired = TRUE)\n\n#Accuracy Analysis\nExp1_Err <-group_by(Exp1_Stroop, Participant, Congruency) %>%\n  dplyr::summarise(n = n(),\n                   mean = mean(Correct, na.rm = TRUE),\n                   sum = sum(Correct),\n  )\nExp1.ErrComp <- t.test(sum() ~ Congruency, data = Exp1_Err, paired = TRUE)\nExp1.ErrComp\ncohens_d(sum ~ Congruency, data = Exp1_Err, paired = TRUE)\n\n#Experiment 2 Analysis\nCombExp <- read.csv(""Exp2+3_Stroop.csv"")\nExp2_nofb <- filter(CombExp, Experiment ==""2"")\nstr(Exp2_nofb)\nExp2_nofb$Participant <- factor(Exp2_nofb$Participant)\nExp2_nofb$PairCondition_reduced <- factor(Exp2_nofb$PairCondition_reduced)\nExp2_nofb$Congruency <- factor(Exp2_nofb$Congruency)\n\n#Descriptive Statistics\n#mean and sd for RT Trimmed\ndescribeBy(Exp2_nofb$TrimmedRT, list(Exp2_nofb$Congruency, Exp2_nofb$PairCondition_reduced))\n#descriptive statistics for errors\nsum(is.na(Exp2_nofb$RT))\ngroup_by(Exp2_nofb, PairCondition_reduced, Congruency) %>% \n  dplyr::summarise(sumNA = sum(is.na(RT))) #this shows errors before RTs excluded\ngroup_by(Exp2_nofb, PairCondition_reduced, Congruency) %>% \n  dplyr::summarise(sumNA = sum(is.na(TrimmedRT))) #this is overall and includes exclusions\n#graph\nnofb.BarGraph<-ggplot(Exp2_nofb, aes(Congruency, TrimmedRT, fill=Congruency)) +\n  geom_bar(stat=""summary"", fun=""mean"", position = ""dodge"") + \n  facet_grid(.~PairCondition_reduced) +\n  xlab(""Condition"") + ylab(""Reaction Time"") +\n  scale_fill_brewer(palette=""Dark2"") +\n  theme(legend.position=""none"")\nnofb.BarGraph\n#with error bars\ndata_summary <- function(data, varname, groupnames){\n  require(plyr)\n  summary_func <- function(x, col){\n    c(mean = mean(x[[col]], na.rm=TRUE),\n      sd = sd(x[[col]], na.rm=TRUE))\n  }\n  data_sum<-ddply(data, groupnames, .fun=summary_func,\n                  varname)\n  data_sum <- rename(data_sum, c(""mean"" = varname))\n  return(data_sum)\n}\ndata2 <- data_summary(nofbstroop, varname=""TrimmedRT"", \n                    groupnames=c(""PairCondition_reduced"", ""Condition""))\nhead(data2)\nggplot(data2, aes(x=Condition, y=TrimmedRT, fill=Condition)) + \n  geom_bar(stat=""summary"", fun = ""mean"", color=""black"", \n           position=position_dodge()) +\n  geom_errorbar(aes(ymin=TrimmedRT-sd, ymax=TrimmedRT+sd), width=.2,\n                position=position_dodge(.9))+\n  facet_grid(.~PairCondition_reduced) +\n  xlab(""Condition"") + ylab(""Reaction Time"") +\n  theme(legend.position=""none"") +\n  theme_minimal()\n\n#ANOVA\n#RT Analysis\n# Exclude the missing observations\nExp2_nofb_drop <-Exp2_nofb %>%\n  na.omit()\t\t\ndim(Exp2_nofb_drop)\n# IV (between): PairCondition_reduced\n# IV (within): Condition\n# DV:          TrimmedRT\nExp2_nofb_drop$Participant <- factor(Exp2_nofb_drop$Participant)\nExp2_nofb_drop$PairCondition_reduced <- factor(Exp2_nofb_drop$PairCondition_reduced)\nExp2_nofb_drop$Congruency <- factor(Exp2_nofb_drop$Congruency)\n\naov_nofb <- aov_car(TrimmedRT ~ PairCondition_reduced*Congruency + Error(Participant/Congruency), data=Exp2_nofb_drop, anova_table = list(es = ""pes""))\nnice(aov_nofb)\nemmeans(aov_nofb, specs = pairwise ~ Congruency|PairCondition_reduced, type = ""response"")\n\n#Accuracy Analysis - uses the data with NAs included!\naov_nofb_err <- aov_car(Errors ~ PairCondition_reduced*Congruency + Error(Participant/Congruency), data=Exp2_nofb, anova_table = list(es = ""pes""))\nnice(aov_nofb_err)\nemmeans(aov_nofb_err, specs = pairwise ~ Congruency|PairCondition_reduced, type = ""response"")\n\n#Experiment 3 Analysis\nExp3_fb <- filter(CombExp, Experiment ==""3"")\nstr(Exp3_fb)\nExp3_fb$Participant <- factor(Exp3_fb$Participant)\nExp3_fb$PairCondition_reduced <- factor(Exp3_fb$PairCondition_reduced)\nExp3_fb$Congruency <- factor(Exp3_fb$Congruency)\n\n#Descriptive Statistics\n#mean and sd for RT Trimmed\ndescribeBy(Exp3_fb$TrimmedRT, list(Exp3_fb$Congruency, Exp3_fb$PairCondition_reduced))\n#descriptive statistics for errors\ngroup_by(Exp3_fb, PairCondition_reduced, Congruency) %>% \n  dplyr::summarise(sumNA = sum(is.na(RT))) #this shows errors before RTs excluded\ngroup_by(Exp3_fb, PairCondition_reduced, Congruency) %>% \n  dplyr::summarise(sumNA = sum(is.na(TrimmedRT))) #this is overall, and includes exclusions\n#graph\nfb.BarGraph<-ggplot(Exp3_fb, aes(Congruency, TrimmedRT, fill=Congruency)) +\n  geom_bar(stat=""summary"", fun=""mean"", position = ""dodge"") + \n  facet_grid(.~PairCondition_reduced) +\n  xlab(""Condition"") + ylab(""Reaction Time"") +\n  scale_fill_brewer(palette=""Dark2"") +\n  theme(legend.position=""none"")\nfb.BarGraph\n#with error bars\ndata_summary <- function(data, varname, groupnames){\n  require(plyr)\n  summary_func <- function(x, col){\n    c(mean = mean(x[[col]], na.rm=TRUE),\n      sd = sd(x[[col]], na.rm=TRUE))\n  }\n  data_sum<-ddply(data, groupnames, .fun=summary_func,\n                  varname)\n  data_sum <- rename(data_sum, c(""mean"" = varname))\n  return(data_sum)\n}\ndata2 <- data_summary(Exp3_fb, varname=""TrimmedRT"", \n                      groupnames=c(""PairCondition_reduced"", ""Congruency""))\nhead(data2)\nggplot(data2, aes(x=Congruency, y=TrimmedRT, fill=Congruency)) + \n  geom_bar(stat=""summary"", fun = ""mean"", color=""black"", \n           position=position_dodge()) +\n  geom_errorbar(aes(ymin=TrimmedRT-sd, ymax=TrimmedRT+sd), width=.2,\n                position=position_dodge(.9))+\n  facet_grid(.~PairCondition_reduced) +\n  xlab(""Condition"") + ylab(""Reaction Time"") +\n  theme(legend.position=""none"") +\n  theme_minimal()\n\n#ANOVA\n#RT Analysis\n# Exclude the missing observations\nExp3_fb_drop <-Exp3_fb %>%\n  na.omit()\t\t\ndim(Exp3_fb_drop)\n# IV (between): PairCondition_reduced\n# IV (within): Condition\n# DV:          TrimmedRT\nExp3_fb_drop$Participant <- factor(Exp3_fb_drop$Participant)\nExp3_fb_drop$PairCondition_reduced <- factor(Exp3_fb_drop$PairCondition_reduced)\nExp3_fb_drop$Congruency <- factor(Exp3_fb_drop$Congruency)\n\naov_fb <- aov_car(TrimmedRT ~ PairCondition_reduced*Congruency + Error(Participant/Congruency), data=Exp3_fb_drop, anova_table = list(es = ""pes""))\nnice(aov_fb)\nemmeans(aov_fb, specs = pairwise ~ Congruency|PairCondition_reduced, type = ""response"")\n\n#Accuracy Analysis - uses the data with NAs included!\naov_fb_err <- aov_car(Errors ~ PairCondition_reduced*Congruency + Error(Participant/Congruency), data=Exp3_fb, anova_table = list(es = ""pes""))\nnice(aov_fb_err)\nemmeans(aov_fb_err, specs = pairwise ~ Congruency|PairCondition_reduced, type = ""response"")\n\n#CombinedAnalysis\n#RT Analysis\n# Exclude the missing observations\nCombExp_drop <-CombExp %>%\n  na.omit()\t\t\ndim(CombExp_drop)\n# IV (between): Experiment\n# IV (between): PairCondition_reduced\n# IV (within): Condition\n# DV:          TrimmedRT\nCombExp_drop$Experiment <- factor(CombExp_drop$Experiment)\nCombExp_drop$Participant <- factor(CombExp_drop$Participant)\nCombExp_drop$PairCondition_reduced <- factor(CombExp_drop$PairCondition_reduced)\nCombExp_drop$Congruency <- factor(CombExp_drop$Congruency)\n\naov_both <- aov_car(TrimmedRT ~ Experiment*PairCondition_reduced*Congruency + Error(Participant/Congruency), data=CombExp_drop, anova_table = list(es = ""pes""))\nnice(aov_both)\n\n#Accuracy Analysis - uses the data with NAs included!\naov_both_err <- aov_car(Errors ~ Experiment*PairCondition_reduced*Congruency + Error(Participant/Congruency), data=CombExp, anova_table = list(es = ""pes""))\nnice(aov_both_err)\n']","Interference in the shared-stroop task: a comparison of self- and other-monitoring Co-acting participants represent and integrate each other's actions, even when they are not required to monitor one another. However, monitoring the actions of a partner is an important component of successful interactions, and particularly of linguistic interactions. Moreover, monitoring others may rely on similar mechanisms to those that are involved in self-monitoring. In order to investigate the effect of monitoring on shared linguistic representations, we combined a monitoring task with the shared Stroop task. In the shared Stroop task, one participant named the colour of words in one colour (e.g., red) while ignoring stimuli in the other colour (e.g., green); the other participant either named the colour of words in the other colour or did not respond. Crucially, participants either had to provide feedback about the correctness of their partner's response (Experiment 3) or did not (Experiment 2). The results showed that interference was greater when both participants responded than when they did not, but only when partners provided feedback. We argue that feedback increased joint task interference because in order to monitor their partner, participants had to represent their target utterance, and this representation interfered with self-monitoring of their own utterance.",1
"Data repository for the publication ""Economic Interests Cloud Hazard Reductions in the European Regulation of Substances of Very High Concern""","This repository contains the data and scripts associated with the article Economic Interests Cloud Hazard Reductions in the European Regulation of Substances of Very High Concern, written by Jessica Coria, Erik Kristiansson and Mikael Gustavsson.",,"Data repository for the publication ""Economic Interests Cloud Hazard Reductions in the European Regulation of Substances of Very High Concern"" This repository contains the data and scripts associated with the article Economic Interests Cloud Hazard Reductions in the European Regulation of Substances of Very High Concern, written by Jessica Coria, Erik Kristiansson and Mikael Gustavsson.",1
Sub-national Water-Food-Labour Nexus in Colombia,"Poorer countries often face a severe trade-off: the need to improve socio-economic conditions is hard to balance with the maintenance of key ecological processes. As a case study, we select Colombia, a Latin American country with almost 10% of its inhabitants living in extreme poverty. We elaborate a water-food-labour (WFL) nexus grounded on a sub-national Environmentally Extended Input-Output (EEIO) analysis to assess the virtual water trade (VWT) and virtual informal labour (VIL) flows across administrative departments and economic sectors related to domestic trade.The main results are the following: high cross-departmental resource interdependence both in terms of VWT and VIL, rich departments highly depend on the resources of their neighbouring trading partners, extreme poverty conditions are shown by economically isolated departments, and considerable income inequality in the food production sectors. Moreover, departments that are net exporters of virtual water suffer from water stress that might be exacerbated by future high rainfall variability due to climate change. These results suggest that strategies to attain sustainable development goals (SDGs) must deal with the biophysical constraints and the economic and political feasibility of the proposed solutions. In this vein, we argue that a holistic framework, grounded on quantitative analyses, is necessary to support informed policy decisions for the simultaneous achievement of multiple (possibly contrasting) goals. Moreover, severe spatial imbalances call for local policy responses coordinated at the national level.","['---\r\ntitle: ""Chord DiagramS""\r\nauthor: ""Tatiana Builes""\r\ndate: ""04/11/2020""\r\n---\r\n  \r\nlibrary(circlize)\r\nlibrary(RColorBrewer)\r\n\r\nsetwd(""D:/PROYECTOS/OTROS/08-FOOTPRINT"")\r\n\r\ncircos.clear()\r\ngrid.col = rev(brewer.pal(10, \'Spectral\'))\r\n\r\ncircos.par(gap.after= c(5), start.degree = 85, clock.wise = TRUE)\r\n\r\n#=================================================================================================================================================================\r\n\r\n#MONETARY VALUES\r\nmatMV=as.matrix(read.table(\'VirtualFlowsS8Vf.csv\', sep=\',\', dec=\'.\', skip=1, nrows=9, \r\n                           colClasses=c(rep(\'NULL\',1), rep(\'numeric\', 10))))\r\n\r\ndimnames (matMV)= list(c(\'Agr\',\'Coff\',\'Anim\',\'Wood\',\'Fish\',\'Food\',\'Enregy\',\'Others\',\'RoW\'),\r\n                       c(\'Agr\',\'Coff\',\'Anim\',\'Wood\',\'Fish\',\'Food\',\'Enregy\',\'Others\',\'Final Demand\',\'RoW\'))\r\nmatMV\r\n\r\ncircos.clear()\r\n\r\nX11()\r\nMV=chordDiagram(matMV, grid.col=grid.col, transparency = 0, self.link = 1, \r\n             link.sort = TRUE, link.decreasing = TRUE, directional = 1, \r\n             direction.type = c(\'arrows\'),link.arr.type = \'big.arrow\', scale = FALSE, \r\n             annotationTrack = \'grid\', preAllocateTracks = list(track.height = mm_h(5)))\r\n              for(si in get.all.sector.index()) \r\n              {circos.axis(h = ""top"", labels.cex = 1.7, sector.index = si, track.index = 2)}\r\n\r\ntitle(""Monetary Flows"", cex.main= 2)\r\n\r\nlegend=legend(1, 1, legend=c(\'Agriculture\',\'Coffee\',\'Animal\',\'Forestry and Wood\',\'Fish\',\'Food Process\',\'Energy\',\'Others\',\'RoW\',\'Final demand\'), fill=grid.col, title= \'Economic sectors\',\r\n              bty=1, cex=1.25, box.lty=1, box.lwd =0 , inset=0.02, box.col=\'white\')\r\n\r\n\r\n#=================================================================================================================================================================\r\n\r\n#VIRTUAL WATER \r\nmatVW=as.matrix(read.table(\'VirtualFlowsS8Vf.csv\', sep=\',\', dec=\'.\', skip=12, nrows=9, \r\n                           colClasses=c(rep(\'NULL\',1), rep(\'numeric\', 10))))\r\n\r\ndimnames (matVW)= list(c(\'Agr\',\'Coff\',\'Anim\',\'Wood\',\'Fish\',\'Food\',\'Enregy\',\'Others\',\'RoW\'),\r\n                       c(\'Agr\',\'Coff\',\'Anim\',\'Wood\',\'Fish\',\'Food\',\'Enregy\',\'Others\',\'Final Demand\',\'RoW\'))\r\nmatVW\r\n\r\ncircos.clear()\r\n\r\ncircos.par(gap.after= c(5), start.degree = 85, clock.wise = TRUE)\r\n\r\nX11()\r\nVW=chordDiagram(matVW, grid.col=grid.col, transparency = 0, self.link = 1, \r\n                link.sort = TRUE, link.decreasing = TRUE, directional = 1, \r\n                direction.type = c(\'arrows\'),link.arr.type = \'big.arrow\', scale = FALSE, \r\n                annotationTrack = \'grid\', preAllocateTracks = list(track.height = mm_h(5)))\r\n                for(si in get.all.sector.index()) \r\n                {circos.axis(h = ""top"", labels.cex = 1.7, sector.index = si, track.index = 2)}\r\n\r\ntitle(""Virtual Water Trade"", cex.main= 2)\r\n\r\nlegend=legend(1, 1, legend=c(\'Agriculture\',\'Coffee\',\'Animal\',\'Forestry and Wood\',\'Fish\',\'Food Process\',\'Energy\',\'Others\',\'RoW\',\'Final demand\'), fill=grid.col, title= \'Economic sectors\',\r\n              bty=1, cex=1.25, box.lty=1, box.lwd =0 , inset=0.02, box.col=\'white\')\r\n\r\n#=================================================================================================================================================================\r\n\r\n#LABOUR \r\nmatLB=as.matrix(read.table(\'VirtualFlowsS8Vf.csv\', sep=\',\', dec=\'.\', skip=23, nrows=9, \r\n                           colClasses=c(rep(\'NULL\',1), rep(\'numeric\', 10))))\r\n\r\ndimnames (matLB)= list(c(\'Agr\',\'Coff\',\'Anim\',\'Wood\',\'Fish\',\'Food\',\'Enregy\',\'Others\',\'RoW\'),\r\n                       c(\'Agr\',\'Coff\',\'Anim\',\'Wood\',\'Fish\',\'Food\',\'Enregy\',\'Others\',\'Final Demand\',\'RoW\'))\r\nmatLB\r\n\r\ncircos.clear()\r\n\r\ncircos.par(gap.after= c(5), start.degree = 85, clock.wise = TRUE)\r\n\r\n\r\nX11()\r\nLB=chordDiagram(matLB, grid.col=grid.col, transparency = 0, self.link = 1, \r\n                link.sort = TRUE, link.decreasing = TRUE, directional = 1, \r\n                direction.type = c(\'arrows\'),link.arr.type = \'big.arrow\', scale = FALSE, \r\n                annotationTrack = \'grid\', preAllocateTracks = list(track.height = mm_h(5)))\r\n                for(si in get.all.sector.index()) \r\n                {circos.axis(h = ""top"", labels.cex = 1.7, sector.index = si, track.index = 2)}\r\n\r\ntitle(""Virtual Informal Labour"", cex.main= 2)\r\n\r\nlegend=legend(1, 1, legend=c(\'Agriculture\',\'Coffee\',\'Animal\',\'Forestry and Wood\',\'Fish\',\'Food Process\',\'Energy\',\'Others\',\'RoW\',\'Final demand\'), fill=grid.col, title= \'Economic sectors\',\r\n              bty=1, cex=1.25, box.lty=1, box.lwd =0 , inset=0.02, box.col=\'white\')\r\n\r\n']","Sub-national Water-Food-Labour Nexus in Colombia Poorer countries often face a severe trade-off: the need to improve socio-economic conditions is hard to balance with the maintenance of key ecological processes. As a case study, we select Colombia, a Latin American country with almost 10% of its inhabitants living in extreme poverty. We elaborate a water-food-labour (WFL) nexus grounded on a sub-national Environmentally Extended Input-Output (EEIO) analysis to assess the virtual water trade (VWT) and virtual informal labour (VIL) flows across administrative departments and economic sectors related to domestic trade.The main results are the following: high cross-departmental resource interdependence both in terms of VWT and VIL, rich departments highly depend on the resources of their neighbouring trading partners, extreme poverty conditions are shown by economically isolated departments, and considerable income inequality in the food production sectors. Moreover, departments that are net exporters of virtual water suffer from water stress that might be exacerbated by future high rainfall variability due to climate change. These results suggest that strategies to attain sustainable development goals (SDGs) must deal with the biophysical constraints and the economic and political feasibility of the proposed solutions. In this vein, we argue that a holistic framework, grounded on quantitative analyses, is necessary to support informed policy decisions for the simultaneous achievement of multiple (possibly contrasting) goals. Moreover, severe spatial imbalances call for local policy responses coordinated at the national level.",1
"Dataset accompanying the article ""Freedoms and free-market capitalism beyond utopias and dystopias  A World Without Money revised""","The dataset in this repository provides all the data used in the charts contained in the article ""Freedoms and free-market capitalism beyond utopias and dystopias  A World Without Money revised"" (2022) by Fabio Ashtar Telarico published by Foundation for Economic Education.","['# Title:  Figure 2\r\n\r\n# Purpose : This script was created for the\r\n#           article ""Freedoms and free-market\r\n#           capitalism beyond utopias and\r\n#           dystopias  A World Without Money\r\n#           revised""\r\n\r\n# Project number: #0010\r\n\r\n# Author: Fabio A. Telarico\r\n# Contact details: fatelatico@gmail.com\r\n\r\n# Date script created: Sat Jan 29 2022\r\n\r\n# Libraries\r\nif(!required(ggplot2))install.packages(""ggplot2"")\r\nif(!required(ggthemes))install.packages(""ggthemes"")\r\nif(!required(tidyr))install.packages(""tidyr"")\r\n\r\nlibrary(ggplot2)\r\nlibrary(ggthemes)\r\nlibrary(tidyr)\r\n\r\n# Data input\r\nFigure2Data<-\r\n  data.frame(Year=c(1990,1993,1996,1999,2002,2005,2008,2010,2011,2012,2013,2015),\r\n             RelativePoor=c(2351000000,2363000000,2318000000,2394000000,2387000000,2301000000,2275000000,2247000000,2193000000,2181000000,2120000000,2086000000),\r\n             AbsolutePoor=c(1903000000,1876000000,1698000000,1728000000,1609000000,1351000000,1222000000,1089000000,962000000,907000000,803000000,734000000)\r\n             )\r\n\r\n# Data manipulation\r\nFigure2<-data.frame(\r\n  pivot_longer(\r\n    data = Figure2Data,\r\n    cols = c(""RelativePoor"",""AbsolutePoor""),\r\n    names_to = ""Data"")\r\n)\r\n\r\npattern <- unique(Figure2$Data)\r\nFigure2$Data<-gsub(pattern = ""RelativePoor"",\r\n                   replacement = ""In relative poverty"",\r\n                   x = Figure2$Data)\r\nFigure2$Data<-gsub(pattern = ""AbsolutePoor"",\r\n                   replacement = ""In absolute poverty"",\r\n                   x = Figure2$Data)\r\n\r\n# Plot\r\n\r\nbase_text_size<-22\r\n\r\npath<-getwd()\r\n\r\nthemes<-\r\n  theme_wsj()+\r\n  # theme_fivethirtyeight(base_size = base_text_size)+\r\n  # theme_stata(scheme = ""s2mono"")+\r\n  theme(text = element_text(family = ""serif""),\r\n        plot.title=element_text(size=base_text_size, face=""bold""),\r\n        axis.text.x=element_text(size=2*base_text_size, margin = margin(b=.8*base_text_size,t=.25*base_text_size),angle = 90,vjust = 0.5),\r\n        axis.text.y.left=element_text(size=2*base_text_size, margin = margin(l=.8*base_text_size, r=.4*base_text_size),hjust = 1),\r\n        axis.title.x=element_text(size=2.5*base_text_size,margin=margin(b=.1*base_text_size)),\r\n        axis.title.y.left=element_text(size=2.5*base_text_size),\r\n        legend.text = element_text(size=1.5*base_text_size),\r\n        # legend.margin = margin(l=40*base_text_size,t=.1*base_text_size),\r\n        legend.title = element_text(size=1.5*base_text_size,face = ""bold""),\r\n        legend.position=""bottom"",\r\n        legend.direction = ""vertical"",\r\n        legend.box = ""horizontal"",\r\n        legend.box.margin = margin(b=.1*base_text_size),\r\n        panel.grid.major.y = element_line(size = .02*base_text_size,colour = ""gray"",linetype = ""solid""),\r\n        plot.margin = unit(c(.1,.1,0,.1),units = ""cm"")\r\n        \r\n  )\r\n\r\nPlot2<-\r\n  ggplot(data = Figure2, aes(x = Year,y=value/1000000))+\r\n  geom_line(aes(fill=Data,color=Data,linetype=""Data""),\r\n                  size=.13*base_text_size)+\r\n  geom_smooth(aes(fill=Data,color=Data,linetype=""Trend""),\r\n              se = F, size=.13*base_text_size)+\r\n  geom_jitter(aes(fill=Data,color=Data),\r\n              size=.3*base_text_size,\r\n              shape=21)+\r\n  scale_y_continuous(\r\n    name = ""Millions of people"",\r\n    n.breaks = 10\r\n  )+\r\n  scale_x_continuous(\r\n    name = """",\r\n    breaks = seq(1990,2015,1)\r\n  )+\r\n  scale_color_brewer(""Colour"",palette = ""Dark2"",\r\n                     aesthetics = c(""color"",""fill""),direction = 1)+\r\n  scale_linetype_manual(""Line"",values = c(1,3))+\r\n  themes\r\n\r\n# Print plot\r\n\r\npng(filename = paste0(path,""/Plot2.png""),\r\n    width = 1920,height = 1080,units = ""px"")\r\nplot(Plot2)\r\ndev.off()\r\n\r\n# Export data\r\nsaveRDS(object = Figure2,\r\n        file = paste0(path,""/Figure2.RDS""))\r\n\r\nsaveRDS(object = Figure2Data,\r\n        file = paste0(path,""/Figure2_Data.RDS""))\r\n']","Dataset accompanying the article ""Freedoms and free-market capitalism beyond utopias and dystopias  A World Without Money revised"" The dataset in this repository provides all the data used in the charts contained in the article ""Freedoms and free-market capitalism beyond utopias and dystopias  A World Without Money revised"" (2022) by Fabio Ashtar Telarico published by Foundation for Economic Education.",1
Data from: Species interactions limit the occurrence of urban-adapted birds in cities,"Urbanization represents an extreme transformation of more natural systems. Populations of most species decline or disappear with urbanization, and yet some species persist and even thrive in cities. What determines which species persist or thrive in urban habitats? Direct competitive interactions among species can influence their distributions and resource use, particularly along gradients of environmental challenge. Given the challenges of urbanization, similar interactions may be important for determining which species persist or thrive in cities; however, their role remains poorly understood. Here we use a global dataset to test among three alternative hypotheses for how direct competitive interactions and behavioral dominance may influence the breeding occurrence of birds in cities. We find evidence to support the Competitive Interference Hypothesis: behaviorally dominant species were more widespread in urban habitats than closely-related subordinate species, but only in taxa that thrive in urban environments (hereafter, urban-adapted), and only when dominant and subordinate species overlapped their geographic ranges. This result was evident across diverse phylogenetic groups, but varied significantly with a country's level of economic development. Urban-adapted, dominant species were more widespread than closely-related subordinate species in cities in developed, but not developing, countries; countries in economic transition showed an intermediate pattern. Our results provide evidence that competitive interactions broadly influence species responses to urbanization, and that these interactions have asymmetric effects on subordinate species that otherwise could be widespread in urban environments. Results further suggest that economic development might accentuate the consequences of competitive interactions, thereby reducing local diversity in cities.",,"Data from: Species interactions limit the occurrence of urban-adapted birds in cities Urbanization represents an extreme transformation of more natural systems. Populations of most species decline or disappear with urbanization, and yet some species persist and even thrive in cities. What determines which species persist or thrive in urban habitats? Direct competitive interactions among species can influence their distributions and resource use, particularly along gradients of environmental challenge. Given the challenges of urbanization, similar interactions may be important for determining which species persist or thrive in cities; however, their role remains poorly understood. Here we use a global dataset to test among three alternative hypotheses for how direct competitive interactions and behavioral dominance may influence the breeding occurrence of birds in cities. We find evidence to support the Competitive Interference Hypothesis: behaviorally dominant species were more widespread in urban habitats than closely-related subordinate species, but only in taxa that thrive in urban environments (hereafter, urban-adapted), and only when dominant and subordinate species overlapped their geographic ranges. This result was evident across diverse phylogenetic groups, but varied significantly with a country's level of economic development. Urban-adapted, dominant species were more widespread than closely-related subordinate species in cities in developed, but not developing, countries; countries in economic transition showed an intermediate pattern. Our results provide evidence that competitive interactions broadly influence species responses to urbanization, and that these interactions have asymmetric effects on subordinate species that otherwise could be widespread in urban environments. Results further suggest that economic development might accentuate the consequences of competitive interactions, thereby reducing local diversity in cities.",1
Detecting the socio-economic drivers of confidence in government with eXplainable Artificial Intelligence,"Datasets and scripts associated with the articleBellantuono, L., Palmisano, F., Amoroso, N. et al. Detecting the socio-economic drivers of confidence in government with eXplainable Artificial Intelligence. Sci Rep 13, 839 (2023). https://doi.org/10.1038/s41598-023-28020-5","['#install.packages(\'igraph\')\n#install.packages(\'igraphdata\')\n#install.packages(""readxl"")\n\nlibrary(igraph)\nlibrary(igraphdata)\nlibrary(weights)\nlibrary(readxl)\n\n# ASSORTATIVITY OF REGION NETWORK WITH RESPECT TO THE EQI SCORE\n\n# **********************************************************************************************\n# Adjacency matrix of the region network \n# (the imported file adjacency_filtered.csv is obtained in the script EQI_prediction_zenodo.py)\ndf_A_sr_filtered = read.csv(\'/output_files/adjacency_filtered.csv\', header = F)\nA_sr_filtered = as.matrix(df_A_sr_filtered)\n\nA_sr_filtered_non_neg = A_sr_filtered\nA_sr_filtered_non_neg[A_sr_filtered<0] = 0\n\n# **********************************************************************************************\n# Dataframe with a row for each positive-weight link, and columns reporting the EQI score of the source, the EQI score of the target and the weight of the link between them\nto_compute_significance_weighted_correlations_filtered_EQI = read_excel(\'/output_files/to_compute_significance_weighted_correlation_filtered_non_neg_EQI.xlsx\')\n\n# **********************************************************************************************\n\n# Let us compute the assortativity and the associated standard errors and p-values, using the weighted\n# Pearson correlation\nassortativity_filtered_EQI_with_signif = wtd.cor(to_compute_significance_weighted_correlations_filtered_EQI$EQI_source,y=to_compute_significance_weighted_correlations_filtered_EQI$EQI_target, weight=to_compute_significance_weighted_correlations_filtered_EQI$weight)\n\nView(assortativity_filtered_EQI_with_signif)\n# Output:\n# correlation     std.err t.value p.value\n# Y   0.2579705 0.005773249 44.68377       0\n']","Detecting the socio-economic drivers of confidence in government with eXplainable Artificial Intelligence Datasets and scripts associated with the articleBellantuono, L., Palmisano, F., Amoroso, N. et al. Detecting the socio-economic drivers of confidence in government with eXplainable Artificial Intelligence. Sci Rep 13, 839 (2023). https://doi.org/10.1038/s41598-023-28020-5",1
GreenStimuliEARIOModel/GSEARIOModel,"GSEARIOModel: Version 2.0 by Yaxin Zhang (zyx602533716@gmail.com), Xinzhu Zheng------------------------------------------------------------------GSEARIOModelG: GreenS: StimuliE: ExtendedARIO: Adaptive Regional Input-OutputModel------------------------------------------------------------------GSEARIOModel is developed based on Wang (2020)'s model*, to explore the impact of Green stimuli on the economy, job demand and social equity. * Daoping WANG. (2020). DaopingW/economic-impact-model: Disaster Footprint Model (v1.0). Zenodo. https://doi.org/10.5281/zenodo.4290117",,"GreenStimuliEARIOModel/GSEARIOModel GSEARIOModel: Version 2.0 by Yaxin Zhang (zyx602533716@gmail.com), Xinzhu Zheng------------------------------------------------------------------GSEARIOModelG: GreenS: StimuliE: ExtendedARIO: Adaptive Regional Input-OutputModel------------------------------------------------------------------GSEARIOModel is developed based on Wang (2020)'s model*, to explore the impact of Green stimuli on the economy, job demand and social equity. * Daoping WANG. (2020). DaopingW/economic-impact-model: Disaster Footprint Model (v1.0). Zenodo. https://doi.org/10.5281/zenodo.4290117",1
Test Assets for ckanext-mongodatastore (Reinhart&Rogoff Experiment),"This is a collection of digital assets, that are required for the evaluation of the CKAN plugin 'ckanext-mongodatastore'.The assets are based on the data provided by the publication 'Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff'","['## This code and the related data are open-source under the BSD 2-clause license.\n## See http://www.tldrlegal.com/l/BSD2 for details\n## Michael Ash, Thomas Herndon, and Robert Pollin April-May 2013\n## Replicate and extend Reinhart and Rogoff (2010) using data from RR working spreadsheet\n\nlibrary(plyr)\nlibrary(ggplot2)\nlibrary(car)\nlibrary(foreign)\nlibrary(xlsx)\noptions(scipen=10000)\noptions(digits=4)\n\nrm(list = ls())\n\nlibrary(httr)\nlibrary(rlist)\nlibrary(jsonlite)\nlibrary(dplyr)\n\nRR <- read.csv(\'countries_dataset.csv.csv\')\n\n## Cut to postwar analysis\nRR <- subset(RR,Year>=1946 & Year<=2009)\n\n## Italy uses another data series through 1946 and is excluded from GITD postwar until 1951\nRR <- subset(RR, !(Year<1951 & Country==""Italy""))\n\n## Potential data years 1946-2009\navail.data <- ddply(RR, ~Country, summarize,\n                    count.year.GDP=sum(!is.na(dRGDP)),count.year.debt=sum(!is.na(debtgdp)), count.year.both=sum(!is.na(dRGDP) & !is.na(debtgdp)))\navail.data[order(avail.data[,""count.year.both""]),]\n\n## Slow\nRR$dgcat.lm <- cut(RR$debtgdp, breaks=c(0,30,60,90,Inf))\nRR$dgcat <- factor(RR$dgcat.lm, labels = c(""0-30%"",""30-60%"",""60-90%"",""Above 90%""),ordered=TRUE)\n#write.xlsx2(subset(RR,TRUE,select=c(Country,Year,debtgdp,dgcat,dRGDP) ),""RR-keycolumns.xlsx"",row.names=FALSE)\n\n\n## Limit to actually available data\nRR <- subset(RR,  !is.na(dRGDP) & !is.na(debtgdp))\n\n## Actually available data years 1946-2009\navail.data <- ddply(RR, ~Country, summarize, min.year=min(Year), count.year=sum(!is.na(dRGDP) & !is.na(debtgdp)))\navail.data[order(avail.data[,""min.year""]),]\n\nwith(RR,table(Year))\nwith(RR,table(Country))\n\n## Create RR public debt/GDP categories\nRR$dgcat.lm <- cut(RR$debtgdp, breaks=c(0,30,60,90,Inf))\nRR$dgcat <- factor(RR$dgcat.lm, labels = c(""0-30%"",""30-60%"",""60-90%"",""Above 90%""),ordered=TRUE)\n\n## Create expanded public debt/GDP categories\nRR$dgcat2.lm <- cut(RR$debtgdp, breaks=c(0,30,60,90,120,Inf))\nRR$dgcat2 <- factor(RR$dgcat2.lm, labels = c(""0-30%"",""30-60%"",""60-90%"",""90-120%"",""Above 120%""),ordered=TRUE)\n\n## Regression analysis of categories\nsummary(dgcat.lm <- lm(dRGDP ~ dgcat.lm, data=RR))\nsummary(dgcat2.lm <- lm(dRGDP ~ dgcat2.lm, data=RR))\nlinearHypothesis(dgcat2.lm,\n                 verbose=TRUE,\n                 paste( c(""dgcat2.lm(30,60]=dgcat2.lm(60,90]"", ""dgcat2.lm(30,60]=dgcat2.lm(90,120]"", ""dgcat2.lm(30,60]=dgcat2.lm(120,Inf]"") ))\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""dgcat2.lm(30,60]=dgcat2.lm(60,90]"", ""dgcat2.lm(30,60]=dgcat2.lm(90,120]"")))\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""dgcat2.lm(30,60]=dgcat2.lm(60,90]"")))\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""dgcat2.lm(60,90]=dgcat2.lm(90,120]"")))\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""dgcat2.lm(30,60]=dgcat2.lm(90,120]"")))\n\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""(Intercept) + dgcat2.lm(90,120] = 3"") ))\n\n\n## Country-Year average by debtgdp (""correct weights"")\n## Table 3 Corrected\n(RR.correct.sd <- with(RR, tapply( dRGDP, dgcat, sd, na.rm=TRUE )))\n(RR.correct.mean <- with(RR, tapply( dRGDP, dgcat, mean, na.rm=TRUE )))\nRR.correct.mean.df <- data.frame(RR.correct.mean, dgcat=names(RR.correct.mean) )\n## Averaged Country averages by debtgdp (""equal weights"")\n(RR.equalwt.mean <- with(RR, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\n## NYT Appendix input to Table 1 Line 3\n(RR.equalwt.median <- with(RR, tapply( dRGDP, list(Country,dgcat), median, na.rm=TRUE )))\n## Table 3 Country equal weighting\nsummary(RR.equalwt.mean)\n\n## Country-Year average by debtgdp (""correct weights"") expanded categories\n(RR.correct.mean.2 <- with(RR, tapply( dRGDP, dgcat2, mean, na.rm=TRUE )))\nRR.correct.mean.2.df <- data.frame(RR.correct.mean.2, dgcat=names(RR.correct.mean.2) )\n## Averaged Country averages by debtgdp (""equal weights"")\n(RR.ex.equalwt.mean <- with(RR, tapply( dRGDP, list(Country,dgcat2), mean, na.rm=TRUE )))\nsummary(RR.ex.equalwt.mean)\n\n\n## Selective treatment of early years\nRR.selective <- subset(RR,\n                       !((Year<1950 & Country==""New Zealand"") | (Year<1951 & Country==""Australia"") | (Year<1951 & Country==""Canada"") ))\n(RR.selective.mean <- with(RR.selective, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\n(RR.selective.median <- with(RR.selective, tapply( dRGDP, list(Country,dgcat), median, na.rm=TRUE )))\n## Equal weights\n## Table 3 Weights,Exclusion\nsummary(RR.selective.mean)\n## Correct weights\n## Table 3 Selective years exclusion\nwith(RR.selective, tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n\n## Spreadsheet error\nRR.spreadsheet <- subset(RR, ! Country %in% c(""Australia"",""Austria"",""Belgium"",""Canada"",""Denmark"") )\n(RR.spreadsheet.mean <- with(RR.spreadsheet, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\n(RR.spreadsheet.median <- with(RR.spreadsheet, tapply( dRGDP, list(Country,dgcat), median, na.rm=TRUE )))\n## Table 3 Spreadsheet, Weights\nsummary(RR.spreadsheet.mean)\n## Table 3 Spreadsheet error\nwith(RR.spreadsheet, tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n\n## Selective treatment of early years and spreadsheet error\nRR.selective.spreadsheet <- subset(RR.selective, ! Country %in% c(""Australia"",""Austria"",""Belgium"",""Canada"",""Denmark"") )\nRR.selective.spreadsheet.mean <- with(RR.selective.spreadsheet, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE ))\n(RR.selective.spreadsheet.eqweight.median <- with(RR.selective.spreadsheet, tapply( dRGDP, list(Country,dgcat), median, na.rm=TRUE )))\n## Equal weights\n## Table 3 Weights,Exclusion,Spreadsheet Error\nsummary(RR.selective.spreadsheet.mean)\n## Correct weights\n## Table 3 Exclusion,Spreadsheet Error\nwith(RR.selective.spreadsheet, tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n\n## Actually available data years 1946-2009 with selective exclusion and spreadsheet error\navail.data <- ddply(RR.selective.spreadsheet, ~Country, summarize, min.year=min(Year), count.year=sum(!is.na(dRGDP) & !is.na(debtgdp)))\navail.data[order(avail.data[,""min.year""]),]\n\n## And New Zealand transcription error\n## selective.spreadsheet.transcription <- with(RR.selective.spreadsheet, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE ))\nRR.selective.spreadsheet.mean.transcription <- RR.selective.spreadsheet.mean\nRR.selective.spreadsheet.mean.transcription[""New Zealand"",4] <- -7.9\nsummary(RR.selective.spreadsheet.mean.transcription)\n## Table 3 Weights,Exclusion,Spreadsheet Error,Transcription\n(RR.published.mean <- apply(RR.selective.spreadsheet.mean.transcription,2,mean,na.rm=TRUE))\nRR.published.mean.df <- data.frame(RR.published.mean , dgcat=names(RR.published.mean) )\n\n\n## Medians\n## NYT Appendix Table 1 Line 4\n(RR.correct.median <- with(RR, tapply( dRGDP, dgcat, median, na.rm=TRUE )))\n(RR.correct.selective.median <- with(RR.selective, tapply( dRGDP, dgcat, median, na.rm=TRUE )))\n(RR.correct.spreadsheet.median <- with(RR.spreadsheet, tapply( dRGDP, dgcat, median, na.rm=TRUE )))\n## NYT Appendix Table 1 Line 3 (Use Median line)\n(RR.eqweight.median <- summary(RR.equalwt.median))\nsummary(RR.spreadsheet.median)\n## NYT Appendix Table 1 Line 2 (Dataset is ""RR.selective"" because it EXCLUDES early years but spreadsheet is corrected)\nsummary(RR.selective.median)\n(RR.correct.ex.median <- with(RR, tapply( dRGDP, dgcat2, median, na.rm=TRUE )))\n(RR.selective.spreadsheet.mean.median <- with(RR.selective.spreadsheet, tapply( dRGDP, dgcat, median, na.rm=TRUE )))\n(RR.published.median <- apply(RR.selective.spreadsheet.eqweight.median,2,median,na.rm=TRUE))\n\n\n## Counts of years\nwith(RR, table(Country,dgcat))\napply(with(RR,table( Country,dgcat)),2,sum)\n\nwith(RR.selective,table( Country,dgcat))\napply(with(RR.selective,table( Country,dgcat)),2,sum)\n\nwith(RR.selective.spreadsheet,table( Country,dgcat))\napply(with(RR.selective.spreadsheet,table( Country,dgcat)),2,sum)\n\n\nRR.newzealand.1951 <- subset(RR.selective.spreadsheet,Country==""New Zealand"" & Year==1951)\n\n\n## Categorical scatterplot\nn <- ggplot(RR, aes(x=dgcat,y=dRGDP)) + geom_point(shape=3,color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Category"")\nn <- n + geom_point(RR.published.mean.df, mapping=aes(x=dgcat,y=RR.published.mean), shape=5,  size=5 )\nn <- n + geom_text(RR.published.mean.df, mapping=aes(x=dgcat,y=RR.published.mean,label=round(RR.published.mean,1)),hjust=-0.7,size=3,color=\'darkgray\')\nn <- n + geom_point(RR.correct.mean.df,  mapping=aes(x=dgcat,y=RR.correct.mean,label=RR.correct.mean), shape=16, size=4 )  + theme_bw()\nn <- n + geom_text(RR.correct.mean.df,  mapping=aes(x=dgcat,y=RR.correct.mean,label=round(RR.correct.mean,1)), hjust=1.7,size=3,color=\'darkgray\')\nn <- n + geom_point(RR.newzealand.1951,mapping=aes(x=dgcat,y=dRGDP), shape=0, size=3 )\nn <- n + geom_text(RR.newzealand.1951,mapping=aes(x=dgcat,y=dRGDP,label=paste(round(dRGDP,1))), hjust=-0.7,size=3,color=\'darkgray\')\nn <- n + geom_text(RR.newzealand.1951,mapping=aes(x=dgcat,y=dRGDP,label=paste(""NZ"",Year)), hjust=1.2,size=3,color=\'darkgray\')\nprint(n)\n\n\n## Create legend for categorical scatterplot\nplot(3,10,pch=0,ylim=c(0,70),xlim=c(0,5.5))\ntext(3.2,10,""New Zealand 1951"",adj=0)\npoints(0,15,pch=16)\ntext(0.2,15,""Correct average real GDP growth"",adj=0)\npoints(0,10,pch=5,cex=1.5)\ntext(0.2,10,""RR average real GDP growth"",adj=0)\npoints(3,15,pch=3,col=\'darkgray\')\ntext(3.2,15,""Country-Year real GDP growth"",adj=0)\n\n## Categorical scatterplot for expanded categories\no <- ggplot(RR, aes(x=dgcat2,y=dRGDP)) + geom_point(shape=3,color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Category"")\no <- o + geom_point(RR.correct.mean.2.df,  mapping=aes(x=dgcat,y=RR.correct.mean.2), shape=16, size=4 )  + theme_bw()\no <- o + geom_text(RR.correct.mean.2.df, mapping=aes(x=dgcat,y=RR.correct.mean.2,label=round(RR.correct.mean.2,1)), hjust=1.7, size=3,color=\'darkgray\')\nprint(o)\n\n## Scatterplot (continuous)\nlibrary(mgcv)\nRR.gam <- gam(dRGDP ~ s(debtgdp, bs=""cs""),data=RR)\n\n## Cross-validation technique for loess parameters\n## http://stats.stackexchange.com/questions/2002/how-do-i-decide-what-span-to-use-in-loess-regression-in-r\nm <- ggplot(RR, aes(x=debtgdp,y=dRGDP))\nm1 <- m + geom_vline(xintercept=90,color=\'lightgray\',size=1.5)\nm1 <- m1 + geom_point(color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Ratio"") + scale_x_continuous(breaks=seq(0,240,30)) + theme_bw()\n## m1 <- m1 + geom_smooth(method=\'loess\',span=1.0,color=\'black\') + geom_smooth(method=\'loess\',span=0.2,color=\'black\')\nm1 <- m1 + geom_smooth(method=gam, color=\'black\',formula= y ~ s(x, bs = ""cs""))\n## m1 <- m1 + geom_smooth(method=\'auto\', color=\'black\')\nprint(m1)\n\n## Categorical scatterplot later years\nRR2000 <- subset(RR, Year>=2000)\n(RR.later.mean <- with(RR2000, tapply( dRGDP, dgcat, mean, na.rm=TRUE )))\nRR.later.mean.df <- data.frame(RR.later.mean, dgcat=names(RR.later.mean) )\n\nL <- ggplot(RR2000, aes(x=dgcat,y=dRGDP)) + geom_point(shape=3,color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Category"")\n## L <- L + geom_text(mapping=aes(label=paste(Country,Year) ), size=2, hjust=-0.2,color=\'darkgray\')\nL <- L + geom_point(RR.later.mean.df,  mapping=aes(x=dgcat,y=RR.later.mean,label=RR.later.mean), shape=16, size=4 )  + theme_bw()\nL <- L + geom_text(RR.later.mean.df,  mapping=aes(x=dgcat,y=RR.later.mean,label=round(RR.later.mean,1)), hjust=1.7,size=3,color=\'darkgray\')\n## L <- L + coord_cartesian(ylim=c(-12, 30))\nprint(L)\n\n\n## Scatterplot closeup\npdf(""closeup.pdf"",height=4,width=7)\nm2 <- m + geom_point(color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Ratio"") + scale_x_continuous(breaks=seq(0,240,30)) + theme_bw() +  geom_vline(xintercept=90,color=\'lightgray\',size=1.5)\n## m2 <- m2 + geom_smooth(method=\'loess\',span=0.75,color=\'black\') + geom_smooth(method=\'loess\',span=0.4,color=\'black\')\n## m2 <- m2 + geom_smooth(method=\'auto\',color=\'black\')\nm2 <- m2 + geom_smooth(method=gam, color=\'black\',formula= y ~ s(x, bs = ""cs""))\nm2 <- m2 + coord_cartesian(ylim=c(0, 7),xlim=c(0,150)) + scale_y_continuous(breaks=c(0,1,2,3,4,5,6,7)) + theme_bw()\nprint(m2)\n\n\n## Get the range for which the lower bound and upper bound include 3 percent\nsubset(ggplot_build(m1)$data[[3]],  (ymin<3.1 & ymin>2.9) | (ymax<3.1 & ymax>2.9 ))\n\nsubset(RR,\n       Country %in% c(""Australia"",""Belgium"",""Canada"",""Greece"",""Ireland"",""Italy"",""Japan"",""New Zealand"",""UK"",""US""),\n       select=c(Country,Year,dgcat,debtgdp,dRGDP))\n\nsubset(RR,\n       debtgdp>90,\n       select=c(Country,Year,dgcat,debtgdp,dRGDP))\n\n## Look at the public debt / GDP series\n## p <- ggplot(RR, aes(x=Year,y=debtgdp,color=Country)) + geom_point() +  facet_grid(. ~ Country) + opts(legend.position=""bottom"")\n## print(p)\n\n\n## Average growth by debtgdp for more recent samples\n## country-year weights\nwith(subset(RR, Year>=1950), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1950), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1960), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1960), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1970), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1970), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1980), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1980), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1990), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1990), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## Post-2000\n## NYT Appendix Table 4 Line 1\n(mean2000 <- with(RR2000, tapply( dRGDP, dgcat, mean, na.rm=TRUE )))\n(sd2000 <- with(RR2000, tapply( dRGDP, dgcat, sd, na.rm=TRUE )))\n(length2000 <- with(RR2000, tapply( dRGDP, dgcat, length )))\n\n## NYT Appendix Table 4 Line 1 Standard errors\nsd2000 / sqrt(length2000)\n\n## country-year weights\n(RR2000.equalwt.mean <- with(RR2000, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\n\nwith(RR2000, table(Country,dgcat))\n\n## Regression analysis of categories\nsummary(dgcat.lm <- lm(dRGDP ~ dgcat.lm, data=RR2000))\n\n## country weights\napply(RR2000.equalwt.mean,2,mean,na.rm=TRUE)\napply(RR2000.equalwt.mean,2,sd,na.rm=TRUE)\napply(with(RR2000,table( Country,dgcat)),2,sum)\n\n\n\n## Median analysis of two periods ## Mean has problems because of very high value for Belgium in 1947\n## country-year weights\nwith(subset(RR, Year<1980), tapply( dRGDP, dgcat, median, na.rm=TRUE ))\n## country weights  ## Beware ridiculous value for Belgian GDP growth in 1947\napply(with(subset(RR, Year<1980), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,median,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1980), tapply( dRGDP, dgcat, median, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1980), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,median,na.rm=TRUE)\n\n## country-year weights\nwith(RR, tapply( dRGDP, dgcat, median, na.rm=TRUE ))\n## country weights\napply(with(RR, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,median,na.rm=TRUE)\n\n## Median analysis of two periods ## Mean has problems because of very high value for Belgium in 1947\n## 1955-1980 has only Britain in the highest public debt category!!\n## country-year weights\nwith(subset(RR, Year>=1955 & Year<1980), tapply( dRGDP, dgcat, median, na.rm=TRUE ))\n## country weights  ## Beware ridiculous value for Belgian GDP growth in 1947\napply(with(subset(RR, Year>=1955 & Year<1980), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,median,na.rm=TRUE)\n\n\n\n#write.xlsx2(subset(RR,Year>=1990 & debtgdp>90,select=c(Country,Year,debtgdp,dgcat,dRGDP)),""RR-post1990.xlsx"",row.names=FALSE)\n\n\nsubset(RR,dRGDP>10,select=c(Country,Year,dRGDP,dgcat,debtgdp))\nsubset(RR,dRGDP< -7,select=c(Country,Year,dRGDP,dgcat,debtgdp))\n\nrm(list = ls())', 'print(""THIS IS A MODIFIED VERSION OF THE R SCRIPT!"")\n', '## This code and the related data are open-source under the BSD 2-clause license.\n## See http://www.tldrlegal.com/l/BSD2 for details\n## Michael Ash, Thomas Herndon, and Robert Pollin April-May 2013\n## Replicate and extend Reinhart and Rogoff (2010) using data from RR working spreadsheet\n\nlibrary(plyr)\nlibrary(ggplot2)\nlibrary(car)\nlibrary(foreign)\nlibrary(xlsx)\noptions(scipen=10000)\noptions(digits=4)\n\nrm(list = ls())\n\nlibrary(httr)\nlibrary(rlist)\nlibrary(jsonlite)\nlibrary(dplyr)\n\nretrieveSubset <- function(pid, handle_url) {\n\turl <- sprintf(""%s/api/handles/%s"", handle_url, pid)\n\tresp <- GET(url)\n\tjsonRespo <- content(resp, as=""parsed"")\n\n\tfor (x in jsonRespo$values){\n\t\tif(x$type == ""API_URL"") {\n\t\t\tresource_url <- x$data$value\n\t\t}\n\t}\n\n\treturn(fromJSON(content(GET(resource_url), as=""text""))$result$records)\n}\n\npid_file <- ""pid""\npid <- readChar(pid_file, file.info(pid_file)$size)\n\npid\n\nRR <- retrieveSubset(pid, ""http://localhost:8000"")\n\nRR\n\n## Potential data years 1946-2009\navail.data <- ddply(RR, ~Country, summarize,\n                    count.year.GDP=sum(!is.na(dRGDP)),count.year.debt=sum(!is.na(debtgdp)), count.year.both=sum(!is.na(dRGDP) & !is.na(debtgdp)))\navail.data[order(avail.data[,""count.year.both""]),]\n\n## Slow\nRR$dgcat.lm <- cut(RR$debtgdp, breaks=c(0,30,60,90,Inf))\nRR$dgcat <- factor(RR$dgcat.lm, labels = c(""0-30%"",""30-60%"",""60-90%"",""Above 90%""),ordered=TRUE)\n\n\n## Limit to actually available data\nRR <- subset(RR,  !is.na(dRGDP) & !is.na(debtgdp))\n\n## Actually available data years 1946-2009\navail.data <- ddply(RR, ~Country, summarize, min.year=min(Year), count.year=sum(!is.na(dRGDP) & !is.na(debtgdp)))\navail.data[order(avail.data[,""min.year""]),]\n\nwith(RR,table(Year))\nwith(RR,table(Country))\n\n## Create RR public debt/GDP categories\nRR$dgcat.lm <- cut(RR$debtgdp, breaks=c(0,30,60,90,Inf))\nRR$dgcat <- factor(RR$dgcat.lm, labels = c(""0-30%"",""30-60%"",""60-90%"",""Above 90%""),ordered=TRUE)\n\n## Create expanded public debt/GDP categories\nRR$dgcat2.lm <- cut(RR$debtgdp, breaks=c(0,30,60,90,120,Inf))\nRR$dgcat2 <- factor(RR$dgcat2.lm, labels = c(""0-30%"",""30-60%"",""60-90%"",""90-120%"",""Above 120%""),ordered=TRUE)\n\n## Regression analysis of categories\nsummary(dgcat.lm <- lm(dRGDP ~ dgcat.lm, data=RR))\nsummary(dgcat2.lm <- lm(dRGDP ~ dgcat2.lm, data=RR))\nlinearHypothesis(dgcat2.lm,\n                 verbose=TRUE,\n                 paste( c(""dgcat2.lm(30,60]=dgcat2.lm(60,90]"", ""dgcat2.lm(30,60]=dgcat2.lm(90,120]"", ""dgcat2.lm(30,60]=dgcat2.lm(120,Inf]"") ))\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""dgcat2.lm(30,60]=dgcat2.lm(60,90]"", ""dgcat2.lm(30,60]=dgcat2.lm(90,120]"")))\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""dgcat2.lm(30,60]=dgcat2.lm(60,90]"")))\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""dgcat2.lm(60,90]=dgcat2.lm(90,120]"")))\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""dgcat2.lm(30,60]=dgcat2.lm(90,120]"")))\n\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""(Intercept) + dgcat2.lm(90,120] = 3"") ))\n\n\n## Country-Year average by debtgdp (""correct weights"")\n## Table 3 Corrected\n(RR.correct.sd <- with(RR, tapply( dRGDP, dgcat, sd, na.rm=TRUE )))\n(RR.correct.mean <- with(RR, tapply( dRGDP, dgcat, mean, na.rm=TRUE )))\nRR.correct.mean.df <- data.frame(RR.correct.mean, dgcat=names(RR.correct.mean) )\n## Averaged Country averages by debtgdp (""equal weights"")\n(RR.equalwt.mean <- with(RR, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\n## NYT Appendix input to Table 1 Line 3\n(RR.equalwt.median <- with(RR, tapply( dRGDP, list(Country,dgcat), median, na.rm=TRUE )))\n## Table 3 Country equal weighting\nsummary(RR.equalwt.mean)\n\n## Country-Year average by debtgdp (""correct weights"") expanded categories\n(RR.correct.mean.2 <- with(RR, tapply( dRGDP, dgcat2, mean, na.rm=TRUE )))\nRR.correct.mean.2.df <- data.frame(RR.correct.mean.2, dgcat=names(RR.correct.mean.2) )\n## Averaged Country averages by debtgdp (""equal weights"")\n(RR.ex.equalwt.mean <- with(RR, tapply( dRGDP, list(Country,dgcat2), mean, na.rm=TRUE )))\nsummary(RR.ex.equalwt.mean)\n\n\n## Selective treatment of early years\nRR.selective <- subset(RR,\n                       !((Year<1950 & Country==""New Zealand"") | (Year<1951 & Country==""Australia"") | (Year<1951 & Country==""Canada"") ))\n(RR.selective.mean <- with(RR.selective, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\n(RR.selective.median <- with(RR.selective, tapply( dRGDP, list(Country,dgcat), median, na.rm=TRUE )))\n## Equal weights\n## Table 3 Weights,Exclusion\nsummary(RR.selective.mean)\n## Correct weights\n## Table 3 Selective years exclusion\nwith(RR.selective, tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n\n## Spreadsheet error\nRR.spreadsheet <- subset(RR, ! Country %in% c(""Australia"",""Austria"",""Belgium"",""Canada"",""Denmark"") )\n(RR.spreadsheet.mean <- with(RR.spreadsheet, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\n(RR.spreadsheet.median <- with(RR.spreadsheet, tapply( dRGDP, list(Country,dgcat), median, na.rm=TRUE )))\n## Table 3 Spreadsheet, Weights\nsummary(RR.spreadsheet.mean)\n## Table 3 Spreadsheet error\nwith(RR.spreadsheet, tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n\n## Selective treatment of early years and spreadsheet error\nRR.selective.spreadsheet <- subset(RR.selective, ! Country %in% c(""Australia"",""Austria"",""Belgium"",""Canada"",""Denmark"") )\nRR.selective.spreadsheet.mean <- with(RR.selective.spreadsheet, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE ))\n(RR.selective.spreadsheet.eqweight.median <- with(RR.selective.spreadsheet, tapply( dRGDP, list(Country,dgcat), median, na.rm=TRUE )))\n## Equal weights\n## Table 3 Weights,Exclusion,Spreadsheet Error\nsummary(RR.selective.spreadsheet.mean)\n## Correct weights\n## Table 3 Exclusion,Spreadsheet Error\nwith(RR.selective.spreadsheet, tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n\n## Actually available data years 1946-2009 with selective exclusion and spreadsheet error\navail.data <- ddply(RR.selective.spreadsheet, ~Country, summarize, min.year=min(Year), count.year=sum(!is.na(dRGDP) & !is.na(debtgdp)))\navail.data[order(avail.data[,""min.year""]),]\n\n## And New Zealand transcription error\n## selective.spreadsheet.transcription <- with(RR.selective.spreadsheet, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE ))\nRR.selective.spreadsheet.mean.transcription <- RR.selective.spreadsheet.mean\nRR.selective.spreadsheet.mean.transcription[""New Zealand"",4] <- -7.9\nsummary(RR.selective.spreadsheet.mean.transcription)\n## Table 3 Weights,Exclusion,Spreadsheet Error,Transcription\n(RR.published.mean <- apply(RR.selective.spreadsheet.mean.transcription,2,mean,na.rm=TRUE))\nRR.published.mean.df <- data.frame(RR.published.mean , dgcat=names(RR.published.mean) )\n\n\n## Medians\n## NYT Appendix Table 1 Line 4\n(RR.correct.median <- with(RR, tapply( dRGDP, dgcat, median, na.rm=TRUE )))\n(RR.correct.selective.median <- with(RR.selective, tapply( dRGDP, dgcat, median, na.rm=TRUE )))\n(RR.correct.spreadsheet.median <- with(RR.spreadsheet, tapply( dRGDP, dgcat, median, na.rm=TRUE )))\n## NYT Appendix Table 1 Line 3 (Use Median line)\n(RR.eqweight.median <- summary(RR.equalwt.median))\nsummary(RR.spreadsheet.median)\n## NYT Appendix Table 1 Line 2 (Dataset is ""RR.selective"" because it EXCLUDES early years but spreadsheet is corrected)\nsummary(RR.selective.median)\n(RR.correct.ex.median <- with(RR, tapply( dRGDP, dgcat2, median, na.rm=TRUE )))\n(RR.selective.spreadsheet.mean.median <- with(RR.selective.spreadsheet, tapply( dRGDP, dgcat, median, na.rm=TRUE )))\n(RR.published.median <- apply(RR.selective.spreadsheet.eqweight.median,2,median,na.rm=TRUE))\n\n\n## Counts of years\nwith(RR, table(Country,dgcat))\napply(with(RR,table( Country,dgcat)),2,sum)\n\nwith(RR.selective,table( Country,dgcat))\napply(with(RR.selective,table( Country,dgcat)),2,sum)\n\nwith(RR.selective.spreadsheet,table( Country,dgcat))\napply(with(RR.selective.spreadsheet,table( Country,dgcat)),2,sum)\n\n\nRR.newzealand.1951 <- subset(RR.selective.spreadsheet,Country==""New Zealand"" & Year==1951)\n\n\n## Categorical scatterplot\nn <- ggplot(RR, aes(x=dgcat,y=dRGDP)) + geom_point(shape=3,color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Category"")\nn <- n + geom_point(RR.published.mean.df, mapping=aes(x=dgcat,y=RR.published.mean), shape=5,  size=5 )\nn <- n + geom_text(RR.published.mean.df, mapping=aes(x=dgcat,y=RR.published.mean,label=round(RR.published.mean,1)),hjust=-0.7,size=3,color=\'darkgray\')\nn <- n + geom_point(RR.correct.mean.df,  mapping=aes(x=dgcat,y=RR.correct.mean,label=RR.correct.mean), shape=16, size=4 )  + theme_bw()\nn <- n + geom_text(RR.correct.mean.df,  mapping=aes(x=dgcat,y=RR.correct.mean,label=round(RR.correct.mean,1)), hjust=1.7,size=3,color=\'darkgray\')\nn <- n + geom_point(RR.newzealand.1951,mapping=aes(x=dgcat,y=dRGDP), shape=0, size=3 )\nn <- n + geom_text(RR.newzealand.1951,mapping=aes(x=dgcat,y=dRGDP,label=paste(round(dRGDP,1))), hjust=-0.7,size=3,color=\'darkgray\')\nn <- n + geom_text(RR.newzealand.1951,mapping=aes(x=dgcat,y=dRGDP,label=paste(""NZ"",Year)), hjust=1.2,size=3,color=\'darkgray\')\nprint(n)\n\n\n## Create legend for categorical scatterplot\nplot(3,10,pch=0,ylim=c(0,70),xlim=c(0,5.5))\ntext(3.2,10,""New Zealand 1951"",adj=0)\npoints(0,15,pch=16)\ntext(0.2,15,""Correct average real GDP growth"",adj=0)\npoints(0,10,pch=5,cex=1.5)\ntext(0.2,10,""RR average real GDP growth"",adj=0)\npoints(3,15,pch=3,col=\'darkgray\')\ntext(3.2,15,""Country-Year real GDP growth"",adj=0)\n\n## Categorical scatterplot for expanded categories\no <- ggplot(RR, aes(x=dgcat2,y=dRGDP)) + geom_point(shape=3,color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Category"")\no <- o + geom_point(RR.correct.mean.2.df,  mapping=aes(x=dgcat,y=RR.correct.mean.2), shape=16, size=4 )  + theme_bw()\no <- o + geom_text(RR.correct.mean.2.df, mapping=aes(x=dgcat,y=RR.correct.mean.2,label=round(RR.correct.mean.2,1)), hjust=1.7, size=3,color=\'darkgray\')\nprint(o)\n\n## Scatterplot (continuous)\nlibrary(mgcv)\nRR.gam <- gam(dRGDP ~ s(debtgdp, bs=""cs""),data=RR)\n\n## Cross-validation technique for loess parameters\n## http://stats.stackexchange.com/questions/2002/how-do-i-decide-what-span-to-use-in-loess-regression-in-r\nm <- ggplot(RR, aes(x=debtgdp,y=dRGDP))\nm1 <- m + geom_vline(xintercept=90,color=\'lightgray\',size=1.5)\nm1 <- m1 + geom_point(color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Ratio"") + scale_x_continuous(breaks=seq(0,240,30)) + theme_bw()\n## m1 <- m1 + geom_smooth(method=\'loess\',span=1.0,color=\'black\') + geom_smooth(method=\'loess\',span=0.2,color=\'black\')\nm1 <- m1 + geom_smooth(method=gam, color=\'black\',formula= y ~ s(x, bs = ""cs""))\n## m1 <- m1 + geom_smooth(method=\'auto\', color=\'black\')\nprint(m1)\n\n## Categorical scatterplot later years\nRR2000 <- subset(RR, Year>=2000)\n(RR.later.mean <- with(RR2000, tapply( dRGDP, dgcat, mean, na.rm=TRUE )))\nRR.later.mean.df <- data.frame(RR.later.mean, dgcat=names(RR.later.mean) )\n\nL <- ggplot(RR2000, aes(x=dgcat,y=dRGDP)) + geom_point(shape=3,color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Category"")\n## L <- L + geom_text(mapping=aes(label=paste(Country,Year) ), size=2, hjust=-0.2,color=\'darkgray\')\nL <- L + geom_point(RR.later.mean.df,  mapping=aes(x=dgcat,y=RR.later.mean,label=RR.later.mean), shape=16, size=4 )  + theme_bw()\nL <- L + geom_text(RR.later.mean.df,  mapping=aes(x=dgcat,y=RR.later.mean,label=round(RR.later.mean,1)), hjust=1.7,size=3,color=\'darkgray\')\n## L <- L + coord_cartesian(ylim=c(-12, 30))\nprint(L)\n\n\n## Scatterplot closeup\npdf(""closeup.pdf"",height=4,width=7)\nm2 <- m + geom_point(color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Ratio"") + scale_x_continuous(breaks=seq(0,240,30)) + theme_bw() +  geom_vline(xintercept=90,color=\'lightgray\',size=1.5)\n## m2 <- m2 + geom_smooth(method=\'loess\',span=0.75,color=\'black\') + geom_smooth(method=\'loess\',span=0.4,color=\'black\')\n## m2 <- m2 + geom_smooth(method=\'auto\',color=\'black\')\nm2 <- m2 + geom_smooth(method=gam, color=\'black\',formula= y ~ s(x, bs = ""cs""))\nm2 <- m2 + coord_cartesian(ylim=c(0, 7),xlim=c(0,150)) + scale_y_continuous(breaks=c(0,1,2,3,4,5,6,7)) + theme_bw()\nprint(m2)\n\n\n## Get the range for which the lower bound and upper bound include 3 percent\nsubset(ggplot_build(m1)$data[[3]],  (ymin<3.1 & ymin>2.9) | (ymax<3.1 & ymax>2.9 ))\n\nsubset(RR,\n       Country %in% c(""Australia"",""Belgium"",""Canada"",""Greece"",""Ireland"",""Italy"",""Japan"",""New Zealand"",""UK"",""US""),\n       select=c(Country,Year,dgcat,debtgdp,dRGDP))\n\nsubset(RR,\n       debtgdp>90,\n       select=c(Country,Year,dgcat,debtgdp,dRGDP))\n\n## Look at the public debt / GDP series\n## p <- ggplot(RR, aes(x=Year,y=debtgdp,color=Country)) + geom_point() +  facet_grid(. ~ Country) + opts(legend.position=""bottom"")\n## print(p)\n\n\n## Average growth by debtgdp for more recent samples\n## country-year weights\nwith(subset(RR, Year>=1950), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1950), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1960), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1960), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1970), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1970), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1980), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1980), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1990), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1990), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## Post-2000\n## NYT Appendix Table 4 Line 1\n(mean2000 <- with(RR2000, tapply( dRGDP, dgcat, mean, na.rm=TRUE )))\n(sd2000 <- with(RR2000, tapply( dRGDP, dgcat, sd, na.rm=TRUE )))\n(length2000 <- with(RR2000, tapply( dRGDP, dgcat, length )))\n\n## NYT Appendix Table 4 Line 1 Standard errors\nsd2000 / sqrt(length2000)\n\n## country-year weights\n(RR2000.equalwt.mean <- with(RR2000, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\n\nwith(RR2000, table(Country,dgcat))\n\n## Regression analysis of categories\nsummary(dgcat.lm <- lm(dRGDP ~ dgcat.lm, data=RR2000))\n\n## country weights\napply(RR2000.equalwt.mean,2,mean,na.rm=TRUE)\napply(RR2000.equalwt.mean,2,sd,na.rm=TRUE)\napply(with(RR2000,table( Country,dgcat)),2,sum)\n\n\n\n## Median analysis of two periods ## Mean has problems because of very high value for Belgium in 1947\n## country-year weights\nwith(subset(RR, Year<1980), tapply( dRGDP, dgcat, median, na.rm=TRUE ))\n## country weights  ## Beware ridiculous value for Belgian GDP growth in 1947\napply(with(subset(RR, Year<1980), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,median,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1980), tapply( dRGDP, dgcat, median, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1980), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,median,na.rm=TRUE)\n\n## country-year weights\nwith(RR, tapply( dRGDP, dgcat, median, na.rm=TRUE ))\n## country weights\napply(with(RR, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,median,na.rm=TRUE)\n\n## Median analysis of two periods ## Mean has problems because of very high value for Belgium in 1947\n## 1955-1980 has only Britain in the highest public debt category!!\n## country-year weights\nwith(subset(RR, Year>=1955 & Year<1980), tapply( dRGDP, dgcat, median, na.rm=TRUE ))\n## country weights  ## Beware ridiculous value for Belgian GDP growth in 1947\napply(with(subset(RR, Year>=1955 & Year<1980), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,median,na.rm=TRUE)\n\n\n\nsubset(RR,dRGDP>10,select=c(Country,Year,dRGDP,dgcat,debtgdp))\nsubset(RR,dRGDP< -7,select=c(Country,Year,dRGDP,dgcat,debtgdp))\n\nrm(list = ls())\n\n']","Test Assets for ckanext-mongodatastore (Reinhart&Rogoff Experiment) This is a collection of digital assets, that are required for the evaluation of the CKAN plugin 'ckanext-mongodatastore'.The assets are based on the data provided by the publication 'Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff'",1
"Data and code for the publication ""Day-to-day temperature variability reduces economic growth""","This repository contains code and data for the reproduction of analysis and figures from:Kotz, Wenz, Stechemesser, Kalkuhl and Levermann (2020). ~ Data included: GADM (https://gadm.org/data.html) dataset of regional shapefiles: gadm36_levels.gpkg Calculated regional climate variables: T.5_???_measure.npy and P.5_???_measure.npy, contained in ZIPPED folders T5_measures.zip & P5_measures.zip (??? denotes three letter country code).World bank GDP and population data: WB_GDP.csvEconomic and regional climate data: T_econ.dta. Economic data is provided by Matthias Kalkuhl and is documented at https://doi.org/10.1016/j.jeem.2020.102360.Economic and regoinal climate data with lagged variables: T_econ_5_lags.dtaEconomic and regional climate data restricted to regions with certain seasonal temp. differences: T_econ_seas_g*.dtaDaily climate data used to plot Fig_1: corrientes_ARG_T_days.npy & ocampo_MEX_T_days.npy ~ Data not included:Raw ERA-5 daily temperature and precipitation data, 1979-2018, interpolated to 0.5x0.5 grid by ISIMIP. Available from ISIMIP (https://www.isimip.org/) or from authors upon request. ~ Code included: T_scatter.py - code to calculate and aggregate the main temperature variables (annual average and day-to-day variability) from ERA-5 grid to regional level.P_scatter.py - code to calculate and aggregate precipitation variables (total annual) from ERA-5 grid to regional level.Regression_Table_1 - Stata do file including code to run regressions for Table 1 of the manuscript.Table_SX - Stata do files including code to run regressions for Tables S1-11 of the SI.partitions.py - code to partition data based on national and regional income, to export sub-data sets for analysis in R, and for plotting Fig S4.partition_regressions.R - code to calculate marginal effects used for plotting in Figs 4, S1, and S5.partitions_plot.py - code to plot Figs 4, S1 and S5.plot_Fig_1.pyplot_Fig_2.pyplot_Fig_3.py ~ Order: 1. T_scatter.py and P_scatter.py to calculate regional climate variables using raw ERA-5 data and GADM regional shapefile data. (Note: set correct directory for raw ERA-5 data on l101 of T_scatter.py and l102 of P_scatter.py). 2. Regression_Table_1 and Table_SX to run main and supplementary regressions in Stata using the economic and climate data set T_econ.dta (and T_econ_5_lags.dta for Table_S_10 and T_econ_seas_g*.dta for Table_S_2). 3. plot_Fig_1.py using corrientes_ARG_T_days.npy & ocampo_MEX_T_days.npy. 4. plot_Fig_2.py using T_econ.dta and gadm36_levels.gpkg. 5. plot_Fig_3.py using T_econ.dta, T.5_???_measure.npy and WB_GDP.csv. 6. partitions.py to partition data based on regional and national income and to plot Fig. S4 using T_econ.dta and gadm36_levels.gpkg. 7. partition_regressions.R to estimate marginal effects of partitioned data, using the output of partitions.py. 8. partitions_plot.py, plot Figs. 4, S1 and S5 using the output of partition_regressions.R and T_econ.dta.","['library(interplot)\nlibrary(plm)\nlibrary(plotMElm)\nlibrary(clubSandwich)\nlibrary(lmtest)\nlibrary(ggeffects)\nlibrary(emmeans)\nlibrary(haven)\n\nsetwd("""")\n\n#load data table\ntable<-read_stata(\'for_R/T.5_poor_regions.dta\')\n\n#estimate regression model\nmod<-lm(dlgdp_pc_usd ~ P5_totalpr + T5_delta + T5_mean_m:T5_delta + T5_delta_1 + T5_mean_m:T5_delta_1 + T5_varm + T5_varm:T5_seas_diff_m + factor(ID) + factor(year),data=table)\n\n#get coefficients and covariance matrix\nbeta.hat <- coef(mod) \ncov <- vcovCR(mod,cluster=table$ID,type=\'CR0\')\n\n#calculate standard errors and confidence intervals at different levels of seasonal temperaure difference\nz0 <- seq(0,50, length.out = 1000)\ndy.dx <- beta.hat[""T5_varm""] + beta.hat[""T5_varm:T5_seas_diff_m""]*z0\nse.dy.dx <- sqrt(cov[""T5_varm"", ""T5_varm""] + z0^2*cov[""T5_varm:T5_seas_diff_m"", ""T5_varm:T5_seas_diff_m""] + 2*z0*cov[""T5_varm"", ""T5_varm:T5_seas_diff_m""])\nupr <- dy.dx + 1.96*se.dy.dx\nlwr <- dy.dx - 1.96*se.dy.dx\n\nwrite.csv(dy.dx,\'from_R/ID_part_poor_dydx.csv\')\nwrite.csv(upr,\'from_R/ID_part_poor_ID_clust_upr.csv\')\nwrite.csv(lwr,\'from_R/ID_part_poor_ID_clust_lwr.csv\')\n\ntable<-read_stata(\'for_R/T.5_rich_regions.dta\')\nmod<-lm(dlgdp_pc_usd ~ P5_totalpr + T5_delta + T5_mean_m:T5_delta + T5_delta_1 + T5_mean_m:T5_delta_1 + T5_varm + T5_varm:T5_seas_diff_m + factor(ID) + factor(year),data=table)\n\nbeta.hat <- coef(mod) \ncov <- vcovCR(mod,cluster=table$ID,type=\'CR0\')\nz0 <- seq(0,50, length.out = 1000)\ndy.dx <- beta.hat[""T5_varm""] + beta.hat[""T5_varm:T5_seas_diff_m""]*z0\nse.dy.dx <- sqrt(cov[""T5_varm"", ""T5_varm""] + z0^2*cov[""T5_varm:T5_seas_diff_m"", ""T5_varm:T5_seas_diff_m""] + 2*z0*cov[""T5_varm"", ""T5_varm:T5_seas_diff_m""])\nupr <- dy.dx + 1.96*se.dy.dx\nlwr <- dy.dx - 1.96*se.dy.dx\n\nwrite.csv(dy.dx,\'from_R/ID_part_rich_dydx.csv\')\nwrite.csv(upr,\'from_R/ID_part_rich_ID_clust_upr.csv\')\nwrite.csv(lwr,\'from_R/ID_part_rich_ID_clust_lwr.csv\')\n\ntable<-read_stata(\'for_R/T.5_poor_countries.dta\')\n\nmod<-lm(dlgdp_pc_usd ~ P5_totalpr + T5_delta + T5_mean_m:T5_delta + T5_delta_1 + T5_mean_m:T5_delta_1 + T5_varm + T5_varm:T5_seas_diff_m + factor(ID) + factor(year),data=table)\n\nbeta.hat <- coef(mod) \ncov <- vcovCR(mod,cluster=table$ID,type=\'CR0\')\nz0 <- seq(0,50, length.out = 1000)\ndy.dx <- beta.hat[""T5_varm""] + beta.hat[""T5_varm:T5_seas_diff_m""]*z0\nse.dy.dx <- sqrt(cov[""T5_varm"", ""T5_varm""] + z0^2*cov[""T5_varm:T5_seas_diff_m"", ""T5_varm:T5_seas_diff_m""] + 2*z0*cov[""T5_varm"", ""T5_varm:T5_seas_diff_m""])\nupr <- dy.dx + 1.96*se.dy.dx\nlwr <- dy.dx - 1.96*se.dy.dx\n\nwrite.csv(dy.dx,\'from_R/iso_part_poor_dydx.csv\')\nwrite.csv(upr,\'from_R/iso_part_poor_ID_clust_upr.csv\')\nwrite.csv(lwr,\'from_R/iso_part_poor_ID_clust_lwr.csv\')\n\ntable<-read_stata(\'for_R/T.5_rich_countries.dta\')\n\nmod<-lm(dlgdp_pc_usd ~ P5_totalpr + T5_delta + T5_mean_m:T5_delta + T5_delta_1 + T5_mean_m:T5_delta_1 + T5_varm + T5_seas_diff_m:T5_varm + factor(ID) + factor(year),data=table)\n\nbeta.hat <- coef(mod) \ncov <- vcovCR(mod,cluster=table$ID,type=\'CR0\')\nz0 <- seq(0,50, length.out = 1000)\ndy.dx <- beta.hat[""T5_varm""] + beta.hat[""T5_varm:T5_seas_diff_m""]*z0\nse.dy.dx <- sqrt(cov[""T5_varm"", ""T5_varm""] + z0^2*cov[""T5_varm:T5_seas_diff_m"", ""T5_varm:T5_seas_diff_m""] + 2*z0*cov[""T5_varm"", ""T5_varm:T5_seas_diff_m""])\nupr <- dy.dx + 1.96*se.dy.dx\nlwr <- dy.dx - 1.96*se.dy.dx\n\nwrite.csv(dy.dx,\'from_R/iso_part_rich_dydx.csv\')\nwrite.csv(upr,\'from_R/iso_part_rich_ID_clust_upr.csv\')\nwrite.csv(lwr,\'from_R/iso_part_rich_ID_clust_lwr.csv\')\n\ntable<-read_stata(\'T_econ.dta\')\n\nmod<-lm(dlgdp_pc_usd ~ P5_totalpr + T5_delta + T5_mean_m:T5_delta + T5_delta_1 + T5_mean_m:T5_delta_1 + T5_varm + T5_seas_diff_m:T5_varm + factor(ID) + factor(year),data=table)\n\nbeta.hat <- coef(mod) \ncov <- vcovCR(mod,cluster=table$ID,type=\'CR0\')\nz0 <- seq(0,50, length.out = 1000)\ndy.dx <- beta.hat[""T5_varm""] + beta.hat[""T5_varm:T5_seas_diff_m""]*z0\nse.dy.dx <- sqrt(cov[""T5_varm"", ""T5_varm""] + z0^2*cov[""T5_varm:T5_seas_diff_m"", ""T5_varm:T5_seas_diff_m""] + 2*z0*cov[""T5_varm"", ""T5_varm:T5_seas_diff_m""])\nupr <- dy.dx + 1.96*se.dy.dx\nlwr <- dy.dx - 1.96*se.dy.dx\n\nwrite.csv(dy.dx,\'from_R/main_dydx.csv\')\nwrite.csv(upr,\'from_R/main_ID_clust_upr.csv\')\nwrite.csv(lwr,\'from_R/main_ID_clust_lwr.csv\')']","Data and code for the publication ""Day-to-day temperature variability reduces economic growth"" This repository contains code and data for the reproduction of analysis and figures from:Kotz, Wenz, Stechemesser, Kalkuhl and Levermann (2020). ~ Data included: GADM (https://gadm.org/data.html) dataset of regional shapefiles: gadm36_levels.gpkg Calculated regional climate variables: T.5_???_measure.npy and P.5_???_measure.npy, contained in ZIPPED folders T5_measures.zip & P5_measures.zip (??? denotes three letter country code).World bank GDP and population data: WB_GDP.csvEconomic and regional climate data: T_econ.dta. Economic data is provided by Matthias Kalkuhl and is documented at https://doi.org/10.1016/j.jeem.2020.102360.Economic and regoinal climate data with lagged variables: T_econ_5_lags.dtaEconomic and regional climate data restricted to regions with certain seasonal temp. differences: T_econ_seas_g*.dtaDaily climate data used to plot Fig_1: corrientes_ARG_T_days.npy & ocampo_MEX_T_days.npy ~ Data not included:Raw ERA-5 daily temperature and precipitation data, 1979-2018, interpolated to 0.5x0.5 grid by ISIMIP. Available from ISIMIP (https://www.isimip.org/) or from authors upon request. ~ Code included: T_scatter.py - code to calculate and aggregate the main temperature variables (annual average and day-to-day variability) from ERA-5 grid to regional level.P_scatter.py - code to calculate and aggregate precipitation variables (total annual) from ERA-5 grid to regional level.Regression_Table_1 - Stata do file including code to run regressions for Table 1 of the manuscript.Table_SX - Stata do files including code to run regressions for Tables S1-11 of the SI.partitions.py - code to partition data based on national and regional income, to export sub-data sets for analysis in R, and for plotting Fig S4.partition_regressions.R - code to calculate marginal effects used for plotting in Figs 4, S1, and S5.partitions_plot.py - code to plot Figs 4, S1 and S5.plot_Fig_1.pyplot_Fig_2.pyplot_Fig_3.py ~ Order: 1. T_scatter.py and P_scatter.py to calculate regional climate variables using raw ERA-5 data and GADM regional shapefile data. (Note: set correct directory for raw ERA-5 data on l101 of T_scatter.py and l102 of P_scatter.py). 2. Regression_Table_1 and Table_SX to run main and supplementary regressions in Stata using the economic and climate data set T_econ.dta (and T_econ_5_lags.dta for Table_S_10 and T_econ_seas_g*.dta for Table_S_2). 3. plot_Fig_1.py using corrientes_ARG_T_days.npy & ocampo_MEX_T_days.npy. 4. plot_Fig_2.py using T_econ.dta and gadm36_levels.gpkg. 5. plot_Fig_3.py using T_econ.dta, T.5_???_measure.npy and WB_GDP.csv. 6. partitions.py to partition data based on regional and national income and to plot Fig. S4 using T_econ.dta and gadm36_levels.gpkg. 7. partition_regressions.R to estimate marginal effects of partitioned data, using the output of partitions.py. 8. partitions_plot.py, plot Figs. 4, S1 and S5 using the output of partition_regressions.R and T_econ.dta.",1
Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff,"The provided assets are the source code of the conducted experiments in the publication ""Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff"" written by Thomas Herndon, Michael Ash and Robert Pollin. The assets were retrieved from following URL:https://www.peri.umass.edu/images/WP322HAP-RR-GITD-code-2013-05-17.zipModifications to the original content:The provided zip file was unzipped and the internal folder structure was flattened (i.e. the files RR-200.R and RR-200.Rout were moved to the root level of the archive)Information Text from the website of the University of Massachusetts Amherst:Herndon, Ash and Pollin replicate Reinhart and Rogoffand find that coding errors, selective exclusion of available data, and unconventional weighting of summary statistics lead to serious errors that inaccurately represent the relationship between public debt and GDP growth among 20 advanced economies in the post-war period. They find that when properly calculated, the average real GDP growth rate for countries carrying a public-debt-to-GDP ratio of over 90 percent is actually 2.2 percent, not -0:1 percent as published in Reinhart and Rogoff. That is, contrary to RR, average GDP growth at public debt/GDP ratios over 90 percent is not dramatically different than when debt/GDP ratios are lower.The authors also show how the relationship between public debt and GDP growth varies significantly by time period and country. Overall, the evidence we review contradicts Reinhart and Rogoff's claim to have identified an important stylized fact, that public debt loads greater than 90 percent of GDP consistently reduce GDP growth.","['## This code and the related data are open-source under the BSD 2-clause license.\n## See http://www.tldrlegal.com/l/BSD2 for details\n## Michael Ash, Thomas Herndon, and Robert Pollin April-May 2013\n## Replicate and extend Reinhart and Rogoff (2010) using data from RR working spreadsheet\n\nlibrary(plyr)\nlibrary(ggplot2)\nlibrary(car)\nlibrary(foreign)\nlibrary(xlsx)\noptions(scipen=10000)\noptions(digits=4)\n\nrm(list = ls())\n\n## Read the individual country data (dumped from RR.xls with Save All to CSV)\nAustralia     <- read.csv(""RR - Australia.csv"") \nAustria       <- read.csv(""RR - Austria.csv"")   \nBelgium       <- read.csv(""RR - Belgium.csv"")   \nCanada        <- read.csv(""RR - Canada.csv"")    \nDenmark       <- read.csv(""RR - Denmark.csv"")   \nFinland       <- read.csv(""RR - Finland.csv"")   \nFrance        <- read.csv(""RR - France.csv"")    \nGermany       <- read.csv(""RR - Germany.csv"")   \nGreece        <- read.csv(""RR - Greece.csv"")    \nIreland       <- read.csv(""RR - Ireland.csv"")   \nItaly         <- read.csv(""RR - Italy.csv"")     \nJapan         <- read.csv(""RR - Japan.csv"")     \nNetherlands   <- read.csv(""RR - Netherlands.csv"")\nNewZealand    <- read.csv(""RR - New Zealand.csv"")\nNorway        <- read.csv(""RR - Norway.csv"")\nPortugal      <- read.csv(""RR - Portugal.csv"")\nSpain         <- read.csv(""RR - Spain.csv"")\nSweden        <- read.csv(""RR - Sweden.csv"")\nUK            <- read.csv(""RR - UK.csv"")\nUS            <- read.csv(""RR - US.csv"")\n\n## Stack the data\nRR <- merge(Australia,Austria,all=TRUE)\nRR <- merge(RR,Belgium    ,all=TRUE)\nRR <- merge(RR,Canada     ,all=TRUE)\nRR <- merge(RR,Denmark    ,all=TRUE)\nRR <- merge(RR,Finland    ,all=TRUE)\nRR <- merge(RR,France     ,all=TRUE)\nRR <- merge(RR,Germany    ,all=TRUE)\nRR <- merge(RR,Greece     ,all=TRUE)\nRR <- merge(RR,Ireland    ,all=TRUE)\nRR <- merge(RR,Italy      ,all=TRUE)\nRR <- merge(RR,Japan      ,all=TRUE)\nRR <- merge(RR,Netherlands,all=TRUE)\nRR <- merge(RR,NewZealand ,all=TRUE)\nRR <- merge(RR,Norway     ,all=TRUE)\nRR <- merge(RR,Portugal   ,all=TRUE)\nRR <- merge(RR,Spain      ,all=TRUE)\nRR <- merge(RR,Sweden     ,all=TRUE)\nRR <- merge(RR,UK         ,all=TRUE)\nRR <- merge(RR,US         ,all=TRUE)\n\nwith(RR,table(Country))\n\n## Convert public debt/GDP\nRR$debtgdp <- RR$debtgdp\n\nwrite.dta(RR,""RR-basic.dta"")\n\n## Follow rules in RR working spreadsheet for calculating public debt/GDP for each country-year\nRR <- within(RR, debtgdp <- ifelse(Country==""Australia"",ifelse(Year<=1948,100*Debt/GDP1,100*Debt/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Austria"",ifelse(Year<=1979,100*Debt1/GDP1,100*Debt2/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Belgium"",ifelse(Year<=1979,100*Debt/GDP1,100*Debt/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Canada"",ifelse(Year<=1948,100*Debt/GDP1,100*Debt/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Denmark"",ifelse(Year<=1949,100*Debt1/GDP1,100*Debt1/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Finland"",ifelse(Year<=1977,100*Debt1/GDP1,100*Debt2/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""France"",ifelse(Year<=1948, 100*Debt1 / GDP1, ifelse(Year<=1978, 100*Debt1 / GDP2,100*Debt2/GDP2)),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Germany"",ifelse(Year<=1950,100*Debt1/GDP1,100*Debt2/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Greece"",ifelse((Year>=1884 & Year<=1913) | (Year>=1919 & Year<=1939) | (Year>=1970 & Year<=1992),100*Debt/GDP1, ifelse(Year==2009,100,debtgdp)),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Ireland"",ifelse(Year<=2010,100*Debt/GDP2,debtgdp),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Italy"",ifelse(Year<=1913,100*Debt/GDP1,ifelse(Year<=1946,100*Debt/GNI,ifelse(Year<=1998,100*Debt/GDP1,100*Debt/GDP2))),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Japan"",ifelse(Year<=1953,100*Debt/GDP1,100*Debt/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Netherlands"",ifelse(Year<=1956,100*Debt/GDP1,100*Debt/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""New Zealand"",ifelse(Year<=1947,100*Debt/GDP1,100*Debt/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Norway"",ifelse(Year<=1948,100*Debt/GDP1,100*Debt/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Portugal"",ifelse(Year<=1999,100*Debt1/GDP1,100*Debt2/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Spain"",ifelse(Year<=1957,100*Debt/GDP1,100*Debt/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""Sweden"",ifelse(Year<=1949,100*Debt/GDP1,100*Debt/GDP2),debtgdp))\nRR <- within(RR, debtgdp <- ifelse(Country==""UK"" , 100*Debt/GDP, debtgdp ))\nRR <- within(RR, debtgdp <- ifelse(Country==""US"" , 100*Debt/GDP, debtgdp ))\n\nRR$RGDP <- as.numeric(RR$RGDP)\nRR$RGDP1 <- as.numeric(RR$RGDP1)\nRR$RGDP2 <- as.numeric(RR$RGDP2)\n\n## Calculate real GDP growth using rules per RR spreadsheet \nlg<-function(x)c(NA,x[1:(length(x)-1)])\nRR <- ddply( RR, ~Country, transform, lRGDP=lg(RGDP), lRGDP1=lg(RGDP1), lRGDP2=lg(RGDP2)  )\nRR <- within(RR, dRGDP <- ifelse( !is.na(dRGDP), dRGDP,\n                                   ifelse( !is.na( RGDP / lRGDP - 1 ), 100*(RGDP / lRGDP - 1) ,\n                                          ifelse( !is.na( RGDP2 / lRGDP2 - 1 ), 100*(RGDP2 / lRGDP2 - 1) ,\n                                                 ifelse( !is.na( RGDP1 / lRGDP1 - 1 ), 100*(RGDP1 / lRGDP1 - 1),dRGDP )))))\n\nwrite.dta(RR,""RR-200-processed.dta"")\n\n## Cut to postwar analysis\nRR <- subset(RR,Year>=1946 & Year<=2009)\n\n## Italy uses another data series through 1946 and is excluded from GITD postwar until 1951\nRR <- subset(RR, !(Year<1951 & Country==""Italy""))\n\n## Potential data years 1946-2009\navail.data <- ddply(RR, ~Country, summarize,\n                    count.year.GDP=sum(!is.na(dRGDP)),count.year.debt=sum(!is.na(debtgdp)), count.year.both=sum(!is.na(dRGDP) & !is.na(debtgdp)))\navail.data[order(avail.data[,""count.year.both""]),]\n\nwrite.dta(RR,""RR-processed.dta"")\n## Slow\nRR$dgcat.lm <- cut(RR$debtgdp, breaks=c(0,30,60,90,Inf))\nRR$dgcat <- factor(RR$dgcat.lm, labels = c(""0-30%"",""30-60%"",""60-90%"",""Above 90%""),ordered=TRUE)\nwrite.xlsx2(subset(RR,TRUE,select=c(Country,Year,debtgdp,dgcat,dRGDP) ),""RR-keycolumns.xlsx"",row.names=FALSE)\n\n\n## Limit to actually available data\nRR <- subset(RR,  !is.na(dRGDP) & !is.na(debtgdp))\n\n## Actually available data years 1946-2009\navail.data <- ddply(RR, ~Country, summarize, min.year=min(Year), count.year=sum(!is.na(dRGDP) & !is.na(debtgdp)))\navail.data[order(avail.data[,""min.year""]),]\n\nwith(RR,table(Year))\nwith(RR,table(Country))\n\n## Create RR public debt/GDP categories\nRR$dgcat.lm <- cut(RR$debtgdp, breaks=c(0,30,60,90,Inf))\nRR$dgcat <- factor(RR$dgcat.lm, labels = c(""0-30%"",""30-60%"",""60-90%"",""Above 90%""),ordered=TRUE)\n\n## Create expanded public debt/GDP categories\nRR$dgcat2.lm <- cut(RR$debtgdp, breaks=c(0,30,60,90,120,Inf))\nRR$dgcat2 <- factor(RR$dgcat2.lm, labels = c(""0-30%"",""30-60%"",""60-90%"",""90-120%"",""Above 120%""),ordered=TRUE)\n\n## Regression analysis of categories\nsummary(dgcat.lm <- lm(dRGDP ~ dgcat.lm, data=RR))\nsummary(dgcat2.lm <- lm(dRGDP ~ dgcat2.lm, data=RR))\nlinearHypothesis(dgcat2.lm,\n                 verbose=TRUE,\n                 paste( c(""dgcat2.lm(30,60]=dgcat2.lm(60,90]"", ""dgcat2.lm(30,60]=dgcat2.lm(90,120]"", ""dgcat2.lm(30,60]=dgcat2.lm(120,Inf]"") ))\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""dgcat2.lm(30,60]=dgcat2.lm(60,90]"", ""dgcat2.lm(30,60]=dgcat2.lm(90,120]"")))\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""dgcat2.lm(30,60]=dgcat2.lm(60,90]"")))\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""dgcat2.lm(60,90]=dgcat2.lm(90,120]"")))\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""dgcat2.lm(30,60]=dgcat2.lm(90,120]"")))\n\nlinearHypothesis(dgcat2.lm, verbose=TRUE, paste( c(""(Intercept) + dgcat2.lm(90,120] = 3"") ))\n\n\n## Country-Year average by debtgdp (""correct weights"")\n## Table 3 Corrected\n(RR.correct.sd <- with(RR, tapply( dRGDP, dgcat, sd, na.rm=TRUE )))\n(RR.correct.mean <- with(RR, tapply( dRGDP, dgcat, mean, na.rm=TRUE )))\nRR.correct.mean.df <- data.frame(RR.correct.mean, dgcat=names(RR.correct.mean) )\n## Averaged Country averages by debtgdp (""equal weights"")\n(RR.equalwt.mean <- with(RR, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\n## NYT Appendix input to Table 1 Line 3\n(RR.equalwt.median <- with(RR, tapply( dRGDP, list(Country,dgcat), median, na.rm=TRUE )))\n## Table 3 Country equal weighting\nsummary(RR.equalwt.mean)\n\n## Country-Year average by debtgdp (""correct weights"") expanded categories\n(RR.correct.mean.2 <- with(RR, tapply( dRGDP, dgcat2, mean, na.rm=TRUE )))\nRR.correct.mean.2.df <- data.frame(RR.correct.mean.2, dgcat=names(RR.correct.mean.2) )\n## Averaged Country averages by debtgdp (""equal weights"")\n(RR.ex.equalwt.mean <- with(RR, tapply( dRGDP, list(Country,dgcat2), mean, na.rm=TRUE )))\nsummary(RR.ex.equalwt.mean)\n\n\n## Selective treatment of early years\nRR.selective <- subset(RR,\n                       !((Year<1950 & Country==""New Zealand"") | (Year<1951 & Country==""Australia"") | (Year<1951 & Country==""Canada"") ))\n(RR.selective.mean <- with(RR.selective, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\n(RR.selective.median <- with(RR.selective, tapply( dRGDP, list(Country,dgcat), median, na.rm=TRUE )))\n## Equal weights\n## Table 3 Weights,Exclusion\nsummary(RR.selective.mean)\n## Correct weights\n## Table 3 Selective years exclusion\nwith(RR.selective, tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n\n## Spreadsheet error \nRR.spreadsheet <- subset(RR, ! Country %in% c(""Australia"",""Austria"",""Belgium"",""Canada"",""Denmark"") )\n(RR.spreadsheet.mean <- with(RR.spreadsheet, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\n(RR.spreadsheet.median <- with(RR.spreadsheet, tapply( dRGDP, list(Country,dgcat), median, na.rm=TRUE )))\n## Table 3 Spreadsheet, Weights\nsummary(RR.spreadsheet.mean)\n## Table 3 Spreadsheet error\nwith(RR.spreadsheet, tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n\n## Selective treatment of early years and spreadsheet error\nRR.selective.spreadsheet <- subset(RR.selective, ! Country %in% c(""Australia"",""Austria"",""Belgium"",""Canada"",""Denmark"") )\nRR.selective.spreadsheet.mean <- with(RR.selective.spreadsheet, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE ))\n(RR.selective.spreadsheet.eqweight.median <- with(RR.selective.spreadsheet, tapply( dRGDP, list(Country,dgcat), median, na.rm=TRUE )))\n## Equal weights\n## Table 3 Weights,Exclusion,Spreadsheet Error\nsummary(RR.selective.spreadsheet.mean)\n## Correct weights\n## Table 3 Exclusion,Spreadsheet Error\nwith(RR.selective.spreadsheet, tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n\n## Actually available data years 1946-2009 with selective exclusion and spreadsheet error\navail.data <- ddply(RR.selective.spreadsheet, ~Country, summarize, min.year=min(Year), count.year=sum(!is.na(dRGDP) & !is.na(debtgdp)))\navail.data[order(avail.data[,""min.year""]),]\n\n## And New Zealand transcription error\n## selective.spreadsheet.transcription <- with(RR.selective.spreadsheet, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE ))\nRR.selective.spreadsheet.mean.transcription <- RR.selective.spreadsheet.mean\nRR.selective.spreadsheet.mean.transcription[""New Zealand"",4] <- -7.9\nsummary(RR.selective.spreadsheet.mean.transcription)\n## Table 3 Weights,Exclusion,Spreadsheet Error,Transcription\n(RR.published.mean <- apply(RR.selective.spreadsheet.mean.transcription,2,mean,na.rm=TRUE))\nRR.published.mean.df <- data.frame(RR.published.mean , dgcat=names(RR.published.mean) )\n\n\n## Medians\n## NYT Appendix Table 1 Line 4\n(RR.correct.median <- with(RR, tapply( dRGDP, dgcat, median, na.rm=TRUE )))\n(RR.correct.selective.median <- with(RR.selective, tapply( dRGDP, dgcat, median, na.rm=TRUE )))\n(RR.correct.spreadsheet.median <- with(RR.spreadsheet, tapply( dRGDP, dgcat, median, na.rm=TRUE )))\n## NYT Appendix Table 1 Line 3 (Use Median line)\n(RR.eqweight.median <- summary(RR.equalwt.median))\nsummary(RR.spreadsheet.median)\n## NYT Appendix Table 1 Line 2 (Dataset is ""RR.selective"" because it EXCLUDES early years but spreadsheet is corrected)\nsummary(RR.selective.median)\n(RR.correct.ex.median <- with(RR, tapply( dRGDP, dgcat2, median, na.rm=TRUE )))\n(RR.selective.spreadsheet.mean.median <- with(RR.selective.spreadsheet, tapply( dRGDP, dgcat, median, na.rm=TRUE )))\n(RR.published.median <- apply(RR.selective.spreadsheet.eqweight.median,2,median,na.rm=TRUE))\n\n\n## Counts of years\nwith(RR, table(Country,dgcat))\napply(with(RR,table( Country,dgcat)),2,sum)\n\nwith(RR.selective,table( Country,dgcat))\napply(with(RR.selective,table( Country,dgcat)),2,sum)\n\nwith(RR.selective.spreadsheet,table( Country,dgcat))\napply(with(RR.selective.spreadsheet,table( Country,dgcat)),2,sum)\n\n\nRR.newzealand.1951 <- subset(RR.selective.spreadsheet,Country==""New Zealand"" & Year==1951)\n\n\n## Categorical scatterplot\nn <- ggplot(RR, aes(x=dgcat,y=dRGDP)) + geom_point(shape=3,color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Category"")\nn <- n + geom_point(RR.published.mean.df, mapping=aes(x=dgcat,y=RR.published.mean), shape=5,  size=5 ) \nn <- n + geom_text(RR.published.mean.df, mapping=aes(x=dgcat,y=RR.published.mean,label=round(RR.published.mean,1)),hjust=-0.7,size=3,color=\'darkgray\')\nn <- n + geom_point(RR.correct.mean.df,  mapping=aes(x=dgcat,y=RR.correct.mean,label=RR.correct.mean), shape=16, size=4 )  + theme_bw()\nn <- n + geom_text(RR.correct.mean.df,  mapping=aes(x=dgcat,y=RR.correct.mean,label=round(RR.correct.mean,1)), hjust=1.7,size=3,color=\'darkgray\')\nn <- n + geom_point(RR.newzealand.1951,mapping=aes(x=dgcat,y=dRGDP), shape=0, size=3 )\nn <- n + geom_text(RR.newzealand.1951,mapping=aes(x=dgcat,y=dRGDP,label=paste(round(dRGDP,1))), hjust=-0.7,size=3,color=\'darkgray\')\nn <- n + geom_text(RR.newzealand.1951,mapping=aes(x=dgcat,y=dRGDP,label=paste(""NZ"",Year)), hjust=1.2,size=3,color=\'darkgray\')\nprint(n)\n\n\n## Create legend for categorical scatterplot\nplot(3,10,pch=0,ylim=c(0,70),xlim=c(0,5.5))\ntext(3.2,10,""New Zealand 1951"",adj=0)\npoints(0,15,pch=16)\ntext(0.2,15,""Correct average real GDP growth"",adj=0)\npoints(0,10,pch=5,cex=1.5)\ntext(0.2,10,""RR average real GDP growth"",adj=0)\npoints(3,15,pch=3,col=\'darkgray\')\ntext(3.2,15,""Country-Year real GDP growth"",adj=0)\n\n## Categorical scatterplot for expanded categories\no <- ggplot(RR, aes(x=dgcat2,y=dRGDP)) + geom_point(shape=3,color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Category"")\no <- o + geom_point(RR.correct.mean.2.df,  mapping=aes(x=dgcat,y=RR.correct.mean.2), shape=16, size=4 )  + theme_bw()\no <- o + geom_text(RR.correct.mean.2.df, mapping=aes(x=dgcat,y=RR.correct.mean.2,label=round(RR.correct.mean.2,1)), hjust=1.7, size=3,color=\'darkgray\')\nprint(o)\n\n## Scatterplot (continuous)\nlibrary(mgcv)\nRR.gam <- gam(dRGDP ~ s(debtgdp, bs=""cs""),data=RR)\n\n## Cross-validation technique for loess parameters\n## http://stats.stackexchange.com/questions/2002/how-do-i-decide-what-span-to-use-in-loess-regression-in-r\nm <- ggplot(RR, aes(x=debtgdp,y=dRGDP))\nm1 <- m + geom_vline(xintercept=90,color=\'lightgray\',size=1.5)\nm1 <- m1 + geom_point(color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Ratio"") + scale_x_continuous(breaks=seq(0,240,30)) + theme_bw()\n## m1 <- m1 + geom_smooth(method=\'loess\',span=1.0,color=\'black\') + geom_smooth(method=\'loess\',span=0.2,color=\'black\')\nm1 <- m1 + geom_smooth(method=gam, color=\'black\',formula= y ~ s(x, bs = ""cs""))\n## m1 <- m1 + geom_smooth(method=\'auto\', color=\'black\')\nprint(m1)\n\n## Categorical scatterplot later years\nRR2000 <- subset(RR, Year>=2000)\n(RR.later.mean <- with(RR2000, tapply( dRGDP, dgcat, mean, na.rm=TRUE )))\nRR.later.mean.df <- data.frame(RR.later.mean, dgcat=names(RR.later.mean) )\n\nL <- ggplot(RR2000, aes(x=dgcat,y=dRGDP)) + geom_point(shape=3,color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Category"")\n## L <- L + geom_text(mapping=aes(label=paste(Country,Year) ), size=2, hjust=-0.2,color=\'darkgray\') \nL <- L + geom_point(RR.later.mean.df,  mapping=aes(x=dgcat,y=RR.later.mean,label=RR.later.mean), shape=16, size=4 )  + theme_bw()\nL <- L + geom_text(RR.later.mean.df,  mapping=aes(x=dgcat,y=RR.later.mean,label=round(RR.later.mean,1)), hjust=1.7,size=3,color=\'darkgray\')\n## L <- L + coord_cartesian(ylim=c(-12, 30)) \nprint(L)\n\n\n## Scatterplot closeup\npdf(""closeup.pdf"",height=4,width=7)\nm2 <- m + geom_point(color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Ratio"") + scale_x_continuous(breaks=seq(0,240,30)) + theme_bw() +  geom_vline(xintercept=90,color=\'lightgray\',size=1.5)\n## m2 <- m2 + geom_smooth(method=\'loess\',span=0.75,color=\'black\') + geom_smooth(method=\'loess\',span=0.4,color=\'black\') \n## m2 <- m2 + geom_smooth(method=\'auto\',color=\'black\')\nm2 <- m2 + geom_smooth(method=gam, color=\'black\',formula= y ~ s(x, bs = ""cs""))\nm2 <- m2 + coord_cartesian(ylim=c(0, 7),xlim=c(0,150)) + scale_y_continuous(breaks=c(0,1,2,3,4,5,6,7)) + theme_bw()\nprint(m2)\n\n\n## Get the range for which the lower bound and upper bound include 3 percent\nsubset(ggplot_build(m1)$data[[3]],  (ymin<3.1 & ymin>2.9) | (ymax<3.1 & ymax>2.9 ))\n\nsubset(RR,\n       Country %in% c(""Australia"",""Belgium"",""Canada"",""Greece"",""Ireland"",""Italy"",""Japan"",""New Zealand"",""UK"",""US""),\n       select=c(Country,Year,dgcat,debtgdp,dRGDP))\n\nsubset(RR,\n       debtgdp>90,\n       select=c(Country,Year,dgcat,debtgdp,dRGDP))\n\n## Look at the public debt / GDP series\n## p <- ggplot(RR, aes(x=Year,y=debtgdp,color=Country)) + geom_point() +  facet_grid(. ~ Country) + opts(legend.position=""bottom"")\n## print(p)\n\n\n## Average growth by debtgdp for more recent samples\n## country-year weights\nwith(subset(RR, Year>=1950), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1950), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1960), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1960), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1970), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1970), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1980), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1980), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1990), tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1990), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,mean,na.rm=TRUE)\n\n## Post-2000\n## NYT Appendix Table 4 Line 1\n(mean2000 <- with(RR2000, tapply( dRGDP, dgcat, mean, na.rm=TRUE )))\n(sd2000 <- with(RR2000, tapply( dRGDP, dgcat, sd, na.rm=TRUE )))\n(length2000 <- with(RR2000, tapply( dRGDP, dgcat, length )))\n\n## NYT Appendix Table 4 Line 1 Standard errors\nsd2000 / sqrt(length2000)\n\n## country-year weights\n(RR2000.equalwt.mean <- with(RR2000, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\n\nwith(RR2000, table(Country,dgcat))\n\n## Regression analysis of categories\nsummary(dgcat.lm <- lm(dRGDP ~ dgcat.lm, data=RR2000))\n\n## country weights\napply(RR2000.equalwt.mean,2,mean,na.rm=TRUE)\napply(RR2000.equalwt.mean,2,sd,na.rm=TRUE)\napply(with(RR2000,table( Country,dgcat)),2,sum)\n\n\n\n## Median analysis of two periods ## Mean has problems because of very high value for Belgium in 1947\n## country-year weights\nwith(subset(RR, Year<1980), tapply( dRGDP, dgcat, median, na.rm=TRUE ))\n## country weights  ## Beware ridiculous value for Belgian GDP growth in 1947\napply(with(subset(RR, Year<1980), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,median,na.rm=TRUE)\n\n## country-year weights\nwith(subset(RR, Year>=1980), tapply( dRGDP, dgcat, median, na.rm=TRUE ))\n## country weights\napply(with(subset(RR, Year>=1980), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,median,na.rm=TRUE)\n\n## country-year weights\nwith(RR, tapply( dRGDP, dgcat, median, na.rm=TRUE ))\n## country weights\napply(with(RR, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,median,na.rm=TRUE)\n\n## Median analysis of two periods ## Mean has problems because of very high value for Belgium in 1947\n## 1955-1980 has only Britain in the highest public debt category!!\n## country-year weights\nwith(subset(RR, Year>=1955 & Year<1980), tapply( dRGDP, dgcat, median, na.rm=TRUE ))\n## country weights  ## Beware ridiculous value for Belgian GDP growth in 1947\napply(with(subset(RR, Year>=1955 & Year<1980), tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )),2,median,na.rm=TRUE)\n\n\n\nwrite.xlsx2(subset(RR,Year>=1990 & debtgdp>90,select=c(Country,Year,debtgdp,dgcat,dRGDP)),""RR-post1990.xlsx"",row.names=FALSE)\n\n\nsubset(RR,dRGDP>10,select=c(Country,Year,dRGDP,dgcat,debtgdp))\nsubset(RR,dRGDP< -7,select=c(Country,Year,dRGDP,dgcat,debtgdp))\n\nrm(list = ls())\n\n', 'library(plyr)\nlibrary(ggplot2)\nlibrary(car)\nlibrary(foreign)\nlibrary(xlsx)\noptions(scipen=10000)\noptions(digits=4)\n\nrm(list = ls())\n\nRR <- read.dta(""../RR-200-processed.dta"")\n\n## Brief 200 years analysis\nRR <- subset(RR,(Year>=1791 & Country==""US"")\n             | (Year>=1830 & Country==""UK"")\n             | (Year>=1880 & Country==""Sweden"")\n             | (Year>=1850 & Country==""Spain"")\n             | (Year>=1880 & Country==""Portugal"")\n             | (Year>=1880 & Country==""Norway"")\n             | (Year>=1932 & Country==""New Zealand"")\n             | (Year>=1880 & Country==""Netherlands"")\n             | (Year>=1885 & Country==""Japan"")\n             | (Year>=1880 & Country==""Italy"")             \n             | (Year>=1949 & Country==""Ireland"")                          \n             | (Year>=1885 & Country==""Japan"")\n             | (Year>=1884 & Country==""Greece"")\n             | (Year>=1880 & Country==""Germany"")             \n             | (Year>=1880 & Country==""France"")\n             | (Year>=1914 & Country==""Finland"")\n             | (Year>=1880 & Country==""Denmark"")\n             | (Year>=1925 & Country==""Canada"")\n             | (Year>=1835 & Country==""Belgium"")\n             | (Year>=1880 & Country==""Austria"")\n             | (Year>=1902 & Country==""Australia"")\n             )\n\n\n## Limit to actually available data\nRR <- subset(RR,  !is.na(dRGDP) & !is.na(debtgdp))\n\n## Actually available data years \navail.data <- ddply(RR, ~Country, summarize, min.year=min(Year), count.year=sum(!is.na(dRGDP) & !is.na(debtgdp)))\navail.data[order(avail.data[,""min.year""]),]\n\n## Create RR public debt/GDP categories\nRR$dgcat.lm <- cut(RR$debtgdp, breaks=c(0,30,60,90,Inf))\nRR$dgcat <- factor(RR$dgcat.lm, labels = c(""0-30%"",""30-60%"",""60-90%"",""Above 90%""),ordered=TRUE)\n\n## Create expanded public debt/GDP categories\nRR$dgcat2.lm <- cut(RR$debtgdp, breaks=c(0,30,60,90,120,Inf))\nRR$dgcat2 <- factor(RR$dgcat2.lm, labels = c(""0-30%"",""30-60%"",""60-90%"",""90-120%"",""Above 120%""),ordered=TRUE)\n\n\n(RR.equalwt.mean <- with(RR, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\nsummary(RR.equalwt.mean)\n## NYT Appendix Table 2 Line 2\n(RR.correct.mean <- with(RR, tapply( dRGDP, dgcat, mean, na.rm=TRUE )))\nRR.correct.mean.df <- data.frame(RR.correct.mean, dgcat=names(RR.correct.mean) )\n\n\n## country weights\napply(RR.equalwt.mean,2,mean,na.rm=TRUE)\napply(RR.equalwt.mean,2,sd,na.rm=TRUE)\nwith(RR, table(Country,dgcat))\napply(with(RR,table( Country,dgcat)),2,sum)\n\n\n## Test effect of spreadsheet error in 200-year analysis\nRR.spreadsheet <- subset(RR, ! Country %in% c(""Australia"",""Austria"",""Belgium"",""Canada"",""Denmark"") )\n(RR.spreadsheet.mean <- with(RR.spreadsheet, tapply( dRGDP, list(Country,dgcat), mean, na.rm=TRUE )))\nsummary(RR.spreadsheet.mean)\nwith(RR.spreadsheet, tapply( dRGDP, dgcat, mean, na.rm=TRUE ))\n\n## Expanded categories \n(RR.ex.equalwt.mean <- with(RR, tapply( dRGDP, list(Country,dgcat2), mean, na.rm=TRUE )))\nsummary(RR.ex.equalwt.mean)\n## NYT Appendix Table 3 \n(RR.ex.correct.mean <- with(RR, tapply( dRGDP, dgcat2, mean, na.rm=TRUE )))\nRR.ex.correct.mean.df <- data.frame(RR.ex.correct.mean, dgcat=names(RR.ex.correct.mean) )\n\n\n## Compute standard errors\n(mean200 <- with(RR, tapply( dRGDP, dgcat2, mean, na.rm=TRUE )))\n(sd200 <- with(RR, tapply( dRGDP, dgcat2, sd, na.rm=TRUE )))\n(length200 <- with(RR, tapply( dRGDP, dgcat2, length )))\nsd200 / sqrt(length200)\n\n\nL <- ggplot(RR, aes(x=dgcat2,y=dRGDP)) + geom_point(shape=3,color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Category"")\n## L <- L + geom_text(mapping=aes(label=Country ), size=2, hjust=-0.7,color=\'darkgray\') \nL <- L + geom_point(RR.ex.correct.mean.df,  mapping=aes(x=dgcat,y=RR.ex.correct.mean,label=RR.ex.correct.mean), shape=16, size=4 )  + theme_bw()\nL <- L + geom_text(RR.ex.correct.mean.df,  mapping=aes(x=dgcat,y=RR.ex.correct.mean,label=round(RR.ex.correct.mean,1)), hjust=1.7,size=3,color=\'darkgray\')\nprint(L)\n\n\n## Scatterplot (continuous)\nlibrary(mgcv)\nRR.gam <- gam(dRGDP ~ s(debtgdp, bs=""cs""),data=RR)\n\n## Cross-validation technique for loess parameters\n## http://stats.stackexchange.com/questions/2002/how-do-i-decide-what-span-to-use-in-loess-regression-in-r\nm <- ggplot(RR, aes(x=debtgdp,y=dRGDP))\nm1 <- m + geom_vline(xintercept=90,color=\'lightgray\',size=1.5)\nm1 <- m1 + geom_point(color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Ratio"") + scale_x_continuous(breaks=seq(0,240,30)) + theme_bw()\n## m1 <- m1 + geom_smooth(method=\'loess\',span=1.0,color=\'black\') + geom_smooth(method=\'loess\',span=0.2,color=\'black\')\nm1 <- m1 + geom_smooth(method=gam, color=\'black\',formula= y ~ s(x, bs = ""cs""))\n## m1 <- m1 + geom_smooth(method=\'auto\', color=\'black\')\nprint(m1)\n\n## Scatterplot closeup\npdf(""closeup.pdf"",height=4,width=7)\nm2 <- m + geom_point(color=\'darkgray\') + ylab(""Real GDP Growth"") + xlab(""Public Debt/GDP Ratio"") + scale_x_continuous(breaks=seq(0,240,30)) + theme_bw() +  geom_vline(xintercept=90,color=\'lightgray\',size=1.5)\n## m2 <- m2 + geom_smooth(method=\'loess\',span=0.75,color=\'black\') + geom_smooth(method=\'loess\',span=0.4,color=\'black\') \n## m2 <- m2 + geom_smooth(method=\'auto\',color=\'black\')\nm2 <- m2 + geom_smooth(method=gam, color=\'black\',formula= y ~ s(x, bs = ""cs""))\nm2 <- m2 + coord_cartesian(ylim=c(0, 7),xlim=c(0,150)) + scale_y_continuous(breaks=c(0,1,2,3,4,5,6,7)) + theme_bw()\nprint(m2)\n\n']","Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff The provided assets are the source code of the conducted experiments in the publication ""Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff"" written by Thomas Herndon, Michael Ash and Robert Pollin. The assets were retrieved from following URL:https://www.peri.umass.edu/images/WP322HAP-RR-GITD-code-2013-05-17.zipModifications to the original content:The provided zip file was unzipped and the internal folder structure was flattened (i.e. the files RR-200.R and RR-200.Rout were moved to the root level of the archive)Information Text from the website of the University of Massachusetts Amherst:Herndon, Ash and Pollin replicate Reinhart and Rogoffand find that coding errors, selective exclusion of available data, and unconventional weighting of summary statistics lead to serious errors that inaccurately represent the relationship between public debt and GDP growth among 20 advanced economies in the post-war period. They find that when properly calculated, the average real GDP growth rate for countries carrying a public-debt-to-GDP ratio of over 90 percent is actually 2.2 percent, not -0:1 percent as published in Reinhart and Rogoff. That is, contrary to RR, average GDP growth at public debt/GDP ratios over 90 percent is not dramatically different than when debt/GDP ratios are lower.The authors also show how the relationship between public debt and GDP growth varies significantly by time period and country. Overall, the evidence we review contradicts Reinhart and Rogoff's claim to have identified an important stylized fact, that public debt loads greater than 90 percent of GDP consistently reduce GDP growth.",1
Controlling SARS-CoV-2 in schools using repetitive testing strategies,"Simulation model developed to assess testing strategies in primary schools. The manuscript investigating testing strategies in which the simulator has been used is ""Controlling SARS-CoV-2 in schools using repetitive testing strategies"", published in eLife (DOI: https://doi.org/10.7554/eLife.75593).",,"Controlling SARS-CoV-2 in schools using repetitive testing strategies Simulation model developed to assess testing strategies in primary schools. The manuscript investigating testing strategies in which the simulator has been used is ""Controlling SARS-CoV-2 in schools using repetitive testing strategies"", published in eLife (DOI: https://doi.org/10.7554/eLife.75593).",1
Data for: Combining range and phenology shifts offers a winning strategy for boreal Lepidoptera,"Species can adapt to climate change by adjusting in situ or by dispersing to new areas, and these strategies may complement or enhance each other. Here, we investigate temporal shifts in phenology and spatial shifts in northern range boundaries for 289 Lepidoptera species by using long-term data sampled over two decades. While 40% of the species neither advanced phenology nor moved northward, nearly half (45%) used one of the two strategies. The strongest positive population trends were observed for the minority of species (15%) that both advanced flight phenology and shifted their northern range boundaries northward. We show that, for boreal Lepidoptera, a combination of phenology and range shifts is the most viable strategy under a changing climate. Effectively, this may divide species into winners and losers based on their propensity to capitalize on this combination, with potentially large consequences on future community composition.","['#Script for conducting linear regression on day of year for Lepidoptera observations\r\n#\r\n# Maria Hllfors, Juha Pyry, Janne Helil, et al. Combining range and phenology shifts offers a winning strategy\r\n# for boreal Lepidoptera. Ecology Letters, accepted April 14th 2021. DOI:10.1111/ele.13774\r\n#\r\n\r\n##############\r\n# sessionInfo()\r\n# R version 4.0.5 (2021-03-31)\r\n# Platform: x86_64-w64-mingw32/x64 (64-bit)\r\n# Running under: Windows 10 x64 (build 18363)\r\n##############\r\n\r\n#####\r\n#Install and load required packages\r\n#####\r\n\r\n# Package names\r\npackages <- c(""broom.mixed"", ""dplyr"", ""lme4"", ""lmerTest"")\r\n\r\n# Install packages not yet installed\r\ninstalled_packages <- packages %in% rownames(installed.packages())\r\nif (any(installed_packages == FALSE)) {\r\n  install.packages(packages[!installed_packages])\r\n}\r\n\r\n#load libraries\r\nlibrary(dplyr)\r\nlibrary(broom.mixed)\r\nlibrary(lme4)\r\nlibrary(lmerTest)\r\n\r\n#########\r\n#Set wd and read in data\r\n#########\r\n\r\n#set working directory\r\n#setwd(""MY/PATH/"")\r\n\r\n#read in data\r\ndata=read.csv(""Data_Phenology_Butterflies_Moths.csv"")\r\n\r\n\r\n################\r\n#Linear regression of Day of year as a function of Year, with Abundance as weights and\r\n#Site as random factor, separate lmer for each species\r\n################\r\n\r\n#centre Year\r\ndata$YearC=scale(data$Year, scale=FALSE)\r\n\r\n#separate models per sp \r\nmodelsfit<- data %>%\r\n  group_by(Species) %>%\r\n  do(tidy(lmer(Dayofyear ~ YearC + (1|Site), weights=Abundance, data=.)))\r\n\r\n#extract statistics only for slope on Year\r\nB_modCoef = modelsfit %>%\r\n  dplyr::filter(term==""YearC"")\r\n\r\n\r\n# save output as .csv \r\n# write.csv(B_modCoef%>%select(Species, estimate, std.error, statistic, p.value), file=""Results_Phenology_Butterflies_Moths.csv"")\r\n', '#Script for subsampling distribution data to produce a sata with equally many observations \r\n#in each time period (here, three time periods, time periods 1 and 3 used in Hllfors et al. 2021)\r\n#\r\n# Maria Hllfors, Juha Pyry, Janne Helil, et al. Combining range and phenology shifts offers a winning strategy\r\n# for boreal Lepidoptera. Ecology Letters, accepted April 14th 2021. DOI:10.1111/ele.13774\r\n#\r\n############ DISCLAIMER ON REPRODUCIBILITY #############\r\n#NB! The subsampling is not determenistic, but a different subset will be produced each \r\n#time. Thus, the resulting NRBs and prevalence measures will not be exactly the same as in \r\n#the original paper.\r\n\r\n##############\r\n# sessionInfo()\r\n# R version 4.0.5 (2021-03-31)\r\n# Platform: x86_64-w64-mingw32/x64 (64-bit)\r\n# Running under: Windows 10 x64 (build 18363)\r\n##############\r\n\r\n\r\n#####\r\n#Install and load required packages\r\n#####\r\n\r\n# Package names\r\npackages <- c(""dplyr"", ""reshape2"", ""tidyr"", ""purrr"")\r\n\r\n# Install packages not yet installed\r\ninstalled_packages <- packages %in% rownames(installed.packages())\r\nif (any(installed_packages == FALSE)) {\r\n  install.packages(packages[!installed_packages])\r\n}\r\n\r\n#load libraries\r\nlibrary(dplyr)\r\nlibrary(reshape2)\r\nlibrary(tidyr)\r\nlibrary(purrr)\r\n\r\n\r\n#########\r\n#Set wd and read in data\r\n#########\r\n\r\n#set working directory\r\n# setwd(""MY/PATH/"")\r\n\r\n#read in data\r\n\r\nBR0=read.csv(""Data_Distribution_Raw_Butterflies.csv"")\r\nMR0=read.csv(""Data_Distribution_Raw_Moths.csv"")\r\n\r\n\r\n############\r\n#PART 1 - SUBSAMPLING OF DATA\r\n############\r\n#This is done separately for the butterflies and moths\r\n\r\n\r\n#####################\r\n#2a.1 Subsample Butterflies\r\n#####################\r\n\r\n#define number of presences to subsample from each Time period (equaling to the number of observations in TP1)\r\nNobs=46602 \r\n\r\n#subsample TPs to have equal presences\r\nBR1=BR0 %>%\r\n  group_by(TP) %>%                               \r\n  tidyr::nest() %>%         \r\n  dplyr::mutate(v = purrr::map2(data, Nobs, sample_n, replace=FALSE)) %>% \r\n  tidyr::unnest(v)\r\n\r\n# write.csv(BR1, file=""Data_Distribution_Subsampled_Butterflies.csv"")\r\n\r\n\r\n####################\r\n#2a.2 subsample for moths\r\n####################\r\n\r\n#define number of presences to subsample from each Time period (equaling to the number of observations in TP1)\r\nNobs=135442\r\n\r\n#subsample TPs to have equal presences\r\nMR1=MR0 %>%\r\n  group_by(TP) %>%                              \r\n  nest() %>%        \r\n  dplyr::mutate(v = map2(data, Nobs, sample_n, replace=FALSE)) %>%  \r\n  unnest(v)\r\n\r\n# write.csv(MR1, file=""Data_Distribution_Subsampled_Moths.csv"")', '#Script for calculating shift in northern range boundary and prevalence from subsampled distribution data on Lepidoptera \r\n\r\n# Maria Hllfors, Juha Pyry, Janne Helil, et al. Combining range and phenology shifts offers a winning strategy\r\n# for boreal Lepidoptera. Ecology Letters, accepted April 14th 2021. DOI:10.1111/ele.13774\r\n#\r\n\r\n##############\r\n# sessionInfo()\r\n# R version 4.0.5 (2021-03-31)\r\n# Platform: x86_64-w64-mingw32/x64 (64-bit)\r\n# Running under: Windows 10 x64 (build 18363)\r\n##############\r\n\r\n\r\n#####\r\n#Install and load required packages\r\n#####\r\n\r\n# Package names\r\npackages <- c(""dplyr"", ""reshape2"", ""tidyr"")\r\n\r\n# Install packages not yet installed\r\ninstalled_packages <- packages %in% rownames(installed.packages())\r\nif (any(installed_packages == FALSE)) {\r\n  install.packages(packages[!installed_packages])\r\n}\r\n\r\n#load libraries\r\nlibrary(dplyr)\r\nlibrary(reshape2)\r\nlibrary(tidyr)\r\n\r\n#########\r\n#Set wd and read in data\r\n#########\r\n\r\n#set working directory\r\n# setwd(""MY/PATH/"")\r\n\r\nBR=read.csv(""Data_Distribution_Subsampled_Butterflies.csv"", sep="";"", header=TRUE)\r\nMR=read.csv(""Data_Distribution_Subsampled_Moths.csv"", sep="";"", header=TRUE)\r\n\r\n\r\n###############\r\n###############\r\n#2b.1 Butterflies\r\n###############\r\n###############\r\n\r\n#make new variable to define 10*10km grids\r\nBR$unique1010=paste(BR$Y10, BR$X10)\r\n#make new variable that tells us in how many 10*10km grids is each species present in each period\r\nBR = BR %>% \r\n  group_by(Species, TP) %>%\r\n  dplyr::mutate(PresN=length(unique(unique1010))) %>% \r\n  ungroup()\r\n\r\n#make new variable that tells us in how many 10*10km grids do we have presence (across all species) in each period\r\nBR = BR %>% \r\n  group_by(TP) %>%\r\n  dplyr::mutate(PresNTot=length(unique(unique1010))) %>% \r\n  ungroup()\r\n\r\n\r\n#############\r\n#Calculate Time-period-specific prevalence for each species\r\n#############\r\n\r\n#prevalence, i.e. number of presences of a species in 10*10km grid as proportion of all occupied 10*10km grids\r\nPrevBR = BR %>% \r\n  group_by(Species, TP) %>%\r\n  dplyr::mutate(Prev=PresN/PresNTot) %>% \r\n  ungroup()\r\n\r\n#Transform data to wide format\r\n\r\nmPrevBR=melt(PrevBR, id.vars=c(""TP"", ""Species""), measure.vars= c(""Prev""))\r\nwPrevBR=dcast(mPrevBR, Species ~ variable + TP)\r\n\r\n\r\n#############\r\n#Calculate Time-period-specific mean of 10 most northern latitudes for each species\r\n#############\r\nShiftBR = BR%>%\r\n  group_by(Species, TP)%>%\r\n  dplyr::arrange(desc(Y10))%>%\r\n  dplyr::summarise(meanTop10 = mean(Y10[1:10]))\r\n\r\n#Transform data to wide format\r\nmShiftBR=reshape2::melt(ShiftBR, id.vars=c(""TP"", ""Species""), measure.vars= c(""meanTop10""))\r\nwShiftBR=dcast(mShiftBR, Species ~ variable + TP)\r\n\r\n\r\n\r\n############\r\n#Calculate shift in Northern range boundaries (NRB)\r\n############\r\nwShiftBR=wShiftBR%>%\r\n  group_by(Species)%>%\r\n  dplyr::mutate(NRBshift=10*(meanTop10_3-meanTop10_1)) #multiply by 10 to get shift in kilometers\r\n\r\n############\r\n#Calculate shift in Prevalence\r\n############\r\nwPrevBR=wPrevBR%>%\r\n  group_by(Species)%>%\r\n  dplyr::mutate(PrevChange=(Prev_3-Prev_1))\r\n\r\n\r\n#Combine both both data frames\r\ncBR=left_join(wShiftBR, wPrevBR, by=""Species"")\r\nhead(cBR)\r\n\r\n#save output as .csv\r\n# write.csv(cBR, file=""Results_NRBshift_PrevChange_Butterflies.csv"")\r\n\r\n\r\n###############\r\n###############\r\n#2b.2 Moths\r\n###############\r\n###############\r\n\r\n#make new variable to define 10*10km grids\r\nMR$unique1010=paste(MR$Y10, MR$X10)#unique 10*10km grids\r\n\r\n#make new variable that tells us in how many 10*10km grids is each species present in each period\r\nMR = MR %>% \r\n  group_by(Species, TP) %>%\r\n  dplyr::mutate(PresN=length(unique(unique1010))) %>% \r\n  ungroup()\r\n\r\n#make new variable that tells us in how many 10*10km grids do we have presence (across all species) in each period\r\nMR = MR %>% \r\n  group_by(TP) %>%\r\n  dplyr::mutate(PresNTot=length(unique(unique1010))) %>% #in how many 10*10km grids do we have presence (across all species) in each period\r\n  ungroup()\r\n\r\n\r\n#############\r\n#Calculate Time-period-specific prevalence for each species\r\n#############\r\n\r\n#prevalence, i.e. number of presences of a species in 10*10km grid as proportion of all occupied 10*10km grids\r\nPrevMR = MR %>% \r\n  group_by(Species, TP) %>%\r\n  dplyr::mutate(Prev=PresN/PresNTot) %>% \r\n  ungroup()\r\n\r\n#Transform data frame to wide format\r\nmPrevMR=melt(PrevMR, id.vars=c(""TP"", ""Species""), measure.vars= c(""Prev""))\r\nwPrevMR=dcast(mPrevMR, Species ~ variable + TP)\r\n\r\n#############\r\n#Calculate Time-period-specific mean of 10 most northern latitudes for each species\r\n#############\r\nShiftMR = MR%>%\r\n  group_by(Species, TP)%>%\r\n  dplyr::arrange(desc(Y10))%>%\r\n  dplyr::summarise(meanTop10 = mean(Y10[1:10]))\r\n\r\n#Transform data frame to wide format\r\nmShiftMR=reshape2::melt(ShiftMR, id.vars=c(""TP"", ""Species""), measure.vars= c(""meanTop10""))\r\nwShiftMR=dcast(mShiftMR, Species ~ variable + TP)\r\n\r\n############\r\n#Calculate shift in Northern range boundaries (NRB)\r\n############\r\nwShiftMR=wShiftMR%>%\r\n  group_by(Species)%>%\r\n  dplyr::mutate(NRBshift=10*(meanTop10_3-meanTop10_1))\r\n\r\n\r\n############\r\n#Calculate shift in Prevalence\r\n############\r\nwPrevMR=wPrevMR%>%\r\n  group_by(Species)%>%\r\n  dplyr::mutate(PrevChange=(Prev_3-Prev_1)) #multiply by 10 to get shift in kilometers\r\n\r\n\r\n#Combine both data frames\r\ncMR=left_join(wShiftMR, wPrevMR, by=""Species"")\r\nhead(cMR)\r\n\r\n\r\n#save output as .csv\r\n# write.csv(cMR, file=""Results_NRBshift_PrevChange_Moths.csv"")\r\n\r\n', '# Butterfly_population_trends_FIBMS.R\r\n#\r\n# Maria Hllfors, Juha Pyry, Janne Helil, et al. Combining range and phenology shifts offers a winning strategy\r\n# for boreal Lepidoptera. Ecology Letters, accepted April 14th 2021. DOI:10.1111/ele.13774\r\n#\r\n# Step 1. load and organise Butterfly Monitoring data\r\n# Step 2. Compute site abundance indices with imputed missing counts from species phenology\r\n# Step 3. Calculate a time-series of collated abundance indices to estimate the linear trend.\r\n#\r\n#=============================================\r\n# sessionInfo()\r\n# R version 4.0.3 (2020-10-10)\r\n# Platform: x86_64-w64-mingw32/x64 (64-bit)\r\n# Running under: Windows 10 x64 (build 14393)\r\n# Matrix products: default\r\n# locale:\r\n# [1] LC_COLLATE=English_United Kingdom.1252\r\n# [2] LC_CTYPE=English_United Kingdom.1252\r\n# [3] LC_MONETARY=English_United Kingdom.1252\r\n# [4] LC_NUMERIC=C\r\n# [5] LC_TIME=English_United Kingdom.1252\r\n# attached base packages:\r\n# [1] stats     graphics  grDevices utils     datasets  methods   base\r\n# other attached packages:\r\n# [1] rbms_1.1.0        data.table_1.14.0\r\n#=============================================\r\n\r\nif (!requireNamespace(""data.table"")) install.packages(""data.table"")\r\nif (!requireNamespace(""devtools"")) install.packages(""devtools"")\r\ndevtools::install_github(""RetoSchmucki/rbms"")\r\nlibrary(data.table)\r\nlibrary(rbms)\r\nsessionInfo()\r\n\r\nif (!dir.exists(""output"")) dir.create(""output"")\r\n\r\n# STEP.1\r\n# ======\r\n# load data from the Finish Butterfly Monitoring Scheme and the annual phenology of\r\n# butterfly species from the G region informed from the Finish and Swedish BMS data\r\n# (see eBMS: https://butterfly-monitoring.net/ebms-data-summary)\r\n\r\nfi_visit <- fread(""Data_FIBMS_visit.csv"")\r\nfi_count <- fread(""Data_FIBMS_count.csv"")\r\nfi_transect <- fread(""Data_FIBMS_transect.csv"")[!is.na(TRANS_LON), ]\r\nfi_pheno <- fread(""Data_FIBMS_pheno.csv"")\r\n\r\n# Define start and end year from the monitoring data\r\nstart.year <- fi_visit[, range(YEAR)][1]\r\nend.year <- fi_visit[, range(YEAR)][2]\r\n\r\n# Build a time series for the period covered by the BMS data\r\n# (see rbms details: https://retoschmucki.github.io/rbms/)\r\n# see help(""ts_dwmy_table"")\r\nts_date <- rbms::ts_dwmy_table(InitYear = start.year, LastYear = end.year, WeekDay1 = ""monday"")\r\n\r\n# Define the monitoring season for the BMS data and add ""zeros"" to anchor\r\n# the flight curve at the beginning and the end of the monitoring season\r\n# (see rbms details: https://retoschmucki.github.io/rbms/)\r\n# or help(""ts_monit_season"")\r\nts_season <- rbms::ts_monit_season(ts_date,\r\n    StartMonth = 4,\r\n    EndMonth = 9,\r\n    StartDay = 1,\r\n    EndDay = NULL,\r\n    CompltSeason = TRUE,\r\n    Anchor = TRUE,\r\n    AnchorLength = 3,\r\n    AnchorLag = 2,\r\n    TimeUnit = ""w""\r\n)\r\n\r\n# Identify site visit along the monitoring season time-series\r\n# (see rbms details: https://retoschmucki.github.io/rbms/)\r\n# or help(""ts_monit_site"")\r\nts_season_visit <- ts_monit_site(fi_visit, ts_season)\r\nsp.list <- fi_count[order(SPECIES), unique(SPECIES)]\r\n\r\n# STEP.2\r\n# ======\r\n# Add observed weekly butterfly counts to the time-series and impute estimated count value for\r\n# weeks with missing count. Estimate are derived from the phenology curve observed in the climate region\r\n# >NOTE: the flight curve was derived from count data observed in Sweden and Finland, borrowing strength\r\n# from adjacent BMS available in the eBMS database. Similar flight curves can be computed from the Finish\r\n# data but with less precision. (see help(""flight_curve"")).\r\n# Using the observed and imputed weekly counts, compute site indices for each year and species having\r\n# sufficient data. (see rbms details: https://retoschmucki.github.io/rbms/)\r\n# or help(""ts_monit_count_site"")\r\n# or help(""impute_count"")\r\n\r\nfor (sp.i in seq_along(sp.list)) {\r\n    ts_season_count <- ts_monit_count_site(ts_season_visit, fi_count, sp = sp.list[sp.i])\r\n    pheno <- fi_pheno[SPECIES == sp.list[sp.i], ]\r\n    if (nrow(pheno) == 0) next()\r\n    cat(paste(sp.list[sp.i], ""- ""))\r\n    imp <- impute_count(\r\n        ts_season_count = ts_season_count,\r\n        ts_flight_curve = pheno,\r\n        YearLimit = NULL,\r\n        TimeUnit = ""w""\r\n    )\r\n    sindex <- unique(imp[!is.na(SINDEX) & TOTAL_NM > 0.05, .(SPECIES, SITE_ID, M_YEAR, SINDEX, TOTAL_NM)])\r\n    sindex <- merge(sindex, fi_transect[, .(SITE_ID, EST_TRANSECT_LENGTH, TRANS_LON, TRANS_LAT)], by = ""SITE_ID"") # , all.x = TRUE)\r\n    sindex[, SINDEX_1KM := SINDEX * (EST_TRANSECT_LENGTH / 1000)][, SINDEX_1ha := round(2 * SINDEX_1KM)]\r\n    saveRDS(sindex[order(as.numeric(substr(SITE_ID, 7, nchar(SITE_ID))), M_YEAR), ], file = paste0(""output/si_"", gsub("" "", ""_"", sp.list[sp.i]), "".rds""))\r\n}\r\n\r\n# STEP.3\r\n# ======\r\n# Generate a collated abundance index from the individual site indices. The collated indices represent the\r\n# annual relative abundance for the entire region monitored (Finland), here we used an index scaled to one\r\n# hectare, using the length of the transect to standardize to 1km and multiply by two to cover 10000 m2\r\n# (monitoring is conducted within a 5 m wide area along that the transect).\r\n# see rbms details: https://retoschmucki.github.io/rbms/\r\n# or help(""collated_index"")\r\n# From the collated index, we excluded extreme indices (<0.0001 & >100,000), and rescale each annual index on \r\n# a log(10) scale. The log collate index is centred to the mean of two (log(100) base 10) and the linear \r\n# temporal trend calculated with a simple linear model, function lm(), to extract the slope coefficient and \r\n# the associated p-value.\r\n\r\ncollated_index_res <- list()\r\nfor (sp.i in seq_along(sp.list)) {\r\n    sindex <- try(readRDS(paste0(""output/si_"", gsub("" "", ""_"", sp.list[sp.i]), "".rds"")), silent = TRUE)\r\n    if (any(class(sindex) == ""try-error"", nrow(sindex) == 0)) next()\r\n    co_index <- collated_index(\r\n        data = sindex,\r\n        s_sp = sp.list[sp.i],\r\n        sindex_value = ""SINDEX_1ha"",\r\n        glm_weights = TRUE,\r\n        rm_zero = TRUE\r\n    )\r\n    co_index_logInd <- co_index$col_index[BOOTi == 0 & COL_INDEX > 0.0001 & COL_INDEX < 100000, .(M_YEAR, COL_INDEX)][\r\n                                            , log(COL_INDEX + 0.00001) / log(10), by = M_YEAR][, mean_logInd := mean(V1)]\r\n    setnames(co_index_logInd, ""V1"", ""logInd"")\r\n    co_index <- merge(co_index$col_index, co_index_logInd, by = ""M_YEAR"", all.x = TRUE)\r\n    setkey(co_index, BOOTi, M_YEAR)\r\n    co_index[, boot_logInd := log(COL_INDEX + 0.001) / log(10)]\r\n    b2 <- data.table(M_YEAR = co_index[BOOTi == 0, M_YEAR], LCI = 2 + co_index[BOOTi == 0, logInd] - co_index[BOOTi == 0, mean_logInd])\r\n    lm_mod <- try(lm(LCI ~ M_YEAR, data = b2), silent = TRUE)\r\n    trend_est <- coef(summary(lm_mod))[, ""Estimate""][""M_YEAR""]\r\n    pvalue <- coef(summary(lm_mod))[, ""Pr(>|t|)""][""M_YEAR""]\r\n    collated_index_res[[sp.i]] <- data.table(BMS_ID = ""FIBMS"", SPECIES = sp.list[sp.i], YEAR = b2$M_YEAR, LogCollateIndex = b2$LCI,\r\n                                            OBS_TREND = round(trend_est, 3), Pvalue = round(pvalue, 3))\r\n}\r\ncollated_index_res <- rbindlist(collated_index_res)\r\n\r\n# # Save annual Log Collated indices derived from the Finish BMS 1999-2017 and the linear trend over the same period.\r\n# fwrite(collated_index_res[, .(BMS_ID, SPECIES, YEAR, LogCollateIndex)], paste0(""FI_butterfly_log_collated_indices.csv""))\r\n# fwrite(unique(collated_index_res[, .(SPECIES, OBS_TREND, Pvalue)])[order(OBS_TREND), ], paste0(""FI_butterfly_linear_log_trend.csv""))', '### Estimation of population trends for moth species using the TRIM method (rtrim package)\r\n#\r\n# Maria Hllfors, Juha Pyry, Janne Helil, et al. Combining range and phenology shifts offers a winning strategy\r\n# for boreal Lepidoptera. Ecology Letters, accepted April 14th 2021. DOI:10.1111/ele.13774\r\n#\r\n### Script produces moth trend estimates\r\n### Author: Ilmari Kohonen\r\n### ilmari.kohonen@helsinki.fi\r\n\r\n# set working directory\r\nsetwd("""")\r\n\r\n# load (and if required install) the rtrim package\r\nif(!require(""rtrim"")) install.packages(""rtrim""); library(rtrim)\r\n\r\n# read in data\r\n\r\ndat <- read.table(file = ""Data_Trends_Moths.txt"", header = T, stringsAsFactors = F, colClasses = c(""factor"", ""integer"", ""integer"", ""factor""), sep = "";"")\r\n\r\n# create the table where species-specific trim results are saved\r\n\r\nresults <- as.data.frame(matrix(NA, nrow = length(unique(dat$species)), ncol = 12))\r\ncolnames(results) <- c(""species"", ""from"", ""upto"", ""add"", ""se_add"", ""mul"", \r\n                       ""se_mul"", ""p"", ""meaning"", ""model_type"", ""serialcor"", ""count"")\r\n\r\nresults$species <- sort(unique(dat$species))\r\n\r\n# plots of annual indices per species are saved in a pdf file \r\n\r\npdf(file = ""trim figures.pdf"")\r\npar(mfrow=c(1,1))\r\n\r\n### the following large chunk of code implements the trim analysis for each species.\r\n# before actually fitting the trim model, some pre-processing of data is done to ensure that TRIM works.\r\n# (more details in the code below)\r\n\r\nfor(i in results[,1]) {\r\n  species_i <- subset(dat, species == i) # species-specific subset of the data\r\n  \r\n  # Limit time frame of data between the first and last year of observation for the species\r\n  # (this is something the trim() function is sensitive to)\r\n  \r\n  count_per_year <- aggregate(species_i[,3], by=list(species_i$year), function(x){sum(x, na.rm = T)})\r\n  \r\n  first_year <- min(count_per_year[which(count_per_year[,2]>0,),][,1]) # first year when the examined species is observed in the data set\r\n  final_year <- max(count_per_year[which(count_per_year[,2]>0,),][,1]) # last year of observation\r\n  species_i <- species_i[species_i$year>=first_year & species_i$year<=final_year,] # limit the time frame of the data\r\n  \r\n  ## Next the actual model fitting\r\n  # The code starts from the most complex TRIM model (""model 3"") \r\n  # If fitting the model fails, the next most complex model is tried\r\n  # If the fitting fails again, the model is further simplified etc. (see comments in code below for more)\r\n  # If none of the tried models worked, the row is set to NA\r\n  \r\n  # (Note: for the species included in Hllfors et al. 2021, all species were fitted with \r\n  # the most complex model, with the exception of Protodeltote pygarga)\r\n  \r\n  ## key functions used repeatedly in the code below:\r\n  # trim() fits the trim model\r\n  # overall() outputs information about the overall long term trend slope\r\n  \r\n  fail <- F # create a flag of failure\r\n  trim_i <- NA # create another object\r\n  \r\n  tryCatch(suppressWarnings(\r\n    # model 3 (""time effects"")\r\n    expr = {\r\n      trim_i <- trim(count ~ site + year, data=species_i, model=3, serialcor=T, overdisp=TRUE, autodelete = T)\r\n    }),\r\n    error = function(x) {fail <<- TRUE}\r\n  )\r\n  if(fail == F) {\r\n    trimOverall_i <- overall(trim_i)\r\n    results[results$species==i,2:9] <- trimOverall_i$slope\r\n    results[results$species==i,4:8] <- as.numeric(format(round(results[results$species==i,4:8], 3), nsmall = 3))\r\n    results[results$species==i, 10:12] <- c(3, ""TRUE"", sum(count_per_year[,2]))\r\n    plot(index(trim_i, level = 0.95), xlab = paste0(i, "" ("", results[results$species==i,4], ""  "", results[results$species==i,5], "", p = "", results[results$species==i,8], "", n = "", sum(count_per_year[,2]), "")""))\r\n  } else {\r\n    fail = F\r\n    tryCatch(suppressWarnings(\r\n      # model 2 (""linear trend""), serial correlation T\r\n      expr = {\r\n        trim_i <- trim(count ~ site + year, data=species_i, model=2, changepoints = ""all"", stepwise = T, serialcor=T, overdisp=TRUE, autodelete = T)\r\n      }), \r\n      error = function(x) {fail <<- TRUE}\r\n    ) \r\n    if(fail == F) {\r\n      trimOverall_i <- overall(trim_i)\r\n      results[results$species==i,2:9] <- trimOverall_i$slope\r\n      results[results$species==i,4:8] <- as.numeric(format(round(results[results$species==i,4:8], 3), nsmall = 3))\r\n      results[results$species==i, 10:12] <- c(2, ""TRUE"", sum(count_per_year[,2]))\r\n      plot(index(trim_i, level = 0.95), xlab = paste0(i, "" ("", results[results$species==i,4], ""  "", results[results$species==i,5], "", p = "", results[results$species==i,8], "", n = "", sum(count_per_year[,2]), "")""))\r\n    } else {\r\n      fail <- F\r\n      tryCatch(suppressWarnings(\r\n        # model 2, serial correlation F\r\n        expr = {\r\n          trim_i <- trim(count ~ site + year, data=species_i, model=2, changepoints = ""all"", stepwise = T, serialcor=F, overdisp=TRUE, autodelete = T)\r\n        }), \r\n        error = function(x) {fail <<- TRUE}\r\n      )\r\n      if(fail == F) {\r\n        trimOverall_i <- overall(trim_i)\r\n        results[results$species==i,2:9] <- trimOverall_i$slope\r\n        results[results$species==i,4:8] <- as.numeric(format(round(results[results$species==i,4:8], 3), nsmall = 3))\r\n        results[results$species==i, 10:12] <- c(2, ""FALSE"", sum(count_per_year[,2]))\r\n        plot(index(trim_i, level = 0.95), xlab = paste0(i, "" ("", results[results$species==i,4], ""  "", results[results$species==i,5], "", p = "", results[results$species==i,8], "", n = "", sum(count_per_year[,2]), "")""))\r\n      } else {\r\n        tryCatch(suppressWarnings(\r\n          # model 2, serial correlation F, no changepoints\r\n          expr = {\r\n            trim_i <- trim(count ~ site + year, data=species_i, model=2, serialcor=F, overdisp=TRUE, autodelete = T)\r\n            trimOverall_i <- overall(trim_i)\r\n            results[results$species==i,2:9] <- trimOverall_i$slope\r\n            results[results$species==i,4:8] <- as.numeric(format(round(results[results$species==i,4:8], 3), nsmall = 3))\r\n            results[results$species==i, 10:12] <- c(2, ""FALSE & no changepoints"", sum(count_per_year[,2]))\r\n            plot(index(trim_i, level = 0.95), xlab = paste0(i, "" ("", results[results$species==i,4], ""  "", results[results$species==i,5], "", p = "", results[results$species==i,8], "", n = "", sum(count_per_year[,2]), "")""))\r\n          }),\r\n          error = function(x){\r\n            results[results$species==i,2:12] <- NA # if none of the models above worked for the species, set NA\r\n          }\r\n        )\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\ndev.off()\r\n\r\n## write the results into a file\r\n# ""add"" is the ""additive slope"" of TRIM, ""se_add"" its standard error, ""p"" its p-value\r\nresults_table <- results[,c(""species"", ""add"", ""se_add"", ""p"")]\r\n\r\n# write.table(results_table, file = ""results_moth_trends_rtrim_20210426.csv"", sep = "";"", row.names = F)\r\n\r\n', '#Script for producing figures 2-4 in\r\n#\r\n# Maria Hllfors, Juha Pyry, Janne Helil, et al. Combining range and phenology shifts offers a winning strategy\r\n# for boreal Lepidoptera. Ecology Letters, accepted April 14th 2021. DOI:10.1111/ele.13774\r\n#\r\n\r\n##############\r\n# sessionInfo()\r\n# R version 4.0.5 (2021-03-31)\r\n# Platform: x86_64-w64-mingw32/x64 (64-bit)\r\n# Running under: Windows 10 x64 (build 18363)\r\n##############\r\n\r\n\r\n#####\r\n#Install and load required packages\r\n#####\r\n\r\n# Package names\r\npackages <- c(""dplyr"", ""reshape2"", ""tidyr"", ""sf"", ""data.table"", ""raster"", ""terra"", ""ggplot2"", ""cowplot"", ""ape"", ""plyr"")\r\n\r\n# Install packages not yet installed\r\ninstalled_packages <- packages %in% rownames(installed.packages())\r\nif (any(installed_packages == FALSE)) {\r\n  install.packages(packages[!installed_packages])\r\n}\r\n\r\n#load libraries\r\nlibrary(dplyr)\r\nlibrary(reshape2)\r\nlibrary(tidyr)\r\nlibrary(sf)\r\nlibrary(data.table)\r\nlibrary(raster)\r\nlibrary(terra) ## only needed for faster elevation extraction\r\nlibrary(ggplot2)\r\nlibrary(cowplot)\r\nlibrary(ape)\r\nlibrary(plyr)\r\n\r\n\r\n#########\r\n#Set wd \r\n#########\r\n\r\n# setwd(""MY/PATH/"")\r\n\r\n\r\n###############\r\n###############\r\n#Figure 2\r\n###############\r\n###############\r\n\r\n## ===\r\n\r\n## Prepared raster tiles for plotting in ggplot framework from DEM data file\r\n## Requires original DEM data. EU-DEM v1.1, are available \r\n## at http://land.copernicus.eu/pan-european/satellite-derived-products/eu-dem/eu-dem-v1.1/view\r\n## ===== PRE-PROCESSING =====\r\n# masked_fin_dem <- raster(""masked_fin_dem_50m.tif"")\r\n# x <- masked_fin_dem\r\n# x <- sampleRegular(x, 100000, asRaster=TRUE)\r\n# coords <- xyFromCell(x, seq_len(ncell(x)))\r\n# dat_elev <- stack(as.data.frame(getValues(x)))\r\n# names(dat_elev) <- c(\'elev_50m\', \'variable\')\r\n# dat_elev <- cbind(coords, dat_elev)\r\n## ===== END PRE-PROCESSING =====\r\n\r\n## FIGURE 2\r\n##======================================\r\n## load preprocessed site location data\r\nm_range_sub <- fread(""Data_Distribution_Subsampled_Moths.csv"")\r\nb_range_sub <- fread(""Data_Distribution_Subsampled_Butterflies.csv"")\r\nb_pheno <- fread(""Data_Butterfly_Phenology_unique_sites.csv"")\r\nm_pheno <- fread(""Data_Moth_Phenology_unique_sites.csv"")\r\ndat_elev <- fread(""Data_FIN_dem_50m_resampled.csv"")\r\n\r\nsetnames(b_pheno, ""SiteSector"", ""SiteID"")\r\nb_pheno$taxa <- ""butterfly""\r\nm_pheno$taxa <- ""moth""\r\nbm_pheno <- rbind(b_pheno, m_pheno)\r\nbm_range_sub <- rbind(b_range_sub, m_range_sub[, keep := NULL])\r\n\r\n#make new variable that defines 10*10km grids\r\nbm_range_sub$Unique1010=paste(bm_range_sub$Y10, bm_range_sub$X10)\r\n\r\n## calculate the number of observation per grid cell (Unique1010) for each period\r\nobs_grid <- bm_range_sub[ , Obs := .N, by = .(Unique1010, TP)][!duplicated(paste0(Unique1010,""_"",TP)), ]\r\n\r\nobs_grid[, ObsC := "">100""]\r\nobs_grid[Obs<=100, ObsC :=""51-100""]\r\nobs_grid[Obs<=50, ObsC :=""11-50""]\r\nobs_grid[Obs<=10, ObsC :=""1-10""]\r\nobs_grid$ObsC = factor(obs_grid$ObsC, levels=c(""1-10"", ""11-50"", ""51-100"", "">100""))\r\n\r\n# base map\r\nfin <- sf::st_as_sf(raster::getData(""GADM"", country = ""FIN"", level = 0))\r\nala <- sf::st_as_sf(raster::getData(""GADM"", country = ""ALA"", level = 0))\r\nfin_ala <- rbind(fin, ala)\r\nfin_3035 <- sf::st_transform(fin_ala, 3035)\r\n\r\n# spatial object for range and pheno data\r\nNRB_T1_sf <- st_transform(st_as_sf(obs_grid[!is.na(XEUREF) & TP == 1, ], coords = c(""XEUREF"", ""YEUREF""), crs = 3067), 3035)\r\nNRB_T2_sf <- st_transform(st_as_sf(obs_grid[!is.na(XEUREF) & TP == 3, ], coords = c(""XEUREF"", ""YEUREF""), crs = 3067), 3035)\r\nPHENO_sf <- st_transform(st_as_sf(bm_pheno, coords = c(""XEUREF"", ""YEUREF""), crs = 3067), 3035)\r\n\r\np <- ggplot(data = dat_elev, aes(x=x, y=y)) + \r\n  geom_tile(aes(fill = elev_50m)) + \r\n  scale_fill_gradientn(colours = rev(terrain.colors(n = 100)), na.value=""transparent"") +\r\n  geom_sf(data = fin_3035, inherit.aes = FALSE, fill = NA, colour = ""grey75"") +\r\n  coord_sf(xlim = as.numeric(st_bbox(fin_3035)[c(1, 3)]) + c(1, -1),\r\n           ylim = as.numeric(st_bbox(fin_3035)[c(2, 4)]) + c(2, -2)) +\r\n  labs(x = ""Longitude"",  y = ""Latitude"", fill = ""Elevation (m)"") +\r\n  theme(panel.grid.major = element_line(color = ""grey75"", linetype = 1),\r\n        panel.background = element_rect(fill = ""transparent"", colour = ""grey10""),\r\n        axis.text = element_text(size = 10),\r\n        axis.title = element_text(size = 12),\r\n        plot.margin = unit(c(0,0,0,0), ""cm""),\r\n        legend.position = ""none"")\r\n\r\ndat <- PHENO_sf\r\nobj_size = 2\r\nf2a <- p + \r\n  geom_sf(data=dat[dat$taxa == ""butterfly"", ], \r\n          inherit.aes = FALSE, col=""#006594"", shape=17, size=obj_size) +\r\n  geom_sf(data=dat[dat$taxa == ""moth"", ], \r\n          inherit.aes = FALSE, col=""#2d3143"", shape=19, size=obj_size)\r\n\r\n\r\ndat <- NRB_T1_sf\r\npt_size = 1.2\r\nf2b = p + \r\n  geom_sf(data=dat[dat$ObsC==""1-10"",], inherit.aes = FALSE, col=""#6699cc"", shape=16, size=pt_size) +\r\n  geom_sf(data=dat[dat$ObsC==""11-50"",], inherit.aes = FALSE, col=""#006594"", shape=16, size=pt_size) +\r\n  geom_sf(data=dat[dat$ObsC==""51-100"",], inherit.aes = FALSE, col=""#4e5d73"", shape=16, size=pt_size) +\r\n  geom_sf(data=dat[dat$ObsC=="">100"",], inherit.aes = FALSE, col=""#2d3143"", shape=16, size=pt_size) +\r\n  ylab("""")\r\n\r\ndat <- NRB_T2_sf\r\nf2c = p + \r\n  geom_sf(data=dat[dat$ObsC==""1-10"",], inherit.aes = FALSE, col=""#6699cc"", shape=16, size=pt_size) +\r\n  geom_sf(data=dat[dat$ObsC==""11-50"",], inherit.aes = FALSE, col=""#006594"", shape=16, size=pt_size) +\r\n  geom_sf(data=dat[dat$ObsC==""51-100"",], inherit.aes = FALSE, col=""#4e5d73"", shape=16, size=pt_size) +\r\n  geom_sf(data=dat[dat$ObsC=="">100"",], inherit.aes = FALSE, col=""#2d3143"", shape=16, size=pt_size) +\r\n  ylab("""")\r\n\r\n# compose and add legend to figure save images\r\n\r\nf2c_ <- f2c + guides(fill = guide_colorbar(title = ""Elevation (m)"",\r\n                                           title.position = ""top"", title.hjust = 0,\r\n                                           title.theme = element_text(size=8),\r\n                                           label.theme = element_text(size=7),\r\n                                           barwidth = 0.7,\r\n                                           barheight = 5)) +\r\n  theme(legend.position = c(0.86,0.87))\r\n\r\nfigure2_c <- plot_grid(f2a, f2b, f2c_, ncol = 3, align = ""h"")\r\n\r\n\r\n# save_plot(""figure2_elevation.png"", figure2_c,\r\n#           ncol = 3,\r\n#           base_height = 6,\r\n#           base_asp = 0.6,\r\n#           dpi = 600)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n###############\r\n###############\r\n#Figures 3 & 4\r\n###############\r\n###############\r\n\r\n#read in data\r\nTree=read.nexus(""Lepidoptera_tree.nex"") #phylogeny\r\ndata=read.csv(""Data_Results.csv"", stringsAsFactors = TRUE, sep="","")\r\nhead(data)\r\n\r\n\r\n\r\n###############\r\n#Figure 3\r\n###############\r\n\r\n#plot phylogeny\r\nplot.phylo(Tree, type=""phylogram"", use.edge.length = FALSE, show.tip.label=FALSE)\r\n\r\n#pprepare heat map\r\nd2 <- data.frame(id=Tree$tip.label, value = data$NRBDir)\r\nd3 <- data.frame(id=Tree$tip.label, value = data$PhenDir)\r\nd4 <- data.frame(id=Tree$tip.label, value = data$PopDir)\r\nd5 <- data.frame(id=Tree$tip.label, value = data$Response)\r\nd2$value=factor(d2$value, levels=c(""EXP"", ""STAB"",""CONTR""))\r\nd3$value=factor(d3$value, levels=c(""ADV"", ""STAB"",""DEL""))\r\nd4$value=factor(d4$value, levels=c(""POS"", ""STAB"",""NEG""))\r\nd5$value=factor(d5$value, levels=c(""ADVEXP"", ""ADVCONTR"", ""DELEXP"",""DELCONTR""))\r\nd2$id <- as.character(d2$id)\r\nd2$id <- factor(d2$id, levels=unique(d2$id))\r\nd3$id <- as.character(d3$id)\r\nd3$id <- factor(d3$id, levels=unique(d3$id))\r\nd4$id <- as.character(d4$id)\r\nd4$id <- factor(d4$id, levels=unique(d4$id))\r\nd5$id <- as.character(d5$id)\r\nd5$id <- factor(d5$id, levels=unique(d5$id))\r\npp2=ggplot(data = d2, mapping = aes( y = id, x=0, fill = value)) +\r\n  geom_tile()+ theme_void() + theme(legend.position = ""none"") + \r\n  scale_fill_manual(values = c(""#a86672"", ""#b5b280"", ""black""))\r\npp3=ggplot(data = d3, mapping = aes( y = id, x=0, fill = value)) +\r\n  geom_tile()+ theme_void() + theme(legend.position = ""none"") + \r\n  scale_fill_manual(values = c(""#a86672"", ""#b5b280"", ""black""))\r\npp4=ggplot(data = d4, mapping = aes( y = id, x=0, fill = value)) +\r\n  geom_tile()+ theme_void() + theme(legend.position = ""none"") + \r\n  scale_fill_manual(values = c(""#a86672"", ""#b5b280"", ""black""))\r\npp5=ggplot(data = d5, mapping = aes( y = id, x=0, fill = value)) +\r\n  geom_tile()+ theme_void() + theme(legend.position = ""none"") + \r\n  scale_fill_manual(values = c(""#9cb6c2"", ""#3b6475"", ""#3b6475"", ""grey24""))\r\n\r\n#plot heat map\r\nplot_grid(pp2, pp3, pp4, pp5, ncol = 4)\r\n\r\n\r\n\r\n#calculate proportion of species responding in different directions\r\n#percentages of species shifting their NRB in different directions\r\nt=table(data$NRBDir)\r\nprop.table(t)%>%\r\n  `*`(100)%>%\r\n  round(1)\r\n#percentages of species shifting their phenology in different directions\r\nt=table(data$PhenDir)\r\nprop.table(t)%>%\r\n  `*`(100)%>%\r\n  round(1)\r\n#percentages of species with popualtion trends in different directions\r\nt=table(data$PopDir)\r\nprop.table(t)%>%\r\n  `*`(100)%>%\r\n  round(1)\r\n#percentages of species with different combined response\r\nt=table(data$Response)\r\nprop.table(t)%>%\r\n  `*`(100)%>%\r\n  round(1)\r\n\r\n\r\n#the phylogeny, heatmap, and percentages where put together in powerpoint\r\n#to produce figure 3 in Hllfors et al. 2021\r\n\r\n\r\n\r\n\r\n###############\r\n#Figure 4\r\n###############\r\n\r\ndata$poptrendc=scale(data$Poptrend, center = FALSE)\r\n#calculate means and se for the groups\r\nse <- function(x, ...) sqrt(var(x, ...)/length(x))\r\n\r\ndata_summary <- function(data, varname, groupnames){\r\n  require(plyr)\r\n  summary_func <- function(x, col){\r\n    c(mean = mean(x[[col]], na.rm=TRUE),\r\n      se = se(x[[col]], na.rm=TRUE))\r\n  }\r\n  data_sum<-plyr::ddply(data, groupnames, .fun=summary_func,\r\n                        varname)\r\n  data_sum <- plyr::rename(data_sum, c(""mean"" = varname))\r\n  return(data_sum)\r\n}\r\n\r\n\r\ndf3 <- data_summary(data=data, varname=""poptrendc"", \r\n                    groupnames=c(""NRBDir"", ""PhenDir""))\r\ndf4 <- data_summary(data, varname=""poptrendc"", \r\n                    groupnames=(""Hypothesis""))\r\n\r\n#place factors levels in correct order\r\ndf3$NRBDir=factor(df3$NRBDir, levels=c(""CONTR"", ""STAB"", ""EXP""))\r\ndf3$PhenDir=factor(df3$PhenDir, levels=c(""ADV"", ""STAB"", ""DEL""))\r\ndf4$Hypothesis=factor(df4$Hypothesis, levels=c(""zero"", ""one"", ""two""))\r\n\r\n#Plot 4a\r\nggplot(df3, aes(x=PhenDir, y=poptrendc, fill=PhenDir)) + \r\n  geom_bar(stat=""identity"", position=position_dodge()) +\r\n  scale_fill_manual(values = c(""#a86672"", ""#b5b280"", ""black""))+\r\n  facet_wrap(~NRBDir)+\r\n  geom_hline(yintercept=0)+\r\n  geom_errorbar(aes(ymin=poptrendc-se, ymax=poptrendc+se), col=""#464646"", width=.2,\r\n                position=position_dodge(.9))+\r\n  theme_classic()\r\n\r\n#Plot 4b\r\nggplot(df4, aes(x=Hypothesis, y=poptrendc, fill=Hypothesis)) + \r\n  geom_bar(stat=""identity"", position=position_dodge()) +\r\n  scale_fill_manual(values = c(""grey24"", ""#3b6475"",""#9cb6c2""))+\r\n  geom_hline(yintercept=0)+\r\n  geom_errorbar(aes(ymin=poptrendc-se, ymax=poptrendc+se), col=""#464646"", width=.2,\r\n                position=position_dodge(.9))+\r\n  theme_classic()\r\n\r\n#the  parts of figures where enhanced and put together in powerpoint\r\n#to produce figure 4 in Hllfors et al. 2021\r\n']","Data for: Combining range and phenology shifts offers a winning strategy for boreal Lepidoptera Species can adapt to climate change by adjusting in situ or by dispersing to new areas, and these strategies may complement or enhance each other. Here, we investigate temporal shifts in phenology and spatial shifts in northern range boundaries for 289 Lepidoptera species by using long-term data sampled over two decades. While 40% of the species neither advanced phenology nor moved northward, nearly half (45%) used one of the two strategies. The strongest positive population trends were observed for the minority of species (15%) that both advanced flight phenology and shifted their northern range boundaries northward. We show that, for boreal Lepidoptera, a combination of phenology and range shifts is the most viable strategy under a changing climate. Effectively, this may divide species into winners and losers based on their propensity to capitalize on this combination, with potentially large consequences on future community composition.",1
Socio-economic development drives solid waste management performance in cities: A global analysis using machine learning,"Here you can find the input and summary output datasets and the analysis protocol and R code associated with the publciation.The independent variables dataset analysed here refer to specific indicators of the WABI methodology (https://www.sciencedirect.com/science/article/pii/S0956053X14004905) that generates solid waste management and resource recovery profiles for cities. It was applied here for 40 cities around the world. Input file:Metadata info used by R codesFull data set for the WABI, used by the R codesData required for plotting the map in Figure 1Summary output file:Metadata info used by R codesSummary of results for two modelling approaches (machine learning: Conditional random-forest and non-linear regression)",,"Socio-economic development drives solid waste management performance in cities: A global analysis using machine learning Here you can find the input and summary output datasets and the analysis protocol and R code associated with the publciation.The independent variables dataset analysed here refer to specific indicators of the WABI methodology (https://www.sciencedirect.com/science/article/pii/S0956053X14004905) that generates solid waste management and resource recovery profiles for cities. It was applied here for 40 cities around the world. Input file:Metadata info used by R codesFull data set for the WABI, used by the R codesData required for plotting the map in Figure 1Summary output file:Metadata info used by R codesSummary of results for two modelling approaches (machine learning: Conditional random-forest and non-linear regression)",1
Robust SOM Clustering,"This R script performs a combined SOM/SuperSOM clustering of the 640 administrative districs of India. Fowlkes-Mallows Similarity Index is used to identify robust initializations of the clustering. Data_India.txt is a specially conceived geographic database of 55 indicators, covering issues of economic activity, urban structure, socio-demographic development, consumption levels, infrastructure endowment and basic geographical positioning within the Indian space. Data refer to 2011 or to 2001-2011 evolutions.Warnng: This is an old version of the project. Version 1.2 available at https://zenodo.org/record/2563213#.XGQl01xKiiz","['####################################################################################\r\n#                                                                                  #\r\n# An R script to perform combined SOM/superSOM data clustering                     #\r\n#                                                                                  #\r\n# AuthorS: Joan Perez                                                              #\r\n#          Univ. Avignon Pays du Vaucluse, CNRS, ESPACE, UMR7300                   #\r\n#          Case Pasteur, Avignon                                                   #\r\n#          E-mail: joan.perez.etu@gmail.com                                        # \r\n#                                                                                  #\r\n#          Giovanni Fusco                                                          #\r\n#          Univ. Cte d\'Azur, CNRS, ESPACE, UMR7300                                #\r\n#          98 Bd Herriot, BP 3209, 06204 Nice, France                              #\r\n#          E-mail: giovanni.fusco@unice.fr                                         #\r\n#                                                                                  #\r\n####################################################################################\r\n\r\n# This script performs a combined SOM/SuperSOM clustering of the 640 administrative districts of India.\r\n# Fowlkes-Mallows Similarity Index is used to identify robust initializations of the clustering.\r\n# Data_India.txt is a specially conceived geographic database of 55 indicators, covering issues of economic activity,\r\n# urban structure, socio-demographic development, consumption levels, infrastructure endowment and \r\n# basic geographical positioning within the Indian space. Data refer to 2011 or to 2001-2011 evolutions.\r\n\r\n# References:\r\n# [1] Wehrens R., Buydens L., 2007, Self- and Super-organizing Maps in R: The ko-honen Package,\r\n#     Journal of Statistical Software, Vol. 21, Issue 5.\r\n# [2] Fusco G., Perez J., 2015, Spatial Analysis of the Indian Subcontinent: the Complexity Investigated \r\n#     through Neural Networks, CUPUM 2015 - 14th International Conference on Computers in Urban Planning \r\n#     and Urban Management, MIT, Cambridge (Ma.), July 5th-7th 2015, Proceedings, 287, 1-20,\r\n#     http://web.mit.edu/cron/project/CUPUM2015/proceedings/Content/analytics/287_fusco_h.pdf \r\n# [3] Perez J., 2015, Spatial Structures in India in the Age of Globalisation. A Data-Driven Approach, Phd in geography,\r\n#     University of Avignon (France)\r\n\r\n\r\n####################################################################################\r\n# 0.1 : Environment Preparation                                                    #\r\n####################################################################################\r\n\r\n# Packages loading (needs to be previously installed)\r\nlibrary(MASS)\r\nlibrary(class)\r\nlibrary(kohonen)\r\nsuppressPackageStartupMessages(library(dendextend))\r\n\r\n####################################################################################\r\n# 0.2. General Functions and Utilities                                             #\r\n####################################################################################\r\n\r\n## 0.2.1 Automated Number of Primes Selection / Initialization                       \r\n\r\nselect.primes <- function(nb)\r\n{\r\n  primes <- vector()\r\n  t <- 0\r\n  a <- 3\r\n  while (length(primes)<nb) {for (i in a) { for (j in 2:(i-1)) {if (i %% j == 0){t <- (t+1)} } \r\n                                            if (t==0){primes<- c(primes,i)}\r\n                                            t <- 0\r\n  }\r\n  a <- (a+1)\r\n  }\r\n  return(primes)\r\n}\r\n\r\n## 0.2.2 Calculation of the Best Similarity Score for each Seed\r\n\r\n# function returns two arguments, first one is the best seed in chr\r\n# second one is the average similarity score for each seed\r\nFM.Similarity <- function(results.list, clusters)\r\n{\r\nFM.matrix <- matrix(nrow=length(results.list), ncol=length(results.list))\r\nfor(i in 1:length(results.list)) {for(j in 1:length(results.list)) \r\n{FM.matrix[i,j]<-FM_index_R(clusters[,i+1],clusters[,j+1],assume_sorted_vectors = TRUE) }}\r\nrownames(FM.matrix) <- c(paste0(""seed"", primes))\r\ncolnames(FM.matrix) <- c(paste0(""seed"", primes))\r\naverage <- apply(FM.matrix, 1, mean)\r\nFM.df <- as.data.frame(cbind(FM.matrix, average))\r\nbest.seed <- row.names (FM.df[which.max(FM.df$average),])\r\nbest.seed <- paste0(""cluster."",best.seed)\r\noutput <- list(best.seed, average)\r\nreturn(output)\r\n}\r\n\r\n####################################################################################\r\n# 0.3 Prerequise on Df Origin                                                      #\r\n####################################################################################\r\n\r\n# Importation of the main database completed using bayesian inference, including NA\r\ndf.origin <- read.delim2(""Data_India.txt"")\r\n\r\n# Base 10 log transformation of Air_Flows\r\n# Replace -Inf by 0 within the dataframe\r\n# Replace NA by 0 within MACRO_AREA_COMPACITY\r\ndf.origin$AIR_FLOWS <- log10(df.origin$AIR_FLOWS)\r\ndf.origin[ df.origin == ""-Inf"" ] = 0\r\ndf.origin$MACRO_AREA_COMPACITY <- replace(df.origin$MACRO_AREA_COMPACITY, \r\n                                          is.na(df.origin$MACRO_AREA_COMPACITY), as.numeric(0.0))\r\n\r\n####################################################################################\r\n# 0.4 : Data modification according to the SOM standards                           #\r\n####################################################################################\r\n\r\n# Removing 31 lines with NA in order to perform the SOM\r\ndf.origin.no.NA <- df.origin[rowSums(is.na(df.origin))==0, ]\r\nrow.names(df.origin.no.NA) <- df.origin.no.NA[,2]\r\n\r\n# ID removing \r\n# the data-frame for analysis is now df.no.id\r\ndf.no.id <- df.origin.no.NA[,c(-1, -2)]\r\n\r\n# default method to center and scale the columns of a dataframe\r\n# the data-frame for analysis is now df.SOM.scale \r\ndf.SOM.scale <- scale(df.no.id)\r\n\r\n####################################################################################\r\n# 0.5 : Data modification according to the superSOM standards                      #\r\n####################################################################################\r\n\r\n# ID removing \r\n# A dataframe ready for supersom\r\ndf.supersom <- df.origin\r\nid <- df.supersom[2]\r\ndf.supersom.no.id <- df.supersom[,c(-1, -2)]\r\n\r\n# default method to centers and scales the columns of a dataframe\r\ndf.supersom.scale <- as.data.frame(scale(df.supersom.no.id))\r\nrm(df.supersom.no.id, df.supersom)\r\n\r\n####################################################################################\r\n# 0.6 Number of initializations selection                                          #\r\n####################################################################################\r\n\r\n# number of primes to be inserted in the vector primes\r\nprimes <- select.primes(20)\r\n\r\n####################################################################################\r\n#                                                                                  #\r\n#                                   MAIN 1                                         #\r\n#                               STANDARD SOM                                       #\r\n#                                                                                  #\r\n####################################################################################\r\n####################################################################################\r\n# 1.1 : SOM                                                                        #\r\n####################################################################################\r\n\r\n## 1.2.1 Self-organising maps of variables for mapping high-dimensional data to 2D grid using nb different seeds.\r\nresults.standard.SOM.list <- list()\r\nfor(i in primes)\r\n{\r\n  set.seed(i)\r\n  som.seed<- som(data = df.SOM.scale, grid = somgrid(4, 4, ""hexagonal""),\r\n                 rlen = 10000, toroidal = FALSE, radius = 3)\r\n  results.standard.SOM.list[[length(results.standard.SOM.list)+1]] <- som.seed\r\n  print(i)\r\n}\r\nrm(som.seed)\r\n\r\nnames(results.standard.SOM.list) <- c(paste0(""cluster.seed"", primes))\r\n\r\n## 1.2.2 Clustering results for each seed\r\n# Combine the results for each seed in one data.frame\r\n\r\nclusters.standard.SOM <- do.call(cbind, lapply(1:length(results.standard.SOM.list),\r\n                                               function(i) cbind(results.standard.SOM.list[[i]][[8]])))\r\nobservation.names <- row.names(df.origin.no.NA)\r\nclusters.standard.SOM <- as.data.frame(cbind(observation.names, clusters.standard.SOM))\r\nnames(clusters.standard.SOM) <- c(""id_observations"", (paste0(""cluster.seed"", primes)))\r\nrm(observation.names)\r\n\r\n####################################################################################\r\n# 1.2 : Pick Up the Best Seed                                                      #\r\n####################################################################################\r\n\r\nbest.seed.standard.SOM <- FM.Similarity(results.standard.SOM.list,clusters.standard.SOM)[[1]]\r\n\r\n# Extract the best standardSOM\r\nbest.standard.SOM <- results.standard.SOM.list[[best.seed.standard.SOM]]\r\n\r\n####################################################################################\r\n#                                                                                  #\r\n#                                   MAIN 2                                         #\r\n#                         STANDARD SOM on factors + superSOM                       #\r\n#                                                                                  #\r\n####################################################################################\r\n####################################################################################\r\n# 2.1 : Data modification for Performing SOM on Factors                            #\r\n####################################################################################\r\n\r\n# transposing of df.scale.no.id in order to perform SOM on variables\r\n# the data-frame for analysis is now df.scale.no.id.t\r\ndf.SOM.scale.t <- t(df.SOM.scale)\r\n\r\n####################################################################################\r\n# 2.2 : SOM on Variables                                                           #\r\n####################################################################################\r\n\r\n## 2.2.1 Self-organising maps of variables for mapping high-dimensional data to 2D grid using nb different seeds.\r\nresults.SOM.list <- list()\r\nfor(i in primes)\r\n{\r\n  set.seed(i)\r\n  som.seed<- som(data = df.SOM.scale.t, grid = somgrid(4, 4, ""hexagonal""),\r\n                 rlen = 10000, toroidal = FALSE, radius = 3)\r\n  results.SOM.list[[length(results.SOM.list)+1]] <- som.seed\r\n  print(i)\r\n}\r\nrm(som.seed)\r\nnames(results.SOM.list) = c(paste0(""som.seed"", primes))\r\n\r\n## 2.2.2 Clustering results for each seed\r\n# Combine the results for each seed in one data.frame\r\nclusters.SOM <- do.call(cbind, lapply(1:length(results.SOM.list), function(i) cbind(results.SOM.list[[i]][[8]])))\r\nvariable.names <- row.names(df.SOM.scale.t)\r\nclusters.SOM <- as.data.frame(cbind(variable.names, clusters.SOM ))\r\nnames(clusters.SOM) <- c(""id.factor"", (paste0(""cluster.seed"", primes)))   \r\n\r\n####################################################################################\r\n# 2.3 : Pick Up the Best Seed                                                      #\r\n####################################################################################\r\n\r\n## Fowlkes-Mallows Similarity Index Calculation\r\nbest.seed.SOM <- FM.Similarity(results.SOM.list,clusters.SOM)[[1]]\r\n\r\n####################################################################################\r\n# 2.4 : Assign Factors                                                             #\r\n####################################################################################\r\n\r\n# cluster results for the best seed as a dataframe with first column as row names\r\nrownames(clusters.SOM) <- clusters.SOM[,1]\r\nclusters.SOM <- clusters.SOM[,-1]\r\n\r\n# assign each row.names cluster result in a results.supersom.list of vectors\r\n# results.SOM.list[[1]][[2]][[2]] and results.SOM.list[[1]][[2]][[3]] are the dimensions of the SOM grid which was calculated, and hence the nb of factors\r\nFactor.lst.superSOM <- lapply(1:(results.SOM.list[[1]][[2]][[2]]*results.SOM.list[[1]][[2]][[3]]), \r\n                              function(i) row.names(subset(clusters.SOM , get(best.seed.SOM) ==i)))\r\n\r\n####################################################################################\r\n# 2.5 : SuperSOM Layers Preparation                                                #\r\n####################################################################################\r\n\r\n# Link the main database with the factors\r\n# function Map(\'[\', ...]) extracts data accordint fo Factot.lst from df.supersom.scale\r\n# supersom.layers is a results.supersom.list of data.frames (one per factor) with the id of each spatial unit and values for the variables within the each factor\r\nsupersom.layers <- c(id, Map(`[`, list(df.supersom.scale), Factor.lst.superSOM))\r\n\r\n#combine in a vector of character strings the names of the combined variables\r\nFactor.titles <- sapply(Factor.lst.superSOM, paste, collapse="" "")\r\n\r\n# Put a title for each factor\r\nnames(supersom.layers) <- c(""ID"",Factor.titles)\r\nrm(Factor.titles)\r\n\r\n# Define the weight of each layer according to the number of variables within it\r\nsupersom.w <- do.call(c,lapply(supersom.layers, NCOL))\r\n\r\n# supersom.layers is transpormed in a results.supersom.list of matrices, as supersom function needs matrices\r\n# lapply could also be used\r\nsupersom.layers <- lapply(supersom.layers, as.matrix)\r\n#supersom.layers2 <- sapply(supersom.layers, as.matrix)\r\n\r\n####################################################################################\r\n# 2.6 : SuperSOM 3x3                                                               #\r\n####################################################################################\r\n\r\nresults.supersom.list = list()\r\nfor(i in primes)\r\n{\r\n  set.seed(i)\r\n  supersom.seed <- supersom(supersom.layers, somgrid(3, 3, ""hexagonal""), \r\n                            rlen = 20000, alpha = c(0.05, 0.01), radius = 2,\r\n           contin = TRUE, toroidal = FALSE, n.hood = ""circular"",\r\n           weights = supersom.w/55 , maxNA.fraction = 10 , whatmap = 2:17 , keep.data = TRUE)\r\n  results.supersom.list[[length(results.supersom.list)+1]] <- supersom.seed\r\n  print(i)\r\n}\r\nnames(results.supersom.list) = c(paste0(""cluster.seed"", primes))\r\nrm(supersom.w)\r\n\r\n# Combine the results for each seed in one data.frame\r\nclusters.superSOM <- do.call(cbind, lapply(1:length(results.supersom.list),\r\n                                           function(i) cbind(results.supersom.list[[i]][[4]])))\r\nclusters.superSOM <- as.data.frame(cbind(supersom.layers[[1]], clusters.superSOM))\r\nnames(clusters.superSOM) <- c(""id.factor"", (paste0(""seed"", primes)))  \r\n\r\n####################################################################################\r\n# 2.7 : Pick Up the Best Seed                                                      #\r\n####################################################################################\r\n\r\n# 2.9.1 Fowlkes-Mallows Similarity Index Calculation\r\nbest.seed.superSOM <- FM.Similarity(results.supersom.list,clusters.superSOM)[[1]]\r\n\r\nbest.superSOM <- results.supersom.list[[best.seed.superSOM]]\r\n\r\n####################################################################################\r\n## 2.8 ANOVA By Variables                                                         #\r\n####################################################################################\r\n\r\ncluster.best.supersom <- best.superSOM[[4]]\r\nresults.best.superSOM.anova.ready <- cbind(df.supersom.scale, cluster.best.supersom)\r\nanova.best.superSOM <- apply(results.best.superSOM.anova.ready, 2,\r\n                             function(i) summary(aov(i ~ cluster.best.supersom,\r\n                                                     data=results.best.superSOM.anova.ready)))\r\np.vector <- do.call(c, lapply(1:length(anova.best.superSOM),\r\n                              function(i) cbind(anova.best.superSOM[[i]][[1]][[5]][1])))\r\np.vector <- p.vector [-length(p.vector)]\r\nnames(p.vector) <- variable.names\r\nvar.010significance.best.superSOM <- p.vector < 0.10\r\nnames(var.010significance.best.superSOM) <- variable.names\r\nsignificant.variables <- names(subset(var.010significance.best.superSOM, var.010significance.best.superSOM == TRUE))\r\nbest.superSOM.anova.results <- cbind(p.vector,var.010significance.best.superSOM)\r\nrm(var.010significance.best.superSOM,cluster.best.supersom,results.best.superSOM.anova.ready,\r\n   anova.best.superSOM,p.vector)\r\n\r\n\r\n####################################################################################\r\n#                                                                                  #\r\n#                                   MAIN 3                                         #\r\n#                         STANDARD SOM on factors + superSOM + ANOVA               #\r\n#                                                                                  #\r\n####################################################################################\r\n####################################################################################\r\n## 3.1 Remove non Significant Variables in the Data                                #\r\n####################################################################################\r\n\r\n# remove non significant variables in SOM/Supersom data\r\ndf.SOM.scale.bis <- df.SOM.scale[,significant.variables]\r\ndf.SOM.scale.t.bis <- t(df.SOM.scale.bis)\r\n\r\n####################################################################################\r\n# 3.2 : Som on Factors                                                             #\r\n####################################################################################\r\n\r\n## 3.2.1 Self-organising maps of variables for mapping high-dimensional data to 2D grid using nb different seeds.\r\nresults.SOM.list.bis <- list()\r\nfor(i in primes)\r\n{\r\n  set.seed(i)\r\n  som.seed<- som(data = df.SOM.scale.t.bis, grid = somgrid(4, 4, ""hexagonal""),\r\n                 rlen = 10000, toroidal = FALSE, radius = 3)\r\n  results.SOM.list.bis[[length(results.SOM.list.bis)+1]] <- som.seed\r\n  print(i)\r\n}\r\nrm(som.seed)\r\n\r\nnames(results.SOM.list.bis) = c(paste0(""som.seed"", primes))\r\n\r\n## 3.2.2 Clustering results for each seed\r\n# Combine the results for each seed in one data.frame\r\nclusters.SOM.bis <- do.call(cbind, lapply(1:length(results.SOM.list.bis), \r\n                                          function(i) cbind(results.SOM.list.bis[[i]][[8]])))\r\nvariable.names.bis <- row.names(df.SOM.scale.t.bis)\r\nclusters.SOM.bis <- as.data.frame(cbind(variable.names.bis, clusters.SOM.bis ))\r\nnames(clusters.SOM.bis) <- c(""id.factor"", (paste0(""cluster.seed"", primes)))    \r\n\r\n####################################################################################\r\n# 3.3 : Pick Up the Best Seed                                                     # \r\n####################################################################################\r\n\r\n## Fowlkes-Mallows Similarity Index Calculation\r\nbest.seed.SOM.bis <- FM.Similarity(results.SOM.list.bis,clusters.SOM.bis)[[1]]\r\n\r\n####################################################################################\r\n# 3.4 : Assign Factors                                                            #\r\n####################################################################################\r\n\r\n# cluster results for the best seed as dataframe with first column as row names\r\nrownames(clusters.SOM.bis) <- clusters.SOM.bis[,1]\r\nclusters.SOM.bis <- clusters.SOM.bis[,-1]\r\n\r\n# assign each row.names cluster result in a results.supersom.list of vectors\r\n# results.SOM.list[[1]][[2]][[2]] and results.SOM.list[[1]][[2]][[3]] are the dimensions of the SOM grid which was calculated, and hence the nb of factors\r\nFactor.lst.bis <- lapply(1:(results.SOM.list.bis[[1]][[2]][[2]]*results.SOM.list.bis[[1]][[2]][[3]]), \r\n                         function(i) row.names(subset(clusters.SOM.bis , get(best.seed.SOM.bis) ==i)))\r\n\r\n####################################################################################\r\n# 3.5 : superSOM Layer Preparation                                                 #\r\n####################################################################################\r\n\r\ndf.supersom.scale.bis <- df.supersom.scale[,significant.variables]\r\n\r\n# Link the main database with the factors\r\n# function Map(\'[\', ...]) extracts data accordint fo Factot.lst from df.supersom.scale\r\n# supersom.layers is a results.supersom.list of data.frames (one per factor) with the id of each spatial unit and values for the variables within the each factor\r\nsupersom.layers.bis <- c(id, Map(`[`, list(df.supersom.scale.bis), Factor.lst.bis))\r\n\r\n#combine in a vector of character strings the names of the combined variables\r\n# Put a title for each factor\r\nFactor.titles.bis <- sapply(Factor.lst.bis, paste, collapse="" "")\r\nnames(supersom.layers.bis) <- c(""ID"",Factor.titles.bis)\r\n\r\n# Define the weight of each layer according to the number of variables within it\r\nsupersom.w.bis <- do.call(c,lapply(supersom.layers.bis, NCOL))\r\n\r\n# supersom.layers is transpormed in a results.supersom.list of matrices, as supersom function needs matrices\r\nsupersom.layers.bis <- lapply(supersom.layers.bis, as.matrix)\r\n\r\n####################################################################################\r\n# 3.6 : SUPERSOM 3x3                                                              #\r\n####################################################################################\r\n\r\n## 3.6.1 superSOM\r\n# The following primes can not be optimized and are thus replaced\r\nprimes[6] <- 79\r\nprimes[10]<- 89\r\nprimes[11]<- 101\r\nprimes[12]<- 103\r\n\r\nresults.supersom.list.bis = list()\r\nfor(i in primes)\r\n{\r\n  set.seed(i)\r\n  supersom.seed <- supersom(supersom.layers.bis, somgrid(3, 3, ""hexagonal""), rlen = 20000, \r\n                            alpha = c(0.05, 0.01), radius = 2,\r\n                            contin = TRUE, toroidal = FALSE, n.hood = ""circular"",\r\n                            weights = supersom.w.bis/47 , maxNA.fraction = 10 , whatmap = 2:17 , keep.data = TRUE)\r\n  results.supersom.list.bis[[length(results.supersom.list.bis)+1]] <- supersom.seed\r\n  print(i)\r\n}\r\nnames(results.supersom.list.bis) = c(paste0(""cluster.seed"", primes))\r\n\r\n## 3.6.2 combine the learning steps for each seed in one matriX\r\n# Combine the results for each seed in one data.frame\r\nclusters.superSOM.bis <- do.call(cbind, lapply(1:length(results.supersom.list.bis),\r\n                                               function(i) cbind(results.supersom.list.bis[[i]][[4]])))\r\nclusters.superSOM.bis <- as.data.frame(cbind(supersom.layers.bis[[1]], clusters.superSOM.bis))\r\nnames(clusters.superSOM.bis) <- c(""id.factor"", (paste0(""seed"", primes)))  \r\n\r\n####################################################################################\r\n# 3.7 : Pick Up the Best Seed                                                      #\r\n####################################################################################\r\n\r\n## Fowlkes-Mallows Similarity Index Calculation\r\nbest.seed.superSOM.bis <- FM.Similarity(results.supersom.list.bis,clusters.superSOM.bis)[[1]]\r\n\r\nbest.superSOM.bis <- results.supersom.list.bis[[best.seed.superSOM.bis]]\r\n\r\n####################################################################################\r\n## 3.8 Anova By variables                                                         #\r\n####################################################################################\r\n\r\ncluster.best.supersom.bis <- best.superSOM.bis[[4]]\r\nresults.best.superSOM.bis.anova.ready <- cbind(df.supersom.scale.bis, cluster.best.supersom.bis)\r\nanova.best.superSOM.bis <- apply(results.best.superSOM.bis.anova.ready, 2,\r\n                             function(i) summary(aov(i ~ cluster.best.supersom.bis,\r\n                                                     data=results.best.superSOM.bis.anova.ready)))\r\np.vector <- do.call(c, lapply(1:length(anova.best.superSOM.bis),\r\n                              function(i) cbind(anova.best.superSOM.bis[[i]][[1]][[5]][1])))\r\np.vector <- p.vector [-length(p.vector)]\r\nnames(p.vector) <- variable.names.bis\r\nvar.010significance.best.superSOM.bis <- p.vector < 0.10\r\nnames(var.010significance.best.superSOM.bis) <- variable.names.bis\r\nsignificant.variables.bis <- names(subset(var.010significance.best.superSOM.bis,\r\n                                          var.010significance.best.superSOM.bis == TRUE))\r\nbest.superSOM.anova.results.bis <- cbind(p.vector,var.010significance.best.superSOM.bis)\r\n\r\nrm(var.010significance.best.superSOM.bis,cluster.best.supersom.bis,results.best.superSOM.bis.anova.ready,\r\n   anova.best.superSOM.bis,p.vector)\r\n\r\n####################################################################################\r\n#                                                                                  #\r\n#                                   APPENDIX                                       #\r\n#                         Save the Data-Frame Results                              #\r\n#                                                                                  #\r\n####################################################################################\r\n\r\nmain.directory <- getwd()\r\ndir.create(""Dataframe_Results"")\r\nsetwd(paste0(main.directory,\'/Dataframe_Results\'))\r\n\r\n## Save the standardSOM\r\nwrite.csv(cbind(best.standard.SOM[[1]], best.standard.SOM[[8]], best.standard.SOM[[9]]),\r\n                ""results.standard.SOM.csv"")\r\n\r\n## Save the SOM + superSOM \r\nwrite.csv(cbind(best.superSOM[[1]][[1]], df.supersom.scale, best.superSOM[[4]], best.superSOM[[5]]),\r\n          ""results.supersom.csv"")\r\n\r\n## Save the SOM + superSOM + Anova\r\nwrite.csv(cbind(best.superSOM.bis[[1]][[1]], df.supersom.scale.bis, best.superSOM.bis[[4]], \r\n                best.superSOM.bis[[5]]), ""results.supersom.bis.csv"")\r\n\r\nsetwd(main.directory)\r\n\r\n\r\n']","Robust SOM Clustering This R script performs a combined SOM/SuperSOM clustering of the 640 administrative districs of India. Fowlkes-Mallows Similarity Index is used to identify robust initializations of the clustering. Data_India.txt is a specially conceived geographic database of 55 indicators, covering issues of economic activity, urban structure, socio-demographic development, consumption levels, infrastructure endowment and basic geographical positioning within the Indian space. Data refer to 2011 or to 2001-2011 evolutions.Warnng: This is an old version of the project. Version 1.2 available at https://zenodo.org/record/2563213#.XGQl01xKiiz",1
Friends with or without benefits? An empirical evaluation of bilateral trade and economic integration between some of the post-Soviet economies.,"This dataset reproduces empirical results for the paper Friends with or without benefits? An empirical review of trade integration between some of the post-Soviet economies. Eurasian Economic Review (2022). It includes i) augmented gravity analysis and ii) impact of bilateral\multilateral FTA/PTA agreements for the EUEA members: Armenia, Belarus, Russia, Kazakhstan, and Kyrgyzstan. This research was funded in whole by National Science Centre, Poland under PRELUDIUM 20 grant 2021/41/N/HS4/00759. For the purpose of Open Access, the author has applied a CC-BY public copyright license to any Author Accepted Manuscript (AAM) version arising from this submission.","['# #----------------------------------------------------\r\n# This code produces graphs for the paper:\r\n# Cielik, A., Gurshev, O. Friends with or without benefits? \r\n# An empirical evaluation of bilateral trade and economic integration\r\n# between some of the post-Soviet economies. Eurasian Econ Rev (2022). \r\n# https://doi.org/10.1007/s40822-022-00213-9\r\n#\r\n# Date revised: 10.07.2022\r\n#----------------------------------------------------\r\nlibrary(ggplot2)\r\nlibrary(RColorBrewer)\r\nlibrary(forcats)\r\nlibrary(wesanderson)\r\n#----------------------------------------------------\r\n# directory \r\nsetwd()\r\ngetwd()\r\n#----------------------------------------------------\r\n# Loading data\r\n\r\ndata <-\r\n  read.table(\'cu.csv\',\r\n             sep = "","",\r\n             dec = ""."",\r\n             header = TRUE)\r\n\r\n\r\nexp <- \r\n  read.table(\'exp.csv\',\r\n             sep = "","",\r\n             dec = ""."",\r\n             header = TRUE)\r\n\r\nimp <- \r\n  read.table(\'imp.csv\',\r\n             sep = "","",\r\n             dec = ""."",\r\n             header = TRUE)\r\n\r\ndata$logcu <- log(data$cu*1000000000)\r\n\r\n#---------------------------------------------------\r\n# This code uses cu.csv and produces Figure A1: Russia and CU\r\n\r\nggplot(data, aes(x=year)) + \r\n  geom_line(aes(y = logcu, color =""CU/EUEA""), size = 2) +\r\n  geom_line(aes(y = Russia, color =""Russia""), size = 2) +\r\n  labs (x =""year"", y = ""Log of total trade"", color = \'Entity\') +\r\n  geom_vline(xintercept = 2010 , color = ""red"", linetype = ""dashed"") + \r\n  geom_vline(xintercept = 2012 , color = ""red"", linetype = ""dashed"") + \r\n  geom_vline(xintercept = 2015 , color = ""red"", linetype = ""dashed"") +\r\n  geom_vline(xintercept = 2016 , color = ""red"", linetype = ""dashed"") +\r\n  theme_classic() + \r\n  scale_x_continuous(breaks = scales::pretty_breaks(10)) + scale_color_brewer(palette=""Paired"")\r\n#---------------------------------------------------\r\n# This code uses cu.csv and produces Figure A2: total trade\r\n\r\nggplot(data, aes(x=year)) + \r\n  geom_line(aes(y = logcu, color = ""CU/EUEA""), size = 2, linetype = 4) +\r\n  geom_line(aes(y = Belarus, color = ""Belarus""), size = 1) +\r\n  geom_line(aes(y = Kazakhstan, color = ""Kazakhstan""), size = 1) +\r\n  geom_line(aes(y = Russia, color = ""Russia""), size = 1) +\r\n  geom_line(aes(y = Kyrgyzstan, color = ""Kyrgyzstan""), size = 1) +\r\n  geom_line(aes(y = Armenia, color = ""Armenia""), size = 1) +\r\n  labs (x =""year"", y = ""Log of total trade"", color = \'Entity\') +\r\n  geom_vline(xintercept = 2010 , color = ""red"", linetype = ""dashed"") + \r\n  geom_vline(xintercept = 2012 , color = ""red"", linetype = ""dashed"") + \r\n  geom_vline(xintercept = 2015 , color = ""red"", linetype = ""dashed"") +\r\n  geom_vline(xintercept = 2016 , color = ""red"", linetype = ""dashed"") +\r\n  theme_classic() + \r\n  scale_x_continuous(breaks = scales::pretty_breaks(10)) + scale_color_brewer(palette=""Paired"")\r\n\r\n#---------------------------------------------------\r\n# Graph without Russia, total trade\r\n\r\nggplot(data, aes(x=year)) + \r\n  geom_line(aes(y = logcu, color = ""CU/EUEA""), size = 2, linetype = 4) +\r\n  geom_line(aes(y = Belarus, color = ""Belarus""), size = 1) +\r\n  geom_line(aes(y = Kazakhstan, color = ""Kazakhstan""), size = 1) +\r\n  geom_line(aes(y = Kyrgyzstan, color = ""Kyrgyzstan""), size = 1) +\r\n  geom_line(aes(y = Armenia, color = ""Armenia""), size = 1) +\r\n  labs (x =""year"", y = ""Log of total trade"", color = \'Entity\') +\r\n  geom_vline(xintercept = 2010 , color = ""red"", linetype = ""dashed"") + \r\n  geom_vline(xintercept = 2012 , color = ""red"", linetype = ""dashed"") + \r\n  geom_vline(xintercept = 2015 , color = ""red"", linetype = ""dashed"") +\r\n  geom_vline(xintercept = 2016 , color = ""red"", linetype = ""dashed"") +\r\n  theme_classic() + \r\n  scale_x_continuous(breaks = scales::pretty_breaks(10)) + scale_color_brewer(palette=""Paired"")\r\n\r\n#---------------------------------------------------\r\n# This code uses exp.csv and produces Figure A3: exports\r\n\r\nggplot(exp, aes(x=year)) + \r\n  geom_line(aes(y = cu, color = ""CU/EUEA""), size = 2, linetype = 4) +\r\n  geom_line(aes(y = Belarus, color = ""Belarus""), size = 1) +\r\n  geom_line(aes(y = Kazakhstan, color = ""Kazakhstan""), size = 1) +\r\n  geom_line(aes(y = Russia, color = ""Russia""), size = 1) +\r\n  geom_line(aes(y = Kyrgyzstan, color = ""Kyrgyzstan""), size = 1) +\r\n  geom_line(aes(y = Armenia, color = ""Armenia""), size = 1) +\r\n  labs (x =""year"", y = ""Log of total exports"", color = \'Entity\') +\r\n  geom_vline(xintercept = 2010 , color = ""red"", linetype = ""dashed"") + \r\n  geom_vline(xintercept = 2012 , color = ""red"", linetype = ""dashed"") + \r\n  geom_vline(xintercept = 2015 , color = ""red"", linetype = ""dashed"") +\r\n  geom_vline(xintercept = 2016 , color = ""red"", linetype = ""dashed"") +\r\n  theme_classic() + \r\n  scale_x_continuous(breaks = scales::pretty_breaks(10)) + scale_color_brewer(palette=""Paired"")\r\n\r\n#---------------------------------------------------\r\n# This code uses imp.csv and produces Figure A4: imports\r\n\r\nggplot(imp, aes(x=year)) + \r\n  geom_line(aes(y = cu, color = ""CU/EUEA""), size = 2, linetype = 4) +\r\n  geom_line(aes(y = Belarus, color = ""Belarus""), size = 1) +\r\n  geom_line(aes(y = Kazakhstan, color = ""Kazakhstan""), size = 1) +\r\n  geom_line(aes(y = Russia, color = ""Russia""), size = 1) +\r\n  geom_line(aes(y = Kyrgyzstan, color = ""Kyrgyzstan""), size = 1) +\r\n  geom_line(aes(y = Armenia, color = ""Armenia""), size = 1) +\r\n  labs (x =""year"", y = ""Log of total imports"", color = \'Entity\') +\r\n  geom_vline(xintercept = 2010 , color = ""red"", linetype = ""dashed"") + \r\n  geom_vline(xintercept = 2012 , color = ""red"", linetype = ""dashed"") + \r\n  geom_vline(xintercept = 2015 , color = ""red"", linetype = ""dashed"") +\r\n  geom_vline(xintercept = 2016 , color = ""red"", linetype = ""dashed"") +\r\n  theme_classic() + \r\n  scale_x_continuous(breaks = scales::pretty_breaks(10)) + scale_color_brewer(palette=""Paired"")\r\n']","Friends with or without benefits? An empirical evaluation of bilateral trade and economic integration between some of the post-Soviet economies. This dataset reproduces empirical results for the paper Friends with or without benefits? An empirical review of trade integration between some of the post-Soviet economies. Eurasian Economic Review (2022). It includes i) augmented gravity analysis and ii) impact of bilateral\multilateral FTA/PTA agreements for the EUEA members: Armenia, Belarus, Russia, Kazakhstan, and Kyrgyzstan. This research was funded in whole by National Science Centre, Poland under PRELUDIUM 20 grant 2021/41/N/HS4/00759. For the purpose of Open Access, the author has applied a CC-BY public copyright license to any Author Accepted Manuscript (AAM) version arising from this submission.",1
Biodiversity and infrastructure interact to drive tourism to and within Costa Rica,"Significance Tourism accounts for roughly 10% of global gross domestic product, with nature-based tourism its fastest-growing sector in the past 10 years. Nature-based tourism can theoretically contribute to local and sustainable development by creating attractive livelihoods that support biodiversity conservation, but whether tourists prefer to visit more biodiverse destinations is poorly understood. We examine this question in Costa Rica and find that more biodiverse places tend indeed to attract more tourists, especially where there is infrastructure that makes these places more accessible. Safeguarding terrestrial biodiversity is critical to preserving the substantial economic benefits that countries derive from tourism. Investments in both biodiversity conservation and infrastructure are needed to allow biodiverse countries to rely on tourism for their sustainable development.",,"Biodiversity and infrastructure interact to drive tourism to and within Costa Rica Significance Tourism accounts for roughly 10% of global gross domestic product, with nature-based tourism its fastest-growing sector in the past 10 years. Nature-based tourism can theoretically contribute to local and sustainable development by creating attractive livelihoods that support biodiversity conservation, but whether tourists prefer to visit more biodiverse destinations is poorly understood. We examine this question in Costa Rica and find that more biodiverse places tend indeed to attract more tourists, especially where there is infrastructure that makes these places more accessible. Safeguarding terrestrial biodiversity is critical to preserving the substantial economic benefits that countries derive from tourism. Investments in both biodiversity conservation and infrastructure are needed to allow biodiverse countries to rely on tourism for their sustainable development.",1
White paper: The effect of septic systems on wastewater-based epidemiology,"Households with septic systems are more economically advantaged than households on public sewers. Septic users have higher median household income, higher educational attainment, and greater food security.Wastewater surveillance in rural areas likely oversamples economically disadvantaged populations. This is a desirable feature because economic stability is an important social determinant of health and because economically disadvantaged populations may be underrepresented in public health surveillance systems that rely on interactions with healthcarePeople living in homes that use septic systems can still contribute to public wastewater when they use restrooms in public buildings, like workplaces, schools, restaurants, libraries, and shopping malls.Septic systems need not be a barrier to wastewater surveillance programs nationwide.","['library(tidyverse)\nlibrary(srvyr)\nlibrary(survey)\n\n# Set up output logging --------------------------------------------------------\n\nlog_path <- ""log.txt""\n\n# future log lines will be appended. add a newline after a log section\nmy_log <- function(..., title = NULL) {\n  write_lines(c(title, ..., """"), log_path, append = TRUE)\n}\n\n# start the log with the date\nmy_log(date())\n\n# Download data ----------------------------------------------------------------\n\n# Main data file: AHS 2019 National PUF v1.1 CSV\n# https://www.census.gov/programs-surveys/ahs/data/2019/ahs-2019-public-use-file--puf-/ahs-2019-national-public-use-file--puf-.html\n# Codebook:\n# https://www.census.gov/data-tools/demo/codebook/ahs/ahsdict.html\n\n# Download the Census data in zip format; pull just data we need\nzip_url <- ""https://www2.census.gov/programs-surveys/ahs/2019/AHS%202019%20National%20PUF%20v1.1%20CSV.zip""\nlocal_zip <- tempfile()\ndownload.file(zip_url, local_zip)\n\n# Load data --------------------------------------------------------------------\n\n# Read in household data (streaming directly from zip file)\nhh_raw <- unz(local_zip, ""household.csv"") %>%\n  read_csv(\n    # read all columns as strings; parse them later\n    col_types = cols(.default = ""c""),\n    # read in only select columns\n    col_select = c(\n      DIVISION, SEWTYPE, NUMPEOPLE, OMB13CBSA,\n      HHRACE, HHSPAN, HHGRAD,\n      HINCP, FSSTATUS,\n      INTSTATUS, WEIGHT, starts_with(""REPWEIGHT"")\n    ),\n    # read_csv by default expects double quotes; this file uses single quotes\n    quote = ""\'""\n  )\n\n# Census Divisions are labeled 1 through 9\ndivisions <- c(\n  ""New England"", ""Middle Atlantic"", ""East North Central"",\n  ""West North Central"", ""South Atlantic"", ""East South Central"",\n  ""West South Central"", ""Mountain"", ""Pacific""\n)\n\n# Clean up the household file\nhh <- hh_raw %>%\n  mutate(\n    across(DIVISION, parse_integer),\n    across(c(NUMPEOPLE, HINCP), parse_number, na = ""-6""),\n    across(FSSTATUS, parse_integer, na = c(""5"", ""-6"")),\n    across(c(WEIGHT, starts_with(""REPWEIGHT"")), parse_number),\n  ) %>%\n  mutate(\n    # census division - set most populous (South Atlantic) as baseline\n    division = fct_relevel(divisions[DIVISION], ""South Atlantic""),\n    # in a metro area? (CBSA) - set metro as baseline\n    metro = factor(\n      if_else(OMB13CBSA == ""99999"", ""rural"", ""metro""),\n      levels = c(""metro"", ""rural"")\n    ),\n    # public sewer vs. septic+cesspool, and ""other"" - set sewer as baseline\n    waste = factor(case_when(\n      SEWTYPE == ""01"" ~ ""sewer"",\n      SEWTYPE %in% c(""02"", ""03"", ""04"", ""05"", ""06"") ~ ""septic"",\n      TRUE ~ ""other""\n    ), levels = c(""sewer"", ""septic"", ""other"")),\n    # NH white only?\n    white = as.integer(HHRACE == ""01"" & HHSPAN == ""2""),\n    # bachelors degree or more\n    bachelors = as.integer(HHGRAD %in% c(\'44\', \'45\', \'46\', \'47\')),\n    # high food security? (ie not marginal, low, or very low food)\n    food_secure = as.integer(FSSTATUS == 1),\n    # income in thousands\n    income = HINCP / 1e3\n  )\n\n# Number of entries\nmy_log(\n  ""Entries in the dataset:"",\n  prettyNum(nrow(hh), big.mark = "","")\n)\n\n## Survey design\ndesign <- as_survey_rep(\n  hh,\n  weights = WEIGHT, repweights = starts_with(""REPWEIGHT""),\n  type = ""Fay"", rho = 1 - 1 / sqrt(4), mse = TRUE\n)\n\n# subset to occupied units at time of interview\noccupied <- filter(design, INTSTATUS == ""1"")\n\n\n# Basic stats and geography ---------------------------------------------------\n\n# Total # of households\nmy_log(\n  ""Total number of households:"",\n  prettyNum(survey_tally(occupied)$n, big.mark = "","")\n)\n\n# Households by waste system and metro status\nhh_by_wastemetro <- occupied %>%\n  survey_count(waste, metro) %>%\n  mutate(\n    f = n / sum(n),\n    percent = scales::percent(f, accuracy = 1)\n  )\n\nmy_log(\n  ""Households by waste system & metro status:"",\n  format(hh_by_wastemetro)\n)\n\n# % by waste system\nhh_by_wastemetro %>%\n  group_by(waste) %>%\n  summarize(percent = scales::percent(sum(f), accuracy = 1)) %>%\n  format() %>%\n  my_log(title = ""Households by waste system:"")\n\n# % by metro status\nhh_by_wastemetro %>%\n  group_by(metro) %>%\n  summarize(percent = scales::percent(sum(f), accuracy = 1)) %>%\n  format() %>%\n  my_log(title = ""Households by metro status:"")\n\n# how it varies by census region\noccupied %>%\n  group_by(division) %>%\n  summarize(septic = survey_mean(waste == ""septic"")) %>%\n  arrange(desc(septic)) %>%\n  format() %>%\n  my_log(title = ""Septic % by division:"")\n\n# Equity -----------------------------------------------------------------------\n\n# Economic metrics by metro status\nequity_by_metro <- occupied %>%\n  group_by(metro) %>%\n  summarize(\n    across(income, survey_median),\n    across(c(food_secure, bachelors, white), survey_mean, na.rm = TRUE)\n  )\n\n# Income by waste, metro, and region\noccupied %>%\n  group_by(division, waste, metro) %>%\n  summarize(across(income, survey_median), .groups = ""drop"") %>%\n  filter(waste != ""other"") %>%\n  mutate(label = str_c(metro, ""."", waste)) %>%\n  select(division, name = label, value = income) %>%\n  pivot_wider() %>%\n  select(division, metro.septic, metro.sewer, rural.septic, rural.sewer) %>%\n  format() %>%\n  my_log(title = ""Income by waste, metro, and division:"")\n\n# appears as if Pacific has wrong ordering, but this is because of wide SEs\noccupied %>%\n  filter(division == ""Pacific"") %>%\n  group_by(waste, metro) %>%\n  summarize(across(income, survey_median)) %>%\n  format() %>%\n  my_log(title = ""Pacific region has wide SEs:"")\n\n# septic is more disadvantaged overall\nequity_by_waste <- occupied %>%\n  group_by(waste) %>%\n  summarize(\n    across(income, survey_median),\n    across(c(food_secure, bachelors, white), survey_mean, na.rm = TRUE)\n  )\n\n# but, septic is more advantaged, when stratifying by metro status\nequity_by_wastemetro <- occupied %>%\n  group_by(waste, metro) %>%\n  summarize(\n    across(income, survey_median),\n    across(c(food_secure, bachelors, white), survey_mean, na.rm = TRUE)\n  )\n\n# combine equity tables into one\nequity <- bind_rows(\n  equity_by_waste,\n  equity_by_metro,\n  equity_by_wastemetro\n) %>%\n  mutate(across(where(is.factor), as.character)) %>%\n  replace_na(list(metro = """", waste = """")) %>%\n  filter(waste != ""other"") %>%\n  mutate(label = factor(\n    str_c(metro, ""."", waste),\n    levels = c(\n      ""metro."", ""rural."",\n      "".septic"", "".sewer"",\n      ""metro.septic"", ""metro.sewer"",\n      ""rural.septic"", ""rural.sewer""\n    )\n  ))\n\n# bar chart of incomes\nplot <- equity %>%\n  ggplot(aes(label, income * 1e3, fill = fct_rev(label))) +\n  geom_col() +\n  scale_fill_manual(values = c(\n    rural. = ""gray40"",\n    metro. = ""gray40"",\n    .septic = ""gray80"",\n    .sewer = ""gray80"",\n    rural.sewer = ""#99E9CD"",\n    rural.septic = ""#03AB72"",\n    metro.sewer = ""#A0BACD"",\n    metro.septic = ""#08355C""\n  )) +\n  scale_y_continuous(\n    labels = scales::label_dollar(),\n    limits = c(0, 80 * 1e3),\n    expand = c(0, 0)\n  ) +\n  labs(\n    title = ""Median household income""\n  ) +\n  cowplot::theme_half_open() +\n  theme(\n    axis.title = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = ""none""\n  )\n\nggsave(""incomes.pdf"")\n\n# table\nequity %>%\n  select(name = label, income, bachelors, food_secure, white) %>%\n  mutate(\n    across(c(bachelors, food_secure, white), scales::percent, accuracy = 1),\n    across(income, function(x) str_glue(""${round(x)},000""))\n  ) %>%\n  pivot_longer(!name, names_to = ""group"") %>%\n  pivot_wider() %>%\n  select(group, metro., metro.septic, metro.sewer, rural., rural.septic, rural.sewer) %>%\n  format() %>%\n  my_log(title = ""Main table:"")\n\n# Supplement -------------------------------------------------------------------\n\n# Benjamini-Hochberg significance testing\nmy_sig <- function(x, alpha = 0.01) {\n  case_when(\n    p.adjust(x, ""BH"") < alpha ~ ""**"",\n    x < alpha ~ ""*"",\n    TRUE ~ """"\n  )\n}\n\n# big logistic regression\nlogistic_results <- svyglm(\n  I(waste == ""septic"") ~ division + metro + income + bachelors + white,\n  family = quasibinomial, design = occupied\n) %>%\n  broom::tidy(conf.int = TRUE) %>%\n  select(term, estimate, p.value, conf.low, conf.high) %>%\n  mutate(\n    across(c(estimate, conf.low, conf.high), exp),\n    sig = my_sig(p.value)\n  )\n\nmy_log(\n  ""Logistic regression: septic? ~ predictors:"",\n  ""(Coefficients are odds ratios, not log odds)"",\n  format(logistic_results),\n  ""** FDR < 0.01; * p < 0.01""\n)\n\n# big linear regression\nlinear_results <- svyglm(\n  income ~ waste + metro + bachelors + white + division,\n  design = occupied\n) %>%\n  broom::tidy(conf.int = TRUE) %>%\n  select(term, estimate, p.value, conf.low, conf.high) %>%\n  mutate(sig = my_sig(p.value))\n\nmy_log(\n  ""Linear regression: income ($k) ~ predictors:"",\n  format(linear_results),\n  ""** FDR < 0.01; * p < 0.01""\n)\n']","White paper: The effect of septic systems on wastewater-based epidemiology Households with septic systems are more economically advantaged than households on public sewers. Septic users have higher median household income, higher educational attainment, and greater food security.Wastewater surveillance in rural areas likely oversamples economically disadvantaged populations. This is a desirable feature because economic stability is an important social determinant of health and because economically disadvantaged populations may be underrepresented in public health surveillance systems that rely on interactions with healthcarePeople living in homes that use septic systems can still contribute to public wastewater when they use restrooms in public buildings, like workplaces, schools, restaurants, libraries, and shopping malls.Septic systems need not be a barrier to wastewater surveillance programs nationwide.",1
"Data and code for: Microalgae-blend tilapia feed eliminates fishmeal and fish oil, improves growth, and is cost viable","Aquafeed manufacturers have reduced, but not fully eliminated, fishmeal and fish oil and are seeking cost competitive replacements. We combined two commercially available microalgae, to produce a high-performing fish-free feed for Nile tilapia (Oreochromis niloticus) the world's second largest group of farmed fish. We substituted protein-rich defatted biomass of Nannochloropsis oculata (leftover after oil extraction for nutraceuticals) for fishmeal and whole cells of docosahexaenoic acid (DHA)-rich Schizochytrium sp. as substitute for fish oil. Here, we provide the datasets and code that we used to estimate the price of fish-free experimental and reference diets of tilpia in the Scientific Reports manuscript entitled, ""Microalgae-blend tilapia feed eliminates fishmeal and fish oil, improves growth, and is cost viable"". We include the Rstudio and supporting .csv files for a hedonic analysis of defatted N. oculata meal and whole-cell Schizochytrium sp., non-parametric bootstraps to estimate the median and 95% confidence intervals of commodity and market prices for the formulated tilapia feed ingredients, and for Fig. 2 in the manuscript.","['#Set working directory to the file path where the csv file and output runs stored \r\nsetwd(""C:/Hedonic"")\r\n\r\n\r\n\r\ninstall.packages(""boot"")\r\n#bootstrap code from https://rcompanion.org/handbook/E_04.html\r\n\r\ndata_corn_gluten_meal <- read.csv(""Corn_gluten_meal_annual_price.csv"", header = TRUE)\r\nCorn_gluten_meal_boot = boot(data_corn_gluten_meal$Price,\r\n             function(x,i) median(x[i]),\r\n             R=10000)\r\n\r\nboot.ci(Corn_gluten_meal_boot,\r\n        conf = 0.95,\r\n        type = c(""norm"", ""basic"" ,""perc"", ""bca"")\r\n)\r\nmedian(data_corn_gluten_meal$Price)\r\n\r\ndata_fish_meal <- read.csv(""Fish_meal_annual_price.csv"",header = TRUE)\r\nFish_meal_boot = boot(data_fish_meal$Price,\r\n              function(x,i) median(x[i]),\r\n              R=10000)\r\n\r\nboot.ci(Fish_meal_boot,\r\n        conf = 0.95,\r\n        type = c(""norm"", ""basic"" ,""perc"", ""bca"")\r\n)\r\nmedian(data_fish_meal$Price)\r\n\r\ndata_soybean_meal <- read.csv(""Soybean_meal_annual_price.csv"",header = TRUE)\r\nSoybean_meal_boot = boot(data_soybean_meal$Price,\r\n                      function(x,i) median(x[i]),\r\n                      R=10000)\r\n\r\nboot.ci(Soybean_meal_boot,\r\n        conf = 0.95,\r\n        type = c(""norm"", ""basic"" ,""perc"", ""bca"")\r\n)\r\nmedian(data_soybean_meal$Price)\r\n\r\ndata_wheat_flour <- read.csv(""Wheat_flour_annual_price.csv"",header = TRUE)\r\nWheat_flour_boot = boot(data_wheat_flour$Price,\r\n                         function(x,i) median(x[i]),\r\n                         R=10000)\r\n\r\nboot.ci(Wheat_flour_boot,\r\n        conf = 0.95,\r\n        type = c(""norm"", ""basic"" ,""perc"", ""bca"")\r\n)\r\nmedian(data_wheat_flour$Price)\r\n\r\ndata_fish_oil <- read.csv(""Fish_oil_annual_price.csv"",header = TRUE)\r\nFish_oil_boot = boot(data_fish_oil$Price,\r\n                        function(x,i) median(x[i]),\r\n                        R=10000)\r\n\r\nboot.ci(Fish_oil_boot,\r\n        conf = 0.95,\r\n        type = c(""norm"", ""basic"" ,""perc"", ""bca"")\r\n)\r\nmedian(data_fish_oil$Price)\r\n\r\ndata_lysine <- read.csv(""Lysine_annual_price.csv"",header = TRUE)\r\nLysine_boot = boot(data_lysine$Mean_price,\r\n                     function(x,i) median(x[i]),\r\n                     R=10000)\r\n\r\nboot.ci(Lysine_boot,\r\n        conf = 0.95,\r\n        type = c(""norm"", ""basic"" ,""perc"", ""bca"")\r\n)\r\nmedian(data_lysine$Price)\r\n\r\ndata_choline_chloride <- read.csv(""Choline_chloride_annual_price.csv"",header = TRUE)\r\nCholine_chloride_boot = boot(data_choline_chloride$Price,\r\n                   function(x,i) median(x[i]),\r\n                   R=10000)\r\n\r\nboot.ci(Choline_chloride_boot,\r\n        conf = 0.95,\r\n        type = c(""norm"", ""basic"" ,""perc"", ""bca"")\r\n)\r\nmedian(data_choline_chloride$Price)\r\n\r\ndata_DCP <- read.csv(""DCP_annual_price.csv"",header = TRUE)\r\nDCP_boot = boot(data_DCP$Price,\r\n                             function(x,i) median(x[i]),\r\n                             R=10000)\r\n\r\nboot.ci(DCP_boot,\r\n        conf = 0.95,\r\n        type = c(""norm"", ""basic"" ,""perc"", ""bca"")\r\n)\r\nmedian(data_DCP$Price)\r\n\r\ndata_nanno_meal <- read.csv(""Nanno_meal_annual_price.csv"",header = TRUE)\r\nNanno_meal_boot = boot(data_nanno_meal$Price,\r\n                function(x,i) median(x[i]),\r\n                R=10000)\r\n\r\nboot.ci(Nanno_meal_boot,\r\n        conf = 0.95,\r\n        type = c(""norm"", ""basic"" ,""perc"", ""bca"")\r\n)\r\nmedian(data_nanno_meal$Price)\r\n\r\ndata_schizo <- read.csv(""Whole_schizo_annual_price.csv"",header = TRUE)\r\nSchizo_boot = boot(data_schizo$Price,\r\n                       function(x,i) median(x[i]),\r\n                       R=10000)\r\n\r\nboot.ci(Schizo_boot,\r\n        conf = 0.95,\r\n        type = c(""norm"", ""basic"" ,""perc"", ""bca"")\r\n)\r\nmedian(data_schizo$Price)\r\n', '#Set working directory to the file path where the csv file and output runs stored \r\nsetwd(""C:/Hedonic"")\r\n\r\n\r\ndf <- read.csv(""Feed_price.csv"", header = TRUE)\r\n\r\ndf1 <- df[c(1:52),c(1:3)]\r\ndf2 <- df[c(53:56),c(1:5)]\r\ndf1$Scenario <- factor(df1$Scenario, levels = c(""Ref"", ""33NS"",""66NS"",""100NS""))\r\n\r\n\r\n\r\npng(""Figure_2_v2.png"",height=4,width=4,units=\'in\',res=300)\r\nggplot(df, aes(x=Scenario,y=Median_price))+\r\n  geom_bar(data=df1, aes(x=Scenario,y=Median_price, fill=Ingredient), position=""stack"", stat=""identity"",color=""black"") +\r\n  geom_point(data=df2, color=""black"", size = 2, show_guide = FALSE)+\r\n  geom_errorbar(data=df2, aes(y=Median_price,x=Scenario,ymin=Median_price-LBCI,ymax=Median_price+UBCI), width = 0.30)+\r\n  scale_fill_brewer(palette = ""Paired"")+\r\n  theme_classic2() +\r\n  ylab(expression(""Economic conversion ratio (USD$/kg tilapia)""))+\r\n  ylim(0,1.5) +\r\n  theme(axis.title.x = element_blank(), legend.title = element_blank(), panel.border = element_rect(colour = ""black"", fill=NA, size=0.5))\r\ndev.off() ', '#Set working directory to the file path where the csv file and output runs stored \r\nsetwd(""C:/Hedonic"")\r\n\r\n#data frame \'df\'\r\ndf <- read.csv(""df_meal_CP_EE_plus_aminos.csv"", header = TRUE)\r\n#mixed-effect model\r\nmodel_meal_lmer <- lmer(Mean_price ~ I(CP^2) + I(Met^2) +  I(Lys^2) + EE + (CP|Year) + (EE|Year), control = lmerControl(optimizer = ""nloptwrap"", calc.derivs = FALSE, optCtrl = list(algorithm = ""NLOPT_LN_BOBYQA"", maxeval = 1000, xtol_abs = 1e-6, ftol_abs = 1e-6)), REML = FALSE, data = df)\r\nsummary(model_meal_lmer)\r\n#fixed-effects only model\r\nmodel_meal_lm <- lm(Mean_price ~ I(CP^2) + I(Met^2) +  I(Lys^2) + EE, data = df)\r\nsummary(model_meal_lm)\r\nplot(model_meal_lm)\r\n#ANOVA to compare AICs of mixed-effects vs. fixed-effects only\r\nanova(model_meal_lmer, model_meal_lm)\r\n#coefficient of determination\r\nr.squaredGLMM(model_meal_lmer)\r\n#graphical test of linearity\r\nplot(model_meal_lmer)\r\n#graphical test of normality\r\nqqnorm(residuals(model_meal_lmer))\r\nqqline(residuals(model_meal_lmer))\r\nranef(model_meal_lmer)\r\n#identification of influential observations\r\ninf_model_meal_lm <- influence.measures(model_meal_lm)\r\nsummary(inf_model_meal_lm)\r\n\r\n#Levene test for homogeneity\r\nres_model_meal_lmer <- residuals(model_meal_lmer)\r\nres_sq_model_meal_lmer <- I(res_model_meal_lmer^2)\r\nLevene_model_meal <- lm(res_sq_model_meal_lmer ~ I(CP^2) + I(Met^2) +  I(Lys^2) + EE, data = df)\r\nanova(Levene_model_meal)\r\n\r\n', '#Set working directory to the file path where the csv file and output runs stored \r\nsetwd(""C:/Hedonic"")\r\n#Code to install packages\r\ninstall.packages(""nlme"")\r\ninstall.packages(""lmerTest"")\r\n#Code to call the library\r\nlibrary(nlme)\r\nlibrary(lmerTest)\r\n\r\n#data frame \'df\'\r\ndf <- read.csv(""df_scaled_oil_4_23_2020.csv"", header = TRUE)\r\n#mixed-effect model\r\nmodel_oil_lmer <- lmer(Mean_price ~ I(L_20_5_n3^2) + I(L_14_0^2) + I(L_16_1_n7^2) + L_14_0 +L_16_0 + (L_14_0|Year) + (L_16_0|Year) + (L_16_1_n7|Year), control = lmerControl(optimizer = ""nloptwrap"", calc.derivs = FALSE, optCtrl = list(algorithm = ""NLOPT_LN_BOBYQA"", maxeval = 1000, xtol_abs = 1e-6, ftol_abs = 1e-6)), REML = FALSE, data = df)\r\nsummary(model_oil_lmer)\r\n\r\n#fixed-effects only model\r\nmodel_oil_lm <- lm(Mean_price ~ I(L_20_5_n3^2) + I(L_14_0^2) + I(L_16_1_n7^2) + L_14_0 +L_16_0, data = df)\r\nsummary(model_oil_lm)\r\n#ANOVA to compare AICs of mixed-effects vs. fixed-effects only\r\nanova(model_oil_lmer, model_oil_lm)\r\n#coefficient of determination\r\nr.squaredGLMM(model_oil_lmer)\r\n#graphical test of linearity\r\nplot(model_oil_lmer)\r\n#graphical test of normality\r\nqqnorm(residuals(model_oil_lmer))\r\nqqline(residuals(model_oil_lmer))\r\n#print random effects by year\r\nranef(model_oil_lmer)\r\n#identification of influential observations\r\ninf_model_oil_lm <- influence.measures(model_oil_lm)\r\nsummary(inf_model_oil_lm)\r\n\r\n#Levene test for homogeneity\r\nres_model_oil_lmer <- residuals(model_oil_lmer)\r\nres_sq_model_oil_lmer <- I(res_model_oil_lmer^2)\r\nLevene_model_oil <- lm(res_sq_model_oil_lmer ~ I(L_20_5_n3^2) + I(L_14_0^2) + I(L_16_1_n7^2) + L_14_0 +L_16_0, data = df)\r\nanova(Levene_model_oil)\r\n\r\n#Breusch-Pagan test for heterskedacity\r\nbptest(Mean_price ~ I(L_20_5_n3^2) + I(L_14_0^2) + I(L_16_1_n7^2) + L_14_0 +L_16_0, studentize = TRUE, data=df)\r\n\r\n\r\n\r\n']","Data and code for: Microalgae-blend tilapia feed eliminates fishmeal and fish oil, improves growth, and is cost viable Aquafeed manufacturers have reduced, but not fully eliminated, fishmeal and fish oil and are seeking cost competitive replacements. We combined two commercially available microalgae, to produce a high-performing fish-free feed for Nile tilapia (Oreochromis niloticus) the world's second largest group of farmed fish. We substituted protein-rich defatted biomass of Nannochloropsis oculata (leftover after oil extraction for nutraceuticals) for fishmeal and whole cells of docosahexaenoic acid (DHA)-rich Schizochytrium sp. as substitute for fish oil. Here, we provide the datasets and code that we used to estimate the price of fish-free experimental and reference diets of tilpia in the Scientific Reports manuscript entitled, ""Microalgae-blend tilapia feed eliminates fishmeal and fish oil, improves growth, and is cost viable"". We include the Rstudio and supporting .csv files for a hedonic analysis of defatted N. oculata meal and whole-cell Schizochytrium sp., non-parametric bootstraps to estimate the median and 95% confidence intervals of commodity and market prices for the formulated tilapia feed ingredients, and for Fig. 2 in the manuscript.",1
An escape theory model for directionally moving prey and an experimental test in juvenile Chinook salmon,"Prey evaluate risk and make decisions based on the balance between the costs of predation and those of engaging in antipredator behavior. Economic escape theory has been valuable in understanding responses of stationary prey under predation risk; however, current models are not applicable for directionally moving prey.Here we present an extension of existing escape theory that predicts how much predation risk is perceived by directionally moving prey. Perceived risk is measured by the extent antipredator behavior causes a change in travel speed (the distance to a destination divided by the total time to reach that destination). Cryptic or cautious antipredator behavior slows travel speed, while prey may also speed up to reduce predator-prey overlap. Next, we applied the sensitization hypothesis to our model, which predicts that prey with more predator experience should engage in more antipredator behavior, which leads to a larger change in travel speed under predation risk. We then compared the qualitative predictions of our model to the results of a behavioral assay with juvenile Chinook salmon (Oncorhynchus tshawytscha) that varied in their past predator experience.We timed salmon swimming downstream through a mesh enclosure in the river with and without predator cues present to measure their reaction to a predator. Hatchery salmon had the least predator experience, followed by wild salmon captured upstream (wild-upstream) and wild-salmon captured downstream (wild-downstream).Both wild salmon groups slowed down in response to predator cues, while hatchery salmon did not change travel speed. The magnitude of reaction to predator cues by salmon group followed the gradient of previous predator experience, supporting the sensitization hypothesis.Moving animals are conspicuous and vulnerable to predators. Here we provide a novel conceptual framework for understanding how directionally moving prey perceive risk and make antipredator decisions. Our study extends the scope of economic escape theory and improves general understanding of non-lethal effects of predators on moving prey.","['###   BEHAVIORAL ASSAY ANALYSIS\r\n###   HOW DOES PREDATION RISK AFFECT JUVENILE SALMON TRAVEL SPEED?\r\n###   Authors: M.C. Sabal, J.E. Merz, S.H. Alonzo, E.P. Palkovacs\r\n\r\n##################################################################\r\n\r\n\r\n# load required packages\r\nlibrary(coxme); library(car); library(lsmeans); library(effsize); library(plyr); \r\nlibrary(ggplot2); library(chron)\r\n\r\n\r\n# load data for behavioral assay analysis - 63 salmon x 2 trials x 2 splits = 252\r\nassay <- read.table(""sabal_et_al_2020_behavioral_assay_data.txt"", header = TRUE, sep = """")\r\nassay$time_start <-times(assay$time_start) #make times into time format.\r\n\r\nenvi_dat<-read.table(""sabal_et_al_2020_environmental_data.txt"", header = TRUE, sep ="""")\r\n\r\n\r\n#### ANALYSIS ####\r\n## mixed-effects cox model\r\ncoxme_mod<-coxme(Surv((assay$time.secs), assay$censor) ~ salmon_group + pred_treat + salmon_group:pred_treat + split + salmon_group:split + split:pred_treat + (1|fish_no), data=assay)\r\n\r\n## ANOVA - Table #1\r\nAnova(coxme_mod, type=""III"")\r\n\r\n## Linear contrasts - Table #2\r\n# contrasts: for each salmon group, does predator treatment affect speed?\r\nlsmeans(coxme_mod, pairwise ~ pred_treat | salmon_group)\r\n\r\n# contrasts: for each salmon group, does split affect speed?\r\n#lsmeans(coxme_mod, pairwise ~ split | salmon_group)\r\n\r\n# contrasts: for each salmon group, does speed differ?\r\n#lsmeans(coxme_mod, pairwise ~ salmon_group)\r\n\r\n\r\n\r\n## Effect sizes - Table #2\r\n#salmon group and predator treatment\r\nhatch_pred<-assay[assay$salmon_group == ""hatchery"" & assay$pred_treat == ""pred"", c(""speed.ms"")]\r\nhatch_nopred<-assay[assay$salmon_group == ""hatchery"" & assay$pred_treat == ""no_pred"", c(""speed.ms"")]\r\ncohen.d(hatch_pred, hatch_nopred, paired=FALSE, na.rm=TRUE, hedges.correction = TRUE)\r\n\r\nwildup_pred<-assay[assay$salmon_group == ""wild-upstream"" & assay$pred_treat == ""pred"", c(""speed.ms"")]\r\nwildup_nopred<-assay[assay$salmon_group == ""wild-upstream"" & assay$pred_treat == ""no_pred"", c(""speed.ms"")]\r\ncohen.d(wildup_pred, wildup_nopred, paired=FALSE, na.rm=TRUE, hedges.correction = TRUE)\r\n\r\nwilddown_pred<-assay[assay$salmon_group == ""wild-downstream"" & assay$pred_treat == ""pred"", c(""speed.ms"")]\r\nwilddown_nopred<-assay[assay$salmon_group == ""wild-downstream"" & assay$pred_treat == ""no_pred"", c(""speed.ms"")]\r\ncohen.d(wilddown_pred, wilddown_nopred, paired=FALSE, na.rm=TRUE, hedges.correction = TRUE)\r\n\r\n#salmon group and split\r\n#hatch12<-assay[assay$salmon_group == ""hatchery"" & assay$split == ""A1_A2"", c(""speed.ms"")]\r\n#hatch23<-assay[assay$salmon_group == ""hatchery"" & assay$split == ""A2_A3"", c(""speed.ms"")]\r\n#cohen.d(hatch12, hatch23, paired=FALSE, na.rm=TRUE, hedges.correction = TRUE)\r\n\r\n#wildup12<-assay[assay$salmon_group == ""wild-upstream"" & assay$split == ""A1_A2"", c(""speed.ms"")]\r\n#wildup23<-assay[assay$salmon_group == ""wild-upstream"" & assay$split == ""A2_A3"", c(""speed.ms"")]\r\n#cohen.d(wildup12, wildup23, paired=FALSE, na.rm=TRUE, hedges.correction = TRUE)\r\n\r\n#wilddown12<-assay[assay$salmon_group == ""wild-downstream"" & assay$split == ""A1_A2"", c(""speed.ms"")]\r\n#wilddown23<-assay[assay$salmon_group == ""wild-downstream"" & assay$split == ""A2_A3"", c(""speed.ms"")]\r\n#cohen.d(wilddown12, wilddown23, paired=FALSE, na.rm=TRUE, hedges.correction = TRUE)\r\n\r\n\r\n\r\n## Non-target variables on reaction of salmon to predation risk.\r\n# In the tests below, we are interested if there is a significant \r\n# interaction between the target variable and predator treatment on speed.\r\n# In this case, we are not interested in main effects as these are \r\n# captured in the main analysis.\r\n\r\n# time of day\r\ntimeofday_mod<-lm(log(speed.ms) ~ time_start*pred_treat, data=assay)\r\nsummary(timeofday_mod)\r\n\r\n# order\r\norder_mod<-lm(log(speed.ms) ~ order*pred_treat, data=assay)\r\nsummary(order_mod)\r\n\r\n# time between trials\r\ntimebtw_fun<-function(x){\r\n  out<-max(x$time_start) - min(x$time_start)\r\n  out}  # custom function to calculate the time between predator and no predator trials for an individual salmon.\r\n\r\n# apply function over dataframe to get times for each salmon.\r\nout_timebtw<-ddply(assay, .(fish_no), timebtw_fun); colnames(out_timebtw)[2]<-""time_btw_trials""\r\n\r\n# add back into main dataframe\r\nassay<-join(out_timebtw, assay)\r\n\r\n# analysis\r\ntimebtw_mod<-lm(log(speed.ms) ~ time_btw_trials*pred_treat, data=assay)\r\nsummary(timebtw_mod)\r\n\r\n## Summarize environmental data\r\nmean(envi_dat$airtempC)\r\nsd(envi_dat$airtempC)\r\n\r\nmean(envi_dat$watertempC)\r\nsd(envi_dat$watertempC)\r\n\r\nmean(envi_dat$turbidity, na.rm=T)\r\nsd(envi_dat$turbidity, na.rm=T)\r\n\r\nmean(envi_dat$watervelocity)\r\nsd(envi_dat$watervelocity)\r\n\r\n## Figure #3\r\n# make reaction norm\r\nfmt_dcimals <- function(decimals=0){ function(x) as.character(round(x,decimals)) } # set to two decimal places\r\n\r\nrxndat<-assay[,c(""fish_no"", ""speed.ms"",""split"", ""salmon_group"", ""pred_treat"")]\r\nrxndat<-unique(rxndat)\r\n\r\n### speed by salmon group + split + pred_treat\r\naggr.dat<-aggregate(speed.ms ~ salmon_group + pred_treat + split, data=rxndat, mean)\r\na<-aggregate(speed.ms ~ salmon_group + pred_treat + split, data=rxndat, sd); colnames(a)[4]<-""sd""\r\nb<-aggregate(speed.ms ~ salmon_group + pred_treat + split, data=rxndat, length); colnames(b)[4]<-""n""\r\naggr.dat<-join(aggr.dat, a); aggr.dat<-join(aggr.dat, b)\r\naggr.dat$se<-aggr.dat$sd / sqrt(aggr.dat$n)\r\n\r\naggr.dat$pred_treat<-as.factor(aggr.dat$pred_treat)\r\naggr.dat$split<-as.factor(aggr.dat$split)\r\n\r\nlevels(aggr.dat$pred_treat)<-c(""no predator"", ""predator"")\r\nlevels(aggr.dat$split)<-c(""A1 to A2"", ""A2 to A3"")\r\n\r\nfig3.final<-ggplot(data=subset(aggr.dat, split != ""A1 to A3""), aes(x=pred_treat, y=speed.ms, fill=salmon_group)) +\r\n  geom_rect(xmin = -Inf,xmax = 1.5, ymin = -Inf,ymax = Inf,alpha = 0.3, fill=""gray92"") +\r\n  geom_line(aes(group=salmon_group, color=salmon_group), size=0.5, position=position_dodge(.3)) +\r\n  geom_errorbar(aes(ymax=speed.ms + se, ymin=speed.ms - se, color=salmon_group), width=0, position=position_dodge(.3), size=0.5) +\r\n  geom_point(pch=21, size=1.5, position=position_dodge(.3)) + facet_wrap(~split, ncol=2) +\r\n  scale_fill_manual(values=c(""firebrick3"", ""royalblue"", ""steelblue1"")) + theme_bw() +\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title.x=element_blank()) +\r\n  theme(strip.text.x = element_text(face=""bold"")) +\r\n  theme(strip.background = element_rect(color=""black"", fill=""white"")) +\r\n  scale_color_manual(values=c(""firebrick3"",""royalblue"", ""steelblue1"")) +\r\n  ylab(""Speed (m/s)"") + theme(legend.position = ""bottom"") +\r\n  theme(legend.title = element_blank(), strip.background=element_blank()) +\r\n  theme(axis.text=element_text(size=12), axis.title=element_text(size=12), \r\n        legend.text=element_text(size=10), strip.text = element_text(size=12)) +\r\n  theme(legend.position=c(0.88,0.84), legend.background = element_rect(fill=""transparent"")) +\r\n  ylim(c(0,0.115)) + theme(axis.line = element_line(colour = ""black""), panel.border = element_blank(), panel.background = element_blank())\r\n\r\nfig3.final\r\n', '###   SUPPLEMENTAL ANALYSES\r\n###   Authors: M.C. Sabal, J.E. Merz, S.H. Alonzo, E.P. Palkovacs\r\n\r\n##################################################################\r\n\r\n\r\n# load required packages\r\nlibrary(ggplot2); library(coxme); library(lme4); library(gridExtra); library(car)\r\n\r\n\r\n# load data for behavioral assay analysis - 63 salmon x 2 trials x 2 splits = 252\r\nassay <- read.table(""sabal_et_al_2020_behavioral_assay_data.txt"", header = TRUE, sep = """")\r\n\r\n\r\n#### APPENDIX S1 ####\r\n## Additional analyses on physical traits of salmon groups (hatchery, wild-upstream, wild-downstream).\r\n\r\n# Do salmon traits vary by salmon group?\r\n\r\n# Get dataframe only of individual salmon and their physical traits\r\nsam <- unique(assay[,c(""date"", ""fish_no"", ""salmon_group"", ""FL"", ""WT"", ""Kbcond"", ""ATPresid"")])\r\n\r\n# Check if physical traits correlated. They are  not.\r\ncordat<-sam[,c(""FL"", ""Kbcond"", ""ATPresid"")]\r\ncordat<-na.omit(cordat)\r\ncor(cordat)\r\n\r\n# MANOVA - Do multiple physical traits vary by group? -Yes.\r\nsummary(manova(cbind(FL, ATPresid, Kbcond) ~ salmon_group, data=sam))\r\n\r\n# Subsequent ANOVAs on each trait\r\nsummary(aov(Kbcond ~ salmon_group, data=sam)); TukeyHSD(aov(Kbcond ~ salmon_group, data=sam))\r\nsummary(aov(FL ~ salmon_group, data=sam)); TukeyHSD(aov(FL ~ salmon_group, data=sam))\r\nsummary(aov(ATPresid ~ salmon_group, data=sam)); TukeyHSD(aov(ATPresid ~ salmon_group, data=sam))\r\n\r\n\r\n# Do salmon traits affect salmon\'s reaction to predation risk? - Table S1\r\n# Mixed-effects cox model: look at only interactions of physical traits with predator treatment\r\ncoxme.traits<-coxme(Surv((assay$time.cox), assay$censor) ~ ATPresid*pred_treat + FL*pred_treat + Kbcond*pred_treat + (1|salmon_group/fish_no), data=assay)\r\nAnova(coxme.traits, type=""III"") #Supplemental Table S1\r\n\r\n\r\n\r\n#### APPENDIX S2 ####\r\n\r\n## Calculate speed as body lengths per second (bl/s)\r\n# split length is 0.9144 meters convereted to mm\r\nassay$split.length.mm <- 0.9144 * 1000\r\n\r\n# split length in body lengths for individual salmon\r\nassay$split.length.bl <- assay$split.length.mm / assay$FL\r\n\r\n# speed in bl/s\r\nassay$speed.bls <- assay$split.length.bl / assay$time.secs\r\n\r\n\r\n## Linear mixed-effects model on truncated dataset:\r\n## does salmon speed vary by predator treatment, split, or salmon group?\r\n\r\n### M/S ###\r\n## Mixed-effects cox model (speed m/s)\r\nlmer_mod_ms<-lmer(log(speed.ms) ~ salmon_group + pred_treat + salmon_group:pred_treat + split + salmon_group:split + split:pred_treat + (1|fish_no), data=assay)\r\n\r\n## ANOVA\r\nAnova(lmer_mod_ms, type=""III"") #Supplemental Table S2\r\n\r\n## Linear contrasts - Table S3\r\n# contrasts: for each salmon group, does predator treatment affect speed (m/s)?\r\nlsmeans(lmer_mod_ms, pairwise ~ pred_treat | salmon_group)\r\n\r\n### BL/S ###\r\n## Mixed-effects cox model (speed m/s)\r\nlmer_mod_bls<-lmer(log(speed.bls) ~ salmon_group + pred_treat + salmon_group:pred_treat + split + salmon_group:split + split:pred_treat + (1|fish_no), data=assay)\r\n\r\n## ANOVA\r\nAnova(lmer_mod_bls, type=""III"") #Supplemental Table S2\r\n\r\n## Linear contrasts - Table S3\r\n# contrasts: for each salmon group, does predator treatment affect speed (bl/s)?\r\nlsmeans(lmer_mod_bls, pairwise ~ pred_treat | salmon_group)\r\n\r\n\r\n#### APPENDIX S3 ####\r\n# contrasts: for each salmon group, does speed (m/s) differ? - Table S4\r\nlsmeans(lmer_mod_ms, pairwise ~ salmon_group | split)\r\n\r\n# contrasts: for each salmon group, does speed (bl/s) differ? - Table S4\r\nlsmeans(lmer_mod_bls, pairwise ~ salmon_group | split)\r\n\r\n\r\n#### FIGURES ####\r\n#### Supplemental Figure S2\r\noptions(scipen=999)\r\n\r\n#change order of factor levels from low predator experience to high\r\nassay$salmon_group<-factor(assay$salmon_group, levels=c(""hatchery"", ""wild-upstream"", ""wild-downstream""))\r\n\r\n\r\n#Body Condition plot\r\nKbcond.plot<-ggplot(data=assay, aes(x=salmon_group, y=Kbcond, fill=salmon_group)) + geom_boxplot(size=0.5, width=0.8, outlier.shape=NA) +\r\n  scale_fill_manual(values=c(""firebrick3"", ""steelblue1"", ""royalblue"")) + theme_bw() +\r\n  theme(axis.title.x = element_blank()) + theme(legend.position = ""bottom"") +\r\n  theme(axis.text=element_text(size=12), axis.title=element_text(size=12), \r\n        legend.text=element_text(size=12), strip.text = element_text(size=12)) +\r\n  theme(legend.title=element_blank()) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\r\n  theme(legend.position=c(.85, .85)) +  theme(legend.position=""none"") + ylab(""Condition factor (K)"") +\r\n  annotate(""text"", x=c(1, 2, 3), y=c(0.00142, 0.00142, 0.00142), label = c(""a"", ""b"", ""b""), size=4) +\r\n  annotate(""text"", x=0.7, y=0.0014, label=""A"", fontface=""bold"", size=6) +\r\n  theme(axis.line = element_line(colour = ""black""),panel.border = element_blank(), panel.background = element_blank()) \r\nKbcond.plot\r\n\r\n#ATPase plot\r\natp.plot<-ggplot(data=assay, aes(x=salmon_group, y=ATPresid, fill=salmon_group)) + geom_boxplot(size=0.5, width=0.8, outlier.shape=NA) +\r\n  scale_fill_manual(values=c(""firebrick3"", ""steelblue1"", ""royalblue"")) + theme_bw() +\r\n  theme(axis.title.x = element_blank()) + theme(legend.position = ""bottom"") +\r\n  theme(axis.text=element_text(size=12), axis.title=element_text(size=12), \r\n        legend.text=element_text(size=12), strip.text = element_text(size=12)) +\r\n  theme(legend.title=element_blank()) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\r\n  theme(legend.position=""none"") + ylab(""ATPase residual"") + ylim(c(-2, 1.4)) +\r\n  annotate(""text"", x=c(1, 2, 3), y=c(1.34, 1.34, 1.34), label = c(""a"", ""a"", ""b""), size=4) +\r\n  annotate(""text"", x=0.65, y=1.2, label=""B"", fontface=""bold"", size=6) +\r\n  theme(axis.line = element_line(colour = ""black""),panel.border = element_blank(), panel.background = element_blank()) \r\natp.plot\r\n\r\nwt.ln.plot<-ggplot(data=assay, aes(x=FL, y=WT)) + geom_point(pch=21, size=2, aes(fill=salmon_group)) + theme_bw() +\r\n  theme(panel.grid.major=element_blank(), panel.grid.minor=element_blank()) + scale_fill_manual(values=c(""firebrick3"", ""steelblue1"", ""royalblue"")) +\r\n  theme(legend.title=element_blank()) + theme(legend.position=c(.7,.2)) +\r\n  ylab(""Weight (g)"") + xlab(""Fork length (mm)"") +\r\n  theme(axis.text=element_text(size=12), axis.title=element_text(size=12), \r\n        legend.text=element_text(size=10)) +  theme(legend.position=""none"") +\r\n  geom_smooth(method=\'loess\', color=""black"", alpha=0.2, size=0.5) +\r\n  annotate(""text"", x=60, y=15.5, label=""C"", fontface=""bold"", size=6) +\r\n  theme(legend.position=c(.35, .8), legend.background = element_rect(colour = NA)) +\r\n  theme(axis.line = element_line(colour = ""black""),panel.border = element_blank(), panel.background = element_blank()) +\r\n  annotate(""text"", x=95, y=3, label=""all pair-wise  groups\\nsignificantly different"", size=3.5)\r\nwt.ln.plot\r\n\r\n# Supplemental Figure S2\r\ngrid.arrange(Kbcond.plot, atp.plot, wt.ln.plot, ncol=2)\r\n\r\n\r\n# Supplemental Figure S3\r\n# make reaction norm\r\nfmt_dcimals <- function(decimals=0){ function(x) as.character(round(x,decimals)) } # set to two decimal places\r\n\r\nrxndat<-assay[,c(""fish_no"", ""speed.bls"",""split"", ""salmon_group"", ""pred_treat"")]\r\nrxndat<-unique(rxndat)\r\n\r\n### speed by salmon group + split + pred_treat\r\naggr.dat<-aggregate(speed.bls ~ salmon_group + pred_treat + split, data=rxndat, mean)\r\na<-aggregate(speed.bls ~ salmon_group + pred_treat + split, data=rxndat, sd); colnames(a)[4]<-""sd""\r\nb<-aggregate(speed.bls ~ salmon_group + pred_treat + split, data=rxndat, length); colnames(b)[4]<-""n""\r\naggr.dat<-join(aggr.dat, a); aggr.dat<-join(aggr.dat, b)\r\naggr.dat$se<-aggr.dat$sd / sqrt(aggr.dat$n)\r\n\r\naggr.dat$pred_treat<-as.factor(aggr.dat$pred_treat)\r\naggr.dat$split<-as.factor(aggr.dat$split)\r\n\r\nlevels(aggr.dat$pred_treat)<-c(""no predator"", ""predator"")\r\nlevels(aggr.dat$split)<-c(""A1 to A2"", ""A2 to A3"")\r\n\r\nfigS3<-ggplot(data=subset(aggr.dat, split != ""A1 to A3""), aes(x=pred_treat, y=speed.bls, fill=salmon_group)) +\r\n  geom_rect(xmin = -Inf,xmax = 1.5, ymin = -Inf,ymax = Inf,alpha = 0.3, fill=""gray92"") +\r\n  geom_line(aes(group=salmon_group, color=salmon_group), size=0.5, position=position_dodge(.3)) +\r\n  geom_errorbar(aes(ymax=speed.bls + se, ymin=speed.bls - se, color=salmon_group), width=0, position=position_dodge(.3), size=0.5) +\r\n  geom_point(pch=21, size=1.5, position=position_dodge(.3)) + facet_wrap(~split, ncol=2) +\r\n  scale_fill_manual(values=c(""firebrick3"", ""royalblue"", ""steelblue1"")) + theme_bw() +\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.title.x=element_blank()) +\r\n  theme(strip.text.x = element_text(face=""bold"")) +\r\n  theme(strip.background = element_rect(color=""black"", fill=""white"")) +\r\n  scale_color_manual(values=c(""firebrick3"",""royalblue"", ""steelblue1"")) +\r\n  ylab(""Speed (bl/s)"") + theme(legend.position = ""bottom"") +\r\n  theme(legend.title = element_blank(), strip.background=element_blank()) +\r\n  theme(axis.text=element_text(size=12), axis.title=element_text(size=12), \r\n        legend.text=element_text(size=10), strip.text = element_text(size=12)) +\r\n  theme(legend.position=c(0.88,0.84), legend.background = element_rect(fill=""transparent"")) +\r\n  ylim(c(0,1.3)) + theme(axis.line = element_line(colour = ""black""), panel.border = element_blank(), panel.background = element_blank())\r\n\r\nfigS3\r\n']","An escape theory model for directionally moving prey and an experimental test in juvenile Chinook salmon Prey evaluate risk and make decisions based on the balance between the costs of predation and those of engaging in antipredator behavior. Economic escape theory has been valuable in understanding responses of stationary prey under predation risk; however, current models are not applicable for directionally moving prey.Here we present an extension of existing escape theory that predicts how much predation risk is perceived by directionally moving prey. Perceived risk is measured by the extent antipredator behavior causes a change in travel speed (the distance to a destination divided by the total time to reach that destination). Cryptic or cautious antipredator behavior slows travel speed, while prey may also speed up to reduce predator-prey overlap. Next, we applied the sensitization hypothesis to our model, which predicts that prey with more predator experience should engage in more antipredator behavior, which leads to a larger change in travel speed under predation risk. We then compared the qualitative predictions of our model to the results of a behavioral assay with juvenile Chinook salmon (Oncorhynchus tshawytscha) that varied in their past predator experience.We timed salmon swimming downstream through a mesh enclosure in the river with and without predator cues present to measure their reaction to a predator. Hatchery salmon had the least predator experience, followed by wild salmon captured upstream (wild-upstream) and wild-salmon captured downstream (wild-downstream).Both wild salmon groups slowed down in response to predator cues, while hatchery salmon did not change travel speed. The magnitude of reaction to predator cues by salmon group followed the gradient of previous predator experience, supporting the sensitization hypothesis.Moving animals are conspicuous and vulnerable to predators. Here we provide a novel conceptual framework for understanding how directionally moving prey perceive risk and make antipredator decisions. Our study extends the scope of economic escape theory and improves general understanding of non-lethal effects of predators on moving prey.",1
The unique spatial ecology of human hunters,"Human hunters are described as 'superpredators' with a unique ecology. Chronic Wasting Disease among cervids and African swine fever among wild boar are emerging wildlife diseases in Europe with huge economic and cultural repercussions. Understanding hunter movements at broad scales has implications for how to control their spread. Here we show, based on the analysis of the settlement patterns and movements of reindeer (n = 9,685), red deer (n = 47,845), moose (n = 60,365), and roe deer (n = 42,530) hunters from across Norway (2001-2017), that hunter density was more closely linked to human density than prey density, that hunters were largely migratory, aggregated with increasing regional prey densities and often used dogs. Hunter movements extended across Europe and to other continents. Our results provide extensive evidence that the broad-scale movements and residency patterns of post-industrial hunters relative to their prey differ from those of large carnivores.","['#####################################\r\n#Read data                        ###\r\n#####################################\r\n#Spatial data set\r\ndatS<-read.table(file=""datS.txt"",sep=""\\t"",header=T)\r\nnames(datS)\r\n\r\n#Adjacency matrix for spatial INLA model\r\nadj_red<-read.table(file=""adj.red.txt"",sep=""\\t"")\r\nrownames(adj_red)<-as.character(seq(1,424,1))\r\n\r\n#Make log-density data for PCA\r\nsd<-cbind(datS[,c(""Region"",""fylkenr"",""idnr"",""densMoose16"",""densRed16"",""densRoe16"",""densreinhunters"",""densmoosehunters""         \r\n                   ,""densredhunters"" ,""densroehunters"",""densHuman17"" ,""kdensrein17"")])   \r\n\r\nsd$logdenshumans<-log(sd$densHuman17)\r\nsd$logdensMoose<-log(sd$densMoose16+0.001)\r\nsd$logdensRed<-log(sd$densRed16+0.001)\r\nsd$logdensRoe<-log(sd$densRoe16+0.01)\r\nsd$logdensRein<-log(sd$kdensrein17+0.00001)\r\nsd$logreinshunterD<-log(sd$densreinhunter+0.01)\r\nsd$logmoosehunterD<-log(sd$densmoosehunter+0.01)\r\nsd$logredhunterD<-log(sd$densredhunter+0.01)\r\nsd$logroehunterD<-log(sd$densroehunter+0.01)\r\n\r\nsd1<-sd[,c(""Region"",""fylkenr"",""idnr"",""logdenshumans"",""logdensMoose"",""logdensRed"",""logdensRoe"",""logdensRein"",""logreinshunterD"",\r\n           ""logmoosehunterD"",""logredhunterD"",""logroehunterD"")]\r\n\r\n\r\n#############################################################################\r\n### Spatial Analysis                                                      ###\r\n#############################################################################\r\n\r\n######################\r\n#1. PCA analysis\r\n######################\r\nlibrary(ggplot2)\r\nlibrary(ggfortify)\r\nlibrary(ggrepel)\r\n\r\n#Remove northern counties \r\nsd2<-sd1[!(sd1$fylkenr %in% c(""18"",""19"",""20"")),]\r\n\r\n#help(princomp)\r\n#Use data with log-densities\r\nxx<-sd2[,-c(1:3)]\r\nnames(xx)\r\n\r\n#Make shorter names\r\nnames(xx)<-c(""Inhabitants"",""Moose"",""Red_deer"",""Roe_deer"",""Reindeer"",""Reindeer_hunters"",""Moose_hunters"",""Reddeer_hunters"",""Roedeer_hunters"")\r\npc.cr <- princomp(xx, cor = TRUE)\r\nbplot<-autoplot(pc.cr, data = sd2, colour = \'Region\',size=1.5,loadings=T,\r\n                loadings.colour = grey(0.5),\r\n                loadings.label = TRUE, loadings.label.size = 4.0,loadings.label.colour=\'darkblue\',\r\n                loadings.label.repel=T)+ theme_bw()+\r\n  theme(legend.justification=c(0.0,0.0), legend.position=c(0.0,0.0),legend.title=element_blank(),\r\n        legend.background = element_blank(),\r\n        legend.box.background = element_blank(),\r\n        legend.key = element_blank()) +\r\n  scale_color_manual(values=c(""pink"",""snow3"",""lightsalmon"",""lightblue""))\r\n\r\nbplot$layers[[2]]$aes_params$size <- 1.0 #Line thickness\r\nbplot$layers[[2]]$geom_params$arrow$length <- unit(6, units = ""points"")\r\nbplot\r\n\r\n#PCA_Loadings\r\nround(pc.cr$loadings[,1:3],4)\r\n\r\n\r\n###########################\r\n#2. Pairwise correlations\r\n###########################\r\n\r\n#Pairwise correlations\r\n\r\nlibrary(boot)\r\n\r\ndxy <- data.frame(cbind(V1,V2))\r\nBrep = 10000\r\nn=length(V1)\r\n\r\npearson <- function(d,i=c(1:n)){\r\n  d2 <- d[i,]\r\n  return(cor(d2$V1,d2$V2))\r\n}\r\n\r\nspearman <- function(d,i=c(1:n)){\r\n  d2 <- d[i,]\r\n  return(cor(d2$V1,d2$V2,method=""spearman""))\r\n}\r\n\r\n#Pairwise correlation with bootstrap confidence interval (results given in supplementary tables)\r\n\r\n#Example: correlation between log hunter density (red deer) and log red deer density in region West\r\nsdat1<-datS[datS$Region==""West"",]\r\nV1 <- log(sdat1$densredhunter+1)\r\nV2 <- log(sdat1$densRed16+0.01)\r\n\r\nBrep = 10000\r\nn=length(V1)\r\n\r\ndxy <- data.frame(cbind(V1,V2))\r\n\r\nbootcorr <- boot(data=dxy,statistic=pearson,R=Brep)\r\n#bootcorr  \r\nbb<-boot.ci(bootcorr,conf=.95)\r\ncc1<-c(bb[2]$t0,bb[5]$basic[1,c(4,5)])\r\nround(cc1,2) #Pearson correlation coefficient with lower and upper limit of bootstrap CI\r\n\r\nbootcorrS <- boot(data=dxy,statistic=spearman,R=Brep)\r\n#bootcorrS  \r\nsbb<-boot.ci(bootcorrS,conf=.95)\r\nsc1<-c(sbb[2]$t0,sbb[5]$basic[1,c(4,5)])\r\nround(sc1,2) #Spearman correlation coefficient with lower and upper limit of bootstrap CI\r\n\r\n###################################################\r\n#3. Spatial regression models in INLA (BYM model)\r\n###################################################\r\n\r\nrequire(sp)\r\nrequire(rgdal)\r\nrequire(INLA)\r\n\r\nsdat<-datS\r\nadj1<-as.matrix(adj_red, ""dgTMatrix"")\r\n#unique(sdat$idnr) #idnr refers to the corresponding row/column number in adj1 matrix\r\n\r\ng = inla.read.graph(adj1)\r\nsummary(g)\r\n\r\n#Data set without north\r\nsdatR<-sdat[sdat$Region!=""North"",]\r\nsdatR$Region<-factor(sdatR$Region,levels=c(""West"",""South"",""East""))\r\ntable(sdatR$Region)\r\n\r\n#All Norway\r\ntable(sdat$Region)\r\n\r\n#Results for Table 1\r\n\r\n#Offset for density\r\nsdat$E=sdat$totareal\r\nsdatR$E=sdatR$totareal\r\n\r\n#A. Red deer hunters\r\nfbym1.st <- redhunters17 ~  1 +\r\n  scale(log(densHuman17)) +\r\n  scale(log(densRed16+0.01))+\r\n  f(idnr, model = ""bym2"",graph=g)  \r\n\r\nmbym1 = inla(fbym1.st, \r\n             data = sdatR, \r\n             control.inla = list(tolerance = 1e-9), \r\n             control.compute=list(dic=TRUE,waic=TRUE,cpo=TRUE),\r\n             family = ""poisson"",\r\n             offset=log(E))\r\n\r\nsummary(mbym1)\r\n\r\nmodR1<-rbind(summary(mbym1)$fixed[,1:6],\r\n             summary(mbym1)$hyperpar)\r\nmodR1\r\n\r\n#B. Moose hunters\r\nfbymm1.st <- moosehunters17 ~  1 +\r\n  scale(log(densHuman17)) +\r\n  scale(log(densMoose16+0.01))+\r\n  f(idnr, model = ""bym2"",graph=g)  \r\n\r\nmbymm1 = inla(fbymm1.st, \r\n              data = sdat, \r\n              control.inla = list(tolerance = 1e-9), \r\n              control.compute=list(dic=TRUE,waic=TRUE,cpo=TRUE),\r\n              family = ""poisson"",\r\n              offset=log(E))\r\nsummary(mbymm1)\r\n\r\nmodM1<-rbind(summary(mbymm1)$fixed[,1:6],\r\n             summary(mbymm1)$hyperpar)\r\nmodM1\r\n\r\n#C. Reindeer hunters\r\nfbymrr1.st <- reinhunters17 ~  1 +\r\n  scale(log(densHuman17)) +\r\n  scale(log(kdensrein17+0.01))+\r\n  f(idnr, model = ""bym2"",graph=g)  \r\n\r\nmbymrr1 = inla(fbymrr1.st, \r\n               data = sdatR, \r\n               control.inla = list(tolerance = 1e-9), \r\n               control.compute=list(dic=TRUE,waic=TRUE,cpo=TRUE),\r\n               family = ""poisson"",\r\n               offset=log(E))\r\n\r\nsummary(mbymrr1)\r\nmodRr1<-rbind(summary(mbymrr1)$fixed[,1:6],\r\n             summary(mbymrr1)$hyperpar)\r\nmodRr1\r\n\r\n\r\n#D. Roe deer hunters\r\nfbymroe1.st <- roehunters ~  1 +\r\n  scale(log(densHuman17)) +\r\n  scale(log(densRoe16+0.01))+\r\n  f(idnr, model = ""bym2"",graph=g)  \r\n\r\nmbymroe1 = inla(fbymroe1.st, \r\n               data = sdatR, \r\n               control.inla = list(tolerance = 1e-9), \r\n               control.compute=list(dic=TRUE,waic=TRUE,cpo=TRUE),\r\n               family = ""poisson"",\r\n               offset=log(E))\r\n\r\nsummary(mbymroe1)\r\nmodRo1<-rbind(summary(mbymroe1)$fixed[,1:6],\r\n             summary(mbymroe1)$hyperpar)\r\nmodRo1\r\n\r\n#Table 1\r\nmodR1\r\nmodM1\r\nmodRr1\r\nmodRo1\r\n\r\n\r\n#Results for Table 2\r\n\r\n#Offset for incidence\r\nsdatR$E1<-sdatR$humans2017\r\nsdat$E1<-sdat$humans2017\r\n\r\n#A. Red deer hunter incidence\r\nfbym.st <- redhunters17 ~  1 +\r\n  scale(log(densRed16+0.01))+\r\n  f(idnr, model = ""bym2"",graph=g)  \r\n\r\nmbym1i = inla(fbym.st, \r\n             data = sdatR, \r\n             control.inla = list(tolerance = 1e-9), \r\n             control.compute=list(dic=TRUE,waic=TRUE,cpo=TRUE),\r\n             family = ""poisson"",\r\n             offset=log(E1))\r\nsummary(mbym1i)\r\n\r\n\r\nmodRei<-rbind(summary(mbym1i)$fixed[,1:6],\r\n               summary(mbym1i)$hyperpar)\r\nmodRei\r\n\r\n\r\n#B. Moose hunters\r\n#Alternative model formulation with bym (bym2 will not run)\r\nfbymm.st <- moosehunters17 ~  1 +\r\n  scale(log(densMoose16+0.01))*Region+\r\n  f(idnr, model = ""bym"",graph=g)  \r\n\r\nmbymm1i = inla(fbymm.st, \r\n               data = sdat, \r\n               control.inla = list(tolerance = 1e-9), \r\n               control.compute=list(dic=TRUE,waic=TRUE,cpo=TRUE),\r\n               family = ""poisson"",\r\n               offset=log(E1))\r\nsummary(mbymm1i)\r\n\r\n#Vary by region and only spatial effect\r\nfbm.st <- moosehunters17 ~  1 +\r\n  scale(log(densMoose16+0.01))*Region+\r\n  f(idnr, model = ""besag"",graph=g)  \r\n\r\nmbm1i = inla(fbm.st, \r\n               data = sdat, \r\n               control.inla = list(tolerance = 1e-9), \r\n               control.compute=list(dic=TRUE,waic=TRUE,cpo=TRUE),\r\n               family = ""poisson"",\r\n               offset=log(E1))\r\nsummary(mbm1i)\r\n\r\nmbymm1i$dic$dic\r\nmbm1i$dic$dic\r\n#select the region model \r\n\r\nmodM1i<-rbind(summary(mbm1i)$fixed[,1:6],\r\n              summary(mbm1i)$hyperpar)\r\nmodM1i\r\n\r\n\r\n#C. Reindeer hunters\r\nfbymrr.st <- reinhunters17 ~  1 +\r\n  scale(log(kdensrein17+0.01))+\r\n  f(idnr, model = ""bym2"",graph=g)  \r\n\r\nmbymrri = inla(fbymrr.st, \r\n               data = sdatR, \r\n               control.inla = list(tolerance = 1e-9), \r\n               control.compute=list(dic=TRUE,waic=TRUE,cpo=TRUE),\r\n               family = ""poisson"",\r\n               offset=log(E1))\r\n\r\nsummary(mbymrri)\r\nmodRri<-rbind(summary(mbymrri)$fixed[,1:6],\r\n              summary(mbymrri)$hyperpar)\r\nmodRri\r\n\r\n\r\n#D. Roe deer hunters\r\nfbymroe.st <- roehunters17 ~  1 +\r\n  scale(log(densRoe16+0.01))+\r\n  f(idnr, model = ""bym2"",graph=g)  \r\n\r\nmbymroe1i = inla(fbymroe.st, \r\n                data = sdatR, \r\n                control.inla = list(tolerance = 1e-9), \r\n                control.compute=list(dic=TRUE,waic=TRUE,cpo=TRUE),\r\n                family = ""poisson"",\r\n                offset=log(E1))\r\n\r\nsummary(mbymroe1i)\r\n\r\nfbymroe2.st <- roehunters ~  1 +\r\n  scale(log(densRoe16+0.01))*Region +\r\n  f(idnr, model = ""bym2"",graph=g)  \r\n\r\nmbymroe2i = inla(fbymroe2.st, \r\n                 data = sdatR, \r\n                 control.inla = list(tolerance = 1e-9), \r\n                 control.compute=list(dic=TRUE,waic=TRUE,cpo=TRUE),\r\n                 family = ""poisson"",\r\n                 offset=log(E1))\r\n\r\nsummary(mbymroe2i)\r\n\r\nmbymroe1i$dic$dic\r\nmbymroe2i$dic$dic #Better model\r\n\r\n\r\nmodRo2<-rbind(summary(mbymroe2i)$fixed[,1:6],\r\n              summary(mbymroe2i)$hyperpar)\r\nmodRo2\r\n\r\n#Table 2\r\nmodRei\r\nmodM1i\r\nmodRri\r\nmodRo2\r\n\r\n\r\n\r\n', 'library(nlme)\r\nlibrary(MuMIn) #providing function for AICc (AIC corrected for small sample sizes)\r\n\r\ntdat<-read.table(file=""tdat.txt"",header=T,sep=""\\t"")\r\n#table(tdat$Region,tdat$County)\r\n\r\n###########\r\n#West #####\r\n###########\r\nddW<-tdat[tdat$Region==""West"",]\r\nunique(ddW$County)\r\nvf=varIdent(form=~1|County)\r\n\r\n#Delta AIC for table 3\r\nfitW<-gls(R_redhunters~(R_redharvest.West_tl1)+relevel(as.factor(County),ref=""14""),method=""ML"",dat=ddW)\r\nsummary(fitw)\r\nfitW1<-gls(R_redhunters~(R_redharvest.West_tl1)+relevel(as.factor(County),ref=""14""),weights=vf,method=""ML"",dat=ddW)\r\nfitW_w<-gls(R_redhunters~relevel(as.factor(County),ref=""14""),method=""ML"",dat=ddW)\r\nfitW_f<-gls(R_redhunters~(R_redharvest.West_tl1),method=""ML"",dat=ddW)\r\nfitWc<-gls(R_redhunters~(R_redharvest.West_tl1)+relevel(as.factor(County),ref=""14""),\r\n           correlation=corAR1(form=~1|as.factor(County)),method=""ML"",dat=ddW)\r\n\r\nround(AICc(fitW,fitW1,fitW_w,fitW_f,fitWc)-AICc(fitW),1)\r\n\r\n#Model estimates for table 3 by REML\r\ntfitW<-gls(R_redhunters~(R_redharvest.West_tl1)+relevel(as.factor(County),ref=""14""),method=""REML"",dat=ddW)\r\nsummary(tfitW)\r\n\r\n\r\n############\r\n#East ######\r\n############\r\nddE<-tdat[tdat$Region==""East"",]\r\nunique(ddE$County)\r\nvf=varIdent(form=~1|County)\r\n\r\n#Delta AIC for table 3\r\nfitE1<-gls(R_redhunters ~ R_redharvest.West_tl1+R_redharvest.East_tl1+R_moosehunters +as.factor(County),weights=vf,method=""ML"",dat=ddE)\r\nsummary(fit14)\r\nfitE1_Mh<-gls(R_redhunters ~ R_redharvest.West_tl1 +R_redharvest.Region_tl1+as.factor(County),weights=vf,method=""ML"",dat=ddE)\r\nfitE1_W<-gls(R_redhunters ~ R_redharvest.Region_tl1+R_moosehunters+as.factor(County),weights=vf,method=""ML"",dat=ddE)\r\nfitE1_M<-gls(R_redhunters ~ R_redharvest.West_tl1+R_moosehunters +as.factor(County),weights=vf,method=""ML"",dat=ddE)\r\nfitE1_f<-gls(R_redhunters ~ R_redharvest.West_tl1 +R_redharvest.Region_tl1+R_moosehunters,weights=vf,method=""ML"",dat=ddE)\r\nfitE1c<-gls(R_redhunters ~ R_redharvest.West_tl1 +R_redharvest.Region_tl1+R_moosehunters+as.factor(County),weights=vf,\r\n            correlation=corAR1(form=~1|as.factor(County) ),method=""ML"",dat=ddE)\r\n\r\nround(AICc(fitE1,fitE1_W,fitE1_M,fitE1_Mh,fitE1_f,fitE1c)-AICc(fitE1),1)\r\n\r\n#Model estimates for table 3 by REML\r\ntfitE1<-gls(R_redhunters ~ R_redharvest.West_tl1+R_redharvest.Region_tl1+R_moosehunters+as.factor(County),weights=vf,method=""ML"",dat=ddE)\r\nsummary(tfitE1)\r\n\r\n\r\n############\r\n#South #####\r\n############\r\nddS<-tdat[tdat$Region==""South"",]\r\nunique(ddS$County)\r\nvf=varIdent(form=~1|County)\r\n\r\n#Delta AIC for table 3\r\nfitS1<-gls(R_redhunters~(R_redharvest.West_tl1)+ (R_redharvest.Region_tl1)+I(year-2010)+I((year-2010)^2)+as.factor(County),method=""ML"",weights=vf,dat=ddS)\r\nsummary(fitS1)\r\nfitS1_W<-gls(R_redhunters~(R_redharvest.Region_tl1)+I(year-2010)+I((year-2010)^2)+as.factor(County),method=""ML"",weights=vf,dat=ddS)\r\nfitS1_R<-gls(R_redhunters~(R_redharvest.West_tl1)+I(year-2010)+I((year-2010)^2)+as.factor(County),method=""ML"",weights=vf,dat=ddS)\r\nfitS1_Y2<-gls(R_redhunters~(R_redharvest.West_tl1)+ (R_redharvest.Region_tl1)+I(year-2010)+as.factor(County),method=""ML"",weights=vf,dat=ddS)\r\nfitS1_Y<-gls(R_redhunters~(R_redharvest.West_tl1)+ (R_redharvest.Region_tl1)+as.factor(County),method=""ML"",weights=vf,dat=ddS)\r\nfitS1_B<-gls(R_redhunters~(R_redharvest.West_tl1)+ (R_redharvest.Region_tl1)+I(year-2010)+I((year-2010)^2),method=""ML"",weights=vf,dat=ddS)\r\nfitS1c<-gls(R_redhunters~(R_redharvest.West_tl1)+ (R_redharvest.Region_tl1)+I(year-2010)+I((year-2010)^2)+as.factor(County),\r\n            method=""ML"",correlation=corAR1(form=~1|as.factor(County)),weights=vf,dat=ddS)\r\n\r\nround(AICc(fitS1,fitS1_W,fitS1_R,fitS1_Y,fitS1_Y2,fitS1_B,fitS1c)-AICc(fitS1),1)\r\n\r\n#Model estimates for table 3 by REML\r\ntfitS1<-gls(R_redhunters~(R_redharvest.West_tl1)+ (R_redharvest.Region_tl1)+I(year-2010)+I((year-2010)^2)+as.factor(County),method=""REML"",weights=vf,dat=ddS)\r\nsummary(tfitS1)\r\n\r\n\r\n############\r\n#North######\r\n############\r\nddN<-tdat[tdat$Region==""Mid"",]\r\nunique(ddN$County)\r\nvf1=varIdent(form=~1|County)\r\n\r\n#Delta AIC for table 3\r\nfitN1<-gls(R_redhunters~(R_redharvest.West_tl1)+ (R_redharvest.Region_tl1)+factor(County),weights=vf1,method=""ML"",dat=ddN)\r\nsummary(fitN1)\r\nfitN1_W<-gls(R_redhunters~(R_redharvest.Region_tl1)+factor(County),method=""ML"",weights=vf1,dat=ddN)\r\nfitN1_R<-gls(R_redhunters~(R_redharvest.West_tl1)+factor(County),method=""ML"",weights=vf1,dat=ddN)\r\nfitN1_f<-gls(R_redhunters~(R_redharvest.West_tl1)+ (R_redharvest.Region_tl1),method=""ML"",weights=vf1,dat=ddN)\r\nfitN1c<-gls(R_redhunters~(R_redharvest.West_tl1)+ (R_redharvest.Region_tl1)+factor(County),weights=vf1,\r\n            correlation=corAR1(form=~1|as.factor(County)),method=""ML"",dat=ddN)\r\n\r\nround(AICc(fitN1,fitN1_W,fitN1_R,fitN1_f,fitN1c)-AICc(fitN1),1)\r\n\r\n#Model estimates for table 3 by REML\r\ntfitN1<-gls(R_redhunters~(R_redharvest.West_tl1)+ (R_redharvest.Region_tl1)+factor(County),weights=vf1,method=""REML"",dat=ddN)\r\nsummary(tfitN1)\r\n\r\n###########\r\n']","The unique spatial ecology of human hunters Human hunters are described as 'superpredators' with a unique ecology. Chronic Wasting Disease among cervids and African swine fever among wild boar are emerging wildlife diseases in Europe with huge economic and cultural repercussions. Understanding hunter movements at broad scales has implications for how to control their spread. Here we show, based on the analysis of the settlement patterns and movements of reindeer (n = 9,685), red deer (n = 47,845), moose (n = 60,365), and roe deer (n = 42,530) hunters from across Norway (2001-2017), that hunter density was more closely linked to human density than prey density, that hunters were largely migratory, aggregated with increasing regional prey densities and often used dogs. Hunter movements extended across Europe and to other continents. Our results provide extensive evidence that the broad-scale movements and residency patterns of post-industrial hunters relative to their prey differ from those of large carnivores.",1
"Workflow supplement to ""Aromatics and agriculture: A spatial approach to long-distance trade and the local economy of the Nabataeans""","This R script contains the workflow used to calculate the total amount of runoff water available in the territories around points. It was used in Weaverdyck, E. J. S. forthcoming. ""Aromatics and agriculture: A spatial approach to long-distance trade and the local economy of the Nabataeans."" In S. von Reden (ed.). Handbook of Ancient Afro-Eurasian Economies, vol. 3. Oldenbourg: De Gruyter to analyze the locations of Nabataean agricultural sites.The point data and rasters used in the R script are also provided. Data sources are provided in the chapter","['# Runoff_Total\r\n# This workflow calculates the total amount of runoff available within a given\r\n# radius of a point, as described in ""\r\n# Aromatics and agriculture: A spatial approach to long-distance trade and the \r\n# local economy of the Nabataeans.""\r\n\r\n# You will need to install the sf, raster, and parallel packages from CRAN\r\n# and the flowdem package from https://github.com/KennethTM/flowdem\r\n\r\n# The data used in ""Aromanics and agriculture"" has been uploaded to Zenodo\r\n# along with this script\r\n\r\n# Set your working directory and load the required packages\r\n\r\nsetwd() # enter the path of the directory to which you downloaded the data\r\nlibrary(sf)\r\nlibrary(raster)\r\nlibrary(flowdem)\r\nlibrary(parallel)\r\n\r\n# Read the data into R\r\nall_points<-sf::st_read(""all_points.geojson"")\r\nflow.acc<-raster::raster(""flow.acc.tif"")\r\nflow.dir<-raster::raster(""flow.dir.tif"")\r\n\r\n# The WaterAvailable function calculates the total amount of runoff water\r\n# available within a circular territory of a given radius around a single point,\r\n# using flow accumulation and flow direction rasters.\r\n# The function isolates all cells in the flow accumulation raster that fall\r\n# within the territory using a mask, finds the maximum value and stores it.\r\n# It then establishes a \'pour point\' at the location of\r\n# the cell with the maximum value and uses the flow direction raster to \r\n# delineate the watershed, encompassing all cells flowing into that cell.\r\n# It then erases these cells from the masked flow accumulation raster.\r\n# At this point, the algorithm repeats, finding the next maximum value and\r\n# adding it to the previous maximum value. It delineates the watershed flowing\r\n# into the cell with that maximum value and erases these cells. The process\r\n# repeats until the maximum value increases the total water by less than 1% and\r\n# over 95% of the territory has been accounted for. These limits drastically\r\n# reduce processing time and have a negligeable effect on the outcome.\r\n# NB: The flow direction raster has to be an integer raster with values between\r\n# 1 and 8. The value of a cell [*] reflects the direction of flow out of a cell\r\n# according to the following schema:\r\n#     234\r\n#     1*5\r\n#     876\r\n# So if a cell\'s lowest neighbor is found to the upper-right, it will have a\r\n# value of 2.\r\n\r\nWaterAvailable <- function(flow.acc, flow.dir, point, radius){\r\n  # Make sure that the values of the flow.dir raster are stored as integers\r\n  storage.mode(flow.dir[])<-""integer""\r\n  # Buffer the point to define the territory\r\n  buff<-as(sf::st_buffer(point, radius), ""Spatial"")\r\n  # Use the buffer to crop and mask the flow accumulation raster\r\n  faclip<-raster::mask(raster::crop(flow.acc, buff), buff)\r\n  # Count the number of cells in the territory\r\n  total.cells<-raster::cellStats(!is.na(faclip), stat = sum)\r\n  # Define a total.water variable to store the results\r\n  total.water = 0\r\n  # Count how many cells remain to be analyzed\r\n  remaining.cells<-raster::cellStats(!is.na(faclip), stat = sum)\r\n  # Define an addition variable to keep track of how much each iteration\r\n  # increases the total\r\n  addition <- 1\r\n  # The while statement stops the algorithm from analyzing the entire territory\r\n  while (remaining.cells/total.cells > 0.05 | addition > 0.01){\r\n    print(paste(""Portion of territory remaining:"", remaining.cells/total.cells))\r\n    print(paste(""total water:"", total.water))\r\n    # Find the maximum value in the masked flow accumulation raster\r\n    maxval<-raster::cellStats(x=faclip, stat=max, na.rm=TRUE)\r\n    print(paste(""Maximum value remaining:"", maxval))\r\n    # Add the maximum value to the total.water variable\r\n    total.water <- total.water + maxval\r\n    # Calculate how much of the total.water variable was added by this iteration\r\n    addition<-maxval/total.water\r\n    print(paste (""portion of total added:"",addition))\r\n    # Establish a pour point in the cell with the maximum value\r\n    maxpt<-raster::rasterToPoints(x = faclip, fun = function(x){x == maxval},\r\n                                  spatial = TRUE)\r\n    # Delineate the watershed of cells flowing into that point\r\n    ws<-flowdem::watershed(dirs = as.integer(flow.dir), target = maxpt)\r\n    # Include the cell with the pour point in the watershed\r\n    ws[maxpt]<-1\r\n    # Erase the cells in the watershed from the masked flow accumulation raster\r\n    faclip<-raster::mask(faclip, raster::crop(ws, faclip), inverse = TRUE)\r\n    # Count the number of remaining cells\r\n    remaining.cells<-raster::cellStats(!is.na(faclip), stat = sum)\r\n    # If there are no remaining cells, stop iterating\r\n    if (remaining.cells == 0){\r\n      break\r\n    }\r\n  }\r\n  # Remove all unused data from memory\r\n  gc()\r\n  return(total.water)\r\n}\r\n\r\n# Prepare to apply the WaterAvailable function to every point in a dataset using\r\n# parallel processing on all cores\r\nn.cores<- parallel::detectCores()\r\nclust<-parallel::makeCluster(n.cores)\r\n\r\n# Apply the WaterAvailable function to every point, keeping track of how long it\r\n# takes. You may want to subdivide your points and run the analysis in batches\r\n\r\n# Define a radius\r\nradius = 500\r\n# Define the points to be analyzed\r\npoint<-all_points[1:5000,]\r\n# Run the following block all at once to keep track of how long it takes.\r\n# This will allow you to determine a reasonable batch size\r\nstart.par<-Sys.time()\r\nrunoff_1_5000<-parallel::parLapply(\r\n  cl = clust,\r\n  st_geometry(point),\r\n  WaterAvailable, flow.acc = flow.acc, flow.dir = flow.dir, radius = radius)\r\nend.par<-Sys.time()\r\nprint(end.par - start.par)\r\ngc()\r\n\r\n# Repeat the process with the next set of points\r\npoint<-all_points[5001:10351,]\r\nstart.par<-Sys.time()\r\nrunoff_5001_10351<-parallel::parLapply(\r\n  cl = clust,\r\n  st_geometry(point),\r\n  WaterAvailable, flow.acc = flow.acc, flow.dir = flow.dir, radius = radius)\r\nend.par<-Sys.time()\r\nprint(end.par - start.par)\r\ngc()\r\n\r\n# Combine the resulting lists into a single vector and append it to the points\r\nrunoff<-c(unlist(runoff_1_5000, runoff_5001_10351))\r\nall_points$runoff <- runoff\r\n\r\n# Write the results to the working directory\r\nsf::st_write(all_points, ""all_points_runoff.geojson"", driver = ""GeoJSON"")']","Workflow supplement to ""Aromatics and agriculture: A spatial approach to long-distance trade and the local economy of the Nabataeans"" This R script contains the workflow used to calculate the total amount of runoff water available in the territories around points. It was used in Weaverdyck, E. J. S. forthcoming. ""Aromatics and agriculture: A spatial approach to long-distance trade and the local economy of the Nabataeans."" In S. von Reden (ed.). Handbook of Ancient Afro-Eurasian Economies, vol. 3. Oldenbourg: De Gruyter to analyze the locations of Nabataean agricultural sites.The point data and rasters used in the R script are also provided. Data sources are provided in the chapter",1
Data from: Scaling functional traits to ecosystem processes: towards a mechanistic understanding in peat mosses,"1. The role of trait trade-offs and environmental filtering in explaining the variability of functional traits and ecosystem processes has received considerable attention for vascular plants but less so for bryophytes. Thus, we do not know whether the same forces also shape the phenotypic variability of bryophytes. Here we assess how environmental gradients and trade-offs shape functional traits and subsequently ecosystem processes for peat mosses (Sphagnum), a globally important plant genus for carbon accumulation. We used piecewise Structural Equation Modeling (SEM) to understand how environmental gradients influence vital processes across levels of biological organization. 2. We gathered data on functional traits for 15 globally important Sphagnum species covering a wide range of ecological preferences. Phenotypes lie along well-established axes of the plant economic spectrum characterizing trade-offs between vital physiological functions. Using SEM we clarified the mechanisms of trait covariation and scaling to ecosystem processes. We tested whether peat mosses, like vascular plants, constrain trait variability between a fast turnover strategy based on resource acquisition via fast traits and processes, and a strategy of resource conservation, via slow traits and processes. 3. We parameterized a process-based model estimating ecosystem processes linking environmental drivers with architectural and functional traits. In our SEM approach the amount of variance explained varied substantially (0.29  R2  0.82) among traits and processes in Sphagnum, and the model could predict some of them with high to intermediate accuracy for an independent dataset. R2 variability was mainly explained by traits and species identity, and poorly by environmental filtering. 4. Some Sphagnum species avoid the stress caused by periodic desiccation in hollows via resource acquisition based on fast photosynthesis and growth, while other species are adapted to grow high above the water table on hummocks by slow physiological traits and processes to conserve resources. 5. Synthesis. We contribute to a unified theory generating individual fitness, canopy dynamics and ecosystem processes from trait variation. As for vascular plants, the functional traits in the Sphagnum economic spectrum are linked into an integrated phenotypic network partly filtered by the environment and shaped by trade-offs in resource acquisition and conservation.","['\r\n### Fitting Piecewise SEM for 15 Sphagnum species\r\n\r\n# Load required libraries for piecewise Structural Equation Modeling \r\n\r\nlibrary(piecewiseSEM)\r\n\r\n# Load required libraries for linear mixed effects models\r\n\r\nlibrary(lme4)\r\nlibrary(nlme)\r\n\r\n### Load standardized data for environmental gradients, traits and processes\r\n\r\nsphagnum = read.csv2(""JEcol-T2.csv"", header=TRUE)\r\n\r\n# Create list of individual-based models corresponding to SEM\r\n\r\n\r\nsphagnum.modlist = list(\r\n\r\nlme(Mcap ~ HWT + shade_HWT, data = sphagnum, random = ~1|Species, na.action = na.omit),\r\nlme(Npi ~ Mcap + LM + Nlit, data = sphagnum, random = ~1|Species, na.action = na.omit),\r\nlme(Gi ~ HWT + Mcap + Npi, data = sphagnum, random = ~1|Species, na.action = na.omit),\r\nlme(L ~ shade + LM, data = sphagnum, random = ~1|Species, na.action = na.omit),\r\nlme(Nlit ~ shade, data = sphagnum, random = ~1|Species, na.action = na.omit)\r\n\r\n)\r\n\r\n\r\n###Run Shipley tests\r\n\r\nsem.fit(sphagnum.modlist, sphagnum)\r\n\r\n###Extract standardized path coefficients\r\n\r\nsem.coefs(sphagnum.modlist, sphagnum, standardize = ""scale"", intercept=TRUE)\r\n\r\n###Get marginal and conditional R2 for individual models\r\n\r\nsem.model.fits(sphagnum.modlist)\r\n\r\n# Create list of area-based models corresponding to SEM \r\n\r\nsphagnum.modlist = list(\r\n\r\nlme(n ~ HWT + shade_HWT, data = sphagnum, random = ~1|Species, na.action = na.omit),\r\nlme(Npa ~ n + LM + Nlit, data = sphagnum, random = ~1|Species, na.action = na.omit),\r\nlme(Ga ~ HWT + n + Npa, data = sphagnum, random = ~1|Species, na.action = na.omit),\r\nlme(L ~ shade + LM, data = sphagnum, random = ~1|Species, na.action = na.omit),\r\nlme(Nlit ~ shade, data = sphagnum, random = ~1|Species, na.action = na.omit)\r\n\r\n)\r\n\r\n# Repeat again Shipleys test, extract path coefficients and get R2 for area-based model \r\n\r\n\r\n### Specification of the R version and platform used\r\n\r\nsessionInfo()\r\n\r\nR version 3.4.3 (2017-11-30)\r\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 15063)\r\n\r\n\r\n']","Data from: Scaling functional traits to ecosystem processes: towards a mechanistic understanding in peat mosses 1. The role of trait trade-offs and environmental filtering in explaining the variability of functional traits and ecosystem processes has received considerable attention for vascular plants but less so for bryophytes. Thus, we do not know whether the same forces also shape the phenotypic variability of bryophytes. Here we assess how environmental gradients and trade-offs shape functional traits and subsequently ecosystem processes for peat mosses (Sphagnum), a globally important plant genus for carbon accumulation. We used piecewise Structural Equation Modeling (SEM) to understand how environmental gradients influence vital processes across levels of biological organization. 2. We gathered data on functional traits for 15 globally important Sphagnum species covering a wide range of ecological preferences. Phenotypes lie along well-established axes of the plant economic spectrum characterizing trade-offs between vital physiological functions. Using SEM we clarified the mechanisms of trait covariation and scaling to ecosystem processes. We tested whether peat mosses, like vascular plants, constrain trait variability between a fast turnover strategy based on resource acquisition via fast traits and processes, and a strategy of resource conservation, via slow traits and processes. 3. We parameterized a process-based model estimating ecosystem processes linking environmental drivers with architectural and functional traits. In our SEM approach the amount of variance explained varied substantially (0.29  R2  0.82) among traits and processes in Sphagnum, and the model could predict some of them with high to intermediate accuracy for an independent dataset. R2 variability was mainly explained by traits and species identity, and poorly by environmental filtering. 4. Some Sphagnum species avoid the stress caused by periodic desiccation in hollows via resource acquisition based on fast photosynthesis and growth, while other species are adapted to grow high above the water table on hummocks by slow physiological traits and processes to conserve resources. 5. Synthesis. We contribute to a unified theory generating individual fitness, canopy dynamics and ecosystem processes from trait variation. As for vascular plants, the functional traits in the Sphagnum economic spectrum are linked into an integrated phenotypic network partly filtered by the environment and shaped by trade-offs in resource acquisition and conservation.",1
"Code and summary statistics for ""Gene-environment correlations across geographic regions affect genome-wide association studies""","This code is used for the analyses in the article ""Gene-environment correlations across geographic regions affect genome-wide association studies"", scheduled for publication in Nature Genetics.The code consists of three parts, which can be distinguished by the start of the filenames:- part_0_extract_regions: these are R-scripts that assign geographic regions (MSOA & Local Authority) to UK Biobank participants- part_I_PRS: these are R-scripts used to conduct the analyses of part I of our paper, in which we analyzed polygenic scores in sibling pairs- part_II_GWAS: these are GWAS commands and R-scripts used to conduct analyses of part II of our paper, in which we run GWASs on 56 complex traits, corrected and uncorrected for geographic regions, and compute the change in heritability and genetic correlations with SES (education and income) after correcting for geographyFurthermore, we have included the GWAS summary statistics of all the GWASs before and after controlling for geographic region in the file LDSC_format_sumstats.zip.Please find the abstract of our article below:Gene-environment correlations affect associations between genetic variants and complex traits in genome-wide association studies (GWASs). Here, we showed in up to 43,516 British siblings that educational attainment polygenic scores capture gene-environment correlations, and that migration extends these gene-environment correlations beyond the family to broader geographic regions. We then ran GWASs on 56 complex traits in up to 254,387 British individuals. Controlling for geographic region significantly decreased the heritability for socio-economic status (SES)-related traits, most strongly for educational attainment and income. For most traits, controlling for region significantly reduced genetic correlations with educational attainment and income, most significantly for BMI/body fat, sedentary behaviour, and substance use, consistent with gene-environment correlations related to regional socio-economic differences. The effects of controlling for birth place and current address suggest both passive and active sources of gene-environment correlations. Our results show that the geographic clustering of DNA and SES introduces gene-environment correlations that affect GWAS results.",,"Code and summary statistics for ""Gene-environment correlations across geographic regions affect genome-wide association studies"" This code is used for the analyses in the article ""Gene-environment correlations across geographic regions affect genome-wide association studies"", scheduled for publication in Nature Genetics.The code consists of three parts, which can be distinguished by the start of the filenames:- part_0_extract_regions: these are R-scripts that assign geographic regions (MSOA & Local Authority) to UK Biobank participants- part_I_PRS: these are R-scripts used to conduct the analyses of part I of our paper, in which we analyzed polygenic scores in sibling pairs- part_II_GWAS: these are GWAS commands and R-scripts used to conduct analyses of part II of our paper, in which we run GWASs on 56 complex traits, corrected and uncorrected for geographic regions, and compute the change in heritability and genetic correlations with SES (education and income) after correcting for geographyFurthermore, we have included the GWAS summary statistics of all the GWASs before and after controlling for geographic region in the file LDSC_format_sumstats.zip.Please find the abstract of our article below:Gene-environment correlations affect associations between genetic variants and complex traits in genome-wide association studies (GWASs). Here, we showed in up to 43,516 British siblings that educational attainment polygenic scores capture gene-environment correlations, and that migration extends these gene-environment correlations beyond the family to broader geographic regions. We then ran GWASs on 56 complex traits in up to 254,387 British individuals. Controlling for geographic region significantly decreased the heritability for socio-economic status (SES)-related traits, most strongly for educational attainment and income. For most traits, controlling for region significantly reduced genetic correlations with educational attainment and income, most significantly for BMI/body fat, sedentary behaviour, and substance use, consistent with gene-environment correlations related to regional socio-economic differences. The effects of controlling for birth place and current address suggest both passive and active sources of gene-environment correlations. Our results show that the geographic clustering of DNA and SES introduces gene-environment correlations that affect GWAS results.",1
Data from: Predicting novel trophic interactions in a non-native world,"Humans are altering the global distributional ranges of plants, while their co-evolved herbivores are frequently left behind. Native herbivores often colonise non-native plants, potentially reducing invasion success or causing economic loss to introduced agricultural crops. We developed a predictive model to forecast novel interactions and verified it with a data set containing hundreds of observed novel plantinsect interactions. Using a food network of 900 native European butterfly and moth species and 1944 native plants, we built an herbivore host-use model. By extrapolating host use from the native herbivoreplant food network, we accurately forecasted the observed novel use of 459 non-native plant species by native herbivores. Patterns that governed herbivore host breadth on co-evolved native plants were equally important in determining non-native hosts. Our results make the forecasting of novel herbivore communities feasible in order to better understand the fate and impact of introduced plants.","['###This is the basic code used in Pearse and Altermatt (2013) Ecol. Lett.  in order to predict the novel host use\r\n### of European moths on non-native host plants.  The code was developed in R version 2.10.1\r\n\r\n\r\n##libraries & functions\r\nlibrary(ape)\r\nlibrary(vegan)\r\nlibrary(ROCR)\r\nse<-function(x) sd(x, na.rm=T)/sqrt(length(x))\r\nK_fold <- function(Nobs,K=5){\r\n    rs <- runif(Nobs)\r\n    id <- seq(Nobs)[order(rs)]\r\n    k <- as.integer(Nobs*seq(1,K-1)/K)\r\n    k <- matrix(c(0,rep(k,each=2),Nobs),ncol=2,byrow=TRUE)\r\n    k[,1] <- k[,1]+1\r\n    l <- lapply(seq.int(K),function(x,k,d) \r\n                list(train=d[!(seq(d) %in% seq(k[x,1],k[x,2]))],\r\n                     test=d[seq(k[x,1],k[x,2])]),k=k,d=id)\r\n   return(l)\r\n}\r\n\r\n##data\r\ntr<-read.tree(""plant_phylo_2010_11_30.tre"")\r\nplant<-read.table(""plant_information.csv"", header=TRUE, as.is=TRUE)\r\n#oldmoth<-read.table(""matrix_species_20101108.txt"",header=TRUE, as.is=TRUE)  ##old data\r\nmoth<-read.table(""matrix_species_20120901.txt"",header=TRUE, as.is=TRUE)\r\nmoth[867,537]<-1\r\nplant$species<-tolower(plant$species)\r\nmothdat<-read.table(""Lepidoptera_information.csv"", header=T, as.is=T)\r\n##culling full plant list to phylogeny.  This is getting rid of ferns\r\nplant<-plant[which(plant$species %in% tr$tip.label),]\r\nall(plant$species %in% tr$tip.label)\r\n##should be TRUE\r\ncolnames(moth)<-tolower(colnames(moth))\r\na<-tolower(colnames(moth)) %in% plant$species\r\nmoth2<-moth[,a]\r\nrownames(moth2)<-moth[,1]\r\nmoth<-moth2\r\n\r\n##adding plants with no moths into matrix\r\neatplant<-plant$species[which(plant$species %in% colnames(moth2))]\r\nnoeatplant<-plant$species[-which(plant$species %in% eatplant)]\r\nlength(noeatplant)\r\n##should be 1756\r\nnullmat<-matrix(0,nrow(moth),length(noeatplant))\r\ncolnames(nullmat)<-noeatplant\r\nmoth<-cbind(moth,nullmat)\r\nall(colnames(moth) %in% tr$tip.label)\r\nall(plant$species %in% colnames(moth))\r\n\r\n###this dataset is now ready for analysis\r\nnative<-plant[-which(plant$status == ""ornamental_plant"" | plant$status ==""neophyte""),]\r\nmoth<-t(moth)\r\nexotic<-plant[which(plant$status == ""ornamental_plant"" | plant$status ==""neophyte""),]\r\n\r\n\r\n###try using k-fold to estimate within-native parameters for logistic model\r\n#separate data set\r\ntrmat<-cophenetic(tr)\r\ntrmat<-trmat/max(trmat)\r\nmothnat<-moth[-which(rownames(moth)%in%exotic$species),]\r\nmothnat[which(mothnat>1)]<-1\r\nmiss<-colnames(mothnat[,which(colSums(mothnat)==0)])\r\nmothnat<-mothnat[,-which(colnames(mothnat)%in%miss)]\r\nplantrange<-plant$raster_frequency_BW\r\nnames(plantrange)<-plant$species\r\nK=5\r\nset.seed(7402356)\r\nkmoth<-K_fold(nrow(mothnat), K=K)\r\nnhost<-apply(mothnat, 2,sum)\r\nglist<-rep(NA, nrow(mothnat))\r\nfor(jjj in 1:nrow(mothnat)){\r\n\tmmm<-rownames(mothnat)[jjj]\r\n\tglist[jjj]<-plant$genus[which(plant$species==mmm)]\r\n}\r\nmothnatgen<-aggregate(x=mothnat, by=list(glist), FUN=mean)\r\nmothnatgen[mothnatgen>0]<-1\r\nmothnatgen<-mothnatgen[,-1]\r\nngenhost<-apply(mothnatgen,2,sum)\r\n#nhost<-ngenhost\r\n\t\r\n\t\r\n\r\n\r\n###run models for each K-fold\r\nmodlist<-list()\r\nmodlist_range<-list()\r\nfor(h in 1:K){\r\n\tprint(paste(""K-fold"", h, ""of"", K))\r\n\tflush.console()\r\n\ttrain<-mothnat[kmoth[[h]]$train,]\r\n\ttrain<-t(train)\r\n\ttrain<-train[which(rowSums(train)>0),]\r\n\ttest<-mothnat[kmoth[[h]]$test,]\r\n\ttest<-t(test)\r\n\ttest<-test[which(rownames(test)%in%rownames(train)),]\r\n\tmdistmat<-matrix(NA, nrow=ncol(test), ncol=nrow(train))\r\n\trownames(mdistmat)<-colnames(test)\r\n\tcolnames(mdistmat)<-rownames(train)\r\n\tprint(""estimating phylo distance"")\r\n\tflush.console()\r\n\tfor(i in 1:nrow(train)){\r\n\t\tmdat<-train[i,]\r\n\t\tm<-row.names(train)[i]\r\n\t\thostpl<-names(mdat[-which(mdat==0)])\r\n\t\thostdat<-trmat[which(row.names(trmat)%in% hostpl),]\r\n\t\tdmat<-matrix(NA, nrow=ncol(test), ncol=length(hostpl))\r\n\t\trow.names(dmat)<-colnames(test)\r\n\t\tif(length(hostpl)==1){\r\n\t\t\tpdist<-hostdat[which(names(hostdat)%in% colnames(test))]\r\n\t\t\tpdist<-pdist[which(duplicated(names(pdist))==F)]\r\n\t\t\tpdist<-pdist[colnames(test)]\r\n\t\t\tdmat[,1]<-pdist\r\n\t\t\t}\r\n\t\tif(length(hostpl)>1){\r\n\t\t\tfor(j in 1:length(hostpl)){\r\n\t\t\t\th1<-hostdat[j,]\r\n\t\t\t\tpdist<-h1[which(names(h1)%in% colnames(test))]\r\n\t\t\t\tpdist<-pdist[which(duplicated(names(pdist))==F)]\r\n\t\t\t\tpdist<-pdist[colnames(test)]\r\n\t\t\t\tdmat[,j]<-pdist\r\n\t\t\t}\r\n\t\t}\r\n\t\tdmin<-apply(dmat, 1, min)\r\n\t\t####Check that this matches up....\r\n\t\tmdistmat[,i]<-dmin\r\n\t}\r\n\tnum_host<-rep(NA, nrow(test)*ncol(test))\r\n\tnum_genhost<-rep(NA, nrow(test)*ncol(test))\r\n\tnhost_cor<-nhost[which(names(nhost)%in%rownames(test))]\r\n\tngenhost_cor<-ngenhost[which(names(ngenhost)%in%rownames(test))]\r\n\tfor(i in 1:length(nhost_cor)){\r\n\t\tnho<-rep(nhost_cor[i], ncol(test))\r\n\t\tnum_host[((i-1)*ncol(test)+1):((i-1)*ncol(test)+ncol(test))]<-nho\r\n\t}\t\r\n\tfor(i in 1:length(ngenhost_cor)){\r\n\t\tnho<-rep(ngenhost_cor[i], ncol(test))\r\n\t\tnum_genhost[((i-1)*ncol(test)+1):((i-1)*ncol(test)+ncol(test))]<-nho\r\n\t}\t\r\n\tmothnam<-rownames(test)\r\n\tmothmoth<-rep(NA, nrow(test)*ncol(test))\r\n\tfor(i in 1:length(mothnam)){\r\n\t\tmnam<-rep(mothnam[i], ncol(test))\r\n\t\tmothmoth[((i-1)*ncol(test)+1):((i-1)*ncol(test)+ncol(test))]<-mnam\r\n\t}\r\n\tplantplant<-rep(colnames(test), nrow(test))\r\n\t\r\n\tlogdat<-data.frame(moth=mothmoth, plant=plantplant, real_int=as.numeric(t(test)), min_phylo=as.numeric(mdistmat), nhost=num_host, ngenhost=num_genhost)\r\n\tlogdat$min_phylo[which(logdat$min_phylo==""Inf"")]<-1000\r\n\tplran<-plantrange[as.character(logdat$plant)]\r\n\tlogdat$BW_range<-plran\r\n\tprint(""running models in high heels"")\r\n\tflush.console()\r\n\tmod1<-glm(real_int ~ min_phylo*nhost, data=logdat, family=""binomial"")\r\n\tmod2<-glm(real_int ~ min_phylo*ngenhost, data=logdat, family=""binomial"")\r\n\t#mod2<-glm(real_int ~ min_phylo*nhost+BW_range*min_phylo+BW_range*nhost, data=logdat, family=""binomial"")\r\n\tmodlist[[h]]<-mod1\r\n\tmodlist_range[[h]]<-mod2\r\n}\r\nlogdat_nat<-logdat\r\n\r\nmodmat<-matrix(NA,nrow=length(modlist), ncol=length(modlist[[1]]$coeff))\r\nmodmatgen<-matrix(NA,nrow=length(modlist_range), ncol=length(modlist_range[[1]]$coeff))\r\n###currently, this is sort of a wierd work around, where I am making an lm object from a previous model, but \r\n###replacing its coefficients with the averaged coefficients from the Kfolds\r\nfor(i in 1:length(modlist)){modmat[i,]<-modlist[[i]]$coeff}\r\nfor(i in 1:length(modlist_range)){modmatgen[i,]<-modlist_range[[i]]$coeff}\r\nAveCoeff<-apply(modmat,2,mean)\r\nAveCoeffgen<-apply(modmatgen,2,mean)\r\nnames(AveCoeff)<-names(modlist[[1]]$coeff)\r\nmodAve<-modlist[[1]]\r\nmodAve$coefficients<-AveCoeff\r\n\r\n###create predictors for non-natives and use the previous model to predict interactions based\r\n###on these values\r\n\r\n##non-native interaction matrix\r\nexint<-moth[which(rownames(moth)%in% exotic$species),]\r\nexint<-exint[which(duplicated(rownames(exint))==F),]\r\nexint[exint>0]<-1\r\nexint<-exint[,-which(colnames(exint)%in%miss)]\r\nmdistmat<-matrix(NA, nrow=nrow(exint), ncol=ncol(exint))\r\nrownames(mdistmat)<-rownames(exint)\r\ncolnames(mdistmat)<-colnames(exint)\r\nmothnat<-t(mothnat)\r\nfor(i in 1:nrow(mothnat)){\r\n\tm<-row.names(mothnat)[i]\r\n\tmdat<-mothnat[i,]\r\n\thostpl<-names(mdat[-which(mdat==0)])\r\n\thostdat<-trmat[which(row.names(trmat)%in% hostpl),]\r\n\tdmat<-matrix(NA, nrow=nrow(exint), ncol=length(hostpl))\r\n\trow.names(dmat)<-rownames(mdistmat)\r\n\t\r\n\tif(length(hostpl)==1){\r\n\t\texdist<-hostdat[which(names(hostdat)%in% rownames(dmat))]\r\n\t\texdist<-exdist[which(duplicated(names(exdist))==F)]\r\n\t\texdist<-exdist[rownames(dmat)]\r\n\t\tdmat[,1]<-exdist\r\n\t}\r\n\tif(length(hostpl)>1){\r\n\t\tfor(j in 1:length(hostpl)){\r\n\t\t\th1<-hostdat[j,]\r\n\t\t\texdist<-h1[which(names(h1)%in% rownames(dmat))]\r\n\t\t\texdist<-exdist[which(duplicated(names(exdist))==F)]\r\n\t\t\texdist<-exdist[rownames(dmat)]\r\n\t\t\tdmat[,j]<-exdist\r\n\t\t}\r\n\t}\r\n\t\r\n\tdmin<-apply(dmat, 1, min)\t\r\n\tdmin<-dmin[rownames(mdistmat)]\r\n\tmdistmat[,i]<-dmin\r\n\t#print(paste(i, ""AND"", m))\r\n}\r\n\r\nnhost<-apply(mothnat, 1,sum)\r\nnum_host<-rep(NA, nrow(exint)*ncol(exint))\r\nfor(i in 1:length(nhost)){\r\n\tnho<-rep(nhost[i], nrow(exint))\r\n\tnum_host[((i-1)*nrow(exint)+1):((i-1)*nrow(exint)+nrow(exint))]<-nho\r\n}\r\n\r\nmothnam<-colnames(exint)\r\nmothmoth<-rep(NA, nrow(exint)*ncol(exint))\r\nfor(i in 1:length(mothnam)){\r\n\tmnam<-rep(mothnam[i], nrow(exint))\r\n\tmothmoth[((i-1)*nrow(exint)+1):((i-1)*nrow(exint)+nrow(exint))]<-mnam\r\n}\r\nplantplant<-rep(rownames(exint), ncol(exint))\r\n\t\r\nlogdat<-data.frame(moth=mothmoth, plant=plantplant, real_int=as.numeric(exint), min_phylo=as.numeric(mdistmat), nhost=num_host)\r\nlogdat$BW_range<-plantrange[logdat$plant]\r\nlogdat_nn<-logdat\r\n###predict the natives model onto the nonnative plants\r\nmodpred<-prediction(predictions=predict(modAve, newdata=logdat), labels=logdat$real_int)\r\nmodperf<-performance(modpred, measure=""tpr"", x.measure=""fpr"")\r\nmodperf_all<-modperf\r\nplot(modperf)\r\nauc<-performance(modpred, measure=""auc"")\r\nauc\r\n# n=1000\r\n# n_auc<-vector()\r\n# for(i in 1:n){\r\n\t# print(i); flush.console()\r\n\t# randat<-logdat\r\n\t# randat$min_phylo<-sample(logdat$min_phylo)\r\n\t# randat$nhost<-sample(logdat$nhost)\r\n\t# n_modpred<-prediction(predictions=predict(modAve, newdata=randat), labels=randat$real_int)\r\n\t# auci<-performance(n_modpred, measure=""auc"")\r\n\t# n_auc[i]<-auci@y.values[[1]]\r\n# }\r\n# save(n_auc, file=""rando_auc.Rdata"")\r\n###make a list of all predictions with real moth and plant names\r\nlogdat$predictions<-modpred@predictions[[1]]\r\n\r\nlogdat$mothname<-rep(NA, nrow(logdat))\r\nfor(i in 1:nrow(mothdat)){\r\n\tmm<-mothdat[i,]\r\n\tlogdat$mothname[which(logdat$moth==mm$Lep_nr)]<-mm$Lep_name\r\n}\r\nrealdat<-logdat[which(logdat$real_int==1),]\r\n\r\n###Figure 1A ROC curve\r\njpeg(filename = ""Fig1A_ROC_curve.jpeg"", width = 6, height = 6, units = ""in"", \r\n\tpointsize = 12, bg = ""transparent"", res = 600, restoreConsole = TRUE)\r\nplot(modperf, colorize=F, lwd=3,xaxis.cex.axis=1.5, yaxis.cex.axis=1.5, cex.lab=1.5)\r\nxs<-modperf@x.values[[1]]; ys<-modperf@y.values[[1]]\r\n#polygon(y=c(ys,0), x=c(xs,1), col=""light blue"")\r\npolygon(y=c(ys,order(ys)/length(ys)), x=c(xs,order(ys)/length(ys)), col=""light green"")\r\nsegments(x0=0,y0=0,x1=1,y1=1,lty=""dashed"", lwd=2)\r\ndev.off()\r\n###get values to make arrows to predictions of particular examples...did the drawing in Photoshop to get\r\n###nice looking arrows.  \r\n\r\nfalsedat<-logdat[which(logdat$real_int==0),]\r\nrealdat<-logdat[which(logdat$real_int==1),]\r\n##Eupethecia virginuata on Solidago canadensis/gigantea score = -2.363075\r\nxval<-nrow(falsedat[which(falsedat$predictions>-2.363075),])/nrow(falsedat)\r\nyval<-nrow(realdat[which(realdat$predictions>-2.363075),])/nrow(realdat)\r\n##Dendrolimus_pini on pseudotsuga_menziesii:  score = -6.535287\r\n\r\nxval<-nrow(falsedat[which(falsedat$predictions>-6.535287),])/nrow(falsedat)\r\nyval<-nrow(realdat[which(realdat$predictions>-6.535287),])/nrow(realdat)\r\n\r\n##Synanthedon_tipuliformis on ribes_aureum:  score= -3.300335\r\nxval<-nrow(falsedat[which(falsedat$predictions>-3.300335),])/nrow(falsedat)\r\nyval<-nrow(realdat[which(realdat$predictions>-3.300335),])/nrow(realdat)\r\n##Calliteara_pudibunda on quercus_rubra:  score= -2.801016\r\nxval<-nrow(falsedat[which(falsedat$predictions>-2.801016),])/nrow(falsedat)\r\nyval<-nrow(realdat[which(realdat$predictions>-2.801016),])/nrow(realdat)\r\n###model parameters\r\ncolnames(modmat)<-c(""intercept"", ""min_phylo"", ""nhost"", ""min_phylo*nhost"")\r\nrownames(modmat)<-c(""K_1"", ""K_2"", ""K_3"", ""K_4"", ""K_5"")\r\nmodmat\r\nAveCoeff\r\n']","Data from: Predicting novel trophic interactions in a non-native world Humans are altering the global distributional ranges of plants, while their co-evolved herbivores are frequently left behind. Native herbivores often colonise non-native plants, potentially reducing invasion success or causing economic loss to introduced agricultural crops. We developed a predictive model to forecast novel interactions and verified it with a data set containing hundreds of observed novel plantinsect interactions. Using a food network of 900 native European butterfly and moth species and 1944 native plants, we built an herbivore host-use model. By extrapolating host use from the native herbivoreplant food network, we accurately forecasted the observed novel use of 459 non-native plant species by native herbivores. Patterns that governed herbivore host breadth on co-evolved native plants were equally important in determining non-native hosts. Our results make the forecasting of novel herbivore communities feasible in order to better understand the fate and impact of introduced plants.",1
United States recycled content standards for lithium-ion batteries,"Lithium-ion battery recycling can decrease life cycle environmental impacts of electric vehicles (EVs) and assist in securing domestic supply chains. However, the US, the third largest market for EVs, has no policies for recycling of batteries at their end-of-life. The European Union has proposed recycled content standards (RCSs) to help drive a circular battery ecosystem. This analysis calculates feasible RCSs for the US based on future sale projections, techno-economic assessment, life cycle assessment, and material flow analysis. Using a 95% confidence interval, results show that 1112% of cobalt, 78% of lithium, and 1012% of nickel demand in 2030 and 1518%, 911%, and 1517%, respectively, in 2035, could be met by retired supply assuming closed-loop recycling. While domestic recycling can be profitable at scale and reduce environmental impacts, it is more expensive than exporting to China for recycling. Consequently, policy is likely needed to ensure critical materials are recycled domestically.",,"United States recycled content standards for lithium-ion batteries Lithium-ion battery recycling can decrease life cycle environmental impacts of electric vehicles (EVs) and assist in securing domestic supply chains. However, the US, the third largest market for EVs, has no policies for recycling of batteries at their end-of-life. The European Union has proposed recycled content standards (RCSs) to help drive a circular battery ecosystem. This analysis calculates feasible RCSs for the US based on future sale projections, techno-economic assessment, life cycle assessment, and material flow analysis. Using a 95% confidence interval, results show that 1112% of cobalt, 78% of lithium, and 1012% of nickel demand in 2030 and 1518%, 911%, and 1517%, respectively, in 2035, could be met by retired supply assuming closed-loop recycling. While domestic recycling can be profitable at scale and reduce environmental impacts, it is more expensive than exporting to China for recycling. Consequently, policy is likely needed to ensure critical materials are recycled domestically.",1
Understanding emergence of the UN Sustainable Development Goals Research,"Emerging new scientific knowledge associated with the UN Sustainable Development Goals (SDGs) can contribute to dealing with the complexities of social, economic, and environmental challenges. Identifying and understanding the underlying conditions that enable and constrain the emergence of SDGs research can help to increase the transformative potential of scientific knowledge. We investigate the role of universities in facilitating the emergence and consolidation of SDGs research using Utrecht University as a case. Our approach offers a systematic understanding of the development of research areas associated with the SDGs -including opportunities and constraints- to support universities in identifying the thematic orientation of their current research in the framework of the SDGs. This approach can thus increase reflexibility about the contribution of universities to implement the SDGs. Our results reveal that universities can enhance awareness about the SDGs, generate reflections on SDG research, offer institutional opportunities to interconnect diverse knowledge domains and diverse social actors, and nurture and develop emerging research associated with the SDGs.","['# Set working directory\nsetwd(""~/Documents/Work/SDG_project/Phase2/Temp"")\n\n# Using the functions (all names and values are examples)\nReadFiles(\'output\')\nCoBibMatrix(\'output\', 16)\nThreshold_Eval(\'output\', 16, 5, 30, 0.5)\nThreshold_Cleaner(\'output\', \'output_clean\', 17)\nSearch_Function(\'output_clean\', \'SDG_pubs\', \'ramirez_thesaurus\', 16)\nSDG_Triads(\'SDG_pubs\', \'output_clean\', \'SDG_triads\', 16)\nSDG_Communities(\'SDG_pubs\', \'output_clean\', 0.4, 0.4, \'yes\')\n\n# Execute all lines manually\n##########################################################################\n\n################################\n### STEP 1: DATA PREPARATION ###\n################################\n\n# Input values\noutput = \'output\'\n\n###\ndirect=getwd()\nsetwd(paste(direct,\'Data\',sep=\'/\'))\nlis=list.files(,pattern="".txt"")\n\n# Define the column that should be included\na=c(\'PT\',\'UT\',\'PY\',\'TI\',\'AB\',\'ID\',\'DE\',\'SO\',\'AU\',\'C1\',\'WC\',\'SC\',\'CR\',\'Z9\',\'DA\',\'FU\'); mm=c()\n\n# Read data and paste into matrix \nfor(k in lis){x=read.delim(k,skip=2, header = F,encoding=""UTF-8"", sep = ""\\t"",quote="""")\n  pos=c(which(substring(x[,1],1,2)==""PT""),dim(x)[1])\n  m=matrix(,length(pos)-1,length(a));colnames(m)=a\n  for(i in 1:dim(m)[1]){y=as.character(x[pos[i]:(pos[i+1]-1),1])\n    b=substring(y,1,2)[substring(y,1,2)!=\'  \']\n    m[i,]=trimws(unlist(lapply(a,function(j){if(length(y[substring(y,1,2)==j])!=0){if(j%in%c(""CR"",\'AU\')){trimws(substring(paste(trimws(y[which(substring(y,1,2)==j):(which(substring(y,1,2)%in%b)[which(substring(y,1,2)%in%b)>which(substring(y,1,2)==j)][1]-1)]),collapse=\'##\'),3))}else{\n    trimws(substring(paste(trimws(y[which(substring(y,1,2)==j):(which(substring(y,1,2)%in%b)[which(substring(y,1,2)%in%b)>which(substring(y,1,2)==j)][1]-1)]),collapse=\' \'),3))}}else{NA}})))}\n    mm=rbind(mm,m)}\n\n# Create folder \'Analysis\' and write output\nsetwd(direct)\ndir.create(\'Analysis\'); setwd(paste(direct,\'Analysis\',sep=\'/\'))\nwrite.csv(unique(mm),paste(output,\'.csv\',sep=\'\'),row.names=F)\nsetwd(direct)\n\n#######################################\n### STEP 2: CO-BIBLIOGRAPHY NETWORK ###\n#######################################\n\n# Input values\nfile=\'output\'\ncores=16\n\n####\ndirect=getwd()\nsetwd(paste(direct,\'Analysis\',sep=\'/\'))\ndata=read.csv(paste(file,\'.csv\',sep=\'\'))\n\n# Load libraries and extract citation data \nlibrary(parallel);library(Matrix);library(igraph)\nUT=unlist(mclapply(1:dim(data)[1],function(i){rep(as.character(data[i,""UT""]),length(trimws(toupper(unlist(strsplit(as.character(data[i,""CR""]),""##""))))))},mc.cores=cores))\nCR=unlist(mclapply(1:dim(data)[1],function(i){trimws(toupper(unlist(strsplit(as.character(data[i,""CR""]),""##""))))},mc.cores=cores))\nUT=substring(UT,1,30)\n\n# Create matrix UTxUT with number of shared citations \ndata=cbind(UT,CR)\ndata=data[data[,\'CR\']%in%rownames(as.matrix(table(data[,""CR""])[table(data[,""CR""])>1])),]\ndata=data[data[,\'UT\']%in%rownames(as.matrix(table(data[,""UT""])[table(data[,""UT""])>1])),]\nM=Matrix(0,length(unique(data[,""UT""])),length(unique(data[,""UT""])))\ncolnames(M)=rownames(M)=sort(as.character(unique(data[,""UT""])))\na=c(round(seq(1,dim(M)[1],dim(M)[1]/100)),dim(M)[1])\nfor(i in 1:(length(a)-2)){\n  x=mclapply((i+1):(length(a)-1),function(j){datas=data[data[,\'UT\']%in%c(colnames(M)[a[i]:a[i+1]],colnames(M)[a[j]:a[j+1]]),]\n  n=Matrix(table(datas[,\'UT\'],datas[,\'CR\']))\n  n=n%*%t(n)\n  as.matrix(100*round(n/sqrt(diag(n)%*%t(diag(n))),3))},mc.cores=cores)\n  for(j in 1:length(x)){\n    b=as.data.frame(x[j])\n    M[rownames(M)%in%rownames(b),rownames(M)%in%rownames(b)]=as.matrix(b)}}\n\n# Create igraph network and write output\ng=graph_from_adjacency_matrix(M, mode = ""undirected"",weighted = T, diag = F)\nwrite.graph(g,paste(file,\'network_full.graphml\',sep=\'_\'),format=\'graphml\')\nsetwd(direct)\n\n\n#################################################\n### STEP 3: ANALYSING CO-BIBLIOGRAPHY NETWORK ###\n#################################################\n\n# Input values\nfile=\'output\'\ncores=10\nmin_val=5; max_val=30; step=0.5\n\n####\ndirect=getwd()\nsetwd(paste(direct,\'Analysis\',sep=\'/\'))\n\n# Load packages and set up matrix\nlibrary(igraph);library(parallel)\ng=read.graph(paste(file,\'network_full.graphml\',sep=\'_\'),format=\'graphml\')\na=seq(min_val,max_val,step)\nm=matrix(0,length(a),14);rownames(m)=a\n\n# Fill matrix as function of thresholds\nx=mclapply(a,function(i){g2=delete.edges(g,E(g)[!E(g)$weight>i])\n  g2=delete.vertices(g2,V(g2)[degree(g2)==0]);clu=cluster_louvain(g2)\n  b1=paste(i,max(components(g2)$csize),sep=""#"")\n  x=components(g2);g2=delete.vertices(g2,V(g2)[x$membership!=which(x$csize==max(x$csize))]);clu=cluster_louvain(g2)\n  b2=paste(length(unique(clu$membership)),modularity(clu),length(table(clu$membership)[table(clu$membership)>10]),\n         sum(table(clu$membership)[table(clu$membership)>10]),length(triangles(g2))/3,sep=""#"")\n  paste(b1,b2,sep=""#"")},mc.cores=cores)\nm=matrix(unlist(strsplit(unlist(x),""#"")),byrow=T,ncol=7)\ncolnames(m)=c(""Threshold"",""Nodes"",""#Com"",""Modularity"",""#Com.10"",""NodesCom.10"",""Triangles"")\n\n# Write output \nwrite.csv(m,paste(file,""T-Eval.csv"",sep=\'_\'),row.names=F)\nsetwd(direct)\n\n\n#################################\n### STEP 4: CLEANING THE DATA ###\n#################################\n\n# Input values\nfile=\'output\'\noutput=\'output_cleaned\'\nvalue=17\n\n####\ndirect=getwd()\nsetwd(paste(direct,\'Analysis\',sep=\'/\'))\nlibrary(Matrix);library(igraph)\ng=read.graph(paste(file,\'network_full.graphml\',sep=\'_\'),format=\'graphml\')\ndata=read.csv(paste(file,\'.csv\',sep=\'\'))\ndata[,\'UT\']=substring(data[,\'UT\'],1,30)\n\n# Remove the edges and nodes based on the given threshold\ng=delete.edges(g,E(g)[!E(g)$weight>value]);x=components(g)\ng=delete.vertices(g,V(g)[x$membership!=which(x$csize==max(x$csize))])\n\n# Implement Louvain clustering algorithm\nclu=cluster_louvain(g);modularity(clu)\nclu$membership[clu$membership%in%rownames(as.matrix(table(clu$membership)[!table(clu$membership)>10]))]=0\ng=delete.vertices(g,V(g)[clu$membership==0])\nclu=cluster_louvain(g); modularity(clu) #mod = 0.9839205\ng=set_vertex_attr(g,\'COM\',V(g),value=clu$membership)\n\n# Add the communities to the data\ndatas=data[unlist(lapply(1:dim(data)[1],function(i){which(data[,""UT""]==names(V(g))[i])})),]\ndatas = datas[!duplicated(datas[,\'UT\']),]\ndatas=cbind(datas,paste(\'COM\', clu$membership,sep=\'\'))\ncolnames(datas)[dim(datas)[2]]=""COM""\ndatas=datas[datas[,""COM""]!=0,]\n\n# Write output\nwrite.csv(datas,paste(output,\'.csv\',sep=\'\'),row.names=F)\nwrite.graph(g,paste(output,\'.graphml\',sep=\'\'),format=\'graphml\')\nsetwd(direct)\n\n\n##############################\n### STEP 5: KEYWORD SEARCH ###\n##############################\n\n# Input values\nfile = \'output\'\noutput = \'data_searched_full\'\nthesaurus = \'ramirez_thesaurus\'\ncores = 16\n\n###\ndirect=getwd()\nsetwd(paste(direct,\'Analysis\',sep=\'/\'))\nlibrary(textstem);library(tm);library(parallel)\ndata=as.matrix(read.csv(paste(file,"".csv"",sep="""")))\nwords=as.matrix(read.csv(paste(thesaurus,\'.csv\',sep=\'\')))\n\n# The search function\nSearch=function(j){\n  if(words[j,\'Mode\']==\'stemmed\'){\n    if(words[j,\'type.1\']==1){\n      if(length(grep(paste(""\\\\b"", tolower(words[j,\'stemmed\']), ""\\\\b"", sep=""""),datas_stemmed))!=0){paste(grep(paste(""\\\\b"", tolower(words[j,\'stemmed\']), ""\\\\b"", sep=""""),datas_stemmed),j,\n              unlist(lapply(datas_stemmed[grep(paste(""\\\\b"", tolower(words[j,\'stemmed\']), ""\\\\b"", sep=""""),datas_stemmed)],function(k){\n                              length(unlist(gregexpr(paste(""\\\\b"", tolower(words[j,\'stemmed\']), ""\\\\b"", sep=""""),k)))})),sep=\'#\')}}\n    else{\n      x=table(unlist(lapply(unlist(strsplit(words[j,\'stemmed\'],\'#\')),function(h){grep(paste(""\\\\b"",tolower(h),""\\\\b"",sep=""""),datas_stemmed)})))\n      if(length(x[x==length(unlist(strsplit(words[j,\'stemmed\'],\'#\')))])!=0){paste(names(x[x==length(unlist(strsplit(words[j,\'stemmed\'],\'#\')))]),j,\n              apply(matrix(unlist(lapply(unlist(strsplit(words[j,\'stemmed\'],\'#\')),function(h){unlist(lapply(datas_stemmed[as.numeric(names(x[x==length(unlist(strsplit(words[j,\'stemmed\'],\'#\')))]))],\n              function(k){length(unlist(gregexpr(paste(""\\\\b"",h,""\\\\b"",sep=""""),k)))}))})),byrow=T,ncol=length(unlist(strsplit(words[j,\'stemmed\'],\'#\')))),1,min),sep=\'#\')}}}\n  else{\n    if(words[j,\'type.1\']==1){\n      if(length(grep(paste(\'\\\\b\',tolower(words[j,\'corrected\']),sep=""""),datas))!=0){paste(grep(paste(\'\\\\b\',tolower(words[j,\'corrected\']),sep=""""),datas),j,\n              unlist(lapply(datas[grep(paste(\'\\\\b\',tolower(words[j,\'corrected\']),sep=""""),datas)],function(k){\n                              length(unlist(gregexpr(paste(\'\\\\b\',tolower(words[j,\'corrected\']),sep=""""),k)))})),sep=\'#\')}}\n    else{\n      x=table(unlist(lapply(unlist(strsplit(words[j,\'corrected\'],\'#\')),function(h){grep(paste(\'\\\\b\',tolower(h),sep=""""),datas)})))\n      if(length(x[x==length(unlist(strsplit(words[j,\'corrected\'],\'#\')))])!=0){paste(names(x[x==length(unlist(strsplit(words[j,\'corrected\'],\'#\')))]),j,\n              apply(matrix(unlist(lapply(unlist(strsplit(words[j,\'corrected\'],\'#\')),function(h){unlist(lapply(datas[as.numeric(names(x[x==length(unlist(strsplit(words[j,\'corrected\'],\'#\')))]))],\n              function(k){length(unlist(gregexpr(paste(\'\\\\b\',h,sep=""""),k)))}))})),byrow=T,ncol=length(unlist(strsplit(words[j,\'corrected\'],\'#\')))),1,min),sep=\'#\')}}}\n}\n\n# Prepare thesaurus and data\nwords[,\'corrected\']=unlist(lapply(words[,\'corrected\'], function(x) gsub(\'\', \'\\\\\\\\b\', x)))\nwords[,\'corrected\']=unlist(lapply(words[,\'corrected\'], function(x) gsub(\'\\\\*\', \'\', x)))\ndatas=data[,c(\'TI\',\'AB\',\'DE\')]\ndata[,\'UT\']=substring(data[,\'UT\'],1,30)\ndatas=gsub(""  "","" "",gsub(\'[[:digit:]]+\', \'\', gsub(\'  \',\' \',gsub(\'[[:punct:] ]+\',\' \',tolower(unlist(mclapply(1:dim(datas)[1],function(i){paste(datas[i,1],datas[i,2],datas[i,3])},mc.cores=cores)))))))\ndatas=gsub(""  "","" "",unlist(mclapply(datas,function(i){paste(unlist(strsplit(i,\' \'))[!unlist(strsplit(i,\' \'))%in%stopwords()],collapse="" "")},mc.cores=cores)))\ndatas_stemmed=gsub(""  "","" "",unlist(mclapply(datas,function(i){paste(stem_words(unlist(strsplit(i,\' \'))[!unlist(strsplit(i,\' \'))%in%stopwords()]),collapse="" "")},mc.cores=cores)))\n\n# Search each publication for the keywords \nsearch=matrix(unlist(strsplit(unlist(mclapply(1:dim(words)[1],Search,mc.cores=cores)),""#"")),byrow=T,ncol=3)\nm=matrix(unlist(strsplit(unlist(lapply(1:dim(search)[1],function(i){paste(data[as.numeric(search[i,1]),\'UT\'], words[as.numeric(search[i,2]),2],words[as.numeric(search[i,2]),1],words[as.numeric(search[i,2]),3],search[i,3],sep=\'##\')})),\'##\')),byrow=T,ncol=5)\ncolnames(m)=c(\'UT\',\'KeyWord\',\'SDG\',\'Type\',\'Freq\')\nwrite.csv(m,paste(output,""Words_Searched.csv"",sep=\'_\'),row.names=F)\n\n# Determine the SDG(s) for each publication\ndatas=xtabs(as.numeric(m[,\'Freq\'])~m[,\'UT\']+m[,\'SDG\'])\ndatas=cbind(datas,rowSums(datas))\ncolnames(datas)=c(paste(\'SDG\', 1:17, sep = \'.\'),\'FreqT\');class(datas)=\'numeric\'\ndata=data[data[,\'UT\']%in%rownames(datas),]\ndatas=datas[unlist(mclapply(1:dim(data)[1],function(i){which(rownames(datas)==data[i,\'UT\'])},mc.cores=cores)),]\ndata=data.frame(data,datas)\nSDG=unlist(mclapply(1:dim(data)[1],function(i){ifelse(sum(sort(data[i,paste(\'SDG.\',1:17, sep=\'\')]/data[i,\'FreqT\'],decreasing=T)[1])>.75,\n                                                      paste(names(sort(data[i,paste(\'SDG.\',1:17, sep=\'\')]/data[i,\'FreqT\'],decreasing=T))[1],collapse=\'#\'),\n                                                      ifelse(sum(sort(data[i,paste(\'SDG.\',1:17, sep=\'\')]/data[i,\'FreqT\'],decreasing=T)[1:2])>.60,\n                                                             paste(names(sort(data[i,paste(\'SDG.\',1:17, sep=\'\')]/data[i,\'FreqT\'],decreasing=T))[1:2],collapse=\'#\'),\n                                                             ifelse(sum(sort(data[i,paste(\'SDG.\',1:17, sep=\'\')]/data[i,\'FreqT\'],decreasing=T)[1:3])>.50,\n                                                                    paste(names(sort(data[i,paste(\'SDG.\',1:17, sep=\'\')]/data[i,\'FreqT\'],decreasing=T))[1:3],collapse=\'#\'),\n                                                                    paste(names(sort(data[i,paste(\'SDG.\',1:17, sep=\'\')]/data[i,\'FreqT\'],decreasing=T))[1:3],collapse=\'#\'))))},mc.cores=cores))\n\ndata=cbind(data,SDG)\n\nfields = c(\'UT\', \'ACOUSTICS\', \'ASTRONOMY & ASTROPHYSICS\', \'CRYSTALLOGRAPHY\', \'OPTICS\')\ndelete=NULL\nfor (index in grep(paste(fields,collapse=""|""),data$SC)){field=data[index,\'SC\']\nif(length(unlist(strsplit(field,"";"")))==1){delete=append(delete,data[index,\'UT\'])}\nelse if(length(grep(paste(fields,collapse=""|""),unlist(strsplit(field,"";""))))==length(unlist(strsplit(field,"";"")))){\n  delete=append(delete,data[index,\'UT\'])}}\ndata=subset(data,!UT%in%delete)\n\n# Write output \nwrite.csv(data,paste(output,\'.csv\',sep=\'\'),row.names=F)\nsetwd(direct)\n\n\n#####################################\n### STEP 6: TRIADS IN THE NETWORK ###\n#####################################\n\n# Input values\ninput_file=\'data_searched_full\'\ninput_network=\'output_network_full\'\noutput= \'SDG_triads\'\ncores=16\n\n###\ndirect=getwd()\nsetwd(paste(direct,\'Analysis\',sep=\'/\'))\nSDG=c(\'TD\',\'ST\',\'ST\',\'ST\',\'TD\',\'ST\',\'ST\',\'TD\',\'ST\',\'TD\',\'ST\',\'TD\',\'TD\',\'ST\',\'TD\',\'FC\',\'FC\')\nlibrary(igraph);library(parallel)\ndata=read.csv(paste(input_file,"".csv"",sep=""""))\ng=read.graph(paste(input_network,"".graphml"",sep=""""),format=""graphml"")\n\'%notin%\' <- Negate(`%in%`)\ng=delete.vertices(g,names(V(g))[names(V(g)) %notin% data$UT])\n\n# Prepare the data\na=b=c();for(i in 1:dim(data)[1]){a=c(a,paste(as.character(data[i,""UT""]),1:length(unlist(strsplit(as.character(data[i,""SDG""]),""#""))),sep=""-""))\nb=c(b,unlist(strsplit(as.character(data[i,""SDG""]),""#"")))}\ndatas=cbind(a,b);colnames(datas)=c(""UT"",""SDG"")\ng=as_edgelist(g);g[,1]=paste(g[,1],\'-1\',sep=\'\');g[,2]=paste(g[,2],\'-1\',sep=\'\')\nfor(i in 1:dim(data)[1]){for(j in 1:length(unlist(strsplit(as.character(data[i,""SDG""]),""#"")))){\n  if(j!=1){g1=g[g[,1]==paste(as.character(data[i,""UT""]),1,sep=\'-\')|g[,2]==paste(as.character(data[i,""UT""]),1,sep=\'-\'),]\n  g1[g1==paste(as.character(data[i,""UT""]),1,sep=\'-\')]=paste(as.character(data[i,""UT""]),j,sep=""-"")\n  g=rbind(g,g1)}}}\ng=graph_from_edgelist(g, directed = F)\n\n# Triads for the whole network\ntris=matrix(names(V(g))[triangles(g)],byrow=T,ncol=3)\nTrigs=unlist(mclapply(1:dim(tris)[1],function(i){mar=sort(gsub(""SDG."","""",c(as.character(datas[datas[,""UT""]==as.character(tris[i,1]),""SDG""]),as.character(datas[datas[,""UT""]==as.character(tris[i,2]),""SDG""]),as.character(datas[datas[,""UT""]==as.character(tris[i,3]),""SDG""]))))\n              mar2=sort(SDG[as.numeric(mar)]);paste(paste(mar[1],mar[2],mar[3],sep=""_""),paste(mar2[1],mar2[2],mar2[3],sep=""_""),sep=""#"")},mc.cores=cores))\nM=matrix(unlist(strsplit(Trigs,""#"")),byrow=T,ncol=2)\nwrite.csv(M, paste(output,\'full_network.csv\',sep=\'\'),row.names=F)  \n\n# Triads per community\nif (""COM"" %in% colnames(data)){\n  M=c()\n  for(j in 1:length(unique(data[,""COM""]))){\n    UTS=paste(sort(rep(as.character(data[data[,""COM""]==unique(data[,""COM""])[j],""UT""]),17)),1:17,sep=""-"")\n    if(length(rownames(as.matrix(triangles(delete.vertices(g,rownames(as.matrix(V(g)))[!rownames(as.matrix(V(g)))%in%UTS])))))!=0){\n      tris=matrix(rownames(as.matrix(triangles(delete.vertices(g,rownames(as.matrix(V(g)))[!rownames(as.matrix(V(g)))%in%UTS])))) ,byrow=T,ncol=3)\n      Trigs=unlist(mclapply(1:dim(tris)[1],function(i){\n        mar=sort(gsub(""SDG."","""",c(as.character(datas[datas[,""UT""]==as.character(tris[i,1]),""SDG""]),as.character(datas[datas[,""UT""]==as.character(tris[i,2]),""SDG""]),as.character(datas[datas[,""UT""]==as.character(tris[i,3]),""SDG""]))))\n        mar2=sort(SDG[as.numeric(mar)])\n        paste(paste(mar[1],mar[2],mar[3],sep=""_""),paste(mar2[1],mar2[2],mar2[3],sep=""_""),sep=""#"")},mc.cores=cores))\n      m=matrix(unlist(strsplit(Trigs,""#"")),byrow=T,ncol=2)\n      M=rbind(M,cbind(m,rep(as.character(unique(data[,""COM""])[j]),dim(m)[1])))}}\n  write.csv(M, paste(output,\'communities.csv\',sep=\'\'),row.names=F)}\nsetwd(direct)\n          \n\n###############################\n### STEP 7: SDG communities ###\n###############################\n\n# Input values \nfile_sdg=\'SDG_pubs\'\nfile_data=\'output_clean\'\nT4_Cut=0.4\nTotal_Cut=0.3\nsave_fig=\'yes\'; figname=\'SDG_share\'\n\n###\ndirect=getwd(); dir.create(\'Images\')\nsetwd(paste(direct,\'Analysis\',sep=\'/\'))\nlibrary(dplyr);library(ggplot2)\nsdg_pubs = read.csv(paste(file_sdg,\'.csv\',sep=""""))\nall_pubs <- read.csv(paste(file_data,\'.csv\',sep=""""))\n\n#SDG share per year \npubs_yr = merge(data.frame(table(all_pubs$COM, all_pubs$PY)), data.frame(table(sdg_pubs$COM, sdg_pubs$PY)), by = c(\'Var1\', \'Var2\'), all.x = T)\npubs_yr[is.na(pubs_yr)]=0\npubs_yr$share.xy = pubs_yr$Freq.y/pubs_yr$Freq.x\npubs_yr = reshape(pubs_yr, idvar = ""Var1"", timevar = ""Var2"", direction = ""wide"", new.row.names=1:length(unique(all_pubs$COM))) #; names(pubs_years)= c(\'COM\', paste(\'pubs\', 2000:2020, sep = \'_\'))\nnames(pubs_yr)=gsub(x = names(pubs_yr), pattern = ""Freq.x."", replacement = ""pubs_""); names(pubs_yr)=gsub(x = names(pubs_yr), pattern = ""Freq.y."", replacement = ""sdgpubs_""); names(pubs_yr)=gsub(x = names(pubs_yr), pattern = ""share.xy."", replacement = ""share_""); names(pubs_yr)=gsub(x = names(pubs_yr), pattern = ""Var1"", replacement = ""COM"") \npubs_yr$total_pubs = rowSums(pubs_yr[, c(paste(\'pubs\', unique(all_pubs$PY), sep = \'_\'))]); pubs_yr$total_sdgpubs = rowSums(pubs_yr[, c(paste(\'sdgpubs\', unique(all_pubs$PY), sep = \'_\'))])\npubs_yr$total_sdg_share=pubs_yr$total_sdgpubs/pubs_yr$total_pubs\n\nsdg_share=pubs_yr[c(\'COM\', paste(\'share\', unique(all_pubs$PY), sep = \'_\'))]\nsdg_share[is.na(sdg_share)]=0\nfits <- lm.fit(cbind(1, seq_len(ncol(sdg_share[,-1]))), t(sdg_share[,-1]))\n\n#SDG publications in communities over the years (increase/decrease), timeframes of 5 years \npubs_tf = data.frame(table(all_pubs$COM, all_pubs$PY)) \n\ntimeframes=cut(min(all_pubs$PY):max(all_pubs$PY),4)\nintervals=unlist(lapply(levels(timeframes),function(x) regmatches(x, gregexpr(""[[:digit:]]+"", x))))\n\npubs_tf$Var2=gsub(paste(intervals[1]:intervals[2], collapse=""|""), \'t1\', as.character(pubs_tf$Var2)); pubs_tf$Var2=gsub(paste((as.numeric(intervals[3])+1):intervals[4], collapse=""|""), \'t2\', pubs_tf$Var2); pubs_tf$Var2=gsub(paste((as.numeric(intervals[5])+1):intervals[6], collapse=""|""), \'t3\', pubs_tf$Var2); pubs_tf$Var2=gsub(paste((as.numeric(intervals[7])+1):intervals[8], collapse=""|""), \'t4\', pubs_tf$Var2)\npubs_tf = pubs_tf %>% group_by(Var1, Var2) %>% summarise(Freq = sum(Freq))\nsdg_pubs_tf = data.frame(table(sdg_pubs$COM, sdg_pubs$PY))\nsdg_pubs_tf$Var2=gsub(paste(intervals[1]:intervals[2], collapse=""|""), \'t1\', as.character(sdg_pubs_tf$Var2)); sdg_pubs_tf$Var2=gsub(paste((as.numeric(intervals[3])+1):intervals[4], collapse=""|""), \'t2\', sdg_pubs_tf$Var2); sdg_pubs_tf$Var2=gsub(paste((as.numeric(intervals[5])+1):intervals[6], collapse=""|""), \'t3\', sdg_pubs_tf$Var2); sdg_pubs_tf$Var2=gsub(paste((as.numeric(intervals[7])+1):intervals[8], collapse=""|""), \'t4\', sdg_pubs_tf$Var2)\nsdg_pubs_tf = sdg_pubs_tf %>% group_by(Var1, Var2) %>% summarise(Freq = sum(Freq))\n\npubs_tf = merge(pubs_tf, sdg_pubs_tf, by = c(\'Var1\', \'Var2\'), all.x = T)\npubs_tf$share.xy = pubs_tf$Freq.y/pubs_tf$Freq.x\npubs_tf = reshape(pubs_tf, idvar = ""Var1"", timevar = ""Var2"", direction = ""wide"", new.row.names=1:length(unique(all_pubs$COM))) #; names(pubs_tf)= c(\'COM\', paste(\'pubs\', 2000:2020, sep = \'_\'))\nnames(pubs_tf)=gsub(x = names(pubs_tf), pattern = ""Freq.x."", replacement = ""pubs_""); names(pubs_tf)=gsub(x = names(pubs_tf), pattern = ""Freq.y."", replacement = ""sdgpubs_""); names(pubs_tf)=gsub(x = names(pubs_tf), pattern = ""share.xy."", replacement = ""share_""); names(pubs_tf)=gsub(x = names(pubs_tf), pattern = ""Var1"", replacement = ""COM"") \npubs_tf$total_pubs = rowSums(pubs_tf[, c(paste(\'pubs\', paste(\'t\', 1:4, sep=""""), sep = \'_\'))]); pubs_tf$total_sdgpubs = rowSums(pubs_tf[, c(paste(\'sdgpubs\', paste(\'t\', 1:4, sep=""""), sep = \'_\'))])\npubs_tf$total_sdg_share=pubs_tf$total_sdgpubs/pubs_tf$total_pubs\n\nsdg_share_tf=pubs_tf[c(\'COM\', paste(\'share\', paste(\'t\', 1:4, sep=""""), sep = \'_\'), \'total_sdg_share\')]\nsdg_share_tf[is.na(sdg_share_tf)]=0\nsdg_share_tf <- cbind(sdg_share_tf, t(coef(fits))[,2]); names(sdg_share_tf)[-(1:6)] <- c(""Slope"")\n\n#Analyse cutoff points \np_t4 = data.frame(sort(sdg_share_tf$share_t4)) %>% rename(x = sort.sdg_share_tf.share_t4.) %>%\n  ggplot() + geom_bar(aes(x=seq_along(x),y=x), stat=\'identity\', width=1, color=\'black\') +\n  ggtitle(\'T4 SDG share\') + labs(x=\'\', y=\'SDG pub share\') +  theme_classic() + theme(plot.title=element_text(hjust=0.5, size=14), plot.subtitle=element_text(size=12,hjust=0.5), \n                                                                                     axis.text.x=element_text(angle=90), legend.position = \'none\') + \n  geom_hline(yintercept=T4_Cut, color=\'red\', linetype=\'dashed\', size=0.5) + ylim(0, 1)\n\np_total = data.frame(sort(sdg_share_tf$total_sdg_share)) %>% rename(x = sort.sdg_share_tf.total_sdg_share.) %>%\n  ggplot() +  geom_bar(aes(x=seq_along(x),y=x), stat=\'identity\', width=1, color=\'black\') +\n  ggtitle(\'Total SDG share\') + labs(x=\'\', y=\'SDG pub share\') + theme_classic() + theme(plot.title=element_text(hjust=0.5, size=14), plot.subtitle=element_text(size=12,hjust=0.5), \n                                                                                       axis.text.x=element_text(angle=90), legend.position = \'none\') + \n  geom_hline(yintercept=Total_Cut, color=\'red\', linetype=\'dashed\', size=0.5) + ylim(0,1)\n\np=ggarrange(p_t4, p_total, ncol=2, nrow=1, align=\'h\')\nprint(p)\nif (save_fig == \'yes\'){png(paste(direct,\'/Images/\',figname,\'.png\',sep=\'\'), width=600, height=300);print(p);dev.off()}\n    \n#Extra communities based on different criteria \nsdg_communities = as.character(merge(sdg_share_tf[(sdg_share_tf$Slope > 0) & (sdg_share_tf$total_sdg_share > Total_Cut), ], \n                                     sdg_share_tf[(sdg_share_tf$share_t4 > sdg_share_tf$share_t3) & (sdg_share_tf$share_t4 > T4_Cut), ], all.x = T, all.y = T)[,\'COM\']); \nsdg_pubs$sdg_com = ifelse(sdg_pubs$COM %in% sdg_communities, \'yes\', \'no\')\nwrite.csv(sdg_pubs, paste(file_sdg,\'sdg_com.csv\',sep=""_""), row.names=F)\n\nsdg_communities=unique(sdg_pubs[sdg_pubs$sdg_com == \'yes\',\'COM\'])\nall_pubs$sdg_com = ifelse(all_pubs$COM %in% sdg_communities, \'yes\', \'no\')\nall_pubs = plyr::join(all_pubs,sdg_pubs[,c(\'UT\',\'SDG\')])\nwrite.csv(all_pubs, paste(file_data,\'sdg_com.csv\',sep=""_""), row.names=F)\n\nsetwd(direct)\n\n\n###############################\n### STEP 8: HCPC clustering ###\n###############################\n\nfile=\'output_clean_sdg_com\'\nClustSDG=6\nClustNoSDG=6\n\ndirect=getwd()\nsetwd(paste(direct,\'Analysis\',sep=\'/\'))\nlibrary(FactoMineR)\ndata=as.matrix(read.csv(paste(file,\'.csv\',sep=\'\')))\ndatas=data[data[,\'sdg_com\']==\'yes\',]\nm=matrix(unlist(strsplit(unlist(lapply(1:dim(datas)[1],function(i){paste(rep(datas[i,\'COM\'],length(unlist(strsplit(datas[i,\'SDG\'],\'#\')))),unlist(strsplit(datas[i,\'SDG\'],\'#\')),sep=\'#\')})),\'#\')),byrow=T,ncol=2)\nm=table(m[,1],m[,2])\nm=m[,colnames(m)!=\'NA\']%*%t(m[,colnames(m)!=\'NA\'])\nSC=HCPC(CA(m,graph=F),graph=F,nb.clust=ClustSDG)$data.clust\ndatas=data[data[,\'sdg_com\']==\'no\',]\nm=matrix(unlist(strsplit(unlist(lapply(1:dim(datas)[1],function(i){paste(rep(datas[i,\'COM\'],length(unlist(strsplit(datas[i,\'SC\'],\'; \')))),unlist(strsplit(datas[i,\'SC\'],\'; \')),sep=\'#\')})),\'#\')),byrow=T,ncol=2)\nm=table(m[,1],m[,2])\nm=m[,colnames(m)!=\'NA\']%*%t(m[,colnames(m)!=\'NA\'])\nNC=HCPC(CA(m,graph=F),graph=F,nb.clust=ClustNoSDG)$data.clust\nm=cbind(c(rownames(SC),rownames(NC)),c(paste(\'Y\',SC[,\'clust\'],sep=\'\'),paste(\'N\',NC[,\'clust\'],sep=\'\')))\nCluster=unlist(lapply(data[,\'COM\'],function(i){m[m[,1]==i,2]}))\n\ndata=cbind(data,Cluster)\n\nl=list()\nfor (clust in unique(data[,\'Cluster\'])){\n  dat=data[data[,\'Cluster\']==clust,]\n  sc=sort(table(unlist(sapply(strsplit(dat[,\'SC\'], "";""), function(x) trimws(x)))),decreasing=T)\n  l[[clust]]=data.frame(SC_1=names(sc[1]),SC_2=names(sc[2]),SC_3=names(sc[3]))}\n\nm=cbind(names(l),as.data.frame(matrix(unlist(l), nrow=length(l), byrow=TRUE)))\nnames(m)=c(\'Cluster\',\'SC1\',\'SC2\',\'SC3\')\ndata=plyr::join(as.data.frame(data[,1:20]),m,by=\'Cluster\')\n\nwrite.csv(data,paste(file,\'.csv\',sep=\'\'), row.names=F)\n\n\n']","Understanding emergence of the UN Sustainable Development Goals Research Emerging new scientific knowledge associated with the UN Sustainable Development Goals (SDGs) can contribute to dealing with the complexities of social, economic, and environmental challenges. Identifying and understanding the underlying conditions that enable and constrain the emergence of SDGs research can help to increase the transformative potential of scientific knowledge. We investigate the role of universities in facilitating the emergence and consolidation of SDGs research using Utrecht University as a case. Our approach offers a systematic understanding of the development of research areas associated with the SDGs -including opportunities and constraints- to support universities in identifying the thematic orientation of their current research in the framework of the SDGs. This approach can thus increase reflexibility about the contribution of universities to implement the SDGs. Our results reveal that universities can enhance awareness about the SDGs, generate reflections on SDG research, offer institutional opportunities to interconnect diverse knowledge domains and diverse social actors, and nurture and develop emerging research associated with the SDGs.",1
Data from: Eating up the world's food web and the human trophic level,"Trophic levels are critical for synthesizing species' diets, depicting energy pathways, understanding food web dynamics and ecosystem functioning, and monitoring ecosystem health. Specifically, trophic levels describe the position of species in a food web, from primary producers to apex predators (range, 15). Small differences in trophic level can reflect large differences in diet. Although trophic levels are among the most basic information collected for animals in ecosystems, a human trophic level (HTL) has never been defined. Here, we find a global HTL of 2.21, i.e., the trophic level of anchoveta. This value has increased with time, consistent with the global trend toward diets higher in meat. National HTLs ranging between 2.04 and 2.57 reflect a broad diversity of diet, although cluster analysis of countries with similar dietary trends reveals only five major groups. We find significant links between socio-economic and environmental indicators and global dietary trends. We demonstrate that the HTL is a synthetic index to monitor human diets and provides a baseline to compare diets between countries.","['########################################################\n#### MAIN SCRIPT TO:                               #####\n#### - EXTRACT THE MATERIALS OF THE MANUSCRIPT     #####\n#### - RUN THE SCRIPTS FOR THE FIGURES             #####\n#### - UNDERSTAND THE ARCHITECTURE OF THE SCRIPTS  #####\n########################################################\n\n########################################################\n#### CREATE THE FOLDER                              ####\n#### WHERE YOU WANT TO SAVE                         ####\n#### THE DATA AND SCRIPTS                           ####\n#### FOR HTL CALCULATIONS                           ####\n#### AND FIGURES                                    ####\n########################################################\nmydir                <- \'/home/sbonhomm/Dropbox_gmail/Dropbox/htl/for_paper/\'\nmydir_data           <- paste(mydir,""data/"", sep="""")\nmydir_scripts        <- paste(mydir,""script_R/"", sep="""")\nmydir_figures        <- paste(mydir,""Figures/"", sep="""")\nmydir_figures_supmat <- paste(mydir,""supmat/"", sep="""")\n\n### Uncomment if you want to start from \ndir.create(mydir, recursive=T)\nsystem(""wget -r \'ftp://ftp.ifremer.fr/ifremer/divers/HTL/*\'"")\n\n#########################################################\n### get_fao.R                                        ####\n#########################################################\n### GET FAO DATA FROM THE WEBSITE                    ####\n### !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!   ####\n### !!!!!! DO not run since you already have  !!!!   ####\n### !!!!!! everything  in the download. It is just   #### \n### !!!!!! for repetability and for people who want  ####\n### !!!!!! to download                               ####\n#########################################################\n\n#########################################################\n#########################################################\n### get_fao.R\n#############\n#### Script to get and clean the data from FAO\n#### INPUTS download on FAO website: http://faostat.fao.org/Portals/_Faostat/Downloads/zip_files/\n#### OUTPUTS: \n####   veggies.Rdata:   file with all food supply data (""Food supply quantity (kg/capita/yr)"") for crop\n####   veg_sub.Rdata:   same as veggies.Rdata with food supply just for countries (exclude totals)\n####   meat.Rdata:      file with all food supply data (""Food supply quantity (kg/capita/yr)"") for livestocks\n####   meat_sub.Rdata:  same as meat.Rdatae with food supply just for countries (exclude totals)\n####   conso_sub.Rdata: file with meat_sub.Rdata and veg_sub.Rdata (variable name conso_sub)\n#########################################################\n\n#source(paste(mydir_data,""get_fao.R"", sep=""""))\n\n#########################################################\n#########################################################\n#### tl_table.R\n###############\n#### Script to calculate HTL for each country over\n#### INPUTS: \n####   conso_sub.Rdata: see get_fao.R\n####   tl_fao_2012.csv: this file corresponds to table S1 (see SOM for how this value have been calculated)\n####   geopol.csv     : since some countries became independent, we arbitrarly attribute the quantity and diet of their former country\n####   iso3convert.csv: ISO3 code of countries\n####   pop.csv        : population size for each country from 1950-2100  (downloaded from UN website: http://esa.un.org/unpd/wpp/Excel-Data/DB04_Population_ByAgeSex_Annual/WPP2010_DB4_F1A_POPULATION_BY_AGE_BOTH_SEXES_ANNUAL_1950-2010.XLS and http://esa.un.org/unpd/wpp/Excel-Data/DB04_Population_ByAgeSex_Annual/WPP2010_DB4_F1B_POPULATION_BY_AGE_BOTH_SEXES_ANNUAL_2011-2100.XLS)\n####   conso.Rdata    : previous file used when this analysis was done in 2007 and used now to retrieve DR Congo data that has not been updated by FAO yet (Jan 10 2013)...\n#### OUTPUTS: \n####   conso_table.Rdata: table with all food supply per country per year (variable name: conso5)\n####   tl_table.Rdata   : table of HTL per country per year (variable name: tl2)\n#########################################################\n\n#source(paste(mydir_data,""tl_table.R"", sep=""""))\n\n#########################################################\n#########################################################\n#### Figure1.R\n###############\n#### Script to plot the Figure 1 (trends and map of HTL)\n#### INPUTS: \n####   tl_table.Rdata: see tl_table.R above\n#### OUTPUTS: 2 figures with 2 format (eps and pdf): Figure1A and Figure1B\n#### Function used: mapCountryData2.R\n#########################################################\n\n#########################################################\n#########################################################\n#### Figure2.R\n###############\n#### Script to plot the Figure 2 (trends for each group and map of the groups)\n#### INPUTS: \n####   tl_table.Rdata: see tl_table.R above\n#### OUTPUTS:\n####    ddtw.Rdata: file containing the distance matrix for the clustering method\n####    tree1.Rdata     : data of the clustering method\n####    rezgapstat.Rdata: data of the gap statistics\n####    gptl.Rdata      : data with the group associated with each country\n####    3 figures with 2 format (eps and pdf): Figure1A-F and gapstat.eps (for SOM)\n#### Function used: mapCountryData2.R\n#########################################################\n\n#########################################################\n#########################################################\n#### Figure3.R\n###############\n#### Script to plot the Figure 3 (link between HTL and the World Data Bank developmental indicators)\n#### INPUTS: \n####   wb.Rdata   : developmental indicators from the World Bank (see inside Figure3.R for the transformation) and downloaded from http://databank.worldbank.org/databank/download/WDI_csv\n####   tl_table.Rdata: see tl_table.R above\n####   iso3convert.csv: ISO3 code of countries\n####   MINE.jar: java script to run the Maximum Information Coefficient (Reshef et al. 2011) downloaded from http://www.exploredata.net/ftp/MINE.jar\n####   names_WB.txt: correspondance between the number given for each indicator and their actual name\n####   gptl.Rdata: table of the association between countries and their group (see Figure2.R)\n#### OUTPUTS:\n####   bunch/heaps/pretty lots of rezmicXX.Rdata are saved in the MIC directory as intermediary file (as well as .csv files generated by MINE.jar) \n####   listvar1.Rdata: file that contains the name of the indicators that are significantly (p-value corrected for multiple testing) correlated to HTL over the 49 years and for at least 75% of the countries\n####   4 figures with 2 format (eps and pdf) + 14 figures for the SOM\n#### Function used: plot_function.R\n#########################################################\n\n#########\n## END ##\n#########\n']","Data from: Eating up the world's food web and the human trophic level Trophic levels are critical for synthesizing species' diets, depicting energy pathways, understanding food web dynamics and ecosystem functioning, and monitoring ecosystem health. Specifically, trophic levels describe the position of species in a food web, from primary producers to apex predators (range, 15). Small differences in trophic level can reflect large differences in diet. Although trophic levels are among the most basic information collected for animals in ecosystems, a human trophic level (HTL) has never been defined. Here, we find a global HTL of 2.21, i.e., the trophic level of anchoveta. This value has increased with time, consistent with the global trend toward diets higher in meat. National HTLs ranging between 2.04 and 2.57 reflect a broad diversity of diet, although cluster analysis of countries with similar dietary trends reveals only five major groups. We find significant links between socio-economic and environmental indicators and global dietary trends. We demonstrate that the HTL is a synthetic index to monitor human diets and provides a baseline to compare diets between countries.",1
Data from: Timing and probability of arrival for sea lice dispersing between salmon farms,"Sea lice are a threat to the health of both wild and farmed salmon and an economic burden for salmon farms. With a free-living larval stage, sea lice can disperse tens of kilometers in the ocean between salmon farms, leading to connected sea lice populations that are difficult to control in isolation. In this paper, we develop a simple analytical model for the dispersal of sea lice between two salmon farms. From the model we calculate the arrival time distribution of sea lice dispersing between farms, as well as the level of cross-infection of sea lice. We also use numerical flows from a hydrodynamic model, coupled with a particle tracking model, to directly calculate the arrival time of sea lice dispersing between two farms in the Broughton Archipelago, BC, in order to fit our analytical model and find realistic parameter estimates. Using the parametrized analytical model we show that there is often an intermediate inter-farm spacing that maximizes the level of cross-infection between farms, and that increased temperatures will lead to increased levels of cross-infection.","['rm(list=ls())\r\nlibrary(raster)\r\nlibrary(tidyverse)\r\nlibrary(ncdf4)\r\nlibrary(plyr)\r\n\r\n# set the working directory\r\nsetwd(""E:/ConnectivityModel/netCDF by sp"")\r\n\r\n\r\n\r\n####get the netCDF files into RDS files############################################\r\nsp <- dir()\r\nsp\r\n\r\n# read the names of the files\r\n# loop through the filenames \r\n#for arrival time instead of looping, just pick j=""sp50"" to get CRD 50\r\n#for (j in sp) {\r\nj=""sp50""  \r\n  # read the names of the files in the folder called sp50 \r\nfiles <- list.files(paste0(""E:/ConnectivityModel/netCDF by sp/"",j))\r\ndat <- data.frame()\r\n\r\n\r\nfor (i in files) {\r\n  # extract each variable you are interested in\r\n  ind <- raster(paste0("""",j,""/"",i), varname = ""indomain"") %>% as.data.frame()\r\n  lon <- raster(paste0("""",j,""/"",i), varname = ""x"") %>% as.data.frame()\r\n  lat <- raster(paste0("""",j,""/"",i), varname = ""y"") %>% as.data.frame()\r\n  dep <- raster(paste0("""",j,""/"",i), varname = ""z"") %>% as.data.frame()\r\n  mor <- raster(paste0("""",j,""/"",i), varname = ""m"") %>% as.data.frame()\r\n  mat <- raster(paste0("""",j,""/"",i), varname = ""tm"") %>% as.data.frame()\r\n  sal <- raster(paste0("""",j,""/"",i), varname = ""salt"") %>% as.data.frame()\r\n  tmp <- raster(paste0("""",j,""/"",i), varname = ""temp"") %>% as.data.frame()\r\n  \r\n  # create a data frame with above vectors \r\n  df1 <- data.frame(expand.grid(nlag  = as.character(1:1000), \r\n                                 time = rev(1:(length(tmp[,1])/1000))\r\n                                ),\r\n                    farm = as.numeric(rep(1:20, \r\n                                            each = 50, \r\n                                            times = length(tmp[,1])/1000)\r\n                                        ),\r\n                    lon, lat, dep, mor, mat, sal, tmp, ind, i\r\n                    )\r\n  \r\n  # short names are easier to type ;)\r\n  names(df1) <- c(""nlag"", ""time"", ""farm"", ""lon"", ""lat"", ""dep"", \r\n                  ""surv"", ""mat"", ""sal"", ""tmp"", ""ind"", ""source"")\r\n  \r\n\r\n  \r\n  dat <- filter(df1, as.numeric(time) %in% c(seq(1,793,3))) %>%   #c(seq(73, 793, 72))) %>% #this is where every 24hrs is sampled, now modified so sampled every hour up to 11 days\r\n    bind_rows(dat)  \r\n  \r\n\r\n}\r\n\r\n\r\n\r\ndat1 <- dat %>%\r\n  separate(source, c(""br"", ""r"", ""date"", ""br2"", ""pulse""), extra = ""drop"") %>% #this switches source file name from ""....br524_25.nc"" to br2=br524, pulse=25\r\n  mutate(pulse = as.numeric(pulse)) %>%\r\n  arrange(pulse) %>%\r\n  dplyr::select(-br, -r, -date) #changed so that it will run\r\n\r\ndat2 <- dat1%>%\r\nmutate(lat2 = lat + 5500206.4,\r\n       lon2 = lon + 495942.1,\r\n       mor  = (1 - surv)\r\n)\r\n\r\n#choose where to save datafile\r\nsaveRDS(dat2, file = paste0(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/"",j, "".RDS"")) #put in new folder for arrival time\r\n\r\n#}\r\n\r\n\r\n\r\n\r\n\r\n\r\n', 'rm(list=ls())\r\nlibrary(tidyverse)\r\nlibrary(readr)\r\n#set working directory to where you saved previous file\r\nsetwd(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time"")\r\n\r\n#create new directory for data split by source farm\r\ndir.create(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/arrival split by farm"")\r\n\r\nsp50 <- read_rds(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/sp50.RDS"")\r\n\r\n#subset into source farms\r\n######sp50\r\ndf_list <- split(sp50, sp50$farm)\r\nnew_names <- paste0(""f"", 1:20, ""_sp50"")\r\nsetwd(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/arrival split by farm"")\r\nfor(i in 1:length(df_list)) {\r\n  assign(new_names[i], df_list[[i]])\r\n  saveRDS(object = df_list[[i]],\r\n          file  = paste0(new_names[i], "".RDS""))\r\n}\r\n\r\n#if you want to subset all farms by source farm, copy above code for all farms\r\n\r\n', 'rm(list=ls())\r\nlibrary(""maptools"")\r\nlibrary(""rgdal"")\r\nlibrary(""tidyverse"")\r\nlibrary(""maps"")\r\nlibrary(""sp"")\r\nproj_custom <- CRS(""+proj=tmerc +zone=9 +lat_0=0 +lon_0=-127 +k=0.9996 +x_0=500000 +y_0=0 +datum=WGS84 +units=m"")\r\nproj_utm9n  <- CRS(""+proj=utm +zone=9 +lat_0=0 +lon_0=-127 +k=0.9996 +x_0=500000 +y_0=0 +datum=WGS84 +units=m"")\r\n\r\n#set working directory to location of data split by source farm\r\n  setwd(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/arrival split by farm"")\r\n  listRDS <- dir(pattern = ""*.RDS"")\r\n\r\n  #create shapefiles of spatial locations of sea lice post release\r\ndir.create(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/arrival points"")\r\n    for (i in 1:length(listRDS)) {\r\n    file <- read_rds(listRDS[i])\r\n    coordinates(file) <-  ~lon2+lat2 \r\n    proj4string(file) <-  proj_custom\r\n    file <- spTransform(file, proj_utm9n)\r\n    file1 <- subset(file, select = -(13))\r\n    #note that this above line is because for some reason when importing it creates a    column of 1\'s with no name, and it throws off the next step \r\n  \r\n      writeOGR(file1, \r\n             dsn    = ""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/arrival points"",\r\n             layer  = paste0(listRDS[i]), \r\n             driver = ""ESRI Shapefile"",\r\n             overwrite_layer = T\r\n             \r\n    )\r\n  }\r\n\r\n\r\n', 'rm(list=ls())\r\nlibrary(""maptools"")\r\nlibrary(""rgdal"")\r\nlibrary(""tidyverse"")\r\nlibrary(""maps"")\r\nlibrary(""sp"")\r\nlibrary(""raster"")\r\nlibrary(""spatstat"")\r\n\r\n\r\n#set working directory to fvcom grid\r\nsetwd(""E:/ConnectivityModel/DanielleFiles/Files for Peter/fvcom grid"")\r\npoly <- readOGR(""250m_both.shp"")\r\ngridsize <- 100\r\nxrange_poly <- extent(poly)[2] - extent(poly)[1]\r\nyrange_poly <- extent(poly)[4] - extent(poly)[3]\r\ncellsx <- xrange_poly/gridsize\r\ncellsy <- yrange_poly/gridsize\r\ndimuse <- c(cellsy, cellsx)\r\nfarm_loc <- readOGR(""E:/ConnectivityModel/DanielleFiles/Files for Peter/farms shp"")\r\n#cycle through wd one at a time to run the loops for each sp\r\n\r\n############################################################################\r\n#this loop extracts arrival time for all farms, where lice are leaving farm 11\r\n####################################################################\r\n\r\n\r\n  setwd(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/arrival points"")\r\n  dir.create(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density values"")\r\n  farm11shp = readOGR(""f11_sp50.RDS.shp"") #this is for release from farm 11\r\n  times =sort(unique(farm11shp$time), decreasing = FALSE)\r\n  datalist = list()\r\n  \r\n  for (i in 1:length(times)) {\r\n    \r\n    \r\n    points <- farm11shp[farm11shp$time==times[i], ] #take points from release farm 11 (Glacier)\r\n    xrange_points <- extent(points)[2] - extent(points)[1]\r\n    yrange_points <- extent(points)[4] - extent(points)[3]\r\n    bandwidth <- min(xrange_points,yrange_points)/8\r\n    dat.ow <- as(as(poly, ""SpatialPolygons""), ""owin"")\r\n    dat.pp <- as(points, ""SpatialPoints"")\r\n    dat.ppp <- ppp(x = coordinates(dat.pp)[,1], y = coordinates(dat.pp)[,2], window = dat.ow)\r\n    out.den <- density.ppp(dat.ppp, sigma = bandwidth, dimyx = dimuse, edge = TRUE, diggle = TRUE)\r\n    out.spdf <- as(as(out.den, ""SpatialGridDataFrame""), ""SpatialPixelsDataFrame"")\r\n    raster_out <- brick(raster(out.spdf))\r\n    #Set projection \r\n    proj4string(raster_out) = CRS(""+proj=utm +zone=9 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"")\r\n    #Convert from m2 to km2\r\n    raster_out <- raster_out*1000000\r\n    #extract connectivity\r\n    writeRaster(raster_out, \r\n                filename = paste0(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density values/"", i-1),""GTiff"", overwrite = T)\r\n    farm_densities <- as.data.frame(raster::extract(raster_out,farm_loc))\r\n    farm_densities$farm <- farm_loc$farm\r\n    write.csv(x = farm_densities, file = paste0(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density values/"", i-1, "".csv""))\r\n    farm_densities$time = i-1\r\n    datalist[[i]] = farm_densities\r\n    \r\n  }\r\n#}\r\n\r\n  \r\narrivaltime = do.call(rbind, datalist)\r\nwrite.csv(x = arrivaltime, file = ""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density values/arrivaltime.csv"")\r\nplot(arrivaltime[arrivaltime$farm==11, ]$time, arrivaltime[arrivaltime$farm==11, ]$v)\r\n\r\n\r\n\r\n#######################\r\n#this is if something went wrong during the run, and you need to load the csv \r\n# files manually to create the arrival time in order to save\r\ndatalist=list()\r\n\r\nfor (i in 1:265){\r\n  farm_densities = read.csv(paste0(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density values/"", i-1, "".csv""))\r\n  farm_densities$time=i-1\r\n  datalist[[i]]=farm_densities\r\n}\r\n\r\n\r\n', 'rm(list=ls())\r\nlibrary(""maptools"")\r\nlibrary(""rgdal"")\r\nlibrary(""tidyverse"")\r\nlibrary(""maps"")\r\nlibrary(""sp"")\r\nlibrary(""raster"")\r\nlibrary(""spatstat"")\r\n\r\n\r\n#set working directory to location of fvcom grid\r\nsetwd(""E:/ConnectivityModel/DanielleFiles/Files for Peter/fvcom grid"")\r\npoly <- readOGR(""250m_both.shp"")\r\ngridsize <- 100\r\nxrange_poly <- extent(poly)[2] - extent(poly)[1]\r\nyrange_poly <- extent(poly)[4] - extent(poly)[3]\r\ncellsx <- xrange_poly/gridsize\r\ncellsy <- yrange_poly/gridsize\r\ndimuse <- c(cellsy, cellsx)\r\nfarm_loc <- readOGR(""E:/ConnectivityModel/DanielleFiles/Files for Peter/farms shp"")\r\n\r\n############################################################################\r\n#this loop extracts arrival time for all farms, where lice are leaving farm 11 includes maturation and survival\r\n####################################################################\r\n\r\n\r\n  setwd(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time CRD59/arrival points"")\r\n  dir.create(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time CRD59/farm 11/density surv and mat"")\r\n  farm11shp = readOGR(""f11_sp50.RDS.shp"")\r\n  times =sort(unique(farm11shp$time), decreasing = FALSE)\r\n  datalist = list()\r\n  \r\n  for (i in 1:length(times)) {\r\n    \r\n    \r\n    points <- farm11shp[farm11shp$time==times[i], ]\r\n    xrange_points <- extent(points)[2] - extent(points)[1]\r\n    yrange_points <- extent(points)[4] - extent(points)[3]\r\n    bandwidth <- min(xrange_points,yrange_points)/8\r\n    #subset only lice that have matured\r\n    points <- subset(points, mat >= 0.8)\r\n    #weight by probability of survival\r\n    weights <- points$surv\r\n    dat.ow <- as(as(poly, ""SpatialPolygons""), ""owin"")\r\n    dat.pp <- as(points, ""SpatialPoints"")\r\n    dat.ppp <- ppp(x = coordinates(dat.pp)[,1], y = coordinates(dat.pp)[,2], window = dat.ow)\r\n    out.den <- density.ppp(dat.ppp, sigma = bandwidth, dimyx = dimuse, edge = TRUE, diggle = TRUE,  weights = weights)\r\n    out.spdf <- as(as(out.den, ""SpatialGridDataFrame""), ""SpatialPixelsDataFrame"")\r\n    raster_out <- brick(raster(out.spdf))\r\n    #Set projection \r\n    proj4string(raster_out) = CRS(""+proj=utm +zone=9 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"")\r\n    #Convert from m2 to km2\r\n    raster_out <- raster_out*1000000\r\n    #extract connectivity\r\n    writeRaster(raster_out, \r\n                filename = paste0(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density surv and mat/"", i-1),""GTiff"", overwrite = T)\r\n    farm_densities <- as.data.frame(raster::extract(raster_out,farm_loc))\r\n    farm_densities$farm <- farm_loc$farm\r\n    write.csv(x = farm_densities, file = paste0(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density surv and mat/"", i-1, "".csv""))\r\n    farm_densities$time = i-1\r\n    datalist[[i]] = farm_densities\r\n    \r\n  }\r\n#}\r\n\r\narrivaltimeSurvMat = do.call(rbind, datalist)\r\nwrite.csv(x = arrivaltimeSurvMat, file = ""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density surv and mat/arrivaltime.csv"")\r\n\r\n#######################\r\n#this is if something went wrong during the run, and you need to load the csv \r\n# files manually to create the arrival time in order to save\r\ndatalist=list()\r\n\r\nfor (i in 1:265){\r\n  farm_densities = read.csv(paste0(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density surv and mat/"", i-1, "".csv""))\r\n  farm_densities$time=i-1\r\n  datalist[[i]]=farm_densities\r\n}\r\n\r\n', 'rm(list=ls())\r\nlibrary(""maptools"")\r\nlibrary(""rgdal"")\r\nlibrary(""tidyverse"")\r\nlibrary(""maps"")\r\nlibrary(""sp"")\r\nlibrary(""raster"")\r\nlibrary(""spatstat"")\r\n\r\n\r\n#set working directory to include fvcom grid\r\nsetwd(""E:/ConnectivityModel/DanielleFiles/Files for Peter/fvcom grid"")\r\npoly <- readOGR(""250m_both.shp"")\r\ngridsize <- 100\r\nxrange_poly <- extent(poly)[2] - extent(poly)[1]\r\nyrange_poly <- extent(poly)[4] - extent(poly)[3]\r\ncellsx <- xrange_poly/gridsize\r\ncellsy <- yrange_poly/gridsize\r\ndimuse <- c(cellsy, cellsx)\r\nfarm_loc <- readOGR(""E:/ConnectivityModel/DanielleFiles/Files for Peter/farms shp"")\r\n\r\n\r\n############################################################################\r\n#this loop extracts arrival time for all farms, where lice are leaving farm 11 and includes survival\r\n####################################################################\r\n\r\n\r\n\r\n  setwd(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time CRD59/arrival points"")\r\n  dir.create(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time CRD59/farm 11/density surv"")\r\n  farm11shp = readOGR(""f11_sp50.RDS.shp"")\r\n  times =sort(unique(farm11shp$time), decreasing = FALSE)\r\n  datalist = list()\r\n  \r\n  for (i in 1:length(times)) {\r\n    \r\n    \r\n    points <- farm11shp[farm11shp$time==times[i], ]\r\n    xrange_points <- extent(points)[2] - extent(points)[1]\r\n    yrange_points <- extent(points)[4] - extent(points)[3]\r\n    bandwidth <- min(xrange_points,yrange_points)/8\r\n    #weight points by survival\r\n    weights <- points$surv\r\n    dat.ow <- as(as(poly, ""SpatialPolygons""), ""owin"")\r\n    dat.pp <- as(points, ""SpatialPoints"")\r\n    dat.ppp <- ppp(x = coordinates(dat.pp)[,1], y = coordinates(dat.pp)[,2], window = dat.ow)\r\n    out.den <- density.ppp(dat.ppp, sigma = bandwidth, dimyx = dimuse, edge = TRUE, diggle = TRUE,  weights = weights)\r\n    out.spdf <- as(as(out.den, ""SpatialGridDataFrame""), ""SpatialPixelsDataFrame"")\r\n    raster_out <- brick(raster(out.spdf))\r\n    #Set projection \r\n    proj4string(raster_out) = CRS(""+proj=utm +zone=9 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"")\r\n    #Convert from m2 to km2\r\n    raster_out <- raster_out*1000000\r\n    #extract connectivity\r\n    writeRaster(raster_out, \r\n                filename = paste0(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density surv/"", i-1),""GTiff"", overwrite = T)\r\n    farm_densities <- as.data.frame(raster::extract(raster_out,farm_loc))\r\n    farm_densities$farm <- farm_loc$farm\r\n    write.csv(x = farm_densities, file = paste0(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density surv/"", i-1, "".csv""))\r\n    farm_densities$time = i-1\r\n    datalist[[i]] = farm_densities\r\n    \r\n  }\r\n#}\r\n\r\narrivaltimesurv = do.call(rbind, datalist)\r\nwrite.csv(x = arrivaltimesurv, file = ""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density surv/arrivaltime.csv"")\r\n\r\n#######################\r\n#this is if something went wrong during the run, and you need to load the csv \r\n# files manually to create the arrival time in order to save\r\ndatalist=list()\r\n\r\nfor (i in 1:265){\r\n  farm_densities = read.csv(paste0(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density surv/"", i-1, "".csv""))\r\n  farm_densities$time=i-1\r\n  datalist[[i]]=farm_densities\r\n}\r\n\r\n', 'rm(list=ls())\r\nlibrary(minpack.lm)\r\nlibrary(""maptools"")\r\nlibrary(""rgdal"")\r\nlibrary(""tidyverse"")\r\nlibrary(""maps"")\r\nlibrary(""sp"")\r\nlibrary(fields)\r\nlibrary(latex2exp)\r\nlibrary(scales)\r\nlibrary(akima)\r\n\r\n#define the error function\r\nerf <- function(x) 2 * pnorm(x * sqrt(2)) - 1\r\n\r\n#read in arrival time or inert particles and those that include survival\r\narrivaltime=read.csv(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density values/arrivaltime.csv"")\r\narrivaltimeSurv=read.csv(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density surv/arrivaltime.csv"")\r\n\r\n#These are lice arriving at Burdwood (farm 15)\r\n#1200 is number of particles released, 24hrs*50/hr, so we divide by particles released to get pdf\r\nf=(0.1^2)*arrivaltime[arrivaltime$farm==15, ]$v/1200 #0.01 is area of farm in km^2\r\nt=arrivaltime[arrivaltime$farm==15, ]$t\r\n\r\nfsurv=(0.1^2)*arrivaltimeSurv[arrivaltimeSurv$farm==15, ]$v/1200\r\n\r\n\r\n########first fit by eye to get initial param values\r\n\r\nv=0.065 #km/hr\r\nd=0.945 #km^2/hr\r\nalpha=0.02 \r\n\r\n\r\nL=0.1 #km\r\nx0=-13.5 #km\r\n\r\nfpt=alpha*(erf((x0+v*t)/sqrt(4*d*t))-erf((x0+v*t-L)/sqrt(4*d*t)))\r\n\r\nplot(t,f)\r\nlines(t, fpt)\r\n\r\n#######fit model to data\r\nL=0.1\r\nx0=-13.5\r\n\r\nv_start=0.065 #km/hr\r\nd_start=0.945 #km^2/hr\r\nalpha_start=0.02\r\n\r\n\r\n\r\nfptForm=formula(f~alpha*(erf((x0+v*t)/sqrt(4*d*t))-erf((x0+v*t-L)/sqrt(4*d*t))))\r\n\r\n#fit the model using non-linear least squares\r\nfptFit=nlsLM(fptForm, start=list(v=v_start, d=d_start, alpha=alpha_start), trace=TRUE)\r\n\r\n\r\n#best fit coefficients\r\nvfit=coef(fptFit)[1]\r\ndfit=coef(fptFit)[2]\r\nalphafit=coef(fptFit)[3]\r\n\r\n#best fit line\r\nfptBestFit=alphafit*(erf((x0+vfit*t)/sqrt(4*dfit*t))-erf((x0+vfit*t-L)/sqrt(4*dfit*t)))\r\n\r\nsetwd((\'C:\\\\Users\\\\Peter\\\\Documents\\\\Research\\\\ConnectivityPaper\'))\r\n#this is the first plot needed for paper\r\npdf(\'paperplot.pdf\', width=4, height=2.75)\r\npar(ps=10, mar=c(3,6,1,1)) \r\nplot(t,f,xlab="""", ylab="""", pch=20, axes=F, col=\'white\', family=""serif"")\r\nbox()\r\naxis(side=2, las=2)\r\naxis(side=1, las=1, labels=FALSE, at=seq(0,250,50))\r\ntitle(ylab=""f(t)"", family=""serif"", line=4.5, srt=45)\r\ntitle(xlab=""t (hours)"", family=""serif"", line=2)\r\npoints(t, f, pch=20, cex=1.1, col=alpha(\'black\', 0.3))\r\nlines(t,fptBestFit, lwd=2)\r\nmtext(seq(0,250,50), side=1, line=0.75, at=seq(0,250,50))\r\ndev.off()\r\n\r\n\r\n############Arrival with survival - now including survival\r\n\r\n#fit by eye to find reasonable values\r\nalpha=alphafit\r\nd=0.15\r\nv=0.2\r\nu=0.02\r\n\r\n#delete first (t,f) pair so not dividing by zero.\r\nfsurvnew=fsurv[c(2:length(fsurv))]\r\ntnew=t[c(2:length(t))]\r\n\r\nplot(tnew, fsurvnew)\r\nlines(tnew, exp(-u*tnew)*alpha*(erf((x0+v*tnew)/sqrt(4*d*tnew))-erf((x0+v*tnew-L)/sqrt(4*d*tnew))))\r\n\r\n\r\nfptSurvForm=formula(fsurvnew~exp(-u*tnew)*alpha*(erf((x0+v*tnew)/sqrt(4*d*tnew))-erf((x0+v*tnew-L)/sqrt(4*d*tnew))))\r\n\r\nu_start=0.03#/hr\r\nd_start=0.15\r\nv_start=0.15\r\n\r\n#fit arrival time with survival using NLS\r\nfptSurvFit=nlsLM(fptSurvForm, start=list(v=v_start, d=d_start, alpha=alphafit, u=u_start), trace=TRUE)\r\n\r\n\r\n#best fit coefficients\r\nvSurvfit=coef(fptSurvFit)[1]\r\ndSurvfit=coef(fptSurvFit)[2]\r\nalphaSurvfit=coef(fptSurvFit)[3]\r\nuSurvfit=coef(fptSurvFit)[4]\r\n\r\n#best fit line\r\nfptSurvBestFit=exp(-uSurvfit*tnew)*alphaSurvfit*(erf((x0+vSurvfit*tnew)/sqrt(4*dSurvfit*tnew))-erf((x0+vSurvfit*tnew-L)/sqrt(4*dSurvfit*tnew)))\r\n\r\n\r\n########### paper plot #2\r\npdf(\'paperplot2.pdf\', width=4, height=2.75)\r\npar(ps=10, mar=c(3,6,1,1)) \r\nplot(tnew,fsurvnew,xlab="""", ylab="""", pch=20, axes=F, col=\'white\', family=""serif"")\r\nbox()\r\naxis(side=2, las=2)\r\naxis(side=1, las=1, labels=FALSE, at=seq(0,250,50))\r\ntitle(ylab=TeX(\'$e^{-\\\\mu t} f(t)$\'), family=""serif"", line=4.5, srt=45)\r\ntitle(xlab=""t (hours)"", family=""serif"", line=2)\r\npoints(tnew, fsurvnew, pch=20, cex=1.1, col=alpha(\'black\', 0.3))\r\nlines(tnew,fptSurvBestFit, lwd=2)\r\nmtext(seq(0,250,50), side=1, line=0.75, at=seq(0,250,50))\r\ndev.off()\r\n\r\n\r\n\r\n\r\n##########Arrival with survival and maturation\r\n\r\narrivaltimeSurvMat=read.csv(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/farm 11/density surv and mat/arrivaltime.csv"")\r\n\r\nfsurvmat=(0.1^2)*arrivaltimeSurvMat[arrivaltimeSurvMat$farm==15, ]$v/1200\r\n\r\n#remove first (t,f) pair so not dividing by 0.\r\nt2=t[c(2:length(t))]\r\nfsurvmat2=fsurvmat[c(2:length(fsurvmat))]\r\n\r\n#####Figure out when most lice mature and what maturation function to use\r\n\r\n#load full rds file of lice released from farm 11\r\nf11=read_rds(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/arrival split by farm/f11_sp50.RDS"")\r\ntimes =sort(unique(f11$time), decreasing = FALSE)\r\n\r\nmat80=rep(0, length(times))\r\nfor (i in 1:length(times)) {\r\n  mat80[i]= sum(f11[f11$time==times[i], ]$mat>=0.8)\r\n}\r\n\r\n\r\n\r\n\r\n########Fit total maturation to constant rate, min development time and constant rate, and weibull\r\n\r\nSmat=1-mat80/1200 #survival function\r\nH = function(x) as.numeric(x>0) #define step function\r\n\r\n##First, fit constant rate\r\nconstForm=formula(Smat~exp(-m*t))\r\n\r\n#fit via NLS\r\nconstFit=nlsLM(constForm, start=list(m=0.005), trace=TRUE)\r\n#best fit parameter estimate\r\nmConst=coef(constFit)[1]\r\n\r\nplot(t, Smat)\r\nlines(t, exp(-mConst*t))\r\n\r\n\r\n##Fit min development time, then constant rate\r\nmindevForm=formula(Smat~1-H(t-tmin)*(1-exp(-m*(t-tmin))))\r\n\r\n#fit via NLS\r\nmindevFit=nlsLM(mindevForm, start=list(m=0.005, tmin=5*24), trace=TRUE)\r\n#best fit parameter estimates\r\nmDev=coef(mindevFit)[1]\r\ntminFit=coef(mindevFit)[2]\r\n\r\nplot(t, Smat)\r\nlines(t, 1-H(t-tminFit)*(1-exp(-mDev*(t-tminFit))))\r\n\r\n\r\n#Fit to Weibull\r\nweibForm=formula(Smat~exp(-log(2)*(t/dm)^ds))\r\n\r\n#fit via NLS\r\nweibFit=nlsLM(weibForm, start=list(dm=250, ds=9), trace=TRUE)\r\n#best fit coefficients\r\ndmfit=coef(weibFit)[1]\r\ndsfit=coef(weibFit)[2]\r\n\r\n\r\n\r\n\r\n####### plot for paper\r\npdf(\'paperplot4.pdf\', width=4, height=2.75)\r\npar(ps=10, mar=c(3,6,1,1)) \r\nplot(t,Smat,xlab="""", ylab="""", pch=20, axes=F, col=\'white\', family=""serif"")\r\nbox()\r\naxis(side=2, las=2)\r\naxis(side=1, las=1, labels=FALSE, at=seq(0,250,50))\r\ntitle(ylab=""Proportion of lice not yet infectious"", family=""serif"", line=3, srt=45)\r\ntitle(xlab=""t (hours)"", family=""serif"", line=2)\r\npoints(t, Smat, pch=20, cex=1.1, col=alpha(\'black\', 0.3))\r\nlines(t, exp(-log(2)*(t/dmfit)^dsfit), lwd=2)\r\nlines(t, 1-H(t-tminFit)*(1-exp(-mDev*(t-tminFit))), lwd=2, lty=""dashed"")\r\nlines(t, exp(-mConst*t), lwd=2, lty=""dotted"")\r\nmtext(seq(0,250,50), side=1, line=0.75, at=seq(0,250,50))\r\nop <- par(cex = 0.6)\r\nlegend(x=""bottomleft"", \r\n       legend =c (""constant maturation rate"",""min development time, then constant rate"",""Weibull maturation""), \r\n       lty = c(""dotted"", ""dashed"", ""solid""),\r\n       lwd = 2,\r\n       bty = \'n\')\r\ndev.off()\r\n\r\n\r\n#Use Weibull for arrival time fit below\r\n\r\n\r\n\r\nfsurvmatfn=function(t2, alpha, v, d, u1, u2, dm, ds, x0, L){\r\nfsm3=rep(0, length(t2))\r\nfor(i in 1:length(t2)){\r\n  fsm3[i]=alpha*integrate(function(x){\r\n    sapply(x, function(x){\r\n      integrate(function(tau){\r\n        sapply(tau, function(tau){\r\n          integrate(function(xi) (1/sqrt(4*pi*d*(t2[i]-tau)))*exp(-exp(u1)*(t2[i]-tau))*exp(-(x-xi-v*(t2[i]-tau))^2/(4*d*(t2[i]-tau)))*(log(2)*ds*dm^(-ds)*tau^(ds-1))*exp(-u2*tau)*(exp(-log(2)*(tau/dm)^ds))*(1/sqrt(4*pi*d*tau))*exp(-(xi-x0-v*tau)^2/(4*d*tau)), -Inf,Inf)$value\r\n        })\r\n      }, 0, t2[i])$value\r\n    })\r\n  }, 0, L)$value\r\n}\r\nreturn(fsm3)\r\n}\r\n\r\n\r\n#first try parameter values\r\n\r\nv=0.15\r\nd=0.6\r\nalpha=0.06\r\ndm=245\r\nds=9\r\nu1=log(0.02)\r\nu2=0.009\r\nx0=-13.5\r\nL=0.1\r\n\r\nplot(t2, fsurvmat2)\r\nlines(t2, fsurvmatfn(t2=t2, alpha=0.006, v=0.148602, d=0.6, u1=log(0.01186204), u2=0.009229426, dm=250, ds=9, x0=-13.5, L=0.1))\r\n\r\nfptSurvMatForm=formula(fsurvmat2~fsurvmatfn(t2, alpha, v, d, u1, u2, dm, ds, x0, L))\r\n\r\n#fit via NLS\r\nfptSurvMat=nlsLM(fptSurvMatForm, start=list(alpha=0, v=0.148602, d=0.6166, u1=log(0.01186204), u2=0.009229426, dm=250.6623, ds=8.94), trace=TRUE)\r\n\r\n\r\n#best fit parameter estimates\r\nalphaSurvMatfit=coef(fptSurvMat)[1]\r\nvSurvMatfit=coef(fptSurvMat)[2]\r\ndSurvMatfit=coef(fptSurvMat)[3]\r\nu1SurvMatfit=coef(fptSurvMat)[4]\r\nu2SurvMatfit=coef(fptSurvMat)[5]\r\ndmSurvMatfit=coef(fptSurvMat)[6]\r\ndsSurvMatfit=coef(fptSurvMat)[7]\r\n\r\n\r\nplot(t2, fsurvmat2, xlab=\'t (hours)\', ylab= \'exp(-u_c t)f_c (t)\')\r\nlines(t2, fsurvmatfn(t2=t2, alpha=alphaSurvMatfit, v=vSurvMatfit, d=dSurvMatfit, u1=u1SurvMatfit, u2=u2SurvMatfit, dm=dmSurvMatfit, ds=dsSurvMatfit, x0=x0, L=L))\r\n\r\n####### plot for paper\r\npdf(\'paperplot3.pdf\', width=4, height=2.75)\r\npar(ps=10, mar=c(3,6,1,1)) \r\nplot(t2,fsurvmat2,xlab="""", ylab="""", pch=20, axes=F, col=\'white\', family=""serif"")\r\nbox()\r\naxis(side=2, las=2)\r\naxis(side=1, las=1, labels=FALSE, at=seq(0,250,50))\r\ntitle(ylab=TeX(\'$e^{-\\\\mu_c t} f_c(t)$\'), family=""serif"", line=4.5, srt=45)\r\ntitle(xlab=""t (hours)"", family=""serif"", line=2)\r\npoints(t2, fsurvmat2, pch=20, cex=1.1, col=alpha(\'black\', 0.3))\r\nlines(t2, fsurvmatfn(t2=t2, alpha=alphaSurvMatfit, v=vSurvMatfit, d=dSurvMatfit, u1=u1SurvMatfit, u2=u2SurvMatfit, dm=dmSurvMatfit, ds=dsSurvMatfit, x0=x0, L=L), lwd=2)\r\nmtext(seq(0,250,50), side=1, line=0.75, at=seq(0,250,50))\r\ndev.off()\r\n\r\n\r\n\r\n########first, vary v and D with constant x0\r\n\r\nvvec=seq(0.01, 1, by=0.01)\r\ndvec=seq(0.01, 1, by=0.01)\r\nfmatrix=matrix(0, nrow=length(vvec), ncol=length(vvec))\r\n\r\n###get total infection pressure for different v and d\r\nfor (i in 1:length(vvec)) {\r\n  print(i)\r\n  for (j in 1:length(dvec)){\r\n    fmatrix[i,j]=integrate(function(t) fsurvmatfn(t2=t, alpha=alphaSurvMatfit, v=vvec[i], d=dvec[j], u1=u1SurvMatfit, u2=u2SurvMatfit, dm=dmSurvMatfit, ds=dsSurvMatfit, x0=x0, L=L), 0, 264)$value\r\n  }\r\n  \r\n}\r\n\r\nwrite.csv(x= fmatrix, file = ""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/vdmatrix.csv"")\r\nsaveRDS(fmatrix, file = ""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/vdmatrix.RDS"")\r\n\r\n\r\nfmatrix=readRDS(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/vdmatrix.RDS"")\r\n\r\nmaxZ=0.0001\r\nnewfmatrix=fmatrix\r\nnewfmatrix[]=ifelse(fmatrix[]>=maxZ,maxZ,NA)\r\n\r\n\r\n#plot for paper\r\npdf(\'paperplotvd.pdf\', width=4, height=2.75)\r\npar(ps=10, mar=c(3,4,1,1)) \r\nimage.plot(t(fmatrix),axis.args = list(at=seq(0, 0.0001, length.out = 6), labels=c(0,2e-05,4e-05,6e-05,8e-05,TeX(\'$\\\\geq 1e-04$\'))), xlab="""", axes=FALSE, ylab="""", zlim=c(0,0.0001), ylim=c(0,0.3))\r\nimage.plot(t(newfmatrix), axis.args = list(at=seq(0, 0.0001, length.out = 6), labels=c(0,2e-05,4e-05,6e-05,8e-05,TeX(\'$\\\\geq 1e-04$\'))), xlab="""", family=""serif"", axes=FALSE, ylab="""", zlim=c(0,0.0001),add=T, ylim=c(0,0.3))\r\naxis(side=2, las=2)\r\naxis(side=1, las=1)\r\ntitle(ylab=""Advection, v (km/h)"", family=""serif"", line=3, srt=45)\r\nmtext(TeX(\'Diffusion, D ($km^2/h$)\'), family=""serif"", side = 1, line=2)\r\ndev.off()\r\n\r\n\r\n\r\n#now vary v and x0\r\nvvec2=seq(0, 0.5, length.out = 101)\r\nx0vec=seq(0, -20, length.out = 101)\r\nfmatrix2=matrix(0, nrow=length(vvec2), ncol=length(vvec2))\r\n\r\n\r\nfor (i in 1:length(vvec2)) {\r\n  print(i)\r\n  for (j in 1:length(x0vec)){\r\n    \r\n    fmatrix2[i,j]=integrate(function(t) fsurvmatfn(t2=t, alpha=alphaSurvMatfit, v=vvec2[i], d=dSurvMatfit, u1=u1SurvMatfit, u2=u2SurvMatfit, dm=dmSurvMatfit, ds=dsSurvMatfit, x0=x0vec[j], L=L), 0, 264)$value\r\n  }\r\n}\r\n\r\nsaveRDS(fmatrix2, file = ""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/vxmatrix.RDS"")\r\nfmatrix2=readRDS(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/vxmatrix2.RDS"")\r\n\r\n\r\n###########plot for paper\r\npdf(\'paperplotvx.pdf\', width=4, height=2.75)\r\npar(ps=10, mar=c(3,4,1,1)) \r\nimage.plot(t(fmatrix2), xlab="""", ylab="""", axes=F, ylim=c(0,0.6))\r\naxis(2, at=seq(0, 1, 0.1), labels = seq(0,0.5,0.05), las=2)\r\naxis(1, at=seq(0,1, 0.2), labels = seq(0,-20,-4), las=1)\r\ntitle(ylab=""Avection, v (km/h)"", family=""serif"", line=3, srt=45)\r\nmtext(TeX(\'Position of initial farm, $x_0$ (km)\'), family=""serif"", side = 1, line=2)\r\ndev.off()\r\n\r\n\r\n#Now vary dm and x0\r\ndmvec=seq(70, 245, length.out = 101)\r\nfmatrix3=matrix(0, nrow=length(dmvec), ncol=length(dmvec))\r\nfor (i in 1:length(dmvec)) {\r\n  print(i)\r\n  for (j in 1:length(x0vec)){\r\n    print(j)\r\n    fmatrix3[i,j]=integrate(function(t) fsurvmatfn(t2=t, alpha=alphaSurvMatfit, v=vSurvMatfit, d=dSurvMatfit, u1=u1SurvMatfit, u2=u2SurvMatfit, dm=dmvec[i], ds=dsSurvMatfit, x0=x0vec[j], L=L), 0, 264)$value\r\n  }\r\n}\r\n\r\nsaveRDS(fmatrix3, file = ""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/dxmatrix.RDS"")\r\nfmatrix3=readRDS(""E:/ConnectivityModel/DanielleFiles/Files for Peter/arrival time/dxmatrix.RDS"")\r\n\r\n###########plot for paper\r\npdf(\'paperplotdx.pdf\', width=4, height=2.75)\r\npar(ps=10, mar=c(3,4,1,1)) \r\nimage.plot(t(fmatrix3), xlab="""", ylab="""", axes=F)\r\naxis(2, at=seq(0, 1, 0.2), labels = seq(70,245, 35), las=2)\r\naxis(1, at=seq(0,1, 0.2), labels = seq(0,-20,-4), las=1)\r\ntitle(ylab=TeX(\'Median maturation time, $\\\\delta_m$ (hours)\'), family=""serif"", line=3, srt=45)\r\nmtext(TeX(\'Position of initial farm, $x_0$ (km)\'), family=""serif"", side = 1, line=2)\r\ndev.off()\r\n\r\n\r\n']","Data from: Timing and probability of arrival for sea lice dispersing between salmon farms Sea lice are a threat to the health of both wild and farmed salmon and an economic burden for salmon farms. With a free-living larval stage, sea lice can disperse tens of kilometers in the ocean between salmon farms, leading to connected sea lice populations that are difficult to control in isolation. In this paper, we develop a simple analytical model for the dispersal of sea lice between two salmon farms. From the model we calculate the arrival time distribution of sea lice dispersing between farms, as well as the level of cross-infection of sea lice. We also use numerical flows from a hydrodynamic model, coupled with a particle tracking model, to directly calculate the arrival time of sea lice dispersing between two farms in the Broughton Archipelago, BC, in order to fit our analytical model and find realistic parameter estimates. Using the parametrized analytical model we show that there is often an intermediate inter-farm spacing that maximizes the level of cross-infection between farms, and that increased temperatures will lead to increased levels of cross-infection.",1
"Decline of the brown-throated sloth (Bradypus variegatus Schinz, 1825) in an Atlantic Forest protected area","Estimates on demographic parameters altogether with social factors are integral and can be very useful to assess the risks that a population may face in the future. Rescue operations may provide a unique opportunity to gather data on individuals of an area and thus provide population information. Animal rescue data provided by the Rio de Janeiro Botanical Garden fauna team were used to understand the structure of a population of Bradypus variegatus in an urban remnant of the Atlantic Forest (Tijuca National Park, PNT). This study aims to provide data on the abundance, density estimation, sex ratio, and occurrence of this population in the PNT. We rescued 44 sloths, four of whom were dead. The population density was estimated at 0.6 ind / ha, a low-density value compared to other urban remnants (8.5 to 12.5 ind / ha). Our model suggests a unstable and in decline population, which could be a delayed reflection of years of deforestation in the Atlantic forest. Although B. variegatus isn't, yet, considered threatened due to their broad distribution, they can be locally extirpated due to population unfeasibility in forest remnants of Atlantic Forest regions, suggesting we should evaluate its threat levels at population level.","['attach(morfo)\r\nrm <- morfometria\r\nrm <- variegatusvariegatus\r\n\r\nvariegatus <- B_variegatus_JB_comcoordenadas\r\nshapiro.test(variegatus[HBL])\r\n\r\n\r\nsexo\r\nsexo <- factor(c(""F"", ""M"", ""M"", ""F""))\r\nlevels(sexo)\r\nclasse <- factor(c(""AD"", ""JV"", ""AD"", ""JV""))\r\nlevels(classe)\r\n\r\nshapiro.test(HBL)\r\nshapiro.test(peso)\r\n\r\nwilcox.test(peso[sexo==""F""], peso[sexo==""M""])\r\nwilcox.test(peso[classe==""AD""], peso[classe==""JV""])\r\n\r\nwilcox.test(HBL[sexo==""F""], HBL[sexo==""M""])\r\nwilcox.test(HBL[classe==""AD""], HBL[classe==""JV""])\r\n\r\nsummary(HBL[sexo==""F""])\r\nsummary(HBL[sexo==""M""])\r\nsummary(peso[sexo==""F""])\r\nsummary(peso[sexo==""M""])\r\n\r\nlibrary(Rcmdr)\r\n', 'library(pwt8)\r\n\r\nsazonal <- ts(B_variegatus_JB_comcoordenadas, frequency = 12, start = 2015.5)\r\nView(sazonal)\r\nsazonal\r\n\r\nshapiro.test(sazonal)\r\n#nao normal\r\n\r\ndecomp <-decompose(sazonal)\r\n\r\nplot(decomp)\r\n\r\nplot(decomp$seasonal)\r\n\r\nlibrary(seastests)\r\nsummary(wo(sazonal))\r\n\r\nplot(sazonal, xlab=""Time"", ylab=""Captures"")\r\ntext.default(2015.9, 6, labels = ""WO-Test = 0\r\n             p-values > 0.5"", cex = .8)\r\n\r\nbirths <- scan(""http://robjhyndman.com/tsdldata/data/nybirths.dat"")\r\n\r\nView(births)\r\nbirths\r\n']","Decline of the brown-throated sloth (Bradypus variegatus Schinz, 1825) in an Atlantic Forest protected area Estimates on demographic parameters altogether with social factors are integral and can be very useful to assess the risks that a population may face in the future. Rescue operations may provide a unique opportunity to gather data on individuals of an area and thus provide population information. Animal rescue data provided by the Rio de Janeiro Botanical Garden fauna team were used to understand the structure of a population of Bradypus variegatus in an urban remnant of the Atlantic Forest (Tijuca National Park, PNT). This study aims to provide data on the abundance, density estimation, sex ratio, and occurrence of this population in the PNT. We rescued 44 sloths, four of whom were dead. The population density was estimated at 0.6 ind / ha, a low-density value compared to other urban remnants (8.5 to 12.5 ind / ha). Our model suggests a unstable and in decline population, which could be a delayed reflection of years of deforestation in the Atlantic forest. Although B. variegatus isn't, yet, considered threatened due to their broad distribution, they can be locally extirpated due to population unfeasibility in forest remnants of Atlantic Forest regions, suggesting we should evaluate its threat levels at population level.",1
Data from: Comparing the impact of future cropland expansion on global biodiversity and carbon storage across models and scenarios,"Land-use change is a direct driver of biodiversity and carbon storage loss. Projections of future land-use often include notable expansion of cropland areas in response to changes in climate and food demand, although there are large uncertainties in results between models and scenarios. This study examines these uncertainties by comparing three different socio-economic scenarios (SSP1-3) across three models (IMAGE, GLOBIOM and PLUMv2). It assesses the impacts on biodiversity metrics and direct carbon loss from biomass and soil as a direct consequence of cropland expansion. Results show substantial variation between models and scenarios, with little overlap across all nine projections. Although SSP1 projects the least impact, there are still significant impacts projected. IMAGE and GLOBIOM project the greatest impact across carbon storage and biodiversity metrics due to both extent and location of cropland expansion. Furthermore, for all the biodiversity and carbon metrics used, there is a greater proportion of variance explained by model used. This demonstrates the importance of improving the accuracy of land-based models. Incorporating effects of land-use change in biodiversity impact assessments would also help better prioritise future protection of biodiverse and carbon-rich areas.","['setwd(\'C:/Users/r07am15.UOA/Documents/My papers/SSP comparison/New AZE sites\')\r\n\r\nlibrary(\'ggplot2\')\r\n\r\npar(mfrow = c(2,2))\r\n\r\n#D)\r\ndatad<-read.csv(\'R_AZE.csv\', na.strings=""<Null>"")\r\nstr(datad)\r\n\r\nggplot(data=datad, aes(x=Model, y=AZE_sites, fill=Model)) + \r\n  geom_bar(stat=""identity"")  +\r\n  facet_grid(~SSP)+ggtitle(""d) Global comparison"") + labs(y=""AZE sites"") + theme_bw()+ theme(panel.grid.minor = element_blank(), strip.text.x = element_text(size = 14), axis.text=element_text(size=12),axis.title.y = element_text(size=14), axis.title.x = element_blank(), panel.grid.major=element_blank(), panel.grid.major.y = element_line( size=0.5, color=""grey"" ), panel.grid.minor.y = element_line( size=0.5, color=""grey""))\r\n\r\n\r\n\r\n\r\nggplot(data=datad, aes(x=Model, y=AZE_sites, fill=Model)) + \r\n  geom_bar(stat=""identity"")  +\r\n  facet_grid(~SSP)+ggtitle(""d) Global comparison"") + labs(y=""AZE sites"") + theme_bw()+ theme(legend.direction = ""horizontal"", strip.text.x = element_text(size = 12), axis.text=element_text(size=12),axis.title.y = element_text(size=14), panel.grid.minor = element_blank(), axis.title.x = element_blank(), panel.grid.major=element_blank(), panel.grid.major.y = element_line( size=0.5, color=""grey"" ), panel.grid.minor.y = element_line( size=0.5, color=""grey""))\r\n\r\ndev.off()\r\n\r\npar(mfrow = c(2,2))\r\n\r\n\r\n#A)\r\n\r\ndataa<-read.csv(\'All_AZE_1.csv\', na.strings=""<Null>"")\r\nstr(dataa)\r\n\r\nggplot(data=dataa, aes(x=Model, y=AZE_sites, fill=Model)) +\r\n  geom_bar(stat=""identity"") +  ylim(0, 100) +\r\n  facet_grid(~Region) + ggtitle(""(a) SSP1"") + labs(y=""AZE sites"") + theme_bw () +  theme(panel.grid.minor = element_blank(), strip.text.x = element_text(size = 12), axis.text=element_text(size=12), panel.grid.major=element_blank(), axis.title.x = element_blank(), axis.title.y = element_text(size=14), panel.grid.major.y = element_line( size=0.5, color=""grey"")\r\n      ) \r\n#B)\r\n\r\ndatab<-read.csv(\'All_AZE_2.csv\', na.strings=""<Null>"")\r\nstr(data)\r\n\r\nggplot(data=datab, aes(x=Model, y=AZE_sites, fill=Model)) + \r\n  geom_bar(stat=""identity"") + \r\n  facet_grid(~Region)+ggtitle(""(b) SSP2"")+ labs(y=""AZE sites"") + theme_bw() + theme(panel.grid.minor = element_blank(), strip.text.x = element_text(size = 12), axis.text=element_text(size=12),axis.title.y = element_text(size=14), panel.grid.major=element_blank(), axis.title.x = element_blank(), panel.grid.major.y = element_line( size=0.5, color=""grey"" ))\r\n\r\n#C)\r\n\r\ndatac<-read.csv(\'All_AZE_3.csv\', na.strings=""<Null>"")\r\nstr(data)\r\n\r\nggplot(data=datac, aes(x=Model, y=AZE_sites, fill=Model)) + \r\n  geom_bar(stat=""identity"") + \r\n  facet_grid(~Region)+ggtitle(""(c) SSP3"")+ labs(y=""AZE sites"") + theme_bw() + theme(panel.grid.minor = element_blank(), strip.text.x = element_text(size = 12), axis.text=element_text(size=12),axis.title.y = element_text(size=14), panel.grid.major=element_blank(), axis.title.x = element_blank(), panel.grid.major.y = element_line( size=0.5, color=""grey"" ))\r\n\r\n\r\n', 'install.packages(\r\n  ""ggplot2"")\r\n\r\nlibrary(ggplot2)\r\n\r\nsetwd(\'C:/Users/r07am15.UOA/Documents/My papers/SSP comparison/\')\r\n\r\n#d\r\n\r\ndata<-read.csv(\'R_carbonloss.csv\', na.strings=""<Null>"")\r\nstr(data)\r\n\r\n\r\nggplot(data=data, aes(x=Model, y=Loss, fill=Carbon)) + geom_bar(stat=""identity"")\r\n\r\ndata <- with(data, data[order(Region, SSP, Loss,])\r\n\r\n\r\nggplot(data=data, aes(x=Model, y=Loss, fill=Carbon)) + \r\n  geom_bar(stat=""identity"") + labs(y=""Carbon loss (tonnes)"") +\r\n  facet_grid(~SSP) +ggtitle(""d) Model comparison"") + theme_bw() + theme(legend.direction = ""horizontal"", panel.grid.minor = element_blank(), axis.title.x = element_blank(), panel.grid.major=element_blank(), panel.grid.major.y = element_line( size=0.5, color=""grey"" ))\r\n\r\nggplot(data=data, aes(x=Model, y=Loss, fill=Carbon)) + \r\n  geom_bar(stat=""identity"") + labs(y=""Carbon loss (tonnes)"") +\r\n  facet_grid(~SSP) +ggtitle(""(d) Global comparison"") + theme_bw() + theme(panel.grid.minor = element_blank(), axis.title.x = element_blank(), panel.grid.major=element_blank(), panel.grid.major.y = element_line( size=0.5, color=""grey"" ))\r\n\r\n\r\n#a)\r\n\r\npar(mfrow=c(2,2))\r\n\r\ndata1<-read.csv(\'R_carbonloss_SSP1.csv\', na.strings=""<Null>"")\r\n\r\nstr(data1)\r\n\r\npar(mfrow=c(2,2))  \r\ndata1 <- with(data1, data1[order(Region, SSP, Loss,])\r\n                        \r\n                                                \r\nggplot(data=data1, aes(x=Model, y=Loss, fill=Carbon)) + \r\n                          geom_bar(stat=""identity"") + labs(y=""Carbon loss (tonnes)"") + ylim(0, 2.0e+10) +\r\n                          facet_grid(~Region) +ggtitle(""(a) SSP1"")+ theme_bw() + theme(panel.grid.minor = element_blank(), axis.title.x = element_blank(), panel.grid.major=element_blank(), panel.grid.major.y = element_line( size=0.5, color=""grey"" ))\r\n\r\n#B)\r\n\r\ndata2<-read.csv(\'R_carbonloss_SSP2.csv\', na.strings=""<Null>"")\r\nstr(data2)\r\n\r\n\r\ndata2 <- with(data2, data2[order(Region, SSP, Loss,])\r\n                           \r\n                           \r\n                           ggplot(data=data2, aes(x=Model, y=Loss, fill=Carbon)) + \r\n                             geom_bar(stat=""identity"") + labs(y=""Carbon loss (tonnes)"") + ylim(0, 2.0e+10) +\r\n                             facet_grid(~Region) +ggtitle(""(b) SSP2"")+ theme_bw() + theme(panel.grid.minor = element_blank(), axis.title.x = element_blank(), panel.grid.major=element_blank(), panel.grid.major.y = element_line( size=0.5, color=""grey"" ))\r\n                           \r\n#C)\r\n                           \r\ndata3<-read.csv(\'R_carbonloss_SSP3.csv\', na.strings=""<Null>"")\r\n                           str(data2)\r\n                           \r\n                           \r\n                           data3 <- with(data3, data3[order(Region, SSP, Loss,])\r\n                                                      \r\n                                                      \r\n                                                      ggplot(data=data3, aes(x=Model, y=Loss, fill=Carbon)) + \r\n                                                        geom_bar(stat=""identity"") + labs(y=""Carbon loss (tonnes)"") +\r\n                                                        facet_grid(~Region) +ggtitle(""(c) SSP3"")+ theme_bw() + theme(panel.grid.minor = element_blank(), axis.title.x = element_blank(), panel.grid.major=element_blank(), panel.grid.major.y = element_line( size=0.5, color=""grey"" ))\r\n', '\r\n\r\nrequire(data.table)\r\nrequire(ggplot2)\r\nrequire(gridExtra)\r\nlibrary(raster)\r\n\r\nrequire(reshape2)\r\nrequire(maptools)\r\n\r\ndata(wrld_simpl)\r\ndirectory=""C:\\\\Users\\\\rhenry2\\\\Desktop\\\\molotoks_et_al""\r\nshpFile = crop(readShapePoly(file.path(directory,""ne_110m_admin_0_countries_lakes.shp""), proj4string=CRS(""+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"")), extent(-180,180, -55, 90))\r\nallSppRichness=fread(file.path(directory, ""richnessData.csv""))\r\nareaFile = fread(file.path(directory,""LandUse2010.txt""))\r\narea= areaFile[,list(Lat,Lon,area)]\r\narea[, c(\'Lon\', \'Lat\') := list(Lon+0.25, Lat+0.25)] ##move to bottom left hand corner \r\nareaRaster= rasterFromXYZ(area[, list(Lon, Lat,area)], crs = ""+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"")\r\n\r\nsetDataTable = function(rasterIn2050, rasterIn2010){\r\n  \r\n  rasterDiff = data.table(as.data.frame((raster(rasterIn2050, header=TRUE)*areaRaster) - \r\n                                          (raster(rasterIn2010, header=TRUE)*areaRaster),xy=TRUE))\r\n  \r\n  setnames(rasterDiff, names(rasterDiff), c(\'Lon\', \'Lat\', \'croplandDiff\'))\r\n  rasterDiff[, c(\'Lon\', \'Lat\') := list(Lon-0.25, Lat-0.25)] ##move to bottom left hand corner \r\n  \r\n  raster2010=data.table(as.data.frame(raster(rasterIn2010, header=TRUE)*areaRaster,xy=TRUE))\r\n  setnames(raster2010, names(raster2010), c(\'Lon\', \'Lat\', \'cropland2010\'))\r\n  raster2010[, c(\'Lon\', \'Lat\') := list(Lon-0.25, Lat-0.25)] ##move to bottom left hand corner \r\n  \r\n  rasterDt = merge(rasterDiff,raster2010, by=c(\'Lat\',\'Lon\'))\r\n  dtAll = merge(rasterDt,allSppRichness,by=c(\'Lat\',\'Lon\'),all=TRUE)\r\n}\r\n\r\nsetDataTablePlum = function(rasterIn2050, rasterIn2010){\r\n  \r\n  rasterDiff = data.table(as.data.frame((raster(rasterIn2050, header=TRUE)) - \r\n                                          (raster(rasterIn2010, header=TRUE)),xy=TRUE))\r\n  \r\n  setnames(rasterDiff, names(rasterDiff), c(\'Lon\', \'Lat\', \'croplandDiff\'))\r\n  rasterDiff[, c(\'Lon\', \'Lat\') := list(Lon-0.25, Lat-0.25)] ##move to bottom left hand corner \r\n  \r\n  raster2010=data.table(as.data.frame(raster(rasterIn2010, header=TRUE),xy=TRUE))\r\n  setnames(raster2010, names(raster2010), c(\'Lon\', \'Lat\', \'cropland2010\'))\r\n  raster2010[, c(\'Lon\', \'Lat\') := list(Lon-0.25, Lat-0.25)] ##move to bottom left hand corner \r\n  \r\n  rasterDt = merge(rasterDiff,raster2010, by=c(\'Lat\',\'Lon\'))\r\n  dtAll = merge(rasterDt,allSppRichness,by=c(\'Lat\',\'Lon\'),all=TRUE)\r\n}\r\n\r\n\r\n\r\nIMAGE_SSP1= setDataTable(file.path(directory,""IMAGE\\\\IMAGE\\\\2050_crop_ssp1.asc""),file.path(directory,""IMAGE\\\\IMAGE\\\\2010_crop_ssp1.asc""))\r\nIMAGE_SSP2= setDataTable(file.path(directory,""IMAGE\\\\IMAGE\\\\2050_crop_ssp2.asc""),file.path(directory,""IMAGE\\\\IMAGE\\\\2010_crop_ssp2.asc""))\r\nIMAGE_SSP3= setDataTable(file.path(directory,""IMAGE\\\\IMAGE\\\\2050_crop_ssp3.asc""),file.path(directory,""IMAGE\\\\IMAGE\\\\2010_crop_ssp3.asc""))\r\n\r\nGLOBIOM_SSP1= setDataTable(file.path(directory,""GloBIOM\\\\GloBIOM\\\\GB_c_2050_SSP1.asc""),file.path(directory,""GloBIOM\\\\GloBIOM\\\\GB_c_2010_SSP1.asc""))\r\nGLOBIOM_SSP2= setDataTable(file.path(directory,""GloBIOM\\\\GloBIOM\\\\GB_c_2050_SSP2.asc""),file.path(directory,""GloBIOM\\\\GloBIOM\\\\GB_c_2010_SSP2.asc""))\r\nGLOBIOM_SSP3= setDataTable(file.path(directory,""GloBIOM\\\\GloBIOM\\\\GB_c_2050_SSP3.asc""),file.path(directory,""GloBIOM\\\\GloBIOM\\\\GB_c_2010_SSP3.asc""))\r\n\r\nPLUM_SSP1= setDataTablePlum(file.path(directory,""PLUM\\\\SSP1_croplandArea2050.asc""),file.path(directory,""PLUM\\\\SSP1_croplandArea2010.asc""))\r\nPLUM_SSP2= setDataTablePlum(file.path(directory,""PLUM\\\\SSP2_croplandArea2050.asc""),file.path(directory,""PLUM\\\\SSP2_croplandArea2010.asc""))\r\nPLUM_SSP3= setDataTablePlum(file.path(directory,""PLUM\\\\SSP3_croplandArea2050.asc""),file.path(directory,""PLUM\\\\SSP3_croplandArea2010.asc""))\r\n\r\nwhereExpansion = function(label, n, plumDt, imageDt, globiomDt, species){\r\n  \r\n  \r\n  colN = n\r\n  #dt = allInfo[get(paste0(colN)) > 0.5]\r\n  #dt = allInfo[get(paste0(colN)) >= median(allInfo[,get(paste0(colN))], na.rm=TRUE)]\r\n  PLUM_expansion = plumDt[get(paste0(colN)) >= quantile(plumDt[,get(paste0(colN))],0.9,type=7 ,na.rm=TRUE) & croplandDiff > 0 , list(Lat, Lon,metric=1)]\r\n  IMAGE_expansion = imageDt[get(paste0(colN)) >= quantile(imageDt[,get(paste0(colN))],0.9,type=7 ,na.rm=TRUE) & croplandDiff > 0, list(Lat, Lon,metric=1)]\r\n  GLOBIOM_expansion = globiomDt[get(paste0(colN)) >= quantile(globiomDt[,get(paste0(colN))],0.9,type=7 ,na.rm=TRUE) & croplandDiff > 0, list(Lat, Lon,metric=1)]\r\n  \r\n  mergeOne = merge(PLUM_expansion,IMAGE_expansion, by=c(\'Lat\',\'Lon\'))\r\n  mergeTwo = merge(mergeOne,GLOBIOM_expansion, by=c(\'Lat\',\'Lon\'))\r\n  \r\n  percCells = nrow(mergeTwo)/nrow(plumDt[get(paste0(colN)) >= quantile(plumDt[,get(paste0(colN))],0.9,type=7 ,na.rm=TRUE)])*100\r\n  \r\n  plot(shpFile,col=""gray97"",border=""gray97"", lwd=0.05)\r\n  plot(rasterFromXYZ(mergeTwo[,list(Lon,Lat,metric.x=1)]),col=\'red\', legend=FALSE, add=TRUE)\r\n  \r\n  bound = rasterFromXYZ(plumDt[get(paste0(colN)) >= quantile(plumDt[,get(paste0(colN))],0.9,type=7 ,na.rm=TRUE),list(Lon, Lat, get(paste0(colN)))], crs = ""+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"")\r\n  r <- reclassify(bound, cbind(-Inf, Inf, 1))\r\n  pp <- rasterToPolygons(r, dissolve=TRUE)\r\n  plot(pp, lwd=0.5, border=\'gray48\', add=TRUE)\r\n  \r\n  mtext(label, side=3, line=-16, cex=0.75)\r\n  \r\n  mtext(paste0(""In "",round(percCells,1),""% of "",species,"" cells models agree cropland expansion will occur""), side=3, line=-18, cex=0.75)\r\n  \r\n}\r\n\r\n\r\npng(width=6,height=8, units=""in"",res=100, file = file.path(directory,""figures\\\\hotspots.png""))\r\npar(mfrow=c(3, 1), mar=c(2, 2,2, 2))\r\n\r\nwhereExpansion(""(a) SSP1"", ""hotspots"", PLUM_SSP1,IMAGE_SSP1,GLOBIOM_SSP1, species=\'hotspots\')\r\n\r\nwhereExpansion(""(b) SSP2"", ""hotspots"", PLUM_SSP2,IMAGE_SSP2,GLOBIOM_SSP2, species=\'hotspots\')\r\n\r\nwhereExpansion(""(c) SSP3"", ""hotspots"", PLUM_SSP3,IMAGE_SSP3,GLOBIOM_SSP3, species=\'hotspots\')\r\n\r\ndev.off()\r\n\r\npng(width=6,height=8, units=""in"",res=100, file = file.path(directory,""figures\\\\mammals.png""))\r\npar(mfrow=c(3, 1), mar=c(2, 2,2, 2))\r\n\r\nwhereExpansion(""(a) SSP1"", ""mammals_wgs"", PLUM_SSP1,IMAGE_SSP1,GLOBIOM_SSP1, species=\'mammal rich\')\r\n\r\nwhereExpansion(""(b) SSP2"", ""mammals_wgs"", PLUM_SSP2,IMAGE_SSP2,GLOBIOM_SSP2, species=\'mammal rich\')\r\n\r\nwhereExpansion(""(c) SSP3"", ""mammals_wgs"", PLUM_SSP3,IMAGE_SSP3,GLOBIOM_SSP3, species=\'mammal rich\')\r\n\r\ndev.off()\r\n\r\npng(width=6,height=8, units=""in"",res=100, file = file.path(directory,""figures\\\\birds.png""))\r\npar(mfrow=c(3, 1), mar=c(2, 2,2, 2))\r\n\r\nwhereExpansion(""(a) SSP1"", ""birds_wgs"", PLUM_SSP1,IMAGE_SSP1,GLOBIOM_SSP1, species=\'bird rich\')\r\n\r\nwhereExpansion(""(b) SSP2"", ""birds_wgs"", PLUM_SSP2,IMAGE_SSP2,GLOBIOM_SSP2, species=\'bird rich\')\r\n\r\nwhereExpansion(""(c) SSP3"", ""birds_wgs"", PLUM_SSP3,IMAGE_SSP3,GLOBIOM_SSP3, species=\'bird rich\')\r\n\r\ndev.off()\r\n\r\npng(width=6,height=8, units=""in"",res=100,  file = file.path(directory,""figures\\\\amphibian.png""))\r\npar(mfrow=c(3, 1), mar=c(2, 2,2, 2))\r\n\r\nwhereExpansion(""(a) SSP1"", ""amphib_wgs"", PLUM_SSP1,IMAGE_SSP1,GLOBIOM_SSP1, species=\'amphibian rich\')\r\n\r\nwhereExpansion(""(b) SSP2"", ""amphib_wgs"", PLUM_SSP2,IMAGE_SSP2,GLOBIOM_SSP2, species=\'amphibian rich\')\r\n\r\nwhereExpansion(""(c) SSP3"", ""amphib_wgs"", PLUM_SSP3,IMAGE_SSP3,GLOBIOM_SSP3, species=\'amphibian rich\')\r\n\r\ndev.off()\r\n\r\n\r\n\r\n\r\n\r\nplotRiskIndex = function() {\r\n  \r\n  PLUM_cropFractChange_SSP1 =  raster(file.path(directory,""PLUM\\\\SSP1_croplandFractChange.asc""), header=TRUE)\r\n  IMAGE_cropFractChange_SSP1 = raster(file.path(directory,""IMAGE\\\\IMAGE\\\\Fraction_change_crop_ssp1.asc""), header=TRUE)\r\n  GLOBIOM_cropFractChange_SSP1 = raster(file.path(directory,""GloBIOM\\\\GloBIOM\\\\gb_fractionchange_SSP1.asc""), header=TRUE)\r\n  \r\n  PLUM_cropFractChange_SSP2 =  raster(file.path(directory,""PLUM\\\\SSP2_croplandFractChange.asc""), header=TRUE)\r\n  IMAGE_cropFractChange_SSP2 = raster(file.path(directory,""IMAGE\\\\IMAGE\\\\Fraction_change_crop_ssp2.asc""), header=TRUE)\r\n  GLOBIOM_cropFractChange_SSP2 = raster(file.path(directory,""GloBIOM\\\\GloBIOM\\\\gb_fractionchange_SSP2.asc""), header=TRUE)\r\n  \r\n  PLUM_cropFractChange_SSP3 =  raster(file.path(directory,""PLUM\\\\SSP3_croplandFractChange.asc""), header=TRUE)\r\n  IMAGE_cropFractChange_SSP3 = raster(file.path(directory,""IMAGE\\\\IMAGE\\\\Fraction_change_crop_ssp3.asc""), header=TRUE)\r\n  GLOBIOM_cropFractChange_SSP3 = raster(file.path(directory,""GloBIOM\\\\GloBIOM\\\\gb_fractionchange_SSP3.asc""), header=TRUE)\r\n  \r\n  \r\n  \r\n  plotBiodivOverlap = function(r, label, mars=c(0,0.5,0,0.5), plotLegendOnly=FALSE, meanRaster) {\r\n    \r\n    lossDt = data.table(as.data.frame(meanRaster, xy=TRUE))\r\n    \r\n    setnames(lossDt, names(lossDt), c(\'Lon\', \'Lat\', \'croplandInc\'))\r\n    lossDt = lossDt[croplandInc <0, croplandInc := 0]\r\n    lossDt[,Lon := round(Lon,2)-0.25] ##rounding to make rows match , 1e+14 difference, throws merge function off\r\n    lossDt[,Lat := round(Lat,2)-0.25]\r\n    mergedDt = merge(lossDt, r, by=c(\'Lat\',\'Lon\'), all=TRUE)\r\n    \r\n    par(mar=mars)\r\n    breakpoints <- c(seq(from=0,to=1,by=0.01))#0,seq(from=0,to=0.1,by=0.02))\r\n    colfuncR <- colorRampPalette(c(""gray97"",""chocolate1"",""firebrick1""))\r\n    colors <- c(colfuncR(100))\r\n    #colors = (blue2red(100))\r\n    \r\n    if(!plotLegendOnly){\r\n      plot(shpFile, col=""gray97"",border=""gray97"", lwd=0.05)\r\n      plot(rasterFromXYZ(mergedDt[,list(Lon, Lat, percentile_biodiv*(croplandInc))], \r\n                         crs = ""+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs""),\r\n           col=colors,breaks=breakpoints, box=FALSE, legend=FALSE, add=TRUE)\r\n      abline(h=23.5, col=""blue"", lty=\'dotted\')\r\n      abline(h=-23.5, col=""blue"", lty=\'dotted\')\r\n      plot(shpFile, add=TRUE,border=""black"", lwd=0.05)\r\n      mtext(label, side=3, line=-5, cex=1.75)\r\n    }\r\n    else{\r\n      plot(rasterFromXYZ(mergedDt[,list(Lon, Lat, percentile_biodiv*(croplandInc))], \r\n                         crs = ""+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs""), \r\n           col=colors,breaks=c(seq(from=0,to=1,by=0.01)), box=FALSE, \r\n           legend.only=TRUE, legend.width=0.7, horizontal=TRUE,legend.mar=1.5, legend.shrink=0.3,\r\n           axis.args=list(at=seq(0, 1, 0.2), labels=seq(0, 1, 0.2), cex.axis=1.5,  mgp=c(3, .3, 0)),\r\n           legend.args=list(text="""", side=4, line=2.5, cex=1.5))\r\n    }\r\n    \r\n    \r\n    \r\n  }\r\n  \r\n  \r\n  png(width=20,height=14, units=\'in\', res=200, file = file.path(directory,""figures//sppThreatMapsMean.png""))\r\n  \r\n  par(mfrow=c(3, 3), mar=c(1, 1, 1, 1))\r\n  \r\n  plotBiodivOverlap(allSppRichness[,list(Lat,Lon,percentile_biodiv = ecdf(allSppRichness$totalAllRichness)(allSppRichness$totalAllRichness))],label=""(a) PLUMv2 SSP1\\n "", meanRaster =PLUM_cropFractChange_SSP1 )\r\n  plotBiodivOverlap(allSppRichness[,list(Lat,Lon,percentile_biodiv = ecdf(allSppRichness$totalAllRichness)(allSppRichness$totalAllRichness))],label=""(b) IMAGE SSP1\\n "", meanRaster =IMAGE_cropFractChange_SSP1 )\r\n  plotBiodivOverlap(allSppRichness[,list(Lat,Lon,percentile_biodiv = ecdf(allSppRichness$totalAllRichness)(allSppRichness$totalAllRichness))],label=""(c) GLOBIOM SSP1\\n "", meanRaster =GLOBIOM_cropFractChange_SSP1 )\t\r\n  plotBiodivOverlap(allSppRichness[,list(Lat,Lon,percentile_biodiv = ecdf(allSppRichness$totalAllRichness)(allSppRichness$totalAllRichness))],label=""(d) PLUMv2 SSP2\\n "", meanRaster =PLUM_cropFractChange_SSP2 )\r\n  plotBiodivOverlap(allSppRichness[,list(Lat,Lon,percentile_biodiv = ecdf(allSppRichness$totalAllRichness)(allSppRichness$totalAllRichness))],label=""(e) IMAGE SSP2\\n "", meanRaster =IMAGE_cropFractChange_SSP2 )\r\n  plotBiodivOverlap(allSppRichness[,list(Lat,Lon,percentile_biodiv = ecdf(allSppRichness$totalAllRichness)(allSppRichness$totalAllRichness))],label=""(f) GLOBIOM SSP2\\n "", meanRaster =GLOBIOM_cropFractChange_SSP2 )\t\r\n  plotBiodivOverlap(allSppRichness[,list(Lat,Lon,percentile_biodiv = ecdf(allSppRichness$totalAllRichness)(allSppRichness$totalAllRichness))],label=""(g) PLUMv2 SSP3\\n "", meanRaster =PLUM_cropFractChange_SSP3 )\r\n  plotBiodivOverlap(allSppRichness[,list(Lat,Lon,percentile_biodiv = ecdf(allSppRichness$totalAllRichness)(allSppRichness$totalAllRichness))],label=""(h) IMAGE SSP3\\n "", meanRaster =IMAGE_cropFractChange_SSP3 )\r\n  plotBiodivOverlap(allSppRichness[,list(Lat,Lon,percentile_biodiv = ecdf(allSppRichness$totalAllRichness)(allSppRichness$totalAllRichness))],label=""(i) GLOBIOM SSP3\\n "", meanRaster =GLOBIOM_cropFractChange_SSP3 )\r\n  \r\n  \r\n  par(mfrow=c(1, 1), mar=c(1,1, 1, 1), new=FALSE)\r\n  plotBiodivOverlap(allSppRichness[,list(Lat,Lon,percentile_biodiv = ecdf(allSppRichness$totalAllRichness)(allSppRichness$totalAllRichness))],label="""",plotLegendOnly=TRUE,meanRaster =PLUM_cropFractChange_SSP1)\r\n  \r\n  mtext(""Threat index"", side = 3, line = -68, outer = TRUE, cex=1.75)\r\n  dev.off()\r\n  \r\n}\r\n\r\nplotRiskIndex()\r\n', 'setwd(\'C:/Users/r07am15/Documents/Papers/Model comparison paper/Revisions/Minor\')\r\n\r\nlibrary(\'ggplot2\')\r\n\r\ndata<-read.csv(\'Figure2data.csv\', na.strings=""<Null>"")\r\nstr(data)\r\n\r\n#CI hotspots\r\nggplot(data, aes(x=Total.crop.exp, y=CI.Hotspots, shape=SSP, color=Model))+geom_point(size=3) + theme (panel.background = element_blank(), axis.line = element_line(colour = ""black"")) + theme(axis.text=element_text(size=12), axis.title.y = element_text(size=12), axis.title.x = element_text(size=12))+xlab(""Total cropland expansion (sq km)"") +ylab(""Cropland expansion within species rich area (sq km)"") + ggtitle(""d) CI hotspots"") \r\n\r\n\r\n#amphibians                                                                                                \r\nggplot(data, aes(x=Total.crop.exp, y=Amphibians, shape=SSP, color=Model))+geom_point(size=3) + theme (panel.background = element_blank(), axis.line = element_line(colour = ""black"")) + theme(axis.text=element_text(size=12), axis.title.y = element_text(size=12), axis.title.x = element_text(size=12))+xlab(""Total cropland expansion (sq km)"") +ylab(""Cropland expansion within species rich area (sq km)"") + ggtitle(""a) Amphibian species rich regions"") \r\n\r\n\r\n#birds\r\nggplot(data, aes(x=Total.crop.exp, y=Birds, shape=SSP, color=Model))+geom_point(size=3) + theme (panel.background = element_blank(), axis.line = element_line(colour = ""black"")) + theme(axis.text=element_text(size=12), axis.title.y = element_text(size=12), axis.title.x = element_text(size=12))+xlab(""Total cropland expansion (sq km)"") +ylab(""Cropland expansion within species rich area (sq km)"") + ggtitle(""b) Bird species rich regions"") \r\n\r\n#mammals\r\n\r\nggplot(data, aes(x=Total.crop.exp, y=Mammals, shape=SSP, color=Model))+geom_point(size=3) + theme (panel.background = element_blank(), axis.line = element_line(colour = ""black"")) + theme(axis.text=element_text(size=12), axis.title.y = element_text(size=12), axis.title.x = element_text(size=12))+xlab(""Total cropland expansion (sq km)"") +ylab(""Cropland expansion within species rich area (sq km)"") + ggtitle(""c) Mammal species rich regions"") \r\n']","Data from: Comparing the impact of future cropland expansion on global biodiversity and carbon storage across models and scenarios Land-use change is a direct driver of biodiversity and carbon storage loss. Projections of future land-use often include notable expansion of cropland areas in response to changes in climate and food demand, although there are large uncertainties in results between models and scenarios. This study examines these uncertainties by comparing three different socio-economic scenarios (SSP1-3) across three models (IMAGE, GLOBIOM and PLUMv2). It assesses the impacts on biodiversity metrics and direct carbon loss from biomass and soil as a direct consequence of cropland expansion. Results show substantial variation between models and scenarios, with little overlap across all nine projections. Although SSP1 projects the least impact, there are still significant impacts projected. IMAGE and GLOBIOM project the greatest impact across carbon storage and biodiversity metrics due to both extent and location of cropland expansion. Furthermore, for all the biodiversity and carbon metrics used, there is a greater proportion of variance explained by model used. This demonstrates the importance of improving the accuracy of land-based models. Incorporating effects of land-use change in biodiversity impact assessments would also help better prioritise future protection of biodiverse and carbon-rich areas.",1
Flows of invasive ants worldwide to the United States,"International trade and human movements have accidentally transported thousands of species worldwide at an unprecedented scale. The resulting biological invasions are among the greatest drivers of species extinctions and can cause enormous economic losses. Understanding how globalization affects the accidental transport of species is urgent to prevent new invasions. However, global trade networks have had mixed success so far in explaining intercontinental species movements. Here, we show that commonly used proxies of global trade flows such as general imports and agricultural imports differed greatly from flows of alien ants from their donor regions to the United States. The analysis of 97 individual commodity flows revealed instead that plants and fruit imports, which are a small subset of all agricultural commodities, were associated with invasion flows. All 95 other commodities differed from flows of alien ants, including most ""agricultural"" commodities which had extremely heterogenous geographic origins. This highlights the need to know precisely which commodities serve as introduction pathways for a particular taxonomic group in order to explain invasion flows and identify likely source regions of future invasions in a world of changing trade relationships.","['####\t####\t####\t####\r\n####\tTrade knowledge is needed to understand invasion flows\r\n# Revised the 25th of June 2021 / the 04th of AUgust 2021\r\n####\t####\t####\t####\r\n\r\n#### Packages\r\nlibrary(sf)\r\nlibrary(sp)\r\nlibrary(raster)\r\nlibrary(cartography)\r\n\r\nlibrary(ade4)\r\nlibrary(ape)\r\nlibrary(dendextend) \r\nlibrary(phytools)\r\nlibrary(vegan) \r\n\r\nlibrary(tradestatistics)\r\nlibrary(circlize)\r\n\r\nlibrary(plotrix)\r\nlibrary(png)\r\nlibrary(TeachingDemos)\r\nlibrary(xlsx)\r\n\r\nwindowsFonts()\r\nlibrary(extrafont)\r\nloadfonts(device = ""win"")\r\nwindowsFonts()\r\n\r\n\r\n#### Functions and datasets\r\n### Functions \r\n## Clustering\r\n# ""rtest.hclust"" : non parametric test of Greenacre, M. (2011). A Simple Permutation Test for Clusteredness. Economics Working Papers, (April).\r\n""rtest.hclust"" <- function(li, method = ""ward.D"", nsim = 999){\r\n        d <- dist(li)\r\n        hc <- hclust(d, method)\r\n        obs <- hc$height\r\n\r\n        sim <- matrix(0, nrow = nsim, ncol = length(obs))\r\n        for(i in 1:nsim){\r\n                li_sim <- apply(li, 2, function(x) sample(x, replace = FALSE))\r\n                hc_sim <- hclust(dist(li_sim), method)\r\n                sim[i,] <- hc_sim$height\r\n        # print(i)\r\n        }\r\n\r\n        pval <- numeric(0)\r\n        for(i in 1:length(obs)){\r\n                p <- sum(sim[,i] < obs[i]) / nsim\r\n                pval <- c(pval, p)\r\n        }\r\n        s <- 0.05 / (length(obs))\r\n        n <- sum(pval < s)\r\n        print(paste(""% of significative nodes :"", n / length(obs) * 100))\r\n\r\n        index <- which(pval < s)\r\n        if(length(index) == 0){\r\n                cut <- 0\r\n                }       else{\r\n                                m <- max(index)\r\n                                cut <- mean(obs[m:(m+1)])                               \r\n                        }\r\nreturn(cut)\r\n}\r\n\r\n### Datasets\r\nload(""./Papers/2021-Frontiers in Ecology and the Environment/last version/data_dryad.RData"")\r\n\t## Trade data : see Trade section in CleoGroup\r\n\t# ""TS"" : USA trade data from tradestatistics\r\n\t# ""X"", ""XX"", ""XXXX"" : commodities names by categories\r\n\t# distri : profil of commodity flows (XX scale)\r\n\t# distri_s : normalized profil of commodity flows\r\n\t# distridistri : profil of commodity flows (XXXX scale)\r\n\t# distridistri_s : normalized profil of commodity flows\r\n\t# df : normalized profil of total commodity flows\r\n\r\n\t## Data associated to countries\r\n\t# ""col3"" : color code for continents\r\n\r\n\t## Icons : see icons\r\n\t# ""rast"" : icons for type of trade in raster format\r\n\t# ""wheat"" : icon for agriculture trade in png format\r\n\r\n\r\n#### Analysis\r\n### Correspondance analysis\r\ncoa <- dudi.coa(distri_s, scannf = F, nf = 5)\r\nsum(coa$eig[1:2]) / sum(coa$eig) * 100\r\nsup <- suprow(coa, df)\r\nli <- rbind.data.frame(coa$li, sup$lisup)\r\n\r\n## Kaiser-Guttman criterion\r\neig <- coa$eig\r\nm <- mean(eig)\r\nbarplot(eig, main = ""Eigenvalue"", col = ""bisque"", las = 2)\r\nabline(h = m, col = ""red"")\r\n\r\n## Brocken-Stick model\r\n# See Frontier :  tude de la dcroissance des valeurs propres dans une analyse en composantes principales - 1976\r\nl <- sum(eig)\r\nS <- length(eig)\r\nEj <- NULL\r\nfor(j in 1:S){\r\n\tres <- 0\r\n\tfor(i in 0:(S-j)){\r\n\t\tres <- res + 1/(j+i)\r\n\t}\r\n\tEj <- c(Ej, res * l/S)\r\n}\r\n\r\n# See Legendre : Numerical ecology with R - 2018 (section 5.4.2)\r\nca <- cca(distri_s)\r\nbsm <- bstick(ca)\r\n\r\n### Classification on factorial coordinates\r\nd <- dist(li)\r\nhc <- hclust(d, method = ""ward.D"")\r\ndend <- as.dendrogram(hc)\r\n\r\n### Permutation test for clusterdness with Ward\'s criterion\r\ncut <- rtest.hclust(li)\r\nfac <- cutree(hc, h = cut)\r\nfac <- as.factor(fac)\r\n\r\n### Comparison with other clustering methods\r\nmethods <- c(""ward.D2"", ""single"", ""complete"", ""average"", ""mcquitty"", ""ward.D"")\r\nres <- vector(""list"", length(methods))\r\nnames(res) <- methods\r\nfor(m in methods){\r\n\thc <- hclust(d, method = m)\r\n\tcut <- rtest.hclust(li, method = m)\r\n\tfac <- cutree(hc, h = cut)\r\n\tfac <- as.factor(fac)\r\n\tres[[m]] <- names(fac[fac == fac[""Alien ants""]])\r\n}\r\nres\r\n\r\n### Reorder dendrogram\r\nindex.row <- hc$order\r\nlocal <- fac[index.row]\r\nlevels(fac)[unique(local)] <- 1:nlevels(fac)\r\nfac[index.row]\r\n\r\n\r\n#### Supplemntary analysis for reviewers\r\n#--> Do not apply this part : for answer to reviewer only\r\n### Reorder dendrogram\r\n## Reorder dendrogram (for 2008 only)\r\n# Inverse cluster 17 and 18 \r\nlevels(fac)[levels(fac) == ""18""] <- ""19""\r\nlevels(fac)[levels(fac) == ""17""] <- ""18""\r\nlevels(fac)[levels(fac) == ""19""] <- ""17""\r\n\r\n## Reorder dendrogram\r\nlab <- labels(dend)\r\no <- c(1:97, 99:101, 98)\r\ndend <- dendextend::rotate(dend, lab[o])\r\nlab <- labels(dend)\r\n\r\n## Reorder dendrogram according to factorial score (axis 1)\r\no <- li[,1]\r\nnames(o) <- row.names(li)\r\nlocal <- split(o[lab], fac[lab])\r\nlocal <- local[as.character(1:nlevels(fac))]\r\nlocal <- lapply(local, function(x) names(sort(x, decreasing = TRUE)))\r\no <- unlist(local)\r\ndend <- dendextend::rotate(dend, o)\r\nlab <- labels(dend)\r\n\r\n## Reorder dendrogram (for Cumsum only)\r\nlab <- labels(dend)\r\no <- c(1:39, 41:40, 42:101)\r\ndend <- dendextend::rotate(dend, lab[o])\r\nlab <- labels(dend)\r\n\r\n### Chisq-tests\r\n## With global trade patterns\r\nchisq.test(unlist(Exo[3,]), p = unlist(Cum_s[1,]), simulate.p.value = T)\r\nchisq.test(unlist(Exo[3,]), p = unlist(Cum_s[2,]), simulate.p.value = T)\r\nchisq.test(unlist(Exo[3,]), p = unlist(Cum_s[3,]), simulate.p.value = T)\r\n\r\n## With categories in 06, 07 and 08 sections\r\n# Chisq test\r\nindex <- which((XXXX$XX_code == ""06"") | (XXXX$XX_code == ""07"") | (XXXX$XX_code == ""08""))\r\nnames(index) <- row.names(distridistri_s[index,])\r\nlength(index)\r\n\r\npval <- NULL\r\nfor(i in index){\r\n\tpval <- c(pval, chisq.test(unlist(Exo[3,]), p = unlist(distridistri_s[i,]), simulate.p.value = T)$p.value)\r\n}\r\nnames(pval) <- names(index)\r\n\r\nlocal <- which(pval > (0.05 / length(index)))\r\nXXXX[index[names(local)],]\r\n\r\n# Clustering\r\ncoabis <- dudi.coa(distridistri_s, scannf = F, nf = 5)\r\nsum(coabis$eig[1:2]) / sum(coabis$eig) * 100\r\nsupbis <- suprow(coabis, df)\r\nlibis <- rbind.data.frame(coabis$li, supbis$lisup)\r\ndbis <- dist(libis)\r\nhcbis <- hclust(dbis, method = ""ward.D"")\r\ncutbis <- rtest.hclust(libis)\r\nfacbis <- cutree(hcbis, h = cutbis)\r\nfacbis <- as.factor(facbis)\r\ncat <- names(facbis[facbis == facbis[""Alien ants""]])\r\n\r\ndendbis <- as.dendrogram(hcbis)\r\nplot(dendbis)\r\nabline(h = cutbis, col = ""red"", lty = 2)\r\n\r\ncat02 <- substring(cat, 1, 2)\r\nindex <- which((cat02 == ""06"") | (cat02 == ""07"") | (cat02 == ""08""))\r\ncat <- cat[index]\r\nXXXX[match(cat, XXXX$XXXX_code),]\r\n\r\n\t#--> circlize plot\r\n\t\t#--> plot XX names\r\nindex <- which((XXXX$XX_code == ""06"") | (XXXX$XX_code == ""07"") | (XXXX$XX_code == ""08""))\r\nfac <- as.factor(as.character(XXXX$XX_code[index]))\r\nmat <- as.matrix(distridistri_s[index,])\r\nxlim <- cbind(0, table(fac))\r\ncircos.initialize(sectors = levels(fac), xlim = xlim)\r\ncircos.track(ylim = c(0, 1), panel.fun = function(x, y) {\r\n    circos.text(CELL_META$xcenter, CELL_META$ycenter + 0.5, CELL_META$sector.index, facing = ""clockwise"", niceFacing = TRUE, adj = c(1, 0.5), cex = 1)\r\n}, bg.border = NA)\r\n\t\t#--> plot barplots\r\ncircos.track(ylim = c(0, 1))\r\nfor(i in levels(fac)){\r\n\tindex <- which(fac == i)\r\n\tif(length(index) > 1){\r\n\t\tval <- mat[index,]\r\n\t}\telse{\r\n\t\t\tval <- t(as.matrix(mat[index,]))\r\n\t\t}\r\n\tcircos.barplot(val, 1:nrow(val) - 0.5, col = col3, sector.index = i, track.index = 2, bar_width = 0.5, border = ""grey"")\r\n}\r\ncircos.clear()\r\n\t\t#--> plot XXXX names\r\nindex <- which((XXXX$XX_code == ""06"") | (XXXX$XX_code == ""07"") | (XXXX$XX_code == ""08""))\r\nsec <- as.character(XXXX$XXXX_code[index])\r\nlab <- XXXX$XXXX_shortname[index]\r\ncol <- rep(""grey"", length(sec))\r\nnames(col) <- sec\r\ncol[cat] <- ""red""\r\npar(new = T)\r\ncircos.par(""canvas.xlim"" = c(-1.75, 1.75), ""canvas.ylim"" = c(-1.75, 1.75))\r\ncircos.initialize(sectors = sec, xlim = c(0,1))\r\ncircos.track(ylim = c(0, 1), panel.fun = function(x, y) {\r\n    circos.text(CELL_META$xcenter, CELL_META$ycenter + 0.5, CELL_META$sector.index, facing = ""clockwise"", niceFacing = TRUE, adj = c(1, 0.5), cex = 0.5)\r\n}, bg.border = NA)\r\nfor(i in cat){\r\n\tdraw.sector(get.cell.meta.data(""cell.start.degree"", sector.index = i),\r\n\t get.cell.meta.data(""cell.end.degree"", sector.index = i),\r\n\t rou1 = 0.75, rou2 = 0.75, border = ""red"")\r\n}\r\ncircos.clear()\r\n#--> Do not apply this part : for response to reviewer only\r\n\r\n\r\n####\t####\r\n#### Figure 1 : Mapping flows by radarchart\r\n####\t####\r\n\r\n tiff(""./Papers/2021-Frontiers in Ecology and the Environment/last version/figures/Last/Ollier Figure 1.tif"", width = 11.5, height = 11.5 / 3.5, units = ""cm"", res = 600)\r\n\r\npar(mfrow = c(1,3))\r\npar(family = ""Arial"")\r\npar(ps = 8)\r\npar(cex.axis = 0.55)\t# for labels of the radial plots\r\npar(cex.lab = 0.75)\t# for circles label of the radial plot\r\npar(xpd = NA)\r\n\r\nrast_size <- c(0.55, 0.6, 0.5)-0.325\r\nred_pol <- col2rgb(""red"")\r\nred_pol <- rgb(red_pol[1], red_pol[2], red_pol[3], alpha = 125, max = 255)\r\nblack_pol <- col2rgb(""black"")\r\nblack_pol <- rgb(black_pol[1], black_pol[2], black_pol[3], alpha = 125, max = 255) \r\n\r\nfor(i in 1:3){\r\n\tradial.plot(df[c(i,4),], labels = c(""Africa"", ""Asia"", ""Europe"", ""L. America"", ""N. America"", ""Oceania""), \r\n\t rp.type = ""p"", lwd = 2, line.col = c(""black"", ""red""), poly.col = c(black_pol, red_pol),\r\n\t radial.labels = c("""", ""0.2"", """", 0.6, """"), label.prop = c(1.1, 1.05, 1.05, 1.35, 1.1, 1.1) + 0.2,\r\n\t radlab = FALSE, boxed.radial = FALSE, mar = c(0, 3, 0, 2))\r\n\ttext(0, 1.2, row.names(df)[i], font = 2)\r\n\tif(i == 1){\r\n\t\tlegend(x = -1.8, y = 1.45, leg = c(""Alien ants"", ""Trade""), cex = 0.75, text.font = 3, fill = c(red_pol, black_pol), border = c(""red"", ""black""), bty = ""n"")\r\n\t}\r\n\tsubplot(plot(rast[[i]], col = ""black"", axes = F, box = FALSE, legend = FALSE), x = 0, y = -0.5, size = rep(rast_size[i], 2))\r\n}\r\n dev.off()\r\n\r\n\r\n####\t####\r\n#### Figure 2 : Biplot of Correspondance analysis on compositional data\r\n####\t####\r\n\r\n## Configure device\r\n tiff(""./Papers/2021-Frontiers in Ecology and the Environment/last version/figures/Last/Ollier Figure 2.tif"", width = 11.5, height = 11.5, units = ""cm"", res = 600)\r\n\r\npar(mfrow = c(1,1))\r\npar(mar = c(0,0,0,0))\r\npar(family = ""Arial"")\r\npar(ps = 8)\r\npar(xpd = NA)\r\n\r\n## Prepare axes\r\nxy <- li[,1:2]\r\nxlim <- c(min(c(xy[,1], -0.1)), max(c(xy[,1], 0.1)))\r\nylim <- c(min(c(xy[,2], -0.1)), max(c(xy[,2], 0.1)))\r\nplot(xy, type = ""n"", axes = F, xlab = """", ylab = """", xlim = xlim, ylim = ylim)\r\nabline(h = 0, lwd = 2, col = ""grey25"", lty = 1)\r\nabline(v = 0, lwd = 2, col = ""grey25"", lty = 1)\r\n\r\n## Chull of trade groups\r\n""fun"" <- function(xy){\r\n\tx1 <- xy[,1]\r\n\ty1 <- xy[,2]\r\n\tnum <- chull(x1, y1)\r\n      x2 <- x1[num]\r\n      y2 <- y1[num]\r\n      res <- cbind(x2,y2)\r\nreturn(res)\r\n}\r\n\r\nlocal <- split(xy, fac)\r\nlocallocal <- lapply(local, fun)\r\ncol_pol <- col2rgb(""grey"")\r\ncol_pol <- rgb(col_pol[1], col_pol[2], col_pol[3], alpha = 125, max = 255)\r\nfor(i in 1:length(local)){\r\n\tpolygon(locallocal[[i]], col = col_pol, border = ""grey50"", lwd = 0.5)\r\n}\r\n\r\nj <- which(unlist(lapply(local, nrow)) == 2)\r\nfor(i in j)\tlines(local[[i]], col = ""grey85"", lwd = 2)\r\n\r\npoints(xy[21:97,], pch = 20, col = ""grey50"", cex = 0.75)\r\npoints(xy[98:100,], pch = 20, col = ""black"", cex = 0.75)\r\npoints(xy[101,], pch = 20, col = ""red"", cex = 0.75)\r\n\r\ncoord_pol <- t(as.data.frame(lapply(locallocal, function(x) apply(x, 2, mean))))\r\n\r\n## Add agricultural points\r\nlag <- 0.05\r\nfor(i in 1:20){\r\n\trasterImage(wheat, xy[i,1]-lag*1.1, xy[i,2]-lag*1.3, xy[i,1]+lag*1.1, xy[i,2]+lag*1.3, adj = 0.5)\r\n}\r\n\r\n## Add piechart\r\nlocal <- split(xy, fac)\r\ncoord <- as.data.frame(lapply(local, function(x) apply(x, 2, mean)))\r\ncoord <- as.data.frame(t(coord))\r\nnames(coord) <- names(xy)\r\n\r\nlocal <- split(distri, fac[1:nrow(distri)])\r\ndistri_fac <- as.data.frame(lapply(local, function(x) apply(x, 2, sum)))\r\ndistri_fac <- as.data.frame(t(distri_fac))\r\ndistri_fac_s <- sweep(distri_fac, 1, apply(distri_fac, 1, sum), ""/"")\r\n\r\nlocal <- split(rbind.data.frame(distri, df), fac)\r\nprofil_fac <- as.data.frame(lapply(local, function(x) apply(x, 2, sum)))\r\nprofil_fac <- as.data.frame(t(profil_fac))\r\nprofil_fac_s <- sweep(profil_fac, 1, apply(profil_fac, 1, sum), ""/"")\r\n\r\nfor(i in 1:nlevels(fac)){\r\n\tsubplot(pie(unlist(profil_fac[i,]), col = col3, labels = NA, border = ""grey"") , coord[i,1], coord[i,2], size = rep(1/3, 2))\r\n}\r\n\r\npar(new = T)\r\nplot(xy, type = ""n"", axes = F, xlab = """", ylab = """", xlim = xlim, ylim = ylim)\r\ntext(coord[,1], coord[,2], levels(fac), cex = 1, font = 2)\r\ntext(xy[98,], row.names(xy)[98], font = 2, col = ""black"", cex = 1, pos = 1, offset = 0.15)\r\ntext(xy[99,], row.names(xy)[99], font = 2, col = ""black"", cex = 1, pos = 1, offset = 0.15)\r\ntext(xy[100,], row.names(xy)[100], font = 2, col = ""black"", cex = 1, pos = 4, offset = 0.15)\r\ntext(xy[101,], row.names(xy)[101], font = 2, col = ""red"", cex = 1, pos = 3, offset = 0.15)\r\n\r\n## Add legend\r\n# Main legend\r\nlegend(x = 0.75, y = 1.45, leg = names(col3), fill = col3, border = col3, text.font = 3, cex = 0.9, bty = ""n"")\r\n\r\n# Agricultural legend\r\nrasterImage(wheat, -1.1-lag, -0.85-lag, -1.1+lag, -0.85+lag, adj = 0.5, col = ""black"")\r\ntext(x = -1.1, y = -0.85, ""Agricultural commodities"", font = 3, cex = 0.9, pos = 4, offset = 0.75)\r\npoints(cbind(-1.1, -0.95), pch = 20, col = ""grey50"", cex = 1)\r\ntext(x = -1.1, y = -0.95, ""Other commodities"", font = 3, cex = 0.9, pos = 4, offset = 0.75)\r\n\r\n## Add eigenvalues barplot\r\nadd.scatter.eig(coa$eig, xax = 1, yax = 2, posi = ""topleft"", ratio = 0.2, sub = """", inset = 0.05)\r\ntext(1.5, 0.275, ""Eigenvalues"", cex = 0.9, font = 3, pos = 4, offset = 0)\r\n\r\n dev.off()\r\n\r\n\r\n####\t####\r\n#### Figure 3 - circular : Barplot of compositional data and classification of profiles\r\n####\t####\r\n\r\n tiff(""./Papers/2021-Frontiers in Ecology and the Environment/last version/figures/Last/Ollier Figure 3.tif"", width = 11.5, height = 11.5, units = ""cm"", res = 600)\r\n\r\n### Device parameters\r\npar(mfrow = c(1,1))\r\npar(mar = c(0,0,0,0))\r\npar(family = ""Arial"")\r\npar(ps = 8)\r\npar(xpd = NA)\r\n\r\n### Prepare dendrogram\r\nindex.row <- hc$order\r\nlocal <- fac[index.row]\r\nlevels(fac)[unique(local)] <- 1:nlevels(fac)\r\nlab <- labels(dend)\r\ndend <- color_branches(dend, k = 2, col = rep(""grey65"", 2))\r\n\r\nindex.col <- order(coa$c1[,1])\r\ntab <- rbind.data.frame(distri_s, df)\r\nh <- tab[lab, index.col]\r\nbarcolor <- col3[index.col]\r\nborder.col <- ""white""\r\nlabel.color <- rep(""black"", length(lab))\r\nlabel.color[nchar(lab) > 2] <- ""black""\r\nlabel.color[lab == ""Alien ants""] <- ""red""\r\n\r\n### Make circos\r\ncircos.clear()\r\ncircos.par(cell.padding = c(0,0,0,0), gap.degree = 0)\r\ncircos.initialize(""a"", xlim = c(0, nrow(h)))\r\n\r\ncircos.track(ylim = c(0,1), bg.border = NA, track.height = 0.28, track.margin = c(0.01,0),\r\n  panel.fun = function(x,y){\r\n   for(i in 1:nrow(h)){\r\n     circos.text(i-0.5, 0, lab[i], adj = c(0,0.5), facing = ""clockwise"", niceFacing = TRUE, cex = 1, col = label.color[i])\r\n   }\r\n  })\r\n\r\ncircos.track(ylim = c(0,1), track.margin = c(0,0), track.height = 0.35, bg.lty = 0, \r\n panel.fun = function(x,y){\r\n  mat <- as.matrix(h)\r\n  pos <- 1:nrow(mat)-0.5\r\n  barwidth <- 1\r\n  for(i in 1:ncol(mat)){\r\n    seq1 <- rowSums(mat[,seq(i-1), drop = FALSE])\r\n    seq2 <- rowSums(mat[,seq(i), drop = FALSE])\r\n    circos.rect(pos - barwidth/2, if(i == 1){0}\telse{seq1}, pos + barwidth/2, seq2, col = barcolor[i], border = border.col, lwd = 1.25)\r\n  }\r\n })\r\n\r\ncircos.track(ylim = c(0, attr(dend,""height"")), bg.border = NA, track.margin = c(0,0.0015), track.height = 0.35,\r\n panel.fun = function(x,y)\tcircos.dendrogram(dend))\r\n\r\ncircos.clear()\r\n\r\n### Add images\r\nangle.deg <- seq(360 / length(lab) / 2, 360, by = 360 / length(lab))\r\nangle.rad <- angle.deg / 360 * 2 * pi\r\nangle.rad <- rev(angle.rad)\r\n\r\nx <- cos(angle.rad) * 0.85\r\ny <- sin(angle.rad) * 0.85\r\ncoords <- as.data.frame(cbind(x, y))\r\n\r\n## Add icons\r\nindex <- match(XX$XX_code[1:20], lab)\r\nfor(i in index){\r\n\trasterImage(wheat, x[i]-0.025*1.1, y[i]-0.045*1.3, x[i]+0.025*1.1, y[i]+0.045*1.3)\r\n}\r\n\r\n### Add groups\r\n## Add limits\r\nangle.deg <- seq(0, 360, by = 360 / length(lab))\r\nangle.rad <- angle.deg / 360 * 2 * pi\r\nangle.rad <- rev(angle.rad)\r\nnm <- unclass(table(fac[lab])[as.character(1:nlevels(fac))])\r\ncnm <- cumsum(nm)+1\r\nangle.lim <- angle.rad[cnm]\r\n\r\nx1 <- cos(angle.lim) * (1-0.28-0.01)\r\ny1 <- sin(angle.lim) * (1-0.28-0.01)\r\n\r\nx0 <- cos(angle.lim) * (1-0.28-0.35)\r\ny0 <- sin(angle.lim) * (1-0.28-0.35)\r\n\r\nsegments(x0, y0, x1, y1)\r\n\r\n## Add names group\r\nx2 <- cos(angle.lim) * (1-0.28-0.35/2)\r\ny2 <- sin(angle.lim) * (1-0.28-0.35/2)\r\ncoords2 <- as.data.frame(cbind(x2, y2))\r\ncoords2 <- rbind(coords2[nrow(coords2),], coords2)\r\nfor(i in 1:nlevels(fac)){\r\n\tlocal <- coords2[i:(i+1),]\r\n\tpts <- apply(local, 2, mean)\r\n\ttext(pts[1], pts[2], i, font = 2)\t\t\r\n}\r\n\r\n### Add legend\r\n## Main legend\r\nlegend(x = -1.1, y = 1, leg = names(col3)[1:3], fill = col3[1:3], border = col3[1:3], text.font = 3, cex = 0.9, bty = ""n"")\r\nlegend(x = 0.5, y = 1, leg = names(col3)[4:6], fill = col3[4:6], border = col3[4:6], text.font = 3, cex = 0.9, bty = ""n"")\r\n\r\n## Agricultural legend\r\nrasterImage(wheat, -1-0.025, -1-0.045, -1+0.025, -1+0.045, adj = 0.5, col = ""black"")\r\ntext(x = -1, y = -1, ""Agricultural commodities"", font = 3, cex = 0.9, pos = 4, offset = 0.75)\r\n\r\ndev.off()\r\n']","Flows of invasive ants worldwide to the United States International trade and human movements have accidentally transported thousands of species worldwide at an unprecedented scale. The resulting biological invasions are among the greatest drivers of species extinctions and can cause enormous economic losses. Understanding how globalization affects the accidental transport of species is urgent to prevent new invasions. However, global trade networks have had mixed success so far in explaining intercontinental species movements. Here, we show that commonly used proxies of global trade flows such as general imports and agricultural imports differed greatly from flows of alien ants from their donor regions to the United States. The analysis of 97 individual commodity flows revealed instead that plants and fruit imports, which are a small subset of all agricultural commodities, were associated with invasion flows. All 95 other commodities differed from flows of alien ants, including most ""agricultural"" commodities which had extremely heterogenous geographic origins. This highlights the need to know precisely which commodities serve as introduction pathways for a particular taxonomic group in order to explain invasion flows and identify likely source regions of future invasions in a world of changing trade relationships.",1
Data and files for: Decoupling cooperation and punishment in humans shows that punishment is not an altruistic trait,"Economic experiments have suggested that cooperative humans will altruistically match local levels of cooperation ('conditional cooperation') and pay to punish non-cooperators ('altruistic punishment'). Evolutionary models have suggested that if altruists punish non-altruists this could favour the evolution of costly helping behaviours (cooperation) among strangers. An often-key requirement is that helping behaviours and punishing behaviours form one single, conjoined trait ('strong reciprocity'). Previous economics experiments have provided support for the hypothesis that punishment and cooperation form one conjoined, altruistically motivated, trait. However, such a conjoined trait may be evolutionarily unstable, and previous experiments have confounded a fear of being punished with being surrounded by cooperators, two factors that could favour cooperation. Here, we experimentally decouple the fear of punishment from a cooperative environment and allow cooperation and punishment behaviour to freely separate (420 participants). We show, that if a minority of individuals are made immune to punishment, they (1) learn to stop cooperating on average despite being surrounded by high levels of cooperation, contradicting the idea of conditional cooperation; and (2) often continue to punish, 'hypocritically', showing that cooperation and punishment do not form one, altruistically motivated, linked trait.",,"Data and files for: Decoupling cooperation and punishment in humans shows that punishment is not an altruistic trait Economic experiments have suggested that cooperative humans will altruistically match local levels of cooperation ('conditional cooperation') and pay to punish non-cooperators ('altruistic punishment'). Evolutionary models have suggested that if altruists punish non-altruists this could favour the evolution of costly helping behaviours (cooperation) among strangers. An often-key requirement is that helping behaviours and punishing behaviours form one single, conjoined trait ('strong reciprocity'). Previous economics experiments have provided support for the hypothesis that punishment and cooperation form one conjoined, altruistically motivated, trait. However, such a conjoined trait may be evolutionarily unstable, and previous experiments have confounded a fear of being punished with being surrounded by cooperators, two factors that could favour cooperation. Here, we experimentally decouple the fear of punishment from a cooperative environment and allow cooperation and punishment behaviour to freely separate (420 participants). We show, that if a minority of individuals are made immune to punishment, they (1) learn to stop cooperating on average despite being surrounded by high levels of cooperation, contradicting the idea of conditional cooperation; and (2) often continue to punish, 'hypocritically', showing that cooperation and punishment do not form one, altruistically motivated, linked trait.",1
"Characterizing the spatio-temporal threats, conservation hotspots and conservation gaps for the most extinction-prone bird family (Aves: Rallidae)","Here we worked on the rails (bird family Rallidae), looking at whether the current threats are consistent with those that led to recent extinctions, and ultimately, what conservation actions might be necessary to mitigate further losses. We undertook a global synthesis of the temporal and spatial threat patterns for Rallidae and determined conservation priorities and gaps.","['################################\r\n#### Ranking countries following rails\' conservation and gaps\r\n#### last update: 13/02/20 - included the updated version of ICUN - 2019-3 \r\n####\r\n#### author: Lucile Leveque (lucile.leveque@utas.edu.au)\r\n#### used in article ""Characterising the spatio-temporal threats, conservation hotspots, \r\n#### and conservation gaps for the most extinction-prone bird family (Aves: Rallidae)""\r\n#### 2021\r\n################################\r\n\r\n## libraries\r\nlibrary(dplyr)\r\nlibrary(data.table) # for ""frank"" (ranking function)\r\nse <- function(x) sd(x, na.rm=T)/sqrt(length(x)) # create the function standard error\r\n\r\n# REGIONS AND COUNTRIES\r\n# Notes: \r\n# Palearctic_and_Indomalaya # FOR CHINA\r\n# Indomalaya_and_Australasia # FOR INDONESIA\r\n# Nearctic_and_Neotropic # FOR MEXICO\r\n# Atlantic_Ocean # FOR ""Other""\r\n\r\n\r\n\r\n# 1. WORLDWIDE PATTERN (GLOBAL): CHARACTERISING THREATENED SPECIES\r\n\r\nglobal <- read.table(""Rfile_ranking_database2019.csv"", header =T, sep="","", na.strings =""N/A"")\r\nsummary(global) # 140sp.\r\ngloT <- global %>% filter(iucn==""T""); summary(gloT$iucn)\r\n\r\n# encode the variables              \r\ngloT$research <- ifelse(gloT$research == ""needed"", 1, 0) \r\ngloT$habitat.prot <- ifelse(gloT$habitat.prot == ""needed"", 1, 0) \r\ngloT$management <- ifelse(gloT$management == ""needed"", 1, 0) \r\ngloT$education <- ifelse(gloT$education == ""needed"", 1, 0) \r\ngloT$law <- ifelse(gloT$law == ""needed"", 1, 0) \r\n\r\n# calculate the average number of conservation gaps per threatnd sp.\r\ngloT %>% mutate(lack = research + habitat.prot + management + education + law) %>%  \r\n  summarise(mean(lack)) # mean\r\ngloT %>% mutate(lack = research + habitat.prot + management + education + law) %>%  \r\n  summarise(se(lack)) # standard error\r\n# mean 2.45 +/- 0.17\r\n\r\n# how many species with more than 2 conservations needed\r\ngloT %>% mutate(lack = research + habitat.prot + management + education + law) %>%  \r\n  filter(lack>2) %>%\r\n  summarise(length(lack))           \r\n\r\n\r\n# 2. PER COUNTRIES\r\n\r\ndata <- read.table(""Rfile_ranking_countrydatabase2019.csv"", header =T, sep="","", na.strings =""N/A"")\r\n# nb.countries is not updated since overseas territories under country name\r\ndata <- data %>% select(-c(common.name, nb.countries, bioregion))\r\nlength(unique(data$country)) # 204 countries\r\ndata <-data[!duplicated(data),] # because of islands owned by the same country, over different bioregions.\r\n\r\n# remove bioregion so that countries can be pooled no matter what bioregion \r\n# avoid the problem of overseas terr present in different bioregion\r\nagg <- count(data, country, ID, iucn0, iucn, degraded.status,  \r\n             country.ende, island.ende, flightless,\r\n             research, habitat.prot, management, education, law)\r\n\r\n\r\n# SERIOUSNESS (THREAT RANK)\r\n# n rpz the frequency of results following the 2 variables\r\nthreat <- as.data.frame(xtabs(n ~ country + iucn, data = agg)) \r\nthreat <- threat[threat$iucn == ""T"",] # nb of threatened sp per country\r\ncolnames(threat)[3] <- ""threatened""\r\n\r\ndegrad <- as.data.frame(xtabs(n ~ country + degraded.status, data = agg))\r\ndegrad <- degrad[degrad$degraded.status == ""yes"",]\r\ncolnames(degrad)[3] <- ""degrad""\r\n\r\n\r\n# UNIQUENESS (HERITAGE RANK)\r\n\r\nrichness <- aggregate(ID ~ country, data, FUN=length)\r\ncolnames(richness)[2] <- ""sp.richness""\r\n\r\nislands <- as.data.frame(xtabs(n ~ country + island.ende, data = agg))\r\nislands <- islands[islands$island.ende == ""yes"",]\r\ncolnames(islands)[3] <- ""island.ende""\r\n\r\nflight <- as.data.frame(xtabs(n ~ country + flightless, data = agg))\r\nflight <- flight[flight$flightless == ""flightless"",]\r\ncolnames(flight)[3] <- ""flightless""\r\n\r\ncountry.end <- as.data.frame(xtabs(n ~ country + country.ende, data = agg))\r\ncountry.end <- country.end[country.end$country.ende == ""yes"",]\r\ncolnames(country.end)[3] <- ""country.ende""\r\n\r\n\r\n# CONSERVATION GAPS\r\n\r\nresearch <- as.data.frame(xtabs(n ~ country + research, data = agg))\r\nresearch <- research[research$research == ""needed"",]\r\ncolnames(research)[3] <- ""research""\r\n\r\nhabitat.prot <- as.data.frame(xtabs(n ~ country + habitat.prot, data = agg))\r\nhabitat.prot <- habitat.prot[habitat.prot$habitat.prot == ""needed"",]\r\ncolnames(habitat.prot)[3] <- ""habitat.prot""\r\n\r\nmanagement <- as.data.frame(xtabs(n ~ country + management, data = agg))\r\nmanagement <- management[management$management == ""needed"",]\r\ncolnames(management)[3] <- ""management""\r\n\r\neducation <- as.data.frame(xtabs(n ~ country + education, data = agg))\r\neducation <- education[education$education == ""needed"",]\r\ncolnames(education)[3] <- ""education""\r\n\r\nlaw <- as.data.frame(xtabs(n ~ country + law, data = agg))\r\nlaw <- law[law$law == ""needed"",]\r\ncolnames(law)[3] <- ""law""\r\n\r\n\r\n# Put together a dataframe with all information\r\ndf <- data.frame(islands[1], #country names\r\n                 richness[2], flight[3], islands[3], country.end[3], \r\n                 threat[3], degrad[3], \r\n                 research[3], habitat.prot[3], management[3], education[3], law[3])\r\n\r\n\r\n\r\n\r\n########### RANKING ############\r\n\r\n# Hotspot: number of species\r\n# Uniqueness: 1. Flightless 2. Island endemic 3. Country endemic\r\n\r\n# /!\\ the Giant Coot is not considered flightless (can still disperse etc.)\r\ndf <-df[with(df, order(-flightless, -island.ende, -country.ende)),] # ordering for visualisation \r\ndf <- within(df, rank.unik <- frank(df, -flightless, -island.ende, -country.ende, ties.method=""dense""))\r\n\r\ndf$rank.unik[df$flightless ==""0"" & df$island.ende==""0"" & df$country.ende==""0""] <- 204 # lowest rank possible\r\n\r\ndf$sp.unik <- rowSums(df[,c(3:5)])\r\n\r\ndf<- (within(df, topunik <- frank(df, rank.unik, -sp.richness, ties.method=""first"")))\r\n\r\nhead(df[,c(1:5,13,14)])\r\n\r\n\r\n# Seriousness: 1. Threatened 2. Degraded status\r\n\r\ndf <-df[with(df, order(-threatened, -degrad)),] # ordering for visualisation\r\ndf <- within(df, rank.serious <- frank(df, -threatened, -degrad, ties.method=""dense""))\r\n\r\ndf$rank.serious[df$threatened ==""0"" & df$degrad==""0""] <- 204 # lowest rank possible\r\n\r\nhead(df[,c(1:7,13,14,15)])\r\n\r\ndf<- (within(df, topserious <- frank(df, rank.serious, -sp.richness, ties.method=""first"")))\r\n\r\n\r\n # write.csv(df, file = ""COUNTRY results_2021.csv"")\r\n\r\n# COMMON TOP 20\r\ntop20 <- df %>% filter(df$topunik < 21 | df$topserious < 21) %>%\r\n  select(country, sp.richness, topunik, topserious)\r\n\r\ntop20$toptop <- ifelse(top20$topunik < 21 & top20$topserious < 21, 1, 0)\r\n\r\n#proportion of countries present in the 2 top20\r\ntable(top20$toptop)\r\n\r\n# Summary\r\nranks <- df[,c(1,2,13:17)] # country, rank.sp, richness, rank.uniqueness, nb.sp.unik, rank.serious\r\nhead(ranks,10)\r\n\r\n# write.csv(ranks, file = ""COUNTRY ranking results2021.csv"")\r\n\r\n# nb of countries with island endemic # with flightless species\r\ndf %>% filter(island.ende > 0 | flightless > 0) %>% summarise(length(country))  # 14 countries hold island ende\r\ndf %>% filter(flightless > 0) %>% summarise(length(country)) # 10 countries hold island ende\r\n\r\n\r\n\r\n################################################################\r\n### GAPS IN CONSERVATION PROGRAMS \r\n\r\ngaps <- df %>% select (country, sp.richness, research, habitat.prot, management,\r\n                       education, law)\r\nhead(gaps) # a number rpz the nb of species that would need reinforcement in each category (1=""needed"")\r\n\r\n\r\n#############################################\r\n### GAPS IN CONSERVATION PROGRAMS \r\n# FOR THREATENED SPECIES\r\n\r\nresearch3 <- as.data.frame(xtabs(n ~ country + research + iucn, data = agg))\r\nresearch3 <- research3[research3$research == ""needed"",]\r\ncolnames(research3)[4] <- ""research""\r\n\r\nhabitat.prot3 <- as.data.frame(xtabs(n ~ country + habitat.prot +iucn, data = agg))\r\nhabitat.prot3 <- habitat.prot3[habitat.prot3$habitat.prot == ""needed"",]\r\ncolnames(habitat.prot3)[4] <- ""habitat.prot""\r\n\r\nmanagement3 <- as.data.frame(xtabs(n ~ country + management +iucn, data = agg))\r\nmanagement3 <- management3[management3$management == ""needed"",]\r\ncolnames(management3)[4] <- ""management""\r\n\r\neducation3 <- as.data.frame(xtabs(n ~ country + education +iucn, data = agg))\r\neducation3 <- education3[education3$education == ""needed"",]\r\ncolnames(education3)[4] <- ""education""\r\n\r\nlaw3 <- as.data.frame(xtabs(n ~ country + law +iucn, data = agg))\r\nlaw3 <- law3[law3$law == ""needed"",]\r\ncolnames(law3)[4] <- ""law""\r\n\r\n\r\n# proportion threatened species per country\r\ndf2 <- data.frame(research3[,c(1,3,4)], habitat.prot3[,c(3,4)],\r\n                      management3[,c(3,4)], education3[,c(3,4)], law3[,c(3,4)])\r\nthrd <- df2[df2$iucn==""T"",]\r\nthrd <- thrd[,-c(2,4,6,8,10)] # removing iucn=T\r\ncolnames(thrd)[2:6] <- c(""T.research"", ""T.habitat"", ""T.management"", ""T.education"", ""T.law"")\r\n\r\n\r\n# create a big database with threatened and non-thrd sp\r\ngaps2 <- merge(gaps, thrd, by.x=""country"", by.y=""country"") # when checking threatened sp.\r\n\r\n\r\n# PERCENTAGES\r\n\r\n# research\r\ngaps2$researchperc <- round((gaps2[,3]/gaps2[,2])*100, digits = 1)\r\ngaps2$research.T <- round((gaps2[,9]/gaps2[,2])*100, digits = 1) # % threatened sp.\r\n\r\n# habitat prot\r\ngaps2$habitatperc <- round((gaps2[,4]/gaps2[,2])*100, digits = 1)\r\ngaps2$habitat.T <- round((gaps2[,10]/gaps2[,2])*100, digits = 1) # % threatened sp.\r\n\r\n# management\r\ngaps2$managperc <- round((gaps2[,5]/gaps2[,2])*100, digits = 1)\r\ngaps2$manag.T <- round((gaps2[,11]/gaps2[,2])*100, digits = 1) # % threatened sp.\r\n\r\n# education\r\ngaps2$eduperc <- round((gaps2[,6]/gaps2[,2])*100, digits = 1)\r\ngaps2$edu.T <- round((gaps2[,12]/gaps2[,2])*100, digits = 1) # % threatened sp.\r\n\r\n# law\r\ngaps2$lawperc <- round((gaps2[,7]/gaps2[,2])*100, digits = 1)\r\ngaps2$law.T <- round((gaps2[,13]/gaps2[,2])*100, digits = 1) # % threatened sp.\r\n\r\nhead(gaps2)\r\n\r\n# summing number of categories to improve for species = the rank\r\n# perc makes no sense for country that have few sp\r\ngaps2$gaps <- gaps2$research + gaps2$habitat.prot + gaps2$management + gaps2$education + gaps2$law\r\ngaps2 <- gaps2[with(gaps2, order(-gaps2$gaps)),] # ordering for visualisation # minus makes it descending\r\n\r\n\r\n\r\n  # write.csv(gaps2, file = ""COUNTRY gaps results2021.csv"")\r\n\r\n\r\n', '################################\r\n#### Ranking bioregions following rails\' conservation and gaps\r\n#### last update: 13/02/20 - included the updated version of ICUN - 2019-3 \r\n####\r\n#### author: Lucile Leveque (lucile.leveque@utas.edu.au)\r\n#### used in article ""Characterising the spatio-temporal threats, conservation hotspots, \r\n#### and conservation gaps for the most extinction-prone bird family (Aves: Rallidae)""\r\n#### 2021\r\n################################\r\n\r\n\r\n## libraries\r\nlibrary(dplyr)\r\nlibrary(data.table) # for ""frank"" (ranking function)\r\n\r\n# REGIONS AND COUNTRIES\r\n# Notes: \r\n# Palearctic_and_Indomalaya # FOR CHINA\r\n# Indomalaya_and_Australasia # FOR INDONESIA\r\n# Nearctic_and_Neotropic # FOR MEXICO\r\n# Atlantic_Ocean # FOR ""Other""\r\n\r\n\r\n#########################################################\r\n###  BIOREGIONS\r\n### data : a line is a species: same sp can be found many times for different bioregions\r\n#########################################################\r\n\r\nreg <- read.table(""Rfile_ranking_regiondatabase2019.csv"", header =T, sep="","", na.strings =""N/A"")\r\n\r\n# pooling Australasia and Oceania together\r\nreg$bioregion <- as.character(reg$bioregion)\r\nreg$bioregion[reg$bioregion == ""Australasia"" |\r\n                reg$bioregion == ""Oceania""] <- ""Australasia/Oceania""\r\n\r\nreg <- reg[!duplicated(reg),] # because of the fusion Australia and Oceania (same sp. in both)\r\n\r\naggreg <- count(reg, bioregion, ID, iucn0, iucn, degraded.status,  \r\n             country.ende, island.ende, flightlessness,\r\n             research, habitat.prot, management, education, law)\r\n\r\n# RICHNESS - HOTSPOT\r\n\r\nrichness2 <- aggregate(ID ~ bioregion, aggreg, FUN=length)\r\ncolnames(richness2)[2] <- ""sp.richness""\r\nrichness2\r\n\r\n# SERIOUSNESS (THREAT RANK)\r\n\r\nthreat2 <- as.data.frame(xtabs(n ~ bioregion + iucn, data = aggreg)) # n rpz the frequency of results following the 2 variables\r\nthreat2 <- threat2[threat2$iucn == ""T"",]\r\ncolnames(threat2)[3] <- ""threatened""\r\nhead(threat2)\r\n\r\n\r\ndegrad2 <- as.data.frame(xtabs(n ~ bioregion + degraded.status, data = aggreg))\r\ndegrad2 <- degrad2[degrad2$degraded.status == ""yes"",]\r\ncolnames(degrad2)[3] <- ""degrad""\r\n\r\n# UNIQUENESS (HERITAGE RANK)\r\n\r\nislands2 <- as.data.frame(xtabs(n ~ bioregion + island.ende, data = aggreg))\r\nislands2 <- islands2[islands2$island.ende == ""yes"",]\r\ncolnames(islands2)[3] <- ""island.ende""\r\n\r\ncountry.end2 <- as.data.frame(xtabs(n ~ bioregion + country.ende, data = aggreg))\r\ncountry.end2 <- country.end2[country.end2$country.ende == ""yes"",]\r\ncolnames(country.end2)[3] <- ""country.ende""\r\n\r\nflight2 <- as.data.frame(xtabs(n ~ bioregion + flightlessness, data = aggreg))\r\nflight2 <- flight2[flight2$flightless == ""flightless"",]\r\ncolnames(flight2)[3] <- ""flightless""\r\n\r\n\r\n\r\n\r\n# CONSERVATION GAPS \r\n# see below for threatened species\r\n\r\nresearch2 <- as.data.frame(xtabs(n ~ bioregion + research, data = aggreg))\r\nresearch2 <- research2[research2$research == ""needed"",]\r\ncolnames(research2)[3] <- ""research""\r\n\r\nhabitat.prot2 <- as.data.frame(xtabs(n ~ bioregion + habitat.prot, data = aggreg))\r\nhabitat.prot2 <- habitat.prot2[habitat.prot2$habitat.prot == ""needed"",]\r\ncolnames(habitat.prot2)[3] <- ""habitat.prot""\r\n\r\nmanagement2 <- as.data.frame(xtabs(n ~ bioregion + management, data = aggreg))\r\nmanagement2 <- management2[management2$management == ""needed"",]\r\ncolnames(management2)[3] <- ""management""\r\n\r\neducation2 <- as.data.frame(xtabs(n ~ bioregion + education, data = aggreg))\r\neducation2 <- education2[education2$education == ""needed"",]\r\ncolnames(education2)[3] <- ""education""\r\n\r\nlaw2 <- as.data.frame(xtabs(n ~ bioregion + law, data = aggreg))\r\nlaw2 <- law2[law2$law == ""needed"",]\r\ncolnames(law2)[3] <- ""law""\r\n\r\n\r\n# putting a dataframe together with all data\r\ndf.reg0 <- data.frame(richness2[1],# ""richness2[1]"" just for bioregion names\r\n                 richness2[2], flight2[3], islands2[3], country.end2[3], \r\n                 threat2[3], degrad2[3],\r\n                 research2[3], habitat.prot2[3], management2[3], education2[3], law2[3])\r\n\r\nhead(df.reg0)\r\n# write.csv(df.reg0, file = ""REGION global table2021.csv"")\r\n\r\n\r\n# don\'t include Other in the ranking but add it to the final table\r\ndf.reg <- df.reg0[df.reg0$bioregion != ""Other"",]\r\n\r\n\r\n########### REGIONS RANKING ############\r\n\r\n# Hotspot: number of species\r\n\r\n# Uniqueness (Heritage): 1. Flightless 2. Island endemic 3. Country endemic\r\n\r\ndf.reg <-df.reg[with(df.reg, order(-flightless, -island.ende, -country.ende)),] # ordering for visualisation # minus makes it descending\r\ndf.reg <- within(df.reg, rank.unik <- frank(df.reg, -flightless, -island.ende, -country.ende, ties.method=""dense""))\r\nhead(df.reg)\r\n\r\n\r\n# Seriousness (Threat): 1. Threatened 2. Degraded status\r\n\r\ndf.reg <-df.reg[with(df.reg, order(-threatened, -degrad)),] # ordering for visualisation # minus makes it descending\r\ndf.reg <- within(df.reg, rank.serious <- frank(df.reg, -threatened, -degrad, ties.method=""dense""))\r\nhead(df.reg)\r\n\r\n\r\n\r\n# write.csv(df.reg, file = ""REGION ranking global table2021.csv"")\r\n\r\n\r\n\r\n# Summary\r\n\r\nranks2 <- df.reg[,c(1,2,13,14)] # bioregion, richness, rank.serious, rank.uniqueness\r\nhead(ranks2)\r\n\r\n# write.csv(ranks2, file = ""REGION ranking results2021.csv"")\r\n\r\n\r\n\r\n\r\n\r\n#############################################\r\n### GAPS IN CONSERVATION PROGRAMS # REGIONS #\r\n\r\n# FOR THREATENED SPECIES\r\n# CONSERVATION GAPS \r\n\r\nresearch3 <- as.data.frame(xtabs(n ~ bioregion + research + iucn, data = aggreg))\r\nresearch3 <- research3[research3$research == ""needed"",]\r\ncolnames(research3)[4] <- ""research""\r\n\r\nhabitat.prot3 <- as.data.frame(xtabs(n ~ bioregion + habitat.prot +iucn, data = aggreg))\r\nhabitat.prot3 <- habitat.prot3[habitat.prot3$habitat.prot == ""needed"",]\r\ncolnames(habitat.prot3)[4] <- ""habitat.prot""\r\n\r\nmanagement3 <- as.data.frame(xtabs(n ~ bioregion + management +iucn, data = aggreg))\r\nmanagement3 <- management3[management3$management == ""needed"",]\r\ncolnames(management3)[4] <- ""management""\r\n\r\neducation3 <- as.data.frame(xtabs(n ~ bioregion + education +iucn, data = aggreg))\r\neducation3 <- education3[education3$education == ""needed"",]\r\ncolnames(education3)[4] <- ""education""\r\n\r\nlaw3 <- as.data.frame(xtabs(n ~ bioregion + law +iucn, data = aggreg))\r\nlaw3 <- law3[law3$law == ""needed"",]\r\ncolnames(law3)[4] <- ""law""\r\n\r\n\r\n\r\n# proportion threatened species per bioregion\r\ndf.reg2 <- data.frame(research3[,c(1,3,4)], habitat.prot3[,c(3,4)],\r\n                      management3[,c(3,4)], education3[,c(3,4)], law3[,c(3,4)])\r\nthrd <- df.reg2[df.reg2$iucn==""T"",]\r\nthrd <- thrd[,-c(2,4,6,8,10)]\r\ncolnames(thrd)[2:6] <- c(""T.research"", ""T.habitat"", ""T.management"", ""T.education"", ""T.law"")\r\n\r\n# don\'t include Other \r\nthrd <- thrd[thrd$bioregion != ""Other"",]\r\n\r\n\r\n# create a second database for threatened sp. only\r\nreggaps <- df.reg[, c(1,2,8:12)] # bioregions, research, habitat.prot, management, education, law\r\nreggaps22 <- merge(reggaps, thrd, by.x=""bioregion"", by.y=""bioregion"") # when checking threatened sp.\r\n\r\n\r\n# PERCENTAGES\r\n\r\n# research\r\nreggaps$researchperc <- round((reggaps[,3]/reggaps[,2])*100, digits = 1)\r\nreggaps22$research.T <- round((reggaps22[,8]/reggaps22[,2])*100, digits = 1) # % threatened sp.\r\n\r\n# habitat prot\r\nreggaps$habitatperc <- round((reggaps[,4]/reggaps[,2])*100, digits = 1)\r\nreggaps22$habitat.T <- round((reggaps22[,9]/reggaps22[,2])*100, digits = 1) # % threatened sp.\r\n\r\n# management\r\nreggaps$managperc <- round((reggaps[,5]/reggaps[,2])*100, digits = 1)\r\nreggaps22$manag.T <- round((reggaps22[,10]/reggaps22[,2])*100, digits = 1) # % threatened sp.\r\n\r\n# education\r\nreggaps$eduperc <- round((reggaps[,6]/reggaps[,2])*100, digits = 1)\r\nreggaps22$edu.T <- round((reggaps22[,11]/reggaps22[,2])*100, digits = 1) # % threatened sp.\r\n\r\n# law\r\nreggaps$lawperc <- round((reggaps[,7]/reggaps[,2])*100, digits = 1)\r\nreggaps22$law.T <- round((reggaps22[,12]/reggaps22[,2])*100, digits = 1) # % threatened sp.\r\n\r\n\r\n# summary\r\nreggaps22[,c(1,2, 13:17)] # threatened sp\r\n\r\n# summing number of categories to improve for species = the rank\r\nreggaps$gaps <- reggaps$researchperc + reggaps$habitatperc + reggaps$managperc + reggaps$eduperc + reggaps$lawperc\r\nreggaps <- reggaps[with(reggaps, order(-reggaps$gaps)),] # ordering for visualisation # minus makes it descending\r\n\r\n\r\ntotal <- merge(reggaps[c(1,2,8:13)], reggaps22[c(1,13:17)], by.x=""bioregion"", by.y=""bioregion"") # merge all results\r\ntotal <- total[with(total, order(-total$gaps)),] # ordering for visualisation # minus makes it descending\r\n\r\n # write.csv(total, file = ""REGION gaps all results2021.csv"") \r\n\r\n# end']","Characterizing the spatio-temporal threats, conservation hotspots and conservation gaps for the most extinction-prone bird family (Aves: Rallidae) Here we worked on the rails (bird family Rallidae), looking at whether the current threats are consistent with those that led to recent extinctions, and ultimately, what conservation actions might be necessary to mitigate further losses. We undertook a global synthesis of the temporal and spatial threat patterns for Rallidae and determined conservation priorities and gaps.",1
Shodagor Women Cooperate Across Domains of Work and Childcare to Solve an Adaptive Problem,"Across human societies, womens economic production and their contributions to childcare are critical in supporting reproductive fitness for themselves, their spouses, and children. Yet, the necessity of performing both work and childcare tasks presents women with an adaptive problem in which they must determine how best to allocate their time and energy between these tasks. Women often utilize cooperative relationships with alloparents to solve this problem, but whether or not women cooperate across different domains (e.g., work and childcare) to access alloparents remains relatively under-explored. Using social network data collected with Shodagor households in Bangladesh, we show that women who need childcare help in order to work draw on cooperative work partners as potential alloparents, and that all women rely heavily on kin, but not reciprocal cooperation for childcare help. These results indicate that Shodagor women strategize to create work and childcare relationships in ways that help solve the adaptive problem they face. We discuss the implications of our results and the example provided by Shodagor women for a broader understanding of womens cooperative relationships, including the importance of socioecological circumstances and gendered divisions of labor in shaping womens cooperative strategies.","['## Code for Starkweather KE, Reynolds AZ, Zohora F, Alam N. (2022) Shodagor women traders cooperate across domains of work and childcare to solve an adaptive problem. Philosophical Transactions of the Royal Society B.\n\n\noptions(stringsAsFactors=FALSE)\n\n\n### LOAD DATA AND MAKE GRAPH OBJECTS\n\nrequire(igraph)\n\nnodelist <- read.csv(""Nodelist_deidentified.csv"")\ncc_edges <- read.csv(""ChildcareEdges_deidentified.csv"")\nww_edges <- read.csv(""WorkEdges_deidentified.csv"")\nr_edges <- read.csv(""RelatednessEdges_deidentified.csv"")\n\n\ncc <- make_empty_graph() + vertices(nodelist$ID, hh=nodelist$HH, color=nodelist$Color, shape=nodelist$Shape)\nfor(i in 1:nrow(cc_edges)) {\n\tcc <- cc + edge(cc_edges[i,1], cc_edges[i,2])\n}\n\nww <- make_empty_graph() + vertices(nodelist$ID, hh=nodelist$HH, color=nodelist$Color, shape=nodelist$Shape)\nfor(i in 1:nrow(ww_edges)) {\n\tww <- ww + edge(ww_edges[i,1], ww_edges[i,2])\n}\nww_for_model <- as.undirected(ww, mode=""collapse"")\n\nr <- make_empty_graph() + vertices(nodelist$ID, hh=nodelist$HH, color=nodelist$Color, shape=nodelist$Shape)\nfor(i in 1:nrow(r_edges)) {\n\tr <- r + edge(r_edges$Source[i], r_edges[i,2], weight=r_edges[i,3])\n}\nr <- as.undirected(r, mode=""collapse"")\n\n\n\n### FIGURE 2\n\ncc_plot <- delete.vertices(cc, which(degree(cc)==0))\nww_plot <- delete.vertices(ww, which(degree(ww)==0))\n\npdf(""Network Graphs.pdf"", width=11, height=6)\npar(mfrow=c(1, 2))\nplot(ww_plot, vertex.size=5, vertex.color=V(ww_plot)$color, vertex.shape=V(ww_plot)$shape, vertex.label=NA, edge.color=""black"", edge.width=1, edge.arrow.size=.25, main=""Work"")\nplot(cc_plot, vertex.size=5, vertex.color=V(cc_plot)$color, vertex.shape=V(cc_plot)$shape, vertex.label=NA, edge.color=""black"", edge.width=1, edge.arrow.size=.25, main=""Childcare"")\ndev.off()\n\n\n\n\n### ERGM\n\nrequire(intergraph)\nrequire(statnet)\n\ncc_net <- asNetwork(simplify(cc))\nww_net <- asNetwork(simplify(ww_for_model))\nrel_net <- asNetwork(simplify(r))\n\nrel_adj <- get.adjacency(r, attr=""weight"")\nww_adj <- get.adjacency(ww)\nww_rel <- as.matrix(rel_adj) * as.matrix(ww_adj)\ninteraction <- graph_from_adjacency_matrix(ww_rel, mode=""undirected"", weighted=TRUE)\nint_net <- asNetwork(interaction)\n\nergm <- ergm(cc_net ~ edges + edgecov(rel_net, ""weight"") + edgecov(ww_net) + edgecov(int_net) + mutual + nodematch(\'hh\') + idegree(0) + odegree(0))\nsummary(ergm)\n\nrequire(broom)\ntidy(ergm, conf.int=TRUE, conf.level=0.95)\n\n\n\n\n\n\n\n### FIGURE 3\n\nrequire(ggplot2)\n\nwork <- read.csv(""EgoWorkTies_deidentified.csv"")\n\nA <- aggregate(work, by=list(work$NumAlter, work$Egojob), FUN=mean, na.rm=T)\nA$GenderWork <- NA\nA$GenderWork[A$Group.2==""Fish""] <- ""Women\'s fishing ties""\nA$GenderWork[A$Group.2==""Trade""] <- ""Women\'s trading ties""\ndf1 <- data.frame(Work=A$GenderWork, Alters=A$NumAlter, AvgCount=A$NumKin, Kin=""Kin"")\ndf2 <- data.frame(Work=A$GenderWork, Alters=A$NumAlter, AvgCount=A$NumNonKin, Kin=""Non-kin"")\ndf <- rbind(df1, df2)\n\nggplot(df, aes(x = Alters, fill = Kin,\n                 y = ifelse(test = Kin == ""Kin"",\n                            yes = -AvgCount, no = AvgCount))) + \n\tgeom_bar(stat = ""identity"") +\n\tscale_y_continuous(breaks=seq(-6,6), labels = abs, limits = 6 * c(-1,1)) +\n\tlabs(x = ""Number of Alters"", y = ""Split of Alters that are Kin/Non-kin"") + \n\tscale_colour_manual(values = c(""darkorchid"", ""chartreuse3""), aesthetics = c(""colour"", ""fill"")) +\n\tscale_x_continuous(breaks=seq(1,7)) +\n\tcoord_flip() +\n\tfacet_wrap(~ Work) +\n\ttheme_bw() + \n\ttheme(legend.position = ""right"",\n\t\tlegend.title=element_text(color=""black"", size=12),\n\t\tlegend.text=element_text(size=12),\n\t\tplot.margin=unit(c(.5,.5,.5,.5), ""cm""),\n\t\tpanel.grid.major = element_blank(),\n\t\tpanel.grid.minor = element_blank(),\n\t\taxis.title.y=element_text(size=rel(1), margin=margin(0,15,0,0)),\n\t\taxis.title.x=element_text(size=rel(1), margin=margin(15,0,0,0)),\n\t\taxis.text.x=element_text(color=""black"", size=12),\n\t\taxis.text.y=element_text(color=""black"", size=12))\n\n\n\n\n### FIGURE 4\n\nrequire(ggplot2)\n\nchildcare <- read.csv(""EgoChildcareTies_deidentified.csv"")\n\nA <- aggregate(childcare, by=list(childcare$NumAlter, childcare$Egojob), FUN=mean, na.rm=T)\nA$GenderChildcare <- NA\nA$Genderchildcare[A$Group.2==""fish""] <- ""Women who fish""\nA$Genderchildcare[A$Group.2==""trade""] <- ""Women who trade""\ndf1 <- data.frame(childcare=A$Genderchildcare, Alters=A$NumAlter, AvgCount=A$NumKin, Kin=""Kin"")\ndf2 <- data.frame(childcare=A$Genderchildcare, Alters=A$NumAlter, AvgCount=A$NumNonKin, Kin=""Non-kin"")\ndf <- rbind(df1, df2)\n\n\nggplot(df, aes(x = Alters, fill = Kin,\n                 y = ifelse(test = Kin == ""Kin"",\n                            yes = -AvgCount, no = AvgCount))) + \n\tgeom_bar(stat = ""identity"") +\n\tscale_y_continuous(breaks=seq(-6,6), labels = abs, limits = 6 * c(-1,1)) +\n\tlabs(x = ""Number of Childcare Ties"", y = ""Split of Alters that are Kin/Non-kin"") + \n\tscale_colour_manual(values = c(""darkcyan"", ""tomato""), aesthetics = c(""colour"", ""fill"")) +\n\tscale_x_continuous(breaks=seq(1,7)) +\n\tcoord_flip() +\n\tfacet_wrap(~ childcare) +\n\ttheme_bw() + \n\ttheme(legend.position = ""right"",\n\t\tlegend.title=element_text(color=""black"", size=12),\n\t\tlegend.text=element_text(size=12),\n\t\tplot.margin=unit(c(.5,.5,.5,.5), ""cm""),\n\t\tpanel.grid.major = element_blank(),\n\t\tpanel.grid.minor = element_blank(),\n\t\taxis.title.y=element_text(size=rel(1), margin=margin(0,15,0,0)),\n\t\taxis.title.x=element_text(size=rel(1), margin=margin(15,0,0,0)),\n\t\taxis.text.x=element_text(color=""black"", size=12),\n\t\taxis.text.y=element_text(color=""black"", size=12))\n\n']","Shodagor Women Cooperate Across Domains of Work and Childcare to Solve an Adaptive Problem Across human societies, womens economic production and their contributions to childcare are critical in supporting reproductive fitness for themselves, their spouses, and children. Yet, the necessity of performing both work and childcare tasks presents women with an adaptive problem in which they must determine how best to allocate their time and energy between these tasks. Women often utilize cooperative relationships with alloparents to solve this problem, but whether or not women cooperate across different domains (e.g., work and childcare) to access alloparents remains relatively under-explored. Using social network data collected with Shodagor households in Bangladesh, we show that women who need childcare help in order to work draw on cooperative work partners as potential alloparents, and that all women rely heavily on kin, but not reciprocal cooperation for childcare help. These results indicate that Shodagor women strategize to create work and childcare relationships in ways that help solve the adaptive problem they face. We discuss the implications of our results and the example provided by Shodagor women for a broader understanding of womens cooperative relationships, including the importance of socioecological circumstances and gendered divisions of labor in shaping womens cooperative strategies.",1
Life Cycle Modeling of Tech & Strategies for a Sustainable Freight System in California,"California's freight transportation system is a vital part of the state's economy, but generates a high portion of local pollution in parts of the state with poor air quality. In recognition of these challenges, Executive Order B-32-15 encourages adoption of advanced vehicle technologies and infrastructure, as well as the use of alternative energy and fuels in the freight sector. These measures are echoed in the state's Sustainable Freight Action Plan.Most emissions reductions from freight vehicle activities are expected to come from the deployment of new emissions control devices, efficiency improvements, and zero emissions vehicle technologies for on-road trucks. Where emissions occur, and how emissions of different pollutants are affected by factors including vocation, duty cycle, powertrain configuration, and fuel pathway, will influence the effectiveness and economic costs of emissions reduction strategies. This research will apply a life cycle perspective to assess the energy use, greenhouse gas emissions, air quality impacts, and costs of on-road freight vehicle technologies and operational strategies identified under the Sustainable Freight Action Plan. Findings will be synthesized and reported as abatement costs and will be a first step in building a supply curve for GHG mitigation from the freight sector.",,"Life Cycle Modeling of Tech & Strategies for a Sustainable Freight System in California California's freight transportation system is a vital part of the state's economy, but generates a high portion of local pollution in parts of the state with poor air quality. In recognition of these challenges, Executive Order B-32-15 encourages adoption of advanced vehicle technologies and infrastructure, as well as the use of alternative energy and fuels in the freight sector. These measures are echoed in the state's Sustainable Freight Action Plan.Most emissions reductions from freight vehicle activities are expected to come from the deployment of new emissions control devices, efficiency improvements, and zero emissions vehicle technologies for on-road trucks. Where emissions occur, and how emissions of different pollutants are affected by factors including vocation, duty cycle, powertrain configuration, and fuel pathway, will influence the effectiveness and economic costs of emissions reduction strategies. This research will apply a life cycle perspective to assess the energy use, greenhouse gas emissions, air quality impacts, and costs of on-road freight vehicle technologies and operational strategies identified under the Sustainable Freight Action Plan. Findings will be synthesized and reported as abatement costs and will be a first step in building a supply curve for GHG mitigation from the freight sector.",1
Characteristics of the urban sewer system and rat presence in Seattle,"Rats are abundant and ubiquitous in urban environments. There has been increasing attention to the need for evidence-based, integrated rat management and surveillance approaches because rats can compromise public health and impose economic costs. Yet there are few studies that characterize rat distributions in sewers and there are no studies that incorporate the complexity of sewer networks that encompass multiple sewer lines, all comprised of their own unique characteristics. To address this knowledge gap, this study identifies sewer characteristics that are associated with rat presence in the city of Seattle's urban sewer system. We obtained sewer baiting data from 1752 geotagged manholes to monitor rat presence and constructed generalized additive models to account for spatial autocorrelation. Sewer rats were unevenly distributed across sampled manholes with clusters of higher rat presence at upper elevations, within sanitary pipes, narrower pipes, pipes at a shallower depth, and older pipes. These findings are important because identifying features of urban sewers that promote rat presence may allow municipalities to target areas for rat control activities and sewer maintenance. These findings suggest the need to evaluate additional characteristics of the surface environment and identify the factors driving rat movement within sewers, across the surface, and between the surface and the sewers.","['###############################################################################\n### Key R Codes for DO1 10.1007/s11252-022-01255-2 Statistical Analyses\n### Characteristics of the Urban Sewer System and Rat Presence in Seattle\n### Urban Ecosystems\n\n###############################################################################\n### load dataset for analysis\n\ncleaneddata <- read.table(file = ""Data/Urban Ecosystems/Guo et al_10.1007_s11252-022-01255-2.csv"", header=T, sep="","")\n\ncleaneddata$Replicates <- factor(cleaneddata$Replicates,\n                                 levels = c(""1"",""2"",""3"",""4""))\ncleaneddata$Replicates_M <- factor(cleaneddata$Replicates_M,\n                                   levels = c(""1"",""2"",""3"",""4""))\ncleaneddata$BaitingArea <- factor(cleaneddata$BaitingArea,\n                                  levels = c(""CS"",""MQ"",""NE"",""NW"",""SS"",""SW""))\ncleaneddata$Service_Season <- factor(cleaneddata$Service_Season,\n                                     levels = c(""Q1"",""Q2"",""Q3"",""Q4""))\ncleaneddata$Complaint <- factor(cleaneddata$Complaint,\n                                levels = c(""N"",""Y""))\ncleaneddata$Result <- factor(cleaneddata$Result,\n                             levels = c(""N"",""Y""))\ncleaneddata$MainlineFEATYPE <- factor(cleaneddata$MainlineFEATYPE,\n                                      levels = c(""Mainline"",""Stub""))\ncleaneddata$MainlineFeatureType <- factor(cleaneddata$MainlineFeatureType,\n                                          levels = c(""Mainline"",""Stub"",""Junction""))\ncleaneddata$MainlineConnected <- factor(cleaneddata$MainlineConnected,\n                                        levels = c(""N"",""Y""))\ncleaneddata$StubConnected <- factor(cleaneddata$StubConnected,\n                                    levels = c(""N"",""Y""))\ncleaneddata$MainlinePRBL_FLOW <- factor(cleaneddata$MainlinePRBL_FLOW,\n                                        levels = c(""Combined"",""Sanitary""))\ncleaneddata$ProbableFlow <- factor(cleaneddata$ProbableFlow,\n                                   levels = c(""Combined"",""Sanitary"",""Junction""))\ncleaneddata$SanitaryConnected_PF <- factor(cleaneddata$SanitaryConnected_PF,\n                                           levels = c(""N"",""Y""))\ncleaneddata$CombinedConnected_PF <- factor(cleaneddata$CombinedConnected_PF,\n                                           levels = c(""N"",""Y""))\ncleaneddata$SanitaryAll_PF <- factor(cleaneddata$CombinedConnected_PF,\n                                           levels = c(""Y"",""N""),\n                                     labels = c(""No"",""Yes""))\ncleaneddata$CombinedAll_PF <- factor(cleaneddata$SanitaryConnected_PF,\n                                     levels = c(""Y"",""N""),\n                                     labels = c(""No"",""Yes""))\ncleaneddata$MainlinePERMIT <- factor(cleaneddata$MainlinePERMIT,\n                                     levels = c(""Combined"",""Sanitary""))\ncleaneddata$PermittedUse <- factor(cleaneddata$PermittedUse,\n                                   levels = c(""Combined"",""Sanitary"",""Junction""))\ncleaneddata$SanitaryConnected_PU <- factor(cleaneddata$SanitaryConnected_PU,\n                                           levels = c(""N"",""Y""))\ncleaneddata$CombinedConnected_PU <- factor(cleaneddata$CombinedConnected_PU,\n                                           levels = c(""N"",""Y""))\ncleaneddata$SanitaryAll_PU <- factor(cleaneddata$CombinedConnected_PU,\n                                     levels = c(""Y"",""N""),\n                                     labels = c(""No"",""Yes""))\ncleaneddata$CombinedAll_PU <- factor(cleaneddata$SanitaryConnected_PU,\n                                     levels = c(""Y"",""N""),\n                                     labels = c(""No"",""Yes""))\ncleaneddata$MainlineMATERIAL <- factor(cleaneddata$MainlineMATERIAL,\n                                       levels = c(""AC"",""BRK"",""CON"",""DIP"",""RCP"",""VC""))\ncleaneddata$MaterialType <- factor(cleaneddata$MaterialType,\n                                   levels = c(""AC"",""BRK"",""CON"",""DIP"",""RCP"",""VC"",""Mix""))\ncleaneddata$BRK_VC_Included <- factor(cleaneddata$BRK_VC_Included,\n                                      levels = c(""N"",""Y""))\ncleaneddata$RCP_Included <- factor(cleaneddata$RCP_Included,\n                                   levels = c(""N"",""Y""))\ncleaneddata$RCP_DIP_Included <- factor(cleaneddata$RCP_DIP_Included,\n                                       levels = c(""N"",""Y""))\ncleaneddata$CON_Included <- factor(cleaneddata$CON_Included,\n                                   levels = c(""N"",""Y""))\ncleaneddata$CON_RCP_Included <- factor(cleaneddata$CON_RCP_Included,\n                                       levels = c(""N"",""Y""))\ncleaneddata$DIP_Included <- factor(cleaneddata$DIP_Included,\n                                   levels = c(""N"",""Y""))\ncleaneddata$AC_Included <- factor(cleaneddata$AC_Included,\n                                  levels = c(""N"",""Y""))\ncleaneddata$BRK_Included <- factor(cleaneddata$BRK_Included,\n                                   levels = c(""N"",""Y""))\ncleaneddata$VC_Included <- factor(cleaneddata$VC_Included,\n                                  levels = c(""N"",""Y""))\ncleaneddata$CON_RCP_AC_Included <- factor(cleaneddata$CON_RCP_AC_Included,\n                                          levels = c(""N"",""Y""))\ncleaneddata$MainlineLINING_FLAG <- factor(cleaneddata$MainlineLINING_FLAG,\n                                          levels = c(""N"",""Y""))\ncleaneddata$LiningFlag <- factor(cleaneddata$LiningFlag,\n                                 levels = c(""N"",""Y""))\ncleaneddata$LiningFlag_Y <- factor(cleaneddata$LiningFlag_Y,\n                                   levels = c(""N"",""Y""))\ncleaneddata$VCAll <- factor(cleaneddata$VCAll,\n                            levels = c(""N"",""Y""))\ncleaneddata$BRKAll <- factor(cleaneddata$BRKAll,\n                             levels = c(""N"",""Y""))\ncleaneddata$DIPAll <- factor(cleaneddata$DIPAll,\n                             levels = c(""N"",""Y""))\ncleaneddata$ACAll <- factor(cleaneddata$ACAll,\n                            levels = c(""N"",""Y""))\ncleaneddata$CONAll <- factor(cleaneddata$CONAll,\n                             levels = c(""N"",""Y""))\ncleaneddata$RCPAll <- factor(cleaneddata$RCPAll,\n                             levels = c(""N"",""Y""))\ncleaneddata$LiningFlagAll <- factor(cleaneddata$LiningFlagAll,\n                                    levels = c(""N"",""Y""))\ncleaneddata$mainlineAll <- factor(cleaneddata$mainlineAll,\n                                  levels = c(""N"",""Y""))\ncleaneddata$stubAll <- factor(cleaneddata$stubAll,\n                              levels = c(""N"",""Y""))\ncleaneddata$RCP_DIP_All <- factor(cleaneddata$RCP_DIP_All,\n                                  levels = c(""N"",""Y""))\ncleaneddata$ManholeELEV_Cat <- factor(cleaneddata$ManholeELEV_Cat,\n                                      levels = c(""<720"","">720""))\ncleaneddata$Length_Min_Cat <- factor(cleaneddata$Length_Min_Cat,\n                                     levels = c(""<170"","">170""))\ncleaneddata$Diameter_Max_Cat <- factor(cleaneddata$Diameter_Max_Cat,\n                                       levels = c(""<=8"","">8""))\ncleaneddata$Service_Season <- factor(cleaneddata$Service_Season,\n                                     levels = c(""Q2"",""Q1"",""Q3"",""Q4""))\ncleaneddata$CLASS_DESC <- factor(cleaneddata$CLASS_DESC,\n                                 levels = c(""Single Family"",""Multi-Family""))\ncleaneddata$Simulated_MH_Flooding_5yr <- factor(cleaneddata$Simulated_MH_Flooding_5yr,\n                                                levels = c(""N"",""Y""))\ncleaneddata$ImperviousSurface <- factor(cleaneddata$ImperviousSurface,\n                                        levels = c(""N"",""Y""))\ncleaneddata$zoning <- factor(cleaneddata$zoning,\n                             levels = c(""Residential"",""Commercial/Mixed Use""))\ncleaneddata$ManholeFeatureType <- factor(cleaneddata$ManholeFeatureType,\n                                         levels = c(""Drop MH"",""Flush Tank"",""Maintenance Hole"",""Overflow MH"",""Plug"",""Tee""))\ncleaneddata$ManholeOwnerName <- as.factor(cleaneddata$ManholeOwnerName)\ncleaneddata$MainlineOwnerName <- as.factor(cleaneddata$MainlineOwnerName)\ncleaneddata$MainlinePIPE_SHP <- as.factor(cleaneddata$MainlinePIPE_SHP)\ncleaneddata$PrivateConnected <- as.factor(cleaneddata$PrivateConnected)\ncleaneddata$OvalConnected <- as.factor(cleaneddata$OvalConnected)\ncleaneddata$CLASS_DESC <- factor(cleaneddata$CLASS_DESC,\n                                 levels = c(""Single Family"",""Multi-Family""))\n\n###############################################################################\n### sub set\n\nsub.complete <- cleaneddata[cleaneddata$Replicates %in% c(""1"",""2"",""3"",""4""),]\n\n###############################################################################\n### subset all variables after initial univariable analysis\n\nlibrary(dplyr)\n\nsub.complete_initial <- sub.complete %>%\n  select(EQNUM,Result,BaitingArea,Service_Season,Precipitation,Temp,Complaint,\n         ManholeFeatureType,CVR_ELEV,ManholeELEV_Cat,MainlineConnected,\n         StubConnected,CombinedAll_PF,SanitaryAll_PF,\n         CombinedAll_PU,SanitaryAll_PU,BRKAll,VCAll,ACAll,CONAll,\n         RCP_DIP_All,Length_Min,Diameter_Max_Cat,Depth_Min,\n         X_COORD,Y_COORD,CLASS_DESC,Simulated_MH_Flooding_5yr,ImperviousSurface,\n         Slope_Min,Installation_Min,PrivateConnected,OvalConnected,LiningFlagAll,\n         Replicates,Length_Max,Diameter_Min_Cat,Depth_Max,Slope_Max,Installation_Max,\n         Diameter_Max)\n\nsub.complete_initial_naomit <- na.omit(sub.complete_initial)\n\nlibrary(mgcv)\n\n###############################################################################\n### bivariable GAM\n\n### try k for coordinates\nlibrary(mgcv)\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=5),\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nsummary(mod1)\ngam.check(mod1)\n\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=10),\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nsummary(mod1)\ngam.check(mod1)\n\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=20),\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nsummary(mod1)\ngam.check(mod1)\n\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=30),\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nsummary(mod1)\ngam.check(mod1)\n\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=40),\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nsummary(mod1)\ngam.check(mod1)\n\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=50),\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nsummary(mod1)\ngam.check(mod1)\n\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=60),\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nsummary(mod1)\ngam.check(mod1)\n\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=70),\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nsummary(mod1)\ngam.check(mod1)\n\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=100),\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nsummary(mod1)\ngam.check(mod1)\n\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=150),\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nsummary(mod1)\ngam.check(mod1)\n\n# same k-index at 0.85\n# pick k=40\n\n###############################################################################\n### Bivariable analysis, ML\n\ndevtools::install_github(""samclifford/mgcv.helper"")\nlibrary(mgcv.helper)\nlibrary(mgcv)\n\n###############################################################################\n### use k=40 for coordinates, ML\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=40),\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod1)\ngam.check(mod1)\n\n### service_season\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Service_Season,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### Precipitation\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Precipitation,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### Temp\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Temp,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### CVR_ELEV (Surface elevation)\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\nconfint.gam(mod2)\n\n### ManholeELEV_Cat (pipe elevation)\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + ManholeELEV_Cat,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\nconfint.gam(mod2)\n\n### StubConnected\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + StubConnected,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\nconfint.gam(mod2)\n\n### CombinedAll_PF\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CombinedAll_PF,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### SanitaryAll_PF\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + SanitaryAll_PF,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\nconfint.gam(mod2)\n\n### CombinedAll_PU\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CombinedAll_PU,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### SanitaryAll_PU\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + SanitaryAll_PU,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### VC_All\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + VCAll,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### CON_All\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CONAll,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### BRKAll\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + BRKAll,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\ntable(sub.complete_initial_naomit$Result,sub.complete_initial_naomit$BRKAll)\n\n### ACAll\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + ACAll,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### RCP_DIP_All\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + RCP_DIP_All,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\ntable(sub.complete_initial_naomit$Result,sub.complete_initial_naomit$RCP_DIP_All)\nconfint.gam(mod2)\n\n### length\nmod3 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Length_Max,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\n\nmod4 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Length_Min,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\n\nsummary(mod3)\nsummary(mod4)\nAIC(mod3)\nAIC(mod4)\n# keep length_min\n\n### Length_Min\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Length_Min,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### diameter\nmod3 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Diameter_Max_Cat,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\n\nmod4 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Diameter_Min_Cat,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\n\nsummary(mod3)\nsummary(mod4)\nAIC(mod3)\nAIC(mod4)\n# keep diameter_max_cat\n\n### Diameter_Max_Cat\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Diameter_Max_Cat,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\nconfint.gam(mod2)\n\n### slope\nmod3 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Slope_Max,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\n\nmod4 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Slope_Min,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\n\nsummary(mod3)\nsummary(mod4)\nAIC(mod3)\nAIC(mod4)\n# keep slope_min\n\n### Slope_Min\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Slope_Min,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### depth\nmod3 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Depth_Max,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\n\nmod4 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Depth_Min,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\n\nsummary(mod3)\nsummary(mod4)\nAIC(mod3)\nAIC(mod4)\n# keep depth_min\n\n### Depth_Min\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Depth_Min,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\nconfint.gam(mod2)\n\n### installation\nmod3 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Installation_Max,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\n\nmod4 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Installation_Min,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\n\nsummary(mod3)\nsummary(mod4)\nAIC(mod3)\nAIC(mod4)\n# keep installation_max\n\n### Installation_Max\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Installation_Max,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\nconfint.gam(mod2)\n\n### ImperviousSurface\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + ImperviousSurface,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\nconfint.gam(mod2)\n\n### Complaint\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Complaint,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\nconfint.gam(mod2)\n\n### ManholeFeatureType\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + ManholeFeatureType,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### MainlineConnected\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + MainlineConnected,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### PrivateConnected\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + PrivateConnected,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### OvalConnected\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + OvalConnected,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\ntable(sub.complete_initial_naomit$Result,sub.complete_initial_naomit$OvalConnected)\n\n### LiningFlagAll\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + LiningFlagAll,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\n\n### CLASS_DESC\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CLASS_DESC,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\ntable(sub.complete_initial_naomit$Result,sub.complete_initial_naomit$CLASS_DESC)\n\n### Simulated_MH_Flooding\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Simulated_MH_Flooding_5yr,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nanova(mod1,mod2,test = ""LRT"")\ntable(sub.complete_initial_naomit$Result,sub.complete_initial_naomit$ImperviousSurface)\n\n###############################################################################\n\n### backward selection, ML\n### k=40 for coordinates\n\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Service_Season + CVR_ELEV\n            + ManholeELEV_Cat + StubConnected + SanitaryAll_PF \n            + RCP_DIP_All + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + Complaint + ImperviousSurface,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod1)\nAIC(mod1)\nBIC(mod1)\n\n# Remove ManholeELEV\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Service_Season + CVR_ELEV\n            + StubConnected + SanitaryAll_PF \n            + RCP_DIP_All + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + Complaint + ImperviousSurface,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod2)\nAIC(mod2)\nBIC(mod2)\nanova(mod2,mod1,test = ""LRT"")\n\n# Remove Complaint\nmod3 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + Service_Season + CVR_ELEV\n            + StubConnected + SanitaryAll_PF \n            + RCP_DIP_All + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + ImperviousSurface,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod3)\nAIC(mod3)\nBIC(mod3)\nanova(mod3,mod2,test = ""LRT"")\n\n# Remove Impervious Service_Season\nmod4 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV\n            + StubConnected + SanitaryAll_PF \n            + RCP_DIP_All + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + ImperviousSurface,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod4)\nAIC(mod4)\nBIC(mod4)\nanova(mod3,mod4,test = ""LRT"")\n\n# Remove RCP_DIP\nmod5 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV\n            + StubConnected + SanitaryAll_PF \n            + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + ImperviousSurface,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod5)\nAIC(mod5)\nBIC(mod5)\nanova(mod5,mod4,test = ""LRT"")\n\n# Remove StubConnected\nmod6 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV\n            + SanitaryAll_PF \n            + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + ImperviousSurface,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod6)\nAIC(mod6)\nBIC(mod6)\nanova(mod5,mod6,test = ""LRT"")\n\n# Remove Impervious Surface\nmod7 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV\n            + SanitaryAll_PF \n            + Diameter_Max_Cat + Depth_Min + Installation_Max,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod7)\nAIC(mod7)\nBIC(mod7)\nanova(mod7,mod6,test = ""LRT"")\n\n# Remove SanitaryAll_PF\nmod8 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV \n            + Diameter_Max_Cat + Depth_Min + Installation_Max,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""ML"")\nsummary(mod8)\nAIC(mod8)\nBIC(mod8)\nanova(mod7,mod8,test = ""LRT"")\n# p<0.05\n# stop here, keep SanitaryAll_PF\n\n###############################################################################\n### backward selection, final model, ML, k=40\nmod_b1 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV \n              + SanitaryAll_PF \n              + Diameter_Max_Cat + Depth_Min + Installation_Max,\n              family = ""binomial"",\n              data = sub.complete_initial_naomit,\n              method = ""ML"")\n\nsummary(mod_b1)\nexp(coef(mod_b1))\ngam.check(mod_b1)\n\nconfint.gam(mod_b1)\n\n###############################################################################\n### backward selection, based on ML\n### convert to REML\nmod_b2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV \n              + SanitaryAll_PF \n              + Diameter_Max_Cat + Depth_Min + Installation_Max,\n              family = ""binomial"",\n              data = sub.complete_initial_naomit,\n              method = ""REML"")\n\nsummary(mod_b1)\nsummary(mod_b2)\nAIC(mod_b1)\nBIC(mod_b1)\nAIC(mod_b2)\nBIC(mod_b2)\n\nconfint.gam(mod_b2)\n\nlogLik.gam(mod_b2)\n\nvis.gam(mod_b2,\n        view = c(""X_COORD"",""Y_COORD""),\n        type = ""response"",\n        plot.type = ""contour"",\n        color = ""heat"")\n\n### model prediction, convert to probability\nplot(mod_b2,\n     pages = 1,\n     trans = plogis,\n     all.terms = F,\n     residuals = T,\n     pch=20)\n\n###############################################################################\n### try interactions for mod_b2 when k=40, REML\n\n# slope and precipitation\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV \n            + SanitaryAll_PF \n            + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + Slope_Min + Precipitation,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV \n            + SanitaryAll_PF \n            + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + Slope_Min + Precipitation + Slope_Min*Precipitation,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nanova(mod1,mod2,test = ""LRT"")\n\n# slope and diameter\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV \n            + SanitaryAll_PF \n            + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + Slope_Min,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV \n            + SanitaryAll_PF \n            + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + Slope_Min + Slope_Min*Diameter_Max_Cat,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nanova(mod1,mod2,test = ""LRT"")\n\n# precipitation and temperature\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV \n            + SanitaryAll_PF \n            + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + Precipitation + Temp,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV \n            + SanitaryAll_PF \n            + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + Precipitation + Temp + Precipitation*Temp,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nanova(mod1,mod2,test = ""LRT"")\n\n# material and installation\nmod1 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV \n            + SanitaryAll_PF \n            + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + RCP_DIP_All,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nmod2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV \n            + SanitaryAll_PF \n            + Diameter_Max_Cat + Depth_Min + Installation_Max\n            + RCP_DIP_All + Installation_Min*RCP_DIP_All,\n            family = ""binomial"",\n            data = sub.complete_initial_naomit,\n            method = ""REML"")\nanova(mod1,mod2,test = ""LRT"")\n\n###############################################################################\n### ROC\nlibrary(pROC)\n\nroc(sub.complete_initial_naomit$Result,mod_b2$fitted.values,plot = TRUE,\n    legacy.axes=TRUE,\n    percent = TRUE,\n    xlab=""False Positive Percentage"",\n    ylab=""True Positive Percentage"",\n    col=""blue"",\n    lwd=3,\n    main=""ROC curve for Backward k=40 REML"")\n\n###############################################################################\n### final model\nmod_b2 <- gam(Result ~ s(X_COORD,Y_COORD,k=40) + CVR_ELEV \n              + SanitaryAll_PF \n              + Diameter_Max_Cat + Depth_Min + Installation_Max,\n              family = ""binomial"",\n              data = sub.complete_initial_naomit,\n              method = ""REML"")\n\nsummary(mod_b2)\nexp(coef(mod_b2))\ngam.check(mod_b2)\n\n\n\n\n\n\n\n']","Characteristics of the urban sewer system and rat presence in Seattle Rats are abundant and ubiquitous in urban environments. There has been increasing attention to the need for evidence-based, integrated rat management and surveillance approaches because rats can compromise public health and impose economic costs. Yet there are few studies that characterize rat distributions in sewers and there are no studies that incorporate the complexity of sewer networks that encompass multiple sewer lines, all comprised of their own unique characteristics. To address this knowledge gap, this study identifies sewer characteristics that are associated with rat presence in the city of Seattle's urban sewer system. We obtained sewer baiting data from 1752 geotagged manholes to monitor rat presence and constructed generalized additive models to account for spatial autocorrelation. Sewer rats were unevenly distributed across sampled manholes with clusters of higher rat presence at upper elevations, within sanitary pipes, narrower pipes, pipes at a shallower depth, and older pipes. These findings are important because identifying features of urban sewers that promote rat presence may allow municipalities to target areas for rat control activities and sewer maintenance. These findings suggest the need to evaluate additional characteristics of the surface environment and identify the factors driving rat movement within sewers, across the surface, and between the surface and the sewers.",1
Data from: Consistent trait-environment relationships within and across tundra plant communities,"A fundamental assumption in trait-based ecology is that relationships between traits and environmental conditions are globally consistent. We use field-quantified microclimate and soil data to explore if trait-environment relationships are generalisable across plant communities and spatial scales. We collected data from 6720 plots and 217 species across four distinct tundra regions from both hemispheres. We combine this data with over 76000 database trait records to relate local plant community trait composition to broad gradients of key environmental drivers: soil moisture, soil temperature, soil pH, and potential solar radiation. Results revealed strong, consistent trait-environment relationships across Arctic and Antarctic regions. This indicates that the detected relationships are transferable between tundra plant communities also when fine-scale environmental heterogeneity is accounted for, and that variation in local conditions heavily influences both structural and leaf economic traits. Our results strengthen the biological and mechanistic basis for climate change impact predictions of vulnerable high-latitude ecosystems.Kemppinen, Niittynen, le Roux, Momberg, Happonen, Aalto, Rautakoski, Enquist, Vandvik, Halbritter, Maitner & Luoto (2021). Consistent trait-environment relationships within and across tundra plant communities. Nature Ecology and EvolutionThese are the data and codes from Kemppinen et al. (2021).","['library(tidyverse) # for everything\r\n\r\n#set directory\r\nsetwd(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus"")\r\ndirectory = getwd()\r\n\r\n#set factors\r\nresponse = c (""heig"", ""SLA"", ""seed"", ""LDMC"", ""leaA"", ""leaN"", ""leaP"")\r\nsites = c (""SVA"", ""KAN"", ""KIL"", ""MAR"")\r\n\r\n#create dataframe\r\neffects = data.frame()\r\n\r\n#specify files\r\nfiles = dir(pattern = ""_effects.txt"", full.names = TRUE, ignore.case = TRUE)\r\n\r\nfor (i in files){\r\n  \r\n  #i = ""./heig_effects.txt""\r\n  \r\n  #specify file\r\n  file  = read.table(paste0 (directory,i))\r\n  \r\n  #add response name to new column\r\n  file[""trait""] = substr(strsplit(i,""_"")[[1]][1], 3, 1000)\r\n  \r\n  #reoderd columns\r\n  file %>% select(trait, 1:(ncol(file)-1)) %>% \r\n    mutate(site = factor(site, levels = sites)) %>% \r\n    arrange(site) -> file\r\n  \r\n  #file = (file[, c(5, 1, 2, 3, 4)])\r\n  \r\n  effects <- bind_rows(effects, file)\r\n}\r\n\r\neffects %>% mutate(trait = factor(trait, levels = response)) %>% \r\n  arrange(trait) -> effects\r\n\r\nwrite.csv(effects, file = ""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus/effects.csv"", dec = ""."", row.names = F)\r\n', 'library(tidyverse) # for everything\r\n\r\n#set directory\r\nsetwd(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus_coarse"")\r\ndirectory = getwd()\r\n\r\n#set factors\r\nresponse = c (""heig"", ""SLA"", ""seed"", ""LDMC"", ""leaA"", ""leaN"", ""leaP"")\r\nsites = c (""SVA"", ""KAN"", ""KIL"", ""MAR"")\r\n\r\n#create dataframe\r\neffects = data.frame()\r\n\r\n#specify files\r\nfiles = dir(pattern = ""_effects.txt"", full.names = TRUE, ignore.case = TRUE)\r\n\r\nfor (i in files){\r\n  \r\n  #i = ""./heig_effects.txt""\r\n  \r\n  #specify file\r\n  file  = read.table(paste0 (directory,i))\r\n  \r\n  #add response name to new column\r\n  file[""trait""] = substr(strsplit(i,""_"")[[1]][1], 3, 1000)\r\n  \r\n  #reoderd columns\r\n  file %>% select(trait, 1:(ncol(file)-1)) %>% \r\n    mutate(site = factor(site, levels = sites)) %>% \r\n    arrange(site) -> file\r\n  \r\n  #file = (file[, c(5, 1, 2, 3, 4)])\r\n  \r\n  effects <- bind_rows(effects, file)\r\n}\r\n\r\neffects %>% mutate(trait = factor(trait, levels = response)) %>% \r\n  arrange(trait) -> effects\r\n\r\nwrite.csv(effects, file = ""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus_coarse/effects.csv"", dec = ""."", row.names = F)\r\n', '################################################################################################################\r\n################################################################################################################\r\n#### Calculate response curves with HGAM #######################################################################\r\n################################################################################################################\r\n################################################################################################################\r\n\r\nlibrary(tidyverse) # for everything\r\nlibrary(colormap) #generating color codes\r\nlibrary(mgcv) # model\r\nlibrary(gridExtra) #for printing plots on one page\r\nlibrary(gratia) # extended plotting and analysis of mgcv models, uses ggplot\r\nlibrary(itsadug) #plot_smooth function for gam\r\nlibrary(lattice) # print pdf\r\nlibrary(reshape)\r\nlibrary(stringr)\r\n\r\n# read data\r\ntrait <- read.csv(""F:/ARTIKKELIT/ARTIKKELI4/r/data/trait_genus.csv"", sep="","")\r\n\r\n# output path\r\nfilepath = file.path(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus/"")\r\n\r\n# create column with site\r\ntrait$site = as.factor(substr(trait$plot,1,3))\r\n\r\n# create column with grid\r\ntrait$grid <- as.factor(unlist(lapply(trait$plot, function(x) str_extract_all(x, ""[0-9]+"")[[1]][1])))\r\n\r\n# keep only sites with full set of variables\r\n# trait <- trait[complete.cases(trait),]\r\n\r\n############################################################################################\r\n### model and predict ######################################################################\r\n############################################################################################\r\n\r\n# preparation for the loop\r\nnmrInd <- 7 # number of responses\r\n\r\nvariables <- c(""heig"", ""SLA"", ""leaA"", ""LDMC"", ""leaN"", ""leaP"", ""seed"",\r\n  ""mois"", ""temp"", ""pH"", ""radi"") # all variables, first the response(s)\r\n\r\nrespvars <- variables[1:nmrInd] # determine the responses\r\nexpvars <- variables[!(variables %in% respvars)] # determine the predictors\r\n\r\nlen_respvars <- 1:length(respvars) # the number of responses\r\nlen_expvars <- 1:length(expvars) # the number of predictors\r\n\r\npred <- trait # create new data set\r\n\r\npred$heig = log(pred$heig) # log-transform what needs it\r\npred$SLA  = log(pred$SLA)\r\npred$leaA = log(pred$leaA)\r\npred$LDMC = (pred$LDMC)\r\npred$leaN = (pred$leaN)\r\npred$leaF = (pred$leaF)\r\npred$seed = log(pred$seed)\r\n\r\n# loop through all responses to model and predict\r\nfor(ii in len_respvars){\r\n  \r\n  #ii <- 1\r\n  \r\n  # print which model you\'re working on\r\n  print(paste0(""modelling "", ii, "". trait: "", respvars[ii]))\r\n  print(Sys.time())\r\n  \r\n  ### model and predict ###################################################################################\r\n  \r\n  # rename the response ii in new data set\r\n  temptrait <- pred\r\n  colnames(temptrait)[colnames(temptrait)==paste0(respvars[ii])] <- ""response""\r\n  \r\n  temptrait %>% filter(complete.cases(temptrait %>% \r\n                                        select(plot,mois,temp,pH,\r\n                                               radi,response))) -> temptrait\r\n  \r\n  # cor(temptrait$temp, temptrait$response, use = ""pairwise.complete.obs"")\r\n  # hist(temptrait$temp)\r\n  # use the response ii in the full model\r\n  model_hgam = gam (response ~\r\n                      s(mois, k=20, m=2) + s(mois, site, k=20, m=2, bs=""fs"") +\r\n                      s(temp, k=20, m=2) + s(temp, site, k=20, m=2, bs=""fs"") +\r\n                      s(pH  , k=20, m=2) + s(pH  , site, k=20, m=2, bs=""fs"") +\r\n                      s(radi, k=20, m=2) + s(radi, site, k=20, m=2, bs=""fs"") +\r\n                      s(site, grid, bs = ""re""),\r\n                    method=""REML"", data=temptrait,\r\n                    min.sp = c(rep(1,times=16),0))\r\n  \r\n  # predict with the model\r\n  terms <- predict.gam(model_hgam, type = ""terms"", se.fit = TRUE)\r\n  \r\n  ### model and predict ###################################################################################\r\n  \r\n  # loop through predictors\r\n  for(iii in len_expvars){\r\n    \r\n    #iii <- 1\r\n    \r\n    # print which model you\'re working on\r\n    print(paste0(""modelling "", ii, "". trait: "", respvars[ii], "" and its "", iii, "". predictor: "", expvars[iii]))\r\n    \r\n    ### model without expvars iii ###################################################################################\r\n    \r\n    # rename the predictor iii in the new data set\r\n    vartrait <- temptrait\r\n    colnames(vartrait)[colnames(vartrait)==paste0(expvars[iii])] <- ""var1""\r\n    \r\n    # rename the rest of the responses\r\n    names(vartrait)[which(names(vartrait) %in% expvars[-iii])] <- paste0(""var"",2:4)\r\n    \r\n    # use the response ii in the model excluding s(expvars[iii], site)\r\n    model_hgam_global_var1 = gam (response ~\r\n                                    s(var1, k=20, m=2) +\r\n                                    s(var2, k=20, m=2) + s(var2, site, k=20, m=2, bs=""fs"") +\r\n                                    s(var3, k=20, m=2) + s(var3, site, k=20, m=2, bs=""fs"") +\r\n                                    s(var4, k=20, m=2) + s(var4, site, k=20, m=2, bs=""fs"") +\r\n                                    s(site, grid, bs = ""re""),\r\n                                  method=""REML"", data=vartrait,\r\n                                  sp = model_hgam$sp[-2],\r\n                                  min.sp = c(rep(1,times=13),0)) #this ensures the reduced model uses the same smoothing terms as the full model, to ensure comparability\r\n\r\n    # use the response ii in the model excluding s(expvars[iii]) & s(expvars[iii], site)\r\n    model_hgam_exclude_var1 = gam (response ~\r\n                                     s(var2, k=20, m=2) + s(var2, site, k=20, m=2, bs=""fs"") +\r\n                                     s(var3, k=20, m=2) + s(var3, site, k=20, m=2, bs=""fs"") +\r\n                                     s(var4, k=20, m=2) + s(var4, site, k=20, m=2, bs=""fs"") +\r\n                                     s(site, grid, bs = ""re""),\r\n                                   method=""REML"", data=vartrait,\r\n                                   sp = model_hgam$sp[-c(1,2)],\r\n                                   min.sp = c(rep(1,times=12),0)) #this ensures the reduced model uses the same smoothing terms as the full model, to ensure comparability\r\n    \r\n    # create the deviance matrix\r\n    deviance_explained_table = data.frame(\r\n      model_name = c(""full"", ""global_var1"", ""excluded_var1""),\r\n      deviance = c(summary(model_hgam)$dev.expl,\r\n                   summary(model_hgam_global_var1)$dev.expl,\r\n                   summary(model_hgam_exclude_var1)$dev.expl))\r\n    \r\n    # rename the deviance matrix\r\n    a = paste0(filepath, ""/"", respvars[ii],""_deviance_"",expvars[iii],"".csv"")\r\n    \r\n    # save the deviance matrix\r\n    write.csv(deviance_explained_table, a, dec = ""."", row.names = F)\r\n    \r\n    # rename model_hgam_global_var1\r\n    assign(paste0(respvars[ii],""_model_global_"",expvars[iii]), model_hgam_global_var1)\r\n    \r\n    # rename model_hgam_exclude_var1\r\n    assign(paste0(respvars[ii],""_model_exclude_"",expvars[iii]), model_hgam_exclude_var1)\r\n    \r\n    ### model without expvars iii ###################################################################################\r\n    ### model statistics ###################################################################################\r\n    \r\n    # choose the predictor iii\r\n    predictor <- expvars[iii]    \r\n    \r\n    # specify to columns of predictor iii\r\n    smoother = paste(c(""s(""), as.character(predictor), c("")""),sep="""")\r\n    smootherSite = paste(c(""s(""), as.character(predictor), c("",site)""),sep="""")\r\n    \r\n    # calculate the global mean of predictor iii\r\n    pred[complete.cases(pred %>% select(mois,temp,pH,radi,respvars[ii])),""globmean""] <- terms$fit[,smoother] # mean of the global effect\r\n    \r\n    # calculate the mean of predictor iii\r\n    pred[complete.cases(pred %>% select(mois,temp,pH,radi,respvars[ii])),""mean""] <- rowSums(terms$fit[,c(smoother,smootherSite)]) # mean of the global and site specific effect\r\n    \r\n    # calculate the se of predictor iii\r\n    pred[complete.cases(pred %>% select(mois,temp,pH,radi,respvars[ii])),""se""] <- sqrt(terms$se.fit[,smoother]^2 + terms$se.fit[,smootherSite]^2) # SE of the global and site specific effect\r\n    \r\n    # rename new columns\r\n    names(pred)[names(pred) == ""globmean""] <- c(paste0(respvars[ii], ""_"", expvars[iii], ""_globmean""))\r\n    names(pred)[names(pred) == ""mean""] <- c(paste0(respvars[ii], ""_"", expvars[iii], ""_mean""))\r\n    names(pred)[names(pred) == ""se""] <- c(paste0(respvars[ii], ""_"", expvars[iii], ""_se""))\r\n    \r\n    ### model statistics ###################################################################################\r\n\r\n  }\r\n  \r\n  ### save model ###################################################################################\r\n  \r\n  # rename model_hgam\r\n  assign(paste0(respvars[ii],""_model""), model_hgam)\r\n  \r\n  # print the summary of response ii\r\n  print(paste0(""summary with response: "", respvars[ii]))\r\n  print(summary(model_hgam))\r\n  \r\n  # save the model\r\n  pdf(paste(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus/"",respvars[ii],""_appraise.pdf""))\r\n  print(appraise(model_hgam))\r\n  dev.off ()\r\n\r\n  pdf(paste(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus/"",respvars[ii],""_plot.pdf""))\r\n  plot(model_hgam,pages=1)\r\n  dev.off ()\r\n  \r\n  sink(paste0(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus/"",respvars[ii],""_summary.txt""))\r\n  print(summary(model_hgam))\r\n  sink()\r\n\r\n  ### save model ###################################################################################\r\n  ### compare R2s ###################################################################################\r\n  \r\n  # build dataframe\r\n  splines <- predict.gam(model_hgam, type = ""terms"") %>% as.data.frame()\r\n\r\n  # add model\'s intercept\r\n  splines$Intercept <- model_hgam$coefficients[1]\r\n\r\n  # add model\'s response\r\n  splines$response <- temptrait$response\r\n\r\n  # add site\r\n  splines$site <- temptrait$site\r\n  \r\n  # create and model with random splines\r\n  splines$predictor1 <- splines$Intercept + splines$`s(site,grid)`\r\n\r\n  # create and model with random and global splines\r\n  splines$predictor2 <- splines$predictor1 + rowSums(splines[c(""s(mois)"",""s(temp)"",""s(pH)"",""s(radi)"")])\r\n\r\n  # create and model with random, global and site splines\r\n  splines$predictor3 <- splines$predictor2 + rowSums(splines[c(""s(mois,site)"",""s(temp,site)"",""s(pH,site)"",""s(radi,site)"")])\r\n\r\n  # correlations between splines and response\r\n  s1 = splines %>% group_by(site) %>% summarize(randomE = cor(predictor1,response)^2,\r\n                                                globalE = cor(predictor2,response)^2,\r\n                                                siteE = cor(predictor3,response)^2) %>% as.data.frame()\r\n  # calculate R2 differences between sites\r\n  s1$siteE <- s1$siteE - s1$globalE\r\n  s1$globalE <- s1$globalE - s1$randomE\r\n  \r\n  # save R2 differences between sites\r\n  sink(paste0(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus/"",respvars[ii],""_effects.txt""))\r\n  print(s1)\r\n  sink()  \r\n  \r\n  # plot R2 comparison between global and site\r\n  s1$site <- relevel(s1$site, ""SVA"")\r\n  s3 = c(""#3f0e5eff"", ""#902567ff"", ""#da513aff"", ""#f9a319ff"")\r\n  \r\n  s2 = s1 %>%\r\n    gather(""Var"",""Val"",-1) %>%\r\n    filter(Var != ""randomE"") %>%\r\n    mutate(Var = factor(Var, levels = c(""globalE"",""siteE""), ordered = TRUE)) %>%\r\n    ggplot(aes(reorder (site, site),Val, fill=site, alpha=Var)) + \r\n    geom_bar(stat = ""identity"", position = ""fill"", width=0.5) +\r\n    scale_fill_manual(values = c(""#3f0e5eff"", ""#902567ff"", ""#da513aff"", ""#f9a319ff"")) +\r\n    scale_alpha_manual(values = c(1, 0.5)) +\r\n    coord_flip() +\r\n    ggtitle(respvars[ii]) +\r\n\r\n    #set theme\r\n    theme(axis.text.y=element_text(colour=""black"", size= 10,hjust=0.5),\r\n          axis.title.y=element_text(""black"", size= 10),\r\n          axis.line.y=element_line(""black"", size= 0.2),\r\n          \r\n          panel.background = element_blank(),\r\n          \r\n          axis.ticks = element_blank(),\r\n          \r\n          axis.text.x=element_text(colour=""black"", size= 10,hjust=0.5),\r\n          axis.title.x=element_text(""black"", size= 10),\r\n          axis.line.x=element_line(""black"", size= 0.2),\r\n          \r\n          aspect.ratio = 1)\r\n  \r\n  # save plot as pdf\r\n  ggsave(file = paste(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus/"", respvars[ii], ""_effects.pdf""), s2, device = cairo_pdf)  ## save plot\r\n  \r\n  ### compare R2s ###################################################################################\r\n  \r\n  }\r\n\r\nwrite.csv(pred, file = ""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus/prediction.csv"", dec = ""."", row.names = F)\r\n\r\nSys.time() #19:23 - 23:14\r\n\r\n##########################################################################################\r\n### response curves ######################################################################\r\n##########################################################################################\r\n\r\npred <- read.csv(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus/prediction.csv"", sep="","")\r\n\r\n\r\n# generate colors per site - use only the middle ones to avoid w/b\r\nmagma100 = colormap(colormap = colormaps$inferno, nshades = 6, format = ""hex"",\r\n                    alpha = 1, reverse = FALSE)\r\n\r\n# preparation for the loop\r\nnmrInd <- 7 # number of responses\r\n\r\nvariables <- c(""heig"", ""SLA"", ""leaA"", ""LDMC"", ""leaN"", ""leaP"", ""seed"",\r\n               ""mois"", ""temp"", ""pH"", ""radi"") # all variables, first the response(s)\r\n\r\nrespvars <- variables[1:nmrInd] # determine the responses\r\nexpvars <- variables[!(variables %in% respvars)] # determine the predictors\r\n\r\nlen_respvars <- 1:length(respvars) # the number of responses\r\nlen_expvars <- 1:length(expvars) # the number of predictors\r\n\r\nplot_pred <- pred # create new data set\r\n\r\n# loop through responses and their predictors to plot response curves\r\ndev.off()\r\nfor(ii in len_respvars){\r\n  \r\n  #ii <- 1\r\n  \r\n  # create list where to collect plots per response and predictor\r\n  plotlist = list()\r\n  \r\n  # create new dataset\r\n  temptrait <- plot_pred\r\n\r\n  # choose columns with predictions of response ii and keep site column, loose irrelevant columns\r\n  temptrait2 = select(temptrait, site, expvars, starts_with(respvars[ii]), -ends_with(""var""))\r\n\r\n  for(i in len_expvars){\r\n    \r\n    #i = 2\r\n    \r\n    # create new dataset\r\n    temptrait3 = temptrait2 \r\n    #names (temptrait3)\r\n    \r\n    #create new dataset with predictor i and keep site column\r\n    temptrait4 = select(temptrait3, site, expvars[i], contains(expvars[i]))\r\n    #names (temptrait4)\r\n    \r\n    #change column names\r\n    colnames(temptrait4)<- c(""site"", ""predictor"", ""pred1"", ""pred2"", ""pred3"")\r\n    temptrait4 <- temptrait4 %>% filter(complete.cases(.))\r\n    \r\n    # plot response curves\r\n    rc = temptrait4 %>%\r\n      ggplot(aes(x = predictor)) +\r\n      coord_cartesian(ylim=c(-0.5, 0.5)) + # adjust the scale if needed\r\n      geom_ribbon(aes(ymax = pred2 + 1.97*pred3,\r\n                      ymin = pred2 - 1.97*pred3,\r\n                      fill = site),\r\n                      alpha = 0.4) +\r\n      geom_line(aes(y = pred2, color = site), linetype = 2, size=0.5) +\r\n      geom_line(aes(y = pred1), linetype = 1, size=0.5) +\r\n      scale_fill_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                                   ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n      scale_color_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                                    ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n      ggtitle(respvars[ii], expvars[i]) +\r\n      \r\n      #set theme\r\n      theme(axis.text.y=element_text(colour=""black"", size= 10,hjust=0.5),\r\n            axis.title.y=element_text(""black"", size= 10),\r\n            axis.line.y=element_line(""black"", size= 0.2),\r\n            \r\n            panel.background = element_blank(),\r\n\r\n            axis.ticks = element_blank(),\r\n            \r\n            axis.text.x=element_text(colour=""black"", size= 10,hjust=0.5),\r\n            axis.title.x=element_text(""black"", size= 10),\r\n            axis.line.x=element_line(""black"", size= 0.2),\r\n            \r\n            aspect.ratio = 1)\r\n    \r\n    plotlist[[i]] = rc\r\n  }\r\n  \r\n  # save as .pdf the four plot per response\r\n  ggsave(file = paste(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus/response_curves/scale0_5/"", respvars[ii], ""_curves.pdf""),\r\n         grid.arrange(grobs = plotlist, ncol = 2, nrow = 2, width=11, height=8.5),\r\n         device = cairo_pdf)  ## save plot\r\n\r\n  }\r\n\r\n\r\n#\r\ndev.off()', '################################################################################################################\r\n################################################################################################################\r\n#### Calculate response curves with HGAM using coarse-scale dataset ############################################\r\n################################################################################################################\r\n################################################################################################################\r\n\r\nlibrary(tidyverse) # for everything\r\nlibrary(colormap) #generating color codes\r\nlibrary(mgcv) # model\r\nlibrary(gridExtra) #for printing plots on one page\r\nlibrary(gratia) # extended plotting and analysis of mgcv models, uses ggplot\r\nlibrary(itsadug) #plot_smooth function for gam\r\nlibrary(lattice) # print pdf\r\nlibrary(reshape)\r\nlibrary(stringr)\r\n\r\n# read data\r\ntrait <- read.csv(""F:/ARTIKKELIT/ARTIKKELI4/r/data/trait_genus_coarse.csv"", sep="","")\r\n\r\n# output path\r\nfilepath = file.path(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus_coarse/"")\r\n\r\n# create column with site\r\ntrait$site = as.factor(substr(trait$plot,1,3))\r\n\r\n# create column with grid\r\ntrait$grid <- as.factor(unlist(lapply(trait$plot, function(x) str_extract_all(x, ""[0-9]+"")[[1]][1])))\r\n\r\n# keep only sites with full set of variables\r\n# trait <- trait[complete.cases(trait),]\r\n\r\n############################################################################################\r\n### model and predict ######################################################################\r\n############################################################################################\r\n\r\n# preparation for the loop\r\nnmrInd <- 7 # number of responses\r\n\r\nvariables <- c(""heig"", ""SLA"", ""leaA"", ""LDMC"", ""leaN"", ""leaP"", ""seed"",\r\n  ""mois"", ""temp"", ""pH"", ""radi"") # all variables, first the response(s)\r\n\r\nrespvars <- variables[1:nmrInd] # determine the responses\r\nexpvars <- variables[!(variables %in% respvars)] # determine the predictors\r\n\r\nlen_respvars <- 1:length(respvars) # the number of responses\r\nlen_expvars <- 1:length(expvars) # the number of predictors\r\n\r\npred <- trait # create new data set\r\n\r\npred$heig = log(pred$heig) # log-transform what needs it\r\npred$SLA  = log(pred$SLA)\r\npred$leaA = log(pred$leaA)\r\npred$LDMC = (pred$LDMC)\r\npred$leaN = (pred$leaN)\r\npred$leaF = (pred$leaF)\r\npred$seed = log(pred$seed)\r\n\r\n# loop through all responses to model and predict\r\nfor(ii in len_respvars){\r\n  \r\n  #ii <- 1\r\n  \r\n  # print which model you\'re working on\r\n  print(paste0(""modelling "", ii, "". trait: "", respvars[ii]))\r\n  print(Sys.time())\r\n  \r\n  ### model and predict ###################################################################################\r\n  \r\n  # rename the response ii in new data set\r\n  temptrait <- pred\r\n  colnames(temptrait)[colnames(temptrait)==paste0(respvars[ii])] <- ""response""\r\n  \r\n  temptrait %>% filter(complete.cases(temptrait %>% \r\n                                        select(plot,mois,temp,pH,\r\n                                               radi,response))) -> temptrait\r\n  \r\n  # cor(temptrait$temp, temptrait$response, use = ""pairwise.complete.obs"")\r\n  # hist(temptrait$temp)\r\n  # use the response ii in the full model\r\n  model_hgam = gam (response ~\r\n                      s(mois, k=10, m=2) +\r\n                      s(temp, k=10, m=2) +\r\n                      s(pH  , k=10, m=2) +\r\n                      s(radi, k=10, m=2) +\r\n                      s(site, grid, bs = ""re""),\r\n                    method=""REML"", data=temptrait,\r\n                    min.sp = c(rep(1,times=4),0))\r\n  \r\n  # predict with the model\r\n  terms <- predict.gam(model_hgam, type = ""terms"", se.fit = TRUE)\r\n  \r\n  ### model and predict ###################################################################################\r\n  \r\n  # # loop through predictors\r\n  # for(iii in len_expvars){\r\n  #   \r\n  #   #iii <- 1\r\n  #   \r\n  #   # print which model you\'re working on\r\n  #   print(paste0(""modelling "", ii, "". trait: "", respvars[ii], "" and its "", iii, "". predictor: "", expvars[iii]))\r\n  #   \r\n  #   ### model without expvars iii ###################################################################################\r\n  #   \r\n  #   # rename the predictor iii in the new data set\r\n  #   vartrait <- temptrait\r\n  #   colnames(vartrait)[colnames(vartrait)==paste0(expvars[iii])] <- ""var1""\r\n  #   \r\n  #   # rename the rest of the responses\r\n  #   names(vartrait)[which(names(vartrait) %in% expvars[-iii])] <- paste0(""var"",2:4)\r\n  #   \r\n  #   # use the response ii in the model excluding s(expvars[iii], site)\r\n  #   model_hgam_global_var1 = gam (response ~\r\n  #                                   s(var1, k=20, m=2) +\r\n  #                                   s(var2, k=20, m=2) + s(var2, site, k=20, m=2, bs=""fs"") +\r\n  #                                   s(var3, k=20, m=2) + s(var3, site, k=20, m=2, bs=""fs"") +\r\n  #                                   s(var4, k=20, m=2) + s(var4, site, k=20, m=2, bs=""fs"") +\r\n  #                                   s(site, grid, bs = ""re""),\r\n  #                                 method=""REML"", data=vartrait,\r\n  #                                 sp = model_hgam$sp[-2],\r\n  #                                 min.sp = c(rep(1,times=13),0)) #this ensures the reduced model uses the same smoothing terms as the full model, to ensure comparability\r\n  # \r\n  #   # use the response ii in the model excluding s(expvars[iii]) & s(expvars[iii], site)\r\n  #   model_hgam_exclude_var1 = gam (response ~\r\n  #                                    s(var2, k=20, m=2) + s(var2, site, k=20, m=2, bs=""fs"") +\r\n  #                                    s(var3, k=20, m=2) + s(var3, site, k=20, m=2, bs=""fs"") +\r\n  #                                    s(var4, k=20, m=2) + s(var4, site, k=20, m=2, bs=""fs"") +\r\n  #                                    s(site, grid, bs = ""re""),\r\n  #                                  method=""REML"", data=vartrait,\r\n  #                                  sp = model_hgam$sp[-c(1,2)],\r\n  #                                  min.sp = c(rep(1,times=12),0)) #this ensures the reduced model uses the same smoothing terms as the full model, to ensure comparability\r\n  #   \r\n  #   # create the deviance matrix\r\n  #   deviance_explained_table = data.frame(\r\n  #     model_name = c(""full"", ""global_var1"", ""excluded_var1""),\r\n  #     deviance = c(summary(model_hgam)$dev.expl,\r\n  #                  summary(model_hgam_global_var1)$dev.expl,\r\n  #                  summary(model_hgam_exclude_var1)$dev.expl))\r\n  #   \r\n  #   # rename the deviance matrix\r\n  #   a = paste0(filepath, ""/"", respvars[ii],""_deviance_"",expvars[iii],"".csv"")\r\n  #   \r\n  #   # save the deviance matrix\r\n  #   write.csv(deviance_explained_table, a, dec = ""."", row.names = F)\r\n  #   \r\n  #   # rename model_hgam_global_var1\r\n  #   assign(paste0(respvars[ii],""_model_global_"",expvars[iii]), model_hgam_global_var1)\r\n  #   \r\n  #   # rename model_hgam_exclude_var1\r\n  #   assign(paste0(respvars[ii],""_model_exclude_"",expvars[iii]), model_hgam_exclude_var1)\r\n  #   \r\n  #   ### model without expvars iii ###################################################################################\r\n  #   ### model statistics ###################################################################################\r\n  #   \r\n  #   # choose the predictor iii\r\n  #   predictor <- expvars[iii]    \r\n  #   \r\n  #   # specify to columns of predictor iii\r\n  #   smoother = paste(c(""s(""), as.character(predictor), c("")""),sep="""")\r\n  #   smootherSite = paste(c(""s(""), as.character(predictor), c("",site)""),sep="""")\r\n  #   \r\n  #   # calculate the global mean of predictor iii\r\n  #   pred[complete.cases(pred %>% select(mois,temp,pH,radi,respvars[ii])),""globmean""] <- terms$fit[,smoother] # mean of the global effect\r\n  #   \r\n  #   # calculate the mean of predictor iii\r\n  #   pred[complete.cases(pred %>% select(mois,temp,pH,radi,respvars[ii])),""mean""] <- rowSums(terms$fit[,c(smoother,smootherSite)]) # mean of the global and site specific effect\r\n  #   \r\n  #   # calculate the se of predictor iii\r\n  #   pred[complete.cases(pred %>% select(mois,temp,pH,radi,respvars[ii])),""se""] <- sqrt(terms$se.fit[,smoother]^2 + terms$se.fit[,smootherSite]^2) # SE of the global and site specific effect\r\n  #   \r\n  #   # rename new columns\r\n  #   names(pred)[names(pred) == ""globmean""] <- c(paste0(respvars[ii], ""_"", expvars[iii], ""_globmean""))\r\n  #   names(pred)[names(pred) == ""mean""] <- c(paste0(respvars[ii], ""_"", expvars[iii], ""_mean""))\r\n  #   names(pred)[names(pred) == ""se""] <- c(paste0(respvars[ii], ""_"", expvars[iii], ""_se""))\r\n  #   \r\n  #   ### model statistics ###################################################################################\r\n  # \r\n  # }\r\n  \r\n  ### save model ###################################################################################\r\n  \r\n  # rename model_hgam\r\n  assign(paste0(respvars[ii],""_model""), model_hgam)\r\n  \r\n  # print the summary of response ii\r\n  print(paste0(""summary with response: "", respvars[ii]))\r\n  print(summary(model_hgam))\r\n  \r\n  # save the model\r\n  pdf(paste(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus_coarse/"",respvars[ii],""_appraise.pdf""))\r\n  print(appraise(model_hgam))\r\n  dev.off ()\r\n\r\n  pdf(paste(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus_coarse/"",respvars[ii],""_plot.pdf""))\r\n  plot(model_hgam,pages=1)\r\n  dev.off ()\r\n  \r\n  sink(paste0(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus_coarse/"",respvars[ii],""_summary.txt""))\r\n  print(summary(model_hgam))\r\n  sink()\r\n\r\n  ### save model ###################################################################################\r\n  ### compare R2s ###################################################################################\r\n\r\n  # build dataframe\r\n  splines <- predict.gam(model_hgam, type = ""terms"") %>% as.data.frame()\r\n\r\n  # add model\'s intercept\r\n  splines$Intercept <- model_hgam$coefficients[1]\r\n\r\n  # add model\'s response\r\n  splines$response <- temptrait$response\r\n\r\n  # add site\r\n  splines$site <- temptrait$site\r\n  # \r\n  # create and model with random splines\r\n  splines$predictor1 <- splines$Intercept + splines$`s(site,grid)`\r\n\r\n  # create and model with random and global splines\r\n  splines$predictor2 <- splines$predictor1 + rowSums(splines[c(""s(mois)"",""s(temp)"",""s(pH)"",""s(radi)"")])\r\n  # \r\n  # # create and model with random, global and site splines\r\n  # splines$predictor3 <- splines$predictor2 + rowSums(splines[c(""s(mois,site)"",""s(temp,site)"",""s(pH,site)"",""s(radi,site)"")])\r\n\r\n  # correlations between splines and response\r\n  s1 = splines %>% group_by(site) %>% summarize(randomE = cor(predictor1,response)^2,\r\n                                                globalE = cor(predictor2,response)^2#,\r\n                                                # siteE = cor(predictor3,response)^2\r\n                                                ) %>% as.data.frame()\r\n  # calculate R2 differences between sites\r\n  # s1$siteE <- s1$siteE - s1$globalE\r\n  s1$globalE <- s1$globalE - s1$randomE\r\n\r\n  # save R2 differences between sites\r\n  sink(paste0(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus_coarse/"",respvars[ii],""_effects.txt""))\r\n  print(s1)\r\n  sink()\r\n  # \r\n  # # plot R2 comparison between global and site\r\n  # s1$site <- relevel(s1$site, ""SVA"")\r\n  # s3 = c(""#3f0e5eff"", ""#902567ff"", ""#da513aff"", ""#f9a319ff"")\r\n  # \r\n  # s2 = s1 %>%\r\n  #   gather(""Var"",""Val"",-1) %>%\r\n  #   filter(Var != ""randomE"") %>%\r\n  #   mutate(Var = factor(Var, levels = c(""globalE"",""siteE""), ordered = TRUE)) %>%\r\n  #   ggplot(aes(reorder (site, site),Val, fill=site, alpha=Var)) + \r\n  #   geom_bar(stat = ""identity"", position = ""fill"", width=0.5) +\r\n  #   scale_fill_manual(values = c(""#3f0e5eff"", ""#902567ff"", ""#da513aff"", ""#f9a319ff"")) +\r\n  #   scale_alpha_manual(values = c(1, 0.5)) +\r\n  #   coord_flip() +\r\n  #   ggtitle(respvars[ii]) +\r\n  # \r\n  #   #set theme\r\n  #   theme(axis.text.y=element_text(colour=""black"", size= 10,hjust=0.5),\r\n  #         axis.title.y=element_text(""black"", size= 10),\r\n  #         axis.line.y=element_line(""black"", size= 0.2),\r\n  #         \r\n  #         panel.background = element_blank(),\r\n  #         \r\n  #         axis.ticks = element_blank(),\r\n  #         \r\n  #         axis.text.x=element_text(colour=""black"", size= 10,hjust=0.5),\r\n  #         axis.title.x=element_text(""black"", size= 10),\r\n  #         axis.line.x=element_line(""black"", size= 0.2),\r\n  #         \r\n  #         aspect.ratio = 1)\r\n  # \r\n  # # save plot as pdf\r\n  # ggsave(file = paste(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus_coarse/"", respvars[ii], ""_effects.pdf""), s2, device = cairo_pdf)  ## save plot\r\n  # \r\n  # ### compare R2s ###################################################################################\r\n  \r\n  }\r\n\r\n# write.csv(pred, file = ""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus_coarse/prediction.csv"", dec = ""."", row.names = F)\r\n# \r\n# Sys.time() #19:23 - 23:14', 'library(tidyverse) # for everything\r\n\r\n#set directory\r\nsetwd(""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus"")\r\ndirectory = getwd()\r\n\r\n#set factors\r\nresponse = c (""heig"", ""SLA"", ""seed"", ""LDMC"", ""leaA"", ""leaN"", ""leaP"")\r\npredictors = c (""mois"", ""temp"", ""pH"", ""radi"")\r\n\r\n#create dataframe\r\nvarimp = data.frame()\r\n\r\n#specify files\r\nfiles = dir(pattern = ""deviance"", full.names = TRUE, ignore.case = TRUE)\r\n\r\nfor (i in files){\r\n  \r\n  \r\n  #specify file\r\n  file  = read.csv(paste0 (directory,i))\r\n  \r\n  df_temp <- data.frame(trait = substr(strsplit(i,""_"")[[1]][1], 3, 1000),\r\n                        var = gsub("".csv"", """", strsplit(i,""_"")[[1]][3]),\r\n                        file %>% filter(model_name == ""excluded_var1"") %>% \r\n                          select(deviance))\r\n  \r\n  varimp <- bind_rows(varimp, df_temp)\r\n}\r\n\r\nvarimp %>% pivot_wider(id_cols = trait,\r\n                       names_from = var,\r\n                       values_from = deviance) %>% \r\n  select(1,predictors) %>% \r\n  mutate(trait = factor(trait, levels = response)) %>% \r\n  arrange(trait) -> varimp\r\n\r\nwrite.csv(varimp, file = ""F:/ARTIKKELIT/ARTIKKELI4/r/results/hgam_m2_m2_genus/varimp.csv"", dec = ""."", row.names = F)\r\n', '################################################################################################################\r\n################################################################################################################\r\n#### PCA analysis ##############################################################################################\r\n################################################################################################################\r\n################################################################################################################\r\n\r\nlibrary(FactoMineR) #for pca\r\nlibrary(factoextra)\r\nlibrary(ggplot2)\r\nlibrary(tidyr)\r\nlibrary(dplyr)\r\nlibrary(MASS)\r\nlibrary(reshape2)\r\nlibrary(cowplot)\r\nlibrary(gridExtra)\r\n\r\n# start with a clean slate\r\nrm(list=ls(all=TRUE)) \r\n\r\n# read data\r\ntrait <- read.csv(""F:/ARTIKKELIT/ARTIKKELI4/r/data/trait_genus.csv"", sep="","")\r\n\r\n# create column with site\r\ntrait$site = as.factor(substr(trait$plot,1,3))\r\n\r\n# keep only sites with full set of variables\r\ntrait <- trait[complete.cases(trait),]\r\n\r\n# log-transform what needs it\r\ntrait$heig = log(trait$heig) \r\ntrait$SLA  = log(trait$SLA)\r\ntrait$leaA = log(trait$leaA)\r\ntrait$LDMC = (trait$LDMC)\r\ntrait$leaN = (trait$leaN)\r\ntrait$leaF = (trait$leaF)\r\ntrait$seed = log(trait$seed)\r\n\r\n# PCA for abiotics\r\nabi.pca = PCA(trait[,2:5])\r\nabi.pca$eig # matrix with eigenvalues\r\nabi.pca$var$coord # correlations between variables and PCs\r\nhead(abi.pca$ind$coord) # PCs (aka scores)\r\n\r\n# PCA for trait\r\ntra.pca = PCA(trait[,6:12])\r\ntra.pca$eig # matrix with eigenvalues\r\ntra.pca$var$coord # correlations between variables and PCs\r\nhead(tra.pca$ind$coord) # PCs (aka scores)\r\n\r\n# extract pc scores for first two component and add to  dataframe\r\ntrait$abi.pca1 <- abi.pca$ind$coord[, 1] # abiotics: indexing the first column\r\ntrait$abi.pca2 <- abi.pca$ind$coord[, 2]  # abiotics: indexing the second column\r\ntrait$tra.pca1 <- tra.pca$ind$coord[, 1] # abiotics: indexing the first column\r\ntrait$tra.pca2 <- tra.pca$ind$coord[, 2]  # abiotics: indexing the second column\r\n\r\n# plot pca abiotics vs pcs trait\r\nplot (trait$abi.pca1, trait$tra.pca2)\r\nabline (0,1, col = ""red"")\r\nabline(lm(tra.pca2 ~ abi.pca1, data=trait), col=""blue"")\r\n\r\nplot (trait$abi.pca2, trait$tra.pca1)\r\nabline (0,1, col = ""red"")\r\nabline(lm(tra.pca1 ~ abi.pca2, data=trait), col=""blue"")\r\n\r\n# extract variable contributions\r\nabi.pca.vars <- abi.pca$var$coord %>% data.frame\r\nabi.pca.vars$vars <- rownames(abi.pca.vars)\r\nabi.pca.vars.m <- melt(abi.pca.vars, id.vars = ""vars"")\r\n\r\ntra.pca.vars <- tra.pca$var$coord %>% data.frame\r\ntra.pca.vars$vars <- rownames(tra.pca.vars)\r\ntra.pca.vars.m <- melt(tra.pca.vars, id.vars = ""vars"")\r\n\r\n# a circle for variable contribution plot (radius of 1)\r\ncircleFun <- function(center = c(0,0),diameter = 1, npoints = 100){\r\n  r = diameter / 2\r\n  tt <- seq(0,2*pi,length.out = npoints)\r\n  xx <- center[1] + r * cos(tt)\r\n  yy <- center[2] + r * sin(tt)\r\n  return(data.frame(x = xx, y = yy))\r\n}\r\ncirc <- circleFun(c(0,0),2,npoints = 500)\r\n\r\n# basic pca plot\r\nggplot(data = trait, aes(x = abi.pca1, y = abi.pca2)) +\r\n  geom_point()\r\nggplot(data = trait, aes(x = tra.pca1, y = tra.pca2)) +\r\n  geom_point()\r\n\r\n# custom pca plot for abiotics\r\na = ggplot(data = trait, aes(x = abi.pca1, y = abi.pca2, color = site)) +\r\n  scale_color_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                                ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n  scale_fill_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                               ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n  geom_hline(yintercept = 0, lty = 2) +\r\n  geom_vline(xintercept = 0, lty = 2) +\r\n  geom_point(alpha = 0.2, size=2) +\r\n  stat_ellipse(geom=""polygon"", aes(fill = site), alpha = 0.1, show.legend = FALSE, level = 0.95) +\r\n  xlab(""PC 1 (58.77%)"") + \r\n  ylab(""PC 2 (20.87%)"") +\r\n  \r\n  #set theme\r\n  theme(axis.text.y=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.y=element_text(""black"", size= 10),\r\n        axis.line.y=element_line(""black"", size= 0.2),\r\n    \r\n        panel.background = element_blank(),\r\n        \r\n        axis.ticks = element_blank(),\r\n        \r\n        axis.text.x=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.x=element_text(""black"", size= 10),\r\n        axis.line.x=element_line(""black"", size= 0.2),\r\n        \r\n        aspect.ratio = 1,\r\n        \r\n        legend.position = ""none"")\r\n\r\n# custom pca plot for traits\r\nb = ggplot(data = trait, aes(x = tra.pca1, y = tra.pca2, color = site)) +\r\n  scale_color_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                                ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n  scale_fill_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                               ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n  geom_hline(yintercept = 0, lty = 2) +\r\n  geom_vline(xintercept = 0, lty = 2) +\r\n  geom_point(alpha = 0.2, size=2) +\r\n  stat_ellipse(geom=""polygon"", aes(fill = site), alpha = 0.1, show.legend = FALSE, level = 0.95) +\r\n  xlab(""PC 1 (53.33%)"") + \r\n  ylab(""PC 2 (21.04%)"") +\r\n  \r\n  #set theme\r\n  theme(axis.text.y=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.y=element_text(""black"", size= 10),\r\n        axis.line.y=element_line(""black"", size= 0.2),\r\n        \r\n        panel.background = element_blank(),\r\n        \r\n        axis.ticks = element_blank(),\r\n        \r\n        axis.text.x=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.x=element_text(""black"", size= 10),\r\n        axis.line.x=element_line(""black"", size= 0.2),\r\n        \r\n        aspect.ratio = 1,\r\n        \r\n        legend.position = ""none"")\r\n\r\n# custom variable contributions plot for abiotics\r\nc = ggplot() +\r\n  geom_path(data = circ,aes(x,y), lty = 2) +\r\n  geom_hline(yintercept = 0, lty = 2) +\r\n  geom_vline(xintercept = 0, lty = 2) +\r\n  geom_segment(data = abi.pca.vars, aes(x = 0, xend = Dim.1, y = 0, yend = Dim.2),\r\n               arrow = arrow(length = unit(0.025, ""npc""), type = ""open""),\r\n               lwd = 1) + \r\n  \r\n  geom_text(data = abi.pca.vars, \r\n            aes(x = Dim.1*1.15, y =  Dim.2*1.15, \r\n                label = c(""mois"",\r\n                          ""temp"",\r\n                          ""pH"",\r\n                          ""radi"")), \r\n            check_overlap = F, size = 3) +\r\n  \r\n  xlab(""PC 1"") + \r\n  ylab(""PC2"") +\r\n  coord_equal() +\r\n  \r\n  #set theme\r\n  theme(axis.text.y=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.y=element_text(""black"", size= 10),\r\n        axis.line.y=element_line(""black"", size= 0.2),\r\n        \r\n        panel.background = element_blank(),\r\n        \r\n        axis.ticks = element_blank(),\r\n        \r\n        axis.text.x=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.x=element_text(""black"", size= 10),\r\n        axis.line.x=element_line(""black"", size= 0.2),\r\n        \r\n        aspect.ratio = 1)\r\n\r\n# custom variable contributions plot for traits\r\nd = ggplot() +\r\n  geom_path(data = circ,aes(x,y), lty = 2) +\r\n  geom_hline(yintercept = 0, lty = 2) +\r\n  geom_vline(xintercept = 0, lty = 2) +\r\n  geom_segment(data = tra.pca.vars, aes(x = 0, xend = Dim.1, y = 0, yend = Dim.2),\r\n               arrow = arrow(length = unit(0.025, ""npc""), type = ""open""),\r\n               lwd = 1) + \r\n  \r\n  geom_text(data = tra.pca.vars, \r\n            aes(x = Dim.1*1.15, y =  Dim.2*1.15, \r\n                label = c(""heig"",\r\n                          ""SLA"",\r\n                          ""leaA"",\r\n                          ""LDMC"",\r\n                          ""leaN"",\r\n                          ""leaP"",\r\n                          ""seed"")), \r\n            check_overlap = F, size = 3) +\r\n  \r\n  xlab(""PC 1"") + \r\n  ylab(""PC2"") +\r\n  coord_equal() +\r\n  \r\n  #set theme\r\n  theme(axis.text.y=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.y=element_text(""black"", size= 10),\r\n        axis.line.y=element_line(""black"", size= 0.2),\r\n        \r\n        panel.background = element_blank(),\r\n        \r\n        axis.ticks = element_blank(),\r\n        \r\n        axis.text.x=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.x=element_text(""black"", size= 10),\r\n        axis.line.x=element_line(""black"", size= 0.2),\r\n        \r\n        aspect.ratio = 1)\r\n\r\n# save as .pdf the four plot per response\r\ndev.off()\r\nggsave(file = paste(""F:/ARTIKKELIT/ARTIKKELI4/r/results/plot_pca/pca_log.pdf""),\r\n       grid.arrange(c, d, a, b, nrow = 2),\r\n       device = cairo_pdf)  ## save plot\r\n\r\n# custom plot for abiotics pca1 and trait pca1\r\ne = ggplot(data = trait, aes(x = abi.pca1, y = tra.pca1, color = site)) +\r\n  scale_color_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                                ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n  scale_fill_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                                ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n  #geom_hline(yintercept = 0, lty = 2) +\r\n  #geom_vline(xintercept = 0, lty = 2) +\r\n  geom_point(alpha = 0.2, size=0.5, shape=1) +\r\n  stat_smooth(aes(fill= site), method = ""lm"", size=0.3, alpha=0.2, linetype = 2) +\r\n  stat_smooth(aes(group = 1), method = ""lm"", fill=NA, colour=""black"", size=0.3) +\r\n  xlab(""Environmental drivers PC 1 (58.77%)"") + \r\n  ylab(""Plant functional traits PC 1 (53.33%)"") +\r\n  \r\n  #set theme\r\n  theme(axis.text.y=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.y=element_text(""black"", size= 10),\r\n        axis.line.y=element_line(""black"", size= 0.2),\r\n        \r\n        panel.background = element_blank(),\r\n        \r\n        axis.ticks = element_blank(),\r\n        \r\n        axis.text.x=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.x=element_text(""black"", size= 10),\r\n        axis.line.x=element_line(""black"", size= 0.2),\r\n        \r\n        aspect.ratio = 1,\r\n        \r\n        legend.position = ""none"")\r\n\r\n# custom plot for abiotics pca1 and trait pca2\r\nf = ggplot(data = trait, aes(x = abi.pca1, y = tra.pca2, color = site)) +\r\n  scale_color_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                                ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n  scale_fill_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                               ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n  #geom_hline(yintercept = 0, lty = 2) +\r\n  #geom_vline(xintercept = 0, lty = 2) +\r\n  geom_point(alpha = 0.2, size=0.5, shape=1) +\r\n  stat_smooth(aes(fill= site), method = ""lm"", size=0.3, alpha=0.2, linetype = 2) +\r\n  stat_smooth(aes(group = 1), method = ""lm"", fill=NA, colour=""black"", size=0.3) +\r\n  xlab(""Environmental drivers PC 1 (58.77%)"") + \r\n  ylab(""Plant functional traits PC 2 (21.04%)"") +\r\n  \r\n  #set theme\r\n  theme(axis.text.y=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.y=element_text(""black"", size= 10),\r\n        axis.line.y=element_line(""black"", size= 0.2),\r\n        \r\n        panel.background = element_blank(),\r\n        \r\n        axis.ticks = element_blank(),\r\n        \r\n        axis.text.x=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.x=element_text(""black"", size= 10),\r\n        axis.line.x=element_line(""black"", size= 0.2),\r\n        \r\n        aspect.ratio = 1,\r\n        \r\n        legend.position = ""none"")\r\n\r\n# custom plot for abiotics pca2 and trait pca1\r\ng = ggplot(data = trait, aes(x = abi.pca2, y = tra.pca1, color = site)) +\r\n  scale_color_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                                ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n  scale_fill_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                               ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n  #geom_hline(yintercept = 0, lty = 2) +\r\n  #geom_vline(xintercept = 0, lty = 2) +\r\n  geom_point(alpha = 0.2, size=0.5, shape=1) +\r\n  stat_smooth(aes(fill= site), method = ""lm"", size=0.3, alpha=0.2, linetype = 2) +\r\n  stat_smooth(aes(group = 1), method = ""lm"", fill=NA, colour=""black"", size=0.3) +\r\n  xlab(""Environmental drivers PC 2 (20.87%)"") + \r\n  ylab(""Plant functional traits PC 1 (53.33%)"") +\r\n  \r\n  #set theme\r\n  theme(axis.text.y=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.y=element_text(""black"", size= 10),\r\n        axis.line.y=element_line(""black"", size= 0.2),\r\n        \r\n        panel.background = element_blank(),\r\n        \r\n        axis.ticks = element_blank(),\r\n        \r\n        axis.text.x=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.x=element_text(""black"", size= 10),\r\n        axis.line.x=element_line(""black"", size= 0.2),\r\n        \r\n        aspect.ratio = 1,\r\n        \r\n        legend.position = ""none"")\r\n\r\n# custom plot for abiotics pca2 and trait pca2\r\nh = ggplot(data = trait, aes(x = abi.pca2, y = tra.pca2, color = site)) +\r\n  scale_color_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                                ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n  scale_fill_manual(values = c(""SVA""=""#3f0e5eff"", ""KAN""=""#902567ff"",\r\n                               ""KIL""=""#da513aff"", ""MAR""= ""#f9a319ff"")) +\r\n  #geom_hline(yintercept = 0, lty = 2) +\r\n  #geom_vline(xintercept = 0, lty = 2) +\r\n  geom_point(alpha = 0.2, size=0.5, shape=1) +\r\n  stat_smooth(aes(fill= site), method = ""lm"", size=0.3, alpha=0.2, linetype = 2) +\r\n  stat_smooth(aes(group = 1), method = ""lm"", fill=NA, colour=""black"", size=0.3) +\r\n  xlab(""Environmental drivers PC 2 (20.87%)"") + \r\n  ylab(""Plant functional traits PC 2 (21.04%)"") +\r\n  \r\n  #set theme\r\n  theme(axis.text.y=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.y=element_text(""black"", size= 10),\r\n        axis.line.y=element_line(""black"", size= 0.2),\r\n        \r\n        panel.background = element_blank(),\r\n        \r\n        axis.ticks = element_blank(),\r\n        \r\n        axis.text.x=element_text(colour=""black"", size= 10,hjust=0.5),\r\n        axis.title.x=element_text(""black"", size= 10),\r\n        axis.line.x=element_line(""black"", size= 0.2),\r\n        \r\n        aspect.ratio = 1,\r\n        \r\n        legend.position = ""none"")\r\n\r\n# save as .pdf the four plot per response\r\ndev.off()\r\nggsave(file = paste(""F:/ARTIKKELIT/ARTIKKELI4/r/results/plot_pca/pca_correlations_log.pdf""),\r\n       grid.arrange(f, h, e, g, nrow = 2),\r\n       device = cairo_pdf)  ## save plot\r\n\r\n# correlation between abiotics and trait\r\ncor(trait$abi.pca1, trait$tra.pca1, method=""spearman"")\r\ncor(trait$abi.pca2, trait$tra.pca2, method=""spearman"")\r\ncor(trait$abi.pca1, trait$tra.pca2, method=""spearman"")\r\ncor(trait$abi.pca2, trait$tra.pca1, method=""spearman"")\r\n\r\n# PC scores into two dataframes\r\nabi.res = as.data.frame(abi.pca$ind$coord)\r\ntra.res = as.data.frame(tra.pca$ind$coord)\r\n\r\n# correlation between abiotics and trait\r\ncor(abi.res[,1], tra.res[,1],method=""spearman"")\r\ncor(abi.res[,2], tra.res[,2],method=""spearman"")\r\ncor(abi.res[,1], tra.res[,2],method=""spearman"")\r\ncor(abi.res[,2], tra.res[,1],method=""spearman"")']","Data from: Consistent trait-environment relationships within and across tundra plant communities A fundamental assumption in trait-based ecology is that relationships between traits and environmental conditions are globally consistent. We use field-quantified microclimate and soil data to explore if trait-environment relationships are generalisable across plant communities and spatial scales. We collected data from 6720 plots and 217 species across four distinct tundra regions from both hemispheres. We combine this data with over 76000 database trait records to relate local plant community trait composition to broad gradients of key environmental drivers: soil moisture, soil temperature, soil pH, and potential solar radiation. Results revealed strong, consistent trait-environment relationships across Arctic and Antarctic regions. This indicates that the detected relationships are transferable between tundra plant communities also when fine-scale environmental heterogeneity is accounted for, and that variation in local conditions heavily influences both structural and leaf economic traits. Our results strengthen the biological and mechanistic basis for climate change impact predictions of vulnerable high-latitude ecosystems.Kemppinen, Niittynen, le Roux, Momberg, Happonen, Aalto, Rautakoski, Enquist, Vandvik, Halbritter, Maitner & Luoto (2021). Consistent trait-environment relationships within and across tundra plant communities. Nature Ecology and EvolutionThese are the data and codes from Kemppinen et al. (2021).",1
Artificial shorelines lack natural structural complexity across scales (dataset),"From microbes to humans, habitat structural complexity plays a direct role in the provision of physical living space and increased complexity supports higher biodiversity and ecosystem functioning across biomes. Natural coastlines are structurally complex transition zones between land and sea that support diverse ecological communities but are under increasing pressure from human activity. Coastal development and the construction of artificial shorelines are changing our landscape and altering biodiversity patterns as humans seek both socio-economic benefits and protection from coastal storms, flooding, and erosion. In this study, we evaluate how much structural complexity is missing, and at which scales, with the creation of artificial structures compared to naturally occurring rocky shores. We quantified the structural complexity of both artificial and natural shores at resolutions from 1 mm through to 10s of m using three remote sensing platforms (handheld camera, terrestrial laser scanner and uncrewed aerial vehicles) across both artificial and natural shorelines. Natural shorelines were approximately 20-50 % more structurally complex and offered greater structural variation between locations. In contrast, artificial shorelines were more structurally homogenous and typically deficient in structural complexity across scales. Our findings reinforce concerns that replacing natural rocky shorelines with artificial structures simplifies coastlines at organism-relevant scales. Furthermore, we offer much-needed insight into how structures might be modified to more closely capture the complexity of natural shorelines that support biodiversity.","['\r\n## DATA WAS COMPILED AS PART THE ECOSTRUCTURE ##\r\n## R SCRIPTS WRITTEN BY PETER LAWRENCE ##\r\n\r\n# PREREQU. #\r\nlibrary(vegan)\r\nlibrary(MASS)\r\nlibrary(Hotelling)\r\n\r\n# DATA PREP. #\r\n# load the following files #\r\nall_scal <- read.csv(file.choose(),header=T,sep="","") # all_clean_mean.csv\r\nall_scal <- all_scal[c(1:4, 7:12, 14:24),] # remove UAV missing lines as NMDS\r\n\r\n# ID data from different inclinations #\r\nNH <- which(all_scal$struc == ""NaturalH"")\r\nNv <- which(all_scal$struc == ""NaturalV"")\r\nWa <- which(all_scal$struc == ""Wall"")\r\nRA <- which(all_scal$struc == ""Riprap"")\r\n\r\n# refine the catagories of hbatiat when cros scale #\r\nall_scal$str2[all_scal$struc==""NaturalH""]<- ""Natural""\r\nall_scal$str2[all_scal$struc==""NaturalV""]<- ""Natural""\r\nall_scal$str2[all_scal$struc==""Wall""]<- ""Wall""\r\nall_scal$str2[all_scal$struc==""Riprap""]<- ""Riprap""\r\n\r\n# maintain random seed #\r\nset.seed(2)\r\n\r\n# Create matrix of each habitats multiscale data #\r\nnatural <- as.matrix(all_scal[all_scal$str2==""Natural"", 3:14])\r\narmr <- as.matrix(all_scal[all_scal$str2==""Riprap"", 3:14])\r\nwall <- as.matrix(all_scal[all_scal$str2==""Wall"", 3:14])\r\n\r\n# Run each permutational combination of these matrix to compare mean #\r\nh1 <- hotelling.test(natural, armr, perm = T, shrinkage = T)\r\nh2 <- hotelling.test(natural, wall, perm = T, shrinkage = T)\r\nh3 <- hotelling.test(armr, wall, perm = T, shrinkage = T)\r\n\r\n# Create dissimialrty measure.. not true community, hetrogenous, mixed scales #\r\nmag_mean_nmds <- metaMDS(all_scal[,3:14], distance = ""gower"", autotransform = F, trymax = 999, symmetric = T, k = 5, scale = F)\r\n\r\n# fit variable if needed later #\r\nmag_mean_fit <- envfit(mag_mean_nmds, all_scal[,3:14], perm = 999, na.rm = T)\r\n\r\npar(mfrow=c(1,1))\r\n\r\n# plot nmds by each set of points in turn #\r\nplot(mag_mean_nmds, type = ""n"", xlim = c(-.5, .5), ylim = c(-.6, .6))\r\npoints(mag_mean_nmds$points[NH, 1], mag_mean_nmds$points[NH, 2], pch = 24, bg = ""darkgreen"", cex = 1.25)\r\npoints(mag_mean_nmds$points[Nv, 1], mag_mean_nmds$points[Nv, 2], pch = 22, bg = f0, cex = 1.25)\r\npoints(mag_mean_nmds$points[Wa, 1], mag_mean_nmds$points[Wa, 2], pch = 22, bg = f2, cex = 1.25)\r\npoints(mag_mean_nmds$points[RA, 1], mag_mean_nmds$points[RA, 2], pch = 24, bg = f1, cex = 1.25)\r\ntreat <- c(""n"", ""w"", ""n"", ""r"", \r\n           ""w"", ""n"", ""r"", ""n"", \r\n           ""n"", ""r"", ""r"", ""r"", \r\n           ""n"", ""w"", ""n"", ""w"", \r\n           ""n"", ""w"", ""n"", ""w"", ""n"")\r\nordihull(mag_mean_nmds, groups=treat, draw=""polygon"", col=""grey90"",label=F)', '\r\n## DATA WAS COMPILED AS PART THE ECOSTRUCTURE ##\r\n## R SCRIPTS WRITTEN BY PETER LAWRENCE ##\r\n\r\n# PREREQU. #\r\nlibrary(vegan)\r\nlibrary(matrixStats)\r\nlibrary(matrixTests)\r\nlibrary(coin)\r\nlibrary(FSA)\r\nlibrary(rcompanion)\r\nlibrary(multcompView)\r\nlibrary(vioplot)\r\n\r\n# DATA PREP. #\r\n# load the following files #\r\nquad_to <- read.csv(file.choose(),header=T,sep="","") # sfm_clean.csv\r\nlas_to <- read.csv(file.choose(),header=T,sep="","") # las_clean.csv\r\nuav_to <- read.csv(file.choose(),header=T,sep="","") # uav_clean.csv\r\n\r\n\r\n# BOOTSTRAP MATRIX CREATION (SfM) #\r\nB <- 1000\r\nsfm.nh <- length(wal$Str2[wal$Str2==""NaturalH""])\r\nsfm.nv <- length(wal$Str2[wal$Str2==""NaturalV""])\r\nsfm.r <- length(wal$Str2[wal$Str2==""Riprap""])\r\nsfm.w <- length(wal$Str2[wal$Str2==""Wall""])\r\n\r\n# BOOTSTRAP MATRIX CREATION (LAS) #\r\nlas.n <- length(las$Structure[las$Structure==""Natural""])\r\nlas.r <- length(las$Structure[las$Structure==""Riprap""])\r\nlas.w <- length(las$Structure[las$Structure==""Wall""])\r\n\r\n# BOOTSTRAP MATRIX CREATION (UAV) #\r\nuav.n <- length(uav$Structure[uav$Structure==""Natural""])\r\nuav.r <- length(uav$Structure[uav$Structure==""Riprap""])\r\nuav.w <- length(uav$Structure[uav$Structure==""Wall""])\r\n\r\n\r\n# BOOTSTRAP SAMPLing MATRIX (SfM) #\r\n# 1mm scale #\r\nrug_min_NatH <- matrix(sample(wal$rug_min[wal$Str2==""NaturalH""], \r\n                              size = sfm.nh*B, replace = T), nrow = sfm.nh, ncol = B)\r\nrug_min_NatV <- matrix(sample(wal$rug_min[wal$Str2==""NaturalV""], \r\n                              size = sfm.nv*B, replace = T), nrow = sfm.nv, ncol = B)\r\nrug_min_Rip <- matrix(sample(wal$rug_min[wal$Str2==""Riprap""], \r\n                             size = sfm.r*B, replace = T), nrow = sfm.r, ncol = B)\r\nrug_min_Wal <- matrix(sample(wal$rug_min[wal$Str2==""Wall""], \r\n                             size = sfm.w*B, replace = T), nrow = sfm.w, ncol = B)\r\n\r\n# 5mm scale #\r\nrug_meso_NatH <- matrix(sample(wal$rug_meso[wal$Str2==""NaturalH""], \r\n                               size = sfm.nh*B, replace = T), nrow = sfm.nh, ncol = B)\r\nrug_meso_NatV <- matrix(sample(wal$rug_meso[wal$Str2==""NaturalV""], \r\n                               size = sfm.nv*B, replace = T), nrow = sfm.nv, ncol = B)\r\nrug_meso_Rip <- matrix(sample(wal$rug_meso[wal$Str2==""Riprap""], \r\n                              size = sfm.r*B, replace = T), nrow = sfm.r, ncol = B)\r\nrug_meso_Wal <- matrix(sample(wal$rug_meso[wal$Str2==""Wall""], \r\n                              size = sfm.w*B, replace = T), nrow = sfm.w, ncol = B)\r\n\r\n# 1cm scale #\r\nrug_max_NatH <- matrix(sample(wal$rug_max[wal$Str2==""NaturalH""], \r\n                              size = sfm.nh*B, replace = T), nrow = sfm.nh, ncol = B)\r\nrug_max_NatV <- matrix(sample(wal$rug_max[wal$Str2==""NaturalV""], \r\n                              size = sfm.nv*B, replace = T), nrow = sfm.nv, ncol = B)\r\nrug_max_Rip <- matrix(sample(wal$rug_max[wal$Str2==""Riprap""], \r\n                             size = sfm.r*B, replace = T), nrow = sfm.r, ncol = B)\r\nrug_max_Wal <- matrix(sample(wal$rug_max[wal$Str2==""Wall""], \r\n                             size = sfm.w*B, replace = T), nrow = sfm.w, ncol = B)\r\n\r\n# BOOTSTRAP SAMPLing MATRIX (LAS) #\r\n# 10cm scale #\r\nl1.Nat <- matrix(sample(las$rug.m1[las$Structure==""Natural""], \r\n                        size = las.n*B, replace = T), nrow = las.n, ncol = B)\r\nl1.Rip <- matrix(sample(las$rug.m1[las$Structure==""Riprap""], \r\n                        size = las.r*B, replace = T), nrow = las.r, ncol = B)\r\nl1.Wal <- matrix(sample(las$rug.m1[las$Structure==""Wall""], \r\n                        size = las.w*B, replace = T), nrow = las.w, ncol = B)\r\n\r\n# 20cm scale #\r\nl2.Nat <- matrix(sample(las$rug.m2[las$Structure==""Natural""], \r\n                        size = las.n*B, replace = T), nrow = las.n, ncol = B)\r\nl2.Rip <- matrix(sample(las$rug.m2[las$Structure==""Riprap""], \r\n                        size = las.r*B, replace = T), nrow = las.r, ncol = B)\r\nl2.Wal <- matrix(sample(las$rug.m2[las$Structure==""Wall""], \r\n                        size = las.w*B, replace = T), nrow = las.w, ncol = B)\r\n\r\n# 30cm scale #\r\nl3.Nat <- matrix(sample(las$rug.m3[las$Structure==""Natural""], \r\n                        size = las.n*B, replace = T), nrow = las.n, ncol = B)\r\nl3.Rip <- matrix(sample(las$rug.m3[las$Structure==""Riprap""], \r\n                        size = las.r*B, replace = T), nrow = las.r, ncol = B)\r\nl3.Wal <- matrix(sample(las$rug.m3[las$Structure==""Wall""], \r\n                        size = las.w*B, replace = T), nrow = las.w, ncol = B)\r\n\r\n# 40cm scale #\r\nl4.Nat <- matrix(sample(las$rug.m4[las$Structure==""Natural""], \r\n                        size = las.n*B, replace = T), nrow = las.n, ncol = B)\r\nl4.Rip <- matrix(sample(las$rug.m4[las$Structure==""Riprap""], \r\n                        size = las.r*B, replace = T), nrow = las.r, ncol = B)\r\nl4.Wal <- matrix(sample(las$rug.m4[las$Structure==""Wall""], \r\n                        size = las.w*B, replace = T), nrow = las.w, ncol = B)\r\n\r\n# 50cm scale #\r\nl5.Nat <- matrix(sample(las$rug.m5[las$Structure==""Natural""], \r\n                        size = las.n*B, replace = T), nrow = las.n, ncol = B)\r\nl5.Rip <- matrix(sample(las$rug.m5[las$Structure==""Riprap""], \r\n                        size = las.r*B, replace = T), nrow = las.r, ncol = B)\r\nl5.Wal <- matrix(sample(las$rug.m5[las$Structure==""Wall""], \r\n                        size = las.w*B, replace = T), nrow = las.w, ncol = B)\r\n\r\n# BOOTSTRAP SAMPLing MATRIX (UAV) #\r\n# 1m scale #\r\nu1.Nat <- matrix(sample(uav$rug1[uav$Structure==""Natural""], \r\n                        size = uav.n*B, replace = T), nrow = uav.n, ncol = B)\r\nu1.Rip <- matrix(sample(uav$rug1[uav$Structure==""Riprap""], \r\n                        size = uav.r*B, replace = T), nrow = uav.r, ncol = B)\r\nu1.Wal <- matrix(sample(uav$rug1[uav$Structure==""Wall""], \r\n                        size = uav.w*B, replace = T), nrow = uav.w, ncol = B)\r\n\r\n# 2m scale\r\nu2.Nat <- matrix(sample(uav$rug2[uav$Structure==""Natural""], \r\n                        size = uav.n*B, replace = T), nrow = uav.n, ncol = B)\r\nu2.Rip <- matrix(sample(uav$rug2[uav$Structure==""Riprap""], \r\n                        size = uav.r*B, replace = T), nrow = uav.r, ncol = B)\r\nu2.Wal <- matrix(sample(uav$rug2[uav$Structure==""Wall""], \r\n                        size = uav.w*B, replace = T), nrow = uav.w, ncol = B)\r\n\r\n# 5m scale #\r\nu3.Nat <- matrix(sample(uav$rug5[uav$Structure==""Natural""], \r\n                        size = uav.n*B, replace = T), nrow = uav.n, ncol = B)\r\nu3.Rip <- matrix(sample(uav$rug5[uav$Structure==""Riprap""], \r\n                        size = uav.r*B, replace = T), nrow = uav.r, ncol = B)\r\nu3.Wal <- matrix(sample(uav$rug5[uav$Structure==""Wall""], \r\n                        size = uav.w*B, replace = T), nrow = uav.w, ncol = B)\r\n\r\n# 10m scale #\r\nu4.Nat <- matrix(sample(uav$rug10[uav$Structure==""Natural""], \r\n                        size = uav.n*B, replace = T), nrow = uav.n, ncol = B)\r\nu4.Rip <- matrix(sample(uav$rug10[uav$Structure==""Riprap""], \r\n                        size = uav.r*B, replace = T), nrow = uav.r, ncol = B)\r\nu4.Wal <- matrix(sample(uav$rug10[uav$Structure==""Wall""], \r\n                        size = uav.w*B, replace = T), nrow = uav.w, ncol = B)\r\n\r\n# MEDIAN AND CI INTERAL CALCULATIIONS FOR PERMUTATIONS (SfM)#\r\n# 1mm scale #\r\nrug_min_diff.NR <- colMeans(rug_min_Rip)- colMeans(rug_min_NatH)\r\nrug_min_NR.CI <- quantile(rug_min_diff.NR, prob = c(0.05, 0.5, 0.95))\r\nrug_min_diff.NW <- colMeans(rug_min_Wal) - colMeans(rug_min_NatV)\r\nrug_min_NW.CI <- quantile(rug_min_diff.NW, prob = c(0.05, 0.5, 0.95))\r\n\r\n# 5mm scale #\r\nrug_meso_diff.NR <- colMeans(rug_meso_Rip) - colMeans(rug_meso_NatH)\r\nrug_meso_NR.CI <- quantile(rug_meso_diff.NR, prob = c(0.05, 0.5, 0.95))\r\nrug_meso_diff.NW <- colMeans(rug_meso_Wal) - colMeans(rug_meso_NatV)\r\nrug_meso_NW.CI <- quantile(rug_meso_diff.NW, prob = c(0.05, 0.5, 0.95))\r\n\r\n# 1cm scale #\r\nrug_max_diff.NR <- colMeans(rug_max_Rip) - colMeans(rug_max_NatH)\r\nrug_max_NR.CI <- quantile(rug_max_diff.NR, prob = c(0.05, 0.5, 0.95))\r\nrug_max_diff.NW <- colMeans(rug_max_Wal) - colMeans(rug_max_NatV)\r\nrug_max_NW.CI <- quantile(rug_max_diff.NW, prob = c(0.05, 0.5, 0.95))\r\n\r\n# SAMPLE THE BOOTSTRAPS SfM #\r\nn = 1000 # selection\r\n\r\nsfm_rug1 <- data.frame(rug = c(sample(rug_min_NatH, n, replace = T), sample(rug_min_Rip, n, replace = T), sample(rug_min_NatV, n, replace = T), sample(rug_min_Wal, n, replace = T)), type = c(rep(""natH"", n), rep(""rip"", n), rep(""natV"", n), rep(""wal"", n)))\r\n\r\nsfm_rug2 <- data.frame(rug = c(sample(rug_meso_NatH, n, replace = T), sample(rug_meso_Rip, n, replace = T), sample(rug_meso_NatV, n, replace = T), sample(rug_meso_Wal, n, replace = T)), type = c(rep(""natH"", n), rep(""rip"", n), rep(""natV"", n), rep(""wal"", n)))\r\n\r\nsfm_rug3 <- data.frame(rug = c(sample(rug_max_NatH, n, replace = T), sample(rug_max_Rip, n, replace = T), sample(rug_max_NatV, n, replace = T), sample(rug_max_Wal, n, replace = T)), type = c(rep(""natH"", n), rep(""rip"", n), rep(""natV"", n), rep(""wal"", n)))\r\n\r\n# MEDIAN AND CI INTERAL CALCULATIIONS FOR PERMUTATIONS (LAS) #\r\n# 10cm scale #\r\nl1.NR <- colMeans(l1.Rip) - colMeans(l1.Nat) \r\nl1_NR.CI <- quantile(l1.NR, prob = c(0.05, 0.5, 0.95))\r\nl1.NW <- colMeans(l1.Wal) - colMeans(l1.Nat)\r\nl1_NW.CI <- quantile(l1.NW, prob = c(0.05, 0.5, 0.95))\r\n\r\n# 20cm scale #\r\nl2.NR <- colMeans(l2.Rip) - colMeans(l2.Nat)\r\nl2_NR.CI <- quantile(l2.NR, prob = c(0.05, 0.5, 0.95))\r\nl2.NW <- colMeans(l2.Wal) - colMeans(l2.Nat)\r\nl2_NW.CI <- quantile(l2.NW, prob = c(0.05, 0.5, 0.95))\r\n\r\n# 30cm scale #\r\nl3.NR <- colMeans(l3.Rip) - colMeans(l3.Nat)\r\nl3_NR.CI <- quantile(l3.NR, prob = c(0.05, 0.5, 0.95))\r\nl3.NW <- colMeans(l3.Wal) - colMeans(l3.Nat)\r\nl3_NW.CI <- quantile(l3.NW, prob = c(0.05, 0.5, 0.95))\r\n\r\n# 40cm scale #\r\nl4.NR <- colMeans(l4.Rip) - colMeans(l4.Nat)\r\nl4_NR.CI <- quantile(l4.NR, prob = c(0.05, 0.5, 0.95))\r\nl4.NW <- colMeans(l4.Wal) - colMeans(l4.Nat) \r\nl4_NW.CI <- quantile(l4.NW, prob = c(0.05, 0.5, 0.95))\r\n\r\n# 50cm scale #\r\nl5.NR <- colMeans(l5.Rip) - colMeans(l5.Nat)\r\nl5_NR.CI <- quantile(l5.NR, prob = c(0.05, 0.5, 0.95))\r\nl5.NW <- colMeans(l5.Wal) - colMeans(l5.Nat)\r\nl5_NW.CI <- quantile(l5.NW, prob = c(0.05, 0.5, 0.95))\r\n\r\n# SAMPLE THE BOOTSTRAPS LAS #\r\nn = 1000 # selection\r\n\r\nlas_rug1 <- data.frame(rug = c(sample(l1.Nat, n, replace = T), sample(l1.Rip, n, replace = T), sample(l1.Wal, n, replace = T)), type = c(rep(""nat"", n), rep(""rip"", n), rep(""wal"", n)))\r\n\r\nlas_rug2 <- data.frame(rug = c(sample(l2.Nat, n, replace = T), sample(l2.Rip, n, replace = T), sample(l2.Wal, n, replace = T)), type = c(rep(""nat"", n), rep(""rip"", n), rep(""wal"", n)))\r\n\r\nlas_rug3 <- data.frame(rug = c(sample(l3.Nat, n, replace = T), sample(l3.Rip, n, replace = T), sample(l3.Wal, n, replace = T)), type = c(rep(""nat"", n), rep(""rip"", n), rep(""wal"", n)))\r\n\r\nlas_rug4 <- data.frame(rug = c(sample(l4.Nat, n, replace = T), sample(l4.Rip, n, replace = T), sample(l4.Wal, n, replace = T)), type = c(rep(""nat"", n), rep(""rip"", n), rep(""wal"", n)))\r\n\r\nlas_rug5 <- data.frame(rug = c(sample(l5.Nat, n, replace = T), sample(l5.Rip, n, replace = T), sample(l5.Wal, n, replace = T)), type = c(rep(""nat"", n), rep(""rip"", n), rep(""wal"", n)))\r\n\r\n\r\n# MEDIAN AND CI INTERAL CALCULATIIONS FOR PERMUTATIONS (UAV) #\r\n# 1m scale #\r\nu1.NR <- colMeans(u1.Rip) - colMeans(u1.Nat)\r\nu1_NR.CI <- quantile(u1.NR, prob = c(0.05, 0.5, 0.95))\r\nu1.NW <- colMeans(u1.Wal) - colMeans(u1.Nat)\r\nu1_NW.CI <- quantile(u1.NW, prob = c(0.05, 0.5, 0.95))\r\n\r\n# 2m scale #\r\nu2.NR <- colMeans(u2.Rip) - colMeans(u2.Nat)\r\nu2_NR.CI <- quantile(u2.NR, prob = c(0.05, 0.5, 0.95))\r\nu2.NW <- colMeans(u2.Wal) - colMeans(u2.Nat)\r\nu2_NW.CI <- quantile(u2.NW, prob = c(0.05, 0.5, 0.95))\r\n\r\n# 5m scale #\r\nu3.NR <- colMeans(u3.Rip) - colMeans(u3.Nat)\r\nu3_NR.CI <- quantile(u3.NR, prob = c(0.05, 0.5, 0.95))\r\nu3.NW <- colMeans(u3.Wal) - colMeans(u3.Nat)\r\nu3_NW.CI <- quantile(u3.NW, prob = c(0.05, 0.5, 0.95))\r\n\r\n# 10m scale #\r\nu4.NR <- colMeans(u4.Rip) - colMeans(u4.Nat)\r\nu4_NR.CI <- quantile(u4.NR, prob = c(0.05, 0.5, 0.95))\r\nu4.NW <- colMeans(u4.Wal) - colMeans(u4.Nat)\r\nu4_NW.CI <- quantile(u4.NW, prob = c(0.05, 0.5, 0.95))\r\n\r\n# SAMPLE THE BOOTSTRAPS UAV #\r\nn = 1000 # selection\r\n\r\nuav_rug1 <- data.frame(rug = c(sample(u1.Nat, n, replace = T), sample(u1.Rip, n, replace = T), sample(u1.Wal, n, replace = T)), type = c(rep(""nat"", n), rep(""rip"", n), rep(""wal"", n)))\r\n\r\nuav_rug2 <- data.frame(rug = c(sample(u2.Nat, n, replace = T), sample(u2.Rip, n, replace = T), sample(u2.Wal, n, replace = T)), type = c(rep(""nat"", n), rep(""rip"", n), rep(""wal"", n)))\r\n\r\nuav_rug3 <- data.frame(rug = c(sample(u3.Nat, n, replace = T), sample(u3.Rip, n, replace = T), sample(u3.Wal, n, replace = T)), type = c(rep(""nat"", n), rep(""rip"", n), rep(""wal"", n)))\r\n\r\nuav_rug4 <- data.frame(rug = c(sample(u4.Nat, n, replace = T), sample(u4.Rip, n, replace = T), sample(u4.Wal, n, replace = T)), type = c(rep(""nat"", n), rep(""rip"", n), rep(""wal"", n)))\r\n\r\n\r\n# PLOTING THE DIFFERENCE IN SAMPLES AGAINST 0 AKA NATURAL LEVELS #\r\n# graphical parameters #\r\np <- seq(1, 200, by = 2.5)\r\nd <- 0.9\r\npar(mfrow = c(1,3))\r\nm <- matrix(c(1:3), 1)\r\nlayout(m, c(3.5, 5, 4))\r\nmargin <- c(5, 4, 4, 2) + 0.1\r\nmargin2 <- c(5.1, 5.1, 4.1, 2.1)\r\npar(mar = margin2)\r\nf1 <- ""tan1""\r\nf2 <- ""gray75""\r\nc1 <- ""black""\r\nc2 <- ""black""\r\nln1 <- ""black"" \r\nln2 <- ""black""\r\n\r\n# SFM #\r\n# main plot#\r\nvioplot(rug_min_diff.NR, rug_min_diff.NW,  rug_meso_diff.NR, rug_meso_diff.NW,  rug_max_diff.NR, rug_max_diff.NW, col = c(f1, f2, f1, f2, f1, f2), border = c(c1, c2, c1, c2, c1, c2), drawRect = F, centerline = T, at = c(p[1], p[1]+d, p[2], p[2]+d, p[3], p[3]+d), cex.main = 1.33, main = ""Structure from motion (SfM)"", ylim = c(-0.0008, 0.0004), yaxt=""n"")\r\n\r\n# add text #\r\nmtext(expression(paste(Delta,""Rugosity (mm)"")), side=2, line=3.25, cex = 1.1)\r\n\r\n# add axis and 0 like aka natural #\r\naxis(side = 1, at = c(p[1]+d/2, p[2]+d/2, p[3]+d/2), labels = c(""1 mm"", ""5 mm"", ""1 cm""), cex.axis = 1.2)\r\naxis(side = 2, at = seq(-0.0008, 0.0004, by = 0.0002), labels = c(""-0.8"", ""-0.6"", ""-0.4"", ""-0.2"", ""0.0"", ""0.2"", ""0.4""), las = 1, cex.axis = 1.2)\r\nabline(h=0, lty=2, col = ""red"", lwd = 1.5)\r\n\r\n# add CI markers #\r\narrows(p[1], rug_min_NR.CI[1], p[1], rug_min_NR.CI[3], length = 0.05, angle = 90, code = 3, col = ln1)\r\npoints(p[1], rug_min_NR.CI[2], pch = 21, bg = ln1)\r\npoints(p[1], 0.00035, pch = ""+"", col = ""red"", cex=1.5)\r\n\r\narrows(p[1]+d, rug_min_NW.CI[1], p[1]+d, rug_min_NW.CI[3], length = 0.05, angle = 90, code = 3, col = ln2)\r\npoints(p[1]+d, rug_min_NW.CI[2], pch = 21, bg =ln2)\r\npoints(p[1]+d, 0.00035, pch = ""*"", col = ""red"", cex=1.5)\r\n\r\narrows(p[2], rug_meso_NR.CI[1], p[2], rug_meso_NR.CI[3], length = 0.05, angle = 90, code = 3, col = ln1)\r\npoints(p[2], rug_meso_NR.CI[2], pch = 21, bg =ln1)\r\npoints(p[2], 0.00035, pch = ""+"", col = ""red"", cex=1.5)\r\n\r\narrows(p[2]+d, rug_meso_NW.CI[1], p[2]+d, rug_meso_NW.CI[3], length = 0.05, angle = 90, code = 3, col = ln2)\r\npoints(p[2]+d, rug_meso_NW.CI[2], pch = 21, bg =ln2)\r\npoints(p[2]+d, 0.00035, pch = ""*"", col = ""red"", cex=1.5)\r\n\r\narrows(p[3], rug_max_NR.CI[1], p[3], rug_max_NR.CI[3], length = 0.05, angle = 90, code = 3, col = ln1)\r\npoints(p[3], rug_max_NR.CI[2], pch = 21, bg =ln1)\r\npoints(p[3], 0.00035, pch = ""+"", col = ""red"", cex=1.5)\r\n\r\narrows(p[3]+d, rug_max_NW.CI[1], p[3]+d, rug_max_NW.CI[3], length = 0.05, angle = 90, code = 3, col = ln2)\r\npoints(p[3]+d, rug_max_NW.CI[2], pch = 21, bg =ln2)\r\npoints(p[3]+d, 0.00035, pch = ""*"", col = ""red"", cex=1.5)\r\n\r\n# tidy up #\r\nbox()\r\nposit <- ((p[3]+d) + p[1]) / 2\r\n\r\n\r\n\r\n# LAS #\r\n# main plot#\r\nvioplot(l1.NR, l1.NW,  l2.NR, l2.NW,  l3.NR, l3.NW,  l4.NR, l4.NW,  l5.NR, l5.NW,\r\n        col = c(f1, f2, f1, f2, f1, f2, f1, f2, f1, f2), border = c(c1, c2, c1, c2, c1, c2, c1, c2, c1, c2), drawRect = F, centerline = T, at = c(p[1], p[1]+d, p[2], p[2]+d, p[3], p[3]+d, p[4], p[4]+d, p[5], p[5]+d), cex.main = 1.33, main = ""Terrestrial laser scanner (TLS)"", ylim = c(-0.04, 0.02), yaxt=""n"")\r\n\r\n# add text #\r\nmtext(expression(paste(Delta,""Rugosity (cm)"")), side=2, line=3.25, cex=1.1)\r\nmtext(""Window size"", side=1, line=3.25, cex=1.1)\r\n\r\n# add axis and 0 like aka natural #\r\naxis(side = 1, at = c(p[1]+d/2, p[2]+d/2, p[3]+d/2, p[4]+d/2, p[5]+d/2), labels = c(""10 cm"", ""20 cm"", ""30 cm"", ""40 cm"", ""50 cm""), cex.axis = 1.2)\r\naxis(side = 2, at = seq(-0.04, 0.02, by = 0.01), labels = c(""-4.0"", ""-3.0"", ""-2.0"", ""-1.0"", ""0.0"", ""1.0"", ""2.0""), las = 1, cex.axis = 1.2)\r\nabline(h=0, lty=2, col = ""red"", lwd = 1.5)\r\n\r\n# add CI markers #\r\narrows(p[1], l1_NR.CI[1], p[1], l1_NR.CI[3], length = 0.05, angle = 90, code = 3, col = ln1)\r\npoints(p[1], l1_NR.CI[2], pch = 21, bg = ln1)\r\n\r\narrows(p[1]+d, l1_NW.CI[1], p[1]+d, l1_NW.CI[3], length = 0.05, angle = 90, code = 3, col = ln2)\r\npoints(p[1]+d, l1_NW.CI[2], pch = 21, bg = ln2)\r\npoints(p[1]+d, 0.0175, pch = ""*"", col = ""red"", cex=1.5)\r\n\r\narrows(p[2], l2_NR.CI[1], p[2], l2_NR.CI[3], length = 0.05, angle = 90, code = 3, col = ln1)\r\npoints(p[2], l2_NR.CI[2], pch = 21, bg = ln1)\r\n\r\narrows(p[2]+d, l2_NW.CI[1], p[2]+d, l2_NW.CI[3], length = 0.05, angle = 90, code = 3, col = ln2)\r\npoints(p[2]+d, l2_NW.CI[2], pch = 21, bg = ln2)\r\npoints(p[2]+d, 0.0175, pch = ""*"", col = ""red"", cex=1.5)\r\n\r\narrows(p[3], l3_NR.CI[1], p[3], l3_NR.CI[3], length = 0.05, angle = 90, code = 3, col = ln1)\r\npoints(p[3], l3_NR.CI[2], pch = 21, bg = ln1)\r\n\r\narrows(p[3]+d, l3_NW.CI[1], p[3]+d, l3_NW.CI[3], length = 0.05, angle = 90, code = 3, col = ln2)\r\npoints(p[3]+d, l3_NW.CI[2], pch = 21, bg = ln2)\r\npoints(p[3]+d, 0.0175, pch = ""*"", col = ""red"", cex=1.5)\r\n\r\narrows(p[4], l4_NR.CI[1], p[4], l4_NR.CI[3], length = 0.05, angle = 90, code = 3, col = ln1)\r\npoints(p[4], l4_NR.CI[2], pch = 21, bg = ln1)\r\n\r\narrows(p[4]+d, l4_NW.CI[1], p[4]+d, l4_NW.CI[3], length = 0.05, angle = 90, code = 3, col = ln2)\r\npoints(p[4]+d, l4_NW.CI[2], pch = 21, bg = ln2)\r\npoints(p[4]+d, 0.0175, pch = ""*"", col = ""red"", cex=1.5)\r\n\r\narrows(p[5], l5_NR.CI[1], p[5], l5_NR.CI[3], length = 0.05, angle = 90, code = 3, col = ln1)\r\npoints(p[5], l5_NR.CI[2], pch = 21, bg = ln1)\r\npoints(p[5], 0.0175, pch = ""+"", col = ""red"", cex=1.5)\r\n\r\narrows(p[5]+d, l5_NW.CI[1], p[5]+d, l5_NW.CI[3], length = 0.05, angle = 90, code = 3, col = ln2)\r\npoints(p[5]+d, l5_NW.CI[2], pch = 21, bg = ln2)\r\npoints(p[5]+d, 0.0175, pch = ""*"", col = ""red"", cex=1.5)\r\n\r\n# add legend #\r\nlegend(""bottomleft"", legend = c(expression(paste(Delta, ""Riprap:Natural""^horiz.)),\r\n                                expression(paste(Delta, ""Seawall:Natural""^vert.))),\r\n       fill = c(f1, f2), text.font=2, box.lty=0, cex = 1.3)\r\n\r\n# tidy up #\r\nbox()\r\n\r\n\r\n\r\n# UAV #\r\n# main plot#\r\nvioplot(u1.NR, u1.NW,  u2.NR, u2.NW,  u3.NR, u3.NW,  u4.NR, u4.NW,\r\n        col = c(f1, f2, f1, f2, f1, f2, f1, f2), border = c(c1, c2, c1, c2, c1, c2, c1, c2), drawRect = F, centerline = T, \r\n        at = c(p[1], p[1]+d, p[2], p[2]+d, p[3], p[3]+d, p[4], p[4]+d), cex.main = 1.33,\r\n        main = ""Unmanned aerial vehicle (UAV)"",\r\n        ylim = c(-0.6, 0.3), yaxt=""n"")\r\n\r\n# add text #\r\nmtext(expression(paste(Delta,""Rugosity (m)"")), side=2, line=3.75, cex=1.1)\r\n\r\n# add axis and 0 like aka natural #\r\naxis(side = 1, at = c(p[1]+d/2, p[2]+d/2, p[3]+d/2, p[4]+d/2), labels = c("" 1m"", ""2 m"", ""5 m"", ""10 m""), cex.axis = 1.2)\r\naxis(side = 2, at = seq(-0.6, 0.3, by = 0.15), labels = c(""-0.6"", ""-0.45"", ""-0.3"", ""-0.15"", ""0.0"", ""0.15"", ""0.3""), las = 1, cex.axis = 1.2)\r\nabline(h=0, lty=2, col = ""red"", lwd = 1.5)\r\n\r\n# add CI markers #\r\narrows(p[1], u1_NR.CI[1], p[1], u1_NR.CI[3], length = 0.05, angle = 90, code = 3, col = ln1)\r\npoints(p[1], u1_NR.CI[2], pch = 21, bg = ln1)\r\n\r\narrows(p[1]+d, u1_NW.CI[1], p[1]+d, u1_NW.CI[3], length = 0.05, angle = 90, code = 3, col = ln2)\r\npoints(p[1]+d, u1_NW.CI[2], pch = 21, bg = ln2)\r\n\r\narrows(p[2], u2_NR.CI[1], p[2], u2_NR.CI[3], length = 0.05, angle = 90, code = 3, col = ln1)\r\npoints(p[2], u2_NR.CI[2], pch = 21, bg = ln1)\r\n\r\narrows(p[2]+d, u2_NW.CI[1], p[2]+d, u2_NW.CI[3], length = 0.05, angle = 90, code = 3, col = ln2)\r\npoints(p[2]+d, u2_NW.CI[2], pch = 21, bg = ln2)\r\n\r\narrows(p[3], u3_NR.CI[1], p[3], u3_NR.CI[3], length = 0.05, angle = 90, code = 3, col = ln1)\r\npoints(p[3], u3_NR.CI[2], pch = 21, bg = ln1)\r\n\r\narrows(p[3]+d, u3_NW.CI[1], p[3]+d, u3_NW.CI[3], length = 0.05, angle = 90, code = 3, col = ln2)\r\npoints(p[3]+d, u3_NW.CI[2], pch = 21, bg = ln2)\r\npoints(p[3]+d, 0.2615, pch = ""*"", col = ""red"", cex=1.5)\r\n\r\narrows(p[4], u4_NR.CI[1], p[4], u4_NR.CI[3], length = 0.05, angle = 90, code = 3, col = ln1)\r\npoints(p[4], u4_NR.CI[2], pch = 21, bg = ln1)\r\npoints(p[4], 0.2615, pch = ""+"", col = ""red"", cex=1.5)\r\n\r\narrows(p[4]+d, u4_NW.CI[1], p[4]+d, u4_NW.CI[3], length = 0.05, angle = 90, code = 3, col = ln2)\r\npoints(p[4]+d, u4_NW.CI[2], pch = 21, bg = ln2)\r\npoints(p[4]+d, 0.2615, pch = ""*"", col = ""red"", cex=1.5)\r\n\r\n# tidy up #\r\nbox()\r\n\r\n\r\n# PERMU TEST #\r\n# set up subsets #\r\nnat_wal_sfm <- subset(quad_to, quad_to$Str2 == ""NaturalV"" | quad_to$Str2 == ""Wall"")\r\nnat_rip_sfm <- subset(quad_to, quad_to$Str2 == ""NaturalH"" | quad_to$Str2 == ""Riprap"")\r\nnat_wal_las <- subset(las_to, las_to$Structure == ""Natural"" | las_to$Structure == ""Wall"")\r\nnat_rip_las <- subset(las_to, las_to$Structure == ""Riprap"" | las_to$Structure == ""Natural"")\r\nnat_wal_uav <- subset(uav_to, uav_to$Structure == ""Wall"" | uav_to$Structure == ""Natural"")\r\nnat_rip_uav <- subset(uav_to, uav_to$Structure == ""Riprap"" | uav_to$Structure == ""Natural"")\r\n\r\n# SfM tests #\r\ni1r <- independence_test(rug_min ~ Str2, data = nat_rip_sfm, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\ni1w <- independence_test(rug_min ~ Str2, data = nat_wal_sfm, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\ni2r <- independence_test(rug_meso ~ Str2, data = nat_rip_sfm, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\ni2w <- independence_test(rug_meso ~ Str2, data = nat_wal_sfm, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\ni3r <- independence_test(rug_max ~ Str2, data = nat_rip_sfm, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\ni3w <- independence_test(rug_max ~ Str2, data = nat_wal_sfm, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\n\r\n# LAS tests #\r\ni4r <- independence_test(rug.m1 ~ Structure, data = nat_rip_las, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\ni4w <- independence_test(rug.m1 ~ Structure, data = nat_wal_las, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\ni5r <- independence_test(rug.m2 ~ Structure, data = nat_rip_las, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\ni5w <- independence_test(rug.m2 ~ Structure, data = nat_wal_las, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\ni6r<- independence_test(rug.m3 ~ Structure, data = nat_rip_las, \r\n                        distribution = ""approximate"")\r\ni6w <- independence_test(rug.m3 ~ Structure, data = nat_wal_las, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\ni7r <- independence_test(rug.m4 ~ Structure, data = nat_rip_las, alternative = ""less"", \r\n                         distribution = ""approximate"")\r\ni7w <- independence_test(rug.m4 ~ Structure, data = nat_wal_las, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\ni8r <- independence_test(rug.m5 ~ Structure, data = nat_rip_las, alternative = ""less"", \r\n                         distribution = ""approximate"")\r\ni8w <- independence_test(rug.m5 ~ Structure, data = nat_wal_las, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\n\r\n# UAV tests #\r\ni9r <- independence_test(rug1 ~ Structure, data = nat_rip_uav, alternative = ""less"", \r\n                         distribution = ""approximate"")\r\ni9w <- independence_test(rug1 ~ Structure, data = nat_wal_uav, alternative = ""greater"", \r\n                         distribution = ""approximate"")\r\ni10r <- independence_test(rug2 ~ Structure, data = nat_rip_uav, \r\n                          distribution = ""approximate"")\r\ni10w <- independence_test(rug2 ~ Structure, data = nat_wal_uav, alternative = ""greater"", \r\n                          distribution = ""approximate"")\r\ni11r <- independence_test(rug5 ~ Structure, data = nat_rip_uav, alternative = ""greater"", \r\n                          distribution = ""approximate"")\r\ni11w <- independence_test(rug5 ~ Structure, data = nat_wal_uav, alternative = ""greater"", \r\n                          distribution = ""approximate"")\r\ni12r <- independence_test(rug10 ~ Structure, data = nat_rip_uav, alternative = ""greater"", \r\n                          distribution = ""approximate"")\r\ni12w <- independence_test(rug10 ~ Structure, data = nat_wal_uav, alternative = ""greater"", \r\n                          distribution = ""approximate"")\r\n']","Artificial shorelines lack natural structural complexity across scales (dataset) From microbes to humans, habitat structural complexity plays a direct role in the provision of physical living space and increased complexity supports higher biodiversity and ecosystem functioning across biomes. Natural coastlines are structurally complex transition zones between land and sea that support diverse ecological communities but are under increasing pressure from human activity. Coastal development and the construction of artificial shorelines are changing our landscape and altering biodiversity patterns as humans seek both socio-economic benefits and protection from coastal storms, flooding, and erosion. In this study, we evaluate how much structural complexity is missing, and at which scales, with the creation of artificial structures compared to naturally occurring rocky shores. We quantified the structural complexity of both artificial and natural shores at resolutions from 1 mm through to 10s of m using three remote sensing platforms (handheld camera, terrestrial laser scanner and uncrewed aerial vehicles) across both artificial and natural shorelines. Natural shorelines were approximately 20-50 % more structurally complex and offered greater structural variation between locations. In contrast, artificial shorelines were more structurally homogenous and typically deficient in structural complexity across scales. Our findings reinforce concerns that replacing natural rocky shorelines with artificial structures simplifies coastlines at organism-relevant scales. Furthermore, we offer much-needed insight into how structures might be modified to more closely capture the complexity of natural shorelines that support biodiversity.",1
Insights into natal origins of migratory Nearctic hover flies (Diptera: Syrphidae): New evidence from stable isotope (2H) assignment analyses,"Hover flies (Diptera: Syrphidae) are an important group of insects that provide a multitude of key ecosystem services including pollination and biological control, yet many of their major life history traits are not understood. Some Palearctic hover fly species are known to migrate in response to changing seasonal conditions, yet this behavior is almost entirely unrecognized in Nearctic species. At least one species, Eupeodes americanus (Wiedemann 1830), is partially migratory during autumn while Allograpta obliqua may be non-migratory, but it is unknown where these insects originate and how far they may travel. We examined natal origins of two Nearctic hover fly species, Allograpta obliqua and Eupeodes americanus, using stable hydrogen isotope (2H) measurements of metabolically inactive tissues (wings and legs) to derive a hover fly 2H isoscape. While Allograpta obliqua was mostly of local origin, several Eupeodes americanus were sourced from northern latitudes in the Midwestern United States and Canada, representing travel distances of up to 3,000 km likely using seasonally favorable air currents. This phenomenon is expected to have major ecological and economic ramifications, especially in the realm of plant pollination ecology and biological control.","['ALiso2019 = read.csv(""C:/.../ALHoverFlies.csv"", header = TRUE)\r\nlibrary(ggplot2)\r\n\r\n#AL Specimens for publication\r\n#1000X525\r\nALiso2019$Group = factor(ALiso2019$Group, levels = c(""grp1"",""grp2"",""grp3"",""grp4"",""grp5""))\r\nggplot(ALiso2019, aes(x=Group, y=sample_value, fill=Species)) + ggtitle(""Alabama Hover Fly Isotopes"") + \r\n  geom_point(aes(shape=Species, colour = Species, fill=Species), size = 4, position=position_jitter(width = 0.2, height = 0.2)) +\r\n  scale_shape_manual(values = c(""circle"",""triangle""))+\r\n  scale_color_manual(values = c(""firebrick1"", ""forestgreen""))+\r\n  scale_y_reverse() +\r\n  annotate(""rect"", xmin = 0, xmax = 6, ymin = -93.6, ymax = -105, fill = ""cyan3"", alpha = 0.1) + \r\n  annotate(""rect"", xmin = 0, xmax = 6, ymin = -80.3, ymax = -91.7, fill = ""red2"", alpha = 0.1) + \r\n  geom_hline(yintercept=-93.6, linetype=""dashed"", color = ""cyan3"", size = 1) +\r\n  geom_hline(yintercept=-80.3, linetype=""dashed"", color = ""red2"", size = 1) +\r\n  geom_hline(aes(yintercept=-105,linetype = ""Eupeodes americanus""), colour= \'cyan3\', size = 1) +\r\n  geom_hline(aes(yintercept=-91.7,linetype = ""Allograpta obliqua""), colour= \'red2\', size = 1) +\r\n  scale_linetype_manual(name = ""Local range"", values = c(2, 2), guide = guide_legend(override.aes = list(color = c(""red2"", ""cyan3"")))) +\r\n  xlab(""Week"") + ylab(""Isotope ratio (D2H)"") + theme_bw() + ylim(-50,-200) + \r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = ""black"")) +\r\n  theme(axis.text = element_text(color = ""black"", size = 18), axis.title=element_text(size=20)) + \r\n  stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom=""pointrange"", \r\n               color=""black"", size = 0.8, mapping = aes(x = as.numeric(Group)+.1), show.legend = FALSE) +\r\n  scale_x_discrete(labels=c(""grp1"" = ""38"", ""grp2"" = ""38"",""grp3"" = ""41"",""grp4"" = ""41"",""grp5"" = ""45"")) +\r\n  theme(plot.title = element_text(size=23,face=""bold"",hjust = 0.5)) +\r\n  theme(legend.text = element_text(face = ""italic"", size = 15)) +\r\n  theme(legend.title=element_text(size=18))+\r\n  ylab(expression(Isotope~Ratio~delta^{""2""}~H))+\r\n  geom_vline(xintercept=2.5, colour = ""black"") + \r\n  geom_vline(xintercept=4.5, colour = ""black"") +\r\n  geom_text(label = ""Sept"", size =7, colour = ""black"", mapping=aes(x=1.5, y=-50)) +\r\n  geom_text(label = ""Oct"", size =7, colour = ""black"", mapping=aes(x=3.5, y=-50)) +\r\n  geom_text(label = ""Nov"", size =7, colour = ""black"", mapping=aes(x=5, y=-50))\r\n\r\nsetwd(""C:/..."")\r\n\r\nggsave(""Figure3_weighted.tiff"", units=""in"", width=9, height=5, dpi=600, compression = \'lzw\')\r\n\r\n#AL Specimens Assignment P-values using geom_point\r\nALiso2019$Group = factor(ALiso2019$Group, levels = c(""grp1"",""grp2"",""grp3"",""grp4"",""grp5""))\r\nggplot(ALiso2019, aes(x=Group, y=Local_P_NEW.weighted, fill=Species)) + ggtitle(""Alabama Hover Fly Local Assignments"") + \r\n  geom_point(aes(shape=Species, colour = Species, fill=Species), size = 4, position=position_dodge2(width = 0.5)) +\r\n  scale_shape_manual(values = c(""circle"",""triangle""))+\r\n  scale_color_manual(values = c(""firebrick1"", ""forestgreen""))+\r\n  geom_hline(yintercept=0.1, linetype=""dashed"", color = ""black"", size = 1) +\r\n  scale_linetype_manual(name = ""Local range"", values = c(1, 1), guide = guide_legend(override.aes = list(color = c(""black"")))) +\r\n  xlab(""Week"") + ylab(""P-value"") + theme_bw() + ylim(-0.1,1) +\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = ""black"")) +\r\n  theme(axis.text = element_text(color = ""black"", size = 15), axis.title=element_text(size=20)) + \r\n  scale_x_discrete(labels=c(""grp1"" = ""38"", ""grp2"" = ""38"",""grp3"" = ""41"",""grp4"" = ""41"",""grp5"" = ""45"")) +\r\n  theme(plot.title = element_text(size=20,face=""bold"",hjust = 0.5)) +\r\n  theme(legend.text = element_text(face = ""italic"", size = 15)) +\r\n  theme(legend.title=element_text(size=18))+\r\n  geom_vline(xintercept=2.5, colour = ""black"") + \r\n  geom_vline(xintercept=4.5, colour = ""black"") +\r\n  geom_text(label = ""Sept"", size =7, colour = ""black"", mapping=aes(x=1.5, y=-0.1)) +\r\n  geom_text(label = ""Oct"", size =7, colour = ""black"", mapping=aes(x=3.5, y=-0.1)) +\r\n  geom_text(label = ""Nov"", size =7, colour = ""black"", mapping=aes(x=5, y=-0.1))\r\n\r\n\r\nsetwd(""C:/..."")\r\n\r\nggsave(""SuppFig2.tiff"", units=""in"", width=9, height=5, dpi=600, compression = \'lzw\')\r\n', 'rawGNIP <- read.csv(""C:/..../NorthAmericaGNIPBulkData_AllYears.csv"")\r\nlibrary(IsoriX)\r\nlibrary(dplyr)\r\nlibrary(Cairo)\r\n\r\n###shortcut to Part 4 (These are the results from parts 1-3), make sure active directory is correct.\r\nload(file = ""NAFit_weighted.rda"") #this is the precipitation weighted model\r\nload(file = ""NAIsoscape_weighted.rda"") #this is the precipitation isoscape\r\n\r\n##Part 1 Processing raw data\r\nrawGNIP$year.begin  <- as.numeric(format(as.Date(rawGNIP$Begin.of.Period), ""%Y""))\r\nrawGNIP$year.end    <- as.numeric(format(as.Date(rawGNIP$End.of.Period), ""%Y""))\r\nrawGNIP$year.span   <- rawGNIP$year.begin - rawGNIP$year.end\r\nrawGNIP$month.begin <- as.numeric(format(as.Date(rawGNIP$Begin.of.Period), ""%m""))\r\nrawGNIP$month.end   <- as.numeric(format(as.Date(rawGNIP$End.of.Period), ""%m""))\r\nrawGNIP$day.span    <- as.Date(rawGNIP$Begin.of.Period) - as.Date(rawGNIP$End.of.Period)\r\nrawGNIP$Year        <- as.numeric(format(as.Date(rawGNIP$Date), ""%Y""))\r\nrawGNIP$Month       <- as.numeric(format(as.Date(rawGNIP$Date), ""%m""))\r\n\r\n\r\nrows_missing_or_unreliable_info <- is.na(rawGNIP$H2) |\r\n  is.na(rawGNIP$day.span) |\r\n  rawGNIP$day.span > -25 |\r\n  rawGNIP$day.span < -35 |\r\n  is.na(rawGNIP$Precipitation)\r\n\r\ncolumns_to_keep <- c(""Site"", ""Latitude"", ""Longitude"", ""Altitude"",\r\n                     ""Year"", ""Month"", ""H2"", ""Precipitation"")\r\nGNIPData_with_precip <- rawGNIP[!rows_missing_or_unreliable_info, columns_to_keep]\r\ncolnames(GNIPData_with_precip) <- c(""source_ID"", ""lat"", ""long"", ""elev"", ""year"", ""month"", ""source_value"", ""precip"")\r\n\r\nGNIPData_with_precip\r\n\r\nnrow(GNIPData)\r\nnrow(GNIPData_with_precip)\r\n\r\nGNIPData_with_precip |>\r\n  group_by(source_ID, year) |>\r\n  filter(n() > 6) |> ## only select years with more than 6 months of data\r\n  mutate(w = precip/sum(precip)) |>\r\n  summarize(total_precip = sum(precip),\r\n            mean_source_value = sum(w*source_value),\r\n            var_source_value = sum(w*(source_value - mean(source_value))^2),\r\n            n_source_value = n(),\r\n            lat = unique(lat),\r\n            long = unique(long),\r\n            elev = unique(elev)) |>\r\n  as.data.frame() -> GNIPData_with_precipNAagg_yearly\r\n\r\nGNIPData_with_precipNAagg_yearly\r\n\r\nGNIPData_with_precipNAagg_yearly |>\r\n  group_by(source_ID) |>\r\n  summarize(mean_source_value = mean(mean_source_value),\r\n            var_source_value = mean(var_source_value),\r\n            n_source_value = sum(n_source_value),\r\n            long = unique(long),\r\n            lat = unique(lat),\r\n            elev = unique(elev)) |>\r\n  as.data.frame() -> GNIPData_with_precipNAagg\r\n\r\nGNIPData_with_precipNAagg\r\n\r\n##Part 2 fitting geostatistical models\r\nNAFit_weighted <- isofit(data = GNIPData_with_precipNAagg,\r\n                         mean_model_fix = list(elev = TRUE, lat_abs = TRUE))\r\n\r\nsave(NAFit_weighted, file = ""NAFit_weighted.rda"", compress = ""xz"")\r\nload(file = ""NAFit_weighted.rda"")\r\n\r\n##Part 3 building isoscape\r\nElevWorld <- raster(""gmted2010_30mn.tif"")\r\nElevWorld\r\nElevNA <- prepraster(raster = ElevWorld,\r\n                     isofit = NAFit,\r\n                     aggregation_factor = 4)\r\nplot(ElevNA)\r\n\r\nlevelplot(ElevNA,\r\n          margin = FALSE,\r\n          main = ""Structural raster"")+ \r\n  layer(sp.polygons(CountryBorders)) +\r\n  layer(sp.polygons(OceanMask, fill = ""cyan""))\r\n\r\nNAIsoscape_weighted <- isoscape(raster = ElevNA, isofit = NAFit_weighted)\r\n\r\nsave(NAIsoscape_weighted, file = ""NAIsoscape_weighted.rda"", compress = ""xz"")\r\nload(file = ""NAIsoscape_weighted.rda"")\r\n\r\nplot(NAIsoscape_weighted)\r\n#saving isoscape image\r\nlibrary(Cairo)\r\nCairoPNG(filename = ""NAIsoscape_weighted.png"",\r\n         height = 1080,\r\n         width = 1920,\r\n         res = 300)\r\nplot(NAIsoscape_weighted)\r\ndev.off()\r\n\r\ntiff(filename = ""NAIsoscape_weighted.tiff"", width = 2500, height = 1080, res = 300, units = ""px"")\r\nplot(NAIsoscape_weighted)\r\ndev.off()\r\n\r\n#temporal variation (residVar), point prediction uncertainty (predVar)\r\nNAIsoscape_weighted_mean_residVar = plot(NAIsoscape_weighted, which = ""mean_residVar"")\r\nNAIsoscape_weighted_mean_predVar = plot(NAIsoscape_weighted, which = ""mean_predVar"")\r\n#replace filename designation as appropriate\r\nCairoPNG(filename = ""NAIsoscape_weighted_predVar.png"",\r\n         height = 1080,\r\n         width = 1920,\r\n         res = 300)\r\nplot(NAIsoscape_weighted_mean_predVar)\r\ndev.off()\r\n\r\ntiff(filename = ""NAIsoscape_weighted_predVar.tiff"", width = 2500, height = 1080, res = 300, units = ""px"")\r\nplot(NAIsoscape_weighted_mean_predVar)\r\ndev.off()\r\n\r\n\r\n\r\n\r\n\r\n#Part 4: assigning samples - Eupeodes americanus\r\n#Eupeodes calibration\r\nRearingEXP = read.csv(""C:/.../Rearing results_for IsoriX_Eupeodes.csv"", header = TRUE)\r\nhead(RearingEXP)\r\ncalibEUP = calibfit(method = ""lab"", data = RearingEXP, isofit = NAFit_weighted)\r\n\r\nplot(calibEUP)\r\nsummary(calibEUP)\r\n\r\n\r\n\r\n##Part 4A: Eupeodes individual assignments\r\nALHoverIsos <- read.csv(""C:/.../ALHoverFlies.csv"")\r\nAllEUP = subset(ALHoverIsos, Species == ""Eupeodes americanus"")\r\n#Eupeodes americanus isoscape\r\nAssignEupeodes <- isofind(data = ALHoverIsos,\r\n                          isoscape = NAIsoscape_weighted,\r\n                          calibfit = calibEUP)\r\nplot(AssignEupeodes)\r\n\r\n#examining migratory status (Mig_Status) of AL specimens (yes or no; non-local or local), number corresponds to row in dataset\r\nextract(AssignEupeodes$sample$pv[[25]], cbind(ALHoverIsos$Longitude[25], ALHoverIsos$Latitude[25]))\r\n\r\n#non-local individual collected in AL\r\n#code for mapping maximum likely origin; this was not used in the publication\r\ncoord <- coordinates(AssignEupeodes$sample$pv$S19EUPAL3)\r\nMaxLocation <- coord[which.max(values(AssignEupeodes$sample$pv$S19EUPAL3)),]\r\nMaximum <- data.frame(long = MaxLocation[1], lat = MaxLocation[2])\r\nMaximum\r\n\r\nplot(AssignEupeodes, who = ""S19EUPAL3"", sources = list(draw = FALSE)) +\r\n  xyplot(34.801 ~ -86.949,\r\n         pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)+\r\n  xyplot(Maximum$lat ~ Maximum$long,\r\n         pch = 13, col = ""orange"", cex = 5, lwd = 2, panel = panel.points)\r\n\r\ncairo_pdf(file = ""EUPSEP.pdf"",onefile = FALSE,fallback_resolution=600)\r\nplot(EUPSEP)\r\ndev.off() #use this code then convert it to a tiff in PDF editor\r\n\r\ntiff(filename = ""EUPSEP.tiff"", width = 2500, height = 1080, res = 300, units = ""px"")\r\nplot(EUPSEP)\r\ndev.off() #for saving as a tiff\r\n\r\nplot(AssignEupeodes, who = ""N19EUPAL4"", sources = list(draw = FALSE)) +\r\n  xyplot(32.2743 ~ -85.269,\r\n         pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\n#building panels of September, October, November specimens \r\nEUPSEPpanels_redX = plot(AssignEupeodes,\r\n                         who = 25:34,\r\n                         sources = list(draw = FALSE),\r\n                         calibs = list(draw = FALSE),\r\n                         assigns = list(draw = FALSE)) +\r\n  xyplot(34.801 ~ -86.949, pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\nEUPSEPpanels = plot(AssignEupeodes,\r\n                    who = 25:34,\r\n                    sources = list(draw = FALSE),\r\n                    calibs = list(draw = FALSE),\r\n                    assigns = list(draw = FALSE))\r\nsave(EUPSEPpanels, file = ""EUPSEPpanels_weighted.rda"", compress = ""xz"")\r\nload(file = ""EUPSEPpanels_weighted.rda"")\r\n\r\nEUPOCTpanels_redX = plot(AssignEupeodes,\r\n                         who = 35:44,\r\n                         sources = list(draw = FALSE),\r\n                         calibs = list(draw = FALSE),\r\n                         assigns = list(draw = FALSE))+\r\n  xyplot(34.801 ~ -86.949, pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\nEUPOCTpanels = plot(AssignEupeodes,\r\n                    who = 35:44,\r\n                    sources = list(draw = FALSE),\r\n                    calibs = list(draw = FALSE),\r\n                    assigns = list(draw = FALSE))\r\nsave(EUPOCTpanels, file = ""EUPOCTpanels.rda"", compress = ""xz"")\r\nload(file = ""EUPOCTpanels.rda"")\r\n\r\nEUPNOVpanels_redX = plot(AssignEupeodes,\r\n                         who = 45:49,\r\n                         sources = list(draw = FALSE),\r\n                         calibs = list(draw = FALSE),\r\n                         assigns = list(draw = FALSE))+\r\n  xyplot(32.589 ~ -85.505, pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\nEUPNOVpanels = plot(AssignEupeodes,\r\n                    who = 45:49,\r\n                    sources = list(draw = FALSE),\r\n                    calibs = list(draw = FALSE),\r\n                    assigns = list(draw = FALSE))\r\nsave(EUPNOVpanels, file = ""EUPNOVpanels.rda"", compress = ""xz"")\r\nload(file = ""EUPNOVpanels.rda"") #didn\'t save this\r\n\r\n#saving panels\r\nlibrary(Cairo)\r\nCairoPNG(filename = ""EUPSEPpanels_weighted.png"",\r\n         height = 1080,\r\n         width = 2500, res = 200)\r\ntiff(filename = ""EUPSEPpanels_weighted.tiff"", width = 2500, height = 1080, res = 300, units = ""px"")\r\nplot(EUPSEPpanels_redX)\r\ndev.off()\r\n\r\nCairoPNG(filename = ""EUPOCTpanels_weighted.png"",\r\n         height = 1080,\r\n         width = 2500, res = 200)\r\ntiff(filename = ""EUPOCTpanels_weighted.tiff"", width = 2500, height = 1080, res = 300, units = ""px"")\r\nplot(EUPOCTpanels_redX)\r\ndev.off()\r\n\r\nCairoPNG(filename = ""EUPNOVpanels_weighted.png"",\r\n         height = 1080,\r\n         width = 2500, res = 200)\r\ntiff(filename = ""EUPNOVpanels_weighted.tiff"", width = 2500, height = 1080, res = 300, units = ""px"")\r\nplot(EUPNOVpanels_redX)\r\ndev.off()\r\n\r\n\r\n\r\n#Part 4B: Eupeodes group assignment\r\n#TIFF 800X500\r\nALHoverIsos <- read.csv(""C:/.../ALHoverFlies.csv"")\r\nALnonlocalEUP = subset(ALHoverIsos, Mig_Status == ""Yes""& Species == ""Eupeodes americanus"")\r\nALlocalEUP = subset(ALHoverIsos, Mig_Status == ""No"" & Species == ""Eupeodes americanus"")\r\n\r\n#non-local specimens\r\nAssignEUPNonLocal <- isofind(data = ALnonlocalEUP,\r\n                             isoscape = NAIsoscape_weighted, calibfit = calibEUP)\r\n\r\n\r\nEUPGRPNonLocal = plot(AssignEUPNonLocal, who = ""group"", sources = list(draw = FALSE)) +\r\n  xyplot(34.801 ~ -86.949,\r\n         pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\n\r\n#local specimens\r\nAssignEUPLocal <- isofind(data = ALlocalEUP,\r\n                          isoscape = NAIsoscape_weighted, calibfit = calibEUP)\r\n\r\nEUPGRPLocal = plot(AssignEUPLocal, who = ""group"", sources = list(draw = FALSE)) +\r\n  xyplot(34.801 ~ -86.949,\r\n         pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\n#saving group assignment maps\r\nCairoPNG(filename = ""EUPGRPNonLocal_weighted.png"",\r\n         height = 1080,\r\n         width = 2500, res = 200)\r\ntiff(filename = ""EUPGRPNonLocal_weighted.tiff"", width = 2500, height = 1080, res = 300, units = ""px"")\r\nplot(EUPGRPNonLocal)\r\ndev.off()\r\n\r\nCairoPNG(filename = ""EUPGRPLocal_weighted.png"",\r\n         height = 1080,\r\n         width = 2500, res = 200)\r\ntiff(filename = ""EUPGRPLocal_weighted.tiff"", width = 2500, height = 1080, res = 300, units = ""px"")\r\nplot(EUPGRPLocal)\r\ndev.off()\r\n\r\n#per month Eupeodes group assignments\r\nALSeptEupeodes = subset(ALHoverIsos, GroupTime == ""EupSept"")\r\nALOctEupeodes = subset(ALHoverIsos, GroupTime == ""EupOct"")\r\n\r\nAssignEupeodesSept <- isofind(data = ALSeptEupeodes,\r\n                              isoscape = NAIsoscape, calibfit = calibEUP)\r\nplot(AssignEupeodesSept, who = ""group"", sources = list(draw = FALSE)) +\r\n  xyplot(34.801 ~ -86.949,\r\n         pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\nAssignEupeodesOct <- isofind(data = ALOctEupeodes,\r\n                             isoscape = NAIsoscape, calibfit = calibEUP)\r\nplot(AssignEupeodesOct, who = ""group"", sources = list(draw = FALSE)) +\r\n  xyplot(34.801 ~ -86.949,\r\n         pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\n\r\n\r\n\r\n#Part 5: Assigning Allograpta obliqua\r\n#Allograpta calibration\r\nRearingEXP2 = read.csv(""C:/.../Rearing results_for IsoriX_Allograpta.csv"", header = TRUE)\r\nhead(RearingEXP)\r\ncalibALO = calibfit(method = ""lab"", data = RearingEXP2, isofit = NAFit_weighted)\r\n\r\nplot(calibALO, who = 1:4)\r\n\r\nALHoverIsos <- read.csv(""C:/.../ALHoverFlies.csv"")\r\n#Allograpta obliqua isoscape\r\nAssignAllograpta <- isofind(data = ALHoverIsos,\r\n                            isoscape = NAIsoscape_weighted, calibfit = calibALO)\r\nAssignAllograpta\r\n\r\n#Part 5A: Allograpta obliqua individual assignments\r\n#examining migratory status (Mig_Status) of AL specimens (yes or no; non-local or local), number corresponds to row in dataset\r\nextract(AssignAllograpta$sample$pv[[24]], cbind(ALHoverIsos$Longitude[24], ALHoverIsos$Latitude[24]))\r\n\r\n\r\n#example Allograpta collected in AL\r\n#If this is switched with AssignEupeodes we get different results with Allograpta being southern\r\nplot(AssignAllograpta, who = ""S19ALOAL9"", sources = list(draw = FALSE)) +\r\n  xyplot(34.801 ~ -86.949,\r\n         pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\n#Allograpta panels\r\n#September specimens\r\nALOSEPpanels = plot(AssignAllograpta,\r\n                          who = 1:12,\r\n                          sources = list(draw = FALSE),\r\n                          calibs = list(draw = FALSE),\r\n                          assigns = list(draw = FALSE))+\r\n  xyplot(34.801 ~ -86.949, pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\nsave(ALOSEPpanels, file = ""ALOSEPpanels.rda"", compress = ""xz"")\r\nload(file = ""ALOSEPpanels.rda"")\r\n\r\nALOSEPpanels_first10 = plot(AssignAllograpta,\r\n                          who = 1:10,\r\n                          sources = list(draw = FALSE),\r\n                          calibs = list(draw = FALSE),\r\n                          assigns = list(draw = FALSE))+\r\n  xyplot(34.801 ~ -86.949, pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\n\r\nALOOCTpanels = plot(AssignAllograpta,\r\n                         who = 13:24,\r\n                         sources = list(draw = FALSE),\r\n                         calibs = list(draw = FALSE),\r\n                         assigns = list(draw = FALSE))+\r\n  xyplot(34.801 ~ -86.949, pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\nsave(ALOOCTpanels, file = ""ALOOCTpanels.rda"", compress = ""xz"")\r\nload(file = ""ALOOCTpanels.rda"")\r\n\r\n#saving panels\r\nCairoPNG(filename = ""ALOSEPpanels.png"",\r\n         height = 1080,\r\n         width = 2500, res = 200)\r\ntiff(filename = ""ALOSEPpanels.tiff"", width = 2500, height = 1080, res = 300, units = ""px"")\r\nplot(ALOSEPpanels2_redX)\r\ndev.off()\r\n\r\nCairoPNG(filename = ""ALOOCTpanels.png"",\r\n         height = 1080,\r\n         width = 2500, res = 200)\r\ntiff(filename = ""ALOOCTpanels.tiff"", width = 2500, height = 1080, res = 300, units = ""px"")\r\nplot(ALOOCTpanels_redX)\r\ndev.off()\r\n\r\n#Part 5A: Allograpta obliqua group assignments\r\nALHoverIsos <- read.csv(""C:/.../ALHoverFlies.csv"")\r\nALnonlocalALO = subset(ALHoverIsos, Mig_Status == ""Yes""& Species == ""Allograpta obliqua"")\r\nALlocalALO = subset(ALHoverIsos, Mig_Status == ""No"" & Species == ""Allograpta obliqua"")\r\n\r\n#Non-local specimens\r\nAssignALONonLocal <- isofind(data = ALnonlocalALO,\r\n                             isoscape = NAIsoscape_weighted, calibfit = calibALO)\r\nALOGRPNonLocal = plot(AssignALONonLocal, who = ""group"", sources = list(draw = FALSE)) +\r\n  xyplot(34.801 ~ -86.949,\r\n         pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\n#Local specimens\r\nAssignALOLocal <- isofind(data = ALlocalALO,\r\n                          isoscape = NAIsoscape_weighted, calibfit = calibALO)\r\nALOGRPLocal = plot(AssignALOLocal, who = ""group"", sources = list(draw = FALSE)) +\r\n  xyplot(34.801 ~ -86.949,\r\n         pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\n#Saving group assignment maps\r\nCairoPNG(filename = ""ALOGRPNonLocal.png"",\r\n         height = 1080,\r\n         width = 2500, res = 200)\r\ntiff(filename = ""ALOGRPNonLocal.tiff"", width = 2500, height = 1080, res = 300, units = ""px"")\r\nplot(ALOGRPNonLocal)\r\ndev.off()\r\n\r\nCairoPNG(filename = ""ALOGRPLocal.png"",\r\n         height = 1080,\r\n         width = 2500, res = 200)\r\ntiff(filename = ""ALOGRPLocal.tiff"", width = 2500, height = 1080, res = 300, units = ""px"")\r\nplot(ALOGRPLocal)\r\ndev.off()\r\n\r\n#per month Allograpta group assignments\r\nALSeptAllograpta = subset(ALHoverIsos, GroupTime == ""AllSept"")\r\nALOctAllograpta = subset(ALHoverIsos, GroupTime == ""AllOct"")\r\nAssignAllograptaSept <- isofind(data = ALSeptAllograpta,\r\n                                isoscape = NAIsoscape, calibfit = calibALO)\r\nplot(AssignAllograptaSept, who = ""group"", sources = list(draw = FALSE)) +\r\n  xyplot(34.801 ~ -86.949,\r\n         pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n\r\nAssignAllograptaOct <- isofind(data = ALOctAllograpta,\r\n                               isoscape = NAIsoscape, calibfit = calibALO)\r\nplot(AssignAllograptaOct, who = ""group"", sources = list(draw = FALSE)) +\r\n  xyplot(34.785 ~ -86.961,\r\n         pch = 4, col = ""red"", cex = 1, lwd = 1, panel = panel.points)\r\n']","Insights into natal origins of migratory Nearctic hover flies (Diptera: Syrphidae): New evidence from stable isotope (2H) assignment analyses Hover flies (Diptera: Syrphidae) are an important group of insects that provide a multitude of key ecosystem services including pollination and biological control, yet many of their major life history traits are not understood. Some Palearctic hover fly species are known to migrate in response to changing seasonal conditions, yet this behavior is almost entirely unrecognized in Nearctic species. At least one species, Eupeodes americanus (Wiedemann 1830), is partially migratory during autumn while Allograpta obliqua may be non-migratory, but it is unknown where these insects originate and how far they may travel. We examined natal origins of two Nearctic hover fly species, Allograpta obliqua and Eupeodes americanus, using stable hydrogen isotope (2H) measurements of metabolically inactive tissues (wings and legs) to derive a hover fly 2H isoscape. While Allograpta obliqua was mostly of local origin, several Eupeodes americanus were sourced from northern latitudes in the Midwestern United States and Canada, representing travel distances of up to 3,000 km likely using seasonally favorable air currents. This phenomenon is expected to have major ecological and economic ramifications, especially in the realm of plant pollination ecology and biological control.",1
Data from: Ecological drivers of jellyfish blooms  the complex life history of a 'well-known' medusa (Aurelia aurita),"Jellyfish blooms are conspicuous demographic events with significant ecological and socio-economic impact. Despite worldwide concern about an increased frequency and intensity of such mass occurrences, predicting their booms and busts remains challenging.Forecasting how jellyfish populations may respond to environmental change requires considering their complex life histories. Metagenic life cycles, which include a benthic polyp stage, can boost jellyfish mass occurrences via asexual recruitment of pelagic medusae.Here we present stage-structured matrix population models with monthly, individual-based demographic rates of all life stages of the moon jellyfish Aurelia aurita L. (sensu stricto). We investigate the life stage-dynamics of these complex populations under low and high food conditions to illustrate how changes in medusa density depend on non-medusa stage dynamics.We show that increased food availability can be an important ecological driver of jellyfish mass occurrences, as it can temporarily shift the population structure from polyp- to medusa- dominated. Projecting populations for a winter warming scenario enhanced the booms and busts of jellyfish blooms.We identify demographic key variables that control the intensity and frequency of jellyfish blooms in response to environmental drivers such as habitat eutrophication and climate change. By contributing to an improved understanding of mass occurrence phenomena, our findings provide perspective for future management of ecosystem health.","['##############################################################################################################\r\n#ONLINE SUPPORTING INFORMATION                                                                               #\r\n#to the article                                                                                              #\r\n#ECOLOGICAL DRIVERS OF JELLYFISH BLOOMS - THE COMPLEX LIFE HISTORY OF A \'WELL-KNOWN\' MEDUSA (Aurelia aurita) #\r\n#JOURNAL OF ANIMAL ECOLOGY                                                                                   #\r\n#                                                                                                            #\r\n#Appendix S4: R code for constructing and analyzing population matrices                                      #\r\n##############################################################################################################\r\n\r\n#############################\r\n#Created: 01 August 2016    #\r\n#Last modified: 26 Oct 2019 #\r\n#File: ""Matrix model.R""     #\r\n#############################\r\n\r\n#Clear list\r\nrm(list = ls())\r\n\r\n#set print options for matrix output\r\noptions(max.print=999999)\r\n\r\n#Import monthly stage-specific demographic rates of Aurelia aurita (sensu stricto) under low and high food conditions\r\nL_month <- read.table(""Low_food.txt"", header=TRUE, dec=""."", sep=""\\t"",row.names=1)\r\nH_month <- read.table(""High_food.txt"", header=TRUE, dec=""."", sep=""\\t"",row.names=1)\r\n\r\n\r\n#######################\r\n# MATRIX CONSTRUCTION #\r\n#######################\r\n\r\n#TIMESTEP: 12 MONTHS\r\n#Construct yearly stage-structured population matrices including sums of monthly transition probabilities and fecundities (A = T + F; cf. Caswell, 2001) for low and high food conditions (1:10). Final stages: polyp (P), ephyra (E) and medusa (M) -> general structure: 12x3 x 12x3 = 36 x 36\r\n\r\n#LOW FOOD\r\n#Transition matrix T: contains corrected monthly (mean) survivorship in each stage i and transition probabilities from stage i to stage j\r\nT_low <- matrix(c(rep(0,33), L_month$t_PP[12],0,0,  #Month 12\r\n                  rep(0,33),0, L_month$t_EE[12],0,\r\n                  rep(0,33), 0, L_month$t_EM_cor[12], L_month$t_MM[12],\r\n                          \r\n                  L_month$t_PP[1],0,0, rep(0,33), #Month 1\r\n                  0, L_month$t_EE[1],0,rep(0,33), \r\n                  0, L_month$t_EM_cor[1], L_month$t_MM[1], rep(0,33), \r\n                          \r\n                  rep(0,3), L_month$t_PP[2],0,0, rep(0,30),  #Month 2\r\n                  rep(0,3), 0, L_month$t_EE[2],0, rep(0,30), \r\n                  rep(0,3), 0, L_month$t_EM_cor[2], L_month$t_MM[2], rep(0,30), \r\n                          \r\n                  rep(0,6), L_month$t_PP[3],0,0, rep(0,27),  #Month 3\r\n                  rep(0,6), 0, L_month$t_EE[3],0,rep(0,27), \r\n                  rep(0,6), 0, L_month$t_EM_cor[3], L_month$t_MM[3], rep(0,27), \r\n                          \r\n                  rep(0,9), L_month$t_PP[4],0,0, rep(0,24),  #Month 4\r\n                  rep(0,9), 0, L_month$t_EE[4],0, rep(0,24), \r\n                  rep(0,9), 0, L_month$t_EM_cor[4], L_month$t_MM[4], rep(0,24), \r\n                          \r\n                  rep(0,12), L_month$t_PP[5],0,0, rep(0,21), #Month 5\r\n                  rep(0,12), 0, L_month$t_EE[5],0, rep(0,21), \r\n                  rep(0,12), 0, L_month$t_EM_cor[5], L_month$t_MM[5], rep(0,21), \r\n                          \r\n                  rep(0,15), L_month$t_PP[6],0,0, rep(0,18), #Month 6\r\n                  rep(0,15), 0, L_month$t_EE[6],0,rep(0,18), \r\n                  rep(0,15), 0, L_month$t_EM_cor[6], L_month$t_MM[6], rep(0,18), \r\n                          \r\n                  rep(0,18), L_month$t_PP[7],0,0, rep(0,15), #Month 7\r\n                  rep(0,18), 0, L_month$t_EE[7],0, rep(0,15), \r\n                  rep(0,18), 0, L_month$t_EM_cor[7], L_month$t_MM[7], rep(0,15), \r\n                          \r\n                  rep(0,21), L_month$t_PP[8],0,0, rep(0,12), #Month 8\r\n                  rep(0,21), 0, L_month$t_EE[8],0, rep(0,12), \r\n                  rep(0,21), 0, L_month$t_EM_cor[8], L_month$t_MM[8], rep(0,12), \r\n                          \r\n                  rep(0,24), L_month$t_PP[9],0,0, rep(0,9), #Month 9\r\n                  rep(0,24), 0, L_month$t_EE[9],0, rep(0,9), \r\n                  rep(0,24), 0, L_month$t_EM_cor[9], L_month$t_MM[9], rep(0,9), \r\n                          \r\n                  rep(0,27), L_month$t_PP[10],0,0, rep(0,6), #Month 10\r\n                  rep(0,27), 0, L_month$t_EE[10],0,rep(0,6), \r\n                  rep(0,27), 0, L_month$t_EM_cor[10], L_month$t_MM[10], rep(0,6), \r\n                          \r\n                  rep(0,30), L_month$t_PP[11],0,0, rep(0,3), #Month 11\r\n                  rep(0,30), 0, L_month$t_EE[11],0,rep(0,3), \r\n                  rep(0,30), 0, L_month$t_EM_cor[11], L_month$t_MM[11], rep(0,3)),byrow=T,nrow=36,ncol=36,dimnames=list(x=c(""1P"",""1E"",""1M"",""2P"",""2E"",""2M"",""3P"",""3E"",""3M"",""4P"",""4E"",""4M"",""5P"",""5E"",""5M"",""6P"",""6E"",""6M"",""7P"",""7E"",""7M"",""8P"",""8E"",""8M"",""9P"",""9E"",""9M"",""10P"",""10E"",""10M"",""11P"",""11E"",""11M"",""12P"",""12E"",""12M""),y=c(""1P"",""1E"",""1M"",""2P"",""2E"",""2M"",""3P"",""3E"",""3M"",""4P"",""4E"",""4M"",""5P"",""5E"",""5M"",""6P"",""6E"",""6M"",""7P"",""7E"",""7M"",""8P"",""8E"",""8M"",""9P"",""9E"",""9M"",""10P"",""10E"",""10M"",""11P"",""11E"",""11M"",""12P"",""12E"",""12M"")))\r\n\r\n#Transition matrix needs to be corrected for reducible transitions by inserting very small values as placeholder (c = 1e-10 /month)\r\n#Ephyrae\r\nT_low[5,2] <- 1e-10\r\nT_low[8,5] <- 1e-10\r\nT_low[11,8] <- 1e-10\r\nT_low[29,26] <- 1e-10\r\nT_low[32,29] <- 1e-10\r\nT_low[35,32] <- 1e-10\r\nT_low[2,35] <- 1e-10\r\n#Medusae\r\nT_low[12,9] <- 1e-10\r\nT_low[15,12] <- 1e-10\r\n\r\n#Fertility matrix F: contains corrected monthly (mean) fecundities of each stage i to produce offspring of stage j\r\nF_low <- matrix(c(rep(0,33),L_month$F_PP_cor[12],0,L_month$F_MP_cor[12],  #Month 12\r\n                  rep(0,33),L_month$F_PE_cor[12],0,0,\r\n                  rep(0,33),L_month$F_PM_cor[12],0,0,\r\n                         \r\n                  L_month$F_PP_cor[1],0,L_month$F_MP_cor[1], rep(0,33), #Month 1\r\n                  L_month$F_PE_cor[1],0,0,rep(0,33), \r\n                  L_month$F_PM_cor[1],0,0, rep(0,33), \r\n                         \r\n                  rep(0,3), L_month$F_PP_cor[2],0,L_month$F_MP_cor[2], rep(0,30),  #Month 2\r\n                  rep(0,3), L_month$F_PE_cor[2],0,0, rep(0,30), \r\n                  rep(0,3), L_month$F_PM_cor[2],0,0, rep(0,30), \r\n                         \r\n                  rep(0,6), L_month$F_PP_cor[3],0,L_month$F_MP_cor[3], rep(0,27),  #Month 3\r\n                  rep(0,6), L_month$F_PE_cor[3],0,0,rep(0,27), \r\n                  rep(0,6), L_month$F_PM_cor[3],0,0, rep(0,27), \r\n                         \r\n                  rep(0,9), L_month$F_PP_cor[4],0,L_month$F_MP_cor[4], rep(0,24),  #Month 4\r\n                  rep(0,9), L_month$F_PE_cor[4],0,0, rep(0,24), \r\n                  rep(0,9), L_month$F_PM_cor[4],0,0, rep(0,24), \r\n                         \r\n                  rep(0,12), L_month$F_PP_cor[5],0,L_month$F_MP_cor[5], rep(0,21), #Month 5\r\n                  rep(0,12), L_month$F_PE_cor[5],0,0,rep(0,21), \r\n                  rep(0,12), L_month$F_PM_cor[5],0,0,rep(0,21), \r\n                         \r\n                  rep(0,15), L_month$F_PP_cor[6],0,L_month$F_MP_cor[6], rep(0,18), #Month 6\r\n                  rep(0,15), L_month$F_PE_cor[6],0,0,rep(0,18), \r\n                  rep(0,15), L_month$F_PM_cor[6],0,0,rep(0,18), \r\n                         \r\n                  rep(0,18), L_month$F_PP_cor[7],0,L_month$F_MP_cor[7], rep(0,15), #Month 7\r\n                  rep(0,18), L_month$F_PE_cor[7],0,0,rep(0,15), \r\n                  rep(0,18), L_month$F_PM_cor[7],0,0,rep(0,15), \r\n                         \r\n                  rep(0,21), L_month$F_PP_cor[8],0,L_month$F_MP_cor[8], rep(0,12), #Month 8\r\n                  rep(0,21), L_month$F_PE_cor[8],0,0,rep(0,12), \r\n                  rep(0,21), L_month$F_PM_cor[8],0,0,rep(0,12), \r\n                         \r\n                  rep(0,24), L_month$F_PP_cor[9],0,L_month$F_MP_cor[9], rep(0,9), #Month 9\r\n                  rep(0,24), L_month$F_PE_cor[9],0,0,rep(0,9), \r\n                  rep(0,24), L_month$F_PM_cor[9],0,0,rep(0,9), \r\n                         \r\n                  rep(0,27), L_month$F_PP_cor[10],0,L_month$F_MP_cor[10], rep(0,6), #Month 10\r\n                  rep(0,27), L_month$F_PE_cor[10],0,0,rep(0,6), \r\n                  rep(0,27), L_month$F_PM_cor[10],0,0,rep(0,6), \r\n                         \r\n                  rep(0,30), L_month$F_PP_cor[11],0,L_month$F_MP_cor[11], rep(0,3), #Month 11\r\n                  rep(0,30), L_month$F_PE_cor[11],0,0,rep(0,3), \r\n                  rep(0,30), L_month$F_PM_cor[11],0,0,rep(0,3)),byrow=T,nrow=36,ncol=36,dimnames=list(x=c(""1P"",""1E"",""1M"",""2P"",""2E"",""2M"",""3P"",""3E"",""3M"",""4P"",""4E"",""4M"",""5P"",""5E"",""5M"",""6P"",""6E"",""6M"",""7P"",""7E"",""7M"",""8P"",""8E"",""8M"",""9P"",""9E"",""9M"",""10P"",""10E"",""10M"",""11P"",""11E"",""11M"",""12P"",""12E"",""12M""),y=c(""1P"",""1E"",""1M"",""2P"",""2E"",""2M"",""3P"",""3E"",""3M"",""4P"",""4E"",""4M"",""5P"",""5E"",""5M"",""6P"",""6E"",""6M"",""7P"",""7E"",""7M"",""8P"",""8E"",""8M"",""9P"",""9E"",""9M"",""10P"",""10E"",""10M"",""11P"",""11E"",""11M"",""12P"",""12E"",""12M"")))\r\n\r\nA_low <- T_low + F_low\r\n\r\nprint(A_low,10)\r\n\r\n#HIGH FOOD\r\n#Transition matrix\r\nT_high <- matrix(c(rep(0,33), H_month$t_PP[12],0,0,  #Month 12\r\n                   rep(0,33),0, H_month$t_EE[12],0,\r\n                   rep(0,33), 0, H_month$t_EM_cor[12], H_month$t_MM[12],\r\n                          \r\n                   H_month$t_PP[1],0,0, rep(0,33), #Month 1\r\n                   0, H_month$t_EE[1],0,rep(0,33), \r\n                   0, H_month$t_EM_cor[1], H_month$t_MM[1], rep(0,33), \r\n                          \r\n                   rep(0,3), H_month$t_PP[2],0,0, rep(0,30),  #Month 2\r\n                   rep(0,3), 0, H_month$t_EE[2],0, rep(0,30), \r\n                   rep(0,3), 0, H_month$t_EM_cor[2], H_month$t_MM[2], rep(0,30), \r\n                          \r\n                   rep(0,6), H_month$t_PP[3],0,0, rep(0,27),  #Month 3\r\n                   rep(0,6), 0, H_month$t_EE[3],0,rep(0,27), \r\n                   rep(0,6), 0, H_month$t_EM_cor[3], H_month$t_MM[3], rep(0,27), \r\n                          \r\n                   rep(0,9), H_month$t_PP[4],0,0, rep(0,24),  #Month 4\r\n                   rep(0,9), 0, H_month$t_EE[4],0, rep(0,24), \r\n                   rep(0,9), 0, H_month$t_EM_cor[4], H_month$t_MM[4], rep(0,24), \r\n                          \r\n                   rep(0,12), H_month$t_PP[5],0,0, rep(0,21), #Month 5\r\n                   rep(0,12), 0, H_month$t_EE[5],0, rep(0,21), \r\n                   rep(0,12), 0, H_month$t_EM_cor[5], H_month$t_MM[5], rep(0,21), \r\n                          \r\n                   rep(0,15), H_month$t_PP[6],0,0, rep(0,18), #Month 6\r\n                   rep(0,15), 0, H_month$t_EE[6],0,rep(0,18), \r\n                   rep(0,15), 0, H_month$t_EM_cor[6], H_month$t_MM[6], rep(0,18), \r\n                          \r\n                   rep(0,18), H_month$t_PP[7],0,0, rep(0,15), #Month 7\r\n                   rep(0,18), 0, H_month$t_EE[7],0, rep(0,15), \r\n                   rep(0,18), 0, H_month$t_EM_cor[7], H_month$t_MM[7], rep(0,15), \r\n                          \r\n                   rep(0,21), H_month$t_PP[8],0,0, rep(0,12), #Month 8\r\n                   rep(0,21), 0, H_month$t_EE[8],0, rep(0,12), \r\n                   rep(0,21), 0, H_month$t_EM_cor[8], H_month$t_MM[8], rep(0,12), \r\n                          \r\n                   rep(0,24), H_month$t_PP[9],0,0, rep(0,9), #Month 9\r\n                   rep(0,24), 0, H_month$t_EE[9],0, rep(0,9), \r\n                   rep(0,24), 0, H_month$t_EM_cor[9], H_month$t_MM[9], rep(0,9), \r\n                          \r\n                   rep(0,27), H_month$t_PP[10],0,0, rep(0,6), #Month 10\r\n                   rep(0,27), 0, H_month$t_EE[10],0,rep(0,6), \r\n                   rep(0,27), 0, H_month$t_EM_cor[10], H_month$t_MM[10], rep(0,6), \r\n                          \r\n                   rep(0,30), H_month$t_PP[11],0,0, rep(0,3), #Month 11\r\n                   rep(0,30), 0, H_month$t_EE[11],0,rep(0,3), \r\n                   rep(0,30), 0, H_month$t_EM_cor[11], H_month$t_MM[11], rep(0,3)),byrow=T,nrow=36,ncol=36,dimnames=list(x=c(""1P"",""1E"",""1M"",""2P"",""2E"",""2M"",""3P"",""3E"",""3M"",""4P"",""4E"",""4M"",""5P"",""5E"",""5M"",""6P"",""6E"",""6M"",""7P"",""7E"",""7M"",""8P"",""8E"",""8M"",""9P"",""9E"",""9M"",""10P"",""10E"",""10M"",""11P"",""11E"",""11M"",""12P"",""12E"",""12M""),y=c(""1P"",""1E"",""1M"",""2P"",""2E"",""2M"",""3P"",""3E"",""3M"",""4P"",""4E"",""4M"",""5P"",""5E"",""5M"",""6P"",""6E"",""6M"",""7P"",""7E"",""7M"",""8P"",""8E"",""8M"",""9P"",""9E"",""9M"",""10P"",""10E"",""10M"",""11P"",""11E"",""11M"",""12P"",""12E"",""12M"")))\r\n\r\n#Transition matrix needs to be corrected for reducible transitions by inserting very small values as placeholder (c = 1e-10 /month)\r\n#Medusae\r\nT_high[8,5] <- 1e-10\r\nT_high[17,14] <- 1e-10\r\nT_high[20,17] <- 1e-10\r\nT_high[23,20] <- 1e-10\r\nT_high[26,23] <- 1e-10\r\nT_high[29,26] <- 1e-10\r\nT_high[32,29] <- 1e-10\r\nT_high[35,32] <- 1e-10\r\nT_high[2,35] <- 1e-10\r\n\r\n#Fertility matrix\r\nF_high <- matrix(c(rep(0,33),H_month$F_PP_cor[12],0,H_month$F_MP_cor[12],  #Month 12\r\n                   rep(0,33),H_month$F_PE_cor[12],0,0,\r\n                   rep(0,33),H_month$F_PM_cor[12],0,0,\r\n                         \r\n                   H_month$F_PP_cor[1],0,H_month$F_MP_cor[1], rep(0,33), #Month 1\r\n                   H_month$F_PE_cor[1],0,0,rep(0,33), \r\n                   H_month$F_PM_cor[1],0,0, rep(0,33), \r\n                         \r\n                   rep(0,3), H_month$F_PP_cor[2],0,H_month$F_MP_cor[2], rep(0,30),  #Month 2\r\n                   rep(0,3), H_month$F_PE_cor[2],0,0, rep(0,30), \r\n                   rep(0,3), H_month$F_PM_cor[2],0,0, rep(0,30), \r\n                         \r\n                   rep(0,6), H_month$F_PP_cor[3],0,H_month$F_MP_cor[3], rep(0,27),  #Month 3\r\n                   rep(0,6), H_month$F_PE_cor[3],0,0,rep(0,27), \r\n                   rep(0,6), H_month$F_PM_cor[3],0,0, rep(0,27), \r\n                         \r\n                   rep(0,9), H_month$F_PP_cor[4],0,H_month$F_MP_cor[4], rep(0,24),  #Month 4\r\n                   rep(0,9), H_month$F_PE_cor[4],0,0, rep(0,24), \r\n                   rep(0,9), H_month$F_PM_cor[4],0,0, rep(0,24), \r\n                         \r\n                   rep(0,12), H_month$F_PP_cor[5],0,H_month$F_MP_cor[5], rep(0,21), #Month 5\r\n                   rep(0,12), H_month$F_PE_cor[5],0,0,rep(0,21), \r\n                   rep(0,12), H_month$F_PM_cor[5],0,0,rep(0,21), \r\n                         \r\n                   rep(0,15), H_month$F_PP_cor[6],0,H_month$F_MP_cor[6], rep(0,18), #Month 6\r\n                   rep(0,15), H_month$F_PE_cor[6],0,0,rep(0,18), \r\n                   rep(0,15), H_month$F_PM_cor[6],0,0,rep(0,18), \r\n                         \r\n                   rep(0,18), H_month$F_PP_cor[7],0,H_month$F_MP_cor[7], rep(0,15), #Month 7\r\n                   rep(0,18), H_month$F_PE_cor[7],0,0,rep(0,15), \r\n                   rep(0,18), H_month$F_PM_cor[7],0,0,rep(0,15), \r\n                         \r\n                   rep(0,21), H_month$F_PP_cor[8],0,H_month$F_MP_cor[8], rep(0,12), #Month 8\r\n                   rep(0,21), H_month$F_PE_cor[8],0,0,rep(0,12), \r\n                   rep(0,21), H_month$F_PM_cor[8],0,0,rep(0,12), \r\n                         \r\n                   rep(0,24), H_month$F_PP_cor[9],0,H_month$F_MP_cor[9], rep(0,9), #Month 9\r\n                   rep(0,24), H_month$F_PE_cor[9],0,0,rep(0,9), \r\n                   rep(0,24), H_month$F_PM_cor[9],0,0,rep(0,9), \r\n                         \r\n                   rep(0,27), H_month$F_PP_cor[10],0,H_month$F_MP_cor[10], rep(0,6), #Month 10\r\n                   rep(0,27), H_month$F_PE_cor[10],0,0,rep(0,6), \r\n                   rep(0,27), H_month$F_PM_cor[10],0,0,rep(0,6), \r\n                         \r\n                   rep(0,30), H_month$F_PP_cor[11],0,H_month$F_MP_cor[11], rep(0,3), #Month 11\r\n                   rep(0,30), H_month$F_PE_cor[11],0,0,rep(0,3), \r\n                   rep(0,30), H_month$F_PM_cor[11],0,0,rep(0,3)),byrow=T,nrow=36,ncol=36,dimnames=list(x=c(""1P"",""1E"",""1M"",""2P"",""2E"",""2M"",""3P"",""3E"",""3M"",""4P"",""4E"",""4M"",""5P"",""5E"",""5M"",""6P"",""6E"",""6M"",""7P"",""7E"",""7M"",""8P"",""8E"",""8M"",""9P"",""9E"",""9M"",""10P"",""10E"",""10M"",""11P"",""11E"",""11M"",""12P"",""12E"",""12M""),y=c(""1P"",""1E"",""1M"",""2P"",""2E"",""2M"",""3P"",""3E"",""3M"",""4P"",""4E"",""4M"",""5P"",""5E"",""5M"",""6P"",""6E"",""6M"",""7P"",""7E"",""7M"",""8P"",""8E"",""8M"",""9P"",""9E"",""9M"",""10P"",""10E"",""10M"",""11P"",""11E"",""11M"",""12P"",""12E"",""12M"")))\r\n\r\nA_high <- T_high + F_high\r\n\r\nprint(A_high,10)\r\n\r\n################\r\n#POPULATION GROWTH RATES OF UNSCALED MATRICES\r\n\r\nlibrary(popbio)\r\ncitation(\'popbio\')\r\n\r\n#Calculate monthly population growth rate lambda (dominant eigenvalue): n(t+1) x lamda = A x n(t)\r\nlambda(A_low)\r\n#1.326167 /month\r\nlambda(A_high)\r\n#2.880539 /month\r\n\r\n#Sensitivity analysis to explore which transition is responsible for unrealistically high values for lambda\r\nsens_low <- sensitivity(A_low,zero=TRUE)\r\nprint(sens_low)\r\n#Under low food conditions, lambda is most sensitive to changes in asexual reproduction rates (F_PP, F_PE in March/April) and survival of polyps (t_PP throughout the year)\r\nsens_high <- sensitivity(A_high,zero=TRUE)\r\nprint(sens_high)\r\n#Under high food conditions, lambda is most sensitive to changes in survial of medusae (t_MM in May/June/July) and survival of polyps (t_PP throughout the year)\r\n#Overlap in sensitivities of lambda between low and high food matrix points out polyp survival as major transition causing high values for lambda\r\n#As polyp survival depends directly on the absorbing stage transition from larva to polyp (t_LP) which was measured under optimum (lab) conditions and is likely to show much lower rates in the field (cf. Xie, Fan, Wang, & Chen 2015), we scale both matrices for t_LP\r\n\r\n################\r\n#MATRIX SCALING\r\n#Calibration: Assumption of stable jellyfish population under low food conditions, i.e. lambda = 1 /month = 1 /year (field reference: Kertinge Nor, Denmark, where a food limited jellyfish population with similar abundances reappears every year; cf. Goldstein & Riisgrd, 2016) \r\n#Scale both low and high food matrix with calibrated monthly (Jan to Dec) transition probabilities from polyp to medusa stage t_MP_cal (= t_LM_cor x t_LP_cor), where all values except 0 for t_LP_cor = 0.00103431419, as obtained via bootstrapping for low food lambda = 1\r\n\r\n#Low food matrix: Replace values (matrix[x,y] = value) in the fertility matrix by F_MP_cal\r\nprint(F_low)\r\nF_low[1,36] <- L_month$F_MP_cal[12]\r\nF_low[4,3] <- L_month$F_MP_cal[1]\r\nF_low[7,6] <- L_month$F_MP_cal[2]\r\nF_low[10,9] <- L_month$F_MP_cal[3]\r\nF_low[13,12] <- L_month$F_MP_cal[4]\r\nF_low[16,15] <- L_month$F_MP_cal[5]\r\nF_low[19,18] <- L_month$F_MP_cal[6]\r\nF_low[22,21] <- L_month$F_MP_cal[7]\r\nF_low[25,24] <- L_month$F_MP_cal[8]\r\nF_low[28,27] <- L_month$F_MP_cal[9]\r\nF_low[31,30] <- L_month$F_MP_cal[10]\r\nF_low[34,33] <- L_month$F_MP_cal[11]\r\nprint(F_low)\r\n\r\nA_low_cal <- T_low + F_low\r\n\r\n#High food matrix: As the difference between larval transition probabilities (t_LP_cor) under low and high food conditions is small (0.9062943 - 0.8479299 = 0.0583644, i.e. ~6 %), we assume the same value for t_LP_cor (= = 0.00103431419) as for the calibrated low food condition to scale the high food matrix\r\nprint(F_high)\r\nF_high[1,36] <- H_month$F_MP_cal[12]\r\nF_high[4,3] <- H_month$F_MP_cal[1]\r\nF_high[7,6] <- H_month$F_MP_cal[2]\r\nF_high[10,9] <- H_month$F_MP_cal[3]\r\nF_high[13,12] <- H_month$F_MP_cal[4]\r\nF_high[16,15] <- H_month$F_MP_cal[5]\r\nF_high[19,18] <- H_month$F_MP_cal[6]\r\nF_high[22,21] <- H_month$F_MP_cal[7]\r\nF_high[25,24] <- H_month$F_MP_cal[8]\r\nF_high[28,27] <- H_month$F_MP_cal[9]\r\nF_high[31,30] <- H_month$F_MP_cal[10]\r\nF_high[34,33] <- H_month$F_MP_cal[11]\r\nprint(F_high)\r\n\r\nA_high_cal <- T_high + F_high\r\n\r\n################\r\n#PROPERTIES OF SCALED MATRICES\r\n\r\nlibrary(popdemo)\r\ncitation(\'popdemo\')\r\n\r\nis.matrix_irreducible(A_low_cal) #TRUE\r\nis.matrix_irreducible(A_high_cal) #TRUE\r\n#Matrices are irreducible\r\n\r\nis.matrix_ergodic(A_low_cal,return.eigvec=T) #FALSE\r\nis.matrix_ergodic(A_high_cal,return.eigvec=T) #FALSE\r\n#Matrices are only weakly ergodic (due to life cycle dynamics)\r\n\r\nis.matrix_primitive(A_low_cal) #FALSE\r\nis.matrix_primitive(A_high_cal) #FALSE\r\n#Matrices are non-primitive (leads to cycles in population dynamics)\r\n\r\n\r\n######################\r\n# MATRIX PROJECTIONs #\r\n######################\r\n\r\n#POPULATION-LEVEL METRICS OF AURELIA AURITA (SENSU STRICTO) UNDER LOW AND HIGH FOOD CONDITIONS UNDER PRESENT WATER TEMPERATURES (PWT)\r\n\r\n#Calculate monthly population growth rate lambda (dominant eigenvalue): n(t+1) x lamda = A x n(t)\r\nprint(lambda(A_low_cal),10) #1 /month = 1 /year\r\nprint(lambda(A_high_cal),10) #1.627692287 /month [= 1.627692287^12] = 345.834 /year\r\n\r\n#Population projection\r\npop.projection(A_low_cal,n=c(1,rep(0,35)),iterations=120)\r\npop.projection(A_high_cal,n=c(1,rep(0,35)),iterations=120)\r\n\r\n#Stable stage distribution (right eigenvector n(t+1); proportion of individuals when projection model has reached stable stage, sum = 1)\r\n#LOW FOOD\r\nw_L <- round(stable.stage(A_low_cal),digits=10)\r\nP_L <- as.vector(w_L[c(1,4,7,10,13,16,19,22,25,28,31,34)])\r\nE_L <- as.vector(w_L[c(2,5,8,11,14,17,20,23,26,29,32,35)])\r\nM_L <- as.vector(w_L[c(3,6,9,12,15,18,21,24,27,30,33,36)])\r\n#Sum for each stage (during one year)\r\nw_P<-sum(w_L[c(1,4,7,10,13,16,19,22,25,28,31,34)]) #[1] 0.9446935\r\nw_E<-sum(w_L[c(2,5,8,11,14,17,20,23,26,29,32,35)]) #[1] 0.03903637\r\nw_M<-sum(w_L[c(3,6,9,12,15,18,21,24,27,30,33,36)]) #[1] 0.01627008\r\n#HIGH FOOD\r\nw_H <- round(stable.stage(A_high_cal),digits=10)\r\nP_H <- as.vector(w_H[c(1,4,7,10,13,16,19,22,25,28,31,34)])\r\nE_H <- as.vector(w_H[c(2,5,8,11,14,17,20,23,26,29,32,35)])\r\nM_H <- as.vector(w_H[c(3,6,9,12,15,18,21,24,27,30,33,36)])\r\n#Sum for each stage (during one year)\r\nw_P<-sum(w_H[c(1,4,7,10,13,16,19,22,25,28,31,34)]) #[1] 0.7608098\r\nw_E<-sum(w_H[c(2,5,8,11,14,17,20,23,26,29,32,35)]) #[1] 0.03844926\r\nw_M<-sum(w_H[c(3,6,9,12,15,18,21,24,27,30,33,36)]) #[1] 0.2007409\r\n#Increased food availability leads to more medusae and less polyps\r\n\r\n#Damping ratio (time until stable stage distribution)\r\neigen.analysis(A_low_cal,zero=TRUE)$damping.ratio #58.61665 months = 4.884721 yrs\r\neigen.analysis(A_high_cal,zero=TRUE)$damping.ratio #2.80158 months\r\n#Stable stage is reached 21x faster under high compared to low food conditions\r\n\r\n#Reproductive value (left eigenvector n(t); relative contribution of an individual in a given stage to future generations)\r\n#LOW FOOD\r\nv_L <- reproductive.value(A_low_cal)\r\n#Sum for each stage (during one year), scaled to polyp stage P (=1)\r\nscale_P<-sum(v_L[c(1,4,7,10,13,16,19,22,25,28,31,34)])\r\nv_P<-sum(v_L[c(1,4,7,10,13,16,19,22,25,28,31,34)])/scale_P #[1] 1\r\nv_E<-sum(v_L[c(2,5,8,11,14,17,20,23,26,29,32,35)])/scale_P #[1] 0.03321923\r\nv_M<-sum(v_L[c(3,6,9,12,15,18,21,24,27,30,33,36)])/scale_P #[1] 0.423033\r\n#HIGH FOOD\r\nv_H <- reproductive.value(A_high_cal)\r\n#Sum for each stage (during one year), scaled to polyp stage P (=1)\r\nscale_P<-sum(v_H[c(1,4,7,10,13,16,19,22,25,28,31,34)])\r\nv_P<-sum(v_H[c(1,4,7,10,13,16,19,22,25,28,31,34)])/scale_P #[1] 1\r\nv_E<-sum(v_H[c(2,5,8,11,14,17,20,23,26,29,32,35)])/scale_P #[1] 0.2694417\r\nv_M<-sum(v_H[c(3,6,9,12,15,18,21,24,27,30,33,36)])/scale_P #[1] 9.128521\r\n#Under low food conditions, the polyp stage has a maximum reproductive value compared to other stages. Under high food conditions, medusae contribute 9.1-fold more to lambda than polyps do.\r\n\r\n#Net reproductive rate (average number of offspring by which an individual will be replaced by the end of its life)\r\nnet.reproductive.rate(A_low_cal,F_low) #1 /ind.\r\nnet.reproductive.rate(A_high_cal,F_high) #56.95205 /ind.\r\n#The net reproductive rate increases by a factor 57 in response to increased food availability.\r\n\r\n#Generation time (average time between two consecutive generations)\r\ngeneration.time(A_low_cal,F_low) #200.0032 months = 16.7 years (i.e. it takes 16.7 years until polyp has produced a new polyp)\r\ngeneration.time(A_high_cal,F_high) #8.297444 months\r\n#The generation time is dramatically shortened for increased food availability. \r\n\r\n#Life expectancy (i.e., mean time to death)\r\n#LOW FOOD\r\nfund_L <- fundamental.matrix(A_low_cal,F_low)\r\n#Life expectancy in each stage\r\nfund_L$meaneta\r\nexp_P <- fund_L$meaneta[c(1,4,7,10,13,16,19,22,25,28,31,34)]\r\nexp_E <- fund_L$meaneta[c(2,5,8,11,14,17,20,23,26,29,32,35)]\r\nexp_M <- fund_L$meaneta[c(3,6,9,12,15,18,21,24,27,30,33,36)]\r\n#Yearly mean life expectancy of each stage\r\nmean(exp_P) #394.4752 months = 32.87293 years\r\nmean(exp_E) #1.710638 months\r\nmean(exp_M) #3.081845 months\r\n#HIGH FOOD\r\nfund_H <- fundamental.matrix(A_high_cal,F_high)\r\n#Life expectancy in each stage\r\nfund_H$meaneta\r\nexp_P <- fund_H$meaneta[c(1,4,7,10,13,16,19,22,25,28,31,34)]\r\nexp_E <- fund_H$meaneta[c(2,5,8,11,14,17,20,23,26,29,32,35)]\r\nexp_M <- fund_H$meaneta[c(3,6,9,12,15,18,21,24,27,30,33,36)]\r\n#Yearly mean life expectancy of each stage\r\nmean(exp_P) #86.674 months = 7.222833 years\r\nmean(exp_E) #2.868061 months\r\nmean(exp_M) #8.044142 months\r\n#Polyps show decreased life expectancy under increased food availability (caloric restriction/trade-off between survival and reproduction?), while the life span of medusae is extended compared to low food levels\r\n\r\n#Sensitivity of lambda to perturbations in transition probabilities and fecundities \r\n#LOW FOOD\r\nsens_low <- sensitivity(A_low_cal,zero=TRUE)\r\nmax(sens_low) #0.08418367 for t_PP from Feb to Mar and constantly ~0.08 throughout the year (i.e., if polyp survival increases by 1, then lambda increases by 8 %), 0.02 for F_PE from Mar to May\r\n#HIGH FOOD\r\nsens_high <- sensitivity(A_high_cal,zero=TRUE) \r\nmax(sens_high) #0.3195281 for t_MM from Jun to Jul (i.e. if medusa survival increases by 1, then lambda increases by 31 %), 0.07 to 0.16 for t_MM from Apr to Oct,  0.06 to 0.13 for t_PP from Oct to Apr, 0.02 to 0.03 for F_PE from Mar to May \r\n#Lambda is most sensitive to changes in polyp survival under low food conditions, and most sensitive to changes in medusa survival under high food conditions\r\n\r\n#Life table response experiment (LTRE) for comparing sensitivity of lambda under high versus low food conditions\r\n#Treatment matrix: high food, reference: low food\r\nA_response <- LTRE(A_high_cal,A_low_cal)\r\nmax(A_response) #0.1262821 for F_PE x t_EM from Apr to May, 0.07 for F_PE x t_EM from Mar to Apr, 0.08 for t_MM from Apr to May, 0.05 to 0.07 for F_ML x t_LP from Aug to Dec\r\n#Increased food availability leads to 7.0 to 12.6% increased probability of medusae developing via asexual reproduction of polyps from March to May, 7.7% increased medusa survival from April to May and 5.1 to 6.8% increased probability of polyps developing via sexual reproduction of medusae from August to December\r\n\r\nlibrary(Matrix)\r\nas(round(A_response,5), ""sparseMatrix"")\r\n\r\n################\r\n\r\n#POPULATION-LEVEL METRICS OF AURELIA AURITA (SENSU STRICTO) UNDER LOW AND HIGH FOOD CONDITIONS IN A WINTER WARMING (WW) SCENARIO\r\n\r\n#Simulated increase in present water temperature from 5 C to 10 C during the cold season is predicted to result in extended strobilation periods (by at least one month) and increased ephyra production per polyp (by factor 1.8; cf. Holst, 2012)\r\n#Parameterization of population matrices for low and high food conditions with corresponding winter warming (WW) variables\r\n\r\n#LOW FOOD\r\n#Transition matrix: t_EE_ww and t_EM_cor_ww\r\nT_low_ww <- T_low\r\nT_low_ww[11,8] <- L_month$t_EE_WW[3] \r\nT_low_ww[12,8] <- L_month$t_EM_cor_WW[3] \r\n#Fertility matrix: F_PE_cor_ww and F_PM_cor_ww\r\nF_low_ww <- F_low\r\nF_low_ww[8,4] <- L_month$F_PE_cor_WW[2] \r\nF_low_ww[11,7] <- L_month$F_PE_cor_WW[3] \r\nF_low_ww[14,10] <- L_month$F_PE_cor_WW[4]\r\nF_low_ww[12,7] <- L_month$F_PM_cor_WW[3]\r\nF_low_ww[15,10] <- L_month$F_PM_cor_WW[4]\r\n\r\nA_low_ww <- T_low_ww + F_low_ww\r\n\r\n#HIGH FOOD\r\n#Transition matrix: t_EE_ww and t_EM_cor_ww \r\nT_high_ww <- T_high\r\nT_high_ww[8,5] <- H_month$t_EE_WW[2] \r\nT_high_ww[9,5] <- H_month$t_EM_cor_WW[2] \r\n#Fertility matrix: F_PE_cor_ww and F_PM_cor_ww\r\nF_high_ww <- F_high\r\nF_high_ww[2,34] <- H_month$F_PE_cor_WW[12] \r\nF_high_ww[5,1] <- H_month$F_PE_cor_WW[1] \r\nF_high_ww[8,4] <- H_month$F_PE_cor_WW[2] \r\nF_high_ww[11,7] <- H_month$F_PE_cor_WW[3] \r\nF_high_ww[14,10] <- H_month$F_PE_cor_WW[4]\r\nF_high_ww[6,1] <- H_month$F_PM_cor_WW[1]\r\nF_high_ww[9,4] <- H_month$F_PM_cor_WW[2]\r\nF_high_ww[12,7] <- H_month$F_PM_cor_WW[3]\r\nF_high_ww[15,10] <- H_month$F_PM_cor_WW[4]\r\n\r\nA_high_ww <- T_high_ww + F_high_ww\r\n\r\n#Calculate monthly population growth rate lambda (dominant eigenvalue): n(t+1) x lamda = A x n(t)\r\nlambda(A_low_ww) #1.005023 /month = 1.061969 /year\r\n#Lambda before scenario: 1/month = 1/year -> Relative difference: 1.061969 /year - 1 /year = 0.061969 /year, i.e. 6.2% /year\r\nlambda(A_high_ww) #1.732095 /month = 729.2232 /year\r\n#Lambda before scenario: 1.627692 /month = 345.8333 /year -> Relative difference: 729.2232 /year - 345.8333 /year = 383.3899 /year, i.e. 110.9%\r\n\r\n#Population projection\r\npop.projection(A_low_ww,n=c(1,rep(0,35)),iterations=120)\r\npop.projection(A_high_ww,n=c(1,rep(0,35)),iterations=120)\r\n\r\n#Stable stage distribution (right eigenvector n(t+1); proportion of individuals when projection model has reached stable stage, sum = 1)\r\n#LOW FOOD\r\nw_L_ww <- round(stable.stage(A_low_ww),digits=10)\r\nP_L_ww <- as.vector(w_L_ww[c(1,4,7,10,13,16,19,22,25,28,31,34)])\r\nE_L_ww <- as.vector(w_L_ww[c(2,5,8,11,14,17,20,23,26,29,32,35)])\r\nM_L_ww <- as.vector(w_L_ww[c(3,6,9,12,15,18,21,24,27,30,33,36)])\r\n#Sum for each stage (during one year)\r\nw_P_ww<-sum(w_L_ww[c(1,4,7,10,13,16,19,22,25,28,31,34)]) #[1] 0.8388422 (before scenario: 0.9446935)\r\nw_E_ww<-sum(w_L_ww[c(2,5,8,11,14,17,20,23,26,29,32,35)]) #[1] 0.1175788 (before scenario: 0.03903637)\r\nw_M_ww<-sum(w_L_ww[c(3,6,9,12,15,18,21,24,27,30,33,36)]) #[1] 0.04357904 (before scenario: 0.01627008)\r\n#Under low food conditions, winter warming shifts stable population structure to less polyps (decrease by 0.9446935 - 0.8388422 = 0.1058513 /year = 10.6% /year), more ephyrae (increase by 0.1175788 - 0.03903637 = 0.07854243 /year = 7.9% /year) and more medusae (increase by 0.04357904 - 0.01627008 = 0.02730896 /year = 2.7% /year)\r\n#HIGH FOOD\r\nw_H_ww <- round(stable.stage(A_high_ww),digits=10)\r\nP_H_ww <- as.vector(w_H_ww[c(1,4,7,10,13,16,19,22,25,28,31,34)])\r\nE_H_ww <- as.vector(w_H_ww[c(2,5,8,11,14,17,20,23,26,29,32,35)])\r\nM_H_ww <- as.vector(w_H_ww[c(3,6,9,12,15,18,21,24,27,30,33,36)])\r\n#Sum for each stage (during one year)\r\nw_P_ww<-sum(w_H_ww[c(1,4,7,10,13,16,19,22,25,28,31,34)]) #[1] 0.6038405 (before scenario: 0.7608098)\r\nw_E_ww<-sum(w_H_ww[c(2,5,8,11,14,17,20,23,26,29,32,35)]) #[1] 0.02133293 (before scenario: 0.03844926)\r\nw_M_ww<-sum(w_H_ww[c(3,6,9,12,15,18,21,24,27,30,33,36)]) #[1] 0.3748265 (before scenario: 0.2007409)\r\n#Under low food conditions, warming shifts stable population structure to less polyps (decrease by 0.7608098 - 0.6038405 = 0.1569693 /year = 15.7% /year), less ephyrae (decrease by 0.03844926 - 0.02133293 = 0.01711633 /year = 1.7% /year) and more medusae (increase by 0.3748265 - 0.2007409 = 0.1740856 /year = 17.4% /year)\r\n#Increased medusa densities (and decreased polyp densities) in response to winter warming under both food conditions\r\n\r\n#Damping ratio (time until stable stage distribution)\r\neigen.analysis(A_low_ww,zero=TRUE)$damping.ratio #59.22334 months = 4.935278 yrs\r\neigen.analysis(A_high_ww,zero=TRUE)$damping.ratio #3.1725 months\r\n#Similar convergence times compared to present water temperatures\r\n\r\n#Reproductive value (left eigenvector n(t); relative contribution of an individual in a given stage to future generations)\r\n#LOW FOOD\r\nv_L_ww <- reproductive.value(A_low_ww)\r\n#Sum for each stage (during one year), scaled to polyp stage P (=1)\r\nscale_P_ww<-sum(v_L_ww[c(1,4,7,10,13,16,19,22,25,28,31,34)])\r\nv_P_ww<-sum(v_L_ww[c(1,4,7,10,13,16,19,22,25,28,31,34)])/scale_P_ww #[1] 1 (before scenario: 1)\r\nv_E_ww<-sum(v_L_ww[c(2,5,8,11,14,17,20,23,26,29,32,35)])/scale_P_ww #[1] 8.528833 (before scenario: 0.03321923)\r\nv_M_ww<-sum(v_L_ww[c(3,6,9,12,15,18,21,24,27,30,33,36)])/scale_P_ww #[1] 2.944482e-07 (before scenario: 0.423033)\r\n#Under low food condtions, the reproductive value of ephyrae increases by factor (8.528833 / 0.03321923 =) 257 in response to winter warming, while the contribution of medusae decreases to near zero.\r\n#HIGH FOOD\r\nv_H_ww <- reproductive.value(A_high_ww)\r\n#Sum for each stage (during one year), scaled to polyp stage P (=1)\r\nscale_P_ww<-sum(v_H_ww[c(1,4,7,10,13,16,19,22,25,28,31,34)])\r\nv_P_ww<-sum(v_H_ww[c(1,4,7,10,13,16,19,22,25,28,31,34)])/scale_P_ww #[1] 1 (before scenario: 1)\r\nv_E_ww<-sum(v_H_ww[c(2,5,8,11,14,17,20,23,26,29,32,35)])/scale_P_ww #[1] 0.1760657 (before scenario: 0.2694417)\r\nv_M_ww<-sum(v_H_ww[c(3,6,9,12,15,18,21,24,27,30,33,36)])/scale_P_ww #[1] 7.427529 (before scenario: 9.128521)\r\n#Under high food levels, winter warming decreases reproductive values of ephyrae and medusae (by factor 0.2694417 / 0.1760657 = 1.5 and 9.128521 / 7.427529 = 1.2, respectively).\r\n\r\n#Net reproductive rate (average number of offspring by which an individual will be replaced by the end of its life)\r\nnet.reproductive.rate(A_low_ww,F_low_ww) #1.750918; before scenario: 1 -> Relative increase by (1.750918 - 1) / 1 = 0.750918 = 75.1% \r\nnet.reproductive.rate(A_high_ww,F_high_ww) #80.87851; before scenario: 56.95205 -> Relative increase under high food: (80.87851 - 56.95205) / 56.95205 = 0.4201159 = 42.0%  \r\n#Increased net reproductive rates due to winter warming, especially pronounced under low food conditions\r\n\r\n#Generation time (average time between two consecutive generations)\r\ngeneration.time(A_low_ww,F_low_ww) #111.7914 months = 9.3 years (before scenario: 200.0032 months = 16.7 years)\r\n#Decrease in generation time as a consequence of winter warming: (200.0032 - 111.7914 months =) 88.2118 months = 7.4 years\r\ngeneration.time(A_high_ww,F_high_ww) #7.996898 months (before scenario: 8.297444 months)\r\n#Decrease in generation time as a consequence of winter warming (8.297444 - 7.996898 =) 0.300546 months = 9 days\r\n#Shortened generation time under both low and high food conditions as a consequence of winter warming, especially pronounced under low compared to high food levels \r\n\r\n#Life expectancy (i.e., mean time to death)\r\n#LOW FOOD\r\nfund_L <- fundamental.matrix(A_low_cal,F_low)\r\n#Life expectancy in each stage\r\nfund_L$meaneta\r\nexp_P <- fund_L$meaneta[c(1,4,7,10,13,16,19,22,25,28,31,34)]\r\nexp_E <- fund_L$meaneta[c(2,5,8,11,14,17,20,23,26,29,32,35)]\r\nexp_M <- fund_L$meaneta[c(3,6,9,12,15,18,21,24,27,30,33,36)]\r\n#Yearly mean life expectancy of each stage\r\nmean(exp_P) #394.4752 months = 32.87293 years\r\nmean(exp_E) #1.710638 months\r\nmean(exp_M) #3.081845 months\r\n#HIGH FOOD\r\nfund_H <- fundamental.matrix(A_high_cal,F_high)\r\n#Life expectancy in each stage\r\nfund_H$meaneta\r\nexp_P <- fund_H$meaneta[c(1,4,7,10,13,16,19,22,25,28,31,34)]\r\nexp_E <- fund_H$meaneta[c(2,5,8,11,14,17,20,23,26,29,32,35)]\r\nexp_M <- fund_H$meaneta[c(3,6,9,12,15,18,21,24,27,30,33,36)]\r\n#Yearly mean life expectancy of each stage\r\nmean(exp_P) #86.674 months = 7.222833 years\r\nmean(exp_E) #2.868061 months\r\nmean(exp_M) #8.044142 months\r\n#Polyps show decreased life expectancy under increased food availability (caloric restriction/trade-off between survival and reproduction?), while the life span of medusae is extended compared to low food levels\r\n\r\n#Life expectancy (i.e., mean time to death)\r\n#LOW FOOD\r\nfund_L_ww <- fundamental.matrix(A_low_ww,F_low_ww)\r\n#Yearly mean ife expectancy of each stage \r\nexp_P_ww <- mean(fund_L_ww$meaneta[c(1,4,7,10,13,16,19,22,25,28,31,34)]) #394.4752 months = 32.87293 years (before scenario: 394.4752 months)\r\nexp_E_ww <- mean(fund_L_ww$meaneta[c(2,5,8,11,14,17,20,23,26,29,32,35)]) #2.101008 months (before scenario: 1.710638 months)\r\nexp_M_ww <- mean(fund_L_ww$meaneta[c(3,6,9,12,15,18,21,24,27,30,33,36)]) #3.081845 months (before scenario: 3.081845 months)\r\n#Under low food conditions, winter warming increases the life expectancy of ephyrae by (2.101008 - 1.710638 =) 0.39037 months = 12 days\r\n#HIGH FOOD\r\nfund_H_ww <- fundamental.matrix(A_high_ww,F_high_ww)\r\n#Yearly mean ife expectancy of each stage \r\nexp_P_ww <- mean(fund_H_ww$meaneta[c(1,4,7,10,13,16,19,22,25,28,31,34)]) #86.674 months = 7.222833 years (before scenario: 86.674 months)\r\nexp_E_ww <- mean(fund_H_ww$meaneta[c(2,5,8,11,14,17,20,23,26,29,32,35)]) #3.454794 months (before scenario: 2.868061 months)\r\nexp_M_ww <- mean(fund_H_ww$meaneta[c(3,6,9,12,15,18,21,24,27,30,33,36)]) #8.044142 months (before scenario: 8.044142 months)\r\n#Under high food conditions, winter warming increases the life expectancy of ephyrae by (3.454794 - 2.868061 =) = 0.586733 months = 18 days \r\n\r\n################\r\n\r\n#JELLYFISH LIFE HISTORIES IN A CHANGING ENVIRONMENT \r\n\r\n#Scenarios to explore the potential of distinct demographic rates in decreasing the population growth rate under high food conditions\r\n#Parameterization of population matrices for high food conditions with corresponding scenario variables (S1-S4) \r\n\r\n#Scenario 1: Decreased survival of polyps (t_PP = 2.3e-5; cf. Xie, Fan, Wang, & Chen 2015)   \r\nT_high_S1 <- T_high\r\nF_high_S1 <- F_high\r\n#Transition matrix: t_PP_S1\r\nT_high_S1[1,34] <- H_month$t_PP_S1[12]\r\nT_high_S1[4,1] <- H_month$t_PP_S1[1]\r\nT_high_S1[7,4] <- H_month$t_PP_S1[2]\r\nT_high_S1[10,7] <- H_month$t_PP_S1[3]\r\nT_high_S1[13,10] <- H_month$t_PP_S1[4]\r\nT_high_S1[16,13] <- H_month$t_PP_S1[5]\r\nT_high_S1[19,16] <- H_month$t_PP_S1[6]\r\nT_high_S1[22,19] <- H_month$t_PP_S1[7]\r\nT_high_S1[25,22] <- H_month$t_PP_S1[8]\r\nT_high_S1[28,25] <- H_month$t_PP_S1[9]\r\nT_high_S1[31,28] <- H_month$t_PP_S1[10]\r\nT_high_S1[34,31] <- H_month$t_PP_S1[11]\r\n#Fertility matrix: F_PP_cor_S1 and F_MP_cor_S1\r\nF_high_S1[1,34] <- H_month$F_PP_cor_S1[12]\r\nF_high_S1[4,1] <- H_month$F_PP_cor_S1[1]\r\nF_high_S1[7,4] <- H_month$F_PP_cor_S1[2]\r\nF_high_S1[10,7] <- H_month$F_PP_cor_S1[3]\r\nF_high_S1[13,10] <- H_month$F_PP_cor_S1[4]\r\nF_high_S1[16,13] <- H_month$F_PP_cor_S1[5]\r\nF_high_S1[19,16] <- H_month$F_PP_cor_S1[6]\r\nF_high_S1[22,19] <- H_month$F_PP_cor_S1[7]\r\nF_high_S1[25,22] <- H_month$F_PP_cor_S1[8]\r\nF_high_S1[28,25] <- H_month$F_PP_cor_S1[9]\r\nF_high_S1[31,28] <- H_month$F_PP_cor_S1[10]\r\nF_high_S1[34,31] <- H_month$F_PP_cor_S1[11]\r\nF_high_S1[1,36] <- H_month$F_MP_cor_S1[12]\r\nF_high_S1[4,3] <- H_month$F_MP_cor_S1[1]\r\nF_high_S1[7,6] <- H_month$F_MP_cor_S1[2]\r\nF_high_S1[10,9] <- H_month$F_MP_cor_S1[3]\r\nF_high_S1[13,12] <- H_month$F_MP_cor_S1[4]\r\nF_high_S1[16,15] <- H_month$F_MP_cor_S1[5]\r\nF_high_S1[19,18] <- H_month$F_MP_cor_S1[6]\r\nF_high_S1[22,21] <- H_month$F_MP_cor_S1[7]\r\nF_high_S1[25,24] <- H_month$F_MP_cor_S1[8]\r\nF_high_S1[28,27] <- H_month$F_MP_cor_S1[9]\r\nF_high_S1[31,30] <- H_month$F_MP_cor_S1[10]\r\nF_high_S1[34,33] <- H_month$F_MP_cor_S1[11]\r\n\r\nA_high_S1 <- T_high_S1 + F_high_S1\r\n\r\nlambda(A_high_S1) #1.443982 /month (before scenario: 1.627692 /month) \r\n#Low impact of polyp survival on growth rate of the jellyfish population under high food levels\r\n\r\n\r\n#Scenario 2: Reduced survival of ephyrae (t_EE = 0.01 /month; cf. Xie, Fan, Wang, & Chen 2015)\r\nT_high_S2 <- T_high\r\nF_high_S2 <- F_high\r\n#Transition matrix: t_EE_S2\r\nT_high_S2[5,2] <- H_month$t_EE_S2[1]\r\nT_high_S2[11,8] <- H_month$t_EE_S2[3]\r\nT_high_S2[14,11] <- H_month$t_EE_S2[4]\r\n#Fertility matrix: F_PE_cor_S2\r\nF_high_S2[2,34] <- H_month$F_PE_cor_S2[12]\r\nF_high_S2[5,1] <- H_month$F_PE_cor_S2[1]\r\nF_high_S2[8,4] <- H_month$F_PE_cor_S2[2]\r\nF_high_S2[11,7] <- H_month$F_PE_cor_S2[3]\r\nF_high_S2[14,10] <- H_month$F_PE_cor_S2[4]\r\nF_high_S2[17,13] <- H_month$F_PE_cor_S2[5]\r\nF_high_S2[20,16] <- H_month$F_PE_cor_S2[6]\r\nF_high_S2[23,19] <- H_month$F_PE_cor_S2[7]\r\nF_high_S2[26,22] <- H_month$F_PE_cor_S2[8]\r\nF_high_S2[29,25] <- H_month$F_PE_cor_S2[9]\r\nF_high_S2[32,28] <- H_month$F_PE_cor_S2[10]\r\nF_high_S2[35,31] <- H_month$F_PE_cor_S2[11]\r\n\r\nA_high_S2 <- T_high_S2 + F_high_S2\r\n\r\nlambda(A_high_S2) #1.623088 /month (before scenario: 1.627692 /month)\r\n#Negligible impact of ephyra survival on growth rate of the jellyfish population under high food levels\r\n\r\n\r\n#Scenario 3: Decreased transition probability from ephyra to medusa (t_EM = 0.0001)\r\nT_high_S3 <- T_high\r\nF_high_S3 <- F_high\r\n#Transition matrix: t_EM_cor_S3\r\nT_high_S3[3,35] <- H_month$t_EM_cor_S3[12] \r\nT_high_S3[6,2] <- H_month$t_EM_cor_S3[1]\r\nT_high_S3[9,5] <- H_month$t_EM_cor_S3[2] \r\nT_high_S3[12,8] <- H_month$t_EM_cor_S3[3]\r\nT_high_S3[15,11] <- H_month$t_EM_cor_S3[4]\r\nT_high_S3[18,14] <- H_month$t_EM_cor_S3[5] \r\nT_high_S3[21,17] <- H_month$t_EM_cor_S3[6] \r\nT_high_S3[24,20] <- H_month$t_EM_cor_S3[7]\r\nT_high_S3[27,23] <- H_month$t_EM_cor_S3[8]\r\nT_high_S3[30,26] <- H_month$t_EM_cor_S3[9]\r\nT_high_S3[33,29] <- H_month$t_EM_cor_S3[10]\r\nT_high_S3[36,32] <- H_month$t_EM_cor_S3[11]\r\n#Fertility matrix: F_PM_cor_S3\r\nF_high_S3[3,34] <- H_month$F_PM_cor_S3[12]\r\nF_high_S3[6,1] <- H_month$F_PM_cor_S3[1]\r\nF_high_S3[9,4] <- H_month$F_PM_cor_S3[2]\r\nF_high_S3[12,7] <- H_month$F_PM_cor_S3[3]\r\nF_high_S3[15,10] <- H_month$F_PM_cor_S3[4]\r\nF_high_S3[18,13] <- H_month$F_PM_cor_S3[5]\r\nF_high_S3[21,16] <- H_month$F_PM_cor_S3[6]\r\nF_high_S3[24,19] <- H_month$F_PM_cor_S3[7]\r\nF_high_S3[27,22] <- H_month$F_PM_cor_S3[8]\r\nF_high_S3[30,25] <- H_month$F_PM_cor_S3[9]\r\nF_high_S3[33,28] <- H_month$F_PM_cor_S3[10]\r\nF_high_S3[36,31] <- H_month$F_PM_cor_S3[11]\r\n\r\nA_high_S3 <- T_high_S3 + F_high_S3\r\n\r\nlambda(A_high_S3) #1.091374 /month (before scenario: 1.627692 /month)\r\n#Major impact of transition from ephyra to medusa on on growth rate of the jellyfish population under high food levels\r\n\r\n\r\n#Scenario 4: Reduced survival of medusae (t_MM = 0.1 /month; cf. Xie, Fan, Wang, & Chen 2015)\r\nT_high_S4 <- T_high\r\nF_high_S4 <- F_high\r\n#Transition matrix: t_MM_S4\r\nT_high_S4[3,36] <- H_month$t_MM_S4[12] \r\nT_high_S4[6,3] <- H_month$t_MM_S4[1]\r\nT_high_S4[9,6] <- H_month$t_MM_S4[2] \r\nT_high_S4[12,9] <- H_month$t_MM_S4[3]\r\nT_high_S4[15,12] <- H_month$t_MM_S4[4]\r\nT_high_S4[18,15] <- H_month$t_MM_S4[5] \r\nT_high_S4[21,18] <- H_month$t_MM_S4[6] \r\nT_high_S4[24,21] <- H_month$t_MM_S4[7]\r\nT_high_S4[27,24] <- H_month$t_MM_S4[8]\r\nT_high_S4[30,27] <- H_month$t_MM_S4[9]\r\nT_high_S4[33,30] <- H_month$t_MM_S4[10]\r\nT_high_S4[36,33] <- H_month$t_MM_S4[11] \r\n#Fertility matrix: t_EM_cor_S4 and F_PM_cor_S4\r\nT_high_S4[3,35] <- H_month$t_EM_cor_S4[12] \r\nT_high_S4[6,2] <- H_month$t_EM_cor_S4[1]\r\nT_high_S4[9,5] <- H_month$t_EM_cor_S4[2] \r\nT_high_S4[12,8] <- H_month$t_EM_cor_S4[3]\r\nT_high_S4[15,11] <- H_month$t_EM_cor_S4[4]\r\nT_high_S4[18,14] <- H_month$t_EM_cor_S4[5] \r\nT_high_S4[21,17] <- H_month$t_EM_cor_S4[6] \r\nT_high_S4[24,20] <- H_month$t_EM_cor_S4[7]\r\nT_high_S4[27,23] <- H_month$t_EM_cor_S4[8]\r\nT_high_S4[30,26] <- H_month$t_EM_cor_S4[9]\r\nT_high_S4[33,29] <- H_month$t_EM_cor_S4[10]\r\nT_high_S4[36,32] <- H_month$t_EM_cor_S4[11]\r\nF_high_S4[3,34] <- H_month$F_PM_cor_S4[12]\r\nF_high_S4[6,1] <- H_month$F_PM_cor_S4[1]\r\nF_high_S4[9,4] <- H_month$F_PM_cor_S4[2]\r\nF_high_S4[12,7] <- H_month$F_PM_cor_S4[3]\r\nF_high_S4[15,10] <- H_month$F_PM_cor_S4[4]\r\nF_high_S4[18,13] <- H_month$F_PM_cor_S4[5]\r\nF_high_S4[21,16] <- H_month$F_PM_cor_S4[6]\r\nF_high_S4[24,19] <- H_month$F_PM_cor_S4[7]\r\nF_high_S4[27,22] <- H_month$F_PM_cor_S4[8]\r\nF_high_S4[30,25] <- H_month$F_PM_cor_S4[9]\r\nF_high_S4[33,28] <- H_month$F_PM_cor_S4[10]\r\nF_high_S4[36,31] <- H_month$F_PM_cor_S4[11]\r\n\r\nA_high_S4 <- T_high_S4 + F_high_S4\r\n\r\nlambda(A_high_S4) #1.095039 /month (before scenario: 1.627692 /month)\r\n#Strong impact of medusa survival on growth rate of the jellyfish population under high food levels\r\n\r\n#Control of medusa survival or of the transition from ephyra to medusa is most sufficient to keep jellyfish populations under high food conditions in check\r\n\r\n\r\n\r\n##############\r\n# References #\r\n##############\r\n\r\n#Caswell, H. (2001). Matrix population models: construction, analysis, and interpretation (2nd ed.). Massachusetts, MA: Sinauer Associates.\r\n#Goldstein, J., & Riisgrd, H. U. (2016). Population dynamics and factors controlling somatic degrowth of the common jellyfish, Aurelia aurita, in a temperate semi-enclosed cove (Kertinge Nor, Denmark). Marine Biology, 163(2), 33-44.\r\n#Holst, S. (2012). Effects of climate warming on strobilation and ephyra production of North Sea scyphozoan jellyfish. Hydrobiologia, 690(1), 127-140.\r\n#Xie, C., Fan, M., Wang, X., & Chen, M. (2015). Dynamic model for life history of scyphozoa. PLoS ONE, 10(6), e0130669.\r\n']","Data from: Ecological drivers of jellyfish blooms  the complex life history of a 'well-known' medusa (Aurelia aurita) Jellyfish blooms are conspicuous demographic events with significant ecological and socio-economic impact. Despite worldwide concern about an increased frequency and intensity of such mass occurrences, predicting their booms and busts remains challenging.Forecasting how jellyfish populations may respond to environmental change requires considering their complex life histories. Metagenic life cycles, which include a benthic polyp stage, can boost jellyfish mass occurrences via asexual recruitment of pelagic medusae.Here we present stage-structured matrix population models with monthly, individual-based demographic rates of all life stages of the moon jellyfish Aurelia aurita L. (sensu stricto). We investigate the life stage-dynamics of these complex populations under low and high food conditions to illustrate how changes in medusa density depend on non-medusa stage dynamics.We show that increased food availability can be an important ecological driver of jellyfish mass occurrences, as it can temporarily shift the population structure from polyp- to medusa- dominated. Projecting populations for a winter warming scenario enhanced the booms and busts of jellyfish blooms.We identify demographic key variables that control the intensity and frequency of jellyfish blooms in response to environmental drivers such as habitat eutrophication and climate change. By contributing to an improved understanding of mass occurrence phenomena, our findings provide perspective for future management of ecosystem health.",1
Code from: Global analysis of trait-trait relationships within and between species,"Some robust trait-trait relationships between species, including the leaf economic spectrum (LES), are regarded as important plant strategies but whether these relationships truly represent plant strategies remains unclear.We propose a novel approach to distinguish trait-trait relationships between species that may represent plant strategies vs. those relationships that are due to common drivers, by comparing the direction and strength of intraspecific trait variation (ITV) vs. interspecific trait variation. We applied this framework using a unique global ITV database we compiled, which included eleven traits related to LES, size and roots, and observations from 2064 species occurring in 1068 communities across 19 countries.Generally, compared to between species, trait-trait relationships within species were much weaker or totally disappeared. Almost only within the LES traits, the between-species trait-trait relationships were translated into positive relationships within-species, which suggests they may represent plant strategies.Moreover, the frequent coincidental trait-trait relationships between species, driven by co-varying common drivers, imply that in future research, decoupling of trait-trait relationships should be seriously considered in model projections of ecosystem functioning. Our study emphasises the importance of describing the mechanisms behind trait-trait relationships, both between and within species, to deepen our understanding of general plant strategies.","['############################################################################~\r\n###### Analysis script associated with the manuscript: \r\n###### Global analysis of trait-trait relationships within and between species\r\n###### Authors: J. Zhou, E. Cieraad, P.M. Van Bodegom\r\n###### With this script and the datasets we provided, our findings in this manuscript can be replicated in three steps: \r\n###### 1. To build up a new ITV database with ITV, species mean trait and other species characteristic values calculated using trait-gradient analysis (by running section A and B);\r\n###### 2. To delete some unreliable ITV and species mean trait values to obtain a final ITV database (by running section C and minor manual edits);\r\n###### 3. To replicate results of ITV-ITV and Trait-trait relationships in our maintext from SMA regression (by running section D).\r\n###### Note: other pairwise results of ITV-ITV and trait-trait relationships indicated in Table S3 in Support Information can be easily obtained by editing the code we provided in section D. \r\n############################################################################~\r\n###### This script requires data files also available on Zenodo.\r\n###### 1. ""trait database.xlsx"" - raw database with plot based species trait data we compiled from our own collected datasets as well as other published datasets\r\n###### 2. ""raw ITV database.xlsx"" - this database compiled from the database obtained from section B and 2 extra datasets using the same trait-gradient analysis (see study 15 & 16 in Table S1 in Support Information)\r\n###### Note:The ""final ITV database.xlsx"" used in section D could be obtained from section C with minor manual edits which also indicated there and the remark column of ""raw ITV database.xlsx"".\r\n############################################################################~\r\n###### This script contains 4 sections\r\n###### A. Set up\r\n###### B. Loop preparing ITV dataset\r\n###### C. Data quality control method\r\n###### D. SMA analysis\r\n############################################################################~\r\n\r\n\r\n\r\n\r\n################################################################################################################~\r\n##### A. Set up ####\r\n################################################################################################################~\r\nlibrary(openxlsx)\r\nlibrary(dplyr)\r\ntraitdata <- read.xlsx(""trait database.xlsx"", sheet = 2, cols = c(1:15), na.strings = c(""na"", ""na "",""NA"")) #### this is the raw database with plot based species trait data we compiled from our own collected datasets as well as other published datasets\r\nstr(traitdata)\r\ntraitdata$species <- as.factor(traitdata$species)\r\ntraitdata$plot <- as.factor(traitdata$plot)\r\nnames(traitdata) # traits of interest are in column 5:15\r\npar(mfrow=c(2,1))\r\n\r\n# create an empty dataframe for the final output - start with only a list of the unique species - add rest after linear models per trait\r\nall.data = data.frame(""species""=sort(unique(traitdata$species)))\r\nall.data$species = as.character(all.data$species)\r\n\r\n\r\n################################################################################################################~\r\n#### B. Loop preparing ITV dataset per trait ####\r\n################################################################################################################~\r\n#### Loop consists of \r\n#### 1. Data check \r\n#### 2. Trait gradient analysis\r\n#### 3. Regression to calculate the slopes (b)- intraspecific variability per species\r\n\r\nfor(i in 5:15) { \r\n  traitname = names(traitdata)[i]  \r\n  print(paste0(""I\'m now working on trait "",traitname))\r\n  # select data for the trait\r\n  data = traitdata[, c(1:4, i)] \r\n  data = na.omit(data)\r\n  data = droplevels(data)  # drop the empty levels (that had NA\'s but were then dropped)\r\n\r\n##### 1. Data check ####\r\n  ## check if the trait data is normal distributed and if not, use log10 transform\r\n  # transform trait variable if necessary (from preliminary analysis, see plots below)\r\n  if(traitname %in% c(""lcc"", ""ldmc"", ""ssd"")) \r\n  {data$trait = data[,5] } else\r\n  {data$trait = log10(data[,5])}\r\n  hist(data[,5], main = traitname)\r\n  hist(data$trait)\r\n  \r\n#### 2. Trait gradient analysis ####\r\n# script in this section was largely derived from the script of Ackerly & Cornwell (2007) for trait gradient analysis\r\n# Calculation of the species mean, plot mean, mean position(beta trait value), Alpha trait value, niche breadth and the counts of observations for\r\n# each species  \r\n\r\n  # plot mean trait value\r\n  plot.data = \r\n    data %>%\r\n    group_by(plot) %>%\r\n    summarise(plotwtd.sum = sum(trait*abund), \r\n              plotwts = sum(abund), \r\n              plotwmean = plotwtd.sum/plotwts)  \r\n  \r\n  # species mean trait value\r\n  sp.data = \r\n    data %>%\r\n    group_by(species) %>%\r\n    summarise(spwtd.sum = sum(trait*abund),\r\n              spwts = sum(abund), \r\n              spwmean =spwtd.sum/spwts)\r\n  \r\n  # counts of observation\r\n  count.data=\r\n    data %>%\r\n    group_by(species)%>%\r\n    summarise(counts=length(species))\r\n  \r\n  head(data)\r\n  data1 = left_join(data, sp.data, by = ""species"")\r\n  data2 = left_join(data1, plot.data, by = ""plot"")\r\n  data3 = left_join(data2,count.data, by = ""species"")\r\n  head(data3)\r\n  \r\n  # site trait mean for sites occupied by species\r\n  # The species\' mean location along the trait gradient (beta values and species niche)\r\n  \r\n  sp.beta.data = \r\n    data3 %>%\r\n    group_by(species) %>%\r\n    summarise(\r\n      plotwmean.sum = sum(plotwmean*abund),\r\n      sp.beta.wts = sum(abund), \r\n      sp.betaT = plotwmean.sum/sp.beta.wts,\r\n      sp.beta.plotT.max = max(plotwmean),\r\n      sp.beta.plotT.min = min(plotwmean))%>%\r\n    mutate(plotT.r = sp.beta.plotT.max - sp.beta.plotT.min)\r\n  \r\n  data4 = left_join(data3, sp.beta.data, by = ""species"")\r\n  head(data4)\r\n  data4$sp.alphaT <- data4$spwmean - data4$sp.betaT\r\n  head(data4)\r\n  \r\n#### 3. Regressions to calculate the slopes (b)- intraspecific variability per species#####\r\n  \r\n  all.spp.list = all.data[,1] # spp.list gives a list of all species and Nsp the length of the list\r\n  #all species for all traits\r\n  #define some variables (b:slopes, int:intercept,bs:standard error)\r\n  # select only those spp that have this trait\r\n  spp.list=sort(unique(data4$species))\r\n  Nsp=length(spp.list)\r\n  b=rep(NA,Nsp)\r\n  int=rep(NA,Nsp)\r\n  bs=rep(NA,Nsp)\r\n  b.sp=rep(NA,Nsp)\r\n  plotT.r = rep(NA,Nsp)\r\n  count = rep(NA,Nsp)\r\n  sp.alphaT = rep(NA,Nsp)\r\n  sp.betaT = rep(NA,Nsp)\r\n  spwmean = rep(NA,Nsp)\r\n  \r\n  # for-loop to calculate the anterior variables of the regression for each species:\r\n  # Note: species needs to be defined (as b_sp) to be able to match the order of species with the input data\r\n  # Note2: The original Ackerly and Cornwell method uses regression weighted by abundance \r\n  \r\n  for (j in 1:Nsp) {\r\n    \r\n    spp <- spp.list[j]\r\n    dj=subset(data4,species==spp)\r\n    model1 <-lm(trait~plotwmean,data=dj,weights=abund,na.action=na.omit)#weighted least squares (WLS) regression \r\n    b[j]=summary(model1)$coef[[2]]\r\n    int[j]=summary(model1)$coef[[1]]\r\n    bs[j]=summary(model1)$coef[[4]]\r\n    count[j] <- dj$counts[1]#keep the top one of the counts (per species 1 value)\r\n    plotT.r[j] <- dj$plotT.r[1]\r\n    sp.alphaT[j] <- dj$sp.alphaT[1]\r\n    sp.betaT[j] <- dj$sp.betaT[1]\r\n    spwmean[j] <- dj$spwmean[1]\r\n  } \r\n  \r\n  tmp = as.data.frame(cbind(as.character(spp.list), b, int, bs, count, plotT.r, sp.alphaT, sp.betaT, spwmean))\r\n  names(tmp)= c(""species"", paste0(traitname, "".b""), \r\n                paste0(traitname, "".int""), \r\n                paste0(traitname, "".bs""), \r\n                paste0(traitname, "".count""),\r\n                paste0(traitname, "".plotT.r""),\r\n                paste0(traitname, "".sp.alphaT""),\r\n                paste0(traitname, "".sp.betaT""),\r\n                paste0(traitname, "".spwmean""))\r\n  \r\n  tmp[, 2:ncol(tmp)] <- sapply(tmp[, 2:ncol(tmp)], as.character)  # convert columns to numeric (not sure why two step needed here, but doesnt work in one step)\r\n  tmp[, 2:ncol(tmp)] <- sapply(tmp[, 2:ncol(tmp)], as.numeric)\r\n  \r\n  tmp1 = left_join(data.frame(species=all.spp.list),tmp, by = ""species"")\r\n  \r\n  all.data = cbind(all.data, tmp1[,-1])\r\n  \r\n } # end of loop\r\n\r\nwarnings() \r\n\r\nwrite.csv(all.data, ""database2_trait_b_bs_all_species.csv"", na = ""NaN"", row.names = FALSE)\r\n#save the database of 1946 species and this database is the major part of the raw ITV database, \r\n#the other 120 species with Ackerly&Kooyman results were added maunually, see our \'Materials and Methods\' for details.\r\n\r\n\r\n################################################################################################################~\r\n#### C. Data quality control        ####\r\n#### See our \'Materials and Methods\' of our manuscript and Fig. S1-S3 of our support information for details\r\n################################################################################################################~\r\n\r\ntraitdata2 <- read.xlsx(""raw ITV database.xlsx"", sheet = 2, cols = c(1:89), na.strings = c(""na"", ""na "",""NaN""))\r\n##This is the ITV database we provided which compiled from above saved dataset with extra Ackerly&Kooyman results manually included as we indicated in line 179-180.\r\n\r\ntraitdata2$species <- as.factor(traitdata2$species)\r\nall.spp.list2 = traitdata2[,1]\r\nthreshold.data = data.frame(species=all.spp.list2)\r\n### use bs and threshold to control b value quality\r\nfor(i in 5:15) { \r\n  traitname = names(traitdata)[i] \r\n  print(paste0(""I\'m now working on trait "",traitname))\r\n  tmp2 = dplyr::select(traitdata2, c(""species"", contains(traitname))) # select species columns with that traitname, note:specify the library while using select()function, otherwise error will appear when select() are also in other libraries\r\n  tmp2.sub   = subset(tmp2, tmp2[,5]>2&tmp2[,4]<1)  # subset those spp with at least 3 counts and bs < 1 (so we can trust b)\r\n  threshold5 = quantile(tmp2.sub[,6], probs=c(0.05)) # set the threshold of plotTr\r\n  tmp2.sub2  = subset(tmp2.sub, tmp2.sub[,6] >=threshold5)# so we only select the spp whose plotTr > 5% quantile (so we select 95% plotTr)\r\n  print(paste0(""threshold"",traitname,"" = "", threshold5))\r\n  tmp3 = left_join(data.frame(species=all.spp.list2),tmp2.sub2, by = ""species"")\r\n  threshold.data = cbind(threshold.data, tmp3[,-1])\r\n  \r\n}\r\n\r\nwrite.csv(threshold.data, ""finaldata.csv"", row.names = FALSE)# save the data\r\n#### this dataset is almost the final ITV database, \r\n# the only thing we edited is that we deleted the b and spmean values for 103 species which were not classified at species level (e.g.Agrostis_sp) except for Poa_spec and trichophorum_spec, \r\n# as well as the b and spmean values of leaf tissue density for Phleum_alpinum as we regarded this as an outlier because of its very small plotT.r\r\n\r\n################################################################################################################~\r\n#### D. SMA analysis        ####\r\n#### Analyses of the ITV-ITV relationships and trait-trait relationships-- see the maintext of the manuscript\r\n################################################################################################################~\r\n\r\nlibrary(smatr)\r\n\r\nfinaldata <- read.xlsx(""final ITV database.xlsx"") ###this database can be obtained from above saved dataset in section C with some manual edits indicated in line 210-211.\r\nstr(finaldata)\r\nitv <- dplyr::select(finaldata, c(""sla.b"",""sla.count"",""sla.plotT.r"", ""sla.spwmean"", ""ldmc.b"",""ldmc.count"",""ldmc.plotT.r"", ""ldmc.spwmean"", ""lnc.b"",""lnc.count"",""lnc.plotT.r"", ""lnc.spwmean"", ""lpc.b"",""lpc.count"",""lpc.plotT.r"", ""lpc.spwmean"", ""mh.b"",""mh.count"",""mh.plotT.r"", ""mh.spwmean"", ""ssd.b"",""ssd.count"",""ssd.plotT.r"", ""ssd.spwmean"", ""ls.b"",""ls.count"",""ls.plotT.r"", ""ls.spwmean"", ""lcc.b"",""lcc.count"",""lcc.plotT.r"", ""lcc.spwmean"", ""leafthick.b"",""leafthick.count"",""leafthick.plotT.r"", ""leafthick.spwmean"", ""ltisdens.b"", ""ltisdens.count"",""ltisdens.plotT.r"", ""ltisdens.spwmean"", ""srl.b"",""srl.count"", ""srl.spwmean"", ""srl.plotT.r""))\r\n\r\n#### Fig 4 main text ####\r\npar(mar=c(4,4,4,2)) ###to decrease plot margins and keep x and y labs\r\npar(mfrow=c(3,4)) \r\n\r\nm1 <- sma(itv$sla.b~itv$ldmc.b,robust = T)\r\nm1\r\nplot(m1,xlab = ""ITV of LDMC"", ylab = ""ITV of SLA"")\r\nmtext(""a. R2 = 0.116; p < 0.001; (325)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt1 <- sma(itv$sla.spwmean~itv$ldmc.spwmean,robust = T)\r\nt1\r\nplot(t1,xlab = ""LDMC (mg g-1)"", ylab = ""SLA (log) (mm2 mg-1)"", col = ""black"")\r\nmtext(""   R2 = 0.396; p < 0.001; (325)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm17 <- sma(itv$ldmc.b~itv$leafthick.b,robust = T)\r\nm17\r\nplot(m17,xlab = ""ITV of Lth"", ylab = ""ITV of LDMC"")\r\nmtext(""d. R2 = 0.063; p = 0.025; (80)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt17 <- sma(itv$ldmc.spwmean~itv$leafthick.spwmean,robust = T)\r\nt17\r\nplot(t17, xlab = ""Lth (log10) (mm)"", ylab = ""LDMC (mg g-1)"",col = ""black"")\r\nmtext(""   R2 = 0.086; p = 0.008; (80)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm8 <- sma(itv$sla.b~itv$leafthick.b,robust = T)\r\nm8\r\nplot(m8,xlab = ""ITV of Lth"", ylab = ""ITV of SLA"")\r\nmtext(""b. R2 = 0.036; p = 0.017; (157)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt8 <- sma(itv$sla.spwmean~itv$leafthick.spwmean,robust = T)\r\nt8\r\nplot(t8, xlab = ""Lth (log10) (mm)"", ylab = ""SLA (log10) (mm2 mg-1)"",col = ""black"")\r\nmtext(""   R2 = 0.412; p < 0.001; (157)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm18 <- sma(itv$ldmc.b~itv$ltisdens.b,robust = T)\r\nm18\r\nplot(m18,which=""default"", xlab = ""ITV of Ltis"",ylab =  ""ITV of LDMC"")\r\nmtext(""e. R2 = 0.149; p = 0.002; (63)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt18 <- sma(itv$ldmc.spwmean~itv$ltisdens.spwmean,robust = T)\r\nt18\r\nplot(t18,xlab = ""Ltis (log10) (mg mm-3)"",ylab = ""LDMC (mg g-1)"",col = ""black"")\r\nmtext(""   R2 = 0.568; p < 0.001; (63)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm9 <- sma(itv$sla.b~itv$ltisdens.b,robust = T)\r\nm9\r\nplot(m9, xlab = ""ITV of Ltis"", ylab = ""ITV of SLA"")\r\nmtext(""c. R2 = 0.194; p < 0.001; (81)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt9 <- sma(itv$sla.spwmean~itv$ltisdens.spwmean,robust = T)\r\nt9\r\nplot(t9,xlab = ""Ltis (log10) (mg mm-3)"", ylab = ""SLA (log10) (mm2 mg-1)"",col = ""black"")\r\nmtext(""   R2 = 0.190; p < 0.001; (81)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm53 <- sma(itv$leafthick.b~itv$ltisdens.b,robust = T)\r\nm53\r\nplot(m53, xlab = ""ITV of Ltis"", ylab = ""ITV of Lth"")\r\nmtext(""f. R2 = 0.202; p < 0.001; (76)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt53 <- sma(itv$leafthick.spwmean~itv$ltisdens.spwmean,robust = T)\r\nt53\r\nplot(t53,xlab = ""Ltis (log10) (mg mm-3)"", ylab = ""Lth (log10) (mm)"",col = ""black"")\r\nmtext(""   R2 = 0.237; p < 0.001; (76)"", side = 3,line = 0.5, adj = 0)\r\n\r\n\r\n#### Fig 5 main text ####\r\npar(mar=c(4,4,4,2))###to decrease plot margins and keep x and y labs\r\npar(mfrow=c(3,6))\r\n\r\nm2 <- sma(itv$sla.b~itv$lnc.b,robust = T)\r\nm2\r\nplot(itv$sla.b~itv$lnc.b,xlab = ""ITV of LNC"", ylab = ""ITV of SLA"", col= ""blue"")\r\nmtext(""a. p = 0.527; (278)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt2 <- sma(itv$sla.spwmean~itv$lnc.spwmean,robust = T)\r\nt2\r\nplot(t2, xlab = ""LNC (log10) (mg g-1)"", ylab = ""SLA (log10) (mm2 mg-1)"", col= ""black"")\r\nmtext("" R2 = 0.458; p < 0.001; (278)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm11 <- sma(itv$ldmc.b~itv$lnc.b,robust = T)\r\nm11\r\nplot(m11, xlab = ""ITV of LNC"", ylab = ""ITV of LDMC"")\r\nmtext(""d. R2 = 0.036; p = 0.010; (181)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt11 <- sma(itv$ldmc.spwmean~itv$lnc.spwmean,robust = T)\r\nt11\r\nplot(t11, xlab = ""LNC (log10) (mg g-1)"", ylab = ""LDMC (mg g-1)"", col = ""black"")\r\nmtext("" R2 = 0.318; p < 0.001; (181)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm20 <- sma(itv$lnc.b~itv$lpc.b,robust = T)\r\nm20\r\nplot(m20, xlab = ""ITV of LPC"", ylab = ""ITV of LNC"")\r\nmtext(""g. R2 = 0.177; p < 0.001; (152)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt20 <- sma(itv$lnc.spwmean~itv$lpc.spwmean,robust = T)\r\nt20\r\nplot(t20, xlab = ""LPC (log10) (mg g-1)"", ylab = ""LNC (log10) (mg g-1)"",col = ""black"")\r\nmtext("" R2 = 0.341; p < 0.001; (152)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm3 <- sma(itv$sla.b~itv$lpc.b,robust = T)\r\nm3\r\nplot(m3,xlab = ""ITV of LPC"", ylab = ""ITV of SLA"")\r\nmtext(""b. R2 = 0.043; p = 0.007; (170)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt3 <- sma(itv$sla.spwmean~itv$lpc.spwmean,robust = T)\r\nt3\r\nplot(t3, xlab = ""LPC (log10) (mg g-1)"", ylab = ""SLA (log10) (mm2 mg-1)"", col = ""black"")\r\nmtext("" R2 = 0.202; p < 0.001; (170)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm12 <- sma(itv$ldmc.b~itv$lpc.b,robust = T)\r\nm12\r\nplot(m12, xlab = ""ITV of LPC"", ylab = ""ITV of LDMC"")\r\nmtext(""e. R2 = 0.087; p < 0.001; (124)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt12 <- sma(itv$ldmc.spwmean~itv$lpc.spwmean,robust = T)\r\nt12\r\nplot(t12,xlab = ""LPC (log10) (mg g-1)"", ylab = ""LDMC (mg g-1)"", col = ""black"")\r\nmtext("" R2 = 0.400; p < 0.001; (124)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm31 <- sma(itv$lpc.b~itv$lcc.b,robust = T)\r\nm31\r\nplot(itv$lpc.b~itv$lcc.b, xlab = ""ITV of LCC"", ylab = ""ITV of LPC"", col = ""blue"")\r\nmtext(""h. p = 0.339; (62)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt31 <- sma(itv$lpc.spwmean~itv$lcc.spwmean,robust = T)\r\nt31\r\nplot(t31, xlab = ""LCC (mg g-1)"", ylab = ""LPC (log10) (mg g-1)"",col = ""black"")\r\nmtext("" R2 = 0.299; p < 0.001; (62)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm7 <- sma(itv$sla.b~itv$lcc.b,robust = T)\r\nm7\r\nplot(itv$sla.b~itv$lcc.b, xlab = ""ITV of LCC"", ylab = ""ITV of SLA"", col = ""blue"")\r\nmtext(""c. p = 0.545; (178)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt7 <- sma(itv$sla.spwmean~itv$lcc.spwmean,robust = T)\r\nt7\r\nplot(t7, xlab = ""LCC (mg g-1)"", ylab = ""SLA (mm2 mg-1)"", col = ""black"")\r\nmtext("" R2 = 0.435; p < 0.001; (178)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm16 <- sma(itv$ldmc.b~itv$lcc.b,robust = T)\r\nm16\r\nplot(m16, xlab = ""ITV of LCC"", ylab = ""ITV of LDMC"")\r\nmtext(""f. R2 = 0.054; p = 0.019; (102)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt16 <- sma(itv$ldmc.spwmean~itv$lcc.spwmean,robust = T)\r\nt16\r\nplot(t16, xlab = ""LCC (mg g-1)"", ylab = ""LDMC (mg g-1)"", col = ""black"")\r\nmtext("" R2 = 0.342; p < 0.001; (102)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm24 <- sma(itv$lnc.b~itv$lcc.b,robust = T)\r\nm24\r\nplot(itv$lnc.b~itv$lcc.b, xlab = ""ITV of LCC"", ylab = ""ITV of LNC"", col = ""blue"")\r\nmtext(""i. p = 0.769; (166)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt24 <- sma(itv$lnc.spwmean~itv$lcc.spwmean,robust = T)\r\nt24\r\nplot(t24, xlab = ""LCC (mg g-1)"", ylab = ""LNC (mg g-1)"", col = ""black"")\r\nmtext("" R2 = 0.161; p < 0.001; (166)"", side = 3,line = 0.5, adj = 0)\r\n\r\n#### Fig 6 main text ####\r\npar(mar=c(4,4,4,2))###to decrease plot margins and keep x and y labs\r\npar(mfrow=c(3,4))\r\n\r\nm6 <- sma(itv$ls.b~itv$sla.b,robust = T)\r\nm6\r\nplot(itv$ls.b~itv$sla.b, xlab = ""ITV of SLA"", ylab = ""ITV of LS"", col = ""blue"")\r\nmtext(""a. p = 0.070; (470)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt6 <- sma(itv$ls.b~itv$sla.spwmean,robust = T)\r\nt6\r\nplot(itv$ls.b~itv$sla.spwmean, xlab = ""SLA (log10) (mm2 mg-1)"", ylab = ""LS (log10) (cm2)"", col = ""black"")\r\nmtext("" p = 0.669; (470)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm30 <- sma(itv$ls.b~itv$lpc.b,robust = T)\r\nm30\r\nplot(itv$ls.b~itv$lpc.b, xlab = ""ITV of LPC"", ylab = ""ITV of LS"", col = ""blue"")\r\nmtext(""d. p = 0.914; (123)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt30 <- sma(itv$ls.spwmean~itv$lpc.spwmean, robust = T)\r\nt30\r\nplot(itv$ls.spwmean~itv$lpc.spwmean, xlab = ""LPC (log10) (mg g-1)"", ylab = ""LS (log10) (cm2)"")\r\nmtext("" p = 0.402; (123)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm15 <- sma(itv$ls.b~itv$ldmc.b,robust = T)\r\nm15\r\nplot(m15,xlab = ""ITV of LDMC"", ylab = ""ITV of LS"")\r\nmtext(""b. R2 = 0.025; p = 0.029; (192)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt15 <- sma(itv$ls.spwmean~itv$ldmc.spwmean,robust = T)\r\nt15\r\nplot(t15, xlab = ""LDMC (mg g-1)"", ylab = ""LS (log10) (cm2)"", col = ""black"")\r\nmtext("" R2 = 0.065; p < 0.001; (192)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm46 <- sma(itv$ls.b~itv$lcc.b,robust = T)\r\nm46\r\nplot(itv$ls.b~itv$lcc.b, xlab = ""ITV of LCC"", ylab = ""ITV of LS"", col = ""blue"")\r\nmtext(""e. p = 0.127; (174)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt46 <- sma(itv$ls.spwmean~itv$lcc.spwmean, robust = T)\r\nt46\r\nplot(t46, xlab = ""LCC (mg g-1)"", ylab = ""LS (log10) (cm2)"", col = ""black"")\r\nmtext("" R2 = 0.232; p < 0.001; (174)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm23 <- sma(itv$ls.b~itv$lnc.b,robust = T)\r\nm23\r\nplot(itv$ls.b~itv$lnc.b, xlab = ""ITV of LNC"", ylab = ""ITV of LS"", col = ""blue"")\r\nmtext(""c. p = 0.552; (234)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt23 <- sma(itv$ls.spwmean~itv$lnc.spwmean,robust = T)\r\nt23\r\nplot(t23, xlab = ""LNC (log10) (mg g-1)"", ylab = ""LS (log10) (cm2)"", col = ""black"")\r\nmtext("" R2 = 0.024; p = 0.017; (234)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm47 <- sma(itv$ls.b~itv$leafthick.b,robust = T)\r\nm47\r\nplot(itv$ls.b~itv$leafthick.b, xlab = ""ITV of Lth"", ylab = ""ITV of LS"", col= ""blue"")\r\nmtext(""f. p = 0.709; (149)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt47 <- sma(itv$ls.spwmean~itv$leafthick.spwmean, robust = T)\r\nt47\r\nplot(t47, xlab = ""Lth (log10) (mm)"", ylab = ""LS (log10) (cm2)"", col = ""black"")\r\nmtext("" R2 = 0.134; p < 0.001; (149)"", side = 3,line = 0.5, adj = 0)\r\n\r\n#### Fig 7 main text ####\r\npar(mar=c(4,4,4,2))###to decrease plot margins and keep x and y labs\r\npar(mfrow=c(2,4))\r\nm36 <- sma(itv$mh.b~itv$ls.b,robust = T)\r\nm36\r\nplot(itv$mh.b~itv$ls.b, xlab= ""ITV of LS"", ylab = ""ITV of MH"", col = ""blue"")\r\nmtext(""a. p = 0.514; (224)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt36 <- sma(itv$mh.spwmean~itv$ls.spwmean, robust = T) \r\nt36\r\nplot(t36, xlab = ""LS (log10) (cm2)"", ylab = ""MH (log10) (m)"", col = ""black"")\r\nmtext("" R2 = 0.326; p < 0.001; (224)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm35 <- sma(itv$mh.b~itv$ssd.b,robust = T)\r\nm35\r\nplot(itv$mh.b~itv$ssd.b, xlab = ""ITV of SSD"", ylab = ""ITV of MH"",col = ""blue"")\r\nmtext(""b. p = 0.059; (98)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt35 <- sma(itv$mh.spwmean~itv$ssd.spwmean, robust = T) \r\nt35\r\nplot(t35, xlab = ""SSD (mg mm-3)"", ylab = ""MH (log10) (m)"", col = ""black"")\r\nmtext("" R2 = 0.364; p <0.001; (98)"", side = 3,line = 0.5, adj = 0)\r\n\r\nm41 <- sma(itv$ssd.b~itv$ls.b,robust = T)\r\nm41\r\nplot(itv$ssd.b~itv$ls.b, xlab = ""ITV of LS"", ylab = ""ITV of SSD"", col= ""blue"")\r\nmtext(""c. p = 0.733; (104)"", side = 3,line = 0.5, adj = 0)\r\n\r\nt41 <- sma(itv$ssd.spwmean~itv$ls.spwmean, robust = T)\r\nt41\r\nplot(itv$ssd.spwmean~itv$ls.spwmean, xlab = ""LS (log10) (cm2)"", ylab = ""SSD (mg mm-3)"", col = ""black"")\r\nmtext("" p = 0.778; (104)"", side = 3,line = 0.5, adj = 0)\r\n']","Code from: Global analysis of trait-trait relationships within and between species Some robust trait-trait relationships between species, including the leaf economic spectrum (LES), are regarded as important plant strategies but whether these relationships truly represent plant strategies remains unclear.We propose a novel approach to distinguish trait-trait relationships between species that may represent plant strategies vs. those relationships that are due to common drivers, by comparing the direction and strength of intraspecific trait variation (ITV) vs. interspecific trait variation. We applied this framework using a unique global ITV database we compiled, which included eleven traits related to LES, size and roots, and observations from 2064 species occurring in 1068 communities across 19 countries.Generally, compared to between species, trait-trait relationships within species were much weaker or totally disappeared. Almost only within the LES traits, the between-species trait-trait relationships were translated into positive relationships within-species, which suggests they may represent plant strategies.Moreover, the frequent coincidental trait-trait relationships between species, driven by co-varying common drivers, imply that in future research, decoupling of trait-trait relationships should be seriously considered in model projections of ecosystem functioning. Our study emphasises the importance of describing the mechanisms behind trait-trait relationships, both between and within species, to deepen our understanding of general plant strategies.",1
Mixed ancestry from wild and domestic lineages contributes to the rapid expansion of invasive feral swine,"Invasive alien species are a significant threat to both economic and ecological systems. Identifying processes that give rise to invasive populations is essential for implementing effective control strategies. We conducted an ancestry analysis of invasive feral swine (Sus scrofa, Linnaeus, 1758), a highly destructive ungulate that is widely distributed throughout the contiguous United States, to describe introduction pathways, sources of newly-emergent populations, and processes contributing to an ongoing invasion. Comparisons of high-density single nucleotide polymorphism genotypes for 6,566 invasive feral swine to a comprehensive reference set of S. scrofa revealed that the vast majority of feral swine were of mixed ancestry, with dominant genetic associations to Western heritage breeds of domestic pig and European populations of wild boar. Further, the rapid expansion of invasive feral swine over the past 30 years was attributable to secondary introductions from established populations of admixed ancestry as opposed to direct introductions of domestic breeds or wild boar. Spatially-widespread genetic associations of invasive feral swine to European wild boar deviated strongly from historical S. scrofa introduction pressure, which was largely restricted to domestic pigs with infrequent, localized wild boar releases. The deviation between historical introduction pressure and contemporary genetic ancestry suggests wild boar-hybridization may contribute to differential fitness in the environment and heightened invasive potential for individuals of admixed domestic pig-wild boar ancestry.","['# Note to users: You will need to first enter the path to your executable PLINK and ADMIXTURE\r\n# files stored locally and the path to the working directory where data files are stored\r\n\r\n# map to PLINK and ADMIXTURE program files\r\nplinkPath <- ""/home/tsmyser/bin/plink""\r\nadmixturePath <- ""/home/tsmyser/bin/admixture/admixture""\r\n\r\n# set working directory\r\nsetwd(""/home/tsmyser/Documents/Smyser_FeralSwineAncestry"")\r\n\r\n# create ""results"" folder within working directory to compile results\r\ncall <- ""mkdir results""\r\nsystem(call)\r\n\r\n# read in Feral Swine genotypes as *.bed, *.bim, *.fam file family to query against Reference Set\r\n# Family ID (column 1 in *.fam file) must be \'-\' in order to conduct a supervised analysis in\r\n# which feral swine of unknown ancestry are blindly queried against Reference Set\r\n# \r\nFeralSwine <- ""FeralSwine_ContinentalUS"" \r\n# create ""FeralSwine"" as an R object for convenience in calling file later in code \r\nFeralSwine_fam <- read.table(""FeralSwine_ContinentalUS.fam"")\r\n\r\n# specify column names for working in R as column names are not included in *.fam files\r\nnames(FeralSwine_fam) <- c(""FamilyID"", ""PatientID"", ""FatherID"", ""MotherID"", ""Sex"", ""AffectionStatus"")\r\nhead(FeralSwine_fam)\r\n\r\n# read in Reference Set [domestic pigs and wild boar] with the defined reference cluster\r\n# for each individual presented in the Family ID column (column 1 in *.fam file).\r\n# Genotypes of Reference Set should be ordered based on the described reference clusters, as the \r\n# order of the reference clusters will correspond with the order of the columns for the ancestry\r\n# vector for queried feral swine\r\n\r\nRefSet <- ""Sus_scrofa_ReferenceSet""\r\n# create ""RefSet"" as an R object for convenience in calling file later in code\r\nRefSet_fam <- read.table(""Sus_scrofa_ReferenceSet.fam"")\r\n# specify column names as above\r\nnames(RefSet_fam) <- c(""FamilyID"", ""PatientID"", ""FatherID"", ""MotherID"", ""Sex"", ""AffectionStatus"")\r\nhead(RefSet_fam)\r\ntable(RefSet_fam$FamilyID)\r\n\r\n###########################################################\r\n# initialize empty data frames to compile results for iterative analyses \r\n\r\n# for this analysis, we are querying feral swine relative to k = 17 reference clusters\r\n\r\nk <- length(unique(RefSet_fam$FamilyID))\r\n\r\nSampleIDs <- as.data.frame(matrix(NA, nrow(FeralSwine_fam), 2))\r\nAncestryResults_Q <- as.data.frame(matrix(0, nrow(FeralSwine_fam), k))\r\nAncestryResults_SE <- as.data.frame(matrix(0, nrow(FeralSwine_fam), k))\r\nAncestryResults_bias <- as.data.frame(matrix(0, nrow(FeralSwine_fam), k))\r\n\r\n############################################################################################\r\n# Beginning of first analysis to generate bootstrapped ancestry estimates of feral swine\r\n# relative to 17 reference clusters for Sus scrofa\r\n############################################################################################\r\n\r\nfor (i in 1:nrow(FeralSwine_fam)){\r\n  \r\n  # identify individual feral swine to be queried in iteration i\r\n  # retain individual feral swine as QueryInd.ped, QueryInd.map\r\n  Keep <- FeralSwine_fam[i,1:2]\r\n  write.table(Keep, ""KeepList"", row.names = F, col.names = F, quote = F)\r\n  call <- paste(plinkPath, ""--bfile"", FeralSwine, ""--keep KeepList --recode --out QueryInd"")\r\n  system(call)\r\n  \r\n  # merge individual feral swine genotype (QueryInd.ped) with Reference Set genotypes for \r\n  # supervised analysis\r\n  call <- paste(plinkPath, ""--bfile"", RefSet, ""--merge QueryInd.ped QueryInd.map --make-bed --out Iteration"")\r\n  system(call)\r\n  \r\n  # ADMIXTURE requires a *.pop file to complement *.bed/*.bim/*.fam files in supervised analyses    \r\n  pop <- read.table(""Iteration.fam"")[,1:2]\r\n  write.table(pop, ""Iteration.pop"", quote = F, row.names = F, col.names = F)\r\n  \r\n  # conduct supervised analysis, querying the individual feral swine (QueryInd.ped) against \r\n  # Reference Set with desired number of bootstrapped iterations (option -B##) and number of \r\n  # threads to be allocated (-j#)\r\n  # \r\n  call <- paste(admixturePath, ""-j4 -B100 --supervised Iteration.bed"", k) \r\n  system(call)\r\n  \r\n  # compile results files.  Given that feral swine, of unknown ancestry, have a Family ID of \'-\'\r\n  # the QueryInd will be appended to top (row 1) of the Reference Set in the formation of the \r\n  # Iteration.bed/Iteration.bim/Iteration.fam.  Accordingly, the ancestry results for QueryInd\r\n  # will also appear in row 1 of the ensuing Iteration.17.Q, Iteration.17.Q_se, and \r\n  # Iteration.17.Q_bias results files.\r\n  SampleIDs[i,1] <- read.table(""Iteration.fam"", stringsAsFactors = F)[1,2]\r\n  SampleIDs[i,2] <- read.table(""Iteration.fam"", stringsAsFactors = F)[1,1]\r\n  \r\n  AncestryResults_Q[i, ] <- read.table(""Iteration.17.Q"")[1,]\r\n  AncestryResults_SE[i, ] <- read.table(""Iteration.17.Q_se"")[1,]\r\n  AncestryResults_bias[i, ] <- read.table(""Iteration.17.Q_bias"")[1,]\r\n  \r\n  FeralSwine_Q <- cbind(SampleIDs, AncestryResults_Q)\r\n  write.csv(FeralSwine_Q, ""./results/FeralSwine_K17Ancestry_BootStrap_Q.csv"", quote = FALSE)\r\n  \r\n  FeralSwine_SE <- cbind(SampleIDs, AncestryResults_SE)\r\n  write.csv(FeralSwine_SE, ""./results/FeralSwine_K17Ancestry_BootStrap_SE.csv"", quote = FALSE)\r\n  \r\n  FeralSwine_bias <- cbind(SampleIDs, AncestryResults_bias)\r\n  write.csv(FeralSwine_bias, ""./results/FeralSwine_K17Ancestry_BootStrap_bias.csv"", quote = FALSE)\r\n  \r\n}\r\n\r\n#########################################\r\n#########################################\r\n#########################################\r\n\r\n# identify significant ancestry groups based on results of first analysis of 17 reference \r\n# clusters \r\nQ <- FeralSwine_Q[, 3:19]\r\nSE <- FeralSwine_SE[, 3:19]\r\n\r\n# evaluate whether the confidence interval of the ancestry association for feral swine individual \'i\' \r\n# overlaps with zero* - indicating a non-significant association\r\n# *here we use 1e-05 in place of zero as this is the lowest association returned from ADMIXTURE\r\n\r\nZero <- Q-1.96*SE # assuming normal distribution of Q-values\r\n\r\nSignificance <- matrix(NA, nrow = dim(Q)[1], 17) # a table indicating significant Q-scores""\r\n\r\nfor (i in 1:dim(Q)[1]){\r\n  for (j in 1:17){\r\n    if(Zero[i,j] > 1e-05) {\r\n      Significance[i,j] = 1} \r\n    else if (Zero[i,j] <= 1e-05) {\r\n      Significance[i,j] <- 0 }\r\n    else {Significance[i,j] <- NA}\r\n  }}\r\n\r\nSignificance <- as.data.frame(Significance)\r\nrownames(Significance) <- FeralSwine_Q$V1\r\n\r\nwrite.csv(Significance, ""FeralSwine_K17Ancestry_SignificanceMatrix.csv"", quote = F, row.names = T)\r\n\r\n# Identify and remove feral swine with significant association to a single reference cluster as subsequent \r\n# formation of \'an individually customized reference set composed of a subset of the 17 reference clusters\'\r\n# is uninformative for such individuals.\r\nSignificance$ClusterCount <-rowSums(Significance)\r\nPurebred <- subset(Significance, Significance$ClusterCount == 1)\r\nPurebred <- Purebred[, 1:17]\r\nAdmixed <- subset(Significance, Significance$ClusterCount >= 2)\r\nAdmixed <-Admixed[, 1:17]\r\n\r\nif(nrow(Purebred) >0) {\r\n  write.csv(Purebred, ""Purebred_SigMatrix.csv"", quote = F, row.names = T )\r\n}\r\nif(nrow(Admixed) >0) {\r\n  write.csv(Admixed, ""Admixed_SigMatrix.csv"", quote = F, row.names = T )\r\n}\r\n\r\nrm(Purebred)\r\n\r\n# populate matrix of reference clusters to be uniquely retained for the customized reference\r\n# set for individual feral swine \'i\'\r\n\r\nm1 <- matrix(seq(1:k), k, nrow(Admixed))\r\nm1 <- t(m1)\r\nCombined_Ind_Ref <- Admixed*m1\r\n\r\n###################################################################################\r\n# Remove \'purebred\' feral swine from genotype file, carrying forward feral swine \r\n# of admixed ancestry for analysis with individually customized reference set\r\n\r\nAdmixed_KeepList <- as.data.frame(matrix(NA, nrow(Admixed), 2))\r\nAdmixed_KeepList[,1] <- ""-""\r\nAdmixed_KeepList[,2] <- rownames(Admixed)\r\n\r\nwrite.table(Admixed_KeepList, ""Admixed_KeepList"", row.names = F, col.names = F, quote = F)\r\n\r\ncall <- paste(plinkPath,  "" --bfile "", FeralSwine , "" --keep Admixed_KeepList --make-bed --out Admixed_IndRefSet_"", nrow(Admixed), ""x29375"", sep = """") \r\nsystem(call)\r\n\r\n# create results files for individual reference set analysis\r\nIndRefSet_SampleIDs <- as.data.frame(matrix(NA, nrow(Admixed_KeepList), 2))\r\nIndRefSet_Results_Q <- as.data.frame(matrix(0, nrow(Admixed_KeepList), k))\r\nIndRefSet_Results_SE <- as.data.frame(matrix(0, nrow(Admixed_KeepList), k))\r\nIndRefSet_Results_bias <- as.data.frame(matrix(0, nrow(Admixed_KeepList), k))\r\n\r\n############################################################################################\r\n# Beginning of second analysis to generate bootstrapped ancestry estimates of feral swine\r\n# relative to individually customized reference sets\r\n############################################################################################\r\n\r\nfor (i in 1:nrow(Admixed_KeepList)){\r\n  \r\n  # define individual reference set\r\n  Ind_ref <- Combined_Ind_Ref[i,]\r\n  Ind_ref <- Ind_ref[ Ind_ref != 0 ]\r\n  \r\n  # Using \'keep\' command in plink to only retain significant reference clusters for feral\r\n  # swine \'i\'\r\n  \r\n  Keep <- RefSet_fam[which(RefSet_fam$FamilyID == Ind_ref[1]),1:2]\r\n  write.table(Keep, ""Keep"", col.names = F, row.names = F, quote = F)\r\n  \r\n  call <- paste(plinkPath, ""--bfile"", RefSet, ""--keep Keep --make-bed --out Ind_RefSet"") \r\n  system(call)\r\n  \r\n  \r\n  for(r in 2:length(Ind_ref)){\r\n    \r\n    Keep <- RefSet_fam[which(RefSet_fam$FamilyID == Ind_ref[r]),1:2]\r\n    write.table(Keep, ""Keep"", col.names = F, row.names = F, quote = F)\r\n    \r\n    call <- paste(plinkPath, ""--bfile"", RefSet, ""--keep Keep --recode --out temp"") \r\n    system(call)\r\n    \r\n    call <- paste(plinkPath, ""--bfile Ind_RefSet --merge temp.ped temp.map --make-bed --out Ind_RefSet"") \r\n    system(call)\r\n  }\r\n  \r\n  # with individually customized reference set now assembled, pull out individual feral swine\r\n  # genotype \'i\' for analysis \r\n  \r\n  Keep <- Admixed_KeepList[i,1:2]\r\n  write.table(Keep, ""Keep"", col.names = F, row.names = F, quote = F)\r\n  \r\n  call <- paste(plinkPath, "" --bfile Admixed_IndRefSet_"", nrow(Admixed), ""x29375 --keep Keep --recode --out FeSw_Ind"", sep = """") \r\n  system(call)\r\n  \r\n  call <- paste(plinkPath, ""--bfile Ind_RefSet --merge FeSw_Ind.ped FeSw_Ind.map --make-bed --out FeSw_Query"")\r\n  system(call)\r\n  \r\n  FeSw_Query <- read.table(""FeSw_Query.fam"")\r\n  write.table(FeSw_Query[,1:2], ""FeSw_Query.pop"", col.names = F, row.names = F, quote = F)\r\n  \r\n  # initiate second supervised ADMIXTURE analysis with -BXXX bootstrap iterations \r\n  call <- paste(admixturePath, ""-j4 -B100 --supervised FeSw_Query.bed "", length(unique(Ind_ref)))\r\n  system(call)\r\n  \r\n  \r\n  # compile results from individual reference set analysis and compile ancestry associates in\r\n  # the appropriate columns\r\n  \r\n  Ind_Results <- as.numeric(read.table(paste(""FeSw_Query."", length(unique(Ind_ref)), "".Q"", sep = """" ))[1,])\r\n  for (q in 1:length(unique(Ind_ref))){\r\n    Column_Number <- Ind_ref[q]\r\n    IndRefSet_Results_Q[i,Column_Number] <- Ind_Results[q]\r\n  }\r\n  \r\n  Ind_SE <- as.numeric(read.table(paste(""FeSw_Query."", length(unique(Ind_ref)), "".Q_se"", sep = """" ))[1,])\r\n  for (s in 1:length(unique(Ind_ref))){\r\n    Column_Number <- Ind_ref[s]\r\n    IndRefSet_Results_SE[i,Column_Number] <- Ind_SE[s]\r\n  }\r\n  \r\n  Ind_BIAS <- as.numeric(read.table(paste(""FeSw_Query."", length(unique(Ind_ref)), "".Q_bias"", sep = """" ))[1,])\r\n  for (b in 1:length(unique(Ind_ref))){\r\n    Column_Number <- Ind_ref[b]\r\n    IndRefSet_Results_bias[i,Column_Number] <- Ind_BIAS[b]\r\n  }\r\n  \r\n  IndRefSet_SampleIDs[i,1] <- read.table(""FeSw_Query.fam"", stringsAsFactors = F)[1,2]\r\n  IndRefSet_SampleIDs[i,2] <- read.table(""FeSw_Query.fam"", stringsAsFactors = F)[1,1]\r\n  \r\n  write.csv(cbind(IndRefSet_SampleIDs, IndRefSet_Results_Q), ""./results/FeralSwine_Admixed_IndRefSet_Boostrap_Q.csv"", quote = F)\r\n  write.csv(cbind(IndRefSet_SampleIDs, IndRefSet_Results_SE), ""./results/FeralSwine_Admixed_IndRefSet_Boostrap_SE.csv"", quote = F)\r\n  write.csv(cbind(IndRefSet_SampleIDs, IndRefSet_Results_bias), ""./results/FeralSwine_Admixed_IndRefSet_Boostrap_bias.csv"", quote = F)\r\n}\r\n\r\n', 'rm(list=ls())\r\n# Load packages into session \r\npackages.vec <- c(""sp"", ""fields"", ""rgeos"", ""maps"", ""ggmap"", ""rgdal"", ""gstat"", ""dplyr"",\r\n                  ""ggplot2"", ""scales"", ""magrittr"", \'gridExtra\', \'splancs\', \'parallel\', \'maptools\', \'automap\'\r\n                  , \'plyr\', \'plotrix\', \'readr\')\r\nlapply(packages.vec, require, character.only=TRUE)\r\n\r\n# read in data\r\ndat <- read_csv(""All_Ancestry_K17_and_IndRefSet_Updated.csv"")\r\n# remove points where coordinates are NA\r\ndat2 <- dat[!is.na(dat$Long) & ! is.na(dat$Lat), ] \r\n# remove points outside USA \r\ntop = 49.3457868 # north lat\r\nleft = -124.7844079 # west long\r\nright = -66.9513812 # east long\r\nbottom = 24.7433195 # south lat\r\ndat3 <- dat2[dat2$Lat < top & dat2$Lat > bottom & dat2$Long > left & dat2$Long < right,]\r\ndat2 <- dat3\r\n# create commercial breed column\r\ndat2$commercial_breed <- dat2$Berkshire + dat2$Hampshire + \r\n  dat2$Duroc_Other.Breeds + dat2$Landrace + dat2$Yorkshires\r\ndat2[dat2$State==""NJ"", ""commercial_breed""] <- 0.01\r\n\r\n#- convert dataframe to spatial points datafrmae (SPDF)\r\n# specify which points are the coordinates\r\ncoordinates(dat2) <- ~ Long + Lat\r\n# set projection of the data\r\ndat2@proj4string <- CRS(""+proj=longlat +ellps=WGS84 +datum=WGS84"")\r\n# display the four counts that denote the spatial extent of the data\r\nbbox(dat2); dat2@bbox\r\n\r\n# remove excess points\r\nusa <- map(""state"", fill = TRUE)\r\nIDs <- sapply(strsplit(usa$names, "":""), function(x) x[1])\r\nusa <- map2SpatialPolygons(usa, IDs=IDs, proj4string=CRS(""+proj=longlat +ellps=WGS84  +datum=WGS84""))\r\nstaying <- dat2[!is.na(over(dat2, as(usa, ""SpatialPolygons""))),]\r\ndat2 <- staying\r\n\r\n# dealing with colors\r\nnum.colors <- 20\r\nblue.end.pnts <- c(""#c6dbef"",""#08306b"")\r\nred.end.pnts <- c(""#ffeda0"",""#730022"")\r\n# function for color\r\ncolor.gradient <- function(x, colors=c(""red"",""yellow"",""green""), colsteps=100) {\r\n  return(colorRampPalette(colors,interpolate = c(""spline""))(colsteps)[ findInterval(x, seq(min(x),max(x), length.out=colsteps)) ] \r\n  )\r\n}\r\n\r\n# Blue Palette (cut point at .25 so 0 to .25)\r\nblue.palette <- color.gradient(1:5,colors=blue.end.pnts)\r\n\r\n# Red Palette (.25 to 1)\r\nred.palette <- color.gradient(1:15,colors=red.end.pnts)\r\n\r\n# Combine Palettes into one\r\nred.blue.palette <- c(rev(blue.palette),red.palette)\r\n\r\n#----- mapping Western Heritage, Wild Boar, and Commercial Breeds\r\n#-- Western Heritage:: Calling Western Heritage ""NE"" for convenience\r\nx <- dat2@coords\r\ny <- dat2$Western.Heritage\r\n#tps1 <- Tps(dircos(x), y)\r\ntps1 <- Tps(x, y)\r\n\r\n# extracting values and setting up graphical parameters\r\ndfNE <- data.frame(tps1$x, tps1$fitted.values)\r\ncolnames(dfNE) <- c(\'x\', \'y\', \'z\')\r\nstate <- map_data(""state"")\r\nstate$group2 <- rep(1, nrow(state))\r\npts_sz <- 4\r\ndfNE <- dfNE[order(dfNE$z),]\r\n# plot with ggplot\r\nplotNE <- dfNE %>% as.data.frame %>%\r\n  ggplot(aes(x=x, y=y))+\r\n  geom_polygon(data=state, aes(x=long, y=lat, group=group, fill=""white"", colour=\'white\'), fill=\'white\',col=""black"") + \r\n  geom_point(aes(colour=z), size=pts_sz)+ \r\n  coord_equal() +\r\n  scale_colour_gradientn(colors=red.blue.palette, breaks=seq(0,1,by=.05), guide=F, limits=c(0,1))+\r\n  scale_x_continuous(labels=comma) + scale_y_continuous(labels=comma) +\r\n  guides(fill=FALSE, point=FALSE) + \r\n  ggtitle(\'A. Western heritage breeds\') +\r\n  theme_bw() +\r\n  theme(axis.title.x=element_blank(),\r\n        axis.title.y=element_blank(),\r\n        axis.text.x=element_blank(),\r\n        axis.text.y=element_blank(),\r\n        axis.ticks.y=element_blank(),\r\n        axis.ticks.x=element_blank(),\r\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border=element_blank())\r\nplotNE\r\n\r\n#-- Wild Boar Heritage\r\nx <- dat2@coords\r\ny <- dat2$European.Wild.Boar\r\ntps3 <- Tps(x, y)\r\n\r\n# extracting values and setting up graphical parameters\r\ndfWB <- data.frame(tps3$x, tps3$fitted.values)\r\ncolnames(dfWB) <- c(\'x\', \'y\', \'z\')\r\ndfWB <- dfWB[dfWB$z > 0.001,]\r\nstate <- map_data(""state"")\r\nstate$group2 <- rep(1, nrow(state))\r\ndfWB <- dfWB[order(dfWB$z),]\r\n\r\n# plot with legend\r\nplotWB <- dfWB %>% as.data.frame %>%\r\n  ggplot(aes(x=x, y=y))+\r\n  geom_polygon(data=state, aes(x=long, y=lat, group=group, fill=""white"", colour=\'white\'), fill=\'white\',col=""black"") + \r\n  geom_point(aes(colour=z), size=pts_sz)+ \r\n  coord_equal() +\r\n  scale_colour_gradientn(colors=red.blue.palette, breaks=seq(0,1,by=.2), \r\n                         #*** if making legend, use next line ***#\r\n                         guide=""colourbar"", \r\n                         limits=c(0,1))+\r\n  scale_x_continuous(labels=comma) + scale_y_continuous(labels=comma) +\r\n  labs(colour=\'Proportion\') + \r\n  guides(fill=FALSE, point=FALSE) + \r\n  ggtitle(\'B. European wild boar\') +\r\n  theme_bw() + \r\n  theme(axis.title.x=element_blank(),\r\n        axis.title.y=element_blank(),\r\n        axis.text.x=element_blank(),\r\n        axis.text.y=element_blank(),\r\n        axis.ticks.y=element_blank(),\r\n        axis.ticks.x=element_blank(),\r\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border=element_blank())\r\n#pdf(""WB_withLegend.pdf"")\r\nplotWB\r\n#dev.off()\r\ntheme_get()$legend.title$hjust\r\n\r\n# plot WB without legend\r\nplotWB <- dfWB %>% as.data.frame %>%\r\n  ggplot(aes(x=x, y=y))+\r\n  geom_polygon(data=state, aes(x=long, y=lat, group=group, fill=""white"", colour=\'white\'), fill=\'white\',col=""black"") + \r\n  geom_point(aes(colour=z), size=pts_sz)+ \r\n  coord_equal() +\r\n  scale_colour_gradientn(colors=red.blue.palette, breaks=seq(0,1,by=.05), guide=F, limits=c(0,1))+\r\n  scale_x_continuous(labels=comma) + scale_y_continuous(labels=comma) +\r\n  labs(colour=\'\') + \r\n  guides(fill=FALSE, point=FALSE) + \r\n  ggtitle(\'B. European wild boar\') +\r\n  theme_bw() +\r\n  theme(axis.title.x=element_blank(),\r\n        axis.title.y=element_blank(),\r\n        axis.text.x=element_blank(),\r\n        axis.text.y=element_blank(),\r\n        axis.ticks.y=element_blank(),\r\n        axis.ticks.x=element_blank(),\r\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border=element_blank())\r\nplotWB\r\n\r\n#-- commercial breed\r\nx <- dat2@coords\r\ny <- dat2$commercial_breed\r\ntps4 <- Tps(x, y)\r\n\r\n# extracting values and setting up graphical parameters\r\ndfCB <- data.frame(tps4$x, tps4$fitted.values)\r\ncolnames(dfCB) <- c(\'x\', \'y\', \'z\')\r\nstate <- map_data(""state"")\r\nstate$group2 <- rep(1, nrow(state))\r\ndfCB <- dfCB[order(dfCB$z),]\r\n\r\n# plot CB\r\nplotCB <- dfCB %>% as.data.frame %>%\r\n  ggplot(aes(x=x, y=y))+\r\n  geom_polygon(data=state, aes(x=long, y=lat, group=group, fill=""white"", colour=\'white\'), fill=\'white\',col=""black"") + \r\n  geom_point(aes(colour=z), size=pts_sz)+ \r\n  coord_equal() +\r\n  scale_colour_gradientn(colors=red.blue.palette, breaks=seq(0,1,by=.05), guide=F, limits=c(0,1))+\r\n  scale_x_continuous(labels=comma) + scale_y_continuous(labels=comma) +\r\n  labs(colour=\'\') + \r\n  guides(fill=FALSE, point=FALSE) + \r\n  ggtitle(\'C. Commercial breeds\') +\r\n  theme_bw() +\r\n  theme(axis.title.x=element_blank(),\r\n        axis.title.y=element_blank(),\r\n        axis.text.x=element_blank(),\r\n        axis.text.y=element_blank(),\r\n        axis.ticks.y=element_blank(),\r\n        axis.ticks.x=element_blank(),\r\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border=element_blank())\r\nplotCB\r\n\r\n# plot all panels\r\n#pdf(\'heritage_3panels.pdf\')\r\ngrid.arrange(plotNE, plotWB, plotCB, ncol=1)\r\n#dev.off()\r\n', '# # Note to user: This analysis requires packages ""compositions"" and ""robCompositions"".  For convenience, we use the package ""xlsx"",\r\n# allowing users to directly call appendices provided online with the manuscript.  Saving *.xlsx appendices to text files\r\n# and using read.table() or read.csv() is also an option in R-base without requiring installation of package ""xlsx"".\r\n\r\nlibrary(""xlsx"")\r\nlibrary(""compositions"")\r\nlibrary(""robCompositions"")\r\n\r\n# specify working directory where appendices have been saved\r\nsetwd(""E:/Smyser_FeralSwineAncestry"")\r\n\r\nFeralSwine_K17_Ancestry <- read.xlsx(""S8_InvasiveFeralSwine_CompleteK17RefereceSet_Ancestry.xlsx"", sheetName = ""S8_FeralSwine_K17ReferenceSet"")[,1:19]\r\nhead(FeralSwine_K17_Ancestry)\r\n\r\nFeralSwine_Metadata <- read.xlsx(""S1_InvasiveFeralSwine_Metadata.xlsx"", sheetName = ""S1_InvasiveFeralSwine_Metadata"") \r\nhead(FeralSwine_Metadata)\r\n\r\n# pair ancestry values with metadata\r\nFeSw_Ancestry_Metadata <- merge(x = FeralSwine_Metadata, y = FeralSwine_K17_Ancestry, by.x = ""Individual.ID"", by.y = ""Individual.ID"")\r\n# reorder data frame \r\nFeSw_Ancestry_Metadata <- FeSw_Ancestry_Metadata[order(FeSw_Ancestry_Metadata$Order.x), ] \r\n# remove redundant Order column from the ancestry data frame\r\nFeSw_Ancestry_Metadata <- subset(FeSw_Ancestry_Metadata, select = -(Order.y))\r\n\r\n# view combined data frame\r\nhead(FeSw_Ancestry_Metadata)\r\n\r\n# return the number of individuals from long-established versus newly-emergent populations\r\ntable(FeSw_Ancestry_Metadata$Population.History)\r\n\r\n# separate data frame into a compositional matrix of ancestry and invasion history for construction of a model comparing ancestry between long-established and newly-emergent populations\r\nancestry <- acomp(FeSw_Ancestry_Metadata[,9:25])\r\ninvasion <- FeSw_Ancestry_Metadata$Population.History\r\n\r\n# run the compositional model to evaluate whether there are differences in ancestry between feral swine in long-established and newly-emergent populations\r\nmodel = lm(ilr(ancestry) ~ invasion)\r\nanova(model)\r\n\r\n# given that the overall model demonstrated significant differences in ancestry between long-established and newly-emergent populations, \r\n# we are interetested in whether those differences were attributable to ancestral contributions from European wild boar, Western heritage breeds,\r\n# or commercial breeds.  We need to reconfigure the ancestry matrix so that there are 4 ancestry groups: European wild boar, Western heritage breeds,\r\n# commercial breeds, and Other (summation of the remaining ancestry groups).\r\n\r\nancestry_4 <- FeSw_Ancestry_Metadata[, c(""Individual.ID"", ""Population.History"")]\r\nancestry_4$WildBoar <- FeSw_Ancestry_Metadata$Reference.Cluster.17 \r\nancestry_4$HeritagePig <- FeSw_Ancestry_Metadata$Reference.Cluster.16\r\nancestry_4$Commercial <- rowSums(FeSw_Ancestry_Metadata[,c(9, 10, 16, 17, 21)])\r\nancestry_4$Other <- rowSums(FeSw_Ancestry_Metadata[,c(11, 12, 13, 14, 15, 18, 19, 20, 22, 23)])\r\n\r\nancestry_4_comp <- acomp(ancestry_4[,3:6])\r\n# view reduced ancestry composition matrix\r\nancestry_4_comp[1:6,]\r\n\r\ninvasion <- ancestry_4$Population.History\r\n\r\n# Using the nomenclature presented in  Filzmoser et al. 2018 (section 10.7.1)\r\n# Filzmoser, P., Hron, K., & Temple, M. (2018) Applied compositional data analysis - with worked examples in R. \r\n#     Cham, Switzerland: Springer. doi 10.1007/978-3-319-96422-5\r\n\r\nRegression_models <- vector(""list"", ncol(ancestry_4_comp))\r\n\r\nfor (j in 1:ncol(ancestry_4_comp)){\r\n  zj <- pivotCoord(ancestry_4_comp, pivotvar = j)\r\n  # use only first coordinate\r\n  res <- lm(zj[,1] ~ invasion)\r\n  # result for the first coordinate\r\n  Regression_models[[j]] <- summary(res)\r\n}\r\n\r\nRegression_models\r\n\r\n']","Mixed ancestry from wild and domestic lineages contributes to the rapid expansion of invasive feral swine Invasive alien species are a significant threat to both economic and ecological systems. Identifying processes that give rise to invasive populations is essential for implementing effective control strategies. We conducted an ancestry analysis of invasive feral swine (Sus scrofa, Linnaeus, 1758), a highly destructive ungulate that is widely distributed throughout the contiguous United States, to describe introduction pathways, sources of newly-emergent populations, and processes contributing to an ongoing invasion. Comparisons of high-density single nucleotide polymorphism genotypes for 6,566 invasive feral swine to a comprehensive reference set of S. scrofa revealed that the vast majority of feral swine were of mixed ancestry, with dominant genetic associations to Western heritage breeds of domestic pig and European populations of wild boar. Further, the rapid expansion of invasive feral swine over the past 30 years was attributable to secondary introductions from established populations of admixed ancestry as opposed to direct introductions of domestic breeds or wild boar. Spatially-widespread genetic associations of invasive feral swine to European wild boar deviated strongly from historical S. scrofa introduction pressure, which was largely restricted to domestic pigs with infrequent, localized wild boar releases. The deviation between historical introduction pressure and contemporary genetic ancestry suggests wild boar-hybridization may contribute to differential fitness in the environment and heightened invasive potential for individuals of admixed domestic pig-wild boar ancestry.",1
Simulation of the Resilience of Cultural Diversity in CulSim,"The collection contains:data.csv: data obtained using the CulSim(2016) cultural simulator, and presented in presented in Ulloa & Kacperski (2020)code.R: statistical analysis Ulloa, R. (2016). CulSim: A simulator of emergence and resilience of cultural diversity. SoftwareX, 5, 150155. https://doi.org/10.1016/j.softx.2016.07.005Ulloa, R., & Kacperski, C. (2020). A Simulation of the Resilience of Cultural Diversity in the Face of Large-Scale Events. ArXiv:2003.05322 [Nlin, Physics:Physics]. http://arxiv.org/abs/2003.05322","['library(lme4)\nlibrary(lmerTest)\nlibrary(optimx)\n\n# read data frome file\ndata <- read.csv(\'data.csv\')\n\n# create factors\ndata <- within(data, {\n  distribution <- as.factor(distribution)\n  event <- as.factor(event)\n  diversity <- as.factor(diversity)\n  scenario <- as.factor(scenario)\n  event_level <- as.factor(event_size)\n})\n\n# Specify the order of the events according to the paper\ndata$distribution <- factor(data$distribution, levels=c(""Uniform"",""Normal""))\ndata$event <- factor(data$event, levels=c(""Decimation"", \n                                          ""Settlement"", ""Outsiders"",\n                                          ""Apostasy"",  ""Destruction"",\n                                          ""Partial Content Removal"", ""Full Content Removal"", \n                                          ""Partial Conversion"", ""Full Conversion""))\n\n\n# Appendix A\nsink(""appendix_a.txt"")\n\nfor (ev in levels(as.factor(data$event))){\n  for (dist in levels(as.factor(data$distribution))){\n    DatasetHV <- subset(data, event==ev & distribution==dist)\n    \n    cat(paste(\'\\n\\n# \', ev, dist, \'\\n\'))\n    \n    # Anova for main effects\n    aov1 <- aov(full_sim ~ diversity * event_size * scenario, data=DatasetHV)\n    print(summary(aov1))\n  }\n}\nsink()\n\n\n\n# Table 5 in the paper\nsink(""table_5.txt"")\n\n#  Only applies to divers cultures (only for event sizes between 0.2 and 0.8)\ndiverseData <- subset(data, (diversity == \'Many cultures\' & event_size > 0.0 & event_size < 1.0))\nfor (ev in levels(as.factor(diverseData$event))){\n  for (dist in levels(as.factor(diverseData$distribution))){\n    subDatasetHV <- subset(diverseData, event==ev & distribution==dist)\n    print(paste(ev, dist))\n    \n    # Linear Mixed Effects model to accumulate the effect of pre_cultures\n    lmer1 <- lmer(full_sim ~ pre_cultures_at_least_3 + (scenario|event_level), data=subDatasetHV, \n                  control = lmerControl(\n                    optimizer =\'optimx\', optCtrl=list(method=\'nlminb\')))\n    print(summary(lmer1))\n    \n  }\n}\nsink()\n\n\n\n# Appendix B (correlations of Table 5)\nsink(""appendix_b.txt"")\n\n# Only applies to diverse cultures (extrem event sizes of 0.0 and 1.0 included)\ndiverseData <- subset(data, (diversity == \'Many cultures\'))\ncat(""Event,Distribution,Scenario,Event Size, Correlation\\n"")\nfor (ev in levels(as.factor(diverseData$event))){\n  for (dist in levels(as.factor(diverseData$distribution))){\n    for (sc in levels(as.factor(diverseData$scenario))){\n      for (lvl in levels(as.factor(diverseData$event_size))){\n        subDatasetHV <- subset(diverseData, event==ev & distribution==dist \n                               & scenario == sc & event_size == lvl)\n        cat(paste(ev, dist, sc, lvl, cor(subDatasetHV$full_sim, subDatasetHV$pre_cultures_at_least_3), sep="",""))\n        cat(""\\n"")\n      }\n    }\n  }\n}\nsink()\n\n']","Simulation of the Resilience of Cultural Diversity in CulSim The collection contains:data.csv: data obtained using the CulSim(2016) cultural simulator, and presented in presented in Ulloa & Kacperski (2020)code.R: statistical analysis Ulloa, R. (2016). CulSim: A simulator of emergence and resilience of cultural diversity. SoftwareX, 5, 150155. https://doi.org/10.1016/j.softx.2016.07.005Ulloa, R., & Kacperski, C. (2020). A Simulation of the Resilience of Cultural Diversity in the Face of Large-Scale Events. ArXiv:2003.05322 [Nlin, Physics:Physics]. http://arxiv.org/abs/2003.05322",2
Group and individual social network metrics are robust to changes in resource distribution in experimental populations of forked fungus beetles,"Social interactions drive many important ecological and evolutionary processes. It is therefore essential to understand the intrinsic and extrinsic factors that underlie social patterns. A central tenet of the field of behavioral ecology is the expectation that the distribution of resources shapes patterns of social interactions.We combined experimental manipulations with social network analyses to ask how patterns of resource distribution influence complex social interactions.We experimentally manipulated the distribution of an essential food and reproductive resource in semi-natural populations of forked fungus beetles (Bolitotherus cornutus). We aggregated resources into discrete clumps in half of the populations and evenly dispersed resources in the other half. We then observed social interactions between individually marked beetles. Half-way through the experiment, we reversed the resource distribution in each population, allowing us to control any demographic or behavioral differences between our experimental populations. At the end of the experiment, we compared individual and group social network characteristics between the two resource distribution treatments.We found a statistically significant but quantitatively small effect of resource distribution on individual social network position and detected no effect on group social network structure. Individual connectivity (individual strength) and individual cliquishness (local clustering coefficient) increased in environments with clumped resources, but this difference explained very little of the variance in individual social network position. Individual centrality (individual betweenness) and measures of overall social structure (network density, average shortest path length, and global clustering coefficient) did not differ between environments with dramatically different distributions of resources.Our results illustrate that the resource environment, despite being fundamental to our understanding of social systems, does not always play a central role in shaping social interactions. Instead, our results suggests that sex differences and temporally fluctuating environmental conditions may be more important in determining patterns of social interactions.","['#R code for the analyses on group social network metrics in the manuscript ""Resource distribution alters individual social network positions but not group network structures in experimental populations of forked fungus beetles""\n#This script measures the effect of resource distribution on group social network metrics (average shortest path length, density, global clustering coefficient). This script uses paired t-tests to answer how resource distribution affects population-level social network metrics. The population-level social network metrics are built from networks that include all interactions and both sexes.\n#Data are available at **add dryad link**\n\n#set working directory to where you saved the data file\nsetwd("""")\n\n#load libraries\nlibrary(tidyverse)\nlibrary(car)\n\n#Change R\'s defult to contrasts \noptions(contrasts=c(""contr.sum"", ""contr.poly"")) \n\n#upload data\nObsPopSN <- read.csv(""groupSN.csv"")\n\n#restructure group SN dataframe for paired t-tests####\nObsPopSN %>%\n  filter(Treatment == ""Clumped"") %>%\n  dplyr::select(avg.shortest.path, global.cc, densityw, Condo) %>%\n  dplyr::rename(avg.shortest.path.clump=avg.shortest.path, global.cc.clump=global.cc, densityw.clump=densityw) -> PopSNClump\n\nObsPopSN %>%\n  filter(Treatment == ""Dispersed"") %>%\n  dplyr::select(avg.shortest.path, global.cc, densityw, Condo) %>%\n  dplyr::rename( avg.shortest.path.disperse=avg.shortest.path, global.cc.disperse=global.cc, densityw.disperse=densityw) -> PopSNDisperse\n\nPopSNLong <- merge(PopSNClump, PopSNDisperse, by=""Condo"", all.x=TRUE, all.y=FALSE)\n\n#Do group social network metrics differ across resource distribution treatments?####\n##weighted network density####\n\n###paired t-test\nt.test(PopSNLong$densityw.clump, PopSNLong$densityw.disperse, paired=TRUE, conf.level=0.95)\n\n###check assumptions\ndensityw.difference = PopSNLong$densityw.clump - PopSNLong$densityw.disperse\nhist(densityw.difference)\nqqp(densityw.difference)\n\n##average shortest path length####\n\n###paired t-test\nt.test(PopSNLong$avg.shortest.path.clump, PopSNLong$avg.shortest.path.disperse, paired=TRUE, conf.level=0.95)\n\n###check assumptions\navg.shortest.path.difference = PopSNLong$avg.shortest.path.clump - PopSNLong$avg.shortest.path.disperse\nhist(avg.shortest.path.difference)\nqqp(avg.shortest.path.difference)\n\n##global clustering coefficient####\n\n###paired t-test\nt.test(PopSNLong$global.cc.clump, PopSNLong$global.cc.disperse, paired=TRUE, conf.level=0.95)\n\n###check assumptions\nglobal.cc.difference = PopSNLong$global.cc.clump - PopSNLong$global.cc.disperse\nhist(global.cc.difference)\nqqp(global.cc.difference)', '#R code for the analyses on individual social network metrics in the manuscript ""Resource distribution alters individual social network positions but not group network structures in experimental populations of forked fungus beetles""\n#This script measures the effect of resource distribution on individual social network metrics (strength, betweenness, and clustering coefficient). This script first runs models with the observed dataset. Then, this script performs a node-level permutation to create 5000 randomizations of the observed dataset. This script then generates a null distribution of coefficients from models that use these randomized datasets. After generating a null distribution of coefficients, this script then compares the coefficients from the models that use the observed dataset and calculates p-values.\n#Data are available at **add dryad link**\n\n#set working directory to where you saved the data file\nsetwd(""/Users/robincostello/Dropbox/Manuscripts/SN/For Resubmission"")\n\n#load libraries\nlibrary(lme4)\nlibrary(car)\nlibrary(glmmTMB)\nlibrary(DHARMa)\nlibrary(effects)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(foreach)\nlibrary(doParallel)\nlibrary(ggpubr)\nlibrary(ggeffects)\nlibrary(emmeans)\nlibrary(ggsignif)\n\n#change R\'s default to contrasts \noptions(contrasts=c(""contr.sum"", ""contr.poly"")) \n\n#upload data\nIndividSN <- read.csv(""IndividSN.csv"")\n\n#observed data models####\n\n##function to standardize a variable \nscale_this <- function(x){\n  (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)\n}\n\n##standardize variables\nIndividSN %>%\n  mutate(StnScansSeen = scale_this(ScansSeen)) %>%\n  mutate(StnElytra = scale_this(Elytra)) -> IndividSN\n\n##strength####\nobs.strength.sex.0inflation <- glmmTMB(alpha~Treatment+Survey_Sex+StnScansSeen+StnElytra+Period+(1|Condo/ID), ziformula=~1, data=IndividSN)\nsummary(obs.strength.sex.0inflation)\nAnova(obs.strength.sex.0inflation, type=3)\nplot(allEffects(obs.strength.sex.0inflation))\n\n###test assumptions with DHARMa\nobs.strength.sex.0inflation.resid<-simulateResiduals(obs.strength.sex.0inflation, n=250, integerResponse=F)\nplot(obs.strength.sex.0inflation.resid)\ntestDispersion(obs.strength.sex.0inflation.resid)\ntestZeroInflation(obs.strength.sex.0inflation.resid)\n\n###extract estimates\nobs.strength.treatment.sex.coef.trt<-summary(obs.strength.sex.0inflation)$coefficients$cond[2,1]\nobs.strength.treatment.sex.coef.sex<-summary(obs.strength.sex.0inflation)$coefficients$cond[3,1]\nobs.strength.treatment.sex.coef.scansseen<-summary(obs.strength.sex.0inflation)$coefficients$cond[4,1]\nobs.strength.treatment.sex.coef.elytra<-summary(obs.strength.sex.0inflation)$coefficients$cond[5,1]\nobs.strength.treatment.sex.coef.period<-summary(obs.strength.sex.0inflation)$coefficients$cond[6,1]\n\n###calculate marginal means\nemmeans(obs.strength.sex.0inflation, ""Treatment"")\nemmeans(obs.strength.sex.0inflation, ""Period"")\nemmeans(obs.strength.sex.0inflation, ""Survey_Sex"")\n\n###calculate effect size (Cohen\'s F2)\n####r2full\nr2full <- r2_nakagawa(obs.strength.sex.0inflation)\n####r2treatment\nobs.strength.sex.0inflation.notrt <- glmmTMB(alpha~Survey_Sex+StnScansSeen+StnElytra+Period+(1|Condo/ID), ziformula=~1, data=IndividSN)\nr2treatment <- r2_nakagawa(obs.strength.sex.0inflation.notrt)\n####f2treament\nf2treatment <- (r2full$R2_marginal - r2treatment$R2_marginal) / (1 - r2full$R2_marginal)\n####r2sex\nobs.strength.sex.0inflation.nosex <- glmmTMB(alpha~Treatment+StnScansSeen+StnElytra+Period+(1|Condo/ID), ziformula=~1, data=IndividSN)\nr2sex <- r2_nakagawa(obs.strength.sex.0inflation.nosex)\n####f2sex\nf2sex <- (r2full$R2_marginal - r2sex$R2_marginal) / (1 - r2full$R2_marginal)\n####r2elytra\nobs.strength.sex.0inflation.noelytra <- glmmTMB(alpha~Treatment+Survey_Sex+StnScansSeen+Period+(1|Condo/ID), ziformula=~1, data=IndividSN)\nr2elytra <- r2_nakagawa(obs.strength.sex.0inflation.noelytra)\n####f2elytra\nf2elytra <- (r2full$R2_marginal - r2elytra$R2_marginal) / (1 - r2full$R2_marginal)\n####r2period\nobs.strength.sex.0inflation.noperiod <- glmmTMB(alpha~Treatment+Survey_Sex+StnScansSeen+StnElytra+(1|Condo/ID), ziformula=~1, data=IndividSN)\nr2period <- r2_nakagawa(obs.strength.sex.0inflation.noperiod)\n####f2period\nf2period <- (r2full$R2_marginal - r2period$R2_marginal) / (1 - r2full$R2_marginal)\n####r2scans\nobs.strength.sex.0inflation.noscans <- glmmTMB(alpha~Treatment+Survey_Sex+StnElytra+Period+(1|Condo/ID), ziformula=~1, data=IndividSN)\nr2scans <- r2_nakagawa(obs.strength.sex.0inflation.noscans)\n####f2scans\nf2scans <- (r2full$R2_marginal - r2scans$R2_marginal) / (1 - r2full$R2_marginal)\n\n##betweenness####\nobs.betweenness.sex.0inflation <- glmmTMB(betweenness~Treatment+Survey_Sex+StnScansSeen+StnElytra+Period+(1|Condo/ID), data=IndividSN, ziformula=~1)\nsummary(obs.betweenness.sex.0inflation)\nAnova(obs.betweenness.sex.0inflation, type=3)\nplot(allEffects(obs.betweenness.sex.0inflation))\n\n###test for assumptions with DHARMa\nobs.betweenness.sex.0inflation.resid<-simulateResiduals(obs.betweenness.sex.0inflation, n=250)\nplot(obs.betweenness.sex.0inflation.resid)\ntestDispersion(obs.betweenness.sex.0inflation.resid)\ntestZeroInflation(obs.betweenness.sex.0inflation.resid)\n\n###extract estimates\nobs.betweenness.treatment.sex.coef.trt<-summary(obs.betweenness.sex.0inflation)$coefficients$cond[2,1]\nobs.betweenness.treatment.sex.coef.sex<-summary(obs.betweenness.sex.0inflation)$coefficients$cond[3,1]\nobs.betweenness.treatment.sex.coef.scansseen<-summary(obs.betweenness.sex.0inflation)$coefficients$cond[4,1]\nobs.betweenness.treatment.sex.coef.elytra<-summary(obs.betweenness.sex.0inflation)$coefficients$cond[5,1]\nobs.betweenness.treatment.sex.coef.period<-summary(obs.betweenness.sex.0inflation)$coefficients$cond[6,1]\n\n###calculate marginal means\nemmeans(obs.betweenness.sex.0inflation, ""Treatment"")\nemmeans(obs.betweenness.sex.0inflation, ""Period"")\nemmeans(obs.betweenness.sex.0inflation, ""Survey_Sex"")\n\n###calculate effect size (Cohen\'s F2)\n####r2full\nr2full <- r2_nakagawa(obs.betweenness.sex.0inflation, tolerance = 1e-06)\n####r2treatment\nobs.betweenness.sex.0inflation.notrt <- glmmTMB(betweenness~Survey_Sex+StnScansSeen+StnElytra+Period+(1|Condo/ID), ziformula=~1, data=IndividSN)\nr2treatment <- r2_nakagawa(obs.betweenness.sex.0inflation.notrt, tolerance = 1e-06)\n####f2treament\nf2treatment <- (r2full$R2_marginal - r2treatment$R2_marginal) / (1 - r2full$R2_marginal)\n####r2sex\nobs.betweenness.sex.0inflation.nosex <- glmmTMB(betweenness~Treatment+StnScansSeen+StnElytra+Period+(1|Condo/ID), ziformula=~1, data=IndividSN)\nr2sex <- r2_nakagawa(obs.betweenness.sex.0inflation.nosex, tolerance = 1e-06)\n####f2sex\nf2sex <- (r2full$R2_marginal - r2sex$R2_marginal) / (1 - r2full$R2_marginal)\n####r2elytra\nobs.betweenness.sex.0inflation.noelytra <- glmmTMB(betweenness~Treatment+Survey_Sex+StnScansSeen+Period+(1|Condo/ID), ziformula=~1, data=IndividSN)\nr2elytra <- r2_nakagawa(obs.betweenness.sex.0inflation.noelytra, tolerance = 1e-06)\n####f2elytra\nf2elytra <- (r2full$R2_marginal - r2elytra$R2_marginal) / (1 - r2full$R2_marginal)\n####r2period\nobs.betweenness.sex.0inflation.noperiod <- glmmTMB(betweenness~Treatment+Survey_Sex+StnScansSeen+StnElytra+(1|Condo/ID), ziformula=~1, data=IndividSN)\nr2period <- r2_nakagawa(obs.betweenness.sex.0inflation.noperiod, tolerance = 1e-08)\n####f2period\nf2period <- (r2full$R2_marginal - r2period$R2_marginal) / (1 - r2full$R2_marginal)\n####r2scans\nobs.betweenness.sex.0inflation.noscans <- glmmTMB(betweenness~Treatment+Survey_Sex+StnElytra+Period+(1|Condo/ID), ziformula=~1, data=IndividSN)\nr2scans <- r2_nakagawa(obs.betweenness.sex.0inflation.noscans, tolerance = 1e-36)\n####f2scans\nf2scans <- (r2full$R2_marginal - r2scans$R2_marginal) / (1 - r2full$R2_marginal)\n\n##clustering coefficient####\nobs.cc.sex.0inflation<-glmmTMB(am~Treatment+Survey_Sex+StnScansSeen+StnElytra+Period+(1|Condo/ID), data=IndividSN, na.action=na.omit, ziformula=~1)\nsummary(obs.cc.sex.0inflation)\nAnova(obs.cc.sex.0inflation, type=3)\nplot(allEffects(obs.cc.sex.0inflation))\n\n###test for assumptions with DHARMa\nobs.cc.sex.0inflation.resid<-simulateResiduals(obs.cc.sex.0inflation, n=250, integerResponse=FALSE)\nplot(obs.cc.sex.0inflation.resid)\ntestDispersion(obs.cc.sex.0inflation.resid)\ntestZeroInflation(obs.cc.sex.0inflation.resid)\n\n###extract estimates\nobs.cc.treatment.sex.coef.trt<-summary(obs.cc.sex.0inflation)$coefficients$cond[2,1]\nobs.cc.treatment.sex.coef.sex<-summary(obs.cc.sex.0inflation)$coefficients$cond[3,1]\nobs.cc.treatment.sex.coef.scansseen<-summary(obs.cc.sex.0inflation)$coefficients$cond[4,1]\nobs.cc.treatment.sex.coef.elytra<-summary(obs.cc.sex.0inflation)$coefficients$cond[5,1]\nobs.cc.treatment.sex.coef.period<-summary(obs.cc.sex.0inflation)$coefficients$cond[6,1]\n\n###calculate marginal means\nemmeans(obs.cc.sex.0inflation, ""Treatment"")\nemmeans(obs.cc.sex.0inflation, ""Period"")\nemmeans(obs.cc.sex.0inflation, ""Survey_Sex"")\n\n###calculate effect size (Cohen\'s F2)\n####r2full\nr2full <- r2_nakagawa(obs.cc.sex.0inflation, tolerance = 1e-12)\n####r2treatment\nobs.cc.sex.0inflation.notrt <- glmmTMB(am~Survey_Sex+StnScansSeen+StnElytra+Period+(1|Condo/ID), ziformula=~1, data=IndividSN, na.action=na.omit)\nr2treatment <- r2_nakagawa(obs.cc.sex.0inflation.notrt, tolerance = 1e-12)\n####f2treament\nf2treatment <- (r2full$R2_marginal - r2treatment$R2_marginal) / (1 - r2full$R2_marginal)\n####r2sex\nobs.cc.sex.0inflation.nosex <- glmmTMB(am~Treatment+StnScansSeen+StnElytra+Period+(1|Condo/ID), ziformula=~1, na.action=na.omit, data=IndividSN)\nr2sex <- r2_nakagawa(obs.cc.sex.0inflation.nosex, tolerance = 1e-11)\n####f2sex\nf2sex <- (r2full$R2_marginal - r2sex$R2_marginal) / (1 - r2full$R2_marginal)\n####r2elytra\nobs.cc.sex.0inflation.noelytra <- glmmTMB(am~Treatment+Survey_Sex+StnScansSeen+Period+(1|Condo/ID), ziformula=~1, na.action=na.omit, data=IndividSN)\nr2elytra <- r2_nakagawa(obs.cc.sex.0inflation.noelytra, tolerance = 1e-11)\n####f2elytra\nf2elytra <- (r2full$R2_marginal - r2elytra$R2_marginal) / (1 - r2full$R2_marginal)\n####r2period\nobs.cc.sex.0inflation.noperiod <- glmmTMB(am~Treatment+Survey_Sex+StnScansSeen+StnElytra+(1|Condo/ID), ziformula=~1, na.action=na.omit, data=IndividSN)\nr2period <- r2_nakagawa(obs.cc.sex.0inflation.noperiod, tolerance = 1e-12)\n####f2period\nf2period <- (r2full$R2_marginal - r2period$R2_marginal) / (1 - r2full$R2_marginal)\n####r2scans\nobs.cc.sex.0inflation.noscans <- glmmTMB(am~Treatment+Survey_Sex+StnElytra+Period+(1|Condo/ID), ziformula=~1, na.action=na.omit, data=IndividSN)\nr2scans <- r2_nakagawa(obs.cc.sex.0inflation.noscans, tolerance = 1e-11)\n####f2scans\nf2scans <- (r2full$R2_marginal - r2scans$R2_marginal) / (1 - r2full$R2_marginal)\n\n#create permuted datasets####\n\n##set number of permutations\npermutations=10000\n\n##shuffle observed data\nfor(i in 1:permutations) {\n  PermIndividSN[[i]] <- IndividSN %>% mutate(StnElytra=sample(StnElytra)) %>% mutate(StnScansSeen=sample(StnScansSeen)) %>% mutate(Treatment=sample(Treatment)) %>% mutate(Survey_Sex=sample(Survey_Sex)) %>% mutate(Period=sample(Period))\n}\n\n##name each permutation sequentially\nnames(PermIndividSN) <- paste(""perm"", 1:length(PermIndividSN), sep=""_"")\n\n#permuted data models####\n##function that extracts the model summary\nModel.Summary=function(x){\n  setTimeLimit(cpu = Inf,  transient = FALSE)\n  return(tryCatch(summary(x), error=function(e) NULL))\n}\n\n##strength####\n###function that runs the model\nperm.strength = function(PermIndividSN){\n  perm.strength.mod<-glmmTMB(alpha~Treatment+Survey_Sex+StnScansSeen+StnElytra+Period+(1|Condo/ID), ziformula=~1, data=PermIndividSN)\n  return(perm.strength.mod)\n}\n\n###run the model on each permutation\nperm.strength.results<-lapply(PermIndividSN, FUN=perm.strength)\n\n###extract model summary for each permutation\nperm.strength.summary<-lapply(perm.strength.results, FUN=Model.Summary)\n\n###remove models that do not converge (NA AIC values and NA P values)\n####first extract AIC values from models\nperm.strength.AIC<-foreach(i = 1:length(perm.strength.summary)) %dopar% {\n  perm.strength.summary[[i]][[""AICtab""]][[""AIC""]]\n}\nnames(perm.strength.AIC) <- paste(""perm"", 1:length(perm.strength.AIC), sep=""_"")\n####make list of permutations with NA AIC values\nperm.strength.AIC.NA<-perm.strength.AIC[is.na(perm.strength.AIC)]\n####now, remove models that do not have AIC values\nperm.strength.summary.converged<-perm.strength.summary[names(perm.strength.summary) %sans% names(perm.strength.AIC.NA)]\n####because this doesn\'t account for all the models that did not converge, also need to remove models with NA P values. extra P values from models\nperm.strength.p<-foreach(i = 1:length(perm.strength.summary.converged)) %dopar% {\n  perm.strength.summary.converged[[i]][[""coefficients""]][[""cond""]][[11]]\n}\nnames(perm.strength.p) <- names(perm.strength.summary.converged)\n####make list of permutations with NA P values\nperm.strength.p.NA<-perm.strength.p[is.na(perm.strength.p)]\n####remove models that have NA P values\nperm.strength.summary.converged<-perm.strength.summary.converged[names(perm.strength.summary.converged) %sans% names(perm.strength.p.NA)]\n\n##select only first 5000 permutations\nperm.strength.summary.converged %>%\n  head(5000) -> perm.strength.summary.converged\n\n##extract model estimate for treatment from model summary\nperm.strength.treatment.coef<-foreach(i = 1:length(perm.strength.summary.converged)) %dopar% {\n  perm.strength.summary.converged[[i]][[""coefficients""]][[""cond""]][[2]]\n}\n##flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.strength.treatment.coef.df<-do.call(rbind, perm.strength.treatment.coef)\n\n##extract model estimate for sex from model summary\nperm.strength.treatment.coef.sex<-foreach(i = 1:length(perm.strength.summary.converged)) %dopar% {\n  perm.strength.summary.converged[[i]][[""coefficients""]][[""cond""]][[3]]\n}\n##flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.strength.treatment.coef.sex.df<-do.call(rbind, perm.strength.treatment.coef.sex)\n\n##extract model estimate for scans seen from model summary\nperm.strength.treatment.coef.scansseen<-foreach(i = 1:length(perm.strength.summary.converged)) %dopar% {\n  perm.strength.summary.converged[[i]][[""coefficients""]][[""cond""]][[4]]\n}\n##flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.strength.treatment.coef.scansseen.df<-do.call(rbind, perm.strength.treatment.coef.scansseen)\n\n##extract model estimate for elytra from model summary\nperm.strength.treatment.coef.elytra<-foreach(i = 1:length(perm.strength.summary.converged)) %dopar% {\n  perm.strength.summary.converged[[i]][[""coefficients""]][[""cond""]][[5]]\n}\n##flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.strength.treatment.coef.elytra.df<-do.call(rbind, perm.strength.treatment.coef.elytra)\n\n##extract model estimate for period from model summary\nperm.strength.treatment.coef.period<-foreach(i = 1:length(perm.strength.summary.converged)) %dopar% {\n  perm.strength.summary.converged[[i]][[""coefficients""]][[""cond""]][[6]]\n}\n##flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.strength.treatment.coef.period.df<-do.call(rbind, perm.strength.treatment.coef.period)\n\n##betweenness####\n###function that runs the model\nperm.betweenness = function(PermIndividSN){\n  perm.betweenness.mod<-glmmTMB(betweenness~Treatment+Survey_Sex+StnScansSeen+StnElytra+Period+(1|Condo/ID), data=PermIndividSN, ziformula=~1)\n  return(perm.betweenness.mod)\n}\n\n####run the model on each permutation\nperm.betweenness.results<-lapply(PermIndividSN, FUN=perm.betweenness)\n\n###extract model summary for each permutation\nperm.betweenness.summary<-lapply(perm.betweenness.results, FUN=Model.Summary)\n\n###remove models that do not converge (NA AIC values and NA P values)\n####first extract AIC values from models\nperm.betweenness.AIC<-foreach(i = 1:length(perm.betweenness.summary)) %dopar% {\n  perm.betweenness.summary[[i]][[""AICtab""]][[""AIC""]]\n}\nnames(perm.betweenness.AIC) <- paste(""perm"", 1:length(perm.betweenness.AIC), sep=""_"")\n####make list of permutations with NA AIC values\nperm.betweenness.AIC.NA<-perm.betweenness.AIC[is.na(perm.betweenness.AIC)]\n####now, remove models that do not have AIC values\nperm.betweenness.summary.converged<-perm.betweenness.summary[names(perm.betweenness.summary) %sans% names(perm.betweenness.AIC.NA)]\n####because this doesn\'t account for all the models that did not converge, also need to remove models with NA P values. extra P values from models\nperm.betweenness.p<-foreach(i = 1:length(perm.betweenness.summary.converged)) %dopar% {\n  perm.betweenness.summary.converged[[i]][[""coefficients""]][[""cond""]][[11]]\n}\nnames(perm.betweenness.p) <- names(perm.betweenness.summary.converged)\n####make list of permutations with NA P values\nperm.betweenness.p.NA<-perm.betweenness.p[is.na(perm.betweenness.p)]\n####remove models that have NA P values\nperm.betweenness.summary.converged<-perm.betweenness.summary.converged[names(perm.betweenness.summary.converged) %sans% names(perm.betweenness.p.NA)]\n\n###select only first 5000 permutations\nperm.betweenness.summary.converged %>%\n  head(5000) -> perm.betweenness.summary.converged\n\n###extract model estimate for treatment from model summary\nperm.betweenness.treatment.coef<-foreach(i = 1:length(perm.betweenness.summary.converged)) %dopar% {\n  perm.betweenness.summary[[i]][[""coefficients""]][[""cond""]][[2]]\n}\n###flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.betweenness.treatment.coef.df<-do.call(rbind, perm.betweenness.treatment.coef)\n\n###extract model estimate for sex from model summary\nperm.betweenness.treatment.coef.sex<-foreach(i = 1:length(perm.betweenness.summary.converged)) %dopar% {\n  perm.betweenness.summary.converged[[i]][[""coefficients""]][[""cond""]][[3]]\n}\n###flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.betweenness.treatment.coef.sex.df<-do.call(rbind, perm.betweenness.treatment.coef.sex)\n\n###extract model estimate for scans seen from model summary\nperm.betweenness.treatment.coef.scansseen<-foreach(i = 1:length(perm.betweenness.summary.converged)) %dopar% {\n  perm.betweenness.summary.converged[[i]][[""coefficients""]][[""cond""]][[4]]\n}\n###flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.betweenness.treatment.coef.scansseen.df<-do.call(rbind, perm.betweenness.treatment.coef.scansseen)\n\n###extract model estimate for elytra from model summary\nperm.betweenness.treatment.coef.elytra<-foreach(i = 1:length(perm.betweenness.summary.converged)) %dopar% {\n  perm.betweenness.summary.converged[[i]][[""coefficients""]][[""cond""]][[5]]\n}\n###flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.betweenness.treatment.coef.elytra.df<-do.call(rbind, perm.betweenness.treatment.coef.elytra)\n\n###extract model estimate for period from model summary\nperm.betweenness.treatment.coef.period<-foreach(i = 1:length(perm.betweenness.summary.converged)) %dopar% {\n  perm.betweenness.summary.converged[[i]][[""coefficients""]][[""cond""]][[6]]\n}\n###flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.betweenness.treatment.coef.period.df<-do.call(rbind, perm.betweenness.treatment.coef.period)\n\n##clustering coefficient####\n###function that runs the model\nperm.cc = function(PermIndividSN){\n  perm.cc.mod<-glmmTMB(am~Treatment+Survey_Sex+StnScansSeen+StnElytra+Period+(1|Condo/ID), data=PermIndividSN, na.action=na.omit, ziformula=~1)\n  return(perm.cc.mod)\n}\n\n###run the model on each permutation\nperm.cc.results<-lapply(PermIndividSN, FUN=perm.cc)\n\n###extract model summary for each permutation\nperm.cc.summary<-lapply(perm.cc.results, FUN=Model.Summary)\n\n###remove models that do not converge (NA AIC values and NA P values)\n####first extract AIC values from models\nperm.cc.AIC<-foreach(i = 1:length(perm.cc.summary)) %dopar% {\n  perm.cc.summary[[i]][[""AICtab""]][[""AIC""]]\n}\nnames(perm.cc.AIC) <- paste(""perm"", 1:length(perm.cc.AIC), sep=""_"")\n####make list of permutations with NA AIC values\nperm.cc.AIC.NA<-perm.cc.AIC[is.na(perm.cc.AIC)]\n####now, remove models that do not have AIC values\nperm.cc.summary.converged<-perm.cc.summary[names(perm.cc.summary) %sans% names(perm.cc.AIC.NA)]\n####because this doesn\'t account for all the models that did not converge, also need to remove models with NA P values. extra P values from models\nperm.cc.p<-foreach(i = 1:length(perm.cc.summary.converged)) %dopar% {\n  perm.cc.summary.converged[[i]][[""coefficients""]][[""cond""]][[11]]\n}\nnames(perm.cc.p) <- names(perm.cc.summary.converged)\n####make list of permutations with NA P values\nperm.cc.p.NA<-perm.cc.p[is.na(perm.cc.p)]\n####remove models that have NA P values\nperm.cc.summary.converged<-perm.cc.summary.converged[names(perm.cc.summary.converged) %sans% names(perm.cc.p.NA)]\n\n###select only first 5000 permutations\nperm.cc.summary.converged %>%\n  head(5000) -> perm.cc.summary.converged\n\n###extract model estimate for treatment from model summary\nperm.cc.treatment.coef<-foreach(i = 1:length(perm.cc.summary.converged)) %dopar% {\n  perm.cc.summary.converged[[i]][[""coefficients""]][[""cond""]][[2]]\n}\n###flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.cc.treatment.coef.df<-do.call(rbind, perm.cc.treatment.coef)\n\n###extract model estimate for sex from model summary\nperm.cc.treatment.coef.sex<-foreach(i = 1:length(perm.cc.summary.converged)) %dopar% {\n  perm.cc.summary.converged[[i]][[""coefficients""]][[""cond""]][[3]]\n}\n###flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.cc.treatment.coef.sex.df<-do.call(rbind, perm.cc.treatment.coef.sex)\n\n###extract model estimate for scans seen from model summary\nperm.cc.treatment.coef.scansseen<-foreach(i = 1:length(perm.cc.summary.converged)) %dopar% {\n  perm.cc.summary.converged[[i]][[""coefficients""]][[""cond""]][[4]]\n}\n###flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.cc.treatment.coef.scansseen.df<-do.call(rbind, perm.cc.treatment.coef.scansseen)\n\n###extract model estimate for elytra from model summary\nperm.cc.treatment.coef.elytra<-foreach(i = 1:length(perm.cc.summary.converged)) %dopar% {\n  perm.cc.summary.converged[[i]][[""coefficients""]][[""cond""]][[5]]\n}\n###flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.cc.treatment.coef.elytra.df<-do.call(rbind, perm.cc.treatment.coef.elytra)\n\n###extract model estimate for period from model summary\nperm.cc.treatment.coef.period<-foreach(i = 1:length(perm.cc.summary.converged)) %dopar% {\n  perm.cc.summary.converged[[i]][[""coefficients""]][[""cond""]][[6]]\n}\n###flatten the list of permutations into a dataframe with one column that contains all the estimates from every permutation\nperm.cc.treatment.coef.period.df<-do.call(rbind, perm.cc.treatment.coef.period)\n\n#calculate p-values####\n# FUNCTION: CWW (2015) Calculates one- and two-tailed p-values from a permuted distribution  \n# Author Corlett Wolfe Wood\n\n# One-tailed: The proportion of permuted values that are GREATER THAN the observed value\n# Two-tailed: The proportion of permuted values that are MORE EXTREME than the observed\n\n# NB: Calculation of two-tailed p-values on asymmetric null distributions\n# On symmetric distributions, one-tailed p-values can be doubled to obtained two-tailed ones\n# This doesn\'t work with asymmetric distributions because of skewness\n# --> SOLUTION: This function below calculates two-tailed p-values as the proportion of\n#     permuted.values that have a probability <= the probability of the observed value\n# --> SOURCE: Pratt, J. W. and J. D. Gibbons. 1981. Concepts in Nonparametric Theory. \n#     Springer-Verlag, New York, NY pp. 29-32\n\n# NB: p-values should NEVER be zero in a permutation test\n# --> the observed data is one possible permutation, so at least one value\n#     in the permuted data can be equal to the observed data (P>0)\n# --> SOURCE: North, B. V., D. Curtis, and P. C. Sham. 2002. A note on the calculation of empirical \n#     p values from Monte Carlo procedures. Am. J. Hum. Genet. 71:439-441.\n# --> SOURCE: Davison, A. C. and D. V. Hinkley. 1997. Bootstrap methods and their application. \n#     Cambridge University Press, Cambridge, United Kingdom\n# --> SOLUTION: add 1 to the numerator and the denominator\n\npvals = function(observed.value, permuted.values){  \n  # ONE-TAILED\n  # Proportion of permuted values >= (if in upper tail) OR <= (if in lower tail) the observed value\n  k.greater = length(which(permuted.values>=observed.value))\n  k.less = length(which(permuted.values<=observed.value))\n  n = length(permuted.values)\n  # One-tailed p-value\n  pval.onetail = (min(k.greater, k.less)+1)/(n+1) # See NB above for explanation of +1  \n  \n  # TWO-TAILED\n  # Proportion of permuted values with PROBABILITIES <= the observed value\n  df <- data.frame(permuted.values=permuted.values, prob=NA)\n  names(df) <- c(""permuted.values"", ""prob"")\n  for(p in 1:nrow(df)){\n    # Calculate probability of each permuted value\n    # Proportion of permuted values >= (if in upper tail) OR <= (if in lower tail) each permuted value\n    k.greater = length(which(df$permuted.values>=df$permuted.values[p]))\n    k.less = length(which(df$permuted.values<=df$permuted.values[p]))\n    n = nrow(df)\n    df$prob[p] = (min(k.greater, k.less)+1)/(n+1) # See NB above for explanation of +1\n  }\n  \n  # Proportion of permuted.values with a have a probability <= the probability of the observed value\n  prob.less <- length(which(df$prob<=pval.onetail))\n  pval.twotail <- (prob.less+1)/(n+1) # See NB above for explanation of +1\n  \n  return(c(one.tailed=pval.onetail, \n           two.tailed=pval.twotail))\n}\n\n##strength####\nstrength.pvals<-pvals(obs.strength.treatment.sex.coef.trt, perm.strength.treatment.coef.df)\nstrength.pvals.sex<-pvals(obs.strength.treatment.sex.coef.sex, perm.strength.treatment.coef.sex.df)\nstrength.pvals.scansseen<-pvals(obs.strength.treatment.sex.coef.scansseen, perm.strength.treatment.coef.scansseen.df)\nstrength.pvals.elytra<-pvals(obs.strength.treatment.sex.coef.elytra, perm.strength.treatment.coef.elytra.df)\nstrength.pvals.period<-pvals(obs.strength.treatment.sex.coef.period, perm.strength.treatment.coef.period.df)\n\n##betweenness####\nbetweenness.pvals<-pvals(obs.betweenness.treatment.sex.coef.trt, perm.betweenness.treatment.coef.df)\nbetweenness.pvals.sex<-pvals(obs.betweenness.treatment.sex.coef.sex, perm.betweenness.treatment.coef.sex.df)\nbetweenness.pvals.scansseen<-pvals(obs.betweenness.treatment.sex.coef.scansseen, perm.betweenness.treatment.coef.scansseen.df)\nbetweenness.pvals.elytra<-pvals(obs.betweenness.treatment.sex.coef.elytra, perm.betweenness.treatment.coef.elytra.df)\nbetweenness.pvals.period<-pvals(obs.betweenness.treatment.sex.coef.period, perm.betweenness.treatment.coef.period.df)\n\n##clustering coefficient####\ncc.pvals<-pvals(obs.cc.treatment.sex.coef.trt, perm.cc.treatment.coef.df)\ncc.pvals.sex<-pvals(obs.cc.treatment.sex.coef.sex, perm.cc.treatment.coef.sex.df)\ncc.pvals.scansseen<-pvals(obs.cc.treatment.sex.coef.scansseen, perm.cc.treatment.coef.scansseen.df)\ncc.pvals.elytra<-pvals(obs.cc.treatment.sex.coef.elytra, perm.cc.treatment.coef.elytra.df)\ncc.pvals.period<-pvals(obs.cc.treatment.sex.coef.period, perm.cc.treatment.coef.period.df)']","Group and individual social network metrics are robust to changes in resource distribution in experimental populations of forked fungus beetles Social interactions drive many important ecological and evolutionary processes. It is therefore essential to understand the intrinsic and extrinsic factors that underlie social patterns. A central tenet of the field of behavioral ecology is the expectation that the distribution of resources shapes patterns of social interactions.We combined experimental manipulations with social network analyses to ask how patterns of resource distribution influence complex social interactions.We experimentally manipulated the distribution of an essential food and reproductive resource in semi-natural populations of forked fungus beetles (Bolitotherus cornutus). We aggregated resources into discrete clumps in half of the populations and evenly dispersed resources in the other half. We then observed social interactions between individually marked beetles. Half-way through the experiment, we reversed the resource distribution in each population, allowing us to control any demographic or behavioral differences between our experimental populations. At the end of the experiment, we compared individual and group social network characteristics between the two resource distribution treatments.We found a statistically significant but quantitatively small effect of resource distribution on individual social network position and detected no effect on group social network structure. Individual connectivity (individual strength) and individual cliquishness (local clustering coefficient) increased in environments with clumped resources, but this difference explained very little of the variance in individual social network position. Individual centrality (individual betweenness) and measures of overall social structure (network density, average shortest path length, and global clustering coefficient) did not differ between environments with dramatically different distributions of resources.Our results illustrate that the resource environment, despite being fundamental to our understanding of social systems, does not always play a central role in shaping social interactions. Instead, our results suggests that sex differences and temporally fluctuating environmental conditions may be more important in determining patterns of social interactions.",2
"Replication files for ""Building social cohesion between Christians and Muslims through soccer in post-ISIS Iraq""","Replication files for main analyses, and supplementary analyses (comparison group, Muslim player attitudes, fan attitudes, match-level data) for:Mousa, Salma. ""Building social cohesion between Christians and Muslims through soccer in Post-ISIS Iraq."" Science. Vol. 369, Issue 6505, pp. 866-870. DOI: 10.1126/science.abb3153Each R file describes the needed datasets at the top of the script.",,"Replication files for ""Building social cohesion between Christians and Muslims through soccer in post-ISIS Iraq"" Replication files for main analyses, and supplementary analyses (comparison group, Muslim player attitudes, fan attitudes, match-level data) for:Mousa, Salma. ""Building social cohesion between Christians and Muslims through soccer in Post-ISIS Iraq."" Science. Vol. 369, Issue 6505, pp. 866-870. DOI: 10.1126/science.abb3153Each R file describes the needed datasets at the top of the script.",2
Do We Adopt the Intentional Stance Toward Humanoid Robots?,"In daily social interactions, we need to be able to navigate efficiently through our social environment. According to Dennett (1971), explaining and predicting others behavior with reference to mental states (adopting the intentional stance) allows efficient social interaction. Today we also routinely interact with artificial agents: from Apples Siri to GPS navigation systems. In the near future, we might start casually interacting with robots. This paper addresses the question of whether adopting the intentional stance can also occur with respect to artificial agents. We propose a new tool to explore if people adopt the intentional stance toward an artificial agent (humanoid robot). The tool consists in a questionnaire that probes participants stance by requiring them to choose the likelihood of an explanation (mentalistic vs. mechanistic) of a behavior of a robot iCub depicted in a naturalistic scenario (a sequence of photographs). The results of the first study conducted with this questionnaire showed that although the explanations were somewhat biased toward the mechanistic stance, a substantial number of mentalistic explanations were also given. This suggests that it is possible to induce adoption of the intentional stance toward artificial agents, at least in some contexts.","['### Script to analyze InStance data\r\n\r\n#instance <- read.table(""C:/Users/ebaykara/Documents/InStance_Questionnaire/data_s4hri_2018-05-17_14-57.txt"", header=TRUE, sep=""\\t"",dec=""."", na.strings = "" "",quote = """")\r\n\r\n### This code reads the file with the new data, name it differently than the main data file\r\ninstq <- read.csv(""C:/Users/ebaykara/Documents/InStance_Questionnaire/data_s4hri_2018-05-17_14-57.csv"", header=TRUE, sep="","",dec=""."", na.strings = "" "")\r\nnew_test <- read.csv(""C:/Users/ebaykara/Documents/InStance_Questionnaire/data_s4hri_2018-05-21_10-23.csv"", header=TRUE, sep="","",dec=""."", na.strings = "" "")\r\n\r\n## Run the function files to include them in the Global Environment\r\nsubt <- function(x){  ##subt is to subtract 1 from the scoring of each question (1-101->0-100)\r\n  \r\n  x <- x-1\r\n  return (x)\r\n\r\n}\r\n\r\nrev <- function(x){   ##Reverse the scoring for questions 19-35, where intentional and mechanistic are reversed in the questionnaire.\r\n  \r\n  x <- 100-x\r\n  return (x)\r\n  \r\n}\r\n  \r\n\r\nff <- dput(grep(""^IS.._..$"", names(new_test), value = TRUE)) # two letter variable names\r\n\r\nqq <- data.frame(mapply(subt, new_test[,ff[1:35]]))\r\nrr <- mapply(rev, qq[,ff[19:35]])\r\nqdata <- cbind(qq[,1:18],rr)\r\n\r\nnew_test <- new_test[ , -which(names(new_test) %in% ff[1:35])]  ## Remove the original scoring data (1-101, reverse ordered)\r\nnew_test <- cbind(new_test,qdata)  ## Add the new calculated scores for each question\r\n\r\ninstance$Avg_Score <- rowMeans(instance[,ff[2:35]])  ### Calculate average score for each participant\r\n\r\n\r\ninstance$Fam <- ifelse(instance$IS43==1,1,2)  ## Code Familiarity as 1=No, 2=yes\r\n\r\n\r\n### Coding of the gender 1=F, 2=M\r\ninstance$Gender[instance$IS37_02 == ""f""] <- 1\r\ninstance$Gender[instance$IS37_02 == ""F""] <- 1\r\ninstance$Gender[instance$IS37_02 == ""m""] <- 2\r\ninstance$Gender[instance$IS37_02 == ""M""] <- 2\r\n\r\n\r\n### Analysis \r\n\r\nfit <- lm(Avg_Score~Fam, data=instance)\r\nsummary(fit)\r\n\r\naggregate(test$Avg_Score, by=list(test$Fam), FUN=mean)\r\nlength(which(instance$Fam==1))\r\nlength(which(instance$Fam==2))\r\n\r\nfit <- lm(Avg_Score~Gender + IS37_03 + IS41_01 + IS37_04 + IS37_05, data=NonFam_cl)\r\nsummary(fit)\r\n\r\n### Create a subset of participants who are not familiar with robots\r\nNonFam <- subset(test,Fam==1)\r\n\r\ncor.test(NonFam$Avg_Score,NonFam$MindEyes_Correct,method=""spearman"")\r\ncor.test(NonFam$Avg_Score,NonFam$Vollm_Correct,method=""spearman"")\r\n\r\n## Gender\r\nfit <- lm(Avg_Score~Gender, data=NonFam_cl)\r\nsummary(fit)\r\n\r\n## Age\r\nfit <- lm(Avg_Score~IS37_03, data=NonFam_cl)\r\nsummary(fit)\r\n\r\n## Education\r\nfit <- lm(Avg_Score~IS41_01, data=NonFam_cl)\r\nsummary(fit)\r\n\r\n### Children\r\nfit <- lm(Avg_Score~IS37_04, data=NonFam_cl)\r\nsummary(fit)\r\n\r\n### Siblings\r\nfit <- lm(Avg_Score~IS37_05, data=NonFam_cl)\r\nsummary(fit)\r\n\r\n### Chi-Square of the distribution of mechanistic vs intentional answers\r\n\r\nNonFam$Group[NonFam$Avg_Score > 50] <- 2\r\nNonFam$Group[NonFam$Avg_Score < 50] <- 1\r\n\r\ncs <- table(NonFam$Group)\r\nchisq.test(cs)\r\n\r\n\r\n\r\n### Compare score of different groups to 50 (0 of our scale)\r\naggregate(x_notfam$Avg_Score, by=list(x_notfam$Group), FUN=mean)\r\nG1 <- NonFam$Avg_Score[NonFam$Group==1]\r\nG2 <- NonFam$Avg_Score[NonFam$Group==2]\r\n\r\nt.test(G1,mu=50)\r\nt.test(G2,mu=50)\r\n\r\n### Standard error of means\r\nsd(G1, na.rm=TRUE) / sqrt(length(G1[!is.na(G1)])) \r\nsd(G2, na.rm=TRUE) / sqrt(length(G2[!is.na(G2)])) \r\n', 'instance_nov2018<- read.table(""//IITSCHRIWS001/S4HRI_localrepo/Experiments/InStance_Questionnaire/instance_Nov2018.txt"", header=TRUE, sep=""\\t"",dec="","", na.strings = "" "",quote = """")\r\nView(instance_nov2018)\r\n\r\nmean(instance_nov2018$IS37_03)#mean age\r\nsd(instance_nov2018$IS37_03)\r\nmin(instance_nov2018$IS37_03)\r\nmax(instance_nov2018$IS37_03)\r\nlength(which(instance_nov2018$Gender==1))\r\nmean(instance_nov2018$IS41_01)#mean Education\r\nsd(instance_nov2018$IS41_01)\r\nmin(instance_nov2018$IS41_01)\r\nmax(instance_nov2018$IS41_01)\r\n### Analysis (sample =106)\r\nshapiro.test(instance_nov2018$Avg_Score)\r\nmean(instance_nov2018$Avg_Score)\r\nsd(instance_nov2018$Avg_Score)\r\nISS_106<-instance_nov2018$Avg_Score\r\nt.test(ISS_106,mu=0)\r\n#Multiple regression for Gender, Age, Education, Children, Siblings\r\nfit2 <- lm(Avg_Score~Gender + IS37_03 + IS41_01 + IS37_04 + IS37_05, data=instance_nov2018)\r\nsummary(fit2)\r\n  #Correlation with MindEyes and Vollm\r\n  \r\ncor.test(instance_nov2018$Avg_Score, instance_nov2018$MindEyes_Correct,method=""spearman"")\r\ncor.test(instance_nov2018$Avg_Score,instance_nov2018$Vollm_Correct,method=""spearman"")\r\n\r\n\r\n#linear regression for familiarity\r\nlength(which(instance_nov2018$Fam==1))\r\nlength(which(instance_nov2018$Fam==2))\r\nfit <- lm(Avg_Score~Fam, data=instance_nov2018)\r\nsummary(fit)\r\n\r\n\r\n### Create a subset of participants who are not familiar with robots\r\nNonFam_nov2018 <- subset(instance_nov2018, Fam==1)\r\nFam_nov2018 <- subset(instance_nov2018, Fam==2)\r\nmean(NonFam_nov2018$Avg_Score)\r\nmean(Fam_nov2018$Avg_Score)\r\nsd(NonFam_nov2018$Avg_Score)\r\nsd(Fam_nov2018$Avg_Score)\r\nISS_Fam<-Fam_nov2018$Avg_Score\r\nt.test(ISS_Fam,mu=0)\r\nISS_NonFam<-NonFam_nov2018$Avg_Score\r\nt.test(ISS_NonFam, mu=0)\r\n### Standard error of means\r\n\r\nperc_NonFam<-length(which(instance_nov2018$Fam==1))/106\r\nperc_Fam<-length(which(instance_nov2018$Fam==2))/106\r\n\r\n\r\n### Analysis on Non Familiar (sample =106)\r\n\r\nshapiro.test(NonFam_nov2018$Avg_Score)\r\n\r\n#Multiple regression for Gender, Age, Education, Children, Siblings\r\nNonFam_nov2018_fit2 <- lm(Avg_Score~Gender + IS37_03 + IS41_01 + IS37_04 + IS37_05, data=NonFam_nov2018)\r\nsummary(NonFam_nov2018_fit2)\r\n\r\ncor.test(NonFam_nov2018$Avg_Score,NonFam_nov2018$MindEyes_Correct,method=""spearman"")\r\ncor.test(NonFam_nov2018$Avg_Score,NonFam_nov2018$Vollm_Correct,method=""spearman"")\r\n\r\n## Gender\r\nfit <- lm(Avg_Score~Gender, data=NonFam_cl)\r\nsummary(fit)\r\n\r\n## Age\r\nfit <- lm(Avg_Score~IS37_03, data=NonFam_cl)\r\nsummary(fit)\r\n\r\n## Education\r\nfit <- lm(Avg_Score~IS41_01, data=NonFam_cl)\r\nsummary(fit)\r\n\r\n### Children\r\nfit <- lm(Avg_Score~IS37_04, data=NonFam_cl)\r\nsummary(fit)\r\n\r\n### Siblings\r\nfit <- lm(Avg_Score~IS37_05, data=NonFam_cl)\r\nsummary(fit)\r\n\r\n### Chi-Square of the distribution of mechanistic vs intentional answers\r\n\r\nNonFam_nov2018$Group[NonFam_nov2018$Avg_Score > 50] <- 2\r\nNonFam_nov2018$Group[NonFam_nov2018$Avg_Score < 50] <- 1\r\n\r\ncs <- table(NonFam_nov2018$Group)\r\nchisq.test(cs)\r\n\r\n\r\n\r\n### Compare score of different groups to 50 (0 of our scale)\r\n#Not familiar group\r\naggregate(x_notfam$Avg_Score, by=list(x_notfam$Group), FUN=mean)\r\nG1 <- NonFam_nov2018$Avg_Score[NonFam_nov2018$Group==1]\r\nG2 <- NonFam_nov2018$Avg_Score[NonFam_nov2018$Group==2]\r\nlength(which(NonFam_nov2018$Group==1))\r\nlength(which(NonFam_nov2018$Group==2))\r\nperc_NonFam_mec<-length(which(NonFam_nov2018$Group==1))/89\r\nperc_NonFam_ment<-length(which(NonFam_nov2018$Group==2))/89\r\n\r\n\r\nt.test(G1,mu=50)\r\nt.test(G2,mu=50)\r\n#Frequencies Familar group\r\nFam_nov2018$Group[Fam_nov2018$Avg_Score > 50] <- 2\r\nFam_nov2018$Group[Fam_nov2018$Avg_Score < 50] <- 1\r\naggregate(x_fam$Avg_Score, by=list(x_fam$Group), FUN=mean)\r\nG1_Fam_nov_2018 <- Fam_nov2018$Avg_Score[Fam_nov2018$Group==1]\r\nG2_Fam_nov2018 <- Fam_nov2018$Avg_Score[Fam_nov2018$Group==2]\r\nlength(which(Fam_nov2018$Group==1))\r\nlength(which(Fam_nov2018$Group==2))\r\n\r\nperc_Fam_mec<-length(which(Fam_nov2018$Group==1))/17\r\nperc_Fam_ment<-length(which(Fam_nov2018$Group==2))/17\r\n\r\n### Standard error of means\r\nsd(G1, na.rm=TRUE) / sqrt(length(G1[!is.na(G1)])) \r\nsd(G2, na.rm=TRUE) / sqrt(length(G2[!is.na(G2)])) \r\n\r\n#plots\r\nhist(instance_nov2018$Avg_Score, main=""InStance Scores"", xlab=""Scores"", border=""black"", col=""grey"",xlim=c(0,100),ylim=c(0,0.04),las=1, \r\n     breaks=25, prob = TRUE)\r\nlines(density(instance_nov2018$Avg_Score))\r\nhist(NonFam_nov2018$Avg_Score, main=""InStance Scores: Not familiar group"", xlab=""Scores"", border=""black"", col=""grey"",xlim=c(0,100),ylim=c(0,0.04),las=1, \r\n     breaks=25, prob = TRUE)\r\nlines(density(NonFam_nov2018$Avg_Score))\r\n\r\nNonFamMec_nov2018 <- subset(NonFam_nov2018, NonFam_nov2018$Group==1)\r\nNonFamMent_nov2018 <- subset(NonFam_nov2018, NonFam_nov2018$Group==2)\r\n\r\nhist(NonFamMec_nov2018$Avg_Score, main=""InStance Scores:Non Familiar InStance Mechanistic Group"", xlab=""Scores"", border=""black"", col=""grey"",xlim=c(0,100),ylim=c(0,0.06),las=1, breaks=25, prob = TRUE)\r\nlines(density(NonFamMec_nov2018$Avg_Score))\r\nhist(NonFamMent_nov2018$Avg_Score, main=""InStance Scores:Not Familiar InStance Mentalistic Gruop"", xlab=""Scores"", border=""black"", col=""grey"",xlim=c(0,100),ylim=c(0,0.25),las=1, \r\n     breaks=25, prob = TRUE)\r\nlines(density(NonFamMent_nov2018$Avg_Score))\r\n\r\n#test per Bimodal distribution\r\nlibrary(diptest)\r\ndip(NonFam_nov2018$Avg_Score)#result=0.03133407\r\ndip(NonFam_nov2018_mec$Avg_Score) #result=0.03430837\r\ndip(NonFamMent_nov2018$Avg_Score)#result=0.07412467\r\ndip(instance_nov2018$Avg_Score) #result=0.0265179\r\nshapiro.test(NonFamMec_nov2018$Avg_Score)#W = 0.96418, p-value = 0.06735\r\nshapiro.test(NonFamMent_nov2018$Avg_Score)#W = 0.93106, p-value = 0.07339']","Do We Adopt the Intentional Stance Toward Humanoid Robots? In daily social interactions, we need to be able to navigate efficiently through our social environment. According to Dennett (1971), explaining and predicting others behavior with reference to mental states (adopting the intentional stance) allows efficient social interaction. Today we also routinely interact with artificial agents: from Apples Siri to GPS navigation systems. In the near future, we might start casually interacting with robots. This paper addresses the question of whether adopting the intentional stance can also occur with respect to artificial agents. We propose a new tool to explore if people adopt the intentional stance toward an artificial agent (humanoid robot). The tool consists in a questionnaire that probes participants stance by requiring them to choose the likelihood of an explanation (mentalistic vs. mechanistic) of a behavior of a robot iCub depicted in a naturalistic scenario (a sequence of photographs). The results of the first study conducted with this questionnaire showed that although the explanations were somewhat biased toward the mechanistic stance, a substantial number of mentalistic explanations were also given. This suggests that it is possible to induce adoption of the intentional stance toward artificial agents, at least in some contexts.",2
Learning strategies and long-term memory in Asian short-clawed otters (Aonyx cinereus) data,"Data submitted here, are those used in the writing of our manuscript entitled ""Learning strategies and long-term memory in Asian short-clawed otters (Aonyx cinereus)"" which has been submitted to Royal Society Open Science for publication. Abstract for that manuscript is belowSocial learning, namely learning from information acquired from others or their products, is widespread throughout the animal kingdom. There is growing evidence that animals selectively employ 'social learning strategies', which for example, determine when they should copy others instead of learning asocially, and whom they should copy. Furthermore, once animals have acquired new information, it is beneficial for them to commit it to long-term memory, especially when it concerns the discovery of profitable resources. Research into social learning strategies and long-term memory has covered a wide range of taxa. However, otters (subfamily Lutrinae), popular in zoos due to their sociability and playfulness, remained neglected until a recent study provided evidence of social learning in captive smooth-coated otters (Lutrogale perspicillata), but not in Asian short-clawed otters (Aonyx cinereus). We investigated Asian short-clawed otters' learning strategies and long-term memory performance in a foraging context. We presented novel extractive foraging tasks twice to captive family groups and used network-based diffusion analysis to provide evidence of social learning and long-term memory in this species. A major cause of wild Asian short-clawed otter declines is prey scarcity. Furthering our understanding of how they learn about and remember novel food sources could inform key conservation strategies.","['#To load the NBDA package, you first need to install the package ""devtools"" in the usual way\r\n#Next load it up as follows:\r\nlibrary(devtools)\r\n#Then download and install my NBDA package from GitHub:\r\ndevtools::install_github(""whoppitt/NBDA"")\r\nlibrary(NBDA)\r\n\r\n#Define ILVs of each individual in the network\r\nsex1<-c(0,0,0,1,1)\r\nage1<-c(13,11,11,12,12)\r\n\r\n#Put age and sex into a columns\r\nsex1<-cbind(sex1)\r\nage1<-cbind(age1)\r\n\r\n#And add it to the asoc vector\r\nasoc1<-c(""sex1"",""age1"")\r\n\r\n#Create association matrix of social network\r\namg1<-matrix(data=c(0.00, 0.60, 0.43, 0.50, 0.52,\t\r\n\t\t\t  0.60, 0.00, 0.47, 0.51, 0.49,\t\r\n\t\t\t  0.43, 0.47, 0.00, 0.42, 0.39,\t\r\n\t\t\t  0.50, 0.51, 0.42, 0.00, 0.41,\t\r\n\t\t\t  0.52, 0.49, 0.39, 0.41, 0.00), nrow=5)\r\n#Create order of solve data objects\r\noag1t1<-c(2,1,4,5,3)\r\noag1t2<-c(1,2,4,5,3)\r\noag1t3<-c(3,2,4,5,1)\r\noag1t4<-c(1,2,3,4,5)\r\noag1t5<-c(1,2,3,4,5)\r\n\r\n#Check the dimensions of the social network\r\ndim(amg1)\r\n\r\n#Now we want to specify asoc_ilv = asoc- the ILVs for the additive model and multi_ilv=asoc, the ILVs for the multiplicative model, though we do not have these in the same model together\r\n\r\n#Make array with room for 6 networks full of 0s\r\nassMatrix<-array(0,dim=c(5,5,10))\r\n#Enter network into slot 1 for task 1\r\nassMatrix[,,1]<-amg1\r\n#And 1s in slot 6 for the group network\r\nassMatrix[,,6]<-1\r\nng1t1 <-nbdaData(label=""ng1t1"",assMatrix=assMatrix, asoc_ilv=asoc1, multi_ilv=asoc1,orderAcq=oag1t1, idname=c(""Flint"",""Magma"",""Fossil"",""Earth"",""Moon""),timeAcq=c(56,57,62,62,65),endTime=176)\r\n\r\n#Make array with room for 6 networks full of 0s\r\nassMatrix<-array(0,dim=c(5,5,10))\r\n#Enter network into slot 2 for task 2\r\nassMatrix[,,2]<-amg1\r\n#And 1s in slot 6 for the group network\r\nassMatrix[,,7]<-1\r\nng1t2 <-nbdaData(label=""ng1t2"",assMatrix=assMatrix, asoc_ilv=asoc1, multi_ilv=asoc1, orderAcq=oag1t2, idname=c(""Flint"",""Magma"",""Fossil"",""Earth"",""Moon""),timeAcq=c(75,76,76,81,85),endTime=261)\r\n\r\n#Make array with room for 6 networks full of 0s\r\nassMatrix<-array(0,dim=c(5,5,10))\r\n#Enter network into slot 3 for task 3\r\nassMatrix[,,3]<-amg1\r\n#And 1s in slot 6 for the group network\r\nassMatrix[,,8]<-1\r\nng1t3 <-nbdaData(label=""ng1t3"",assMatrix=assMatrix, asoc_ilv=asoc1, multi_ilv=asoc1, orderAcq=oag1t3, idname=c(""Flint"",""Magma"",""Fossil"",""Earth"",""Moon""),timeAcq=c(55,55,58,62,155),endTime=643)\r\n\r\n#Make array with room for 6 networks full of 0s\r\nassMatrix<-array(0,dim=c(5,5,10))\r\n#Enter network into slot 4 for task 4\r\nassMatrix[,,4]<-amg1\r\n#And 1s in slot 6 for the group network\r\nassMatrix[,,9]<-1\r\nng1t4 <-nbdaData(label=""ng1t4"",assMatrix=assMatrix, asoc_ilv=asoc1, multi_ilv=asoc1, orderAcq=oag1t4, idname=c(""Flint"",""Magma"",""Fossil"",""Earth"",""Moon""),timeAcq=c(30,32,36,41,43),endTime=545)\r\n\r\n#Make array with room for 6 networks full of 0s\r\nassMatrix<-array(0,dim=c(5,5,10))\r\n#Enter network into slot 5 for task 5\r\nassMatrix[,,5]<-amg1\r\n#And 1s in slot 6 for the group network\r\nassMatrix[,,10]<-1\r\nng1t5 <-nbdaData(label=""ng1t5"",assMatrix=assMatrix, asoc_ilv=asoc1, multi_ilv=asoc1, orderAcq=oag1t5, idname=c(""Flint"",""Magma"",""Fossil"",""Earth"",""Moon""),timeAcq=c(65,67,70,72,74),endTime=735)\r\n\r\n\r\n#Repeat for group 2 (Newquay)\r\nsex2<-c(0,1,1,1,0,0,1,1)\r\nage2<-c(9,7,4,1,0.75,0.75,0.75,0.75)\r\nsex2<-cbind(sex2)\r\nage2<-cbind(age2)\r\nasoc2<-c(""sex2"",""age2"")\r\namg2<-matrix(data=c(0.00, 0.67, 0.56, 0.52, 0.52, 0.52, 0.50, 0.45,\r\n\t\t\t  0.67, 0.00, 0.59, 0.57, 0.61, 0.54, 0.52, 0.53,\t\r\n\t\t\t  0.56, 0.59, 0.00, 0.52, 0.43, 0.44, 0.42, 0.45,\r\n     \t\t\t  0.52, 0.57, 0.52, 0.00, 0.55, 0.55, 0.56, 0.51,\t\r\n\t\t\t  0.52, 0.61, 0.43, 0.55, 0.00, 0.67, 0.67, 0.67,\t\r\n\t\t\t  0.52, 0.54, 0.44, 0.55, 0.67, 0.00, 0.63, 0.68,\r\n\t\t\t  0.50, 0.52, 0.42, 0.56, 0.67, 0.63, 0.00, 0.59,\r\n  \t\t\t  0.45, 0.53, 0.45, 0.51, 0.67, 0.68, 0.59, 0.00), nrow=8)\r\noag2t1<-c(1,3,2,6,4)\r\noag2t2<-c(2,5,7,8,4,1,3)\r\noag2t3<-c(3,1,4,2,5,6,7,8)\r\noag2t4<-c(4,5,2,6,8,3,7,1)\r\noag2t5<-c(1,4,6,8,3,5,2,7)\r\n\r\ndim(amg2)\r\n\r\nassMatrix<-array(0,dim=c(8,8,10))\r\nassMatrix[,,1]<-amg2\r\nassMatrix[,,6]<-1\r\nng2t1 <-nbdaData(label=""ng2t1"",assMatrix=assMatrix, asoc_ilv=asoc2, multi_ilv=asoc2,orderAcq=oag2t1, idname=c(""Tope"",""Jam"",""Pod"",""Meg"",""Rosie"",""Dot"",""Charlie"",""Biscuit""),timeAcq=c(20,77,81,155,165),endTime=406)\r\n\r\nassMatrix<-array(0,dim=c(8,8,10))\r\nassMatrix[,,2]<-amg2\r\nassMatrix[,,7]<-1\r\nng2t2 <-nbdaData(label=""ng2t2"",assMatrix=assMatrix, asoc_ilv=asoc2, multi_ilv=asoc2, orderAcq=oag2t2, idname=c(""Tope"",""Jam"",""Pod"",""Meg"",""Rosie"",""Dot"",""Charlie"",""Biscuit""),timeAcq=c(21,25,25,38,40,48,82,93),endTime=251)\r\n\r\nassMatrix<-array(0,dim=c(8,8,10))\r\nassMatrix[,,3]<-amg2\r\nassMatrix[,,8]<-1\r\nng2t3 <-nbdaData(label=""ng2t3"",assMatrix=assMatrix, asoc_ilv=asoc2, multi_ilv=asoc2, orderAcq=oag2t3, idname=c(""Tope"",""Jam"",""Pod"",""Meg"",""Rosie"",""Dot"",""Charlie"",""Biscuit""),timeAcq=c(11,13,16,62,124,127,138,202),endTime=518)\r\n\r\nassMatrix<-array(0,dim=c(8,8,10))\r\nassMatrix[,,4]<-amg2\r\nassMatrix[,,9]<-1\r\nng2t4 <-nbdaData(label=""ng2t4"",assMatrix=assMatrix, asoc_ilv=asoc2, multi_ilv=asoc2, orderAcq=oag2t4, idname=c(""Tope"",""Jam"",""Pod"",""Meg"",""Rosie"",""Dot"",""Charlie"",""Biscuit""),timeAcq=c(19,21,22,22,24,44,51,90),endTime=1213)\r\n\r\nassMatrix<-array(0,dim=c(8,8,10))\r\nassMatrix[,,5]<-amg2\r\nassMatrix[,,10]<-1\r\nng2t5 <-nbdaData(label=""ng2t5"",assMatrix=assMatrix, asoc_ilv=asoc2, multi_ilv=asoc2, orderAcq=oag2t5, idname=c(""Tope"",""Jam"",""Pod"",""Meg"",""Rosie"",""Dot"",""Charlie"",""Biscuit""),timeAcq=c(21,23,26,27,49,51,53,57),endTime=654)\r\n\r\n#Repeat for group 3 (Tamar)\r\nsex3<-c(1,0,1,0,0,0,1,1,0,0,1,0)\r\nage3<-c(10,9,5,5,3,3,1,1,1,1,0.75,0.75)\r\nsex3<-cbind(sex3)\r\nage3<-cbind(age3)\r\nasoc3<-c(""sex3"",""age3"")\r\namg3<-matrix(data=c(0.0, 0.46, 0.44, 0.41, 0.44, 0.41, 0.36, 0.48, 0.39, 0.38, 0.40, 0.41,\t\r\n\t\t\t  0.46, 0.0, 0.49, 0.52, 0.56, 0.57, 0.43, 0.51, 0.46, 0.47, 0.34, 0.32, \r\n \t\t\t  0.44, 0.49, 0.0, 0.56, 0.47, 0.44, 0.49, 0.56, 0.51, 0.49, 0.36, 0.37, \r\n\t\t\t  0.41, 0.52, 0.56, 0.0, 0.54, 0.53, 0.48, 0.59, 0.48, 0.52, 0.36, 0.38, \r\n\t\t\t  0.44, 0.56, 0.47, 0.54, 0.0, 0.62, 0.47, 0.51, 0.44, 0.46, 0.34, 0.34, \r\n\t\t\t  0.41, 0.57, 0.44, 0.53, 0.62, 0.0, 0.42, 0.46, 0.43, 0.41, 0.29, 0.28, \r\n\t\t\t  0.36, 0.43, 0.49, 0.48, 0.47, 0.42, 0.0, 0.57, 0.56, 0.49, 0.36, 0.34, \r\n\t\t\t  0.48, 0.51, 0.56, 0.59, 0.51, 0.46, 0.57, 0.0, 0.56, 0.57, 0.39, 0.42, \r\n\t\t\t  0.39, 0.46, 0.51, 0.48, 0.44, 0.43, 0.56, 0.56, 0.0, 0.56, 0.32, 0.34,\r\n   \t\t\t  0.38, 0.47, 0.49, 0.52, 0.46, 0.41, 0.49, 0.57, 0.56, 0.0, 0.38, 0.41, \r\n\t\t\t  0.40, 0.34, 0.36, 0.36, 0.34, 0.29, 0.36, 0.39, 0.32, 0.38, 0.0, 0.58, \r\n  \t\t\t  0.41, 0.32, 0.37, 0.38, 0.34, 0.28, 0.34, 0.42, 0.34, 0.41, 0.58, 0.0), nrow=12)\r\noag3t1<-c(4,9,6,2,1)\r\noag3t2<-c(1,8,4,6,9,12,5,3,7)\r\noag3t3<-c(6,9,8,12,1,4,7,3,5,2,10,11)\r\noag3t4<-c(1,4,5,7,6,9,2,12,8,3,10,11)\r\noag3t5<-c(3,1,4,6,9,10,5,7,11,8,2,12)\r\n\r\ndim(amg3)\r\n\r\nassMatrix<-array(0,dim=c(12,12,10))\r\nassMatrix[,,1]<-amg3\r\nassMatrix[,,6]<-1\r\nng3t1 <-nbdaData(label=""ng3t1"",assMatrix=assMatrix, asoc_ilv=asoc3, multi_ilv=asoc3,orderAcq=oag3t1, idname=c(""Leah"",""Feet"",""India"",""Chai"",""Cassia"",""Cameron"",""Hazel"",""Daisy"",""Harry"",""Dougie"",""Rani"",""Khan""),timeAcq=c(3,5,15,28,40),endTime=73)\r\n\r\nassMatrix<-array(0,dim=c(12,12,10))\r\nassMatrix[,,2]<-amg3\r\nassMatrix[,,7]<-1\r\nng3t2 <-nbdaData(label=""ng3t2"",assMatrix=assMatrix, asoc_ilv=asoc3, multi_ilv=asoc3, orderAcq=oag3t2, idname=c(""Leah"",""Feet"",""India"",""Chai"",""Cassia"",""Cameron"",""Hazel"",""Daisy"",""Harry"",""Dougie"",""Rani"",""Khan""),timeAcq=c(3,5,8,8,9,9,28,59,77),endTime=108)\r\n\r\nassMatrix<-array(0,dim=c(12,12,10))\r\nassMatrix[,,3]<-amg3\r\nassMatrix[,,8]<-1\r\nng3t3 <-nbdaData(label=""ng3t3"",assMatrix=assMatrix, asoc_ilv=asoc3, multi_ilv=asoc3, orderAcq=oag3t3, idname=c(""Leah"",""Feet"",""India"",""Chai"",""Cassia"",""Cameron"",""Hazel"",""Daisy"",""Harry"",""Dougie"",""Rani"",""Khan""),timeAcq=c(2,18,19,19,21,21,21,22,30,53,55,138),endTime=270)\r\n\r\nassMatrix<-array(0,dim=c(12,12,10))\r\nassMatrix[,,4]<-amg3\r\nassMatrix[,,9]<-1\r\nng3t4 <-nbdaData(label=""ng3t4"",assMatrix=assMatrix, asoc_ilv=asoc3, multi_ilv=asoc3, orderAcq=oag3t4, idname=c(""Leah"",""Feet"",""India"",""Chai"",""Cassia"",""Cameron"",""Hazel"",""Daisy"",""Harry"",""Dougie"",""Rani"",""Khan""),timeAcq=c(4,4,4,4,4,8,10,11,12,17,57,85),endTime=193)\r\n\r\nassMatrix<-array(0,dim=c(12,12,10))\r\nassMatrix[,,5]<-amg3\r\nassMatrix[,,10]<-1\r\nng3t5 <-nbdaData(label=""ng3t5"",assMatrix=assMatrix, asoc_ilv=asoc3, multi_ilv=asoc3, orderAcq=oag3t5, idname=c(""Leah"",""Feet"",""India"",""Chai"",""Cassia"",""Cameron"",""Hazel"",""Daisy"",""Harry"",""Dougie"",""Rani"",""Khan""),timeAcq=c(5,8,8,8,9,10,11,14,14,16,21,148),endTime=263)\r\n\r\n#Create model vectors\r\nconstraintsVectMatrix<-rbind(\r\n##Social network models\r\n#Task 1 \r\n##models fit to group networks where social transmission is constrained to be the same across tasks\r\nc(0,0,0,0,0,1,1,1,1,1,0,0,0,0), #No ILVs\r\nc(0,0,0,0,0,1,1,1,1,1,0,0,2,0), #Additive - Sex\r\nc(0,0,0,0,0,1,1,1,1,1,0,0,0,2), #Additive - Age\r\nc(0,0,0,0,0,1,1,1,1,1,0,0,2,3), #Additive - Sex and Age\r\nc(0,0,0,0,0,1,1,1,1,1,2,0,0,0), #Multiplicative - Sex\r\nc(0,0,0,0,0,1,1,1,1,1,0,2,0,0), #Multiplicative - Age\r\nc(0,0,0,0,0,1,1,1,1,1,2,3,0,0), #Multiplicative - Sex and Age\r\n##models fit to group networks where social transmission is constrained to be the different across tasks\r\nc(0,0,0,0,0,1,2,3,4,5,0,0,0,0),\r\nc(0,0,0,0,0,1,2,3,4,5,0,0,6,0),\r\nc(0,0,0,0,0,1,2,3,4,5,0,0,0,6),\r\nc(0,0,0,0,0,1,2,3,4,5,0,0,6,7),\r\nc(0,0,0,0,0,1,2,3,4,5,6,0,0,0),\r\nc(0,0,0,0,0,1,2,3,4,5,0,6,0,0),\r\nc(0,0,0,0,0,1,2,3,4,5,6,7,0,0),\r\n##models fit to social networks where social transmission is constrained to be the same across tasks\r\nc(1,1,1,1,1,0,0,0,0,0,0,0,0,0),\r\nc(1,1,1,1,1,0,0,0,0,0,0,0,2,0),\r\nc(1,1,1,1,1,0,0,0,0,0,0,0,0,2),\r\nc(1,1,1,1,1,0,0,0,0,0,0,0,2,3),\r\nc(1,1,1,1,1,0,0,0,0,0,2,0,0,0),\r\nc(1,1,1,1,1,0,0,0,0,0,0,2,0,0),\r\nc(1,1,1,1,1,0,0,0,0,0,2,3,0,0),\r\n####models fit to social networks where social transmission is constrained to be the different across tasks\r\nc(1,2,3,4,5,0,0,0,0,0,0,0,0,0), \r\nc(1,2,3,4,5,0,0,0,0,0,0,0,6,0),\r\nc(1,2,3,4,5,0,0,0,0,0,0,0,0,6),\r\nc(1,2,3,4,5,0,0,0,0,0,0,0,6,7),\r\nc(1,2,3,4,5,0,0,0,0,0,6,0,0,0),\r\nc(1,2,3,4,5,0,0,0,0,0,0,6,0,0),\r\nc(1,2,3,4,5,0,0,0,0,0,6,7,0,0),\r\n##asocial models\r\nc(0,0,0,0,0,0,0,0,0,0,0,0,0,0),\r\nc(0,0,0,0,0,0,0,0,0,0,0,0,1,0),\r\nc(0,0,0,0,0,0,0,0,0,0,0,0,0,1),\r\nc(0,0,0,0,0,0,0,0,0,0,0,0,1,2))\r\n\r\n#Fit constant and gamma baseline rates of asocial learning to models \r\nconstraintsVectMatrix2<-rbind(constraintsVectMatrix,constraintsVectMatrix)\r\nbaselineVect<-rep(c(""constant"",""gamma""),each=dim(constraintsVectMatrix)[1])\r\n\r\n#Create AIC table\r\naicTable_tada<-tadaAICtable(nbdadata=c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"",""ng2t1"", ""ng2t2"", ""ng2t3"", ""ng2t4"", ""ng2t5"",""ng3t1"", ""ng3t2"", ""ng3t3"", ""ng3t4"", ""ng3t5""),\r\n                            constraintsVectMatrix= constraintsVectMatrix2,baselineVect=baselineVect, iterations=1000)\r\n\r\nprint(aicTable_tada)\r\n\r\n#Get support for each type of model, with each baseline rate, fit to each network type\r\ntypeByNetworksSupport(aicTable_tada)\r\n\r\nwrite.csv(aicTable_tada@printTable,file=""aicTable_tada.csv"")\r\n\r\n#Can also get support for the different network combinations across all model types and baselines\r\nnetworksSupport(aicTable_tada)\r\n#                         support numberOfModels\r\n#0:0:0:0:0:0:0:0:0:0 1.540617e-13              8  asocial learning\r\n#0:0:0:0:0:1:1:1:1:1 1.232553e-04             14  social learning through the group network where rate of social transmission is the same across all task types\r\n#0:0:0:0:0:1:2:3:4:5 7.513868e-02             14  social learning through the group network where rate of social transmission is the different across all task types\r\n#1:1:1:1:1:0:0:0:0:0 7.381589e-04             14  social learning through the social network where rate of social transmission is the same across all task types\r\n#1:2:3:4:5:0:0:0:0:0 9.239999e-01             14  social learning through the social network where rate of social transmission is the different across all task types\r\n\r\n#Because all the variables have been specified in the data objects, new constrained data objects with only the variables\r\n#in the best model need to be created. \r\n#The easiest way to do this is to look at which model is top (56) and then use row 9 of the constraintsVectMatrix to constrain the data objects:\r\nng1t1_bestModel<-constrainedNBDAdata(ng1t1,constraintsVect =constraintsVectMatrix2[56,])\r\nng1t2_bestModel<-constrainedNBDAdata(ng1t2,constraintsVect =constraintsVectMatrix2[56,])\r\nng1t3_bestModel<-constrainedNBDAdata(ng1t3,constraintsVect =constraintsVectMatrix2[56,])\r\nng1t4_bestModel<-constrainedNBDAdata(ng1t4,constraintsVect =constraintsVectMatrix2[56,])\r\nng1t5_bestModel<-constrainedNBDAdata(ng1t5,constraintsVect =constraintsVectMatrix2[56,])\r\nng2t1_bestModel<-constrainedNBDAdata(ng2t1,constraintsVect =constraintsVectMatrix2[56,])\r\nng2t2_bestModel<-constrainedNBDAdata(ng2t2,constraintsVect =constraintsVectMatrix2[56,])\r\nng2t3_bestModel<-constrainedNBDAdata(ng2t3,constraintsVect =constraintsVectMatrix2[56,])\r\nng2t4_bestModel<-constrainedNBDAdata(ng2t4,constraintsVect =constraintsVectMatrix2[56,])\r\nng2t5_bestModel<-constrainedNBDAdata(ng2t5,constraintsVect =constraintsVectMatrix2[56,])\r\nng3t1_bestModel<-constrainedNBDAdata(ng3t1,constraintsVect =constraintsVectMatrix2[56,])\r\nng3t2_bestModel<-constrainedNBDAdata(ng3t2,constraintsVect =constraintsVectMatrix2[56,])\r\nng3t3_bestModel<-constrainedNBDAdata(ng3t3,constraintsVect =constraintsVectMatrix2[56,])\r\nng3t4_bestModel<-constrainedNBDAdata(ng3t4,constraintsVect =constraintsVectMatrix2[56,])\r\nng3t5_bestModel<-constrainedNBDAdata(ng3t5,constraintsVect =constraintsVectMatrix2[56,])\r\n\r\nbestModelNew<-tadaFit(list(ng1t1_bestModel,ng1t2_bestModel,ng1t3_bestModel,ng1t4_bestModel,ng1t5_bestModel,\r\n\t\t\t\t   ng2t1_bestModel,ng2t2_bestModel,ng2t3_bestModel,ng2t4_bestModel,ng2t5_bestModel,\r\n\t\t\t\t   ng3t1_bestModel,ng3t2_bestModel,ng3t3_bestModel,ng3t4_bestModel,ng3t5_bestModel),\r\n\t\t\t    baseline=""gamma"")\r\nbestModelNew\r\nbestModelNew@aicc\r\nprint(aicTable_tada)[1,]\r\n\r\n#A neat way to extract the fitted parameters is:\r\ndata.frame(\r\nVariable=bestModelNew@varNames,\r\nMLE=bestModelNew@outputPar)\r\n\r\n#                 Variable          MLE\r\n#1         Scale (1/rate): 2072.6557292\r\n#2                   Shape    0.5996747\r\n#3 1 Social transmission 1    1.0881123 #Social transmission rate in Task 1\r\n#4 2 Social transmission 2    2.6084788 #Social transmission rate in Task 2\r\n#5 3 Social transmission 3    3.2617435 #Social transmission rate in Task 3\r\n#6 4 Social transmission 4    6.0562472 #Social transmission rate in Task 4\r\n#7 5 Social transmission 5    8.3298699 #Social transmission rate in Task 5\r\n#8 6 Social= asocial: age1    0.1160667 #Effect of age on learning rate\r\n\r\n\r\n#The second number on the left indicates which parameter numbers to ask for below when ascertaining 95% confidence intervals for s parameters.\r\n#So paramter 1 is s1 and so on:\r\n\r\n#Plot profile likelihoods for s1 to find range for lower and upper CIs  \r\nplotProfLik(which=1,model=bestModelNew,range=c(0,20),resolution=10)\r\nplotProfLik(which=1,model=bestModelNew,range=c(0,10),resolution=30)\r\n#Ascertain lower and upper 95% CIs\r\nprofLikCI(which = 1,model=bestModelNew,upperRange = c(4,6)) #In this case only had to constrain upper range as lower CI cannot be below zero for s parameters\r\n#Lower CI   Upper CI \r\n#[1] 0.00 5.133396   \r\n\r\n#Reapeat for parameters 2-6\r\nplotProfLik(which=2,model=bestModelNew,range=c(0,60),resolution=10)\r\nplotProfLik(which=2,model=bestModelNew,range=c(0,10),resolution=30)\r\nprofLikCI(which = 2,model=bestModelNew,upperRange = c(7,9),lowerRange=c(0,2))\r\n#Lower CI   Upper CI \r\n#[2]  0.5450606 7.7512870   \r\n\r\nplotProfLik(which=3,model=bestModelNew,range=c(0,50),resolution=10)\r\nplotProfLik(which=3,model=bestModelNew,range=c(0,20),resolution=30)\r\nprofLikCI(which = 3,model=bestModelNew,upperRange = c(7,10),lowerRange=c(0,3))\r\n# Lower CI   Upper CI \r\n#[3] 0.8008696 9.4143231 \r\n\r\nplotProfLik(which=4,model=bestModelNew,range=c(0,50),resolution=10)\r\nplotProfLik(which=4,model=bestModelNew,range=c(0,20),resolution=30)\r\nprofLikCI(which = 4,model=bestModelNew,upperRange = c(15,18),lowerRange = c(0,4))\r\n# Lower CI   Upper CI \r\n#[4] 1.992449 15.520164  \r\n\r\nplotProfLik(which=5,model=bestModelNew,range=c(0,60),resolution=10)\r\nplotProfLik(which=5,model=bestModelNew,range=c(0,10),resolution=30)\r\nplotProfLik(which=5,model=bestModelNew,range=c(20,23),resolution=30)\r\nprofLikCI(which = 5,model=bestModelNew,upperRange = c(31,33),lowerRange = c(2,4))\r\n#Lower CI Upper CI \r\n#[5] 2.607495 31.000082\r\n\r\nexp(0.1160667)\r\n#[Age] 1.123071\r\nplotProfLik(which=6,model=bestModelNew,range=c(0,0.5),resolution=50)\r\nprofLikCI(which = 6,model=bestModelNew,upperRange = c(0.1,0.2),lowerRange = c(0,0.1))\r\n#Lower CI         Upper CI \r\n#[6] 0.06172406   0.16503709\r\nexp(0.06172406)\r\nexp(0.16503709)\r\n#[6] 1.063669     1.179437\r\n\r\n#Get the proportion of events estimated to due to social transmission\r\nnbdaPropSolveByST.byevent(model=bestModelNew)\r\nnbdaPropSolveByST(model=bestModelNew)\r\n\r\n#As it stands these figures are the proportion of all events explained by each network\r\n#e.g. the proportion of acquisition events across all tasks explained by the network for task 1.\r\n#Instead we want the proportion of task 1 acquisition events explained by the task 1 network etc.\r\n#We therefore make the following adjustment:\r\n\r\n#Divide P(Network 1) by the number of acquisition events for task 1 (excluding the innovators - the first otters in each group to interact with task 1 apparatus)/\r\n#the total number of aquisition events across all task types (excluding the innovators)\r\n0.06856/(12/96)\r\n#[1] 0.54848\r\n\r\n#Repeat for tasks 2 to 5\r\n0.14739/(18/96)\r\n#[2] 0.78608\r\n        \r\n0.19091/(22/96)\r\n#[3] 0.8330618\r\n\r\n0.20661/(22/96)\r\n#[4] 0.9015709\r\n\r\n0.21199/(22/96)\r\n#[5] 0.9250473\r\n\r\n#Ascertain lower 95% confidence interval for the number of acquisition events that were due to social learning for task 1\r\nng1t1_lower<-constrainedNBDAdata(ng1t1_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng1t2_lower<-constrainedNBDAdata(ng1t2_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng1t3_lower<-constrainedNBDAdata(ng1t3_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng1t4_lower<-constrainedNBDAdata(ng1t4_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng1t5_lower<-constrainedNBDAdata(ng1t5_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng2t1_lower<-constrainedNBDAdata(ng2t1_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng2t2_lower<-constrainedNBDAdata(ng2t2_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng2t3_lower<-constrainedNBDAdata(ng2t3_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng2t4_lower<-constrainedNBDAdata(ng2t4_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng2t5_lower<-constrainedNBDAdata(ng2t5_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng3t1_lower<-constrainedNBDAdata(ng3t1_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng3t2_lower<-constrainedNBDAdata(ng3t2_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng3t3_lower<-constrainedNBDAdata(ng3t3_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng3t4_lower<-constrainedNBDAdata(ng3t4_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\nng3t5_lower<-constrainedNBDAdata(ng3t5_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(0,0,0,0,0,0))\r\ntadabestGamma_lower<-tadaFit(nbdadata=c(""ng1t1_lower"", ""ng1t2_lower"", ""ng1t3_lower"", ""ng1t4_lower"", ""ng1t5_lower"",\r\n\t\t\t\t\t\t    ""ng2t1_lower"", ""ng1t2_lower"", ""ng2t3_lower"", ""ng2t4_lower"", ""ng2t5_lower"",\r\n \t\t\t\t\t\t    ""ng3t1_lower"", ""ng3t2_lower"", ""ng3t3_lower"", ""ng3t4_lower"", ""ng3t5_lower""),baseline=""gamma"",iterations=10000)\r\ntadabestGamma_lower@outputPar\r\nnbdaPropSolveByST.byevent(model=tadabestGamma_lower)\r\nnbdaPropSolveByST(model=tadabestGamma_lower)\r\n0.00000/(12/96)\r\n#[1] 0\r\n\r\n#Ascertain higher 95% confidence interval for the number of acquisition events that were due to social learning for task 1\r\nng1t1_higher<-constrainedNBDAdata(ng1t1_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng1t2_higher<-constrainedNBDAdata(ng1t2_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng1t3_higher<-constrainedNBDAdata(ng1t3_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng1t4_higher<-constrainedNBDAdata(ng1t4_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng1t5_higher<-constrainedNBDAdata(ng1t5_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng2t1_higher<-constrainedNBDAdata(ng2t1_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng2t2_higher<-constrainedNBDAdata(ng2t2_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng2t3_higher<-constrainedNBDAdata(ng2t3_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng2t4_higher<-constrainedNBDAdata(ng2t4_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng2t5_higher<-constrainedNBDAdata(ng2t5_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng3t1_higher<-constrainedNBDAdata(ng3t1_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng3t2_higher<-constrainedNBDAdata(ng3t2_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng3t3_higher<-constrainedNBDAdata(ng3t3_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng3t4_higher<-constrainedNBDAdata(ng3t4_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\nng3t5_higher<-constrainedNBDAdata(ng3t5_bestModel,constraintsVect=c(0,1,2,3,4,5),offsetVect = c(5.133396,0,0,0,0,0))\r\ntadabestGamma_higher<-tadaFit(nbdadata=c(""ng1t1_higher"", ""ng1t2_higher"", ""ng1t3_higher"", ""ng1t4_higher"", ""ng1t5_higher"",\r\n\t\t\t\t\t\t     ""ng2t1_higher"", ""ng2t2_higher"", ""ng2t3_higher"", ""ng2t4_higher"", ""ng2t5_higher"",\r\n\t\t\t\t\t\t     ""ng3t1_higher"", ""ng3t2_higher"", ""ng3t3_higher"", ""ng3t4_higher"", ""ng3t5_higher""),baseline=""gamma"",iterations=10000)\r\ntadabestGamma_higher@outputPar\r\nnbdaPropSolveByST.byevent(model=tadabestGamma_higher)\r\nnbdaPropSolveByST(model=tadabestGamma_higher)\r\n0.10528 /(12/96)\r\n#[1] 0.84224\r\n\r\n#Repeat for tasks 2 to 5\r\nng1t1_lower2<-constrainedNBDAdata(ng1t1_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng1t2_lower2<-constrainedNBDAdata(ng1t2_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng1t3_lower2<-constrainedNBDAdata(ng1t3_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng1t4_lower2<-constrainedNBDAdata(ng1t4_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng1t5_lower2<-constrainedNBDAdata(ng1t5_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng2t1_lower2<-constrainedNBDAdata(ng2t1_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng2t2_lower2<-constrainedNBDAdata(ng2t2_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng2t3_lower2<-constrainedNBDAdata(ng2t3_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng2t4_lower2<-constrainedNBDAdata(ng2t4_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng2t5_lower2<-constrainedNBDAdata(ng2t5_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng3t1_lower2<-constrainedNBDAdata(ng3t1_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng3t2_lower2<-constrainedNBDAdata(ng3t2_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng3t3_lower2<-constrainedNBDAdata(ng3t3_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng3t4_lower2<-constrainedNBDAdata(ng3t4_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\nng3t5_lower2<-constrainedNBDAdata(ng3t5_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,0.5450606,0,0,0,0))\r\ntadabestGamma_lower2<-tadaFit(nbdadata=c(""ng1t1_lower2"", ""ng1t2_lower2"", ""ng1t3_lower2"", ""ng1t4_lower2"", ""ng1t5_lower2"",\r\n\t\t\t\t\t\t     ""ng2t1_lower2"", ""ng1t2_lower2"", ""ng2t3_lower2"", ""ng2t4_lower2"", ""ng2t5_lower2"",\r\n \t\t\t\t\t\t     ""ng3t1_lower2"", ""ng3t2_lower2"", ""ng3t3_lower2"", ""ng3t4_lower2"", ""ng3t5_lower2""),baseline=""gamma"",iterations=10000)\r\ntadabestGamma_lower2@outputPar\r\nnbdaPropSolveByST.byevent(model=tadabestGamma_lower2)\r\nnbdaPropSolveByST(model=tadabestGamma_lower2)\r\n0.07406/(18/96)\r\n#[2] 0.3949867\r\nng1t1_higher2<-constrainedNBDAdata(ng1t1_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng1t2_higher2<-constrainedNBDAdata(ng1t2_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng1t3_higher2<-constrainedNBDAdata(ng1t3_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng1t4_higher2<-constrainedNBDAdata(ng1t4_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng1t5_higher2<-constrainedNBDAdata(ng1t5_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng2t1_higher2<-constrainedNBDAdata(ng2t1_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng2t2_higher2<-constrainedNBDAdata(ng2t2_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng2t3_higher2<-constrainedNBDAdata(ng2t3_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng2t4_higher2<-constrainedNBDAdata(ng2t4_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng2t5_higher2<-constrainedNBDAdata(ng2t5_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng3t1_higher2<-constrainedNBDAdata(ng3t1_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng3t2_higher2<-constrainedNBDAdata(ng3t2_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng3t3_higher2<-constrainedNBDAdata(ng3t3_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng3t4_higher2<-constrainedNBDAdata(ng3t4_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\nng3t5_higher2<-constrainedNBDAdata(ng3t5_bestModel,constraintsVect=c(1,0,2,3,4,5),offsetVect = c(0,7.7512870,0,0,0,0))\r\ntadabestGamma_higher2<-tadaFit(nbdadata=c(""ng1t1_higher2"", ""ng1t2_higher2"", ""ng1t3_higher2"", ""ng1t4_higher2"", ""ng1t5_higher2"",\r\n\t\t\t\t\t\t      ""ng2t1_higher2"", ""ng1t2_higher2"", ""ng2t3_higher2"", ""ng2t4_higher2"", ""ng2t5_higher2"",\r\n \t\t\t\t\t\t      ""ng3t1_higher2"", ""ng3t2_higher2"", ""ng3t3_higher2"", ""ng3t4_higher2"", ""ng3t5_higher2""),baseline=""gamma"",iterations=10000)\r\ntadabestGamma_higher2@outputPar\r\nnbdaPropSolveByST.byevent(model=tadabestGamma_higher2)\r\nnbdaPropSolveByST(model=tadabestGamma_higher2)\r\n0.15381/(18/96) \r\n#[2] 0.96555\r\n\r\nng1t1_lower3<-constrainedNBDAdata(ng1t1_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng1t2_lower3<-constrainedNBDAdata(ng1t2_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng1t3_lower3<-constrainedNBDAdata(ng1t3_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng1t4_lower3<-constrainedNBDAdata(ng1t4_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng1t5_lower3<-constrainedNBDAdata(ng1t5_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng2t1_lower3<-constrainedNBDAdata(ng2t1_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng2t2_lower3<-constrainedNBDAdata(ng2t2_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng2t3_lower3<-constrainedNBDAdata(ng2t3_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng2t4_lower3<-constrainedNBDAdata(ng2t4_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng2t5_lower3<-constrainedNBDAdata(ng2t5_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng3t1_lower3<-constrainedNBDAdata(ng3t1_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng3t2_lower3<-constrainedNBDAdata(ng3t2_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng3t3_lower3<-constrainedNBDAdata(ng3t3_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng3t4_lower3<-constrainedNBDAdata(ng3t4_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\nng3t5_lower3<-constrainedNBDAdata(ng3t5_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,0.8008696,0,0,0))\r\ntadabestGamma_lower3<-tadaFit(nbdadata=c(""ng1t1_lower3"", ""ng1t2_lower3"", ""ng1t3_lower3"", ""ng1t4_lower3"", ""ng1t5_lower3"",\r\n\t\t\t\t\t\t     ""ng2t1_lower3"", ""ng1t2_lower3"", ""ng2t3_lower3"", ""ng2t4_lower3"", ""ng2t5_lower3"",\r\n \t\t\t\t\t\t     ""ng3t1_lower3"", ""ng3t2_lower3"", ""ng3t3_lower3"", ""ng3t4_lower3"", ""ng3t5_lower3""),baseline=""gamma"",iterations=10000)\r\ntadabestGamma_lower3@outputPar\r\nnbdaPropSolveByST.byevent(model=tadabestGamma_lower3)\r\nnbdaPropSolveByST(model=tadabestGamma_lower3)\r\n0.13704/(22/96)\r\n#[3] 0.5979927\r\nng1t1_higher3<-constrainedNBDAdata(ng1t1_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng1t2_higher3<-constrainedNBDAdata(ng1t2_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng1t3_higher3<-constrainedNBDAdata(ng1t3_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng1t4_higher3<-constrainedNBDAdata(ng1t4_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng1t5_higher3<-constrainedNBDAdata(ng1t5_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng2t1_higher3<-constrainedNBDAdata(ng2t1_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng2t2_higher3<-constrainedNBDAdata(ng2t2_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng2t3_higher3<-constrainedNBDAdata(ng2t3_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng2t4_higher3<-constrainedNBDAdata(ng2t4_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng2t5_higher3<-constrainedNBDAdata(ng2t5_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng3t1_higher3<-constrainedNBDAdata(ng3t1_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng3t2_higher3<-constrainedNBDAdata(ng3t2_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng3t3_higher3<-constrainedNBDAdata(ng3t3_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng3t4_higher3<-constrainedNBDAdata(ng3t4_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\nng3t5_higher3<-constrainedNBDAdata(ng3t5_bestModel,constraintsVect=c(1,2,0,3,4,5),offsetVect = c(0,0,9.4143231,0,0,0))\r\ntadabestGamma_higher3<-tadaFit(nbdadata=c(""ng1t1_higher3"", ""ng1t2_higher3"", ""ng1t3_higher3"", ""ng1t4_higher3"", ""ng1t5_higher3"",\r\n\t\t\t\t\t\t      ""ng2t1_higher3"", ""ng1t2_higher3"", ""ng2t3_higher3"", ""ng2t4_higher3"", ""ng2t5_higher3"",\r\n \t\t\t\t\t\t      ""ng3t1_higher3"", ""ng3t2_higher3"", ""ng3t3_higher3"", ""ng3t4_higher3"", ""ng3t5_higher3""),baseline=""gamma"",iterations=10000)\r\ntadabestGamma_higher3@outputPar\r\nnbdaPropSolveByST.byevent(model=tadabestGamma_higher3)\r\nnbdaPropSolveByST(model=tadabestGamma_higher3)\r\n0.21792/(22/96)\r\n#[3] 0.9509236\r\n\r\nng1t1_lower4<-constrainedNBDAdata(ng1t1_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng1t2_lower4<-constrainedNBDAdata(ng1t2_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng1t3_lower4<-constrainedNBDAdata(ng1t3_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng1t4_lower4<-constrainedNBDAdata(ng1t4_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng1t5_lower4<-constrainedNBDAdata(ng1t5_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng2t1_lower4<-constrainedNBDAdata(ng2t1_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng2t2_lower4<-constrainedNBDAdata(ng2t2_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng2t3_lower4<-constrainedNBDAdata(ng2t3_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng2t4_lower4<-constrainedNBDAdata(ng2t4_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng2t5_lower4<-constrainedNBDAdata(ng2t5_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng3t1_lower4<-constrainedNBDAdata(ng3t1_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng3t2_lower4<-constrainedNBDAdata(ng3t2_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng3t3_lower4<-constrainedNBDAdata(ng3t3_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng3t4_lower4<-constrainedNBDAdata(ng3t4_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\nng3t5_lower4<-constrainedNBDAdata(ng3t5_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,1.992449,0,0))\r\ntadabestGamma_lower4<-tadaFit(nbdadata=c(""ng1t1_lower4"", ""ng1t2_lower4"", ""ng1t3_lower4"", ""ng1t4_lower4"", ""ng1t5_lower4"",\r\n\t\t\t\t\t\t     ""ng2t1_lower4"", ""ng1t2_lower4"", ""ng2t3_lower4"", ""ng2t4_lower4"", ""ng2t5_lower4"",\r\n \t\t\t\t\t\t     ""ng3t1_lower4"", ""ng3t2_lower4"", ""ng3t3_lower4"", ""ng3t4_lower4"", ""ng3t5_lower4""),baseline=""gamma"",iterations=10000)\r\ntadabestGamma_lower4@outputPar\r\nnbdaPropSolveByST.byevent(model=tadabestGamma_lower4)\r\nnbdaPropSolveByST(model=tadabestGamma_lower4)\r\n0.17915/(22/96)\r\n#[4] 0.7817455\r\n\r\nng1t1_higher4<-constrainedNBDAdata(ng1t1_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng1t2_higher4<-constrainedNBDAdata(ng1t2_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng1t3_higher4<-constrainedNBDAdata(ng1t3_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng1t4_higher4<-constrainedNBDAdata(ng1t4_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng1t5_higher4<-constrainedNBDAdata(ng1t5_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng2t1_higher4<-constrainedNBDAdata(ng2t1_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng2t2_higher4<-constrainedNBDAdata(ng2t2_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng2t3_higher4<-constrainedNBDAdata(ng2t3_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng2t4_higher4<-constrainedNBDAdata(ng2t4_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng2t5_higher4<-constrainedNBDAdata(ng2t5_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng3t1_higher4<-constrainedNBDAdata(ng3t1_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng3t2_higher4<-constrainedNBDAdata(ng3t2_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng3t3_higher4<-constrainedNBDAdata(ng3t3_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng3t4_higher4<-constrainedNBDAdata(ng3t4_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\nng3t5_higher4<-constrainedNBDAdata(ng3t5_bestModel,constraintsVect=c(1,2,3,0,4,5),offsetVect = c(0,0,0,15.520164,0,0))\r\ntadabestGamma_higher4<-tadaFit(nbdadata=c(""ng1t1_higher4"", ""ng1t2_higher4"", ""ng1t3_higher4"", ""ng1t4_higher4"", ""ng1t5_higher4"",\r\n\t\t\t\t\t\t      ""ng2t1_higher4"", ""ng1t2_higher4"", ""ng2t3_higher4"", ""ng2t4_higher4"", ""ng2t5_higher4"",\r\n \t\t\t\t\t\t      ""ng3t1_higher4"", ""ng3t2_higher4"", ""ng3t3_higher4"", ""ng3t4_higher4"", ""ng3t5_higher4""),baseline=""gamma"",iterations=10000)\r\ntadabestGamma_higher4@outputPar\r\nnbdaPropSolveByST.byevent(model=tadabestGamma_higher4)\r\nnbdaPropSolveByST(model=tadabestGamma_higher4)\r\n0.22415/(22/96)\r\n#[4] 0.9781091\r\n\r\nng1t1_lower5<-constrainedNBDAdata(ng1t1_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng1t2_lower5<-constrainedNBDAdata(ng1t2_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng1t3_lower5<-constrainedNBDAdata(ng1t3_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng1t4_lower5<-constrainedNBDAdata(ng1t4_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng1t5_lower5<-constrainedNBDAdata(ng1t5_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng2t1_lower5<-constrainedNBDAdata(ng2t1_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng2t2_lower5<-constrainedNBDAdata(ng2t2_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng2t3_lower5<-constrainedNBDAdata(ng2t3_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng2t4_lower5<-constrainedNBDAdata(ng2t4_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng2t5_lower5<-constrainedNBDAdata(ng2t5_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng3t1_lower5<-constrainedNBDAdata(ng3t1_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng3t2_lower5<-constrainedNBDAdata(ng3t2_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng3t3_lower5<-constrainedNBDAdata(ng3t3_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng3t4_lower5<-constrainedNBDAdata(ng3t4_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\nng3t5_lower5<-constrainedNBDAdata(ng3t5_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,2.607495,0))\r\ntadabestGamma_lower5<-tadaFit(nbdadata=c(""ng1t1_lower5"", ""ng1t2_lower5"", ""ng1t3_lower5"", ""ng1t4_lower5"", ""ng1t5_lower5"",\r\n\t\t\t\t\t\t     ""ng2t1_lower5"", ""ng1t2_lower5"", ""ng2t3_lower5"", ""ng2t4_lower5"", ""ng2t5_lower5"",\r\n \t\t\t\t\t\t     ""ng3t1_lower5"", ""ng3t2_lower5"", ""ng3t3_lower5"", ""ng3t4_lower5"", ""ng3t5_lower5""),baseline=""gamma"",iterations=10000)\r\ntadabestGamma_lower5@outputPar\r\nnbdaPropSolveByST.byevent(model=tadabestGamma_lower5)\r\nnbdaPropSolveByST(model=tadabestGamma_lower5)\r\n0.18854/(22/96)\r\n#[5] 0.82272\r\n\r\nng1t1_higher5<-constrainedNBDAdata(ng1t1_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng1t2_higher5<-constrainedNBDAdata(ng1t2_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng1t3_higher5<-constrainedNBDAdata(ng1t3_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng1t4_higher5<-constrainedNBDAdata(ng1t4_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng1t5_higher5<-constrainedNBDAdata(ng1t5_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng2t1_higher5<-constrainedNBDAdata(ng2t1_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng2t2_higher5<-constrainedNBDAdata(ng2t2_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng2t3_higher5<-constrainedNBDAdata(ng2t3_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng2t4_higher5<-constrainedNBDAdata(ng2t4_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng2t5_higher5<-constrainedNBDAdata(ng2t5_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng3t1_higher5<-constrainedNBDAdata(ng3t1_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng3t2_higher5<-constrainedNBDAdata(ng3t2_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng3t3_higher5<-constrainedNBDAdata(ng3t3_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng3t4_higher5<-constrainedNBDAdata(ng3t4_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\nng3t5_higher5<-constrainedNBDAdata(ng3t5_bestModel,constraintsVect=c(1,2,3,4,0,5),offsetVect = c(0,0,0,0,31.000082,0))\r\ntadabestGamma_higher5<-tadaFit(nbdadata=c(""ng1t1_higher5"", ""ng1t2_higher5"", ""ng1t3_higher5"", ""ng1t4_higher5"", ""ng1t5_higher5"",\r\n\t\t\t\t\t\t      ""ng2t1_higher5"", ""ng1t2_higher5"", ""ng2t3_higher5"", ""ng2t4_higher5"", ""ng2t5_higher5"",\r\n \t\t\t\t\t\t      ""ng3t1_higher5"", ""ng3t2_higher5"", ""ng3t3_higher5"", ""ng3t4_higher5"", ""ng3t5_higher5""),baseline=""gamma"",iterations=10000)\r\ntadabestGamma_higher5@outputPar\r\nnbdaPropSolveByST.byevent(model=tadabestGamma_higher5)\r\nnbdaPropSolveByST(model=tadabestGamma_higher5)\r\n0.22889/(22/96)\r\n#[5] 0.9987927 \r\n\r\n\r\n\r\n', '### Otter memory test ###\r\nrequire(""lme4"")\r\nrequire(""MuMIn"")\r\nlibrary(""ggplot2"")\r\n#install.packages(""Rmisc"")\r\nlibrary(""Rmisc"")\r\n\r\n###TASK SOLVING MEMORY###\r\n\r\n##Read Data into R##\r\nsolvemem<-read.csv(file.choose(), fileEncoding=""UTF-8-BOM"")\r\nnames(solvemem)\r\nsummary(solvemem)\r\nstr(solvemem)\r\n\r\n#Remove NAs##\r\nsolvemem<-subset(solvemem,!is.na(SolveI)) \r\nsolvemem$SolveI\r\n\r\n##Make R read discrete variables as discrete instead of continuous##\r\nsolvemem$Presentation<-as.factor(solvemem$Presentation) \r\nsolvemem$Presentation\r\nsolvemem$Difficulty<-as.factor(solvemem$Difficulty)\r\nsolvemem$Difficulty\r\nsolvemem$Sex<-as.factor(solvemem$Sex)\r\nsolvemem$Sex\r\n\r\n##Create Model##\r\nsmem <- glmer(SolveI~ solvemem$Presentation  + solvemem$Sex + solvemem$Difficulty + AgeCon + (1|OtterID),\r\n                     data = solvemem, family = Gamma(link=log))\r\n\r\n##Check Model assumptions##\r\nsummary(smem)\r\nplot(resid(smem) ~ fitted(smem))\r\noptions(scipen = 999)\r\nhist(resid(smem))\r\nqqnorm(resid(smem))\r\nqqline(resid(smem))\r\n\r\n\r\n##Need this to set what to do if encounter an NA, will break otherwise##\r\noptions(na.action = ""na.fail"")\r\n\r\n##Rank all possible combinations of variables##\r\nAIC_Models <- dredge(smem, rank=""AIC"")\r\nAIC_Models\r\n\r\n##Subset by delta AIC cut-off (<2; can also be set to <6 to be more lenient)##\r\nAIC_ModelsNEW <- subset(AIC_Models, delta<=2)\r\nAIC_ModelsNEW \r\n\r\n##Apply nesting rule to eliminate more complex models from the top model set##\r\nAIC_ModelsSUB <- subset(AIC_ModelsNEW, !nested(.))\r\nAIC_ModelsSUB\r\n\r\n##Run best model\r\nsmemBEST <- glmer(SolveI~ solvemem$Presentation  + solvemem$Sex + solvemem$Difficulty + (1|OtterID),\r\n                     data = solvemem, family = Gamma(link=log)) #Gamma(link=log)as data positively skewed\r\n\r\n##Check Assumptions again##\r\nsummary(smemBEST)\r\nplot(resid(smemBEST) ~ fitted(smemBEST))\r\nhist(resid(smemBEST))\r\nqqnorm(resid(smemBEST))\r\nqqline(resid(smemBEST))\r\n\r\n##Extract fitted data and attach to database##\r\nsmemPRD<-cbind(solvemem, Fit=predict(smemBEST, newdata=solvemem, type=\'response\'))\r\n\r\n##Subset fitted values for variables of interest to report in manuscript##\r\nsmemRound1<-subset(smemPRD, Presentation==""1"")\r\nsummary(smemRound1$Fit)\r\nsd(smemRound1$Fit)\r\nsmemRound2<-subset(smemPRD, Presentation==""2"")\r\nsummary(smemRound2$Fit)\r\nsd(smemRound2$Fit)\r\nsmemTask1<-subset(smemPRD, Difficulty==""1"")\r\nsummary(smemTask1$Fit)\r\nsd(smemTask1$Fit)\r\nsmemTask2<-subset(smemPRD, Difficulty==""2"")\r\nsummary(smemTask2$Fit)\r\nsd(smemTask2$Fit)\r\nsmemTask3<-subset(smemPRD, Difficulty==""3"")\r\nsummary(smemTask3$Fit)\r\nsd(smemTask3$Fit)\r\nsmemTask4<-subset(smemPRD, Difficulty==""4"")\r\nsummary(smemTask4$Fit)\r\nsd(smemTask4$Fit)\r\nsmemTask5<-subset(smemPRD, Difficulty==""5"")\r\nsummary(smemTask5$Fit)\r\nsd(smemTask5$Fit)\r\nsmemMale<-subset(smemPRD, Sex==""0"")\r\nsummary(smemMale$Fit)\r\nsd(smemMale$Fit)\r\nsmemFemale<-subset(smemPRD, Sex==""1"")\r\nsummary(smemFemale$Fit)\r\nsd(smemFemale$Fit)\r\n\r\n##Plot fitted values##\r\nP1<-ggplot(smemPRD, aes(x = Presentation, y = Fit)) + \r\n  #geom_violin(width = 1)+\r\n  geom_boxplot()+\r\n  scale_y_continuous(limits=c(0,800),breaks=c(0,200,400,600,800))+\r\n  theme_bw()+\r\n        theme(panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank())+\r\n  xlab(""Task Presentation Round"") + ylab(""Solve Latency (sec)"") +\r\n  ggtitle(""(a)"")+\r\n        theme(plot.title = element_text(vjust = - 10))+\r\n        theme(plot.title = element_text(hjust = ""0.96""))+     \r\n        theme(plot.title = element_text(color = ""black"", size = 13, face = ""bold""))\r\n\r\nP2<-ggplot(smemPRD, aes(x = Difficulty, y = Fit)) + \r\n  geom_boxplot()+\r\n  scale_y_continuous(limits=c(0,800),breaks=c(0,200,400,600,800))+\r\n   theme_bw()+\r\n        theme(panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank())+\r\n  xlab(""Task Type"") +\r\n  theme(axis.title.y=element_blank())+\r\n  #theme(axis.text.y = element_text( color=""White""))+\r\n  ggtitle(""(b)"")+\r\n        theme(plot.title = element_text(vjust = - 10))+\r\n        theme(plot.title = element_text(hjust = ""0.96""))+     \r\n        theme(plot.title = element_text(color = ""black"", size = 13, face = ""bold""))\r\n\r\nP3<-ggplot(smemPRD, aes(x = Sex, y = Fit)) + \r\n  geom_boxplot()+\r\n  scale_y_continuous(limits=c(0,800),breaks=c(0,200,400,600,800))+\r\n   theme_bw()+\r\n        theme(panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank())+\r\n  scale_x_discrete(labels=c(""0"" = ""Male"", ""1"" = ""Female""))+\r\n  xlab(""Sex"") +\r\n  theme(axis.title.y=element_blank())+\r\n  #theme(axis.text.y = element_text( color=""White""))+\r\n  ggtitle(""(c)"")+\r\n        theme(plot.title = element_text(vjust = - 10))+\r\n        theme(plot.title = element_text(hjust = ""0.96""))+     \r\n        theme(plot.title = element_text(color = ""black"", size = 13, face = ""bold""))\r\n\r\nmultiplot(P1,P2,P3,cols=3)']","Learning strategies and long-term memory in Asian short-clawed otters (Aonyx cinereus) data Data submitted here, are those used in the writing of our manuscript entitled ""Learning strategies and long-term memory in Asian short-clawed otters (Aonyx cinereus)"" which has been submitted to Royal Society Open Science for publication. Abstract for that manuscript is belowSocial learning, namely learning from information acquired from others or their products, is widespread throughout the animal kingdom. There is growing evidence that animals selectively employ 'social learning strategies', which for example, determine when they should copy others instead of learning asocially, and whom they should copy. Furthermore, once animals have acquired new information, it is beneficial for them to commit it to long-term memory, especially when it concerns the discovery of profitable resources. Research into social learning strategies and long-term memory has covered a wide range of taxa. However, otters (subfamily Lutrinae), popular in zoos due to their sociability and playfulness, remained neglected until a recent study provided evidence of social learning in captive smooth-coated otters (Lutrogale perspicillata), but not in Asian short-clawed otters (Aonyx cinereus). We investigated Asian short-clawed otters' learning strategies and long-term memory performance in a foraging context. We presented novel extractive foraging tasks twice to captive family groups and used network-based diffusion analysis to provide evidence of social learning and long-term memory in this species. A major cause of wild Asian short-clawed otter declines is prey scarcity. Furthering our understanding of how they learn about and remember novel food sources could inform key conservation strategies.",2
A mid-20th century inventory-based estimate of global terrestrial vegetation carbon stocks,This link contains the supporting figures and datasets for Bhan et al. (in review); A mid-20th century inventory-based estimate of global terrestrial vegetation carbon stocks.,,A mid-20th century inventory-based estimate of global terrestrial vegetation carbon stocks This link contains the supporting figures and datasets for Bhan et al. (in review); A mid-20th century inventory-based estimate of global terrestrial vegetation carbon stocks.,2
Vocalizations of the squirrel family,"The dataset Squirrel_Calls is a collection of vocal records (defined as primary literature that numerically describes the vocalization of at least 1 squirrel species) where each row corresponds to a single call type of one species. The details of the row include a summary of the literature metadata, categorical descriptions of the call and the caller as well as numerical values of the call frequencies. The dataset Squirrel_Ecological_Traits is a corresponding set of ecological traits for all the species listed in the Squirrel_Calls dataset. The traits listed (mass, time partitioning, gliding capabilities, habitat, and sociality) reflect hypotheses and predictions explored in the associated article. At the end of this document, there is a complete list of the literature references used to assemble these datasets. Squirrel_Script is the R script used to produce the statistics and models used in the corresponding paper. Squirrel_Tree is a nexus file compiling the data of 1000 trees downloaded from VertLife.org which were subsetted from their published mammalian supertree. The nexus file was used in the R script.","['################# Get this party started #################\r\n###Set Working Directory###\r\nsetwd(""C:/Users/sasha/OneDrive/Graduate Studies/Thesis/Think Before They Squeak/Submissions/Raw Data/"")\r\n\r\n###Load packages###\r\nlibrary(ape)\r\nlibrary(caper) #PGLS Modeling\r\nlibrary(dplyr) #Y\r\nlibrary(geiger) #Phylogenetic tree and pgls\r\nlibrary(ggmap) #Mapping with ggplot\r\nlibrary(ggplot2) #Plotting\r\nlibrary(ggpubr) #ggplot extension for combining plots\r\nlibrary(glmm)\r\nlibrary(phytools)\r\nlibrary(plyr)\r\nlibrary(reshape2) #Melting, reshaping, and merging dataframes\r\nlibrary(tidyr) #Y\r\nlibrary(tidyverse)\r\nlibrary(viridis) #Beautiful colour schemes for ggplot - colour blind approved?\r\n\r\n################# Load and Merge Datasets #################\r\n#Call Data\r\nSqu_Calls <- read.csv(""Squirrel_Calls.csv"", header=T, na.strings=c("""",""NA""))\r\nSqu_Calls$ScName <- Squ_Calls$Scientific_Name\r\nSqu_Calls$Scientific_Name <- as.factor(gsub("" "", ""_"", Squ_Calls$Scientific_Name))\r\nstr(Squ_Calls)\r\n\r\n#Ecological Data\r\nSqu_Eco <- read.csv(""Squirrel_Ecological_Traits.csv"", header = T, na.strings = c("""",""NA""))\r\nSqu_Eco$Scientific_Name <- gsub("" "", ""_"", Squ_Eco$Scientific_Name)\r\nstr(Squ_Eco)\r\n\r\n#Merge Datasets\r\nSquirrels <- merge(Squ_Calls, Squ_Eco, by = ""Scientific_Name"", all.x = T)\r\nSquirrels <- Squirrels[, -which(names(Squirrels) %in% c(""Common_Name.y""))]\r\n\r\n#Cleaning up categorical variables\r\nSquirrels$Openness <- factor(Squirrels$H_Openness)\r\nSquirrels$Sociality <- factor(Squirrels$Sociality)\r\nSquirrels$Soc <- as.factor(ifelse(Squirrels$Sociality == ""CFTM"", ""Social"", \r\n                                  ifelse(Squirrels$Sociality == ""CFWM"", ""Social"", \r\n                                         ifelse(Squirrels$Sociality == ""Colonial"", ""Social"", \r\n                                                ifelse(Squirrels$Sociality == ""HFCM"", ""Social"", \r\n                                                       ifelse(Squirrels$Sociality == ""HFTM"", ""Social"", \r\n                                                              ifelse(Squirrels$Sociality == ""Alternate"", ""Social"",\r\n                                                                     ifelse(Squirrels$Sociality == ""Monogamous"", ""Social"", \r\n                                                                            ""Solitary""))))))))\r\n\r\nSquirrels <- Squirrels[!(Squirrels$Sex == ""Male""),]\r\nSquirrels <- Squirrels[!(Squirrels$Age == ""Pup""),]\r\n\r\n################# Frequencies #################\r\n\r\n#Fundamental Frequency\r\nFreq_fun_temp <- Squirrels %>%\r\n  group_by(Scientific_Name) %>%\r\n  slice(which.max(Fun_Freq_kHz))\r\nFreq_fun <- as.data.frame(Freq_fun_temp[,c(""Scientific_Name"", ""ScName"", ""Mass_g_Females"", ""Time_Partitioning"", ""Openness"", ""Soc"", ""Fun_Freq_kHz"", ""Year"", ""Ana_Hz_Max"")])\r\nFreq_fun$Ana_Max <- log(Freq_fun$Ana_Hz_Max + 1)\r\nFreq_fun$Time_Partitioning[Freq_fun$Time_Partitioning == ""Crepuscular""] <- NA\r\nFreq_fun <- Freq_fun %>%\r\n  drop_na(Time_Partitioning, Soc, Ana_Max)\r\nrow.names(Freq_fun) <- Freq_fun$Scientific_Name\r\nFreq_fun$Time_Partitioning <- factor(Freq_fun$Time_Partitioning)\r\n\r\n###Dominant Frequency\r\nFreq_dom_temp <- Squirrels %>%\r\n  group_by(Scientific_Name) %>%\r\n  slice(which.max(Dom_Freq_kHz))\r\nFreq_dom <- as.data.frame(Freq_dom_temp[,c(""Scientific_Name"", ""ScName"", ""Mass_g_Females"", ""Time_Partitioning"", ""Openness"", ""Soc"", ""Dom_Freq_kHz"", ""Year"", ""Ana_Hz_Max"")])\r\nFreq_dom$Ana_Max <- log(Freq_dom$Ana_Hz_Max + 1)\r\nFreq_dom$Time_Partitioning[Freq_dom$Time_Partitioning == ""Crepuscular""] <- NA\r\nFreq_dom <- Freq_dom %>%\r\n  drop_na(Time_Partitioning, Soc, Ana_Max)\r\nrow.names(Freq_dom) <- Freq_dom$Scientific_Name\r\nFreq_dom$Time_Partitioning <- factor(Freq_dom$Time_Partitioning)\r\n\r\n#Minimum Dominant Frequency\r\nFreq_min <- Squirrels %>%\r\n  group_by(Scientific_Name) %>%\r\n  slice(which.min(Min_Freq_kHz))\r\nFreq_min <- as.data.frame(Freq_min[,c(""Scientific_Name"", ""ScName"", ""Mass_g_Females"", ""Time_Partitioning"", ""Openness"", ""Soc"", ""Min_Freq_kHz"", ""Year"", ""Ana_Hz_Max"")])\r\nFreq_min$Ana_Max <- log(Freq_min$Ana_Hz_Max + 1)\r\nFreq_min$Time_Partitioning[Freq_min$Time_Partitioning == ""Crepuscular""] <- NA\r\nFreq_min <- Freq_min %>%\r\n  drop_na(Time_Partitioning, Soc, Ana_Max)\r\nrow.names(Freq_min) <- Freq_min$Scientific_Name\r\nFreq_min$Time_Partitioning <- factor(Freq_min$Time_Partitioning)\r\n\r\n#Maximum Frequency\r\nFreq_max <- Squirrels %>%\r\n  group_by(Scientific_Name) %>%\r\n  top_n(1, Max_Freq_kHz)\r\nFreq_max <- as.data.frame(Freq_max[,c(""Scientific_Name"", ""ScName"", ""Mass_g_Females"", ""Time_Partitioning"", ""Openness"", ""Soc"", ""Max_Freq_kHz"", ""Year"", ""Ana_Hz_Max"")])\r\nFreq_max$Ana_Max <- log(Freq_max$Ana_Hz_Max + 1)\r\nFreq_max$Time_Partitioning[Freq_max$Time_Partitioning == ""Crepuscular""] <- NA\r\nFreq_max <- Freq_max %>%\r\n  drop_na(Time_Partitioning, Soc, Ana_Max)\r\nrow.names(Freq_max) <- Freq_max$Scientific_Name\r\nFreq_max$Time_Partitioning <- factor(Freq_max$Time_Partitioning)\r\n\r\n#Maximum Visible Harmonic\r\nHarm_max <- Squirrels %>%\r\n  group_by(Scientific_Name) %>%\r\n  top_n(1, Highest_Harmonic_kHz)\r\nHarm_max <- Harm_max[!duplicated(Harm_max$Scientific_Name),]\r\nHarm_max <- as.data.frame(Harm_max[,c(""Scientific_Name"", ""ScName"", ""Mass_g_Females"", ""Time_Partitioning"", ""Openness"", ""Soc"", ""Highest_Harmonic_kHz"", ""Year"", ""Ana_Hz_Max"")])\r\nHarm_max$Ana_Max <- log(Harm_max$Ana_Hz_Max + 1)\r\nHarm_max$Time_Partitioning[Harm_max$Time_Partitioning == ""Crepuscular""] <- NA\r\nHarm_max <- Harm_max %>%\r\n  drop_na(Time_Partitioning, Soc, Ana_Max)\r\nrow.names(Harm_max) <- Harm_max$Scientific_Name\r\nHarm_max$Time_Partitioning <- factor(Harm_max$Time_Partitioning)\r\n\r\n###Combining Frequencies\r\nFrequencies <- merge(Freq_fun, Freq_dom, by = ""ScName"")\r\nFrequencies <- merge(Frequencies, Freq_max, by = ""ScName"")\r\nFrequencies <- merge(Frequencies, Freq_min, by = ""ScName"")\r\nFrequencies <- merge(Frequencies, Harm_max, by = ""ScName"")\r\ncolnames(Frequencies) <- c(""ScName"", ""Scientific.Name.ff"", ""Body.Size...Females..g..ff"", ""Temporality.ff"", ""Openness.ff"", ""Soc.ff"", \r\n                           ""Fundamental.Freq..kHz."", ""Year.ff"", ""Ana_Max.Hz.ff"", ""Scientific.Name.df"", ""Body.Size...Females..g..df"",\r\n                           ""Temporality.df"", ""Openness.df"", ""Soc.df"", ""Dom.Frequency..mean"", ""Year.df"", ""Ana_Max.Hz.df"", \r\n                           ""Scientific.Name.mxf"", ""Body.Size...Females..g..mxf"", ""Temporality.mxf"", ""Openness.mxf"", ""Soc.mxf"", \r\n                           ""Max.Freq..kHz."", ""Year.mxf"", ""Ana_Max.Hz.mxf"", ""Scientific.Name.mnf"", ""Body.Size...Females..g..mnf"", \r\n                           ""Temporality.mnf"", ""Openness.mnf"", ""Soc.mnf"", ""Min.Freq..kHz."", ""Year.mnf"", ""Ana_Max.Hz.mnf"", \r\n                           ""Scientific.Name.hmf"", ""Body.Size...Females..g..hmf"", ""Temporality.hmf"", ""Openness.hmf"", ""Soc.hmf"", \r\n                           ""Mean_of_Highest_Harmonic_Visible"", ""Year.hmf"", ""Ana_Max.Hz.hmf"")\r\n\r\n################# Phylogenetic Tree #################\r\n#Load Tree\r\nVertLife_Trees <- read.nexus(""output_200402.nex"")\r\nSquirrel_Tree <- VertLife_Trees$tree_2033 \r\n#sample(1:100, 1) #Randomly selecting tree #321: tree_2033\r\n\r\n#Add tips to tree\r\nSquirrels_sci_name <- unique(Squirrels$Scientific.Name)\r\nname.check(Squirrel_Tree, Squirrels_sci_name)\r\nSquirrel_Tree <- bind.tip(Squirrel_Tree, ""Glaucomys_sabrinus_coloratus"", edge.length = 1, where = 65, position = 1)\r\nSquirrel_Tree <- bind.tip(Squirrel_Tree, ""Xerospermophilus_spilosoma_annectens"", edge.length = 1, where = 40, position = 1)\r\nSquirrel_Tree <- bind.tip(Squirrel_Tree, ""Xerospermophilus_spilosoma_marginatus"", edge.length = 1, where = 40, position = 1)\r\nSquirrel_Tree <- bind.tip(Squirrel_Tree, ""Spermophilus_musicus"", edge.length = 1, where = 28, position = 1)\r\n\r\n\r\n#Rename any tips\r\nSquirrel_Tree$tip.label[Squirrel_Tree$tip.label == ""Sciurus_aberti""] <- ""Sciurus_aberti_kaibabensis""\r\nSquirrel_Tree$tip.label[Squirrel_Tree$tip.label == ""Sciurus_niger""] <- ""Sciurus_niger_rufiventer""\r\nSquirrel_Tree$tip.label[Squirrel_Tree$tip.label == ""Callosciurus_erythraeus""] <- ""Callosciurus_erythraeus_thaiwanensis""\r\n\r\n#Drop tips\r\nSquirrel_Tree <- drop.tip(Squirrel_Tree, 41)\r\n\r\n\r\npar(mar = c(0,0,0,0))\r\nplot(Squirrel_Tree, type = ""p"", no.margin = TRUE, cex = 0.5, direction = ""right"", \r\n     use.edge.length = TRUE)\r\n#nodelabels()\r\nadd.scale.bar(cex = 0.7, font = 2, col = ""red"")\r\n\r\nlibrary(png)\r\nlibrary(grid)\r\n\r\nPtero_image <- readPNG(""Glaucomys_volans.png"")\r\nSciu_image <- readPNG(""Tamiasciurus_hudsonicus.png"")\r\nCall_image <- readPNG(""Callosciurus.png"")\r\nMarm_image <- readPNG(""Marmota_marmota.png"")\r\nSper_image <- readPNG(""Urocitellus_richardsonii.png"")\r\nTam_image <- readPNG(""Tamias_striatus.png"")\r\nXer_image <- readPNG(""Xerus_inauris.png"")\r\n\r\nPtero <- Squirrel_Tree$tip.label[c(66:70)] \r\nSciu <- Squirrel_Tree$tip.label[c(61:65)]\r\nCallo <- Squirrel_Tree$tip.label[c(57:60)]\r\nMarm <- Squirrel_Tree$tip.label[c(43:52)]\r\nSper <- Squirrel_Tree$tip.label[c(20:42,53:58)]\r\nTam <- Squirrel_Tree$tip.label[c(2:20)]\r\nXer <- Squirrel_Tree$tip.label[c(1)]\r\n\r\n#Long form for publication\r\n#dev.new(width = 900, height = 1200, unit = ""px"")\r\ntiff(""Fig2.tiff"", units=""in"", width=6,height=8, res=300)\r\nplot(Squirrel_Tree,tip.color = ifelse(Squirrel_Tree$tip.label %in% Ptero, ""#D55E00"", \r\n                                      ifelse(Squirrel_Tree$tip.label %in% Sciu, ""#E69F00"",\r\n                                             ifelse(Squirrel_Tree$tip.label %in% Callo, ""#999999"",\r\n                                                    ifelse(Squirrel_Tree$tip.label %in% Marm,""#56B4E9"",\r\n                                                           ifelse(Squirrel_Tree$tip.label %in% Sper, ""#0072B2"",\r\n                                                                  ifelse(Squirrel_Tree$tip.label %in% Tam, ""#009E73"", ""#CC79A7"")))))),\r\n     type = ""phylogram"", no.margin = TRUE, cex = 0.65, direction = ""right"") \r\n\r\nadd.scale.bar(x = 55, y = 1.3, cex = 0.65)\r\n\r\nadd.arrow(Squirrel_Tree, ""Glaucomys_sabrinus_coloratus"", col = ""#D55E00"", lwd = 1, offset = 1, arrl = 5, hedl = 2)\r\nadd.arrow(Squirrel_Tree, ""Tamiasciurus_hudsonicus"", col = ""#E69F00"", lwd = 1, offset = 2.5, arrl = 12, hedl = 2)\r\nadd.arrow(Squirrel_Tree, ""Callosciurus_nigrovittatus"", col = ""#999999"", lwd = 1, offset = 2.5, arrl = 5, hedl = 2)\r\nadd.arrow(Squirrel_Tree, ""Marmota_vancouverensis"", col = ""#56B4E9"", lwd = 1, offset = 2.5, arrl = 8, hedl = 2)\r\nadd.arrow(Squirrel_Tree, ""Urocitellus_richardsonii"", col = ""#0072B2"", lwd = 1, offset = 2.5, arrl = 8, hedl = 2)\r\nadd.arrow(Squirrel_Tree, ""Tamias_sonomae"", col = ""#009E73"", lwd = 1, offset = 2.5, arrl = 8, hedl = 2)\r\nadd.arrow(Squirrel_Tree, ""Xerus_inauris"", col = ""#CC79A7"", lwd = 1, offset = 3, arrl = 8, hedl = 2)\r\n\r\ngrid.raster(Ptero_image, 0.85, 0.97, width = 0.15)\r\ngrid.raster(Call_image, 0.83, 0.78, width = 0.15)\r\ngrid.raster(Marm_image, 0.84, 0.65, width = 0.17)\r\ngrid.raster(Sper_image, 0.85, 0.45, width = 0.08)\r\ngrid.raster(Tam_image, 0.82, 0.17, width = 0.1)\r\ngrid.raster(Xer_image, 0.75, 0.07, width = 0.1)\r\ngrid.raster(Sciu_image, 0.92, 0.87, width = 0.1)\r\n\r\ndev.off()\r\n\r\n\r\n################# Minimum Frequency Models #################\r\ncombined_LFQ <- comparative.data(\r\n  Squirrel_Tree, Freq_min, names.col = ""Scientific.Name"", \r\n  vcv = TRUE, na.omit = TRUE)\r\n\r\n#Null\r\nmodel0_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ Ana_Min.Hz, data = combined_LFQ, lambda = ""ML"")\r\n#summary(model0_LFQ)\r\nAIC(model0_LFQ)\r\n\r\n#Body mass\r\nmodel1_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Ana_Min.Hz, data = combined_LFQ, lambda = ""ML"")\r\nsummary(model1_LFQ)\r\nAIC(model1_LFQ)\r\n\r\n#Temporality\r\nmodel2_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ Temporality + Ana_Min.Hz, data = combined_LFQ, lambda = ""ML"")\r\n#summary(model2_LFQ)\r\nAIC(model2_LFQ)\r\n\r\n#Sociality\r\nmodel3_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ Soc + Ana_Min.Hz, data = combined_LFQ, lambda = ""ML"")\r\n#summary(model3_LFQ)\r\nAIC(model3_LFQ)\r\n\r\n#Life History\r\nmodel4_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ Openness + Ana_Min.Hz, data = combined_LFQ, lambda = ""ML"")\r\n#summary(model4_LFQ)\r\nAIC(model4_LFQ)\r\n\r\n#Body mass, temporality\r\nmodel5_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Temporality + Ana_Min.Hz,\r\n                   data = combined_LFQ, lambda = ""ML"")\r\nsummary(model5_LFQ)\r\nAIC(model5_LFQ)\r\n\r\n#Body mass, sociality\r\nmodel6_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Soc + Ana_Min.Hz, \r\n                   data = combined_LFQ, lambda = ""ML"")\r\nsummary(model6_LFQ)\r\nAIC(model6_LFQ)\r\n\r\n#Body mass, life.history\r\nmodel7_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Openness + Ana_Min.Hz, \r\n                   data = combined_LFQ, lambda = ""ML"")\r\nsummary(model7_LFQ)\r\nAIC(model7_LFQ)\r\n\r\n#Body mass, temporality, sociality\r\nmodel8_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Temporality +Soc + Ana_Min.Hz, \r\n                   data = combined_LFQ, lambda = ""ML"")\r\n#summary(model8_LFQ)\r\nAIC(model8_LFQ)\r\n\r\n#Body mass, temporality, life history\r\nmodel9_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Temporality + Openness + Ana_Min.Hz, \r\n                   data = combined_LFQ, lambda = ""ML"")\r\n#summary(model9_LFQ)\r\nAIC(model9_LFQ)\r\n\r\n#Body mass, sociality, life history\r\nmodel10_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Soc + Openness + Ana_Min.Hz, \r\n                    data = combined_LFQ, lambda = ""ML"")\r\n#summary(model10_LFQ)\r\nAIC(model10_LFQ)\r\n\r\n#Body mass, temporality, sociality, life-history FULL MODEL\r\nmodel11_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Temporality + Soc + Openness + Ana_Min.Hz, \r\n                    data = combined_LFQ, lambda = ""ML"")\r\nsummary(model11_LFQ)\r\nAIC(model11_LFQ)\r\nanova(model11_LFQ)\r\n\r\n#Temporality, sociality\r\nmodel12_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ Temporality + Soc + Ana_Min.Hz, data = combined_LFQ, lambda = ""ML"")\r\n#summary(model12_LFQ)\r\nAIC(model12_LFQ)\r\n\r\n#Temporality, life history\r\nmodel13_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ Temporality + Openness + Ana_Min.Hz, data = combined_LFQ, lambda = ""ML"")\r\n#summary(model13_LFQ)\r\nAIC(model13_LFQ)\r\n\r\n#Sociality, life history\r\nmodel14_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ Soc + Openness + Ana_Min.Hz, data = combined_LFQ, lambda = ""ML"")\r\n#summary(model14_LFQ)\r\nAIC(model14_LFQ)\r\n\r\n#Temporality, sociality, life-history\r\nmodel15_LFQ <- pgls(log(Min.Freq..kHz. + 1) ~ Temporality + Soc + Openness + Ana_Min.Hz, data = combined_LFQ, lambda = ""ML"")\r\n#summary(model15_LFQ)\r\nAIC(model15_LFQ)\r\n\r\n\r\n################# Maximum Frequency Models #################\r\ncombined_HFQ <- comparative.data(\r\n  Squirrel_Tree, Freq_max, names.col = ""Scientific.Name"", \r\n  vcv = TRUE, na.omit = FALSE)\r\n\r\n#Null\r\nmodel0_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ Ana_Max.Hz, \r\n                   data = combined_HFQ, lambda = ""ML"")\r\nsummary(model0_HFQ)\r\nAIC(model0_HFQ)\r\n\r\n#Body mass\r\nmodel1_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Ana_Max.Hz, \r\n                   data = combined_HFQ, lambda = ""ML"")\r\nsummary(model1_HFQ)\r\nAIC(model1_HFQ)\r\n\r\n#Temporality\r\nmodel2_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ Temporality + Ana_Max.Hz, data = combined_HFQ, \r\n                   lambda = ""ML"")\r\n#summary(model2_HFQ)\r\nAIC(model2_HFQ)\r\n\r\n#Sociality\r\nmodel3_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ Soc + Ana_Max.Hz, data = combined_HFQ, lambda = ""ML"")\r\n#summary(model3_HFQ)\r\nAIC(model3_HFQ)\r\n\r\n#Life History\r\nmodel4_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ Openness + Ana_Max.Hz, data = combined_HFQ, \r\n                   lambda = ""ML"")\r\n#summary(model4_HFQ)\r\nAIC(model4_HFQ)\r\n\r\n#Body mass, temporality\r\nmodel5_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Temporality + Ana_Max.Hz,\r\n                   data = combined_HFQ, lambda = ""ML"")\r\nsummary(model5_HFQ)\r\nAIC(model5_HFQ)\r\n#model5_HFQ_t <- pgls(log(Max.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Temporality,\r\n#                   data = combined_HFQ, lambda = ""ML"")\r\n#summary(model5_HFQ_t)\r\n#AIC(model5_HFQ_t)\r\n\r\n#Body mass, sociality\r\nmodel6_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Soc + Ana_Max.Hz, \r\n                   data = combined_HFQ, lambda = ""ML"")\r\n#summary(model6_HFQ)\r\nAIC(model6_HFQ)\r\n\r\n#Body mass, life.history\r\nmodel7_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Openness + Ana_Max.Hz, \r\n                   data = combined_HFQ, lambda = ""ML"")\r\n#summary(model7_HFQ)\r\nAIC(model7_HFQ)\r\n\r\n#Body mass, temporality, sociality\r\nmodel8_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Temporality +\r\n                     Soc + Ana_Max.Hz, data = combined_HFQ, lambda = ""ML"")\r\n#summary(model8_HFQ)\r\nAIC(model8_HFQ)\r\n\r\n#Body mass, temporality, life history\r\nmodel9_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Temporality + \r\n                     Openness + Ana_Max.Hz, data = combined_HFQ, lambda = ""ML"")\r\n#summary(model9_HFQ)\r\nAIC(model9_HFQ)\r\n\r\n#Body mass, sociality, life history\r\nmodel10_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Soc + \r\n                      Openness + Ana_Max.Hz, data = combined_HFQ, lambda = ""ML"")\r\n#summary(model10_HFQ)\r\nAIC(model10_HFQ)\r\n\r\n#Body mass, temporality, sociality, life-history\r\nmodel11_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Temporality + Soc + Openness* + Ana_Max.Hz, \r\n                    data = combined_HFQ, lambda = ""ML"")\r\nsummary(model11_HFQ)\r\nAIC(model11_HFQ)\r\nanova(model11_HFQ)\r\n\r\n#Temporality, sociality\r\nmodel12_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ Temporality + Soc + \r\n                      Ana_Max.Hz, \r\n                    data = combined_HFQ, lambda = ""ML"")\r\n#summary(model12_HFQ)\r\nAIC(model12_HFQ)\r\n\r\n#Temporality, life history\r\nmodel13_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ Temporality + Openness+ \r\n                      Ana_Max.Hz, \r\n                    data = combined_HFQ, lambda = ""ML"")\r\n#summary(model13_HFQ)\r\nAIC(model13_HFQ)\r\n\r\n#Sociality, life history\r\nmodel14_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ Soc + Openness + Ana_Max.Hz, \r\n                    data = combined_HFQ, lambda = ""ML"")\r\n#summary(model14_HFQ)\r\nAIC(model14_HFQ)\r\n\r\n#Temporality, sociality, life-history\r\nmodel15_HFQ <- pgls(log(Max.Freq..kHz. + 1) ~ Temporality + Soc + Openness+ \r\n                      Ana_Max.Hz, \r\n                    data = combined_HFQ, lambda = ""ML"")\r\n#summary(model15_HFQ)\r\nAIC(model15_HFQ)\r\n\r\n\r\n################# Dominant Frequency Models #################\r\n\r\ncombined_DFQ <- comparative.data(\r\n  Squirrel_Tree, Freq_dom, names.col = ""Scientific.Name"", \r\n  vcv = TRUE, na.omit = FALSE)\r\n\r\n\r\n#Null\r\nmodel0_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ Ana_Max.Hz, data = combined_DFQ, lambda = ""ML"")\r\n#summary(model0_DFQ)\r\nAIC(model0_DFQ)\r\n\r\n#Body mass\r\nmodel1_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ log(Body.Size...Females..g.) + Ana_Max.Hz, data = combined_DFQ, lambda = ""ML"")\r\nsummary(model1_DFQ)\r\nAIC(model1_DFQ)\r\n\r\n#Temporality\r\nmodel2_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ Temporality + Ana_Max.Hz, data = combined_DFQ, lambda = ""ML"")\r\n#summary(model2_DFQ)\r\nAIC(model2_DFQ)\r\n\r\n#Sociality\r\nmodel3_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ Soc + Ana_Max.Hz, data = combined_DFQ, lambda = ""ML"")\r\n#summary(model3_DFQ)\r\nAIC(model3_DFQ)\r\n\r\n#Life History\r\nmodel4_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ Openness + Ana_Max.Hz, data = combined_DFQ, lambda = ""ML"")\r\n#summary(model4_DFQ)\r\nAIC(model4_DFQ)\r\n\r\n#Body mass, temporality\r\nmodel5_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ log(Body.Size...Females..g.) + Temporality + Ana_Max.Hz, \r\n                   data = combined_DFQ, lambda = ""ML"")\r\nsummary(model5_DFQ)\r\nAIC(model5_DFQ)\r\n\r\n#Body mass, sociality\r\nmodel6_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ log(Body.Size...Females..g.) + Soc + Ana_Max.Hz, \r\n                   data = combined_DFQ, lambda = ""ML"")\r\n#summary(model6_DFQ)\r\nAIC(model6_DFQ)\r\n\r\n#Body mass, life.history\r\nmodel7_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ log(Body.Size...Females..g.) + Openness + Ana_Max.Hz, \r\n                   data = combined_DFQ, lambda = ""ML"")\r\n#summary(model7_DFQ)\r\nAIC(model7_DFQ)\r\n\r\n#Body mass, temporality, sociality\r\nmodel8_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ log(Body.Size...Females..g.) +Temporality + Soc + Ana_Max.Hz, \r\n                   data = combined_DFQ, lambda = ""ML"")\r\n#summary(model8_DFQ)\r\nAIC(model8_DFQ)\r\n\r\n#Body mass, temporality, life history\r\nmodel9_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ log(Body.Size...Females..g.) + Temporality + Openness + Ana_Max.Hz, \r\n                   data = combined_DFQ, lambda = ""ML"")\r\nsummary(model9_DFQ)\r\nAIC(model9_DFQ)\r\n\r\n#Body mass, sociality, life history\r\nmodel10_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ log(Body.Size...Females..g.) + Soc + Openness + Ana_Max.Hz, \r\n                    data = combined_DFQ, lambda = ""ML"")\r\n#summary(model10_DFQ)\r\nAIC(model10_DFQ)\r\n\r\n#Body mass, temporality, sociality, life-history\r\nmodel11_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ log(Body.Size...Females..g.) + Temporality + Soc + Openness + Ana_Max.Hz, \r\n                    data = combined_DFQ, lambda = ""ML"")\r\nsummary(model11_DFQ)\r\nAIC(model11_DFQ)\r\nanova(model11_DFQ)\r\n\r\n#Temporality, sociality\r\nmodel12_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ Temporality + Soc + Ana_Max.Hz, \r\n                    data = combined_DFQ, lambda = ""ML"")\r\n#summary(model12_DFQ)\r\nAIC(model12_DFQ)\r\n\r\n#Temporality, life history\r\nmodel13_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ Temporality + Openness + Ana_Max.Hz, \r\n                    data = combined_DFQ, lambda = ""ML"")\r\n#summary(model13_DFQ)\r\nAIC(model13_DFQ)\r\n\r\n#Sociality, life history\r\nmodel14_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ Soc + Openness + Ana_Max.Hz, data = combined_DFQ, lambda = ""ML"")\r\n#summary(model14_DFQ)\r\nAIC(model14_DFQ)\r\n\r\n#Temporality, sociality, life-history\r\nmodel15_DFQ <- pgls(log(Dom.Frequency..mean. + 1) ~ Temporality + Soc + Openness + Ana_Max.Hz, \r\n                    data = combined_DFQ, lambda = ""ML"")\r\n#summary(model15_DFQ)\r\nAIC(model15_DFQ)\r\n\r\n################# Fundamental Frequency Models #################\r\n\r\ncombined_FFQ <- comparative.data(\r\n  Squirrel_Tree, Freq_fun, names.col = ""Scientific.Name"", \r\n  vcv = TRUE, na.omit = FALSE)\r\n\r\n#Null\r\nmodel0_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ Ana_Max.Hz, data = combined_FFQ, lambda = ""ML"")\r\n#summary(model0_FFQ)\r\nAIC(model0_FFQ)\r\n\r\n#Body mass\r\nmodel1_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Ana_Max.Hz, data = combined_FFQ, lambda = ""ML"")\r\nsummary(model1_FFQ)\r\nAIC(model1_FFQ)\r\n\r\n#Temporality\r\nmodel2_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ Temporality + Ana_Max.Hz, data = combined_FFQ, lambda = ""ML"")\r\n#summary(model2_FFQ)\r\nAIC(model2_FFQ)\r\n\r\n#Sociality\r\nmodel3_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ Soc + Ana_Max.Hz, data = combined_FFQ, lambda = ""ML"")\r\n#summary(model3_FFQ)\r\nAIC(model3_FFQ)\r\n\r\n#Life History\r\nmodel4_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ Openness + Ana_Max.Hz, data = combined_FFQ, lambda = ""ML"")\r\n#summary(model4_FFQ)\r\nAIC(model4_FFQ)\r\n\r\n#Body mass, temporality\r\nmodel5_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Temporality + Ana_Max.Hz, \r\n                   data = combined_FFQ, lambda = ""ML"")\r\nsummary(model5_FFQ)\r\nAIC(model5_FFQ)\r\n\r\n#Body mass, sociality\r\nmodel6_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Soc + Ana_Max.Hz, \r\n                   data = combined_FFQ, lambda = ""ML"")\r\n#summary(model6_FFQ)\r\nAIC(model6_FFQ)\r\n\r\n#Body mass, life.history\r\nmodel7_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Openness + Ana_Max.Hz, \r\n                   data = combined_FFQ, lambda = ""ML"")\r\n#summary(model7_FFQ)\r\nAIC(model7_FFQ)\r\n\r\n#Body mass, temporality, sociality\r\nmodel8_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) +Temporality + Soc + Ana_Max.Hz, \r\n                   data = combined_FFQ, lambda = ""ML"")\r\n#summary(model8_FFQ)\r\nAIC(model8_FFQ)\r\n\r\n#Body mass, temporality, life history\r\nmodel9_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Temporality + Openness + Ana_Max.Hz, \r\n                   data = combined_FFQ, lambda = ""ML"")\r\nsummary(model9_FFQ)\r\nAIC(model9_FFQ)\r\n\r\n#Body mass, sociality, life history\r\nmodel10_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Soc + Openness + Ana_Max.Hz, \r\n                    data = combined_FFQ, lambda = ""ML"")\r\n#summary(model10_FFQ)\r\nAIC(model10_FFQ)\r\n\r\n#Body mass, temporality, sociality, life-history\r\nmodel11_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ log(Body.Size...Females..g.) + Temporality + Soc + Openness + Ana_Max.Hz, \r\n                    data = combined_FFQ, lambda = ""ML"")\r\nsummary(model11_FFQ)\r\nAIC(model11_FFQ)\r\nanova(model11_FFQ)\r\n\r\n#Temporality, sociality\r\nmodel12_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ Temporality + Soc + Ana_Max.Hz, \r\n                    data = combined_FFQ, lambda = ""ML"")\r\n#summary(model12_FFQ)\r\nAIC(model12_FFQ)\r\n\r\n#Temporality, life history\r\nmodel13_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ Temporality + Openness + Ana_Max.Hz, \r\n                    data = combined_FFQ, lambda = ""ML"")\r\n#summary(model13_FFQ)\r\nAIC(model13_FFQ)\r\n\r\n#Sociality, life history\r\nmodel14_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ Soc + Openness + Ana_Max.Hz, data = combined_FFQ, lambda = ""ML"")\r\n#summary(model14_FFQ)\r\nAIC(model14_FFQ)\r\n\r\n#Temporality, sociality, life-history\r\nmodel15_FFQ <- pgls(log(Fundamental.Freq..kHz. + 1) ~ Temporality + Soc + Openness + Ana_Max.Hz, \r\n                    data = combined_FFQ, lambda = ""ML"")\r\n#summary(model15_FFQ)\r\nAIC(model15_FFQ)\r\n\r\n################# Harmonic Frequency Models #################\r\n\r\ncombined_HMF <- comparative.data(\r\n  Squirrel_Tree, Harm_max, names.col = ""Scientific.Name"", \r\n  vcv = TRUE, na.omit = FALSE)\r\n\r\n#Null\r\nmodel0_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ Ana_Max.Hz, data = combined_HMF, lambda = ""ML"")\r\n#summary(model0_HMF)\r\nAIC(model0_HMF)\r\n\r\n#Body mass\r\nmodel1_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ log(Body.Size...Females..g.) + Ana_Max.Hz, data = combined_HMF, lambda = ""ML"")\r\nsummary(model1_HMF)\r\nAIC(model1_HMF)\r\n\r\n#Temporality\r\nmodel2_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ Temporality + Ana_Max.Hz, data = combined_HMF, lambda = ""ML"")\r\n#summary(model2_HMF)\r\nAIC(model2_HMF)\r\n\r\n#Sociality\r\nmodel3_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ Soc + Ana_Max.Hz, data = combined_HMF, lambda = ""ML"")\r\n#summary(model3_HMF)\r\nAIC(model3_HMF)\r\n\r\n#Life History\r\nmodel4_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ Openness + Ana_Max.Hz, data = combined_HMF, lambda = ""ML"")\r\n#summary(model4_HMF)\r\nAIC(model4_HMF)\r\n\r\n#Body mass, temporality\r\nmodel5_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ log(Body.Size...Females..g.) + Temporality + Ana_Max.Hz, \r\n                   data = combined_HMF, lambda = ""ML"")\r\nsummary(model5_HMF)\r\nAIC(model5_HMF)\r\n\r\n#Body mass, sociality\r\nmodel6_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ log(Body.Size...Females..g.) + Soc + Ana_Max.Hz, \r\n                   data = combined_HMF, lambda = ""ML"")\r\nsummary(model6_HMF)\r\nAIC(model6_HMF)\r\n\r\n#Body mass, life.history\r\nmodel7_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ log(Body.Size...Females..g.) + Openness + Ana_Max.Hz, \r\n                   data = combined_HMF, lambda = ""ML"")\r\nsummary(model7_HMF)\r\nAIC(model7_HMF)\r\n\r\n#Body mass, temporality, sociality\r\nmodel8_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ log(Body.Size...Females..g.) +Temporality + Soc + Ana_Max.Hz, \r\n                   data = combined_HMF, lambda = ""ML"")\r\n#summary(model8_HMF)\r\nAIC(model8_HMF)\r\n\r\n#Body mass, temporality, life history\r\nmodel9_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ log(Body.Size...Females..g.) + Temporality + Openness + Ana_Max.Hz, \r\n                   data = combined_HMF, lambda = ""ML"")\r\nsummary(model9_HMF)\r\nAIC(model9_HMF)\r\n\r\n#Body mass, sociality, life history\r\nmodel10_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ log(Body.Size...Females..g.) + Soc + Openness + Ana_Max.Hz, \r\n                    data = combined_HMF, lambda = ""ML"")\r\nsummary(model10_HMF)\r\nAIC(model10_HMF)\r\n\r\n#Body mass, temporality, sociality, life-history\r\nmodel11_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ log(Body.Size...Females..g.) + Openness + Temporality + Soc + Ana_Max.Hz, \r\n                    data = combined_HMF, lambda = ""ML"")\r\nsummary(model11_HMF)\r\nAIC(model11_HMF)\r\nanova(model11_HMF)\r\n\r\n#Temporality, sociality\r\nmodel12_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ Temporality + Soc + Ana_Max.Hz, \r\n                    data = combined_HMF, lambda = ""ML"")\r\n#summary(model12_HMF)\r\nAIC(model12_HMF)\r\n\r\n#Temporality, life history\r\nmodel13_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ Temporality + Openness + Ana_Max.Hz, \r\n                    data = combined_HMF, lambda = ""ML"")\r\n#summary(model13_HMF)\r\nAIC(model13_HMF)\r\n\r\n#Sociality, life history\r\nmodel14_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ Soc + Openness + Ana_Max.Hz, data = combined_HMF, lambda = ""ML"")\r\n#summary(model14_HMF)\r\nAIC(model14_HMF)\r\n\r\n#Temporality, sociality, life-history\r\nmodel15_HMF <- pgls(log(Mean_of_Highest_Harmonic_Visible) ~ Temporality + Soc + Openness, \r\n                    data = combined_HMF, lambda = ""ML"")\r\n#summary(model15_HMF)\r\nAIC(model15_HMF)\r\n################# Bandwidth Models #################\r\n\r\ncombined_BW <- comparative.data(\r\n  Squirrel_Tree, Freq_BW, names.col = ""Scientific.Name"")\r\n\r\n#Null\r\nmodel0_BW <- pgls(log(BW) ~ 1, \r\n                  data = combined_BW, lambda = ""ML"")\r\nsummary(model0_BW)\r\nAIC(model0_BW)\r\n#Body mass\r\nmodel1_BW <- pgls(log(BW) ~ log(Body.Size...Females..g.), \r\n                  data = combined_BW, lambda = ""ML"")\r\nsummary(model1_BW)\r\nAIC(model1_BW)\r\n#Temporality\r\nmodel2_BW <- pgls(log(BW) ~ Temporality, \r\n                  data = combined_BW, lambda = ""ML"")\r\nsummary(model2_BW)\r\nAIC(model2_BW)\r\n#Sociality\r\nmodel3_BW <- pgls(log(BW) ~ Soc, \r\n                  data = combined_BW, lambda = ""ML"")\r\nsummary(model3_BW)\r\nAIC(model3_BW)\r\n#Life History\r\nmodel4_BW <- pgls(log(BW) ~ Life.History, \r\n                  data = combined_BW, lambda = ""ML"")\r\nsummary(model4_BW)\r\nAIC(model4_BW)\r\n#Body mass, temporality\r\nmodel5_BW <- pgls(log(BW) ~ log(Body.Size...Females..g.) + \r\n                    Temporality, data = combined_BW, lambda = ""ML"")\r\nsummary(model5_BW)\r\nAIC(model5_BW)\r\n#Body mass, sociality\r\nmodel6_BW <- pgls(log(BW) ~ log(Body.Size...Females..g.) + Soc,\r\n                  data = combined_BW, lambda = ""ML"")\r\nsummary(model6_BW)\r\nAIC(model6_BW)\r\n#Body mass, life.history\r\nmodel7_BW <- pgls(log(BW) ~ log(Body.Size...Females..g.) + \r\n                    Life.History, data = combined_BW, lambda = ""ML"")\r\nsummary(model7_BW)\r\nAIC(model7_BW)\r\n#Body mass, temporality, sociality\r\nmodel8_BW <- pgls(log(BW) ~ log(Body.Size...Females..g.) + \r\n                    Temporality + Soc, data = combined_BW, lambda = ""ML"")\r\nsummary(model8_BW)\r\nAIC(model8_BW)\r\n#Body mass, temporality, life history\r\nmodel9_BW <- pgls(log(BW) ~ log(Body.Size...Females..g.) + \r\n                    Temporality + Life.History, data = combined_BW, lambda = ""ML"")\r\nsummary(model9_BW)\r\nAIC(model9_BW)\r\n#Body mass, sociality, life history\r\nmodel10_BW <- pgls(log(BW) ~ log(Body.Size...Females..g.) + Soc + \r\n                     Life.History, data = combined_BW, lambda = ""ML"")\r\nsummary(model10_BW)\r\nAIC(model10_BW)\r\n#Body mass, temporality, sociality, life-history\r\nmodel11_BW <- pgls(log(BW) ~ log(Body.Size...Females..g.) + \r\n                     Temporality + Soc + Life.History + Year, data = combined_BW, lambda = ""ML"")\r\nsummary(model11_BW)\r\nAIC(model11_BW)\r\n#Temporality, sociality\r\nmodel12_BW <- pgls(log(BW) ~ Temporality + Soc, \r\n                   data = combined_BW, lambda = ""ML"")\r\nsummary(model12_BW)\r\nAIC(model12_BW)\r\n#Temporality, life history\r\nmodel13_BW <- pgls(log(BW) ~ Temporality + Life.History, \r\n                   data = combined_BW, lambda = ""ML"")\r\nsummary(model13_BW)\r\nAIC(model13_BW)\r\n#Sociality, life history\r\nmodel14_BW <- pgls(log(BW) ~ Soc + Life.History, \r\n                   data = combined_BW, lambda = ""ML"")\r\nsummary(model14_BW)\r\nAIC(model14_BW)\r\n#Temporality, sociality, life-history\r\nmodel15_BW <- pgls(log(BW) ~ Temporality + Soc + Life.History, \r\n                   data = combined_BW, lambda = ""ML"")\r\nsummary(model15_BW)\r\nAIC(model15_BW)\r\n\r\n\r\n################# Let\'s Plot These Models! #################\r\npar(mfrow = c(2,2))\r\n#Minimum\r\nplot(model1_LFQ)\r\n#Maximum\r\nplot(model8_HFQ)\r\nplot(model2_HFQ)\r\nplot(model5_HFQ)\r\nplot(model5_HFQ_t)\r\n#Dominant\r\nplot(model8_DFQ)\r\nplot(model2_DFQ)\r\n#Bandwidth\r\nplot(model0_BW)\r\nplot(model2_BW)\r\n#Full models\r\nplot(model11_LFQ)\r\nplot(model11_HFQ)\r\nplot(model11_DFQ)\r\nplot(model11_BW)\r\npar(mfrow = c(1,1))\r\n\r\n################# Alarm Calls Only #################\r\nAlarm_Types <- c(""Aerial Predator"", ""Alarm"", ""Alarm; Mating; Locomotion"", ""Snake Mobbing"",\r\n                 ""Terrestrial Predator"")\r\nAlarm_Types <- c(""Aerial Predator"", ""Alarm"", ""Terrestrial Predator"")\r\nAlarm <- filter(Squirrels, Call.Type %in% Alarm_Types)\r\n\r\nAlarm$Scientific.Name <- factor(Alarm$Scientific.Name)\r\nAlarm_names <- unique(Alarm$Scientific.Name)\r\nname.check(Squirrel_Tree, Alarm_names)\r\n\r\ncombined_LFQ <- comparative.data(\r\n  Squirrel_Tree, Freq_min, names.col = ""Scientific.Name"")\r\n\r\nAlarm$Sex[Alarm$Sex == ""Unknown""] <- NA\r\nAlarm$Sex[Alarm$Sex == ""Both""] <- NA\r\nAlarm$Sex <- factor(Alarm$Sex)\r\n\r\nAlarm$Age[Alarm$Age == ""All""] <- NA\r\nAlarm$Age[Alarm$Age == ""Pup""] <- NA\r\nAlarm$Age[Alarm$Age == ""Unknown""] <- NA\r\nAlarm$Age[Alarm$Age == ""Both""] <- NA\r\nAlarm$Age <- factor(Alarm$Age)\r\n\r\nlibrary(lme4)\r\nalarm1 <- lmer(log(Max.Freq..kHz.) ~ Sex + Age + Year + \r\n                 log(Body.Size...Females..g. + 1) + (1|Scientific.Name), data = Alarm)\r\nalarm2 <- lmer(log(Dom.Frequency..mean.) ~ Sex + Age + Year + \r\n                 log(Body.Size...Females..g. + 1) + (1|Scientific.Name), data = Alarm)\r\nlibrary(car)\r\nAnova(alarm1)\r\n\r\n###### YOU ARE RIGHT HERE#####\r\n\r\nplot(log(Max.Freq..kHz.) ~ log(Body.Size...Females..g.+ 1), data = Alarm)\r\nplot(log(Dom.Frequency..mean.) ~ Year, data = Alarm)\r\nplot(log(Max.Freq..kHz.) ~ Sex, data = Alarm)\r\n\r\nggplot(Alarm, aes(x = Year, y = value, color = variable)) +\r\n  geom_point(aes(y = log(Max.Freq..kHz.), col = ""Maximum"")) +\r\n  geom_smooth(aes(y = log(Max.Freq..kHz.), col = ""Maximum""),\r\n              method = ""lm"", se = F) +\r\n  geom_point(aes(y = log(Dom.Frequency..mean.), col = ""Dominant"")) +\r\n  geom_smooth(aes(y = log(Dom.Frequency..mean.), col = ""Dominant""),\r\n              method = ""lm"", se = F) +\r\n  geom_point(aes(y = log(Min.Freq..kHz. + 1), col = ""Minimum"")) +\r\n  geom_smooth(aes(y = log(Min.Freq..kHz. + 1), col = ""Minimum""),\r\n              method = ""lm"", se = F) +\r\n  labs(color = ""Frequency Type"", x = ""Year"", y = ""log(Frequency (kHz))"") +\r\n  theme_classic(base_size = 9) +\r\n  theme(legend.position = c(0.90, 0.85),\r\n        legend.background = element_rect(linetype = ""solid"", colour = ""lightblue"")) +\r\n  scale_color_brewer(palette = ""Dark2"")\r\n']","Vocalizations of the squirrel family The dataset Squirrel_Calls is a collection of vocal records (defined as primary literature that numerically describes the vocalization of at least 1 squirrel species) where each row corresponds to a single call type of one species. The details of the row include a summary of the literature metadata, categorical descriptions of the call and the caller as well as numerical values of the call frequencies. The dataset Squirrel_Ecological_Traits is a corresponding set of ecological traits for all the species listed in the Squirrel_Calls dataset. The traits listed (mass, time partitioning, gliding capabilities, habitat, and sociality) reflect hypotheses and predictions explored in the associated article. At the end of this document, there is a complete list of the literature references used to assemble these datasets. Squirrel_Script is the R script used to produce the statistics and models used in the corresponding paper. Squirrel_Tree is a nexus file compiling the data of 1000 trees downloaded from VertLife.org which were subsetted from their published mammalian supertree. The nexus file was used in the R script.",2
Data from: The contribution of road-based citizen science efforts to the conservation of pond-breeding amphibians,"1. Road-side amphibian citizen science programs bring together volunteers focused on collecting scientific data while working to mitigate population declines by directly reducing road mortality of pond-breeding amphibians. Despite the international popularity of these movement-based road-side conservation efforts (i.e., 'big nights', 'bucket brigades' and 'toad patrols'), direct benefits to conservation have rarely been quantified or evaluated. 2. As a case study, we used a population simulation approach to evaluate how volunteer intensity, frequency and distribution influence three conservation outcomes (minimum population size, population growth rate, and years to extinction) of the spotted salamander (Ambystoma maculatum)  a common, focal pond-breeding amphibian of citizen science and conservation programs in the United States. 3. Sensitivity analysis supported the expectation that populations were primarily recruitment-driven. Thus, conservation outcomes were highest when volunteers focused on out-migration of metamorphs as opposed to in-migration of adults contrary to the typical timing of such volunteer events. 4. Almost every volunteer strategy resulted in increased conservation outcomes compared to a no-volunteer strategy. Specifically, volunteer frequency during metamorph migration increased outcomes more than the same increases in volunteer effort during adult migration. Small population sizes resulted in a negligible effect of volunteer intensity. Volunteers during the first adult in-migration had a relatively small effect compared to most other strategies. 5. Synthesis and applications. Although citizen science focused conservation actions could directly benefit declining populations, other conservation measures are additionally needed to halt or reverse local amphibian declines. This study demonstrates a need to evaluate the effectiveness of focusing citizen science mitigation efforts on the metamorph stage, as opposed the adult stage, which may be challenging, compared to other management actions such as road-crossing infrastructure. Current amphibian citizen science programs will be challenged to balance implementing evidence-based conservation measures on the most limiting life stage while retaining the social and community benefits to volunteers.","[""# bprob.r: function that uses breeding probability to split population into breeders and nonbreeders at start of breeding season\n# input: population size, breeding probability (1 - temporary emigration; estimated from multistate mark-recapture model)\n# processing: determine whether each adult breeds or doesn't breed\n# output: return vector with 1/0 for breeding/nonbreeding\nbprob <- function(pop, prob){\n out <- rbinom(pop, 1, prob)\n return(out)\n# could modify function to deal with Markovian temporary emigration\n}\n\n\n# not run:\n# adults <- 100\n# bprob(adults,.5) -> breeders\n\n"", 'fecund <- function(pop, fec){\n  # function that creates number of individuals produced by adults with Poisson distribution\n  # could focus on overall number of metamorphs relative to number of adults, skipping larval dynamics\n  return(rpois(pop, fec))\n}\n', 'migrate <- function(num, night_prob, rmort, num_guards, guard_limit){   #,fec, bmort){\n#   input: number of crossing adults susceptible to road mortality, proportion of adults migrating each night, fecunedity, road mortality, number xing guards each night,\n#       maximum number of animals saved per guard per night # drop breeding mortality\n#   processing:\n#       mortality of adults crossing road per night\n#       #drop: fecundity of surviving adults\n#       # drop: mortality of adults that survive inbound migration\n#   output:\n#       adults that make it into the pond to breed on each night\n#       # drop: number of adults that will make outbound migration\n  #print(flag) debugging parameter, set unique integer to flag different function calls\n  if(length(num)>1) print(""Check input - pop vector rather than pop total"")\n\n  # create object to store migrants each night, should fix to get whole numbers\n  num * night_prob -> nxing\n  round(nxing) -> nxing\n\n  # calculate potential mortality\n  pmort <- rep(NA, length(nxing))\n  for(i in 1:length(nxing)){\n     pmort[i] <- sum(rbinom(nxing[i], 1, rmort))\n  }\n  # number of salamanders saved from cars\n  num_guards*guard_limit -> numsave\n  # \'save\' salamanders\n  pmort - numsave -> mort\n  # set mortality to zero if guards can save more salamanders than those that could be killed\n  mort[mort<0] <- 0\n  # subtract killed individuals from individuals that crossed road\n  nxing - mort -> numcrossed\n  return(numcrossed)\n\n}\n\n\n', ""# scenario simulations - first run\n#out <- array(dim=c(50,2,100), data=NA)\n\n#system.time(for(i in 1:100){\n#out[,,i] <- popsim(nyears=50, init_pop=c(100,100), dphi_sa = .9986, pr_b= .4230, nb_dphi = .9992, a_in_night_prob = c(0.333,0.333,0.334), \n#       a_rmort = 0.0407, a_in_num_guards = c(0,0,0), guard_limit = 10, a_p_dphi = 0.9994, p_days = 14, fec = 165, lmort = 0.0523, \n#       a_out_night_prob = c(0.333,0.333,0.334), rmort2 = 0.0407, a_out_nguard = c(0,0,0), met_night_prob = c(0.333,0.333,0.334), \n#       rmort_met = 0.0407, ng_met = c(0,0,0))\n#})  \n\n# load data\nsource('./R/popsim.r')\nsource('./R/bprob.r')\nsource('./R/migrate.r')\nsource('./R/fecund.r')\nsource('./R/surv.r')\n# fecund, migrate\nload('./data/scenarios.Rdata')\n\n# create object for simulation output\nmy_fun <- function(){\n  list(d=array(dim=c(50,2,500), data=NA))\n}\nreplicate(361, my_fun(), simplify=TRUE)->out\n\n# list option\n# loop through first 100 scenarios, going to do 500 iterations\nsystem.time(for(i in 1:361){\n  \n   # pull in road xing guard data from scenarios object\n  gds_ai <- c(scenarios$night1_adult_in_migtation[i],scenarios$night2_adult_inout_migtation[i],scenarios$night3_adult_inout_migtation[i])\n  gds_ao <- c(scenarios$night2_adult_inout_migtation[i],scenarios$night3_adult_inout_migtation[i],scenarios$night4_adult_out_migtation[i])\n  gd_m <- c(scenarios$night5_meta_out_migration[i], scenarios$night6_meta_out_migration[i], scenarios$night7_meta_out_migration[i])\n  # loop through for 500 iterations\n    for(j in 1:500){\n    out[[i]][,,j] <- popsim(nyears=50, init_pop=c(100,100), dphi_sa = .9986, pr_b= .4230, nb_dphi = .9992, a_in_night_prob = c(0.333,0.333,0.334), \n            a_rmort = 0.0407, a_in_num_guards = gds_ai, guard_limit = 10, a_p_dphi = 0.9994, p_days = 14, fec = 165, lmort = 0.0523, \n            a_out_night_prob = c(0.333,0.333,0.334), rmort2 = 0.0407, a_out_nguard = gds_ao, met_night_prob = c(0.333,0.333,0.334), \n            rmort_met = 0.0407, ng_met = gd_m)\n    \n      } #j\n    if(i%%1==0) {save(out, file='./data/outdata.Rdata')}\n  }) #i\n\n"", 'surv <- function(pop, phi, days){\n  # function to calculate fate of POP individuals with daily survival PHI after DAYS\n  # input: pop, phi, days\n  # processing\n  pop <- rep(1,pop)\n  #tmp <- rbinom(pop, 1, phi)\n  pop\n  for(i in 1:days){\n    pop <- pop*(rbinom(pop,1,phi))\n    #print(i)\n    #print(pop)\n  }\n  return(pop)\n}\n\n']","Data from: The contribution of road-based citizen science efforts to the conservation of pond-breeding amphibians 1. Road-side amphibian citizen science programs bring together volunteers focused on collecting scientific data while working to mitigate population declines by directly reducing road mortality of pond-breeding amphibians. Despite the international popularity of these movement-based road-side conservation efforts (i.e., 'big nights', 'bucket brigades' and 'toad patrols'), direct benefits to conservation have rarely been quantified or evaluated. 2. As a case study, we used a population simulation approach to evaluate how volunteer intensity, frequency and distribution influence three conservation outcomes (minimum population size, population growth rate, and years to extinction) of the spotted salamander (Ambystoma maculatum)  a common, focal pond-breeding amphibian of citizen science and conservation programs in the United States. 3. Sensitivity analysis supported the expectation that populations were primarily recruitment-driven. Thus, conservation outcomes were highest when volunteers focused on out-migration of metamorphs as opposed to in-migration of adults contrary to the typical timing of such volunteer events. 4. Almost every volunteer strategy resulted in increased conservation outcomes compared to a no-volunteer strategy. Specifically, volunteer frequency during metamorph migration increased outcomes more than the same increases in volunteer effort during adult migration. Small population sizes resulted in a negligible effect of volunteer intensity. Volunteers during the first adult in-migration had a relatively small effect compared to most other strategies. 5. Synthesis and applications. Although citizen science focused conservation actions could directly benefit declining populations, other conservation measures are additionally needed to halt or reverse local amphibian declines. This study demonstrates a need to evaluate the effectiveness of focusing citizen science mitigation efforts on the metamorph stage, as opposed the adult stage, which may be challenging, compared to other management actions such as road-crossing infrastructure. Current amphibian citizen science programs will be challenged to balance implementing evidence-based conservation measures on the most limiting life stage while retaining the social and community benefits to volunteers.",2
"Materials for the  ""Haciendo Ciencia Abierta: Diseo De Experimentos, Software y Uso de Datos en la Investigacin"" workshop","The Haciendo Ciencia Abierta: Diseo De Experimentos, Software y Uso de Datos en la Investigacin (``Doing Open Science: Design of experiments, software and use of data in research'') workshop took place on May 4, 5, and 6th, 2022, from 5:30 to 8:30 pm virtually at the University of Los Andes and was taught by Ignacio Sarmiento-BarbieriThis was possible thanks to the generous Catalyst Grant from the Berkeley Initiative for Transparency in the Social Sciences (BITSS), managed by the Center for Effective Global Action (CEGA).This supplements and archives the material available at https://ignaciomsarmiento.github.io/teaching/HCA","['##########################################################\n# Analisis Zenodo\n# Taller Haciendo Ciencia Abierta\n# autor: Ignacio Sarmiento-Barbieri\n# Basado en Lars Vihuber Tutorial\n##########################################################\n\n\n\n\nlibrary(rjson)\nlibrary(tidyr)\nlibrary(dplyr)\n  \n\n\nzenodo.prefix <- ""10.5072/zenodo""\n\nzenodo.id <- ""1058800""\n\n\nzenodo.api = ""https://sandbox.zenodo.org/api/records/""\n\npaste0(zenodo.api,zenodo.id)\n\n\ndataloc<-\'~/Desktop/PruebaHCA/\'\n  \ndownload.file(paste0(zenodo.api,zenodo.id),destfile=file.path(dataloc,""metadata.json""))\n\nlatest <- fromJSON(file=file.path(dataloc,""metadata.json""))\nlatest\n\nfile.list <- as.data.frame(latest$files) %>% select(starts_with(""self"")) %>% gather()\nfile.list\n\n\n  \nworkpath<-\'~/Desktop/PruebaHCA/\'\n\nfor ( value in file.list$value ) {\n  print(value)\n  if ( grepl("".csv"",value ) ) {\n    print(""Downloading..."")\n    file.name <- basename(value)\n    download.file(value,destfile=file.path(workpath,basename(value)))\n  } else {\n    print(""Skipping."")\n  }\n}\n\n\n?read.csv  \n\nbrowser_survey <- read.csv(file.path(workpath,""encuesta.csv""))\n\n\nlatest.doi <- latest$doi\nlatest.doi\n\n\n\ntabla<-browser_survey %>% \n  group_by(Navegador) %>%\n  summarize(Pestanas,.groups=""drop"") \n\ntabla\n\n', '##########################################################\n# Diseo experimento Baru\n# Taller Haciendo Ciencia Abierta\n# autor: Ignacio Sarmiento-Barbieri\n##########################################################\n\n# Algunos datos interesantes\n#Poblacin de cartagena en2020 \npob<- 1028736\n#De las cuales el 16% son nias entre 10 y 19 aos, para ser conservador voy a tomar que es el 15% hasta los 18\nporc<-.15\n#Es decir hay casi 154.311 nias en Cartagena\nceiling(pob*porc)\n#El ministerio de educacion busca que haya 180 dias de clases\n\n\nlibrary(tidyverse)\nlibrary(DeclareDesign)\nset.seed(1010101)\n\nefect_mc<- -0.003\n\nmodelo<-declare_model(\n  nina = add_level(\n    N = 1000\n  ),\n  dias_clase = add_level(\n    N = 180,\n    U=rnorm(N,sd=0.001),\n    periodo= rbinom(N, size = 1, prob = 0.08),\n    Y_Z_0 = rbinom(N, size = 1, prob = 0.85-0.025*periodo+U),\n    Y_Z_1 = rbinom(N, size = 1, prob = 0.85-(0.025+efect_mc)*periodo+U)\n  )\n)\n\nm<-modelo()\n\n\n\n\ninquiry<-declare_inquiry(PATE = mean(Y_Z_1 - Y_Z_0))\n\n#Experimento\n\n#sampling\nsampling<-declare_sampling(S = complete_rs(N, n = 50))\n\n#assignment\n\nassignment<-declare_assignment(Z = complete_ra(N, prob = 0.5))\n\n# measurement\nmeasurement <- \n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) \n\n#estimator\nestimator <- \n  declare_estimator(\n    Y ~ Z, model = difference_in_means, estimand = ""PATE""\n  )\n\ndesign <- \n  modelo + inquiry + sampling + assignment + measurement + estimator\n\n\nsimulation_df <- simulate_design(design)\n\n\nstudy_diagnosands <- declare_diagnosands(\n  bias = mean(estimate - estimand),\n  power = mean(p.value <= 0.05)\n)\n\ndiagnose_design(simulation_df, diagnosands = study_diagnosands)\n\n\n', '##########################################################\n# Diseo experimento Baru\n# Taller Haciendo Ciencia Abierta\n# autor: Ignacio Sarmiento-Barbieri\n##########################################################\n\n\nlibrary(tidyverse)\nlibrary(DeclareDesign)\n\n# En stata los comentarios son con *, */ /*\n# en python\nimport pandas as pd\n\n# help paquete\n?fabricatr\n\n# help de alguna funcion\n?fabricate\n\nprob_periodo<-0.08\n\n#fabriquemos datos (db)\ndb<-fabricate(\n  nina=add_level(\n    N=1000 #asumo 1000 nias que hay en Bar\n  ),\n  dias_clase=add_level(\n    N=180, # 180 dias de clase\n    U=runif(N,min= -0.01,max=0.01),\n    periodo=rbinom(N,size=1,prob=0.08),\n    asistio=rbinom(N,size=1,prob=0.85-0.05*periodo+U)\n  )\n)\n\nView(db) #para ver los datos\n180*mean(db$periodo)\nmean(db$asistio)\n\ndb_resumen<- db %>% \n              group_by(nina) %>% \n              summarize(mean_periodo=mean(periodo), \n                      mean_asistencia=mean(asistio),\n                      n=n())\n\ndb_resumen\n\nproducto<-0.1 #el producto aumenta la probabilidad de ir (me edue la de faltar)\n#fabriquemos datos con el mundo de resultados potenciales (db)\ndb_pot<-fabricate(\n  nina=add_level(\n    N=1000 #asumo 1000 nias que hay en Bar\n  ),\n  dias_clase=add_level(\n    N=180, # 180 dias de clase\n    #U=runif(N,min= -0.01,max=0.01),\n    U=rnorm(N,mean = 0,sd = 0.01),\n    periodo=rbinom(N,size=1,prob=0.08),\n    Y_Z_0=rbinom(N,size=1,prob=0.85-0.05*periodo+U),\n    Y_Z_1=rbinom(N,size=1,prob=0.85-(0.05-producto)*periodo+U)\n  )\n) %>% group_by(nina) %>% summarize(Y_Z_0=sum(Y_Z_0),\n                                   Y_Z_1=sum(Y_Z_1))\n\n\n\n#Entramos en el marco MIDA\n#comenzamos declarando el modelo (M)\nmodelo<-declare_model(db_pot)\n\n#Inquiry es el ATE\ninquiry<-declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))\n\n#Experimento\n\n#sampling\nsampling<-declare_sampling(S = complete_rs(N, n = 200))\n\n#como vamos?\nm<-draw_data(modelo+inquiry+sampling)\n\n#assignment\nassignment<-declare_assignment(Z = complete_ra(N, prob = 0.5))\n\n#como vamos?\nm<-draw_data(modelo+inquiry+sampling+assignment)\n\n# measurement Yobs=Z*Y_Z_1 + (1-Z) *Y_Z_0\nmeasurement <- \n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) \n\n#estimator Answer strategy\nestimator <- \n  declare_estimator(\n    Y ~ Z, model = difference_in_means, inquiry = ""ATE""\n  )\n\n#design completo\ndesign <- \n  modelo + inquiry + sampling + assignment + measurement + estimator\n\n#simulo\nsimulation_df <- simulate_design(design)\n\n#Defino los diagnosticos\nstudy_diagnosands <- declare_diagnosands(\n  bias = mean(estimate - estimand),\n  power = mean(p.value <= 0.05)\n)\n\n#corro el diagnostico\ndiag_output<-diagnose_design(simulation_df, diagnosands = study_diagnosands)\n\n#ver resultado de diagnostico\nView(diag_output$diagnosands_df)\n\n\n\n', '##########################################################\n# Estimar la edad promedio de los ciudadanos de un peque�o pueblo de Italia\n# Taller Haciendo Ciencia Abierta\n# autor: Ignacio Sarmiento-Barbieri\n# Basado en https://book.declaredesign.org/what-is-a-research-design.html\n##########################################################\n\n#Carga de Paquetes\nrequire(DeclareDesign)\n\n\nset.seed(10101) \n\n\n\nn_samp<-3\n\nmod_edad <-\n  declare_model(N = 100, age = sample(0:80, size = N, replace = TRUE)) +\n  declare_inquiry(mean_age = mean(age)) +\n  declare_sampling(S = complete_rs(N = N, n = n_samp)) +\n  declare_estimator(age ~ 1, model = lm) ', '##########################################################\n# Sorpresa de Octubre\n# Taller Haciendo Ciencia Abierta\n# autor: Ignacio Sarmiento-Barbieri\n# Basado en https://book.declaredesign.org/what-is-a-research-design.html\n##########################################################\n\n\n#Carga de Paquetes\nrequire(""tidyverse"")\nrequire(""DeclareDesign"")\nrequire(""modelr"") # para predicciones\n\nset.seed(10101) #para garantizar reproducibilidad\n\n# Carrera estable ---------------------------------------------------------\n\n#Preferencia Candidato A\nprefs_A <- 0.51\n#Individuos\nN <- 1000\n#Carrera estable\ntendencia<-0\n\n# Declaramos el modelo\nmodelo_carrera_estable <-declare_model(\n    N = N,\n    Y_time_1 = rbinom(N, size = 1, prob = prefs_A + 0 * tendencia),\n    Y_time_2 = rbinom(N, size = 1, prob = prefs_A + 1 * tendencia),\n    Y_time_3 = rbinom(N, size = 1, prob = prefs_A + 2 * tendencia)\n  ) \n\n\nm<-modelo_carrera_estable()\n\nmean(m$Y_time_3)\n\n#Declaramos la pregunta que le hacemos al modelo\npregunta_carrera_estable <-declare_inquiry(A_voteshare = rnorm(n = 1, mean = prefs_A, sd = 0.01))\n\n#MI\ncarrera_estable<- modelo_carrera_estable + pregunta_carrera_estable\n\n# Declaramos la estrategia de datos (una sola encuesta al momento 1)\nuna_encuesta_medicion <- declare_measurement(Y_obs = Y_time_1) \n\n\n#Declaramos la respuesta\nuna_encuesta_respuesta<-  declare_estimator(Y_obs ~ 1, model = lm)\n\n#DA una encuesta\nuna_encuesta <- una_encuesta_medicion + una_encuesta_respuesta\n\n\n# Combinamos para tener todo el diseo: MIDA\ndiseno_carrera_estable <- carrera_estable +una_encuesta\n\n\n#definimos como vamos a diagnosticar\ndiagnosands <- declare_diagnosands(\n    correct_call_rate = mean((estimate > 0.5) == (estimand > 0.5))\n  )\n\n#corremos la simulacin de diagnostico 500 veces (el default)\ndiagnostico_una_encuesta <-\n  diagnose_design(design = diseno_carrera_estable,\n                  diagnosands = diagnosands)\n\n\ndiagnostico_una_encuesta\n\n\n# Sorpresa de octubre -----------------------------------------------------\n\n\nprefs_A <- 0.51\ntendencia <- -0.01 #  tendencia ""sorprendente"" que se aleja del candidato  A\nN <- 1000\n\n# Declaramos el nuevo modelo y el diagnostico\nsorpresa_de_octubre <- \n  declare_model(\n    N = N,\n    Y_time_1 = rbinom(N, size = 1, prob = prefs_A + 0 * tendencia),\n    Y_time_2 = rbinom(N, size = 1, prob = prefs_A + 1 * tendencia),\n    Y_time_3 = rbinom(N, size = 1, prob = prefs_A + 2 * tendencia)\n  ) +\n  declare_inquiry(\n    A_voteshare = \n      rnorm(n = 1, mean = prefs_A + 3 * tendencia, sd = 0.01))\n\n\n# Declaramos los nuevos datos y la estrategia\ntres_encuestas <-\n  declare_assignment(time = complete_ra(N, conditions = 1:3)) +\n  declare_measurement(Y_obs = reveal_outcomes(Y ~ time)) +\n  declare_estimator(\n    Y_obs ~ time,\n    model = lm_robust,\n    model_summary = ~add_predictions(model = ., \n                                     data = data.frame(time = 4), \n                                     var = ""estimate"")\n  ) \n\n# Combinamos para tener todo el diseo\ndiseno_sorpresa <- sorpresa_de_octubre + tres_encuestas\n\ndiagnostico_tres_encuestas <-\n  diagnose_design(design = diseno_sorpresa,\n                  diagnosands = diagnosands)\n\n\ndiagnostico_tres_encuestas\n\n# Comparacion -------------------------------------------------------------\n\n\ndiagnostico_multiple <- \n  diagnose_design(\n    design_1 = carrera_estable + una_encuesta,\n    design_2 = carrera_estable + tres_encuestas,\n    design_3 = sorpresa_de_octubre + una_encuesta,\n    design_4 = sorpresa_de_octubre + tres_encuestas,\n    diagnosands = diagnosands\n  )\n\ndiagnostico_multiple\n']","Materials for the  ""Haciendo Ciencia Abierta: Diseo De Experimentos, Software y Uso de Datos en la Investigacin"" workshop The Haciendo Ciencia Abierta: Diseo De Experimentos, Software y Uso de Datos en la Investigacin (``Doing Open Science: Design of experiments, software and use of data in research'') workshop took place on May 4, 5, and 6th, 2022, from 5:30 to 8:30 pm virtually at the University of Los Andes and was taught by Ignacio Sarmiento-BarbieriThis was possible thanks to the generous Catalyst Grant from the Berkeley Initiative for Transparency in the Social Sciences (BITSS), managed by the Center for Effective Global Action (CEGA).This supplements and archives the material available at https://ignaciomsarmiento.github.io/teaching/HCA",2
Electronic Supplementary Material for: A comprehensive and cost-effective approach for investigating passive dispersal in minute invertebrates with case studies of phytophagous eriophyid mites,"Filename: ESM_1.pdfPhotos and technical drawings of the 'wind-transience' tunnel, 'vector-transience' tunnel and 'departure' tunnel, and the scheme of Lego Mindstorms NXT 2.0 - engine program for a robotic mammalian vector simulation. Filename: ESM_2.R and ESM_3.RReproducible examples on how to estimate dispersal parameters using the methodology proposed in this article. Filename: ESM_4.pdfEstimated costs of devices construction in USD. Filename: ESM_5.mp4Video 3D model of the wind-transience tunnel. Filename: ESM_6.mp4Video 3D model of the vector-transience tunnel. Filename: ESM_7.mp4Video 3D model of the departure tunnel. Filename: ESM_8.mp4Video of eriophyid mite behaviors observed within the departure tunnel.","['\r\n# Online Resource 2. \r\n\r\n# Reproducible example on how to estimate dispersal parameters using the methodology proposed in:\r\n# Kuczynski L., Radwanska A., Karpicka-Ignatowska K., Laska A., Lewandowski M., Rector B.G., Majer A., Raubic J., Skoracka A. \r\n# ""A comprehensive and cost-effective approach for investigating passive dispersal in minute invertebrates with case studies of phytophagous eriophyid mites""\r\n\r\n# Code by: Lechosaw Kuczyski, lechu@amu.edu.pl\r\n\r\n# Case studies 1 and 3: testing wheat curl mite (WCM) dispersal via wind and via vector towards wheat (Triticum aestivum)\r\n\r\n\r\n# Wind-transience data ---------------------------------------------------\r\n\r\n# Population size on the source patch:\r\nN <- c(1500, 1100, 1200, 3100, 2700, 1000, 2000, 1200, 1500, 1100)\r\n\r\n# No. of dispersers, i.e. individuals that leave the source patch:\r\nD <- c(300, 150, 500, 1900, 1100, 300, 900, 300, 350, 200)\r\n\r\n# No. of colonisers, i.e. population size on the target patch after several generations:\r\nC <- c(783, 1768, 2360, 433, 783, 1195, 6321, 2992, 1284, 708)\r\n\r\njags.data <- list(N = N, D = D, C = C, n = length(N))\r\njags.data\r\n\r\n\r\n# Vector-transience data -------------------------------------------------\r\n\r\n# Population size on the source patch:\r\nN <- c(350, 250, 400, 1200, 250, 350, 400, 400, 400)\r\n\r\n# No. of dispersers, i.e. individuals that leave the source patch:\r\nD <- c(200, 130, 330, 1010, 10, 0, 60, 270, 290)\r\n\r\n# No. of colonisers, i.e. population size on the target patch after several generations:\r\nC <- c(57, 83, 276, 177, 0, 0, 0, 0, 0)\r\n\r\njags.data <- list(N = N, D = D, C = C, n = length(N))\r\njags.data\r\n\r\n\r\n# JAGS model definition ---------------------------------------------------\r\n\r\nsink(""model.jags"")\r\n\r\ncat(""model {\r\n# Priors \r\n  q ~ dbeta(1, 1)          # vague prior for dispersal rate \r\n  mu ~ dnorm(0, 0.01)      # the logit-scale parameter for dispersal effectiveness \r\n  r ~ dnorm(4.02, 8.54)    # informative prior for population growth rate (on wheat)\r\n  sd ~  dt(0, 1, 1) T(0, ) # standard deviation of the random intercept (half-Cauchy)\r\n  tau <- pow(sd, -2)       # precision for the random intercept\r\n\r\n# Likelihood\r\n  for (i in 1:n) {\r\n    D[i] ~ dbin(q, N[i])            # no. of dispersers (observed)\r\n    S[i] ~ dbin(p[i], D[i])         # no. of settlers (latent)\r\n    logit(p[i]) <- eta[i]\r\n    eta[i] ~ dnorm(mu, tau)\r\n    lambda[i] <- S[i] * (2 ^ r - 1) # expectation for C given S and r\r\n    C[i] ~ dpois(lambda[i])         # no. of colonisers (observed)\r\n  }\r\n  \r\n  # Derived quantities\r\n    mp <- ilogit(mu)      # Mean dispersal effectiveness\r\n    mS <- mean(S[])       # Mean no. of settlers\r\n    k <- p * (2 ^ r - 1)  # Colonisation potential\r\n    mk <- mean(k[])\r\n}\r\n"", fill = TRUE)\r\nsink()\r\n\r\n\r\n# Run MCMC ----------------------------------------------------------------\r\n\r\nlibrary(coda)\r\nlibrary(jagsUI)\r\nlibrary(MCMCvis)\r\n\r\nparams <- c(""q"", ""p"", ""r"", ""S"", ""mp"", ""sd"", ""mu"", ""mS"", ""k"", ""mk"")\r\n\r\njm <- jags(data = jags.data, inits = NULL, params, model.file = ""model.jags"", n.chains = 3, n.adapt = 10000, n.burnin = 1e5, n.iter = 11e5, n.thin = 100, parallel = TRUE)\r\n\r\nsummary(jm)\r\n\r\n# Traceplots\r\nMCMCtrace(jm, params = c(""q"", ""mu""), ind = TRUE, pdf = FALSE)\r\nMCMCtrace(jm, params = c(""r"", ""sd""), ind = TRUE, pdf = FALSE)\r\nMCMCtrace(jm, params = c(""mp"", ""mk""), ind = TRUE, pdf = FALSE)\r\n\r\n\r\n# Estimates\r\nMCMCsummary(jm, params = c(""q"", ""mp"", ""mS"", ""r"", ""mk"", ""sd""), round = 3, n.eff = TRUE)\r\n', '\r\n# Online Resource 3. \r\n\r\n# Reproducible example on how to estimate dispersal parameters using the methodology proposed in:\r\n# Kuczynski L., Radwanska A., Karpicka-Ignatowska K., Laska A., Lewandowski M., Rector B.G., Majer A., Raubic J., Skoracka A. \r\n# ""A comprehensive and cost-effective approach for investigating passive dispersal in minute invertebrates with case studies of phytophagous eriophyid mites""\r\n\r\n# Code by: Lechosaw Kuczyski, lechu@amu.edu.pl\r\n\r\n# Case study 2: testing wheat curl mite (WCM) wind dispersal towards smooth brome (Bromopsis inermis)\r\n\r\n# Data --------------------------------------------------------------------\r\n\r\n# Population size on the source patch:\r\nN <- c(2500, 2700, 2500, 1700, 2400, 1100, 1700, 3500)\r\n\r\n# No. of dispersers, i.e. individuals that leave the source patch:\r\nD <- c(200, 200, 500, 100, 200, 100, 200, 800)\r\n\r\n# No. of colonisers, i.e. population size on the target patch after several generations:\r\nC <- c(1, 1, 11, 0, 3, 4, 2, 1)\r\n\r\njags.data <- list(N = N, D = D, C = C, n = length(N))\r\njags.data\r\n\r\n\r\n# JAGS model definition ---------------------------------------------------\r\n\r\nsink(""model.jags"")\r\n\r\ncat(""model {\r\n# Priors \r\n  q ~ dbeta(1, 1)          # vague prior for dispersal rate \r\n  mu ~ dnorm(0, 0.01)      # the logit-scale parameter for dispersal effectiveness \r\n  r ~ dnorm(1.60,  8.07)       # informative prior for population growth rate (on brome)\r\n  sd ~  dt(0, 1, 1) T(0, ) # standard deviation of the random intercept (half-Cauchy)\r\n  tau <- pow(sd, -2)       # precision for the random intercept\r\n\r\n# Likelihood\r\n  for (i in 1:n) {\r\n    D[i] ~ dbin(q, N[i])            # no. of dispersers (observed)\r\n    S[i] ~ dbin(p[i], D[i])         # no. of settlers (latent)\r\n    logit(p[i]) <- eta[i]\r\n    eta[i] ~ dnorm(mu, tau)\r\n    lambda[i] <- S[i] * (2 ^ r - 1) # expectation for C given S and r\r\n    C[i] ~ dpois(lambda[i])         # no. of colonisers (observed)\r\n  }\r\n  \r\n  # Derived quantities\r\n    mp <- ilogit(mu)      # Mean dispersal effectiveness\r\n    mS <- mean(S[])       # Mean no. of settlers\r\n    k <- p * (2 ^ r - 1)  # Colonisation potential\r\n    mk <- mean(k[])\r\n}\r\n"", fill = TRUE)\r\nsink()\r\n\r\n\r\n# Run MCMC ----------------------------------------------------------------\r\n\r\nlibrary(coda)\r\nlibrary(jagsUI)\r\nlibrary(MCMCvis)\r\n\r\nparams <- c(""q"", ""p"", ""r"", ""S"", ""mp"", ""sd"", ""mu"", ""mS"", ""k"", ""mk"")\r\ninits <- function() list(S = rep(1, length(N)))\r\n\r\njm <- jags(data = jags.data, inits = inits, params, model.file = ""model.jags"", n.chains = 3, n.adapt = 10000, n.burnin = 1e5, n.iter = 11e5, n.thin = 100, parallel = TRUE)\r\n\r\nsummary(jm)\r\n\r\n# Traceplots\r\nMCMCtrace(jm, params = c(""q"", ""mu""), ind = TRUE, pdf = FALSE)\r\nMCMCtrace(jm, params = c(""r"", ""sd""), ind = TRUE, pdf = FALSE)\r\nMCMCtrace(jm, params = c(""mp"", ""mk""), ind = TRUE, pdf = FALSE)\r\n\r\n\r\n# Estimates\r\nMCMCsummary(jm, params = c(""q"", ""mp"", ""mS"", ""r"", ""mk"", ""sd""), round = 3, n.eff = TRUE)\r\n']","Electronic Supplementary Material for: A comprehensive and cost-effective approach for investigating passive dispersal in minute invertebrates with case studies of phytophagous eriophyid mites Filename: ESM_1.pdfPhotos and technical drawings of the 'wind-transience' tunnel, 'vector-transience' tunnel and 'departure' tunnel, and the scheme of Lego Mindstorms NXT 2.0 - engine program for a robotic mammalian vector simulation. Filename: ESM_2.R and ESM_3.RReproducible examples on how to estimate dispersal parameters using the methodology proposed in this article. Filename: ESM_4.pdfEstimated costs of devices construction in USD. Filename: ESM_5.mp4Video 3D model of the wind-transience tunnel. Filename: ESM_6.mp4Video 3D model of the vector-transience tunnel. Filename: ESM_7.mp4Video 3D model of the departure tunnel. Filename: ESM_8.mp4Video of eriophyid mite behaviors observed within the departure tunnel.",2
"Supplementary data for: Plutniak 2021, ""Assyrian Merchants meet Nuclear Physicists: History of the Early Contributions from Social Sciences to Computer Science. The Case of Automatic Pattern Detection in Graphs (1950s1970s)""","Supplementary data for: Plutniak S. 2021, Assyrian Merchants meet Nuclear Physicists: History of the Early Contributions from Social Sciences to Computer Science. The Case of Automatic Pattern Detection in Graphs (1950s1970s), Interdisciplinary Science Reviews.ContentsR implementation of Harary & Ross 1957 method for clique detection;R code for a comparison between 5 clique detection algorithms applied on the data about the Ancient Assyrian trade network from Gardin & Garelli 1961.ReferencesGardin, Jean-Claude and Paul Garelli. 1961. tude des tablissements assyriens en Cappadoce par ordinateur, Annales. conomies, socits, civilisations, 16, 5, p. 837-876, doi: 10.3406/ahess.1961.420758.Harary, Frank and Ian C. Ross. 1957. A Procedure for Clique Detection using the Group Matrix, Sociometry, 20, 3, p. 205-215, doi: 10.2307/2785673.","['# install and load required packages ####\nif (! requireNamespace(""igraph"", quietly = TRUE)){\n  install.packages(""igraph"")\n}\nif (! requireNamespace(""sna"", quietly = TRUE)){\n  install.packages(""sna"")\n  }\nif (! requireNamespace(""RBGL"", quietly = TRUE)){\n  install.packages(""BiocManager"")\n  BiocManager::install(""RBGL"")\n}\nif (! requireNamespace(""qpgraph"", quietly = TRUE)){\n  BiocManager::install(""qpgraph"")\n}\n\nlibrary(igraph)\nlibrary(sna)\nlibrary(RBGL)\nlibrary(qpgraph)\n\n# Harary and Ross 1951 implementation ####\n.getC <- function(M, unicliquals, Mrowsums){\n  Cgroup.list <- list()\n  CgroupIndex.list <- list()\n  CgroupPrime.list <- list()\n  for(i in 1:nrow(M) ){\n    # skip if the point already belongs to a clique:\n    if(i %in% unlist(CgroupIndex.list)) next  \n    # skip if no unicliqual points:\n    if(! i %in% unicliquals ) next\n    CgroupIndex <- sort(c(i, which(M[, i] != 0)))\n    Cgroup <- sort(colnames(M)[c(i, which(M[, i] != 0))])\n    CgroupPrime <- which( Mrowsums == Mrowsums[i] ) \n    CgroupPrime <- CgroupPrime[CgroupPrime %in% CgroupIndex]\n    # add to results \n    Cgroup.list <- append(Cgroup.list, list(Cgroup))\n    CgroupIndex.list <- append(CgroupIndex.list, list(CgroupIndex))\n    CgroupPrime.list <- append(CgroupPrime.list, list(CgroupPrime))\n  }\n  list(Cgroup.list = Cgroup.list,\n       CgroupPrime.list = CgroupPrime.list)\n}\n\n.substract.matrix <- function(mat, CgroupPrime.list){\n  i <- 1:nrow(mat)\n  i <- i[ ! 1:nrow(mat) %in% unlist(CgroupPrime.list) ]\n  mat[i, i]\n}\n\n.extract.cliques <- function(mat){\n  cliques.list <- list() \n  M <- mat * mat %*% t(mat)\n  np <- apply(mat, 1, sum) # similar to vertices\' degrees\n  Mrowsums <- rowSums(M)\n  unicliquals <- which( Mrowsums == np * (np - 1) )\n  \n  if(length(unicliquals) > 0){\n    res <- .getC(M, unicliquals, Mrowsums)\n    cliques <- res$Cgroup.list\n    # get only cliques with at least 3 vertices:\n    cliques <- cliques[sapply(cliques, function(x) length(x) > 2 )]\n    cliques.list <- append(cliques.list, cliques)\n    mat <- .substract.matrix(mat, res$CgroupPrime.list)\n  }\n  else{\n    CgroupIndex <- sort(c(1, which(M[, 1] != 0)))\n    Cgroup <- c(1, which(mat[, 1] != 0))\n    submat1 <- mat[Cgroup, Cgroup]\n    submat2 <- mat[-1, -1]\n    mat <- list(submat1, submat2)\n  }\n  list(mat, cliques.list)\n}\n\nhaross.cliques <- function(mat){\n  # initial tests:\n  if( ! is.matrix(mat) ) stop(""The argument is not a matrix."") \n  if( ncol(mat) != nrow(mat) ) stop(""A square matrix is required."") \n  if( is.null(colnames(mat)) & is.null(rownames(mat)) ){\n    colnames(mat) <- 1:ncol(mat)\n    rownames(mat) <- 1:nrow(mat)\n  }\n  # set variables:\n  cliques.list.final <- list() \n  mat.list <- list(mat)\n  \n  repeat{ # repeat while the sum of the matrix values > 0\n    # run the main function:\n    res <- lapply(mat.list, .extract.cliques)\n    # sort results:\n    #   1) extract and add the cliques to the list:\n    cliques.list.final <- append(cliques.list.final, \n                                 lapply(res, function(x) x[[2]])\n    )\n    #   2) extract the list of matrices:\n    mat.list <- lapply(res, function(x) x[[1]] )\n    # if the list is too nested, unnest:\n    if( is.list(mat.list[[1]]) & length(mat.list[[1]]) > 1 ) {\n      mat.list <- unlist(mat.list, recursive = F)\n    }\n    # keep only the matrices with more than 2 points:\n    mat.list <- mat.list[ sapply(mat.list, function(x) sum(x) > 2 ) ]\n    # if there is no more matrices, break\n    if( length(mat.list) == 0 ) break\n  }\n  \n  unlist(cliques.list.final, recursive = F)\n}\n\n# Make table from Gardin & Garelli 1961, fig. 10 p. 868: ####\nGardinGarelli.df <- as.matrix(rbind(\n  c(""amur-ishtar"", ""laqipun""),\n  c(""hina"", ""amur-ishtar""),\n  c(""hina"", ""im(i)d-ilum""),\n  c(""hina"", ""laqipun""),\n  c(""im(i)d-ilum"", ""amur-ishtar""),\n  c(""im(i)d-ilum"", ""laqipun""),\n  c(""pushu-kin"", ""hina""),\n  c(""pushu-kin"", ""laqipun""),\n  c(""amur-ishtar"", ""assur-nada""),\n  c(""assur-imitti"", ""amur-ishtar""),\n  c(""assur-imitti"", ""assur-nada""),\n  c(""assur-taklaku"", ""amur-ishtar""),\n  c(""assur-taklaku"", ""assur-nada""),\n  c(""assur-taklaku"", ""pushu-kin""),\n  c(""pushu-kin"", ""assur-nada""),\n  c(""assur-tab"", ""im(i)d-ilum""),\n  c(""assur-tab"", ""pushu-kin""),\n  c(""buzazu"", ""enna-sin""),\n  c(""buzazu"", ""shu-belim""),\n  c(""mannum-balum-assur"", ""enna-sin""),\n  c(""mannum-balum-assur"", ""shu-belim""),\n  c(""amur-ishtar"", ""pushu-kin""),\n  c(""im(i)d-ilum"", ""pushu-kin""),\n  c(""enna-sin"", ""shu-belim"")\n))\ncolnames(GardinGarelli.df) <- c(""from"", ""to"")\n\n# cliques given by Gardin and Garelli 1961:\nGardinGarelli.res <- list(\n     c(""amur-ishtar"", ""pushu-kin"", ""hina"", ""im(i)d-ilum"", ""laqipun""),\n     c(""amur-ishtar"", ""pushu-kin"", ""assur-nada"", ""assur-taklaku""),\n     c(""amur-ishtar"", ""assur-imitti"", ""assur-nada""),\n     c(""pushu-kin"", ""assur-tab"", ""im(i)d-ilum""),\n     c(""buzazu"", ""enna-sin"", ""shu-belim""),\n     c(""enna-sin"", ""mannum-balum-assur"", ""shu-belim"")\n)\n\n# Generate different formats: ####\n# matrix:\nGardinGarelli.g <- igraph::graph_from_data_frame(GardinGarelli.df, directed=F)\nGardinGarelli.df <- igraph::as_adjacency_matrix(GardinGarelli.g, sparse=F)\n# graphNEL:\nGardinGarelli.nel <- igraph::igraph.to.graphNEL(GardinGarelli.g)\n# sna network:\nGardinGarelli.net <- network::network(GardinGarelli.df, directed=F)\n\n\n# Clique algorithms: ####\n\n## .... Harary and Ross 1957 ####\nharary.res <- haross.cliques(GardinGarelli.df)\nharary.res <- harary.res[ order(sapply(harary.res, length), decreasing=T) ]\n\n## .... Bron and Kerbosch 1973 (RBGL) ####\nbron.res <- RBGL::maxClique(GardinGarelli.nel)$maxCliques\n\n## .... Makino and Uno 2004 (sna) ####\nmakino.res <- sna::clique.census(GardinGarelli.net, mode=""graph"",\n                                 tabulate.by.vertex=F)$cliques\nmakino.res <- lapply(makino.res, function(x)\n  lapply(x, function(y) network.vertex.names(GardinGarelli.net)[y]) )\nmakino.res <- unlist(makino.res, recursive = F)\n\n## .... Ostergard 2001 (qpgraph) ####\nostergard.res <- qpgraph::qpGetCliques(GardinGarelli.df)\nostergard.res <- lapply(ostergard.res,\n                        function(i) rownames(GardinGarelli.df)[i]) \n\n## .... Eppstein et al. 2010 (igraph)  ####\neppstein.res <- igraph::max_cliques(GardinGarelli.g, min=3)\neppstein.res <- lapply(eppstein.res, names)\n\n# Compare results: ####\n\nres.list <- list(\n  ""Gardin 1961"" = GardinGarelli.res,\n  ""Harary 1957"" = harary.res,\n  ""Bron 1973"" = bron.res,\n  ""Makino 2004"" = makino.res,\n  ""Osertgard 2001"" = ostergard.res,\n  ""Eppstein 2010"" = eppstein.res\n)\n\nres.list <- lapply(res.list, function(x) lapply(x,  sort))\nres.list <- lapply(res.list, function(x) lapply(x,  paste, collapse=""/""))\n\nres.tab <- lapply(res.list, function(x)  res.list[[2]] %in% x)\nres.tab <- do.call(""rbind"", res.tab)\nres.tab <- t(res.tab)\n\nres.tab <- cbind(id = c(1:6, NA, NA),\n      size = sapply(harary.res, length), \n      res.tab)\nres.tab\n\n']","Supplementary data for: Plutniak 2021, ""Assyrian Merchants meet Nuclear Physicists: History of the Early Contributions from Social Sciences to Computer Science. The Case of Automatic Pattern Detection in Graphs (1950s1970s)"" Supplementary data for: Plutniak S. 2021, Assyrian Merchants meet Nuclear Physicists: History of the Early Contributions from Social Sciences to Computer Science. The Case of Automatic Pattern Detection in Graphs (1950s1970s), Interdisciplinary Science Reviews.ContentsR implementation of Harary & Ross 1957 method for clique detection;R code for a comparison between 5 clique detection algorithms applied on the data about the Ancient Assyrian trade network from Gardin & Garelli 1961.ReferencesGardin, Jean-Claude and Paul Garelli. 1961. tude des tablissements assyriens en Cappadoce par ordinateur, Annales. conomies, socits, civilisations, 16, 5, p. 837-876, doi: 10.3406/ahess.1961.420758.Harary, Frank and Ian C. Ross. 1957. A Procedure for Clique Detection using the Group Matrix, Sociometry, 20, 3, p. 205-215, doi: 10.2307/2785673.",2
Data from: How to estimate kinship,"The concept of kinship permeates many domains of fundamental and applied biology ranging from social evolution to conservation science to quantitative and human genetics. Until recently, pedigrees were the gold standard to infer kinship, but the advent of next generation sequencing and the availability of dense genetic markers in many species make it a good time to (re) evaluate the usefulness of genetic markers in this context. Using three published data sets where both pedigrees and markers are available, we evaluate two common and a new genetic estimator of kinship. We show discrepancies between pedigree values and marker estimates of kinship, and explore via simulations the possible reasons for these. We find these discrepancies are attributable to two main sources: pedigree errors and heterogeneity in the origin of founders. We also show that our new marker-based kinship estimator has very good statistical properties and behavior, and is particularly well suited for situations where the source population is of small size, as will often be the case in conservation biology, and where high levels of kinship are expected, as is typical in social evolution studies.","['#######################################\r\n#######################################\r\nbuildped<-function(founders=50,fert=2.0,ngen=2){\r\n  nf<- founders\r\n  sanc<-nf #sum of ancestors\r\n  ni<-nf #number of inds each generation\r\n  parents<-1:nf\r\n  #ncoup<-nf %/% 2 #monogamy\r\n  ped<-data.frame(ind=1:nf,mum=rep(NA,nf),dad=rep(NA,nf))\r\n  \r\n  draw.parents<-function(parents,fert,sanc){\r\n    ncoup<-length(parents) %/% 2\r\n    nbabs<-rpois(ncoup,fert)\r\n    allbabs<-sum(nbabs)\r\n    thisgen.ped<-data.frame(ind=integer(allbabs),\r\n                            mum=integer(allbabs),\r\n                            dad=integer(allbabs))\r\n    for (ic in 1:ncoup){\r\n      if(nbabs[ic]>0){  \r\n        # no selfing, monogamy, but can reuse parents\r\n        par.drawn<-sample(parents,size=2,replace=FALSE) \r\n        #  parents<-parents[-par.drawn]\r\n        nbabsc<-c(0,cumsum(nbabs))\r\n        x<-(nbabsc[ic]+1):nbabsc[ic+1]\r\n        thisgen.ped[x,]<-cbind(sanc+x,rep(par.drawn[1],nbabs[ic]),\r\n                               rep(par.drawn[2],nbabs[ic]))\r\n      }\r\n    }\r\n    return(thisgen.ped)  \r\n  }\r\n  \r\n  for (igen in 1:ngen){\r\n    thisgen.ped<-draw.parents(parents,fert,sanc)\r\n    ped<-rbind(ped,thisgen.ped)\r\n    ni<-c(ni,dim(thisgen.ped)[1])\r\n    sanc<-sum(ni)  \r\n    parents<-(sum(ni[1:igen])+1):sanc\r\n  }\r\n  return(ped)\r\n}\r\n#######################################\r\n#######################################\r\nbuildped.rm<-function(founders=50,fert=2.0,ngen=2){\r\n  nf<- founders\r\n  sanc<-nf #sum of ancestors\r\n  ni<-nf #number of inds each generation\r\n  parents<-1:nf\r\n  ped<-data.frame(ind=1:nf,mum=rep(NA,nf),dad=rep(NA,nf))\r\n  \r\n  draw.parents<-function(parents,fert,sanc){\r\n    ncoup<-length(parents) %/% 2\r\n    nbabs<-rpois(ncoup,fert)\r\n    allbabs<-sum(nbabs)\r\n    thisgen.ped<-data.frame(ind=integer(allbabs),\r\n                            mum=integer(allbabs),\r\n                            dad=integer(allbabs))\r\n    for (ic in 1:allbabs){\r\n        # promiscuity, no selfing, 1 offs per mating\r\n        par.drawn<-sample(parents,size=2,replace=FALSE) \r\n        #  parents<-parents[-par.drawn]\r\n        #nbabsc<-c(0,cumsum(nbabs))\r\n        thisgen.ped[ic,]<-cbind(sanc+ic,par.drawn[1],par.drawn[2])\r\n      }\r\n   \r\n    return(thisgen.ped)  \r\n  }\r\n  \r\n  for (igen in 1:ngen){\r\n    thisgen.ped<-draw.parents(parents,fert,sanc)\r\n    ped<-rbind(ped,thisgen.ped)\r\n    ni<-c(ni,dim(thisgen.ped)[1])\r\n    sanc<-sum(ni)  \r\n    parents<-(sum(ni[1:igen])+1):sanc\r\n  }\r\n  return(ped)\r\n}\r\n##########################################################\r\ndraw.offsprings.ped.SNPs<-function(ped=ped,founders.genotypes=dat,ndigits=2){\r\n  #assumes a pedigree (3 columns, ind, dam,sire)\r\n  #and a data frame dat with founders genotypes\r\n  #encoded as allelic dosage (0,1,2)\r\n  #dam and sire of founders are entered as NA  \r\n  \r\n  nfounders<-dim(founders.genotypes)[1]\r\n  nloc<-dim(founders.genotypes)[2]\r\n  noffs<-dim(ped)[1]-nfounders\r\n  genos<-data.frame(matrix(numeric(nloc*(nfounders+noffs)),ncol=nloc))\r\n  names(genos)<-names(founders.genotypes)\r\n  genos[1:nfounders,]<-founders.genotypes\r\n  seqoffs<-(nfounders+1):(nfounders+noffs)\r\n  mumid<-0\r\n  dadid<-0\r\n  for (io in seqoffs){\r\n    if (ped[io,2]!=mumid | ped[io,3]!=dadid){\r\n      mumid<-ped[io,2]\r\n      dadid<-ped[io,3]           \r\n      mum<-genos[mumid,]\r\n      dad<-genos[dadid,]\r\n      #      dat<-rbind(mum,dad)\r\n      #only draw heterozygous positions. Not sure it speeds things up.   \r\n      het.mum<-which(mum==1)\r\n      lhm<-length(het.mum)\r\n      het.dad<-which(dad==1)\r\n      lhd<-length(het.dad)\r\n    }\r\n    tmp1<-rbinom(lhm,1,.5)\r\n    tmp2<-rbinom(lhd,1,.5)\r\n    gam.mum<-mum %/%2\r\n    gam.mum[het.mum]<-tmp1\r\n    gam.dad<-dad %/% 2\r\n    gam.dad[het.dad]<-tmp2\r\n    genos[io,]<-gam.mum+gam.dad \r\n  }\r\n  genos\r\n}\r\n######################################\r\nbeta.coan.SNPs<-function(dat){\r\n  #dat is a data frame with individuals in rows and allelic dosage for each locus in colums  \r\n  #uses matching proba -same equation as for population i.e. Mij=[xiXj+(2-xi)(2-xj)]/4\r\n  dat<-as.matrix(dat)\r\n  nl<-dim(dat)[2]\r\n  Mij<-(tcrossprod(dat)+tcrossprod(2-dat))/4\r\n  diag(Mij)<-NA\r\n  Mb<-mean(Mij,na.rm=T)\r\n  (Mij-Mb)/(nl-Mb)\r\n}\r\n######################################\r\nGCTA.SNPs<-function(dat){\r\n  #weighted estimate of GCTA i.e sum_nl[(xil-2pl)(xjl-2pl)]/4 sum_nl[pl(1-pl)]\r\n  dat<-as.matrix(dat)\r\n  sfs<-colSums(dat)\r\n  ni<-dim(dat)[1]\r\n  nl<-dim(dat)[2]\r\n  datc<-sweep(dat,2,sfs/ni,FUN=""-"")\r\n  num<-tcrossprod(datc)\r\n  diag(num)<-NA\r\n  den<-sum(sfs/2/ni*(1-sfs/2/ni))*4\r\n  num/den\r\n}\r\nGCTAu.SNPs<-function(dat){\r\n  #unweighted estimate of GCTA i.e sum_nl[(xil-2pl)(xjl-2pl)]/4 sum_nl[pl(1-pl)]\r\n  dat<-as.matrix(dat)\r\n  sfs<-colMeans(dat,na.rm=TRUE)\r\n  pol<-which(sfs>0 & sfs<2)\r\n  dat<-dat[,pol]\r\n  ni<-dim(dat)[1]\r\n  nl<-dim(dat)[2]\r\n  p<-sfs[pol]/2\r\n  w<-(p*(1-p))^.5*2\r\n # ni<-dim(dat)[1]\r\n # nl<-dim(dat)[2]\r\n  datc<-sweep(dat,2,2*p,FUN=""-"")\r\n  datc[is.na(datc)]<-0.0\r\n  datcr<-sweep(datc,2,w,FUN=""/"")\r\n  res<-tcrossprod(datcr)/nl\r\n  diag(res)<-NA\r\n  res\r\n  }\r\n']","Data from: How to estimate kinship The concept of kinship permeates many domains of fundamental and applied biology ranging from social evolution to conservation science to quantitative and human genetics. Until recently, pedigrees were the gold standard to infer kinship, but the advent of next generation sequencing and the availability of dense genetic markers in many species make it a good time to (re) evaluate the usefulness of genetic markers in this context. Using three published data sets where both pedigrees and markers are available, we evaluate two common and a new genetic estimator of kinship. We show discrepancies between pedigree values and marker estimates of kinship, and explore via simulations the possible reasons for these. We find these discrepancies are attributable to two main sources: pedigree errors and heterogeneity in the origin of founders. We also show that our new marker-based kinship estimator has very good statistical properties and behavior, and is particularly well suited for situations where the source population is of small size, as will often be the case in conservation biology, and where high levels of kinship are expected, as is typical in social evolution studies.",2
Data and code for: Social phenotype-dependent selection of social environment in wild great and blue tits: An experimental study,"There is growing evidence that individuals actively assess the match between their phenotype and their environment when making habitat choice decisions (so-called matching habitat choice). However, to our knowledge, no studies have considered how the social environment may interact with social phenotype in determining habitat choice, despite habitat choice being an inherently social process and growing evidence for individual variation in sociability. We conducted an experiment using wild great and blue tits to understand how birds integrate their social phenotype and social environment when choosing where and how to feed. We used programmable feeders to (i) record social interactions and estimate social phenotype and (ii) experimentally manipulate the local density experienced by birds of differing social phenotype. By tracking feeder usage, we estimated how social environment and social phenotype predicted feeder choice and feeding behaviour. Both social environment and social phenotype predicted feeder usage, but a bird's decision to remain in a particular social environment did not depend on their social phenotype. In contrast, for feeding behaviour, responses to the social environment depended on social phenotype. Our results provide rare evidence of matching habitat choice and shed light on the dependence of habitat choice on between-individual differences in social phenotype.","['##########################################\r\n###   SOCIAL HABITAT CHOICE ANALYSES   ###\r\n##########################################\r\n\r\nlibrary(lme4)\r\nlibrary(MuMIn)\r\nlibrary(glmmTMB)\r\nlibrary(dplyr)\r\nlibrary(lmtest)\r\n\r\n#first reading in the data and ensuring it\'s suitably restricted\r\n\r\ndata <- read.csv(""Social_hab_choice_daily_scale.csv"")\r\n\r\n#removing individuals with few pre-experiment observations (100 obs is the point at which a correlation between number of obs and degree is no longer present)\r\n\r\ndata <- subset(data, no.obs.pre.exp > 99)\r\n\r\ndata <- subset(data, seen.after.end.feb == ""Yes"" | max.day.seen > 27)\r\n\r\ndata$species <- as.factor(as.character(data$species))\r\n\r\ndata$density.restriction <- as.factor(as.character(data$density.restriction))\r\n\r\n# Probability of being seen on home feeder --------------------------------\r\n\r\non.home.feeder.dat <- subset(data, !is.na(on.home.feeder) & !is.na(on.other.feeder.in.pair))\r\n\r\non.home.feeder.dat$experimental.day.factor <- as.factor(as.character(on.home.feeder.dat$experimental.day))\r\n\r\nprob.home.feeder.1.autocorr <- glmmTMB(on.home.feeder ~ species + experimental.day + deg.weighted + density.restriction + (1|feeder.site) + (1|tag) + ar1(experimental.day.factor +0|tag), data = on.home.feeder.dat, family = ""binomial"")\r\n\r\nsummary(prob.home.feeder.1.autocorr)\r\n\r\nprob.home.feeder.2.autocorr <- glmmTMB(on.home.feeder ~ species + experimental.day + deg.weighted * density.restriction + (1|feeder.site) + (1|tag) + ar1(experimental.day.factor +0|tag), data = on.home.feeder.dat, family = ""binomial"")\r\n\r\nsummary(prob.home.feeder.2.autocorr)\r\n\r\n#comparing models\r\n\r\nAICc(prob.home.feeder.1.autocorr, prob.home.feeder.2.autocorr)\r\n\r\nlrtest(prob.home.feeder.2.autocorr, prob.home.feeder.1.autocorr)\r\n\r\n#no indication that birds with different social phenotypes respond differently to the 2 treatments\r\n\r\n# Probability of being seen on the other feeder in the pair ---------------\r\n\r\non.other.feeder.dat <- subset(data, !is.na(on.other.feeder.in.pair) & !is.na(on.home.feeder))\r\n\r\non.other.feeder.dat$experimental.day.factor <- as.factor(as.character(on.other.feeder.dat$experimental.day))\r\n\r\nprob.other.feeder.1.autocorr <- glmmTMB(on.other.feeder.in.pair ~ species + experimental.day + deg.weighted + density.restriction + (1|feeder.site) + (1|tag) + ar1(experimental.day.factor +0|tag), data = on.other.feeder.dat, family = ""binomial"")\r\n\r\nsummary(prob.other.feeder.1.autocorr)\r\n\r\nprob.other.feeder.2.autocorr <- glmmTMB(on.other.feeder.in.pair ~ species + experimental.day + deg.weighted * density.restriction + (1|feeder.site) + (1|tag) + ar1(experimental.day.factor +0|tag), data = on.other.feeder.dat, family = ""binomial"")\r\n\r\nsummary(prob.other.feeder.2.autocorr)\r\n\r\nlrtest(prob.other.feeder.2.autocorr, prob.other.feeder.1.autocorr)\r\n\r\n# Looking at feeding behaviour at a site ----------------------------------\r\n\r\n#even if birds stay at their feeding site, they may adjust their feeding behaviour in response to the social environment\r\n#to look at this, I\'ve used information on how long their feeding bouts were and how long intervals between them were for any given day\r\n\r\nvisit.length.1 <- glmer(mean.visit.length ~ species + scale(experimental.day) + scale(deg.weighted) + density.restriction + (1|feeder.site) + (1|tag), data = on.home.feeder.dat, family = Gamma(link = log))\r\n\r\n#feeder.site singular so remove and re-run\r\n\r\nvisit.length.1 <- glmer(mean.visit.length ~ species + scale(experimental.day) + scale(deg.weighted) + density.restriction + (1|tag), data = on.home.feeder.dat, family = Gamma(link = log))\r\n\r\nsummary(visit.length.1)\r\n\r\nvisit.length.2 <- glmer(mean.visit.length ~ species + scale(experimental.day) + scale(deg.weighted) * density.restriction + (1|tag), data = on.home.feeder.dat, family = Gamma(link = log), glmerControl(optimizer = ""bobyqa""))\r\n\r\nsummary(visit.length.2)\r\n\r\nlrtest(visit.length.2, visit.length.1)\r\n\r\n#NOW LOOKING AT INTER VISIT INTERVALS\r\n\r\ninterval.length.1382.cutoff.1 <- glmer(mean.interval.length.1382.cutoff ~  species + scale(experimental.day) + scale(deg.weighted) + density.restriction + (1|feeder.site) + (1|tag), data = on.home.feeder.dat, family = Gamma(link = log))\r\n\r\n#feeder.site singular so remove and re-run\r\n\r\ninterval.length.1382.cutoff.1 <- glmer(mean.interval.length.1382.cutoff ~  species + scale(experimental.day) + scale(deg.weighted) + density.restriction + (1|tag), data = on.home.feeder.dat, family = Gamma(link = log), glmerControl(optimizer = ""bobyqa""))\r\n\r\nsummary(interval.length.1382.cutoff.1)\r\n\r\ninterval.length.1382.cutoff.2 <- glmer(mean.interval.length.1382.cutoff ~  species + scale(experimental.day) + scale(deg.weighted) * density.restriction + (1|tag), data = on.home.feeder.dat, family = Gamma(link = log), glmerControl(optimizer = ""bobyqa""))\r\n\r\nsummary(interval.length.1382.cutoff.2)\r\n\r\nlrtest(interval.length.1382.cutoff.2, interval.length.1382.cutoff.1)\r\n', '####----------------------------------------------------------------------------------------------\r\n# Density manipulation \r\n####----------------------------------------------------------------------------------------------\r\n\r\n\r\n\r\n# Packages \r\n####-------------------------\r\n\r\n\r\nlibrary(data.table)\r\nlibrary(ggplot2)\r\nlibrary(patchwork)\r\nlibrary(dplyr)\r\nlibrary(effects)\r\nlibrary(glmmTMB)\r\nlibrary(emmeans)\r\n\r\n# Load data\r\nload(""data_density.RData"")\r\n# period: pre=prior to density manipulation, during=during density manipulation, exp_day: experimental day, \r\n# N_rec: Number of recordings, N_ids: Number of individuals visited, total_rec: total number of recordings to both loggers, prop: proportion of total recordings\r\n\r\n\r\n#   1)  Number of recordings \r\n####-------------------------\r\n\r\nm1 <- glmmTMB(cbind(N_rec, total_rec-N_rec) ~ period*treatment + exp_day + (1|Site) + ar1(factor(exp_day)+0|logger), \r\n              data=data_density, family = \'binomial\') \r\n\r\n# Post-hoc comparisons between combinations of period and manipulation \r\nemmeans(m1, list(pairwise ~ period*treatment), adjust = ""tukey"", type = ""response"")    \r\n\r\n# Extract effects\r\ne <- allEffects(m1)$`period:treatment` %>% data.frame %>% data.table\r\n\r\n# Plot\r\ncol.treatment <- c(""high"" = ""orange"", ""low"" = ""darkslategrey"")\r\n(plot1 <- ggplot() +\r\n    geom_jitter(data=data_density, aes(y = prop, x = period, colour=treatment, group=treatment), alpha=.15,width = 0.1) +\r\n    geom_point(data = e, aes(y = fit, x = period, colour=treatment, group=treatment), size=2,position=position_dodge(width = 0.2)) + \r\n    geom_line(data = e, aes(y = fit, x = period, colour=treatment, group=treatment), position=position_dodge(width = 0.2)) +\r\n    geom_errorbar(data = e, aes(x = period , ymin=(lower), ymax=(upper), width=.1, colour=treatment), position=position_dodge(width = 0.2)) + \r\n    theme_classic() + ylab(""Proportion of recordings"") + scale_color_manual(values=col.treatment) + \r\n    theme(axis.text.x = element_text( color=""black"", size=15), \r\n          axis.text.y = element_text( color=""black"", size=15), \r\n          axis.title.x = element_blank(), axis.title.y = element_text(size=16), legend.position = ""none""))\r\n\r\n\r\n\r\n#   2)  Number of individuals \r\n####-------------------------\r\n\r\nm2 <- glmmTMB(N_ids ~ period*treatment + exp_day + (1|Site) + ar1(factor(exp_day)+0|logger), \r\n              data=data_density, family = \'poisson\') \r\n\r\n# Post-hoc comparisons between combinations of period and manipulation \r\nemmeans(m2, list(pairwise ~ period*treatment), adjust = ""tukey"", type = ""response"")  \r\n\r\n# Extract effects\r\ne <- allEffects(m2)$`period:treatment` %>% data.frame %>% data.table\r\n\r\n# Plot\r\ncol.treatment <- c(""high"" = ""orange"", ""low"" = ""darkslategrey"")\r\n(plot2 <- ggplot() +\r\n    geom_jitter(data=data_density, aes(y = N_ids, x = period, colour=treatment, group=treatment), alpha=.15,width = 0.1) +\r\n    geom_point(data = e, aes(y = fit, x = period, colour=treatment, group=treatment), size=2,position=position_dodge(width = 0.2)) + \r\n    geom_line(data = e, aes(y = fit, x = period, colour=treatment, group=treatment), position=position_dodge(width = 0.2)) +\r\n    geom_errorbar(data = e, aes(x = period , ymin=(lower), ymax=(upper), width=.1, colour=treatment), position=position_dodge(width = 0.2)) + \r\n    theme_classic() + ylab(""Number of individuals"") + scale_color_manual(values=col.treatment) + \r\n    theme(axis.text.x = element_text( color=""black"", size=15), \r\n          axis.text.y = element_text( color=""black"", size=15), \r\n          axis.title.x = element_blank(), axis.title.y = element_text(size=16),\r\n          strip.text = element_blank(), legend.position = ""none""))\r\n\r\nplot1+plot2\r\n\r\n\r\n\r\n\r\n\r\n\r\n']","Data and code for: Social phenotype-dependent selection of social environment in wild great and blue tits: An experimental study There is growing evidence that individuals actively assess the match between their phenotype and their environment when making habitat choice decisions (so-called matching habitat choice). However, to our knowledge, no studies have considered how the social environment may interact with social phenotype in determining habitat choice, despite habitat choice being an inherently social process and growing evidence for individual variation in sociability. We conducted an experiment using wild great and blue tits to understand how birds integrate their social phenotype and social environment when choosing where and how to feed. We used programmable feeders to (i) record social interactions and estimate social phenotype and (ii) experimentally manipulate the local density experienced by birds of differing social phenotype. By tracking feeder usage, we estimated how social environment and social phenotype predicted feeder choice and feeding behaviour. Both social environment and social phenotype predicted feeder usage, but a bird's decision to remain in a particular social environment did not depend on their social phenotype. In contrast, for feeding behaviour, responses to the social environment depended on social phenotype. Our results provide rare evidence of matching habitat choice and shed light on the dependence of habitat choice on between-individual differences in social phenotype.",2
Social Network Analysis,"Social Network Analysis course @ Master SoBigDataThis course introduces students to the theories, concepts and measures of Social Network Analysis (SNA), that is aimed at characterizing the structure of large-scale Online Social Networks (OSNs). The course presents both classroom teaching to introduce theoretical concepts, and hands-on computer work to apply the theory on real large-scale datasets obtained from OSNs like Facebook and Twitter. The course aims to discuss in particular how the structural properties of social networks can be analysed through SNA techniques, and how these properties can be used to characterize social phenomena arising in the society.The author did not intend to violate any copyright on figures or content. In case you are the legal owner of any copyrighted content, please contact info@sobigdata.eu and we will immediately remove it","['---\ntitle: \'Master in Big Data Analytics and Social Mining: SNA - Lab R Basics\'\nauthor: ""Andrea Passarella""\ndate: ""May 2018""\noutput:\n  pdf_document: default\n  html_document: default\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n## R Markdown\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.\n\nWhen you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this.\n\n### Step #1 \nInstall (if needed) the MASS package and load it\n```{r step 1}\n#install.packages(""MASS"")\nlibrary(MASS)\n```\n\n### Step #2\nLoad the Animals data set\n```{r step 2}\nls(Animals)\nAnimals\n```\n\n### Step #3\nCalculate the ratio between animals\' brain size and their body size, adding the result as a new column called proportions to the Animals data frame\n```{r step 3}\nproportions = Animals$body/Animals$brain\nAnimals$proportions = proportions\nAnimals\n```\n\n\n### Step #4\nCompute average and standard deviation of the proportions\n```{r step 4}\navg = mean(Animals$proportions)\nstdDev = sd(Animals$proportions)\ncat(""Proportions: mean="", avg, "", sd="", stdDev, ""\\n\\n"", sep = """")\n```\n\n\n### Step #5\nRemove the column proportions from the data frame\n```{r step 5}\nAnimals$proportions = NULL\nAnimals\n```\n\n\n### Step #6\nPrint the records of animals with body size > 100\n```{r step 6}\nAnimals[Animals$body>100,]\n```\n\n### Step #7\nGet a list of animals\' names with body size > 100 and brain size > 100\n```{r step 7}\nbigAnimals = Animals[Animals$body>100 & Animals$brain>100,]\nbigAnimals\nrow.names(bigAnimals)\n# equivalent to row.names(Animals[Animals$body>100 & Animals$brain>100,])\n```\n\n### Step 8\nFind the average and std of bodes and brain sizes for the first 10 animals in the dataset\n```{r step 8}\nfirstTen = Animals[1:10,]\nfirstTen\nstatsBodyFirstTen = list(m=mean(firstTen$body), s=sd(firstTen$body))\nstatsBrainFirstTen = list(m=mean(firstTen$brain), s=sd(firstTen$brain))\ncat(""Bodies of big animals: mean="", statsBodyFirstTen$m, "" sd="", statsBodyFirstTen$s, ""\\n"", sep = """")\ncat(""Brains of big animals: mean="", statsBrainFirstTen$m, "" sd="", statsBrainFirstTen$s, ""\\n"", sep = """")\n```\n\n### Step 9\nWrite a function that returns a vector of two elements containing the mean value and the standard deviation of a vector of elements\n\n- Apply this to the body and brain sizes of Animals\n```{r step 9}\nstatsVector = function(x) {\n    avg = mean(x)\n    s = sd(x)\n    ret = c(avg,s)\n    return(ret)\n}\n\nstatsBody = statsVector(Animals$body)\ncat(""Bodies of all animals: mean="", statsBody[1], "" sd="", statsBody[2], ""\\n"", sep = """")\nstatsBrain = statsVector(Animals$brain)\ncat(""Brains of all animals: mean="", statsBrain[1], "" sd="", statsBrain[2], ""\\n"", sep = """")\n```\n\n### Step 10\nCreate a vector called body_norm with 100 samples from a Normal random variable with average and standard deviation equal to those of body sizes in the Animals dataset\n\n- print the summary of the generated dataset\n- compare the summary with another dataset of 100 samples with same average and $\\texttt{sd = 1}$\n```{r step 10}\nbody_norm = rnorm(100, statsBody[1], statsBody[2])\nsummary(body_norm)\nbody_norm1 = rnorm(100, statsBody[1], 1)\nsummary(body_norm1)\n```\n\n### Step 11\nSave the Animals data frame to a file named animals_a.txt with row and column names\n```{r step 11}\nwrite.table(Animals, ""animals_a.txt"", sep=\'\\t\')\n```\n\n### Step 12\nCreate a copy of the file named animals_b.txt, then\n\n- modify some data in it\n- Read the file into a new data frame, Animals_b\n- Write a function that returns the rows that differ between Animals and Animals_b\n- NOTE: if you want to put more data frames in a unique data structure, use a List\n```{r step 12}\n# creation and modification of animals_b.txt outside of R\nAnimals_b = read.table(""animals_b.txt"")\n\ndiffDF = function(a,b) {\n    condition = ((a$body != b$body) | (a$brain != b$brain)\n                 | (rownames(a) != rownames(b)))\n    df = a[condition,]\n    df1 = b[condition,]\n    ret = list(a.diff=df, b.diff=df1)\n    return(ret)\n}\n\ndifferences = diffDF(Animals, Animals_b)\ndifferences$a.diff\ndifferences$b.diff\n```\n\n### Step 13\nSave the workspace to a file, clean the workspace, restore the workspace from the file\n```{r step 13}\nsave.image("".RData_exerciseR"")\nrm(list=ls())\nls()\nload("".RData_exerciseR"")\nls()\n```\n\n']","Social Network Analysis Social Network Analysis course @ Master SoBigDataThis course introduces students to the theories, concepts and measures of Social Network Analysis (SNA), that is aimed at characterizing the structure of large-scale Online Social Networks (OSNs). The course presents both classroom teaching to introduce theoretical concepts, and hands-on computer work to apply the theory on real large-scale datasets obtained from OSNs like Facebook and Twitter. The course aims to discuss in particular how the structural properties of social networks can be analysed through SNA techniques, and how these properties can be used to characterize social phenomena arising in the society.The author did not intend to violate any copyright on figures or content. In case you are the legal owner of any copyrighted content, please contact info@sobigdata.eu and we will immediately remove it",2
Data from: Juvenile social dynamics reflect adult reproductive strategies in bottlenose dolphins,"The juvenile period is a challenging life history stage, especially in species with a high degree of fission-fusion dynamics, such as bottlenose dolphins, where maternal protection is virtually absent. Here, we examined how juvenile male and female bottlenose dolphins navigate this vulnerable period. Specifically, we examined their grouping patterns, activity budget, network dynamics, and social associations in the absence of adults. We found that juveniles live in highly dynamic groups, with group composition changing every 10 minutes on average. Groups were generally segregated by sex, and segregation was driven by same-sex preference rather than opposite-sex avoidance. Juveniles formed strong associations with select individuals, especially kin and same-sex partners, and both sexes formed cliques with their preferred partners. Sex-specific strategies in the juvenile period reflected adult reproductive strategies, in which the exploration of potential social partners may be more important for males (which form long-term alliances in adulthood) than females (which preferentially associate with kin in adulthood). Females spent more time alone and were more focused on foraging than males, but still formed close same-sex associations, especially with kin. Males cast a wider social net than females, with strong same-sex associations and many male associates. Males engaged in more affiliative behavior than females. These results are consistent with the social bonds and skills hypothesis and suggest that delayed sexual maturity in species with relational social complexity may allow individuals to assess potential associates and explore a complex social landscape without the risks associated with sexual maturity (e.g. adult reproductive competition; inbreeding).",,"Data from: Juvenile social dynamics reflect adult reproductive strategies in bottlenose dolphins The juvenile period is a challenging life history stage, especially in species with a high degree of fission-fusion dynamics, such as bottlenose dolphins, where maternal protection is virtually absent. Here, we examined how juvenile male and female bottlenose dolphins navigate this vulnerable period. Specifically, we examined their grouping patterns, activity budget, network dynamics, and social associations in the absence of adults. We found that juveniles live in highly dynamic groups, with group composition changing every 10 minutes on average. Groups were generally segregated by sex, and segregation was driven by same-sex preference rather than opposite-sex avoidance. Juveniles formed strong associations with select individuals, especially kin and same-sex partners, and both sexes formed cliques with their preferred partners. Sex-specific strategies in the juvenile period reflected adult reproductive strategies, in which the exploration of potential social partners may be more important for males (which form long-term alliances in adulthood) than females (which preferentially associate with kin in adulthood). Females spent more time alone and were more focused on foraging than males, but still formed close same-sex associations, especially with kin. Males cast a wider social net than females, with strong same-sex associations and many male associates. Males engaged in more affiliative behavior than females. These results are consistent with the social bonds and skills hypothesis and suggest that delayed sexual maturity in species with relational social complexity may allow individuals to assess potential associates and explore a complex social landscape without the risks associated with sexual maturity (e.g. adult reproductive competition; inbreeding).",2
Data from: Strong social relationships are associated with decreased longevity in a facultatively social mammal,"Humans in strong social relationships are more likely to live longer because social relationships may buffer stressors and thus have protective effects. However, a shortcoming of human studies is that they often rely on self-reporting of these relationships. By contrast, observational studies of nonhuman animals permit detailed analyses of the specific nature of social relationships. Thus, discoveries that some social animals live longer and healthier lives if they are involved in social grooming, forage together, or have more affiliative associates emphasizes the potential importance of social relationships on health and longevity. Previous studies have focused on the impact of social metrics on longevity in obligately social species. However, if sociality indeed has a key role in longevity, we might expect that affiliative relationships should also influence longevity in less social species. We focused on socially flexible yellow-bellied marmots (Marmota flaviventer) and asked whether female longevity covaries with the specific nature of social relationships. We quantified social relationships with social network statistics that were based on affiliative interactions, and then estimated the correlation between longevity and sociality using bivariate models. We found a significant negative phenotypic correlation between affiliative social relationship strength and longevity; marmots with greater degree, closeness, and those with a greater negative average shortest path length died at younger ages. We conclude that sociality plays an important role in longevity, but how it does so may depend on whether a species is obligately or facultatively social.","['## load library\r\nlibrary(MCMCglmm)\r\n\r\n########################################################\r\n## define names of social variables\r\nWsocial <- c( ""indegree"", \r\n              ""outdegree"", \r\n              ""betweenness"", \r\n              ""outcloseness"", \r\n              ""incloseness"", \r\n              ""local_clustering"", \r\n              ""global_clustering"", \r\n              ""ave_shortest_path"", \r\n              ""eigenv"", \r\n              ""outstrength"", \r\n              ""instrength"", \r\n              ""outcloseness_weight"", \r\n              ""incloseness_weight"", \r\n              ""eigenv_weight""\r\n             )\r\ndataNames <- paste(Wsocial, rep(""FWaff"",length(Wsocial)),sep=""."")\r\n###############################################\r\n## load the data\r\ndat <- read.csv(""finaldata_longevity_sociality.csv"")\r\n\r\n##Prior for MCMCglmm model\r\nprior <- list(\r\n  R = list (V =diag(c(1,0.00002)), nu = 1.002, fix = 2), \r\n  G = list( G1 = list(V = diag(2), nu = 3, alpha.mu = rep(0,2), alpha.V=diag(25^2,2)),\r\n           G2 = list( V = 1, nu = 0.002),\r\n           G3 = list( V = 1, nu = 0.002)\r\n  ) \r\n)\r\n\r\n##Loop running bivariate models for each SNT and longevity.\r\nfor (j in 1:length(dataNames)) {\r\n    dat$SNT <- scale(dat[[dataNames[j]]])\r\n    dat$loglg <- scale(log(dat$longevity))\r\n    m1 <- MCMCglmm(c(SNT, loglg) ~ trait -1 + at.level(trait,1):log(cort_ngL) + at.level(trait,1):age + trait:valley,\r\n               random = ~ us(trait):uid + idh(at.level(trait,2)):yrborn + idh(at.level(trait,1)):year,\r\n               rcov = ~ idh(trait):units,\r\n               data=dat, \r\n               family= c(""gaussian"",""gaussian""),\r\n               prior=prior,\r\n               nitt = 2300000, thin = 200, burnin = 300000,\r\n               pr = TRUE\r\n               )\r\n    summary(m1)\r\n\r\n    assign( paste0(""m_"",dataNames[j]), m1, envir = .GlobalEnv )\r\n}\r\n\r\n##saving all the models in a R object\r\nsave( list = ls(pattern=""m_""), file = ""FWaff_results_scale.rda"", compress = TRUE)\r\n']","Data from: Strong social relationships are associated with decreased longevity in a facultatively social mammal Humans in strong social relationships are more likely to live longer because social relationships may buffer stressors and thus have protective effects. However, a shortcoming of human studies is that they often rely on self-reporting of these relationships. By contrast, observational studies of nonhuman animals permit detailed analyses of the specific nature of social relationships. Thus, discoveries that some social animals live longer and healthier lives if they are involved in social grooming, forage together, or have more affiliative associates emphasizes the potential importance of social relationships on health and longevity. Previous studies have focused on the impact of social metrics on longevity in obligately social species. However, if sociality indeed has a key role in longevity, we might expect that affiliative relationships should also influence longevity in less social species. We focused on socially flexible yellow-bellied marmots (Marmota flaviventer) and asked whether female longevity covaries with the specific nature of social relationships. We quantified social relationships with social network statistics that were based on affiliative interactions, and then estimated the correlation between longevity and sociality using bivariate models. We found a significant negative phenotypic correlation between affiliative social relationship strength and longevity; marmots with greater degree, closeness, and those with a greater negative average shortest path length died at younger ages. We conclude that sociality plays an important role in longevity, but how it does so may depend on whether a species is obligately or facultatively social.",2
Data from: A framework for the identification of long-term social avoidance in longitudinal datasets,"Animal sociality is of significant interest to evolutionary and behavioural ecologists, with efforts focused on the patterns, causes and fitness outcomes of social preference. However, individual social patterns are the consequence of both attraction to (preference for) and avoidance of conspecifics. Despite this, social avoidance has received far less attention than social preference. Here, we detail the necessary steps to generate a spatially explicit, iterative null model which can be used to identify non-random social avoidance in longitudinal studies of social animals. We specifically identify and detail parameters which will influence the validity of the model. To test the usability of this model, we applied it to two longitudinal studies of social animals (Eastern water dragons (Intellegama leseurii) and bottlenose dolphins (Tursiops aduncus) to identify the presence of social avoidances. Using this model allowed us to identify the presence of social avoidances in both species. We hope that the framework presented here inspires interest in addressing this critical gap in our understanding of animal sociality, in turn allowing for a more holistic understanding of social interactions, relationships and structure.","['#Digidolph- create spatially-explicit null association model from dolphin data\r\n#V. Foroughirad\r\n#vjf2@duke.edu\r\n#Created August 8th, 2016\r\n#Modified June 28th, 2017 \r\n\r\nlibrary(adehabitatHR)\r\nlibrary(maptools)\r\nlibrary(rgdal)\r\nlibrary(spatstat)\r\nlibrary(Digiroo2)\r\nlibrary(coda)\r\nlibrary(spdep)\r\nlibrary(raster)\r\nlibrary(PBSmapping)\r\nlibrary(gdata)\r\nlibrary(rgeos)\r\n\r\noptions(stringsAsFactors = FALSE)\r\n\r\nsource(""digidolph_helper_functions.R"")\r\n\r\n#optional parameters to set for digidolph\r\n\r\n#minimum number of sightings per animal\r\n\r\nmin_sightings<-45\r\n\r\n#number of simulations to run\r\n\r\nnum_sim<-2\r\n\r\n#add in optimum gprox to test, or derive from model\r\n\r\ngprox_opt<-1000\r\n\r\n#filter raw sightings to include only those which fell within an area with yearly coverage\r\n\r\nraw_sightings<-read.csv(""dryad_dolphin_data.csv"")\r\n\r\nraw_sightings$Date<-as.Date(raw_sightings$Date, format=c(""%d-%b-%Y""))\r\n\r\nraw_sightings$year<-format(raw_sightings$Date,""%Y"") \r\n\r\nxydata<-cbind(raw_sightings$gps_east,raw_sightings$gps_south)\r\nxydata2<-as.data.frame(project(xydata, ""+proj=tmerc +lat_0=-25 +lon_0=113 +k=0.99999 +x_0=50000 +y_0=100000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs""))\r\nxydata3<-cbind(raw_sightings[,c(""Date"", ""observation_id"", ""dolphin_id"", ""year"")],trunc(xydata2,0))\r\ncolnames(xydata3)<-c(""Date"", ""observation_id"", ""dolphin_id"", ""year"",""X"",""Y"")\r\n\r\nyearly_xydata<-SpatialPointsDataFrame(xydata3[,c(""X"",""Y"")],xydata3[""year""])\r\n\r\nmcps<-mcp(yearly_xydata[,1], percent=100, unin=c(""m""), unout=c(""m2""))\r\n\r\n#plot(mcps, col=c(1:10), axes=TRUE)\r\n\r\nmcpolyset<-SpatialPolygons2PolySet(mcps)\r\nmcp_allyears<-joinPolys(mcpolyset, operation=""INT"")\r\nmcp_allyears<-PolySet2SpatialPolygons(mcp_allyears)\r\n#plot(mcp_allyears, add=TRUE, col=""green"")\r\n\r\n#add buffer to intersection\r\n\r\nbuffered<-raster::buffer(mcp_allyears, width=2500)\r\n#plot(buffered, axes=TRUE, col=""yellow"")\r\n\r\n#intersect buffered with hrxydata to get surveys that fall in that range\r\n\r\nsighting_xydata<-SpatialPointsDataFrame(xydata3[,c(""X"",""Y"")],xydata3[""observation_id""])\r\n\r\nbuff_surveys<-intersect(sighting_xydata, buffered)\r\n#plot(buff_surveys, add=TRUE)\r\n\r\nbuff_surveys<-as.data.frame(buff_surveys)\r\n\r\nfs<-subset(xydata3, xydata3$observation_id %in% buff_surveys$observation_id)\r\n\r\n#calculate number of sightings for each individual in set\r\n\r\nfs$sightings<-sapply(fs$dolphin_id, function(i) length(fs$dolphin_id[which(fs$dolphin_id==i)]))\r\n\r\n#filter based on number of sightings\r\n\r\nfs45<-subset(fs, fs$sightings>=min_sightings)\r\n\r\n#Calculate the real HWI for this data set\r\n\r\navailability <- read.csv(""dryad_dolphin_availability.csv"")\r\navailability$entry<-as.Date(availability$entry, format=c(""%d-%b-%Y""))\r\navailability$depart<-as.Date(availability$depart, format=c(""%d-%b-%Y""))\r\n\r\nrealHWI<-hwi_filtered(sightings=fs45, group_variable=""observation_id"", dates=""Date"", IDs=""dolphin_id"", symmetric=FALSE, availability=availability)\r\n\r\n#Construct home ranges for each animal\r\n\r\n#Create grid with 5km buffer\r\n\r\ngrid_buffer=5000\r\n\r\nx <- seq(min(fs45[,""X""])-grid_buffer,max(fs45[,""X""])+grid_buffer,by=100) # where resolution is the pixel size you desire\r\ny <- seq(min(fs45[,""Y""])-grid_buffer,max(fs45[,""Y""])+grid_buffer,by=100)\r\n\r\nxy <- expand.grid(x=x,y=y)\r\ncoordinates(xy) <- ~x+y\r\ngridded(xy) <- TRUE\r\n\r\n#Create UDs for each animal and extract h values (need to manually select h for boundary method)\r\n\r\nhrxydata<-SpatialPointsDataFrame(fs45[,c(""X"",""Y"")],fs45[""dolphin_id""])\r\n\r\nuds_href<-kernelUD(hrxydata[,1],grid=xy)\r\n\r\nhvalues<-list()\r\nfor (i in 1:length(uds_href)) {\r\n  h<-uds_href[[i]]@h$h\r\n  id<-names(uds_href)[[i]]\r\n  hvalues[[i]]<-c(h, id)\r\n}\r\n\r\nh<-as.data.frame(do.call(""rbind"", hvalues), stringsAsFactors=FALSE)\r\n\r\nnames(h)<-c(""h_opt"", ""dolphin_id"")\r\n\r\nh$h_opt<-as.numeric(h$h_opt)\r\n\r\n#Create simplified coastline, length of segments must be greater than 3*h\r\nbound <- structure(list(x = c(122000,122000,116500,110000,108000), y = c(1000,10500,14500,20800,31280)), .Names = c(""x"", ""y""))\r\nbound <- do.call(""cbind"",bound)\r\nSlo1 <- Line(bound)\r\nSli1 <- Lines(list(Slo1), ID=""frontier1"")\r\nbarrier <- SpatialLines(list(Sli1))\r\n\r\noptud<-list()\r\n\r\n#Filter out unnecesary parts from loop, extract polygons by ID\r\n\r\nfor (i in 1:dim(h)[1]){\r\n\r\n  cdol<-hrxydata[hrxydata$dolphin_id==h$dolphin_id[i],]\r\n  hopt<-h$h_opt[i]\r\n  uds_man<-kernelUD(cdol,h=hopt,grid=xy, boundary=barrier)\r\n  optud[[i]]<-uds_man\r\n  cat(i)\r\n}\r\n\r\nuddf<-unlist(optud)\r\nclass(uddf)<-""estUDm""\r\n\r\n\r\n#remove land from new estimates\r\n\r\ncoast_polygon<-readOGR(""coastpolygon"", ""coastpolygon"")\r\n\r\ncoast_polygon<-spTransform(coast_polygon, CRS(""+proj=tmerc +lat_0=-25 +lon_0=113 +k=0.99999 +x_0=50000 +y_0=100000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs""))\r\n\r\n#plot(coast, axes=TRUE, lwd=2);plot(kernelcontours90, add=TRUE, col=""blue"");plot(coast_polygon, add=TRUE, lwd=2)\r\n\r\nudsgdf <- as(estUDm2spixdf(uddf),""SpatialGridDataFrame"")\r\n\r\n#use coast polygon as mask for spatial grid\r\n\r\nrgrid <- raster(udsgdf)\r\n#plot(rgrid)\r\nrgrid_msk <- mask(rgrid,coast_polygon, inverse=TRUE)\r\n#plot(rgrid_msk)\r\n\r\n#convert back to sgdf\r\n\r\ngrid_ae <- as(rgrid_msk, \'SpatialGridDataFrame\')\r\ngridded(grid_ae) <- TRUE\r\ngrid_ae[[1]] <- as.numeric(!is.na(grid_ae[[1]])) \r\n\r\n## Then, you just have to multiply each column of udsgdf by the mask \r\nresu <- lapply(1:ncol(udsgdf), function(i) { udsgdf[[i]] * grid_ae[[1]] }) \r\n#Alternatively, you can re-standardize\r\n\r\nresu <- lapply(1:ncol(udsgdf), function(i) {udsgdf[[i]] * grid_ae[[1]] / sum(udsgdf[[i]] * grid_ae[[1]]) })\r\n\r\nresu <- as.data.frame(resu) \r\nnames(resu) <- names(udsgdf@data) \r\n\r\n## and define it as data slot for udsgdf \r\nudsgdf@data <- resu \r\n\r\n#Will also need to do this for contours using the masked grid we just creasted \r\n\r\nmasked_grid<-udsgdf\r\n\r\nfullgrid(masked_grid) <- FALSE \r\nre <- lapply(1:ncol(masked_grid), function(i) { \r\n  so <- new(""estUD"", masked_grid[,i]) \r\n  so@h <- list(h=0, meth=""specified"") # fake value \r\n  so@vol <- FALSE  \r\n  return(so) \r\n}) \r\nnames(re) <- names(masked_grid) \r\nclass(re) <- ""estUDm"" \r\n\r\n#Create daily minimum convex polygons with 1km buffers, always including launch point as one of the vertices.\r\n\r\ngroupings<-unique(fs45[,c(""Date"", ""observation_id"", ""X"", ""Y"")])\r\n\r\ndays<-split(groupings, groupings$Date)\r\n\r\n#Add a few points near to launch area to create mcp\r\n\r\nlaunch<-c(""2001-01-01"",""launch"",122241,11966)\r\n\r\nlaunch<-as.data.frame(t(launch))\r\n\r\nnames(launch)<-names(days[[1]])    \r\n\r\nlaunch[2,]<-c(""2001-01-01"",""launch"",122241,11965)\r\nlaunch[3,]<-c(""2001-01-01"",""launch"",122241,11964)\r\nlaunch[4,]<-c(""2001-01-01"",""launch"",122241,11963)\r\n\r\nlaunch[,1]<-as.Date(launch[,1])\r\n\r\ndays1<-lapply(days, function(x) add_launch(x))\r\n\r\nsurvey_days<-as.data.frame(do.call(""rbind"", days1))\r\n\r\nsurvey_days[,c(""X"",""Y"")]<-apply(survey_days[,c(""X"",""Y"")],2, as.numeric)\r\n\r\ndaily_xydata<-SpatialPointsDataFrame(survey_days[,c(""X"",""Y"")],survey_days[""Date""])\r\n\r\nmcps<-mcp(daily_xydata[,1], percent=100, unin=c(""m""), unout=c(""m2""))\r\n\r\n#Add buffer, make sure whole area is covered\r\n\r\nbuff_days<-gBuffer(mcps, byid=TRUE,width=1000)\r\n\r\n#Number of animals in study\r\nn<-length(unique(fs45$dolphin_id))\r\n\r\n#Number of survey days\r\nd<-length(unique(fs45$Date))\r\ndates<-unique(fs45$Date)\r\n\r\n#Get availability matrix\r\n\r\ndolphins<-sort(unique(fs45$dolphin_id))\r\n\r\nmatnames<-list(dates,dolphins)\r\n\r\nalive<-Vectorize(FUN=function(r,c) isTRUE(r>=availability$entry[which(availability$dolphin_id==c)] & r<=availability$depart[which(availability$dolphin_id==c)]))\r\n\r\nschedule<-outer(dates, dolphins, FUN=alive)\r\ndimnames(schedule)<-matnames\r\n\r\ndolphin_density_per_km<-dim(fs45)[1]/(sum(area(buff_days))/1000000)\r\n\r\n#################Ok now set up gprox testing?\r\nGprox_set<-sort(rep(seq(600, 1400, by=100),10))\r\n\r\ndigidolph_gprox<-list()\r\n\r\nfor (i in 1:length(Gprox_set)){\r\ndaily_sim<- one_sim(d=d,\r\n            buff_days=buff_days, \r\n            udsgdf=udsgdf, \r\n            dolphin_density_per_km=dolphin_density_per_km, \r\n            schedule=schedule, \r\n            gprox=Gprox_set[i])\r\n\r\nAssoctable<-do.call(""rbind"",daily_sim)\r\n\r\nAssoctable<-digu(Assoctable)\r\n\r\ndigidolph_gprox[[i]]<-Assoctable\r\n}\r\n\r\nrand_mats<-lapply(digidolph_gprox,function(x) hwi_filtered(sightings=x,group_variable=""Group"", dates=""Permutation"", IDs=""IDs"", symmetric=TRUE, availability=availability))\r\nrm<-mergeMatrices(rand_mats)\r\n\r\n#check output to determine optimum gprox\r\n\r\n#set optimum gprox\r\n\r\ngprox<-gprox_opt\r\n\r\ndigidolph<-list()\r\n\r\nfor (k in 1:num_sim){\r\n  daily_sim<- one_sim(d=d,\r\n                      buff_days=buff_days, \r\n                      udsgdf=udsgdf, \r\n                      dolphin_density_per_km=dolphin_density_per_km, \r\n                      schedule=schedule, \r\n                      gprox=gprox)\r\n  \r\n  Assoctable<-do.call(""rbind"",daily_sim)\r\n  \r\n  Assoctable<-digu(Assoctable)\r\n  \r\n  digidolph[[k]]<-Assoctable\r\n}\r\n\r\nrand_mats<-lapply(digidolph,function(x) hwi_filtered(sightings=x,group_variable=""Group"", dates=""Permutation"", IDs=""IDs"", symmetric=TRUE, availability=availability))\r\nrm<-mergeMatrices(rand_mats)\r\n\r\nrealh<-as.data.frame(na.omit(unmatrix(realHWI)))\r\nnames(realh)<-""realHWI""\r\n\r\n#Merge real association index with random values\r\n\r\noutput<-merge(rm, realh, by=""row.names"") \r\n\r\n\r\n\r\n\r\n', '#Digidolph helper functions\r\n#Vivienne Foroughirad\r\n#vjf2@duke.edu\r\n#Created August 8th, 2016\r\n#Modified December 8, 2016 \r\n\r\n\r\n#Creates unique observation IDs for Digiroo2::fAssoctable output\r\n\r\ndigu<-function(x){x$Group<-paste0(x$Permutation,""-"", x$Group);return(x)}\r\n\r\n#Convert association matrices to linear format and merge into dataframe\r\n\r\nmergeMatrices<-function(lmat) {\r\n  rands<-lapply(lmat, function(mat) mat<-na.omit(unmatrix(mat)))\r\n  rands<-lapply(rands, function(x) x<-x[order(names(x))])\r\n  rands<-as.data.frame(do.call(""cbind"",rands))\r\n  return(rands)\r\n}\r\n\r\n#This feature adds the launch point and extra points so a sampling area can always be calculated\r\n\r\nadd_launch<-function(x) {\r\n  launch[1]<-x[1,1]\r\n  x<-rbind(x, launch)\r\n  return(x)\r\n}\r\n\r\n#Calculates the half-weight index with a one-day sampling period\r\n#Filters sightings for joint availabilty for each pair\r\n#Takes an availability dataframe with ID, entry date, and depart date\r\n\r\nhwi_filtered<-function(sightings=sightings, group_variable=group_variable, dates=dates, IDs=IDs, symmetric=TRUE, availability=availability){\r\n  z<-sightings\r\n  dolphins<-sort(unique(z[,IDs]))\r\n  n<-length(dolphins)\r\n  matnames<-list(dolphins,dolphins)\r\n  currentmat<-matrix(c(rep(NA,n^2)),nrow=n,dimnames=matnames)\r\n  currentdflist<-split(z, z[,IDs],drop=TRUE)\r\n  for (i in 1:nrow(currentmat)) {\r\n    ego<-row.names(currentmat)[i]\r\n    #Get the list element for each dolphin\r\n    ego_start<-availability$entry[availability$dolphin_id==ego]\r\n    ego_end<-availability$depart[availability$dolphin_id==ego]\r\n    \r\n    #all_ego<-get(ego, currentdflist)\r\n    for (j in i:ncol(currentmat)) {\r\n      alter<-colnames(currentmat)[j]\r\n      #Get the list element for each dolphin\r\n      \r\n      alter_start<-availability$entry[availability$dolphin_id==alter]\r\n      alter_end<-availability$depart[availability$dolphin_id==alter]\r\n      \r\n      all_ego<-get(ego, currentdflist)[,c(dates,group_variable)]\r\n      all_alter<-get(alter, currentdflist)[,c(dates,group_variable)]\r\n      \r\n      hstart<-max(ego_start, alter_start)\r\n      hend<-min(alter_end, ego_end)\r\n      tp<-hend-hstart\r\n      if(tp<1){currentmat[i,j]<-NA} else {\r\n        \r\n        all_ego<-subset(all_ego, all_ego[,dates]>=hstart & all_ego[,dates]<=hend)\r\n        all_alter<-subset(all_alter, all_alter[,dates]>=hstart & all_alter[,dates]<=hend)    \r\n        \r\n        #Take the intersection of the partycomp_dolphins to see when the dolphins were in the same group_variable\r\n        set<-(intersect(all_ego[,group_variable], all_alter[,group_variable]))\r\n        sample<-subset(all_ego, all_ego[,group_variable] %in% set)\r\n        #Numerator for HWC\r\n        X<-length(unique(sample[,dates]))\r\n        if (X>0){\r\n          \r\n          X<-length(unique(all_ego[,dates][all_ego[,group_variable] %in% set]))\r\n          #Ego without alter\r\n          Ya<-length(setdiff(all_ego[,dates], all_alter[,dates]))\r\n          #Alter without ego\r\n          Yb<-length(setdiff(all_alter[,dates], all_ego[,dates]))\r\n          #Both seen but not together\r\n          Yab<-length(intersect(all_ego[,dates], all_alter[,dates]))-X\r\n          #Half weight coefficient\r\n          HWC<-(X/(X+0.5*(Ya+Yb)+Yab))\r\n          currentmat[i,j]<-HWC}\r\n        else{currentmat[i,j]<-0}\r\n      }}\r\n  }\r\n  diag(currentmat)<-NA\r\n  if(symmetric==TRUE){\r\n    currentmat[lower.tri(currentmat)]=t(currentmat)[lower.tri(currentmat)]\r\n  }\r\n  return(currentmat)\r\n}\r\n\r\n#This function returns one set of simulations for the whole study period\r\n\r\none_sim<-function(d=d,buff_days=buff_days, udsgdf=udsgdf, \r\n                  dolphin_density_per_km=dolphin_density_per_km, \r\n                  schedule=schedule, \r\n                  gprox=gprox){\r\n  \r\n  for (i in 1:d){\r\n    bound<-buff_days[i,]\r\n    areakm<-area(bound)/1000000\r\n    nd<-round(areakm*dolphin_density_per_km)\r\n    nd<-ifelse(nd==1, nd<-2, nd) \r\n    dailygrid<-udsgdf\r\n    rgrid <- raster(dailygrid)\r\n    rgrid_msk <- mask(rgrid,bound, inverse=FALSE)\r\n    \r\n    #convert back to sgdf\r\n    \r\n    grid_ae <- as(rgrid_msk, \'SpatialGridDataFrame\')\r\n    gridded(grid_ae) <- TRUE\r\n    grid_ae[[1]] <- as.numeric(!is.na(grid_ae[[1]])) \r\n    \r\n    ## Then, you just have to multiply each column of udsgdf by the mask \r\n    resu <- lapply(1:ncol(dailygrid), function(i) { \r\n      dailygrid[[i]] * grid_ae[[1]] }) \r\n    resu <- as.data.frame(resu) \r\n    names(resu) <- names(dailygrid@data) \r\n    \r\n    ## and define it as data slot for udsgdf \r\n    dailygrid@data <- resu \r\n    \r\n    probweights<-colSums(dailygrid@data)\r\n    \r\n    daily_dolphins<-sample(names(probweights)[which(schedule[i,]==TRUE)],size=nd,replace=FALSE,prob=probweights[which(schedule[i,]==TRUE)])\r\n    \r\n    i_dd<-which(colnames(dailygrid@data) %in% daily_dolphins)\r\n    \r\n    rp<-fRanXY(i_dd, dailygrid)\r\n    \r\n    coordinates(rp) <- ~x+y\r\n    \r\n    dnn_digi <- dnearneigh(rp,0,gprox,row.names=as.character(rp$ID))\r\n    \r\n    dayAssoc<-as.data.frame(fAssoctable(dnn_digi))\r\n    dayAssoc$Permutation<-c(rep(as.character(buff_days$id[i]),nd))\r\n    dayAssoc$Permutation<-as.Date(dayAssoc$Permutation, format=c(""%Y-%m-%d""))\r\n    each_days_assoc[[i]]<-dayAssoc\r\n  }\r\n  return(each_days_assoc)\r\n}\r\n\r\n', '##creating a spatially explicit null model for eastern water dragon data \r\n#K.Strickland\r\n\r\nlibrary(adehabitatHR)\r\nlibrary(maptools)\r\nlibrary(rgdal)\r\nlibrary(spatstat)\r\n#library(devtools) # only need for install\r\n#install_git(""git://github.com/gsk3/taRifx.geo.git"")\r\nlibrary(taRifx.geo)\r\nlibrary(Digiroo2)\r\nlibrary(coda)\r\nlibrary(spdep)\r\nlibrary(gtools)\r\nlibrary(gdata)\r\nlibrary(raster)\r\n\r\nsource(""hwi_functions.R"")\r\n\r\n#### read in data and create home ranges; subset to minimum sightings required to create stable home range ##\r\n\r\nHRdata<- read.csv(""dragon_data2.csv"",stringsAsFactors = FALSE)\r\nfs25<-subset(HRdata,HRdata$Sightings>=25)\r\nxydata<-cbind(fs25$X,fs25$Y)\r\nxydata2<-as.data.frame(project(xydata, ""+proj=tmerc +lat_0=-28 +lon_0=153 +k=0.99999 +x_0=50000 +y_0=100000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs""))\r\nxydata3<-cbind(fs25$Name,xydata2)\r\ncolnames(xydata3)<-c(""Name"",""X"",""Y"")\r\n\r\nx <- seq(min(xydata3[,""X""])-20,max(xydata3[,""X""])+20,by=1) # where resolution is the pixel size you desire\r\ny <- seq(min(xydata3[,""Y""])-20,max(xydata3[,""Y""])+20,by=1)\r\nxy <- expand.grid(x=x,y=y)\r\ncoordinates(xy) <- ~x+y\r\ngridded(xy) <- TRUE\r\nclass(xy)\r\n\r\nhrxydata<-SpatialPointsDataFrame(xydata3[,2:3],xydata3[""Name""])\r\nuds7<-kernelUD(hrxydata[,1],h= 7,grid=xy) \r\n\r\nudsgdf <- as(estUDm2spixdf(uds7),""SpatialGridDataFrame"")\r\n\r\n## real half-weights\r\n\r\nrealHWI<-hwi(fs25, group_variable=""observation_id"", dates=""Date"", IDs=""Name"")\r\nrHWI<-unmatrix(realHWI)\r\n\r\n## create probability tables for use in simulations\r\n\r\n#Number of animals in study\r\nn<-length(unique(fs25$Name))\r\n\r\n#Number of survey days\r\nd<-length(unique(fs25$Date))\r\n\r\nID_counts <- tapply(rep(1,length(fs25$Name)),fs25$Name,sum)\r\nNo_SurveyDays <- d\r\npObs <- as.vector(ID_counts)/No_SurveyDays # proportion of time individuals are sighted in surveys\r\nnameObs <- names(ID_counts)\r\nTotalAnimals <- n # Total number of animals in study\r\nsamplesize <- round(dim(fs25)/d) # Number of IDs to include in simulations (with replacement) - average number of ids per survey in the observed data\r\nbootlength <- d # Number of permutations (equal to number of survey days)\r\nExpProb <- data.frame(Kangaroo=nameObs,Probability=pObs) # Table of Expected Probabilities\r\n\r\n########### RUN model to assign the gprox value\r\n\r\n# set distances you wish to test\r\n\r\nGprox_full<-sort(rep(seq(1,10,by=1),10))\r\n\r\nfullgprox<-list()\r\n\r\nfor (k in 1:length(Gprox_full)) {\r\n  \r\n  Gprox<-Gprox_full[[k]]\r\n  pID <- sapply(1:bootlength,function(i) sample(x=TotalAnimals,size=samplesize,replace=FALSE,prob=pObs))\r\n  result_full<- fAssocmatrix(sPerm=1:bootlength,Gprox=Gprox,iextract=udsgdf,iID=pID)\r\n  fullgprox[[k]]<-result_full\r\n  \r\n  #verbose\r\n  cat(k)\r\n  \r\n}\r\n\r\nfull<-lapply(fullgprox, digu)\r\nrand_mats<-lapply(full,function(x) hwi(sightings=x,group_variable=""Group"", dates=""Permutation"", IDs=""IDs"",symmetric = TRUE))\r\nran_m<-mergeMatrices(rand_mats)\r\nrm<-lapply(ran_m,function(x) mean(x))\r\n\r\nrHWI<-na.exclude(rHWI)\r\nob_mean<-mean(rHWI)\r\n\r\n##check output to determine optimum gprox and use the gprox value closest to observed mean (ob_mean)\r\n\r\n## set optimum gprox (e.g. 5m for dragons)\r\n\r\nGprox <- 5\r\n\r\n########### RUN FULL PERMUTATIONS\r\n\r\n\r\nfull<-list()\r\n\r\n for (k in 1:1000) {\r\n  \r\n  pID <- sapply(1:bootlength,function(i) sample(x=TotalAnimals,size=samplesize,replace=FALSE,prob=pObs))\r\n  result_full<- fAssocmatrix(sPerm=1:bootlength,Gprox=Gprox,iextract=udsgdf,iID=pID)\r\n  full[[k]]<-result_full\r\n  #verbose\r\n  cat(k)\r\n  \r\n}\r\n\r\nfull<-lapply(full, digu)\r\nrand_mats<-lapply(full,function(x) hwi(sightings=x,group_variable=""Group"", dates=""Permutation"", IDs=""IDs"",symmetric = TRUE))\r\nfull_mats<-list()\r\n\r\nfor (i in 1:length(rand_mats)){\r\n  \r\n  rand_mats[[i]]->mat\r\n  unmat<-data.frame(unmatrix(mat))\r\n  IDs<-row.names(unmat)\r\n  unmat<-data.frame(cbind(IDs,unmat$unmatrix.mat.))\r\n  colnames(unmat)<-c(""ID"",""HWI"")\r\n  mat_f<-unmat[order(unmat$ID),]\r\n  full_mats[[i]]<-mat_f\r\n  \r\n  #verbose\r\n  \r\n  cat(i)\r\n  \r\n}\r\n\r\nfinal<-do.call(""cbind"",full_mats)\r\nfinalf<-final[ , -which(names(final) %in% c(""ID""))]\r\nfinal_f<-cbind(final[,1],finalf)\r\nrownames(final_f)<-final_f$`final[, 1]`\r\n#Merge real association index with random values\r\n\r\noutput<-merge(final_f, rHWI, by=""row.names"") \r\n', 'hwi<-function(sightings=sightings, group_variable=group_variable, dates=dates, IDs=IDs, symmetric=TRUE){\r\n    z<-sightings\r\n    dolphins<-sort(unique(z[,IDs]))\r\n    n<-length(dolphins)\r\n    matnames<-list(dolphins,dolphins)\r\n    currentmat<-matrix(c(rep(NA,n^2)),nrow=n,dimnames=matnames)\r\n    currentdflist<-split(z, z[,IDs],drop=TRUE)\r\n    for (i in 1:nrow(currentmat)) {\r\n      ego<-row.names(currentmat)[i]\r\n      #Get the list element for each dolphin\r\n      all_ego<-get(ego, currentdflist)\r\n      for (j in i:ncol(currentmat)) {\r\n        alter<-colnames(currentmat)[j]\r\n        #Get the list element for each dolphin\r\n        all_alter<-get(alter, currentdflist)\r\n        #Take the intersection of the partycomp_dolphins to see when the dolphins were in the same group_variable\r\n        set<-(intersect(all_ego[,group_variable], all_alter[,group_variable]))\r\n        sample<-subset(all_ego, all_ego[,group_variable] %in% set)\r\n        #Numerator for HWC\r\n        X<-length(unique(sample[,dates]))\r\n        if (X>0){\r\n          \r\n          X<-length(unique(all_ego[,dates][all_ego[,group_variable] %in% set]))\r\n          #Ego without alter\r\n          Ya<-length(setdiff(all_ego[,dates], all_alter[,dates]))\r\n          #Alter without ego\r\n          Yb<-length(setdiff(all_alter[,dates], all_ego[,dates]))\r\n          #Both seen but not together\r\n          Yab<-length(intersect(all_ego[,dates], all_alter[,dates]))-X\r\n          #Half weight coefficient\r\n          HWC<-(X/(X+Ya+Yab))\r\n          currentmat[i,j]<-HWC}\r\n        else{currentmat[i,j]<-0}\r\n      }\r\n    }\r\n    diag(currentmat)<-NA\r\n    if(symmetric==TRUE){\r\n    currentmat[lower.tri(currentmat)]=t(currentmat)[lower.tri(currentmat)]\r\n    }\r\n    return(currentmat)\r\n}\r\n\r\ndigu<-function(x){x$Group<-paste0(x$Permutation,""-"", x$Group);return(x)}\r\n\r\nmergeMatrices<-function(lmat) {\r\n  rands<-lapply(lmat, function(mat) mat<-na.omit(unmatrix(mat)))\r\n  rands<-lapply(rands, function(x) x<-x[order(names(x))])\r\n  rands<-as.data.frame(do.call(""cbind"",rands))\r\n  return(rands)\r\n  }\r\n\r\n\r\nsample_sightings<-function(x,nums){\r\n  s=sample(row.names(x), nums, replace=FALSE) \r\n  y=subset(x, row.names(x) %in% s, drop=TRUE) \r\n  return(y)\r\n}\r\n\r\n']","Data from: A framework for the identification of long-term social avoidance in longitudinal datasets Animal sociality is of significant interest to evolutionary and behavioural ecologists, with efforts focused on the patterns, causes and fitness outcomes of social preference. However, individual social patterns are the consequence of both attraction to (preference for) and avoidance of conspecifics. Despite this, social avoidance has received far less attention than social preference. Here, we detail the necessary steps to generate a spatially explicit, iterative null model which can be used to identify non-random social avoidance in longitudinal studies of social animals. We specifically identify and detail parameters which will influence the validity of the model. To test the usability of this model, we applied it to two longitudinal studies of social animals (Eastern water dragons (Intellegama leseurii) and bottlenose dolphins (Tursiops aduncus) to identify the presence of social avoidances. Using this model allowed us to identify the presence of social avoidances in both species. We hope that the framework presented here inspires interest in addressing this critical gap in our understanding of animal sociality, in turn allowing for a more holistic understanding of social interactions, relationships and structure.",2
Data from: Social learning in otters,"The use of information provided by others to tackle life's challenges is widespread, but should not be employed indiscriminately if it is to be adaptive. Evidence is accumulating that animals are indeed selective and adopt 'social learning strategies'. However, studies have generally focused on fish, bird and primate species. Here we extend research on social learning strategies to a taxonomic group that has been neglected until now: otters (subfamily Lutrinae). We collected social association data on captive groups of two gregarious species: smooth-coated otters (Lutrogale perspicillata), known to hunt fish cooperatively in the wild, and Asian short-clawed otters (Aonyx cinereus), which feed individually on prey requiring extractive foraging behaviours. We then presented otter groups with a series of novel foraging tasks, and inferred social transmission of task solutions with network-based diffusion analysis. We show that smooth-coated otters can socially learn how to exploit novel food sources and may adopt a 'copy when young' strategy. We found no evidence for social learning in the Asian short-clawed otters. Otters are thus a promising model system for comparative research into social learning strategies, while conservation reintroduction programmes may benefit from facilitating the social transmission of survival skills in these vulnerable species.","['#This code illustrates how the analysis was run for the smooth coated otters\n\n#Install required packages\n#choose a mirror\nchooseCRANmirror()\n#Install\ninstall.packages(""survival"", dependencies=T)\ninstall.packages(""combinat"", dependencies=T)\n\n#Load in the code for running NBDA available at https://lalandlab.st-andrews.ac.uk/freeware/\n\n#Input the association matrix for smooth coated otters\namg1<-matrix(data=c(0.0,\t0.48,\t0.37,\t0.11, 0.32, 0.36, 0.36, 0.48, 0.00,\t0.38,\t0.07,\t0.42,\t0.36,\t0.35, 0.37, 0.38, 0.00, 0.12, 0.41, 0.35, 0.32, 0.11, 0.07, 0.12, 0.00, 0.12, 0.09, 0.08, 0.32, 0.42, 0.41, 0.12, 0.00, 0.48, 0.48, 0.36, 0.36, 0.35, 0.09, 0.48, 0.00, 0.73, 0.36, 0.35, 0.32, 0.08, 0.48, 0.73, 0.00), nrow=7)\n\n#Input the order of acquisition for the 6 tasks\noag1t1<-c(6,7,2,3,5,1,4)\noag1t2<-c(7,6,5,3)\noag1t3<-c(3,7,5,6,4)\r\noag1t4<-c(7,6,5,3,4)\r\noag1t5<-c(5,4,1,6,3,7)\r\noag1t6<-c(5,3,7,4,6,1)\n\n#Input the individual-level variables for each otter\nsex<-c(1,0,1,1,1,0,0)\r\nage<-c(7,7,2,2,2,1,1)\nparent<-c(1,1,0,0,0,0,0)\nasoc<-cbind(sex,age,parent)\n\n#Create the oadata objects\nng1t1 <-oaData(assMatrix=amg1, asoc=asoc, orderAcq=oag1t1, groupid=""1"", taskid=""1"",id=c(""Mum"",""Dad"",""Sis1"",""Sis2"",""Sis3"",""Bro1"",""Bro2""))\nng1t2 <-oaData(assMatrix=amg1, asoc=asoc, orderAcq=oag1t2, groupid=""1"", taskid=""2"",id=c(""Mum"",""Dad"",""Sis1"",""Sis2"",""Sis3"",""Bro1"",""Bro2""))\r\nng1t3 <-oaData(assMatrix=amg1, asoc=asoc, orderAcq=oag1t3, groupid=""1"", taskid=""3"",id=c(""Mum"",""Dad"",""Sis1"",""Sis2"",""Sis3"",""Bro1"",""Bro2""))\r\nng1t4 <-oaData(assMatrix=amg1, asoc=asoc, orderAcq=oag1t4, groupid=""1"", taskid=""4"",id=c(""Mum"",""Dad"",""Sis1"",""Sis2"",""Sis3"",""Bro1"",""Bro2""))\r\nng1t5 <-oaData(assMatrix=amg1, asoc=asoc, orderAcq=oag1t5, groupid=""1"", taskid=""5"",id=c(""Mum"",""Dad"",""Sis1"",""Sis2"",""Sis3"",""Bro1"",""Bro2""))\r\nng1t6 <-oaData(assMatrix=amg1, asoc=asoc, orderAcq=oag1t6, groupid=""1"", taskid=""6"",id=c(""Mum"",""Dad"",""Sis1"",""Sis2"",""Sis3"",""Bro1"",""Bro2""))\n\n#Set up a matrix determining what social learning models will be considered\n#Here we consider models with social learning equal in all tasks and different across all tasks\nsParamMatrix<-rbind(rep(1,6),1:6)\n\n#Run the AIC table function which fits all the combinations of models to be considered.\n#i.e. all combinations of individual level variables, asocial, additive and multiplicative models, and for both rows of the sParamMatrix given above\naicTable1<-aicTable(data=c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6""), asocialVar=1:3, task=F, group=F, sParamMatrix= sParamMatrix, aic=""aicc"", pure=F)\naicTable1<-aicTable1[order(as.numeric(aicTable1[,7])),]\naicTable1[,8]<-as.numeric(aicTable1[,7])-as.numeric(aicTable1[1,7])\naicTable1\n\n#Refit the best model\nbestModel<-multiCoxFit(c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6""), formula=~.+parent)\nsummary(bestModel)\n\n#Do random effects make much difference?\nbestModelRE<-multiCoxFit(c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6""), formula=~.+parent+frailty(id))\nsummary(bestModelRE)\nbestModelRE@coef\n\n#Fit a model with differences in social learning rate (s) among tasks\ndiffSLModel<-multiCoxFit(c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6""),sParam=1:6, formula=~.+parent)\nsummary(diffSLModel)\n\n#To test for an overall difference in s among tasks- take 2* the difference in logLik for with Social Transmission \n2*(37.456-35.246)\n\n#Then look this up in a Chi-sq null distribution\npchisq(4.42,5,lower.tail=F)\n\n#So there was little evidence of a differnce in social learning bewteen tasks (LRT: Chi-sq = 4.42; d.f.= 5; p = 0.490)\n\n#A function to plot the associations for Fig4\nplotAssociations<-function(data,lty=1, symbol=NULL, xlab=""Acquisition event"", ylab=""Total connection to informed individuals"",title=NULL,plotID=T,offset=c(0.1,0),xlim=NULL, ylim=NULL, titlePos=c(0,0)){\nif(class(data)==""oaData""){oadata<-data}\nif(class(data)==""taData""){oadata<-data@oadata}\n\nif(is.null(xlim)){xlim<-c(0,max(oadata@coxdata$time2)+0.5)}\n\n\tif(is.null(symbol)){\n\t\tplot(oadata@coxdata$time2,oadata@coxdata$stMetric,col=oadata@coxdata$status+1,xlab=xlab, ylab=ylab,main="""",xlim=xlim, ylim=ylim);\n\t\tpoints(oadata@coxdata$time2[oadata@coxdata$status==1],oadata@coxdata$stMetric[oadata@coxdata$status==1],col=2);\n\t\tlines(oadata@coxdata$time2[oadata@coxdata$status==1],oadata@coxdata$stMetric[oadata@coxdata$status==1],col=2, lty=lty);\t\t\n\t}else{\n\t\tplot(oadata@coxdata$time2,oadata@coxdata$stMetric,col=oadata@coxdata$status+1,pch=as.numeric(as.factor(oadata@coxdata[,6+symbol])),xlab=xlab, ylab=ylab, main="""",xlim=xlim, ylim=ylim);\n\t\tpoints(oadata@coxdata$time2[oadata@coxdata$status==1],oadata@coxdata$stMetric[oadata@coxdata$status==1],col=2,pch=as.numeric(as.factor(oadata@coxdata[,6+symbol]))[oadata@coxdata$status==1]);\n\t\tlines(oadata@coxdata$time2[oadata@coxdata$status==1],oadata@coxdata$stMetric[oadata@coxdata$status==1],col=2, lty=lty);\n\t}\n\tif(plotID){\n\t\t\ttext(oadata@coxdata$time2[oadata@coxdata$status==1]+offset[1],oadata@coxdata$stMetric[oadata@coxdata$status==1]+offset[2],col=2,labels=oadata@coxdata$id[oadata@coxdata$status==1]);\t\t\n\t}\n\ttext(x=titlePos[1],y=titlePos[2],labels=title)\n}\n\n\npar(mfrow=c(3,2), mar=c(4,4,0.5,0.5))\nplotAssociations(ng1t1,symbol=3,title=""a) Task 1"",titlePos=c(1.35,1.9), offset=c(0.4,0),xlab="""",ylab="""",xlim=c(1,7.5),ylim=c(0,2))\nplotAssociations(ng1t2,symbol=3,title=""b) Task 2"",titlePos=c(1.35,1.9), offset=c(0.4,0),xlab="""",ylab="""",xlim=c(1,7.5),ylim=c(0,2))\nplotAssociations(ng1t3,symbol=3,title=""c) Task 3"",titlePos=c(1.35,1.9), offset=c(0.4,0),xlab="""",xlim=c(1,7.5),ylim=c(0,2))\nplotAssociations(ng1t4,symbol=3,title=""d) Task 4"",titlePos=c(1.35,1.9), offset=c(0.4,0),xlab="""",ylab="""",xlim=c(1,7.5),ylim=c(0,2))\nplotAssociations(ng1t5,symbol=3,title=""e) Task 5"",titlePos=c(1.35,1.9), offset=c(0.4,0),ylab="""",xlim=c(1,7.5),ylim=c(0,2))\nplotAssociations(ng1t6,symbol=3,title=""f) Task 6"",titlePos=c(1.35,1.9), offset=c(0.4,0),ylab="""",xlim=c(1,7.5),ylim=c(0,2))\n\n#Now obtain confidence intervals using profile likelihood technique\noadata<-combineOaCoxData(c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6""))\n\nsVals<-logLik<-seq(0,10,0.1)\n\nfor(i in 1:length(sVals)){\n\tlogLik[i]<-multiCoxLikelihood(sVals[i],oadata, formula=~.+parent)\t\n}\n\nplot(sVals, logLik, type=""l"")\nabline(h= 37.456+1.92, lty=2)\n\nsVals<-logLik<-seq(0.4,0.5,0.001)\n#0.44 lower limit\n\n#we can see that no matter how high we push s it is still within the 95% CI\nmultiCoxLikelihood(9999999,oadata, formula=~.+parent)\n37.456+1.92\n[1] 37.46922\n> 37.456+1.92\n[1] 39.376\n> \n\n\n#Now we estimate the number of events that occured by social transmission (excliuding the innovator)\n\n\tobject<-oadata\n\tobject@mldata<-object@coxdata[object@coxdata$status==1,]\n\t\n\t\tnumber<-sum((37.428352 *object@mldata$stMetric)/(1+ 37.428352 *object@mldata$stMetric))\n\t\ttotal<-dim(object@mldata)[1]-6\n\nnumber/total\n\n#96.0% social transmission excluding innovator\n\n#lower limit of 95%CI [0.4-Inf]\n\n\tobject<-oadata\n\tobject@mldata<-object@coxdata[object@coxdata$status==1,]\n\t\n\t\tnumber<-sum((0.44 *object@mldata$stMetric)/(1+ 0.44 *object@mldata$stMetric))\n\t\ttotal<-dim(object@mldata)[1]-6\n\nnumber/total\n\n#25.7% social transmission\n\n\n#Read in AIC table to get SE\'s and MLE\'s for model averaging across models with equal s across tasks \naicTable<-read.csv(""AICsmoothInput.csv"", header=T)\n\nSE<-MLE<-matrix(NA,nrow=dim(aicTable)[1],ncol=4)\n\nfor(i in 1:dim(aicTable)[1]){\n\n\tilv<-c((!is.na(aicTable[i,2]))*1,(!is.na(aicTable[i,3]))*2,(!is.na(aicTable[i,4]))*3)\n\tilv<-ilv[ilv>0]\n\n\tif(aicTable[i,6]==""social""){\n\t\tif(aicTable[i,1]|is.na(aicTable[i,1]==""NA"")){\n\t\t\tmodel<-optim(par=rep(0,length(ilv)+1),addLikelihood,hessian=T, data=c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6""), bounded=F, asocialVar=ilv)\n\t\t\tse<-sqrt(diag(solve(model$hessian)))\n\t\t\tMLE[i,1]<-model$par[1]\n\t\t\tSE[i,1]<-se[1]\n\t\t\tMLE[i,ilv+1]<-model$par[-1]\n\t\t\tSE[i,ilv+1]<-se[-1]\n\t\t}\n\t\n\t}else{\n\t\t\tmodel<-optim(par=rep(0,length(ilv)),nulladdLikelihood,hessian=T, data=c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6""), bounded=F, asocialVar=ilv)\n\t\t\tse<-sqrt(diag(solve(model$hessian)))\n\t\t\tMLE[i,ilv+1]<-model$par\n\t\t\tSE[i,ilv+1]<-se\t\t\t\t\n\t}\n}\n\n#Fill in Multiplicative models\n\ni<-1\n\tilv<-c((!is.na(aicTable[i,2]))*1,(!is.na(aicTable[i,3]))*2,(!is.na(aicTable[i,4]))*3)\n\tilv<-ilv[ilv>0]\n\nmodel<-optim(par=0,multiCoxLikelihood,hessian=T, oadata=combineOaCoxData(c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6"")), bounded=F, formula=~.+parent)\nse<-sqrt(diag(solve(model$hessian)))\nMLE[i,1]<-model$par[1]\nSE[i,1]<-se[1]\n\nmodel<-multiCoxFit(c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6""), bounded=F, formula=~.+parent)\nse<-as.vector(model@coef[,3])\nmle<-as.vector(model@coef[,1])\nMLE[i,ilv+1]<-mle\nSE[i,ilv+1]<-se\t\n\n\ni<-2\n\tilv<-c((!is.na(aicTable[i,2]))*1,(!is.na(aicTable[i,3]))*2,(!is.na(aicTable[i,4]))*3)\n\tilv<-ilv[ilv>0]\n\nmodel<-optim(par=0,multiCoxLikelihood,hessian=T, oadata=combineOaCoxData(c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6"")), bounded=F, formula=~.+sex+parent)\nse<-sqrt(diag(solve(model$hessian)))\nMLE[i,1]<-model$par[1]\nSE[i,1]<-se[1]\n\nmodel<-multiCoxFit(c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6""), bounded=F, formula=~.+sex)\nse<-as.vector(model@coef[,3])\nmle<-as.vector(model@coef[,1])\nMLE[i,ilv+1]<-mle\nSE[i,ilv+1]<-se\t\n\ni<-10\n\tilv<-c((!is.na(aicTable[i,2]))*1,(!is.na(aicTable[i,3]))*2,(!is.na(aicTable[i,4]))*3)\n\tilv<-ilv[ilv>0]\n\nmodel<-optim(par=0,multiCoxLikelihood,hessian=T, oadata=combineOaCoxData(c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6"")), bounded=F, formula=~.+sex)\nse<-sqrt(diag(solve(model$hessian)))\nMLE[i,1]<-model$par[1]\nSE[i,1]<-se[1]\n\nmodel<-multiCoxFit(c(""ng1t1"", ""ng1t2"", ""ng1t3"", ""ng1t4"", ""ng1t5"", ""ng1t6""), bounded=F, formula=~.+sex)\nse<-as.vector(model@coef[,3])\nmle<-as.vector(model@coef[,1])\nMLE[i,ilv+1]<-mle\nSE[i,ilv+1]<-se\t\n\nwrite.csv(MLE,""MLEsmoothCut.csv"")\nwrite.csv(SE,""SEsmoothCut.csv"")\n']","Data from: Social learning in otters The use of information provided by others to tackle life's challenges is widespread, but should not be employed indiscriminately if it is to be adaptive. Evidence is accumulating that animals are indeed selective and adopt 'social learning strategies'. However, studies have generally focused on fish, bird and primate species. Here we extend research on social learning strategies to a taxonomic group that has been neglected until now: otters (subfamily Lutrinae). We collected social association data on captive groups of two gregarious species: smooth-coated otters (Lutrogale perspicillata), known to hunt fish cooperatively in the wild, and Asian short-clawed otters (Aonyx cinereus), which feed individually on prey requiring extractive foraging behaviours. We then presented otter groups with a series of novel foraging tasks, and inferred social transmission of task solutions with network-based diffusion analysis. We show that smooth-coated otters can socially learn how to exploit novel food sources and may adopt a 'copy when young' strategy. We found no evidence for social learning in the Asian short-clawed otters. Otters are thus a promising model system for comparative research into social learning strategies, while conservation reintroduction programmes may benefit from facilitating the social transmission of survival skills in these vulnerable species.",2
Data from: Viral infection causes sex-specific changes in fruit fly social aggregation behaviour,"Host behavioural changes following infection are common and could be important determinants of host behavioural competence to transmit pathogens. Identifying potential sources of variation in sickness behaviours is therefore central to our understanding of disease transmission. Here, we test how group social aggregation and individual locomotor activity vary between different genotypes of male and female fruit flies (Drosophila melanogaster) following septic infection with Drosophila C Virus. We find genetic-based variation in both locomotor activity and social aggregation but we did not detect an effect of DCV infection on fly activity or sleep patterns within the initial days following infection. However, DCV infection caused sex-specific effects on social aggregation, as male flies in most genetic backgrounds increased the distance to their nearest neighbour when infected. We discuss possible causes for these differences in the context of individual variation in immunity and their potential consequences for disease transmission.","['# Packages ----------------------------------------------------------------\nlibrary(lme4)\nlibrary(plyr)\nlibrary(ggplot2)\n\n# Body Length Data --------------------------------------------------------\nbodylength <- read.csv(""SA Body Lengths.csv"")\n\nbodylength$Line <- factor(bodylength$Line) # define fly line as a factor\n\nBL.summarySE <- function(data=NULL, # function that calculates mean and SE\n                         measurevar,\n                         groupvars=NULL,\n                         na.rm=FALSE,\n                         conf.interval=.95,\n                         .drop=TRUE){\n  \n  length2 <- function(x,\n                      na.rm=FALSE){\n    if (na.rm) sum(!is.na(x))\n    else length(x)\n  }\n  \n  BLmean <- ddply(bodylength,\n                  groupvars,\n                  .drop = .drop,\n                  .fun = function(xx,col){\n                    c(N = length2(xx[[col]],\n                                  na.rm=na.rm),\n                      mean = mean (xx[[col]],\n                                   na.rm=na.rm),\n                      sd = sd (xx[[col]],\n                               na.rm=na.rm)\n                    )\n                  },\n                  measurevar\n  )\n  BLmean <- rename(BLmean,\n                   c(""mean"" = measurevar))\n  \n  BLmean$se <- BLmean$sd/sqrt(BLmean$N)\n  \n  ciMult <- qt(conf.interval/2 + .5,\n               BLmean$N-1)\n  BLmean$ci <- BLmean$se * ciMult\n  \n  return(BLmean)\n}\n\n\nMean_BL.summarytable <- BL.summarySE(bodylength,  # calculate mean and SE for body length for each sex and genetic background combination\n                                     measurevar = ""Length.mm."",\n                                     groupvars = c(""Line"",\n                                                   ""Sex"")\n)\n\nMean_BL.summarytable$Line <-  factor(Mean_BL.summarytable$Line) # define genetic background as a factor\n\n\n# Graphing Body Length ----------------------------------------------------\nggplot(Mean_BL.summarytable,\n       aes(x=reorder(Line,\n                     Length.mm.),\n           y=Length.mm.)\n) +\n  geom_bar(position = ""dodge"",\n           stat = ""identity"",\n           size=.5,\n           color=""black"",\n           aes(fill=Sex)) +\n  geom_errorbar(aes(ymin = Length.mm.-se,\n                    ymax = Length.mm.+se),\n                position = position_dodge(.9),\n                width=.2) +\n  facet_grid(.~Sex) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 60,\n                                   hjust = 1),\n        text = element_text(size=20)\n  ) +\n  labs(x=Genetic~Background,\n       y=Body~Length~(mm))\n\n\n# Model body length -------------------------------------------------------\nmodel.BodyLength <- lm(Length.mm.~ Line * Sex,\n                       data=bodylength)\n\nsummary(model.BodyLength)\nanova(model.BodyLength)\n\n\n# ""--------------------"" ----------------------------------------------------\n# Social Aggregation Data -------------------------------------------------\n# Upload Social Aggregation Experiment Dataframe/packages and Annotate\n## Attach file ""Social Aggregation Measurements.csv"" as dataframe titled ""SA""\nSA <- read.csv(""Social Aggregation Measurements.csv"")\n\nSA$Line <- factor(SA$Line) # Define genetic background as a factor\n\n\nSA.summarySE <- function(data=NULL, # function to calculate mean and SE\n                         measurevar,\n                         groupvars=NULL,\n                         na.rm=FALSE,\n                         conf.interval=.95,\n                         .drop=TRUE){\n  \n  length2 <- function(x,\n                      na.rm=FALSE){\n    if (na.rm) sum(!is.na(x))\n    else length(x)\n  }\n  \n  SA_mean <- ddply(SA,\n                   groupvars,\n                   .drop = .drop,\n                   .fun = function(xx,col){\n                     c(N = length2(xx[[col]],\n                                   na.rm=na.rm),\n                       mean = mean (xx[[col]],\n                                    na.rm=na.rm),\n                       sd = sd (xx[[col]],\n                                na.rm=na.rm)\n                     )\n                   },\n                   measurevar\n  )\n  SA_mean <- rename(SA_mean,\n                    c(""mean"" = measurevar))\n  \n  SA_mean$se <- SA_mean$sd/sqrt(SA_mean$N)\n  \n  ciMult <- qt(conf.interval/2 + .5,\n               SA_mean$N-1)\n  SA_mean$ci <- SA_mean$se * ciMult\n  \n  return(SA_mean)\n}\n\n\n\nMedian_SAmm.summarytable <- SA.summarySE(SA, # Calculate Median Nearest Neighbour Distance in millimetres\n                                         measurevar = ""MedianNND.mm.1"",\n                                         groupvars = c(""Line"",\n                                                       ""Sex"",\n                                                       ""Infection"")\n)\n\nMedian_SAbl.summarytable <- SA.summarySE(SA, # Calculate Median Nearest Neighbour Distance in Body lengths\n                                         measurevar = ""MedianNND.BL.1"",\n                                         groupvars = c(""Line"",\n                                                       ""Sex"",\n                                                       ""Infection"")\n)\n\n\nMedian_SAmm.summarytable$Line <-  factor(Median_SAmm.summarytable$Line) # define genetic background as factor for aggregation in millimetres\nMedian_SAbl.summarytable$Line <-  factor(Median_SAbl.summarytable$Line) # define genetic background as factor for aggregation in body lengths\n\n# Graphing Social Aggregation (mm) -------------------------------------------------\n\nggplot(Median_SAmm.summarytable,\n       aes(x=reorder(Line,\n                     MedianNND.mm.1),\n           y=MedianNND.mm.1,\n           fill=Infection)\n) +\n  geom_bar(position = ""dodge"",\n           stat = ""identity"",\n           size=.5) +\n  geom_errorbar(aes(ymin = MedianNND.mm.1-se,\n                    ymax = MedianNND.mm.1+se),\n                position = position_dodge(.9),\n                width=.2) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 60,\n                                   hjust = 1),\n        text = element_text(size=20)\n  ) +\n  facet_wrap(~Sex) +\n  labs(x=Genetic~Background,\n       y=Median~NND~(mm)\n  )\n\n\n# model Social Aggregation (mm) -------------------------------------------\n\nmodel1.SocAggmm <- lm(MedianNND.mm.1 ~ Line * Sex * Infection,\n                      data=SA)\n\nsummary(model1.SocAggmm)\nanova(model1.SocAggmm)\n\n\n\n# Graphing Social Aggregation Body Lengths --------------------------------\n\nggplot(Median_SAbl.summarytable,\n       aes(x=reorder(Line,\n                     MedianNND.BL.1),\n           y=MedianNND.BL.1,\n           fill=Infection)\n) +\n  geom_bar(position = ""dodge"",\n           stat = ""identity"",\n           size=.5) +\n  geom_errorbar(aes(ymin = MedianNND.BL.1-se,\n                    ymax = MedianNND.BL.1+se),\n                position = position_dodge(.9),\n                width=.2) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 60,\n                                   hjust = 1),\n        text = element_text(size=20)\n  ) +\n  facet_wrap(~Sex) +\n  labs(x=Genetic~Background,\n       y=Median~NND~(Body~Lengths)\n  )\n\n\n# Model Social Aggregation ------------------------------------------------\nmodel1.SocAggBL <- lm(MedianNND.BL.1 ~ Sex * Line *Infection,\n                      data=SA)\n\nsummary(model1.SocAggBL)\nanova(model1.SocAggBL)\n\n\n# ""--------------------"" ----------------------------------------------------\n# DAM total Activity data profile -----------------------------------------\n## attach ""DAM_total.csv"" file and label dataframe: ""df""\ndf <- read.csv(""DAM_total.csv"")\n\ndf2 <- df[!(df$Time_mins > 5770),] # exclude time steps greater than 5770\ndf2$Individual <- factor(df2$Individual) # define individual as a factor\n# code to generate mean values --------------------------------------------------------------------\nDAM.summarySE <- function(data=NULL, # function to calculate mean and SE\n                          measurevar,\n                          groupvars=NULL,\n                          na.rm=FALSE,\n                          conf.interval=.95,\n                          .drop=TRUE){\n  \n  length2 <- function(x,\n                      na.rm=FALSE){\n    if (na.rm) sum(!is.na(x))\n    else length(x)\n  }\n  \n  DAM_mean <- ddply(df2,\n                    groupvars,\n                    .drop = .drop,\n                    .fun = function(xx,col){\n                      c(N = length2(xx[[col]],\n                                    na.rm=na.rm),\n                        mean = mean (xx[[col]],\n                                     na.rm=na.rm),\n                        sd = sd (xx[[col]],\n                                 na.rm=na.rm)\n                      )\n                    },\n                    measurevar\n  )\n  DAM_mean <- rename(DAM_mean,\n                     c(""mean"" = measurevar))\n  \n  DAM_mean$se <- DAM_mean$sd/sqrt(DAM_mean$N)\n  \n  ciMult <- qt(conf.interval/2 + .5,\n               DAM_mean$N-1)\n  DAM_mean$ci <- DAM_mean$se * ciMult\n  \n  return(DAM_mean)\n}\n\n\n\nDAM.summarytable <- DAM.summarySE(df2, # calculate mean and SE values for activity counts at each time point for each combination of sex, genetic background and infection status\n                                  measurevar = ""Activity_Counts"",\n                                  groupvars = c(""DGRP_Line"",\n                                                ""Sex"",\n                                                ""Infection"",\n                                                ""Time_mins"") \n                                  )\n\n\nDAM.summarytable2 <- DAM.summarytable[!(DAM.summarytable$Sex==""Null""),] # remove empty vials from dataframe\n\n# Plot of mean actuvuty counts -----------------------------------------------------\n\nggplot(DAM.summarytable2,\n       aes(x=Time_mins,\n           y=Activity_Counts)\n) +\n  geom_point(alpha=0.05) +\n  geom_smooth(alpha=0.7,\n              se=F,\n              aes(color=Infection)\n  ) +\n  facet_grid(Sex~DGRP_Line) +\n  theme_bw() +\n  ylim(0,15)\n\n\n# ""--------------------"" ----------------------------------------------------\n# Dataframe for Summary Statistics ----------------------------------------\n## Dataframe for summary statistics\n# attach file titled \'DAM Summary Statistics.csv\' as ""DAM""\nDAM <- read.csv(""DAM Summary Statistics.csv"")\n\nDAM <- na.omit(DAM) # Remove empty cells from dtaaframe\n\n# remove all values of Day except \'Sum\' which combines all four days\nDAM <- DAM[!(DAM$Day==""1""), ]\nDAM <- DAM[!(DAM$Day==""2""), ]\nDAM <- DAM[!(DAM$Day==""3""), ]\nDAM <- DAM[!(DAM$Day==""4""), ]\n\n\nDAM <- DAM[!(DAM$Alive==0), ] # Remove dead flies from experiment\nDAM <- DAM[!(DAM$Line==""Null""), ] # Remove empty slots from dataframe\nDAM <- DAM[!(DAM$Line==""Blank""), ] # Remove empty vials from dataframe\nDAM <- DAM[!(DAM$Awake.Activity == ""#DIV/0!""), ] # Remove flies with no activity from average activity when awake statistic\n\n\nDAM <- DAM[!(DAM$Total.Activity==0), ] # Remove flies that were not active\nDAM$Awake.Activity <- as.numeric(DAM$Awake.Activity) # Define average activity when awake as a numeric value\n\nDAM$Sex <- factor(DAM$Sex,\n                  levels=c(""Male"",\n                           ""Female""))\n\n# DAM summary statistics --------------------------------------------------\n# Graphing Total Activity ----------------------------------------------------------\n## MEanSE function\nTA.summarySE <- function(data=NULL, # Function to calculate mean and SE\n                         measurevar,\n                         groupvars=NULL,\n                         na.rm=FALSE,\n                         conf.interval=.95,\n                         .drop=TRUE){\n  \n  length2 <- function(x,\n                      na.rm=FALSE){\n    if (na.rm) sum(!is.na(x))\n    else length(x)\n  }\n  \n  DAM_mean <- ddply(DAM,\n                    groupvars,\n                    .drop = .drop,\n                    .fun = function(xx,col){\n                      c(N = length2(xx[[col]],\n                                    na.rm=na.rm),\n                        mean = mean (xx[[col]],\n                                     na.rm=na.rm),\n                        sd = sd (xx[[col]],\n                                 na.rm=na.rm)\n                      )\n                    },\n                    measurevar\n  )\n  DAM_mean <- rename(DAM_mean,\n                     c(""mean"" = measurevar))\n  \n  DAM_mean$se <- DAM_mean$sd/sqrt(DAM_mean$N)\n  \n  ciMult <- qt(conf.interval/2 + .5,\n               DAM_mean$N-1)\n  DAM_mean$ci <- DAM_mean$se * ciMult\n  \n  return(DAM_mean)\n}\nMean_TA.summarytable <- TA.summarySE(DAM, # calculate average total activity for each combination of sex, genetic background and ifnection treatment\n                                     measurevar = ""Total.Activity"",\n                                     groupvars = c(""Line"",\n                                                   ""Sex"",\n                                                   ""Infection_treatment"")\n)\n\nMean_TA.summarytable$Line <-  factor(Mean_TA.summarytable$Line) # Define genetic background as a factor\n\n\n\n\n# Graphing Total Activity Summary Stat ------------------------------------\nggplot(Mean_TA.summarytable,\n                         aes(x=reorder(Line,\n                                       Total.Activity),\n                             y=Total.Activity,\n                             fill=Infection_treatment)\n) +\n  geom_bar(position = ""dodge"",\n           stat = ""identity"",\n           size=.5,\n           color=""black"") +\n  geom_errorbar(aes(ymin = Total.Activity-se,\n                    ymax = Total.Activity+se),\n                position = position_dodge(.9)\n                ) +\n  facet_grid(.~Sex) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 60,\n                                   hjust = 1),\n        text = element_text(size=20)\n  ) +\n  labs(x=Genetic~Background,\n       y=Total~Activity)\n\n\n\n# model of total activity -------------------------------------------------\nmodel1.TotalActivity <- lm(log(Total.Activity+1) ~ Line*Sex*Infection_treatment,\n            data=DAM)\nsummary(model1.TotalActivity)\nanova(model1.TotalActivity)\n\n\n# Graphing Proportion of time Awake  ----------------------------------------------------------------------\n## MEanSE\nPA.summarySE <- function(data=NULL, # function to calculate mean and SE for proportion of time awake\n                         measurevar,\n                         groupvars=NULL,\n                         na.rm=FALSE,\n                         conf.interval=.95,\n                         .drop=TRUE){\n  \n  length2 <- function(x,\n                      na.rm=FALSE){\n    if (na.rm) sum(!is.na(x))\n    else length(x)\n  }\n  \n  DAM_mean <- ddply(DAM,\n                    groupvars,\n                    .drop = .drop,\n                    .fun = function(xx,col){\n                      c(N = length2(xx[[col]],\n                                    na.rm=na.rm),\n                        mean = mean (xx[[col]],\n                                     na.rm=na.rm),\n                        sd = sd (xx[[col]],\n                                 na.rm=na.rm)\n                      )\n                    },\n                    measurevar\n  )\n  DAM_mean <- rename(DAM_mean,\n                     c(""mean"" = measurevar))\n  \n  DAM_mean$se <- DAM_mean$sd/sqrt(DAM_mean$N)\n  \n  ciMult <- qt(conf.interval/2 + .5,\n               DAM_mean$N-1)\n  DAM_mean$ci <- DAM_mean$se * ciMult\n  \n  return(DAM_mean)\n}\nMean_PA.summarytable <- PA.summarySE(DAM, # calculate mean and SE for proportion of time awake for each combination of line, sex and infection treatment\n                                     measurevar = ""Proportion.Activity"",\n                                     groupvars = c(""Line"",\n                                                   ""Sex"",\n                                                   ""Infection_treatment"")\n)\n\nMean_PA.summarytable$Line <-  factor(Mean_PA.summarytable$Line) # Define genetic background as a factor\n\n\n# Graphing Proportion of Time Spent Awake ---------------------------------\nggplot(Mean_PA.summarytable,\n       aes(x=reorder(Line,\n                     Proportion.Activity),\n           y=Proportion.Activity,\n           fill=Infection_treatment)\n) +\n  geom_bar(position = ""dodge"",\n           stat = ""identity"",\n           size=.5,\n           color=""black"") +\n  geom_errorbar(aes(ymin = Proportion.Activity-se,\n                    ymax = Proportion.Activity+se),\n                position = position_dodge(.9)\n  ) +\n  facet_grid(.~Sex) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 60,\n                                   hjust = 1),\n        text = element_text(size=20)\n  ) +\n  labs(x=Genetic~Background,\n       y=Proportion~Activity)\n\n\n\n# model of proportion -----------------------------------------------------\n\nmodel1.Prop.Activity <- lm(Proportion.Activity ~ Line*Sex*Infection_treatment,\n            data=DAM)\n\nsummary(model1.Prop.Activity)\n\nanova(model1.Prop.Activity)\n\n\n# Graphing Activity when awake -----------------------------------------------------\n## MEanSE\nAA.summarySE <- function(data=NULL, # function to calculate the mean and SE of average awake activitiy\n                         measurevar,\n                         groupvars=NULL,\n                         na.rm=FALSE,\n                         conf.interval=.95,\n                         .drop=TRUE){\n  \n  length2 <- function(x,\n                      na.rm=FALSE){\n    if (na.rm) sum(!is.na(x))\n    else length(x)\n  }\n  \n  DAM_mean <- ddply(DAM,\n                    groupvars,\n                    .drop = .drop,\n                    .fun = function(xx,col){\n                      c(N = length2(xx[[col]],\n                                    na.rm=na.rm),\n                        mean = mean (xx[[col]],\n                                     na.rm=na.rm),\n                        sd = sd (xx[[col]],\n                                 na.rm=na.rm)\n                      )\n                    },\n                    measurevar\n  )\n  DAM_mean <- rename(DAM_mean,\n                     c(""mean"" = measurevar))\n  \n  DAM_mean$se <- DAM_mean$sd/sqrt(DAM_mean$N)\n  \n  ciMult <- qt(conf.interval/2 + .5,\n               DAM_mean$N-1)\n  DAM_mean$ci <- DAM_mean$se * ciMult\n  \n  return(DAM_mean)\n}\nMean_AA.summarytable <- AA.summarySE(DAM, # calculate mean and SE average awake activity for each treatment group combination of sex, genetic background and infection treatment\n                                     measurevar = ""Awake.Activity"",\n                                     groupvars = c(""Line"",\n                                                   ""Sex"",\n                                                   ""Infection_treatment"")\n)\n\nMean_AA.summarytable$Line <-  factor(Mean_AA.summarytable$Line) # Define genetic background as a factor\n\n\n\n# Graphing Average Awake Activity -----------------------------------------\nggplot(Mean_AA.summarytable,\n       aes(x=reorder(Line,\n                     Awake.Activity),\n           y=Awake.Activity,\n           fill=Infection_treatment)\n) +\n  geom_bar(position = ""dodge"",\n           stat = ""identity"",\n           size=.5,\n           color=""black"") +\n  geom_errorbar(aes(ymin = Awake.Activity-se,\n                    ymax = Awake.Activity+se),\n                position = position_dodge(.9)\n  ) +\n  facet_grid(.~Sex) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 60,\n                                   hjust = 1),\n        text = element_text(size=20)\n  ) +\n  labs(x=Genetic~Background,\n       y=Awake~Activity)\n\n\n\n# Model Activity when awake  -----------------------------------------------\nmodel1.AwakeAct <- lm(log(Awake.Activity+1) ~ Line*Sex*Infection_treatment,\n            data=DAM)\nsummary(model1.AwakeAct)\nanova(model1.AwakeAct)\n\n\n\n\n\n']","Data from: Viral infection causes sex-specific changes in fruit fly social aggregation behaviour Host behavioural changes following infection are common and could be important determinants of host behavioural competence to transmit pathogens. Identifying potential sources of variation in sickness behaviours is therefore central to our understanding of disease transmission. Here, we test how group social aggregation and individual locomotor activity vary between different genotypes of male and female fruit flies (Drosophila melanogaster) following septic infection with Drosophila C Virus. We find genetic-based variation in both locomotor activity and social aggregation but we did not detect an effect of DCV infection on fly activity or sleep patterns within the initial days following infection. However, DCV infection caused sex-specific effects on social aggregation, as male flies in most genetic backgrounds increased the distance to their nearest neighbour when infected. We discuss possible causes for these differences in the context of individual variation in immunity and their potential consequences for disease transmission.",2
Data from: Copy-when-uncertain: bumblebees rely on social information when rewards are highly variable,"To understand the relative benefits of social and personal information use in foraging decisions, we developed an agent-based model of social learning that predicts social information should be more adaptive where resources are highly variable and personal information where resources vary little. We tested our predictions with bumblebees and found that foragers relied more on social information when resources were variable than when they were not. We then investigated whether socially salient cues are used preferentially over non-social ones in variable environments. Although bees clearly used social cues in highly variable environments, under the same conditions they did not use non-social cues. These results suggest that bumblebees use a 'copy-when-uncertain' strategy.","['# title: ""Do bumblebees use (social) cues more readily when the task is difficult?""\n# author: ""Marco Smolla""\n# date: ""10 November 2015""\n\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(reshape2)\nlibrary(cowplot)\nlibrary(lme4)\n\np <- ggplot() +\n\ttheme_bw() +\n\txlab(\'Individual\') +\n\ttheme(axis.title.x = element_text(size=15,family=\'Helvetica\'),\n\t\t\t\taxis.title.y = element_text(size=15, angle=90,family=\'Helvetica\'),\n\t\t\t\taxis.text.x = element_text(size=13,family=\'Helvetica\'),\n\t\t\t\taxis.text.y = element_text(size=13,family=\'Helvetica\'),\n\t\t\t\tpanel.grid.major = element_blank(),\n\t\t\t\tpanel.grid.minor = element_blank(),\n\t\t\t\tlegend.title = element_text(size=13, face=\'bold\'),\n\t\t\t\tlegend.text = element_text(size=13, family=\'Helvetica\'),\n\t\t\t\tlegend.key = element_blank(),\n\t\t\t\tplot.title = element_text(face=\'bold\', lineheight=.8, size=10,family=\'Helvetica\'),\n\t\t\t\tstrip.text.x = element_text(face=\'bold\', lineheight=.8, size=13,family=\'Helvetica\'),\n\t\t\t\tstrip.text.y = element_text(face=\'bold\', lineheight=.8, size=13,family=\'Helvetica\')\n\t)\n\n\nq <-  ggplot() +\n\t\t\t\t\ttheme_bw() +\n\t\t\t\t\tscale_y_continuous(lim=c(0,1.02), expand=c(0,0) ,breaks=seq(from = 0.2,to = 1.0,by=0.2)) +\n\t\t\t\t\tscale_fill_manual(values=c(""black"", ""white"")) +\n\t\t\t\t\txlab(\'\') +\n\t\t\t\t\ttheme(\n\t\t\t\t\t\tpanel.border = element_blank(),\n\t\t\t\t\t\taxis.line.x=element_line(),\n\t\t\t\t\t\taxis.line.y=element_line(),\n\t\t\t\t\t\t\taxis.title.x = element_text(size=8,family=\'Helvetica\'),\n\t\t\t\t\t\t\taxis.title.y = element_text(size=8, angle=90,family=\'Helvetica\'),\n\t\t\t\t\t\t\taxis.text.x = element_text(size=6,family=\'Helvetica\'),\n\t\t\t\t\t\t\taxis.text.y = element_text(size=6,family=\'Helvetica\'),\n\t\t\t\t\t\t\tpanel.grid.major = element_blank(),\n\t\t\t\t\t\t\tpanel.grid.minor = element_blank(),\n\t\t\t\t\t\t\tlegend.title = element_text(size=6, face=\'bold\'),\n\t\t\t\t\t\t\tlegend.text = element_text(size=6, family=\'Helvetica\'),\n\t\t\t\t\t\t\tlegend.key = element_blank(),\n\t\t\t\t\t\t\tplot.title = element_text(face=\'bold\', lineheight=.8, size=4,family=\'Helvetica\'),\n\t\t\t\t\t\t\tstrip.text.x = element_text(face=\'bold\', lineheight=.8, size=6,family=\'Helvetica\'),\n\t\t\t\t\t\t\tstrip.text.y = element_text(face=\'bold\', lineheight=.8, size=6,family=\'Helvetica\')\n\t\t\t\t\t\t)\n\n# Standard error\nse <- function(x,na.rm=F){\n\treturn( sd(x,na.rm = na.rm) / sqrt(length(x)) )\n}\n\n# Function to check whether chip landed on is a chip with a cue\nllandingsF <- function(l,c){\n\ttmp <- list()\n\tfor(i in 1:length(l)){\n\t\ttmp[[i]] <- l[[i]]%in%c[[i]]\n\t}\n\treturn(tmp)\n}\n\n# Function to extract basic values from the landings: total landings, total landings on cue, first choice, first 4, first 10, total cue prop\ndata_add_infoF <- function(dat, ll){\n\tdat$total <- unlist(lapply(ll, length))\n\tdat$landedOnCue <- unlist(lapply(ll, sum))\n\tdat$totalCueProp <- unlist(lapply(ll, function(x) return(sum(x)/length(x))))\n\tdat$first10CueProp <-unlist(lapply(ll, function(x) {ifelse(length(x)<10, yes = sum(x)/length(x), no = sum(x[1:10])/10)}))\n\tdat$first4CueProp <- unlist(lapply(ll, function(x) {ifelse(length(x)<4 , yes = sum(x)/length(x), no = sum(x[1:4]) /4 )}))\n\tdat$firstChoice <- unlist(lapply(ll, \'[\',1))\n\treturn(dat)\n}\n\ndata_add_infoF2 <- function(dat, l, c, r){\n\tdat$uniqueFlowers <- unlist(lapply(lapply(l, unique), length))\n\tdat$uniqueCue <- unlist( lapply(c(1:length(l)), function(i) length(unique(l[[i]][l[[i]]%in%c[[i]]]))) )\n\tdat$collectedReward <- unlist( lapply(c(1:length(l)), function(i) length(unique(l[[i]][l[[i]]%in%r[[i]]]))) )\n\n\tvisitsTillReward <- lapply(c(1:length(l)), function(i){\n\t\tfirst <- which(l[[i]]%in%r[[i]])[1]\n\t\trr <- r[[i]][ r[[i]] != l[[i]][first] ]\n\t\tsecond <- which(l[[i]]==rr)[1]\n\t\treturn(c(first, second))\n\t})\n\tdat$visitsTillFirstReward <- unlist(lapply(visitsTillReward, \'[\', 1))\n\tdat$visitsTillSecondReward <- unlist(lapply(visitsTillReward, \'[\', 2))\n\n\treturn(dat)\n}\n\n### Functions for cue training\nlandingsF <- function(dat){\n\t\tlapply(dat$landed, function(x) as.numeric(unlist(strsplit(as.character(x), split=\' \'))))}\n\ncueF <- function(dat){\n\t\tlapply(dat$cue, function(x) as.numeric(unlist(strsplit(as.character(x), split=\' \'))))}\n\nrewardF <- function(dat){\n\t\tlapply(dat$reward, function(x) as.numeric(unlist(strsplit(as.character(x), split=\' \'))))}\n\n\n\n\n\n##### Read in data #####\ndata <- DATA <- read.csv(""~/Desktop/beeLandings.csv"",header=T,sep=\',\')\n### Adding results from the simultions\nload(\'~/Desktop/simulation1\')\nDF_sim_1 <- DF\nload(\'~/Desktop/simulation2\')\nDF_sim_2 <- DF\n### Transform empirical data set\nlandings <- lapply(data$landed, function(x) as.numeric(unlist(strsplit(as.character(x), split=\' \'))))\ncue <- lapply(data$cue, function(x) as.numeric(unlist(strsplit(as.character(x), split=\' \'))))\n### Check length of input\nif(length(landings)!=length(cue)) stop(\'objects have different length\')\n### Create landings list\nllandings <- llandingsF(l=landings, c=cue)\n### Add info to data.frame\ndata <- data_add_infoF(dat=data, ll=llandings)\n### Setting the right world (diff=high-varaince / easy=no-varaince)\ndata$world <- factor(ifelse(test = data$group==\'test_easy\', yes = \'easy\', no = \'diff\'))\n### Extracting worlds for last cue(=dummy) training\ndummy_worlds <- do.call(rbind,lapply(unique(data$id), function(id){\n\tdata.frame(id=id, world=data[data$id==id & data$group!=""dummy_last"", ""world""])\n}))\n### Writing the worlds to the according id\'s\nsort_id <- match(x=dummy_worlds$id, table=data$id[data$group==""dummy_last""])\ndata$world[data$group==""dummy_last""] <- dummy_worlds$world[sort_id]\n### Order data and fill with -1 to bring all vectors the same length\nlongest <- max(unlist(lapply(llandings, length)))\nr <- data.frame(do.call(rbind, lapply(llandings, function(x)c(x,rep(-1,longest-length(x)) ))))\n\n\n\n##### Extract flower choices #####\n### Bee dummy\nx <- data$cueType==""clay"" & data$group%in%c(""test_diff"",""test_easy"")\n### First Flower Choice\ndiffFirstChoice_clay <- data$firstChoice[data$world==\'diff\' & x ]\neasyFirstChoice_clay <- data$firstChoice[data$world==\'easy\' & x]\n### First 4 choices\ndiffFirst4Avg_clay <- data$first4CueProp[data$world==\'diff\' & x]\neasyFirst4Avg_clay <- data$first4CueProp[data$world==\'easy\' & x]\n### First 10 choices\ndiffFirst10Avg_clay <- data$first10CueProp[data$world==\'diff\' & x]\neasyFirst10Avg_clay <- data$first10CueProp[data$world==\'easy\' & x]\n\n### Craft Foam dummy\ny <- data$cueType==""green"" & data$group%in%c(""test_diff"",""test_easy"")\n### First Flower Choice\ndiffFirstChoice_green <- data$firstChoice[data$world==\'diff\' & y]\neasyFirstChoice_green <- data$firstChoice[data$world==\'easy\' & y]\n### First 4 choices\ndiffFirst4Avg_green <- data$first4CueProp[data$world==\'diff\' & y]\neasyFirst4Avg_green <- data$first4CueProp[data$world==\'easy\' & y]\n### First 10 choices\ndiffFirst10Avg_green <- data$first10CueProp[data$world==\'diff\' & y]\neasyFirst10Avg_green <- data$first10CueProp[data$world==\'easy\' & y]\n\n\n\n\n\n##### Main Text Stistical tests #####\n#### Results from Test ####\n### First flower choice different from chance (AS REPORTED IN MAIN TEXT)?\n### Bee dummy - high-variance\nbinom.test(x=c(sum(diffFirstChoice_clay==1),sum(diffFirstChoice_clay==0)), p=1/3)#, alternative=""greater"")\n### Bee dummy - no-variance\nbinom.test(x=c(sum(easyFirstChoice_clay==1),sum(easyFirstChoice_clay==0)), p=1/3)#, alternative=""less"")\n### Craft foam - high-variance\nbinom.test(x=c(sum(diffFirstChoice_green==1),sum(diffFirstChoice_green==0)), p=1/3)#, alternative=""greater"")\n### Craft foam - no-variance\nbinom.test(x=c(sum(easyFirstChoice_green==1),sum(easyFirstChoice_green==0)), p=1/3)#, alternative=""less"")\n\n\n\n#### Logistic Regression ####\n### Selecting test data\ndf <- data[data$group!=""dummy_last"",]\ndf$cueType <- factor(df$cueType, levels=c(""clay"",""green""))\ndf$world <- factor(df$world, levels=c(""diff"",""easy""))\n\n#### Setting 1\'s and 0\'s for cue type and reward distribution\n# 1== bee dummy, 0== craft foam\n# 1== high-variance, 0== no-variance\ndf$cueType <- factor(ifelse(df$cueType==\'clay\',yes=1,no=0))\ndf$world <- factor(ifelse(df$world==\'diff\',yes=1,no=0))\n\n# All variables with colony as random effect (AS REPORTED IN MAIN TEXT)\nmod_col <- glmer(data = df, family = ""binomial"", formula = firstChoice ~ cueType + world + cueType*world + (1|colony))\nsummary(mod_col)\n\n\n### Splitting the data for the two different cues (AS REPORTED IN MAIN TEXT)\nmod_clay <- glmer(data = df[df$cueType==""1"",], family = ""binomial"", formula = firstChoice ~ world + (1|colony))\nsummary(mod_clay)\nmod_green <- glmer(data = df[df$cueType==""0"",], family = ""binomial"", formula = firstChoice ~ world + (1|colony))\nsummary(mod_green)\n\n### Splitting the data for the two different reward distributions (not reported)\nmod_uneven <- glmer(data = df[df$world==0,], family = ""binomial"", formula = firstChoice ~ cueType + (1|colony))\nsummary(mod_uneven)\nmod_even <- glmer(data = df[df$world==1,], family = ""binomial"", formula = firstChoice ~ cueType + (1|colony))\nsummary(mod_even)\n\n\n\n\n\n\n\n\n\n\n\n\n#### Comparisson of empirical data with results from simulations ####\n### Data flower choice step by step ###\ndatas <- do.call(rbind, lapply(c(""clay"",""green""), function(cueT){\n\tdo.call(rbind, lapply(c(\'diff\',\'easy\'), function(w){\n\t\tdo.call(rbind, lapply(c(\'firstChoice\',\'first4CueProp\',\'first10CueProp\'), function(ch){\n\t\t\td <- data[data$world==w & data$cueType==cueT & data$group%in%c(""test_diff"",""test_easy""), colnames(data)==ch]\n\t\t\tdata.frame(world=w, cueType=cueT,choice=ch,avg=mean(d),sd=sd(d),se=se(d),n=length(d))\n\t\t\t}))\n\t\t}))\n\t}))\n### Transform data from simulations\nsim_diff <- unlist(DF_sim_1$meanILprop)\nmean_diff <- 1-mean(sim_diff)\nsim_easy <- unlist(DF_sim_2$meanILprop)\nmean_easy <- 1-mean(sim_easy)\n### Adding a column to combine the test conditions (cueType and world trained)\ndatas_first <- datas[datas$choice==""firstChoice"",]\ndatas_first$test <- paste(datas_first$world, as.character(datas_first$cueType), sep=\'_\')\ndatas_first$test <- factor(datas_first$test, levels=c(""diff_clay"",""easy_clay"",""diff_green"",""easy_green""))\n### Adding simulation data\ndatas_first <- rbind(data.frame(world=c(\'diff\',\'easy\'), cueType=\'simulation\', choice=\'first\', avg=c(mean_diff,mean_easy), se=c(se(sim_diff),se(sim_easy)),sd=c(sd(sim_diff),sd(sim_easy)),n=25, test=c(\'diff_sim\',\'easy_sim\')), datas_first)\n\n### Comparison to Bee Model (AS REPORTED IN MAIN TEXT)\nbinom.test(x=c(sum(diffFirstChoice_clay==1),sum(diffFirstChoice_clay==0)), p=mean_diff)\nbinom.test(x=c(sum(easyFirstChoice_clay==1),sum(easyFirstChoice_clay==0)), p=mean_easy)\n### Comparison to Craft Foam (AS REPORTED IN MAIN TEXT)\nbinom.test(x=c(sum(diffFirstChoice_green==1),sum(diffFirstChoice_green==0)), p=mean_diff)\nbinom.test(x=c(sum(easyFirstChoice_green==1),sum(easyFirstChoice_green==0)), p=mean_easy)\n\n\n\n\n\n\n\n\n\n#### Results from last training round ####\n### Did bees differ in training success?  Last Training Data ###\n### Subsetting raw data for training with bee models and craft foam\ndata_lastTraining <- data[data$group==\'dummy_last\', ]\n### Transforming strings of landings and places of cues to numeric vectors for cue\nlandings_lastTraining <- landingsF(data_lastTraining)\ncue_lastTraining <- cueF(data_lastTraining)\nreward_lastTraining <- rewardF(data_lastTraining)\n### Did the chips landed on were with cue? Returns vectors with T and F\nllandings_lastTraining <- llandingsF(l=landings_lastTraining, c=cue_lastTraining)\n### Extracting additional information from the landings and adding to the data.frames\ndata_lastTraining <- data_add_infoF(dat = data_lastTraining, ll=llandings_lastTraining)\n### ... and more data that could be important: unique flowers visited, unique cues landed on, collected rewards, visits befpre first and second reward\ndata_lastTraining <- data_add_infoF2(dat=data_lastTraining, l=landings_lastTraining, c=cue_lastTraining, r=reward_lastTraining)\n### Adding informaiton whether bees were tested later on in easy or diff\ndata_lastTraining$world <- data$world[ match(c(as.character(data_lastTraining$id)), as.character(data$id)) ]\n\n### First Flower Choice last training ###\n### Bee Model\nx <- data_lastTraining$cueType==""clay""# & data_lastTraining$group==""dummy_last""\ndiffFirstChoice_clay_last <- data_lastTraining$firstChoice[data_lastTraining$world==\'diff\' & x ]\neasyFirstChoice_clay_last <- data_lastTraining$firstChoice[data_lastTraining$world==\'easy\' & x ]\n### Craft Foam Dummy\ny <- data_lastTraining$cueType==""green""# & data_lastTraining$group==""dummy_last""\ndiffFirstChoice_green_last <- data_lastTraining$firstChoice[data_lastTraining$world==\'diff\' & y ]\neasyFirstChoice_green_last <- data_lastTraining$firstChoice[data_lastTraining$world==\'easy\' & y ]\n\n\n#### For First Choice (test of equal proportions) in the last cue learning round, do groups differ from each other? (NOT REPORTED)\n### Bee Model\nfirstChoice_last <- matrix(c(sum(diffFirstChoice_clay_last),sum(!diffFirstChoice_clay_last), sum(easyFirstChoice_clay_last),sum(!easyFirstChoice_clay_last)), nrow=2)\ncolnames(firstChoice_last) <- c(\'diff\',\'easy\')\nrownames(firstChoice_last) <- c(\'cue\',\'nocue\')\nfirstChoice_last\nfisher.test(firstChoice_last)\n### Craft Foam Dummy\nfirstChoice_last <- matrix(c(sum(diffFirstChoice_green_last),sum(!diffFirstChoice_green_last), sum(easyFirstChoice_green_last),sum(!easyFirstChoice_green_last)), nrow=2)\ncolnames(firstChoice_last) <- c(\'diff\',\'easy\')\nrownames(firstChoice_last) <- c(\'cue\',\'nocue\')\nfirstChoice_last\nfisher.test(firstChoice_last)\n\n\n### Is the cue use from bees trained in high/no variance significantly different from chance (1/3) in last cue training round? (AS REPORTED IN MAIN TEXT)\n# Bee Model - High-variance\nbinom.test(x=c(sum(diffFirstChoice_clay_last==1),sum(diffFirstChoice_clay_last==0)), p=1/3)\n# Bee Model - No-variance\nbinom.test(x=c(sum(easyFirstChoice_clay_last==1),sum(easyFirstChoice_clay_last==0)), p=1/3)\n# Craft Foam - high-variance\nbinom.test(x=c(sum(diffFirstChoice_green_last==1),sum(diffFirstChoice_green_last==0)), p=1/3)\n# Craft Foam - no-variance\nbinom.test(x=c(sum(easyFirstChoice_green_last==1),sum(easyFirstChoice_green_last==0)), p=1/3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n##### Main Text Figures #####\n### Figure 1a\n# Error bars\nlimits <- aes(x=world, y=avg, ymax = avg + se, ymin=avg - se)\nq+geom_bar(data=datas_first[datas_first$cueType==\'simulation\',], aes(x=world, y=avg, fill=world, group=world), colour=c(NA,""black""), width=1, stat=\'identity\') + geom_errorbar(data=datas_first[datas_first$cueType==\'simulation\',], limits, width=.1, size=.25, colour=c(\'grey25\',\'black\'))+ylab(\'\')+guides(fill=FALSE) + theme(axis.ticks.x=element_blank())\n\n### Figure 1b\n# Confidence intervalls, using http://www.graphpad.com/quickcalcs/ConfInterval1.cfm\ndatas_first$CI_lower <- c(NA, NA, 0.6562, 0.1068, 0.3555, 0.2195)\ndatas_first$CI_upper <- c(NA, NA, 0.9773, 0.4365, 0.7805, 0.6445)\nq + geom_bar(data=datas_first[datas_first$cueType!=\'simulation\',], aes(x=cueType, y=avg, fill=world), colour=""black"", position=\'dodge\', width=.8, stat=\'identity\') + ylab(\'\')+guides(fill=FALSE)+ geom_hline(yintercept=1/3, linetype=\'dashed\', colour=\'black\', size=.5)+ geom_errorbar(data=datas_first[datas_first$cueType!=\'simulation\',], aes(x=cueType, y=avg, ymin=CI_lower, ymax=CI_upper, group=world), width=.1, position = position_dodge(0.8), colour=\'grey40\', size=.25)\n\n\n\n\n\n\n\n\n\n\n\n##### SI #####\n#### Gini index for simulation ####\ngini <- function(x, na.rm = FALSE){\n\tif (!is.numeric(x)){\n\t\twarning(""\'x\' is not numeric; returning NA"")\n\t\treturn(NA)\n\t}\n\tif (!na.rm && any(na.ind <- is.na(x)))\n\t\tstop(""\'x\' contain NAs"")\n\tif (na.rm)\n\t\tx <- x[!na.ind]\n\tn <- length(x)\n\tmu <- mean(x)\n\tN <- n * (n + 1)\n\tox <- x[order(x)]\n\tdsum <- drop(2*crossprod(1:n,  ox))\n\t(dsum / (mu * N)) - 1\n}\n\nMEAN <- 100/12 #(100l in 12 flowers)\nK <- (MEAN^2)/var(c(rep(0,10),rep(50,2)))\nTHETA <- MEAN/K\nrand <- rgamma(n = 100000, shape = K, scale=THETA)\n### Calculating Gini index of resource distribution used in our experiment to compare with Smolla, Gilman, Galla, and Shultz (2015)\ngini(rand)\n\n\n\n#### SI figures ####\n### Print version for Figure S1a and S2a:\n\n### Adding factor levels\ndatas$choice <- factor(c(\'first\',\'first 4\',\' first 10\'), levels=c(c(\'first\',\'first 4\',\' first 10\')))\n### Setting standard error bars\nlimits <- aes(x=world, y=avg, ymax = avg + sd, ymin=avg - sd)\n### Poprotion of landings in first, first four, and first ten landings on flowers with a cue\nx <- \'clay\' # Bee dummy\n# x <- \'green\' # Craft foam dummy\ndat <- datas[datas$cueType==x,]\nqchoices <- q + geom_bar(data=dat, aes(x=world, y=avg, fill=world), stat=\'identity\', colour=c(\'black\'), width=1)  + geom_errorbar(data=dat[dat$choice!=\'first\',], limits, width=.1, size=.25, colour=c(\'grey25\',\'black\',\'grey25\',\'black\')) + xlab(label=\'\') + geom_hline(yintercept=1/3, linetype=\'dashed\', colour=\'grey25\', size=.5) + facet_grid(.~choice) + ylab(\'\') + guides(fill=FALSE) + theme(axis.ticks.x=element_blank(), panel.margin = unit(.1, ""lines""), strip.text=element_blank()) + theme(axis.title.x = element_text(size=15,family=\'Helvetica\'),\n\t\t\t\taxis.title.y = element_text(size=15, angle=90,family=\'Helvetica\'),\n\t\t\t\taxis.text.x = element_text(size=13,family=\'Helvetica\'),\n\t\t\t\taxis.text.y = element_text(size=13,family=\'Helvetica\'),\n\t\t\t\tpanel.grid.major = element_blank(),\n\t\t\t\tpanel.grid.minor = element_blank(),\n\t\t\t\tlegend.title = element_text(size=13, face=\'bold\'),\n\t\t\t\tlegend.text = element_text(size=13, family=\'Helvetica\'))\n### Landings per reward distribution, step-by-step\ncountData <-\n\tdo.call(rbind, lapply(c(""clay"",""green""), function(cueT){\n\t\tdo.call(rbind, lapply(c(1:10), function(i){\n\t\tx <-  data$cueType==cueT & data$group%in%c(""test_diff"",""test_easy"")\n\t\tdata.frame(cueType=cueT, step=i,\n\t\t\t\t\t\t\t diff=sum(r[data$world==\'diff\' & x,i]==1),#/length(r[data$world==\'diff\' & x,i]),\n\t\t\t\t\t\t\t easy=sum(r[data$world==\'easy\' & x,i]==1))#/length(r[data$world==\'easy\' & x,i]))\n\t}))\n}))\ncountData$id <- 1:nrow(countData)\nstep_cue_landing_melted <- melt(countData,value.name=c(\'landingOnCue\'), varnames=\'landing\', id=c(\'id\',\'step\',\'cueType\'))\n\n### Print Version for Figure S1b and S2b:#SP15HJ\nx <- \'clay\' # Bee dummy\n# x <- \'green\' # Craft foam dummy\nqsteps <- q+geom_bar(data=step_cue_landing_melted[step_cue_landing_melted$cueType==x,], aes(x=factor(step), y=landingOnCue, fill=variable), colour=\'black\', size=.2, stat=\'identity\', position=\'dodge\') + xlab(\'Landing number\') + ylab(\'\') + guides(fill=FALSE) + scale_fill_manual(values=c(""black"", ""white"")) + scale_y_continuous(lim=c(0,15.2), expand=c(0,0.00)) + theme(axis.title.x = element_text(size=15,family=\'Helvetica\'),\n\t\t\t\taxis.title.y = element_text(size=15, angle=90,family=\'Helvetica\'),\n\t\t\t\taxis.text.x = element_text(size=13,family=\'Helvetica\'),\n\t\t\t\taxis.text.y = element_text(size=13,family=\'Helvetica\'),\n\t\t\t\tpanel.grid.major = element_blank(),\n\t\t\t\tpanel.grid.minor = element_blank(),\n\t\t\t\tlegend.title = element_text(size=13, face=\'bold\'),\n\t\t\t\tlegend.text = element_text(size=13, family=\'Helvetica\'))\n### Plot figures\nggdraw() +\n  draw_plot(qchoices, 0, 0, .33, 1) +\n  draw_plot(qsteps, .33, 0, .66, 1) +\n  draw_plot_label(c(""a"", ""b""), fontface=\'italic\', c(0, .33), c(1, 1), size = 15)\n\n\n\n#### SI Statistics ####\n### Extracting landings from the last first 4 and first 10 landings\nfirst4 <- unlist(lapply(llandings, function(x) {ifelse(length(x)<4 , yes = sum(x), no = sum(x[1:4]) )}))\nfirst10 <- unlist(lapply(llandings, function(x) {ifelse(length(x)<10 , yes = sum(x), no = sum(x[1:10]) )}))\n\nx <- data$cueType==""clay"" & data$group%in%c(""test_diff"",""test_easy"")\n### First 4 choices\ndiffFirst4_clay <- first4[data$world==\'diff\' & x]\neasyFirst4_clay <- first4[data$world==\'easy\' & x]\n### First 10 choices\ndiffFirst10_clay <- first10[data$world==\'diff\' & x]\neasyFirst10_clay <- first10[data$world==\'easy\' & x]\n\ny <- data$cueType==""green"" & data$group%in%c(""test_diff"",""test_easy"")\n### First 4 choices\ndiffFirst4_green <- first4[data$world==\'diff\' & y]\neasyFirst4_green <- first4[data$world==\'easy\' & y]\n### First 10 choices\ndiffFirst10_green <- first10[data$world==\'diff\' & y]\neasyFirst10_green <- first10[data$world==\'easy\' & y]\n\n\n#### For first 4 and first 10 choices testing whether landings differ from random choice\n### First four for bee dummy\nbinom.test(x=c(sum(diffFirst4_clay),(length(diffFirst4_clay)*4)-sum(diffFirst4_clay)), p=1/3)\nbinom.test(x=c(sum(easyFirst4_clay),(length(easyFirst4_clay)*4)-sum(easyFirst4_clay)), p=1/3)\n### First ten for bee foam\nbinom.test(x=c(sum(diffFirst10_clay),(length(diffFirst10_clay)*10)-sum(diffFirst10_clay)), p=1/3)\nbinom.test(x=c(sum(easyFirst10_clay),(length(easyFirst10_clay)*10)-sum(easyFirst10_clay)), p=1/3)\n\n### First four for craft foam\nbinom.test(x=c(sum(diffFirst4_green),(length(diffFirst4_green)*4)-sum(diffFirst4_green)), p=1/3)\nbinom.test(x=c(sum(easyFirst4_green),(length(easyFirst4_green)*4)-sum(easyFirst4_green)), p=1/3)\n### First ten for craft foam\nbinom.test(x=c(sum(diffFirst10_green),(length(diffFirst10_green)*10)-sum(diffFirst10_green)), p=1/3)\nbinom.test(x=c(sum(easyFirst10_green),(length(easyFirst10_green)*10)-sum(easyFirst10_green)), p=1/3)\n\n\n### Do groups trained in different reward distributions differ between each other for the first 4 and first 10 landings?\n### Bee dummy first four landings\nfirst4ChoiceM <- matrix(c(sum(diffFirst4_clay),(length(diffFirst4_clay)*4)-sum(diffFirst4_clay), sum(easyFirst4_clay),(length(easyFirst4_clay)*4)-sum(easyFirst4_clay)), nrow=2)\ncolnames(first4ChoiceM) <- c(\'diff\',\'easy\')\nrownames(first4ChoiceM) <- c(\'cue\',\'nocue\')\nfirst4ChoiceM\nchisq.test(first4ChoiceM)\n\n### Bee dummy first ten landings\nfirst10ChoiceM <- matrix(c(sum(diffFirst10_clay),(length(diffFirst10_clay)*10)-sum(diffFirst10_clay), sum(easyFirst10_clay),(length(easyFirst10_clay)*10)-sum(easyFirst10_clay)), nrow=2)\ncolnames(first10ChoiceM) <- c(\'diff\',\'easy\')\nrownames(first10ChoiceM) <- c(\'cue\',\'nocue\')\nchisq.test(first10ChoiceM)\n\n### Craft foam first four landings\nfirst4ChoiceM <- matrix(c(sum(diffFirst4_green),(length(diffFirst4_green)*4)-sum(diffFirst4_green), sum(easyFirst4_green),(length(easyFirst4_green)*4)-sum(easyFirst4_green)), nrow=2)\ncolnames(first4ChoiceM) <- c(\'diff\',\'easy\')\nrownames(first4ChoiceM) <- c(\'cue\',\'nocue\')\nchisq.test(first4ChoiceM)\n\n### Craft foam first ten landings\nfirst10ChoiceM <- matrix(c(sum(diffFirst10_green),(length(diffFirst10_green)*10)-sum(diffFirst10_green), sum(easyFirst10_green),(length(easyFirst10_green)*10)-sum(easyFirst10_green)), nrow=2)\ncolnames(first10ChoiceM) <- c(\'diff\',\'easy\')\nrownames(first10ChoiceM) <- c(\'cue\',\'nocue\')\nchisq.test(first10ChoiceM)\n']","Data from: Copy-when-uncertain: bumblebees rely on social information when rewards are highly variable To understand the relative benefits of social and personal information use in foraging decisions, we developed an agent-based model of social learning that predicts social information should be more adaptive where resources are highly variable and personal information where resources vary little. We tested our predictions with bumblebees and found that foragers relied more on social information when resources were variable than when they were not. We then investigated whether socially salient cues are used preferentially over non-social ones in variable environments. Although bees clearly used social cues in highly variable environments, under the same conditions they did not use non-social cues. These results suggest that bumblebees use a 'copy-when-uncertain' strategy.",2
Data from: Hierarchical social networks shape gut microbial composition in wild Verreaux's sifaka,"In wild primates, social behaviour influences exposure to environmentally acquired and directly transmitted microorganisms. Prior studies indicate that gut microbiota reflect pairwise social interactions among chimpanzee and baboon hosts. Here, we demonstrate that higher-order social network structurebeyond just pairwise interactionsdrives gut bacterial composition in wild lemurs, which live in smaller and more cohesive groups than previously studied anthropoid species. Using 16S rRNA gene sequencing and social network analysis of grooming contacts, we estimate the relative impacts of hierarchical (i.e. multilevel) social structure, individual demographic traits, diet, scent-marking, and habitat overlap on bacteria acquisition in a wild population of Verreaux's sifaka (Propithecus verreauxi) consisting of seven social groups. We show that social group membership is clearly reflected in the microbiomes of individual sifaka, and that social groups with denser grooming networks have more homogeneous gut microbial compositions. Within social groups, adults, more gregarious individuals, and individuals that scent-mark frequently harbour the greatest microbial diversity. Thus, the community structure of wild lemurs governs symbiotic relationships by constraining transmission between hosts and partitioning environmental exposure to microorganisms. This social cultivation of mutualistic gut flora may be an evolutionary benefit of tight-knit group living.",,"Data from: Hierarchical social networks shape gut microbial composition in wild Verreaux's sifaka In wild primates, social behaviour influences exposure to environmentally acquired and directly transmitted microorganisms. Prior studies indicate that gut microbiota reflect pairwise social interactions among chimpanzee and baboon hosts. Here, we demonstrate that higher-order social network structurebeyond just pairwise interactionsdrives gut bacterial composition in wild lemurs, which live in smaller and more cohesive groups than previously studied anthropoid species. Using 16S rRNA gene sequencing and social network analysis of grooming contacts, we estimate the relative impacts of hierarchical (i.e. multilevel) social structure, individual demographic traits, diet, scent-marking, and habitat overlap on bacteria acquisition in a wild population of Verreaux's sifaka (Propithecus verreauxi) consisting of seven social groups. We show that social group membership is clearly reflected in the microbiomes of individual sifaka, and that social groups with denser grooming networks have more homogeneous gut microbial compositions. Within social groups, adults, more gregarious individuals, and individuals that scent-mark frequently harbour the greatest microbial diversity. Thus, the community structure of wild lemurs governs symbiotic relationships by constraining transmission between hosts and partitioning environmental exposure to microorganisms. This social cultivation of mutualistic gut flora may be an evolutionary benefit of tight-knit group living.",2
Social selection is density dependent but makes little contribution to total selection in New Zealand giraffe weevils,"Social selection occurs when traits of interaction partners influence an individual's fitness and can alter total selection strength. However, we have little idea of what factors influence social selection's strength. Further, social selection only contributes to overall selection when there is phenotypic assortment, but simultaneous estimates of social selection and phenotypic assortment are rare. Here we estimated social selection on body size in a wild population of New Zealand giraffe weevils (Lasiorhynchus barbicornis). We measured phenotypic assortment by body size and tested whether social selection varied with sex-ratio, density, and interacted with the body size of the focal individual. Social selection was limited and unaffected by sex ratio or the size of the focal individual. However, at high densities social selection was negative for both sexes, consistent with size-based competitive interactions for access to mates. Phenotypic assortment was always close to zero, indicating negative social selection at high densities will not impede the evolution of larger body sizes. Despite its predicted importance, social selection may only influence evolutionary change in specific contexts, leaving direct selection to drive evolutionary change.","['#START#####\r\n\r\n#R code for the analysis in the manuscript ""Social selection is density dependent but makes little contribution to total selection in New Zealand giraffe weevils""\r\n#Data are provided as supplemental files\r\n#Data are available at https://datadryad.org/stash/dataset/doi:10.5061/dryad.8vs71c3\r\n\r\n#Load packages####\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\nlibrary(car)\r\nlibrary(glmmTMB)\r\nlibrary(ggplot2)\r\nlibrary(grid)\r\nlibrary(gridExtra)\r\n\r\n#Load and wrangle data####\r\n\r\nsetwd(....) #to be completed by user\r\n\r\nweevil_raw_m = read.csv(""weevil_obs_m.csv"", header=T)#pre-xmas males\r\nweevil_raw_f = read.csv(""weevil_obs_f.csv"", header=T)#pre-xmas females\r\nweevil_raw_x_m = read.csv(""weevil_obs_x_m.csv"", header=T)#post-xmas males\r\nweevil_raw_x_f = read.csv(""weevil_obs_x_f.csv"", header=T)#post-xmas females\r\nmeasure_raw_x = read.csv(""measurements_x.csv"", header=T) #as measurements for post-xmas are in a separate file \r\n\r\n\r\nweevil_f = weevil_raw_f %>% mutate(sex = ""F"", season=""pre"", weevil=as.character(weevil), tree=as.character(tree)) %>%\r\n  select(weevil, sex, date, time, season, tree, body_length,  num_mates = num_males, total_cop) \r\n\r\nweevil_m = weevil_raw_m %>% mutate(sex = ""M"", season=""pre"", weevil=as.character(weevil), tree=as.character(tree)) %>%\r\n  select(weevil, sex, date, time, season, tree, body_length, num_mates = num_females, total_cop)   \r\n\r\nmeasure_x = measure_raw_x %>% select(weevil, sex, body_length) %>%\r\n  mutate(weevil = as.character(weevil))\r\n\r\nweevil_x_f = weevil_raw_x_f %>% mutate(season=""post"") %>%\r\n  left_join(measure_x, by = c(""weevil"")) %>%\r\n  select(weevil,sex, date, time, season, tree, body_length, num_mates = num_males, total_cop)\r\n\r\nweevil_x_m = weevil_raw_x_m  %>% mutate(season=""post"") %>%\r\n  left_join(measure_x, by = c(""weevil""))%>%\r\n  select(weevil,sex, date, time, season, tree, body_length, num_mates = num_females, total_cop)\r\n\r\nweevil_comb_both = rbind(weevil_f,weevil_m,weevil_x_f,weevil_x_m) \r\n\r\nfor (i in 1:length(1:nrow(weevil_comb_both))) {\r\n  weevil_comb_both$neigh_mean_blength[i] = mean(weevil_comb_both$body_length[weevil_comb_both$tree==weevil_comb_both$tree[i] & #same tree\r\n                                                                       weevil_comb_both$date==weevil_comb_both$date[i] & #same day, with date don\'t need to also specify season\r\n                                                                         weevil_comb_both$sex==weevil_comb_both$sex[i] & #same sex\r\n                                                                        weevil_comb_both$weevil!=weevil_comb_both$weevil[i] ]) #NOT including focal individual\r\n  \r\n}\r\n\r\nfor (i in 1:length(1:nrow(weevil_comb_both))) {\r\n  weevil_comb_both$sex_ratio[i] = nrow(weevil_comb_both[weevil_comb_both$tree==weevil_comb_both$tree[i] & #same tree\r\n                                                        weevil_comb_both$date==weevil_comb_both$date[i] & #same day, with date don\'t need to also specify season\r\n                                                        weevil_comb_both$sex==""M"" , ]) / #counting males so higher SR = more male biased\r\n                                                                                  #then divide by number of individuals\r\n                                          nrow(weevil_comb_both[weevil_comb_both$tree==weevil_comb_both$tree[i] & #same tree\r\n                                                                weevil_comb_both$date==weevil_comb_both$date[i]  #same day, with date don\'t need to also specify season\r\n                                                                , ])\r\n}\r\n\r\nfor (i in 1:length(1:nrow(weevil_comb_both))) {\r\n  weevil_comb_both$density[i] = nrow(weevil_comb_both[\r\n    weevil_comb_both$tree==weevil_comb_both$tree[i] & #same tree\r\n    weevil_comb_both$date==weevil_comb_both$date[i] , ]) #& same day\r\n}\r\n\r\n\r\n\r\n#who was measured once?\r\nweevil_once = weevil_comb_both %>%\r\n  count(weevil) %>%\r\n  filter(n == 1)\r\n\r\n#who was only measured in the last week for each season?\r\n\r\nweevil_last = weevil_comb_both %>% \r\n  group_by(season) %>%\r\n  mutate(date = dmy(date),\r\n         last_week = max(date) - 7) %>%\r\n  group_by(weevil) %>%\r\n  mutate(first_obs = min(date)) %>%\r\n  filter(first_obs > last_week)\r\n\r\n\r\nweevil_comb_complete = weevil_comb_both %>% filter(!is.na(body_length),\r\n                                                   !is.na(neigh_mean_blength),\r\n                                                   !is.infinite(sex_ratio),\r\n                                                   !weevil %in% weevil_once$weevil,\r\n                                                   !weevil %in% weevil_last$weevil) %>%\r\n  mutate(body_length_z = scale(body_length),\r\n         body_length_z2 = 0.5*body_length_z^2, #mean centre, then square, then half\r\n         neigh_mean_blength_z = scale(neigh_mean_blength),\r\n         neigh_mean_blength_z2 = 0.5*neigh_mean_blength_z^2,\r\n         time2 = as.numeric(hms(time)),\r\n         time2_z = scale(time2),\r\n         time2_z2 = 0.5*time2_z^2)\r\n\r\n#how many of each sex?\r\nweevil_comb_complete %>%\r\n  group_by(sex) %>%\r\n  summarise(counts = n_distinct(weevil))\r\n\r\n#not using an offset of mean fitness as in log-linear models using absolute fitness the regression coefficients on the predictor scale are directional selection gradients \r\n#see: https://www.biorxiv.org/content/10.1101/040618v1.full\r\n\r\n#Is either sex under linear or quadratic social selection for body size? Model 1####\r\n\r\n\r\nblength_mlselec_m1 = glmmTMB(num_mates ~  sex*(body_length_z + neigh_mean_blength_z +\r\n                                                body_length_z2 + neigh_mean_blength_z2) +\r\n                                                time2_z + time2_z2 +\r\n                                                 (1|date) + (1|weevil) + (1|tree) ,\r\n                              family=""poisson"",  \r\n                             data = weevil_comb_complete)          \r\nsummary(blength_mlselec_m1)\r\nAnova(blength_mlselec_m1, type=""II"")\r\n\r\nhist(resid(blength_mlselec_m1)) #long tail but otherwise OK\r\nplot(resid(blength_mlselec_m1), fitted(blength_mlselec_m1)) #looks like what you\'d expect from a Pois model, bands of scores but no obvious directional pattern\r\n\r\n#Does social selection interact with body size? Model 2####\r\n\r\nblength_mlselec_m4 = glmmTMB(num_mates ~ sex * body_length_z * neigh_mean_blength_z +\r\n                               time2_z + time2_z2 + \r\n                               (1|date) + (1|weevil) + (1|tree), \r\n                             family=""poisson"", \r\n                             data = weevil_comb_complete)            \r\nsummary(blength_mlselec_m4)\r\nAnova(blength_mlselec_m4, type=""II"")\r\n\r\n#Does social selection interact with male strategy? Model 3####\r\n\r\n#Make new variable, guarding male (>40mm), sneaking male (<=40mm), or female\r\nweevil_comb_complete$sex2 = ifelse(weevil_comb_complete$sex==""F"", ""female"",\r\n                                   ifelse(weevil_comb_complete$body_length>40, ""Gmale"", ""Smale""))\r\n\r\nblength_mlselec_m5 = glmmTMB(num_mates ~ sex2*(body_length_z + neigh_mean_blength_z) +\r\n                               time2_z + time2_z2 + \r\n                               (1|date) + (1|weevil) + (1|tree), \r\n                             family=""poisson"", \r\n                             data = weevil_comb_complete)            \r\nsummary(blength_mlselec_m5)\r\nAnova(blength_mlselec_m5, type=""II"")\r\n\r\n#Does social selection interact with density? Model 4####\r\n\r\nblength_mlselec_m6 = glmmTMB(num_mates ~ sex*scale(density)*\r\n                               (body_length_z + neigh_mean_blength_z ) +\r\n                               time2_z + time2_z2 +\r\n                               (1|date) + (1|weevil) + (1|tree), \r\n                             family=""poisson"", \r\n                             data = weevil_comb_complete)            \r\nsummary(blength_mlselec_m6)\r\nAnova(blength_mlselec_m6, type=""II"")\r\n\r\n#Does social selection interact with sex ratio? Model 5####\r\n\r\nblength_mlselec_m3 = glmmTMB(num_mates ~ sex*scale(sex_ratio)*\r\n                               (body_length_z + neigh_mean_blength_z) +\r\n                                time2_z + time2_z2 + \r\n                                 (1|date) + (1|weevil) + (1|tree), \r\n                             family=""poisson"",\r\n                             data = weevil_comb_complete)         \r\nsummary(blength_mlselec_m3)\r\nAnova(blength_mlselec_m3, type=""II"")\r\n\r\n\r\n#Plot how social selection changes with density####\r\n\r\nwith(weevil_comb_complete, hist(density)) #natural break after 40, \r\n\r\nweevil_comb_complete$densityf = ifelse(weevil_comb_complete$density>40, ""High"", ""Low"")\r\n\r\nweevil_comb_complete$cop_pred = predict(blength_mlselec_m6, type=""response"") #using model with density\r\n\r\nnum_mates_den_f = ggplot(weevil_comb_complete, \r\n                         aes(x = neigh_mean_blength, \r\n                             y = num_mates, \r\n                             col = densityf)) +\r\n  geom_jitter(height=0.05) + \r\n  theme_classic() +\r\n  scale_color_manual(values = c(""dodgerblue"", ""black"")) +\r\n  xlab(""Mean size of rivals (mm)"") + \r\n  ylab (""Number of mating partners"") +\r\n  labs(col = ""Density"") +\r\n  theme(axis.title.x = element_text(size = 16),\r\n        axis.text.x = element_text(size = 14),\r\n        axis.title.y = element_text(size = 16),\r\n        axis.text.y = element_text(size = 14),\r\n        axis.line.x = element_line(color=""black"", size = 1),\r\n        axis.line.y = element_line(color=""black"", size = 1),\r\n        legend.key.size =  unit(0.5, ""in"")) +\r\n  stat_smooth(inherit.aes=F, aes(x = neigh_mean_blength, \r\n                                 y = cop_pred, \r\n                                 group = densityf, \r\n                                col=densityf),\r\n              show.legend = T,\r\n              method = ""glm"", se = T,\r\n              method.args = list(family = ""poisson"")) +\r\n  facet_wrap(~sex, scale=""free"") \r\n\r\n\r\n#Estimating the interactant covariance####\r\n\r\nwith(weevil_comb_complete[weevil_comb_complete$sex==""F"",], \r\n     cor.test(scale(body_length),scale(neigh_mean_blength)))\r\nwith(weevil_comb_complete[weevil_comb_complete$sex==""M"",],\r\n     cor.test(scale(body_length),scale(neigh_mean_blength))) \r\n\r\n#test whether assortment changes with density with a model\r\n\r\nweevil_comb_complete$neigh_mean_blength_z = as.numeric(weevil_comb_complete$neigh_mean_blength_z)\r\n\r\nassort_m1 = glmmTMB(neigh_mean_blength_z ~ sex * body_length_z * scale(density) + \r\n                               (1|date) + (1|weevil) + (1|tree), \r\n                             data = weevil_comb_complete)            \r\nsummary(assort_m1)\r\nAnova(assort_m1, type=""II"")\r\n#sig body length-density interaction, so relationship gets more positive as density increase. Starts negative, changes to be positive\r\n\r\n#Plot how phenotypic assortment changes with density####\r\n\r\nassort_den_f = ggplot(weevil_comb_complete, \r\n                         aes(x = body_length,\r\n                             y = neigh_mean_blength, \r\n                             col = densityf)) +\r\n  geom_jitter(height=0.05) + \r\n  theme_classic() +\r\n  scale_color_manual(values = c(""dodgerblue"", ""black"")) +\r\n  ylab(""Mean size of rivals (mm)"") + \r\n  xlab (""Body length (mm)"") +\r\n  labs(col = ""Density"") +\r\n  theme(axis.title.x = element_text(size = 16),\r\n        axis.text.x = element_text(size = 14),\r\n        axis.title.y = element_text(size = 16),\r\n        axis.text.y = element_text(size = 14),\r\n        axis.line.x = element_line(color=""black"", size = 1),\r\n        axis.line.y = element_line(color=""black"", size = 1),\r\n        legend.key.size =  unit(0.5, ""in"")) +\r\n  stat_smooth(aes(group = densityf, \r\n                  col = densityf), \r\n              show.legend = T,\r\n              method = ""lm"", se = T) +\r\n  facet_wrap(~sex, scale=""free"") \r\n\r\n\r\n#So the negative SS at high densities may have limited impact if assortment is near zero and stays that way across densities\r\n\r\n#What are the overall selection differentials?####\r\n\r\n#Females:\r\n(fixef(blength_mlselec_m1)[[1]][3] + \r\n   fixef(blength_mlselec_m1)[[1]][4]* with(weevil_comb_complete[weevil_comb_complete$sex==""F"",], \r\n                                           cor.test(scale(body_length),scale(neigh_mean_blength))$est) )*\r\n  var(weevil_comb_complete$body_length[weevil_comb_complete$sex == ""F""])\r\n# = 5.568\r\n\r\n#Males:\r\n(fixef(blength_mlselec_m1)[[1]][3] + fixef(blength_mlselec_m1)[[1]][9] +\r\n    (fixef(blength_mlselec_m1)[[1]][4] + fixef(blength_mlselec_m1)[[1]][10])* \r\n    with(weevil_comb_complete[weevil_comb_complete$sex==""M"",], \r\n         cor.test(scale(body_length),scale(neigh_mean_blength))$est) )*\r\n  var(weevil_comb_complete$body_length[weevil_comb_complete$sex == ""M""])\r\n# = 41.177\r\n\r\n\r\n\r\n#Plot how selection differentials change with density####\r\n\r\n\r\n#range density (scaled) from min to max\r\ndensity = seq(min(as.numeric(scale(weevil_comb_complete$density))),\r\n              max(as.numeric(scale(weevil_comb_complete$density))),0.1)\r\n\r\n#Selection differentials for females\r\nds_f =  fixef(blength_mlselec_m6)[[1]][4] + density * fixef(blength_mlselec_m6)[[1]][11]\r\nss_f = fixef(blength_mlselec_m6)[[1]][5] + density * fixef(blength_mlselec_m6)[[1]][12]\r\ncij_f = fixef(assort_m1)[[1]][3] + density * fixef(assort_m1)[[1]][7]\r\nvar_f = var(weevil_comb_complete$body_length[weevil_comb_complete$sex == ""F""])\r\nS_f = var_f*(ds_f+ss_f*cij_f) #gradual decrease with density\r\n\r\n#Selection differentials for males\r\nds_m = fixef(blength_mlselec_m6)[[1]][4] + fixef(blength_mlselec_m6)[[1]][9] +\r\n  density * (fixef(blength_mlselec_m6)[[1]][11] + fixef(blength_mlselec_m6)[[1]][13])\r\nss_m = fixef(blength_mlselec_m6)[[1]][5] + fixef(blength_mlselec_m6)[[1]][10] + \r\n  density * (fixef(blength_mlselec_m6)[[1]][12] + fixef(blength_mlselec_m6)[[1]][14])\r\ncij_m = fixef(assort_m1)[[1]][3] +  fixef(assort_m1)[[1]][5] +\r\n  density * (fixef(assort_m1)[[1]][7] + fixef(assort_m1)[[1]][8])\r\nvar_m = var(weevil_comb_complete$body_length[weevil_comb_complete$sex == ""M""])\r\nS_m = var_m*(ds_m+ss_m*cij_m) #increase with density\r\n\r\n\r\nreal_density = density * sd(weevil_comb_complete$density) + mean(weevil_comb_complete$density)\r\n\r\n#single panel\r\npar(mfrow=c(1,3), mar = c(5,5,4,2))\r\n\r\nplot(S_f ~ real_density, \r\n     ylim = c(0,60),type=""l"", \r\n     xlab = """",\r\n     ylab = ""Total selection differential"",\r\n     cex.lab = 1.5,\r\n     frame = F, las=1, main=""a."")\r\nbox(which = ""plot"", bty = ""l"")\r\npoints(S_m ~ real_density, lty = 2, type=""l"")\r\n\r\nplot(ds_f*var_f ~ real_density, \r\n     ylim = c(0,60),type=""l"", \r\n     xlab = ""Density (weevils per tree)"",\r\n     ylab = ""Direct selection differential"",\r\n     cex.lab = 1.5,\r\n     frame = F, las=1, main=""b."")\r\nbox(which = ""plot"", bty = ""l"")\r\npoints(ds_m*var_m ~ real_density, lty = 2, type=""l"")\r\n\r\nplot(ss_f*cij_f*var_f ~ real_density, \r\n     ylim = c(-1.5,60),type=""l"", \r\n     xlab = """",\r\n     ylab = ""Social selection differential"",\r\n     cex.lab = 1.5,\r\n     frame = F, las=1, main=""c."")\r\nbox(which = ""plot"", bty = ""l"")\r\npoints(ss_m*cij_m*var_m ~ real_density, lty = 2, type=""l"")\r\nlegend(""topleft"",\r\n       legend=c(""Females"", ""Males""),\r\n       cex = 1.5,\r\n       col=""black"",text.col=""black"",\r\n       lty=c(1,2),\r\n       #bty=""n"",\r\n       inset = c(0.05,0.05))\r\n\r\n#END####\r\n\r\n']","Social selection is density dependent but makes little contribution to total selection in New Zealand giraffe weevils Social selection occurs when traits of interaction partners influence an individual's fitness and can alter total selection strength. However, we have little idea of what factors influence social selection's strength. Further, social selection only contributes to overall selection when there is phenotypic assortment, but simultaneous estimates of social selection and phenotypic assortment are rare. Here we estimated social selection on body size in a wild population of New Zealand giraffe weevils (Lasiorhynchus barbicornis). We measured phenotypic assortment by body size and tested whether social selection varied with sex-ratio, density, and interacted with the body size of the focal individual. Social selection was limited and unaffected by sex ratio or the size of the focal individual. However, at high densities social selection was negative for both sexes, consistent with size-based competitive interactions for access to mates. Phenotypic assortment was always close to zero, indicating negative social selection at high densities will not impede the evolution of larger body sizes. Despite its predicted importance, social selection may only influence evolutionary change in specific contexts, leaving direct selection to drive evolutionary change.",2
Data science with R,"The purpose of this course is to present researchers and scientists with R implementation of Machine Learning methods. The first part of the course will consist of introductory lectures on popular Machine Learning algorithms including unsupervised methods (Clustering, Association Rules) and supervised ones (Decision Trees, Naive Bayes, Random Forests and Deep Neural Network). Basic Machine Learning concepts such as training set, test set, validation set, overfitting, bagging, boosting will be introduced as well as performance evaluation for supervised and unsupervised methods.The second part will consist of practical exercises such as reading data, using packages and building machine learning applications. Different options for parallel programming will be shown using specific R packages (parallel, h2o,). For Deep Learning applications the Keras package will be presented. The examples will cover the analysis of large datasets and images datasets. Participants will use R on Cineca HPC facilities for practical assignments.Skills:At the end of the course, the student will be expected to have acquired:  the ability to perform basic operations on matrices and dataframes  the ability to manage packages  the ability to navigate in the RStudio interface  a general knowledge of Machine and Deep Learning methods  a general knowledge of the most popular packages for Machine and Deep Learning  a basic knowledge of different parallel programming techniques  the ability to build machine learning applications with large datasets and images datasetsTarget audience:Students and researchers with different backgrounds, looking for technologies and methods to analyze a large amount of data.Pre-requisites:Participants must have a basic statistics knowledge. Participants must also be familiar with basic Linux and R language.","['library(caret)\r\nlibrary(dplyr)\r\nlibrary(lime)\r\nlibrary(doParallel)\r\ncluster <- makeCluster(detectCores()-1) # convention to leave 1 core for OS\r\nregisterDoParallel(cluster)\r\nsetwd(""C:/Users/g.pedrazzi/Documents/corsi/bbs/R_supervised"")\r\ncovtype = read.csv(""data/covtype.full.csv"",header=TRUE, sep="","", stringsAsFactors=FALSE)\r\ncovtype<-droplevels(covtype)\r\ntable(covtype$Cover_Type)\r\ncovtype=covtype %>% mutate_if(is.character,as.factor)\r\ndt = sort(sample(nrow(covtype), nrow(covtype)*.8))\r\ntrain<-covtype[dt,]\r\ntest<-covtype[-dt,]\r\ntrain <- train %>% group_by(Cover_Type) %>% sample_n(2000)\r\ntable(train$Cover_Type)\r\ntc <- trainControl(method = ""cv"", number = 5,verboseIter = FALSE,p = 0.8)\r\ngrid <- expand.grid(interaction.depth = seq(1,6, by = 2),\r\n                    n.trees = c(10,20,30),\r\n                    shrinkage = c(0.1),\r\n                    n.minobsinnode = c(3,6))\r\n\r\n############## run the model\r\n\r\ngbm1_cv <- train(Cover_Type~.,\r\n                 data = train, method = ""gbm"",\r\n                 trControl = tc , tuneGrid = grid,metric=\'Accuracy\') # here\r\n\r\ngbm1_cv\r\npred_gbm1<-predict(gbm1_cv, newdata=test)\r\nconfusionMatrix(pred_gbm1, test$Cover_Type)\r\nlibrary(gbm)\r\npar(mar = c(5, 8, 1, 1))\r\nsummary(\r\n  gbm1_cv, \r\n  cBars = 15,\r\n  method = relative.influence, # also can use permutation.test.gbm\r\n  las = 2\r\n)\r\n\r\nexplainer <- lime(train, gbm1_cv)\r\nexplanation <- explain(test[1,], explainer, n_features = 5,n_labels = 4)\r\nplot_features(explanation)\r\n\r\n\r\n#Challenge: use two more classification algorithm and discuss the results\r\n', '#### installation of required packages - first time execution ####\r\ninstall.packages(""devtools"")\r\nlibrary(""devtools"")\r\ndevtools::install_github(""rstudio/reticulate"")\r\ndevtools::install_github(""rstudio/tensorflow"")\r\ndevtools::install_github(""rstudio/keras"")\r\n#reticulate da devtools\r\n\r\n#install.packages(""base64enc"")\r\n\r\n#### set libraries and  variables #### \r\nlibrary(tensorflow)\r\nlibrary(keras)\r\ninstall_tensorflow()\r\ninstall_keras()\r\nsetwd(""C:/Users/g.pedrazzi/Documents/data"")\r\ntrain_directory <- ""train""\r\nvalidation_directory <- ""validation""\r\naugment_directory<-""dataaug/""\r\nimg_width <- 150\r\nimg_height <- 150\r\nbatch_size <- 32\r\nepochs <- 30\r\n#epochs <- 2\r\ntrain_samples = 2048\r\nvalidation_samples = 832\r\n\r\n\r\n#### read images from directories for train and validation ####\r\n# By applying random transformation to our train set, we artificially enhance our dataset with new unseen images.\r\n# This will hopefully reduce overfitting and allows better generalization capability for our network.\r\naugment <- image_data_generator(rescale=1./255,\r\n                                shear_range=0.2,\r\n                                zoom_range=0.2,\r\n                                horizontal_flip=TRUE)\r\ntrain_generator <- flow_images_from_directory(train_directory, generator = augment,\r\n                                              target_size = c(img_width, img_height), color_mode = ""rgb"",\r\n                                              class_mode = ""categorical"", batch_size = batch_size, shuffle = TRUE,\r\n                                              seed = 123)\r\n\r\nvalidation_generator <- flow_images_from_directory(validation_directory, generator = image_data_generator(rescale=1./255),\r\n                                                   target_size = c(img_width, img_height), color_mode = ""rgb"",\r\n                                                   classes=NULL,\r\n                                                   class_mode = ""categorical"", batch_size = batch_size, shuffle = TRUE,\r\n                                                   seed = 123)\r\n\r\n\r\n\r\n\r\n\r\n#### Model architecture definition ####\r\n\r\nmodel <- keras_model_sequential()\r\n\r\nmodel %>%\r\n  layer_conv_2d(filter = 32, kernel_size = c(3,3), input_shape = c(img_width, img_height, 3)) %>%\r\n  layer_activation(""relu"") %>%\r\n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \r\n  \r\n  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%\r\n  layer_activation(""relu"") %>%\r\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\r\n  \r\n  layer_conv_2d(filter = 64, kernel_size = c(3,3)) %>%\r\n  layer_activation(""relu"") %>%\r\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\r\n  \r\n  layer_flatten() %>%\r\n  layer_dense(64) %>%\r\n  layer_activation(""relu"") %>%\r\n  layer_dropout(0.5) %>%\r\n  layer_dense(2) %>%\r\n  layer_activation(""softmax"")\r\n\r\n#### model generation ####\r\nmodel %>% compile(\r\n  loss = ""categorical_crossentropy"",\r\n  optimizer = optimizer_rmsprop(lr = 0.0001, decay = 1e-6),\r\n  metrics = ""categorical_accuracy""\r\n)\r\n\r\n\r\n\r\nmodel %>% fit_generator(\r\n  train_generator,\r\n  steps_per_epoch = as.integer(train_samples/batch_size), \r\n  epochs = 5, \r\n  validation_data = validation_generator,\r\n  validation_steps = as.integer(validation_samples/batch_size),\r\n  verbose=2\r\n)\r\n\r\n#### model summary, saving and evaluation ####\r\nsummary(model)\r\nsave_model_hdf5(model, \'basic_cnn_30_epochsR_cat.h5\', overwrite = TRUE)\r\nmodel<-load_model_hdf5(\'basic_cnn_30_epochsR_cat.h5\')\r\n\r\n\r\n#### classification of images in a directory #####\r\nsetwd(""C:/Users/g.pedrazzi/Documents/data"")\r\ntrain_directory <- ""train""\r\nvalidation_directory <- ""validation""\r\ntest_generator <- flow_images_from_directory(validation_directory, generator = augment,\r\n                                             target_size = c(img_width, img_height), color_mode = ""rgb"",\r\n                                             classes=NULL,\r\n                                             class_mode = ""categorical"", batch_size = 832, shuffle = FALSE,\r\n                                             save_to_dir = augment_directory ,\r\n                                             save_prefix = ""aug_"",\r\n                                             save_format = ""png"", follow_links = TRUE,\r\n                                             seed = 123)\r\nreticulate::iter_next(test_generator)\r\n\r\n#### check accuracy results on validation_set ####\r\nresults_tot<-as.data.frame(predict_generator(model, test_generator,1,verbose=1))\r\nresults_tot$numrow <- as.integer(rownames(results_tot)) \r\nresults_tot$correct<-ifelse((results_tot$numrow<=416) & (results_tot$V1>=results_tot$V2),1,ifelse((results_tot$numrow>416) & (results_tot$V2>=results_tot$V1),1,0))\r\nsum(results_tot$correct)/832\r\n', 'library(keras)\r\nconv_base <- application_vgg16(\r\n  weights = ""imagenet"",\r\n  include_top = FALSE,\r\n  input_shape = c(150, 150, 3)\r\n)\r\nsummary(conv_base)\r\nsetwd(""C:/Users/g.pedrazzi/Documents/data2"")\r\ntrain_directory <- ""train""\r\nvalidation_directory <- ""validation""\r\ndatagen <- image_data_generator(rescale = 1/255)\r\nbatch_size <- 1\r\n###extract feature from vgg16\r\n# Feature extraction consists of using the representations learned\r\n# by a previous network to extract interesting features from new samples.\r\n# These features are then run through a new classifier, which is trained from scratch.\r\nextract_features <- function(directory, sample_count) {\r\n  \r\n  features <- array(0, dim = c(sample_count, 4, 4, 512))  \r\n  labels <- array(0, dim = c(sample_count))\r\n  \r\n  generator <- flow_images_from_directory(\r\n    directory = directory,\r\n    generator = datagen,\r\n    target_size = c(150, 150),\r\n    batch_size = batch_size,\r\n    class_mode = ""binary""\r\n  )\r\n  \r\n  i <- 0\r\n  while(TRUE) {\r\n    batch <- generator_next(generator)\r\n    inputs_batch <- batch[[1]]\r\n    labels_batch <- batch[[2]]\r\n    features_batch <- conv_base %>% predict(inputs_batch)\r\n    \r\n    index_range <- ((i * batch_size)+1):((i + 1) * batch_size)\r\n    features[index_range,,,] <- features_batch\r\n    labels[index_range] <- labels_batch\r\n    \r\n    i <- i + 1\r\n    if (i * batch_size >= sample_count)\r\n      # Note that because generators yield data indefinitely in a loop, \r\n      # you must break after every image has been seen once.\r\n      break\r\n  }\r\n  \r\n  list(\r\n    features = features, \r\n    labels = labels\r\n  )\r\n}\r\n\r\ntrain <- extract_features(train_directory, 28)\r\nvalidation <- extract_features(validation_directory, 12)\r\n\r\nreshape_features <- function(features) {\r\n  array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))\r\n}\r\ntrain$features <- reshape_features(train$features)\r\nvalidation$features <- reshape_features(validation$features)\r\n\r\n\r\nmodel <- keras_model_sequential() %>% \r\n  layer_dense(units = 256, activation = ""relu"", \r\n              input_shape = 4 * 4 * 512) %>% \r\n  layer_dropout(rate = 0.5) %>% \r\n  layer_dense(units = 1, activation = ""sigmoid"")\r\n\r\nmodel %>% compile(\r\n  optimizer = optimizer_rmsprop(lr = 2e-5),\r\n  loss = ""binary_crossentropy"",\r\n  metrics = c(""accuracy"")\r\n)\r\nhistory <- model %>% fit(\r\n  train$features, train$labels,\r\n  epochs = 100,\r\n  batch_size = 20,\r\n  validation_data = list(validation$features, validation$labels)\r\n)\r\nplot(history)', 'library(dplyr)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(h2o)\nlibrary(randomForest)\n#library(e1071)\nlibrary(mlbench)\n\n\ndata <- read.csv(""DIAB_ML_BBS2.csv"",sep="";"",header=TRUE)\ndim(data)\n\nnewdata <- data[ which(data$ANNO==2017 | data$ANNO==2018), ]\ndim(newdata)\n\n\n\nnewdata$SESSO <- as.character(newdata$SESSO)\n\nnewdata[newdata == ""M""] <- 1\nnewdata[newdata == ""F""] <- 2\n\n\n# excluding  variables that I don\'t need \n\nmyvars <- names(newdata) %in% c(""USA_FARMACI"",""ANNO"",""ID_PAZ"",""DIABETE"",""PEZ_FT"",""SPESA_FT"",""SPESA_SDO"",""SPESA_SPA"",""SPE_TOT"")\nnewdata <- newdata[!myvars]\n\ndim(newdata)\n\n\nfor (colname in colnames(newdata)) {\n  if (colname != ""ETAX"" & colname!=""CHARLSON_INDEX"") \n  {newdata[[colname]] <- as.factor(newdata[[colname]])\n  }}\n\n\ntraining.samples <- newdata$DIAB_RIC %>% \n  createDataPartition(p = 0.75, list = FALSE)\ntrainset  <- newdata[training.samples, ]\ntestset <- newdata[-training.samples, ]\ndim(testset)\ndim(trainset)\n\n# RANDOM FOREST 1\nset.seed(123)\nrf <- randomForest(DIAB_RIC ~ .,data=trainset, ntree=500)\nrf\n\n\n# predict new obs and confusion table\nrf1_pred <- predict(rf, newdata=testset,type=""class"")\ncm= confusionMatrix(rf1_pred, testset$DIAB_RIC, positive=""1"")\ncm\n\n\n\n\ncm = as.matrix(table(Actual = testset$DIAB_RIC, Predicted = rf1_pred)) # create the confusion matrix\ncm\n\nn = sum(cm) # number of instances\nnc = nrow(cm) # number of classes\ndiag = diag(cm) # number of correctly classified instances per class \nrowsums = apply(cm, 1, sum) # number of instances per class\ncolsums = apply(cm, 2, sum) # number of predictions per class\np = rowsums / n # distribution of instances over the actual classes\nq = colsums / n # distribution of instances over the predicted classes\n\naccuracy = sum(diag) / n \n\nprecision = diag / colsums \nrecall = diag / rowsums \nf1 = 2 * precision * recall / (precision + recall) \ndata.frame(precision, recall, f1,accuracy) \n\nmacroPrecision = mean(precision)\nmacroRecall = mean(recall)\nmacroF1 = mean(f1)\n\n\nrfimp <- varImp(rf, scale = FALSE)\nrfimp\n\n# RANDOM FOREST 2 - CARET\n\ncontrol <- trainControl(method=\'cv\', \n                        number=10)\n#Metric compare model is Accuracy\nmetric <- ""Accuracy""\nset.seed(123)\nmtry <- sqrt(ncol(testset))\n\ntunegrid <- expand.grid(.mtry=mtry)\nrf_caret <- train(DIAB_RIC~., \n                    data=trainset, \n                    method=\'rf\', \n                    metric=\'Accuracy\', \n                    tuneGrid=tunegrid, \n                    trControl=control)\nprint(rf_caret)\n\n\nrf2_pred <- predict(rf_caret, newdata=testset,type=""prob"")\n\n\ncm2 = as.matrix(table(Actual = testset$DIAB_RIC, Predicted = rf2_pred)) # create the confusion matrix\ncm2\n\nn = sum(cm2) # number of instances\nnc = nrow(cm2) # number of classes\ndiag = diag(cm2) # number of correctly classified instances per class \nrowsums = apply(cm2, 1, sum) # number of instances per class\ncolsums = apply(cm2, 2, sum) # number of predictions per class\np = rowsums / n # distribution of instances over the actual classes\nq = colsums / n # distribution of instances over the predicted classes\n\naccuracy = sum(diag) / n \n\nprecision = diag / colsums \nrecall = diag / rowsums \nf1 = 2 * precision * recall / (precision + recall) \ndata.frame(precision, recall, f1,accuracy) \n\nrfimp_caret <- varImp(rf, scale = FALSE)\nrfimp_caret\n\n# H20 AUTOML\n\nlibrary(h2o)\nh2o.init()\n\n\n#prova automl\n\nindexes <- sample(1:nrow(newdata), size=0.25*nrow(newdata))\n# Split data\ntest = as.h2o(newdata[indexes,])\ntrain = as.h2o(newdata[-indexes,])\n\ny <- names(newdata[3])\nx <- setdiff(names(train), y)\n\n# For binary classification, response should be a factor\ntrain[, y] <- as.factor(train[, y])\ntest[, y] <- as.factor(test[, y])\n\n# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\naml <- h2o.automl(x = x, y = y,\n                  training_frame = train,\n                  max_models = 10,\n                  seed = 1234)\n\n# View the AutoML Leaderboard\nlb <- aml@leaderboard\nprint(lb, n = nrow(lb))  # Print all rows instead of default (6 rows)\n\n# Best model variable importance \n\nm <- h2o.getModel(""GBM_2_AutoML_20210615"")\nh2o.varimp(m)\nh2o.varimp_plot(m)\n\n\n# to see all the varimp\n\nmodel_ids <- as.data.frame(lb$model_id)[,1]\n\nfor (model_id in model_ids) {\n  print(model_id)\n  m <- h2o.getModel(model_id)\n  h2o.varimp(m)\n  h2o.varimp_plot(m)\n}\n\n\n']","Data science with R The purpose of this course is to present researchers and scientists with R implementation of Machine Learning methods. The first part of the course will consist of introductory lectures on popular Machine Learning algorithms including unsupervised methods (Clustering, Association Rules) and supervised ones (Decision Trees, Naive Bayes, Random Forests and Deep Neural Network). Basic Machine Learning concepts such as training set, test set, validation set, overfitting, bagging, boosting will be introduced as well as performance evaluation for supervised and unsupervised methods.The second part will consist of practical exercises such as reading data, using packages and building machine learning applications. Different options for parallel programming will be shown using specific R packages (parallel, h2o,). For Deep Learning applications the Keras package will be presented. The examples will cover the analysis of large datasets and images datasets. Participants will use R on Cineca HPC facilities for practical assignments.Skills:At the end of the course, the student will be expected to have acquired:  the ability to perform basic operations on matrices and dataframes  the ability to manage packages  the ability to navigate in the RStudio interface  a general knowledge of Machine and Deep Learning methods  a general knowledge of the most popular packages for Machine and Deep Learning  a basic knowledge of different parallel programming techniques  the ability to build machine learning applications with large datasets and images datasetsTarget audience:Students and researchers with different backgrounds, looking for technologies and methods to analyze a large amount of data.Pre-requisites:Participants must have a basic statistics knowledge. Participants must also be familiar with basic Linux and R language.",2
"Data from: Evidence for personality conformity, not social niche specialization in social jays","Animal personality traits are defined as consistent individual differences in behavior over time and across contexts. Occasionally this inflexibility results in maladaptive behavioral responses to external stimuli. However, in social groups inflexible behavioral phenotypes might be favored as this could lead to more predictable social interactions. Two hypotheses seek to describe the optimal distribution of personality types within groups. The social niche specialization hypothesis states that individuals within groups should partition social roles, like personality types, to avoid conflict. Whereas the conformity hypothesis states that individuals should assort with conspecifics of similar personality. However no research so far has compared these hypotheses using data from wild animal systems. We tested boldness in the wild on two species with different social systems, the Mexican Jay and California Scrub-Jay. We found support for the conformity hypothesis over the social niche specialization hypothesis because individuals within groups of the social species had more similar personalities, and consequently there was a statistically significant group effect. The most likely mechanism for this conformity is social learning of behaviors through development, but more explicit research on this is needed.","['library(WriteXLS)\nlibrary(gdata)\nlibrary(igraph)\nlibrary(plyr)\nlibrary(stats)\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(reshape)\nlibrary(nlme)\nlibrary(lme4)\nlibrary(RInSp)\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(psych)\nlibrary(parallel)\nlibrary(lubridate)\nlibrary(rptR)\nlibrary(lemon)\n\nsummarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,\n                      conf.interval=.95, .drop=TRUE) {\n  library(plyr)\n  \n  # New version of length which can handle NA\'s: if na.rm==T, don\'t count them\n  length2 <- function (x, na.rm=FALSE) {\n    if (na.rm) sum(!is.na(x))\n    else       length(x)\n  }\n  \n  # This does the summary. For each group\'s data frame, return a vector with\n  # N, mean, and sd\n  datac <- ddply(data, groupvars, .drop=.drop,\n                 .fun = function(xx, col) {\n                   c(N    = length2(xx[[col]], na.rm=na.rm),\n                     mean = mean   (xx[[col]], na.rm=na.rm),\n                     sd   = sd     (xx[[col]], na.rm=na.rm)\n                   )\n                 },\n                 measurevar\n  )\n  \n  # Rename the ""mean"" column    \n  datac <- rename(datac, c(""mean"" = measurevar))\n  \n  datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean\n  \n  # Confidence interval multiplier for standard error\n  # Calculate t-statistic for confidence interval: \n  # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1\n  ciMult <- qt(conf.interval/2 + .5, datac$N-1)\n  datac$ci <- datac$se * ciMult\n  \n  return(datac)\n}\n\n\n\n############## Novel object analyses ##########\n\nNO.o = read.csv(""Novel object.csv"")\ncolnames(NO.o)[3]<- ""Group""\n\nNO = NO.o[-which(NO.o$Response.time==999),]\nNO$ID = as.factor(as.character(NO$ID))\nNO$SEC = ifelse(is.na(NO$SEC), NO$Response.time, NO$SEC)\n\n\n### Treatment effect? ####\nNO$DISTANCE[which(NO$LEVEL == ""1"")]<- 0.4\nNO$DISTANCE[which(NO$LEVEL == ""2"")]<- 0.2\nNO$DISTANCE[which(NO$LEVEL == ""3"")]<- 0.01\n\n\ncavers.no = aggregate(DISTANCE ~ ID + TREATMENT, data = NO[which(NO$Species == ""CASJ""),], FUN = ""min"")\ncavers.no$minDist = cavers.no$DISTANCE*100\nct1 = glmer(minDist ~ TREATMENT + (1|ID), family = ""poisson"", data = cavers.no)\nsummary(ct1)\n# beta = 2.86, z = 18.48, p < 0.01... as you switch from control to treatment, minimum approach distance increases by 3.1\n\nmavers.no = aggregate(DISTANCE ~ ID + TREATMENT, data = NO[which(NO$Species == ""MEJA""),], FUN = ""min"")\nmavers.no$minDist = mavers.no$DISTANCE*100\n#remove hatch year birds\nmavers.no = mavers.no[-which(mavers.no$ID == ""MOR-OX"" | mavers.no$ID == ""WBR-XSK"" |mavers.no$ID == ""KY-X"" |\n                   mavers.no$ID == ""SY-X"" |mavers.no$ID == ""PBR-BOX"" |\n                   mavers.no$ID == ""GOY-KOX"" |mavers.no$ID == ""POW-RXR""),]\nmt1 = glmer(minDist ~ TREATMENT + (1|ID), family = ""poisson"", data = mavers.no)\nsummary(mt1)\n# beta = 1.63, z = 25.13, p < 0.001\n\ndescribe(cavers.no$minDist[which(cavers.no$TREATMENT == ""CON"")])\n# CON mean = 1.73 se = 0.73 \ndescribe(cavers.no$minDist[which(cavers.no$TREATMENT == ""EXP"")])\n# EXP mean = 37.94 se = 11.1\ndescribe(mavers.no$minDist[which(mavers.no$TREATMENT == ""CON"")])\n# CON mean = 8.08 se = 1.99\ndescribe(mavers.no$minDist[which(mavers.no$TREATMENT == ""EXP"")])\n# EXP mean = 52.26 se = 11.27\n\n### Species differences? ###\ncavers.no$Species = ""CASJ""\nmavers.no$Species = ""MEJA""\nboth.no = rbind(mavers.no,cavers.no)\nb1 = glmer(minDist ~ Species + (1|ID), family = ""poisson"", data = both.no)\nsummary(b1)\n# Beta(MEJA) = 0.46, z = 1.17, p = 0.24\n\n\n\n### Include order of first approach ###\nNOe2 = NO[which(NO$Trial == 1 & NO$TREATMENT == ""EXP""),]\nNOe2$order = NA\n\nfor(i in unique(NOe2$Group)){\n  tmp = NOe2[which(NOe2$Group == i),]\n  NOe2 = NOe2[-which(NOe2$Group == i),]\n  tmp = tmp[order(tmp$SEC),]\n  tmp$order = 1:nrow(tmp)\n  NOe2 = rbind(tmp, NOe2)\n}\n\n\nNOe3 = NO[which(NO$Trial == 2 & NO$TREATMENT == ""EXP""),]\nNOe3$order = NA\n\nfor(i in unique(NOe3$Group)){\n  tmp = NOe3[which(NOe3$Group == i),]\n  NOe3 = NOe3[-which(NOe3$Group == i),]\n  tmp = tmp[order(tmp$SEC),]\n  tmp$order = 1:nrow(tmp)\n  NOe3 = rbind(tmp, NOe3)\n}\n\n\nNOe = rbind(NOe2, NOe3)\n\nNOe$LEVEL = as.numeric(NOe$LEVEL)\nNOe$Response.time = as.numeric(NOe$Response.time)\nNOe$SEC = as.numeric(NOe$SEC)\ncor(NOe[,c(6,7,10,13)])\n# Level and distance correlated >= 0.7\n\nsummary(glmer(order ~ DISTANCE + (1|Group),family = ""poisson"", data = NOe[which(NOe$Trial ==1),]))\n# Distance significant p < 0.01, B = -0.34 ... as order increases, distance decreases\nsummary(glmer(order ~ DISTANCE + (1|Group),family = ""poisson"", data = NOe[which(NOe$Trial ==2),]))\n# Distance also significant p < 0.01, B = -0.68 ... as order increases, distance decreases\n# When species analyzed separately, order effect is stronger in CASJ\n\n\n### So NO model needs to include order of first approach\n# Aggregate data to create variables\n\n# First approach order on each trial - how many jays approached before focal jay\ntmp = aggregate(NOe$order ~ NOe$ID + NOe$Trial + NOe$Species + NOe$Group, FUN = ""min"")\ncolnames(tmp) = c(""ID"", ""Trial"", ""Species"",""Group"", ""min.order"")\n\n# closest approach to the duck\ntmp1 = aggregate(NOe$DISTANCE ~ NOe$ID + NOe$Trial, FUN = ""min"")\ncolnames(tmp1) = c(""ID"", ""Trial"", ""MINDistance"")\n\n# Average time between peanuts\nNOe$Latency[which(NOe$Latency == NOe$Response.time)]<- 0\ntmp2 = aggregate(Latency ~ ID + Trial,data = NOe, FUN = ""mean"")\ncolnames(tmp2) = c(""ID"", ""Trial"", ""Latency"")\n\n# How long did it take it to come down to the setup at the beginning of the trial\ntmp3 = aggregate(Response.time ~ ID + Trial, data = NOe, FUN = ""max"")\ncolnames(tmp3) = c(""ID"", ""Trial"", ""Response.time"")\n\n# Average order of approach over all approaches within a trial\ntmp4 = aggregate(order ~ ID + Trial, data = NOe, FUN = ""mean"")\ncolnames(tmp4) = c(""ID"", ""Trial"", ""MeanOrder"")\n\ndata.e = merge(tmp, tmp1, by = c(""ID"", ""Trial""))\ndata.e = merge(data.e, tmp2, by = c(""ID"", ""Trial""))\ndata.e = merge(data.e,tmp3, by = c(""ID"", ""Trial""))\ndata.e = merge(data.e,tmp4, by = c(""ID"", ""Trial""))\n\n# average first approach order across the two trials\nmin.order = aggregate(min.order ~ ID, data = data.e, FUN = ""mean"")\ncolnames(min.order) = c(""ID"", ""min.order2"")\ndata.e = merge(min.order, data.e, by = ""ID"", all = T)\n\ncor(data.e[,c(2,6:10)])\n# order variables all correlated\n\n\n###### NO - CASJ Repeatability ########\ncdata.e = data.e[which(data.e$Species == ""CASJ""),]\ncdata.e$minDist = cdata.e$MINDistance*100 # meters to centimeters\n## individual\n# Keep territories with data from only 1 jay, remove jays with only 1 trial\ncdata.rpt2.e = cdata.e[-which(cdata.e$ID == ""BORX"" | cdata.e$ID == ""BRYX"" |\n                                cdata.e$ID == ""GGYX"" |cdata.e$ID == ""GWRX"" |\n                                cdata.e$ID == ""OGGX"" |cdata.e$ID == ""OOBX"" |\n                                cdata.e$ID == ""OYBX"" |cdata.e$ID == ""PRWX"" |\n                                cdata.e$ID == ""RBBX"" |cdata.e$ID == ""RBGX"" |\n                                cdata.e$ID == ""RGGX"" |cdata.e$ID == ""RGWX"" |\n                                cdata.e$ID == ""WRRX"" |cdata.e$ID == ""WWWX"" |\n                                cdata.e$ID == ""YYYX-UB""),]\n\nc.or.rpt = rpt(min.order ~ (1|ID), grname = c(""ID"", ""Residual""), datatype = ""Poisson"",\n               nboot = 1000, npermut= 100, ratio = T, data = cdata.rpt2.e)\nc.or.rpt # R = 0, p = 1... order of first approach not repeatable within jay\n\ncrpt = rpt(minDist ~ min.order2 + (1|ID), grname = c(""ID"",""Residual""), datatype = ""Poisson"",\n             nboot = 1000, npermut= 100, ratio = T, data = cdata.rpt2.e)\ncrpt # repeatable within jay R = 0.43, p_LRT = 0.04\n\n## Adjusted for group\n# only jays with 2 trials AND data from both jays on territory\ncdata.rpt.e = cdata.e[which(cdata.e$Group == ""4"" | cdata.e$Group == ""6"" |\n                              cdata.e$Group == ""10"" |cdata.e$Group == ""11"" |\n                              cdata.e$Group == ""14"" |cdata.e$Group == ""18""),]\n\ncrpt2 = rpt(minDist ~ min.order2 + (1|ID) + (1|Group), grname = c(""ID"", ""Group"", ""Residual""), datatype = ""Poisson"",\n             nboot = 1000, npermut= 100, ratio = T, data = cdata.rpt.e)\ncrpt2 # Bird R = 0.35, p =0.12. Group NOT repeatable R = 0, p = 0.5\n\n\nc3 = glm(minDist ~ min.order2, family = ""poisson"", data = cdata.rpt2.e)\nc4 = glmer(minDist ~  min.order2 + (1|ID), family = ""poisson"", data = cdata.rpt2.e)\nc4.2 = glmer(minDist ~  min.order2 + (1|ID), family = ""poisson"", data = cdata.rpt.e)\nc5 = glmer(minDist ~  (1|ID) + (1|Group), family = ""poisson"", data = cdata.rpt.e)\nanova(c4,c3) # ID cluster variable improves fit X^2 = 1036.6, p << 0.001\nanova(c4.2,c5) # Group cluster variable does not improve model fit X^2 = 0, p = 1\n\n\n\n################ NO - MEJA repeatability ###########\n\n# data from jays with more than 1 trial\nmdata.rpt.e = data.e[which(data.e$ID == ""/O"" | data.e$ID == ""BBB-YXY"" | data.e$ID == ""BXB-BSB"" | \n                              data.e$ID == ""BYP-OWX"" | data.e$ID == ""OBO-OXO"" | data.e$ID == ""ORV-OXB"" | \n                              data.e$ID == ""XSS-ROR"" | data.e$ID == ""YBW-WBW"" | data.e$ID == ""WVB-RXO"" | \n                              data.e$ID == ""YOW-WRX"" | data.e$ID == ""YOX-OYR"" | data.e$ID == ""GOR-RYX"" |\n                              data.e$ID == ""GRG-XRS"" | data.e$ID == ""X-VYV"" | data.e$ID == ""OOM-XRR"" |\n                              data.e$ID == ""XBB-VSB"" | data.e$ID == ""XGR-RGY"" | data.e$ID == ""XYG-GRO""),]\n\nmdata.rpt.e$Group[which(mdata.rpt.e$Group == ""PL"")]<-""KI""\nmdata.rpt.e$minDist = mdata.rpt.e$MINDistance*100 # meters to centimeters\n\nm.or.rpt = rpt(min.order ~ (1|ID), grname = c(""ID"", ""Residual""), datatype = ""Poisson"",\n          nboot = 1000, npermut= 100, ratio = T, data = mdata.rpt.e)\nm.or.rpt # R = 0.48, p = 0.02\n\nmrpt = rpt(minDist ~ min.order2 + (1|ID) + (1|Group), grname = c(""ID"", ""Group"", ""Residual""), datatype = ""Poisson"",\n             nboot = 1000, npermut= 100, ratio = T, data = mdata.rpt.e)\nmrpt # ID R = 0, p = 0.5; Group R = 0.54, p < 0.001\n\nmrpt2 = rpt(minDist ~ min.order2 + (1|ID), grname = c(""ID"",""Residual""), datatype = ""Poisson"",\n             nboot = 1000, npermut= 100, ratio = T, data = mdata.rpt.e)\nmrpt2 # R = 0.42, p = 0.04\n\n\nm3 = glm(minDist ~ min.order2, family = ""poisson"", data = mdata.rpt.e)\nm4 = glmer(minDist ~ min.order2 + (1|ID), family = ""poisson"", data = mdata.rpt.e)\nm5 = glmer(minDist ~ min.order2 + (1|ID) + (1|Group), family = ""poisson"", data = mdata.rpt.e)\nanova(m4,m3) # inclusion of ID cluster variable significantly improves fit X^2 = 1515.8, p << 0.001\nanova(m4,m5) # inclusion of Group cluster variable also significantly improves fit X^2 = 9.62, p = 0.002\n\n\n##### Flight intiation distance analyses #####\n\nFID.o = read.csv(""Flight initiation distance.csv"")\nFID = FID.o[,-c(3,5,7,9)]\ncolnames(FID)[5]<-""FID""\n\n\n### FID Breeding season effect? ###\nCFID = FID[which(FID$Species == ""CASJ""),]\nCFID = aggregate(FID ~ ID + BS., data = CFID, FUN = ""mean"")\n\n\nMFID = FID[which(FID$Species == ""MEJA""),]\nMFID = aggregate(FID ~ ID + BS., data = MFID, FUN = ""mean"")\n\nt.test(FID ~ BS., data = CFID)\n# t = 0.77, df = 48.97, p = 0.44\nt.test(FID ~ BS., data = MFID)\n# t = 1.23, df = 44.8, p = 0.22\n\n###### FID - CASJ repeatability #################\nCFID = FID[which(FID$Species == ""CASJ""),]\n# remove jays with only 1 FID\nCFID.rpt = CFID[-which(CFID$ID == ""BRYX"" | CFID$ID == ""BWGX"" | CFID$ID == ""GGOX"" | \n                         CFID$ID == ""OYWX"" | CFID$ID == ""RBGX"" | CFID$ID == ""WRRX"" | \n                         CFID$ID == ""YBOX"" | CFID$ID == ""YGBX"" | CFID$ID == ""YOOX"" | \n                         CFID$ID == ""YYBX""),]\ncount(CFID.rpt$ID)\n# 28\n\n# remove groups with data from only 1 of pair\nCFID.rpt2 = CFID.rpt[-which(CFID.rpt$Group == 1 | CFID.rpt$Group == 3 | CFID.rpt$Group == 2|\n                          CFID.rpt$Group == 6 | CFID.rpt$Group == 8 | \n                          CFID.rpt$Group == 12 | CFID.rpt$Group == 15 |\n                          CFID.rpt$Group == 9 | CFID.rpt$Group == 20 | \n                          CFID.rpt$ID == ""ROWX""),]\ncount(CFID.rpt2$ID)\n# 18\n\n\nlm = lm(log(FID) ~ 1, data = CFID.rpt2)\nhist(log(CFID.rpt2$FID)) # actually fairly normally distributed, not a count variable because there are some .5\'s in there\nshapiro.test(lm$residuals) # N.S.\n\ncfrpt = rpt(log(FID) ~ (1|ID), grname = c(""ID"",""Residual""), datatype = ""Gaussian"",\n              nboot = 1000, npermut = 100, ratio = T, data = CFID.rpt)\ncfrpt # R = 0, p = 1\n# R = 0, lcl = 0, ucl = 0.233, p = 1\ncfrpt2 = rpt(log(FID) ~ (1|ID) + (1|Group), grname = c(""ID"",""Group"", ""Residual""), datatype = ""Gaussian"",\n            nboot = 1000, npermut = 100, ratio = T, data = CFID.rpt2)\ncfrpt2 # for both ID and group, R = 0, p = 1\n\ncfid1 = lm(log(FID) ~ 1, data = CFID.rpt)\ncfid2 = lmer(log(FID) ~ 1 + (1|ID), data = CFID.rpt)\ncfid2.2 = lmer(log(FID) ~ 1 + (1|ID), data = CFID.rpt2)\ncfid3 = lmer(log(FID) ~ 1 + (1|ID) + (1|Group), data = CFID.rpt2)\nanova(cfid2, cfid1) # ID does not improve model fit X^2 = 0, p = 1\nanova(cfid2.2, cfid3) # Group does not improve model fit X^2 = 0, p = 1\n\n\n\n####### FID - MEJA repeatability #######################\nMFID = FID[which(FID$Species== ""MEJA""),]\n# remove jays with only one FID\nMFID.rpt = MFID[-which(MFID$ID == ""BSX-SVG"" | MFID$ID == ""GOR-RYX"" | MFID$ID == ""MOR-OX"" | \n                         MFID$ID == ""OSG-XKV"" | MFID$ID == ""PBR-BOX"" | MFID$ID == ""POW-RXR"" | \n                         MFID$ID == ""PYK-KYX"" | MFID$ID == ""PYR-ROX"" | MFID$ID == ""SPX-PSP"" | \n                         MFID$ID == ""SY-X"" | MFID$ID == ""UB-UC"" | MFID$ID == ""WRY-KY"" | \n                         MFID$ID == ""XPO-OPR"" | MFID$ID == ""XW-"" | MFID$ID == ""XYY-SRS""),]\n\ncount(MFID.rpt$ID)\n# 46 jays\n\nlm = lm(log(FID) ~ 1, data = MFID.rpt)\nhist(log(MFID.rpt$FID)) # actually fairly normally distributed, not a count variable because there are some .5\'s in there\nshapiro.test(lm$residuals) # N.S.\n\nMFID.rpt$Trial = as.factor(MFID.rpt$Trial)\n\nmfrpt = rpt(log(FID) ~ (1|ID), grname = c(""ID"", ""Residual""), datatype = ""Gaussian"",\n              nboot = 1000, npermut = 100, ratio = T, data = MFID.rpt)\nmfrpt # R = 0.44, p < 0.001\n\nmfrpt2 = rpt(log(FID) ~ (1|ID) + (1|Group), grname = c(""ID"", ""Group"", ""Residual""), datatype = ""Gaussian"",\n              nboot = 1000, npermut = 100, ratio = T, data = MFID.rpt)\nmfrpt2 # ID R = 0.12, p = 0.01; Group R = 0.37, p < 0.001\n\n\n\nmfid1 = lm(log(FID) ~ 1, data = MFID.rpt)\nmfid2 = lmer(log(FID) ~ 1 + (1|ID), data = MFID.rpt)\nmfid3 = lmer(log(FID) ~ 1 + (1|ID) + (1|Group), data = MFID.rpt)\nanova(mfid2, mfid1) # ID cluster variable improves fit of model X^2 = 33.39, p < 0.001\nanova(mfid2, mfid3) # Group cluster variable improves fit of model X^2 = 17.26, p < 0.001\n\n\n\n### Correlation of boldness measures ####\nCFID = aggregate(FID ~ ID, data = CFID, FUN = ""mean"")\nc.pers = merge(CFID, cavers.no[which(cavers.no$TREATMENT == ""EXP""),], by = ""ID"")\n# 28 jays with both NO and FID values\ncor.test(c.pers$FID, c.pers$minDist)\n# r = -0.11, p = 0.57\n\nMFID = aggregate(FID ~ ID, data = MFID, FUN = ""mean"")\nm.pers = merge(MFID, mavers.no[which(mavers.no$TREATMENT == ""EXP""),], by = ""ID"")\n# 34 jays with both NO and FID values\ncor.test(m.pers$FID, m.pers$minDist)\n# r = 0.05, p = 0.79\n\n\n\n\n##### Code for reproducing figures #####\n\n######## Scatterplot figure ######\n# remove groups with data from only one of pair\nCNOe = NOe[which(NOe$Species ==""CASJ"" & NOe$TREATMENT ==""EXP""),]\ncno = CNOe[-which(CNOe$Group == ""1"" | CNOe$Group == ""13"" |CNOe$Group == ""2"" |CNOe$Group == ""20"" | CNOe$Group == ""3""),]\ncno = aggregate(DISTANCE ~ ID + Trial + Group, data = cno, FUN = ""min"")\ncno$dist = cno$DISTANCE*100\nse.cno = summarySE(cno, measurevar=""dist"", groupvars=c(""Group"", ""ID""))\nse.cno$se[is.na(se.cno$se)]<- 0\n\nmNOe = NOe[which(NOe$Species ==""MEJA"" & NOe$TREATMENT ==""EXP""),]\nmNOe$Group[which(mNOe$Group==""PL"")]<-""KI""\nmNOe$Group = as.factor(as.character(mNOe$Group))\nmno = aggregate(DISTANCE ~ ID + Trial + Group, data = mNOe, FUN = ""min"")\nmno$dist = mno$DISTANCE*100\nse.mno = summarySE(mno, measurevar=""dist"", groupvars=c(""Group"", ""ID""))\nse.mno$se[is.na(se.mno$se)]<- 0\n\n\nCFID = FID[which(FID$Species== ""CASJ""),] # recreate CFID to remove >1 trial constraint\n# remove groups with data from only one of pair\ncfid = CFID[-which(is.na(CFID$Group) | CFID$Group == ""1"" | CFID$Group == ""12"" | CFID$Group == ""15"" |\n                          CFID$Group == ""20"" | CFID$Group == ""3"" | CFID$Group == ""6"" |\n                          CFID$Group == ""17"" |CFID$Group == ""8""),]\nse.cfid = summarySE(cfid, measurevar=""FID"", groupvars=c(""Group"", ""ID""))\nse.cfid$se[is.na(se.cfid$se)]<- 0\n\n\nMFID = FID[which(FID$Species==""MEJA""),]\nse.mfid = summarySE(MFID, measurevar=""FID"", groupvars=c(""Group"", ""ID""))\nse.mfid$Group[se.mfid$Group==""PL""]<- ""KI""\nse.mfid$N = as.numeric(se.mfid$N)\nse.mfid$FID = as.numeric(se.mfid$FID)\nse.mfid$sd = as.numeric(se.mfid$sd)\nse.mfid$se = as.numeric(se.mfid$se)\nse.mfid$ci = as.numeric(se.mfid$ci)\n\n\nse.cfid$Species = ""California Scrub-Jay""\nse.cfid$Group = as.factor(as.character(se.cfid$Group))\nse.mfid$Species = ""Mexican Jay""\nse.fid = rbind(se.cfid, se.mfid)\n\nse.cno$Species = ""California Scrub-Jay""\ncolnames(se.cno)[1]<-""Group""\nse.mno$Species = ""Mexican Jay""\nse.no = rbind(se.cno, se.mno)\nse.no$Group = as.factor(as.character(se.no$Group))\n\ng5 = ggplot(se.no, aes(x=as.character(Group), y=dist))+theme_bw()+\n  facet_wrap(~Species, scales = ""free_x"") +\n  geom_pointrange(aes(ymin=dist-se,ymax=dist+se),color = ""black"", \n                  position=position_jitter(width=0.13), size=0.45, alpha=0.5) +\n  theme(legend.position = ""none"", strip.background = element_rect(fill = ""white""), \n        strip.text = element_text(size=10, face=""bold""),\n        axis.text.y = element_text(size = 8),\n        axis.text.x=element_blank(),\n        axis.title.y = element_text(size = 10,face = ""bold"", vjust=2.4),\n        panel.grid=element_blank(), axis.ticks.length = unit(0.3,""cm"")) + \n  labs(x = NULL, y = ""Novel object \n       approach (cm)"") \n\n\ng6 = ggplot(se.fid, aes(x=as.character(Group), y=FID))+theme_bw()+\n  facet_wrap(~Species, scales = ""free_x"") +\n  geom_pointrange(aes(ymin=FID-se,ymax=FID+se),color = ""black"", \n                  position=position_jitter(width=0.13), size=0.45, alpha=0.5) +\n  theme(legend.position = ""none"", strip.background = element_rect(fill = ""white""), \n        strip.text = element_blank(),\n        axis.text.y = element_text(size = 8),\n        axis.text.x=element_blank(),\n        axis.title.y = element_text(size = 10,face = ""bold"", vjust=2.4),\n        axis.title.x = element_text(size = 8,face = ""bold"", vjust=-2),\n        panel.grid=element_blank(), axis.ticks.length = unit(0.3,""cm"")) + \n  labs(x = ""Group"", y = "" Flight initiation \n       distance (m)"") \n\n\ntiff(filename = ""McCune_Fig4.tif"",height = 100, width = 129, units = ""mm"", res = 1200)\ngrid.arrange(arrangeGrob(g5 + theme(legend.position=""none""),\n                         g6 + theme(legend.position=""none""),\n                         nrow=2),\n             vp=viewport(width=0.95, height=0.98),\n             heights=c(7, 0.2))\ndev.off()\n\n#### Estimate and CI graph ####\nr.mfid = NULL\nr.mfid$estimate = c(0.44, 0.12) # Bird repeatability, then adjusted with group effect\nr.mfid$measure = ""Flight initiation distance""\nr.mfid$lcl = c(0.25,0.0)\nr.mfid$ucl = c(0.60, 0.27)\nr.mfid = as.data.frame(r.mfid)\nr.mfid$species = ""Mexican Jay""\n\nr.mno = NULL\nr.mno$estimate = c(0.42, 0.0)\nr.mno = as.data.frame(r.mno)\nr.mno$measure = ""Novel object approach""\nr.mno$lcl = c(0.0,0.0)\nr.mno$ucl = c(0.72,0.31)\nr.mno$species = ""Mexican Jay""\nr.meja = rbind(r.mfid, r.mno)\n\n\nr.cfid = NULL\nr.cfid$estimate = c(0.0, 0.0)\nr.cfid = as.data.frame(r.cfid)\nr.cfid$measure = ""Flight initiation distance""\nr.cfid$lcl = c(0.0,0.0)\nr.cfid$ucl = c(0.23, 0.24)\nr.cfid$species = ""California Scrub-Jay""\n\nr.cno = NULL\nr.cno$estimate = c(0.43, 0.35)\nr.cno = as.data.frame(r.cno)\nr.cno$measure = ""Novel object approach""\nr.cno$lcl = c(0.0,0.0)\nr.cno$ucl = c(0.72,0.68)\nr.cno$species = ""California Scrub-Jay""\nr.casj = rbind(r.cfid, r.cno)\n\nr.total = rbind(r.casj, r.meja)\nr.total$r.type = c(1,2,1,2,1,2,1,2)\nr.total$r.type = as.factor(r.total$r.type)\n\nvar.cno = rpt(minDist ~ min.order2 + (1|ID) + (1|Group), grname = c(""ID"", ""Group"", ""Residual""), datatype = ""Poisson"",\n              nboot = 100, npermut= 10, ratio = F, data = cdata.rpt.e)\nvar.cno\nvar.mno = rpt(minDist ~ min.order2 + (1|ID) + (1|Group), grname = c(""ID"", ""Group"", ""Residual""), datatype = ""Poisson"",\n    nboot = 100, npermut= 10, ratio = F, data = mdata.rpt.e)\nvar.mno\n\nvar.cfid = rpt(FID ~ (1|ID) + (1|Group), grname = c(""ID"", ""Group"", ""Residual""), datatype = ""Gaussian"",\n               nboot = 100, npermut= 10, ratio = F, data = CFID.rpt2)\nvar.cfid\nvar.mfid = rpt(FID ~ (1|ID) + (1|Group), grname = c(""ID"", ""Group"", ""Residual""), datatype = ""Gaussian"",\n               nboot = 100, npermut= 10, ratio = F, data = MFID.rpt)\nvar.mfid\n\nvariances = data.frame(var=c(3.86,0,0,0.86,\n                             14.61,1.13,45.98,1.61,\n                             9.8,1.33,0,0), \n                       lcl=c(0.29,0,0,0,\n                             10.48,0.4,29.06,0.43,\n                             0.28,0,0,0),\n                       ucl=c(7.80,0.65,8.56,3.01,\n                             18.65,1.68,63.04,3.14,\n                             22.47,4.2,8.54,0.99),\n                       component=c(""BIC"",""BIC"",""BIC"",""BIC"",\n                                   ""WIC"",""WIC"",""WIC"",""WIC"",\n                                   ""Group"",""Group"",""Group"",""Group""),\n                       measure=c(""FID"",""NO"",""FID"",""NO"",\n                                 ""FID"",""NO"",""FID"",""NO"",\n                                 ""FID"",""NO"",""FID"",""NO""),\n                       species=c(""MEJA"",""MEJA"",""CASJ"",""CASJ"",\n                                 ""MEJA"",""MEJA"",""CASJ"",""CASJ"",\n                                 ""MEJA"",""MEJA"",""CASJ"",""CASJ""))\n\nlevels(variances$measure) <- c(""Flight initiation distance"", ""Novel object approach"")\nlevels(variances$species) <- c(""California Scrub-Jay     "",""Mexican Jay"")\n\n\n\ng7 = ggplot(r.total, aes(x=r.type, y=estimate, color=species))+theme_bw()+\n  theme(panel.grid=element_blank())+\n  geom_errorbar(aes(ymin=lcl, ymax=ucl), position=position_dodge(width=0.5), \n                width=0.2)+\n  geom_point(position=position_dodge(width=0.5),size=2)+facet_wrap(~measure, scales=""free"") +\n  theme(strip.background = element_rect(fill = ""white""), strip.text = element_text(size=10, face=""bold"")) +\n  labs(x = NULL, y = ""Repeatability"") + theme(legend.position=""none"") +\n  scale_color_manual(name= ""Species:"", values=c(""black"",""dark grey"")) + \n  theme(axis.text.x = element_text(size = 8))+\n  theme(axis.text.y = element_text(size = 8))+\n  theme(axis.title.y = element_text(size = 10,face = ""bold"", vjust=2.4))+\n  scale_x_discrete(breaks=c(""1"",""2""),\n                   labels=c(""Individual"",""Adjusted""))\n\ng8 = ggplot(variances, aes(x=component, y=var, color=species))+theme_bw()+\n  theme(panel.grid=element_blank())+\n  geom_errorbar(aes(ymin=lcl, ymax=ucl), position=position_dodge(width=0.5), \n                width=0.2)+\n  geom_point(position=position_dodge(width=0.5),size=2)+facet_wrap(~measure, scales=""free"") +\n  theme(strip.background = element_blank(), strip.text = element_blank()) +\n  labs(x = NULL, y = ""Variance"") + theme(legend.position=""bottom"") +\n  theme(legend.key.width = unit(3,""line"")) + theme(legend.text=element_text(size=8)) +\n  scale_color_manual(name= """", values=c(""black"",""dark grey"")) + \n  theme(axis.text.x = element_text(size = 8))+\n  theme(axis.title.x = element_text(size = 8,face = ""bold"", vjust=-2))+\n  theme(axis.text.y = element_text(size = 8))+\n  theme(axis.title.y = element_text(size = 10,face = ""bold"", vjust=24))+\n  scale_x_discrete(breaks=c(""WIC"",""BIC"",""Group""),\n                   labels=c(""Residual"",""Bird"", ""Group""))\n\nmylegend2<-g_legend(g8)\ntiff(filename = ""McCune_Fig3.tif"",height = 100, width = 129, units = ""mm"", res = 1200)\ngrid.arrange(arrangeGrob(g7 + theme(legend.position=""none""),\n                         g8 + theme(legend.position=""none""),\n                         nrow=2),\n             vp=viewport(width=0.95, height=0.98),\n             mylegend2, heights=c(7, 1))\ndev.off()\n\n\n######### Variance component predictions figure #########\n\nx.m = data.frame(Group = c(""1"",""1"",""1"",""1"",""1"",\n                           ""2"",""2"",""2"",""2"",""2"",\n                           ""3"",""3"",""3"",""3"",""3"",\n                           ""1"",""1"",""1"",""1"",""1"",\n                           ""2"",""2"",""2"",""2"",""2"",\n                           ""3"",""3"",""3"",""3"",""3"",\n                           ""1"",""1"",\n                           ""2"",""2"",\n                           ""3"",""3"",\n                           ""1"",""1"",\n                           ""2"",""2"",\n                           ""3"",""3""), \n                 score = c(1,2.5,4,5.5,7,\n                           1.5,3,4.5,6,7.5,\n                           2,3.5,5,6.5,8,\n                           0.2,0.9,1.6,2.3,3,\n                           2.7,3.4,4.1,4.8,5.5,\n                           5.2,5.9,6.6,7.3,8,\n                           2, 5.5,\n                           2.5, 6,\n                           3, 6.5,\n                           0.95,1.95,\n                           2.7,3.7,\n                           4.7,5.7),\n                 Hypothesis = c(""Social Niche Specialization"",""Social Niche Specialization"", ""Social Niche Specialization"",""Social Niche Specialization"",""Social Niche Specialization"",\n                                ""Social Niche Specialization"",""Social Niche Specialization"", ""Social Niche Specialization"",""Social Niche Specialization"", ""Social Niche Specialization"",\n                                ""Social Niche Specialization"",""Social Niche Specialization"", ""Social Niche Specialization"",""Social Niche Specialization"", ""Social Niche Specialization"",\n                                ""Conformity"", ""Conformity"",""Conformity"", ""Conformity"", ""Conformity"", \n                                ""Conformity"",""Conformity"", ""Conformity"",""Conformity"", ""Conformity"", \n                                ""Conformity"", ""Conformity"",""Conformity"",""Conformity"", ""Conformity"",\n                                ""Social Niche Specialization"",""Social Niche Specialization"",\n                                ""Social Niche Specialization"",""Social Niche Specialization"",\n                                ""Social Niche Specialization"",""Social Niche Specialization"",\n                                ""Conformity"", ""Conformity"",\n                                ""Conformity"", ""Conformity"",\n                                ""Conformity"", ""Conformity""),\n                 Species = c(""MEJA"",""MEJA"",""MEJA"",""MEJA"",""MEJA"",\n                             ""MEJA"",""MEJA"",""MEJA"",""MEJA"",""MEJA"",\n                             ""MEJA"",""MEJA"",""MEJA"",""MEJA"",""MEJA"",\n                             ""MEJA"",""MEJA"",""MEJA"",""MEJA"",""MEJA"",\n                             ""MEJA"",""MEJA"",""MEJA"",""MEJA"",""MEJA"",\n                             ""MEJA"",""MEJA"",""MEJA"",""MEJA"",""MEJA"",\n                             ""CASJ"",""CASJ"",\n                             ""CASJ"",""CASJ"",\n                             ""CASJ"",""CASJ"",\n                             ""CASJ"",""CASJ"",\n                             ""CASJ"",""CASJ"",\n                             ""CASJ"",""CASJ"")\n)\nx.m$ucl = ifelse(x.m$Species == ""CASJ"", x.m$score+1.5, x.m$score+0.7)\nx.m$lcl = ifelse(x.m$Species == ""CASJ"", x.m$score-1.5, x.m$score-0.7)\nx.m$ucl = ifelse(x.m$Hypothesis == ""Conformity"", x.m$ucl-0.2, x.m$ucl)\nx.m$lcl = ifelse(x.m$Hypothesis == ""Conformity"", x.m$lcl+0.2, x.m$lcl)\n\nlevels(x.m$Species) = c(""California Scrub-Jay         "", ""Mexican Jay"")\nx.m$Hypothesis = factor(x.m$Hypothesis, levels=c(""Social Niche Specialization"", ""Conformity""))\n\n\ntiff(filename = ""McCune_Fig1.tif"",height = 100, width = 129, units = ""mm"", res = 1200)\nggplot(x.m, aes(x=Group, y=score, color=Species))+theme_bw()+\n  theme(panel.grid=element_blank())+\n  geom_errorbar(aes(ymin=lcl, ymax=ucl), position=position_dodge(width=0.5), \n                width=0.2)+\n  geom_point(position=position_dodge(width=0.5), size=2)+\n  facet_wrap(~Hypothesis) +\n  theme(strip.background = element_rect(fill = ""white""), strip.text = element_text(size=10, face=""bold"")) +\n  labs(x = ""Territory"", y = ""Boldness"") + theme(legend.position=""bottom"") +\n  theme(legend.key.width = unit(2,""line"")) + theme(legend.text=element_text(size=8)) +\n  scale_color_manual(name= """", values=c(""black"",""dark grey"")) + \n  theme(plot.margin=unit(c(1,1,1,1.2),""line"")) +\n  theme(axis.text.x = element_text(size = 8))+\n  theme(axis.title.x = element_text(size = 8,face = ""bold"", vjust=-2))+\n  theme(axis.text.y = element_text(size = 8))+\n  theme(axis.title.y = element_text(size = 10,face = ""bold"", vjust=24))\n\ndev.off()\n\n\n']","Data from: Evidence for personality conformity, not social niche specialization in social jays Animal personality traits are defined as consistent individual differences in behavior over time and across contexts. Occasionally this inflexibility results in maladaptive behavioral responses to external stimuli. However, in social groups inflexible behavioral phenotypes might be favored as this could lead to more predictable social interactions. Two hypotheses seek to describe the optimal distribution of personality types within groups. The social niche specialization hypothesis states that individuals within groups should partition social roles, like personality types, to avoid conflict. Whereas the conformity hypothesis states that individuals should assort with conspecifics of similar personality. However no research so far has compared these hypotheses using data from wild animal systems. We tested boldness in the wild on two species with different social systems, the Mexican Jay and California Scrub-Jay. We found support for the conformity hypothesis over the social niche specialization hypothesis because individuals within groups of the social species had more similar personalities, and consequently there was a statistically significant group effect. The most likely mechanism for this conformity is social learning of behaviors through development, but more explicit research on this is needed.",2
Data for: Similar environmental cues guide timing of breeding and seasonal shifts in songbird social structure,"Seasonally breeding animals often exhibit different social structures during non-breeding and breeding periods that coincide with seasonal environmental variation. Therefore, ongoing climate change may play an important role in determining the future structure of animal societies, especially if climate determines when seasonal shifts in social structure occur. However, we know little about the environmental cues that determine the timing of seasonal shifts in social structure, a lack of knowledge that contrasts with our well-defined knowledge of the environmental cues that trigger a shift to breeding physiology in seasonally breeding species. Here we tested whether the environmental cues that drive seasonal shifts in social structure are similar to those that determine timing of breeding in the red-backed fairywren (Malurus melanocephalus), an Australian songbird. Social network analyses revealed that social groups, which are highly territorial during the breeding season, interact in social ""communities"" on larger ranges during the non-breeding season. Interactions among non-breeding groups were related to rainfall, with more rainfall leading to reductions in home range size and fewer interactions among non-breeding social groups. Similarly, onset of breeding was also determined by rainfall during the non-breeding season, with greater rainfall leading to earlier breeding. These findings reveal that for some species, the cues that determine the timing of shifts in social structure across seasonal boundaries can be similar to those that determine timing of breeding. This study increases our understanding of how social structure and the selection pressures that result from different social structures might respond to changing climates.",,"Data for: Similar environmental cues guide timing of breeding and seasonal shifts in songbird social structure Seasonally breeding animals often exhibit different social structures during non-breeding and breeding periods that coincide with seasonal environmental variation. Therefore, ongoing climate change may play an important role in determining the future structure of animal societies, especially if climate determines when seasonal shifts in social structure occur. However, we know little about the environmental cues that determine the timing of seasonal shifts in social structure, a lack of knowledge that contrasts with our well-defined knowledge of the environmental cues that trigger a shift to breeding physiology in seasonally breeding species. Here we tested whether the environmental cues that drive seasonal shifts in social structure are similar to those that determine timing of breeding in the red-backed fairywren (Malurus melanocephalus), an Australian songbird. Social network analyses revealed that social groups, which are highly territorial during the breeding season, interact in social ""communities"" on larger ranges during the non-breeding season. Interactions among non-breeding groups were related to rainfall, with more rainfall leading to reductions in home range size and fewer interactions among non-breeding social groups. Similarly, onset of breeding was also determined by rainfall during the non-breeding season, with greater rainfall leading to earlier breeding. These findings reveal that for some species, the cues that determine the timing of shifts in social structure across seasonal boundaries can be similar to those that determine timing of breeding. This study increases our understanding of how social structure and the selection pressures that result from different social structures might respond to changing climates.",2
Data from: A cost of being amicable in a hibernating marmot,"Amicable social interactions can enhance fitness in many species, have negligible consequences for some, and reduce fitness in others. For yellow-bellied marmots (Marmota flaviventris), a facultatively social rodent species with demonstrable costs of social relationships during the active season, the effects of sociality on overwinter survival have yet to be fully investigated. Here, we explored how summer social interactions, quantified as social network attributes, influenced marmot survival during hibernation. Using social data collected from 2002 to 2012 on free-living yellow-bellied marmots, we calculated 8 social network measures (in-degree, out-degree, in-closeness, out-closeness, in-strength, out-strength, embeddedness, and clustering coefficient) for both affiliative and agonistic interactions. We performed a principal component analysis (PCA) to reduce those attributes to 3 affiliative (connectedness, strength, and clustering) and 4 agonistic (submissiveness, bullying, strength, and clustering) components. Then, we fitted a generalized linear mixed model to explain variation in overwinter survival as a function of these social components, along with body mass, sex, age, weather conditions, hibernation group size, and hibernation group composition. We found that individuals with stronger amicable relationships were more likely to die during hibernation. This suggests that social relationships, even affiliative ones, need not be beneficial; for yellow-bellied marmots, they can even be fatal.","['# GENERALIZED LINEAR MIXED MODEL\r\n\r\n# 1. Clean memory and remove all objects\r\nrm(list=ls())\r\n\r\n# 2. Set working directory\r\nsetwd(""/Users/jennyweiyang/desktop"")\r\n\r\n# 3. Upload final dataset\r\nsnmdata_final <- read.csv(""/Users/jennyweiyang/Desktop/Yangetal_beheco_2016_owsurvival.csv"")\r\n\r\n# 4. Load lme4\r\nlibrary(lme4)\r\n\r\n\r\n# 4. Model selection\r\n\r\n# A. Full model (AIC = 141.4)\r\nmod_1 <- glmer(ow_survival ~ \r\n              (saff1 + saff2 + saff3 + sagr1 + sagr2 + sagr3 + sagr4) * agecat + (smassaug * sex) + sgroupsize + sgroupcomp + sspringT + swinterT +\r\n              (1 | year) + (1 | uid),\r\n               data = snmdata_final, control = glmerControl(""bobyqa""), family = binomial)\r\nsummary(mod_1)\r\n\r\n# B. Model w/o snm*agecat (AIC = 132.6)\r\nmod_2 <- glmer(ow_survival ~ \r\n              (saff1 + saff2 + saff3 + sagr1 + sagr2 + sagr3 + sagr4) + agecat + (smassaug * sex) + sgroupsize + sgroupcomp + sspringT + swinterT +\r\n              (1 | year) + (1 | uid),\r\n              data = snmdata_final, control = glmerControl(""bobyqa""), family = binomial)\r\nsummary(mod_2)\r\n\r\n# C. Model w/o snm*agecat + saff1 (AIC = 130.6)\r\nmod_3 <- glmer(ow_survival ~ \r\n               (saff2 + saff3 + sagr1 + sagr2 + sagr3 + sagr4) + agecat + (smassaug * sex) + sgroupsize + sgroupcomp + sspringT + swinterT +\r\n               (1 | year) + (1 | uid),\r\n               data = snmdata_final, control = glmerControl(""bobyqa""), family = binomial)\r\nsummary(mod_3)\r\n\r\n# D. Final Model: w/o snm*agecat + saff1 + groupsize (AIC = 128.7)\r\nmod_final <- glmer(ow_survival ~ \r\n                  (saff2 + saff3 + sagr1 + sagr2 + sagr3 + sagr4) + agecat + (smassaug * sex) + sgroupcomp + sspringT + swinterT +\r\n                  (1 | year) + (1 | uid),\r\n                  data = snmdata_final, control = glmerControl(""bobyqa""), family = binomial)\r\nsummary(mod_final)\r\n\r\n\r\n# 5. Calculate pseudo R^2\r\nlibrary(MuMIn)\r\nr.squaredGLMM(mod_final)\r\n\r\n\r\n# 6. Plot results\r\n\r\n# A. Create a new dataset where all variables except aff2 are set to the mean \r\npframe <- with(snmdata_final, data.frame(saff3 = mean(saff3), sagr1 = mean(sagr1), sagr2 = mean(sagr2), sagr3 = mean(sagr3), sagr4 = mean(sagr4),\r\n                                         smassaug = mean(smassaug), agecat = factor(""ad"", levels = levels(agecat)), sex = factor(""M"", levels = levels(sex)),\r\n                                         sgroupcomp = mean(sgroupcomp), sspringT = mean(sspringT), swinterT = mean(swinterT),\r\n                                         saff2 = seq(min(saff2), max(saff2), length.out=241)))\r\n\r\npframe$eta <- predict(mod_final, re.form=NA, newdata = pframe)\r\n\r\nff <- formula(mod_final, fixed.only = TRUE)[-2]\r\nX <- model.matrix(ff, data = pframe)\r\nV <- vcov(mod_final)\r\nse <- sqrt(diag(X %*% V %*% t(X)))\r\npframe <- transform(pframe,\r\n                    ow_survival = 1/ (1 + 1 / exp(eta)),\r\n                    lwr = 1/ (1 + 1 / exp(eta - 1.96 * se)),\r\n                    upr = 1/ (1 + 1 / exp(eta + 1.96 * se)))\r\n\r\nlibrary(ggplot2)\r\n\r\n# Figure 1: Affiliative Relationship Strength\r\npng(file = ""Figure1_2may16.png"", width = 6, height = 4, units = ""in"", res = 1200, pointsize = 12)\r\n      g1 <- ggplot(snmdata_final, aes(x = as.numeric(saff2), y = ow_survival)) +\r\n            geom_point(size = 1, colour = ""gray"", position = position_jitter(w = 0.05, h = 0.05))\r\n      g1 + geom_line(data = pframe, colour = ""black"") +\r\n            geom_ribbon(data = pframe, aes(ymin = lwr, ymax = upr), colour = NA, alpha = 0.3) +\r\n            theme(axis.line = element_line(colour = ""black""),\r\n                  panel.grid.major = element_blank(),\r\n                  panel.grid.minor = element_blank(),\r\n                  panel.border = element_rect(colour = ""black"", fill=NA, size=1), #element_blank(),\r\n                  panel.background = element_blank()) +\r\n            theme(axis.text.x = element_text(face = ""bold"", color = ""black"", size = 14),\r\n                  axis.text.y = element_text(face = ""bold"", color = ""black"", size = 14),\r\n                  axis.title.x = element_text(face = ""bold"", color = ""black"", size = 16, vjust = 0),\r\n                  axis.title.y = element_text(face = ""bold"", color = ""black"", size = 16, vjust = 1)) +\r\n            labs(x = ""Strength of amicable relationships"", y = ""Overwinter survival"")\r\ndev.off()\r\n\r\n\r\n# B. Create a new dataset where all variables except smassaug are set to the mean \r\npframe <- with(snmdata_final, data.frame(saff2 = mean(saff2), saff3 = mean(saff3), sagr1 = mean(sagr1), sagr2 = mean(sagr2), sagr3 = mean(sagr3), sagr4 = mean(sagr4),\r\n                                         agecat = factor(""ad"", levels = levels(agecat)), sex = factor(""M"", levels = levels(sex)),\r\n                                         sgroupcomp = mean(sgroupcomp), sspringT = mean(sspringT), swinterT = mean(swinterT),\r\n                                         smassaug = seq(min(smassaug), max(smassaug), length.out=241)))\r\n\r\npframe$eta <- predict(mod_final, re.form=NA, newdata = pframe)\r\n\r\nff <- formula(mod_final, fixed.only = TRUE)[-2]\r\nX <- model.matrix(ff, data = pframe)\r\nV <- vcov(mod_final)\r\nse <- sqrt(diag(X %*% V %*% t(X)))\r\npframe <- transform(pframe,\r\n                    ow_survival = 1/ (1 + 1 / exp(eta)),\r\n                    lwr = 1/ (1 + 1 / exp(eta - 1.96 * se)),\r\n                    upr = 1/ (1 + 1 / exp(eta + 1.96 * se)))\r\n\r\n# Figure 2: August Mass\r\npng(file = ""Figure2_2may16.png"", width = 6, height = 4, units = ""in"", res = 1200, pointsize = 12)\r\ng2 <- ggplot(snmdata_final, aes(x = as.numeric(smassaug), y = ow_survival)) +\r\n  geom_point(size = 1, colour = ""gray"", position = position_jitter(w = 0.05, h = 0.05))\r\ng2 + geom_line(data = pframe, colour = ""black"") +\r\n  geom_ribbon(data = pframe, aes(ymin = lwr, ymax = upr), colour = NA, alpha = 0.3) +\r\n  theme(axis.line = element_line(colour = ""black""),\r\n        panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        panel.border = element_rect(colour = ""black"", fill=NA, size=1), #element_blank(),\r\n        panel.background = element_blank()) +\r\n  theme(axis.text.x = element_text(face = ""bold"", color = ""black"", size = 14),\r\n        axis.text.y = element_text(face = ""bold"", color = ""black"", size = 14),\r\n        axis.title.x = element_text(face = ""bold"", color = ""black"", size = 16, vjust = 0),\r\n        axis.title.y = element_text(face = ""bold"", color = ""black"", size = 16, vjust = 1)) +\r\n  labs(x = ""August body mass"", y = ""Overwinter survival"")\r\ndev.off()\r\n\r\n\r\n# C. Create a new dataset where all variables except smassaug are set to the mean\r\npframe <- with(snmdata_final, data.frame(saff2 = mean(saff2), saff3 = mean(saff3), sagr1 = mean(sagr1), sagr2 = mean(sagr2), sagr3 = mean(sagr3), sagr4 = mean(sagr4),\r\n                                         agecat = factor(""ad"", levels = levels(agecat)), \r\n                                         sex = factor(c(rep(""M"",250),rep(""F"",250)),levels=levels(sex)),\r\n                                         sgroupcomp = mean(sgroupcomp), swinterT = mean(swinterT), sspringT = mean(sspringT),\r\n                                         smassaug = rep(seq(min(smassaug), max(smassaug), length.out=250),2)))\r\n\r\npframe$eta <- predict(mod_final, re.form=NA, newdata = pframe)\r\n\r\nff <- formula(mod_final, fixed.only = TRUE)[-2]\r\nX <- model.matrix(ff, data = pframe)\r\nV <- vcov(mod_final)\r\nse <- sqrt(diag(X %*% V %*% t(X)))\r\npframe <- transform(pframe,\r\n                    ow_survival = 1/ (1 + 1 / exp(eta)),\r\n                    lwr = 1/ (1 + 1 / exp(eta - 1.96 * se)),\r\n                    upr = 1/ (1 + 1 / exp(eta + 1.96 * se)))\r\n\r\n# Figure 3: Interaction between August mass & Sex\r\npng(file = ""Figure3_2may16.png"", width = 6, height = 4, units = ""in"", res = 1200, pointsize = 12)\r\ng3 <- ggplot(snmdata_final, aes(x = as.numeric(smassaug), y = ow_survival, linetype=sex)) +\r\n  geom_point(size = 1, colour = ""gray"", position = position_jitter(w = 0.05, h = 0.05))\r\ng3 + geom_line(data = pframe, colour = ""black"") +\r\n  geom_ribbon(data = pframe, aes(ymin = lwr, ymax = upr), colour = NA, alpha = 0.1) +\r\n  theme(axis.line = element_line(colour = ""black""),\r\n        panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        panel.border = element_rect(colour = ""black"", fill=NA, size=1), #element_blank(),\r\n        panel.background = element_blank()) +\r\n  theme(axis.text.x = element_text(face = ""bold"", color = ""black"", size = 14),\r\n        axis.text.y = element_text(face = ""bold"", color = ""black"", size = 14),\r\n        axis.title.x = element_text(face = ""bold"", color = ""black"", size = 16, vjust = 0),\r\n        axis.title.y = element_text(face = ""bold"", color = ""black"", size = 16, vjust = 1)) +\r\n  labs(x = ""August body mass"", y = ""Overwinter survival"")\r\ndev.off()\r\n']","Data from: A cost of being amicable in a hibernating marmot Amicable social interactions can enhance fitness in many species, have negligible consequences for some, and reduce fitness in others. For yellow-bellied marmots (Marmota flaviventris), a facultatively social rodent species with demonstrable costs of social relationships during the active season, the effects of sociality on overwinter survival have yet to be fully investigated. Here, we explored how summer social interactions, quantified as social network attributes, influenced marmot survival during hibernation. Using social data collected from 2002 to 2012 on free-living yellow-bellied marmots, we calculated 8 social network measures (in-degree, out-degree, in-closeness, out-closeness, in-strength, out-strength, embeddedness, and clustering coefficient) for both affiliative and agonistic interactions. We performed a principal component analysis (PCA) to reduce those attributes to 3 affiliative (connectedness, strength, and clustering) and 4 agonistic (submissiveness, bullying, strength, and clustering) components. Then, we fitted a generalized linear mixed model to explain variation in overwinter survival as a function of these social components, along with body mass, sex, age, weather conditions, hibernation group size, and hibernation group composition. We found that individuals with stronger amicable relationships were more likely to die during hibernation. This suggests that social relationships, even affiliative ones, need not be beneficial; for yellow-bellied marmots, they can even be fatal.",2
Data from: Presence of kin-biased social associations in a lizard with no parental care,"Numerous studies have observed kin-biased social associations in a variety of species. Many of these studies have focussed on species exhibiting parental care, which may facilitate the transmission of the social environment from parents to offspring. This becomes problematic when disentangling whether kin-biased associations are driven by kin recognition, or are a product of transmission of the social environment during ontogeny, or a combination of both. Studying kin-biased associations in systems that lack parental care may aid in addressing this issue. Furthermore, when studying kin-biased social associations it is important to differentiate whether these originate from preferential choice or occur randomly as a result of habitat use or limited dispersal. Here, we combined high-resolution SNP data with a long-term behavioural dataset of a reptile with no parental care to demonstrate that eastern water dragons (Intellagama lesueurii) bias their non-random social associations toward their kin. In particular, we found that, while the overall social network was not linked to genetic relatedness, individuals associated with kin more than expected given availability in space, and also biased social preferences towards kin. This result opens important opportunities for the study of kinship-driven associations without the confounding effect of vertical transmission of social environments. Furthermore, we present a robust multiple-step approach for determining whether kin-biased social associations are a result of active social decisions, or random encounters resulting from habitat use and dispersal patterns.","['\r\n# Script for filtering SNPs utilized for Piza-Roca et al. 2019 ""Presence of kin-biased social associations in a lizard with no parental care: the eastern water dragon (Intellagama lesueurii)""\r\n# April 2018\r\n# Caveat emptor. The code was not tested after cleaning.\r\n\r\n\r\n############################################################################################\r\n################# SNP FILTERING ############################################################\r\n############################################################################################\r\n\r\n\r\n# Adapted from Vivienne Foroughirad: Foroughirad, V. 2018 DArT_Prep v1.0.0. GitHub repository. https://doi.org/10.5281/zenodo.1308201.\r\n\r\n\r\n#Read in raw data from SNPs and filter based on MAF, CallRate, Errors, RepAvg\r\n\r\noptions(stringsAsFactors = FALSE)\r\nlibrary(HardyWeinberg)\r\n\r\n## Read in raw datasets for single and double file, and ID key. SNPs are rows and various summary statistics and all samples are columns\r\n\r\nsingle_raw<-read.csv(""Report_DWdrag17-2731_SNP_singlerow_1.csv"", colClasses = ""character"", skip=6)\r\ndouble_raw<-read.csv(""Report_DWdrag17-2731_SNP_1.csv"", colClasses = ""character"", skip=6)\r\n\r\n#ID_key contains ""Sample_ID"", ""Animal_ID"", and optionally ""Birthyear"" and ""Sex""\r\nID_key<-read.csv(""ID_key.csv"", colClasses = ""character"")\r\n\r\n## Remove duplicates and problematic samples\r\n\r\nremove <- c(""Palkana"", ""Arnold"", ""Annie"", ""Hagrid"")\r\n\r\ncremove <- ID_key$Sample_ID[ID_key$Animal_ID %in% remove]\r\ncremove2 <- ID_key$Animal_ID[ID_key$Animal_ID %in% remove]\r\ncremove3 <- cbind(cremove, cremove2)\r\ncremove3 # which ones would you like to remove?\r\n\r\nminus <- cremove3[c(1:2, 4:5), 1] #remove one of each duplicate plus conflicting individuals\r\n\r\nsingle <- single_raw[,!names(single_raw) %in% minus]\r\ndouble <- double_raw[,!names(double_raw) %in% minus]\r\n\r\n\r\n## Filter by CallRate for IDs\r\nInd_CR <- apply(single[18:length(single)], 2, function(x) (sum(x!=""-"")/length(x)))\r\nThres <- .65\r\nsum(Ind_CR >= Thres) # how many individuals will I have at this threshold level?\r\nID_key[!ID_key$Sample_ID%in%(names(Ind_CR[Ind_CR>=Thres])),]$Animal_ID # Which ones do we lose\r\nIndex <- which(c(rep(1,17), Ind_CR) >= Thres) # Remove individuals with low coverage at this stage. This threshold will be higher after further filtering\r\nsingle <- single[,Index] # subset according the the specified threshold\r\n\r\n## Calculate HWE (Hardy-Weinberg equilibrium) and MAFs for all alleles\r\n# Convert reference/snp allele to major/minor allele\r\n# adjust indexing, in which columns do you have your IDs?\r\n\r\nsingle$MN<-apply(single[,18:length(single)], 1, function(x) length(x[x==2])) #Heterezigotes\r\nsingle$NN<-apply(single[,18:length(single)], 1, function(x) length(x[x==1])) #SNP allele homozygotes\r\nsingle$MM<-apply(single[,18:length(single)], 1, function(x) length(x[x==0])) #Reference allele homozygotes\r\n\r\nsingle$maxA<-apply(single[,c(""NN"", ""MM"")], 1, max) #major allele count\r\n\r\nsingle$total<-apply(single[,c(""MN"",""NN"", ""MM"")], 1, sum) #total number of sequenced individuals (sequencing depth)\r\n\r\nsingle$MAF<-1-(single[,""maxA""]+((single[,""MN""])/2))/(single[,""total""]) #minor allele frequency\r\n\r\nsingle$MAcount<-single$total-single$maxA-single$MN #minor allele count\r\n\r\n#Loop through and pass one value at a time to AA, AB, BB\r\n\r\nsingle$pval<-rep(NA, dim(single)[1])\r\nsingle$expectedAA<-rep(NA, dim(single)[1])\r\nsingle$expectedAB<-rep(NA, dim(single)[1])\r\nsingle$expectedBB<-rep(NA, dim(single)[1])\r\n\r\n#Will give you warnings for expected counts below 5, but\r\n#we\'re filtering out those anyway\r\n\r\nfor (i in 1:dim(single)[1]){\r\n  AA<-single$maxA[i]\r\n  AB<-single$MN[i]\r\n  BB<-single$MAcount[i]\r\n  \r\n  output<-HWChisq(c(AA=AA, AB=AB, BB=BB), verbose=FALSE)\r\n  \r\n  single$pval[i]<-output$pval\r\n  single$expectedAA[i]<-output$expected[1]\r\n  single$expectedAB[i]<-output$expected[2]\r\n  single$expectedBB[i]<-output$expected[3]\r\n  \r\n}\r\n\r\n#Remove HWE, MAF, and CallRate cutoffs\r\n\r\nsingle<-subset(single,  pval>=0.05 &           #subset based on p-value for HWE\r\n                 MAF>=0.05 &                   #subset based on minor allele frequency\r\n                 CallRate>=0.95 &              #subset based on call rate\r\n                 RepAvg>=0.99                  #subset based on replicability\r\n)\r\n\r\n#Keep only one SNP per contig, using MAF\r\n\r\nsingle <- do.call(rbind,lapply(split(single,single$TrimmedSequence),function(chunk) chunk[which.max(chunk$MAF),]))\r\n\r\n\r\n## Match the single row with double row\r\n\r\n#format Allele ID column for matching by creating unique ID\r\n\r\nAlleleNumber<-strsplit(single$AlleleID,""\\\\|"")\r\nAlleleNumber<-unlist(lapply(AlleleNumber, ""[["",1))\r\n\r\nsingle<-cbind(single, AlleleNumber)\r\n\r\ndouble <- double[,Index] #subset according the the specified threshold for individuals call rate (if used)\r\n\r\nAlleleNumber2<-strsplit(double$AlleleID,""\\\\|"")\r\nAlleleNumber2<-unlist(lapply(AlleleNumber2, ""[["",1))\r\n\r\ndouble<-cbind(double, AlleleNumber2)\r\n\r\n#filter double\r\n\r\ndouble<-double[which(double$AlleleNumber2 %in% single$AlleleNumber),]\r\n\r\nfinal_double<-subset(double, double$AlleleNumber2 %in% single$AlleleNumber)\r\n\r\n#still duplicates because of multiple snps in the same read\r\n#give double a dummy ID\r\n\r\nn<-dim(double)[1]/2 #number of alleles\r\n\r\ndouble$allele_unique<-rep(1:n, each=2)\r\n\r\nfinal_ids<-subset(double$allele_unique, double$AlleleID %in% single$AlleleID)\r\n\r\nfinal_double<-subset(double, double$allele_unique %in% final_ids)\r\n\r\nfinal_double$allele_unique<-rep(1:(nrow(final_double)/2), each=2)\r\n\r\n#write.csv(final_double, ""filtered_SNPs.csv"", row.names = FALSE)\r\n\r\n#What is the individual call rate after all filtering?\r\nInd_CR <- apply(single[18:(length(single)-12)], 2, function(x) (sum(x!=""-"")/length(x)))\r\nmin(Ind_CR)\r\n\r\n\r\n################ Reformat final double PLINK #######################################################\r\n\r\n#final_double <- read.csv(""filtered_SNPs.csv"")\r\nnsnps<-dim(final_double)[1]/2 # number of sequenced SNPs\r\nfinal_double$al<-rep(c(""a"",""b""), nsnps) # create unique names for alleles\r\n\r\n#Make nuc1 and nuc0 columns\r\nlt<-setNames(as.data.frame(final_double[,c(""SNP"",""allele_unique"")], na.strings=c("""", NA)), c(""SNP1"", ""allele_unique""))\r\nlt$SNP1[lt$SNP1==""""] <- NA\r\nlt<-lt[complete.cases(lt),] # get rid of the missing values\r\nfinal_double<-merge(final_double, lt, by=""allele_unique"") # add bases to SNPs\r\n\r\nAlleleLetter<-strsplit(final_double$SNP1,"":"")\r\nfinal_double$AlleleLetter<-unlist(lapply(AlleleLetter, ""[["",2)) # get the allele letters\r\n\r\nRefLetter<-strsplit(final_double$AlleleLetter, "">"")\r\nfinal_double$RefLetter<-unlist(lapply(RefLetter, ""[["",1)) # reference nucleotide\r\nfinal_double$AltLetter<-unlist(lapply(RefLetter, ""[["",2)) # SNP nucleotide\r\n\r\nfinal_double$Nuc0<-ifelse(final_double$al==""b"", final_double$AltLetter, final_double$RefLetter) #get the right letter\r\nfinal_double$Nuc1<-ifelse(final_double$al==""a"", final_double$AltLetter, final_double$RefLetter)\r\n\r\nx<-final_double\r\n\r\nfor (i in 19:(length(final_double)-8)) {\r\n  \r\n  x[x[,i]==""0"", names(x)[i]]<-x[x[,i]==""0"", ""Nuc0""] # change numbers by corresponding letters\r\n  x[x[,i]==""1"", names(x)[i]]<-x[x[,i]==""1"", ""Nuc1""]\r\n  \r\n}\r\n\r\nx[,19:(length(final_double)-8)]<-apply(x[,19:(length(final_double)-8)], 2, function(x) gsub(""-"", 0, x))\r\n\r\nplink<-x\r\n\r\nID_key <- subset(ID_key, !Sample_ID %in% c(minus, ""X7"")) # Remove duplicates and problematic samples\r\n\r\nAllelenames <- unique(plink$AlleleNumber2)\r\nsnpsnames <- paste(plink$AlleleNumber2, plink$al, sep = """")\r\n\r\nplink<-plink[,(19:ncol(plink))] # get IDs and last columns (""AlleleNumber2"", ""al"", ""SNP1"", ""AlleleLetter"", ""RefLetter"",    \r\n#""AltLetter"", ""Nuc0"", ""Nuc1"")\r\n\r\nplink<-t(plink)\r\n\r\nx<-merge(plink, ID_key[,1:2], by.x=""row.names"", by.y=""Sample_ID"") # Add ""Animal_ID""\r\n\r\nx[,1]<-x$Animal_ID # row names get ID names\r\npl<-x[,-ncol(x)] # get rid of the last column (Animal ID)\r\n\r\n\r\n# Set up ped and map file\r\n\r\nm<-matrix(rep(0, 4*nrow(pl)), nrow=nrow(pl)) # create a matrix with 4 columns and rows = number of IDs\r\n\r\npl1<-data.frame(pl[,1],m, pl[,2:ncol(pl)]) # add the 0s columns in between IDs and SNPs\r\nnames(pl1)[1]<-""ID""\r\n\r\ns<-merge(pl1, ID_key[,c(2,4)], by.x=""ID"", by.y=""Animal_ID"") # Add ""Sex""\r\ns$Sex <- with(s, ifelse(Sex == ""M"", 1, ifelse(Sex == ""F"", 2, 0))) # change to number code\r\ns[,4]<-s$Sex # allocate sex to column 5\r\npl2<-s[,-ncol(s)] # get rid of the last column (Animal sex)\r\n\r\npl3<-data.frame(dummy_group=rep(1,nrow(pl2)),pl2) # add a first column of 1s\r\n\r\noriginal.ped<-pl3\r\n\r\nnames(original.ped)[7:ncol(original.ped)]<-snpsnames # give these names to SNPs\r\nhead(original.ped[,1:10])\r\n\r\noriginal.map<-data.frame(rep(1, nsnps), Allelenames, rep(0, nsnps), rep(0, nsnps)) #create an empty map file, all SNPS in dummy chromosome 1\r\n\r\nwrite.table(original.ped, ""filtered.ped"", sep=""\\t"", col.names=FALSE, row.names=FALSE,quote=FALSE)\r\nwrite.table(original.map, ""filtered.map"", sep=""\\t"", col.names=FALSE, row.names=FALSE,quote=FALSE)\r\n\r\n\r\n### Code for PLINK\r\n\r\n# plink --file filtered_CallRate95 --maf 0.05 --indep-pairwise 10000 10000 0.7\r\n# plink --file filtered --extract plink.prune.in --recode12 --out 4433SNPs_for_relatedness\r\n\r\n############################################################################################\r\n################# HOME RANGE ANALYSES AND DIGIROO MODEL ####################################\r\n############################################################################################\r\n\r\n# Adapted from Foroughirad V, 2018b. Digidolph v.1.0.0. GitHub repository. https://doi.org/10.5281/zenodo.1308203.\r\n\r\nlibrary(reshape2)\r\nlibrary(adehabitatHR)\r\nlibrary(maptools)\r\nlibrary(rgdal)\r\nlibrary(spatstat)\r\nlibrary(Digiroo2)\r\nlibrary(coda)\r\nlibrary(spdep)\r\nlibrary(raster)\r\nlibrary(PBSmapping)\r\nlibrary(gdata)\r\nlibrary(rgeos)\r\n\r\n## Create custom functions\r\n\r\nhwi<-function(sightings=sightings, group_variable=group_variable, dates=dates, IDs=IDs, symmetric=TRUE){\r\n  z<-sightings\r\n  dolphins<-sort(unique(z[,IDs]))\r\n  n<-length(dolphins)\r\n  matnames<-list(dolphins,dolphins)\r\n  currentmat<-matrix(c(rep(NA,n^2)),nrow=n,dimnames=matnames)\r\n  currentdflist<-split(z, z[,IDs],drop=TRUE)\r\n  for (i in 1:nrow(currentmat)) {\r\n    ego<-row.names(currentmat)[i]\r\n    #Get the list element for each dolphin\r\n    all_ego<-get(ego, currentdflist)\r\n    for (j in i:ncol(currentmat)) {\r\n      alter<-colnames(currentmat)[j]\r\n      #Get the list element for each dolphin\r\n      all_alter<-get(alter, currentdflist)\r\n      #Take the intersection of the partycomp_dolphins to see when the dolphins were in the same group_variable\r\n      set<-(intersect(all_ego[,group_variable], all_alter[,group_variable]))\r\n      sample<-subset(all_ego, all_ego[,group_variable] %in% set)\r\n      #Numerator for HWC\r\n      X<-length(unique(sample[,dates]))\r\n      if (X>0){\r\n        \r\n        X<-length(unique(all_ego[,dates][all_ego[,group_variable] %in% set]))\r\n        #Ego without alter\r\n        Ya<-length(setdiff(all_ego[,dates], all_alter[,dates]))\r\n        #Alter without ego\r\n        Yb<-length(setdiff(all_alter[,dates], all_ego[,dates]))\r\n        #Both seen but not together\r\n        Yab<-length(intersect(all_ego[,dates], all_alter[,dates]))-X\r\n        #Half weight coefficient\r\n        HWC<-(X/(X+0.5*(Ya+Yb)+Yab))\r\n        currentmat[i,j]<-HWC}\r\n      else{currentmat[i,j]<-0}\r\n    }\r\n  }\r\n  diag(currentmat)<-NA\r\n  if(symmetric==TRUE){\r\n    currentmat[lower.tri(currentmat)]=t(currentmat)[lower.tri(currentmat)]\r\n  }\r\n  return(currentmat)\r\n}\r\n\r\ndigu<-function(x){x$Group<-paste0(x$Permutation,""-"", x$Group);return(x)}\r\n\r\nmergeMatrices<-function(lmat) {\r\n  rands<-lapply(lmat, function(mat) mat<-na.omit(unmatrix(mat)))\r\n  rands<-lapply(rands, function(x) x<-x[order(names(x))])\r\n  rands<-as.data.frame(do.call(""cbind"",rands))\r\n  return(rands)\r\n}\r\n\r\n\r\nsample_sightings<-function(x,nums){\r\n  s=sample(row.names(x), nums, replace=FALSE) \r\n  y=subset(x, row.names(x) %in% s, drop=TRUE) \r\n  return(y)\r\n}\r\n\r\n## Read in the data (this contains individual sighted (ID), date, time, spatial coordinates, social associate (Partner, if any), and other details such as sex and number of total sightings for that individual) )\r\n\r\nHRdata <- read.csv(""Latest_social_linear_fromAug2012.csv"",stringsAsFactors=FALSE,header = TRUE)\r\nHRdata <- subset(HRdata, N_sight>24) #minimum number required for accurate estimations\r\nHRdata$ID.Date <- paste(HRdata$ID, HRdata$Datetime, sep="" "")\r\nHRdata1<-HRdata[!duplicated(HRdata[""ID.Date""]),] #Get unique individual locations for home range\r\n\r\nxydata<-cbind(HRdata1$X,HRdata1$Y)\r\nxydata2<-as.data.frame(project(xydata, ""+proj=tmerc +lat_0=-28 +lon_0=153 +k=0.99999 +x_0=50000 +y_0=100000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs""))\r\nxydata3<-cbind(HRdata1$ID,xydata2)\r\ncolnames(xydata3)<-c(""Name"",""X"",""Y"")\r\n\r\nx <- seq(min(xydata3[,""X""])-20,max(xydata3[,""X""])+20,by=1) # where resolution is the pixel size you desire\r\ny <- seq(min(xydata3[,""Y""])-20,max(xydata3[,""Y""])+20,by=1)\r\nxy <- expand.grid(x=x,y=y)\r\ncoordinates(xy) <- ~x+y\r\ngridded(xy) <- TRUE\r\nclass(xy)\r\n\r\nhrxydata<-SpatialPointsDataFrame(xydata3[,2:3],xydata3[""Name""])\r\n\r\n\r\n#Calculate all home ranges\r\n\r\nuds7<-kernelUD(hrxydata[,1],grid=xy, h= 7) \r\nkernelcontours95<-getverticeshr(uds7,percent=95,unin = c(""m""),unout = c(""m2""))\r\n\r\ncentroids95 <- coordinates(kernelcontours95)\r\nwrite.csv(centroids95, ""Centroids95all.csv"")\r\n\r\n## overlap indices\r\n\r\nUDOIoverlap95 <- kerneloverlaphr(uds7,method = c(""UDOI""),percent=95)\r\nhro95 <- setNames(melt(UDOIoverlap95), c(\'ID1\', \'ID2\', \'HRO95\'))\r\nwrite.csv(hro95, ""UDOI_95_25s.csv"", row.names = FALSE) \r\n\r\n## create another hrxydata from which to subset\r\nxydata4<-cbind(HRdata$X,HRdata$Y)\r\nxydata5<-as.data.frame(project(xydata4, ""+proj=tmerc +lat_0=-28 +lon_0=153 +k=0.99999 +x_0=50000 +y_0=100000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs""))\r\nxydata6<-cbind(HRdata$ID, xydata5, HRdata$Observation, HRdata$Datetime)\r\ncolnames(xydata6)<-c(""Name"",""X"",""Y"", ""observation"", ""Date"")\r\nhrxydata2<-SpatialPointsDataFrame(xydata6[,2:3],xydata6[c(""Name"",""observation"", ""Date"")])\r\n\r\n##intersect the social data with the 95% contours and subset overlapping sightings\r\n\r\nsightingsk95<-intersect(hrxydata2, kernelcontours95)\r\nsk95df<-as.data.frame(sightingsk95)\r\nrestricted95<-subset(sk95df, sk95df$id==sk95df$Name, drop=TRUE)\r\nfullrestricted95<-subset(HRdata,HRdata$Observation%in%restricted95$observation)\r\n\r\n## get number of sightings adjusted\r\nunique.fullrestricted95 <- fullrestricted95[!duplicated(fullrestricted95[""ID.Date""]),]\r\ncounts95 <- as.data.frame(table(unique.fullrestricted95$ID)) #calculate number of sightings per ID\r\nnames(counts95) <- c(""ID"", ""N_sight"") \r\nfullrestricted95$N_sight <- NULL\r\nfullrestricted95 <- merge(fullrestricted95, counts95, by = ""ID"", sort = FALSE) #add number of sightings to spatial data frame\r\nfullrestricted95 <- subset(fullrestricted95, N_sight>24, drop = TRUE)\r\nrestricted95<-subset(restricted95, restricted95$Name%in%fullrestricted95$ID, drop = TRUE)\r\n\r\nfullrestricted95 <- droplevels(fullrestricted95)\r\nrestricted95 <- droplevels(restricted95)\r\n\r\n## and calculate HWI\r\n\r\nrealHWI95<-hwi(sightings=fullrestricted95, group_variable=""Observation"", dates=""Date"", IDs=""ID"",symmetric = TRUE)\r\nrealHWI95um<-setNames(melt(realHWI95), c(""ID1"", ""ID2"", ""HWI""))\r\nwrite.csv(realHWI95um, ""HWI95_25s.csv"", row.names = FALSE)\r\n\r\n\r\n### Optimise the null model \r\n\r\nudsgdf <- as(estUDm2spixdf(uds7),""SpatialGridDataFrame"")\r\n\r\n## creating udsgdf for different HR classes\r\ngrid=udsgdf\r\n\r\ngrid95<- list()\r\ngrid50<-list()\r\n\r\nfor (i in 1:length(kernelcontours95@data$id)){\r\n  \r\n  g95<-grid[i]\r\n  g195<-raster(g95)\r\n  m95<-kernelcontours95[i,]\r\n  msk95<-mask(g195, m95, inverse=FALSE)\r\n  grid_ae95 <- as(msk95, \'SpatialGridDataFrame\')\r\n  gridded(grid_ae95) <- TRUE\r\n  grid_ae95[[1]] <- as.numeric(!is.na(grid_ae95[[1]]))\r\n  resu95 <- lapply(1:ncol(g95), function(i) {g95[[i]] * grid_ae95[[1]] / sum(g95[[i]] * grid_ae95[[1]]) })\r\n  resu95 <- as.data.frame(resu95) \r\n  names(resu95) <- names(g95@data) \r\n  g95@data <- resu95\r\n  grid95[[i]]<-g95\r\n  cat(i)\r\n}\r\n\r\ng95<-do.call(""cbind"", grid95)  ##pixels\r\nudsgdf_95 <- as(g95,""SpatialGridDataFrame"") ##convert to grid for digiroo\r\nsave(udsgdf_95, file = ""udsgdf_95.Rdata"")\r\nload(""udsgdf_95.Rdata"")\r\n\r\n## Adjust the probability of being sighted according to temporal overlap (first and last sightings)\r\n\r\n#Number of survey days\r\n\r\nrestricted95$DateS<-strptime(restricted95$Date, format=c(""%d/%m/%Y %H:%M"")) #for other HR classes continue replacing xydata3 and HRdata for e.g. restricted50\r\nrestricted95$DateS<-as.POSIXct(restricted95$DateS)\r\nhead(restricted95$DateS)\r\n\r\nd<-length(unique(restricted95$DateS))  #number of surveys\r\ndates<-sort(unique(restricted95$DateS)) #surveys by date\r\n\r\n#Get availability matrix\r\n\r\nnames<-sort(unique(restricted95$Name)) #Individual names\r\n\r\nmatnames<-list(dates,names)\r\n\r\navailability <- read.csv(""Entry-departure.csv"", stringsAsFactors=FALSE)\r\navailability <- subset(availability, availability$ID%in%restricted50$Name)\r\navailability$entr<-strptime(availability$entry, format=c(""%Y-%m-%d %H:%M:%S""))\r\navailability$entr<-as.POSIXct(availability$entr)\r\navailability$depart<-strptime(availability$departure, format=c(""%Y-%m-%d %H:%M:%S""))\r\navailability$depart<-as.POSIXct(availability$depart)\r\nhead(availability)\r\n\r\nalive<-Vectorize(FUN=function(r,c) isTRUE(r>=availability$entr[which(availability$ID==c)] &\r\n                                            r<=availability$depart[which(availability$ID==c)]))\r\n\r\nschedule<-outer(dates, names, FUN=alive)\r\ndimnames(schedule)<-matnames\r\nunam<-unique(restricted95[c(""DateS"",""Name"")])\r\n\r\n#Get sighting probabilities corrected for availability\r\nID_counts <- tapply(rep(1,length(unam$Name)),unam$Name,sum) #number of times each animal is seen\r\ns95 <- data.frame(tapply(fullrestricted95$ID, fullrestricted95$Datetime, FUN = length))\r\nsamplesize <- round(mean(s95[,1]),0) # Number of IDs to include in simulations (with replacement)\r\nj<-apply(schedule,2,sum) #number of days each animal was available\r\npObs <- as.vector(ID_counts)/j #probability of shighting that individual (seen/available)\r\n\r\n\r\n## calculate a probability table per home range class (UD contour), i.e. individuals to include in each ""day"" of random model\r\n\r\nbootlength <- d-2 #Can\'t use the last two surveys, too few individuals are left\r\n\r\npID <- sapply(1:bootlength,function(i) sample(which(schedule[i,]==TRUE),size=samplesize,replace=FALSE,\r\n                                              prob=pObs[which(schedule[i,]==TRUE)]))\r\nsave(pID, file=""pID.Rdata"")\r\nload(""pID.Rdata"")\r\n\r\n#Test gprox values 10 times each on full grid and 50 percent contours\r\nGprox_set<-sort(rep(seq(4, 8, by=1),10)) #here a sequence form 1 to 10m, edit according to study system\r\n\r\n\r\n#Test gprox values on full grid and within other percent contours\r\n\r\ngp<-list()\r\nfor (k in 1:50) {\r\n  Gprox=Gprox_set[k]\r\n  result_full<- fAssocmatrix(sPerm=1:bootlength,Gprox=Gprox,iextract=udsgdf_95,iID=pID) #use appropriate bootlength, udsgfd and pID for different HR classes\r\n  gp[[k]]<-result_full\r\n  names(gp)[k]<-paste0(""trial"",k,""_"", Gprox)\r\n  #verbose\r\n  cat(k)\r\n}\r\n\r\ngp<-lapply(gp,digu)\r\n\r\n\r\n##run the hwi and calculate averages for observed and randoms \r\n\r\nrand_mats<-lapply(gp,function(x) hwi(sightings=x,group_variable=""Group"", dates=""Permutation"", IDs=""IDs"",symmetric = TRUE))\r\nrm<-mergeMatrices(rand_mats)\r\nwrite.csv(rm,""OptimisingGprox_randoms_full.csv"")\r\n\r\n\r\n### Run the null model\r\n\r\nGprox95 <- 6\r\n\r\nhrfull<-list()\r\n\r\n\r\nfor (k in 1:100) {\r\n  result<- fAssocmatrix(sPerm=1:bootlength,Gprox=Gprox95,iextract=udsgdf_95,iID=pID)\r\n  result$Group<-paste0(result$Permutation,""-"", result$Group)\r\n  hrfull[[k]]<-result\r\n  #verbose\r\n  cat(k)\r\n}\r\nhrfull <- lapply(hrfull, digu)\r\n\r\n\r\n## calculate hwi\'s for each of these random datasets to assign avoidance/prefs etc.\r\n\r\nrand_mats<-lapply(hrfull,function(x) hwi(sightings=x,group_variable=""Group"", dates=""Permutation"", IDs=""IDs"",symmetric = TRUE))\r\n\r\nlibrary(gdata)\r\n\r\nfull_mats<-list()\r\n\r\nfor (i in 1:length(rand_mats)){\r\n  rand_mats[[i]]->mat\r\n  unmat<-data.frame(unmatrix(mat))\r\n  IDs<-row.names(unmat)\r\n  unmat<-data.frame(cbind(IDs,unmat$unmatrix.mat.))## just check the column name here ($ name)\r\n  colnames(unmat)<-c(""ID"",""HWI"")\r\n  mat_f<-unmat[order(unmat$ID),]\r\n  full_mats[[i]]<-mat_f\r\n  #verbose\r\n  cat(i)\r\n}\r\n\r\nfinal<-do.call(""cbind"",full_mats)\r\nwrite.csv(final,""randomsfull.csv"")\r\n\r\n### Calculate preferences and casual associations \r\n\r\nfinal <- read.csv(""randomsfull.csv"")\r\nfinal[1] <- NULL\r\n\r\ndat <- final[c(1, seq(2, 2000, 2))] #remove repeated ID columns\r\nrm(list=setdiff(ls(), c(""dat""))) #remove everything except random data files from environment\r\n\r\nHWI <- read.csv(""HWI95_25s.csv"")\r\nHWI$ID <- paste(HWI$ID1, HWI$ID2, sep = "":"")\r\nHWI <- setNames(HWI[c(4,3)], c(""ID"", ""Real.HWI""))\r\n\r\ndat <- merge(dat, HWI, by = ""ID"")\r\ndat <- na.omit(dat) #remove HWI to self (NAs)\r\n\r\nprobs <- vector() # calculate probability that random HWI is greater than observed\r\nfor(i in 1:nrow(dat)){\r\n  if (dat$Real.HWI[i] == 0){pr <- dat[i,2:1001] > dat$Real.HWI[i]}\r\n  else {pr <- dat[i,2:1001] >= dat$Real.HWI[i]}\r\n  probs[i] <- sum(pr)\r\n}\r\ndat$prob <- probs/1000\r\n\r\ndat$Class <- ifelse(dat$Real.HWI == 0, ""N"",\r\n             ifelse(dat$prob <= 0.05, ""P"", ""C"")) # Define preferences at fifth percentile\r\n\r\nfdat <- dat[,c(1,1002:1004)]\r\n\r\nfdat$Pref <- ifelse(fdat$Class == ""P"", 1, 0)\r\nfdat$Cas <- ifelse(fdat$Class == ""C"", 1, 0)\r\nhead(fdat)\r\nsummary(fdat)\r\nwrite.csv(fdat, ""Social_class95.csv"", row.names = FALSE)\r\n\r\n\r\n### Add relatedness\r\n\r\n\r\n#Get relatedness\r\nRel.snp <- read.csv(""R_estimates_DyadML_4433SNPs.csv"")\r\nRel.snp <- rbind(Rel.snp, setNames(Rel.snp[,c(2,1,3)], c(""ID1"", ""ID2"", ""rel""))) #Get all possible combinations of ID1 and ID2\r\n\r\n#Get HWIs\r\nHWI <- read.csv(""HWI95_25s.csv"")\r\ndat <- merge(Rel.snp, HWI, by = c(""ID1"", ""ID2"")) #Merge\r\n\r\n#Add sex\r\nSex <- read.csv(""Gender.csv"")\r\nnames(Sex) <- c(""ID"", ""sex"")\r\ndata2 <- merge(dat, Sex, by.x = ""ID1"", by.y = ""ID"", all.x = TRUE)\r\ndata2 <- merge(data2, Sex, by.x = ""ID2"", by.y = ""ID"", all.x = TRUE)\r\ndata2$sex <- paste(data2$sex.x, data2$sex.y, sep = """")\r\ndata2$sex.x <- data2$sex.y <- NULL\r\ndata2$sex <- ifelse(data2$sex == ""FeM"", ""MFe"", data2$sex) #give three unique categories (MM, FeFe, MFe)\r\nhead(data2)\r\n\r\n#Add home range overlap\r\nhro95  <- read.csv(""UDOI_95_25s.csv"")\r\ndata3 <- merge(data2, hro95, by = c(""ID1"", ""ID2""), all.x = TRUE)\r\nhead(data3)\r\n\r\n#Add preferences and casual associations\r\ndigi <- read.csv(""Social_class95.csv"")\r\ndigi$ID <- as.character(digi$ID)\r\ndigi$ID1 <- do.call(rbind, lapply(strsplit(digi$ID, "":""), ""["", 1))\r\ndigi$ID2 <- do.call(rbind, lapply(strsplit(digi$ID, "":""), ""["", 2))\r\ndigi <- digi[,c(""ID1"", ""ID2"", ""Class"", ""Pref"", ""Cas"")]\r\ndata4 <- merge(data3, digi, by = c(""ID1"", ""ID2""), all.x = TRUE)\r\ndata4 <- data4[order(as.character(data4$ID1)),]\r\nhead(data4)\r\n\r\nwrite.csv(data4, ""Pairwise_masterfile_DML.csv"", row.names = FALSE)\r\n\r\n\r\n\r\n############################################################################################################\r\n################# 1- SPATIAL DISTRIBUTION AND AVAILABILITY OF KIN ##########################################\r\n############################################################################################################\r\n\r\n######## 1- Mantel test of relatedness as a function of home-range overlap #######################\r\n\r\nlibrary(reshape2)\r\nlibrary(asnipe)\r\nlibrary(vegan)\r\n\r\ndat <- read.csv(""Pairwise_masterfile_DML.csv"") # all\r\ndat <- subset(dat, sex != ""MM"") # female-female and female-male\r\ndat <- subset(dat, sex != ""MM"" & sex != ""MFe"") # female-female \r\ndat <- subset(dat, sex != ""FeFe"") # Male-male and male-female\r\ndat <- subset(dat, sex != ""FeFe"" & sex != ""MFe"") # Male-male\r\n\r\n#### All ####\r\n\r\n#### Make individual matrices\r\n\r\n# Relatedness matrix\r\nRel<- dat[,c(""ID1"", ""ID2"", ""rel"")] #Get IDs of pairs and the pair-wise value of interest\r\nRel.m <- acast(Rel, ID2~ID1, value.var=""rel"", fill = NA, drop = FALSE) #convert into a matrix rows~columns\r\nRel.m[1:10,1:10]\r\n\r\n# HRO matrices\r\nhro95 <- dat[,c(""ID1"", ""ID2"", ""HRO95"")] #Get IDs of pairs and the pair-wise value of interest\r\nhro95.m <- acast(hro95, ID1~ID2, value.var=""HRO95"") #convert into a matrix rows~columns\r\ndiag(hro95.m) <- NA\r\nhro95.m[1:10,1:10]\r\n\r\n# Mantel test\r\n\r\nman <- mantel(hro95.m, Rel.m, na.rm = TRUE, permutations = 9999)\r\nman\r\n\r\n\r\n###### 2- Prepare data for GenAlEx #############################################################\r\n\r\n## Get genetic data\r\n\r\ngene <- read.table(""4433SNP_ME_letters.ped"", stringsAsFactors = FALSE, sep = "" "", header = FALSE)\r\ngene <- gene[,-c(1,3,4,6)] #get rid of columns for group, (keep ID), paternal ID, maternal ID, (keep sex) and phenotype\r\nfor (i in 3:ncol(gene)) {\r\n  gene[,i][gene[,i]==""A""]<-1\r\n  gene[,i][gene[,i]==""C""]<-2\r\n  gene[,i][gene[,i]==""G""]<-3\r\n  gene[,i][gene[,i]==""T""]<-4\r\n  \r\n}\r\nhead(gene)[1:10]\r\ngene <- as.data.frame(gene)\r\nnames(gene) <- c(""ID"", ""SEX"", paste(paste(""G"", sort(rep(1:((ncol(gene)-2)/2), 2)), sep = """"), rep(1:2, (ncol(gene)-2)/2), sep = "".""))\r\n\r\n## Subset for individuals used in the study\r\n\r\nnames <- read.csv(""Pairwise_masterfile_DML.csv"") \r\nIDs <- unique(names$ID1)\r\ngene <- subset(gene, ID%in%IDs)\r\ncentroid <- read.csv(""Centroids95all.csv"")\r\nnames(centroid) <- c(""ID"", ""X"", ""Y"")\r\ndat <- merge(gene, centroid, by = ""ID"")\r\n\r\ndatM <- subset(dat95, SEX == 1)\r\ndatF <- subset(dat95, SEX == 2)\r\ndatM$SEX <- NULL\r\ndatM$SEX <- NULL\r\n\r\nwrite.csv(datM, ""GenAlEx_input95M_ME4433.csv"", row.names = FALSE)\r\nwrite.csv(datF, ""GenAlEx_input95F_ME4433.csv"", row.names = FALSE)\r\n\r\n\r\n###############################################################################################\r\n############ 2- Non-random use of space and kin-biased associations ###########################\r\n###############################################################################################\r\n\r\n# Reshape Digiroo outputs to suitable format\r\n\r\nload(""hr95.Rdata"")\r\nhead(hr95[[1]])\r\n#> hr95[[1]]\r\n#Permutation  Group        IDs\r\n#               1    1-1     Johnny\r\n#               1    1-1       Quad\r\n#               1    1-2    Pancake\r\n#               1    1-2     Clover\r\n#               1    1-3      Horis\r\n#               1    1-4       Earl\r\n\r\nreshaped <- list()\r\n\r\nfor(i in 1:1000){\r\n  permutation <- hr95[[i]] \r\n  groups <- split(permutation, permutation$Group)\r\n  nicest <- list()\r\n  for(j in 1:length(groups)){\r\n    group <- groups[[j]] \r\n    nice <- list()\r\n    if(nrow(group)==1){\r\n      nicer <- data.frame(ID1 = group$IDs, ID2 = NA, Permutation = group$Permutation, Group = group$Group)\r\n    }else{\r\n      for(k in 1:length(group$IDs)){\r\n        ex <- setNames(expand.grid(group$IDs[k], group$IDs), c(""ID1"", ""ID2"")) \r\n        ex <- subset(ex, ID1 != ID2)\r\n        ex$Permutation <- rep(group[1,1], nrow(ex))\r\n        ex$Group <- rep(group[1,2], nrow(ex))\r\n        nice[[k]] <- ex \r\n      }\r\n      nicer <- do.call(""rbind"", nice)\r\n      nicer <- nicer[!duplicated(nicer), ]\r\n    }\r\n    nicest[[j]] <- nicer  \r\n  }\r\n  reunited <- do.call(""rbind"", nicest)\r\n  reunited <- reunited[order(reunited$Permutation), ] \r\n  reshaped[[i]] <- reunited \r\n  cat(i) \r\n}\r\n\r\nreshaped95 <- reshaped\r\n#save(reshaped95, file=""hr95reshaped.Rdata"")\r\nload(""hr95reshaped.Rdata"")\r\n\r\n\r\n### Calculate probability of finding relatives \r\n\r\nhead(reshaped95[[1]], 4)\r\n#> reshaped95[[1]]\r\n#             ID1        ID2 Permutation  Group\r\n#          Johnny       Quad           1    1-1\r\n#            Quad     Johnny           1    1-1\r\n#            Esme       <NA>           1   1-10\r\n#          Rolley       <NA>           1   1-11\r\n\r\nRel.snp <- read.csv(""HWI/Pairwise_masterfile_DML.csv"") # Get relatedness\r\nRel.snp <- Rel.snp[,c(""ID1"",""ID2"",""sex"", ""rel"")]\r\nthreshold <- 0.25\r\n\r\np.rel95M <- vector()\r\np.rel95F <- vector()\r\n\r\nfor(i in 1:1000){\r\n  permutation <- reshaped95[[i]] #i\r\n  pmerged <- merge(permutation, Rel.snp, by = c(""ID1"", ""ID2"")) \r\n  pmerged$is.rel <- ifelse(pmerged$rel >= threshold, 1, 0)\r\n  datM <- subset(pmerged, sex != ""FF"")\r\n  meansM <- vector()\r\n  for(j in 1:length(unique(datM$ID1))){\r\n    ind <- subset(datM, ID1 == unique(datM$ID1)[j])\r\n    meansM[j] <- mean(ind$is.rel)\r\n  }\r\n  datF <- subset(pmerged, sex != ""MM"")\r\n  meansF <- vector()\r\n  for(j in 1:length(unique(datF$ID1))){\r\n    ind <- subset(datF, ID1 == unique(datF$ID1)[j])\r\n    meansF[j] <- mean(ind$is.rel)\r\n  }\r\n  p.rel95M[i] <- mean(meansM) # mean across idividuals in each random population\r\n  p.rel95F[i] <- mean(meansF)\r\n  cat(i)\r\n}\r\n\r\n### Calculate observed rates\r\n\r\ndat <- read.csv(""fullrestricted95.csv"") # get restricted social data to 95% HRs\r\ndatmerged <- merge(dat[,c(""ID"", ""Partner"", ""SEX"")], Rel.snp[,c(""ID1"", ""ID2"", ""rel"")], by.x = c(""ID"", ""Partner""), by.y = c(""ID1"", ""ID2"")) #gets only interactions\r\ndatmerged$is.rel <- ifelse(datmerged$rel >= threshold, 1, 0)\r\n\r\ndatM <- subset(datmerged, SEX == ""M"")\r\nmeansM <- vector()\r\nfor(i in 1:length(unique(datM$ID))){\r\n  ind <- subset(datM, ID == unique(datM$ID)[i])\r\n  meansM[i] <- mean(ind$is.rel)\r\n}\r\nrealp.rel95M <- mean(meansM) # Observed mean\r\n\r\ndatF <- subset(datmerged, SEX == ""F"")\r\nmeansF <- vector()\r\nfor(i in 1:length(unique(datF$ID))){\r\n  ind <- subset(datF, ID == unique(datF$ID)[i])\r\n  meansF[i] <- mean(ind$is.rel)\r\n}\r\nrealp.rel95F <- mean(meansF) \r\n\r\nsum(p.rel95M >= realp.rel95M)/1000 # p-value\r\nsum(p.rel95F >= realp.rel95F)/1000 \r\n\r\n\r\n############################################################################################\r\n####### 3- Kin-biased long-term social associations ########################################\r\n############################################################################################\r\n\r\n\r\nlibrary(tidyr)\r\nlibrary(lme4)\r\nlibrary(gamm4)\r\nlibrary(reshape2)\r\nlibrary(asnipe)\r\nlibrary(vegan)\r\n\r\n### 1- MRQAP of HWI and relatedness \r\n\r\n\r\ndat <- read.csv(""Pairwise_masterfile_DML.csv"")\r\ndat$is.rel <- ifelse(dat$rel >= 0.25, 1, 0)\r\n\r\n## Make individual matrices\r\n\r\n#Relatedness matrix\r\nRel<- dat[,c(""ID1"", ""ID2"", ""rel"")] #Get IDs of pairs and the pair-wise value of interest\r\nRel.m <- acast(Rel, ID2~ID1, value.var=""rel"")#, fill = NA, drop = FALSE) #convert into a matrix rows~columns\r\n\r\n#Relatedness category matrix\r\nRelcat<- dat[,c(""ID1"", ""ID2"", ""is.rel"")] #Get IDs of pairs and the pair-wise value of interest\r\nRelcat.m <- acast(Relcat, ID2~ID1, value.var=""is.rel"")#, fill = NA, drop = FALSE) #convert into a matrix rows~columns\r\n\r\n#HWI matrix\r\nHWI <- dat[,c(""ID1"", ""ID2"", ""HWI"")] #Get IDs of pairs and the pair-wise value of interest\r\nHWI.m <- acast(HWI, ID1~ID2, value.var=""HWI"") #convert into a matrix rows~columns\r\n\r\n#Preference matrices\r\nPref <- dat[,c(""ID1"", ""ID2"", ""Pref"")] #Get IDs of pairs and the pair-wise value of interest\r\nPref.m <- acast(Pref, ID1~ID2, value.var=""Pref"") #convert into a matrix rows~columns\r\n\r\n#HRO matrices\r\nhro <- dat[,c(""ID1"", ""ID2"", ""HRO95"")] #Get IDs of pairs and the pair-wise value of interest\r\nhro.m <- acast(hro, ID1~ID2, value.var=""HRO95"") #convert into a matrix rows~columns\r\ndiag(hro.m) <- NA\r\n\r\n\r\n## MRQAP analysis \r\n\r\nreg <- mrqap.dsp(HWI.m ~ Rel.m + hro.m,  directed = ""undirected"", test.statistic = ""t-value"", randomisations=1000)\r\nreg # Look at results\r\n\r\n## Relatedness category\r\n\r\nreg <- mrqap.dsp(HWI.m ~ Relcat.m + hro.m,  directed = ""undirected"", test.statistic = ""t-value"", randomisations=1000)\r\nreg # Look at results\r\n\r\n## Preferences\r\n\r\nreg <- mrqap.dsp(Pref.m ~ Rel.m + hro.m,  directed = ""undirected"", test.statistic = ""t-value"", randomisations=1000)\r\nreg \r\n\r\n\r\n### 2- GLMM of preferences and causal associations and relatedness category \r\n\r\n#set a threshold for relatives\r\nrel.threshold <- 0.25\r\n\r\n#Get data\r\npair <- read.csv(""Pairwise_masterfile_DML.csv"")\r\n\r\n#Count the number of overlapping individuals\r\npair$is.HRO95 <- ifelse(pair$HRO95 > 0, 1, 0)\r\nhro95t <- data.frame(n95 = with(pair, tapply(is.HRO95, ID1, FUN = sum)))\r\n\r\n#Which of the overlapping individuals are kin?\r\npair$is.rel <- ifelse(pair$rel > rel.threshold, 1, 0)\r\npair$rel95 <- with(pair, ifelse(is.rel == 1 & is.HRO95 == 1, 1, 0))\r\npair$reltot <- with(pair, ifelse(is.rel == 1, 1, 0))\r\nrel95t <- as.data.frame(with(pair, tapply(rel95, ID1, FUN = sum)))\r\nreltott <- as.data.frame(with(pair, tapply(reltot, ID1, FUN = sum)))\r\nrelt <- setNames(cbind(rel95t, reltott), c(""rel95"", ""relt""))\r\nhrot<- cbind(hro95t, relt)\r\n\r\n#Count the number of preferences and casuals\r\npref <- as.data.frame(with(pair, tapply(Pref, ID1, FUN = sum))) # get the number of preferences per individual\r\ncas <- as.data.frame(with(pair, tapply(Cas, ID1, FUN = sum)))  # get the number of casuals per individual\r\n\r\nfriends <- cbind(pref, cas)\r\nnames(friends) <- c(""Pref"", ""Cas"")\r\ndat <- merge(hrot, friends, by = ""row.names"")\r\nnames(dat)[1] <- ""ID""\r\n\r\n#Calculate preferences and casuals \r\npair$PrefR <- ifelse(pair$rel > rel.threshold & pair$Pref == 1, 1, 0) #are relatives and preference\r\npair$CasR <- ifelse(pair$rel > rel.threshold & pair$Cas == 1, 1, 0) #are relatives and casual\r\n\r\nprefR <- as.data.frame(with(pair, tapply(PrefR, ID1, FUN = sum))) #get the number of preferred kin per individual\r\ncasR <- as.data.frame(with(pair, tapply(CasR, ID1, FUN = sum))) #get the number of casual kin per individual\r\n\r\nfamily <- cbind(prefR, casR)\r\nnames(family) <- c(""PrefR"", ""CasR"")\r\ndat <- merge(dat, family, by.x = ""ID"", by.y = ""row.names"")\r\n\r\n#Calculate the number of non-relatives in the home range\r\ndat$Nrel95 <- dat$n95 - dat$rel95\r\n\r\n#Calculate the number of preferences and causals non-relatives\r\ndat$PrefNR <- dat$Pref - dat$PrefR\r\ndat$CasNR <- dat$Cas - dat$CasR\r\n\r\n#Add gender\r\nsex <- read.csv(""Gender.csv"", stringsAsFactors = FALSE)\r\ndat <- merge(dat, sex, by.x = ""ID"", by.y = ""ANIMAL.NAME"", all.x = TRUE)\r\n\r\n#Get data ready for modelling\r\ndatrel <- dat[,c(""ID"", ""rel95"", ""PrefR"", ""CasR"")]\r\ndatNrel <- dat[,c(""ID"", ""Nrel95"", ""PrefNR"", ""CasNR"")]\r\nnames(datrel) <- names(datNrel) <- c(""ID"", ""n"", ""Pref"", ""Cas"")\r\ndatrel$type <- ""R""\r\ndatNrel$type <- ""NR""\r\ndat2 <- rbind(datrel, datNrel)\r\ndat2 <- dat2[order(dat2$ID),]\r\n\r\n#Add gender\r\nsex <- read.csv(""Gender.csv"", stringsAsFactors = FALSE)\r\ndat2 <- merge(dat2, sex, by.x = ""ID"", by.y = ""ANIMAL.NAME"", all.x = TRUE)\r\n\r\n#Add non-preferences and non-casual individuals\r\ndat2$NPref <- dat2$n - dat2$Pref\r\ndat2$NPref <- ifelse(dat2$NPref < 0 , 0, dat2$NPref)\r\ndat2$NCas <- dat2$n - dat2$Cas\r\ndat2$NCas <- ifelse(dat2$NCas < 0 , 0, dat2$NCas)\r\n\r\nwrite.csv(dat2, ""Kinship data.csv"", row.names = FALSE)\r\nrm(list = ls())\r\n\r\n## MODEL ##\r\n\r\ndat <- read.csv(""Kinship data.csv"")\r\n\r\nm95 <- glmer(cbind(Pref,NPref) ~ type * SEX + (1|ID), data = dat, family = ""binomial"")\r\nsummary(m95)\r\ndrop1(m95, test = ""Chi"")\r\nm95.2 <- update(m95, ~. - type:SEX) # No difference in the effect of type by sex (p = 0.174)\r\ndrop1(m95.2, test = ""Chi"")\r\nm95.3 <- update(m95.2, ~. - SEX)\r\ndrop1(m95.3, test = ""Chi"")\r\nsummary(m95.3) #highly significant p = 7.05e-05\r\n\r\nc95 <- glmer(cbind(Cas,NCas) ~ type * SEX + (1|ID), data = dat, family = ""binomial"")\r\nsummary(c95)\r\ndrop1(c95, test = ""Chi"")\r\nc95.2 <- update(c95, ~. - type:SEX) # No difference in the effect of type by sex (p = 0.06928)\r\nsummary(c95.2) #type is not significant p = 0.797933 \r\n\r\n\r\n#### Randomize social classifications and rerun models \r\n\r\npair1 <- read.csv(""Pairwise_masterfile_DML.csv"")\r\ncoef.pref <- vector()\r\nz.pref <- vector()\r\ncoef.cas <- vector()\r\nz.cas <- vector()\r\n\r\nfor(i in 1:10001){\r\n  \r\n  pair <- pair1\r\n  \r\n  #Randomise digiroo classifications\r\n  pair$RClass <- sample(pair$Class)\r\n  \r\n  #Calculate preferences to related individuals in your home range\r\n  if(i==1){\r\n    pair$Pref <- ifelse(pair$Class == ""P"", 1, 0)\r\n    pair$Cas <- ifelse(pair$Class == ""C"", 1, 0)\r\n  }else{\r\n    pair$Pref <- ifelse(pair$RClass == ""P"", 1, 0)\r\n    pair$Cas <- ifelse(pair$RClass == ""C"", 1, 0)\r\n  }\r\n  \r\n  #Count the number of overlapping individuals\r\n  pair$is.HRO95 <- ifelse(pair$HRO95 > 0, 1, 0)\r\n  hro95t <- data.frame(n95 = with(pair, tapply(is.HRO95, ID1, FUN = sum)))\r\n  \r\n  #Which of the overlapping individuals are kin?\r\n  pair$is.rel <- ifelse(pair$rel > rel.threshold, 1, 0)\r\n  pair$rel95 <- with(pair, ifelse(is.rel == 1 & is.HRO95 == 1, 1, 0))\r\n  pair$reltot <- with(pair, ifelse(is.rel == 1, 1, 0))\r\n  rel95t <- as.data.frame(with(pair, tapply(rel95, ID1, FUN = sum)))\r\n  reltott <- as.data.frame(with(pair, tapply(reltot, ID1, FUN = sum)))\r\n  relt <- setNames(cbind(rel95t, reltott), c(""rel95"", ""relt""))\r\n  hrot<- cbind(hro95t, relt)\r\n  \r\n  #Count the number of preferences and casuals\r\n  pref <- as.data.frame(with(pair, tapply(Pref, ID1, FUN = sum))) # get the number of preferences per individual\r\n  cas <- as.data.frame(with(pair, tapply(Cas, ID1, FUN = sum)))  # get the number of casuals per individual\r\n  \r\n  friends <- cbind(pref, cas)\r\n  names(friends) <- c(""Pref"", ""Cas"")\r\n  dat <- merge(hrot, friends, by = ""row.names"")\r\n  names(dat)[1] <- ""ID""\r\n  \r\n  #Calculate preferences and casuals \r\n  pair$PrefR <- ifelse(pair$rel > rel.threshold & pair$Pref == 1, 1, 0) #are relatives and preference\r\n  pair$CasR <- ifelse(pair$rel > rel.threshold & pair$Cas == 1, 1, 0) #are relatives and casual\r\n  \r\n  prefR <- as.data.frame(with(pair, tapply(PrefR, ID1, FUN = sum))) #get the number of preferred kin per individual\r\n  casR <- as.data.frame(with(pair, tapply(CasR, ID1, FUN = sum))) #get the number of casual kin per individual\r\n  \r\n  family <- cbind(prefR, casR)\r\n  names(family) <- c(""PrefR"", ""CasR"")\r\n  dat <- merge(dat, family, by.x = ""ID"", by.y = ""row.names"")\r\n  \r\n  #Calculate the number of non-relatives in the home range\r\n  dat$Nrel95 <- dat$n95 - dat$rel95\r\n  \r\n  #Calculate the number of preferences and causals non-relatives\r\n  dat$PrefNR <- dat$Pref - dat$PrefR\r\n  dat$CasNR <- dat$Cas - dat$CasR\r\n  \r\n  \r\n  #Get data ready for modelling\r\n  datrel <- dat[,c(""ID"", ""rel95"", ""PrefR"", ""CasR"")]\r\n  datNrel <- dat[,c(""ID"", ""Nrel95"", ""PrefNR"", ""CasNR"")]\r\n  names(datrel) <- names(datNrel) <- c(""ID"", ""n"", ""Pref"", ""Cas"")\r\n  datrel$type <- ""R""\r\n  datNrel$type <- ""NR""\r\n  dat2 <- rbind(datrel, datNrel)\r\n  dat2 <- dat2[order(dat2$ID),]\r\n  \r\n  #Add gender\r\n  sex <- read.csv(""Gender.csv"", stringsAsFactors = FALSE)\r\n  dat2 <- merge(dat2, sex, by.x = ""ID"", by.y = ""ANIMAL.NAME"", all.x = TRUE)\r\n  \r\n  #Add non-preferences and non-casual individuals\r\n  dat2$NPref <- dat2$n - dat2$Pref\r\n  dat2$NPref <- ifelse(dat2$NPref < 0 , 0, dat2$NPref)\r\n  dat2$NCas <- dat2$n - dat2$Cas\r\n  dat2$NCas <- ifelse(dat2$NCas < 0 , 0, dat2$NCas)\r\n  \r\n  dat2$SEX <- as.factor(dat2$SEX)\r\n  dat2$type <- as.factor(dat2$type)\r\n  \r\n  ## MODEL ##\r\n  \r\n  m95 <- glmer(cbind(Pref,NPref) ~ type + (1|ID), data = dat2, family = ""binomial"")\r\n  coef.pref[i] <- summary(m95)$coefficients[2,1] # coeficient for ""type"" (effect of relatedness category)\r\n  z.pref[i] <- summary(m95)$coefficients[2,3] # z value for ""type""\r\n  \r\n  c95 <- glmer(cbind(Cas,NCas) ~ type + SEX + (1|ID), data = dat2, family = ""binomial"")\r\n  coef.cas[i] <- summary(c95)$coefficients[2,1]\r\n  z.cas[i] <- summary(c95)$coefficients[2,3]\r\n  \r\n}\r\n\r\np.pref <- sum(coef.pref[2:length(coef.pref)] >= coef.pref[1])/10000 # p-value for coefficient\r\nmean(z.pref[2:length(z.pref)]) + 1.98*sd(z.pref[2:length(z.pref)]) # upper 95% confidence interval for z-value\r\nmean(z.pref[2:length(z.pref)]) - 1.98*sd(z.pref[2:length(z.pref)]) # lower 95% confidence interval\r\n\r\np.cas <- sum(coef.cas[2:length(coef.cas)] >= coef.cas[1])/10000 \r\nmean(z.cas[2:length(z.cas)]) + 1.98*sd(z.cas[2:length(z.cas)]) \r\nmean(z.cas[2:length(z.cas)]) - 1.98*sd(z.cas[2:length(z.cas)]) \r\n\r\n\r\n']","Data from: Presence of kin-biased social associations in a lizard with no parental care Numerous studies have observed kin-biased social associations in a variety of species. Many of these studies have focussed on species exhibiting parental care, which may facilitate the transmission of the social environment from parents to offspring. This becomes problematic when disentangling whether kin-biased associations are driven by kin recognition, or are a product of transmission of the social environment during ontogeny, or a combination of both. Studying kin-biased associations in systems that lack parental care may aid in addressing this issue. Furthermore, when studying kin-biased social associations it is important to differentiate whether these originate from preferential choice or occur randomly as a result of habitat use or limited dispersal. Here, we combined high-resolution SNP data with a long-term behavioural dataset of a reptile with no parental care to demonstrate that eastern water dragons (Intellagama lesueurii) bias their non-random social associations toward their kin. In particular, we found that, while the overall social network was not linked to genetic relatedness, individuals associated with kin more than expected given availability in space, and also biased social preferences towards kin. This result opens important opportunities for the study of kinship-driven associations without the confounding effect of vertical transmission of social environments. Furthermore, we present a robust multiple-step approach for determining whether kin-biased social associations are a result of active social decisions, or random encounters resulting from habitat use and dispersal patterns.",2
Social dominance status is associated with differences in spatial cognitive flexibility in wild mountain chickadees,"Social dominance has long been used as a model to investigate social stress. However, many studies using such comparisons have been performed in captive environments. These environments may produce unnaturally high antagonistic interactions, exaggerating the stress of social subordination and any associated adverse consequences. One such adverse effect concerns impaired cognitive ability, often thought to be associated with social subordination. Here, we tested whether social dominance rank is associated with differences in spatial learning and memory and in reversal spatial learning (flexibility) abilities in wild food-caching mountain chickadees at different montane elevations. Higher dominance rank was associated with higher spatial cognitive flexibility in harsh environments at higher elevations, but not at lower, milder elevations. In contrast, there were no consistent differences in spatial learning and memory ability associated with dominance rank. Our results suggest that spatial learning and memory ability in specialized food-caching species is a stable trait resilient to social influences. Spatial cognitive flexibility, on the other hand, appears to be more sensitive to environmental influences including social dominance. These findings contradict those from laboratory studies and suggest that it is critical to investigate the biological consequences of social dominance under natural conditions.","['# Extract displacement events from feeder visitation data\n\n# Required libraries\n  library(lubridate)\n  library(EloRating)\n\n# Load in detectdisplacement function\nsource(""eth12720-sup-0001-Code.R"")\n\n  #=============================================================================================\n# Load feeder visitation data\nsf21 = read.csv(""SingleFeederVisits_20-21.csv"")\nsf20 = read.csv(""SingleFeederVisits_19-20.csv"")\n\n# Get time in seconds\nsf21$startTime = as.numeric(ymd_hms(sf21$startTime))\nsf21$endTime = as.numeric(ymd_hms(sf21$endTime))\n  \nsf20$startTime = as.numeric(ymd_hms(sf20$startTime))\nsf20$endTime = as.numeric(ymd_hms(sf20$endTime))\n  \n# Create locationDay variable \nsf21$locationDay = paste(sf21$location, sf21$date, sep = ""_"")\nsf20$locationDay = paste(sf20$location, sf20$date, sep = ""_"")\n\n\n# Detect displacement events, get dominance matrix\nsf21_dom = detectdisplacement(arrivetime = sf21$startTime,\n                                 departtime = sf21$endTime,\n                                 ids = sf21$tag,\n                                 sites = sf21$location,\n                                 divisions = sf21$locationDay,\n                                 displacetime = 1,\n                                 remaintime = 2)\n\n\nsf20_dom = detectdisplacement(arrivetime = sf21$startTime,\n                              departtime = sf21$endTime,\n                              ids = sf21$tag,\n                              sites = sf21$location,\n                              divisions = sf21$locationDay,\n                              displacetime = 1,\n                              remaintime = 2)\n\n# Pull displacement interactions \ndisp_21 = sf21_dom$interactions\ndisp_20 = sf20_dom$interactions\n\n\n# Get groups \ngroups_21 = read.csv(""Communities_20-21.csv"")\ngroups_20 = read.csv(""Communities_19-20.csv"")\n\n\n# Match winner and loser IDs to community IDs\n# Only use interactions where both winner and loser are members of the same community\n\ndisp_21$wgroup = groups_21$finalcommunity[match(unlist(disp_21$win), groups_21$tag)]\ndisp_21$lgroup = groups_21$finalcommunity[match(unlist(disp_21$loss), groups_21$tag)]\ndisp_21$group = ifelse(disp_21$wgroup == disp_21$lgroup, disp_21$wgroup, NA)\ndisp_21 = disp_21[!is.na(disp_21$group),]\ndisp_21$wgroup = NULL\ndisp_21$lgroup = NULL\n\ndisp_20$wgroup = groups_20$finalcommunity[match(unlist(disp_20$win), groups_20$tag)]\ndisp_20$lgroup = groups_20$finalcommunity[match(unlist(disp_20$loss), groups_20$tag)]\ndisp_20$group = ifelse(disp_20$wgroup == disp_20$lgroup, disp_20$wgroup, NA)\ndisp_20 = disp_20[!is.na(disp_20$group),]\ndisp_20 = disp_20 %>% select(-c(wgroup, lgroup))\n\n# Combine seasons\ndisp_21$season = ""S20-21""\ndisp_20$season = ""S19-20""\n\ndisp_21$group = paste0(disp_21$group, ""_20-21"")\ndisp_20$group = paste0(disp_20$group, ""_19-20"")\n\ndisp_both = rbind(disp_20, disp_21)\n\nwrite.csv(disp_both, file=""DisplacementEvents.csv"", row.names=F)\n\n#==================================================================================\n# Get within-group rankings\n\n# Function to subset by group and calculate David\'s scores w/in that group\nscores_bygroup = function(dat, groupname) {\n  \n  dat = dat %>% \n    filter(group == groupname) %>%\n    arrange(dt)\n  \n  seq = elo.seq(winner = dat$win, \n                loser=dat$loss, \n                Date=dat$date, \n                progressbar=F)\n  mat = creatematrix(seq)\n  scores = DS(mat)\n  scores$group = groupname\n  scores$season = dat[1,7]\n  scores$rank = rank(scores[,2])\n  \n  return(scores)\n}\n\n# Loop through groups and get rankings\nsuperlist = vector(""list"", length=length(unique(disp_both$group)))  \n\nfor (i in 1:length(unique(disp_both$group))) {\n  \n  scores = scores_bygroup(disp_both, unique(disp_both$group)[[i]])\n  superlist[[i]] = scores\n}\n\n# Combine scores\nscores = do.call(""rbind"", superlist)\n\n# These scores were then combined with cognitive testing data, etc \n# The final output is saved as ""Ranks_CogScores.csv""\n\n#====================================================================================\n\n', '# Dominance analyses\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)\n\ndat = read.csv(""Ranks_CogScores.csv"")\n\n# Pre-analysis data massaging\n# Get scaled ranks within each group \ndat = dat %>%\n  group_by(season, group) %>%\n  mutate(rank_scaled = rescale(rank, to=c(0,1))) %>%\n# Remove birds that didn\'t complete enough cognition trials  \n# Or that don\'t have cognition data at all\n  filter(InitTrials > 19, RevTrials > 19) \n\n# Group birds into quartiles by rank\ndat = dat %>%\n  group_by(season, elevation, group) %>%\n  arrange(season,elevation, group, rank_scaled) %>%\n  mutate(rank_Q = ntile(rank_scaled, 4)) %>%\n  mutate(rank_Q = as.factor(rank_Q))\n\n# Reshape data in preparation for learning curve analysis \n\n# Initial testing scores\ndat_Ierr = dat %>%\n  ungroup() %>%\n  select(tag, season, group, elevation, rank_scaled, rank_Q, InitTrials, RevTrials,\n         InitScore_20, InitScore_10, InitScore_5, InitScore_3) %>%\n  rename(Trials.03 = InitScore_3,\n         Trials.05 = InitScore_5,\n         Trials.10 = InitScore_10,\n         Trials.20 = InitScore_20) %>%\n  pivot_longer(cols = !c(tag, season, group, elevation, rank_scaled, rank_Q, InitTrials, RevTrials),\n               names_to = ""NumTrials"",\n               values_to = ""MeanInitScore"")\n\n# Reversal testing scores\ndat_Rerr = dat %>%\n  ungroup() %>%\n  select(tag, season, elevation,\n         RevScore_20, RevScore_10, RevScore_5, RevScore_3) %>%\n  rename(Trials.03 = RevScore_3,\n         Trials.05 = RevScore_5,\n         Trials.10 = RevScore_10,\n         Trials.20 = RevScore_20) %>%\n  pivot_longer(cols = !c(tag, season, elevation),\n               names_to = ""NumTrials"",\n               values_to = ""MeanRevScore"")\n\n# Combine\ndat_Qlong = full_join(dat_Ierr, dat_Rerr, by = c(""tag"", ""season"", ""elevation"", ""NumTrials""))\nrm(dat_Ierr, dat_Rerr)\n\n\n#===========================================================================================================\n\n# Regression - 20 trials score vs dominance\nlibrary(car)\nlibrary(ggfortify)\n\n# Split data by seasons \nseason1920 = subset(dat, season == ""S19-20"")\nseason2021 = subset(dat, season == ""S20-21"")\n\n#--- 19-20 ------------------------------------------------------\n# Initial\nmI20_1920 = lm(InitScore_20 ~ elevation*rank_scaled, data = season1920)\nAnova(mI20_1920, type = ""III"")\nsummary(mI20_1920)\nautoplot(mI20_1920)\n\n# Check model without outlier\nmI20_1920_o = lm(InitScore_20 ~ elevation*rank_scaled, data = season1920[-95,])\nAnova(mI20_1920_o, type = ""III"")\nsummary(mI20_1920_o)\n\n# Reversal\nmR20_1920 = lm(RevScore_20 ~ elevation*rank_scaled, data = season1920)\nAnova(mR20_1920, type = ""III"")\nsummary(mR20_1920)\nautoplot(mR20_1920)\n\n# Check model without outlier\nmR20_1920_o = lm(RevScore_20 ~ elevation*rank_scaled, data = season1920[-126,])\nAnova(mR20_1920_o, type = ""III"")\nsummary(mR20_1920_o )\n\n#--- 20-21 -----------------------------\n# Initial\nmI20_2021 = lm(InitScore_20 ~ elevation*rank_scaled, data = season2021)\nAnova(mI20_2021, type = ""III"")\nsummary(mI20_2021)\nautoplot(mI20_2021)\n\n# Reversal\nmR20_2021 = lm(RevScore_20 ~ elevation*rank_scaled, data = season2021)\nAnova(mR20_2021, type = ""III"")\nsummary(mR20_2021)\nautoplot(mR20_2021)\n\n#=================================================================================================================\n# Learning curves \nlibrary(lme4)\nlibrary(emmeans)\n\nseason1920_err = subset(dat_Qlong, season == ""S19-20"")\nseason2021_err = subset(dat_Qlong, season == ""S20-21"")\n\n# ---19-20 ---\n# Initial  \nmLearn1920_3w = lmer(MeanInitScore ~ (elevation*rank_Q*NumTrials) + (1|tag), data = season1920_err)\nAnova(mLearn1920_3w, type = ""III"", test.statistic = ""F"")\n\nmLearn1920 = lmer(MeanInitScore ~ (elevation + rank_Q + NumTrials)^2 + (1|tag), data = season1920_err)\nAnova(mLearn1920, type = ""III"", test.statistic = ""F"")\n\nanova(mLearn1920_3w, mLearn1920)\n\nemmeans(mLearn1920, pairwise ~ elevation*rank_Q | NumTrials)\nemmeans(mLearn1920, pairwise ~ rank_Q | elevation | NumTrials)\n\n# Reversal\nmLearnR1920_3w = lmer(MeanRevScore ~ (elevation*rank_Q*NumTrials) + (1|tag), data = season1920_err)\nAnova(mLearnR1920_3w, type = ""III"", test.statistic = ""F"")\n\nmLearnR1920 = lmer(MeanRevScore ~ (elevation + rank_Q + NumTrials)^2 + (1|tag), data = season1920_err)\nAnova(mLearnR1920, type = ""III"", test.statistic = ""F"")\n\nanova(mLearnR1920_3w, mLearnR1920)\n\nemmeans(mLearnR1920, pairwise ~ elevation*rank_Q | NumTrials)\nemmeans(mLearnR1920, pairwise ~ rank_Q | elevation | NumTrials)\n\n# ---20-21 -----------------------------\n# Initial  \nmLearn2021_3w = lmer(MeanInitScore ~ (elevation*rank_Q*NumTrials) + (1|tag), data = season2021_err)\nAnova(mLearn2021_3w, type = ""III"", test.statistic = ""F"")\n\nmLearn2021 = lmer(MeanInitScore ~ (elevation + rank_Q + NumTrials)^2 + (1|tag), data = season2021_err)\nAnova(mLearn2021, type = ""III"", test.statistic = ""F"")\n\nanova(mLearn2021_3w, mLearn2021)\n\nemmeans(mLearn2021, pairwise ~ elevation*rank_Q | NumTrials)\nemmeans(mLearn2021, pairwise ~ rank_Q | elevation | NumTrials)\n\n\n# Reversal\nmLearnR2021_3w = lmer(MeanRevScore ~ (elevation*rank_Q*NumTrials) + (1|tag), data = season2021_err)\nAnova(mLearnR2021_3w, type = ""III"", test.statistic = ""F"")\nemmeans(mLearnR2021_3w, pairwise ~ elevation*rank_Q | NumTrials)\nemmeans(mLearnR2021_3w, pairwise ~ rank_Q  | elevation | NumTrials)\n\nmLearnR2021 = lmer(MeanRevScore ~ (elevation + rank_Q + NumTrials)^2 + (1|tag), data = season2021_err)\nAnova(mLearnR2021, type = ""III"", test.statistic = ""F"")\n\nanova(mLearnR2021_3w, mLearnR2021)\n\nmLearnR2021_3wH = lmer(MeanRevScore ~ (rank_Q*NumTrials) + (1|tag), data = subset(season2021_err, elevation == ""H""))\nAnova(mLearnR2021_3wH, type = ""III"", test.statistic = ""F"")\nemmeans(mLearnR2021_3wH, pairwise ~ rank_Q  | NumTrials)\n\n#===========================================================================================================\n# Ordered alternatives test\nlibrary(PMCMRplus)\ncuzickTest(season2021_err$MeanInitScore, season2021_err$rank_Q)\njonckheereTest(season2021_err$MeanInitScore, season2021_err$rank_Q, alternative = ""less"")\n\n# Make sure levels go the right way \nrlevels = c(1,2,3,4)\nseason2021_err = season2021_err %>% mutate(rank_Qo=ordered(rank_Q,levels=rlevels))\nseason1920_err = season1920_err %>% mutate(rank_Qo=ordered(rank_Q,levels=rlevels))\n\nseason2021_err %>% filter(NumTrials == ""Trials.03"") %>%\n  with(.,jonckheereTest(MeanInitScore, rank_Qo,alternative = ""greater""))\n\nseason2021_err %>% filter(NumTrials == ""Trials.05"") %>%\n  with(.,jonckheereTest(MeanInitScore, rank_Qo,alternative = ""greater""))\n\nseason2021_err %>% filter(NumTrials == ""Trials.10"") %>%\n  with(.,jonckheereTest(MeanInitScore, rank_Qo,alternative = ""greater""))\n\nseason2021_err %>% filter(NumTrials == ""Trials.20"") %>%\n  with(.,jonckheereTest(MeanInitScore, rank_Qo,alternative = ""greater""))\n\n\nseason2021_err %>% filter(NumTrials == ""Trials.03"") %>%\n  with(.,jonckheereTest(MeanRevScore, rank_Qo, alternative = ""greater""))\n\nseason2021_err %>% filter(NumTrials == ""Trials.05"") %>%\n  with(.,jonckheereTest(MeanRevScore, rank_Qo,alternative = ""greater""))\n\nseason2021_err %>% filter(NumTrials == ""Trials.10"") %>%\n  with(.,jonckheereTest(MeanRevScore, rank_Qo,alternative = ""greater""))\n\nseason2021_err %>% filter(NumTrials == ""Trials.20"") %>%\n  with(.,jonckheereTest(MeanRevScore, rank_Qo,alternative = ""greater""))\n\n\n#===========================================================================================================\n# Figures!\n# Get colors\n\nlibrary(ggplot2)\nlibrary(patchwork)\ncolors = c(""#e66101"", ""#1D50CE"")\n\n#=================================================================================================================\n# Initial\n\nySeq1 = (seq(0, max(season2021$InitScore_20), by = 0.5))\nySeq2 = (seq(0, 3.5, by = 0.5))\n\n# Initial 20-21\np1 = ggplot(subset(season2021), \n            aes(x = rank_scaled,\n                y = InitScore_20)) +\n  geom_jitter(aes(color = elevation),\n              alpha = 0.9,\n              height = 0,\n              width = 0.03) +\n  expand_limits(y=0) +\n  scale_x_continuous(limits = c(0, 1), oob = scales::squish) +\n  scale_y_continuous(limits = c(0, 3), breaks = ySeq2) +\n  scale_color_manual(values = colors,\n                     name = ""Elevation"",\n                     labels = c(""High"", ""Low"")) +\n  xlab(""Dominance rank"") +\n  ylab("""") +\n  theme_bw() +\n  ggtitle(""(b)"") +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(axis.text = element_text(size = 14))  +\n  theme(axis.title = element_text(size = 16)) +\n  theme(legend.justification=c(0,1),\n        legend.position=c(0.03, 0.97),\n        legend.background = element_rect(colour = 1, size = 0.2))\n\n# Initial 19-20\np3 = ggplot(subset(season1920), \n            aes(x = rank_scaled,\n                y = InitScoreTrue_20)) +\n  geom_jitter(aes(color = elevation),\n              alpha = 0.9,\n              height = 0,\n              width = 0.03) +\n  expand_limits(y=c(0)) +\n  scale_x_continuous(limits = c(0, 1), oob = scales::squish) +\n  scale_y_continuous(limits = c(0, 3), breaks = ySeq2) +\n  #  geom_smooth(method = ""lm"", se = F) +\n  scale_color_manual(values = colors,\n                     name = ""Elevation"",\n                     labels = c(""High"", ""Low"")) +\n  xlab(""Dominance rank"") +\n  ylab(""Mean number of location errors per trial"") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(axis.text = element_text(size = 14))  +\n  theme(axis.title = element_text(size = 16)) +\n  ggtitle(""(a)"") +\n  theme(legend.justification=c(0,1),\n        legend.position=c(0.03, 0.97),\n        legend.background = element_rect(colour = 1, size = 0.2))\n\npInitial = p3 + p1 + plot_layout(guides = ""collect"")\npInitial\n\n# ---- Reversal -----------------------\n\n# Reversal 20-21\np2 = ggplot(subset(season2021), \n            aes(x = rank_scaled,\n                y = RevScore_20)) +\n  geom_jitter(aes(color = elevation),\n              alpha = 0.9,\n              height = 0,\n              width = 0.03) +\n  expand_limits(y=0) +\n  scale_x_continuous(limits = c(0, 1), oob = scales::squish) +\n  scale_y_continuous(limits = c(0, 1.5), breaks = ySeq2) +\n  stat_smooth(method = ""lm"", se = T,\n              level = 0.95,\n              color = ""black"") +\n  scale_color_manual(values = colors,\n                     name = ""Elevation"",\n                     labels = c(""High"", ""Low"")) +\n  xlab(""Dominance rank"") +\n  ylab("""") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(axis.text = element_text(size = 14))  +\n  theme(axis.title = element_text(size = 16)) +\n  ggtitle(""(b)"") +\n  theme(legend.justification=c(0,1),\n        legend.position=c(0.03, 0.97),\n        legend.background = element_rect(colour = 1, size = 0.2))\n\n\n# Reversal 19-20\np4 = ggplot(subset(season1920), \n            aes(x = rank_scaled,\n                y = RevScore_20)) +\n  geom_jitter(aes(color = elevation),\n              alpha = 0.9,\n              height = 0,\n              width = 0.03) +\n  expand_limits(y=0) +\n  scale_x_continuous(limits = c(0, 1), oob = scales::squish) +\n  scale_y_continuous(limits = c(0, 1.5), breaks = ySeq2) +\n  scale_color_manual(values = colors,\n                     name = ""Elevation"",\n                     labels = c(""High"", ""Low"")) +\n  xlab(""Dominance rank"") +\n  ylab(""Mean number of location errors per trial"") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  theme(axis.text = element_text(size = 14))  +\n  theme(axis.title = element_text(size = 16)) +\n  ggtitle(""(a)"") +\n  theme(legend.justification=c(0,1),\n        legend.position=c(0.03, 0.97),\n        legend.background = element_rect(colour = 1, size = 0.2))\n\n\npReversal = p4 + p2 + plot_layout(guides = ""collect"")\npReversal\n\n# --- Learning curves ----------------------------------------------------\n\n\nelevation.labs = c(""High"", ""Low"")\nnames(elevation.labs) = c(""H"", ""L"")\n\n\ntask.labs = c(""Spatial learning and memory task"", ""Reversal spatial learning task"")\nnames(task.labs) = c(""MeanInitScore"", ""MeanRevScore"")\n\n#------------------------------------------\nseason2021_err_l = season2021_err %>%\n  pivot_longer(cols = c(MeanInitScore, MeanRevScore),\n               names_to = ""Task"", values_to = ""Score"")\n\nseason1920_err_l = season1920_err %>%\n  pivot_longer(cols = c(MeanInitScore, MeanRevScore),\n               names_to = ""Task"", values_to = ""Score"")\n\n# 20-21\np10 = ggplot(subset(season2021_err_l, !is.na(rank_Q)), aes(x = NumTrials,\n                                                           y = Score,\n                                                           group = rank_Q,\n                                                           color = rank_Q)) +\n  stat_summary(geom = ""point"",\n               position = position_dodge(width = 0.2),\n               na.rm = T,\n               size = 2) +\n  stat_summary(geom = ""errorbar"",\n               width = 0,\n               position = position_dodge(width = 0.2),\n               na.rm = T,\n               show.legend = F) +\n  stat_summary(geom = ""line"",\n               position = position_dodge(width = 0.2),\n               na.rm = T,\n               show.legend = F) +\n  scale_color_brewer(name = ""Rank\\nquartile"",\n                     palette = ""RdBu"") +\n  expand_limits(y=0) +\n  ylab(""Mean number of location errors per trial"") +\n  xlab(""Number of trials"") +\n  scale_x_discrete(labels = c(""3"", ""5"", ""10"", ""20"")) +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  facet_grid(rows = vars(elevation),\n             cols = vars(Task),\n             labeller = labeller(elevation = elevation.labs, Task = task.labs))\n\np10\n\n\n# 19-20\n\np11 = ggplot(subset(season1920_err_l, !is.na(rank_Q)), aes(x = NumTrials,\n                                                           y = Score,\n                                                           group = rank_Q,\n                                                           color = rank_Q)) +\n  stat_summary(geom = ""point"",\n               position = position_dodge(width = 0.2),\n               na.rm = T,\n               size = 2) +\n  stat_summary(geom = ""errorbar"",\n               width = 0,\n               position = position_dodge(width = 0.2),\n               na.rm = T,\n               show.legend = F) +\n  stat_summary(geom = ""line"",\n               position = position_dodge(width = 0.2),\n               na.rm = T,\n               show.legend = F) +\n  scale_color_brewer(name = ""Rank\\nquartile"",\n                     palette = ""RdBu"") +\n  expand_limits(y=0) +\n  ylab(""Mean number of location errors per trial"") +\n  xlab(""Number of trials"") +\n  scale_x_discrete(labels = c(""3"", ""5"", ""10"", ""20"")) +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n  facet_grid(rows = vars(elevation),\n             cols = vars(Task),\n             labeller = labeller(elevation = elevation.labs, Task = task.labs))\n\np11\n', '# Code for inferring displacement events from temporal data, from:\n# Evans JC, Devost I, Jones TB, Morand-Ferron J. 2018 Inferring dominance interactions from automatically recorded temporal data. Ethology 124, 188195. \n# (doi:10.1111/eth.12720)\n\n\ndetectdisplacement = function(arrivetime,departtime=NA,ids,displacetime,remaintime,totaltime=NA,difftime=NA,sites=NA,divisions=NA){\n\trequire(EloRating)\n\n\t##\n\t#arrivetime = time of arrival in seconds \n\t#departtime = time of departure in seconds -not required if difftime/totaltime is provided\n\t#\n\t#ids = vector of corresponding ids\n\t#displace time = difference between one individual leaving and another arriving to be considered a displacement\n\t#\n\t#totaltime = total time individual spends at antennae. Will be calculated automatically if not provided\n\t#difftime = difference in time between an individual departing and an individual arriving. Will be calculated automatically if not provided\n\t#\t\tNOTE: It is reccomended that this is calculated prior, and visits by the same individual within a short period are merged.\n\t#\n\t#sites = vector of sites - if not provided it will be assumed that all individuals  are present at the same site\n\t#divisons = vector of by which to divide the data. This can be less computationally intense than calculating a whole site at once. \n\t#\t\tFor example seperating data into days,sub-sites, or using gmmevents to divide into bursts of activity.\n\t#\n\t#Returns: A dominance matrix based, list of detected interactions.\n\t##\n\n\tif(((anyNA(arrivetime)&length(arrivetime)==1)|(anyNA(departtime)&length(departtime)==1))&(anyNA(difftime)&length(difftime)==1)){\n\t\tstop(""Unable to obtain time differences"")\n\t}\n\n\t#if totaltime is not available, calculate it here\n\n\tif((length(totaltime==1)&anyNA(totaltime))){\n\t\ttotaltime=departtime-arrivetime\n\t}\n\n\t#if sites are not provided, make one here\n\tif((length(sites==1)&anyNA(sites))){\n\t\tsites=rep(1,length(arrivetime))\n\t}\n\n\t#if divisions are not provided, make one here\n\tif((length(divisions==1)&anyNA(divisions))){\n\t\tdivisions=rep(1,length(arrivetime))\n\t}\n\n\t#if difftime is not available, calculate it here\n\tif((length(difftime)==1&anyNA(difftime))){\n\t\tdifftime=rep(NA,length(arrivetime))\n\t\tfor(site in unique(sites)){\n\t\t\tfor(d in unique(divisions[sites==site])){\n\t\t\t\tcurrind=which(sites==site&divisions==d)\n\t\t\t\tif(length(currind)>1){\n\t\t\t\t\tcurrind=currind[2:length(currind)]\n\t\t\t\t\tdifftime[currind]=sapply(currind,function (x) arrivetime[x]-departtime[x-1])\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\twinlossdate<-data.frame(matrix(NA,nrow=0,ncol=4))\n\tfor(sn in unique(sites)){\n\t\tfor(burst in unique(divisions[sites==sn])){\n\t\t\ttempselect=divisions==burst&sites==sn\n\t\t\tdifftime2=difftime[tempselect]\n\t\t\ttotaltime2=totaltime[tempselect]\n\t\t\tids2=ids[tempselect]\n\t\t\tarrivetime2=arrivetime[tempselect]\n\t\t\tpwin<-which(difftime2<displacetime&!is.na(difftime2)&totaltime2>=remaintime)\n\t\t\tif(length(pwin)>0){\n\t\t\t\tdyads<-lapply(pwin,function (i) (c(as.character(ids2[i]),as.character(ids2[i-1]))))\n\t\t\t\tdiffid=which(unlist(lapply(dyads,function (x) x[1] != x[2])))\n\t\t\t\tpwin=pwin[diffid]\n\t\t\t\tdyads=dyads[diffid]\n\t\t\t\tif(is.list(dyads)&length(dyads)>0){\t\t\n\t\t\t\t\twld<-data.frame(do.call(rbind,dyads),arrivetime2[pwin],sn)\n\t\t\t\t\twinlossdate<-rbind(winlossdate,wld)\n\t\t\t\t} else if (length(dyads)>0) {\n\t\t\t\t\twld<-data.frame(dyads,arrivetime[pwin],sn)\n\t\t\t\t\twinlossdate<-rbind(winlossdate,wld)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t\n\t}\n\n\tnames(winlossdate)<-c(""win"",""loss"",""date"",""Site"")\n\twinlossdate$dt<-winlossdate$date\n\twinlossdate$date<-as.Date(as.POSIXct(winlossdate$date,origin=""1970-01-01""))\n\tdommat<-data.frame(matrix(nrow=0,ncol=5))\n\tif(nrow(winlossdate)==0){\n\t\t#insufficient interactions!\n\t\treturn(list(""dominance matrix""=dommat,""interactions""=winlossdate))\n\t}\n\n\tfor (i in unique(winlossdate$Site)){\n\t\twld<-winlossdate[winlossdate$Site==i,]\n\t\tif(nrow(wld)>5){\n\t\t\tSEQ <- elo.seq(winner=wld$win, loser=wld$loss, Date=wld$date,progressbar=F)\n\t\t\tmat <- creatematrix(SEQ)\n\t\t\tdommat2<-DS(mat)\n\t\t\tdommat2$Site<-rep(i,nrow(dommat2))\n\t\t\tdommat2$ranked<-rank(dommat2[,2])\n\t\t\tdommat<-rbind(dommat,dommat2)\n\t\t}else{\n\t\t\tdommat2<-data.frame(ID=unique(ids[sites==i]))\n\t\t\tdommat2$DS<-rep(NA,nrow(dommat2))\n\t\t\tdommat2$normDS<-dommat2$DS\n\t\t\tdommat2$Site<-rep(i,nrow(dommat2))\n\t\t\tdommat2$ranked<-rank(dommat2[,2])\t\t\t\t\t\t\n\t\t\tdommat<-rbind(dommat,dommat2)\n\t\t\t}\n\t\t}\n\tdommat<-dommat[order(dommat$Site),]\n\treturn(list(""dominance_matrix""=dommat,""interactions""=winlossdate))\n}\n\ncomparedom<-function(dommat1,dommat2){\n\t#function to calculate the correlation between two dominance hierarchies\n\t#Where dommat1 and dommat2 are both dataframes with the following columns\n\t#ID - individual ID\n\t#Site - site individual ID is at.\n\t#DS - David\'s score, used for ranking individuals.\n\t#\n\t#Outputs - the comparable dominance matrix, phi (The correlation between the ranks of the hierarchies, controlling for comparable individuals),\n\t#\t\tRho value, p value, a per site phi, a per site rho, a per site p value.\n\top <- options(warn = (-1)) # suppress warnings \n\talldoom<-dommat1[dommat1$ID%in%dommat2$ID,]\n\talldoom<-alldoom[order(alldoom[,1],decreasing=T),]\n\talldoom2<-data.frame(matrix(nrow=0,ncol=3))\n\n\tfor (i in unique(alldoom$Site)){\n\t\talldom<-data.frame(matrix(NA,ncol=0,nrow=length(alldoom$ID[alldoom$Site==i])))\n\t\talldom$id<-alldoom$ID[alldoom$Site==i]\n\t\talldom$DS<-alldoom$DS[alldoom$Site==i]\n\t\talldom$ranked<-rank(alldoom$DS[alldoom$Site==i])\n\t\talldom$Site<-rep(i,nrow(alldom))\n\t\talldoom2<-rbind(alldoom2,alldom)\n\t}\n\n\tnames(alldoom2)[1]<-""ID""\n\n\tdommat3<-dommat2[dommat2$ID%in%alldoom2$ID,]\n\n\tdommat3$ranked<-unlist(sapply(unique(dommat3$Site),function (x) rank(dommat3$DS[dommat3$Site==x])))\n\n\talldoom3<-alldoom2[order(alldoom2$Site),]\n\n\tdoom4<-merge(alldoom3,dommat3,by=""ID"")\n\tdoom4<-doom4[order(doom4$Site.x),]\n\tpersitephi=sapply(unique(doom4$Site.x),function (x) as.numeric(cor.test(doom4$ranked.x[doom4$Site.x==x],doom4$ranked.y[doom4$Site.x==x],method=""spearman"")[[4]])*((nrow(doom4[doom4$Site.x==x,])/nrow(dommat1[dommat1$Site==x,]))))\n\tpersiterho=sapply(unique(doom4$Site.x),function (x) as.numeric(cor.test(doom4$ranked.x[doom4$Site.x==x],doom4$ranked.y[doom4$Site.x==x],method=""spearman"")[[4]]))\n\tpersitep=sapply(unique(doom4$Site.x),function (x) as.numeric(cor.test(doom4$ranked.x[doom4$Site.x==x],doom4$ranked.y[doom4$Site.x==x],method=""spearman"")[[3]]))\n\n\tcortest=cor.test(doom4$ranked.x,doom4$ranked.y,method=""spearman"")\n\trho=cortest[[4]]\n\tpvalue=cortest[[3]]\n\t\n\tr2z=as.numeric(fisher.r2z(persiterho))\n\tr2z[r2z==Inf]=2.6467\n\tr2z[r2z==-Inf]=-2.6467\n\ttpval=t.test(r2z)$p.value\n\tmeanz=mean(r2z)\n\tphi2=meanz*pmin((nrow(doom4)/nrow(dommat1)),1)\n\n\toptions(op)\n\t\n\n\n\treturn(list(dominance_matrix=doom4,phi=phi2,phi2pval=as.numeric(tpval),overall_rho=rho,scaled_rho=as.numeric(rho*pmin((nrow(doom4)/nrow(dommat1)),1)),rho_pval=pvalue,per_site_rho=persiterho,per_site_rho_scaled=persitephi,per_site_p=persitep))\n\n\n}\n\nfisher.r2z <- function(r) { 0.5 * (log(1+r) - log(1-r)) }\n\n']","Social dominance status is associated with differences in spatial cognitive flexibility in wild mountain chickadees Social dominance has long been used as a model to investigate social stress. However, many studies using such comparisons have been performed in captive environments. These environments may produce unnaturally high antagonistic interactions, exaggerating the stress of social subordination and any associated adverse consequences. One such adverse effect concerns impaired cognitive ability, often thought to be associated with social subordination. Here, we tested whether social dominance rank is associated with differences in spatial learning and memory and in reversal spatial learning (flexibility) abilities in wild food-caching mountain chickadees at different montane elevations. Higher dominance rank was associated with higher spatial cognitive flexibility in harsh environments at higher elevations, but not at lower, milder elevations. In contrast, there were no consistent differences in spatial learning and memory ability associated with dominance rank. Our results suggest that spatial learning and memory ability in specialized food-caching species is a stable trait resilient to social influences. Spatial cognitive flexibility, on the other hand, appears to be more sensitive to environmental influences including social dominance. These findings contradict those from laboratory studies and suggest that it is critical to investigate the biological consequences of social dominance under natural conditions.",2
Data from: Detecting and quantifying social transmission using network-based diffusion analysis,"1. Although social learning capabilities are taxonomically widespread, demonstrating that freely interacting animals (whether wild or captive) rely on social learning has proved remarkably challenging.2. Network-based diffusion analysis (NBDA) offers a means for detecting social learning using observational data on freely interacting groups. Its core assumption is that if a target behaviour is socially transmitted, then its spread should follow the connections in a social network that reflects social learning opportunities.3. Here, we provide a comprehensive guide for using NBDA. We first introduce its underlying mathematical framework and present the types of questions that NBDA can address. We then guide researchers through the process of: selecting an appropriate social network for their research question; determining which NBDA variant should be used; and incorporating other variables that may impact asocial and social learning. Finally, we discuss how to interpret an NBDA model's output and provide practical recommendations for model selection.4. Throughout, we highlight extensions to the basic NBDA framework, including incorporation of dynamic networks to capture changes in social relationships during a diffusion and using a multi-network NBDA to estimate information flow across multiple types of social relationship.5. Alongside this information, we provide worked examples and tutorials demonstrating how to perform analyses using the newly developed NBDA package written in the R programming language.",,"Data from: Detecting and quantifying social transmission using network-based diffusion analysis 1. Although social learning capabilities are taxonomically widespread, demonstrating that freely interacting animals (whether wild or captive) rely on social learning has proved remarkably challenging.2. Network-based diffusion analysis (NBDA) offers a means for detecting social learning using observational data on freely interacting groups. Its core assumption is that if a target behaviour is socially transmitted, then its spread should follow the connections in a social network that reflects social learning opportunities.3. Here, we provide a comprehensive guide for using NBDA. We first introduce its underlying mathematical framework and present the types of questions that NBDA can address. We then guide researchers through the process of: selecting an appropriate social network for their research question; determining which NBDA variant should be used; and incorporating other variables that may impact asocial and social learning. Finally, we discuss how to interpret an NBDA model's output and provide practical recommendations for model selection.4. Throughout, we highlight extensions to the basic NBDA framework, including incorporation of dynamic networks to capture changes in social relationships during a diffusion and using a multi-network NBDA to estimate information flow across multiple types of social relationship.5. Alongside this information, we provide worked examples and tutorials demonstrating how to perform analyses using the newly developed NBDA package written in the R programming language.",2
Social polyandry shapes sperm morphology,"Sexual selection is a major driver of trait variation, and the intensity of male competition for mating opportunities has been linked with sperm size across diverse taxa. Mating competition among females may also shape the evolution of sperm traits, but the interplay between female-female competition and male-male competition on sperm morphology is not well understood. We evaluated variation in sperm morphology in two species with socially polyandrous mating systems, in which females compete to mate with multiple males. Northern jacanas (Jacana spinosa) and wattled jacanas (J. jacana) vary in their degree of polyandry and sexual dimorphism, suggesting species differences in the intensity of sexual selection. We compared mean and variance in sperm head, midpiece, and tail length between species and breeding stages, because these measures have been associated with the intensity of sperm competition. We found that the species with greater polyandry, northern jacana, has sperm with longer midpieces and tails, as well as marginally lower intra-ejaculate variation in tail length. Intra-ejaculate variation was also significantly lower in copulating males than in incubating males, suggesting flexibility in sperm production as males cycle between breeding stages. Our results indicate that stronger female-female competition for mating opportunities may also shape more intense male-male competition by selecting for longer and less variable sperm traits. These findings extend frameworks developed in socially monogamous species to reveal that sperm competition may be an important evolutionary force layered atop female-female competition for mates.","['# How female-female competition affects male-male competition: \n# insights on post-copulatory sexual selection from socially polyandrous species.    \n\n# 11.23.2021\n\n# Sara E. Lipshutz, Samuel J. Torneo, Kimberly A. Rosvall\n\n# Set path\nsetwd()\n\n# Read in datasheet - you\'ll have to specify the correct path\nsperm <-read.csv(""jacana_sperm_measurements.csv"") \nsperm.avg <-read.csv(""sperm_measurements_averages.csv"")\n\n# subset dataset for each species\nspinosa.sperm.avg = subset(sperm.avg, Species == ""Northern"")\njacana.sperm.avg = subset(sperm.avg, Species == ""Wattled"")\n\n#Examine structure of the data\nstr(sperm)\nstr(sperm.avg)\n\n# Export figures to pptx using officer and rvg \nrequire (officer)\nrequire(rvg)\nrequire(ggpubr)\nrequire(ggplot2)\n\n# Compare species - rough overlook\nlibrary(ggplot2)\nggplot(sperm, aes(x=Tail, color = Species)) + geom_histogram()\n# spionsa have longer tail\nggplot(sperm, aes(x=Midpiece, color = Species)) + geom_histogram()\n# spionsa have longer midpiece\nggplot(sperm, aes(x=Head, color = Species)) + geom_histogram()\n# Head length seems like its the same between species\n\n\n# Check for outliers - decide whether to exclude any samples\nggplot(sperm, aes(x=Individual, y=Tail, fill=Breeding)) + geom_boxplot() # WAM7 looks like an outlier\nggplot(sperm, aes(x=Individual, y=Midpiece, fill=Breeding)) + geom_boxplot()\nggplot(sperm, aes(x=Individual, y=Head, fill=Breeding)) + geom_boxplot()\n\nlibrary(outliers)\ngrubbs.test(jacana.sperm$Tail, type = 10)\n#G = 2.94924, U = 0.87752, p-value = 0.08774\n#alternative hypothesis: lowest value 56.328 is an outlier\n\ngrubbs.test(spinosa.sperm$Tail, type = 10) \n#G = 2.89249, U = 0.92395, p-value = 0.182\n#alternative hypothesis: lowest value 65.37 is an outlier\n\n#### Decided not to exclude WAM7 - outlier test not significant\n\n\n# Validation of 10 sperm per individual\n\n# Dataset: 50-60 sperm samples from 1 individual of each species, \n# with measurements on total length, tail, midpiece, and head, along with \n# 10 sperm samples from those same individuals\n\nmethod <- read.csv(""sperm_sample_comparison.csv"")\n\n# Total length - methods are comparable\nggplot(method, aes(x=Method, y=Total.Length, fill=Species)) + geom_boxplot()\nggboxplot(method, x = ""Method"", y=""Total.Length"", color=""Species"", add = ""jitter"")\nmethod.total.length <- aov(Total.Length ~ Method * Species, data = method)\nanova(method.total.length)\n\n# Assessing technical repeatability of sperm within an individual\n\n# Repeatability using rptr\n#install.packages(""rptR"")\nlibrary(rptR)\n\nrepeatability <- read.csv(""Repeatability samples.csv"")\n\nrpt(Tail ~ (1 | Sample), grname = ""Sample"", data = repeatability, datatype = ""Gaussian"", nboot = 1000, npermut = 0)\n# R  = 0.851, SE = 0.07, CI = [0.669, 0.938], P  = 2.65e-07 [LRT], NA [Permutation]\nrpt(Midpiece ~ (1 | Sample), grname = ""Sample"", data = repeatability, datatype = ""Gaussian"", nboot = 1000, npermut = 0)\n# R  = 0.803, SE = 0.089, CI = [0.574, 0.918]. P  = 7.68e-07 [LRT]. NA [Permutation]\nrpt(Head ~ (1 | Sample), grname = ""Sample"", data = repeatability, datatype = ""Gaussian"", nboot = 1000, npermut = 0)\n# R  = 0.924, SE = 0.04, CI = [0.82, 0.968], P  = 1.12e-11 [LRT]. NA [Permutation]\n\n\n# MANOVA\n# Followed this R tutorial: https://www.datanovia.com/en/lessons/one-way-manova-in-r/\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(car)\nlibrary(broom)\n\nggboxplot(sperm.avg, x = ""Species"", y = c(""Tail"", ""Midpiece"",""Head""),  merge = TRUE, palette = ""jco"")\n\n# Confirm that we do not violate sample size assumption (the n in each cell > the number of outcome variables)\nsperm.avg %>%\n  group_by(Species) %>%\n  get_summary_stats(Tail, Midpiece, Head, type = ""mean_sd"")\n\nsperm.avg %>%\n  group_by(Breeding) %>%\n  get_summary_stats(Tail, Midpiece, Head, type = ""mean_sd"")\n\n#Check for outliers\nsperm.avg %>%\n  group_by(Species) %>%\n  identify_outliers(Tail)\n\n# Univariate normality assumption\nsperm.avg %>%\n  group_by(Species) %>%\n  shapiro_test(Tail, Midpiece, Head) %>%\n  arrange(variable)\n\n# QQ plot\nggqqplot(sperm.avg, ""Tail"", facet.by = ""Species"",\n         ylab = "" Tail"", ggtheme = theme_bw())\n\nggqqplot(sperm.avg, ""Midpiece"", facet.by = ""Species"",\n         ylab = "" Midpiece"", ggtheme = theme_bw())\n\nggqqplot(sperm.avg, ""Head"", facet.by = ""Species"",\n         ylab = "" Head"", ggtheme = theme_bw())\n\n# Linearity assumption\n# Create a scatterplot matrix by group\nlibrary(GGally)\nresults <- sperm.avg %>%\n  select(Tail, Midpiece, Head, Species) %>%\n  group_by(Species) %>%\n  doo(~ggpairs(.) + theme_bw(), result = ""plots"")\nresults\nresults$plots\n# Looks like there are several non linear relationships - we will lose power\n\n# Homogeneity of covariances\nbox_m(sperm.avg[, c(""Tail"", ""Midpiece"",""Head"")], sperm.avg$Species)\n# statistic p.value parameter method                                             \n# <dbl>   <dbl>     <dbl> <chr>                                              \n#   1      4.18   0.652         6 Box\'s M-test for Homogeneity of Covariance Matrices\n# Covariation is homogeneic!\n\n# Homogeneity of variance\nsperm.avg %>% \n  gather(key = ""variable"", value = ""value"", Tail, Midpiece, Head) %>%\n  group_by(variable) %>%\n  levene_test(value ~ Species)\n\n# variable   df1   df2 statistic     p\n# <chr>    <int> <int>     <dbl> <dbl>\n#   1 Head         1    16     0.147 0.707\n# 2 Midpiece     1    16     0.676 0.423\n# 3 Tail         1    16     0.118 0.736\n\n\n# MANOVA for species comparison\n\nmodel <- lm(cbind(Tail, Midpiece, Head) ~ Species, sperm.avg)\nManova(model, test.statistic = ""Pillai"") # recommended for unbalanced design\n\n# Group the data by variable\ngrouped.data <- sperm.avg %>%\n  gather(key = ""variable"", value = ""value"", Tail, Midpiece, Head) %>%\n  group_by(variable)\n# Type II MANOVA Tests: Pillai test statistic\n#         Df    test stat approx F num Df den Df   Pr(>F)    \n# Species  1    0.8373   24.017      3     14   8.78e-06 ***\n\ngrouped.data %>% anova_test(value ~ Species)\n# variable    Effect    DFn   DFd    F      p     `p<.05`   ges\n# 1 Head     Species     1    16  0.195   0.665      """"     0.012\n# 2 Midpiece Species     1    16  24.0   0.000162   ""*""     0.6  \n# 3 Tail     Species     1    16  48.1   0.00000336 ""*""     0.75 \n\n# Between species, there was a significant difference in Tail length (F(1, 16) = 48.1, p < 0.0001 ) \n# and midpiece length (F(1, 16) = 24.0, p = 0.00016 ), but not head length (F(1, 16) = 0.195, p = 0.665 )\n\n# Bonferroni multiple testing correction: divide 0.05 by # tests (3), so significance criteria is p < 0.01666\np <- c(0.665,0.000162,0.00000336)\np.adjust (p, method = ""bonferroni"")\n# 1.000e+00 4.860e-04 1.008e-05\n\n\n# MANOVA for breeding stage comparison\nmodel <- lm(cbind(Tail, Midpiece, Head) ~ Breeding, sperm.avg)\nManova(model, test.statistic = ""Pillai"") # recommended for unbalanced design\n#           Df test stat approx F num Df den Df Pr(>F)\n# Breeding  1   0.12001   0.6364      3     14 0.6039\n\n# Group the data by variable\ngrouped.data <- sperm.avg %>%\n  gather(key = ""variable"", value = ""value"", Tail, Midpiece, Head) %>%\n  group_by(variable)\n\ngrouped.data %>% anova_test(value ~ Breeding)\n# variable Effect     DFn   DFd     F     p `p<.05`   ges\n# 1 Head     Breeding     1    16 1.37  0.26  """"      0.079\n# 2 Midpiece Breeding     1    16 0.424 0.524 """"      0.026\n# 3 Tail     Breeding     1    16 0.033 0.859 """"      0.002\n\np <- c(0.26,0.524,0.859)\np.adjust (p, method = ""bonferroni"")\n#  0.78 1.00 1.00\n\n\n# Avg Tail length  - species and breeding stage comparisons\nggboxplot(sperm, x = ""Species"", y = ""Tail"", add = ""dotplot"", color = ""Individual"")\n\ntail <- ggboxplot(sperm.avg, x = ""Species"", y = ""Tail"", add = ""dotplot"")\ntail + theme(text=element_text(size=rel(2.2))) + theme(axis.title.x = element_text(size=16))\n\nshapiro.test(jacana.sperm.avg$Tail) # W = 0.89545, p-value = 0.3043\nt.test(sperm.avg$Tail ~ sperm.avg$Species) #t = 6.3913, df = 9.7457, p-value = 8.89e-05\n\nggboxplot(sperm.avg, x = ""Species"", y = ""Tail"", color = ""Breeding"", add = ""dotplot"")\n\nspecies.tail.length <- lm(Tail ~ Species + Breeding, data = sperm.avg)\nanova(species.tail.length)\n#           Df  Sum Sq Mean Sq F value    Pr(>F)    \n#Species    1 225.046 225.046 49.2224 4.168e-06 ***\n#Breeding   1   6.281   6.281  1.3739    0.2594    \n#Residuals 15  68.580   4.572 \n\n\n# Plots that go into Figure 2 - ppt version - Tail Length\ntail.length <- ggboxplot(sperm.avg, x = ""Species"", y = ""Tail"", add = ""dotplot"")\neditable_graph <- dml(ggobj = tail.length)\ndoc <- read_pptx()\ndoc <- add_slide(doc)\ndoc <- ph_with(x = doc, editable_graph,\n               location = ph_location_type(type = ""body"") )\nprint(doc, target = ""tail.length.pptx"")\n\n\n\n# Avg Midpiece Length\nggboxplot(sperm, x = ""Species"", y = ""Midpiece"", add = ""dotplot"", color = ""Individual"")\n\nmidpiece <- ggboxplot(sperm.avg, x = ""Species"", y = ""Midpiece"", add = ""dotplot"")\nmidpiece + theme(text=element_text(size=rel(2.2))) + theme(axis.title.x = element_text(size=16))\n\nshapiro.test(jacana.sperm.avg$Midpiece) # W = 0.91635, p-value = 0.4416\nt.test(sperm.avg$Midpiece ~ sperm.avg$Species) #t = 4.7361, df = 11.546, p-value = 0.0005371\n\nggboxplot(sperm.avg, x = ""Species"", y = ""Midpiece"", color = ""Breeding"", add = ""dotplot"")\n\nspecies.midpiece.length <- lm(Midpiece ~ Species + Breeding, data = sperm.avg)\nanova(species.midpiece.length)\n#         Df Sum Sq Mean Sq F value    Pr(>F)    \n#Species    1 5.2092  5.2092 26.6509 0.0001158 ***\n#Breeding   1 0.5456  0.5456  2.7914 0.1154960    \n#Residuals 15 2.9319  0.1955     \n\n# Plots that go into Figure 2 - ppt version - Midpiece Length\nmidpiece.length <- ggboxplot(sperm.avg, x = ""Species"", y = ""Midpiece"", add = ""dotplot"")\nmidpiece.length\neditable_graph <- dml(ggobj = midpiece.length)\ndoc <- read_pptx()\ndoc <- add_slide(doc)\ndoc <- ph_with(x = doc, editable_graph,\n               location = ph_location_type(type = ""body"") )\nprint(doc, target = ""midpiece.length.pptx"")\n\n\n\n# Head\nggboxplot(sperm, x = ""Species"", y = ""Head"", add = ""dotplot"", color = ""Individual"")\n\nhead <- ggboxplot(sperm.avg, x = ""Species"", y = ""Head"", add = ""dotplot"")\nhead + theme(text=element_text(size=rel(2.2))) + theme(axis.title.x = element_text(size=16))\n\nshapiro.test(jacana.sperm.avg$Head) # W = 0.96137, p-value = 0.8304\nt.test(sperm.avg$Head ~ sperm.avg$Species) # t = -0.42472, df = 11.303, p-value = 0.679\n\nggboxplot(sperm.avg, x = ""Species"", y = ""Head"", color = ""Breeding"", add = ""dotplot"")\n\nspecies.head.length <- lm(Head ~ Species + Breeding, data = sperm.avg)\nanova(species.head.length)\n#Df Sum Sq Mean Sq F value Pr(>F)\n#Species    1 0.0880 0.08804  0.2006 0.6607\n#Breeding   1 0.6353 0.63534  1.4474 0.2476\n#Residuals 15 6.5843 0.43895        \n\n# Plots that go into Figure 2 - ppt version - Head Length\nhead.length <- ggboxplot(sperm.avg, x = ""Species"", y = ""Head"", add = ""dotplot"")\nhead.length\neditable_graph <- dml(ggobj = head.length)\ndoc <- read_pptx()\ndoc <- add_slide(doc)\ndoc <- ph_with(x = doc, editable_graph,\n               location = ph_location_type(type = ""body"") )\nprint(doc, target = ""head.length.pptx"")\n\n\n# Do testes differ between the species?\nggboxplot(sperm.avg, x = ""Species"", y = ""Testes.Mass"", add = ""dotplot"", color = ""Breeding"")\n\n# ANCOVA\n# Adequate sample size. Rule of thumb: the n in each cell > the number of outcome variables.\nsperm.avg %>%\n  group_by(Species) %>%\n  get_summary_stats(Testes.Mass, type = ""mean_sd"")\n\nsperm.avg %>%\n  group_by(Breeding) %>%\n  get_summary_stats(Testes.Mass, type = ""mean_sd"")\n\n\n# Also need to test for homogeneity of variance\nlibrary(car)\nleveneTest(sperm.avg$Testes.Mass ~ sperm.avg$Species)\n# Levene\'s Test for Homogeneity of Variance (center = median)\n#       Df F value Pr(>F)\n# group  1   0.7793 0.3904\n#       16    \n# Variance is equal. \n\nancova_model_species <- aov(sperm.avg$Testes.Mass ~ sperm.avg$Species + sperm.avg$Somatic.Mass)\nAnova(ancova_model_species, type = ""III"")\n# Anova Table (Type III tests)\n# \n# Response: sperm.avg$Testes.Mass\n#                         Sum Sq Df F value Pr(>F)\n# (Intercept)               460  1  0.0146 0.9055\n# sperm.avg$Species        6270  1  0.1988 0.6621\n# sperm.avg$Somatic.Mass  10206  1  0.3236 0.5779\n# Residuals              473105 15               \n\nancova_model_breeding <- aov(sperm.avg$Testes.Mass ~ sperm.avg$Somatic.Mass + sperm.avg$Breeding)\nAnova(ancova_model_breeding, type = ""III"")\n# Anova Table (Type III tests)\n# \n# Response: sperm.avg$Testes.Mass\n# Sum Sq Df F value Pr(>F)\n# (Intercept)             15937  1  0.5410 0.4734\n# sperm.avg$Somatic.Mass    344  1  0.0117 0.9154\n# sperm.avg$Breeding      37524  1  1.2739 0.2768\n# Residuals              441850 15   \n\n\n\n# Testes volume\nggboxplot(sperm.avg, x = ""Species"", y = ""Avg.Testes.Vol"", add = ""dotplot"", color = ""Breeding"")\n\nspecies.testes.vol <- aov(Avg.Testes.Vol ~ Species * Breeding, data = sperm.avg,na.action= na.exclude)\nsummary(species.testes.vol)\n# Df   Sum Sq Mean Sq F value Pr(>F)\n# Species           1   464494  464494   0.401  0.541\n# Breeding          1   789882  789882   0.682  0.428\n# Species:Breeding  1  1733443 1733443   1.497  0.249\n# Residuals        10 11579282 1157928               \n# 4 observations deleted due to missingness\n\nspecies.testes.vol <- aov(Avg.Testes.Vol ~ Species + Breeding, data = sperm.avg,na.action= na.exclude)\nsummary(species.testes.vol)\n# Df   Sum Sq Mean Sq F value Pr(>F)\n# Species      1   464494  464494   0.384  0.548\n# Breeding     1   789882  789882   0.653  0.436\n# Residuals   11 13312725 1210248               \n# 4 observations deleted due to missingness \n\n\n\n\n# Coefficient of variation of each trait - variation across breeding season?\n# Intermale CV for both breeding stages and combined\n\nspinosa.sperm = subset(sperm, Species == ""Northern"")\njacana.sperm = subset(sperm, Species == ""Wattled"")\nspinosa.sperm.court = subset(sperm, Species == ""Northern"" & Breeding == ""Copulation"")\nspinosa.sperm.inc = subset(sperm, Species == ""Northern"" & Breeding == ""Incubation"")\njacana.sperm.court = subset(sperm, Species == ""Wattled"" & Breeding == ""Copulation"")\njacana.sperm.inc = subset(sperm, Species == ""Wattled"" & Breeding == ""Incubation"")\n\n# Intermale CV tail-\nsd(spinosa.sperm$Tail, na.rm=TRUE)/\n  mean(spinosa.sperm$Tail, na.rm=TRUE)*100 # spinosa CV =  4.459243\nsd(spinosa.sperm.court$Tail, na.rm=TRUE)/\n  mean(spinosa.sperm.court$Tail, na.rm=TRUE)*100 # spinosa court CV = 3.677851\nsd(spinosa.sperm.inc$Tail, na.rm=TRUE)/\n  mean(spinosa.sperm.inc$Tail, na.rm=TRUE)*100 # spinosa inc CV = 5.030138\n\nsd(jacana.sperm$Tail, na.rm=TRUE)/\n  mean(jacana.sperm$Tail, na.rm=TRUE)*100 # spinosa CV =  5.746834\nsd(jacana.sperm.court$Tail, na.rm=TRUE)/\n  mean(jacana.sperm.court$Tail, na.rm=TRUE)*100 # spinosa court CV =  4.439126\nsd(jacana.sperm.inc$Tail, na.rm=TRUE)/\n  mean(jacana.sperm.inc$Tail, na.rm=TRUE)*100 # spinosa inc CV = 6.43182\n\n# Intermale CV Midpiece\nsd(spinosa.sperm$Midpiece, na.rm=TRUE)/\n  mean(spinosa.sperm$Midpiece, na.rm=TRUE)*100 # spinosa CV =  8.875077\nsd(spinosa.sperm.court$Midpiece, na.rm=TRUE)/\n  mean(spinosa.sperm.court$Midpiece, na.rm=TRUE)*100 # spinosa court CV = 7.498641\nsd(spinosa.sperm.inc$Midpiece, na.rm=TRUE)/\n  mean(spinosa.sperm.inc$Midpiece, na.rm=TRUE)*100 # spinosa inc CV = 9.705709\n\nsd(jacana.sperm$Midpiece, na.rm=TRUE)/\n  mean(jacana.sperm$Midpiece, na.rm=TRUE)*100 # spinosa CV =   8.065452\nsd(jacana.sperm.court$Midpiece, na.rm=TRUE)/\n  mean(jacana.sperm.court$Midpiece, na.rm=TRUE)*100 # spinosa court CV =  6.628148\nsd(jacana.sperm.inc$Midpiece, na.rm=TRUE)/\n  mean(jacana.sperm.inc$Midpiece, na.rm=TRUE)*100 # spinosa inc CV = 9.632959\n\n# Intermale CV Head\nsd(spinosa.sperm$Head, na.rm=TRUE)/\n  mean(spinosa.sperm$Head, na.rm=TRUE)*100 # spinosa CV =  8.43109\nsd(spinosa.sperm.court$Head, na.rm=TRUE)/\n  mean(spinosa.sperm.court$Head, na.rm=TRUE)*100 # spinosa court CV = 8.785489\nsd(spinosa.sperm.inc$Head, na.rm=TRUE)/\n  mean(spinosa.sperm.inc$Head, na.rm=TRUE)*100 # spinosa inc CV = 7.77009\n\nsd(jacana.sperm$Head, na.rm=TRUE)/\n  mean(jacana.sperm$Head, na.rm=TRUE)*100 # spinosa CV =   9.335285\nsd(jacana.sperm.court$Head, na.rm=TRUE)/\n  mean(jacana.sperm.court$Head, na.rm=TRUE)*100 # spinosa court CV =  9.366753\nsd(jacana.sperm.inc$Head, na.rm=TRUE)/\n  mean(jacana.sperm.inc$Head, na.rm=TRUE)*100 # spinosa inc CV = 9.343497\n\n\n\n# CV equality - test for difference in intraspecific + CVs between northern and wattled\n\n#install.packages(""cvequality"")\nlibrary(cvequality)\nlibrary(ggbeeswarm)\nlibrary(ggplot2)\nlibrary(knitr)\n\nkable(head(sperm.avg), caption = ""Preview of first few rows of the sperm data"")\nspinosa.sperm = subset(sperm, Species == ""Northern"")\njacana.sperm = subset(sperm, Species == ""Wattled"")\n\nsperm.court = subset(sperm.avg, sperm.avg$Breeding == ""Copulating"")\nsperm.inc = subset(sperm.avg, sperm.avg$Breeding == ""Incubating"")\n\n# Supplementary Table 1\n# Copulation - species diffs\nggplot(sperm.court, aes(Species, Tail)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\nspecies_court_tail_length_cv_test_MSLR <- with(sperm.court, mslr_test(nr = 1e4,Tail, Species))\nspecies_court_tail_length_cv_test_MSLR # MSLRT = 0.4430512, p =  0.5056534\n\nggplot(sperm.court, aes(Species, Midpiece)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\nspecies_court_midpiece_length_cv_test_MSLR <- with(sperm.court, mslr_test(nr = 1e4,Midpiece, Species))\nspecies_court_midpiece_length_cv_test_MSLR # MSLRT = 0.02178721, p =  0.8826546\n\nggplot(sperm.court, aes(Species, Head)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\nspecies_court_head_length_cv_test_MSLR <- with(sperm.court, mslr_test(nr = 1e4,Head, Species))\nspecies_court_head_length_cv_test_MSLR # MSLRT = 0.2724925, p =  0.6016646\n\n# Incubation- species diffs\nggplot(sperm.inc, aes(Species, Tail)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\nspecies_inc_tail_length_cv_test_MSLR <- with(sperm.inc, mslr_test(nr = 1e4,Tail, Species))\nspecies_inc_tail_length_cv_test_MSLR # MSLRT = 0.7983578, p =  0.3715848\n\nggplot(sperm.inc, aes(Species, Midpiece)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\nspecies_inc_midpiece_length_cv_test_MSLR <- with(sperm.inc, mslr_test(nr = 1e4,Midpiece, Species))\nspecies_inc_midpiece_length_cv_test_MSLR # MSLRT = 0.4125012, p =  0.5207027\n\nggplot(sperm.inc, aes(Species, Head)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\nspecies_inc_head_length_cv_test_MSLR <- with(sperm.inc, mslr_test(nr = 1e4,Head, Species))\nspecies_inc_head_length_cv_test_MSLR # MSLRT = 1.44006, p =  0.2301296\n\n# Bonferroni multiple testing correction: \np <- c(0.5056534,0.8826546,0.6016646,0.3715848,0.5207027,0.2301296)\np.adjust (p, method = ""bonferroni"")\n\n\n# Supplementary Table 2\n# test of difference in CV in breeding stage, for each species\n# Northern Jacana\nggplot(spinosa.sperm, aes(Breeding, Tail)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\n#spinosa_breeding_tail_length_cv_test <-  with(spinosa.sperm, asymptotic_test(Tail,Breeding))\nspinosa_breeding_tail_length_cv_test_MSLRT <- with(spinosa.sperm, mslr_test(nr = 1e4,Tail, Breeding))\nspinosa_breeding_tail_length_cv_test_MSLRT # MSLRT = 5.273314, p =  0.02463542\n\nggplot(spinosa.sperm, aes(Breeding, Midpiece)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\n#breeding_midpiece_length_cv_test <-  with(spinosa.sperm, asymptotic_test(Midpiece,Breeding))\nbreeding_midpiece_length_cv_test_MSLRT <- with(spinosa.sperm, mslr_test(nr = 1e4,Midpiece, Breeding))\nbreeding_midpiece_length_cv_test_MSLRT # MSLRT = 3.444121, p =  0.06347718\n\nggplot(spinosa.sperm, aes(Breeding, Head)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\n#breeding_midpiece_length_cv_test <-  with(spinosa.sperm, asymptotic_test(Head,Breeding))\nbreeding_head_length_cv_test_MSLRT <- with(spinosa.sperm, mslr_test(nr = 1e4,Head, Breeding))\nbreeding_head_length_cv_test_MSLRT # MSLRT = 0.7968582, p = 0.3720344\n\n# Wattled Jacana\nggplot(jacana.sperm, aes(Breeding, Tail)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\n#breeding_tail_length_cv_test <-  with(jacana.sperm, asymptotic_test(Tail,Breeding))\nbreeding_tail_length_cv_test_MSLRT <- with(jacana.sperm, mslr_test(nr = 1e4,Tail, Breeding))\nbreeding_tail_length_cv_test_MSLRT # MSLRT = 4.519644, p = 0.03350781\n\nggplot(jacana.sperm, aes(Breeding, Midpiece)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\n#breeding_midpiece_length_cv_test <-  with(jacana.sperm, asymptotic_test(Midpiece,Breeding))\nbreeding_midpiece_length_cv_test_MSLRT <- with(jacana.sperm, mslr_test(nr = 1e4,Midpiece, Breeding))\nbreeding_midpiece_length_cv_test_MSLRT # MSLRT = 4.572663, p = 0.03248604\n\nggplot(jacana.sperm, aes(Breeding, Head)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\n#breeding_midpiece_length_cv_test <-  with(jacana.sperm, asymptotic_test(Midpiece,Breeding))\nbreeding_head_length_cv_test_MSLRT <- with(jacana.sperm, mslr_test(nr = 1e4,Head, Breeding))\nbreeding_head_length_cv_test_MSLRT # MSLRT = 0.0009138081, p =  0.9758842\n\n# Bonferroni multiple testing correction: \np <- c(0.02463542,0.06347718,0.3720344,0.03350781,0.03248604,0.9758842)\np.adjust (p, method = ""bonferroni"")\n# 0.1478125 0.3808631 1.0000000 0.2010469 0.1949162 1.0000000\n\n# Intra-ejaculate Coefficient of Variation\n\n# Tail Intra-ejaculate CV \nggboxplot(sperm.avg, x = ""Species"", y = ""CV.Tail"", add = ""dotplot"", color = ""Breeding"")\n\nCV.Tail <- ggboxplot(sperm.avg, x = ""Species"", y = ""CV.Tail"", add = ""dotplot"", color = ""Breeding"", ylim = c(0,15))\nCV.Tail\neditable_graph <- dml(ggobj = CV.Tail)\ndoc <- read_pptx()\ndoc <- add_slide(doc)\ndoc <- ph_with(x = doc, editable_graph,\n               location = ph_location_type(type = ""body"") )\nprint(doc, target = ""CV.Tail.pptx"")\n# CV for tail length changes with breeding stage, marginally different between species.\n# Tail length CV is lower during copulation than incubation\n\nspecies.CV.tail <- aov(CV.Tail ~ Species * Breeding, data = sperm.avg)\nsummary(species.CV.tail)\n# Df Sum Sq Mean Sq F value  Pr(>F)   \n# Species           1  4.338   4.338   4.294 0.05719 . \n# Breeding          1 12.131  12.131  12.010 0.00379 **\n# Species:Breeding  1  0.565   0.565   0.559 0.46693   \n# Residuals        14 14.141   1.010 \n\n# Interaction not significant so we deleted\n\nspecies.CV.tail <- aov(CV.Tail ~ Species + Breeding, data = sperm.avg)\nsummary(species.CV.tail)\n# Df Sum Sq Mean Sq F value  Pr(>F)   \n# Species      1  4.338   4.338   4.424 0.05272 . \n# Breeding     1 12.131  12.131  12.373 0.00311 **\n# Residuals   15 14.706   0.980    \n\n\n# Midpiece Intra-ejaculate CV \nggboxplot(sperm.avg, x = ""Species"", y = ""CV.Midpiece"", add = ""dotplot"", color = ""Breeding"")\n\nspecies.CV.midpiece <- aov(CV.Midpiece ~ Species * Breeding, data = sperm.avg)\nsummary(species.CV.midpiece)\n# Df Sum Sq Mean Sq F value Pr(>F)\n# Species           1  10.83  10.826   1.701  0.213\n# Breeding          1  11.96  11.957   1.879  0.192\n# Species:Breeding  1   0.04   0.039   0.006  0.939\n# Residuals        14  89.08   6.363 \n\nspecies.CV.midpiece <- aov(CV.Midpiece ~ Species + Breeding, data = sperm.avg)\nsummary(species.CV.midpiece)\n#          Df Sum Sq Mean Sq F value Pr(>F)\n# Species      1  10.83  10.826   1.822  0.197\n# Breeding     1  11.96  11.957   2.012  0.176\n# Residuals   15  89.12   5.941  \n\nCV.midpiece <- ggboxplot(sperm.avg, x = ""Species"", y = ""CV.Midpiece"", add = ""dotplot"", color = ""Breeding"", ylim = c(0,15))\nCV.midpiece\neditable_graph <- dml(ggobj = CV.midpiece)\ndoc <- read_pptx()\ndoc <- add_slide(doc)\ndoc <- ph_with(x = doc, editable_graph,\n               location = ph_location_type(type = ""body"") )\nprint(doc, target = ""CV.midpiece.pptx"")\n# CV for midpiece length does not significantly differ w/ breeding stage or species\n\n\n# Head Intra-ejaculate CV \nspecies.CV.head <- aov(CV.Head ~ Species * Breeding, data = sperm.avg)\nsummary(species.CV.head)\n# Df Sum Sq Mean Sq F value Pr(>F)\n# Species           1   2.03   2.027   0.502  0.490\n# Breeding          1   0.91   0.913   0.226  0.642\n# Species:Breeding  1   9.12   9.125   2.261  0.155\n# Residuals        14  56.50   4.036\n\nspecies.CV.head <- aov(CV.Head ~ Species + Breeding, data = sperm.avg)\nsummary(species.CV.head)\n#           Df Sum Sq Mean Sq F value Pr(>F)\n# Species      1   2.03   2.027   0.463  0.506\n# Breeding     1   0.91   0.913   0.209  0.654\n# Residuals   15  65.62   4.375  \n\nggboxplot(sperm.avg, x = ""Species"", y = ""CV.Head"", add = ""dotplot"", color = ""Breeding"",ylim=c(0,14))\nCV.Head <- ggboxplot(sperm.avg, x = ""Species"", y = ""CV.Head"", add = ""dotplot"", color = ""Breeding"",ylim=c(0,15))\neditable_graph <- dml(ggobj = CV.Head)\ndoc <- read_pptx()\ndoc <- add_slide(doc)\ndoc <- ph_with(x = doc, editable_graph,\n               location = ph_location_type(type = ""body"") )\nprint(doc, target = ""CV.Head.pptx"")\n# CV for head length does not significantly differ w/ breeding stage or species\nCV.Head\n\n\n']","Social polyandry shapes sperm morphology Sexual selection is a major driver of trait variation, and the intensity of male competition for mating opportunities has been linked with sperm size across diverse taxa. Mating competition among females may also shape the evolution of sperm traits, but the interplay between female-female competition and male-male competition on sperm morphology is not well understood. We evaluated variation in sperm morphology in two species with socially polyandrous mating systems, in which females compete to mate with multiple males. Northern jacanas (Jacana spinosa) and wattled jacanas (J. jacana) vary in their degree of polyandry and sexual dimorphism, suggesting species differences in the intensity of sexual selection. We compared mean and variance in sperm head, midpiece, and tail length between species and breeding stages, because these measures have been associated with the intensity of sperm competition. We found that the species with greater polyandry, northern jacana, has sperm with longer midpieces and tails, as well as marginally lower intra-ejaculate variation in tail length. Intra-ejaculate variation was also significantly lower in copulating males than in incubating males, suggesting flexibility in sperm production as males cycle between breeding stages. Our results indicate that stronger female-female competition for mating opportunities may also shape more intense male-male competition by selecting for longer and less variable sperm traits. These findings extend frameworks developed in socially monogamous species to reveal that sperm competition may be an important evolutionary force layered atop female-female competition for mates.",2
Social groups with diverse personalities mitigate physiological stress in a songbird,"Social groups often consist of diverse phenotypes, including personality types, and this diversity is known to affect the functioning of the group as a whole. Social selection theory proposes that group composition (i.e. social environment) also influences the performance of individual group members. However, the effect of group behavioural composition on group members remains largely unexplored, and it is still contentious whether individuals benefit more in a social environment with homogeneous or diverse behavioural composition. We experimentally formed groups of house sparrows Passer domesticus with high and low diversity of personality (exploratory behaviour), and found that their physiological state (body condition, physiological stress and oxidative damage) improved with increasing group-level diversity of personality. These findings demonstrate that group personality composition affects the condition of group members and individuals benefit from social heterosis (i.e. associating with a diverse set of behavioural types). This aspect of social life can play a key role in affiliation rules of social animals and might explain the evolutionary coexistence of different personalities in nature.",,"Social groups with diverse personalities mitigate physiological stress in a songbird Social groups often consist of diverse phenotypes, including personality types, and this diversity is known to affect the functioning of the group as a whole. Social selection theory proposes that group composition (i.e. social environment) also influences the performance of individual group members. However, the effect of group behavioural composition on group members remains largely unexplored, and it is still contentious whether individuals benefit more in a social environment with homogeneous or diverse behavioural composition. We experimentally formed groups of house sparrows Passer domesticus with high and low diversity of personality (exploratory behaviour), and found that their physiological state (body condition, physiological stress and oxidative damage) improved with increasing group-level diversity of personality. These findings demonstrate that group personality composition affects the condition of group members and individuals benefit from social heterosis (i.e. associating with a diverse set of behavioural types). This aspect of social life can play a key role in affiliation rules of social animals and might explain the evolutionary coexistence of different personalities in nature.",2
Data from: Wild acorn woodpeckers recognize associations between individuals in other groups,"According to the social intelligence hypothesis, understanding the cognitive demands of the social environment is key to understanding the evolution of intelligence. Many important socio-cognitive abilities, however, have primarily been studied in a narrow subset of the social environmentwithin-group social interactionsdespite the fact that between-group social interactions often have a substantial effect on fitness. In particular, triadic awareness (knowledge about the relationships and associations between others) is critical for navigating many types of complex social interactions, yet no existing study has investigated whether wild animals can track associations between members of other social groups. We investigated inter-group triadic awareness in wild acorn woodpeckers (Melanerpes formicivorus), a socially complex group-living bird. We presented woodpeckers with socially incongruous playbacks that simulated two outsiders from different groups calling together, and socially congruous playbacks that simulated two outsiders from the same group calling together. Subjects responded more quickly to the incongruous playbacks, suggesting that they were aware that the callers belonged to two different groups. This study provides the first demonstration that animals can recognize associations between members of other groups under natural circumstances, and highlights the importance of considering how inter-group social selection pressures may influence the evolution of cognition.","['setwd(""/Users/Mickey/Documents/Acorn Woodpecker Research/TPA_Manuscript/PRSB/Revision_1/"")\ndf1 <- read.csv(""20180405_2016TriadicAwareness_Data_for_pub.csv"")\n\n#loading necessary packages\nlibrary(survival)\nlibrary(coxme)\nlibrary(MASS)\nlibrary(corrplot)\nlibrary(ggplot2)\nlibrary(survminer)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(emmeans)\nlibrary(CircStats)\n\n\n########### SETTING FACTORS AS FACTORS ##############\ndf1$Treatment <- as.factor(df1$Treatment)\ndf1$Female <- as.factor(df1$Female)\ndf1$Stimulus <- as.factor(df1$Stimulus)\n\n############# CREATING HELMERT-CODED VERSIONS OF COVARIATES #############\n#Helmert coding = take each value, subtract the mid-point of the data, then divide\n#by 1/2 of the range\n#This is done so all the covariates will be on the same scale, since when I tried\n#running some of the models with the original covariates I got a warning message\n#saying that they were on very different scales\n\nhelmert <- function(x) {\n  midpoint <- (max(x)+min(x))/2\n  range <- max(x)-min(x)\n  (x-midpoint)/(0.5*range)\n}\n\ndf1$ACSD.helm <- helmert(df1$Avg.Caller.Subj.Dist)\ndf1$Overlap.helm <- helmert(df1$PropOverlap)\ndf1$Lag.helm <- helmert(df1$StimLagTime)\ndf1$sdA.helm <- helmert(df1$sdA)\ndf1$Tot_Dur.helm <- helmert(df1$Tot_Dur)\n\n############### CORRELATION MATRIX ####################\n\n#Create correlation matrix of response variables\n\n#Create a df of the relevant response variables\nresponse <- cbind(df1$F.Time.First.Flight,\n                       df1$F.Time.First.Positive.Flight,\n                       df1$F.Time.React,\n                       df1$F.Time.Closest.Approach,\n                       df1$F.Closest.Distance,\n                       df1$F.First.Flight.Direction,\n                       df1$G.Num.Birds.Rally, \n                       df1$G.Num.Birds.Approaching, \n                  df1$G.Time.First.Call,\n                       df1$G.WK.Diff,\n                       df1$O.Time.Leave.Tree,\n                       df1$O.Time.Closest.Approach,\n                       df1$O.Closest.Distance)\ncolnames(response) <- c(""F.Lat.Flight"", ""F.Lat.Pos.Fl"", ""F.Lat.React"", ""F.Lat.Closest"", \n                             ""F.Dist"", ""F.Direction"",\n                             ""G.Rally"", ""G.Num.App"", ""G.Lat.Call"", ""G.WK.Rate"",\n                             ""O.Lat.Leave.Tr"", ""O.Lat.Closest"", ""O.Dist"")\n\n#create the correlation matrix, deleting NA values on case by case basis\nCorMat <- cor(response, use=""complete.obs"")\ncorrplot(CorMat, method=""number"", tl.pos=""lt"", tl.srt=90, number.cex=0.6)\n\n\n############## SURVIVAL ANALYSES (COX REG) ##############\n\n#Cox regression for F.Time.First.Positive.Flight\nF.LPF.obj <- Surv(time=df1$F.Time.First.Positive.Flight, \n                  event=df1$F.Positive.Flight.Censor)\nF.LPF.coxme1 <- coxme(F.LPF.obj ~ Treatment + (1|Female) + (1|Stimulus), data=df1)\nsummary(F.LPF.coxme1)\nanova(F.LPF.coxme1)\n\n#Now with all covariates\nF.LPF.coxme2 <- coxme(F.LPF.obj ~ Treatment + (1|Female) + (1|Stimulus) + \n                        ACSD.helm + Overlap.helm + Lag.helm + sdA.helm + \n                        Tot_Dur.helm, data=df1)\nsummary(F.LPF.coxme2)\nanova(F.LPF.coxme2) #Treatment still significant. Only sig covar is Tot_Dur.helm\n\n\n#Cox regression for G.Time.First.Call\nG.LCall.obj <- Surv(time=df1$G.Time.First.Call, \n                    event=df1$G.First.Call.Censor)\nG.LCall.coxme1 <- coxme(G.LCall.obj ~ Treatment + \n                          (1|Female) + (1|Stimulus), data=df1)\nsummary(G.LCall.coxme1)\nanova(G.LCall.coxme1)\n\n#Now with all covariates\nG.LCall.coxme2 <- coxme(G.LCall.obj ~ Treatment + \n                         (1|Female) + (1|Stimulus) + \n                         ACSD.helm + Overlap.helm + Lag.helm + sdA.helm + \n                         Tot_Dur.helm, data=df1)\nsummary(G.LCall.coxme2)\nanova(G.LCall.coxme2) #Treatment p=0.572. Overlap.helm p=0.03784\n\n############## Linear Mixed Model ##################\n\n#LMM for O.Closest.Distance\n#rank-transforming response variable\n##tied ranks are randomly assigned an order relative to each other\n### so there are no ties\nLMM.O.Dist1 <- lmer(rank(O.Closest.Distance, na.last=FALSE, \n                         ties.method=""random"") ~ \n                      Treatment + (1|Female) + \n                      (1|Stimulus), data=df1)\nsummary(LMM.O.Dist1)\nanova(LMM.O.Dist1)\n\n#Checking assumptions for LMM.O.Dist1\nhist(resid(LMM.O.Dist1))\nplot(predict(LMM.O.Dist1), resid(LMM.O.Dist1))\n\n#Now with all covariates\nLMM.O.Dist2 <- lmer(rank(O.Closest.Distance, na.last=FALSE, \n                        ties.method=""random"") ~ \n                     Treatment + (1|Female) + \n                     (1|Stimulus) + \n                     ACSD.helm + Overlap.helm + Lag.helm + sdA.helm + \n                     Tot_Dur.helm, data=df1)\nsummary(LMM.O.Dist2)\nanova(LMM.O.Dist2) #Can\'t compute p-value in lmerTest\n\n\n########LMM for WKDiff\n\n#Use linear mixed model with response variable sqrt transformed\nLMM.WKDiff1 <- lmer(sqrt(G.WK.Diff) ~ Treatment + (1|Female) + \n                      (1|Stimulus), data=df1)\n\nsummary(LMM.WKDiff1)\nanova(LMM.WKDiff1)\n\n#Checking model assumptions for LMM.WKDiff\nhist(resid(LMM.WKDiff))  #checking to see if residuals are normally distrib\nplot(predict(LMM.WKDiff), resid(LMM.WKDiff)) #plotting resid vs. predicted values\n\n#Now with all covariates\nLMM.WKDiff2 <- lmer(sqrt(G.WK.Diff) ~ Treatment + (1|Female) + \n                       (1|Stimulus) + \n                     ACSD.helm + Overlap.helm + Lag.helm + sdA.helm + \n                     Tot_Dur.helm, data=df1)\n\nsummary(LMM.WKDiff2)\nanova(LMM.WKDiff2) #nothing sig\n\n\n############# GLMMs ################\n\n#Binomial regression for G.Num.Birds.Approaching\nBinom.NumApp1 <- glmer(cbind(G.Num.Birds.Approaching, \n                             G.Group.Size-G.Num.Birds.Approaching) ~ \n                         Treatment + \n                         (1|Female) + \n                         (1|Stimulus), \n                       family=binomial, data=df1)\n\nBinom.NumApp1.reduc <- glmer(cbind(G.Num.Birds.Approaching, \n                                   G.Group.Size-G.Num.Birds.Approaching) ~\n                               (1|Female) + \n                               (1|Stimulus), \n                             family=binomial, data=df1)\n\nsummary(Binom.NumApp1)\n\nanova(Binom.NumApp1, Binom.NumApp1.reduc)\n\nemmeans(Binom.NumApp1, ~Treatment, type=""response"")\n\n#Checking assumptions for Binom.NumApp1\ntable(df1$G.Num.Birds.Approaching > 0, df1$Treatment)\n\n\n#Now with all covariates\nBinom.NumApp2 <- glmer(cbind(G.Num.Birds.Approaching, \n                            G.Group.Size-G.Num.Birds.Approaching) ~ \n                        Treatment + \n                       (1|Female) + \n                       (1|Stimulus) + \n                        ACSD.helm + Overlap.helm + Lag.helm + sdA.helm + \n                        Tot_Dur.helm, \n                     family=binomial, data=df1)\n\nBinom.NumApp2.reduc <- glmer(cbind(G.Num.Birds.Approaching, \n                            G.Group.Size-G.Num.Birds.Approaching) ~\n                        (1|Female) + \n                        (1|Stimulus) + \n                          ACSD.helm + Overlap.helm + Lag.helm + sdA.helm + \n                          Tot_Dur.helm, \n                      family=binomial, data=df1)\n\nsummary(Binom.NumApp2)\n\nanova(Binom.NumApp2, Binom.NumApp2.reduc)\n\nemmeans(Binom.NumApp2, ~Treatment, type=""response"")\n\n##################\n# Calculating the p-values for the covariates for G.Num.Birds.Approaching\n\n#ACSD.helm\nNBA.ACSD <- glmer(cbind(G.Num.Birds.Approaching, \n                            G.Group.Size-G.Num.Birds.Approaching) ~ \n                        Treatment + \n                        (1|Female) + \n                        (1|Stimulus) + \n                        ACSD.helm + Overlap.helm + Lag.helm + sdA.helm + \n                        Tot_Dur.helm, \n                      family=binomial, data=df1)\n\nNBA.ACSD.reduc <- glmer(cbind(G.Num.Birds.Approaching, \n                              G.Group.Size-G.Num.Birds.Approaching) ~ \n                          Treatment + \n                          (1|Female) + \n                          (1|Stimulus) +\n                          Overlap.helm + Lag.helm + sdA.helm + \n                          Tot_Dur.helm, \n                        family=binomial, data=df1)\n\nanova(NBA.ACSD, NBA.ACSD.reduc) #p=0.944\n\n#Overlap.helm\nNBA.overlap <- glmer(cbind(G.Num.Birds.Approaching, \n                        G.Group.Size-G.Num.Birds.Approaching) ~ \n                    Treatment + \n                    (1|Female) + \n                    (1|Stimulus) + \n                    ACSD.helm + Overlap.helm + Lag.helm + sdA.helm + \n                    Tot_Dur.helm, \n                  family=binomial, data=df1)\n\nNBA.overlap.reduc <- glmer(cbind(G.Num.Birds.Approaching, \n                           G.Group.Size-G.Num.Birds.Approaching) ~ \n                       Treatment + \n                       (1|Female) + \n                       (1|Stimulus) + \n                       ACSD.helm + Lag.helm + sdA.helm + \n                       Tot_Dur.helm, \n                     family=binomial, data=df1)\n\nanova(NBA.overlap, NBA.overlap.reduc) #p=0.4997\n\n#Lag.helm\nNBA.Lag <- glmer(cbind(G.Num.Birds.Approaching, \n                           G.Group.Size-G.Num.Birds.Approaching) ~ \n                       Treatment + \n                       (1|Female) + \n                       (1|Stimulus) + \n                       ACSD.helm + Overlap.helm + Lag.helm + sdA.helm + \n                       Tot_Dur.helm, \n                     family=binomial, data=df1)\n\nNBA.Lag.reduc <- glmer(cbind(G.Num.Birds.Approaching, \n                       G.Group.Size-G.Num.Birds.Approaching) ~ \n                   Treatment + \n                   (1|Female) + \n                   (1|Stimulus) + \n                   ACSD.helm + Overlap.helm + sdA.helm + \n                   Tot_Dur.helm, \n                 family=binomial, data=df1)\n\nanova(NBA.Lag, NBA.Lag.reduc) #p=0.00114\n\n#sdA.helm\nNBA.sdA <- glmer(cbind(G.Num.Birds.Approaching, \n                       G.Group.Size-G.Num.Birds.Approaching) ~ \n                   Treatment + \n                   (1|Female) + \n                   (1|Stimulus) + \n                   ACSD.helm + Overlap.helm + Lag.helm + sdA.helm + \n                   Tot_Dur.helm, \n                 family=binomial, data=df1)\n\nNBA.sdA.reduc <- glmer(cbind(G.Num.Birds.Approaching, \n                       G.Group.Size-G.Num.Birds.Approaching) ~ \n                   Treatment + \n                   (1|Female) + \n                   (1|Stimulus) + \n                   ACSD.helm + Overlap.helm + Lag.helm + \n                   Tot_Dur.helm, \n                 family=binomial, data=df1)\n\nanova(NBA.sdA, NBA.sdA.reduc) #p=0.3741\n\n#Tot_Dur.helm\nNBA.Tot_Dur <- glmer(cbind(G.Num.Birds.Approaching, \n                           G.Group.Size-G.Num.Birds.Approaching) ~ \n                       Treatment + \n                       (1|Female) + \n                       (1|Stimulus) + \n                       ACSD.helm + Overlap.helm + Lag.helm + sdA.helm + \n                       Tot_Dur.helm, \n                     family=binomial, data=df1)\n\nNBA.Tot_Dur.reduc <- glmer(cbind(G.Num.Birds.Approaching, \n                           G.Group.Size-G.Num.Birds.Approaching) ~ \n                       Treatment + \n                       (1|Female) + \n                       (1|Stimulus) + \n                       ACSD.helm + Overlap.helm + Lag.helm + sdA.helm, \n                     family=binomial, data=df1)\n\nanova(NBA.Tot_Dur, NBA.Tot_Dur.reduc) #p=0.0083\n\n###### Testing to see if mean angular moment differed b/t T and C stimuli ########\n########## angular moment = metric of call synchrony\nx <- df1[df1$Treatment==""T"",]\ny <- df1[df1$Treatment==""C"",]\n\n#in package ""CircStats""\nwatson.two(x$CircMean, y$CircMean, plot=TRUE) #p>0.10\n\n\n############ Code for creating survival curves for Figure 1 ###########\n\n#Create survival fit object, which has survival probability and 95CIs\nLPFfit <- survfit(F.LPF.obj ~ Treatment, data=df1)\n\n#### Plot with survminer\nsurvcurve <- ggsurvplot(LPFfit, fun=""event"", conf.int=T, \n                        font.x=c(13),\n                        font.y=c(13),\n                        legend=""none"",\n                        legend.title="""",\n                        legend.labs=c(""Control"", ""Test""),\n                        linetype=c(""twodash"",""solid""))\nsurvcurve$plot <- survcurve$plot + \n  scale_x_continuous(breaks=c(0,30,60,90,120,150,180)) + \n  xlab(""Time Since Start of Playback (s)"") + \n  ylab(""Cumulative Probability of a Positive Flight"") + \n  ggplot2::annotate(""text"", x=188, y=0.935, label=""Test"", size=4.5) + \n  ggplot2::annotate(""text"", x=192, y=0.60, label=""Control"", size=4.5)\nsurvcurve\n\n\n']","Data from: Wild acorn woodpeckers recognize associations between individuals in other groups According to the social intelligence hypothesis, understanding the cognitive demands of the social environment is key to understanding the evolution of intelligence. Many important socio-cognitive abilities, however, have primarily been studied in a narrow subset of the social environmentwithin-group social interactionsdespite the fact that between-group social interactions often have a substantial effect on fitness. In particular, triadic awareness (knowledge about the relationships and associations between others) is critical for navigating many types of complex social interactions, yet no existing study has investigated whether wild animals can track associations between members of other social groups. We investigated inter-group triadic awareness in wild acorn woodpeckers (Melanerpes formicivorus), a socially complex group-living bird. We presented woodpeckers with socially incongruous playbacks that simulated two outsiders from different groups calling together, and socially congruous playbacks that simulated two outsiders from the same group calling together. Subjects responded more quickly to the incongruous playbacks, suggesting that they were aware that the callers belonged to two different groups. This study provides the first demonstration that animals can recognize associations between members of other groups under natural circumstances, and highlights the importance of considering how inter-group social selection pressures may influence the evolution of cognition.",2
Data from: Infection-induced behavioural changes reduce connectivity and the potential for disease spread in wild mice contact networks,"Infection may modify the behaviour of the host and of its conspecifics in a group, potentially altering social connectivity. Because many infectious diseases are transmitted through social contact, social connectivity changes can impact transmission dynamics. Previous approaches to understanding disease transmission dynamics in wild populations were limited in their ability to disentangle different factors that determine the outcome of disease outbreaks. Here we ask how social connectivity is affected by infection and how this relationship impacts disease transmission dynamics. We experimentally manipulated disease status of wild house mice using an immune challenge and monitored social interactions within this free-living population before and after manipulation using automated tracking. The immune-challenged animals showed reduced connectivity to their social groups, which happened as a function of their own behaviour, rather than through conspecific avoidance. We incorporated these disease-induced changes of social connectivity among individuals into models of disease outbreaks over the empirically-derived networks. The models revealed that changes in host behaviour frequently resulted in the disease being contained to very few animals, as opposed to becoming widespread. Our results highlight the importance of considering the role that behavioural alterations during infection can have on social dynamics when evaluating the potential for disease outbreaks.","['### A simple model for disease spread with isolation probability after infection\r\n# net: the square matrix indicating the amount of contact between two network members\r\n# time50: the amount fo contact that results in a 50% contagion probability\r\n# roundsBeforeRmoval: How long a network member is contagious before it is removed\r\n# periods: how many rounds the simulation runs\r\n# pIsolation: The probability for a network member to immediately isoalte after being infected\r\n# is.directed: is the network directed or undirected\r\n\r\ncontagionModel <- function(net, time50=300, roundsBeforeRemoval=1, periods=20, pIsolation=0.3, is.directed=T){\r\n\r\n  if(is.directed){\r\n    # symmetrise the network\r\n    network <- net + t(net)\r\n  } else {\r\n    network <- net\r\n  }\r\n  \r\n  # chose a random network member to be infected first\r\n  init <- rep(0, nrow(network))\r\n  init[floor(runif(1,1,(nrow(network) + 1)))] <- 1\r\n  \r\n  # create a variable whether any network member is sick\r\n  # and make the initator one sick (state 1)\r\n  is.sick <- init\r\n  \r\n  # decide whether the first infected network member isolates,\r\n  # if yes, move it to removed state (-1)\r\n  isolates <- floor(runif(1) + pIsolation)\r\n  if(isolates == 1){\r\n    is.sick[is.sick==1] <- -1\r\n  }\r\n  \r\n  # create varaible that will be used remove network members \r\n  # after they were infectious for the specfied number of rounds\r\n  rounds.sick <- rep(0, nrow(network))\r\n  \r\n  # generate help variable\r\n  all <- 1:nrow(network)\r\n  \r\n  # create output variable as list\r\n  sick <- list()\r\n  \r\n  # loop through all periods\r\n  for(i in 1:periods){\r\n    # write current state in output\r\n    sick[[i]] <- is.sick\r\n    \r\n    # get the sick ones\r\n    who.is.sick <- all[all*(is.sick) > 0]\r\n    \r\n    # loop through all sick ones\r\n    for(sick.one in who.is.sick){\r\n      \r\n      # find the contacts they have that can be infected\r\n      at.risk <- all[network[,sick.one] > 0]\r\n      \r\n      # loop though each one in the risk set and decide whether they will be infected\r\n      for(alter in at.risk){\r\n        \r\n        # see if the alter is already infected or removed\r\n        if(is.sick[alter] == 0){\r\n          \r\n          # determine whether the alter gets infected using the time50\r\n          is.sick[alter] <- floor(runif(1) + (((network[alter,sick.one])/ (time50)) / \r\n                                                (1 + (network[alter,sick.one])/ (time50))))\r\n          \r\n          # if the new other is infected, decide whether it isolates\r\n          # in that case, move it to state removed (-1)\r\n          if(is.sick[alter] == 1){\r\n            isolates <- floor(runif(1) + pIsolation)\r\n            if(isolates == 1){\r\n              is.sick[alter] <- -1\r\n            }\r\n          }\r\n        }\r\n      }\r\n      # find out how long the infector mouse is sick and remove it if for longer than specified\r\n      if(rounds.sick[sick.one] >= roundsBeforeRemoval - 1){\r\n        is.sick[sick.one] <- -1\r\n      } else {\r\n        rounds.sick[sick.one] <- rounds.sick[sick.one] + 1\r\n      }\r\n    }\r\n  }\r\n  \r\n  return(sick)\r\n}\r\n\r\n# example\r\n# generate random network\r\nrandomNetwork <- matrix(floor(runif(225)*100), 15, 15)\r\n\r\n# run simulation\r\nresults <- contagionModel(net = randomNetwork, time50 = 75, pIsolation = 0.2)\r\n\r\n# results are interpreted as follows:\r\n# each element of the results list refers to the status of all network members at the end of that round\r\n# 1 means infected, -1 means removed (i.e. infected in the past but not contagious anymore), 0 means susceptible\r\n']","Data from: Infection-induced behavioural changes reduce connectivity and the potential for disease spread in wild mice contact networks Infection may modify the behaviour of the host and of its conspecifics in a group, potentially altering social connectivity. Because many infectious diseases are transmitted through social contact, social connectivity changes can impact transmission dynamics. Previous approaches to understanding disease transmission dynamics in wild populations were limited in their ability to disentangle different factors that determine the outcome of disease outbreaks. Here we ask how social connectivity is affected by infection and how this relationship impacts disease transmission dynamics. We experimentally manipulated disease status of wild house mice using an immune challenge and monitored social interactions within this free-living population before and after manipulation using automated tracking. The immune-challenged animals showed reduced connectivity to their social groups, which happened as a function of their own behaviour, rather than through conspecific avoidance. We incorporated these disease-induced changes of social connectivity among individuals into models of disease outbreaks over the empirically-derived networks. The models revealed that changes in host behaviour frequently resulted in the disease being contained to very few animals, as opposed to becoming widespread. Our results highlight the importance of considering the role that behavioural alterations during infection can have on social dynamics when evaluating the potential for disease outbreaks.",2
"Data for: Weak, but not strong, ties support coalition formation among wild female chimpanzees","In social species, individuals may be able to overcome competitive constraints on cooperation by leveraging relationships with familiar, tolerant partners. While strong social ties have been linked to cooperation in several social mammals, it is unclear the extent to which weak social ties can support cooperation, particularly among non-kin. We tested the hypothesis that weakly affiliative social relationships support cooperative coalition formation using 10 years of behavioural data on wild female chimpanzees. Female chimpanzees typically disperse and reside with non-kin as adults. Their social relationships are differentiated but often relatively weak, with few dyads sharing strong bonds. Females occasionally form aggressive coalitions together. Three measures of relationship quality - party association, five-meter proximity, and whether a dyad groomed - positively predicted coalitions, indicating that relationship quality influenced coalition partnerships. However, dyads that groomed frequently did not form more coalitions than dyads that groomed occasionally, and kin did not cooperate more than expected given their relationship quality. Thus, strong bonds and kinship did not bolster cooperation. We conclude that cooperative coalitions among female chimpanzees depend on social tolerance but do not require strong bonds. Our findings highlight social tolerance as a distinct pathway through which females can cultivate cooperative relationships.","['##Does social relationship quality predict coalition formation among adult female chimpanzees? \n#Fox et al. 2022\n\n# Load Tools & Libraries ####\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(DHARMa)\nlibrary(effects)\nlibrary(ggeffects)\nlibrary(MuMIn)\nlibrary(gridExtra)\nlibrary(MuMIn)\nlibrary(grid)\nlibrary(cowplot)\nlibrary(emmeans)\nlibrary(progress)\n\nymd <- lubridate::ymd\nmdy <- lubridate::mdy\nmonth <- lubridate::month\nyear <- lubridate::year\nymd_hms <- lubridate::ymd_hms\nhms <- lubridate::hms\n\nselect <- dplyr::select\nfilter <- dplyr::filter\n\ncontr <- glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun=2e5))\n\n## vif.mer function - this is for testing vif within glmer models, is adaptation from rms::vif\n## available here: https://osf.io/p6ahy/?pid=ezkpa \nvif.mer <- function (fit) {\n  ## adapted from rms::vif\n  \n  v <- vcov(fit)\n  nam <- names(fixef(fit))\n  \n  ## exclude intercepts\n  ns <- sum(1 * (nam == ""Intercept"" | nam == ""(Intercept)""))\n  if (ns > 0) {\n    v <- v[-(1:ns), -(1:ns), drop = FALSE]\n    nam <- nam[-(1:ns)]\n  }\n  \n  d <- diag(v)^0.5\n  v <- diag(solve(v/(d %o% d)))\n  names(v) <- nam\n  v\n}\n\n\n\n\n# ANALYSIS 1 ####\n# Load Data ####\nanalysis1_datashare <- read.csv(""analysis1_datashare.csv"", header = TRUE)\n\n# Variable key ####\n\n# ID1 & ID2: females \n# period: A-E\n# partycount_d: number of scans dyad was in the same party\n# coal_count_bi: number of coalitions dyad participated in during the two year period\n# SRI_bi_z: party association index z-score\n# index_5m_bi_z: five-meter index z-score\n# gmdur_index_z: grooming duration index z-score\n# gm_yn_bi: N = did not groom during period, Y = groomed during period\n# ranksumbi_z: summed dyad dominance rank during period (sum of each individual\'s average during the period)\n# immdyad: dyad type regarding female residency, 0 = resident-resident, 1 = immigrant-resident, 2 = immigrant-immigrant\n# kinship: 0 = unrelated, 1 = kin\n\n# Data Subsets ####\nCdata <- analysis1_datashare %>% #full data, 811 rows\n  filter(partycount_d > 200) %>% #must have > 200 party scans together, 713 rows\n  filter(total_AB_party > 0) %>% #must be in party together during a focal, 703 rows\n  filter(!is.na(ranksumbi_z)) #must have dominance data for both IDs, 689 rows\n#to run models on data subset without kin dyads, use this: \nCdata.kin <- Cdata %>% filter(kinship != ""1"")\n#to run models on data subset with only resident females, use this:\nCdata.imm <- Cdata %>% filter(immdyad == ""0"")\n\n\n# Example null model ####\n\nC.mod0 <-\n  glmer(coal_count_bi ~ 1 \n        + ranksumbi_z\n        + kinship\n        + offset(log(partycount_d))\n        + (1|ID1)\n        + (1|ID2)\n        + (1|period),\n        nAGQ = 0,\n        family = poisson,\n        control = contr,\n        data = Cdata)\nsummary(C.mod0)\ntestDispersion(C.mod0)\nplotResiduals(simulateResiduals(C.mod0))\ntestResiduals(C.mod0)\n\n# Example full model ####\n\nC.mod15 <- \n  glmer(coal_count_bi ~ 1 \n        + ranksumbi_z\n        + gmdur_index_z\n        + gm_yn_bi\n        + index_5m_bi_z\n        + SRI_bi_z\n        + kinship\n        + offset(log(partycount_d))\n        + (1|ID1)\n        + (1|ID2)\n        + (1|period),\n        nAGQ = 0,\n        family = poisson,\n        control=contr,\n        data = Cdata)\nsummary(C.mod15)\nplotResiduals(simulationOutput= (simulateResiduals(C.mod15)))\ntestResiduals(C.mod15)\nvif.mer(C.mod15)\n\n# Example model selection ####\n#we used dredge to run the model comparison but we also ran each model individually (variations of the full C.mod15 model) \n#to test model assumptions using code similar to above for each model version\noptions(na.action = ""na.fail"")\n#if you want to run this in the dataset without kin, remove kinship below as a variable in dredge\nC15dredge <- dredge(C.mod15, fixed = c(""ranksumbi_z"", ""offset(log(partycount_d))"", ""kinship""))\nC15.95 <- subset(C15dredge, cumsum(weight) <= .95, recalc.weights = FALSE)\nC15modavg <- model.avg(subset(C15dredge, cumsum(weight) <= .95, recalc.weights = FALSE))\nsummary(C15modavg)\nget.models(C15.95, subset = TRUE)\n\n#r.squaredGLMM is from MuMin package, see Nakagawa et al. 2017 http://doi.org/10.1098/rsif.2017.0213 \nr.squaredGLMM(C.mod15)\n\n# Example graphs ####\n#PARTY ASSOC GRAPH\nset.seed(123)\npredict1 <- ggemmeans(C.mod15, \n                      terms = c(""SRI_bi_z""), \n                      #type = ""zero_inflated"", \n                      back.transform = TRUE)\np1 <- ggplot(predict1, aes(x, predicted)) +\n  theme_bw(base_size = 18) + \n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(), \n        legend.position = ""none"") +\n  scale_y_continuous(breaks = c(0, 2, 4, 6, 8, 10), trans = ""log1p"") +\n  geom_jitter(data = Cdata,\n              aes(x = SRI_bi_z, y = coal_count_bi), \n              height = 0.05, color = ""grey64"", alpha = 0.5, size = 2) +\n  xlab(""Party association index z-score"") + ylab(""Number of Coalitions"") +\n  geom_line(colour = ""purple"") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, alpha = 0.1), fill = ""purple"")\nprint(p1)\n\n#FIVE M GRAPH\nset.seed(123)\npredict2 <- ggemmeans(C.mod15, \n                      terms = c(""index_5m_bi_z""), \n                      #type = ""random"", \n                      back.transform = TRUE)\np2 <- ggplot(predict2, aes(x, predicted)) +\n  theme_bw(base_size = 18) + \n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(), \n        legend.position = ""none"") +\n  scale_y_continuous(breaks = c(0, 2, 4, 6, 8, 10), trans = ""log1p"") + \n  geom_jitter(data = Cdata,\n              aes(x = index_5m_bi_z, y = coal_count_bi), \n              height = 0.05, color = ""grey64"", alpha = 0.5, size = 2) + \n  xlab(""Five meter association index z-score"") + ylab("""") +\n  geom_line(colour = ""purple"") + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, alpha = 0.1), fill = ""purple"")\nprint(p2)\n\n#GROOMING GRAPH *need to make sure it is adding actual data not predicted data?\npredict3 <- ggemmeans(C.mod15, \n                      terms = c(""gmdur_index_z[all]""), \n                      back.transform = TRUE)  \n\np3 <- ggplot(predict3, aes(x, predicted)) +\n  theme_bw(base_size = 18) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = ""none"") +\n  scale_y_continuous(breaks = c(0, 2, 4, 6, 8, 10), trans = ""log1p"") +\n  scale_x_continuous(limits = c(-0.5, 16)) +\n  geom_jitter(data = Cdata, \n              aes(x = gmdur_index_z, y = coal_count_bi),\n              height = 0.0, width = 0, color = ""grey64"", alpha = 0.5, size = 2) +\n  xlab(""Grooming duration index z-score"") + \n  ylab(""Number of Coalitions"") +\n  geom_line(colour = ""purple"") +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, alpha = 0.1), fill = ""purple"")\nprint(p3)\n\n#make p3a to make an insert\np3a <- ggplot(predict3, aes(x, predicted)) +\n  theme_bw(base_size = 18) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = ""none"") +\n  scale_y_continuous(breaks = c(0, 2, 10), trans = ""log1p"") +\n  scale_x_continuous(limits = c(-.5, 1.5), breaks = c(0, 1)) +\n  geom_jitter(data = Cdata, \n              aes(x = gmdur_index_z, y = coal_count_bi),\n              height = 0.0, width = 0, color = ""grey64"", alpha = 0.5, size = 2) +\n  xlab("" "") + ylab("" "") +\n  geom_line(colour = ""purple"") +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, alpha = 0.1), fill = ""purple"")\nprint(p3a)\n\np3_inset <- p3 + annotation_custom(ggplotGrob(p3a), xmin = 5, xmax = 16, ymin = 1.08)\nprint(p3_inset)\n\npredict4 <- ggemmeans(C.mod15,\n                      terms = ""gm_yn_bi"",\n                      back.transform = FALSE)\npred4 <- predict4 %>% data.frame()\nplot(predict4)\n#Plot the real data with mean and CI manually added\np4 <- ggplot() +\n  geom_boxplot(data = Cdata,\n               mapping = aes(gm_yn_bi, y = coal_count_bi),\n               outlier.shape = NA,\n               colour = ""grey"") +\n  geom_jitter(data = Cdata,\n              mapping = aes(gm_yn_bi, y = coal_count_bi),\n              color = ""gray64"", height = 0.1, width = 0.3, alpha = 0.5, size = 2) +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = ""none"",\n        plot.title = element_blank(),\n        text = element_text(size = 18)) +\n  xlab(""Groomed"") + ylab("""") +\n  scale_y_continuous(breaks= c(0, 2, 4, 6, 8, 10), trans = ""log1p"", limits = c(-0.1, NA)) +\n  geom_pointrange(data = pred4, mapping = aes(x = x, y = predicted, ymin = conf.low, ymax = conf.high),\n                  colour = ""purple"", size = 1, shape = ""diamond"")\n\nprint(p4)\nplot_grid(p1, p2, p3_inset, p4, nrow = 2, labels = c(""A"", ""B"", ""C"", ""D""))\n\n\n# Example permutations ####\n# > Example permutation Part 1 ####\n\n# Main data\n# Isolate IDs\n# Sample IDs and turn back into df\n# Rejoin them to data\n# \n# Create Storage for each parameter (empty vector, df, or list)\n# Run model on data\n# Extract coefficient, statistic, and p value and save to storage\n# Do this 1000 times\n# \n# Summarise output as averages of each column and proportion of CI that don\'t cross 0\n# Note that results may shift slightly because IDs were shuffled once for starting dataset provided here (ie. 1 permutation different from paper)\n# Example for Model 0 - ie the NULL Model\nset.seed(5)\nlist.coef <- vector(""list"", length = 1000)\nlist.msTable <- vector(""list"", length = 1000)\nlist.dredge <- vector(""list"", length = 1000)\nlist.confint <- vector(""list"", length = 1000)\noptions(na.action = ""na.fail"")\nfull_data_noID <- Cdata %>% select(-ID1, -ID2)\nset.seed(5)\nt <- Sys.time()\nfor(i in seq(1000)){\n  Sys.sleep(1/1000)\n  \n  IDs <- Cdata %>% \n    select(ID1, ID2) %>%\n    apply(., 1, sample) %>% t() %>% data.frame() %>%\n    rename(ID1 = X1, ID2 = X2)\n  \n  full_data_rID <- full_data_noID %>%\n    cbind(., IDs) \n  \n  C.mod15 <- \n    glmer(coal_count_bi ~ 1 \n          + ranksumbi_z\n          + gmdur_index_z\n          + gm_yn_bi\n          + index_5m_bi_z\n          + SRI_bi_z\n          + kinship\n          + offset(log(partycount_d))\n          + (1|ID1)\n          + (1|ID2)\n          + (1|period),\n          nAGQ = 0,\n          control=contr,\n          family = ""poisson"",\n          data = full_data_rID)\n  \n  C15dredge <- dredge(C.mod15, fixed = c(""ranksumbi_z"", ""offset(log(partycount_d))"", ""kinship""))\n  C15.95 <- subset(C15dredge, cumsum(weight) <= .95, recalc.weights = FALSE)\n  C15modavg <- model.avg(subset(C15dredge, cumsum(weight) <= .95, recalc.weights = FALSE))\n  \n  list.coef[[i]] <- summary(C15modavg)$coefmat.full %>% \n    data.frame() %>%\n    mutate(run = i) %>%\n    rownames_to_column()\n  \n  list.msTable[[i]] <- C15modavg$msTable %>% \n    data.frame() %>%\n    mutate(run = i) %>%\n    rownames_to_column(var = ""modelcontents"")\n  \n  list.dredge[[i]] <- C15dredge %>%\n    data.frame() %>%\n    mutate(run = i) %>%\n    rownames_to_column(var = ""model_num"")\n  \n  list.confint[[i]] <- confint(C15modavg, full = T) %>%\n    data.frame() %>%\n    mutate(run = i) %>%\n    rownames_to_column(var = ""parameter"")\n  \n}\nSys.time() - t\ncoefs.modavg <- do.call(""rbind"", list.coef)\nmsTable.modavg <- do.call(""rbind"", list.msTable)\ndredge.modavg <- do.call(""rbind"", list.dredge)\nconfint.modavg <- do.call(""rbind"", list.confint)\n\nsave(coefs.modavg,\n     msTable.modavg,\n     dredge.modavg,\n     confint.modavg,\n     list.coef,\n     list.msTable,\n     list.dredge,\n     list.confint,\n     file = ""avg of the avg V2.Rdata"")\n\n#load(""avg of the avg V2.Rdata"")\n#now summarise these results: coefs from top models and AIC/weights from all models\n\n#we want to average these coef to get the average B/se and proportion of 95% PI that cross 0 *from the models in the 95% subset* thus want to count how many times each parameter appears\nCI95.avgd.15 <- confint.modavg %>%\n  group_by(parameter) %>%\n  summarise(n = n(), \n            CIprop = (n - length(which(X2.5.. < 0 & X97.5.. > 0)))/n*100) \n\ncoefs.modavg.summary <- coefs.modavg %>%\n  select(-Adjusted.SE) %>%\n  group_by(rowname) %>%\n  summarise(n = n(), #just want to check n to make sure we are getting all the data\n            Est_mean = mean(Estimate), \n            SE_mean = mean(Std..Error), \n            z_mean = mean(z.value),\n            pval_prop = length(which(Pr...z.. < 0.05))/n*100) %>% #exact same as telling us how often CIs cross 0, but we report baseon CIs below (perm part 2)\n  ungroup() %>%\n  left_join(CI95.avgd.15, by = c(""rowname"" = ""parameter"", ""n"" = ""n"")) %>%\n  column_to_rownames(., var = ""rowname"") %>%\n  select(-z_mean, -n, -pval_prop) %>%\n  round(., digits = 3) %>%\n  rotate_df()\nView(coefs.modavg.summary)\nwrite.csv(coefs.modavg.summary, file = ""coefsmodavgsummary2.csv"")\n\n#we want to average the AIC and weights from each run of each model\ndredge.modavg.summary <- dredge.modavg %>%\n  group_by(model_num) %>%\n  summarise(AICc_mean = mean(AICc), \n            df = mean(df), \n            delta = mean(delta), \n            weight = mean(weight)) %>%\n  column_to_rownames(var = ""model_num"") %>%\n  round(., digits = 3) %>%\n  arrange(AICc_mean) %>%\n  rotate_df()\nView(dredge.modavg.summary)  \nwrite.csv(dredge.modavg.summary, file = ""dredgemodavgsummary2.csv"")\n\n#want to check how many times it assigned each model to being in the top 95% confidence set\nset95 <- msTable.modavg %>% \n  as.data.frame() %>%\n  mutate(modelcontents = as.factor(modelcontents)) %>%\n  group_by(modelcontents) %>% tally()\n\n\n# > Example permutations Part 2 ####\n#Now average permuted coefficients based on only the top models that come within the top 95% set more than 95% of the time \n#these are models 14, 16, 10 from dredge, which we can identify below based on what variables they have in them\n\n#load data\nload(""full_data_female_coalit.Rdata"", verbose = T)\n#this is meant to be run after running the female friends and coalitions REMIX main code file (and loading associated packages)\n\n#load tools\nselect <- dplyr::select\nfilter <- dplyr::filter\n\n#only running puermutations for set C, which is the set that includes rank. \n#C. Replicate w rank\ncontr <- glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun=2e5))\n\n#subset: must be in party together more than 200 scans and have rank data \nCdata <- full_data %>%\n  filter(partycount_d > 200) %>%\n  filter(total_AB_party > 0) %>%\n  filter(!is.na(ranksumbi_z))\n\n# Run dredge 1000 times and take model averaged results\n# Each iteration, we remove the IDs, randomly shuffle positions of ID1 and ID2, then reattach  \n# this took my computer about 40 minutes\n\nlist.coef <- vector(""list"", length = 1000)\nlist.msTable <- vector(""list"", length = 1000)\nlist.dredge <- vector(""list"", length = 1000)\nlist.confint <- vector(""list"", length = 1000)\noptions(na.action = ""na.fail"")\nfull_data_noID <- Cdata %>% select(-ID1, -ID2)\nset.seed(5)\nt <- Sys.time()\nfor(i in seq(1000)){\n  Sys.sleep(1/1000)\n  \n  IDs <- Cdata %>% \n    select(ID1, ID2) %>%\n    apply(., 1, sample) %>% t() %>% data.frame() %>%\n    rename(ID1 = X1, ID2 = X2)\n  \n  full_data_rID <- full_data_noID %>%\n    cbind(., IDs) \n  \n  C.mod15 <- \n    glmer(coal_count_bi ~ 1 \n          + ranksumbi_z\n          + gmdur_index_z\n          + gm_yn_bi\n          + index_5m_bi_z\n          + SRI_bi_z\n          + kinship\n          + offset(log(partycount_d))\n          + (1|ID1)\n          + (1|ID2)\n          + (1|period),\n          nAGQ = 0,\n          control=contr,\n          family = ""poisson"",\n          data = full_data_rID)\n  \n  C15dredge <- dredge(C.mod15, fixed = c(""ranksumbi_z"", ""offset(log(partycount_d))"", ""kinship""))\n  #MAKE SUBSET WITH IMPORTANT MODELS ONLY\n  sub <- subset(C15dredge, (has(""SRI_bi_z"", ""index_5m_bi_z"", ""gm_yn_bi"", ""gmdur_index_z"") |\n                              has(""SRI_bi_z"", ""index_5m_bi_z"", ""gm_yn_bi"") & has(!""gmdur_index_z"") | \n                              has(""SRI_bi_z"", ""gm_yn_bi"") & has(!""index_5m_bi_z"", !""gmdur_index_z"")), recalc.weights = FALSE)\n  #MODEL AVERAGE FROM SUBSET ONLY \n  modavgtop3 <- model.avg(sub) #THIS IS THE KEY PIECE HERE!\n  \n  list.coef[[i]] <- summary(modavgtop3)$coefmat.full %>% \n    data.frame() %>%\n    mutate(run = i) %>%\n    rownames_to_column()\n  \n  list.msTable[[i]] <- modavgtop3$msTable %>% \n    data.frame() %>%\n    mutate(run = i) %>%\n    rownames_to_column(var = ""modelcontents"")\n  \n  list.dredge[[i]] <- C15dredge %>%\n    data.frame() %>%\n    mutate(run = i) %>%\n    rownames_to_column(var = ""model_num"")\n  \n  list.confint[[i]] <- confint(modavgtop3, full = T) %>%\n    data.frame() %>%\n    mutate(run = i) %>%\n    rownames_to_column(var = ""parameter"")\n  \n}\nSys.time() - t\ncoefs.modavg <- do.call(""rbind"", list.coef)\nmsTable.modavg <- do.call(""rbind"", list.msTable)\ndredge.modavg <- do.call(""rbind"", list.dredge)\nconfint.modavg <- do.call(""rbind"", list.confint)\n\nsave(coefs.modavg,\n     msTable.modavg,\n     dredge.modavg,\n     confint.modavg,\n     list.coef,\n     list.msTable,\n     list.dredge,\n     list.confint,\n     file = ""avg of the top3.Rdata"")\n\n#now summarise these results: coefs from top models \n\n#we want to average these coef to get the average B and se and proportion of 95% PI that cross 0 *from the models in the 95% subset*#want to count how many times each parameter appears\nCI95.avgd.15 <- confint.modavg %>%\n  group_by(parameter) %>%\n  summarise(n = n(), \n            CIprop = (n - length(which(X2.5.. < 0 & X97.5.. > 0)))/n*100) \n\ncoefs.modavg.summary <- coefs.modavg %>%\n  select(-Adjusted.SE) %>%\n  group_by(rowname) %>%\n  summarise(n = n(), #just want to check n to make sure we are getting all the data\n            Est_mean = mean(Estimate), \n            SE_mean = mean(Std..Error), \n            z_mean = mean(z.value),\n            pval_prop = length(which(Pr...z.. < 0.05))/n*100) %>% \n  ungroup() %>%\n  left_join(CI95.avgd.15, by = c(""rowname"" = ""parameter"", ""n"" = ""n"")) %>%\n  column_to_rownames(., var = ""rowname"") %>%\n  select(-z_mean, -n, -pval_prop) %>%\n  round(., digits = 3) %>%\n  rotate_df()\nView(coefs.modavg.summary)\nwrite.csv(coefs.modavg.summary, file = ""coefsmodavgsummary TOP3.csv"")\n\n# > Example permutations: get info for individual models\n\n#This is an example of how we extracted information for each model (ie. to go into supplementary table S3)\n# Model 0 ####\nset.seed(5)\nlist.coef <- vector(""list"", length = 1000)\nlist.AIC <- vector(""list"", length = 1000)\nlist.confint <- vector(""list"", length = 1000)\nlist.r2 <- vector(""list"", length = 1000)\nfull_data_noID <- Cdata %>% select(-ID1, -ID2)\n\npb <- progress_bar$new(format = "" running [:bar] :percent eta: :eta"",\n                       total = 1000, clear = FALSE, width= 60)\nt <- Sys.time()\nfor(i in seq(1000)){\n  pb$tick()\n  Sys.sleep(1/1000)\n  \n  IDs <- Cdata %>% \n    select(ID1, ID2) %>%\n    apply(., 1, sample) %>% t() %>% data.frame() %>%\n    rename(ID1 = X1, ID2 = X2)\n  \n  full_data_rID <- full_data_noID %>%\n    cbind(., IDs) \n  \n  C.mod0 <-\n    glmer(coal_count_bi ~ 1 \n          + ranksumbi_z\n          + kinship\n          + offset(log(partycount_d))\n          + (1|ID1)\n          + (1|ID2)\n          + (1|period),\n          nAGQ = 0,\n          family = ""poisson"",\n          control = contr,\n          data = full_data_rID)\n  \n  list.coef[[i]] <- coef(summary(C.mod0)) %>% \n    data.frame() %>% \n    rownames_to_column()\n  \n  list.AIC[[i]] <- llikAIC(C.mod0) %>% \n    data.frame() %>% \n    rownames_to_column()\n  \n  list.confint[[i]] <- confint(C.mod0, method = ""Wald"") %>%\n    data.frame() %>%\n    mutate(run = i) %>%\n    rownames_to_column(var = ""parameter"")\n  \n  list.r2[[i]] <- r.squaredGLMM(C.mod0) %>%\n    data.frame() %>%\n    mutate(run = i) %>%\n    rownames_to_column(var = ""rsquared"")\n}\nSys.time() - t\nr.coefs.0 <- do.call(""rbind"", list.coef)\nr.AIC.0 <- do.call(""rbind"", list.AIC) %>% filter(rowname == ""AIC"") %>% summarise(n = n(), AIC_mean = mean(AICtab))\nr.95CI <- do.call(""rbind"", list.confint)\nr.r2tri.0 <- do.call(""rbind"", list.r2) %>% filter(rsquared == ""trigamma"") %>% \n  summarise(n = n(), R2tri_m_mean = mean(R2m), R2tri_c_mean = mean(R2c))\nr.r2log.0 <- do.call(""rbind"", list.r2) %>% filter(rsquared == ""lognormal"") %>% \n  summarise(n = n(), R2log_m_mean = mean(R2m), R2log_c_mean = mean(R2c))\n\n#make these into tables in the format I want\nCI95.avgd.0 <- r.95CI %>%\n  group_by(parameter) %>%\n  summarise(n = n(), \n            CIprop = (n - length(which(X2.5.. < 0 & X97.5.. > 0)))/n*100)\ncoef.avgd.0 <- r.coefs.0 %>%\n  group_by(rowname) %>%\n  summarise(n = n(), \n            Est_mean = mean(Estimate), \n            SE_mean = mean(Std..Error), \n            z_mean = mean(z.value),\n            pval_prop = length(which(Pr...z.. < 0.05))/n*100) %>% \n  ungroup() %>%\n  mutate(model_num = ""model_0"") %>%\n  select(-n) %>%\n  left_join(CI95.avgd.0, by = c(""rowname"" = ""parameter""))\n\nd1 <- coef.avgd.0 %>% filter(rowname == ""(Intercept)"") %>% \n  rename_with(~str_c(""Int_"", .), .cols = c(Est_mean, SE_mean, z_mean, pval_prop, CIprop)) %>%\n  select(-rowname)\nd2 <- coef.avgd.0 %>% filter(rowname == ""ranksumbi_z"")%>% \n  rename_with(~str_c(""Rank_"", .), .cols = c(Est_mean, SE_mean, z_mean, pval_prop, CIprop)) %>%\n  select(-rowname)\nd3 <- coef.avgd.0 %>% filter(rowname == ""kinship1"")%>% \n  rename_with(~str_c(""Kin_"", .), .cols = c(Est_mean, SE_mean, z_mean, pval_prop, CIprop)) %>%\n  select(-rowname)\nmod0_results <- d1 %>%\n  left_join(d2) %>%\n  left_join(d3) %>%\n  left_join(r.AIC.0) %>%\n  left_join(r.r2tri.0) %>%\n  left_join(r.r2log.0)\nView(mod0_results)\n\n\n# ANALYSIS 2 ####\n\n# Load Data ####\nanalysis2_datashare <- read.csv(""analysis2_datashare.csv"", header = TRUE)\n\n# Variable key ####\n\n# ID1: female who was part of coalitions\n# party_member: potential coalition partner (present in party at time of coalition)\n# coal_id: unique coalition event identified\n# event_factor: 1 = partner was chosen for coalition, 0 = partner was not chosen \n# SRI_bi_z: party association index z-score\n# index_5m_bi_z: five-meter index z-score\n# gmdur_index_z: grooming duration index z-score\n# gm_yn_bi: N = did not groom during period, Y = groomed during period\n# ranksumbi_z: summed dyad dominance rank during period (sum of each individual\'s average during the period)\n# immdyad: dyad type regarding female residency, 0 = resident-resident, 1 = immigrant-resident, 2 = immigrant-immigrant\n# kinship: 0 = unrelated, 1 = kin\n# period: A-E\n# party_size_fem_z: z-score number of females in the party (in addition to ID1, so really n+1 = # females in the party)\n\n# Data subsets ####\n#to run without kin dyads use this\nlog15kin <- analysis2_datashare %>% filter(kinship != ""1"")\n#to run with only resident females use this\nlog15imm <- analysis2_datashare %>% filter(immdyad == ""0"")\n\n# Example null model ####\nlog.opp0 <- glmer(event_factor ~ 1 \n                  + party_size_fem_z\n                  + ranksumbi_z\n                  + kinship\n                  + (1|coal_id)\n                  + (1|party_member)\n                  + (1|period), \n                  family = binomial(logit), \n                  data = analysis2_datashare,\n                  control = contr)\nsummary(log.opp0)\ntestDispersion(log.opp0)\nplot(simulateResiduals(log.opp0))\nvif.mer(log.opp0)\n\n# Example full model ####\nlog.opp15 <- glmer(event_factor ~ 1 \n                   + index_5m_bi_z\n                   + SRI_bi_z\n                   + gmdur_index_z\n                   + gm_yn_bi\n                   + kinship\n                   + ranksumbi_z\n                   + party_size_fem_z \n                   + (1|coal_id)\n                   + (1|party_member)\n                   + (1|period), \n                   family = binomial(logit), \n                   data = analysis2_datashare,\n                   control = contr)\nsummary(log.opp15)\ntestDispersion(log.opp15)\nplot(simulateResiduals(log.opp15))\nvif.mer(log.opp15)\n\n# Example model selection ####\n#we used dredge to run the model comparison but we also ran each model individually to test model assumptions using code similar to above for each model  \noptions(na.action = ""na.fail"")\n#if you want to run this in the dataset without kin, remove kinship here\nlog15dredge <- dredge(log.opp15, fixed = c(""party_size_fem_z"", ""ranksumbi_z"", ""kinship""))\nlog15.95 <- subset(log15dredge, cumsum(weight) <= .95, recalc.weights = FALSE)\nlog15modavg <- model.avg(subset(log15dredge, cumsum(weight) <= .95, recalc.weights = FALSE))\nsummary(log15modavg)\nget.models(log15.95, subset = TRUE)\nr.squaredGLMM(log.opp15)\n\n# Example Graphs #### \n\n#PARTY ASSOC GRAPH\npredict5 <- ggeffect(log.opp15, terms = ""SRI_bi_z[all]"", back.transform = TRUE)\np5 <- ggplot(predict5, aes(x, predicted)) +\n  geom_line() + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, alpha = 0.1), fill =""purple"") +\n  theme_bw(base_size = 18) + \n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(), \n        legend.position = ""none"") +\n  geom_point(data = analysis2_datashare, aes(x = SRI_bi_z, y = event_factor), alpha =0.2, size = 2, colour = ""grey64"") +\n  xlab(""Party association index z-score"") + ylab(""Probability of \\n partner selection"") \nprint(p5)\n\n#FIVE M GRAPH\npredict6 <- ggeffect(log.opp15, terms = ""index_5m_bi_z[all]"", back.transform = TRUE)\np6 <- ggplot(predict6, aes(x, predicted)) +\n  geom_line() + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, alpha = 0.1, size = 2), fill = ""purple"") +\n  theme_bw(base_size = 18) + \n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(), \n        legend.position = ""none"") +\n  geom_point(data = analysis2_datashare, aes(x = index_5m_bi_z, y = event_factor), alpha = 0.2, colour = ""grey64"") +\n  xlab(""Five meter association index z-score"") + ylab("""") \nprint(p6)\n\n#GROOMING GRAPHS\npredict7 <- ggeffect(log.opp15, terms = ""gmdur_index_z[all]"", back.transform = TRUE)\np7 <- ggplot(predict7, aes(x, predicted)) +\n  geom_line() + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, alpha = 0.1), fill = ""purple"") +\n  theme_bw(base_size = 18) + \n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(), \n        legend.position = ""none"") +\n  geom_point(data = analysis2_datashare, aes(x = gmdur_index_z, y = event_factor), alpha = 0.2, colour = ""grey64"") +\n  xlab(""Grooming duration index z-score"") +\n  ylab(""Probability of \\n partner selection"") +\n  scale_x_continuous(limits = c(-1, 12), breaks = c(-1, 0, 2, 4, 6, 8, 10, 12))\nprint(p7)\n\npredict8 <- ggeffect(log.opp15, terms = ""gm_yn_bi"", back.transform = TRUE)\npred8 <- predict8 %>% data.frame()\np8 <- ggplot() +\n  geom_pointrange(data = pred8, mapping = aes(x = x, y = predicted, ymin = conf.low, ymax = conf.high), \n                  colour = ""purple"", size = 1, shape = ""diamond"") +\n  theme_bw() + \n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(), \n        legend.position = ""none"", \n        plot.title = element_blank(), \n        text = element_text(size = 18)) +\n  xlab(""Groomed"") + ylab("""")\nprint(p8)\n\nplot_grid(p5, p6, p7, p8, nrow = 2, labels = c(""A"", ""B"", ""C"", ""D""))\n']","Data for: Weak, but not strong, ties support coalition formation among wild female chimpanzees In social species, individuals may be able to overcome competitive constraints on cooperation by leveraging relationships with familiar, tolerant partners. While strong social ties have been linked to cooperation in several social mammals, it is unclear the extent to which weak social ties can support cooperation, particularly among non-kin. We tested the hypothesis that weakly affiliative social relationships support cooperative coalition formation using 10 years of behavioural data on wild female chimpanzees. Female chimpanzees typically disperse and reside with non-kin as adults. Their social relationships are differentiated but often relatively weak, with few dyads sharing strong bonds. Females occasionally form aggressive coalitions together. Three measures of relationship quality - party association, five-meter proximity, and whether a dyad groomed - positively predicted coalitions, indicating that relationship quality influenced coalition partnerships. However, dyads that groomed frequently did not form more coalitions than dyads that groomed occasionally, and kin did not cooperate more than expected given their relationship quality. Thus, strong bonds and kinship did not bolster cooperation. We conclude that cooperative coalitions among female chimpanzees depend on social tolerance but do not require strong bonds. Our findings highlight social tolerance as a distinct pathway through which females can cultivate cooperative relationships.",2
"Data for: Social behavior, ovary size, and population of origin influence cuticular hydrocarbons in the orchid bee, Euglossa dilemma","Cuticular hydrocarbons (CHCs) are waxy compounds on the surface of insects that prevent desiccation and frequently serve as chemical signals mediating social and mating behaviors. Although their function in eusocial species has been heavily investigated, little is known about the evolution of CHC-based communication in species with simpler forms of social organization lacking specialized castes. Here, we investigate factors shaping CHC variation in the orchid bee Euglossa dilemma, which forms casteless social groups of 2-3 individuals. We first assess geographic variation, examining CHC profiles of males and females from three populations. We also consider CHC variation in the sister species, Euglossa viridissima, which occurs sympatrically with one population of E. dilemma. Next, we consider variation associated with female behavioral phases, to test the hypothesis that CHCs reflect ovary size and social dominance. We uncover a striking CHC polymorphism in E. dilemma spanning populations. In addition, we identify a separate set of CHCs that correlate with ovary size, social dominance, and expression of genes associated with social behavior, suggesting that CHCs convey reproductive and social information in E. dilemma. Together, our results reveal complex patterns of variation in which a subset of CHCs reflect the social and reproductive status of nestmates.","['\r\n\r\n### R analyses for Euglossa CHC manuscript. Accompanying data is found in the supplemental files. ####\r\n\r\n\r\n#load necessary packages\r\nlibrary(ecodist)\r\nlibrary(vegan)\r\nlibrary(gplots)\r\nlibrary(MASS)\r\nlibrary(ggplot2)\r\nlibrary(corrplot)\r\nlibrary(car)\r\nlibrary(Steel.Dwass.test)\r\nlibrary(ggpubr)\r\nlibrary(RVAideMemoire)\r\n\r\n############# NMDS for all populations, with 4 outliers removed (data in supplemental files) ###########################\r\n\r\n\r\n#read in and rename data\r\nCHC = read.csv(""All_Bees_CR_FL_MX.csv"")\r\n\r\n#check data import\r\ndim(CHC)\r\nhead(CHC)\r\n\r\n#find relative abundances (i.e. percents)\r\nrsums <- rowSums (CHC[,c(5:15)])\r\nCHC.norm <- CHC[,c(5:15)]/rsums\r\nrowSums(CHC.norm)\r\nhead(CHC.norm)\r\n\r\n#add back in descriptive columns\r\n\r\nCHC <- cbind(CHC[,1:4], CHC.norm)\r\nhead(CHC)\r\n\r\n\r\n#heatmap\r\nheatmap.2(as.matrix(CHC[,5:15]), Colv=NA, col=terrain.colors(250), labRow=paste (CHC$Sample,sep=""_""), main=""Female CHCs"", cexRow=2, cexCol=.5, trace=""none"")\r\n\r\n\r\n#nMDS plots\r\n\r\n#create triangular distance matrix\r\nbc.dist <- bcdist(CHC[,5:15]) \r\nbc.dist\r\n\r\n# nMDS,note: may take a long time to run. Running at 10 iterations is faster and provides very similar grouping with comparable (low) stress value.\r\nCHC\r\nCHC.nmds <- nmds (bc.dist, mindim=2, maxdim=2, nits=100)\r\nCHC.nmin <- nmds.min (CHC.nmds)\r\n\r\nCHC.nmin <- cbind(CHC.nmin, CHC$Sample, CHC$Behavior, CHC$Species_Pop_Sex, CHC$chemotype)\r\nCHC.nmin\r\nhead(CHC.nmin)\r\ncolnames (CHC.nmin)[3]<- ""Sample""\r\ncolnames (CHC.nmin)[4]<- ""Behavior""\r\ncolnames(CHC.nmin)[5] <- ""Species_Pop_Sex""\r\ncolnames(CHC.nmin) [6] <- ""chemotype""\r\nhead(CHC.nmin)\r\n\r\n#quick plot of  nMDS\r\n\r\ncolors <- c(""blue"",""red"",""purple"",""green"", ""orange"", ""olivedrab"",""cyan"",""firebrick"",""gray"",""magenta"",""lightgreen"", ""yellow"", ""tan"", ""thistle"", ""deepskyblue4"",""limegreen"",""wheat"", ""sienna"",""orchid4"",""black"")\r\n\r\nplot(CHC.nmin$X1, CHC.nmin$X2, cex=1.2, main=""Female CHCs"",col=colors[factor(CHC.nmin$Behavior)],pch=c(8,17,16,15,18,14,19,5,6,7)[factor(CHC.nmin$Behavior)], xlab="""", ylab="""")\r\n\r\n#put sample name labels on points\r\ntext(CHC.nmin$X1, CHC.nmin$X2, pos=1, labels=CHC.nmin$Sample, cex=0.2)\r\n\r\n# output NMDS coordinates for ggplot2 plotting\r\n#write.table(CHC.nmin,""NMDS_all_ggplot.txt"")\r\n\r\n# Simper Analysis of chemotypes\r\n\r\ndim(CHC)\r\ncomm = CHC[ ,5:15]\r\nsimpcomm= simper(comm, group = CHC$chemotype)\r\nsummary(simpcomm)\r\nlapply(simpcomm, FUN=function(x){x$overall})\r\n\r\n\r\n# ggplot for nicer NMDS visualization based on previous NMDS output\r\n\r\n# rename NMDS matrix for ggplot2\r\nNMDS_all_ggplot = CHC.nmin\r\n\r\nNMDS_all_ggplot$Behavior = as.factor(NMDS_all_ggplot$Behavior)\r\ngp =ggplot(NMDS_all_ggplot, aes(x=NMDS_all_ggplot$X1, y=NMDS_all_ggplot$X2)) +\r\n  geom_point(aes(fill = NMDS_all_ggplot$Behavior ,shape = NMDS_all_ggplot$Behavior, color= NMDS_all_ggplot$Behavior), size = 3) +\r\n  scale_shape_manual(values = c(21,22,10,7,24,23,9,2))+\r\n  theme_bw()+\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),legend.position=""none"")\r\ngp\r\n\r\ngp + scale_fill_manual(breaks = c(""1"", ""2"", ""3"", ""4"",""5"", ""6"",""7"",""8""),\r\n                       values=c(""red"", ""blue"", ""red3"", ""midnightblue"", ""palegreen4"", ""purple"", ""darkorchid4"", ""darkgreen""))+\r\n  scale_color_manual(breaks = c(""1"", ""2"", ""3"", ""4"",""5"", ""6"",""7"",""8""),\r\n                     values=c(""black"", ""black"", ""red3"", ""midnightblue"", ""black"", ""black"", ""darkorchid4"", ""darkgreen""))\r\n\r\n######################## NMDS For SOcial Behavior #####################################\r\n\r\n\r\n\r\n# read in and rename data\r\nCHC = read.csv(""Social_FL_NewlyEmerged.csv"")\r\n\r\n# note: in data file behavior_num 1 = dominant, 2 = subordinate, 3 = foundress, 4 = guard, 5 = newly emerged\r\n\r\n#check that data read in\r\ndim(CHC)\r\nhead(CHC)\r\n\r\n#find relative abundances (i.e. percents)\r\nrsums <- rowSums (CHC[,c(4:20)])\r\nCHC.norm <- CHC[,c(4:20)]/rsums\r\nrowSums(CHC.norm)\r\nhead(CHC.norm)\r\n\r\n#add back in descriptive columns\r\nCHC <- cbind(CHC[,1:3], CHC.norm)\r\nhead(CHC)\r\n\r\n\r\n#heatmap\r\nheatmap.2(as.matrix(CHC[,4:20]), Colv=NA, col=terrain.colors(250), labRow=paste (CHC$Sample,sep=""_""), main=""Female CHCs"", cexRow=2, cexCol=.5, trace=""none"")\r\n\r\n\r\n#nMDS plots\r\n\r\n#create triangular distance matrix\r\nbc.dist <- bcdist(CHC[,4:20]) \r\nbc.dist\r\n\r\n# nMDS, note: may take a long time to run. Running at 10 iterations is faster and provides very similar grouping with comparable (low) stress value.\r\nCHC\r\nCHC.nmds <- nmds (bc.dist, mindim=2, maxdim=2, nits=100)\r\nCHC.nmin <- nmds.min (CHC.nmds)\r\n\r\nCHC.nmin <- cbind(CHC.nmin, CHC$Sample, CHC$Behavior_num, CHC$Behavior_cat)\r\nCHC.nmin\r\nhead(CHC.nmin)\r\ncolnames (CHC.nmin)[3]<- ""Sample""\r\ncolnames (CHC.nmin)[4]<- ""Behavior_num""\r\ncolnames(CHC.nmin)[5] <- ""Behavior_cat""\r\nhead(CHC.nmin)\r\n\r\n# quick plot of nMDS\r\n\r\n#symbols <- c(pch=16,15,17,18,3)\r\ncolors <- c(""blue"",""red"",""purple"",""green"", ""orange"", ""olivedrab"",""cyan"",""firebrick"",""gray"",""magenta"",""lightgreen"", ""yellow"", ""tan"", ""thistle"", ""deepskyblue4"",""limegreen"",""wheat"", ""sienna"",""orchid4"",""black"")\r\nplot(CHC.nmin$X1, CHC.nmin$X2, cex=1, main=""Female CHCs"",col=colors[factor(CHC.nmin$Behavior_num)],pch=c(8,17,16,15,18,14,19,5,6,7)[factor(CHC.nmin$Behavior_num)], xlab="""", ylab="""")\r\n\r\n# put text labels of sample ID on points\r\ntext(CHC.nmin$X1, CHC.nmin$X2, pos=1, labels=CHC.nmin$Sample, cex=0.2)\r\n\r\n# output NMDS coordinates for ggplot2 plotting\r\n#write.table(CHC.nmin,""Social_NMDS.txt"")\r\n\r\n#SimperAnalysis for contribution of peaks to behavior\r\n\r\ndim(CHC)\r\ncomm = CHC[ ,4:20]\r\nsimpcomm= simper(comm, group = CHC$Behavior_num)\r\nsummary(simpcomm)\r\nlapply(simpcomm, FUN=function(x){x$overall})\r\n\r\n# ggplot for nicer NMDS plot of social individuals, based on output of above NMDS \r\n\r\n#rename NMDS matrix for ggplot2\r\nSocial_NMDS = CHC.nmin\r\n\r\nSocial_NMDS$Behavior_cat = as.factor(Social_NMDS$Behavior_cat)\r\ngp =ggplot(Social_NMDS, aes(x=Social_NMDS$X1, y=Social_NMDS$X2)) +\r\n  geom_point(aes(fill = Social_NMDS$Behavior_cat ,shape = Social_NMDS$Behavior_cat), size = 3) +\r\n  scale_shape_manual(values = c(21,24,23,22,25))+\r\n  theme_bw()+\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),legend.position=""none"")\r\ngp\r\n\r\ngp + scale_fill_manual(breaks = c(""1"", ""2"", ""3"", ""4"",""5""),\r\n                       values=c(""red"", ""blue"", ""palegreen4"", ""purple"", ""light blue""))\r\n\r\n\r\n\r\n################# Correlation Plots made with FL females (foundress, guard, dominant, subordinate) ###########\r\n\r\n#read in and rename data\r\nCHC_social_correlation_matrix = read.csv(""CHC_social_correlation_matrix.csv"")\r\n\r\n#first check the best fit for # of clusters (3 is cleary the best fit here)\r\n\r\n# make correlation plot with three possible clusters\r\n\r\nm = cor(CHC_social_correlation_matrix, method = ""spearman"")\r\ncorrplot(m, method = \'circle\', order = ""hclust"", addrect = 3, hclust.method = ""ward.D2"")\r\n\r\n# make correlation plot with two possible clusters\r\n\r\nm = cor(CHC_social_correlation_matrix, method = ""spearman"")\r\ncorrplot(m, method = \'circle\', order = ""hclust"", addrect = 2, hclust.method = ""ward.D2"")\r\n\r\n# make correlation plot with 4 possible clusters\r\n\r\nm = cor(CHC_social_correlation_matrix, method = ""spearman"")\r\ncorrplot(m, method = \'circle\', order = ""hclust"", addrect = 4, hclust.method = ""ward.D2"")\r\n\r\n\r\n############################CHC Peak Anovas, boxplots, regression #####################\r\n\r\n\r\n\r\nOvData = read.csv(""Social_chemotypes_ovarysize.csv"")\r\n\r\n# lettered variables in data sheet: a = foundress, b = guard, c = dominant, d = subordinate ##\r\n# numbered variables in data sheet: 1 = foundress, 2 = guard, 3 = dominant, 4 = subordinate\r\n\r\n\r\nOI = OvData$OvaryIndex\r\nBehav = OvData$Behavior_lettered\r\ntest = OvData$module_three\r\n\r\n## homogeneity of variances and normality ##\r\nleveneTest(test~Behav, data =OvData)\r\nshapiro.test(test)\r\n\r\n## Fails normality so test transformed ##\r\npeak = sqrt(OvData$module_three)\r\nshapiro.test(peak)\r\n\r\n\r\n#Peaks vs Behavior\r\nfit <- aov(peak ~ Behav , data=OvData)\r\nsummary(fit)\r\nTukeyHSD(fit)\r\n\r\n\r\n#module-three Boxplots \r\n\r\npeak = OvData$module_three\r\nBehav = OvData$Behavior_lettered\r\n\r\ngp = ggplot(OvData, aes(x = Behav, y = peak))+\r\n  geom_boxplot(width=0.25)+\r\n  theme_bw()+\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\r\n\r\ngp + geom_jitter(shape=16, position=position_jitter(0.05)) + ylim(0.1,0.35)\r\n# note: outlier points are duplicated on this figure due to jittering and the duplicate values were removed in final figure editing\r\n\r\n#### module-three vs Ovary Size ####\r\n\r\ngp =ggplot(OvData, aes(x=OvData$OvaryIndex, y=OvData$module_three)) +\r\n  geom_point(aes(fill = OvData$Behavior_lettered ,shape = OvData$Behavior_lettered), size = 4) +\r\n  scale_shape_manual(values = c(23,22,21,24))+\r\n  geom_smooth(color = 1, method=lm, se=FALSE, fullrange=TRUE)+\r\n  stat_cor(method = ""spearman"", color = 1)+\r\n  theme_bw()+\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")\r\ngp\r\n\r\ngp + scale_fill_manual(breaks = c(""a"", ""b"", ""c"", ""d""),\r\n                       values=c(""palegreen4"", ""purple"", ""red"", ""blue""))\r\n\r\n# add data labels\r\ngp +geom_text(aes(label=OvData$Sample),hjust=0, vjust=0)\r\n\r\n\r\n########################### Repeat analysis With only Chemotype A individuals ##############################\r\n\r\n\r\nOvData = read.csv(""Chemotype_A_Social_ovaries.csv"")\r\n# module-three check w/in chemotype A individuals \r\n# lettered variables in data sheet: a = foundress, b = guard, c = dominant, d = subordinate\r\n# numbered variables in data sheet: 1 = foundress, 2 = guard, 3 = dominant, 4 = subordinate\r\n\r\nOI = OvData$OvaryIndex\r\nBehav = OvData$Behavior_lettered\r\npeak = OvData$module_three\r\n\r\n\r\n## homogeneity of variances and normality ##\r\nleveneTest(peak~Behav, data =OvData)\r\nshapiro.test(OvData$module_three)\r\n\r\n## Passes both, so continue\r\n\r\n#Peaks vs Behavior\r\nfit <- aov(peak ~ Behav , data=OvData)\r\nsummary(fit)\r\nTukeyHSD(fit)\r\n\r\n## boxplot of within chemotype A only, signal peaks. Replicates finding of larger dataset ###\r\n\r\ngp = ggplot(OvData, aes(x = Behav, y = peak))+\r\n  geom_boxplot(width=0.25)+\r\n \r\n  theme_bw()+\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\r\n\r\ngp + geom_jitter(shape=16, position=position_jitter(0.05))\r\n\r\n# module-three vs Ovary Size for only chemotype A individuals #\r\n\r\ngp =ggplot(OvData, aes(x=OvData$OvaryIndex, y=OvData$module_three)) +\r\n  geom_point(aes(fill = OvData$Behavior_lettered ,shape = OvData$Behavior_lettered), size = 4) +\r\n  scale_shape_manual(values = c(23,22,21,24))+\r\n  geom_smooth(color = 1, method=lm, se=FALSE, fullrange=TRUE)+\r\n  stat_cor(method = ""pearson"", color = 1)+\r\n  theme_bw()+\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")\r\ngp\r\n\r\ngp + scale_fill_manual(breaks = c(""a"", ""b"", ""c"", ""d""),\r\n                       values=c(""palegreen4"", ""purple"", ""red"", ""blue""))\r\n\r\n# add sample labels\r\ngp +geom_text(aes(label=OvData$Sample),hjust=0, vjust=0)\r\n\r\n# chemotype A check w/in chemotype A individuals\r\n\r\nOI = OvData$OvaryIndex\r\nBehav = OvData$Behavior_lettered\r\npeak = OvData$chemotype_A_peaks\r\n\r\n## homogeneity of variances and normality ##\r\nleveneTest(peak~Behav, data =OvData)\r\nshapiro.test(OvData$chemotype_A_peaks)\r\n\r\n# fails normality so sqrt transform for stats \r\n\r\nsqrtChemoA = sqrt(OvData$chemotype_A_peaks)\r\nleveneTest(sqrtChemoA~Behav, data =OvData)\r\nshapiro.test(sqrtChemoA)\r\n\r\n#chemotype A still failed normality after transformation, so move forward with kruskal-wallis on untransformed data\r\n\r\nkruskal.test(peak~Behav, data = OvData)\r\nSteel.Dwass(OvData$chemotype_A_peaks, group = OvData$Behavior_numbered )\r\n\r\n#ggplot boxplot of chemotype A peaks with chemotype A only individuals\r\n\r\ngp = ggplot(OvData, aes(x = Behav, y = peak))+\r\n  geom_boxplot(width=0.25)+\r\n  #geom_point(aes())+\r\n  theme_bw()+\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\r\n\r\ngp + geom_jitter(shape=16, position=position_jitter(0.05))\r\n# note: outlier points are duplicated on this figure due to jittering and the duplicate values were removed in final figure editing\r\n\r\n# CHC Module 1 (diagnostic of chemotype-A) vs Ovary Size ###\r\n\r\ngp =ggplot(OvData, aes(x=OvData$OvaryIndex, y=OvData$chemotype_A_peaks)) +\r\n  geom_point(aes(fill = OvData$Behavior_lettered ,shape = OvData$Behavior_lettered), size = 4) +\r\n  scale_shape_manual(values = c(23,22,21,24))+\r\n  geom_smooth(color = 1, method=lm, se=FALSE, fullrange=TRUE)+\r\n  stat_cor(method = ""spearman"", color = 1)+\r\n  theme_bw()+\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")\r\ngp\r\n\r\ngp + scale_fill_manual(breaks = c(""a"", ""b"", ""c"", ""d""),\r\n                       values=c(""palegreen4"", ""purple"", ""red"", ""blue""))\r\n\r\n\r\n# chemotype B check w/in chemotype A individuals\r\n\r\nOI = OvData$OvaryIndex\r\nBehav = OvData$Behavior_lettered\r\npeak = OvData$chemotype_B_peaks\r\n\r\n## homogeneity of variances and normality ##\r\nleveneTest(peak~Behav, data =OvData)\r\nshapiro.test(peak)\r\n\r\n# fails normality so proceed with sqrt transformation\r\n\r\nsqrtChemoB = sqrt(peak)\r\nleveneTest(sqrtChemoB~Behav, data = OvData)\r\nshapiro.test(sqrtChemoB)\r\n\r\n# fails homogeneity of variances, proceed with kruskal wallis. \r\n\r\nkruskal.test(peak~Behav, data = OvData)\r\nSteel.Dwass(OvData$chemotype_B_peaks, group = OvData$Behavior_numbered )\r\n\r\n# ggplot boxplot of chemotype B peaks in chemotype A individuals\r\n\r\ngp = ggplot(OvData, aes(x = Behav, y = peak))+\r\n  geom_boxplot(width=0.25)+\r\n  theme_bw()+\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\r\n\r\ngp + geom_jitter(shape=16, position=position_jitter(0.05))\r\n# note: outlier points are duplicated on this figure due to jittering and the duplicate values were removed in final figure editing\r\n\r\n# CHC Module 2 (diagnostic of chemotype-B) vs Ovary Size ###\r\n\r\ngp =ggplot(OvData, aes(x=OvData$OvaryIndex, y=OvData$chemotype_B_peaks)) +\r\n  geom_point(aes(fill = OvData$Behavior_lettered ,shape = OvData$Behavior_lettered), size = 4) +\r\n  scale_shape_manual(values = c(23,22,21,24))+\r\n  geom_smooth(color = 1, method=lm, se=FALSE, fullrange=TRUE)+\r\n  stat_cor(method = ""spearman"", color = 1)+\r\n  theme_bw()+\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")\r\ngp\r\n\r\ngp + scale_fill_manual(breaks = c(""a"", ""b"", ""c"", ""d""),\r\n                       values=c(""palegreen4"", ""purple"", ""red"", ""blue""))\r\n\r\n\r\n################# Gene Expression CHC Correlations ############################\r\n\r\n\r\n####BRAIN ##########\r\n\r\nBrainData = read.csv(""Brains_CHCs_Genes.csv"")\r\n  \r\n#individual correlations can be explored by changing the XY columns in the code below\r\n\r\ngp =ggplot(BrainData, aes(x=BrainData$Edil_04295, y=BrainData$module_three)) +\r\n  geom_point(aes( fill=BrainData$Behavior, shape = BrainData$Behavior),size = 4) +\r\n  scale_shape_manual(values = c(23,22,21,24))+\r\n  geom_smooth(color = 1, method=lm, se=FALSE, fullrange=TRUE)+\r\n  stat_cor(method = ""spearman"", color = 1)+\r\n  theme_bw()+\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")\r\ngp\r\n\r\ngp + scale_fill_manual(breaks = c(""a"", ""b"", ""c"", ""d""),\r\n                       values=c(""palegreen4"", ""purple"", ""red"", ""blue""))\r\n\r\n\r\n\r\n############# Ovaries ##################################\r\n\r\n\r\nOvaryData = read.csv(""Ovaries_CHCs_Genes.csv"")\r\n\r\n#individual correlations can be explored by changing the XY columns in the code below\r\n\r\n  gp =ggplot(OvaryData, aes(x=OvaryData$Edil_04108, y=OvaryData$module_three)) +\r\n  geom_point(aes( fill = OvaryData$Behavior, shape = OvaryData$Behavior), size = 4) +\r\n  scale_shape_manual(values = c(23,22,21,24))+\r\n  geom_smooth(color = 1, method=lm, se=FALSE, fullrange=TRUE)+\r\n  stat_cor(method = ""spearman"", color = 1)+\r\n  theme_bw()+\r\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = ""none"")\r\ngp\r\n\r\ngp + scale_fill_manual(breaks = c(""a"", ""b"", ""c"", ""d""),\r\n                       values=c(""palegreen4"", ""purple"", ""red"", ""blue""))\r\ngp +geom_text(aes(label=OvaryData$Sample),hjust=0, vjust=0)\r\n\r\n\r\n########### Relatedness Histograms for each nest, relative to mean relatedness of all samples #############################\r\n\r\n\r\nRelatednessEstimates = read.csv(""RelatednessEstimates.csv"")\r\n\r\n\r\n#### Nest 100 control ######\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Control Nest ID:100 \\n Mother and Daughters \\n All Chemotype A"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nesmate relatedness\r\nabline(v=0.371,col=""red"", lty =3)   \r\nabline(v=0.404,col=""red"", lty =3)\r\nabline(v=0.349,col=""red"", lty =5)\r\nabline(v=0.374,col=""red"", lty =3)\r\nabline(v=0.354,col=""red"", lty =5)\r\nabline(v=0.293,col=""red"", lty =5)\r\n\r\n\r\n#### Nest 13 control ######\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Control Nest ID:13 \\n Mother and Daughters \\n All Chemotype A"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nestmate relatedness\r\nabline(v=0.51,col=""red"", lty =3)\r\nabline(v=0.499,col=""red"", lty =3)\r\nabline(v=0.472,col=""red"", lty =3)\r\nabline(v=0.331,col=""red"", lty =5)\r\nabline(v=0.443,col=""red"", lty =3)\r\nabline(v=0.449,col=""red"", lty =3)\r\nabline(v=0.453,col=""red"", lty =3)\r\nabline(v=0.35,col=""red"", lty =5)\r\nabline(v=0.324,col=""red"", lty =5)\r\nabline(v=0.342,col=""red"", lty =5)\r\n\r\n#### Nest 28B control ######\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Control Nest ID:28B \\n Non-kin \\n Mixed Chemotypes"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nestmate relatedness\r\nabline(v=0.214,col=""red"", lty =6)\r\n\r\n###### Nest 51B control #####\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Control Nest ID:51B \\n Mother and Daughter \\n Mixed Chemotypes"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nestmate relatedness\r\nabline(v=0.318,col=""red"", lty =5)\r\n\r\n\r\n###### Nest 51A ############\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID: 51B \\n All Chemotype A"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nestmate relatedness\r\nabline(v=0.351,col=""red"", lty =3)\r\nabline(v=0.355,col=""red"", lty =3)\r\nabline(v=0.47,col=""red"", lty =3)\r\nabline(v=0.292,col=""red"", lty =3)\r\nabline(v=0.334,col=""red"", lty =3)\r\nabline(v=0.352,col=""red"", lty =3)\r\n\r\n######## Nest 59A ###############\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:59A \\n All Chemotype A"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\n\r\n# nestmate relatedness\r\nabline(v=0.351,col=""red"", lty =6)\r\n\r\n########## Nest 59B ###########\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:59B \\n All Chemotype A"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\n\r\n# nestmate relatedness\r\nabline(v=0.349,col=""red"", lty =6)\r\n\r\n########### Nest 75 ############\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:75 \\n Mixed Chemotypes"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n#nestmate relatedness\r\nabline(v=0.26,col=""red"", lty =6)\r\n\r\n########## Nest 82 ##############\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:82 \\n All Chemotype A"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nestmate relatedness\r\nabline(v=0.409,col=""red"", lty =6)\r\n\r\n########## Nest 5 ##############\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:5 \\n Mixed Chemotypes"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nestmate relatedness\r\nabline(v=0.175,col=""red"", lty =6)\r\n\r\n####### Nest 47 ############\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:47 \\n All Chemotype B"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n#nestmate relatedness\r\nabline(v=0.376,col=""red"", lty =6)\r\n\r\n####### Nest 44 ############\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:44 \\n All Chemotype A"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$V6),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n#nestmate relatedness\r\nabline(v=0.297,col=""red"", lty =6)\r\n\r\n###### Nest 33 ############\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:33 \\n Mixed Chemotypes"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n#nestmate relatedness\r\nabline(v=0.282,col=""red"", lty =6)\r\nabline(v=0.385,col=""red"", lty =6)  ## chemotype Mix ##\r\nabline(v=0.337,col=""red"", lty =6)  ## chemotype Mix ##\r\n\r\n####### Nest 29 ############\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:29 \\n All Chemotype A"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nestmate relatedness\r\nabline(v=0.449,col=""red"", lty =6)\r\nabline(v=0.392,col=""red"", lty =6)\r\nabline(v=0.385,col=""red"", lty =6)\r\n\r\n###### Nest 28A ##########\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:28A \\n All Chemotype A"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nestmate relatedness\r\nabline(v=0.36,col=""red"", lty =6)\r\nabline(v=0.43,col=""red"", lty =6)\r\nabline(v=0.363,col=""red"", lty =6)\r\nabline(v=0.379,col=""red"", lty =6)\r\nabline(v=0.454,col=""red"", lty =6)\r\n\r\n###### Nest 27 ##########\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:27 \\n All Chemotype A"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nestmate relatedness\r\nabline(v=0.409,col=""red"", lty =6)\r\nabline(v=0.464,col=""red"", lty =6)\r\nabline(v=0.376,col=""red"", lty =6)\r\n\r\n####### Nest 2 #################\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:2 \\n Mixed Chemotypes"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n\r\n#nestmate relatedness\r\nabline(v=0.357,col=""red"", lty =6)\r\n\r\n###### Nest 26 ##########\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:26 \\n All Chemotype B"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\n\r\n\r\n# nestmate relatedness\r\nabline(v=0.236,col=""red"", lty =6)\r\n\r\n###### Nest 20 ##########\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:20 \\n All Chemotype A"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nestmate relatedness\r\nabline(v=0.381,col=""red"", lty =6)\r\nabline(v=0.311,col=""red"", lty =6)\r\nabline(v=0.348,col=""red"", lty =6)\r\n\r\n###### Nest 16A ##########\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:16A \\n All Chemotype A"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nestmate relatedness\r\nabline(v=0.335,col=""red"", lty =6)\r\n\r\n###### Nest 16B ##########\r\nhist(RelatednessEstimates$Wang_Statistic, main = ""Nest ID:16B \\n Mixed Chemotypes"", xlab = ""Relatedness Estimate"", ylab = ""Dyad Frequency"")\r\nabline(v=mean(RelatednessEstimates$Wang_Statistic),col=""blue"")\r\ntext(0.01, 300, paste( ""Mean ="",0.218, ""\\n St. Deviation = 0.067"", ""\\n 90th Percentile = 0.296""))\r\n\r\n# nestmate relatedness\r\nabline(v=0.284,col=""red"", lty =6)\r\n\r\n\r\n\r\n\r\n\r\n################# R code for supplemental comparison of body, abdomen,and wing CHC extractions #########################\r\n\r\n\r\n# rename data\r\nCHC = read.csv(""body_abdomen_wing_extractions.csv"")\r\n#CHC = All_Bees_CR_FL\r\ndim(CHC)\r\nhead(CHC)\r\n\r\n#find relative abundances (i.e. percents)\r\nrsums <- rowSums (CHC[,c(5:21)])\r\nCHC.norm <- CHC[,c(5:21)]/rsums\r\nrowSums(CHC.norm)\r\nhead(CHC.norm)\r\n\r\n#add back in descriptive columns\r\nCHC <- cbind(CHC[,1:4], CHC.norm)\r\nhead(CHC)\r\n#write.csv(CHC, file= ""body_abdomen_wing_histogram.csv"")\r\n\r\n#create triangular distance matrix\r\nbc.dist <- bcdist(CHC[,5:21]) \r\nbc.dist\r\n\r\n# nMDS\r\nCHC\r\nCHC.nmds <- nmds (bc.dist, mindim=2, maxdim=2, nits=10)\r\nCHC.nmin <- nmds.min (CHC.nmds)\r\n\r\nCHC.nmin <- cbind(CHC.nmin, CHC$Sample, CHC$method, CHC$individual, CHC$method_numbered)\r\nCHC.nmin\r\nhead(CHC.nmin)\r\ncolnames (CHC.nmin)[3]<- ""Sample""\r\ncolnames (CHC.nmin)[4]<- ""method""\r\ncolnames(CHC.nmin)[5] <- ""individual""\r\ncolnames(CHC.nmin) [6] <- ""method_numbered""\r\nhead(CHC.nmin)\r\n\r\n#plot nMDS\r\n\r\n\r\ncolors <- c(""blue"",""red"",""purple"",""green"", ""orange"", ""olivedrab"",""cyan"",""firebrick"",""gray"",""magenta"",""lightgreen"", ""yellow"", ""tan"", ""thistle"", ""deepskyblue4"",""limegreen"",""wheat"", ""sienna"",""orchid4"",""black"")\r\n\r\nplot(CHC.nmin$X1, CHC.nmin$X2, cex=1, main=""Female CHCs"",col=colors[factor(CHC.nmin$method)],pch=c(8,17,16,15,18,14,19,5,6,7)[factor(CHC.nmin$method)], xlab="""", ylab="""")\r\n\r\n# add sample names to points\r\ntext(CHC.nmin$X1, CHC.nmin$X2, pos=1, labels=CHC.nmin$Sample, cex=0.2)\r\n\r\n\r\n# Simper Analysis  #\r\n\r\ndim(CHC)\r\ncomm = CHC[ ,5:21]\r\nsimpcomm= simper(comm, group = CHC$method)\r\nsummary(simpcomm)\r\nlapply(simpcomm, FUN=function(x){x$overall})\r\n\r\n\r\n#### statistical and NMDS graphing approach based on Bruckner and Heethoff, 2016 in Chemoecology ########\r\n\r\nmy.data = read.csv(""body_abdomen_wing_permanova.csv"")\r\n# Calculating the distance/dissimilarity matrix -> Euclidean/Bray-Curtis\r\ndistm1 <- vegdist(my.data[,-1], method=""bray"")\r\nmean(distm1)  ### Mean distance of all data in the distance matrix \r\n\r\n\r\n### Permutational multivariate analysis of variance  -> PERMANOVA (Anderson 2001) ###\r\nPERMANOVA <- adonis(distm1 ~ my.data$method, permutations=10000)\r\nPERMANOVA\r\n\r\npairwise.perm.manova(distm1,my.data$method,nperm=10000, p.method = ""fdr"" )\r\n\r\nmds1<-metaMDS(my.data[,-1],""bray"",2)\r\nmds1$stress\r\n\r\nplot(mds1, type=""n"")\r\n#mds1 = ordiplot3d(mds1, angle=45)\r\ncols<-c(""red"", ""blue"", ""darkorchid"", ""darkorange3"", ""chartreuse3"",""darkgreen"",""lightblue"",""darkred"")\r\npoints(mds1, col=cols[my.data$method],pch=c(8,17,16,15,18,20,19,22)[unclass(my.data$method)], cex=1.5)\r\n\r\nef <- envfit(mds1, my.data[,-1], display=""sites"", permu=10000)\r\nplot(ef, col=""black"", p.max=0.001, cex=0.75) \r\n\r\n\r\n######### run statistical tests on specific CHC differences\r\n\r\nCHC = read.csv(""body_abdomen_wing_histogram.csv"")\r\n\r\n#C21\r\npeak= CHC$C21 \r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C23:1a\r\npeak= CHC$C23.1a \r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n\r\n#C23:1b\r\npeak= CHC$C231.b \r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C23\r\npeak= CHC$C23\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C24.1\r\npeak= CHC$C24.1\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C24\r\npeak= CHC$C24\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C25.1\r\npeak= CHC$C25.1\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C25\r\npeak= CHC$C25\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C26.1\r\npeak= CHC$C26.1\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C26\r\npeak= CHC$C26\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C27.1\r\npeak= CHC$C27.1\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C27\r\npeak= CHC$C27\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C29.1\r\npeak= CHC$C29.1\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C29\r\npeak= CHC$C29\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C31.1\r\npeak= CHC$C31.1\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C31\r\npeak= CHC$C31\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n#C33.1\r\npeak= CHC$C33.1\r\nmethod = CHC$method_numbered\r\n\r\nkruskal.test(peak~method, data = CHC)\r\nSteel.Dwass(peak, group = method )\r\n\r\n']","Data for: Social behavior, ovary size, and population of origin influence cuticular hydrocarbons in the orchid bee, Euglossa dilemma Cuticular hydrocarbons (CHCs) are waxy compounds on the surface of insects that prevent desiccation and frequently serve as chemical signals mediating social and mating behaviors. Although their function in eusocial species has been heavily investigated, little is known about the evolution of CHC-based communication in species with simpler forms of social organization lacking specialized castes. Here, we investigate factors shaping CHC variation in the orchid bee Euglossa dilemma, which forms casteless social groups of 2-3 individuals. We first assess geographic variation, examining CHC profiles of males and females from three populations. We also consider CHC variation in the sister species, Euglossa viridissima, which occurs sympatrically with one population of E. dilemma. Next, we consider variation associated with female behavioral phases, to test the hypothesis that CHCs reflect ovary size and social dominance. We uncover a striking CHC polymorphism in E. dilemma spanning populations. In addition, we identify a separate set of CHCs that correlate with ovary size, social dominance, and expression of genes associated with social behavior, suggesting that CHCs convey reproductive and social information in E. dilemma. Together, our results reveal complex patterns of variation in which a subset of CHCs reflect the social and reproductive status of nestmates.",2
Data from: Sexual selection in complex communities: integrating interspecific reproductive interference in structured populations,"The social structure of populations plays a key role in shaping variation in sexual selection. In nature, sexual selection occurs in communities of interacting species, however heterospecifics are rarely included in characterisations of social structure. Heterospecifics can influence the reproductive outcomes of intrasexual competition by interfering with intraspecific sexual interactions (interspecific reproductive interference; IRI). We outline the need for studies of sexual selection to incorporate heterospecifics as part of the social environment. We use simulations to show that classic predictions for the effect of social structure on sexual selection are altered by an interaction between social structure and IRI. This interaction has wide-ranging implications for patterns of sexual conflict and kin-selected reproductive strategies in socially structured populations. Our work bridges the gap between sexual selection research on social structure and IRI, and highlights future directions to study sexual selection in interacting communities.","['\n\n# packages\nlibrary(ggplot2)\nlibrary(plyr)\n\n\n# Lets try a two trait model\n# for negative, random and positive assortment\n\n# (A)  matching selection \n# (i.e. the most competitive trait in intraspecific competition is the most competitive trait when competing with heterospecifics.\n#\n#\n# (1) with 2000 intraspecific competitors at a 50:50 ratio\n#     with zero interspecific competitors\n# (2) with 1500 intraspecific competitors at a 50:50 ratio\n#     with 500 random interspecific competitors (i.e. 25% inter)\n# (3) with 1500 intraspecific competitors at a 50:50 ratio\n#     with 500 random intraspecific competitors removed (i.e. 25%)\n\n\n# (B)  contrasting selection \n# (i.e. the most competitive trait in intraspecific competition is the least competitive trait when competing with heterospecifics.\n#\n#\n# (1) with 1500 intraspecific competitors at a 50:50 ratio\n#     with 500 random interspecific competitors (i.e. 25% inter)\n\n\n# (C)\n#\n#\n# plot.\n\n########################\n# (A1) All Intraspecific \n########################\n\n\n# negative assortment\n# set seed\nset.seed(1986)\n# results\nneg_intra <- rep(NA, 1000)\nfor(j in 1:length(neg_intra)){\n# create 1000 focal individuals, 50:50 with traits 0 or 1\nfocal <-  rep(0:1,each = 500)\n# create 1000 competitor individuals,  50:50 with traits 0 or 1\ncomp <-  rep(1:0, each = 500)\n# pair them in dyads with negative assortment\nnegative1 <-  data.frame(focal = focal, comp = comp)\n# duplicate dyads so we have each pair represented twice\n# with each individual being the focal\nnegative2 <- data.frame(focal = comp, comp = focal)\nnegative <- rbind(negative1, negative2)\n# calculate fintess (W) based on traits\nnegative$W <- ifelse(negative$focal > negative$comp, 2,\n                ifelse(negative$focal == negative$comp, 1, 0) )\n# calculate relative fitness\nw <- negative$W/mean(negative$W)\n# mean standardised trait\nz <- negative$focal/mean(negative$focal)\n# mean standardised selection gradient\nneg_intra[[j]] <- coef(lm(w ~ z))[[2]]\n}\n\n\n# random\n# set seed\nset.seed(1986)\n# results\nran_intra <- rep(NA, 1000)\nfor(j in 1:length(ran_intra)){\n# create 1000 focal individuals, randomly with traits 0 or 1\nfocal <-  sample(0:1, 1000, replace = TRUE)\n# pair them with the a competitor, ensuring 50:50 traits 0 or 2\n# so each each dyad is represented once\ncomp <- sample(c(rep(0, 1000-sum(focal == 0)), rep(1, 1000-sum(focal == 1)) ))\nrandom1 <-  data.frame(focal = focal, comp = comp)\n# calculate fintess (W) based on traits\nrandom1$W <- ifelse(random1$focal > random1$comp, 2,\n                ifelse(random1$focal == random1$comp, 1, 0) )\n# create a data.frame with the other 1000 competitors as focals\nrandom2 <- data.frame(focal = comp, comp = focal)\n# and calculate fintess based on previous outcome\nrandom2$W <- ifelse(random1$W == 2, 0, ifelse(random1$W == 0, 2, 1) )\n# bind them together so we have each pair represented twice\n# with each individual being the focal\nrandom <- rbind(random1, random2)\n# calculate relative fitness\nw <- random$W/mean(random$W)\n# mean standardised trait\nz <- random$focal/mean(random$focal)\n# mean standardised selection gradient\nran_intra[[j]] <- coef(lm(w ~ z))[[2]]\n}\n\n\n# positive\n# set seed\nset.seed(1986)\n# results\npos_intra <- rep(NA, 1000)\nfor(j in 1:length(pos_intra)){\n# create 1000 focal individuals, 50:50 with traits 0 or 1\nfocal <-  rep(0:1,each = 500)\n# create 1000 competitor indidivuals,  50:50 with traits 0 or 1\ncomp <-  rep(0:1, each = 500)\n# pair them in dyads with positive assortment\npositive1 <-  data.frame(focal = focal, comp = comp)\n# duplicate dyads so we have each pair represented twice\n# with each individual being the focal\npositive2 <- data.frame(focal = comp, comp = focal)\npositive <- rbind(positive1, positive2)\n# calculate fintess (W) based on traits\npositive$W <- ifelse(positive$focal > positive$comp, 2,\n                ifelse(positive$focal == positive$comp, 1, 0) )\n# calculate relative fitness\nw <- positive$W/mean(positive$W)\n# mean standardised trait\nz <- positive$focal/mean(positive$focal)\n# mean standardised selection gradient\npos_intra[[j]] <- coef(lm(w ~ z))[[2]]\n}\n\n\n\n# have a look\npar(mfrow=c(1,3))\nplot(neg_intra, ylim = c(-2,2))\nplot(ran_intra, ylim = c(-2,2))\nplot(pos_intra, ylim = c(-2,2))\n\n\n\n################################################################################\n# (A2) With interspecific\n################################################################################\n\n\n\n# negative\n# set seed\nset.seed(1986)\n# results\nneg_inter <- rep(NA, 1000)\nfor(j in 1:length(neg_inter)){\n# create 1000 focal individuals, 50:50 with traits 0 or 1\nfocal <-  rep(0:1,each = 500)\n# create 1000 competitor individuals,  50:50 with traits 0 or 1\ncomp <-  rep(1:0, each = 500)\n# pair them in dyads with negative assortment\nnegative1 <-  data.frame(focal = focal, comp = comp)\n# sample 500 dyads where a focal or competitor will be swapped\ndyads <- sample(1:1000, 500)\n# create a vector to hold the which individual to swap\nswaps <- rep(NA, 500)\nfor(i in 1:length(dyads)){\n# 1 = column 1 which is focal\n# 2 = column 2 which is competitor\nswaps[i] <- sample( c(1, 2), 1 )\n}\n# now select new interspecific traits also from a population with 50:50 0 or 1\ninter_traits <- sample(rep(0:1,each = 1000), 500)\n# now change the traits of the swapped individuals\nfor(k in 1:length(dyads)){\nnegative1[, swaps[k] ][ dyads[k] ] <- inter_traits[k]\n}\n# note which individual was swapped\nnegative1$swapped_focal <- NA\nnegative1$swapped_focal[dyads] <- ifelse(swaps == 1, ""focal_swapped"", NA)\n# duplicate dyads so we have each pair represented twice\n# with each individual being the focal\nnegative2 <- data.frame(focal = negative1$comp, \n                        comp = negative1$focal\n                        )\n# note which individual was swapped\nnegative2$swapped_focal <- NA\nnegative2$swapped_focal[dyads] <- ifelse(swaps == 2, ""focal_swapped"", NA)                                   \n# add together so each pair represented twice, each individual being the focal\nnegative <- rbind(negative1, negative2)\n# calculate fintess (W) based on traits\nnegative$W <- ifelse(negative$focal > negative$comp, 2,\n                ifelse(negative$focal == negative$comp, 1, 0) )\n# subset only focal individuals that are focal species\nnegative <- negative[ is.na(negative$swapped_focal) ,]\n# calculate relative fitness\nw <- negative$W/mean(negative$W)\n# mean standardised trait\nz <- negative$focal/mean(negative$focal)\n# mean standardised selection gradient\nneg_inter[[j]] <- coef(lm(w ~ z))[[2]]\n}\n\n\n\n# random\n# set seed\nset.seed(1986)\n# results\nran_inter <- rep(NA, 1000)\nfor(j in 1:length(ran_inter)){\n# create 1000 focal individuals, randomly with traits 0 or 1\nfocal <-  sample(0:1, 1000, replace = TRUE)\n# pair them with the a competitor, ensuring 50:50 traits 0 or 2\n# so each each dyad is represented once\ncomp <- sample(c(rep(0, 1000-sum(focal == 0)), rep(1, 1000-sum(focal == 1)) ) )\nrandom1 <-  data.frame(focal = focal, comp = comp)\n# sample 500 dyads where a focal or competitor will be swapped\ndyads <- sample(1:1000, 500)\n# create a vector to hold the which individual to swap\nswaps <- rep(NA, 500)\nfor(i in 1:length(dyads)){\n# 1 = column 1 which is focal\n# 2 = column 2 which is competitor\nswaps[i] <- sample( c(1, 2), 1 )\n}\n# now select new interspecific traits also from a population with 50:50  0 or 1\ninter_traits <- sample(rep(0:1,each = 1000), 500)\n# now change the traits of the swapped individuals\nfor(k in 1:length(dyads)){\nrandom1[, swaps[k] ][ dyads[k] ] <- inter_traits[k]\n}\n# note which individual was swapped\nrandom1$swapped_focal <- NA\nrandom1$swapped_focal[dyads] <- ifelse(swaps == 1, ""focal_swapped"", NA)\n# duplicate dyads so we have each pair represented twice\n# with each individual being the focal\nrandom2 <- data.frame(focal = random1$comp, \n                        comp = random1$focal\n                        )\n# note which individual was swapped\nrandom2$swapped_focal <- NA\nrandom2$swapped_focal[dyads] <- ifelse(swaps == 2, ""focal_swapped"", NA)                                   \n# add together so each pair represented twice, each individual being the focal\nrandom <- rbind(random1, random2)\n# calculate fintess (W) based on traits\nrandom$W <- ifelse(random$focal > random$comp, 2,\n                ifelse(random$focal == random$comp, 1, 0) )\n# subset only focal individuals that are focal species\nrandom <- random[ is.na(random$swapped_focal) ,]\n# calculate relative fitness\nw <- random$W/mean(random$W)\n# mean standardised trait\nz <- random$focal/mean(random$focal)\n# mean standardised selection gradient\nran_inter[[j]] <- coef(lm(w ~ z))[[2]]\n}\n\n\n\n# positive\n# set seed\nset.seed(1986)\n# results\npos_inter <- rep(NA, 1000)\nfor(j in 1:length(pos_inter)){\n# create 1000 focal individuals, 50:50 with traits 0 or 1\nfocal <-  rep(0:1,each = 500)\n# create 1000 competitor indidivuals,  50:50 with traits 0 or 1\ncomp <-  rep(0:1, each = 500)\n# pair them in dyads with positive assortment\npositive1 <-  data.frame(focal = focal, comp = comp)\n# sample 500 dyads where a focal or competitor will be swapped\ndyads <- sample(1:1000, 500)\n# create a vector to hold the which individual to swap\nswaps <- rep(NA, 500)\nfor(i in 1:length(dyads)){\n# 1 = column 1 which is focal\n# 2 = column 2 which is competitor\nswaps[i] <- sample( c(1, 2), 1 )\n}\n# now select new interspecific traits also from a population with 50:50  0 or 1\ninter_traits <- sample(rep(0:1,each = 1000), 500)\n# now change the traits of the swapped individuals\nfor(k in 1:length(dyads)){\npositive1[, swaps[k] ][ dyads[k] ] <- inter_traits[k]\n}\n# note which individual was swapped\npositive1$swapped_focal <- NA\npositive1$swapped_focal[dyads] <- ifelse(swaps == 1, ""focal_swapped"", NA)\n# duplicate dyads so we have each pair represented twice\n# with each individual being the focal\npositive2 <- data.frame(focal = positive1$comp, \n                        comp = positive1$focal\n                        )\n# note which individual was swapped\npositive2$swapped_focal <- NA\npositive2$swapped_focal[dyads] <- ifelse(swaps == 2, ""focal_swapped"", NA)                                   \n# add together so each pair represented twice, each individual being the focal\npositive <- rbind(positive1, positive2)\n# calculate fintess (W) based on traits\npositive$W <- ifelse(positive$focal > positive$comp, 2,\n                ifelse(positive$focal == positive$comp, 1, 0) )\n# subset only focal individuals that are focal species\npositive <- positive[ is.na(positive$swapped_focal) ,]\n# calculate relative fitness\nw <- positive$W/mean(positive$W)\n# mean standardised trait\nz <- positive$focal/mean(positive$focal)\n# mean standardised selection gradient\npos_inter[[j]] <- coef(lm(w ~ z))[[2]]\n}\n\n\n# have a look\npar(mfrow=c(1,3))\nplot(neg_inter, ylim = c(-2,2))\nplot(ran_inter, ylim = c(-2,2))\nplot(pos_inter, ylim = c(-2,2))\n\n\n\n\n############################\n# (A3) Intraspecific control\n#############################\n\n# do everything as for inter\n# i.e. pick random individuals\n# mark them as interspecifics\n# exclude them from analyses\n# only thing that is different is that we do not change the traits of individuals\n# this allows us to account for any differences in selection gradients that may occur from random removal of 500 individuals in interspecific\n\n\n# negative\n# set seed\nset.seed(1986)\n# results\nneg_intra_control <- rep(NA, 1000)\nfor(j in 1:length(neg_intra_control)){\n# create 1000 focal individuals, 50:50 with traits 0 or 1\nfocal <-  rep(0:1,each = 500)\n# create 1000 competitor individuals,  50:50 with traits 0 or 1\ncomp <-  rep(1:0, each = 500)\n# pair them in dyads with negative assortment\nnegative1 <-  data.frame(focal = focal, comp = comp)\n# sample 500 dyads where a focal or competitor will be swapped\ndyads <- sample(1:1000, 500)\n# create a vector to hold the which individual to swap\nswaps <- rep(NA, 500)\nfor(i in 1:length(dyads)){\n# 1 = column 1 which is focal\n# 2 = column 2 which is competitor\nswaps[i] <- sample( c(1, 2), 1 )\n}\n# \n#\n# \n#\n#\n#\n# note which individual was swapped\nnegative1$swapped_focal <- NA\nnegative1$swapped_focal[dyads] <- ifelse(swaps == 1, ""focal_swapped"", NA)\n# duplicate dyads so we have each pair represented twice\n# with each individual being the focal\nnegative2 <- data.frame(focal = negative1$comp, \n                        comp = negative1$focal\n                        )\n# note which individual was swapped\nnegative2$swapped_focal <- NA\nnegative2$swapped_focal[dyads] <- ifelse(swaps == 2, ""focal_swapped"", NA)                                   \n# add together so each pair represented twice, each individual being the focal\nnegative <- rbind(negative1, negative2)\n# calculate fintess (W) based on traits\nnegative$W <- ifelse(negative$focal > negative$comp, 2,\n                ifelse(negative$focal == negative$comp, 1, 0) )\n# subset only focal individuals that are focal species\nnegative <- negative[ is.na(negative$swapped_focal) ,]\n# calculate relative fitness\nw <- negative$W/mean(negative$W)\n# mean standardised trait\nz <- negative$focal/mean(negative$focal)\n# mean standardised selection gradient\nneg_intra_control[[j]] <- coef(lm(w ~ z))[[2]]\n}\n\n\n\n\n# random\n# set seed\nset.seed(1986)\n# results\nran_intra_control <- rep(NA, 1000)\nfor(j in 1:length(ran_intra_control)){\n# create 1000 focal individuals, randomly with traits 0 or 1\nfocal <-  sample(0:1, 1000, replace = TRUE)\n# pair them with the a competitor, ensuring 50:50 traits 0 or 2\n# so each each dyad is represented once\ncomp <- sample(c(rep(0, 1000-sum(focal == 0)), rep(1, 1000-sum(focal == 1)) ) )\nrandom1 <-  data.frame(focal = focal, comp = comp)\n# sample 500 dyads where a focal or competitor will be swapped\ndyads <- sample(1:1000, 500)\n# create a vector to hold the which individual to swap\nswaps <- rep(NA, 500)\nfor(i in 1:length(dyads)){\n# 1 = column 1 which is focal\n# 2 = column 2 which is competitor\nswaps[i] <- sample( c(1, 2), 1 )\n}\n# \n#\n# \n#\n#\n#\n# note which individual was swapped\nrandom1$swapped_focal <- NA\nrandom1$swapped_focal[dyads] <- ifelse(swaps == 1, ""focal_swapped"", NA)\n# duplicate dyads so we have each pair represented twice\n# with each individual being the focal\nrandom2 <- data.frame(focal = random1$comp, \n                        comp = random1$focal\n                        )\n# note which individual was swapped\nrandom2$swapped_focal <- NA\nrandom2$swapped_focal[dyads] <- ifelse(swaps == 2, ""focal_swapped"", NA)                                   \n# add together so each pair represented twice, each individual being the focal\nrandom <- rbind(random1, random2)\n# calculate fintess (W) based on traits\nrandom$W <- ifelse(random$focal > random$comp, 2,\n                ifelse(random$focal == random$comp, 1, 0) )\n# subset only focal individuals that are focal species\nrandom <- random[ is.na(random$swapped_focal) ,]\n# calculate relative fitness\nw <- random$W/mean(random$W)\n# mean standardised trait\nz <- random$focal/mean(random$focal)\n# mean standardised selection gradient\nran_intra_control[[j]] <- coef(lm(w ~ z))[[2]]\n}\n\n\n\n\n# positive\n# set seed\nset.seed(1986)\n# results\npos_intra_control <- rep(NA, 1000)\nfor(j in 1:length(pos_intra_control)){\n# create 1000 focal individuals, 50:50 with traits 0 or 1\nfocal <-  rep(0:1,each = 500)\n# create 1000 competitor indidivuals,  50:50 with traits 0 or 1\ncomp <-  rep(0:1, each = 500)\n# pair them in dyads with positive assortment\npositive1 <-  data.frame(focal = focal, comp = comp)\n# sample 500 dyads where a focal or competitor will be swapped\ndyads <- sample(1:1000, 500)\n# create a vector to hold the which individual to swap\nswaps <- rep(NA, 500)\nfor(i in 1:length(dyads)){\n# 1 = column 1 which is focal\n# 2 = column 2 which is competitor\nswaps[i] <- sample( c(1, 2), 1 )\n}\n# \n#\n# \n#\n#\n#\n# note which individual was swapped\npositive1$swapped_focal <- NA\npositive1$swapped_focal[dyads] <- ifelse(swaps == 1, ""focal_swapped"", NA)\n# duplicate dyads so we have each pair represented twice\n# with each individual being the focal\npositive2 <- data.frame(focal = positive1$comp, \n                        comp = positive1$focal\n                        )\n# note which individual was swapped\npositive2$swapped_focal <- NA\npositive2$swapped_focal[dyads] <- ifelse(swaps == 2, ""focal_swapped"", NA)                                   \n# add together so each pair represented twice, each individual being the focal\npositive <- rbind(positive1, positive2)\n# calculate fintess (W) based on traits\npositive$W <- ifelse(positive$focal > positive$comp, 2,\n                ifelse(positive$focal == positive$comp, 1, 0) )\n# subset only focal individuals that are focal species\npositive <- positive[ is.na(positive$swapped_focal) ,]\n# calculate relative fitness\nw <- positive$W/mean(positive$W)\n# mean standardised trait\nz <- positive$focal/mean(positive$focal)\n# mean standardised selection gradient\npos_intra_control[[j]] <- coef(lm(w ~ z))[[2]]\n}\n\n\n\n# have a look\npar(mfrow=c(1,3))\nplot(neg_intra_control, ylim = c(-2,2))\nplot(ran_intra_control, ylim = c(-2,2))\nplot(pos_intra_control, ylim = c(-2,2))\n\n\n######################################\n# (B1) contrasting with interspecific \n######################################\n\n\n# negative\n# set seed\nset.seed(1986)\n# results\nneg_inter_opp <- rep(NA, 1000)\nfor(j in 1:length(neg_inter_opp)){\n# create 1000 focal individuals, 50:50 with traits 0 or 1\nfocal <-  rep(0:1,each = 500)\n# create 1000 competitor individuals,  50:50 with traits 0 or 1\ncomp <-  rep(1:0, each = 500)\n# pair them in dyads with negative assortment\nnegative1 <-  data.frame(focal = focal, comp = comp)\n# sample 500 dyads where a focal or competitor will be swapped\ndyads <- sample(1:1000, 500)\n# create a vector to hold the which individual to swap\nswaps <- rep(NA, 500)\nfor(i in 1:length(dyads)){\n# 1 = column 1 which is focal\n# 2 = column 2 which is competitor\nswaps[i] <- sample( c(1, 2), 1 )\n}\n# now select new interspecific traits also from a population with 50:50  0 or 1\ninter_traits <- sample(rep(0:1,each = 1000), 500)\n# now change the traits of the swapped individuals\nfor(k in 1:length(dyads)){\nnegative1[, swaps[k] ][ dyads[k] ] <- inter_traits[k]\n}\n# note which individual was swapped\nnegative1$swapped_focal <- NA\nnegative1$swapped_focal[dyads] <- ifelse(swaps == 1, ""focal_swapped"", NA)\n# duplicate dyads so we have each pair represented twice\n# with each individual being the focal\nnegative2 <- data.frame(focal = negative1$comp, \n                        comp = negative1$focal\n                        )\n# note which individual was swapped\nnegative2$swapped_focal <- NA\nnegative2$swapped_focal[dyads] <- ifelse(swaps == 2, ""focal_swapped"", NA)                                   \n#\n#\n#\n# calculate fintess (W) based on traits for all dyads in one direction\n# >>> for intraspecific competition\nnegative1$W <- ifelse(negative1$focal > negative1$comp, 2,\n                ifelse(negative1$focal == negative1$comp, 1, 0) )\n# >>> correct this for interspecific dyads\nnegative1$W[dyads] <- \n  ifelse(negative1$focal[dyads]  < negative1$comp[dyads], 2,\n    ifelse(negative1$focal[dyads]  == negative1$comp[dyads] , 1, 0) )\n# replicate this fitness for all dyads in other direction\n# >>> for intraspecific competition\nnegative2$W <- ifelse(negative2$focal > negative2$comp, 2,\n                ifelse(negative2$focal == negative2$comp, 1, 0) )\n# >>> correct this for interspecific dyads\nnegative2$W[dyads] <- \n  ifelse(negative2$focal[dyads]  < negative2$comp[dyads], 2,\n    ifelse(negative2$focal[dyads]  == negative2$comp[dyads] , 1, 0) )\n# bins together\nnegative <- rbind(negative1, negative2)\n#\n#\n# subset only focal individuals that are focal species\nnegative <- negative[ is.na(negative$swapped_focal) ,]\n# calculate relative fitness\nw <- negative$W/mean(negative$W)\n# mean standardised trait\nz <- negative$focal/mean(negative$focal)\n# mean standardised selection gradient\nneg_inter_opp[[j]] <- coef(lm(w ~ z))[[2]]\n}\n\n\n# random\n# set seed\nset.seed(1986)\n# results\nran_inter_opp <- rep(NA, 1000)\nfor(j in 1:length(ran_inter_opp)){\n# create 1000 focal individuals, randomly with traits 0 or 1\nfocal <-  sample(0:1, 1000, replace = TRUE)\n# pair them with the a competitor, ensuring 50:50 traits 0 or 2\n# so each each dyad is represented once\ncomp <- sample(c(rep(0, 1000-sum(focal == 0)), rep(1, 1000-sum(focal == 1)) ) )\nrandom1 <-  data.frame(focal = focal, comp = comp)\n# sample 500 dyads where a focal or competitor will be swapped\ndyads <- sample(1:1000, 500)\n# create a vector to hold the which individual to swap\nswaps <- rep(NA, 500)\nfor(i in 1:length(dyads)){\n# 1 = column 1 which is focal\n# 2 = column 2 which is competitor\nswaps[i] <- sample( c(1, 2), 1 )\n}\n# now select new interspecific traits also from a population with 50:50  0 or 1\ninter_traits <- sample(rep(0:1,each = 1000), 500)\n# now change the traits of the swapped individuals\nfor(k in 1:length(dyads)){\nrandom1[, swaps[k] ][ dyads[k] ] <- inter_traits[k]\n}\n# note which individual was swapped\nrandom1$swapped_focal <- NA\nrandom1$swapped_focal[dyads] <- ifelse(swaps == 1, ""focal_swapped"", NA)\n# duplicate dyads so we have each pair represented twice\n# with each individual being the focal\nrandom2 <- data.frame(focal = random1$comp, \n                        comp = random1$focal\n                        )\n# note which individual was swapped\nrandom2$swapped_focal <- NA\nrandom2$swapped_focal[dyads] <- ifelse(swaps == 2, ""focal_swapped"", NA)                                   \n#\n#\n#\n# calculate fintess (W) based on traits for all dyads in one direction\n# >>> for intraspecific competition\nrandom1$W <- ifelse(random1$focal > random1$comp, 2,\n                ifelse(random1$focal == random1$comp, 1, 0) )\n# >>> correct this for interspecific dyads\nrandom1$W[dyads] <- \n  ifelse(random1$focal[dyads]  < random1$comp[dyads], 2,\n    ifelse(random1$focal[dyads]  == random1$comp[dyads] , 1, 0) )\n# replicate this fitness for all dyads in other direction\n# >>> for intraspecific competition\nrandom2$W <- ifelse(random2$focal > random2$comp, 2,\n                ifelse(random2$focal == random2$comp, 1, 0) )\n# >>> correct this for interspecific dyads\nrandom2$W[dyads] <- \n  ifelse(random2$focal[dyads]  < random2$comp[dyads], 2,\n    ifelse(random2$focal[dyads]  == random2$comp[dyads] , 1, 0) )\n# bins together\nrandom <- rbind(random1, random2)\n#\n#\n# subset only focal individuals that are focal species\n# subset only focal individuals that are focal species\nrandom <- random[ is.na(random$swapped_focal) ,]\n# calculate relative fitness\nw <- random$W/mean(random$W)\n# mean standardised trait\nz <- random$focal/mean(random$focal)\n# mean standardised selection gradient\nran_inter_opp[[j]] <- coef(lm(w ~ z))[[2]]\n}\n\n\n\n# positive\n# set seed\nset.seed(1986)\n# results\npos_inter_opp <- rep(NA, 1000)\nfor(j in 1:length(pos_inter_opp)){\n# create 1000 focal individuals, 50:50 with traits 0 or 1\nfocal <-  rep(0:1,each = 500)\n# create 1000 competitor indidivuals,  50:50 with traits 0 or 1\ncomp <-  rep(0:1, each = 500)\n# pair them in dyads with positive assortment\npositive1 <-  data.frame(focal = focal, comp = comp)\n# sample 500 dyads where a focal or competitor will be swapped\ndyads <- sample(1:1000, 500)\n# create a vector to hold the which individual to swap\nswaps <- rep(NA, 500)\nfor(i in 1:length(dyads)){\n# 1 = column 1 which is focal\n# 2 = column 2 which is competitor\nswaps[i] <- sample( c(1, 2), 1 )\n}\n# now select new interspecific traits also from a population with 50:50  0 or 1\ninter_traits <- sample(rep(0:1,each = 1000), 500)\n# now change the traits of the swapped individuals\nfor(k in 1:length(dyads)){\npositive1[, swaps[k] ][ dyads[k] ] <- inter_traits[k]\n}\n# note which individual was swapped\npositive1$swapped_focal <- NA\npositive1$swapped_focal[dyads] <- ifelse(swaps == 1, ""focal_swapped"", NA)\n# duplicate dyads so we have each pair represented twice\n# with each individual being the focal\npositive2 <- data.frame(focal = positive1$comp, \n                        comp = positive1$focal\n                        )\n# note which individual was swapped\npositive2$swapped_focal <- NA\npositive2$swapped_focal[dyads] <- ifelse(swaps == 2, ""focal_swapped"", NA)                                   \n#\n#\n#\n# calculate fintess (W) based on traits for all dyads in one direction\n# >>> for intraspecific competition\npositive1$W <- ifelse(positive1$focal > positive1$comp, 2,\n                ifelse(positive1$focal == positive1$comp, 1, 0) )\n# >>> correct this for interspecific dyads\npositive1$W[dyads] <- \n  ifelse(positive1$focal[dyads]  < positive1$comp[dyads], 2,\n    ifelse(positive1$focal[dyads]  == positive1$comp[dyads] , 1, 0) )\n# replicate this fitness for all dyads in other direction\n# >>> for intraspecific competition\npositive2$W <- ifelse(positive2$focal > positive2$comp, 2,\n                ifelse(positive2$focal == positive2$comp, 1, 0) )\n# >>> correct this for interspecific dyads\npositive2$W[dyads] <- \n  ifelse(positive2$focal[dyads]  < positive2$comp[dyads], 2,\n    ifelse(positive2$focal[dyads]  == positive2$comp[dyads] , 1, 0) )\n# bins together\npositive <- rbind(positive1, positive2)\n#\n#\n# subset only focal individuals that are focal species\npositive <- positive[ is.na(positive$swapped_focal) ,]\n# calculate relative fitness\nw <- positive$W/mean(positive$W)\n# mean standardised trait\nz <- positive$focal/mean(positive$focal)\n# mean standardised selection gradient\npos_inter_opp[[j]] <- coef(lm(w ~ z))[[2]]\n}\n\n\n# have a look\npar(mfrow=c(1,3))\nplot(neg_inter_opp, ylim = c(-2,2))\nplot(ran_inter_opp, ylim = c(-2,2))\nplot(pos_inter_opp, ylim = c(-2,2))\n\n\n\n########################\n# (C) Plot\n########################\n\n# quick look\npar(mfrow=c(2,2))\n# intra\nplot(neg_intra, ylim = c(-1.5, 1.5), col = ""purple"")\npoints(ran_intra, ylim = c(-1.5, 1.5))\npoints(pos_intra, ylim = c(-1.5, 1.5), col = ""green"")\ntext(500,1.4,""intraspecfic only, matching"")\n# inter\nplot(neg_inter, ylim = c(-1.5, 1.5), col = ""purple"")\npoints(ran_inter, ylim = c(-1.5, 1.5))\npoints(pos_inter, ylim = c(-1.5, 1.5), col = ""green"")\ntext(500,1.4,""interspecfic, matching"")\n# intra control\nplot(neg_intra_control, ylim = c(-1.5, 1.5), col = ""purple"")\npoints(ran_intra_control, ylim = c(-1.5, 1.5))\npoints(pos_intra_control, ylim = c(-1.5, 1.5), col = ""green"")\ntext(500,1.4,""intraspecfic control, matching"")\n# inter opp\nplot(neg_inter_opp, ylim = c(-1.5, 1.5), col = ""purple"")\npoints(ran_inter_opp, ylim = c(-1.5, 1.5))\npoints(pos_inter_opp, ylim = c(-1.5, 1.5), col = ""green"")\ntext(500,1.4,""interspecfic contrasting"")\n\n\n# organise data for combined plot\n# data\n# intraspecific\nintra <- data.frame(B = c(neg_intra, ran_intra, pos_intra),\n                    r = rep(c(""Negative"", ""Random"", ""Positive""), each = 1000),\n                    sim = rep(""Intraspecfic"")\n                    )\n# interspecific matching\ninter <- data.frame(B = c(neg_inter, ran_inter, pos_inter),\n                    r = rep(c(""Negative"", ""Random"", ""Positive""), each = 1000),\n                    sim = rep(""Interspecfic_matching"")\n                    )\n# intraspecific control\nintra_con <- data.frame(B = c(neg_intra_control, ran_intra_control,\n                              pos_intra_control),\n                        r = rep(c(""Negative"",""Random"",""Positive""), each = 1000),\n                        sim = rep(""Intraspecfic control"")\n                        )\n# interspecific contrasting\ninter_opp <- data.frame(B = c(neg_inter_opp, ran_inter_opp, pos_inter_opp),\n                    r = rep(c(""Negative"", ""Random"", ""Positive""), each = 1000),\n                    sim = rep(""Interspecfic_contrasting"")\n                    )        \n\n# combine all data\nalldata <- rbind(intra,inter,intra_con,inter_opp)\n\n# look\nggplot(alldata, aes(x = r,  y = B)) +\n  geom_point() +\n  facet_wrap(~ sim)\n\n# look\nggplot(alldata, aes(x = r,  y = B)) +\n  geom_boxplot(aes(colour = sim), position  = ""dodge"")\n\n# gets mean and 95% range for all simulations\nalldata_means <- ddply(alldata, .(sim, r), summarise,\n                       means = mean(B),\n                       U = quantile(B, 0.975),\n                       L = quantile(B, 0.025)\n                       )\n\n# look at quantiles\nquantiles_gg <- ggplot(alldata_means, aes(x = r,  y = means, group = sim)) +\n                  geom_errorbar(aes(ymin = L, ymax = U), \n                                width = 0.3, \n                                position = position_dodge(width = 1)) +\n                  geom_point(aes(colour = sim), \n                             position = position_dodge(width = 1))\n# plot\nprint(quantiles_gg)\n\n# change order for plot\nalldata_means$r <- factor(alldata_means$r, \n                     levels = c(""Positive"",""Random"",""Negative""))\nalldata_means$sim <- factor(alldata_means$sim, \n                     levels = c(""Intraspecfic"",\n                                ""Intraspecfic control"",\n                                ""Interspecfic_matching"",\n                                ""Interspecfic_contrasting""))\n# look again                    \nquantiles_gg <- ggplot(alldata_means, aes(x = r,  y = means, group = sim)) +\n                  geom_errorbar(aes(ymin = L, ymax = U), \n                                width = 0.8, \n                                position = position_dodge(width = 1)) +\n                  geom_point(aes(fill = sim), shape = 21, size = 3,\n                             position = position_dodge(width = 1)) +\n                  scale_fill_manual("""", values = c(""white"",""grey"",\n                                                   ""black"",""blue"")) +\n                  ylab(""Selection gradient"") + \n                  xlab(""Assortment"")\n# plot\nprint(quantiles_gg)\n\n\n\n\n## plot only the control as the final point\ninter_same <- data.frame(B = c(neg_inter, ran_inter, pos_inter),\n                    r = rep(c(""Negative"", ""Random"", ""Positive""), each = 1000),\n                    sim = rep(""Intra- & Interspecific (matching)"")\n                    )\nintra_con <- data.frame(B = c(neg_intra_control, ran_intra_control,\n                              pos_intra_control),\n                        r = rep(c(""Negative"",""Random"",""Positive""), each = 1000),\n                        sim = rep(""Intraspecific"")\n                        )\ninter_opp <- data.frame(B = c(neg_inter_opp, ran_inter_opp, pos_inter_opp),\n                    r = rep(c(""Negative"", ""Random"", ""Positive""), each = 1000),\n                    sim = rep(""Intra- & Interspecific (contrasting)"")\n                    )\n# add together\ninter_n_con <- rbind(inter_same,intra_con,inter_opp)\n# gets 95% range \ninter_n_con_means <- ddply(inter_n_con, .(sim, r), summarise,\n                       means = mean(B),\n                       U = quantile(B, 0.975),\n                       L = quantile(B, 0.025)\n                       )\n# order levels\ninter_n_con_means$r <- factor(inter_n_con_means$r, \n                     levels = c(""Positive"",""Random"",""Negative""))\ninter_n_con_means$sim <- factor(inter_n_con_means$sim, \n                     levels = c(""Intraspecific"", \n                     ""Intra- & Interspecific (matching)"",\n                     ""Intra- & Interspecific (contrasting)""))\n# plot                    \n# plot                    \nquantiles_gg <- ggplot(inter_n_con_means, aes(x = r,  y = means, group = sim)) +\n                  geom_hline(yintercept = 0, linetype = ""dashed"", \n                             colour  = ""grey"") +\n                  geom_errorbar(aes(ymin = L, ymax = U,colour = sim), \n                                width = 0.2) +\n                  geom_line(aes(colour = sim), size = 0.5) +\n                  geom_point(aes(colour = sim, shape = sim, size = sim), \n                                 fill = ""white"") +\n                  scale_colour_manual("""", values = c(""#1f78b4ff"",\n                                                     ""#000000"",\n                                                     ""#000000"")) +\n                  scale_shape_manual("""", values = c(20,21,24)) +\n                  scale_size_manual("""", values = c(1.5,2,2)) +\n                  ylab(expression(paste(""Selection gradient ("", beta, "")""))) + \n                  xlab(""Assortment"") + \n                  theme(legend.position = ""bottom"") +\n                  guides(colour=guide_legend(nrow=3,byrow=TRUE)) +\n                  scale_y_continuous(breaks = round(seq(-1.2, 1.2, by = 0.1),1))\n\n# look\nprint(quantiles_gg)\n\n\n## stop. :)\n\n#################\n\n\n\n\n\n\n\n\n\n']","Data from: Sexual selection in complex communities: integrating interspecific reproductive interference in structured populations The social structure of populations plays a key role in shaping variation in sexual selection. In nature, sexual selection occurs in communities of interacting species, however heterospecifics are rarely included in characterisations of social structure. Heterospecifics can influence the reproductive outcomes of intrasexual competition by interfering with intraspecific sexual interactions (interspecific reproductive interference; IRI). We outline the need for studies of sexual selection to incorporate heterospecifics as part of the social environment. We use simulations to show that classic predictions for the effect of social structure on sexual selection are altered by an interaction between social structure and IRI. This interaction has wide-ranging implications for patterns of sexual conflict and kin-selected reproductive strategies in socially structured populations. Our work bridges the gap between sexual selection research on social structure and IRI, and highlights future directions to study sexual selection in interacting communities.",2
Data Science in Biomedicine - R scripts,"R scripts for: 1) Google trends analysis regarding the terms ""Data Science"", ""Big Data"" and ""Cloud Computing"" and 2) Read and summarise the Web of Science search for the number of publications associated with the topics ""Data Science"", ""Big Data"" and ""Cloud Computing"" from 2004 to 2019 in nine different countries.","['#--------------------------------------------------------------\r\n# Chapter 2: Data science in biomedicine\r\n#--------------------------------------------------------------\r\n# Install and load the packages\r\n\r\n# install.packages(""gtrendsR"")\r\n# install.packages(""ggplot2"")\r\n# install.packages(""gridExtra"")\r\nlibrary(gtrendsR)\r\nlibrary(ggplot2)\r\nlibrary(gridExtra)\r\n\r\n#------------------------------------------------------------- \r\n# Figure 2.2: Google trends for the terms ""Data Science"" (red), \r\n# ""Big Data"" (green), and ""Cloud Computing"" (blue) for global \r\n# queries.\r\n#------------------------------------------------------------- \r\n\r\nres <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                 ""Data Science""),\r\n               time = ""2004-01-01 2019-12-31"")\r\nplot(res, main = """")\r\n\r\nwindows()\r\nworld <- res$interest_over_time\r\nworld$hits[world$hits == ""<1""] <- 0\r\nworld$hits<- as.numeric(as.character(world$hits))\r\nworld$date <- as.Date(world$date)\r\nworld$keyword <- factor(world$keyword, \r\n                        levels = c(""Data Science"", \r\n                                   ""Big Data"", \r\n                                   ""Cloud Computing""))\r\n\r\n#---\r\n\r\ngg_world <- ggplot(world, aes(x = date, y = hits, \r\n                              color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5)) +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"") +\r\n  theme(legend.position = ""none"")\r\nplot1 <- print(gg_world + ggtitle(""World"")) \r\n\r\n#------------------------------------------------------------- \r\n# Figure 2.3: Google trends for the terms ""Data Science"" (red), \r\n# ""Big Data"" (green), and ""Cloud Computing"" (blue) for some \r\n# countries of Europe\r\n#------------------------------------------------------------- \r\n\r\nres_spain <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                       ""Data Science""), \r\n                     time = ""2004-01-01 2019-12-31"", \r\n                     geo = ""ES"")\r\nres_germany <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                         ""Data Science""), \r\n                       time = ""2004-01-01 2019-12-31"",\r\n                       geo = ""DE"")\r\nres_unitedkingdom <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                               ""Data Science""), \r\n                             time = ""2004-01-01 2019-12-31"",\r\n                             geo = ""GB"")\r\nres_italy <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                       ""Data Science""), \r\n                     time = ""2004-01-01 2019-12-31"", \r\n                     geo = ""IT"")\r\nsp <- res_spain$interest_over_time\r\nsp$hits[sp$hits == ""<1""] <- 0\r\nsp$hits<- as.numeric(as.character(sp$hits))\r\nde <- res_germany$interest_over_time\r\nuk <- res_unitedkingdom$interest_over_time\r\nit <- res_italy$interest_over_time\r\nsp$date <- as.Date(sp$date)\r\nde$date <- as.Date(de$date)\r\nuk$date <- as.Date(uk$date)\r\nit$date <- as.Date(it$date)\r\n\r\nsp$keyword <- factor(sp$keyword, \r\n                     levels = c(""Data Science"", \r\n                                ""Big Data"", \r\n                                ""Cloud Computing""))\r\nde$keyword <- factor(de$keyword, \r\n                     levels = c(""Data Science"", \r\n                                ""Big Data"", \r\n                                ""Cloud Computing""))\r\nuk$keyword <- factor(uk$keyword, \r\n                     levels = c(""Data Science"", \r\n                                ""Big Data"", \r\n                                ""Cloud Computing""))\r\nit$keyword <- factor(it$keyword, \r\n                     levels = c(""Data Science"", \r\n                                ""Big Data"", \r\n                                ""Cloud Computing""))\r\n#---\r\n\r\ngg_sp <- ggplot(sp, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") + \r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_de <- ggplot(de, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_uk <- ggplot(uk, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_it <- ggplot(it, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\n\r\nplot1 <- gg_sp + ggtitle(""Spain"") \r\nplot2 <- gg_de + ggtitle(""Germany"")\r\nplot3 <- gg_uk + ggtitle(""United Kingdom"")\r\nplot4 <- gg_it + ggtitle(""Italy"")\r\nwindows()\r\ngrid.arrange(plot1, plot2, plot3, plot4, ncol = 2) \r\n\r\nplots <- list(x = plot1, y = plot2, z = plot3, \r\n              t = plot4)\r\n\r\n\r\n#------------------------------------------------------------- \r\n# Figure 2.4: Google trends for the terms ""Data Science"" (red), \r\n# ""Big Data"" (green), and ""Cloud Computing"" (blue) for United \r\n# States and some of its states\r\n#------------------------------------------------------------- \r\n\r\nres_usa <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                     ""Data Science""), \r\n                   time = ""2004-01-01 2019-12-31"",  \r\n                   geo = ""US"")\r\nres_usa_ma <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                        ""Data Science""), \r\n                      time = ""2004-01-01 2019-12-31"", \r\n                      geo = ""US-MA"")\r\nres_usa_ca <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                        ""Data Science""), \r\n                      time = ""2004-01-01 2019-12-31"", \r\n                      geo = ""US-CA"")\r\nres_usa_wa <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                        ""Data Science""), \r\n                      time = ""2004-01-01 2019-12-31"", \r\n                      geo = ""US-WA"")\r\n\r\nusa <- res_usa$interest_over_time\r\nusa$hits[usa$hits == ""<1""] <- 0\r\nusa$hits<- as.numeric(as.character(usa$hits))\r\nusa_ma <- res_usa_ma$interest_over_time\r\nusa_ca <- res_usa_ca$interest_over_time\r\nusa_wa <- res_usa_wa$interest_over_time\r\nusa$date <- as.Date(usa$date)\r\nusa_ma$date <- as.Date(usa_ma$date)\r\nusa_ca$date <- as.Date(usa_ca$date)\r\nusa_wa$date <- as.Date(usa_wa$date)\r\n\r\nusa$keyword <- factor(usa$keyword, \r\n                      levels = c(""Data Science"",\r\n                                 ""Big Data"", \r\n                                 ""Cloud Computing""))\r\nusa_ma$keyword <- factor(usa_ma$keyword, \r\n                         levels = c(""Data Science"",\r\n                                    ""Big Data"", \r\n                                    ""Cloud Computing""))\r\nusa_ca$keyword <- factor(usa_ca$keyword, \r\n                         levels = c(""Data Science"", \r\n                                    ""Big Data"", \r\n                                    ""Cloud Computing""))\r\nusa_wa$keyword <- factor(usa_wa$keyword, \r\n                         levels = c(""Data Science"",\r\n                                    ""Big Data"", \r\n                                    ""Cloud Computing""))\r\n\r\n\r\ngg_usa <- ggplot(usa, aes(x = date, y = hits, \r\n                          color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_usa_ma <- ggplot(usa_ma, aes(x = date, y = hits, \r\n                                color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") + \r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_usa_ca <- ggplot(usa_ca, aes(x = date, y = hits, \r\n                                color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_usa_wa <- ggplot(usa_wa, aes(x = date, y = hits, \r\n                                color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\n\r\nplot5 <- gg_usa + ggtitle(""United States"")\r\nplot6 <- gg_usa_ma + ggtitle(""US - Massachusetts"")\r\nplot7 <- gg_usa_ca + ggtitle(""US - California"")\r\nplot8 <- gg_usa_wa + ggtitle(""US - Washington"")\r\nwindows()\r\ngrid.arrange(plot5, plot6, plot7, plot8, ncol = 2)\r\n\r\n#-------------------------------------------------------------\r\n# Figure 2.5: Google trends for the terms ""Data Science"" (red), \r\n# ""Big Data"" (green), and ""Cloud Computing"" (blue) in some \r\n# countries of Asia and in Australia\r\n#------------------------------------------------------------- \r\n\r\nres_china <- gtrends(c(""data science"", ""cloud computing"", \r\n                       ""big data""), \r\n                     time = ""2004-01-01 2019-12-31"", \r\n                     geo = ""CN"")\r\nres_japan <- gtrends(c(""data science"", ""cloud computing"", \r\n                       ""big data""), \r\n                     time = ""2004-01-01 2019-12-31"",\r\n                     geo = ""JP"")\r\nres_india <- gtrends(c(""data science"", ""cloud computing"", \r\n                       ""big data""), \r\n                     time = ""2004-01-01 2019-12-31"",\r\n                     geo = ""IN"")\r\nres_australia <- gtrends(c(""data science"", ""cloud computing"", \r\n                           ""big data""), \r\n                         time = ""2004-01-01 2019-12-31"",\r\n                         geo = ""AU"")\r\n\r\ncn <- res_china$interest_over_time\r\ncn$hits[cn$hits == ""<1""] <- 0\r\ncn$hits<- as.numeric(as.character(cn$hits))\r\nja <- res_japan$interest_over_time\r\nindia <- res_india$interest_over_time\r\nau <- res_australia$interest_over_time\r\ncn$date <- as.Date(cn$date)\r\nja$date <- as.Date(ja$date)\r\nindia$date <- as.Date(india$date)\r\nau$date <- as.Date(au$date)\r\n\r\ncn$keyword <- as.factor(cn$keyword)\r\nja$keyword <- as.factor(ja$keyword)\r\nindia$keyword <- as.factor(india$keyword)\r\nau$keyword <- as.factor(au$keyword)\r\nlevels(cn$keyword) <- c(""Data Science"", ""Big Data"", \r\n                        ""Cloud Computing"")\r\nlevels(ja$keyword) <- c(""Data Science"", ""Big Data"", \r\n                        ""Cloud Computing"")\r\nlevels(india$keyword) <- c(""Data Science"", ""Big Data"", \r\n                           ""Cloud Computing"")\r\nlevels(au$keyword) <- c(""Data Science"", ""Big Data"", \r\n                        ""Cloud Computing"")\r\n\r\ngg_cn <- ggplot(cn, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_ja <- ggplot(ja, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"", \r\n       legend.position = ""none"")\r\n\r\ngg_india <- ggplot(india, aes(x = date, y = hits, \r\n                              color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"", \r\n       legend.position = ""none"")\r\n\r\ngg_au <- ggplot(au, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"", \r\n       legend.position = ""none"")\r\n\r\nplot9 <- gg_cn + ggtitle(""China"")\r\nplot10 <- gg_ja + ggtitle(""Japan"")\r\nplot11 <- gg_india + ggtitle(""India"")\r\nplot12 <- gg_au + ggtitle(""Australia"")\r\nwindows()\r\ngrid.arrange(plot9, plot10, plot11, plot12, ncol = 2)\r\n\r\n', '#--------------------------------------------------------------\r\n# Chapter 2: Data science in biomedicine\r\n#--------------------------------------------------------------\r\n\r\n# Set working directory to source file location\r\n\r\n\r\n# Define the countries\r\ncountries <- c(""USA"", ""UK"", ""JAPAN"", ""GERMANY"", ""AUSTRALIA"",\r\n               ""SPAIN"", ""ITALY"", ""INDIA"", ""CHINA"")\r\nn_countries <- length(countries)\r\nyears <- 2004:2019\r\n\r\n# Creating empty datasets\r\ndata_bg_countries <- matrix(0, ncol = n_countries, \r\n                            nrow = length(years))\r\ndata_ds_countries <- matrix(0, ncol = n_countries, \r\n                            nrow = length(years))\r\ndata_cc_countries <- matrix(0, ncol = n_countries, \r\n                            nrow = length(years))\r\n\r\ncolnames(data_bg_countries) <- countries\r\ncolnames(data_ds_countries) <- countries\r\ncolnames(data_cc_countries) <- countries\r\n\r\n#--------------------------------------------------------------\r\n# Read ""DATA SCIENCE"" databases\r\n#--------------------------------------------------------------\r\nds_UK <- read.table(""DataScience_UK.txt"", header = TRUE)\r\nds_USA <- read.table(""DataScience_USA.txt"", header = TRUE)\r\nds_ITALY <- read.table(""DataScience_Italy.txt"", header = TRUE)\r\nds_SPAIN <- read.table(""DataScience_Spain.txt"", header = TRUE)\r\nds_JAPAN <- read.table(""DataScience_Japan.txt"", header = TRUE)\r\nds_CHINA <- read.table(""DataScience_China.txt"", header = TRUE)\r\nds_GER <- read.table(""DataScience_Germany.txt"", header = TRUE)\r\nds_INDIA <- read.table(""DataScience_India.txt"", header = TRUE)\r\nds_AUS <- read.table(""DataScience_Australia.txt"", header = TRUE)\r\n\r\ndata_ds_countries <- cbind(ds_USA[ds_USA[, 1]%in%years, 2],\r\n                           ds_UK[ds_UK[, 1]%in%years, 2],\r\n                           ds_JAPAN[ds_JAPAN[, 1]%in%years, 2],\r\n                           ds_GER[ds_GER[, 1]%in%years, 2],\r\n                           ds_AUS[ds_AUS[, 1]%in%years, 2], \r\n                           ds_SPAIN[ds_SPAIN[, 1]%in%years, 2],\r\n                           ds_ITALY[ds_ITALY[, 1]%in%years, 2],\r\n                           ds_INDIA[ds_INDIA[, 1]%in%years, 2],\r\n                           ds_CHINA[ds_CHINA[, 1]%in%years, 2])\r\ncolnames(data_ds_countries) <- countries\r\ndata_ds_countries <- data_ds_countries[length(years):1, ]\r\nrownames(data_ds_countries) <- years\r\ndata_ds_countries\r\n\r\n#--------------------------------------------------------------\r\n# Read ""BIG DATA"" databases\r\n#--------------------------------------------------------------\r\n\r\nbg_UK <- read.table(""BigData_UK.txt"", header = TRUE)\r\nbg_USA <- read.table(""BigData_USA.txt"", header = TRUE)\r\nbg_ITALY <- read.table(""BigData_Italy.txt"", header = TRUE)\r\nbg_SPAIN <- read.table(""BigData_Spain.txt"", header = TRUE)\r\nbg_JAPAN <- read.table(""BigData_Japan.txt"", header = TRUE)\r\nbg_CHINA <- read.table(""BigData_China.txt"", header = TRUE)\r\nbg_GER <- read.table(""BigData_Germany.txt"", header = TRUE)\r\nbg_INDIA <- read.table(""BigData_India.txt"", header = TRUE)\r\nbg_AUS <- read.table(""BigData_Australia.txt"", header = TRUE)\r\n\r\n\r\ndata_bg_countries <- cbind(bg_USA[bg_USA[, 1]%in%years, 2],\r\n                           bg_UK[bg_UK[, 1]%in%years, 2],\r\n                           bg_JAPAN[bg_JAPAN[, 1]%in%years, 2],\r\n                           bg_GER[bg_GER[, 1]%in%years, 2],\r\n                           bg_AUS[bg_AUS[, 1]%in%years, 2], \r\n                           bg_SPAIN[bg_SPAIN[, 1]%in%years, 2],\r\n                           bg_ITALY[bg_ITALY[, 1]%in%years, 2],\r\n                           bg_INDIA[bg_INDIA[, 1]%in%years, 2],\r\n                           bg_CHINA[bg_CHINA[, 1]%in%years, 2])\r\ncolnames(data_bg_countries) <- countries\r\ndata_bg_countries <- data_bg_countries[length(years):1, ]\r\nrownames(data_bg_countries) <- years\r\ndata_bg_countries\r\n\r\n#--------------------------------------------------------------\r\n# Read ""CLOUD COMPUTING"" databases\r\n#--------------------------------------------------------------\r\n\r\ncc_UK <- read.table(""CloudComputing_UK.txt"", header = TRUE)\r\ncc_USA <- read.table(""CloudComputing_USA.txt"", header = TRUE)\r\ncc_ITALY <- read.table(""CloudComputing_Italy.txt"", header = TRUE)\r\ncc_SPAIN <- read.table(""CloudComputing_Spain.txt"", header = TRUE)\r\ncc_JAPAN <- read.table(""CloudComputing_Japan.txt"", header = TRUE)\r\ncc_CHINA <- read.table(""CloudComputing_China.txt"", header = TRUE)\r\ncc_GER <- read.table(""CloudComputing_Germany.txt"", header = TRUE)\r\ncc_INDIA <- read.table(""CloudComputing_India.txt"", header = TRUE)\r\ncc_AUS <- read.table(""CloudComputing_Australia.txt"", header = TRUE)\r\n\r\ndata_cc_countries <- cbind(cc_USA[cc_USA[, 1]%in%years, 2],\r\n                           cc_UK[cc_UK[, 1]%in%years, 2],\r\n                           cc_JAPAN[cc_JAPAN[, 1]%in%years, 2],\r\n                           cc_GER[cc_GER[, 1]%in%years, 2],\r\n                           cc_AUS[cc_AUS[, 1]%in%years, 2], \r\n                           cc_SPAIN[cc_SPAIN[, 1]%in%years, 2],\r\n                           cc_ITALY[cc_ITALY[, 1]%in%years, 2],\r\n                           cc_INDIA[cc_INDIA[, 1]%in%years, 2],\r\n                           cc_CHINA[cc_CHINA[, 1]%in%years, 2])\r\ncolnames(data_cc_countries) <- countries\r\ndata_cc_countries <- data_cc_countries[length(years):1, ]\r\nrownames(data_cc_countries) <- years\r\ndata_cc_countries\r\n\r\n#--------------------------------------------------------------\r\n# Complete database and summary\r\n#--------------------------------------------------------------\r\n\r\ndata_countries_total <- matrix(NA, ncol = length(countries),\r\n                               nrow = length(years) * 3)\r\ncolnames(data_countries_total) <- countries\r\nrownames(data_countries_total) <- 1:(length(years) * 3)\r\ndata_countries_total\r\n\r\n#--\r\n\r\nfor(i in 1:length(years)){\r\n rownames(data_countries_total)[1 + (i - 1) * 3] <- paste(""ds"", \r\n                                                          years[i],\r\n                                                          sep = ""_"")\r\n data_countries_total[1 + (i - 1) * 3, ] <- data_ds_countries[i, ]\r\n}\r\ndata_countries_total\r\n\r\n']","Data Science in Biomedicine - R scripts R scripts for: 1) Google trends analysis regarding the terms ""Data Science"", ""Big Data"" and ""Cloud Computing"" and 2) Read and summarise the Web of Science search for the number of publications associated with the topics ""Data Science"", ""Big Data"" and ""Cloud Computing"" from 2004 to 2019 in nine different countries.",2
Social network differences and phenotypic divergence between stickleback ecotypes,"Elucidating the mechanisms underlying differentiation between populations is essential to our understanding of ecological and evolutionary processes. While social network analysis has yielded numerous insights in behavioral ecology in recent years, it has rarely been applied to questions about population differentiation. Here, we use social network analysis to assess the potential role of social behavior in the recent divergence between two three-spined stickleback ecotypes, ""whites"" and ""commons"". These ecotypes differ significantly in their social behavior and mating systems as adults, but it is unknown when or how differences in social behavior develop. We found that as juveniles, the white ecotype was bolder and more active than the common ecotype. Furthermore, while there was no evidence for assortative shoaling preferences, the two ecotypes differed in social network structure. Specifically, groups of the white ecotype had a lower clustering coefficient than groups of the common ecotype, suggesting that groups of the white ecotype were characterized by the formation of smaller subgroups, or 'cliques'. Interestingly, ecotypic differences in clustering coefficient were not apparent in mixed groups composed of whites and commons. The formation of cliques could contribute to population divergence by restricting the social environment that individuals experience, potentially influencing future mating opportunities and preferences. These findings highlight the insights that social network analysis can offer into our understanding of population divergence and reproductive isolation.","['####\n####\n# Differences in social network structure and implications \n# for reproductive isolation in a pair of stickleback ecotypes - code for figures and analysis \n####\n####\n\n#Load in packages\nlibrary(ggplot2)\nlibrary(emmeans)\nlibrary(survcomp)\nlibrary(dplyr)\nlibrary(lme4)\nlibrary(car)\nlibrary(stats)\nlibrary(igraph)\nlibrary(lmerTest)\nlibrary(DirectedClustering)\nlibrary(ggpubr) \nlibrary(wPerm)\nlibrary(ggpattern)\n\n#Load in data files\n# Nodes and edges for adjacency matrix - mixed groups\nnodes_mixed <- read.csv(""~/Desktop/Desktop/fall_shoaling/final data + code/nodes_mixed.csv"")\nedges_mixed <- read.csv(""~/Desktop/Desktop/fall_shoaling/final data + code/edges_mixed.csv"")\n# Nodes and edges for adjacency matrix - white groups\nnodes_white <- read.csv(""~/Desktop/Desktop/fall_shoaling/final data + code/nodes_white.csv"")\nedges_white <- read.csv(""~/Desktop/Desktop/fall_shoaling/final data + code/edges_white.csv"")\n# Nodes and edges for adjacency matrix - common groups\nnodes_common <- read.csv(""~/Desktop/Desktop/fall_shoaling/final data + code/nodes_common.csv"")\nedges_common <- read.csv(""~/Desktop/Desktop/fall_shoaling/final data + code/edges_common.csv"")\n\n# load in all edges as one file for some later analysis\nall_edges <- read.csv(""~/Desktop/Desktop/fall_shoaling/final data + code/all_edges.csv"")\n\n# Individual level data / information\nind_shoaling_data <- read.csv(""~/Desktop/Desktop/fall_shoaling/final data + code/ind_shoaling_data.csv"")\nind_shoaling_data$fish_type <- as.factor(ind_shoaling_data$fish_type)\nind_shoaling_data$group_type <- as.factor(ind_shoaling_data$group_type)\nind_shoaling_data$group <- as.factor(ind_shoaling_data$group)\nind_shoaling_data$family <- as.factor(ind_shoaling_data$family)\nind_shoaling_data$holding_tank <- as.factor(ind_shoaling_data$holding_tank)\n\n# Separate out nodes and edges by group for constructing networks for each group\nsplit_nodes <- split(nodes_mixed,rep(1:8,each=6))\nsplit_edges <- split(edges_mixed,rep(1:8,each=15))\nsplit_nodes_c <- split(nodes_common,rep(1:8,each=6))\nsplit_edges_c <- split(edges_common,rep(1:8,each=15))\nsplit_nodes_w <- split(nodes_white,rep(1:8,each=6))\nsplit_edges_w <- split(edges_white,rep(1:8,each=15))\n\n###########\n# Construct networks from data frame - igraph \ng_MA <- graph_from_data_frame(d=split_edges$\'1\', vertices=split_nodes$\'1\', directed = FALSE)\ng_MB <- graph_from_data_frame(d=split_edges$\'2\', vertices=split_nodes$\'2\', directed = FALSE)\ng_MC <- graph_from_data_frame(d=split_edges$\'3\', vertices=split_nodes$\'3\', directed = FALSE)\ng_MD <- graph_from_data_frame(d=split_edges$\'4\', vertices=split_nodes$\'4\', directed = FALSE)\ng_ME <- graph_from_data_frame(d=split_edges$\'5\', vertices=split_nodes$\'5\', directed = FALSE)\ng_MF <- graph_from_data_frame(d=split_edges$\'6\', vertices=split_nodes$\'6\', directed = FALSE)\ng_MG <- graph_from_data_frame(d=split_edges$\'7\', vertices=split_nodes$\'7\', directed = FALSE)\ng_MH <- graph_from_data_frame(d=split_edges$\'8\', vertices=split_nodes$\'8\', directed = FALSE)\n\ng_CA <- graph_from_data_frame(d=split_edges_c$\'1\', vertices=split_nodes_c$\'1\', directed = FALSE)\ng_CB <- graph_from_data_frame(d=split_edges_c$\'2\', vertices=split_nodes_c$\'2\', directed = FALSE)\ng_CC <- graph_from_data_frame(d=split_edges_c$\'3\', vertices=split_nodes_c$\'3\', directed = FALSE)\ng_CD <- graph_from_data_frame(d=split_edges_c$\'4\', vertices=split_nodes_c$\'4\', directed = FALSE)\ng_CE <- graph_from_data_frame(d=split_edges_c$\'5\', vertices=split_nodes_c$\'5\', directed = FALSE)\ng_CF <- graph_from_data_frame(d=split_edges_c$\'6\', vertices=split_nodes_c$\'6\', directed = FALSE)\ng_CG <- graph_from_data_frame(d=split_edges_c$\'7\', vertices=split_nodes_c$\'7\', directed = FALSE)\ng_CH <- graph_from_data_frame(d=split_edges_c$\'8\', vertices=split_nodes_c$\'8\', directed = FALSE)\n\ng_WA <- graph_from_data_frame(d=split_edges_w$\'1\', vertices=split_nodes_w$\'1\', directed = FALSE)\ng_WB <- graph_from_data_frame(d=split_edges_w$\'2\', vertices=split_nodes_w$\'2\', directed = FALSE)\ng_WC <- graph_from_data_frame(d=split_edges_w$\'3\', vertices=split_nodes_w$\'3\', directed = FALSE)\ng_WD <- graph_from_data_frame(d=split_edges_w$\'4\', vertices=split_nodes_w$\'4\', directed = FALSE)\ng_WE <- graph_from_data_frame(d=split_edges_w$\'5\', vertices=split_nodes_w$\'5\', directed = FALSE)\ng_WF <- graph_from_data_frame(d=split_edges_w$\'6\', vertices=split_nodes_w$\'6\', directed = FALSE)\ng_WG <- graph_from_data_frame(d=split_edges_w$\'7\', vertices=split_nodes_w$\'7\', directed = FALSE)\ng_WH <- graph_from_data_frame(d=split_edges_w$\'8\', vertices=split_nodes_w$\'8\', directed = FALSE)\n\n# Convert networks to adjaceny matrix  to compute clustering \nadj_CA = as_adjacency_matrix(g_CA, sparse=F, attr=""weight"")\nadj_CB = as_adjacency_matrix(g_CB, sparse=F, attr=""weight"")\nadj_CC = as_adjacency_matrix(g_CC, sparse=F, attr=""weight"")\nadj_CD = as_adjacency_matrix(g_CD, sparse=F, attr=""weight"")\nadj_CE = as_adjacency_matrix(g_CE, sparse=F, attr=""weight"")\nadj_CF = as_adjacency_matrix(g_CF, sparse=F, attr=""weight"")\nadj_CG = as_adjacency_matrix(g_CG, sparse=F, attr=""weight"")\nadj_CH = as_adjacency_matrix(g_CH, sparse=F, attr=""weight"")\n#\nadj_WA = as_adjacency_matrix(g_WA, sparse=F, attr=""weight"")\nadj_WB = as_adjacency_matrix(g_WB, sparse=F, attr=""weight"")\nadj_WC = as_adjacency_matrix(g_WC, sparse=F, attr=""weight"")\nadj_WD = as_adjacency_matrix(g_WD, sparse=F, attr=""weight"")\nadj_WE = as_adjacency_matrix(g_WE, sparse=F, attr=""weight"")\nadj_WF = as_adjacency_matrix(g_WF, sparse=F, attr=""weight"")\nadj_WG = as_adjacency_matrix(g_WG, sparse=F, attr=""weight"")\nadj_WH = as_adjacency_matrix(g_WH, sparse=F, attr=""weight"")\n#\nadj_MA = as_adjacency_matrix(g_MA, sparse=F, attr=""weight"")\nadj_MB = as_adjacency_matrix(g_MB, sparse=F, attr=""weight"")\nadj_MC = as_adjacency_matrix(g_MC, sparse=F, attr=""weight"")\nadj_MD = as_adjacency_matrix(g_MD, sparse=F, attr=""weight"")\nadj_ME = as_adjacency_matrix(g_ME, sparse=F, attr=""weight"")\nadj_MF = as_adjacency_matrix(g_MF, sparse=F, attr=""weight"")\nadj_MG = as_adjacency_matrix(g_MG, sparse=F, attr=""weight"")\nadj_MH = as_adjacency_matrix(g_MH, sparse=F, attr=""weight"")\n\n# Compute clustering coefficient for the networks\nca <- as.data.frame(ClustF(adj_CA, type = ""undirected""))\ncb <- as.data.frame(ClustF(adj_CB, type = ""undirected""))\ncc <- as.data.frame(ClustF(adj_CC, type = ""undirected""))\ncd <- as.data.frame(ClustF(adj_CD, type = ""undirected""))\nce <- as.data.frame(ClustF(adj_CE, type = ""undirected""))\ncf <- as.data.frame(ClustF(adj_CF, type = ""undirected""))\ncg <- as.data.frame(ClustF(adj_CG, type = ""undirected""))\nch <- as.data.frame(ClustF(adj_CH, type = ""undirected""))\nclustering_common <- rbind(ca,cb,cc,cd,ce,cf,cg,ch)\n#\nma <- as.data.frame(ClustF(adj_MA, type = ""undirected""))\nmb <- as.data.frame(ClustF(adj_MB, type = ""undirected""))\nmc <- as.data.frame(ClustF(adj_MC, type = ""undirected""))\nmd <- as.data.frame(ClustF(adj_MD, type = ""undirected""))\nme <- as.data.frame(ClustF(adj_ME, type = ""undirected""))\nmf <- as.data.frame(ClustF(adj_MF, type = ""undirected""))\nmg <- as.data.frame(ClustF(adj_MG, type = ""undirected""))\nmh <- as.data.frame(ClustF(adj_MH, type = ""undirected""))\nclustering_mixed <- rbind(ma,mb,mc,md,me,mf,mg,mh)\n#\nwa <- as.data.frame(ClustF(adj_WA, type = ""undirected""))\nwb <- as.data.frame(ClustF(adj_WB, type = ""undirected""))\nwc <- as.data.frame(ClustF(adj_WC, type = ""undirected""))\nwd <- as.data.frame(ClustF(adj_WD, type = ""undirected""))\nwe <- as.data.frame(ClustF(adj_WE, type = ""undirected""))\nwf <- as.data.frame(ClustF(adj_WF, type = ""undirected""))\nwg <- as.data.frame(ClustF(adj_WG, type = ""undirected""))\nwh <- as.data.frame(ClustF(adj_WH, type = ""undirected""))\nclustering_white <- rbind(wa,wb,wc,wd,we,wf,wg,wh)\n#\ncl <- rbind(clustering_common,clustering_mixed,clustering_white)\nind_shoaling_data <- cbind(cl, ind_shoaling_data)\n\n###\n# DATA ANALYSIS \n\n# Boldness - linear model\nboldness_lm <- lm(log(latency_sec) ~ length_mm + fish_type,\n                       data = ind_shoaling_data)\nAnova(boldness_lm, type = ""III"")\n\n# Body size - Kruskal Wallis test\nkruskal.test(ind_shoaling_data$length_mm ~ ind_shoaling_data$fish_type)\n\n# Activity - linear mixed model\nactivity_lm <- lmer(log(distance_swam_m) ~ length_mm + fish_type + group_type +\n                      (1|group), data = ind_shoaling_data)\nanova(activity_lm)\n\n# Clustering coefficient - group level linear model\n# get mean values for clustering by group type\nmeans_cl <- as.data.frame(aggregate(LocalCC ~ group + group_type, data=ind_shoaling_data, mean))\n# run model\nmeans_cl_lm <- lm(LocalCC ~ group_type, data = means_cl)\nAnova(means_cl_lm, type = ""III"")\nemmeans(means_cl_lm, pairwise ~ group_type)\n\n# Clustering coefficient - white in mixed vs white in homogeneous and \n#   common in mixed vs common in homogeneous \n\n# separate data by ecotype\nshoaling_data_white <- ind_shoaling_data[ind_shoaling_data$fish_type == ""W"",]\nshoaling_data_common <- ind_shoaling_data[ind_shoaling_data$fish_type == ""C"",]\n\n# run tests \n# white\nwilcox.test(LocalCC ~ group_type, data = shoaling_data_white)\n# common\nwilcox.test(LocalCC ~ group_type, data = shoaling_data_common)\n\n# Exact permutation test for comparing clustering of\n#     white vs. common individuals within mixed groups \n\nclustering_mixed <- ind_shoaling_data[ind_shoaling_data$group_type == ""M"",]\n\n# reorder so values for each group are together when we split data into separate \n#     dataframesfor each group \nclustering_mixed <- clustering_mixed[order(clustering_mixed$group),]\n#\nclustering_mixed_split <- split(clustering_mixed,rep(1:8,each=6))\n\n#vector to store Kruskal-Wallis test statistics   \nclust_K_MA <- vector()\nclust_K_MB <- vector()\nclust_K_MC <- vector()\nclust_K_MD <- vector()\nclust_K_ME <- vector()\nclust_K_MF <- vector()\nclust_K_MG <- vector()\nclust_K_MH <- vector()\n\n# MA\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  clustering_mixed_split$\'1\'$new <- sample(clustering_mixed_split$\'1\'$LocalCC, replace = FALSE)\n  dfs[[i]] <- clustering_mixed_split$\'1\'\n  kruskal_test <- kruskal.test(new ~ fish_type, data = dfs[[i]])\n  clust_K_MA <- append(clust_K_MA, kruskal_test$statistic)\n}\n\n# MB\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  clustering_mixed_split$\'2\'$new <- sample(clustering_mixed_split$\'2\'$LocalCC, replace = FALSE)\n  dfs[[i]] <- clustering_mixed_split$\'2\'\n  kruskal_test <- kruskal.test(new ~ fish_type, data = dfs[[i]])\n  clust_K_MB <- append(clust_K_MB, kruskal_test$statistic)\n}\n\n# MC\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  clustering_mixed_split$\'3\'$new <- sample(clustering_mixed_split$\'3\'$LocalCC, replace = FALSE)\n  dfs[[i]] <- clustering_mixed_split$\'3\'\n  kruskal_test <- kruskal.test(new ~ fish_type, data = dfs[[i]])\n  clust_K_MC <- append(clust_K_MC, kruskal_test$statistic)\n}\n\n# MD\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  clustering_mixed_split$\'4\'$new <- sample(clustering_mixed_split$\'4\'$LocalCC, replace = FALSE)\n  dfs[[i]] <- clustering_mixed_split$\'4\'\n  kruskal_test <- kruskal.test(new ~ fish_type, data = dfs[[i]])\n  clust_K_MD <- append(clust_K_MD, kruskal_test$statistic)\n}\n\n# ME\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  clustering_mixed_split$\'5\'$new <- sample(clustering_mixed_split$\'5\'$LocalCC, replace = FALSE)\n  dfs[[i]] <- clustering_mixed_split$\'5\'\n  kruskal_test <- kruskal.test(new ~ fish_type, data = dfs[[i]])\n  clust_K_ME <- append(clust_K_ME, kruskal_test$statistic)\n}\n\n# MF\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  clustering_mixed_split$\'6\'$new <- sample(clustering_mixed_split$\'6\'$LocalCC, replace = FALSE)\n  dfs[[i]] <- clustering_mixed_split$\'6\'\n  kruskal_test <- kruskal.test(new ~ fish_type, data = dfs[[i]])\n  clust_K_MF <- append(clust_K_MF, kruskal_test$statistic)\n}\n\n# MG\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  clustering_mixed_split$\'7\'$new <- sample(clustering_mixed_split$\'7\'$LocalCC, replace = FALSE)\n  dfs[[i]] <- clustering_mixed_split$\'7\'\n  kruskal_test <- kruskal.test(new ~ fish_type, data = dfs[[i]])\n  clust_K_MG <- append(clust_K_MG, kruskal_test$statistic)\n}\n\n# MH\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  clustering_mixed_split$\'8\'$new <- sample(clustering_mixed_split$\'8\'$LocalCC, replace = FALSE)\n  dfs[[i]] <- clustering_mixed_split$\'8\'\n  kruskal_test <- kruskal.test(new ~ fish_type, data = dfs[[i]])\n  clust_K_MH <- append(clust_K_MH, kruskal_test$statistic)\n}\n\n###\n\n# actual k scores\nkMA <- kruskal.test(LocalCC ~ fish_type, data = clustering_mixed_split$\'1\')\nkMB <- kruskal.test(LocalCC ~ fish_type, data = clustering_mixed_split$\'2\')\nkMC <- kruskal.test(LocalCC ~ fish_type, data = clustering_mixed_split$\'3\')\nkMD <- kruskal.test(LocalCC ~ fish_type, data = clustering_mixed_split$\'4\')\nkME <- kruskal.test(LocalCC ~ fish_type, data = clustering_mixed_split$\'5\')\nkMF <- kruskal.test(LocalCC ~ fish_type, data = clustering_mixed_split$\'6\')\nkMG <- kruskal.test(LocalCC ~ fish_type, data = clustering_mixed_split$\'7\')\nkMH <- kruskal.test(LocalCC ~ fish_type, data = clustering_mixed_split$\'8\')\n\n# log transform of the Kruskal wallis test statistic distribution is approximately normal, \n#    so we can use t table to see if observed value is significantly different from simulated values\nmean(log(clust_K_MA + 0.0000000000000001))\nsd(log(clust_K_MA + 0.0000000000000001))\nlog(kMA$statistic)\n#\npval_MA_cl <- 1 - pnorm(0.85, mean = -0.87, sd = 1.57)\n\n#\nmean(log(clust_K_MB + 0.0000000000000001))\nsd(log(clust_K_MB + 0.0000000000000001))\nlog(kMB$statistic)\n#\npval_MB_cl <- 1 - pnorm(0.17, mean = -0.97, sd = 1.57)\n\n#\nmean(log(clust_K_MC + 0.0000000000000001))\nsd(log(clust_K_MC + 0.0000000000000001))\nlog(kMC$statistic)\n#\npval_MC_cl <- 1 - pnorm(-0.85, mean = -0.93, sd = 1.57)\n\n#\nmean(log(clust_K_MD + 0.0000000000000001))\nsd(log(clust_K_MD + 0.0000000000000001))\nlog(kMD$statistic)\n#\npval_MD_cl <- 1 - pnorm(0.17, mean = -0.99, sd = 1.58)\n\n#\nmean(log(clust_K_ME + 0.0000000000000001))\nsd(log(clust_K_ME + 0.0000000000000001))\nlog(kME$statistic)\n#\npval_ME_cl <- 1 - pnorm(1.35, mean = -0.91, sd = 1.56)\n\n#\nmean(log(clust_K_MF + 0.0000000000000001))\nsd(log(clust_K_MF + 0.0000000000000001))\nlog(kMF$statistic)\n#\npval_MF_cl <- 1 - pnorm(-0.84, mean = -0.85, sd = 1.52)\n\n#\nmean(log(clust_K_MG + 0.0000000000000001))\nsd(log(clust_K_MG + 0.0000000000000001))\nlog(kMG$statistic)\n#\npval_MG_cl <- 1 - pnorm(-0.84, mean = -0.86, sd = 1.57)\n\n#\nmean(log(clust_K_MH + 0.0000000000000001))\nsd(log(clust_K_MH + 0.0000000000000001))\nlog(kMH$statistic)\n#\npval_MH_cl <- 1 - pnorm(-0.84, mean = -0.96, sd = 1.54)\n\n# Take mean of p values and get p value\npvals_cl <- as.data.frame(rbind(pval_MA_cl, pval_MB_cl, pval_MC_cl, pval_MD_cl,\n                                pval_ME_cl, pval_MF_cl, pval_MG_cl, pval_MH_cl))\nmean(pvals_cl$V1)\n#\n\n# Interaction rate - group level linear model \n# get mean values for interaction rate by group type\nmeans_interaction_rate <- as.data.frame(aggregate(weight ~ group + group_type, data=all_edges, mean))\n# run model\nmeans_interaction_rate_lm <- lm(weight ~ group_type, data = means_interaction_rate)\nAnova(means_interaction_rate_lm, type = ""III"")\n\n# Exact permutation test for comparing interaction rate of \n#       white vs. common individuals within mixed groups \n\n# start with edges which represent all interaction rates \nView(split_edges)\n\n# set up vectors to store test Kruskal-Wallis test statistics   \nkscores_MA <- vector()\nkscores_MB <- vector()\nkscores_MC <- vector()\nkscores_MD <- vector()\nkscores_ME <- vector()\nkscores_MF <- vector()\nkscores_MG <- vector()\nkscores_MH <- vector()\n\n# MA - repeat this for loop for all groups \ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  split_edges$\'1\'$new <- sample(split_edges$\'1\'$weight, replace = FALSE)\n  dfs[[i]] <- split_edges$\'1\'\n  kruskal_test <- kruskal.test(new ~ interaction, data = dfs[[i]])\n  kscores_MA <- append(kscores_MA, kruskal_test$statistic)\n}\n\n# MB\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  split_edges$\'2\'$new <- sample(split_edges$\'2\'$weight, replace = FALSE)\n  dfs[[i]] <- split_edges$\'2\'\n  kruskal_test <- kruskal.test(new ~ interaction, data = dfs[[i]])\n  kscores_MB <- append(kscores_MB, kruskal_test$statistic)\n}\n\n# MC\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  split_edges$\'3\'$new <- sample(split_edges$\'3\'$weight, replace = FALSE)\n  dfs[[i]] <- split_edges$\'3\'\n  kruskal_test <- kruskal.test(new ~ interaction, data = dfs[[i]])\n  kscores_MC <- append(kscores_MC, kruskal_test$statistic)\n}\n\n# MD\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  split_edges$\'4\'$new <- sample(split_edges$\'4\'$weight, replace = FALSE)\n  dfs[[i]] <- split_edges$\'4\'\n  kruskal_test <- kruskal.test(new ~ interaction, data = dfs[[i]])\n  kscores_MD <- append(kscores_MD, kruskal_test$statistic)\n}\n\n# ME\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  split_edges$\'5\'$new <- sample(split_edges$\'5\'$weight, replace = FALSE)\n  dfs[[i]] <- split_edges$\'5\'\n  kruskal_test <- kruskal.test(new ~ interaction, data = dfs[[i]])\n  kscores_ME <- append(kscores_ME, kruskal_test$statistic)\n}\n\n# MF\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  split_edges$\'6\'$new <- sample(split_edges$\'6\'$weight, replace = FALSE)\n  dfs[[i]] <- split_edges$\'6\'\n  kruskal_test <- kruskal.test(new ~ interaction, data = dfs[[i]])\n  kscores_MF <- append(kscores_MF, kruskal_test$statistic)\n}\n\n# MG\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  split_edges$\'7\'$new <- sample(split_edges$\'7\'$weight, replace = FALSE)\n  dfs[[i]] <- split_edges$\'7\'\n  kruskal_test <- kruskal.test(new ~ interaction, data = dfs[[i]])\n  kscores_MG <- append(kscores_MG, kruskal_test$statistic)\n}\n\n# MH\ndfs <- vector(""list"",3)\n\nfor(i in 1:1000){\n  split_edges$\'8\'$new <- sample(split_edges$\'8\'$weight, replace = FALSE)\n  dfs[[i]] <- split_edges$\'8\'\n  kruskal_test <- kruskal.test(new ~ interaction, data = dfs[[i]])\n  kscores_MH <- append(kscores_MH, kruskal_test$statistic)\n}\n\n###\n\n# actual k scores\nkMA <- kruskal.test(weight ~ interaction, data = split_edges$\'1\')\nkMB <- kruskal.test(weight ~ interaction, data = split_edges$\'2\')\nkMC <- kruskal.test(weight ~ interaction, data = split_edges$\'3\')\nkMD <- kruskal.test(weight ~ interaction, data = split_edges$\'4\')\nkME <- kruskal.test(weight ~ interaction, data = split_edges$\'5\')\nkMF <- kruskal.test(weight ~ interaction, data = split_edges$\'6\')\nkMG <- kruskal.test(weight ~ interaction, data = split_edges$\'7\')\nkMH <- kruskal.test(weight ~ interaction, data = split_edges$\'8\')\n\n# log transform of the Kruskal wallis test statistic distribution is approximately normal, \n#    so we can use t table to see if observed value is significantly different from simulated values\nmean(log(kscores_MA + 0.0000000000000001))\nsd(log(kscores_MA + 0.0000000000000001))\nlog(kMA$statistic)\n#\npval_MA <- 1 - pnorm(0.588, mean = 0.117, sd = 2.03)\n\n#\nmean(log(kscores_MB + 0.0000000000000001))\nsd(log(kscores_MB + 0.0000000000000001))\nlog(kMB$statistic)\n#\npval_MB <- 1 - pnorm(0.857, mean = 0.104, sd = 2.60)\n\n#\nmean(log(kscores_MC + 0.0000000000000001))\nsd(log(kscores_MC + 0.0000000000000001))\nlog(kMC$statistic)\n#\npval_MC <- 1 - pnorm(0.857, mean = 0.252, sd = 1.15)\n\n#\nmean(log(kscores_MD + 0.0000000000000001))\nsd(log(kscores_MD + 0.0000000000000001))\nlog(kMD$statistic)\n#\npval_MD <- 1 - pnorm(0.951, mean = 0.06, sd = 2.86)\n\n#\nmean(log(kscores_ME + 0.0000000000000001))\nsd(log(kscores_ME + 0.0000000000000001))\nlog(kME$statistic)\n#\npval_ME <- 1 - pnorm(1.11, mean = 0.24, sd = 1.16)\n\n#\nmean(log(kscores_MF + 0.0000000000000001))\nsd(log(kscores_MF + 0.0000000000000001))\nlog(kMF$statistic)\n#\npval_MF <- 1 - pnorm(1.63, mean = 0.18, sd = 2.00)\n\n#\nmean(log(kscores_MG + 0.0000000000000001))\nsd(log(kscores_MG + 0.0000000000000001))\nlog(kMG$statistic)\n#\npval_MG <- 1 - pnorm(-0.86, mean = 0.14, sd = 2.01)\n\n#\nmean(log(kscores_MH + 0.0000000000000001))\nsd(log(kscores_MH + 0.0000000000000001))\nlog(kMH$statistic)\n#\npval_MH <- 1 - pnorm(1.77, mean = 0.168, sd = 1.19)\n\n# Take mean of p values and get p value\npvals_interaction_rate <- as.data.frame(rbind(pval_MA, pval_MB, pval_MC, pval_MD,\n                                              pval_ME, pval_MF, pval_MG, pval_MH))\nmean(pvals_interaction_rate$V1)\n#\n\n########\n\n# Correlations between behaviors \n\n#Compute correlations between all behaviors for each ecotype \nperm.relation(shoaling_data_white$latency_sec, shoaling_data_white$distance_swam_m,\n              method = ""spearman"", R = 10000)\nperm.relation(shoaling_data_white$latency_sec, shoaling_data_white$LocalCC,\n              method = ""spearman"", R = 10000)\nperm.relation(shoaling_data_white$latency_sec, shoaling_data_white$total_interaction_time,\n              method = ""spearman"", R = 10000)\nperm.relation(shoaling_data_white$total_interaction_time, shoaling_data_white$distance_swam_m,\n              method = ""spearman"", R = 10000)\n#\nperm.relation(shoaling_data_common$latency_sec, shoaling_data_common$distance_swam_m,\n              method = ""spearman"", R = 10000)\nperm.relation(shoaling_data_common$latency_sec, shoaling_data_common$LocalCC,\n              method = ""spearman"", R = 10000)\nperm.relation(shoaling_data_common$latency_sec, shoaling_data_common$total_interaction_time,\n              method = ""spearman"", R = 10000)\nperm.relation(shoaling_common$total_interaction_time, shoaling_data_common$distance_swam_m,\n              method = ""spearman"", R = 10000)\n']","Social network differences and phenotypic divergence between stickleback ecotypes Elucidating the mechanisms underlying differentiation between populations is essential to our understanding of ecological and evolutionary processes. While social network analysis has yielded numerous insights in behavioral ecology in recent years, it has rarely been applied to questions about population differentiation. Here, we use social network analysis to assess the potential role of social behavior in the recent divergence between two three-spined stickleback ecotypes, ""whites"" and ""commons"". These ecotypes differ significantly in their social behavior and mating systems as adults, but it is unknown when or how differences in social behavior develop. We found that as juveniles, the white ecotype was bolder and more active than the common ecotype. Furthermore, while there was no evidence for assortative shoaling preferences, the two ecotypes differed in social network structure. Specifically, groups of the white ecotype had a lower clustering coefficient than groups of the common ecotype, suggesting that groups of the white ecotype were characterized by the formation of smaller subgroups, or 'cliques'. Interestingly, ecotypic differences in clustering coefficient were not apparent in mixed groups composed of whites and commons. The formation of cliques could contribute to population divergence by restricting the social environment that individuals experience, potentially influencing future mating opportunities and preferences. These findings highlight the insights that social network analysis can offer into our understanding of population divergence and reproductive isolation.",2
Data from: Fluctuations in neighbourhood fertility generate variable signaling effort,"Studies of sexual signalling generally focus on interactions between dyadic pairs, yet communication in natural populations often occurs in the context of complex social networks. The ability to survey social environments and adjust signal production appropriately should be a critical component of success in these systems, but has rarely been documented empirically. Here, we used autonomous recording devices to identify 118 472 songs produced by 26 male common yellowthroats (Geothlypis trichas) over two breeding seasons, coupled with detailed surveys of social conditions on each territory. We found strong evidence that common yellowthroat males adjusted their total song production in response to both changes in within-pair social context and changes in the fertility of neighbouring females up to 400 m away. Within the social pair, males drastically reduced their song production when mated, but the magnitude of this reduction depended on both the time of day and on the fertility status of the social mate. By contrast, when fertile females were present on nearby territories, males increased their song output, especially during daytime singing. At this time, it is unclear whether males actively gathered information on neighbouring female fertility or whether the patterns that we observed were driven by changes in social interactions that varied with neighbourhood fertility. Regardless of the mechanism employed, however, subtle changes in the social environment generated substantial variation in signalling effort.","['###### This is the complete code for all talbes, figures, and analyses presented in the paper\n\t### ""Fluctuations in neighbourhood fertility generate variable signaling effort""\n\t### by Conor C Taff, Gail L Patricell, and Corey R Freeman-Gallant. Please see the accompanying\n\t### readme file for explanation of column headers. Code was last run in R for Mac OS X\n\t### version 3.0.2 GUI 1.62 Snow Leopard Build.\n\n###### Set the working directory to folder where data files are stored\n\n\tsetwd("""")\n\n###### Load packages that will (or might) be used. Other dependent packages loaded automatically \n\t## include Matrix v 1.1-0; MASS v 7.3-29; and lattice v 0.20-23\n\n\tlibrary(lme4)  ## v 1.0-5\n\tlibrary(bbmle)  ## v 1.0.15\n\tlibrary(rethinking)  ## v 1.30\n\tlibrary(coda)  ## v 0.16-1\n\tlibrary(plotrix)  ## v 3.5-1\n\tlibrary(glmmADMB)  ## v 0.8.0\n\n###### Load data and parse into the sets that will be used\n\n\td<-read.csv(""ByMale.csv"")    ## All Data\n\td$JDate<-d$JDate-120  ## Recalculate date so that May 1 = 1\n\t\n\tdawnd<-subset(d,d$IncludeDawn==""Yes"")   # Exclude days with no or partial dawn recording\n\tdawnd2<-subset(dawnd,dawnd$Imp==0)   # Exclude males with testosterone implates\n\tdawnd3<-subset(dawnd2,dawnd2$TotalDawn>1)    # Exclude days with 0 detected focal songs\n\tdawnd4<-subset(dawnd,dawnd$TotalDawn>1)  # Dataset including T implanted males\n\t\n\tdayd<-subset(d,d$FullDay==""Yes"")   # Exclude days with no or partial day recording\n\tdayd2<-subset(dayd,dayd$Imp==0)    # Exclude males with testosterone implants\n\tdayd3<-subset(dayd2,dayd2$TotalDay>1)  # Exclude days with 0 detected focal songs\n\tdayd4<-subset(dayd,dayd$TotalDay>1) # Dataset including T implanted males\n\t\n###### Tell r whether to plot to tiff files or to the r window\n\t## if this value = 1, plots go to appropriately scaled tiffs\n\n\tplot.to.dev<-1\n\n###### Define a function to create AICc tables from glmmadmb fit models\n\n\t\tAICcTable<-function(x){\n\t\t\tOutName<-matrix(nrow=length(x),ncol=8)\n\t\t\tnames<-c(""LogL"",""K"",""AICc"",""dAIC"",""expAIC"",""Weight"",""N"",""Groups"")\n\t\t\tcolnames(OutName)<-names\n\t\t\trownames(OutName)<-x\n\t\t\tlikelihood<-vector(length=length(x))\n\t\t\tfor(i in 1:length(x)){\n\t\t\t\tlikelihood[i]<-as.numeric(get(x[i])$loglik[1])\n\t\t\t}\n\t\t\tparameters<-vector(length=length(x))\n\t\t\tfor(i in 1:length(x)){\n\t\t\t\tparameters[i]<-as.numeric(get(x[i])$npar[1])\n\t\t\t}\t\n\t\t\tn<-vector(length=length(x))\n\t\t\tfor(i in 1:length(x)){\n\t\t\t\tn[i]<-as.numeric(get(x[i])$n[1])\n\t\t\t}\n\t\t\tgroups<-vector(length=length(x))\n\t\t\tfor(i in 1:length(x)){\n\t\t\t\tgroups[i]<-as.numeric(get(x[i])$q[1])\n\t\t\t}\n\t\t\tOutName[,1]<-likelihood\n\t\t\tOutName[,2]<-parameters\n\t\t\tOutName[,3]<-(-2)*OutName[,1]+2*OutName[,2]+\n\t\t\t\t((2*OutName[,2]*(OutName[,2]+1))/\n\t\t\t\t\t(30-OutName[,2]-1))\n\t\t\tOutName[,4]<-round(OutName[,3]-min(OutName[,3]),2)\n\t\t\tOutName[,5]<-exp(-.5*OutName[,4])\n\t\t\tstand<-sum(OutName[,5])\n\t\t\tOutName[,7]<-n\n\t\t\tOutName[,8]<-groups\n\t\t\tOutName[,6]<-round(OutName[,5]/stand,3)\n\t\t\ttemp<-order(OutName[,4])\n\t\t\tOutName<-OutName[temp,]\t\n\t\t}\n\t\t\n###### Table 1: Candidate model sets for dawn and daytime singing\n\n\t##### Dawn singing models\n\t\n\t\t\tdawndata<-dawnd3  # subset of data to use for model fits\n\t\t\t\n\t\t## \'Null\' model with intercept only\n\t\t\tdawn.m0<-glmmadmb(TotalDawn~1+(1|Code1),family=""nbinom"",data=dawndata)\n\t\t## Abiotic factors only model with temperature and date\n\t\t\tdawn.m1<-glmmadmb(TotalDawn~NightLow+DawnHigh+JDate+(1|Code1),family=""nbinom"",data=dawndata)\n\t\t## Within pair social context only\n\t\t\tdawn.m2<-glmmadmb(TotalDawn~FF+NF+(1|Code1),family=""nbinom"",data=dawndata)\n\t\t## Both within and neighboring social context considered\n\t\t\tdawn.m3<-glmmadmb(TotalDawn~F400+FF+NF+(1|Code1),family=""nbinom"",data=dawndata)\n\t\t## Full model including all predictors\n\t\t\tdawn.m4<-glmmadmb(TotalDawn~F400+FF+NF+NightLow+DawnHigh+JDate+(1|Code1),family=""nbinom"",data=dawndata)\n\t\t\t\n\t\tDawn.Table<-AICcTable(c(""dawn.m0"",""dawn.m1"",""dawn.m2"",""dawn.m3"",""dawn.m4""))\n\t\t\n\t##### Daytime singing models\n\t\n\t\t\tdaydata<-dayd3  # subset of data to use for model fits\n\t\t\t\n\t\t## \'Null\' model with intercept only\n\t\t\tday.m0<-glmmadmb(TotalDay~1+(1|Code1),family=""nbinom"",data=daydata)\n\t\t## Abiotic factors only model with temperature and date\n\t\t\tday.m1<-glmmadmb(TotalDay~NightLow+DayHigh+JDate+(1|Code1),family=""nbinom"",data=daydata)\n\t\t## Within pair social context only\n\t\t\tday.m2<-glmmadmb(TotalDay~FF+NF+(1|Code1),family=""nbinom"",data=daydata)\n\t\t## Both within and neighboring social context considered\n\t\t\tday.m3<-glmmadmb(TotalDay~F400+FF+NF+(1|Code1),family=""nbinom"",data=daydata)\n\t\t## Full model including all predictors\n\t\t\tday.m4<-glmmadmb(TotalDay~F400+FF+NF+NightLow+DayHigh+JDate+(1|Code1),family=""nbinom"",data=daydata)\n\t\t\t\n\t\tDay.Table<-AICcTable(c(""day.m0"",""day.m1"",""day.m2"",""day.m3"",""day.m4""))\n\t\t\n####### Not reported in detail in the paper. This is the exact same analysis as in Table 1 above, except that males\n\t## that received Testosterone implants are also included. Each candidate model now includes a predictor variable\n\t## that specifies whether a male had a testosterone implant or not.\t\t\n\n\t##### Dawn singing models\n\t\n\t\t\tdawndata2<-dawnd4  # subset of data to use for model fits\n\t\t\t\n\t\t## \'Null\' model with intercept only\n\t\t\tdawn.m0.1<-glmmadmb(TotalDawn~1+(1|Code1),family=""nbinom"",data=dawndata2)\n\t\t## Abiotic factors only model with temperature and date\n\t\t\tdawn.m1.1<-glmmadmb(TotalDawn~NightLow+DawnHigh+JDate+Imp+(1|Code1),family=""nbinom"",data=dawndata2)\n\t\t## Within pair social context only\n\t\t\tdawn.m2.1<-glmmadmb(TotalDawn~FF+NF+Imp+(1|Code1),family=""nbinom"",data=dawndata2)\n\t\t## Both within and neighboring social context considered\n\t\t\tdawn.m3.1<-glmmadmb(TotalDawn~F400+FF+NF+Imp+(1|Code1),family=""nbinom"",data=dawndata2)\n\t\t## Full model including all predictors\n\t\t\tdawn.m4.1<-glmmadmb(TotalDawn~F400+FF+NF+NightLow+DawnHigh+JDate+Imp+(1|Code1),family=""nbinom"",data=dawndata2)\n\t\t\t\n\t\tDawn.Table2<-AICcTable(c(""dawn.m0.1"",""dawn.m1.1"",""dawn.m2.1"",""dawn.m3.1"",""dawn.m4.1""))\n\t\t\n\t##### Daytime singing models\n\t\n\t\t\tdaydata2<-dayd4  # subset of data to use for model fits\n\t\t\t\n\t\t## \'Null\' model with intercept only\n\t\t\tday.m0.1<-glmmadmb(TotalDay~1+(1|Code1),family=""nbinom"",data=daydata2)\n\t\t## Abiotic factors only model with temperature and date\n\t\t\tday.m1.1<-glmmadmb(TotalDay~NightLow+DayHigh+JDate+Imp+(1|Code1),family=""nbinom"",data=daydata2)\n\t\t## Within pair social context only\n\t\t\tday.m2.1<-glmmadmb(TotalDay~FF+NF+Imp+(1|Code1),family=""nbinom"",data=daydata2)\n\t\t## Both within and neighboring social context considered\n\t\t\tday.m3.1<-glmmadmb(TotalDay~F400+FF+NF+Imp+(1|Code1),family=""nbinom"",data=daydata2)\n\t\t## Full model including all predictors\n\t\t\tday.m4.1<-glmmadmb(TotalDay~F400+FF+NF+NightLow+DayHigh+JDate+Imp+(1|Code1),family=""nbinom"",data=daydata2)\n\t\t\t\n\t\tDay.Table2<-AICcTable(c(""day.m0.1"",""day.m1.1"",""day.m2.1"",""day.m3.1"",""day.m4.1""))\n\n###### Table 2: \n\n\t### Fit model coefficients from best supported models above...\t\n\t\n\t\tsummary(dawn.m3)\n\t\tsummary(day.m3)\t\n\t\t\n#### Figure 1. singing profiles by context\n\n\t#### NOTE: this figure uses three separate raw data files that have recordings in bins rather than as daily sums.\n\t#### These bins are just taken from raw observation data with all males pooled.\n\t#### NOTE: references to \'z\' stage here and throughout are code for unmated stage\n\t### NOTE: times are given in seconds in this file midrise = seconds after sunrise, midpoint = seconds after midnight\n\n\t\t## Read data files\n\t\t\n\t\t\tdprofile<-read.csv(""ByBin.csv"")\n\t\t\t\n\t\t## Manipulate data to get ready for plotting\n\t\t\t\n\t\t\tdprofile$Hour<-dprofile$MidRise/60/60\n\t\t\t\n\t\t\tdprofile$z.upper<-(dprofile$Z.Mean+dprofile$Z.Std.Error)/2.5\n\t\t\tdprofile$z.lower<-(dprofile$Z.Mean-dprofile$Z.Std.Error)/2.5\n\t\t\tdprofile$f.upper<-(dprofile$F.Mean+dprofile$F.Std.Error)/2.5\n\t\t\tdprofile$f.lower<-(dprofile$F.Mean-dprofile$F.Std.Error)/2.5\n\t\t\tdprofile$nf.upper<-(dprofile$NF.Mean+dprofile$NF.Std.Error)/2.5\n\t\t\tdprofile$nf.lower<-(dprofile$NF.Mean-dprofile$NF.Std.Error)/2.5\n\t\t\t\n\t\t\th<-dprofile$Hour\n\t\t\t\n\t\t\tcol.1<-rgb(230,159,0,maxColorValue=255)\n\t\t\tcol.2<-rgb(86,180,233,maxColorValue=255)\n\t\t\tcol.3<-rgb(0,158,115,maxColorValue=255)\n\t\t\t\n\t\t\t## Plot loess smoothed lines for each context\n\t\t\t\n\t\t\t\tif(plot.to.dev==1){\n\t\t\t\t\ttiff(""Figure1.tiff"",width=10,height=4,units=""in"",compression=""none"",res=500)\n\t\t\t\t}\n\t\t\t\n\t\t\t\tplot(dprofile$Z.Mean/2.5~dprofile$Hour,type=""n"",ylim=c(0,5),xlab=""Hours After Sunrise"",ylab=""Songs Rate (per minute)"")\n\t\t\t\tmz<-loess(dprofile$Z.Mean/2.5~dprofile$Hour,se=TRUE,span=.2)\n\t\t\t\tzloe<-predict(mz,h,se=TRUE)\n\t\t\t\tlines(h,zloe$fit,col=col.2,lwd=2,lty=4)\n\t\t\t\t\n\t\t\t\tmnf<-loess(dprofile$NF.Mean/2.5~dprofile$Hour,se=TRUE,span=.2)\n\t\t\t\tnfloe<-predict(mnf,h,se=TRUE)\n\t\t\t\tlines(h,nfloe$fit,col=col.1,lwd=2,lty=5)\n\t\t\t\t\n\t\t\t\tmf<-loess(dprofile$F.Mean/2.5~dprofile$Hour,se=TRUE,span=.2)\n\t\t\t\tfloe<-predict(mf,h,se=TRUE)\n\t\t\t\tlines(h,floe$fit,col=col.3,lwd=2)\n\t\t\t\n\t\t\t### Minus std error ofmean\n\t\t\t\n\t\t\t\tmzl<-loess(dprofile$z.lower~dprofile$Hour,se=TRUE,span=.2)\n\t\t\t\tzloel<-predict(mzl,h,se=TRUE)\n\t\t\t\tlines(h,zloel$fit,col=col.2,lwd=1,lty=3)\n\t\t\t\t\n\t\t\t\tmnfl<-loess(dprofile$nf.lower~dprofile$Hour,se=TRUE,span=.2)\n\t\t\t\tnfloel<-predict(mnfl,h,se=TRUE)\n\t\t\t\tlines(h,nfloel$fit,col=col.1,lwd=1,lty=3)\n\t\t\t\t\n\t\t\t\tmfl<-loess(dprofile$f.lower~dprofile$Hour,se=TRUE,span=.2)\n\t\t\t\tfloel<-predict(mfl,h,se=TRUE)\n\t\t\t\tlines(h,floel$fit,col=col.3,lwd=1,lty=3)\n\t\t\t\n\t\t\t## Plus std error of mean\n\t\t\t\n\t\t\t\tmzu<-loess(dprofile$z.upper~dprofile$Hour,se=TRUE,span=.2)\n\t\t\t\tzloeu<-predict(mzu,h,se=TRUE)\n\t\t\t\tlines(h,zloeu$fit,col=col.2,lwd=1,lty=3)\n\t\t\t\t\n\t\t\t\tmnfu<-loess(dprofile$nf.upper~dprofile$Hour,se=TRUE,span=.2)\n\t\t\t\tnfloeu<-predict(mnfu,h,se=TRUE)\n\t\t\t\tlines(h,nfloeu$fit,col=col.1,lwd=1,lty=3)\n\t\t\t\t\n\t\t\t\tmfu<-loess(dprofile$f.upper~dprofile$Hour,se=TRUE,span=.2)\n\t\t\t\tfloeu<-predict(mfu,h,se=TRUE)\n\t\t\t\tlines(h,floeu$fit,col=col.3,lwd=1,lty=3)\n\t\t\t\t\n\t\t\t## Plotting details\n\t\t\t\n\t\t\t\tlegend(11.4,5,c(""Unmated"",""Fertile Mate"",""Post-Fertile Mate"",""Recording Period""),\n\t\t\t\t\tcol=c(col.2,col.3,col.1,""black""),lty=c(4,1,5,1),\n\t\t\t\t\tlwd=c(2,2,2,2))\n\t\t\t\t\t\n\t\t\t\tpoints(h,rep(0,length(h)),pch=15,cex=.5)\t\n\t\t\t\t\n\t\t\t\tif(plot.to.dev==1){dev.off()}\n\t\t\n#### Figure 2. Empirical observations of singing by social context\n\n\t## Dawn Chorus\n\t\n\t\tdawn.z.mean<-mean(subset(dawndata$TotalDawn,dawndata$Un==1))\n\t\tdawn.z.se<-sd(subset(dawndata$TotalDawn,dawndata$Un==1))/sqrt(length(subset(dawndata$TotalDawn,dawndata$Un==1)))\n\t\t\n\t\tdawn.f.mean<-mean(subset(dawndata$TotalDawn,dawndata$FF==1))\n\t\tdawn.f.se<-sd(subset(dawndata$TotalDawn,dawndata$FF==1))/sqrt(length(subset(dawndata$TotalDawn,dawndata$FF==1)))\n\t\t\n\t\tdawn.nf.mean<-mean(subset(dawndata$TotalDawn,dawndata$NF==1))\n\t\tdawn.nf.se<-sd(subset(dawndata$TotalDawn,dawndata$NF==1))/sqrt(length(subset(dawndata$TotalDawn,dawndata$NF==1)))\n\t\t\n\t## Daytime Song\n\t\n\t\tday.z.mean<-mean(subset(daydata$TotalDay,daydata$Un==1))\n\t\tday.z.se<-sd(subset(daydata$TotalDay,daydata$Un==1))/sqrt(length(subset(daydata$TotalDay,daydata$Un==1)))\n\t\t\n\t\tday.f.mean<-mean(subset(daydata$TotalDay,daydata$FF==1))\n\t\tday.f.se<-sd(subset(daydata$TotalDay,daydata$FF==1))/sqrt(length(subset(daydata$TotalDay,daydata$FF==1)))\n\t\t\n\t\tday.nf.mean<-mean(subset(daydata$TotalDay,daydata$NF==1))\n\t\tday.nf.se<-sd(subset(daydata$TotalDay,daydata$NF==1))/sqrt(length(subset(daydata$TotalDay,daydata$NF==1)))\n\t\t\n\t## Combine into one matrix\n\t\n\t\tsocial.est<-matrix(nrow=3,ncol=6)\n\t\tcolnames(social.est)<-c(""Dawn.Z"",""Dawn.F"",""Dawn.NF"",""Day.Z"",""Day.F"",""Day.NF"")\n\t\trownames(social.est)<-c(""Mean"",""Mean-SE"",""Mean+SE"")\n\t\tsocial.est[1,]<-c(dawn.z.mean,dawn.f.mean,dawn.nf.mean,day.z.mean,day.f.mean,day.nf.mean)\n\t\tsocial.est[2,]<-c(dawn.z.mean-dawn.z.se,dawn.f.mean-dawn.f.se,dawn.nf.mean-dawn.nf.se,\n\t\t\tday.z.mean-day.z.se,day.f.mean-day.f.se,day.nf.mean-day.nf.se)\n\t\tsocial.est[3,]<-c(dawn.z.mean+dawn.z.se,dawn.f.mean+dawn.f.se,dawn.nf.mean+dawn.nf.se,\n\t\t\tday.z.mean+day.z.se,day.f.mean+day.f.se,day.nf.mean+day.nf.se)\n\t\t\t\n\t### Plot the two song averages\n\t\n\t\tif(plot.to.dev==1){\n\t\t\ttiff(""Figure2.tiff"",width=8.6,height=4.7,units=""in"",compression=""none"",res=450)\n\t\t}\n\t\n\t\tpar(mfrow=c(1,2))\n\t\tlabels<-c(""Unmated"",""Fertile"",""Post-Fertile"")\n\t\tbar<-0.06   # Set width of error bar top and bottoms\n\t\t\n\t\tbarplot(social.est[1,1:3],ylab=""Songs Produced"",ylim=c(0,200),names.arg=labels,xlab=""Dawn Chorus"")\n\t\t\t\n\t\t\tlines(c(0.7,0.7),social.est[2:3,1])\n\t\t\tlines(c(0.7-bar,0.7+bar),rep(social.est[3,1],2))\n\t\t\tlines(c(0.7-bar,0.7+bar),rep(social.est[2,1],2))\n\t\t\t\n\t\t\tlines(c(1.9,1.9),social.est[2:3,2])\n\t\t\tlines(c(1.9-bar,1.9+bar),rep(social.est[3,2],2))\n\t\t\tlines(c(1.9-bar,1.9+bar),rep(social.est[2,2],2))\n\t\t\t\n\t\t\tlines(c(3.1,3.1),social.est[2:3,3])\n\t\t\tlines(c(3.1-bar,3.1+bar),rep(social.est[3,3],2))\n\t\t\tlines(c(3.1-bar,3.1+bar),rep(social.est[2,3],2))\n\t\t\t\n\t\t\tcorner.label(label=""(a)"")\n\t\t\t\n\t\tbarplot(social.est[1,4:6],ylab=""Songs Produced"",ylim=c(0,500),names.arg=labels,xlab=""Daytime"")\n\t\n\t\t\tlines(c(0.7,0.7),social.est[2:3,4])\n\t\t\tlines(c(0.7-bar,0.7+bar),rep(social.est[3,4],2))\n\t\t\tlines(c(0.7-bar,0.7+bar),rep(social.est[2,4],2))\n\t\t\t\n\t\t\tlines(c(1.9,1.9),social.est[2:3,5])\n\t\t\tlines(c(1.9-bar,1.9+bar),rep(social.est[3,5],2))\n\t\t\tlines(c(1.9-bar,1.9+bar),rep(social.est[2,5],2))\n\t\t\t\n\t\t\tlines(c(3.1,3.1),social.est[2:3,6])\n\t\t\tlines(c(3.1-bar,3.1+bar),rep(social.est[3,6],2))\n\t\t\tlines(c(3.1-bar,3.1+bar),rep(social.est[2,6],2))\n\t\t\t\n\t\t\tcorner.label(label=""(b)"")\n\t\t\t\n\t\t\tif(plot.to.dev==1){dev.off()}\n\t\t\t\n#### Figure 3. Plot predictions from day.m3 acros neighborhood fertility\n\n\t\tif(plot.to.dev==1){\n\t\t\ttiff(""Figure3.tiff"",width=9.8,height=5.3,units=""in"",res=450,compression=""none"")\n\t\t}\n\t\t\n\t\tpar(mfrow=c(1,2))\n\t\t\n\t## Plot the subset of empirical observations of dawn song during unmated stage only\n\t\t## Note these are just for illustration, lines are from fit model\n\t\n\t\tdawn.plot<-subset(dawndata,dawndata$Un==1)\n\t\tplot(dawn.plot$F400,dawn.plot$TotalDawn,col=col.alpha(""slateblue"",0.5),pch=16,\n\t\t\txlab=""Fertile Females in 400m"",ylab=""Dawn Chorus Songs"") \n\t\t\n\t## Calculate and plot MLE and +/- SE of parameter estimates from best model\t\n\t\t\t\n\t\tr<-seq(from=-0.5,to=8,by=0.1)\n\t\tm.dawn<-sapply(r,function(z)exp(fixef(dawn.m3)[1]+fixef(dawn.m3)[2]*z))\n\t\tc.l.dawn<-sapply(r,function(z)exp(fixef(dawn.m3)[1]-stdEr(dawn.m3)[1]\n\t\t\t+(fixef(dawn.m3)[2]*z-stdEr(dawn.m3)[2])))\n\t\tc.h.dawn<-sapply(r,function(z)exp(fixef(dawn.m3)[1]+stdEr(dawn.m3)[1]\n\t\t\t+(fixef(dawn.m3)[2]*z+stdEr(dawn.m3)[2])))\n\t\t\n\t\tlines(r,m.dawn)\t\n\t\tlines(r,c.l.dawn,lty=2)\n\t\tlines(r,c.h.dawn,lty=2)\n\t\t\n\t## Plot empirical means and stderrors for z observations \n\t\n\t\tdawn.m.se<-matrix(nrow=4,ncol=7)\n\t\tfor(i in 1:7){\n\t\t\tdawn.m.se[1,i]<-mean(subset(dawn.plot$TotalDawn,dawn.plot$F400==i-1))\n\t\t}\n\t\tserrors<-vector(length=7)\n\t\tfor(i in 1:7){\n\t\t\tserrors[i]<-sd(subset(dawn.plot$TotalDawn,dawn.plot$F400==i-1))/\n\t\t\t\tsqrt(length(subset(dawn.plot$TotalDawn,dawn.plot$F400==i-1)))\n\t\t}\n\t\tdawn.m.se[2,]<-dawn.m.se[1,]+serrors\n\t\tdawn.m.se[3,]<-dawn.m.se[1,]-serrors\n\t\tfor(i in 1:7){\n\t\t\tdawn.m.se[4,i]<-length(subset(dawn.plot$TotalDawn,dawn.plot$F400==i-1))\n\t\t}\n\t\t\n\t\tfor(i in 1:6){\n\t\t\tlines(rep(i-1+0.1,2),c(dawn.m.se[2,i],dawn.m.se[3,i]))\n\t\t\tpoints(i-1+0.1,dawn.m.se[1,i],pch=16)\n\t\t}\n\t\tpoints(6+0.1,dawn.m.se[1,7],pch=16)\t\t\n\t\t\n\t\tcorner.label(""(a)"")\n\n\t## Plot the subset of empirical observations of full day song during unmated stage only\n\t\t## Note these are just for illustration, lines are from fit model\n\t\n\t\tday.plot<-subset(daydata,daydata$Un==1)\n\t\tplot(day.plot$F400,day.plot$TotalDay,col=col.alpha(""slateblue"",0.5),pch=16,\n\t\t\txlab=""Fertile Females in 400m"",ylab=""Daytime Songs"") \n\t\t\n\t## Calculate and plot MLE and +/- SE of parameter estimates from best model\t\n\t\t\t\n\t\tr<-seq(from=-0.5,to=8,by=0.1)\n\t\tm.day<-sapply(r,function(z)exp(fixef(day.m3)[1]+fixef(day.m3)[2]*z))\n\t\tc.l.day<-sapply(r,function(z)exp(fixef(day.m3)[1]-stdEr(day.m3)[1]\n\t\t\t+(fixef(day.m3)[2]*z-stdEr(day.m3)[2])))\n\t\tc.h.day<-sapply(r,function(z)exp(fixef(day.m3)[1]+stdEr(day.m3)[1]\n\t\t\t+(fixef(day.m3)[2]*z+stdEr(day.m3)[2])))\n\t\t\n\t\tlines(r,m.day)\t\n\t\tlines(r,c.l.day,lty=2)\n\t\tlines(r,c.h.day,lty=2)\n\t\t\n\t## Plot empirical means and stderrors for z observations \n\t\n\t\tday.m.se<-matrix(nrow=4,ncol=7)\n\t\tfor(i in 1:7){\n\t\t\tday.m.se[1,i]<-mean(subset(day.plot$TotalDay,day.plot$F400==i-1))\n\t\t}\n\t\tserrors<-vector(length=7)\n\t\tfor(i in 1:7){\n\t\t\tserrors[i]<-sd(subset(day.plot$TotalDay,day.plot$F400==i-1))/\n\t\t\t\tsqrt(length(subset(day.plot$TotalDay,day.plot$F400==i-1)))\n\t\t}\n\t\tday.m.se[2,]<-day.m.se[1,]+serrors\n\t\tday.m.se[3,]<-day.m.se[1,]-serrors\n\t\tfor(i in 1:7){\n\t\t\tday.m.se[4,i]<-length(subset(day.plot$TotalDay,day.plot$F400==i-1))\n\t\t}\n\t\t\n\t\tfor(i in 1:6){\n\t\t\tlines(rep(i-1+0.1,2),c(day.m.se[2,i],day.m.se[3,i]))\n\t\t\tpoints(i-1+0.1,day.m.se[1,i],pch=16)\n\t\t}\n\t\tpoints(6+0.1,day.m.se[1,7],pch=16)\t\n\t\t\n\t\tcorner.label(""(b)"")\t\n\t\t\n\tif(plot.to.dev==1){dev.off()}\n\t\n\t\t\t\t\t']","Data from: Fluctuations in neighbourhood fertility generate variable signaling effort Studies of sexual signalling generally focus on interactions between dyadic pairs, yet communication in natural populations often occurs in the context of complex social networks. The ability to survey social environments and adjust signal production appropriately should be a critical component of success in these systems, but has rarely been documented empirically. Here, we used autonomous recording devices to identify 118 472 songs produced by 26 male common yellowthroats (Geothlypis trichas) over two breeding seasons, coupled with detailed surveys of social conditions on each territory. We found strong evidence that common yellowthroat males adjusted their total song production in response to both changes in within-pair social context and changes in the fertility of neighbouring females up to 400 m away. Within the social pair, males drastically reduced their song production when mated, but the magnitude of this reduction depended on both the time of day and on the fertility status of the social mate. By contrast, when fertile females were present on nearby territories, males increased their song output, especially during daytime singing. At this time, it is unclear whether males actively gathered information on neighbouring female fertility or whether the patterns that we observed were driven by changes in social interactions that varied with neighbourhood fertility. Regardless of the mechanism employed, however, subtle changes in the social environment generated substantial variation in signalling effort.",2
Distinct decision-making properties underlying the species specificity of group formation of flies,"Many animal species form groups. Group characteristics differ between species, suggesting that the decision-making of individuals for grouping varies across species. However, the actual decision-making properties that lead to interspecific differences in group characteristics remain unclear. Here, we compared the group formation processes of two Drosophilinae fly species, Colocasiomyia alocasiae and Drosophila melanogaster, which form dense and sparse groups, respectively. A high-throughput tracking system revealed that C. alocasiae flies formed groups faster than D. melanogaster flies, and the probability of C. alocasiae remaining in groups was far higher than that of D. melanogaster. C. alocasiae flies joined groups even when the group size was small, whereas D. melanogaster flies joined groups only when the group size was sufficiently large. C. alocasiae flies attenuated their walking speed when the inter-individual distance between flies became small, whereas such behavioural properties were not clearly observed in D. melanogaster. Furthermore, depriving C. alocasiae flies of visual input affected grouping behaviours, resulting in a severe reduction in group formation. These findings show that C. alocasiae decision-making regarding grouping, which greatly depends on vision, is significantly different from D. melanogaster, leading to species-specific group-formation properties.",,"Distinct decision-making properties underlying the species specificity of group formation of flies Many animal species form groups. Group characteristics differ between species, suggesting that the decision-making of individuals for grouping varies across species. However, the actual decision-making properties that lead to interspecific differences in group characteristics remain unclear. Here, we compared the group formation processes of two Drosophilinae fly species, Colocasiomyia alocasiae and Drosophila melanogaster, which form dense and sparse groups, respectively. A high-throughput tracking system revealed that C. alocasiae flies formed groups faster than D. melanogaster flies, and the probability of C. alocasiae remaining in groups was far higher than that of D. melanogaster. C. alocasiae flies joined groups even when the group size was small, whereas D. melanogaster flies joined groups only when the group size was sufficiently large. C. alocasiae flies attenuated their walking speed when the inter-individual distance between flies became small, whereas such behavioural properties were not clearly observed in D. melanogaster. Furthermore, depriving C. alocasiae flies of visual input affected grouping behaviours, resulting in a severe reduction in group formation. These findings show that C. alocasiae decision-making regarding grouping, which greatly depends on vision, is significantly different from D. melanogaster, leading to species-specific group-formation properties.",2
Social information use about novel aposematic prey depends on the intensity of the observed cue,"Animals gather social information by observing the behavior of others, but how the intensity of observed cues influences decision-making is rarely investigated. This is crucial for understanding how social information influences ecological and evolutionary dynamics. For example, observing a predator's distaste of unpalatable prey can reduce predation by nave birds, and help explain the evolution and maintenance of aposematic warning signals. However, previous studies have only used demonstrators that responded vigorously, showing intense beak-wiping after tasting prey. Therefore, here we conducted an experiment with blue tits (Cyanistes caeruleus) informed by variation in predator responses. First, we found that the response to unpalatable food varies greatly, with only few individuals performing intensive beak-wiping. We then tested how the intensity of beak-wiping influences observers' foraging choices using video-playback of a conspecific tasting a novel conspicuous prey item. Observers were provided social information from: (1) no distaste response, (2) a weak distaste response, or (3) a strong distaste response, and were then allowed to forage on evolutionarily novel (artificial) prey. Consistent with previous studies, we found that birds consumed fewer aposematic prey after seeing a strong distaste response, however a weak response did not influence foraging choices. Our results suggest that while beak-wiping is a salient cue, its information content may vary with cue intensity. Furthermore, the number of potential demonstrators in the predator population might be lower than previously thought, although determining how this influences social transmission of avoidance in the wild will require uncovering the effects of intermediate cue salience.","['# Mul, Thorogood & Hmlinen 2022, Behavioral Ecology\r\n# Social information use about novel aposematic prey depends on the intensity of the observed cue\r\n\r\n# packages needed\r\nlibrary(lme4)\r\nlibrary(DHARMa)\r\nlibrary(ggplot2)\r\nlibrary(Rmisc)\r\nlibrary(MASS)\r\n\r\n# Save data as csv files and set a working directory where the data files are saved\r\n\r\n#read the data file      \r\ndata<-read.csv(""Mula et al. data 2022.csv"",header=T, sep  = "";"")\r\nstr(data) \r\n\r\n# change bird and demonstrator ids and experiment duration to be categorical variables\r\ndata$BirdID<-as.factor(data$BirdID)\r\ndata$DemID<-as.factor(data$DemID)\r\ndata$Duration.days<-as.factor(data$Duration.days)\r\n\r\n\r\n########################\r\n\r\n# FIRST PREY CHOICE IN THE EXPERIMENT\r\n\r\n#create a dataset of first foraging trial only\r\ndata2<-subset(data, Trial==""1"") \r\n\r\ntable(data2$First.choice) # first choices\r\nbinom.test(31, 45, p = 0.5,\tconf.level = 0.95) \r\n\r\n# number of successes = 31, number of trials = 45, p-value = 0.01609\r\n# alternative hypothesis: true probability of success is not equal to 0.5\r\n# 95 percent confidence interval:\r\n#  0.5335090 0.8183412\r\n# sample estimates:\r\n# probability of success \r\n# 0.6888889 \r\n\r\n# birds chose a square as a first prey significantly more often\r\n\r\ntable(data2$First.choice, data2$Treatment) # first choices in each treatment\r\n\r\n# Chi-squared test to test whether first choices differ among treatments\r\nchisq.test(data2$First.choice, data2$Treatment,correct=FALSE) \r\n\r\n#\tPearson\'s Chi-squared test\r\n#data:  data2$First.choice and data2$Treatment\r\n#X-squared = 0.20737, df = 2, p-value = 0.9015\r\n\r\n# no evidence that treatment influences first choice\r\n\r\n############################\r\n\r\n# TIME BEFORE ATTACKING THE FIRST PREY \r\n\r\n# GLM with a negative binomial error distribution (response variable time before attack)\r\n\r\n# including all observations, treatment as a fixed effect, demonstrator id as a random effect\r\ntime_model<-glmer.nb(Attack.time~Treatment+(1|DemID), data=data2)\r\nsummary(time_model)\r\n# the model indicates that birds in the palatable treatment were slower to attack the prey\r\n\r\n# the data includes two outliers (both in the palatable treatment) that attacked the prey after 923 and 2768s\r\n\r\n# exclude these observations\r\ndata3<-subset(data, Attack.time<800)\r\nhist(data3$Attack.time)\r\nplot(data3$Attack.time~data3$Treatment)\r\n\r\nmax(data3$Attack.time)\r\nmin(data3$Attack.time)\r\nmean(data3$Attack.time)\r\n# after excluding two outliers, attack time varies from 5-325s, mean = 61s\r\n\r\n# run the same model excluding outliers\r\ntime_model2<-glmer.nb(Attack.time~Treatment+(1|DemID), data=data3)\r\nsummary(time_model2)\r\n\r\n#Random effects:\r\n# Groups Name        Variance Std.Dev.\r\n# DemID  (Intercept) 0.02517  0.1586  \r\n# Number of obs: 43, groups:  DemID, 5\r\n\r\n# Fixed effects:\r\n#                  Estimate  Std. Error z value   Pr(>|z|)    \r\n# (Intercept)      4.21348    0.26774   15.737    <2e-16 ***\r\n# Treatmentstrong -0.33117    0.35501   -0.933     0.351\r\n# Treatmentweak   -0.04987    0.35449   -0.141     0.888    \r\n\r\n# by default, intercept is the palatable treatment, this can be changed with relevel function\r\ndata3$Treatment<-relevel(data3$Treatment, ref=""weak"") # intercept weak treatment \r\ndata3$Treatment<-relevel(data3$Treatment, ref=""strong"") # intercept strong treatment\r\ndata3$Treatment<-relevel(data3$Treatment, ref=""palatable"") # intercept palatable treatment\r\n\r\n# the model indicates that there are no differences in attack times among the treatments\r\n\r\n# plot residuals to investigate model fit\r\nsimulationOutput <- simulateResiduals(fittedModel = time_model2, plot = F)\r\nplot(simulationOutput)\r\n\r\n# Figure 3: differences in attack times (excluding 2 outliers)\r\n\r\nggplot(data3, aes(x=Treatment, y=Attack.time)) + \r\n  geom_violin(trim=FALSE, fill=""gray"")+\r\n  geom_boxplot(width=0.1)+\r\n  scale_x_discrete(breaks=c(""palatable"", ""weak"", ""strong""),labels=c(""Palatable"", ""Weak"", ""Strong""))+\r\n  theme_bw() + theme(plot.background = element_blank(),panel.grid.major = element_blank(),\r\n                     panel.grid.minor = element_blank())+\r\n  ylab(expression(paste(""Time to attack the first prey item (sec)"")))+\r\n  xlab(""Treatment"")+\r\n  theme(axis.title.y = element_text(size = 12))+\r\n  theme(axis.title.x = element_text(size = 12))+\r\n  theme(axis.text = element_text(size = 12))+\r\n  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)))+\r\n  theme(axis.title.x = element_text(margin = margin(t = 20, r = 10, b = 0, l = 0)))+\r\n  theme(legend.position=""none"")\r\n\r\n\r\n#######################\r\n\r\n# PREY CHOICES IN THE FIRST TRIAL\r\n\r\n# GLM with a binomial error distribution (response variable number of squares and crosses consumed)\r\n\r\n# treatment and age as fixed effects and demonstrator id as a random effect \r\nmodel1<-glmer(cbind(Squares,Crosses)~Treatment+(1|DemID)+Age,family=binomial, data=data2)\r\nsummary(model1) \r\n\r\n# Random effects:\r\n# Groups Name        Variance Std.Dev.\r\n# DemID  (Intercept) 0.03377  0.1838  \r\n# Number of obs: 45, groups:  DemID, 5\r\n\r\n# Fixed effects:\r\n#                  Estimate Std. Error  z value   Pr(>|z|)    \r\n# (Intercept)       0.6232     0.1882    3.311   0.000929 ***\r\n# Treatmentstrong  -0.5801     0.1860   -3.118   0.001819 ** \r\n# Treatmentweak    -0.1788     0.1869   -0.957   0.338736    \r\n# AgeJuvenile      -0.3250     0.1692   -1.921   0.054719\r\n\r\n# change intercept with relevel function to compare treatments\r\ndata2$Treatment<-relevel(data2$Treatment, ref=""weak"") # intercept weak treatment \r\ndata2$Treatment<-relevel(data2$Treatment, ref=""strong"") # intercept strong treatment\r\ndata2$Treatment<-relevel(data2$Treatment, ref=""palatable"") # intercept palatable treatment\r\n\r\n# the model indicates that birds in the strong treatment attacked fewer aposematic prey compared to palatable and weak treatments \r\n# there is no difference between weak and palatable treatments\r\n\r\n# plot residuals to investigate model fit\r\nsimulationOutput <- simulateResiduals(fittedModel = model1, plot = F)\r\nplot(simulationOutput)\r\n\r\n############################\r\n\r\n# LEARNING ACROSS TWO TRIALS\r\n\r\n# a model with treatment * trial number interaction, age and experiment duration (1 or 2 days) as fixed effects and demonstrator and observer ids as random effects\r\ntrials_model<-glmer(cbind(Squares,Crosses)~Treatment*Trial+Duration.days+Age+(1|BirdID)+(1|DemID),family=binomial, data=data)\r\nsummary(trials_model) # interaction not significant, no difference in learning rates among treatments \r\n\r\n# a model with experiment duration * trial number interaction, age and treatment as fixed effects and demonstrator and observer ids as random effects\r\ntrials_model2<-glmer(cbind(Squares,Crosses)~Treatment+Duration.days*Trial+Age+(1|BirdID)+(1|DemID),family=binomial, data=data)\r\nsummary(trials_model2) # interaction p = 0.076, close to significant\r\n\r\n# a model with experiment duration * treatment interaction, age and trial number as fixed effects and demonstrator and observer ids as random effects\r\ntrials_model3<-glmer(cbind(Squares,Crosses)~Treatment*Duration.days+Trial+Age+(1|BirdID)+(1|DemID),family=binomial, data=data)\r\nsummary(trials_model3) # interaction not significant\r\n\r\n# a model with no interactions, treatment, duration and age as fixed effects and demonstrator and observer ids as random effects\r\ntrials_model4<-glmer(cbind(Squares,Crosses)~Treatment+Duration.days+Trial+Age+(1|BirdID)+(1|DemID),family=binomial, data=data)\r\nsummary(trials_model4) \r\n\r\n# comparing models with interactions to the model without interactions\r\nanova(trials_model, trials_model4) # treatment*trial number, interaction term not significant\r\nanova(trials_model2, trials_model4) # duration*trial number, interaction term close to significant (p = 0.074) but not at alpha level 0.05\r\nanova(trials_model3, trials_model4) # duration*treatment, interaction term not significant\r\n\r\n# final model\r\ntrials_model4<-glmer(cbind(Squares,Crosses)~Treatment+Duration.days+Trial+Age+(1|BirdID)+(1|DemID),family=binomial, data=data)\r\nsummary(trials_model4) \r\n\r\n#Random effects:\r\n#  Groups Name        Variance Std.Dev.\r\n#  BirdID (Intercept) 0.08271  0.28759 \r\n#  DemID  (Intercept) 0.00119  0.03449 \r\n#  Number of obs: 90, groups:  BirdID, 45; DemID, 5\r\n\r\n# Fixed effects:\r\n#                   Estimate Std. Error  z value  Pr(>|z|)    \r\n#  (Intercept)       1.1263     0.2354   4.785   1.71e-06 ***\r\n#  Treatmentstrong  -0.4346     0.1751  -2.482   0.013049 *  \r\n#  Treatmentweak    -0.1900     0.1728  -1.100   0.271451    \r\n#  Duration.days2    0.6884     0.1783   3.861   0.000113 ***\r\n#  Trial            -0.7779     0.1111  -7.001   2.55e-12 ***\r\n#  AgeJuvenile      -0.1748     0.1485  -1.177   0.239281  \r\n\r\n# change the intercept using relevel function\r\ndata$Treatment<-relevel(data$Treatment, ref=""strong"")\r\ndata$Treatment<-relevel(data$Treatment, ref=""weak"")\r\ndata$Treatment<-relevel(data$Treatment, ref=""palatable"")\r\n\r\n# birds reduced consumption of aposematic prey across the two trials\r\n# slow birds (duration 2 days) consumed more aposematic prey than fast birds\r\n\r\n# plot residuals to investigate model fit\r\nsimulationOutput <- simulateResiduals(fittedModel = trials_model4, plot = F)\r\nplot(simulationOutput)\r\n\r\n# Figure 4: learning across two trials\r\n\r\ndata$Trial<-as.factor(data$Trial) # for the graph, change trial to be a categorical variable\r\n\r\n# Calculate relative predation risk for aposematic prey by dividing the number of squares consumed by 8 \r\n# (because birds consumed 16 prey in each trial, they were expected to consume 8 of each prey type if they chose randomly)\r\ndata$predation_risk<-data$Squares/8\r\n\r\n# This summarizes the predation risk (+ sd, se and ci) in each treatment and trial \r\npred_risk_trials<- summarySE(data, measurevar=""predation_risk"", groupvars=c(""Treatment"",""Trial""), na.rm=T)\r\npred_risk_trials\r\n\r\nggplot(data, aes(x=Trial, y=predation_risk, shape=factor(Treatment, labels=c(""Palatable"", ""Weak"", ""Strong"")),group=Treatment))+ # x axis = trial, y axis = predation risk, plot each treatment separately\r\ngeom_point(position=position_jitterdodge(dodge.width=0.7), color=""gray47"", show.legend=F) +  geom_point(data=pred_risk_trials, aes(x = Trial, y = predation_risk), size=3.5, position=position_dodge(width=0.6))+ # plot individual datapoints\r\n  geom_errorbar(data=pred_risk_trials, aes(ymin=predation_risk-se, ymax=predation_risk+se),width=0.1, position=position_dodge(width=0.6))+ # add mean and error bars\r\n  scale_shape_manual(values=c(16,2,8))+ # define symbols for each treatment \r\n  geom_line(data=pred_risk_trials, aes(linetype=Treatment),position=position_dodge(width=0.6))+ # add lines\r\n  scale_linetype_manual(values=c(""dashed"", ""solid"", ""dotted""), guide=\'none\')+ # define linetypes for each treatment \r\n  theme_bw() + theme(plot.background = element_blank(),panel.grid.major = element_blank(),\r\n                     panel.grid.minor = element_blank())+ \r\n  scale_y_continuous(limits=c(0,2.0), breaks=c(0,0.2,0.4,0.6,0.8,1.0,1.2,1.4, 1.6, 1.8))+   \r\n  scale_x_discrete(breaks=c(""1"", ""2""),labels=c(""Trial 1"", ""Trial 2""))+ \r\n  labs(x="""", \r\n       y=""Relative predation risk for aposematic prey"")+ \r\n  theme(axis.text = element_text(size =11))+ \r\n  theme(axis.text.y = element_text(size =11))+ \r\n  theme(axis.title.y = element_text(size =12))+ \r\n  theme(legend.position = c(0.8, 0.88), legend.title = element_blank())+ \r\n  theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)))+ \r\n  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0)))+ \r\n  geom_hline(yintercept = 1, linetype=3) # add a horizontal line at y = 1 to illustrate predation risk of 1 (when birds consume same number of both prey items)\r\n  \r\n\r\n####################\r\n\r\n# VARIATION IN BEAK WIPING\r\n\r\n# read the data file (second sheet in Excel file)\r\ndata4<-read.csv(""Beak wiping variation.csv"",header=T, sep  = "";"")\r\nstr(data4) \r\n\r\n# Testing the effect of age on beak wiping behaviour\r\n# GLM with a negative binomial error distribution (response variable number of beak wipes)\r\nbeak_wiping_model<-glm.nb(beak.wipes~Age, data=data4)\r\nsummary(beak_wiping_model)\r\n\r\n# Coefficients:\r\n#               Estimate  Std. Error  z value   Pr(>|z|)    \r\n# (Intercept)   3.64388    0.18873    19.308   <2e-16 ***\r\n# AgeJuvenile  -0.01954    0.33146    -0.059    0.953    \r\n\r\n# the model indicates that age does not influence beak wiping behavior\r\n\r\n# plot residuals to investigate model fit\r\nsimulationOutput <- simulateResiduals(fittedModel = beak_wiping_model, plot = F)\r\nplot(simulationOutput)\r\n\r\n# plot variation in beak wiping\r\nhist(data4$beak.wipes, breaks=18, main="""", col=""gray"",\r\n     xlab=""Number of beak wipes in 60s"", xlim=c(0,180), las=1,\r\n     ylab=""Number of birds"", ylim=c(0,10))\r\n\r\n####################\r\n\r\n# Supplementary analysis: \r\n\r\n# did variation in demonstrators\' responses in the strong treatment influence observers\' behaviour?\r\n\r\n# include only birds from strong treatment and choices in the first trial\r\nstrong_treatment<-subset(data, Treatment==""strong"" & Trial==""1"")\r\n\r\n# correlation between the number of demonstrators\' beak wipes and the number of aposematic prey observers attacked\r\ncor.test(strong_treatment$Squares,strong_treatment$Dem.wipes, method = ""pearson"")\r\n# no significant correlation, p = 0.5572\r\n\r\n# Supplementary Fig. S1\r\n\r\nggplot(data = strong_treatment) + \r\n  geom_point(mapping = aes(x = Dem.wipes, y = Squares), size=2)+\r\n  theme_bw() + theme(plot.background = element_blank(),panel.grid.major = element_blank(),\r\n                     panel.grid.minor = element_blank())+\r\n  scale_y_continuous(limits=c(1,14), breaks=c(2,4,6,8,10,12,14))+ \r\n  scale_x_continuous(limits=c(60,90), breaks=c(60,65,70,75,80,85,90))+ \r\n  labs(x=""Number of demonstrator beak wipes"", #  \r\n       y=""Aposematic prey attacked in the first trial"")+\r\n  theme(axis.title.y = element_text(size =12))+\r\n  theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)))+\r\n  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0)))\r\n\r\n\r\n\r\n']","Social information use about novel aposematic prey depends on the intensity of the observed cue Animals gather social information by observing the behavior of others, but how the intensity of observed cues influences decision-making is rarely investigated. This is crucial for understanding how social information influences ecological and evolutionary dynamics. For example, observing a predator's distaste of unpalatable prey can reduce predation by nave birds, and help explain the evolution and maintenance of aposematic warning signals. However, previous studies have only used demonstrators that responded vigorously, showing intense beak-wiping after tasting prey. Therefore, here we conducted an experiment with blue tits (Cyanistes caeruleus) informed by variation in predator responses. First, we found that the response to unpalatable food varies greatly, with only few individuals performing intensive beak-wiping. We then tested how the intensity of beak-wiping influences observers' foraging choices using video-playback of a conspecific tasting a novel conspicuous prey item. Observers were provided social information from: (1) no distaste response, (2) a weak distaste response, or (3) a strong distaste response, and were then allowed to forage on evolutionarily novel (artificial) prey. Consistent with previous studies, we found that birds consumed fewer aposematic prey after seeing a strong distaste response, however a weak response did not influence foraging choices. Our results suggest that while beak-wiping is a salient cue, its information content may vary with cue intensity. Furthermore, the number of potential demonstrators in the predator population might be lower than previously thought, although determining how this influences social transmission of avoidance in the wild will require uncovering the effects of intermediate cue salience.",2
Social learning data in a foraging setting for Heliconius erato,"Insects may acquire social information by active communication and through inadvertent social cues. In a foraging setting, the latter may indicate the presence and quality of resources. Although social learning in foraging contexts is prevalent in eusocial species, this behaviour has been hypothesised to also exist between conspecifics in non-social species with sophisticated behaviours, including Heliconius butterflies. Heliconius are the only butterfly genus with active pollen feeding, a dietary innovation associated with a specialised, spatially faithful foraging behaviour known as trap-lining. Long-standing hypotheses suggest that Heliconius may acquire trap-line information by following experienced individuals. Indeed, Heliconius often aggregate in social roosts, which could act as 'information centres', and present conspecific following behaviour, enhancing opportunities for social learning. Here, we provide a direct test of social learning ability in Heliconius using an associative learning task in which nave individuals completed a colour preference test in the presence of demonstrators trained to feed randomly or with a strong colour preference. We found no evidence that Heliconius erato, which roost socially, used social information in this task. Combined with existing field studies our results add to data which contradict the hypothesised role of social learning in Heliconius foraging behaviour.","['#### Packages ####\n\nlibrary(ggplot2)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(psych)\nlibrary(ggpubr)\nlibrary(DHARMa)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(effects)\n\n#### Adding dataset ####\n\ndata.demonstrators <- read.csv(""Social_learning_demonstrators.csv"", dec = "","", sep = \';\')\nnames(data.demonstrators)\n\ndata.naive <- read.csv(""Social_learning_naive_trials.csv"", dec = "","", sep = \';\')\nnames(data.naive)\n\n\n#### Analysis: demonstrator preferences ####\n### Are group preferences different?\npreference.demonstrators <- glmer(cbind(feeding_P,(feeding_att-feeding_P)) ~ group + \n  (1 | ID), family = binomial, data.demonstrators)\n\nsummary(preference.demonstrators)\n#YES. z = 10.13, p<0.001, n=60, df=57\n\n\n#### Analysis: observer preferences - Trial Day 1 ####\n### Are group preferences different?\npreference.observers.trial1 <- glmer(cbind(feeding_P,(feeding_att-feeding_P)) ~ group + \n                              (1 | ID), family = binomial, subset(data.naive, trial_day == ""1""))\n\nsummary(preference.observers.trial1)\n#NO. z = -0.44, p=0.66, n=30, df=27\n\n\n#### Analysis: observer preferences - Overall (Day 1 to Day 4) ####\n### Did preferences change over time?\npreference.observers.overall <- glmer(cbind(feeding_P,(feeding_att-feeding_P)) ~ trial_day + \n                                       (1 | ID), family = binomial, data.naive)\n\nsummary(preference.observers.overall)\n#YES. z = 3.61, p<0.001, n=35, df=127\n\n\n### Are group preferences different?\n## Model 1: Interaction between trial day and group\npreference.observers.groups <- glmer(cbind(feeding_P,(feeding_att-feeding_P)) ~ trial_day * group + \n                                        (1 | ID), family = binomial, data.naive)\n\nsummary(preference.observers.groups)\n#NO. z = - 0.69, p=0.49, n=35, df=125\n\nplot(allEffects(preference.observers.groups))\n\n## Model 2: With no interaction between trial day and group\npreference.observers.groups.nointer <- glmer(cbind(feeding_P,(feeding_att-feeding_P)) ~ trial_day + group + \n                                       (1 | ID), family = binomial, data.naive)\n\nsummary(preference.observers.groups.nointer)\nplot(allEffects(preference.observers.groups.nointer))\n#NO. z = 0.50, p=0.61, n=35, df=126\n\n## Comparing Models 1 and 2\nanova(preference.observers.groups, preference.observers.groups.nointer)\n#Not different. Model 2 ranked better.\n\n\n#### Analysis: observer preferences - Overall (control and knowledgeable groups separately) ####\n### Control group\npreference.observers.ctrl <- glmer(cbind(feeding_P,(feeding_att-feeding_P)) ~ trial_day + \n                                        (1 | ID), family = binomial, subset(data.naive, group == ""control""))\nsummary(preference.observers.ctrl)\n#YES. z = 3.02, p<0.01, n=17, df=62\n\n\n### Knowledgeable group\npreference.observers.know <- glmer(cbind(feeding_P,(feeding_att-feeding_P)) ~ trial_day + \n                                        (1 | ID), family = binomial, subset(data.naive, group == ""knowledgeable""))\nsummary(preference.observers.know)\n#YES. z = 1.96, p=0.05, n=18, df=62\n\n\n#### Analysis: observers local preference ####\n### Overall - Did naive butterflies copy demonstrators?\nlocal.preference <- glmer(local_preference ~ trial_day + (1 | ID), family = binomial, data.naive)\n\nsummary(local.preference)\n#NO. z = - 0.19, p=0.85, n=35, df=126\n\n\n#### Plot: demonstrator preferences ####\na.demonstrators <- ggplot(data.demonstrators, aes(group, P_percent)) + \n  theme_classic()# + scale_colour_manual(values = c(\'#353b48\', \'#e84118\'))\n\nb.demonstrators <- a.demonstrators + geom_jitter(size =2, alpha = .25, width = 0.02) +\n  stat_summary(\n    geom = \'point\',\n    fun = \'mean\',\n    size = 4.5,\n    shape = 15) +\n  geom_smooth(method = ""lm"", se = FALSE) +\n  stat_summary(geom = \'errorbar\', \n               fun.data = \'mean_cl_normal\', fun.args = list(mult = 2), position = position_dodge(width=.05), width = 0.1) \n\nplot.demonstrators <- b.demonstrators + ylab(""Preference for rewarding colour"") + xlab(""Demonstrators\' Group"") +\n  theme(axis.text=element_text(size=10), axis.title=element_text(size=11,face=""bold""), \n        plot.title = element_text(size=12, hjust=0.5)) + ylim(-0.05, 1.05)\n\nplot.demonstrators\n\n\n#### Plot: observer preferences - Trial Day 1 ####\na.TD1 <- ggplot(subset(data.naive, trial_day == ""1""), aes(group, P_percent)) + \n  theme_classic()# + scale_colour_manual(values = c(\'#353b48\', \'#e84118\'))\n\nb.TD1 <- a.TD1 + geom_jitter(size =2, alpha = .25, width = 0.02) +\n  stat_summary(\n    geom = \'point\',\n    fun = \'mean\',\n    size = 4.5,\n    shape = 15) +\n  geom_smooth(method = ""lm"", se = FALSE) +\n  stat_summary(geom = \'errorbar\', \n               fun.data = \'mean_cl_normal\', fun.args = list(mult = 2), position = position_dodge(width=.05), width = 0.1) \n\nplot.observers.trial1 <- b.TD1 + ylab(""Observers\' preference on trial day 1"") + xlab(""Group"") +\n  theme(axis.text=element_text(size=10), axis.title=element_text(size=11,face=""bold""), \n        plot.title = element_text(size=12, hjust=0.5)) + ylim(-0.05, 1.05)\n\nplot.observers.trial1\n\n\n#### Plot: observers preferences - Day 1 to Day 4 ####\na.overall <- ggplot(data.naive, aes(trial_day, P_percent)) + \n  theme_classic()# + scale_colour_manual(values = c(\'#353b48\', \'#e84118\'))\n\nb.overall <- a.overall + geom_jitter(size =2, alpha = .25, width = 0.02) +\n  stat_summary(\n    geom = \'point\',\n    fun = \'mean\',\n    size = 4.5,\n    shape = 15) +\n  geom_smooth(method = ""lm"", se = FALSE, colour=\'grey\') +\n  stat_summary(geom = \'errorbar\', \n               fun.data = \'mean_cl_normal\', fun.args = list(mult = 2), position = position_dodge(width=.05), width = 0.1) \n\nplot.observers.overall <- b.overall + ylab(\'Preference for rewarding colour\') + xlab(""Trial (day)"") +\n  theme(axis.text=element_text(size=10), axis.title=element_text(size=11,face=""bold""), \n        plot.title = element_text(size=12, hjust=0.5)) + ylim(-0.05, 1.05) + geom_line(aes(group = ID), color = \'grey\')\n\nplot.observers.overall\n\n#### Plot: Control group ####\na.control <- ggplot(subset(data.naive, group == ""control""), aes(trial_day, P_percent)) + \n  theme_classic()# + scale_colour_manual(values = c(\'#353b48\', \'#e84118\'))\n\nb.control <- a.control + geom_jitter(size =2, alpha = .25, width = 0.02) +\n  stat_summary(\n    geom = \'point\',\n    fun = \'mean\',\n    size = 4.5,\n    shape = 15) +\n  geom_smooth(method = ""lm"", se = FALSE, colour=\'dark grey\') +\n  stat_summary(geom = \'errorbar\', \n               fun.data = \'mean_cl_normal\', fun.args = list(mult = 2), position = position_dodge(width=.05), width = 0.1) \n\nplot.observers.control <- b.control + ylab(\'Preference for rewarding colour\') + xlab(""Trial (day)"") + ggtitle(""(a) control group"") +\n  theme(axis.text=element_text(size=10), axis.title=element_text(size=11,face=""bold""), \n        plot.title = element_text(size=12, hjust=0.5)) + ylim(-0.05, 1.05) + geom_line(aes(group = ID), color = \'light grey\')\n\nplot.observers.control\n\n#### Plot: Knowledgeable group #### \na.knowledgeable <- ggplot(subset(data.naive, group == ""knowledgeable""), aes(trial_day, P_percent)) + \n  theme_classic()# + scale_colour_manual(values = c(\'#353b48\', \'#e84118\'))\n\nb.knowledgeable <- a.knowledgeable + geom_jitter(size =2, alpha = .25, width = 0.02) +\n  stat_summary(\n    geom = \'point\',\n    fun = \'mean\',\n    size = 4.5,\n    shape = 15) +\n  geom_smooth(method = ""lm"", se = FALSE, colour=\'dark grey\') +\n  stat_summary(geom = \'errorbar\', \n               fun.data = \'mean_cl_normal\', fun.args = list(mult = 2), position = position_dodge(width=.05), width = 0.1) \n\nplot.observers.knowledgeable <- b.knowledgeable + ylab(\'Preference for rewarding colour\') + xlab(""Trial (day)"") + ggtitle(""(b) knowledgeable group"") +\n  theme(axis.text=element_text(size=10), axis.title=element_text(size=11,face=""bold""), \n        plot.title = element_text(size=12, hjust=0.5)) + ylim(-0.05, 1.05) + geom_line(aes(group = ID), color = \'light grey\')\n\nplot.observers.knowledgeable\n\nggarrange(plot.observers.control, plot.observers.knowledgeable, nrow = 1)\n\n#### End of Analysis ####']","Social learning data in a foraging setting for Heliconius erato Insects may acquire social information by active communication and through inadvertent social cues. In a foraging setting, the latter may indicate the presence and quality of resources. Although social learning in foraging contexts is prevalent in eusocial species, this behaviour has been hypothesised to also exist between conspecifics in non-social species with sophisticated behaviours, including Heliconius butterflies. Heliconius are the only butterfly genus with active pollen feeding, a dietary innovation associated with a specialised, spatially faithful foraging behaviour known as trap-lining. Long-standing hypotheses suggest that Heliconius may acquire trap-line information by following experienced individuals. Indeed, Heliconius often aggregate in social roosts, which could act as 'information centres', and present conspecific following behaviour, enhancing opportunities for social learning. Here, we provide a direct test of social learning ability in Heliconius using an associative learning task in which nave individuals completed a colour preference test in the presence of demonstrators trained to feed randomly or with a strong colour preference. We found no evidence that Heliconius erato, which roost socially, used social information in this task. Combined with existing field studies our results add to data which contradict the hypothesised role of social learning in Heliconius foraging behaviour.",2
Social hierarchy reveals thermoregulatory trade-offs in response to repeated stressors,"Coping with stressors can require substantial energetic investment, and when resources are limited, such investment can preclude simultaneous expenditure on other biological processes. Among endotherms, energetic demands of thermoregulation can also be immense, yet our understanding of whether a stress response is sufficient to induce changes in thermoregulatory investment is limited. Using the black-capped chickadee as a model species, we tested a hypothesis that stress-induced changes in surface temperature (Ts), a well-documented phenomenon across vertebrates, stem from trade-offs between thermoregulation and stress responsiveness. Because social subordination is known to constrain access to resources in this species, we predicted that Ts and dry heat loss of social subordinates, but not social dominants, would fall under stress exposure at low ambient temperatures (Ta), and rise under stress exposure at high Ta, thus permitting a reduction in total energetic expenditure toward thermoregulation. To test our predictions, we exposed four social groups of chickadees to repeated stressors and control conditions across a Ta gradient (n=30 days/treatment/group), whilst remotely monitoring social interactions and Ts. Supporting our hypothesis, we show that: (1) social subordinates (n=12), who fed less than social dominants and alone experienced stress-induced mass-loss, displayed significantly larger changes in Ts following stress exposure than social dominants (n=8), and (2) stress-induced changes in Ts significantly increased heat conservation at low Ta and heat dissipation at high Ta among social subordinates alone. These results suggest that chickadees adjust their thermoregulatory strategies during stress exposure when resources are limited by ecologically relevant processes.",,"Social hierarchy reveals thermoregulatory trade-offs in response to repeated stressors Coping with stressors can require substantial energetic investment, and when resources are limited, such investment can preclude simultaneous expenditure on other biological processes. Among endotherms, energetic demands of thermoregulation can also be immense, yet our understanding of whether a stress response is sufficient to induce changes in thermoregulatory investment is limited. Using the black-capped chickadee as a model species, we tested a hypothesis that stress-induced changes in surface temperature (Ts), a well-documented phenomenon across vertebrates, stem from trade-offs between thermoregulation and stress responsiveness. Because social subordination is known to constrain access to resources in this species, we predicted that Ts and dry heat loss of social subordinates, but not social dominants, would fall under stress exposure at low ambient temperatures (Ta), and rise under stress exposure at high Ta, thus permitting a reduction in total energetic expenditure toward thermoregulation. To test our predictions, we exposed four social groups of chickadees to repeated stressors and control conditions across a Ta gradient (n=30 days/treatment/group), whilst remotely monitoring social interactions and Ts. Supporting our hypothesis, we show that: (1) social subordinates (n=12), who fed less than social dominants and alone experienced stress-induced mass-loss, displayed significantly larger changes in Ts following stress exposure than social dominants (n=8), and (2) stress-induced changes in Ts significantly increased heat conservation at low Ta and heat dissipation at high Ta among social subordinates alone. These results suggest that chickadees adjust their thermoregulatory strategies during stress exposure when resources are limited by ecologically relevant processes.",2
"Code and data from: Familiarity, dominance, sex and season shape common waxbill social networks.","In gregarious animals, social network positions of individuals may influence their life-history and fitness. Although association patterns and the position of individuals in social networks can be shaped by phenotypic differences and by past interactions, few studies have quantified their relative importance. We evaluated how phenotypic differences and familiarity influence social preferences and the position of individuals within the social network. We monitored wild-caught common waxbills (Estrilda astrild) with radio-frequency identifiers in a large mesocosm during the non-breeding and breeding seasons of two consecutive years. We found that social networks were similar, and that the centrality of individuals was repeatable, across seasons and years, indicating a stable social phenotype. Nonetheless, there were seasonal changes in social structure: waxbills associated more strongly with opposite-sex individuals in breeding seasons, while in non-breeding seasons they instead assorted according to similarities in social dominance. We also observed stronger assortment between birds that were introduced to the mesocosm at the same time, indicating long-lasting bonds among familiar individuals. Waxbills that had been introduced to the mesocosm more recently occupied more central network positions, especially during breeding seasons, perhaps indicating that these birds had less socially-differentiated associations with flock members. Finally, individual differences in color ornamentation and behavioral assays of personality, inhibitory control and stress were not related to network centrality or association patterns. Together, these results suggest that, in gregarious species like the common waxbill, social networks may be more strongly shaped by long-lasting associations with familiar individuals than by phenotypic differences among group members.",,"Code and data from: Familiarity, dominance, sex and season shape common waxbill social networks. In gregarious animals, social network positions of individuals may influence their life-history and fitness. Although association patterns and the position of individuals in social networks can be shaped by phenotypic differences and by past interactions, few studies have quantified their relative importance. We evaluated how phenotypic differences and familiarity influence social preferences and the position of individuals within the social network. We monitored wild-caught common waxbills (Estrilda astrild) with radio-frequency identifiers in a large mesocosm during the non-breeding and breeding seasons of two consecutive years. We found that social networks were similar, and that the centrality of individuals was repeatable, across seasons and years, indicating a stable social phenotype. Nonetheless, there were seasonal changes in social structure: waxbills associated more strongly with opposite-sex individuals in breeding seasons, while in non-breeding seasons they instead assorted according to similarities in social dominance. We also observed stronger assortment between birds that were introduced to the mesocosm at the same time, indicating long-lasting bonds among familiar individuals. Waxbills that had been introduced to the mesocosm more recently occupied more central network positions, especially during breeding seasons, perhaps indicating that these birds had less socially-differentiated associations with flock members. Finally, individual differences in color ornamentation and behavioral assays of personality, inhibitory control and stress were not related to network centrality or association patterns. Together, these results suggest that, in gregarious species like the common waxbill, social networks may be more strongly shaped by long-lasting associations with familiar individuals than by phenotypic differences among group members.",2
Data Science in Undergraduate Life Science Education: A Need for Instructor Skills Training - Associated data and code,"The following files are the data set and code for analysis and visualization associated with the manuscript ""Data Science in Undergraduate Life Science Education: A Need for Instructor Skills Training.""",,"Data Science in Undergraduate Life Science Education: A Need for Instructor Skills Training - Associated data and code The following files are the data set and code for analysis and visualization associated with the manuscript ""Data Science in Undergraduate Life Science Education: A Need for Instructor Skills Training.""",2
Key words related to public engagement with science by Society of Freshwater Science journals and conference sessions (1997-2019),"Data set that was used to determine the frequency each of 4 key words (public engagement, education, outreach, or science communication) in the title or abstract of published papers in Freshwater Science (formerly the Journal of the North American Benthological Society) and oral presentations (talks) at the annual Society for Freshwater Science meetings from 1997 to 2019. Does not include any data on talks for 2013-2014 because they were not published during those years.","['#Burdett et al. 2021\r\n#Figure 1 creation code\r\n# Dec 2019\r\n\r\nlibrary(tidyverse)\r\nlibrary(RColorBrewer)\r\nlibrary(wesanderson)\r\nlibrary(patchwork)\r\nlibrary(ggpubr)\r\nlibrary(ggpattern)\r\nlibrary(lemon)\r\n\r\n#To create Figure 1: Stacked bar chart\r\n#Load and tidy data\r\ndf<-read_csv(""Burdett_et_al_2021_Figure_1_Data.csv"")\r\ndf$Keyword <- factor(df$Keyword, levels=c(""Public engagement"", ""Education"", ""Outreach"", ""Science communication""))\r\ntalk<-subset(df, Type==""Talks"")\r\npaper<-subset(df, Type==""Papers"")\r\ndf$Year<-as.factor(df$Year)\r\ndf$Type<-factor(df$Type, levels=c(""Papers"", ""Talks""))\r\n\r\n## Generate plots with facet\r\n#Color version\r\nfig_col<-ggplot(df, aes(x = Count, y = reorder(Year, desc(Year)), fill = Keyword)) +\r\n  geom_col() +theme_classic()+\r\n  facet_rep_grid(. ~ Type) + \r\n  coord_capped_cart(bottom=\'both\', left=\'both\')+\r\n  ylab(""Year"")+\r\n  scale_fill_manual(values=c(""#fdae61"", ""#2c7bb6"", ""#d7191c"", ""#abd9e9""))+\r\n  labs(fill = ""Key word"")+\r\n  theme(axis.title.x=element_text(size=16), \r\n        axis.title.y = element_text(size=16), \r\n        axis.text.x = element_text(size=12, vjust=0.5), \r\n        axis.text.y=element_text(size=12),\r\n        strip.background = element_blank(),\r\n        strip.text.x = element_text(size=16, face=""bold"", hjust=-0.02),\r\n        legend.text=element_text(size=12),\r\n        legend.justification=c(1,1), legend.position=c(0.98,0.98),\r\n        legend.title = element_text(size=12, face=""bold""))\r\nfig_col\r\n\r\nggsave(""C:/Users/owner/Desktop/SFS/Fig1_Color.tiff"",\r\n       plot = fig_col,\r\n       width=7.5,\r\n       height=5,\r\n       units=c(""in""),\r\n       dpi = 600)\r\n\r\n#Greyscale version\r\nfig_gry<-ggplot(df, aes(x = Count, y = reorder(Year, desc(Year)), fill = Keyword)) +\r\n  geom_col() +theme_classic()+\r\n  facet_rep_grid(. ~ Type) + \r\n  coord_capped_cart(bottom=\'both\', left=\'both\')+\r\n  ylab(""Year"")+\r\n  scale_fill_grey()+\r\n  labs(fill = ""Key word"")+\r\n  theme(axis.title.x=element_text(size=16), \r\n        axis.title.y = element_text(size=16), \r\n        axis.text.x = element_text(size=12, vjust=0.5), \r\n        axis.text.y=element_text(size=12),\r\n        strip.background = element_blank(),\r\n        strip.text.x = element_text(size=16, face=""bold"", hjust=-0.02),\r\n        legend.text=element_text(size=12),\r\n        legend.justification=c(1,1), legend.position=c(0.98,0.98),\r\n        legend.title = element_text(size=12, face=""bold""))\r\nfig_gry\r\n\r\nggsave(""C:/Users/owner/Desktop/SFS/Fig1_Grey_12.12.tiff"",\r\n       plot = fig_gry,\r\n       width=7.5,\r\n       height=5,\r\n       units=c(""in""),\r\n       dpi = 600)\r\n']","Key words related to public engagement with science by Society of Freshwater Science journals and conference sessions (1997-2019) Data set that was used to determine the frequency each of 4 key words (public engagement, education, outreach, or science communication) in the title or abstract of published papers in Freshwater Science (formerly the Journal of the North American Benthological Society) and oral presentations (talks) at the annual Society for Freshwater Science meetings from 1997 to 2019. Does not include any data on talks for 2013-2014 because they were not published during those years.",2
"Data from: Genetic Monogamy despite frequent extra-pair Copulations  in ""strictly monogamous"" wild Jackdaws","""Monogamy"" refers to different components of pair exclusiveness: the social pair, sexual partners, and the genetic outcome of sexual encounters. Avian monogamy is usually defined socially or genetically, while quantifications of sexual behavior remain scarce. Jackdaws (Corvus monedula) are considered a rare example of strict monogamy in songbirds, with lifelong pair bonds and little genetic evidence for extra-pair offspring. Yet jackdaw copulations, although accompanied by loud copulation calls, are rarely observed, since they occur visually concealed inside nest cavities. Using full-day nest-box video surveillance and on-bird acoustic bio-logging, we directly observed jackdaw sexual behavior and compared it to the corresponding genetic outcome obtained via molecular parentage analysis. In the video-observed nests, we found genetic monogamy, but frequently detected forced extra-pair sexual behavior, accompanied by characteristic male copulation calls. We thus challenge the long-held notion of strict jackdaw monogamy at the sexual level. Our data suggest that male mate-guarding and frequent intra-pair copulations during the female fertile phase, as well as the forced nature of the copulations could explain the absence of extra-pair offspring. Since extra-pair copulation behavior appeared to be costly for both sexes, we suggest that immediate fitness benefits are an unlikely explanation for its prevalence. Instead, sexual conflict and dominance effects could interact to shape the spatio-temporal pattern of extra-pair sexual behavior in this species. Our results call for larger-scale investigations of jackdaw sexual behavior and parentage, and highlight the importance of combining social, sexual and genetic datasets for a more complete understanding of mating systems.","['setwd(\'X:/Dropbox (Personal)/crypticfolder_encrypted/Doktorarbeit/R/JD_workspace/\')\r\nsetwd(\'C:/Users/gill/Dropbox (Personal)/JaapLisa/Review3/R\')\r\n\r\nrequire(ggplot2)\r\nrequire(doBy)\r\nrequire(lme4)\r\nrequire(arm)\r\n\r\n## Fig. 1, Model 1, Table 1: Resident females spent most time alone inside their nest-box post-fertile ---------------------------------------------\r\nfertile <- read.csv(\'1_fem_fertile.csv\')\r\nfertile$date <- as.Date(fertile$date)\r\ndursumm <- read.csv(\'2_2017_cam_all_summary_timeinnest.csv\')\r\ndursumm$date <- as.Date(dursumm$date)\r\ndursumm$dur <- as.numeric(as.character(dursumm$dur))\r\ndursumm$Female <- fertile$female_henderson[match(paste(dursumm$cam, dursumm$date), paste(fertile$cam, fertile$date))]\r\ndursumm$Female <- factor(dursumm$Female, levels =c(\'pre-fertile\', \'fertile\', \'post-fertile\'))\r\ndursumm$tabsimplestage <- factor(dursumm$tabsimplestage,levels=c(\'pre-laying\',\'laying\',\'incubating\'))\r\ndursumm$dur_int <- as.integer(as.character(dursumm$dur))\r\ndursumm$hours <- dursumm$dur/60\r\ndursumm1 <- dursumm[!(dursumm$date %in% unique(dursumm$date)[1] & dursumm$cam %in% \'cam09\') ,] #remove NAs (missing observations)\r\n\r\n\r\nplotmod <- function(x)\r\n{ par(mfrow=c(2,2))\r\n  plot(fitted(mod), resid(mod))\r\n  abline(h=0, lty=2)\r\n  qqnorm(resid(mod), main=\'normal qq-plot, residuals\')\r\n  qqline(resid(mod))\r\n  scatter.smooth(fitted(mod), sqrt(abs(resid(mod))))\r\n  qqnorm(unlist(ranef(mod)), main=\'normal qq-plot, random effects\')\r\n  qqline(unlist(ranef(mod)))\r\n  par(mfrow=c(1,1))}\r\n\r\n#Model 1:\r\nmod <- lmer(log(dur+1) ~ whos_in * Female + (1|Nest.box), data = dursumm1)\r\nmodnull<- lmer(log(dur+1) ~ whos_in + Female + (1|Nest.box), data = dursumm1)\r\nmodbasic <- lmer(log(dur+1) ~ 1 + (1|Nest.box) , data = dursumm1)\r\nanova(mod, modnull, modbasic)\r\n# refitting model(s) with ML (instead of REML)\r\n# Data: dursumm1\r\n# Models:\r\n#   modbasic: log(dur + 1) ~ 1 + (1 | Nest.box)\r\n# modnull: log(dur + 1) ~ whos_in + Female + (1 | Nest.box)\r\n# mod: log(dur + 1) ~ whos_in * Female + (1 | Nest.box)\r\n# Df    AIC    BIC   logLik deviance  Chisq Chi Df Pr(>Chisq)    \r\n# modbasic  3 2110.2 2123.0 -1052.12   2104.2                             \r\n# modnull   7 1890.5 1920.3  -938.23   1876.5 227.77      4  < 2.2e-16 ***\r\n#   mod      11 1448.5 1495.3  -713.22   1426.5 450.01      4  < 2.2e-16 ***\r\n#   ---\r\n#   Signif. codes:  0 \'***\' 0.001 \'**\' 0.01 \'*\' 0.05 \'.\' 0.1 \' \' 1\r\n\r\nsummary(mod)\r\nplot(resid(mod))\r\nplotmod(mod)\r\nfixef(mod)\r\nranef(mod)\r\nnsim <- 2000\r\nbsim <- sim(mod, n.sim=nsim)\r\napply(bsim@fixef,2,quantile,prob=c(0.025,0.5,0.975))\r\n\r\n#credible intervals:\r\nnewdat <- expand.grid(Female=levels(dursumm$Female), whos_in =levels(dursumm$whos_in))\r\nXmat <- model.matrix(~whos_in*Female, data=newdat)\r\ndim(Xmat)\r\npredmat <- matrix(ncol=nsim, nrow=nrow (newdat))\r\nnewdat$pred <- Xmat%*% fixef(mod)\r\nfor(i in 1:nsim) predmat[,i] <-(Xmat%*%bsim@fixef[i,])\r\nnewdat$lower <- apply(predmat, 1, quantile, prob=0.025)\r\nnewdat$upper <- apply(predmat, 1, quantile, prob=0.975)\r\n\r\n#Table 1:\r\nnewdat\r\n\r\n#Fig. 1:\r\nggplot()+\r\n  geom_jitter(data= dursumm1, aes(x=Female, y=dur+1, shape =Nest.box), height=0, width =0.15, size =1.8, colour=\'gray60\')+\r\n  facet_wrap(~whos_in)+\r\n  scale_shape_manual(name  =""Nest-box"", breaks=levels(dursumm$Nest.box),labels=\r\n                       levels(dursumm$Nest.box), values =c(0:4,6,8,9))+\r\n  theme_bw()+  #theme(axis.text=element_text(size=14,colour=\'black\'),axis.title=element_text(size=16,face=""bold""))+\r\n  geom_point(data=newdat, aes(x= Female, y= exp(pred),colour= whos_in), size = 3)+\r\n  geom_segment(data=newdat, aes(x = Female, y = exp(newdat$lower), xend = Female, \r\n                                yend = exp(newdat$upper), colour = whos_in), size =1.5)+\r\n  scale_y_log10(name=(\'Time spent in nest [minutes]\'))\r\n\r\n# ## R2 following Nakagawa and Schielzeth 2013:\r\nRS <- function(x){\r\n  Fixed <- fixef(mod)[2] * getME(mod,""X"")[, 2] + fixef(mod)[3] * getME(mod,""X"")[, 3] +\r\n    fixef(mod)[4] * getME(mod,""X"")[, 4] + fixef(mod)[5] * getME(mod,""X"")[, 5]+\r\n    fixef(mod)[6] * getME(mod,""X"")[, 6] + fixef(mod)[7] * getME(mod,""X"")[, 7]+\r\n    fixef(mod)[8] * getME(mod,""X"")[, 8] + fixef(mod)[9] * getME(mod,""X"")[, 9]\r\n  VarF <- var(Fixed)\r\n paste(""marginal: "", VarF/(VarF + VarCorr(mod)$Nest.box[1] + attr(VarCorr(mod), ""sc"")^2),""; "",\r\n ""conditional: "",(VarF + VarCorr(mod)$Nest.box[1])/(VarF + VarCorr(mod)$Nest.box[1] +\r\n                                                    (attr(VarCorr(mod), ""sc"")^2)), sep = """")\r\n }\r\n RS(mod) # =  ""marginal: 0.72193172482612; conditional: 0.721931724826121""\r\n\r\n\r\n \r\n## Fig. 2, Model 2, Table 2: Within pairs, full copulations were increasingly replaced by copulation attempts with progressing breeding ---------------------------------------------\r\n fertile <- read.csv(\'1_fem_fertile.csv\')\r\n fertile$date <- as.Date(fertile$date)\r\n fertile$nr_EPevents <- as.numeric(as.character(fertile$nr_EPevents))\r\n fertile$nr_IPevents <- as.numeric(as.character(fertile$nr_IPevents))\r\n \r\n cops <- read.csv(\'3_2017_copulations_datetime.csv\')\r\n cops$date <- as.Date(cops$date) \r\n cops$female_henderson <- fertile$female_henderson[(match(paste(cops$date, cops$cam),\r\n                                                          paste(fertile$date,fertile$cam)))]\r\n copsummary <- data.frame(table(cops$Nest.box, cops$IEP,  cops$att.cop, cops$female_henderson))\r\n names(copsummary) <- c(""Nest.box"",""IEP"",""att.cop"",""Female"",""nr_events"")\r\n copsummary$Female <- factor(copsummary$Female, levels =c(\'pre-fertile\', \'fertile\', \'post-fertile\'))\r\n IP <-   copsummary[copsummary$IEP ==\'IP_IP\',]\r\n \r\n #proportion of copulations out of all events:\r\n IPatt <- IP[IP$att.cop ==\'attempts\',]\r\n IPcop <- IP[IP$att.cop ==\'copulations\',]\r\n IPatt$nr_atts <- IPatt$nr_events\r\n IPatt$nr_cops <- IPcop$nr_events[match(paste(IPatt$Nest.box, IPatt$Female),paste(IPcop$Nest.box, IPcop$Female))]\r\n IPatt$nr_events <- IPatt$nr_atts+ IPatt$nr_cops\r\n IPatt$p <- IPatt$nr_cops/(IPatt$nr_atts+IPatt$nr_cops)\r\n \r\n plotmod <- function(x)\r\n { par(mfrow=c(2,2))\r\n   plot(fitted(mod), resid(mod))\r\n   abline(h=0, lty=2)\r\n   qqnorm(resid(mod), main=\'normal qq-plot, residuals\')\r\n   qqline(resid(mod))\r\n   scatter.smooth(fitted(mod), sqrt(abs(resid(mod))))\r\n   qqnorm(unlist(ranef(mod)), main=\'normal qq-plot, random effects\')\r\n   qqline(unlist(ranef(mod)))\r\n   par(mfrow=c(1,1))}\r\n \r\n \r\n ###binomial:\r\n mod <- glmer(cbind(IPatt$nr_cops, IPatt$nr_atts) ~ Female + (1|Nest.box) , data= IPatt, family =binomial)\r\n modnull <- glmer(cbind(IPatt$nr_cops, IPatt$nr_atts) ~1+ (1|Nest.box) , data= IPatt, family =binomial)\r\n anova(modnull, mod, test =\'Chisq\')\r\n \r\n summary(mod)\r\n anova(mod)\r\n plot(density(resid(mod)))\r\n plotmod(mod)\r\n \r\n # ## R2 following Nakagawa and Schielzeth 2013:\r\n RS <- function(x){\r\n   Fixed <- fixef(mod)[2] * getME(mod,""X"")[, 2] + fixef(mod)[3] * getME(mod,""X"")[, 3]\r\n   VarF <- var(Fixed)\r\n   paste(""marginal: "", VarF/(VarF + VarCorr(mod)$Nest.box[1] + attr(VarCorr(mod), ""sc"")^2),""; "",\r\n         ""conditional: "",(VarF + VarCorr(mod)$Nest.box[1])/(VarF + VarCorr(mod)$Nest.box[1] +\r\n                                                              (attr(VarCorr(mod), ""sc"")^2)), sep = """")\r\n }\r\n RS(mod) #  ""marginal: 0.233897876050841; conditional: 0.734229141847923""\r\n \r\n \r\n nsim <- 2000\r\n bsim <- sim(mod, n.sim=nsim)\r\n apply(bsim@fixef,2,quantile,prob=c(0.025,0.975))\r\n # (Intercept) Femalefertile Femalepost-fertile\r\n # 2.5%   -0.4916125   -1.41822482          -2.943497\r\n # 97.5%   1.9231908    0.07762834          -1.399099\r\n \r\n \r\n newdat <- expand.grid(Female=levels(IPatt$Female))\r\n Xmat <- model.matrix(~Female, data=newdat)\r\n dim(Xmat)\r\n predmat <- matrix(ncol=nsim, nrow=nrow (newdat))\r\n \r\n for(i in 1:nsim) predmat[,i] <- plogis(Xmat %*% bsim@fixef [i,])\r\n newdat$lower <- apply(predmat, 1, quantile, prob=0.025)\r\n newdat$upper <- (apply(predmat, 1, quantile, prob=0.975))\r\n newdat$pred <- plogis(Xmat %*% fixef(mod))\r\n \r\n #Table 2:\r\n newdat \r\n # Female      lower     upper      pred\r\n # pre-fertile 0.38850976 0.8840796 0.6765385\r\n #     fertile 0.28249401 0.7378330 0.5113340\r\n #post-fertile 0.07647847 0.3901926 0.1881760\r\n \r\n #Fig. 2:\r\n ggplot()+\r\n   geom_point(data= IPatt, aes(x=Female, y=p, shape =Nest.box,colour = Nest.box) ,size =3)+\r\n   scale_shape_manual(name  =""Nest-box"", breaks=levels(IPatt$Nest.box),labels=\r\n                        levels(IPatt$Nest.box), values =c(0:4,6,8,9))+\r\n   geom_line(data=IPatt, aes(x=Female, y=p, group = Nest.box, colour =Nest.box))+\r\n   \r\n   theme_bw()+  scale_x_discrete(name=(\'Female\'))+scale_y_continuous(name=\'IP cop/(IP cop + IP att)\')+\r\n   geom_point(aes(x= c(1:3)+.1, y= c((newdat$pred))), size = 3)+\r\n   geom_segment(aes(x = c(1:3)+.1, y = c((newdat$lower)), xend = c(1:3)+.1,\r\n                    yend = c((newdat$upper))), size =1.2)\r\n \r\n \r\n \r\n \r\n\r\n## Fig. 3, correlations: Negative association between a pair\'s chick fledging success and the occurrences of extra-pair sexual behaviour ---------------------------------------------\r\nnests2017 <- read.csv(\'4_workfile_2017_nests.csv\')\r\nfertile <- read.csv(\'1_fem_fertile.csv\')\r\nfertile$date <- as.Date(fertile$date)\r\nfertile$nr_EPevents <- as.numeric(as.character(fertile$nr_EPevents))\r\nfertile$nr_IPevents <- as.numeric(as.character(fertile$nr_IPevents))\r\n\r\nnestscams <- nests2017[nests2017$Nest.box %in% c(levels(nests2017$Nest.box)),]\r\nnestscams$p <- nestscams$max_fledged/nestscams$max_chicks\r\nnestscams$survived <- nestscams$num_fledged\r\nnestscams$died <- nestscams$max_chicks - nestscams$survived\r\n\r\n\r\n#Check: number of EP events in relation to clutch size: \r\ncor(nestscams$nr_EPevents, nestscams$max_eggs) #0.02099755\r\ncor.test(nestscams$nr_EPevents, nestscams$max_eggs)\r\n# Pearson\'s product-moment correlation\r\n# \r\n# data:  nestscams$nr_EPevents and nestscams$max_eggs\r\n# t = 0.051445, df = 6, p-value = 0.9606\r\n# alternative hypothesis: true correlation is not equal to 0\r\n# 95 percent confidence interval:\r\n# -0.6939432  0.7150897\r\n# sample estimates:\r\n# cor \r\n# 0.02099755 \r\n\r\n#Fig.  (not shown in paper)\r\nggplot()+ geom_point(data = nestscams ,\r\n                     aes(x=nr_EPevents, y= max_eggs, shape =Nest.box),size=3, stroke =1.1)+#, width =0, height = 0.4)+\r\n  scale_shape_manual(name  =""Nest-box"", breaks=levels(nestscams$Nest.box),labels=\r\n                       levels(nestscams$Nest.box), values =c(0:4,6,8,9))+\r\n  scale_colour_manual(values =c(\'black\',\'blue\'))+\r\n  theme_bw()+\r\n  scale_y_continuous(name=(\'Clutch size\'))+ scale_x_continuous(name=(\'EP events\'))+\r\ngeom_abline(intercept= 4.843373, slope = 0.006487)\r\n\r\n\r\n\r\n# Correlation: number of EP events in relation to chick survival probability (per nest)\r\ncor(nestscams$nr_EPevents , nestscams$p) # -0.7693077\r\ncor.test(nestscams$nr_EPevents , nestscams$p)\r\n# Pearson\'s product-moment correlation\r\n# \r\n# data:  nestscams$nr_EPevents and nestscams$p\r\n# t = -2.9496, df = 6, p-value = 0.02563\r\n# alternative hypothesis: true correlation is not equal to 0\r\n# 95 percent confidence interval:\r\n# -0.9558205 -0.1411578\r\n# sample estimates:\r\n# cor \r\n# -0.7693077 \r\n\r\n# Fig. 3:\r\nggplot()+ geom_point(data = nestscams ,\r\n                     aes(x=nr_EPevents, y= p, shape =Nest.box),size=3, stroke =1.1)+#, width =0, height = 0.4)+\r\n  scale_shape_manual(name  =""Nest-box"", breaks=levels(nestscams$Nest.box),labels=\r\n                       levels(nestscams$Nest.box), values =c(0:4,6,8,9))+\r\n  theme_bw(base_size = 18)+\r\n  scale_y_continuous(name=(\'Proportion of chicks fledged\'))+ scale_x_continuous(name=(\'EP events\'))+\r\n  geom_abline(intercept= 0.41807, slope = -0.03832) +\r\n  annotate(""text"", x=9, y=0.65, label= ""Pearson\'s r: -0.7693077"", size = 6)\r\n  \r\n\r\n\r\n## Fig.S1: Nest-box occupancy and extra-pair (EP) sexual behaviour over breeding stages ---------------------------------------------\r\n\r\n#time spent:\r\ndur <- read.csv(\'5_workfile_all_durations.csv\'); \r\ndur$date <- as.Date(dur$date, format =\'%Y-%m-%d\');\r\nalone <- read.csv(\'6_2017_fem_alone_summary.csv\')\r\nalone$date <- as.Date(alone$date, format =\'%Y-%m-%d\')\r\nalone$pc_alone <- (alone$time_fem_alone*60)/alone$timeinnest\r\n\r\n#breeding stages:\r\ntabsimplestage <- read.csv(\'7_workfile_breedingstages.csv\')\r\ntabsimplestage$date <- as.Date(tabsimplestage$date, format =\'%Y-%m-%d\')\r\ntabsimplestage <- tabsimplestage[tabsimplestage$sum >0,]\r\ntabsimplestage$simplestage <- factor(tabsimplestage$simplestage,levels=c(\'pre-laying\',\'laying\',\'incubating\'))\r\n\r\n#chick information:\r\nnn1 <- read.csv(\'8_summary_nestchecks_filmedcams.csv\')\r\nnn1$date_hatched <- as.Date(nn1$date_hatched);\r\nnn1$date_laid <- as.Date(nn1$date_laid)\r\n\r\n#copulations:\r\ncops <- read.csv(\'9_2017_copulations_datetime.csv\'); \r\n\r\ncops$date <- as.Date(cops$date, format =\'%Y-%m-%d\')\r\ncops <- cops[cops$IEP %in% c(\'EP_EP\',\'EP_IP\', \'IP_IP\'),];cops$IEP <- factor(cops$IEP, levels =c(\'IP_IP\',\'EP_IP\', \'EP_EP\'))\r\ncops <- cops[cops$att.cop %in% c(\'attempts\',\'copulations\'),]\r\n\r\n#incoming EPCs:\r\ncops_EP_in <- cops[cops$IEP %in% \'EP_IP\',]\r\ncops_EP_in$simplestage <- tabsimplestage[tabsimplestage$sum >0,]$simplestage[match(paste(cops_EP_in$date, cops_EP_in$camera),\r\n                                                                                   paste(tabsimplestage[tabsimplestage$sum >0,]$date, tabsimplestage[tabsimplestage$sum >0,]$cam))]\r\ncops_EP_in$pc_spent <- dur$pc_spent[match(paste(cops_EP_in$date, cops_EP_in$cam), paste(dur$date, dur$cam))]\r\n\r\n#outgoing EPCs:\r\nresidentmales <- read.csv(\'10_names_residentmales.csv\')\r\ncops_EP_out <- cops_EP_in[cops_EP_in$ring_male %in% residentmales$ring_male,]\r\ncops_EP_out$male_from_cam <- residentmales$cam[match(cops_EP_out$ring_male, residentmales$ring_male)]\r\ncops_EP_out$cam <- cops_EP_out$male_from_cam\r\ncops_EP_out$simplestage <- tabsimplestage[tabsimplestage$sum >0,]$simplestage[match(paste(cops_EP_out$date, cops_EP_out$camera),\r\n                                                                                    paste(tabsimplestage[tabsimplestage$sum >0,]$date, tabsimplestage[tabsimplestage$sum >0,]$cam))]\r\n\r\n# Fig. S1:\r\nmypalette = c(""lightgrey"",""#FFFF99"", ""#F0E442"", ""#999999"")\r\n\r\nggplot()+ ggtitle(""Residents\' time spent in nest, breeding stages, \\n hatching success and EP occurrences"")+ \r\n  theme_bw() + facet_wrap(~Nest.box,ncol=2)+ \r\n  # breeding stages:\r\n  geom_bar(data= tabsimplestage,aes(x=date, y=sum,fill=simplestage), stat=\'identity\')+\r\n  scale_fill_manual(values = mypalette , name=""Breeding stage"")+\r\n  scale_y_continuous(name=\'Proportion of day spent in nest\')+\r\n  \r\n  # time in nest for males (turquoise) and females (red):\r\n  geom_line(data= dur[dur$IP==\'IP\' ,], aes(x=date, y=pc_spent,group = ring, colour=sex),size=1.1)  +  \r\n  \r\n  # eggs hatched:\r\n  geom_jitter(data=nn1[nn1$hatched %in% c(\'yes\'),], aes(y= 0.051, x=date_laid, group=egg.ID), size = 1.5, width = 0.05, height = 0, \r\n              shape =16, stroke =1.5)+\r\n  geom_jitter(data=nn1[nn1$hatched %in% c(\'no\'),], aes(y= 0.04, x=date_laid, group=egg.ID), size = 2, width = 0.05, height = 0, \r\n              shape =1)+\r\n  \r\n  # non-resident males intruding in each camera:\r\n  geom_jitter(data= cops_EP_in, aes(x=date, y=0.85, group = ring_male), colour=\'black\', size=1.8, shape = 25,width = 0.0, height = 0.1, \r\n              stroke =1.25)+\r\n  #resident males being filmed by other cameras:  \r\n  geom_jitter(data= cops_EP_out, aes(x=date, y=0.5, group= ring_male), colour=\'blue\', size=1.8, shape = 24,width = 0.0, height = 0.1,\r\n              stroke =1.25)\r\n\r\n\r\n\r\n\r\n## Figs. S2 and S3:  ---------------------------------------------\r\nfertile <- read.csv(\'1_fem_fertile.csv\')\r\nfertile$date <- as.Date(fertile$date)\r\nfertile$nr_EPevents <- as.numeric(as.character(fertile$nr_EPevents))\r\nfertile$nr_IPevents <- as.numeric(as.character(fertile$nr_IPevents))\r\n\r\ncops <- read.csv(\'9_2017_copulations_datetime.csv\')\r\ncops$date <- as.Date(cops$date, format =\'%Y-%m-%d\')\r\ncops <- cops[cops$IEP %in% c(\'EP_EP\',\'EP_IP\', \'IP_IP\'),];\r\ncops$IEP <- factor(cops$IEP, levels =c(\'IP_IP\',\'EP_IP\', \'EP_EP\'))\r\ncops <- cops[cops$att.cop %in% c(\'attempts\',\'copulations\'),]\r\ncops$Female <- fertile$female_henderson[(match(paste(cops$date, cops$Nest.box),\r\n                                                   paste(fertile$date,fertile$Nest.box)))]\r\ncopsummary <- data.frame(table(cops$Nest.box, cops$IEP,  cops$att.cop, cops$Female))\r\nnames(copsummary) <- c(""Nest.box"",""IEP"",""att.cop"",""Female"",""nr_events"")\r\ncopsummary$Female <- factor(copsummary$Female, levels =c(\'pre-fertile\', \'fertile\', \'post-fertile\'))\r\n\r\nEP <-   copsummary[copsummary$IEP ==\'EP_IP\',]\r\nIP <-   copsummary[copsummary$IEP ==\'IP_IP\',]\r\n\r\ntabsimplestage <- read.csv(\'7_workfile_breedingstages.csv\')\r\ntabsimplestage$date <- as.Date(tabsimplestage$date, format =\'%Y-%m-%d\')\r\ntabsimplestage <- tabsimplestage[tabsimplestage$sum >0,]\r\ntabsimplestage$simplestage <- factor(tabsimplestage$simplestage,levels=c(\'pre-laying\',\'laying\',\'incubating\'))\r\n\r\n\r\ndur <- read.csv(\'5_workfile_all_durations.csv\');\r\ndur$date <- as.Date(dur$date, format =\'%Y-%m-%d\');\r\n#exact times:\r\n cops$timeMM_dec <- cops$timeMM/60\r\n cops$time_dec <- as.numeric(paste(cops$timeHH+cops$timeMM_dec))\r\n#first and last entries:\r\nmin <- read.csv(\'11_entries_exits_summary.csv\')\r\nmin$date <- as.Date(min$date)\r\n\r\n#Fig.S2: Residents\' (F, M) \r\nmypalette = c(""lightgrey"",""#FFFF99"", ""#F0E442"", ""#999999"")\r\n\r\nggplot()+ \r\n  geom_segment(data= tabsimplestage[tabsimplestage$sum >0,], \r\n               aes(x=date, xend=date, y=5, yend=sum*21,colour=simplestage), size =4.5)+\r\n  theme_bw() + facet_wrap(~Nest.box,ncol=2)+ \r\n  scale_color_manual(values = mypalette , name=""Breeding stage"")+\r\n  scale_x_date(name =\'Date\', date_breaks = \'4 days\', date_labels = ""%d/%m"")+\r\n  scale_y_continuous(name=\'Time of day\')+\r\n  ggtitle(""Residents\' first and last entries, breeding stages, time of day and \\n intra- and extra-pair copulation attempts and full copulations"")+ \r\n\r\n#  resident female\'s first and last entry per day (red lines):\r\n  geom_line(data =min[min$sex ==\'f\',], aes (y=earliest, x=date),  colour =\'#F8766D\', size =1)  +\r\n  geom_line(data =min[min$sex ==\'f\',], aes (y=latestexit, x=date),  colour =\'#F8766D\', size =1)  +\r\n#  resident male\'s first and last entry per day (turquoise lines):  \r\n  geom_line(data =min[min$sex ==\'m\',], aes (y=latestexit, x=date),  colour =\'#00BFC4\', size =1)  +\r\n  geom_line(data =min[min$sex ==\'m\',], aes (y=earliest, x=date),  colour =\'#00BFC4\', size =1)  +\r\n# IP copulation attempts (black triangles; i.e. resident male with resident female):\r\n  geom_jitter(data =cops[cops$att.cop%in% \'attempts\'& cops$IEP%in% \'IP_IP\',], aes(y=time_dec, x=date), shape =2, size =1.5, height =0, width =0.2)+\r\n# IP copulations (black dots; i.e. resident male with resident female):\r\n  geom_jitter(data =cops[cops$att.cop%in% \'copulations\'& cops$IEP%in% \'IP_IP\',], aes(y=time_dec, x=date), shape =16, size =2, height =0, width =0.2)+\r\n# EP copulation attempts (red triangles, i.e. non-resident male with resident female):  \r\n  geom_jitter(data =cops[cops$att.cop%in% \'attempts\'& cops$IEP%in% \'EP_IP\',], aes(y=time_dec, x=date), shape =17,colour=\'red\', size =2, height =0, width =0.2)+\r\n# EP copulations (red dots, i.e. non-resident male with resident female):  \r\n  geom_jitter(data =cops[cops$att.cop%in% \'copulations\'& cops$IEP%in% \'EP_IP\',], aes(y=time_dec, x=date), shape =16, colour=\'red\', size =2, height =0, width =0.2)+\r\n# non-resident IP copulation attempts (blue triangles, i.e. non-resident male with non-resident female):  \r\n  geom_jitter(data =cops[cops$att.cop%in% \'attempts\'& cops$IEP%in% \'EP_EP\',], aes(y=time_dec, x=date), shape =17,colour=\'blue\', size =2, height =0, width =0.2)+\r\n# non-resident IP copulations (blue dots, i.e. non-resident male with non-resident female):  \r\n  geom_jitter(data =cops[cops$att.cop%in% \'copulations\'& cops$IEP%in% \'EP_EP\',], aes(y=time_dec, x=date), shape =16, colour=\'blue\',size =2, height =0, width =0.2)\r\n\r\n\r\n\r\n## Fig. S3: Boxplots of IP and EP copulation attempts and full copulations at different breeding stages (summary)\r\n## intra-pair:\r\nggplot()+\r\n  geom_boxplot(data= IP, aes(x=Female, y=nr_events))+\r\n  geom_point(data= IP, aes(x=Female, y=nr_events,colour=Nest.box, shape =Nest.box),size=2)+\r\n  geom_line(data= IP, aes(x=Female, y=nr_events, colour=Nest.box, group=Nest.box))+\r\n  scale_shape_manual(name  =""Nest-box"", breaks=levels(IP$Nest.box),labels=\r\n                       levels(IP$Nest.box), values =c(0:4,6,8,9))+\r\n  facet_wrap(~att.cop)+\r\n  theme_bw()+  scale_x_discrete(name=(\'Female\'))+  \r\n  scale_y_continuous(name=(\'IP frequency\'))\r\n## extra-pair:\r\nggplot()+\r\n  geom_boxplot(data= EP, aes(x=Female, y=nr_events))+\r\n  geom_point(data= EP, aes(x=Female, y=nr_events,colour=Nest.box, shape =Nest.box),size=2)+\r\n  geom_line(data= EP, aes(x=Female, y=nr_events, colour=Nest.box, group=Nest.box))+\r\n  scale_shape_manual(name  =""Nest-box"", breaks=levels(EP$Nest.box),labels=\r\n                       levels(EP$Nest.box), values =c(0:4,6,8,9))+\r\n  facet_wrap(~att.cop)+\r\n  theme_bw()+  scale_x_discrete(name=(\'Female\'))+  \r\n  scale_y_continuous(name=(\'EP frequency\'), breaks = c(0,3,6,9))\r\n\r\n\r\n\r\n## Additional information -----\r\n\r\n#time between IP male exit, EP enter and IP male return:\r\ninnest <- read.csv(\'12_allcams_entries.csv\'); innest$date <- as.Date(innest$date, format = \'%Y-%m-%d\')\r\nIP <- innest[innest$IP ==\'IP\',]\r\nIP$cam_male <- paste(IP$cam, ""m"", sep=\'_\')\r\nIP$cam_female <- paste(IP$cam, ""f"", sep=\'_\')\r\ninnest$resident_male <- IP$cam_male[match(innest$cam, IP$cam)]\r\ninnest$resident_female <- IP$cam_female[match(innest$cam, IP$cam)]\r\ninnest$date_cam_ring <- paste(innest$date,innest$cam,innest$ring)\r\ninnest$timein <- strptime(innest$in.,  format = \'%H:%M\')\r\ninnest$timeout <- strptime(innest$out,  format = \'%H:%M\')\r\n\r\n\r\n\r\nEP <- innest[innest$IP ==\'EP\',]\r\nEP$sexual <- NA\r\nEP$sexual[EP$intrusion_sexual %in% c(\'attempt\', \'cop\',\'cop, fight\', \'copulation\')] <- \'yes\'\r\nEP$sexual[EP$intrusion_sexual %in% c(\'fight_both\', \'fight_fem\',\'no\')] <- \'no\'\r\nEP$sexual <- factor(EP$sexual, levels =c(\'yes\',\'no\'))\r\n\r\nEP$EPafterIP_hr <- substr(EP$numEpin_after_Ipout,  1,2)\r\nEP$EPafterIP_min <- substr(EP$numEpin_after_Ipout,  4,5)\r\nEP$EPafterIP_sec <- substr(EP$numEpin_after_Ipout,  7,8)\r\nEP$after_hr <- as.numeric(EP$EPafterIP_hr)\r\nEP$after_min <- as.numeric(EP$EPafterIP_min)/60\r\nEP$after_min[EP$EP$EPafterIP_min %in% c(\'00\') ]  <- 0\r\nEP$after_sec <- as.numeric(EP$EPafterIP_sec)\r\nEP$after_minsec <- EP$after_min + EP$after_sec\r\nEP$after <- EP$after_hr + EP$after_min + EP$after_sec\r\nmean(EP$after) # 0.6641026 hours = 39.84615 mins TOTAL\r\nsd(EP$after)*60 # 90.4514 min\r\nmedian(EP$after)*60 # 16 min\r\nmean(EP[EP$after <= 8,]$after) # 0.4672653 hours = 28.03592 EXCLUDING NIGHTS\r\nsd(EP[EP$after <= 8,]$after)*60 # 33.37693 min\r\nmedian(EP[EP$after <= 8,]$after)*60 # 15 min\r\n\r\nEP$EPbeforeIP_hr <- substr(EP$NumEpout_before_IPin,  1,2)\r\nEP$EPbeforeIP_min <- substr(EP$NumEpout_before_IPin,  4,5)\r\nEP$EPbeforeIP_sec <- substr(EP$NumEpout_before_IPin,  7,8)\r\nEP$before_hr <- as.numeric(EP$EPbeforeIP_hr)\r\nEP$before_min <- as.numeric(EP$EPbeforeIP_min)/60\r\nEP$before_min[EP$EP$EPbeforeIP_min %in% c(\'00\') ]  <- 0\r\nEP$before_sec <- as.numeric(EP$EPbeforeIP_sec)\r\nEP$before_minsec <- EP$before_min + EP$before_sec\r\nEP$before <- EP$before_hr + EP$before_min + EP$before_sec\r\nmean(EP$before) # 0.346401 hours = 20.78406 mins TOTAL\r\nsd(EP$before)*60 # 60.69652 min\r\nmedian(EP$before)*60 # 6 min\r\nmean(EP[EP$before <= 8,]$before) # 0.2669779 hours = 16.01867 minutes EXCLUDING NIGHTS\r\nsd(EP[EP$before <= 8,]$before)*60 #  24.84523 min\r\nmedian(EP[EP$before <= 8,]$before)*60 #5 min\r\n\r\n\r\n## intrusions:\r\nintr <- read.csv(\'13_2017_intrusions.csv\')\r\nintr$date <- as.Date(intr$date, format =\'%Y-%m-%d\')\r\ntable(intr$IP, intr$female_innest) # 1099 events (excluding kestrel and unknown cases), 45 of which when female was inside\r\nintr <- intr[!intr$ring %in% \'kestrel\',]\r\nintr <- intr[!intr$ring %in% \'unknown\',]\r\n\r\n\r\n\r\n\r\n\r\n\r\n']","Data from: Genetic Monogamy despite frequent extra-pair Copulations  in ""strictly monogamous"" wild Jackdaws ""Monogamy"" refers to different components of pair exclusiveness: the social pair, sexual partners, and the genetic outcome of sexual encounters. Avian monogamy is usually defined socially or genetically, while quantifications of sexual behavior remain scarce. Jackdaws (Corvus monedula) are considered a rare example of strict monogamy in songbirds, with lifelong pair bonds and little genetic evidence for extra-pair offspring. Yet jackdaw copulations, although accompanied by loud copulation calls, are rarely observed, since they occur visually concealed inside nest cavities. Using full-day nest-box video surveillance and on-bird acoustic bio-logging, we directly observed jackdaw sexual behavior and compared it to the corresponding genetic outcome obtained via molecular parentage analysis. In the video-observed nests, we found genetic monogamy, but frequently detected forced extra-pair sexual behavior, accompanied by characteristic male copulation calls. We thus challenge the long-held notion of strict jackdaw monogamy at the sexual level. Our data suggest that male mate-guarding and frequent intra-pair copulations during the female fertile phase, as well as the forced nature of the copulations could explain the absence of extra-pair offspring. Since extra-pair copulation behavior appeared to be costly for both sexes, we suggest that immediate fitness benefits are an unlikely explanation for its prevalence. Instead, sexual conflict and dominance effects could interact to shape the spatio-temporal pattern of extra-pair sexual behavior in this species. Our results call for larger-scale investigations of jackdaw sexual behavior and parentage, and highlight the importance of combining social, sexual and genetic datasets for a more complete understanding of mating systems.",2
Social network position predicts male mating success in a small passerine,"Individuals differ in the quantity and quality of associations with conspecifics. The resulting variation in the positions that individuals occupy within their social environment can affect several aspects of life history, including reproductive behavior. While research increasingly shows how social factors can predict dyadic mating patterns (i.e. who will breed with whom), much less is known about how an individual's social position affects it's overall likelihood to acquire mating partner(s). We studied social networks of socially monogamous blue tits (Cyanistes caeruleus) to investigate whether the number and strength of connections to opposite-sex conspecifics, the ratio between same- and opposite-sex connections, and the tendency to move between social groups in the months prior to breeding affects individuals' success in acquiring 1) a breeding partner and 2) an extra-pair partner. After controlling for differences in spatial location, we show that males without previous local breeding experience that moved more often between social groups were more likely to acquire a breeding partner. Moreover, adult males that associated with more females were more likely to sire extra-pair young. The number of female associates also predicted the proportion of familiar female breeding neighbors, suggesting that familiarity among neighbors may facilitate opportunities for extra-pair matings. In females, none of the social network metrics significantly predicted the likelihood of acquiring a breeding or extra-pair partner. Our study suggests that the positioning of males within their social environment prior to breeding can translate into future mating success, adding an important new dimension to studies of (extra-pair) mating behavior.","['\r\n#  R script for the manuscript \r\n#  ""Social network position predicts male mating success in a small passerine""\r\n\r\n# Kristina B. Beck, Damien R. Farine, Bart Kempenaers\r\n\r\n# Email: kbbeck.mail@gmail.com\r\n\r\n\r\n#  Load required packages\r\nrequire(c(""lme4"", ""dplyr"", ""data.table"", ""ggplot2"", ""arm"", ""effects""))\r\n\r\n#  Load data\r\nload(""data.RData"")\r\n\r\n\r\n#    1) Probability to acquire a breeding partner\r\n###-----------------------------------------------------------------------------\r\n\r\n#  Example shown for males\r\n#  For female data select: data[[2]] and run:\r\n#  m <- glm(breeding ~ N_male_asso + betweenness + avg_asso_male + sex_ratio + date_diff + age, data=females, family=""binomial"") \r\n\r\nmales <- data[[1]]\r\n\r\nm <- glm(breeding ~ N_female_asso + betweenness + avg_asso_female + sex_ratio + age + date_diff, data=males, family=""binomial"") \r\nm2 <- standardize(m)\r\nsummary(m2)\r\n# Extract coefficients for observed data\r\ndegree <- m2$coefficients[2]\r\nbetw <- m2$coefficients[3]\r\navg_asso <- m2$coefficients[4]\r\nsex_ratio <- m2$coefficients[5]\r\n\r\n\r\n# Permutations\r\nmales$ii <- paste(males$N_female_asso, males$betweenness, males$avg_asso_female, males$sex_ratio, sep = ""_"")\r\nrand_degree_epp <- rep(0,1000)\r\nrand_betw_epp <- rep(0,1000)\r\nrand_asso_epp <- rep(0,1000)\r\nsex_epp <- rep(0,1000)\r\n\r\nfor (i in c(1:1000)) {\r\n  \r\n  # Run either restricted or unrestricted permutation\r\n  males[, ii_rand:=sample(ii),]    # unrestricted\r\n  #males[, ii_rand:=sample(ii),by=Location]   # control for spatial location\r\n  \r\n  rand <- data.frame(do.call(\'rbind\', strsplit(as.character(males$ii_rand),\'_\',fixed=TRUE)))\r\n  setnames(rand, c(""X1"",""X2"",""X3"",""X4""), c(""N_fem_rand"", ""betweenness_rand"",""avg_asso_female_rand"",""sex_ratio_rand""))\r\n  males_r <- cbind(males, rand)\r\n  males_r$N_fem_rand <- as.numeric(as.character(males_r$N_fem_rand))\r\n  males_r$betweenness_rand <- as.numeric(as.character(males_r$betweenness_rand))\r\n  males_r$avg_asso_female_rand <- as.numeric(as.character(males_r$avg_asso_female_rand))\r\n  males_r$sex_ratio_rand <- as.numeric(as.character(males_r$sex_ratio_rand))\r\n  # GLM\r\n  m_r <- glm(breeding ~ N_fem_rand + betweenness_rand + avg_asso_female_rand + sex_ratio_rand + age + date_diff, data=males_r, family=""binomial"") \r\n  m_r2 <- standardize(m_r)\r\n  rand_degree_epp[i] <- m_r2$coefficients[2]\r\n  rand_betw_epp[i] <- m_r2$coefficients[3]\r\n  rand_asso_epp[i] <- m_r2$coefficients[4]\r\n  sex_epp[i] <- m_r2$coefficients[5]\r\n  \r\n}\r\n\r\n# Get P values\r\nsum(abs(degree) < abs(rand_degree_epp))/1000  \r\nsum(abs(betw) < abs(rand_betw_epp))/1000  \r\nsum(abs(avg_asso) < abs(rand_asso_epp))/1000  \r\nsum(abs(sex_ratio) < abs(sex_epp))/1000  \r\n\r\n# Figure 1A\r\n\r\nm <- glm(breeding ~ N_female_asso + age + betweenness + avg_asso_female + sex_ratio + date_diff, data=males, family=""binomial"") \r\ne <- allEffects(m, xlevels=50)$`betweenness` %>% data.frame %>% data.table\r\n\r\nggplot() +\r\n  geom_count(data=males, aes(y = breeding, x = betweenness), alpha=.5,shape = 1, stroke = 1.0) + scale_size(range = c(2, 10)) +\r\n  geom_line(data = e, aes(y = fit, x = betweenness), size=1) + \r\n  geom_ribbon(data = e, aes(x = betweenness , ymin=lower, ymax=upper, linetype=NA), alpha=.3, fill=""gray46"") + \r\n  theme_classic() + \r\n  theme(axis.text.x = element_text( color=""black"", size=15), axis.text.y = element_text( color=""black"", size=15)) +\r\n  xlab(""Betweenness"") + ylab(""Probability to breed"") + ggtitle(""A"") +\r\n  theme(axis.title.x = element_text(size=16), axis.title.y = element_text(size=16),\r\n        plot.title = element_text(size=19,hjust = 0), legend.position = ""none"") + \r\n  scale_y_continuous(limits = c(0,1))\r\n\r\n\r\n\r\n\r\n#    2) Probability to acquire EPY\r\n###-----------------------------------------------------------------------------\r\n\r\n#  MALES\r\n\r\n#  Example shown for males\r\n#  For female data select: data[[4]] and run:\r\n#  m <- glm(epp ~ N_male_asso + betweenness + avg_asso_male + sex_ratio, data=females, family=""binomial"") \r\n\r\nadult_males <- data[[3]]\r\n  \r\nm <- glm(epp ~ N_female_asso + betweenness + avg_asso_female + sex_ratio, data=adult_males, family=""binomial"") \r\nm2 <- standardize(m)\r\nsummary(m2)\r\ndegree <- m2$coefficients[2]\r\nbetw <- m2$coefficients[3]\r\navg_asso <- m2$coefficients[4]\r\nsex_ratio <- m2$coefficients[5]\r\n\r\n\r\n# Permutation\r\nadult_males$ii <- paste(adult_males$N_female_asso, adult_males$betweenness, adult_males$avg_asso_female, adult_males$sex_ratio, sep = ""_"")\r\nrand_degree_epp <- rep(0,1000)\r\nrand_betw_epp <- rep(0,1000)\r\nrand_asso_epp <- rep(0,1000)\r\nsex_epp <- rep(0,1000)\r\n\r\nfor (i in c(1:1000)) {\r\n  \r\n  #adult_males[, ii_rand:=sample(ii),]    # unrestricted\r\n  adult_males[, ii_rand:=sample(ii),by=Location]   # control for spatial locationl\r\n  \r\n  rand <- data.frame(do.call(\'rbind\', strsplit(as.character(adult_males$ii_rand),\'_\',fixed=TRUE)))\r\n  setnames(rand, c(""X1"",""X2"",""X3"",""X4""), c(""N_fem_rand"", ""betweenness_rand"",""avg_asso_female_rand"",""sex_ratio_rand""))\r\n  males_r <- cbind(adult_males, rand)\r\n  males_r$N_fem_rand <- as.numeric(as.character(males_r$N_fem_rand))\r\n  males_r$betweenness_rand <- as.numeric(as.character(males_r$betweenness_rand))\r\n  males_r$avg_asso_female_rand <- as.numeric(as.character(males_r$avg_asso_female_rand))\r\n  males_r$sex_ratio_rand <- as.numeric(as.character(males_r$sex_ratio_rand))\r\n  # GLM\r\n  m_r <- glm(epp ~ N_fem_rand + betweenness_rand + avg_asso_female_rand + sex_ratio_rand, data=males_r, family=""binomial"") \r\n  m_r2 <- standardize(m_r)\r\n  rand_degree_epp[i] <- m_r2$coefficients[2]\r\n  rand_betw_epp[i] <- m_r2$coefficients[3]\r\n  rand_asso_epp[i] <- m_r2$coefficients[4]\r\n  sex_epp[i] <- m_r2$coefficients[5]\r\n  \r\n} \r\n\r\n# Get P values\r\nsum(abs(degree) < abs(rand_degree_epp))/1000  \r\nsum(abs(betw) < abs(rand_betw_epp))/1000 \r\nsum(abs(avg_asso) < abs(rand_asso_epp))/1000 \r\nsum(abs(sex_ratio) < abs(sex_epp))/1000  \r\n\r\n# Figure 1B\r\n\r\nm <- glm(epp ~ N_female_asso + betweenness + avg_asso_female + sex_ratio, data=adult_males, family=""binomial"") \r\ne <- allEffects(m, xlevels=50)$`N_female_asso` %>% data.frame %>% data.table\r\n\r\nggplot() + \r\n  geom_count(data=adult_males, aes(y = epp, x = N_female_asso), alpha=.5,shape = 1, stroke = 1.0) + scale_size(range = c(2, 10)) +\r\n  geom_line(data = e, aes(y = fit, x = N_female_asso), size=1) + \r\n  geom_ribbon(data = e, aes(x = N_female_asso , ymin=lower, ymax=upper, linetype=NA), alpha=.3, fill=""gray46"") + \r\n  theme_classic() + \r\n  theme(axis.text.x = element_text( color=""black"", size=15), axis.text.y = element_text( color=""black"", size=15)) +\r\n  xlab(""Number of female associates"") + ylab(""Probability to sire EPY"") + ggtitle(""B"") +\r\n  theme(axis.title.x = element_text(size=16), axis.title.y = element_text(size=16),\r\n        plot.title = element_text(size=19,hjust = 0),legend.position = ""none"") +\r\n  scale_y_continuous(limits = c(0,1))\r\n\r\n\r\n\r\n#    3) Does the number of female associates predict number of neighbours/proportion of familiar females?\r\n###-----------------------------------------------------------------------------\r\n\r\nmales_breeding_neigh <- data[[5]]\r\n\r\n#  Number of neighbours (N)\r\nmm <- lm(N ~ rescale(N_female_asso), data=males_breeding_neigh)\r\nsummary(mm)\r\nobs_m <- mm[[1:2]]\r\n#  Proportion of familiar neighbours\r\nmm <- glm(cbind(N_fam_neig, N-N_fam_neig) ~ rescale(N_female_asso), data=males_breeding_neigh, family=""binomial"")\r\nsummary(mm)\r\nobs_m2 <- mm[[1:2]]\r\n\r\n\r\n#  Permutation\r\nrand_m <- rep(0,1000)\r\nrand_m2 <- rep(0,1000)\r\n\r\nfor (i in c(1:1000)) {\r\n  \r\n  males_rand <- males_breeding_neigh\r\n  males_rand[, N_female_asso_rand:= sample(N_female_asso),]  # unrestricted\r\n  #males_rand[, N_female_asso_rand:= sample(N_female_asso),by=Location] # control for spatial location\r\n  males_rand$N_female_asso_rand <- as.numeric(males_rand$N_female_asso_rand)\r\n  \r\n  # model\r\n  mm_r <- lm(N ~ rescale(N_female_asso_rand), data=males_rand) \r\n  rand_m[i] <- mm_r[[1:2]]\r\n  mm_r <- glm(cbind(N_fam_neig, N-N_fam_neig) ~ rescale(N_female_asso_rand), data=males_rand, family=""binomial"") \r\n  rand_m2[i] <- mm_r[[1:2]]\r\n  \r\n}\r\n\r\n# Get the p-values\r\nsum(abs(obs_m) < abs(rand_m))/1000  \r\nsum(abs(obs_m2) < abs(rand_m2))/1000  \r\n\r\n\r\n\r\n\r\n']","Social network position predicts male mating success in a small passerine Individuals differ in the quantity and quality of associations with conspecifics. The resulting variation in the positions that individuals occupy within their social environment can affect several aspects of life history, including reproductive behavior. While research increasingly shows how social factors can predict dyadic mating patterns (i.e. who will breed with whom), much less is known about how an individual's social position affects it's overall likelihood to acquire mating partner(s). We studied social networks of socially monogamous blue tits (Cyanistes caeruleus) to investigate whether the number and strength of connections to opposite-sex conspecifics, the ratio between same- and opposite-sex connections, and the tendency to move between social groups in the months prior to breeding affects individuals' success in acquiring 1) a breeding partner and 2) an extra-pair partner. After controlling for differences in spatial location, we show that males without previous local breeding experience that moved more often between social groups were more likely to acquire a breeding partner. Moreover, adult males that associated with more females were more likely to sire extra-pair young. The number of female associates also predicted the proportion of familiar female breeding neighbors, suggesting that familiarity among neighbors may facilitate opportunities for extra-pair matings. In females, none of the social network metrics significantly predicted the likelihood of acquiring a breeding or extra-pair partner. Our study suggests that the positioning of males within their social environment prior to breeding can translate into future mating success, adding an important new dimension to studies of (extra-pair) mating behavior.",2
Investigating learning strategies in a social interaction task: a simulation study.,"The present study is a simulation study. We theoretically tested a newly developed probabilistic reward-based learning task with social feedback. The task was designed to investigate how individuals respond to social inclusion and exclusion and how these two opposing experiences affect learning and decision making in a dynamic social interaction.Before testing the paradigm in participants, we aimed at determining the size of our hypothesized effect in order to establish how many participants, blocks and trials would be needed to detect the effect.We expect social inclusion and exclusion to affect learning. Thus, we hypothesize different choice patterns in the social compared to the non-social condition. Hypotheses are formulated with respect to the outcome variable reflecting the number of ""correct"" choices, i.e. accuracy.In the non-social learning condition, we expect behavior to be driven by the goal to maximize reward. This behavior has been observed in numerous studies applying probabilistic reinforcement learning tasks. We predicted high rates of accuracy since the ""correct"" option (the option associated with a higher reward probability) is chosen more frequently.In the social learning condition, we expect learning from social feedback to be biased by preferences to respond pro- or antisocially to an exclusion experience. These preferences are reflected in the number of passes played to the excluder which is analogous to accuracy in the non-social task. For the purpose of this study we will define prosocial behavior only with regard to the source of exclusion, i.e. we are interested in how behavior towards the excluder changes once the agent has learned that the probability of receiving a positive social feedback is low compared to the other available option. In a reinforcement learning framework, this means that prosocial agents should display lower accuracy rates than in the non-social condition, since their aim is contrary to reward maximization. If agents exhibit antisocial behavior towards the excluder, their choice pattern and accuracy levels should resemble those observed in the non-social condition. On a group level, accuracy rates should be at chance level.In summary, in the non-social condition, we expect behavior to be the similar for all agents, since the common goal is reward maximization which results in high accuracy. In the social condition, we expect greater variability in behavior, since in this condition some individuals tend to respond prosocially to the source of exclusion (more passes to excluder - low accuracy), while others respond antisocially when excluded (more passes to includer - high accuracy).In terms of the computational models, that implicates that our suggested models should differentially account for behavior in the two conditions. If agents in the social condition show high accuracy levels, this could reflect two things: (1) they chose the option that is more likely to result in positive feedback or (2) they intentionally punished the source of exclusion. By inspecting the behavioral outcome - accuracy - alone, we are not able to tell if this behavior is driven by the goal of reward maximization, or if it reflects antisocial behavior towards the excluder. By formalizing behavior with mathematical models, we aim to determine which motive is more likely to be the driving force behind hypothesized choice patterns. One of our our suggested models incorporates an additional parameter to account for individual preferences to respond pro- or antisocially to exclusion.Following the guidelines proposed by Palminteri et al., (2016), we performed a simulation study to examine the predictive and generative performance of our two suggested computational models. We therefore simulated two data sets that reflected the behavior we expected to observe in the social and the non-social condition of the task. We then fitted a standard reinforcement learning model and a reinforcement learning model with an additional parameter that accounts for prosocial behavior in the social task to both data sets and examined model fits. We further performed model simulations with individual best fitting parameter estimates to investigate whether the winning model to the respective data set was capable of reproducing the effect of interest.",,"Investigating learning strategies in a social interaction task: a simulation study. The present study is a simulation study. We theoretically tested a newly developed probabilistic reward-based learning task with social feedback. The task was designed to investigate how individuals respond to social inclusion and exclusion and how these two opposing experiences affect learning and decision making in a dynamic social interaction.Before testing the paradigm in participants, we aimed at determining the size of our hypothesized effect in order to establish how many participants, blocks and trials would be needed to detect the effect.We expect social inclusion and exclusion to affect learning. Thus, we hypothesize different choice patterns in the social compared to the non-social condition. Hypotheses are formulated with respect to the outcome variable reflecting the number of ""correct"" choices, i.e. accuracy.In the non-social learning condition, we expect behavior to be driven by the goal to maximize reward. This behavior has been observed in numerous studies applying probabilistic reinforcement learning tasks. We predicted high rates of accuracy since the ""correct"" option (the option associated with a higher reward probability) is chosen more frequently.In the social learning condition, we expect learning from social feedback to be biased by preferences to respond pro- or antisocially to an exclusion experience. These preferences are reflected in the number of passes played to the excluder which is analogous to accuracy in the non-social task. For the purpose of this study we will define prosocial behavior only with regard to the source of exclusion, i.e. we are interested in how behavior towards the excluder changes once the agent has learned that the probability of receiving a positive social feedback is low compared to the other available option. In a reinforcement learning framework, this means that prosocial agents should display lower accuracy rates than in the non-social condition, since their aim is contrary to reward maximization. If agents exhibit antisocial behavior towards the excluder, their choice pattern and accuracy levels should resemble those observed in the non-social condition. On a group level, accuracy rates should be at chance level.In summary, in the non-social condition, we expect behavior to be the similar for all agents, since the common goal is reward maximization which results in high accuracy. In the social condition, we expect greater variability in behavior, since in this condition some individuals tend to respond prosocially to the source of exclusion (more passes to excluder - low accuracy), while others respond antisocially when excluded (more passes to includer - high accuracy).In terms of the computational models, that implicates that our suggested models should differentially account for behavior in the two conditions. If agents in the social condition show high accuracy levels, this could reflect two things: (1) they chose the option that is more likely to result in positive feedback or (2) they intentionally punished the source of exclusion. By inspecting the behavioral outcome - accuracy - alone, we are not able to tell if this behavior is driven by the goal of reward maximization, or if it reflects antisocial behavior towards the excluder. By formalizing behavior with mathematical models, we aim to determine which motive is more likely to be the driving force behind hypothesized choice patterns. One of our our suggested models incorporates an additional parameter to account for individual preferences to respond pro- or antisocially to exclusion.Following the guidelines proposed by Palminteri et al., (2016), we performed a simulation study to examine the predictive and generative performance of our two suggested computational models. We therefore simulated two data sets that reflected the behavior we expected to observe in the social and the non-social condition of the task. We then fitted a standard reinforcement learning model and a reinforcement learning model with an additional parameter that accounts for prosocial behavior in the social task to both data sets and examined model fits. We further performed model simulations with individual best fitting parameter estimates to investigate whether the winning model to the respective data set was capable of reproducing the effect of interest.",2
Data from: Flexible decision-making in grooming partner choice in sooty mangabeys and chimpanzees,"Living in permanent social groups forces animals to make decisions about when, how and with whom to interact, requiring decisions to be made that integrate multiple sources of information. Changing social environments can influence this decision-making process by constraining choice or altering the likelihood of a positive outcome. Here, we conceptualised grooming as a choice situation where an individual chooses one of a number of potential partners. Studying two wild populations of sympatric primate species, sooty mangabeys (Cercocebus atys atys) and Western chimpanzees (Pan troglodytes verus), we tested what properties of potential partners influenced grooming decisions, including their relative value based on available alternatives and the social relationships of potential partners with bystanders who could observe the outcome of the decision. Across 1,529 decision events, multiple partner attributes (e.g. dominance ranks, social relationship quality, reproductive state, partner sex) influenced choice. Individuals preferred to initiate grooming with partners of similar global rank, but this effect was driven by a bias towards partners with a high rank compared to other locally available options. Individuals also avoided grooming partners who had strong social relationships with at least one bystander. Results indicated flexible decision-making in grooming interactions in both species, based on a partner's value given the local social environment. Viewing partner choice as a value-based decision-making process allows researchers to compare how different species solve similar social problems.","['########### Model 1\r\nlibrary(lme4)\r\ntest.data.model1=x### load data file for model 1\r\ntest.data.model2=x### load data file for model 2\r\n\r\ncontr=glmerControl(optCtrl = list(maxfun=10000000), calc.derivs=F, optimizer = ""nloptwrap"")\r\n\r\n# Random slopes Model 1-1 (Both Species - Absolute Ranks and Relationships)\r\n### Please note that the random slopes were selected after running the full model with full random intercept and slope structure (all combinations random intercept - fixed effects) and removing those slopes with below 0.00001 variance\r\nreduced.model.slope.model1.1=""(1 + z.rank.partner||focal) +\r\n(1 + z.rank.focal + z.rank.partner + z.average.ddsi.partner + z.max.ddsi.partner||pot.partner) +\r\n(1 + z.rank.focal + z.rank.partner + z.dsi.dyad + z.max.ddsi.partner + z.rank.focal * z.rank.partner||dyad) +\r\n(1|gr.bout.index)""\r\n\r\n# Model Specification Model 1-1\r\nmodel1.1.terms=""gr.initiated~z.rank.focal * (z.rank.partner + I(z.rank.partner^2)) * group + z.ddsi.dyad * group + repr.state.partner * group + z.average.ddsi.partner * group + z.max.ddsi.partner * group + prev.aggression * group +\r\nsex.partner * group + sex.focal * sex.partner + offset(log(1/eff.gr.size))""\r\nmodel1.1=as.formula(paste(model1.1.terms, reduced.model.slope.model1.1, sep=""+""))\r\nnull1.1.terms=""gr.initiated~sex.partner * group + sex.focal * sex.partner + offset(log(1/eff.gr.size))""\r\nnull1.1=as.formula(paste(null1.1.terms, reduced.model.slope.model1.1, sep=""+""))\r\n\r\nfull.model1.1=glmer(model1.1, data=test.data.model1, family=binomial, control=contr)\r\nnull.model1.1=glmer(null1.1, data=test.data.model1, family=binomial, control=contr)\r\nanova(full.model1.1, null.model1.1) #full null model comparison\r\ndrop1.model1.1=drop1(full.model1.1, test=""Chisq"") ###After, remove non-significant interactions from model and rerun to get final model\r\n\r\n\r\n# Random slopes Model 1-2 (Both Species - Relative Ranks and Relationships)\r\n### Please note that the random slopes were selected after running the full model with full random intercept and slope structure (all combinations random intercept - fixed effects) and removing those slopes with below 0.00001 variance\r\nreduced.model.slope.model1.2=""(1 + z.rel.rank.partner + z.rel.rank.focal * z.rel.rank.partner||focal) +\r\n(1 + z.rel.rank.focal + I(z.rel.rank.partner^2) + z.average.ddsi.partner + z.max.ddsi.partner||pot.partner) +\r\n(1 + z.rel.rank.focal + z.rel.rank.partner + z.rel.ddsi.dyad + z.max.ddsi.partner + z.rel.rank.focal * z.rel.rank.partner||dyad) +\r\n(1 + z.rel.rank.partner + z.rel.ddsi.dyad||gr.bout.index)""\r\n\r\n# Model Specification Model 1-2\r\nmodel1.2.terms=""gr.initiated~z.rel.rank.focal * (z.rel.rank.partner + I(z.rel.rank.partner^2)) * group + z.rel.ddsi.dyad * group + repr.state.partner * group + z.average.ddsi.partner * group + z.max.ddsi.partner * group + prev.aggression * group +\r\nsex.partner * group + sex.focal * sex.partner + offset(log(1/eff.gr.size))""\r\nmodel1.2=as.formula(paste(model1.2.terms, reduced.model.slope.model1.2, sep=""+""))\r\nnull1.2.terms=""gr.initiated~sex.partner * group + sex.focal * sex.partner + offset(log(1/eff.gr.size))""\r\nnull1.2=as.formula(paste(null1.2.terms, reduced.model.slope.model1.2, sep=""+""))\r\n\r\nfull.model1.2=glmer(model1.2, data=test.data.model1, family=binomial, control=contr)\r\nnull.model1.2=glmer(null1.2, data=test.data.model1, family=binomial, control=contr)\r\nanova(full.model1.2, null.model1.2)#full null model comparison\r\ndrop1.model1.2=drop1(full.model1.2, test=""Chisq"") ###After, remove non-significant interactions from model and rerun to get final model\r\n\r\n\r\n\r\n####### Model 2\r\n\r\n# Random slopes Model 2-1 (Only Chimpanzees - Absolute Ranks and Relationships)\r\n### Please note that the random slopes were selected after running the full model with full random intercept and slope structure (all combinations random intercept - fixed effects) and removing those slopes with below 0.00001 variance\r\nreduced.model.slope.model2.1=""(1 + z.rank.partner||focal) +\r\n(1 + z.rank.focal + z.rank.partner + z.average.ddsi.partner + z.max.ddsi.partner + I(z.rank.partner^2)||pot.partner) +\r\n(1 + z.rank.focal + z.rank.partner + z.ddsi.dyad + z.max.ddsi.partner||dyad) +\r\n(1|gr.bout.index)""\r\n\r\n# Model Specification Model 2-1\r\nmodel2.1.terms=""gr.initiated~z.rank.focal * (z.rank.partner + I(z.rank.partner^2)) * group * sex.focal + z.ddsi.dyad * sex.focal + repr.state.partner * sex.focal + z.average.ddsi.partner * sex.focal + z.max.ddsi.partner * sex.focal + prev.aggression * group +\r\n  sex.focal * sex.partner * group + offset(log(1/eff.gr.size))""\r\nmodel2.1=as.formula(paste(model2.1.terms, reduced.model.slope.model2.1, sep=""+""))\r\nnull2.1.terms=""gr.initiated~sex.focal * sex.partner * group + offset(log(1/eff.gr.size))""\r\nnull2.1=as.formula(paste(null2.1.terms, reduced.model.slope.model2.1, sep=""+""))\r\n\r\nfull.model2.1=glmer(model2.1, data=test.data.model2, family=binomial, control=contr)\r\nnull.model2.1=glmer(null2.1, data=test.data.model2, family=binomial, control=contr)\r\nanova(full.model2.1, null.model2.1) #full null model comparison\r\ndrop1.model2.1=drop1(full.model2.1, test=""Chisq"") ###After, remove non-significant interactions from model and rerun to get final model\r\n\r\n\r\n# Random slopes Model 2-2 (Only Chimpanzees - Relative Ranks and Relationships)\r\n### Please note that the random slopes were selected after running the full model with full random intercept and slope structure (all combinations random intercept - fixed effects) and removing those slopes with below 0.00001 variance\r\nreduced.model.slope.model2.2=""(1 + z.rel.rank.partner||focal) +\r\n(1 + z.average.ddsi.partner + z.max.ddsi.partner + I(z.rel.rank.partner^2)||pot.partner) +\r\n(1 + z.rel.rank.focal + z.rel.rank.partner + z.rel.ddsi.dyad + z.rel.rank.focal*z.rel.rank.partner||dyad) +\r\n(1 + z.rel.rank.partner + z.rel.ddsi.dyad||gr.bout.index)""\r\n\r\n# Model Specification Model 2-2\r\nmodel2.2.terms=""gr.initiated~z.rel.rank.focal * (z.rel.rank.partner + I(z.rel.rank.partner^2)) * sex.focal + z.rel.ddsi.dyad * sex.focal + repr.state.partner * sex.focal + z.average.ddsi.partner * sex.focal + z.max.ddsi.partner * sex.focal + prev.aggression * group +\r\nsex.focal * sex.partner * group + offset(log(1/eff.gr.size))""\r\nmodel2.2=as.formula(paste(model2.2.terms, reduced.model.slope.model2.2, sep=""+""))\r\nnull2.2.terms=""gr.initiated~sex.focal * sex.partner * group + offset(log(1/eff.gr.size))""\r\nnull2.2=as.formula(paste(null2.2.terms, reduced.model.slope.model2.2, sep=""+""))\r\n\r\nfull.model2.2=glmer(model2.2, data=test.data.model2, family=binomial, control=contr)\r\nnull.model2.2=glmer(null2.2, data=test.data.model2, family=binomial, control=contr)\r\nanova(full.model2.2, null.model2.2) #full null model comparison\r\ndrop1.model2.2=drop1(full.model2.2, test=""Chisq"") ###After, remove non-significant interactions from model and rerun to get final model\r\n\r\n']","Data from: Flexible decision-making in grooming partner choice in sooty mangabeys and chimpanzees Living in permanent social groups forces animals to make decisions about when, how and with whom to interact, requiring decisions to be made that integrate multiple sources of information. Changing social environments can influence this decision-making process by constraining choice or altering the likelihood of a positive outcome. Here, we conceptualised grooming as a choice situation where an individual chooses one of a number of potential partners. Studying two wild populations of sympatric primate species, sooty mangabeys (Cercocebus atys atys) and Western chimpanzees (Pan troglodytes verus), we tested what properties of potential partners influenced grooming decisions, including their relative value based on available alternatives and the social relationships of potential partners with bystanders who could observe the outcome of the decision. Across 1,529 decision events, multiple partner attributes (e.g. dominance ranks, social relationship quality, reproductive state, partner sex) influenced choice. Individuals preferred to initiate grooming with partners of similar global rank, but this effect was driven by a bias towards partners with a high rank compared to other locally available options. Individuals also avoided grooming partners who had strong social relationships with at least one bystander. Results indicated flexible decision-making in grooming interactions in both species, based on a partner's value given the local social environment. Viewing partner choice as a value-based decision-making process allows researchers to compare how different species solve similar social problems.",2
Social groups constrain the spatiotemporal dynamics of wild sifaka gut microbiomes,"Primates acquire gut microbiota from conspecifics through direct social contact and shared environmental exposures. Host behavior is a prominent force in structuring gut microbial communities, yet the extent to which group or individual-level forces shape the long-term dynamics of gut microbiota is poorly understood. We investigated the effects of three aspects of host sociality (social groupings, dyadic interactions, and individual dispersal between groups) on gut microbiome composition and plasticity in 58 wild Verreaux's sifaka (Propithecus verreauxi) from six social groups. Over the course of three dry seasons in a five-year period, the six social groups maintained distinct gut microbial signatures, with the taxonomic composition of individual communities changing in tandem among co-residing group members. Samples collected from group members during each season were more similar than samples collected from single individuals across different years. In addition, new immigrants and individuals with less stable social ties exhibited elevated rates of microbiome turnover across seasons. Our results suggest that permanent social groupings shape the changing composition of commensal and mutualistic gut microbial communities and thus may be important drivers of health and resilience in wild primate populations.",,"Social groups constrain the spatiotemporal dynamics of wild sifaka gut microbiomes Primates acquire gut microbiota from conspecifics through direct social contact and shared environmental exposures. Host behavior is a prominent force in structuring gut microbial communities, yet the extent to which group or individual-level forces shape the long-term dynamics of gut microbiota is poorly understood. We investigated the effects of three aspects of host sociality (social groupings, dyadic interactions, and individual dispersal between groups) on gut microbiome composition and plasticity in 58 wild Verreaux's sifaka (Propithecus verreauxi) from six social groups. Over the course of three dry seasons in a five-year period, the six social groups maintained distinct gut microbial signatures, with the taxonomic composition of individual communities changing in tandem among co-residing group members. Samples collected from group members during each season were more similar than samples collected from single individuals across different years. In addition, new immigrants and individuals with less stable social ties exhibited elevated rates of microbiome turnover across seasons. Our results suggest that permanent social groupings shape the changing composition of commensal and mutualistic gut microbial communities and thus may be important drivers of health and resilience in wild primate populations.",2
"Data from: Differential aging trajectories in motivation, inhibitory control and cognitive flexibility in Barbary macaques (Macaca sylvanus)","Across the life-span, the performance in problem-solving tasks varies strongly, due to age-related variation in cognitive abilities as well as the motivation to engage in a task. Nonhuman primates provide an evolutionary perspective on human cognitive and motivational aging, as they lack an insight into their own limited lifetime, and aging trajectories are not affected by customs and societal norms. To test age-related variation in inhibitory control, cognitive flexibility, and persistence, we presented Barbary macaques (Macaca sylvanus), living at La Fort des Singes in Rocamadour (France), with three problem-solving tasks. We conducted 297 trials with 143 subjects aged 230 years. We found no effect of age on success and latency to succeed in the inhibitory control task. In the cognitive flexibility task, 21 out of 99 monkeys were able to switch their strategy, but there was no evidence for an effect of age. Yet, the persistence in the motivation task as well as the overall likelihood to participate in any of the tasks declined with increasing age. These results suggest that motivation declines earlier than the cognitive abilities assessed in this study, corroborating the notion that non-human primates and humans show similar changes in motivation in old age.","['## ########################################################## ##\r\n## Research article \'Differential aging trajectories in       ##\r\n## motivation, inhibtory control and cognitive flexibility    ##\r\n## in Barbary macaques (Macaca sylvanus)\'                     ##\r\n## by Eva-Maria Rathke & Julia Fischer                        ##\r\n## Statistical analysis                                       ##\r\n## March 2020                                                 ##\r\n## ########################################################## ##\r\n\r\n# data input\r\nrm(list=ls(all=TRUE))\r\nlibrary(gamlss) #\r\nlibrary(DHARMa) #\r\n\r\n# load in data set\r\n\r\ncube <- read.csv(""InhibitoryControlTask.csv"",\r\n                 header=TRUE, sep = "";"", dec = "","", stringsAsFactors = FALSE)\r\n\r\nslide <- read.csv(""CognitiveFlexibilityTask.csv"",\r\n                  header=TRUE, sep = "";"", dec = "","", stringsAsFactors = FALSE)\r\n\r\ntube <- read.csv(""MotivationTask.csv"",\r\n                 header=TRUE, sep = "";"", dec = "","", stringsAsFactors = TRUE)\r\n\r\n# inhibitory control (cube task) ####\r\n# participation in the task (yes/no)\r\n\r\nmodel1 <- glm(participation ~ age + sex,family=binomial(link=""logit""), data = cube)\r\n# diagnostics\r\nm1<-simulateResiduals(model1, n = 2000)\r\nplotSimulatedResiduals(m1)\r\nsummary(model1)\r\n# confidence intervals\r\nconfint(object = model1)\r\n\r\n# success in the task (yes/no)\r\n\r\nmodel2 <- glm(success ~ age+sex, family=binomial(link=""logit""), data = cube)\r\n# diagnostics\r\nm2<-simulateResiduals(model2, n = 2000)\r\nplotSimulatedResiduals(m2)\r\nsummary(model2)\r\n# confidence intervals\r\nconfint(object = model2)\r\n\r\n# success in the task (yes/no) - only for individuals that participated\r\n\r\ncube_withoutzero <- subset(cube, explorationtime>0)\r\nmodel2b <- glm(success ~ age+sex, family=binomial(link=""logit""),\r\n               data = cube_withoutzero)\r\n# diagnostics\r\nm2b<-simulateResiduals(model2b, n = 2000)\r\nplotSimulatedResiduals(m2b)\r\nsummary(model2b)\r\n# confidence intervals\r\nconfint(object = model2b)\r\n\r\n# successful exploration time with regard to age: u-shaped distribution\r\n\r\ncube_withoutzero <- subset(cube, explorationtime>0)\r\ncube_success <- cube_withoutzero[cube_withoutzero$success == ""1"",]\r\nmodel3<-gamlss(explorationtime ~ age + I(age^2) + sex,\r\n               sigma.formula =~ age + sex, family = LOGNO, data = cube_success)\r\n# diagnostics\r\nplot(model3)\r\nres1<-residuals(model3)\r\nhist(res1)\r\nsummary(model3)\r\n\r\n# confidence intervals for mu\r\nconfint(model3)\r\n# confidence intervals for sigma\r\nconfint(model3, what = ""sigma"")\r\n\r\n# successful exploration time with regard to age: linear\r\n\r\nmodel3_linear<-gamlss(explorationtime ~ age + sex,\r\n                      sigma.formula =~ age + sex, family = LOGNO, data = cube_success)\r\n# diagnostics\r\nplot(model3_linear)\r\nres1_linear<-residuals(model3_linear)\r\nhist(res1_linear)\r\nsummary(model3_linear)\r\n\r\n# confidence intervals for mu\r\nconfint(model3_linear)\r\n# confidence intervals for sigma\r\nconfint(model3_linear, what = ""sigma"")\r\n\r\n# exploration - unsuccessful individuals\r\n\r\ncube_unsuccessful <- subset(cube, success == 0 & explorationtime>0)\r\nmodel4<-gamlss(explorationtime ~ age + sex,\r\n               sigma.formula =~ age + sex, family = LOGNO, data = cube_unsuccessful)\r\n# diagnostics\r\nplot(model4)\r\nres2<-residuals(model4)\r\nhist(res2)\r\nsummary(model4)\r\n\r\n# confidence intervals for mu\r\nconfint(model4)\r\n# confidence intervals for sigma\r\nconfint(model4, what = ""sigma"")\r\n\r\n# cognitive flexibility (sliding doors task) ####\r\n# participation in the task (yes/no)\r\n\r\nmodel5 <- glm(participation ~ age + sex, family=binomial(link=""logit""), data = slide)\r\n# diagnostics\r\nm3<-simulateResiduals(model5, n = 2000)\r\nplotSimulatedResiduals(m3)\r\nsummary(model5)\r\n# confidence intervals\r\nconfint(object = model5)\r\n\r\n# exploration time with regard to age\r\n\r\nslide_withoutzero <- subset(slide, explorationtime>0)\r\nslide_withoutzero<-slide_withoutzero[,c(-7)]\r\nmodel6<-gamlss(explorationtime ~ age + sex,\r\n               sigma.formula =~ age+sex, family = LOGNO, data = slide_withoutzero)\r\n# diagnostics\r\nplot(model6)\r\nres3<-residuals(model6)\r\nhist(res3)\r\nsummary(model6)\r\n\r\n# confidence intervals for mu\r\nconfint(model6)\r\n# confidence intervals for sigma\r\nconfint(model6, what = ""sigma"")\r\n\r\n# latency until new strategy (handle switch)\r\n\r\nslide_handleswitch <- subset(slide, handleswitch >0)\r\nmodel7<-gamlss(explorationtime ~ age + sex,\r\n               sigma.formula =~ age + sex, family = LOGNO, data = slide_handleswitch)\r\n# diagnostics\r\nplot(model7)\r\nres4<-residuals(model7)\r\nhist(res4)\r\nsummary(model7)\r\n\r\n# confidence intervals for mu\r\nconfint(model7)\r\n# confidence intervals for sigma\r\nconfint(model7, what = ""sigma"")\r\n\r\n# motivation (tube task) ####\r\n# participation in the task (yes/no)\r\n\r\nmodel8 <- glm(participation ~ age + sex, family=binomial(link=""logit""), data = tube)\r\n# diagnostics\r\nm4<-simulateResiduals(model8, n = 2000)\r\nplotSimulatedResiduals(m4)\r\nsummary(model8)\r\n# confidence intervals\r\nconfint(object = model8)\r\n\r\n# exploration time with regard to age\r\n\r\ntube_withoutzero <- subset(tube, explorationtime > 0)\r\nmodel9 <-gamlss(explorationtime ~ age+sex,\r\n                sigma.formula =~ age+sex, family = LOGNO, data = tube_withoutzero)\r\n# diagnostics\r\nplot(model9)\r\nres5<-residuals(model9)\r\nhist(res5)\r\nsummary(model9)\r\n\r\n# confidence intervals for mu\r\nconfint(model9)\r\n# confidence intervals for sigma\r\nconfint(model9, what = ""sigma"")\r\n\r\n# relationship between the performance in the motivation task and the inhibitory control task\r\ninhib_motiv <- merge(cube,tube,by=""ID"")\r\nonly_success_inhib = subset(total, success == 1)\r\ncor.test(only_success_inhib$explorationtime.x,only_success_inhib$explorationtime.y, method = ""spearman"")\r\n\r\n# Graphics inhibitory control task (cube task) ####\r\n# participation in the task (yes/no)\r\n\r\n# prepare for plot model\r\n# dummy code and center the factor that was not significant\r\ncube$sex <- as.factor(cube$sex)\r\ncube$sex.m <- as.numeric(cube$sex==levels(cube$sex)[2])\r\ncube$sex.m=cube$sex.m-mean(cube$sex.m)\r\nplot.model1 <- glm(participation ~ age + sex.m,\r\n                   family=binomial(link=""logit""), data = cube)\r\n\r\n# prepare for regression line\r\nage_range = pred.data$age\r\nintercept <- plot.model1$coef[1]\r\nage <- plot.model1$coef[2]\r\nsex.m <- plot.model1$coef[3]\r\nprobs <- intercept + age*age_range + sex.m*0\r\nprobability <- exp(probs)/(1 + exp(probs))\r\n\r\n# confidence intervals\r\npred.data=data.frame(age=seq(from=min(cube$age),\r\n                             to=max(cube$age), length.out=100), sex.m=0)\r\nci.plot=predict.glm(object=plot.model1, newdata=pred.data,\r\n                    type=""link"", se.fit=T)\r\nci.plot=data.frame(\r\n  lwr=ci.plot$fit-ci.plot$se.fit*\r\n    abs(qt(p=0.025, df=plot.model1$df.residual)),\r\n  upr=ci.plot$fit+ci.plot$se.fit*\r\n    abs(qt(p=0.025, df=plot.model1$df.residual)))\r\nci.plot=exp(ci.plot)/(1+exp(ci.plot))\r\n\r\npng(file = ""InhibitoryControlExplorationTime.png"",  width=2000,\r\n    height=1500, units = ""px"", res = 300)\r\npar(mgp = c(3, 1, 0),mar = c(4, 5, 1, 1))\r\nfrequency_touch_cube=aggregate(x=1:nrow(cube),\r\n                               by=cube[, c(""age"", ""participation"")],\r\n                               FUN=length)\r\nplot(x=frequency_touch_cube$age,\r\n     y=frequency_touch_cube$participation,\r\n     cex=0.5*sqrt(frequency_touch_cube$x),las=1,\r\n     xlab="""",ylab="""",\r\n     xlim=c(0,30),\r\n     ylim=c(0,1),pch=16,cex.axis=1.6)\r\n\r\nmtext(""age [years]"", side=1, line=2.1, cex=1.5,las=0)\r\nmtext(""probability to explore"",las=0, side=2, line=3.2, cex=1.5)\r\n\r\nlines(age_range, probability, type=""l"", lwd=2, lty=1, col=""black"")\r\nlines(x=pred.data$age, y=ci.plot[, ""lwr""], lty=3)\r\nlines(x=pred.data$age, y=ci.plot[, ""upr""], lty=3)\r\ndev.off()\r\n\r\n# successful exploration time with regard to age\r\n\r\npng(file = ""InhibitoryControlExplorationTime.png"",  width=2000,\r\n    height=1500, units = ""px"", res = 300)\r\npar(mgp = c(3, 1, 0),mar = c(4, 5, 1, 1))\r\nplot(cube_success$explorationtime ~ cube_success$age,xlab="""",ylab="""",\r\n     xlim=c(0,30), ylim=c(0,70),las=1,cex=1.4, pch=16, cex.axis=1.6)\r\nmtext(""age [years]"", side=1, line=2, cex=1.5,las=0)\r\nmtext(""exploration time [s]"",las=0, side=2, line=3, cex=1.5)\r\ndev.off()\r\n\r\n# Graphics cognitive flexibility (sliding doors task) ####\r\n# participation in the task\r\n\r\nslide$sex <- as.factor(slide$sex)\r\nslide$sex.m <- as.numeric(slide$sex==levels(slide$sex)[2])\r\nslide$sex.m=slide$sex.m-mean(slide$sex.m)\r\nplot.model2 <- glm(participation ~ age + sex.m,\r\n                   family=binomial(link=""logit""), data = slide)\r\n\r\n# prepare for regression line\r\nage_range = pred.data$age\r\nintercept <- plot.model2$coef[1]\r\nage <- plot.model2$coef[2]\r\nsex.m <- plot.model2$coef[3]\r\nprobs <- intercept + age*age_range + sex.m*0\r\nprobability <- exp(probs)/(1 + exp(probs))\r\n\r\n# confidence intervals\r\npred.data=data.frame(age=seq(from=min(slide$age),\r\n                             to=max(slide$age), length.out=100), sex.m=0)\r\nci.plot=predict.glm(object=plot.model2, newdata=pred.data,\r\n                    type=""link"", se.fit=T)\r\nci.plot=data.frame(\r\n  lwr=ci.plot$fit-ci.plot$se.fit*\r\n    abs(qt(p=0.025, df=plot.model2$df.residual)),\r\n  upr=ci.plot$fit+ci.plot$se.fit*\r\n    abs(qt(p=0.025, df=plot.model2$df.residual)))\r\nci.plot=exp(ci.plot)/(1+exp(ci.plot))\r\n\r\npng(file = ""CognitiveFlexibilityParticipation.png"",  width=2000,\r\n    height=1500, units = ""px"", res = 300)\r\npar(mgp = c(3, 1, 0),mar = c(4, 5, 1, 1))\r\nfrequency_touch_slide=aggregate(x=1:nrow(slide),\r\n                                by=slide[, c(""age"", ""participation"")],\r\n                                FUN=length)\r\nplot(x=frequency_touch_slide$age,\r\n     y=frequency_touch_slide$participation,\r\n     cex=0.5*sqrt(frequency_touch_slide$x),las=1,\r\n     xlab="""",ylab="""",\r\n     xlim=c(0,30),\r\n     ylim=c(0,1),pch=16,cex.axis=1.6)\r\nmtext(""age [years]"", side=1, line=2.1, cex=1.5,las=0)\r\nmtext(""probability to explore"",las=0, side=2, line=3.2, cex=1.5)\r\nlines(age_range, probability, type=""l"", lwd=2, lty=1, col=""black"")\r\nlines(x=pred.data$age, y=ci.plot[, ""lwr""], lty=3)\r\nlines(x=pred.data$age, y=ci.plot[, ""upr""], lty=3)\r\ndev.off()\r\n\r\n# latency to switch to another strategy (handle switch)\r\n\r\npng(file = ""CognitiveFlexibilityHandleswitch.png"",  width=2000,\r\n    height=1500, units = ""px"", res = 300)\r\npar(mgp = c(3, 1, 0),mar = c(4, 5, 1, 1))\r\nplot(slide_handleswitch$handleswitch ~ slide_handleswitch$age,xlab="""",ylab="""",\r\n     xlim=c(0,30), ylim=c(0,120),las=1,cex=1.4, pch=16, cex.axis=1.6)\r\nmtext(""age [years]"", side=1, line=2.1, cex=1.5,las=0)\r\nmtext(""latency to switch [s]"",las=0, side=2, line=3.2, cex=1.5)\r\ndev.off()\r\n\r\n# Graphics motivation task (tube task) ####\r\n# participation in the task (yes/no)\r\n\r\ntube$sex <- as.factor(tube$sex)\r\ntube$sex.m <- as.numeric(tube$sex==levels(tube$sex)[2])\r\ntube$sex.m=tube$sex.m-mean(tube$sex.m)\r\nplot.model3 <- glm(participation ~ age + sex.m,\r\n                   family=binomial(link=""logit""), data = tube)\r\n\r\n# prepare for regression line\r\nage_range = pred.data$age\r\nintercept <- plot.model3$coef[1]\r\nage <- plot.model3$coef[2]\r\nsex.m <- plot.model3$coef[3]\r\nprobs <- intercept + age*age_range + sex.m*0\r\nprobability <- exp(probs)/(1 + exp(probs))\r\n\r\n# confidence intervals\r\npred.data=data.frame(age=seq(from=min(tube$age),\r\n                             to=max(tube$age), length.out=100), sex.m=0)\r\nci.plot=predict.glm(object=plot.model3, newdata=pred.data,\r\n                    type=""link"", se.fit=T)\r\nci.plot=data.frame(\r\n  lwr=ci.plot$fit-ci.plot$se.fit*\r\n    abs(qt(p=0.025, df=plot.model3$df.residual)),\r\n  upr=ci.plot$fit+ci.plot$se.fit*\r\n    abs(qt(p=0.025, df=plot.model3$df.residual)))\r\nci.plot=exp(ci.plot)/(1+exp(ci.plot))\r\n\r\npng(file = ""MotivationParticipation.png"",  width=2000,\r\n    height=1500, units = ""px"", res = 300) # for manuscript\r\n\r\npar(mgp = c(3, 1, 0),mar = c(4, 5, 1, 1))\r\nfrequency_touch_tube=aggregate(x=1:nrow(tube),\r\n                               by=tube[, c(""age"", ""participation"")],\r\n                               FUN=length)\r\nplot(x=frequency_touch_tube$age,\r\n     y=frequency_touch_tube$participation,\r\n     cex=0.5*sqrt(frequency_touch_tube$x),las=1,\r\n     xlab="""",ylab="""",\r\n     xlim=c(0,30),\r\n     ylim=c(0,1),pch=16,cex.axis=1.6)\r\nmtext(""age [years]"", side=1, line=2.1, cex=1.5,las=0)\r\nmtext(""probability to explore"",las=0, side=2, line=3.2, cex=1.5)\r\nlines(age_range, probability, type=""l"", lwd=2, lty=1, col=""black"")\r\nlines(x=pred.data$age, y=ci.plot[, ""lwr""], lty=3)\r\nlines(x=pred.data$age, y=ci.plot[, ""upr""], lty=3)\r\ndev.off()\r\n\r\n# exploration time with regard to age\r\ntube_withoutzero$sex_code=as.numeric(tube_withoutzero$sex==levels(tube_withoutzero$sex)[2])\r\ntube_withoutzero$sex_code=tube_withoutzero$sex_code-mean(tube_withoutzero$sex_code)\r\nmodel_tube_ws<-gamlss(explorationtime ~ age + sex_code,\r\n                      sigma.formula =~ age + sex_code, family = LOGNO, data = tube_withoutzero)\r\n\r\n# For every age from 1 to 30, compute the expected exploration time, and variance of this exploration time\r\nxpred <- data.frame(age=1,sex_code=0)\r\nmu_pred<-predict(model_tube_ws,what = c(""mu""),newdata=xpred, type = ""response"")\r\nsigma_pred<-predict(model_tube_ws,what = c(""sigma""),newdata=xpred, type = ""response"")\r\nmean_pred<-exp(mu_pred+(sigma_pred^2)/2)\r\nvar_pred<-(exp(sigma_pred^2)-1)*exp(2*mu_pred+sigma_pred^2)\r\n\r\nfor (i in 1:30){\r\n  xpred <- data.frame(age=i,sex_code=0)\r\n  mu_pred[i]<-predict(model_tube_ws,what = c(""mu""),newdata=xpred, type = ""response"")\r\n  sigma_pred[i]<-predict(model_tube_ws,what = c(""sigma""),newdata=xpred, type = ""response"")\r\n  mean_pred[i]<-exp(mu_pred[i]+(sigma_pred[i]^2)/2)\r\n  var_pred[i]<-(exp(sigma_pred[i]^2)-1)*exp(2*mu_pred[i]+sigma_pred[i]^2)\r\n}\r\n\r\n# Compute confidence interval for the mean and the variance \r\nB=500\r\nage_max=30\r\nmean_bs<-matrix(nrow=age_max,ncol=B)\r\nvar_bs<-matrix(nrow=age_max,ncol=B)\r\nnr<-nrow(tube_withoutzero)\r\n\r\n# Classical nonparametric bootstrap\r\nfor (j in 1:B){\r\n  id<-sample(seq(from = 1, to = nr), replace = TRUE) # resample with replacement\r\n  tube_temp<-tube_withoutzero[id,] # generate new data\r\n  # reestimate the model\r\n  model_temp_ws<-gamlss(explorationtime ~ age + sex_code,\r\n                        sigma.formula =~ age + sex_code, family = LOGNO, data = tube_temp)\r\n  # with reestimated parameters, compute mean and variance\r\n  # repeat a large number of time\r\n  for (i in 1:age_max){\r\n    xpred <- data.frame(age=i,sex_code=0)\r\n    mu<-predict(model_temp_ws,what = c(""mu""),newdata=xpred, type = ""response"")\r\n    sigma<-predict(model_temp_ws,what = c(""sigma""),newdata=xpred, type = ""response"")\r\n    mean_bs[i,j]<-exp(mu+(sigma^2)/2)\r\n    var_bs[i,j]<-(exp(sigma^2)-1)*exp(2*mu+sigma^2)\r\n  }\r\n}\r\n\r\n# Percentile bootstrap CI (take the quantile of the boostrap estimates)\r\nub_var<-vector(length = age_max)\r\nlb_var<-vector(length = age_max)\r\nub_mean<-vector(length = age_max)\r\nlb_mean<-vector(length = age_max)\r\n\r\nfor (i in 1:age_max){\r\n  ub_var[i]<-quantile(var_bs[i,],.975)\r\n  lb_var[i]<-quantile(var_bs[i,],.025)\r\n  ub_mean[i]<-quantile(mean_bs[i,],.975)\r\n  lb_mean[i]<-quantile(mean_bs[i,],.025)\r\n}\r\n\r\npng(file = ""MotivationExplorationtimeCI.png"",  width=2000,\r\n    height=1500, units = ""px"", res = 300)\r\npar(mgp = c(3, 1, 0),mar = c(4, 5, 1, 1))\r\nplot(tube_withoutzero$explorationtime ~ tube_withoutzero$age,xlab="""",ylab="""",\r\n     xlim=c(0,30), ylim=c(0,600),las=1,cex=1.4, pch=16, cex.axis=1.6)\r\n\r\nmtext(""age [years]"", side=1, line=2.1, cex=1.5,las=0)\r\nmtext(""exploration time [s]"",las=0, side=2, line=3.5, cex=1.5)\r\nlines(mean_pred, type=""l"", lwd=2, lty=1, col=""black"")\r\nlines(ub_mean, lty=3)\r\nlines(lb_mean, lty=3)\r\ndev.off()\r\n']","Data from: Differential aging trajectories in motivation, inhibitory control and cognitive flexibility in Barbary macaques (Macaca sylvanus) Across the life-span, the performance in problem-solving tasks varies strongly, due to age-related variation in cognitive abilities as well as the motivation to engage in a task. Nonhuman primates provide an evolutionary perspective on human cognitive and motivational aging, as they lack an insight into their own limited lifetime, and aging trajectories are not affected by customs and societal norms. To test age-related variation in inhibitory control, cognitive flexibility, and persistence, we presented Barbary macaques (Macaca sylvanus), living at La Fort des Singes in Rocamadour (France), with three problem-solving tasks. We conducted 297 trials with 143 subjects aged 230 years. We found no effect of age on success and latency to succeed in the inhibitory control task. In the cognitive flexibility task, 21 out of 99 monkeys were able to switch their strategy, but there was no evidence for an effect of age. Yet, the persistence in the motivation task as well as the overall likelihood to participate in any of the tasks declined with increasing age. These results suggest that motivation declines earlier than the cognitive abilities assessed in this study, corroborating the notion that non-human primates and humans show similar changes in motivation in old age.",3
Data from: Temporal change in floral availability leads to periods of resource limitation and affects diet specificity in a generalist pollinator,"Generalist species are core components of ecological networks and crucial for the maintenance of biodiversity. Generalised species and networks are expected to be more resilient, therefore understanding the dynamics of specialisation and generalisation in ecological networks is a key focus in a time of rapid global climate change. Whilst diet generalisation is frequently studied, our understanding of how it changes over time is limited. We explore temporal variation in diet specificity in the honeybee (Apis mellifera), using pollen DNA metabarcoding of honey samples, through the foraging season, over two years. We find that overall, honeybees are generalists that visit a wide range of plants, but there is temporal variation in the degree of specialisation. Temporal specialisation of honeybee colonies corresponds to periods of resource limitation, identified as a lack of honey stores. Honeybees experience a lack of preferred resources in June when switching from flowering trees in spring to shrubs and herbs in summer. Investigating temporal patterns in specialisation can identify periods of resource limitation that may lead to species and network vulnerability. Diet specificity must therefore be explored at different temporal scales in order to fully understand species and network stability in the face of ecological change.",,"Data from: Temporal change in floral availability leads to periods of resource limitation and affects diet specificity in a generalist pollinator Generalist species are core components of ecological networks and crucial for the maintenance of biodiversity. Generalised species and networks are expected to be more resilient, therefore understanding the dynamics of specialisation and generalisation in ecological networks is a key focus in a time of rapid global climate change. Whilst diet generalisation is frequently studied, our understanding of how it changes over time is limited. We explore temporal variation in diet specificity in the honeybee (Apis mellifera), using pollen DNA metabarcoding of honey samples, through the foraging season, over two years. We find that overall, honeybees are generalists that visit a wide range of plants, but there is temporal variation in the degree of specialisation. Temporal specialisation of honeybee colonies corresponds to periods of resource limitation, identified as a lack of honey stores. Honeybees experience a lack of preferred resources in June when switching from flowering trees in spring to shrubs and herbs in summer. Investigating temporal patterns in specialisation can identify periods of resource limitation that may lead to species and network vulnerability. Diet specificity must therefore be explored at different temporal scales in order to fully understand species and network stability in the face of ecological change.",3
Cochlear Implant Compression Optimization for Music Listening - Maplaw and AGC,"Variations in loudness are a fundamental component of the music listening experience. Cochlear implant (CI) processing, including amplitude compression, and a degraded auditory system may further degrade these loudness cues and decrease the enjoyment of music listening. This study aimed to identify optimal CI sound processor compression settings to improve music sound quality for CI users.Fourteen adult MED-EL CI recipients participated (Experiment #1: n=17 ears; Experiment #2: n=11 ears) in the study. A software application using a modified Comparison Category Rating (CCR) test method allowed participants to compare and rate the sound quality of various CI compression settings while listening to 25 real-world music clips. The two compression settings studied were: 1) Maplaw, which informs audibility and compression of soft level sounds, and 2) Automatic Gain Control (AGC), which applies compression to loud sounds. For each experiment, one compression setting (Maplaw or AGC) was held at the default while the other was varied according to the values available in the clinical CI programming software. Experiment #1 compared Maplaw settings of 500, 1000 (default), and 2000. Experiment #2 compared AGC settings of 2.5:1, 3:1 (default), and 3.5:1.In Experiment #1, the group preferred a higher Maplaw setting of 2000 over the default Maplaw setting of 1000 (p = 0.003) for music listening. There was no significant difference in music sound quality between the Maplaw setting of 500 and the default setting (p = 0.278). In Experiment #2, a main effect of AGC setting was found; however, no significant difference in sound quality ratings for pairwise comparisons were found between the experimental settings and the default setting (2.5:1 versus 3:1 at p = 0.546; 3.5:1 versus 3:1 at p = 0.059).CI users reported improvements in music sound quality with higher than default Maplaw or AGC settings. Thus, participants preferred slightly higher compression for music listening, with results having clinical implications for improving music perception in CI users.","['library(ggplot2)\r\nlibrary(dplyr)\r\nlibrary(lme4)\r\nlibrary(readxl)\r\nlibrary(tidyr)\r\nlibrary(emmeans)\r\nlibrary(ez)\r\nlibrary(ggpubr)\r\nlibrary(gridExtra)\r\n\r\nmydata=read_excel(""Maplaw Data.xlsx"",sheet=\'Maplaw\')\r\nmydata$Maplaw <- as.factor(mydata$Maplaw)\r\nmydata$Participant <- as.factor(mydata$Participant)\r\nmydata$TrackNumber <- as.factor(mydata$TrackNumber)\r\nmydata$NumActiveElectrodes <- as.factor(mydata$NumActiveElectrodes)\r\nmydata$NumFineStructureChannels <- as.factor(mydata$NumFineStructureChannels)\r\nmydata$MaplawOwnProgram <- as.factor(mydata$MaplawOwnProgram)\r\n\r\n# plot scores across subjects as a function of maplaw\r\nggplot(mydata,aes(x=Participant, y=Score, color=Maplaw))+\r\n  geom_boxplot()+\r\n  scale_color_manual(""legend"",values=c(\'500\'=rgb(31,119,180, max=255),\'1000\'=\'darkgreen\',\'2000\'=rgb(214,39,40, max=255)))\r\n# There is variance associated with participant. \r\n\r\nmodel0=lm(Score~ Maplaw, data=mydata)\r\nmodel1=lmer(Score~ Maplaw + (1|Participant), data=mydata)\r\nanova(model1,model0)\r\n# The model significantly improved by adding a random intercept by participant [Chi2(1)=12.3, p<0.001].\r\n\r\nmodel1=lmer(Score~ Maplaw + (1|Participant), data=mydata)\r\nmodel2=lmer(Score~ Maplaw + (1+Maplaw|Participant), data=mydata)\r\nanova(model2,model1)\r\n# The model improved greatly by adding a random slope for the effect of Maplaw by participant [Chi2(5)=214.2, p<0.001].\r\n\r\n# Note the warning: boundary (singular) fit: see ?isSingular  when you have (1+Maplaw|Participant)\r\n# If we excluded this condition, it should no longer show this warning:\r\nmysubset = mydata %>%\r\n  filter(Maplaw==500 | Maplaw==2000)\r\n\r\nggplot(mysubset,aes(x=Participant, y=Score, color=Maplaw))+\r\n  geom_boxplot()+\r\n  scale_color_manual(""legend"",values=c(\'500\'=rgb(31,119,180, max=255),\'2000\'=rgb(214,39,40, max=255)))+\r\n  theme_bw(base_size = 15, base_family=\'\')\r\n\r\n################################################################################################\r\n# Is there an effect of side of implantation? (right vs left)\r\ntable(mydata$Ear)\r\n\r\n# More on the right than on the left, but plenty of observations.\r\nggplot(mydata,aes(x=Participant, y=Score, color=Ear))+\r\n  facet_wrap(~ Maplaw) +\r\n  geom_boxplot()\r\n\r\n# I don\'t see anything striking, but let\'s investigate statistically:\r\nmodel2=lmer(Score~ Maplaw + (1+Maplaw|Participant), data=mydata)\r\nmodel2b=lmer(Score~ Maplaw*Ear + (1+Maplaw|Participant), data=mydata)\r\nanova(model2b,model2)\r\n# No improvement in the model [Chi2(3)=1.1, p=0.766]. Ear is definitely NOT critical in this study.\r\n\r\n# Instead of a fixed factor, we could have a look at it as a random factor nested within participant, i.e. splitting the 3 bilateral users into \'6 ears\'\r\nmodel0=lm(Score~ Maplaw, data=mydata)\r\nmodel1b=lmer(Score~ Maplaw + (1|Ear:Participant), data=mydata)\r\nanova(model1b,model0)\r\n\r\n# Perhaps it\'s a little better than what we had with participant alone. [Chi2(1)=20.2, p<0.001]\r\nmodel1b=lmer(Score~ Maplaw + (1|Ear:Participant), data=mydata)\r\nmodel1c=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant), data=mydata)\r\nanova(model1c,model1b)\r\n# As earlier, the model improves with random slopes by ear:participant. [Chi2(5)=259.9, p<0.001]\r\n# Conclusion on the effect of ear: it\'s not really explaining anything per se. \r\n# But it acts like the variable Participant, by accounting for some of the individual differences\r\n# and rather than the 14 participants, this is now 17 \'ears\', giving more power to the model.\r\n\r\n# So, our best model is currently of the form:\r\nmodel1c=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant), data=mydata)\r\nmodel0c=lmer(Score~ 1 + (1+Maplaw|Ear:Participant), data=mydata)\r\nanova(model1c,model0c)\r\n# It shows a main effect of Maplaw [Chi2(2)=11.8, p=0.003]\r\n\r\nsummary(model1c)\r\n\r\n# To change baseline:\r\nmydata$Maplaw <- factor(mydata$Maplaw, levels=c(1000,500,2000))\r\n\r\n# and then, rerun the model\r\nmodel1c=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant), data=mydata)\r\nsummary(model1c)\r\n\r\n\r\ndata_forposthoc = emmeans(model1c, ~Maplaw)\r\ncontrast(data_forposthoc, ""pairwise"", simple=""each"")\r\n\r\nggplot(mydata,aes(x=Maplaw, y=Score))+\r\n  geom_boxplot()\r\n\r\n\r\n################################################################################################\r\n\r\n# The next variable that is a ""classic"" in mixed-effect analyses is item, which in your study refers to TrackNumber. \r\n# Start by plotting the data across TrackNumber to see whether this variable generates important variance.\r\nggplot(mydata,aes(x=TrackNumber, y=Score, color=Maplaw))+\r\n  geom_boxplot() +\r\n  scale_color_manual(""legend"",values=c(\'500\'=rgb(31,119,180, max=255),\'1000\'=\'darkgreen\',\'2000\'=rgb(214,39,40, max=255)))+\r\n  theme_bw(base_size = 15, base_family=\'\')\r\n\r\n# That is not striking: there is not a ton of difference between musical excerpts. \r\n# And that is presumably because of the MUSHRA method (the fact that all are ""standardized"" to the reference, i.e. preventing some excerpts to lead to scores around 10 while others would be around 90)\r\nmodel0=lm(Score~ Maplaw, data=mydata)\r\nmodel1b=lmer(Score~ Maplaw + (1|TrackNumber), data=mydata)\r\nanova(model1b,model0)\r\n\r\n# Indeed, the model DOES NOT improve by adding TrackNumber as a random factor [Chi2(1)=0.1, p=0.713]\r\n# Now, when the random intercept does not help, I generally would advise you to stop there with this variable, and look at other variables. \r\n# (but in this case, because of the MUSHRA method, the effect of TrackNumber might be redirected to differences between programs as opposed to an intercept)\r\nmodel0=lm(Score~ Maplaw, data=mydata)\r\nmodel1c=lmer(Score~ Maplaw + (1+Maplaw|TrackNumber), data=mydata)\r\nanova(model1c,model0)\r\n# No improvement in the model [Chi2(6)=4.3, p=0.632].\r\n# So let\'s just ignore TrackNumber; it\'s not helping at all.\r\n################################################################################################\r\n\r\n# The 25 excerpts were categorized into 5 different genres.\r\n# Let\'s plot it:\r\nggplot(mydata,aes(x=Genre, y=Score, color=Maplaw))+\r\n  geom_boxplot() +\r\n  scale_color_manual(""Maplaw"",values=c(\'500\'=rgb(31,119,180, max=255),\'1000\'=\'darkgreen\',\'2000\'=rgb(214,39,40, max=255)))+\r\n  theme_bw(base_size = 15, base_family=\'\')\r\n\r\n# At first sight, I would say that the benefit of 2000 over the reference program and the deficit of 500 over the reference program are fairly homogeneous between the five genres, but let\'s examine it properly.\r\n# The question is: should you consider Genre as a fixed factor or a random factor?\r\n# This is debatable. TrackNumber is definitely a random factor, because you could have thought of another musical excerpt to add to the study.\r\n# In other words, with your 25 excerpts, you did NOT EXHAUST all the possible musical excerpts that exist in the world.\r\n# In the same idea, one could argue that the five genres that you picked are not sufficient to represent all the possible musical genres that exist, in which case you have to consider it a random factor as well.\r\n# If so, you would do that: \r\nmodel2=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant), data=mydata)\r\nmodel5c=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant) + (1|Genre), data=mydata)\r\nanova(model5c,model2)\r\n# yes, the model improved further (from where we left it) with a random intercept per genre [Chi2(1)=12.3, p<0.001]\r\nmodel5c=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant) + (1|Genre), data=mydata)\r\nmodel5d=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant) + (1+Maplaw|Genre), data=mydata)\r\nanova(model5d,model5c)\r\n# Yes, the model improved still further with a random slope per genre [Chi2(5)=13.2, p=0.021]\r\n# Now, you final model would be quite complicated, of the following form:\r\nmodel5d=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant) + (1+Maplaw|Genre), data=mydata)\r\nsummary(model5d)\r\n\r\n# Conclusion: adding genre as an additional random factor hasn\'t had much impact in your estimates of the fixed effects.\r\n# So, this seems a bit of a waste. In my opinion, it would be better to consider genre as a fixed factor.\r\nmodel2=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant), data=mydata)\r\nmodel3=lmer(Score~ Maplaw+Genre + (1+Maplaw|Ear:Participant), data=mydata)\r\nanova(model3,model2)\r\n# Yes, there was a main effect of Genre [Chi2(4)=24.8, p<0.001]\r\nmodel3=lmer(Score~ Maplaw+Genre + (1+Maplaw|Ear:Participant), data=mydata)\r\nmodel4=lmer(Score~ Maplaw*Genre + (1+Maplaw|Ear:Participant), data=mydata)\r\nanova(model4,model3)\r\n# And the interaction just reached significance [Chi2(8)=16.3, p=0.038]\r\n# To better understand what caused the interaction, we need pairwise comparisons.\r\n\r\ndata_forposthoc = emmeans(model4, ~Maplaw*Genre)\r\ncontrast(data_forposthoc, ""pairwise"", simple=""each"")\r\n\r\n# Overall, this seems to me a decent approach, with a model of the form:\r\nmodel4=lmer(Score~ Maplaw*Genre + (1+Maplaw|Ear:Participant), data=mydata)\r\n\r\n\r\n################################################################################################\r\n# I see that you have a variable InstrumentalVocals. \r\ntable(mydata$Genre,mydata$IntrumentalVocals)\r\n\r\n# so, it\'s acting similarly as the variable \'genre\', just an easier split Classical/Jazz vs. the 3 others.\r\n# Well, we can reiterate the same analysis but replacing Genre by InstrumentalVocals if you think it\'s easier to interpret.\r\nggplot(mydata,aes(x=IntrumentalVocals, y=Score, color=Maplaw))+\r\n  geom_boxplot() +\r\n  labs(x=\'Mode of musical genre\') +\r\n  scale_x_discrete(breaks=c(\'I\',\'V\'), labels=c(\'Instrumentals\',\'Vocals\')) +\r\n  scale_color_manual(""Maplaw"",values=c(\'500\'=rgb(31,119,180, max=255),\'1000\'=\'darkgreen\',\'2000\'=rgb(214,39,40, max=255)))+\r\n  theme_bw(base_size = 15, base_family=\'\')\r\n\r\nmodel2=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant), data=mydata)\r\nmodel3=lmer(Score~ Maplaw+IntrumentalVocals + (1+Maplaw|Ear:Participant), data=mydata)\r\nanova(model3,model2)\r\n# Yes, a main effect of IntrumentalVocals [Chi2(1)=14.9, p<0.001]\r\nmodel3=lmer(Score~ Maplaw+IntrumentalVocals + (1+Maplaw|Ear:Participant), data=mydata)\r\nmodel4=lmer(Score~ Maplaw*IntrumentalVocals + (1+Maplaw|Ear:Participant), data=mydata)\r\nanova(model4,model3)\r\n# and a significant interaction [Chi2(2)=8.5, p=0.014]\r\ndata_forposthoc = emmeans(model4, ~Maplaw*IntrumentalVocals)\r\ncontrast(data_forposthoc, ""pairwise"", simple=""each"")\r\n\r\n###############################################################################################\r\ntable(mydata$UnilatBilat,mydata$Participant)\r\n# you also have a fair split between unilateral and bilateral users\r\n\r\nggplot(mydata,aes(x=Maplaw, y=Score, color=UnilatBilat))+\r\n  geom_boxplot()\r\n\r\nmodel1c=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant), data=mydata)\r\nmodel1d=lmer(Score~ Maplaw+UnilatBilat + (1+Maplaw|Ear:Participant), data=mydata)\r\nanova(model1d,model1c)\r\n# no main effect of UnilatBilat [Chi2(1)<0.1, p=0.882]\r\nmodel1d=lmer(Score~ Maplaw+UnilatBilat + (1+Maplaw|Ear:Participant), data=mydata)\r\nmodel1e=lmer(Score~ Maplaw*UnilatBilat + (1+Maplaw|Ear:Participant), data=mydata)\r\nanova(model1e,model1d)\r\n# no interaction with UnilatBilat [Chi2(2)=1.5, p=0.481]\r\n\r\n################################################################################################\r\n\r\n# Is there any effect of Gender? \r\nggplot(mydata,aes(x=Maplaw, y=Score, color=Gender))+\r\n  geom_boxplot()\r\n# Nothing striking.\r\n# Remember that the more complex model, the more likely it is that a good chunk of the variance is already accounted for by other variables.\r\n# So, to give it a fair chance, I\'ll rewind a little bit to a simpler model. \r\nmodel6a=lmer(Score~ Maplaw + (1|Ear:Participant), data=mydata)\r\nmodel6b=lmer(Score~ Maplaw+Gender + (1|Ear:Participant), data=mydata)\r\nanova(model6b,model6a)\r\n# No, gender serves no purpose as a main effect [Chi2(1)<0.1, p=0.834]\r\nmodel6b=lmer(Score~ Maplaw+Gender + (1|Ear:Participant), data=mydata)\r\nmodel6c=lmer(Score~ Maplaw*Gender + (1|Ear:Participant), data=mydata)\r\nanova(model6c,model6b)\r\n# slightly closer but not enough: gender does not interact with Maplaw [Chi2(2)=4.9, p=0.086]\r\n# Could it be that gender interacts with musical genre instead?\r\nggplot(mydata,aes(x=Genre, y=Score, color=Gender))+\r\n  facet_wrap(~Maplaw) +\r\n  geom_boxplot()\r\n# hardly anything\r\nmodel7a=lmer(Score~ Maplaw*Genre + (1|Ear:Participant), data=mydata)\r\nmodel7b=lmer(Score~ Maplaw*Genre+Gender + (1|Ear:Participant), data=mydata)\r\nanova(model7b,model7a)\r\n# no main effect of Gender [Chi2(1)<0.1, p=0.834]\r\nmodel7b=lmer(Score~ Maplaw*Genre+Gender + (1|Ear:Participant), data=mydata)\r\nmodel7c=lmer(Score~ Maplaw*Genre*Gender + (1|Ear:Participant), data=mydata)\r\nanova(model7c,model7b)\r\n# no interaction involving Gender [Chi2(14)=13.0, p=0.526]\r\n# Conclusion: it\'s safe to ignore gender in this study.\r\n################################################################################################\r\n\r\n# Whether or not participants had formal musical training could in principle make a big difference:\r\nggplot(mydata,aes(x=FormalMusicTrainingBinary, y=Score, color=Maplaw))+\r\n  geom_boxplot()\r\n# Interesting, it seems that non-musicians experience greater difference between programs than musicians\r\nmodel2=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant), data=mydata)\r\nmodel9=lmer(Score~ Maplaw+FormalMusicTrainingBinary + (1+Maplaw|Ear:Participant), data=mydata)\r\nanova(model9,model2)\r\n# But no main effect of FormalMusicTrainingBinary [Chi2(1)=0.1, p=0.702]\r\nmodel9=lmer(Score~ Maplaw+FormalMusicTrainingBinary + (1+Maplaw|Ear:Participant), data=mydata)\r\nmodel9b=lmer(Score~ Maplaw*FormalMusicTrainingBinary + (1+Maplaw|Ear:Participant), data=mydata)\r\nanova(model9b,model9)\r\n# And no interaction [Chi2(2)=2.0, p=0.363]\r\n\r\n# I think this is because the effect of musical training is already accounted for by the individual differences between participants.\r\nmydata$Maplaw <- factor(mydata$Maplaw, levels=c(1000,500,2000))\r\nmodel2=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant), data=mydata)\r\nmycoef=coef(model2)\r\n\r\nslopes500=mycoef[[""Ear:Participant""]]$Maplaw500\r\nslopes2000=mycoef[[""Ear:Participant""]]$Maplaw2000\r\nSUBnames=row.names(mycoef[[""Ear:Participant""]])\r\n\r\nmysubset=mydata %>%\r\n  select(Ear,Participant,Score,FormalMusicTrainingBinary,FormalMusicTrainingYrs,CurrentlyListeningHrs) %>%\r\n  group_by(Ear,Participant,FormalMusicTrainingBinary,FormalMusicTrainingYrs,CurrentlyListeningHrs) %>%\r\n  summarize(moyScore=mean(Score),N=n())\r\nmysubset$slopes500=slopes500\r\nmysubset$slopes2000=slopes2000\r\nmysubset$Participant=SUBnames\r\n\r\nmysubset2=gather(mysubset,""program"",""value"",8:9)\r\n\r\n# if we define Musical Training with a cutoff of minimum 5 years:\r\nmysubset2$FormalMusicTrainingBinary[4]=\'N\'\r\nmysubset2$FormalMusicTrainingBinary[10]=\'N\'\r\nmysubset2$FormalMusicTrainingBinary[11]=\'N\'\r\nmysubset2$FormalMusicTrainingBinary[21]=\'N\'\r\nmysubset2$FormalMusicTrainingBinary[27]=\'N\'\r\nmysubset2$FormalMusicTrainingBinary[28]=\'N\'\r\n\r\nanovaResult <- ezANOVA(data=mysubset2, dv=value,\r\n                       wid = Participant,\r\n                       within = program,\r\n                       between = FormalMusicTrainingBinary, \r\n                       type=3)\r\nprint(anovaResult)\r\n## With the original \'Y\' vs. \'N\' we don\'t get any support for music training\r\n## Main effect of Program [F(1,15)=11.8, p=0.004] \r\n## No main effect of Music Training [F(1,15)=1.5, p=0.234]\r\n## No interaction [F(1,15)=1.6, p=0.228]\r\n## With the cutoff of at least 5 years to be counted as musically trained, we do:\r\n## Main effect of Program [F(1,15)=13.7, p=0.002] \r\n## No main effect of Music Training [F(1,15)=2.5, p=0.133]\r\n## But a sign. interaction [F(1,15)=9.3, p=0.008]\r\n\r\nmysubset2$program <- factor(mysubset2$program, levels=c(\'slopes500\',\'slopes2000\'))\r\nggplot(mysubset2,aes(x=FormalMusicTrainingBinary, y=value, color=program))+\r\n  geom_boxplot()+\r\n  labs(x=\'Music training\',y=\'Sound quality preference re: reference\')+\r\n  scale_x_discrete(labels=c(\'< 4 years\',\'> 4 years\'))+\r\n  scale_color_manual(\'Maplaw\',limits=c(\'slopes500\',\'slopes2000\'),values=c(\'slopes500\'=rgb(31,119,180, max=255),\'slopes2000\'=rgb(214,39,40, max=255)),labels=c(\'500\',\'2000\'))+\r\n  theme_bw(base_size = 15, base_family=\'\')+\r\n  theme(legend.position=\'top\')\r\n\r\n\r\nplot1a <- ggscatter(mysubset, x = ""FormalMusicTrainingYrs"", y = ""slopes500"", \r\n          add = ""reg.line"", conf.int = TRUE, color=rgb(31,119,180, max=255),\r\n          cor.coef = TRUE, cor.coeff.args = list(method=""pearson"",label.x = 15, label.y=-17,r.digits=2,p.digits=2),\r\n          xlab = ""Musical training (years)"", ylab = ""Preference"",title=\'Maplaw = 500\',xlim=c(-0.5,25.5),ylim=c(-30,30))\r\nplot1b <- ggscatter(mysubset, x = ""FormalMusicTrainingYrs"", y = ""slopes2000"", \r\n                   add = ""reg.line"", conf.int = TRUE, color=rgb(214,39,40, max=255),\r\n                   cor.coef = TRUE, cor.coeff.args = list(method=""pearson"",label.x = 15, label.y=15,r.digits=2,p.digits=2),\r\n                   xlab = ""Musical training (years)"", ylab = """",title=\'Maplaw = 2000\',xlim=c(-0.5,25.5),ylim=c(-30,30))\r\nplot2a <- ggscatter(mysubset, x = ""CurrentlyListeningHrs"", y = ""slopes500"", \r\n                   add = ""reg.line"", conf.int = TRUE, color=rgb(31,119,180, max=255),\r\n                   cor.coef = TRUE, cor.coeff.args = list(method=""pearson"",label.x = 0, label.y=20,r.digits=2,p.digits=3),\r\n                   xlab = ""Hours of music listening /week"", ylab = ""Preference"",xlim=c(-0.5,25.5),ylim=c(-30,30))\r\nplot2b <- ggscatter(mysubset, x = ""CurrentlyListeningHrs"", y = ""slopes2000"", \r\n                   add = ""reg.line"", conf.int = TRUE, \r\n                   cor.coef = TRUE, cor.coeff.args = list(method=""pearson"",label.x = 0, label.y=20,r.digits=2,p.digits=3), color=rgb(214,39,40, max=255),\r\n                   xlab = ""Hours of music listening /week"", ylab = """",xlim=c(-0.5,25.5),ylim=c(-30,30))\r\n\r\n\r\nthisway <- rbind(c(1,1,2,2),\r\n                 c(1,1,2,2),\r\n                 c(3,3,4,4),\r\n                 c(3,3,4,4))\r\n\r\ngrid.arrange(plot1a,plot1b,plot2a,plot2b,layout_matrix=thisway)\r\n\r\n\r\nggscatter(mysubset, x = ""slopes500"", y = ""slopes2000"", shape=19, size=4,\r\n          add = ""reg.line"", conf.int = TRUE, color=\'black\',\r\n          cor.coef = TRUE, cor.method = ""pearson"",\r\n          xlab = ""Sound quality preference for Maplaw=500"", ylab = ""Sound quality preference for Maplaw=2000"", title=\'Liking the higher Maplaw was consistent with a dislike of the lower Maplaw\')\r\n\r\n\r\n# We could attempt a random intercept by MusicalTraining rather than Participant\r\nmodel0=lm(Score~ Maplaw, data=mydata)\r\nmodel10a=lmer(Score~ Maplaw + (1+Maplaw|FormalMusicTrainingBinary), data=mydata)\r\nanova(model10a,model0)\r\n# yes, musical training could indeed matter [Chi2(6)=23.3, p<0.001], but it is redundant once considering Participant.\r\n# perhaps a simple t-test between 2 groups of participants with/without musicaltraining\r\nt.test(mysubset$slopes500 ~ mysubset$FormalMusicTrainingBinary, var.equal=TRUE)\r\n# not significant for program 500: [t(15)=-1.4, p=0.178]\r\nt.test(mysubset$slopes2000 ~ mysubset$FormalMusicTrainingBinary, var.equal=TRUE)\r\n# not significant for program 2000: [t(15)=0.7, p=0.492]\r\n\r\n\r\nplot0 <- ggplot(mydata,aes(x=FormalMusicTrainingYrs, y=Score, color=Maplaw))+\r\n  geom_point() +\r\n  facet_wrap(~Maplaw) +\r\n  stat_smooth(method=\'lm\')\r\n# yes, differences between programs fade away with more years of musical training\r\n\r\nthisway <- rbind(c(1,1,1,1), \r\n                 c(1,1,1,1),\r\n                 c(2,2,3,3),\r\n                 c(2,2,3,3))\r\ngrid.arrange(plot0,plot1,plot2,layout_matrix=thisway)\r\n\r\nmodel9=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant), data=mydata)\r\nmodel9c=lmer(Score~ Maplaw*FormalMusicTrainingYrs + (1+Maplaw|Ear:Participant), data=mydata)\r\nanova(model9c,model9)\r\n# But once again, as long as the model includes a random factor to account for individual differences, \r\n# there is not enough power redirected to years of musical training. [Chi2(3)=5.7, p=0.125]\r\n\r\n################################################################################################\r\n# Let\'s look at other continuous variables\r\n# Let\'s start with age.\r\nggplot(mydata,aes(x=AgeYrs, y=Score, color=Maplaw))+\r\n  geom_point()\r\n\r\n# You can examine a continuous variable just like a categorical one, but different calculations are made. \r\n# Once again, let\'s go back to a simpler model.\r\nmodel1b=lmer(Score~ Maplaw + (1|Ear:Participant), data=mydata)\r\nmodel8a=lmer(Score~ Maplaw*AgeYrs + (1|Ear:Participant), data=mydata)\r\nanova(model8a,model1b)\r\n\r\n# Apparently, yes, it would be worth adding age, because it interacts with Maplaw [Chi(3)=8.9, p=0.030]\r\n# To see this, we can examine a linear trend on each program separately.\r\nplot0 <- ggplot(mydata,aes(x=AgeYrs, y=Score, color=Maplaw))+\r\n  geom_point()+\r\n  stat_smooth(method=\'lm\')\r\n# There you go, differences between programs progressively disappear with age.\r\n\r\nmysubset=mydata %>%\r\n  select(Ear,Participant,Score,AgeYrs) %>%\r\n  group_by(Ear,Participant) %>%\r\n  summarize(moyScore=mean(Score),Age=mean(AgeYrs),N=n())\r\nmysubset$slopes500=slopes500\r\nmysubset$slopes2000=slopes2000\r\n\r\nplot3a <- ggscatter(mysubset, x = ""Age"", y = ""slopes500"", \r\n                   add = ""reg.line"", conf.int = TRUE, color=rgb(31,119,180, max=255),\r\n                   cor.coef = TRUE, cor.coeff.args = list(method=""pearson"",label.x = 65, label.y=25,r.digits=1,p.digits=3),\r\n                   xlab = ""Chronological age (years)"", ylab = ""Preference"",xlim=c(34,88),ylim=c(-30,30))\r\nplot3b <- ggscatter(mysubset, x = ""Age"", y = ""slopes2000"", \r\n                   add = ""reg.line"", conf.int = TRUE, color=rgb(214,39,40, max=255),\r\n                   cor.coef = TRUE, cor.coeff.args = list(method=""pearson"",label.x = 65, label.y=-15,r.digits=2,p.digits=3),\r\n                   xlab = ""Chronological age (years)"", ylab = """",xlim=c(34,88),ylim=c(-30,30))\r\n\r\nthisway <- rbind(c(1,1,1,1), \r\n                 c(1,1,1,1),\r\n                 c(2,2,3,3),\r\n                 c(2,2,3,3))\r\ngrid.arrange(plot0,plot3a,plot3b,layout_matrix=thisway)\r\n\r\n\r\n\r\n# With this large range of chronological age, age at implantation must be strongly correlated.\r\nggplot(mydata,aes(x=AgeYrs, y=AgeImplantation))+\r\n  geom_point()+\r\n  stat_smooth(method=\'lm\',col=\'red\')\r\n# yes. So plotting it this way won\'t change much:\r\nggplot(mydata,aes(x=AgeImplantation, y=Score, color=Maplaw))+\r\n  facet_wrap(~Maplaw)+\r\n  geom_point()+\r\n  stat_smooth(method=\'lm\')\r\n# indeed, it\'s the same idea. \r\nmodel1b=lmer(Score~ Maplaw + (1|Ear:Participant), data=mydata)\r\nmodel8b=lmer(Score~ Maplaw*AgeImplantation + (1|Ear:Participant), data=mydata)\r\nanova(model8b,model1b)\r\n# But this time, the model does NOT improve [Chi2(3)=4.9, p=0.176]\r\n# meaning that the former relationship was more related to chronological age than implantation (despite the two covarying)\r\n\r\n\r\n################################################################################################\r\n\r\nmysubset=mydata %>%\r\n  select(Ear,Participant,Score,DurationCIuse) %>%\r\n  group_by(Ear,Participant) %>%\r\n  summarize(moyScore=mean(Score),CIuse=mean(DurationCIuse),N=n())\r\nmysubset$slopes500=slopes500\r\nmysubset$slopes2000=slopes2000\r\n\r\nplot4a <- ggscatter(mysubset, x = ""CIuse"", y = ""slopes500"", \r\n                    add = ""reg.line"", conf.int = TRUE, color=rgb(31,119,180, max=255),\r\n                    cor.coef = TRUE, cor.coeff.args = list(method=""pearson"",label.x = 0, label.y=25,r.digits=2,p.digits=3),\r\n                    xlab = ""Duration of CI use (years)"", ylab = ""Preference"",xlim=c(-0.5,8.5),ylim=c(-30,30))\r\nplot4b <- ggscatter(mysubset, x = ""CIuse"", y = ""slopes2000"", \r\n                    add = ""reg.line"", conf.int = TRUE, color=rgb(214,39,40, max=255),\r\n                    cor.coef = TRUE, cor.coeff.args = list(method=""pearson"",label.x = 0, label.y=-15,r.digits=2,p.digits=3),\r\n                    xlab = ""Duration of CI use (years)"", ylab = """",xlim=c(-0.5,8.5),ylim=c(-30,30))\r\n\r\n\r\nthisway <- rbind(c(1,1,1,1), \r\n                 c(1,1,1,1),\r\n                 c(2,2,3,3),\r\n                 c(2,2,3,3))\r\ngrid.arrange(plot0,plot4a,plot4b,layout_matrix=thisway)\r\n\r\n################################################################################################\r\n\r\n\r\nmysubset=mydata %>%\r\n  select(Ear,Participant,Score,OnsetHLyrs,OnsetSevProfHLyrs) %>%\r\n  group_by(Ear,Participant) %>%\r\n  summarize(moyScore=mean(Score),onsetHL=mean(OnsetHLyrs),onsetPHL=mean(OnsetSevProfHLyrs),N=n())\r\nmysubset$slopes500=slopes500\r\nmysubset$slopes2000=slopes2000\r\n\r\nplot5a <- ggscatter(mysubset, x = ""onsetHL"", y = ""slopes500"", \r\n                    add = ""reg.line"", conf.int = TRUE, color=rgb(31,119,180, max=255),\r\n                    cor.coef = TRUE, cor.coeff.args = list(method=""pearson"",label.x = 0, label.y=15,r.digits=2,p.digits=3),\r\n                    xlab = ""Onset of HL (years)"", ylab = ""Preference"",xlim=c(-0.5,76),ylim=c(-30,30))\r\nplot5b <- ggscatter(mysubset, x = ""onsetHL"", y = ""slopes2000"", \r\n                    add = ""reg.line"", conf.int = TRUE, color=rgb(214,39,40, max=255),\r\n                    cor.coef = TRUE, cor.coeff.args = list(method=""pearson"",label.x = 0, label.y=-5,r.digits=2,p.digits=3),\r\n                    xlab = ""Onset of HL (years)"", ylab = """",xlim=c(-0.5,76),ylim=c(-30,30))\r\n\r\nplot6a <- ggscatter(mysubset, x = ""onsetPHL"", y = ""slopes500"", \r\n                    add = ""reg.line"", conf.int = TRUE, color=rgb(31,119,180, max=255),\r\n                    cor.coef = TRUE, cor.coeff.args = list(method=""pearson"",label.x = 0, label.y=15,r.digits=2,p.digits=3),\r\n                    xlab = ""Onset of profound HL (years)"", ylab = ""Preference"",xlim=c(-0.5,88),ylim=c(-30,30))\r\nplot6b <- ggscatter(mysubset, x = ""onsetPHL"", y = ""slopes2000"", \r\n                    add = ""reg.line"", conf.int = TRUE, color=rgb(214,39,40, max=255),\r\n                    cor.coef = TRUE, cor.coeff.args = list(method=""pearson"",label.x = 0, label.y=-5,r.digits=2,p.digits=3),\r\n                    xlab = ""Onset of profound HL (years)"", ylab = """",xlim=c(-0.5,88),ylim=c(-30,30))\r\n\r\nthisway <- rbind(c(1,1,2,2), \r\n                 c(1,1,2,2),\r\n                 c(3,3,4,4),\r\n                 c(3,3,4,4))\r\ngrid.arrange(plot5a,plot5b,plot6a,plot6b,layout_matrix=thisway)\r\n\r\n\r\n# recombine figures for publication\r\nthisway <- rbind(c(1,1,2,2), \r\n                 c(3,3,4,4),\r\n                 c(5,5,6,6))\r\ngrid.arrange(plot1a,plot1b,plot3a,plot3b,plot4a,plot4b,layout_matrix=thisway)\r\n\r\nthisway <- rbind(c(1,1,2,2), \r\n                 c(3,3,4,4))\r\ngrid.arrange(plot5a,plot5b,plot6a,plot6b,layout_matrix=thisway)\r\n\r\n\r\n################################################################################################\r\n\r\nggplot(mydata,aes(x=AgeYrs, y=CurrentlyListeningHrs, color=FormalMusicTrainingBinary))+\r\n  geom_point()\r\n\r\nggplot(mydata,aes(x=CurrentlyListeningHrs, y=Score, color=Maplaw))+\r\n  facet_wrap(~Maplaw)+\r\n  geom_point()+\r\n  stat_smooth(method=\'lm\')\r\n\r\nmodelF=lmer(Score~ Maplaw + (1|Ear:Participant), data=mydata)\r\nmodel10=lmer(Score~ Maplaw*CurrentlyListeningHrs + (1|Ear:Participant), data=mydata)\r\nanova(model10,modelF)\r\n# [Chi2(3)=10.8, p=0.013] Score rises with higher nb of listening hours, and slopes significantly differ from the baseline program (flat)\r\n\r\nmysubset=mydata %>%\r\n  select(Ear,Participant,Maplaw,CurrentlyListeningHrs,Score) %>%\r\n  group_by(Ear,Participant,Maplaw,CurrentlyListeningHrs) %>%\r\n  summarize(moyScore=mean(Score),N=n(),CLHrs=mean(CurrentlyListeningHrs))\r\n\r\nggplot(mysubset,aes(x=CLHrs, y=moyScore, color=Maplaw))+\r\n  facet_wrap(~Maplaw)+\r\n  geom_point()+\r\n  stat_smooth(method=\'lm\')\r\n\r\nmodelF=lmer(moyScore~ Maplaw + (1|Ear:Participant), data=mysubset)\r\nmodel10=lmer(moyScore~ Maplaw*CLHrs + (1|Ear:Participant), data=mysubset)\r\nanova(model10,modelF)\r\n# Averaging across the 25 observations of a given program: we lost so much power, that this relationship is absent [Chi2(3)=2.6, p=0.450]\r\n\r\n################################################################################################\r\n# Now, we can look at factors related to the device.. although N=17 is definintely not much power to examine as many factors as you put in this table, but let\'s see.\r\ntable(mydata$ThresholdSettings,mydata$Participant)\r\n# The problem is that we cannot look at the effect of Threshold settings independently of Participant.\r\n# I think the only thing I can do is plot the random slopes as a function of Threshold settings.\r\n\r\nplot0 <-ggplot(mydata,aes(x=Maplaw, y=Score, color=ThresholdSettings))+\r\n  geom_boxplot()\r\n\r\n# modelF=lmer(Score~ Maplaw + (1+Maplaw|Ear:Participant), data=mydata)\r\n#model11a=lmer(Score~ Maplaw+ThresholdSettings + (1+Maplaw|Ear:Participant), data=mydata)\r\n#anova(model11a,modelF)\r\n## No main effect of ThresholdSettings [Chi2(2)=0.5, p=0.779]\r\n#model11a=lmer(Score~ Maplaw+ThresholdSettings + (1+Maplaw|Ear:Participant), data=mydata)\r\n#model11b=lmer(Score~ Maplaw*ThresholdSettings + (1+Maplaw|Ear:Participant), data=mydata)\r\n#anova(model11b,model11a)\r\n## No interaction [Chi2(4)=2.6, p=0.635]\r\n## Conclusion: we can drop ThresholdSettings.\r\n\r\nmysubset=mydata %>%\r\n  select(Ear,Participant,Score,ThresholdSettings,ElectrodeArrayType,NumActiveElectrodes,NumFineStructureChannels,ProcessingStrategy) %>%\r\n  group_by(Ear,Participant,ThresholdSettings,ElectrodeArrayType,NumActiveElectrodes,NumFineStructureChannels,ProcessingStrategy) %>%\r\n  summarize(moyScore=mean(Score),N=n())\r\nmysubset$slopes500=slopes500\r\nmysubset$slopes2000=slopes2000\r\nmysubset$Participant=SUBnames\r\n\r\nmysubset2=gather(mysubset,""program"",""value"",10:11)\r\nmysubset2$program <- factor(mysubset2$program, levels=c(\'slopes500\',\'slopes2000\'))\r\n\r\nplot1 <- ggplot(mysubset2,aes(x=ThresholdSettings, y=value, color=program))+\r\n  geom_boxplot()+\r\n  labs(x=\'Threshold setting\',y=\'Sound Quality Preference\')+\r\n  scale_color_manual(\'Maplaw\',limits=c(\'slopes500\',\'slopes2000\'),values=c(\'slopes500\'=rgb(31,119,180, max=255),\'slopes2000\'=rgb(214,39,40, max=255)),labels=c(\'500\',\'2000\'))+\r\n  theme(legend.position=\'none\')\r\n\r\nanovaResult <- ezANOVA(data=mysubset2, dv=value,\r\n                       wid = Participant,\r\n                       within = program,\r\n                       between = ThresholdSettings, \r\n                       type=3)\r\nprint(anovaResult)\r\n## this is similar to SPSS:\r\n## Main effect of Program [F(1,14)=10.4, p=0.006]\r\n## No main effect of ThresholdSettings [F(2,14)=1.2, p=0.332]\r\n## No interaction [F(2,14)=0.6, p=0.576]\r\n\r\ntable(mydata$ElectrodeArrayType,mydata$Participant)\r\nmysubset2$ElectrodeArrayType[3]=\'Flex24\'\r\nmysubset2$ElectrodeArrayType[20]=\'Flex24\'\r\nanovaResult <- ezANOVA(data=mysubset2, dv=value,\r\n                       wid = Participant,\r\n                       within = program,\r\n                       between = ElectrodeArrayType, \r\n                       type=3)\r\nprint(anovaResult)\r\n## Main effect of Program [F(1,14)=9.3, p=0.009]\r\n## No main effect of ElectrodeArrayType [F(2,14)<0.1, p=0.940]\r\n## No interaction [F(2,14)=0.3, p=0.765]\r\nplot2 <-   ggplot(mysubset2,aes(x=ElectrodeArrayType, y=value, color=program))+\r\n  geom_boxplot()+\r\n  labs(x=\'Type of electrode array\',y=\'\')+\r\n  scale_x_discrete(labels=c(\'Flex24 / Medium\',\'Flex28\',\'Standard\'))+\r\n  scale_color_manual(\'Maplaw\',limits=c(\'slopes500\',\'slopes2000\'),values=c(\'slopes500\'=rgb(31,119,180, max=255),\'slopes2000\'=rgb(214,39,40, max=255)),labels=c(\'500\',\'2000\'))+\r\n  theme(legend.position=\'none\')\r\n\r\n\r\n\r\ntable(mydata$ProcessingStrategy,mydata$Participant)\r\n# there\'s only 1 subject who has a HDCIS, which makes for a very unbalanced design. \r\n# but this time, I don\'t think I can put the HDCIS with any other group\r\n# mysubset2$ElectrodeArrayType[8]=\'FS4\'\r\n# mysubset2$ElectrodeArrayType[25]=\'FS4\'\r\nanovaResult <- ezANOVA(data=mysubset2, dv=value,\r\n                       wid = Participant,\r\n                       within = program,\r\n                       between = ProcessingStrategy, \r\n                       type=3)\r\nprint(anovaResult)\r\n## Missed effect of Program [F(1,13)=2.5, p=0.136] .. we lost power with a group of 1 subject\r\n## No main effect of ProcessingStrategy [F(2,14)=0.8, p=0.847]\r\n## No interaction [F(2,14)=0.3, p=0.779]\r\nplot3 <- ggplot(mysubset2,aes(x=ProcessingStrategy, y=value, color=program))+\r\n  geom_boxplot()+\r\n  labs(x=\'Processing Strategy\',y=\'Sound Quality Preference\')+\r\n  scale_color_manual(\'Maplaw\',limits=c(\'slopes500\',\'slopes2000\'),values=c(\'slopes500\'=rgb(31,119,180, max=255),\'slopes2000\'=rgb(214,39,40, max=255)),labels=c(\'500\',\'2000\'))+\r\n  theme(legend.position=\'none\')\r\n\r\n\r\n\r\n\r\ntable(mydata$NumActiveElectrodes,mydata$Participant)\r\nmysubset2$NumActiveElectrodes[6]=10\r\nmysubset2$NumActiveElectrodes[11]=10\r\nmysubset2$NumActiveElectrodes[12]=10\r\nmysubset2$NumActiveElectrodes[23]=10\r\nmysubset2$NumActiveElectrodes[28]=10\r\nmysubset2$NumActiveElectrodes[29]=10\r\n\r\nanovaResult <- ezANOVA(data=mysubset2, dv=value,\r\n                       wid = Participant,\r\n                       within = program,\r\n                       between = NumActiveElectrodes, \r\n                       type=3)\r\nprint(anovaResult)\r\n## Main effect of Program [F(1,15)=10.6, p=0.005] \r\n## Main effect of NumActiveElectrodes [F(1,15)=5.0, p=0.042]\r\n## No interaction [F(1,15)=0.8, p=0.380]\r\n\r\nplot4 <- ggplot(mysubset2,aes(x=NumActiveElectrodes, y=value, color=program))+\r\n  geom_boxplot()+\r\n  labs(x=\'Number of active electrodes\',y=\'\')+\r\n  scale_x_discrete(labels=c(\'10 / 11\',\'12\'))+\r\n  scale_color_manual(\'Maplaw\',limits=c(\'slopes500\',\'slopes2000\'),values=c(\'slopes500\'=rgb(31,119,180, max=255),\'slopes2000\'=rgb(214,39,40, max=255)),labels=c(\'500\',\'2000\'))+\r\n  theme(legend.position=\'none\')\r\n\r\ntable(mydata$NumFineStructureChannels,mydata$Participant)\r\nmysubset2$NumFineStructureChannels[3]=0\r\nmysubset2$NumFineStructureChannels[6]=0\r\nmysubset2$NumFineStructureChannels[8]=0\r\nmysubset2$NumFineStructureChannels[12]=0\r\nmysubset2$NumFineStructureChannels[14]=0\r\nmysubset2$NumFineStructureChannels[20]=0\r\nmysubset2$NumFineStructureChannels[23]=0\r\nmysubset2$NumFineStructureChannels[25]=0\r\nmysubset2$NumFineStructureChannels[29]=0\r\nmysubset2$NumFineStructureChannels[31]=0\r\n\r\nanovaResult <- ezANOVA(data=mysubset2, dv=value,\r\n                       wid = Participant,\r\n                       within = program,\r\n                       between = NumFineStructureChannels, \r\n                       type=3)\r\nprint(anovaResult)\r\n## Main effect of Program [F(1,15)=7.0, p=0.018] \r\n## No main effect of NumFineStructureChannels [F(1,15)=1.8, p=0.201]\r\n## No interaction [F(1,15)=0.1, p=0.730]\r\n\r\nplot5 <-   ggplot(mysubset2,aes(x=NumFineStructureChannels, y=value, color=program))+\r\n  geom_boxplot()+\r\n  labs(x=\'Number of FS channels\',y=\'Random slope\')+\r\n  scale_x_discrete(labels=c(\'0 to 3\',\'4\'))+\r\n  scale_color_manual(\'Maplaw\',limits=c(\'slopes500\',\'slopes2000\'),values=c(\'slopes500\'=rgb(31,119,180, max=255),\'slopes2000\'=rgb(214,39,40, max=255)),labels=c(\'500\',\'2000\'))+\r\n  theme(legend.position=\'none\')\r\n\r\n\r\nthisway <- rbind(c(1,1,2,2), \r\n                 c(1,1,2,2),\r\n                 c(3,3,4,4),\r\n                 c(3,3,4,4))\r\n\r\ngrid.arrange(plot1,plot2,plot3,plot4,layout_matrix=thisway)']","Cochlear Implant Compression Optimization for Music Listening - Maplaw and AGC Variations in loudness are a fundamental component of the music listening experience. Cochlear implant (CI) processing, including amplitude compression, and a degraded auditory system may further degrade these loudness cues and decrease the enjoyment of music listening. This study aimed to identify optimal CI sound processor compression settings to improve music sound quality for CI users.Fourteen adult MED-EL CI recipients participated (Experiment #1: n=17 ears; Experiment #2: n=11 ears) in the study. A software application using a modified Comparison Category Rating (CCR) test method allowed participants to compare and rate the sound quality of various CI compression settings while listening to 25 real-world music clips. The two compression settings studied were: 1) Maplaw, which informs audibility and compression of soft level sounds, and 2) Automatic Gain Control (AGC), which applies compression to loud sounds. For each experiment, one compression setting (Maplaw or AGC) was held at the default while the other was varied according to the values available in the clinical CI programming software. Experiment #1 compared Maplaw settings of 500, 1000 (default), and 2000. Experiment #2 compared AGC settings of 2.5:1, 3:1 (default), and 3.5:1.In Experiment #1, the group preferred a higher Maplaw setting of 2000 over the default Maplaw setting of 1000 (p = 0.003) for music listening. There was no significant difference in music sound quality between the Maplaw setting of 500 and the default setting (p = 0.278). In Experiment #2, a main effect of AGC setting was found; however, no significant difference in sound quality ratings for pairwise comparisons were found between the experimental settings and the default setting (2.5:1 versus 3:1 at p = 0.546; 3.5:1 versus 3:1 at p = 0.059).CI users reported improvements in music sound quality with higher than default Maplaw or AGC settings. Thus, participants preferred slightly higher compression for music listening, with results having clinical implications for improving music perception in CI users.",3
Data from: Population collapse of habitat-forming species in the Mediterranean: a long-term study of gorgonian populations affected by recurrent marine heatwaves,"Understanding the resilience of temperate reefs to climate change requires exploring the recovery capacity of their habitat-forming species from recurrent marine heatwaves (MHWs). Here, we show that, in a Mediterranean highly enforced Marine Protected Area established more than 40 years ago, habitat-forming octocoral populations that were firstly affected by a severe MHW in 2003 have not recovered after 15 years. Contrarily, they have followed collapse trajectories that have brought them to the brink of local ecological extinction. Since 2003, impacted populations of the red gorgonian Paramuricea clavata (Risso, 1826) and the red coral Corallium rubrum (Linnaeus, 1758) have followed different trends in terms of size structure, but a similar progressive reduction in density and biomass. Concurrently, recurrent MHWs were observed in the area during the 2003-2018 study period, which may have hindered populations recovery. The studied octocorals play a unique habitat-forming role in the coralligenous assemblages (i.e., reefs endemic to the Mediterranean Sea home to approximately 10% of its species). Therefore, our results underpin the great risk that recurrent MHWs pose for the long-term integrity and functioning of these emblematic temperate reefs.",,"Data from: Population collapse of habitat-forming species in the Mediterranean: a long-term study of gorgonian populations affected by recurrent marine heatwaves Understanding the resilience of temperate reefs to climate change requires exploring the recovery capacity of their habitat-forming species from recurrent marine heatwaves (MHWs). Here, we show that, in a Mediterranean highly enforced Marine Protected Area established more than 40 years ago, habitat-forming octocoral populations that were firstly affected by a severe MHW in 2003 have not recovered after 15 years. Contrarily, they have followed collapse trajectories that have brought them to the brink of local ecological extinction. Since 2003, impacted populations of the red gorgonian Paramuricea clavata (Risso, 1826) and the red coral Corallium rubrum (Linnaeus, 1758) have followed different trends in terms of size structure, but a similar progressive reduction in density and biomass. Concurrently, recurrent MHWs were observed in the area during the 2003-2018 study period, which may have hindered populations recovery. The studied octocorals play a unique habitat-forming role in the coralligenous assemblages (i.e., reefs endemic to the Mediterranean Sea home to approximately 10% of its species). Therefore, our results underpin the great risk that recurrent MHWs pose for the long-term integrity and functioning of these emblematic temperate reefs.",3
Code for data visualization and statistical analysis,"Original code in R used to process genetic association test results generated from SAIGE and METAL. The code is not generalized, it reads in specific summary statistics results files which are available upon request. The code creates data visualizations, figures for a manuscript, and tests for differences in effect sizes.","['library(ggplot2)\nlibrary(data.table)\n\n############# HUNT and UKBBB dominant and recessive results ############\n\nukbb_dom<-read.table(file=""dominant_model_results.txt"",header=T)\nukbb_rec<-read.table(file=""recessive_model_results.txt"",header=T)\nukbb_add<-read.table(file=""additive_model_results.txt"",header=T)\nukbb_add<-ukbb_add[,c(3,1,2,4,5,6,7,8,9,10,11,12,13,14,15,16,17)]\nnames(ukbb_add)[4]<-""Allele0""\nnames(ukbb_add)[5]<-""Allele1""\nnames(ukbb_add)[6]<-""AC""\nnames(ukbb_add)[7]<-""AF""\nukbb_dom$model<-""dominant""\nukbb_rec$model<-""recessive""\nukbb_add$model<-""additive""\nukbb_add$sample<-""UKBB""\nukbb_dom$sample<-""UKBB""\nukbb_rec$sample<-""UKBB""\n\nhunt_dom<-read.table(file=""HUNT_dominant_results.txt"",header=T)\nhunt_rec<-read.table(file=""HUNT_recessive_results.txt"",header=T)\nhunt_add<-read.table(file=""HUNT_additive_results.txt"",header=T)\nhunt_dom$model<-""dominant""\nhunt_rec$model<-""recessive""\nhunt_add$model<-""additive""\nhunt_dom$sample<-""HUNT""\nhunt_rec$sample<-""HUNT""\nhunt_add$sample<-""HUNT""\n\ndf<-rbind(ukbb_dom,ukbb_rec,ukbb_add,hunt_dom,hunt_rec,hunt_add)\ndf<-df[grep(""55"",df$study,invert=TRUE),] #remove lt 55 and gt 55\ndf<-df[grep(""571_"",df$study,invert=TRUE),] #remove chronic liver disease and keep nonalcoholic liver disease\ndf<-df[grep(""401.22_"",df$study,invert=TRUE),] #remove kidney\ndf$trait<-""NA""\ndf[grep(""IHD"",df$study),]$trait<-""IHD""\ndf[grep(""T2D"",df$study),]$trait<-""T2D""\ndf[grep(""liver"",df$study),]$trait<-""liver""\ndf[grep(""411"",df$study),]$trait<-""IHD""\ndf[grep(""250"",df$study),]$trait<-""T2D""\ndf[grep(""571.5_"",df$study),]$trait<-""liver""\ndf$subset<-""ALL""\ndf[grep(""_MALE"",df$study),]$subset<-""MALE""\ndf[grep(""_FEMALE"",df$study),]$subset<-""FEMALE""\n\n#95% CI\ndf$ub<-df$BETA + 1.96*df$SE\ndf$lb<-df$BETA - 1.96*df$SE\n\ndf$neglogp<- -log10(df$p.value)\n\nsub<-df[df$model==""dominant""| df$model==""recessive"",]\npdf(file=""models.pdf"",height=6,width=6)\nlimits<-aes(xmin=sub$lb,xmax=sub$ub)\nggplot(sub,aes(x=BETA,y=neglogp,color=trait,shape=subset)) + theme_bw() + geom_point(alpha=0.7) + facet_wrap(model~sample) +\n  labs(title=""Dominant and Recessive Models for rs58542926"",y=expression(-log[10](pvalue)),x=expression(italic(hat(beta)))) + geom_vline(color=""black"",xintercept=0,alpha=0.5) + \n  scale_color_manual(values=c(""grey"",""goldenrod3"",""dark blue"")) + geom_errorbarh(limits,height=0.3,alpha=0.7) + theme(plot.title=element_text(hjust=0.5)) + \n  geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(0.05/6))\ndev.off()\n\npdf(file=""all_models.pdf"",height=6,width=8)\nlimits<-aes(xmin=df$lb,xmax=df$ub)\nggplot(df,aes(x=BETA,y=neglogp,color=trait,shape=subset)) + theme_bw() + geom_point(alpha=0.7) + facet_wrap(sample~model,nrow=2,scales=""free"") +\n  labs(title=""Single Variant Association Test Models for rs58542926"",y=expression(-log[10](pvalue)),x=expression(italic(hat(beta)))) + geom_vline(color=""black"",xintercept=0,alpha=0.5) + \n  scale_color_manual(values=c(""grey"",""goldenrod3"",""dark blue"")) + geom_errorbarh(limits,height=0.3,alpha=0.7) + theme(plot.title=element_text(hjust=0.5)) + \n  geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(0.05/6))\ndev.off()\n\nsub<-df[df$model==""additive"",]\npdf(file=""additive.pdf"",height=4,width=5)\nlimits<-aes(xmin=sub$lb,xmax=sub$ub)\nggplot(sub,aes(x=BETA,y=neglogp,color=trait,shape=subset)) + theme_bw() + geom_point(alpha=0.7) + facet_wrap(~sample) +\n  labs(title=""Additive Model for rs58542926"",y=expression(-log[10](pvalue)),x=expression(italic(hat(beta)))) + geom_vline(color=""black"",xintercept=0,alpha=0.5) + \n  scale_color_manual(values=c(""grey"",""goldenrod3"",""dark blue"")) + geom_errorbarh(limits,height=0.3,alpha=0.7) + theme(plot.title=element_text(hjust=0.5)) + \n  geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(0.05/6))\ndev.off()\n\n##odds ratio\ndf$OR<-exp(df$BETA)\ndf$OR_LB<-exp(df$lb)\ndf$OR_UB<-exp(df$ub)\ndf$CI<-paste(sep=""-"",as.character(format(df$OR_LB,digits=2)),trimws(as.character(format(df$OR_UB,digits=2))))\n\nsub<-df[df$model==""dominant""| df$model==""recessive"",]\npdf(file=""models_OR.pdf"",height=6,width=6)\nlimits<-aes(xmin=sub$OR_LB,xmax=sub$OR_UB)\nggplot(sub,aes(x=OR,y=neglogp,color=trait,shape=subset)) + theme_bw() + geom_point(alpha=0.7) + facet_wrap(model~sample) +\n  labs(title=""Dominant and Recessive Models for rs58542926"",y=expression(-log[10](pvalue)),x=""Odds Ratio"") + geom_vline(color=""black"",xintercept=1,alpha=0.5) + \n  scale_color_manual(values=c(""grey"",""goldenrod3"",""dark blue"")) + geom_errorbarh(limits,height=0.3,alpha=0.7) + theme(plot.title=element_text(hjust=0.5)) + \n  geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(0.05/6)) + coord_cartesian(xlim=c(0.5,5))\ndev.off()\n\nsub_to_write<-sub[c(6,7,8,12,11,15,25,28,20,21,18)]\nwrite.table(format(sub_to_write,digits=3),file=""dom_rec_OR.csv"",sep="","",quote=FALSE,row.names=FALSE,col.names=TRUE)\n\n\nsub<-df[df$model==""additive"",]\npdf(file=""additive_OR.pdf"",height=4,width=6)\nlimits<-aes(xmin=sub$OR_LB,xmax=sub$OR_UB)\nggplot(sub,aes(x=OR,y=neglogp,color=trait,shape=subset)) + theme_bw() + geom_point(alpha=0.7) + facet_wrap(model~sample) +\n  labs(title=""Additive association analysis for rs58542926"",y=expression(-log[10](pvalue)),x=""Odds Ratio"") + geom_vline(color=""black"",xintercept=1,alpha=0.5) + \n  scale_color_manual(values=c(""grey"",""goldenrod3"",""dark blue"")) + geom_errorbarh(limits,height=0.3,alpha=0.7) + theme(plot.title=element_text(hjust=0.5)) + \n  geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(0.05/6))\ndev.off()\n\n\n\n#write outptut\nhunt<-df[df$sample==""HUNT"" & df$model==""additive"",c(6,7,8,12,11,15,20,25,26,27,21)]\nwrite.table(format(hunt,digits=3),file=""HUNT_OR.csv"",sep="","",quote=FALSE,row.names=FALSE,col.names=TRUE)\nukbb<-df[df$sample==""UKBB"" & df$model==""additive"",c(6,7,8,12,11,15,20,25,26,27,21)]\nwrite.table(format(ukbb,digits=3),file=""UKBB_OR.csv"",sep="","",quote=FALSE,row.names=FALSE,col.names=TRUE)\n\n##### add MI \n\nMI<-fread(""MI_output.tab"")\nMI$subset<-""NA""\nMI[grepl(""ALL"",MI$study),]$subset<-""ALL""\nMI[grepl(""MALE"",MI$study),]$subset<-""MALE""\nMI[grepl(""FEMALE"",MI$study),]$subset<-""FEMALE""\nMI$ub<-MI$BETA + 1.96*MI$SE\nMI$lb<-MI$BETA - 1.96*MI$SE\nMI$neglogp<- -log10(MI$p.value)\nMI$OR<-exp(MI$BETA)\nMI$OR_LB<-exp(MI$lb)\nMI$OR_UB<-exp(MI$ub)\nMI$CI<-paste(sep=""-"",as.character(format(MI$OR_LB,digits=2)),trimws(as.character(format(MI$OR_UB,digits=2))))\n\ndf2<-rbind(df,MI)\n################ plot METAL meta-analysis results ####################\n\n\nmeta<-read.table(file=""meta_analysis_output.txt"",header=TRUE)\nmeta$study<- recode(meta$study,""MI_ALL""=""MI.ALL"",""MI_FEMALE""=""MI.FEMALE"",""MI_MALE""=""MI.MALE"")\nmeta$trait<-as.factor(sapply(strsplit(as.character(meta$study),""."",fixed=TRUE), \'[\', 1))\nmeta$sample<-as.factor(sapply(strsplit(as.character(meta$study),""."",fixed=TRUE), \'[\', 2))\nlevels(meta$sample)<-c(""ALL"",""FEMALE"",""MALE"")\nmeta$neglogp<- -log10(meta$P.value)\n\npdf(file=""gwas_metal_results_z.pdf"",height=4,width=8,useDingbats=FALSE)\nggplot(meta,aes(x=Zscore,y=neglogp,color=trait,shape=sample)) + geom_point(size=4) + theme_bw() + geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(0.05/6)) + \n  labs(title=""Meta-analysis of sex-stratified GWAS for rs58542926"",y=expression(-log[10](pvalue)),x=""Z-score"") + geom_vline(color=""black"",xintercept=0,alpha=0.5) + \n  scale_color_manual(values=c(""grey"",""goldenrod3"",""dark blue"")) + facet_wrap(~trait) + theme(plot.title=element_text(hjust=0.5))\ndev.off()\n\n\nz_to_b<-data.frame(""beta""=0,""se""=0,""sample""=""test"",""trait""=""test"",stringsAsFactors = F)\ndf2$weight<-1/df2$SE^2\ndf2$bw<-df2$BETA*df2$weight\nfor (i in unique(df2$trait)){\n  for (k in unique(df2$subset)){\n    sub<-df2[df2$trait==i & df2$model==""additive"" & df2$subset==k,]\n    bwsum<-sum(sub$bw)\n    wsum<-sum(sub$weight)\n    se<-sqrt(1/sum(sub$weight))\n    beta<-bwsum/wsum\n    z_to_b<-rbind(z_to_b,c(beta,se,k,i))\n  }}\n\nz_to_b<-z_to_b[-1,]\nmeta2<-merge(z_to_b,meta)\nmeta2$beta<-as.numeric(meta2$beta)\nmeta2$se<-as.numeric(meta2$se)\nmeta2$ub<-meta2$beta+1.96*meta2$se\nmeta2$lb<-meta2$beta-1.96*meta2$se\n\npdf(file=""gwas_metal_results_beta.pdf"",height=4,width=8,useDingbats=FALSE)\nlimits<-aes(xmin=meta2$lb,xmax=meta2$ub)\nggplot(meta2,aes(x=beta,y=neglogp,color=trait,shape=sample)) + geom_point(size=4) + theme_bw() + geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(0.05/6)) + \n  labs(title=""Meta-analysis of sex-stratified GWAS for rs58542926"",y=expression(-log[10](pvalue)),x=expression(italic(hat(beta)))) + geom_vline(color=""black"",xintercept=0,alpha=0.5) + \n  scale_color_manual(values=c(""grey"",""goldenrod3"",""dark blue"")) + facet_wrap(~trait) + theme(plot.title=element_text(hjust=0.5)) +\n  geom_errorbarh(limits,height=0.3,alpha=0.7)\ndev.off()\n\nmeta2$OR<-exp(meta2$beta)\nmeta2$OR_UB<-exp(meta2$ub)\nmeta2$OR_LB<-exp(meta2$lb)\n\nnames(meta2)[1]<-""Sample""\nnames(meta2)[2]<-""Trait""\n\npdf(file=""gwas_metal_results_OR.pdf"",height=4,width=8,useDingbats=FALSE)\nlimits<-aes(xmin=meta2$OR_LB,xmax=meta2$OR_UB)\nggplot(meta2,aes(x=OR,y=neglogp,color=Trait,shape=Sample)) + geom_point(size=3) + theme_bw() + geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(0.05/6)) + \n  labs(title=""Meta-analysis of sex-stratified association test for rs58542926"",y=expression(-log[10](pvalue)),x=""Odds Ratio"") + geom_vline(color=""black"",xintercept=1,alpha=0.5) + \n  scale_color_manual(values=c(""grey"",""goldenrod3"",""dark blue"")) + facet_wrap(~Trait) +\n  theme(plot.title=element_text(hjust=0.5,size=15),legend.text=element_text(size=12),axis.text=element_text(size=12),axis.title.y = element_text(size=15),axis.title.x = element_text(size=15)) +\n  geom_errorbarh(limits,height=0.3,alpha=0.7)\ndev.off()\n\nmt2d<-meta2[meta2$Trait==""T2D"",]\nlimits<-aes(xmin=mt2d$OR_LB,xmax=mt2d$OR_UB)\npdf(file=""gwas_meta_results_OR_T2D.pdf"",height=4,width=5,useDingbats=FALSE)\nggplot(mt2d,aes(x=OR,y=neglogp,shape=Sample)) + geom_point(size=4,color=""dark blue"") + theme_bw() + geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(0.05/6)) + \n  labs(title=""Meta-analysis of sex-stratified association test for rs58542926"",y=expression(-log[10](pvalue)),x=""Odds Ratio"") + geom_vline(color=""black"",xintercept=1,alpha=0.5) + \n  theme(plot.title=element_text(hjust=0.5,size=10),legend.text=element_text(size=12),axis.text=element_text(size=12),axis.title.y = element_text(size=15),axis.title.x = element_text(size=15)) +\n  geom_errorbarh(limits,height=0.3,alpha=0.7,color=""dark blue"")\ndev.off()\n\n\n#write out subset table\nmeta2_sub<-meta2[c(1,2,20,21,22,9,10,11,12,13,15)]\n\nwrite.table(format(meta2_sub,digits=3),file=""meta_analysis_OR.csv"",sep="","",quote=FALSE,row.names=FALSE,col.names=TRUE)\n\n################## diagram + ukbb + hunt meta-analysis for revision ######### (bad idea, DIAGRAM has UKBB)\nmeta<-list(c(""t"",""c"",353333,-3.395,0.0006871, ""---"",53.6,4.312,2,0.1158,""Female""),\n            c(""t"",""c"",347301,-7.529,5.11E-14,""-+-"",84.5,12.876,2,0.0016,""Male""))\nmeta_df<-do.call(rbind.data.frame,meta)\nnames(meta_df)<-c(""Allele1"",""Allele2"",""Weight "",""Zscore"",""Pvalue"",""Direction"",""HetISq"",""HetChiSq"",""HetDf"",""HetPVal"",""study"")\nhunt_ukbb<-df[df$trait==""T2D"" & df$model==""additive"" & (df$subset==""MALE"" | df$subset==""FEMALE""),] #need SE and BETA from the individual studies\ndiagram<-fread(file=""rs58542926_sex_specific_T2D.0310208AMah.txt"")\ndiagram$subset<-c(""FEMALE"",""MALE"")\nss<-rbind(hunt_ukbb,diagram,fill=TRUE) #substudy data\n\nz_to_b<-data.frame(""beta""=0,""se""=0,""sample""=""test"",stringsAsFactors = F)\nss$weight<-1/ss$SE^2\nss$bw<-ss$BETA*ss$weight\nfor (k in unique(ss$subset)){\n  sub<-ss[ss$subset==k,]\n  bwsum<-sum(sub$bw)\n  wsum<-sum(sub$weight)\n  se<-sqrt(1/sum(sub$weight))\n  beta<-bwsum/wsum\n  z_to_b<-rbind(z_to_b,c(beta,se,k))\n}\nz_to_b<-z_to_b[-1,]\nz_to_b$ub<-as.numeric(z_to_b$beta)+1.96*as.numeric(z_to_b$se)\nz_to_b$lb<-as.numeric(z_to_b$beta)-1.96*as.numeric(z_to_b$se)\nz_to_b$OR<-exp(as.numeric(z_to_b$beta))\nz_to_b$OR_UB<-exp(z_to_b$ub)\nz_to_b$OR_LB<-exp(z_to_b$lb)\n\n##difference in effect size for ukbb+hunt+diagram T2D\nmale_beta<-as.numeric(z_to_b[z_to_b$sample==""MALE"",]$beta)\nfemale_beta<-as.numeric(z_to_b[z_to_b$sample==""FEMALE"",]$beta)\nmale_se<-as.numeric(z_to_b[z_to_b$sample==""MALE"",]$se)\nfemale_se<-as.numeric(z_to_b[z_to_b$sample==""FEMALE"",]$se)\nzdiff<-(female_beta-male_beta)/sqrt((female_se^2)+(male_se^2)) ### Ida did male-female\nzdiff<-(male_beta-female_beta)/sqrt((male_se^2)+(female_se^2)) ### Ida did male-female\npval1<-2*pnorm(-abs(zdiff))  #two sided\npval2<-pnorm(zdiff) #lower\npval3<-pnorm(-zdiff) #higher\nprint(c(""T2D DIAGRAM+UKBB_HUNT"",pval1,pval2,pval3))\n\n##difference in effect size for diagram\nmale_beta<-diagram[diagram$subset==""MALE"",]$BETA\nfemale_beta<-diagram[diagram$subset==""FEMALE"",]$BETA\nmale_se<-diagram[diagram$subset==""MALE"",]$SE\nfemale_se<-diagram[diagram$subset==""FEMALE"",]$SE\nzdiff<-(female_beta-male_beta)/sqrt((female_se^2)+(male_se^2))\npval1<-2*pnorm(-abs(zdiff))  #two sided\npval2<-pnorm(zdiff) #lower\npval3<-pnorm(-zdiff) #higher\nprint(c(""T2D DIAGRAM"",pval1,pval2,pval3))\n\n\n##difference in effect size for ukbb+ hunt all traits\n\nfor (t in unique(meta2$Trait)){\n  male_beta<-meta2[meta2$Trait==t & meta2$Sample==""MALE"",]$beta\n  female_beta<-meta2[meta2$Trait==t & meta2$Sample==""FEMALE"",]$beta\n  male_se<-meta2[meta2$Trait==t & meta2$Sample==""MALE"",]$se\n  female_se<-meta2[meta2$Trait==t & meta2$Sample==""FEMALE"",]$se\n  zdiff<-(female_beta-male_beta)/sqrt((female_se^2)+(male_se^2))\n  pval1<-2*pnorm(-abs(zdiff))  #two sided\n  pval2<-pnorm(zdiff) #lower\n  pval3<-pnorm(-zdiff) #higher\n  print(c(t,pval1,pval2,pval3))\n}\n\n#################### forest plots for binary and quantitative traits and final Figure 1 ###########\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(LaCroixColoR)\nlibrary(dplyr)\n\nd<-read.table(""data_for_forest_plot.txt"",header=TRUE)\nd<-d[(d$Trait==""T2D"" & d$Study==""DIAGRAM"") | d$Trait==""HbA1c"" | d$Trait==""Blood_Glucose"" | d$Trait==""NAFLD"",]\n#d<-d[(d$Trait==""T2D"" & d$Study==""DIAGRAM"")| d$Trait==""BMI"" | d$Trait==""WHRadjBMI"" | d$Trait==""NAFLD"" | d$Trait==""IHD"",]\nd$neglog10<- -log10(d$P)\nq<-d[d$Type==""Q"",]\nq$LB<-q$Beta-1.96*q$SE\nq$UB<-q$Beta+1.96*q$SE\n\nb<-d[d$Type==""B"",]\nb$OR<-exp(b$Beta)\nb$UB<-exp(b$Beta+1.96*b$SE)\nb$LB<-exp(b$Beta-1.96*b$SE)\n\n\nb$Trait<-recode(b$Trait,  ""NAFLD""= ""Liver Disease"")\nq$Trait<-recode(q$Trait,""Blood_Glucose""=""Blood Glucose"")\n\npamp<-lacroix_palette(""Pamplemousse"", type = ""discrete"",n=6)\n\npdf(file=""forest.pdf"",height=5,width=8)\nx1<-ggplot(q,aes(x=Beta,y=Trait,shape=Sample,color=Sample)) + geom_point() + geom_errorbarh(aes(xmin=UB,xmax=LB),height=0.3)  + \n  labs(title=""Quantitative Traits"",x=""Effect Size"") + theme_bw() + geom_vline(xintercept=0,linetype=""dashed"",color=""red"") +\n  scale_color_manual(values=c(pamp[2],pamp[3],pamp[5])) + theme(legend.position = ""none"")\nx2<-ggplot(b,aes(x=OR,y=Trait,shape=Sample,color=Sample)) + geom_point() + geom_errorbarh(aes(xmin=UB,xmax=LB),height=0.3) + \n  labs(title=""Binary Traits"",x=""Odds Ratio"") + theme_bw()  + geom_vline(xintercept=1,linetype=""dashed"",color=""red"") +\n  scale_color_manual(values=c(pamp[2],pamp[3],pamp[5])) + guides(fill=guide_legend(title=""Sex""))\nx1+x2\ndev.off()\n\n\npdf(file=""Figure1.pdf"",height=5,width=10,useDingbats=FALSE)\nx1<-ggplot(q,aes(x=Beta,y=neglog10,shape=Sample,color=Sample)) + geom_point() + geom_errorbarh(aes(xmin=UB,xmax=LB),height=0.3)  + \n  labs(title=""Quantitative Traits"",x=""Effect Size"",y=""-log10(p-value)"") + theme_bw() + geom_vline(xintercept=0,linetype=""dashed"",color=""grey"") +\n  scale_color_manual(values=c(pamp[2],pamp[3],pamp[5])) + theme(legend.position = ""none"",strip.background=element_rect(fill=""white"")) + facet_wrap(~Trait,scales=""free_y"") +\n  geom_hline(yintercept=(-log10(0.05/18)),linetype=""dashed"",color=""red"",alpha=0.5)\nx2<-ggplot(b,aes(x=OR,y=neglog10,shape=Sample,color=Sample)) + geom_point() + geom_errorbarh(aes(xmin=UB,xmax=LB),height=0.3) + \n  labs(title=""Binary Traits"",x=""Odds Ratio"",y=""-log10(p-value)"") + theme_bw()  + geom_vline(xintercept=1,linetype=""dashed"",color=""grey"") +\n  scale_color_manual(values=c(pamp[2],pamp[3],pamp[5]))  + facet_wrap(~Trait,scales=""free_y"")+\n  theme(strip.background=element_rect(fill=""white""),legend.title=element_text(""Sample"")) + geom_hline(yintercept=(-log10(0.05/18)),linetype=""dashed"",color=""red"",alpha=0.5)\nx1+x2 +  plot_layout(widths = c(2,3))\ndev.off()\n\npdf(file=""Figure1_fixedy.pdf"",height=5,width=10,useDingbats=FALSE)\nx1<-ggplot(q,aes(x=Beta,y=neglog10,shape=Sample,color=Sample)) + geom_point() + geom_errorbarh(aes(xmin=UB,xmax=LB),height=0.3)  + \n  labs(title=""Quantitative Traits"",x=""Effect Size"",y=""-log10(p-value)"") + theme_bw() + geom_vline(xintercept=0,linetype=""dashed"",color=""grey"") +\n  scale_color_manual(values=c(pamp[2],pamp[3],pamp[5])) + theme(legend.position = ""none"",strip.background=element_rect(fill=""white"")) + facet_wrap(~Trait) +\n  geom_hline(yintercept=(-log10(0.05/18)),linetype=""dashed"",color=""red"",alpha=0.5)\nx2<-ggplot(b,aes(x=OR,y=neglog10,shape=Sample,color=Sample)) + geom_point() + geom_errorbarh(aes(xmin=UB,xmax=LB),height=0.3) + \n  labs(title=""Binary Traits"",x=""Odds Ratio"",y=""-log10(p-value)"") + theme_bw()  + geom_vline(xintercept=1,linetype=""dashed"",color=""grey"") +\n  scale_color_manual(values=c(pamp[2],pamp[3],pamp[5]))  + facet_wrap(~Trait) +\n  theme(strip.background=element_rect(fill=""white""),legend.title=element_text(""Sample"")) + geom_hline(yintercept=(-log10(0.05/18)),linetype=""dashed"",color=""red"",alpha=0.5)\nx1+x2 +  plot_layout(widths = c(2,3))\ndev.off()\n\n\n###version 2\nlibrary(RColorBrewer)\n\nq<-q[q$Trait==""BMI"",]\npdf(file=""Figure1_fixedy_v2.pdf"",height=5,width=10,useDingbats=FALSE)\nx1<-ggplot(q,aes(x=Beta,y=neglog10,shape=Sample,color=Sample)) + geom_point() + geom_errorbarh(aes(xmin=UB,xmax=LB),height=0.3)  + \n  labs(title=""Quantitative Traits"",x=""Effect Size"",y=""-log10(p-value)"") + theme_bw() + geom_vline(xintercept=0,linetype=""dashed"",color=""grey"") +\n  scale_color_manual(values=brewer.pal(3,""Set2"")) + theme(legend.position = ""none"",strip.background=element_rect(fill=""white""),strip.text = element_text(size=14),axis.title=element_text(size=14),axis.text=element_text(size=12)) + facet_wrap(~Trait) +\n  geom_hline(yintercept=(-log10(0.05/18)),linetype=""dashed"",color=""red"",alpha=0.5)\nx2<-ggplot(b,aes(x=OR,y=neglog10,shape=Sample,color=Sample)) + geom_point() + geom_errorbarh(aes(xmin=UB,xmax=LB),height=0.3) + \n  labs(title=""Binary Traits"",x=""Odds Ratio"",y=""-log10(p-value)"") + theme_bw()  + geom_vline(xintercept=1,linetype=""dashed"",color=""grey"") +\n  scale_color_manual(values=brewer.pal(3,""Set2""))  + facet_wrap(~Trait) +\n  theme(strip.background=element_rect(fill=""white""),axis.text=element_text(size=12),axis.title=element_text(size=14),strip.text = element_text(size=14),legend.text=element_text(size=12)) + geom_hline(yintercept=(-log10(0.05/18)),linetype=""dashed"",color=""red"",alpha=0.5)\nx1+x2 +  plot_layout(widths = c(1,3))\ndev.off()\n\n\n#### version 3\npdf(file=""Figure1_fixedy_v3.pdf"",height=5,width=10,useDingbats=FALSE)\nx2<-ggplot(q,aes(x=Beta,y=neglog10,shape=Sample,color=Sample)) + geom_point() + geom_errorbarh(aes(xmin=UB,xmax=LB),height=0.3)  + \n  labs(title=""Quantitative Traits"",x=""Effect Size"",y=""-log10(p-value)"") + theme_bw() + geom_vline(xintercept=0,linetype=""dashed"",color=""grey"") +\n  scale_color_manual(values=c(""black"",""red"",""blue"")) +facet_wrap(~Trait) + \n  geom_hline(yintercept=(-log10(0.05/18)),linetype=""dashed"",color=""red"",alpha=0.5) +\n  theme(strip.background=element_rect(fill=""white""),axis.text=element_text(size=12),axis.title=element_text(size=14),strip.text = element_text(size=14),legend.text=element_text(size=12))\nx1<-ggplot(b,aes(x=OR,y=neglog10,shape=Sample,color=Sample)) + geom_point() + geom_errorbarh(aes(xmin=UB,xmax=LB),height=0.3) +\n  labs(title=""Binary Traits"",x=""Odds Ratio"",y=""-log10(p-value)"") + theme_bw()  + geom_vline(xintercept=1,linetype=""dashed"",color=""grey"") +\n  scale_color_manual(values=c(""black"",""red"",""blue"")) + facet_wrap(~Trait) + \n  theme(legend.position = ""none"",strip.background=element_rect(fill=""white""),strip.text = element_text(size=14),axis.title=element_text(size=14),axis.text=element_text(size=12)) +\n  geom_hline(yintercept=(-log10(0.05/18)),linetype=""dashed"",color=""red"",alpha=0.5)\nx1+x2 +  plot_layout(widths = c(2,2))\ndev.off()\n\n\n\n################## age stratified analysi ####################\nlibrary(data.table)\n\n\nage<-read.table(sep=""\\t"",file=""age_analysis.dat"",header=T)\nage$sample<-NA\nage$trait<-NA\nage[grep(""gt55"",age$study,invert=TRUE),]$sample<-""<55""\nage[grep(""gt55"",age$study,invert=FALSE),]$sample<-"">=55""\nage[grep(""250"",age$study,invert=FALSE),]$trait<-""T2D""\nage[grep(""571"",age$study,invert=FALSE),]$trait<-""liver""\nage[grep(""411"",age$study,invert=FALSE),]$trait<-""IHD""\n\nage$neglogp<- -log10(age$p.value)\nage$ub<-age$BETA+1.96*age$SE\nage$lb<-age$BETA-1.96*age$SE\n\npdf(file=""ukbb_age.pdf"",height=4,width=8)\nlimits<-aes(xmin=age$lb,xmax=age$ub)\nggplot(age,aes(x=BETA,y=neglogp,color=trait,shape=sample)) + geom_point(size=4) + theme_bw() + geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(0.05/6)) + \n  labs(title=""Age-stratified analysis in UKBB Females"",y=expression(-log[10](pvalue)),x=expression(italic(hat(beta)))) + geom_vline(color=""black"",xintercept=0,alpha=0.5) + \n  scale_color_manual(values=c(""grey"",""goldenrod3"",""dark blue"")) + facet_wrap(~trait) + theme(plot.title=element_text(hjust=0.5)) +\n  geom_errorbarh(limits,height=0.3,alpha=0.7)\ndev.off()\n\n\nfemale<-df[df$model==""dominant"" & df$sample==""UKBB"" & df$subset==""FEMALE"",c(9,10,21,24,22,23,20)]\nnames(female)[3]<-""sample""\nage2<-rbind(age[,c(9,10,18,20,21,22,19)],female)\nage2$sample<-factor(age2$sample)\nlevels(age2$sample)[3]<-""All""\n\npdf(file=""ukbb_age_with_combined.pdf"",height=4,width=8)\nlimits<-aes(xmin=age2$lb,xmax=age2$ub)\nggplot(age2,aes(x=BETA,y=neglogp,color=trait,shape=sample)) + geom_point(size=4) + theme_bw() + geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(0.05/6)) + \n  labs(title=""Age-stratified analysis in UKBB Females"",y=expression(-log[10](pvalue)),x=expression(italic(hat(beta)))) + geom_vline(color=""black"",xintercept=0,alpha=0.5) + \n  scale_color_manual(values=c(""grey"",""goldenrod3"",""dark blue"")) + facet_wrap(~trait) + theme(plot.title=element_text(hjust=0.5)) +\n  geom_errorbarh(limits,height=0.3,alpha=0.7)\ndev.off()\n\n\n####### consortia\n\nbeta<-c(0.0579, 0.113, -0.0625 ,-0.1527)\nse<-c(0.0183, 0.016, 0.005,0.005)\npval<-c(0.001557, 1.79E-12, 1.75065e-36, 3.5491e-174)\nsubset<-c(""Female"",""Male"",""Female"",""Male"")\ntrait<-c(""T2D"",""T2D"",""LDL"",""LDL"")\n\ncon<-data.frame(beta,se,pval,subset,trait)\ncon$OR<-exp(con$beta)\ncon$ub<-exp(con$beta+1.96*con$se)\ncon$lb<-exp(con$beta-1.96*con$se)\ncon$ub<-con$beta+1.96*con$se\ncon$lb<-con$beta-1.96*con$se\ncon$neglogp<- -log10(pval)\n\npdf(file=""consortia.pdf"",height=3,width=4)\nlimits<-aes(xmin=con$lb,xmax=con$ub)\nggplot(con,aes(x=beta,y=neglogp,color=trait,shape=subset)) + geom_point(size=3) + theme_bw() + geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=5e-8) + \n  scale_color_manual(values=c(""grey"",""dark blue"")) +  labs(title=""Sex-specific effects in consortia"",y=expression(-log[10](pvalue)),x=expression(italic(hat(beta)))) + \n  geom_vline(color=""black"",xintercept=0,alpha=0.5)  +  geom_errorbarh(limits,height=0.3,alpha=0.7)\ndev.off()\n\n\n################## HUNT and UKBB results ############\n\ndf<-read.table(file=""2018_12_5_results.tab"")\nnames(df)<-c(""AC_Allele2"",\n             ""AF_Allele2"",\n             ""N"",\n             ""BETA"",\n             ""SE"",\n             ""Tstat"",\n             ""Pvalue"",\n             ""Pheno"",\n             ""Sample"",\n             ""Study""\n)\n\n\ndf$Sample<-sapply(strsplit(as.character(df$Sample),""."",fixed=TRUE), \'[\', 1)\ndf[df$Pheno==""X401.22"",]<-NA #move kidney phenotype\ndf<- df[complete.cases(df),]\ndf[df$Pheno==""X571"",]<-NA #remove hronic liver disease and cirrhosis (keeping nonalcoholic liver disease pheno)\ndf<- df[complete.cases(df),]\n\n#make phenotype labels consistent across studies\ndf$Pheno_pretty<-NA\ndf[df$Pheno==""X250.2""| df$Pheno==""T2D.icd"",]$Pheno_pretty<-""T2D""\ndf[df$Pheno==""X571.5""| df$Pheno==""liver"",]$Pheno_pretty<-""Liver_disease""\ndf[df$Pheno==""X411""| df$Pheno==""IHD.icd"",]$Pheno_pretty<-""IHD""\n\ndf$neglog10p<- -log10(df$Pvalue)\n\ndf$ub<-df$BETA + 1.96*df$SE\ndf$lb<-df$BETA - 1.96*df$SE\n\npdf(file=""gwas_comparison.pdf"",height=4,width=6)\nlimits<-aes(xmin=df$lb,xmax=df$ub)\nggplot(df,aes(x=BETA,y=neglog10p,color=Pheno_pretty,shape=Sample)) + facet_wrap(~Study) + geom_point() + theme_bw() + geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(5e-8)) + \n  labs(title=""Sex-stratified GWAS for rs58542926"",y=expression(-log[10](pvalue)),x=expression(italic(hat(beta)))) + geom_errorbarh(limits,height=0.3,alpha=0.3) + geom_vline(color=""black"",xintercept=0,alpha=0.5) + \n  scale_color_manual(values=c(""grey"",""goldenrod3"",""dark blue""))\ndev.off()\n\n\n#not working but could be cool\ndf[df$Sample==""MALE"",]$label<-""\\u2642""\ndf[df$Sample==""FEMALE"",]$label<-""\\u2640""\ndf[df$Sample==""ALL"",]$label<-""+""\n\npdf(file=""gwas_comparison_symbols.pdf"",height=4,width=6)\nlimits<-aes(xmin=df$lb,xmax=df$ub)\nggplot(df,aes(x=BETA,y=neglog10p,color=Pheno_pretty,shape=Sample,label=label)) + facet_wrap(~Study) + geom_point(alpha=0) + theme_bw() + geom_hline(linetype=""dashed"",alpha=0.5,color=""red"",yintercept=-log10(5e-8)) + \n  labs(title=""Sex-stratified GWAS for rs58542926"",y=expression(-log[10](pvalue)),x=expression(italic(hat(beta)))) + geom_errorbarh(limits,height=0.3,alpha=0.3) + geom_vline(color=""black"",xintercept=0,alpha=0.5) + geom_text()\ndev.off()\n  \n\n\n']","Code for data visualization and statistical analysis Original code in R used to process genetic association test results generated from SAIGE and METAL. The code is not generalized, it reads in specific summary statistics results files which are available upon request. The code creates data visualizations, figures for a manuscript, and tests for differences in effect sizes.",3
ZIRFs: zero-inflated random forests for estimating gene regulatory networks from single cell RNA-seq data (assessment of predictive accuracy and VIM stability),We developed a zero-inflated random forests (ZIRFs) algorithm to produce a metric of connection strength between regulator genes and target genes. This file contains SCENIC results for the aorta and diaphragm tissue data sets from the Tabula Muris Consortium results. SCENIC is a genetic regulatory network analysis published by Aibar et al. (2017). The purpose of the data sets and R source code are described by README files in each directory.,,ZIRFs: zero-inflated random forests for estimating gene regulatory networks from single cell RNA-seq data (assessment of predictive accuracy and VIM stability) We developed a zero-inflated random forests (ZIRFs) algorithm to produce a metric of connection strength between regulator genes and target genes. This file contains SCENIC results for the aorta and diaphragm tissue data sets from the Tabula Muris Consortium results. SCENIC is a genetic regulatory network analysis published by Aibar et al. (2017). The purpose of the data sets and R source code are described by README files in each directory.,3
Data from: Public sharing of research datasets: a pilot study of associations,"The public sharing of primary research datasets potentially benefits the research community but is not yet common practice. In this pilot study, we analyzed whether data sharing frequency was associated with funder and publisher requirements, journal impact factor, or investigator experience and impact. Across 397 recent biomedical microarray studies, we found investigators were more likely to publicly share their raw dataset when their study was published in a high-impact journal and when the first or last authors had high levels of career experience and impact. We estimate the USA's National Institutes of Health (NIH) data sharing policy applied to 19% of the studies in our cohort; being subject to the NIH data sharing plan requirement was not found to correlate with increased data sharing behavior in multivariate logistic regression analysis. Studies published in journals that required a database submission accession number as a condition of publication were more likely to share their data, but this trend was not statistically significant. These early results will inform our ongoing larger analysis, and hopefully contribute to the development of more effective data sharing initiatives. **Earlier version presented at ASIS&T and ISSI Pre-Conference: Symposium on Informetrics and Scientometrics 2009**","['#install.packages(\'Hmisc\')\n#install.packages(\'Design\')\n#install.packages(\'tree\')\n#install.packages(\'tree\')\n\nlibrary(Hmisc,T)\nlibrary(Design,T)\nlibrary(tree)\n\n#### READ DATA\nsetwd(""."")\ndat.raw = read.csv(""Piwowar_Metrics2009_rawdata.csv"", header=TRUE, sep="","")\ndim(dat.raw)\nnames(dat.raw) = gsub(""_"", ""."", names(dat.raw))\nnames(dat.raw)\n\n\n### SET UP DERIVED VARIABLES\nfirst.career.length = 2008-dat.raw[""first.first.year""]\nnames(first.career.length) = c(""first.career.length"")\nlast.career.length = 2008-dat.raw[""last.first.year""]\nnames(last.career.length) = c(""last.career.length"")\n\ndat.orig = cbind(dat.raw, first.career.length, last.career.length)\ndat.orig$policy.strength = ordered(dat.orig$policy.strength)\n\n\n#### Compute principal components of author experience\n### FIRST AUTHOR\npc.first = princomp(scale( cbind(log(1+dat.orig$first.hindex), \n\t\t\t\t\t\t\t\tlog(1+dat.orig$first.aindex), \n\t\t\t\t\t\t\t\tdat.orig$first.career.length)))\nsummary(pc.first)\n# NOTE THE NEGATIVE!  This is so that\n# higher hindexes etc correlate with higher author experience scores\nfirst.author.exp = - pc.first$scores[,1]  \npc.first$loadings\n#biplot(pc.first)\nfirst.author.exp.freq = ecdf(first.author.exp)(first.author.exp)\n\n### LAST AUTHOR\npc.last = princomp(scale( cbind(log(1+dat.orig$last.hindex), \n\t\t\t\t\t\t\t\tlog(1+dat.orig$last.aindex), \n\t\t\t\t\t\t\t\tdat.orig$last.career.length)))\nsummary(pc.last)\n# NOTE THE NEGATIVE!  This is so that\n# higher hindexes etc correlate with higher author experience scores\nlast.author.exp = - pc.last$scores[,1]\npc.last$loadings\n#biplot(pc.last)\nlast.author.exp.freq = ecdf(last.author.exp)(last.author.exp)\n\n#### OVERVIEW TABLE\n\nv.orig = names(dat.orig)[3:length(dat.orig)]\nv = c(""is.data.shared"", ""policy.strength"", ""impact.factor"", ""is.usa.address"", \n""is.nih.funded"", ""any.nih.data.sharing.applies"", ""sum.of.max.award.for.each.grant"", \n""any.direct.cost.over.500k"", ""any.new.or.renewed.since.2003"", ""num.authors"",\n""first.author.exp"", ""last.author.exp"", \n""first.career.length"", ""last.career.length""\n)\ndat.extra = cbind(dat.orig, first.author.exp, last.author.exp)\ndat = dat.extra[,v]\t\t\t\t\t\t\ndd = datadist(dat)\noptions(datadist=\'dd\')\noptions(digits=2)\n\ndescribe(dat.orig[,v.orig], listunique=0) \nsummary(dat.orig[,v.orig]) \n\t\n##### FIGURE 1  (mean line was added manually later)\n\ncuteq = function(X, n) cut(X,quantile(X,(0:n)/n),include.lowest=TRUE) \n\t\ndots = NULL\ndots$is.data.shared = dat.orig$is.data.shared\ndots$impact.factor = cut(dat.orig$impact.factor, c(0,6,8,15,50)) \ndots$journal.policy.strength = dat.orig$policy.strength\ndots$num.authors = cuteq(dat.orig$num.authors,3)\ndots$FIRST.author.hindex = cuteq(dat.orig$first.hindex,3) \ndots$FIRST.author.aindex = cuteq(dat.orig$first.aindex,3) \ndots$FIRST.author.career.length = cuteq(dat.orig$first.career.length,3) \ndots$LAST.author.hindex = cuteq(dat.orig$last.hindex,3) \ndots$LAST.author.aindex = cuteq(dat.orig$last.aindex,3)\ndots$LAST.author.career.length = cuteq(dat.orig$last.career.length,3) \ndots$is.usa.address = dat.orig$is.usa.address\ndots$is.nih.funded = dat.orig$is.nih.funded\ndots$nih.funds = cut(dat.orig$sum.of.max.award.for.each.grant/1000, c(-1, 1, 750, 2000, 500000))\ndots$nih.requires.data.sharing.plan = dat.orig$any.nih.data.sharing.applies\ndots$is.nih.grant.number.missing = is.na(dots$nih.requires.data.sharing.plan)\n\ns = summary(is.data.shared ~ ., dat=dots)\ns\n\n\ntiff(""figure1_no_mean_line.tiff"", bg=""white"", width=880, height=1200)\nplot(s)\ntitle(""Proportion of studies with shared datasets"")\ndev.off()\n\n\n### DO IMPUTATION\n\ndo.imputation = function(column) {\n\tdat.orig$temp = na.include(column)\n\tmytree = tree(temp ~ policy.strength + num.authors + \n\t\tis.usa.address + \n\t\tis.nih.funded + \n\t\tfirst.hindex + first.aindex + first.career.length + \n\t\tfirst.num.papers + first.total.pmc.citations + \n\t\tlast.hindex + last.aindex + last.career.length + \n\t\tlast.num.papers + last.total.pmc.citations +\n\t\tlog(impact.factor),\n\t\tdat=dat.orig, \n\t\tcontrol=tree.control(nobs=502, mincut=20) \n\t\t)\n\tsummary(mytree)\n\tplot(mytree); text(mytree)\n\tresponse = impute(column, \n\t\t\tpredict(mytree, dat.orig)[is.na(column)])\n\tresponse = round(response)\n\tprint(response)\n\tresponse\n}\n\npar(mfrow=c(1,2))\ndat$any.nih.data.sharing.applies.imputed = \n\tdo.imputation(dat.orig$any.nih.data.sharing.applies)\ndat$sum.of.max.award.for.each.grant.imputed = \n\tdo.imputation(dat.orig$sum.of.max.award.for.each.grant)\n\n#######  MULTIVARIATE REGRESSION\n\npar(mfrow=c(1,1))\ndat.all = cbind(abs(dots$is.nih.grant.number.missing), dat.extra, dat[15:18])\n\ndat$log.award = log(1+sum.of.max.award.for.each.grant.imputed)\n\ndd = datadist(dat)\noptions(datadist=\'dd\')\noptions(digits=2)\n\t\nf = lrm(formula = is.data.shared ~ ordered(policy.strength) + \n\tis.usa.address*is.nih.funded + \n\tany.nih.data.sharing.applies.imputed + \n\trcs(first.author.exp,4) + rcs(last.author.exp, 4) +\n\trcs(log(1+sum.of.max.award.for.each.grant.imputed), 4) + \n\trcs(log(impact.factor), 4),\n\tdat=dat.all, x=T, y=T\n\t)\nanova(f)\nf\npar(mfrow=c(4,5))\nresid(f, \'partial\', pl=TRUE)\nresid(f, \'gof\')\n\n#### TABLE 1\nanova(f)\n\n\n#####  MODEL FOR ODDS RATIO ANALYSIS\n\nf2 = lrm(formula = is.data.shared ~ ordered(policy.strength) + \n\tis.usa.address + \n\tany.nih.data.sharing.applies.imputed + \n\trcs(first.author.exp,4) + rcs(last.author.exp, 4) +\n\trcs(log(1+sum.of.max.award.for.each.grant.imputed), 4) + \n\trcs(log(impact.factor), 4),\n\tdat=dat, x=T, y=T\n\t)\nf2\nanova(f2)\npar(mfrow=c(4,5))\nresid(f2, \'partial\', pl=TRUE)\nresid(f2, \'gof\')\n\n# The p-value is large (>.3 in that example) indicating no significant lack of fit.\n# from http://www.unc.edu/courses/2006spring/ecol/145/001/docs/solutions/final.htm\nresiduals.lrm(f2,type=\'gof\')\n\n\n####  FIGURE 2\nattach(dat)\n\ntiff(""figure2.tiff"", bg=""white"", width=880)\n\nsumm = summary(f2, \n\tsum.of.max.award.for.each.grant.imputed=c(1,750000),\n\tpolicy.strength=0,\n\timpact.factor=c(5,15), \n\tfirst.author.exp=quantile(first.author.exp, c(0.25,0.75)),\n\tlast.author.exp=quantile(last.author.exp, c(0.25,0.75))\n\t)\nsumm\nplot(summ, log=T)\n\ndev.off()\n\n### FIGURE 3\n\ntiff(""figure3.tiff"", bg=""white"", width=880)\n\ncutHML = function(X) {\n\tn=3\n    cuts = cut(X,quantile(X,(0:n)/n),include.lowest=TRUE, labels=FALSE)\n\tifelse(cuts==1, \'low\', ifelse(cuts==2, \'med\', \'high\'))\n\t}\n\npar(mfrow=c(1,2))  \ngroup = factor(cutHML(first.author.exp))\ngrouplty=NULL\ngrouplty[levels(group)] = c(1:3)\nlabels = c(""high"", ""med"", ""low"")\nplsmo(log(impact.factor), is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\txlim=c(1,4), ylim=c(0,1), label.curves=F,\n\tylab=""Probability of shared data"", xlab= ""Log of impact factor"")\nlegend(""topleft"", labels, lty=grouplty[labels], bty=""n"")\ntitle(""IF by First author experience"")\n\ngroup = factor(cutHML(last.author.exp))\ngrouplty=NULL\ngrouplty[levels(group)] = c(1:3)\nlabels = c(""high"", ""med"", ""low"")\nplsmo(log(impact.factor), is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\txlim=c(1,4), ylim=c(0,1), label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab=""Log of impact factor"")\nlegend(""topleft"", labels, lty=grouplty[labels], bty=""n"")\t\ntitle(""IF by Last author experience"")\t\n\ndev.off()\n\n\n#### FIGURE 4\n\ntiff(""figure4.tiff"", bg=""white"", width=880)\n\npar(mfrow=c(1,3))  \ngroup = policy.strength\nlevels(group) = c(""None"", ""Weak"", ""Strong"")  #### Hrm, not very robust!\ngrouplabels = c(""Strong"", ""Weak"", ""None"")\ngrouplty=NULL\ngrouplty[grouplabels] = c(3:1)\n\n\nplsmo(log(impact.factor), is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F,\n\txlim=c(1,4), ylim=c(0,1), trim=0, label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab= ""Log of impact factor"")\nlegend(""topleft"", grouplabels, lty=grouplty[levels(group)], bty=""n"")\t\ntitle(""IF by Journal policy strength"")\t\t\t\nplsmo(first.author.exp, is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\tylim=c(0,1), label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab= ""First author experience"")\nlegend(""topleft"", grouplabels, lty=grouplty[levels(group)], bty=""n"")\t\t\ntitle(""First author by Journal policy"")\t\t\t\t\nplsmo(last.author.exp, is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\tylim=c(0,1), label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab= ""Last author experience"")\nlegend(""topleft"", grouplabels, lty=grouplty[levels(group)], bty=""n"")\t\t\t\ntitle(""Last author by Journal policy"")\n\ndev.off()\n\n##### FIGURE 5\n\ntiff(""figure5.tiff"", bg=""white"", width=880)\n\npar(mfrow=c(1,3))  \ngroup = factor(any.nih.data.sharing.applies)\n#### Hrm, this isn\'t a very robust ordering of the legend.\n## It is accurate for this version of the data, but may not be accurate if the data are resorted.\n## What is a better way to do this?\nlevels(group) = c(""Not required"", ""NIH sharing plan required"")  \ngrouplabels = c(""NIH sharing plan required"", ""Not required"")\ngrouplty=NULL\ngrouplty[grouplabels] = c(2:1)\n\nplsmo(log(impact.factor), is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\txlim=c(1,4), ylim=c(0,1), trim=0, label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab= ""Log of impact factor"")\nlegend(""topleft"", grouplabels, lty=grouplty[levels(group)], bty=""n"")\t\t\t\ntitle(""IF by NIH policy"")\t\nplsmo(first.author.exp, is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\tylim=c(0,1), label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab= ""First author experience"")\nlegend(""topleft"", grouplabels, lty=grouplty[levels(group)], bty=""n"")\t\t\t\ntitle(""First author by NIH policy"")\t\nplsmo(last.author.exp, is.data.shared, group=group, lty=grouplty,\n\tdatadensity=F, \n\tylim=c(0,1), label.curves=F,\n\tylab = ""Probability of shared data"", \n\txlab= ""Last author experience"")\nlegend(""topleft"", grouplabels, lty=grouplty[levels(group)], bty=""n"")\t\t\t\ntitle(""Last author by NIH policy"")\t\n\ndev.off()\n']","Data from: Public sharing of research datasets: a pilot study of associations The public sharing of primary research datasets potentially benefits the research community but is not yet common practice. In this pilot study, we analyzed whether data sharing frequency was associated with funder and publisher requirements, journal impact factor, or investigator experience and impact. Across 397 recent biomedical microarray studies, we found investigators were more likely to publicly share their raw dataset when their study was published in a high-impact journal and when the first or last authors had high levels of career experience and impact. We estimate the USA's National Institutes of Health (NIH) data sharing policy applied to 19% of the studies in our cohort; being subject to the NIH data sharing plan requirement was not found to correlate with increased data sharing behavior in multivariate logistic regression analysis. Studies published in journals that required a database submission accession number as a condition of publication were more likely to share their data, but this trend was not statistically significant. These early results will inform our ongoing larger analysis, and hopefully contribute to the development of more effective data sharing initiatives. **Earlier version presented at ASIS&T and ISSI Pre-Conference: Symposium on Informetrics and Scientometrics 2009**",3
Data from: A protocol for conducting and presenting results of regression-type analyses,"Scientific investigation is of value only insofar as relevant results are obtained and communicated, a task that requires organizing, evaluating, analysing and unambiguously communicating the significance of data. In this context, working with ecological data, reflecting the complexities and interactions of the natural world, can be a challenge. Recent innovations for statistical analysis of multifaceted interrelated data make obtaining more accurate and meaningful results possible, but key decisions of the analyses to use, and which components to present in a scientific paper or report, may be overwhelming. We offer a 10-step protocol to streamline analysis of data that will enhance understanding of the data, the statistical models and the results, and optimize communication with the reader with respect to both the procedure and the outcomes. The protocol takes the investigator from study design and organization of data (formulating relevant questions, visualizing data collection, data exploration, identifying dependency), through conducting analysis (presenting, fitting and validating the model) and presenting output (numerically and visually), to extending the model via simulation. Each step includes procedures to clarify aspects of the data that affect statistical analysis, as well as guidelines for written presentation. Steps are illustrated with examples using data from the literature. Following this protocol will reduce the organization, analysis and presentation of what may be an overwhelming information avalanche into sequential and, more to the point, manageable, steps. It provides guidelines for selecting optimal statistical tools to assess data relevance and significance, for choosing aspects of the analysis to include in a published report and for clearly communicating information.","['# R code used in:\r\n# A protocol for conducting and presenting results of\r\n# regression-type analyses\r\n\r\n# Authors: Alain F. Zuur and Elena N. Ieno\r\n# Journal: Methods in Ecology and Evolution (2016)\r\n\r\n# Contact information:\r\n# Highland Statistics Ltd.\r\n# www.highstat.com\r\n# highstat@highstat.com\r\n#\r\n#\r\n# This R code is distributed under CC0 license: \r\n# https://creativecommons.org/publicdomain/zero/1.0/\r\n\r\n# The authors (Zuur, Ieno) and Highland Statistics Ltd.\r\n# make no warranties about the R code below, and disclaim \r\n# liability for all uses of the R code, to the fullest extent \r\n# permitted by applicable law. \r\n\r\n\r\n\r\n\r\n####################################################\r\n#Import the data\r\nOwls <- read.table(""Owls2.txt"", \r\n                   header = TRUE,\r\n                   dec = ""."")\r\n\r\n\r\n#Check imported data\r\nnames(Owls)\r\nstr(Owls)\r\n\r\n#This is what we have\r\n# [1] ""Nest""               ""Xcoord""             ""Ycoord""            \r\n# [4] ""FoodTreatment""      ""SexParent""          ""ArrivalTime""       \r\n# [7] ""SiblingNegotiation"" ""BroodSize""          ""NegPerChick""       \r\n# [10] ""NCalls""          \r\n\r\n# These data were taken from:\r\n# Roulin, A. &  Bersier, L. (2007) Nestling barn \r\n# owls beg more intensely in the presence of their \r\n# mother than in the presence of their father. \r\n# Animal Behaviour, 74, 10991106.\r\n\r\n# We also used the data in:\r\n# Zuur, A.F., Saveliev, A.A.& Ieno, E.N. (2012) \r\n# Zero Inflated Models and Generalized Linear \r\n# Mixed Models with R. Highland Statistics Ltd, Newburgh.\r\n\r\n# And in:\r\n# Zuur, A.F., Ieno, E.N., Walker, N., Saveliev, A.A. & Smith, G.M. (2009) \r\n# Mixed Effects Models and Extensions in Ecology. Springer, New York\r\n\r\n# In our 2009 book we modelled SiblingNegotiation  as a function of:\r\n#  -FoodTreatment    (factor)   \r\n#  -SexParent         (factor)\r\n#  -ArrivalTime       (continuous)\r\n#  -FoodTreatment x SexParent\r\n#  -SexParent x ArrivalTime\r\n#  -Nest effect (but nests were selected randomly from a large group)\r\n\r\n\r\n###################################################################\r\n#Load packages\r\nlibrary(lattice)  \r\nlibrary(ggplot2)\r\nlibrary(ggmap)\r\nlibrary(maptools)\r\nlibrary(sp)\r\nlibrary(rgdal)\r\n##################################################################\r\n\r\n\r\n#SiblingNegotiation is too long....use shorter name:\r\nOwls$NCalls <- Owls$SiblingNegotiation      \r\n\r\n\r\n\r\n##########################################\r\n\r\n\r\n\r\n\r\n\r\n\r\n############################################\r\n#Figure 2\r\n\r\n# ggmap works with/expects lat/lon coordinates in wgs84 projection\r\n# The Owls2.txt file contains Swiss coordinates, and\r\n# we need to convert them. The code below is not so\r\n# friendly on the eye, but it does the job: Convert\r\n# the coordinates from Swiss coordinates to WGS84\r\n\r\nOwls.spdf <- Owls \r\ncoordinates(Owls.spdf) <- ~Xcoord + Ycoord\r\nWGS84     <- CRS(""+proj=longlat +datum=WGS84"")\r\nprojSWISS <- CRS(""+init=epsg:21781"")\r\nproj4string(Owls.spdf) <- projSWISS\r\nOwls_wgs84.spdf <- spTransform(Owls.spdf, WGS84)\r\n\r\nkmlPoints(Owls_wgs84.spdf,""./Owls.kml"",name=Owls_wgs84.spdf$Nest)\r\n# You can open this one in Google Earth\r\n\r\n# And add the WGS84 coordinates to our Owls object\r\nOwls$Xcoord2 <- Owls_wgs84.spdf$Xcoord\r\nOwls$Ycoord2 <- Owls_wgs84.spdf$Ycoord\r\n\r\n\r\n# And use ggmap to make Figure 2\r\nglgmap   <- get_map(location = c(6.74, 46.71, 7.162, 46.94),\r\n                    maptype= ""terrain"",\r\n                    col = ""color"")    \r\np <- ggmap(glgmap)\r\np <- p + geom_point(aes(Xcoord2, Ycoord2),\r\n                    data = Owls,\r\n                    size = 4) \r\np <- p + xlab(""Longitude"") + ylab(""Latitude"")  \r\np <- p + theme(text = element_text(size = 15)) \r\np\r\n# END OF FIGURE 2 CODE\r\n##############################################\r\n\r\n\r\n\r\n\r\n##############################################\r\n# This graph is not in the paper,\r\n# but is useful during the analysis. \r\n# It shows the time series nature of the data.\r\np <- ggplot()\r\np <- p + xlab(""Arrival time"") + ylab(""Number of calls"")\r\np <- p + theme(text = element_text(size = 15))\r\np <- p + geom_point(data = Owls, \r\n                    aes(x = ArrivalTime, \r\n                            y = NCalls,\r\n                            color = FoodTreatment),\r\n                    size = 2)\r\np <- p + geom_line(data = Owls, \r\n                    aes(x = ArrivalTime, \r\n                        y = NCalls, \r\n                        group = FoodTreatment,\r\n                        color = FoodTreatment))\r\np <- p + facet_wrap( ~ Nest, ncol = 4)\r\np\r\n###############################################\r\n\r\n\r\n\r\n###############################################\r\n# This graph is not in the paper, but\r\n# is useful during the analysis.\r\nOwls$Date2 <- factor(Owls$Date, \r\n                  levels = c(\r\n""28/05/97"",  ""29/05/97"",\r\n""16/06/97"", ""17/06/97"", ""18/06/97"", ""19/06/97"", ""23/06/97"", ""24/06/97"", \r\n""25/06/97"", ""26/06/97"", ""27/06/97"", ""28/06/97"", ""29/06/97"", ""30/06/97"", \r\n""01/07/97"", ""02/07/97"", ""03/07/97"", ""04/07/97"", ""12/07/97"", ""13/07/97"", \r\n""14/07/97"", ""15/07/97"", ""21/07/97"", ""22/07/97"", ""23/07/97"", ""24/07/97"", \r\n""25/07/97"", ""26/07/97"", ""27/07/97"", ""09/08/97"", ""10/08/97"", ""11/08/97"", \r\n""17/08/97"",  ""18/08/97""))\r\n                  \r\n                  \r\nglgmap   <- get_map(location = c(6.74, 46.71, 7.162, 46.94),\r\n                     maptype= ""terrain"",\r\n                     col = ""bw"")    \r\np <- ggmap(glgmap)\r\np <- p + geom_point(aes(Xcoord2, Ycoord2, col = FoodTreatment),\r\n                    data = Owls) \r\np <- p + xlab(""Longitude"") + ylab(""Latitude"")  \r\np <- p + facet_wrap(~ Date2, ncol = 8)\r\np\r\n\r\n\r\n# Here is code to define DayInYear, which can also be useful.\r\nOwls$DayInYear <- strptime(Owls$Date, ""%d/%m/%Y"")$yday + 1\r\n###################################################\r\n\r\n\r\n\r\n\r\n\r\n####################################################\r\n#Figure 3\r\n\r\n#Import the data from a tab delimited ascii file\r\nOC <- read.table(file = ""OystercatcherData.txt"",\r\n                   header = TRUE,\r\n                   dec = ""."")\r\n\r\n#Inspect the file\r\nnames(OC)\r\nstr(OC)  # Make sure you have num and not factors for \r\n         # the numerical variables!\r\n\r\n\r\n# These data were used in:\r\n# Ieno, E.N. & Zuur, A.F. (2015) A Beginners \r\n# Guide to Data Exploration and Visualisation with R. \r\n# Highland Statistics, Newburgh. \r\n\r\n\r\n\r\n#################################################################\r\n#Underlying question: \r\n#         Investigate whether the ShellLength differs\r\n#         per feeding type, feeding plot and season.\r\n#         We may expect a 3-way interaction.\r\n\r\n\r\n# Here is the code for Figure 3\r\np <- ggplot()\r\np <- p + xlab(""Feeding type"") + ylab(""Shell length (mm)"")\r\np <- p + theme(text = element_text(size = 15)) \r\np <- p + geom_point(data = OC, \r\n                    aes(x = FeedingType, y = ShellLength),\r\n                    position = position_jitter(width = .03),\r\n                    color = grey(0.3),\r\n                    size = 2)\r\n       \r\np <- p + facet_grid(Month ~ FeedingPlot, \r\n                    scales = ""fixed"")\r\np <- p + theme(legend.position=""none"") \r\np <- p + theme(strip.text.y = element_text(size = 15, \r\n                                           colour = ""black"", \r\n                                           angle = 20),\r\n               strip.text.x = element_text(size = 15, \r\n                                           colour = ""black"", \r\n                                           angle = 0)                            \r\n                                           )\r\np\r\n\r\n\r\n\r\n# You can also detect the problem in these tables:\r\ntable(OC$Month, OC$FeedingPlot, OC$FeedingType)\r\n###########################################\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n#################################################\r\n# Figure 5\r\n\r\n# Equation (1) in the paper shows the mathematical\r\n# expression for GLMM for the owl data. To fit the \r\n# model in lme4 it is wise to standardize the \r\n# continuous covariates.\r\n\r\n# And we also need to think about the broodsize.\r\n# We didn\'t want to confuse the readers of the paper \r\n# with an issue like the offset. You can either use\r\n# broodsize as a covariate, or the log of broodsize \r\n# as an offset.\r\n \r\n#For the offset we need:\r\nOwls$LogBroodSize <- log(Owls$BroodSize)\r\n\r\nlibrary(lme4)\r\nM1 <- glmer(NCalls ~ FoodTreatment + ArrivalTime + SexParent +  \r\n                     FoodTreatment : SexParent +\r\n                     FoodTreatment : ArrivalTime + \r\n                     #offset(LogBroodSize) +  #Feel free to include\r\n                     (1 | Nest),\r\n            family = poisson, \r\n            data = Owls)\r\n\r\n# Depending on your lme4 version you may get a warning\r\n# message. See the lme4 discussion group:\r\n# https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models\r\n\r\n# Numerical results:\r\nsummary(M1)\r\n\r\n# Get the dispersion statistic \r\n# (not counting random effects as parameters)\r\nE1 <- resid(M1, type = ""pearson"")\r\nN  <- nrow(Owls)\r\np  <- length(fixef(M1)) + 1\r\nsum(E1^2) / (N - p)\r\n\r\n\r\n# Apply model validation\r\nE1 <- resid(M1, type = ""pearson"") \r\nF1 <- fitted(M1)\r\n\r\n# Not in the paper:\r\n# Heterogeneity?\r\npar(mfrow = c(1,1), mar = c(5,5,2,2), cex.lab = 1.5)\r\nplot(x = F1,\r\n     y = E1,\r\n     xlab = ""Fitted values"",\r\n     ylab = ""Pearson residuals"")\r\nabline(h = 0, lty = 2)     \r\n#Hmmm....\r\n\r\n# Building up to Figure 5\r\n# Do we have residual patterns?\r\npar(mfrow = c(1,1), mar = c(5,5,2,2), cex.lab = 1.5)\r\nplot(x = Owls$ArrivalTime,\r\n     y = E1,\r\n     xlab = ""Arrival time (h)"",\r\n     ylab = ""Pearson residuals"")\r\nabline(h = 0, lty = 2)     \r\n\r\n# Is there a non-linear pattern in here?\r\n# Fit a GAM on the residuals to check the residual pattern.\r\n# If the smoother is not significant, or if it explains\r\n# a small amount of variation, then there are indeed\r\n# no residual patterns.\r\nlibrary(mgcv)\r\nT2 <- gam(E1 ~ s(ArrivalTime), data = Owls)\r\nsummary(T2)\r\nplot(T2)\r\nabline(h = 0, lty = 2)\r\n\r\n# Let\'s now make Figure 5\r\n# -We create a grid of artificial Arrival time values.\r\n# -Use these for the prediction of the smoother.\r\n# -Add the smoother and 95% confidence intervals of the\r\n#  smoother to MyData\r\n# -Plot the whole thing with ggplot2\r\nrange(Owls$ArrivalTime)\r\nMyData <- data.frame(ArrivalTime = seq(21.71, 29.25, length = 100))\r\nP      <- predict(T2, newdata = MyData, se = TRUE)  \r\nMyData$mu   <- P$fit\r\nMyData$SeUp <- P$fit + 1.96 * P$se\r\nMyData$SeLo <- P$fit - 1.96 * P$se\r\n\r\nOwls$E1 <- E1\r\np <- ggplot()\r\np <- p + xlab(""Arrival time  (h)"") + ylab(""Pearson residuals"")\r\np <- p + theme(text = element_text(size = 15)) \r\n\r\np <- p + geom_point(data = Owls, \r\n                    aes(x = ArrivalTime, y = E1),\r\n                    size = 1)\r\n                    \r\np <- p + geom_line(data = MyData, \r\n                   aes(x = ArrivalTime, y = mu), \r\n                   colour = ""black"")\r\n\r\np <- p + geom_ribbon(data = MyData, \r\n                     aes(x = ArrivalTime, \r\n                         ymax = SeUp, \r\n                         ymin = SeLo ),\r\n                     alpha = 0.2)\r\np <- p + geom_hline(yintercept=0)\r\np\r\n\r\n# Yes...there is a non-linear pattern in here.\r\n# We should be doing a GAMM!\r\n################################################\r\n\r\n\r\n\r\n################################################\r\n# Figure 6\r\n# Before diving into the MCMC analysis for the baboons,\r\n# we will show the R code for Figure 6.\r\n\r\n# Sketch fitted values of M1\r\n# A. Specify covariate values for predictions\r\n# B. Create X matrix with expand.grid\r\n# C. Calculate predicted values\r\n# D. Calculate standard errors (SE) for predicted values\r\n# E. Plot predicted values\r\n# F. Plot predicted values +/- 2* SE \r\n\r\n#A: \r\nlibrary(plyr)\r\n\r\nrange(Owls$ArrivalTime)\r\nMyData <- ddply(Owls, \r\n                .(SexParent, FoodTreatment), \r\n                summarize,\r\n                ArrivalTime = seq(from = min(ArrivalTime), \r\n                                  to = max(ArrivalTime),\r\n                                  length = 10))\r\n                       \r\n#B. Create X matrix with expand.grid\r\nX <- model.matrix(~ FoodTreatment + ArrivalTime + SexParent + \r\n                    FoodTreatment:SexParent +  \r\n                    FoodTreatment:ArrivalTime, \r\n                  data = MyData)\r\n\r\n#C. Calculate predicted values\r\nMyData$eta <- X %*% fixef(M1)\r\nMyData$mu  <- exp(MyData$eta)\r\n\r\n\r\n#D. Calculate standard errors (SE) for predicted values\r\n#SE of fitted values are given by the square root of\r\n#the diagonal elements of: X * cov(betas) * t(X)\r\nMyData$SE   <- sqrt(  diag(X %*%vcov(M1) %*% t(X))  )\r\nMyData$seup <- exp(MyData$eta + 1.96 * MyData$SE)\r\nMyData$selo <- exp(MyData$eta - 1.96 * MyData$SE)\r\n\r\n    \r\n# And make a nice ggplot2 graph\r\np <- ggplot()\r\np <- p + xlab(""Arrival time (h)"") + ylab(""Number of calls"")\r\np <- p + theme(text = element_text(size=15))\r\n\r\np <- p + geom_ribbon(data = MyData,\r\n                     aes(x = ArrivalTime, \r\n                         ymax = seup, \r\n                         ymin = selo),\r\n                     alpha = 0.5)\r\np <- p + geom_line(data = MyData, \r\n                    aes(x = ArrivalTime, \r\n                        y = MyData$mu))\r\n\r\np <- p + geom_point(data = Owls, \r\n                    aes(x = ArrivalTime, y = NCalls),\r\n                    position = position_jitter(width = .01),\r\n                    color = grey(0.2),\r\n                    size = 1)\r\n       \r\np <- p + facet_grid(SexParent ~ FoodTreatment, \r\n                    scales = ""fixed"")\r\np <- p + theme(legend.position=""none"") \r\np \r\n################################################\r\n\r\n\r\n\r\n# Code for Figure 7 is given later in this file\r\n\r\n\r\n\r\n################################################\r\n# Code for Figure 8\r\nN <- nrow(Owls)\r\ngg <- simulate(M1, 10000)\r\n\r\nzeros <- vector(length = 10000)\r\nfor (i in 1:10000){\r\n\tzeros[i] <- sum(gg[,i] == 0) / N\r\n}\r\n\r\npar(mar = c(5,5,2,2), cex.lab = 1.5)\r\nplot(table(zeros), \r\n     xlim = c(0, 160 / N),\r\n     axes = FALSE,\r\n     xlab = ""Percentage of zeros"",\r\n     ylab = ""Frequency"")\r\naxis(2)\r\naxis(1, at = c(0.05, .10, 0.15, 0.20, 0.25),\r\n        labels = c(""5%"", ""10%"", ""15%"", ""20%"", ""25%""))     \r\npoints(x = sum(Owls$NCalls==0) / N, y = 0, pch = 16, cex = 5, col = 2)\r\n###########################################################\r\n\r\n\r\n\r\n\r\n\r\n\r\n###########################################################\r\n# Figure 7\r\n# Baboon example\r\n\r\n\r\n# The data is taken from:\r\n# Evidence for varying social strategies across the \r\n# day in chacma baboons\r\n# Claudia Sick, Alecia J. Carter, Harry H. Marshall, \r\n# Leslie A. Knapp, Torben Dabelsteen and Guy Cowlishaw\r\n# Biol. Lett. 2014 10, 20140249, published 9 July 2014\r\n# Data where taken from the online supplemental material:\r\n# http://rsbl.royalsocietypublishing.org/content/suppl/2014/07/08/rsbl.2014.0249.DC1.ht ml\r\n\r\n\r\n# Description of experiment/data:\r\n# 1. Data were collected from two troops of chacma baboons\r\n# 2. Each troop was followed daily from dawn to dusk (ca 06.0018.00 h). \r\n# 3. 1 h long focal follows on all identifiable individuals \r\n# 4. Recording all grooming and dominance interactions\r\n# 5. Unit of analysis:  grooming session\r\n# 6. Multiple observations from the same focal session\r\n# 7. Multiple observations on the same animal.\r\n   \r\n\r\n# Question 1: Subordinates should prefer to groom more \r\n#             dominant animals earlier in the day\r\n# Question 2: Subordinates relative contributions to grooming \r\n#             sessions should be greater earlier in the day, \r\n#             especially with high-ranking partners\r\n\r\n\r\n# What will you learn in this exercise?\r\n# 1A. MCMC code for 2-way nested and crossed beta-GLMM\r\n\r\n\r\n\r\n###########################################################################\r\nMonkeys <- read.table(file = ""MonkeysV2.txt"", \r\n                      header = TRUE)\r\nnames(Monkeys)\r\n\r\n# The code below assumes that you are familiar with Bayesian \r\n# statistics, MCMC, JAGS, the beta distribution, random effects,\r\n# priors, etc. See for example:\r\n#  -Beginner\'s Guide to Zero Inflated Models (2016). Zuur, Ieno\r\n# for beta models and random effects fitted with MCMC\r\n#\r\n# Without this knowledge the code below may look like magic.\r\n\r\n\r\n\r\n# Question 1: Subordinates should prefer to groom more dominant \r\n#             animals earlier in the day\r\n\r\n# For this question we need to analyse only the Subordinate Grooms data:\r\n\r\nMonkeys.sub <- Monkeys[Monkeys$SubordinateGrooms == ""yes"",]\r\nMS <- Monkeys.sub  #Is shorter\r\n\r\n\r\n\r\n# The Sick et al. (2014) paper gives a description of the \r\n# experimental design. Here are the essential parts:\r\n\r\n# 1. Two monkey groups (large vs small). Coded as GroupSize\r\n# 2. There are multiple focal hours per day (coded as ""FocalHour"")\r\n# 3. In a focal hour we look for a groomer (coded as FocalGroomer)\r\n# 4. We also write down the id of the receiver monkey  (coded as Receiver)\r\n# 5. Record the time since sunrise, and the rank difference between \r\n#    focal groomer and receiver\r\n\r\n#Aim: RankDifference = function(Time, Relatedness, GroupSize)\r\n\r\n#Interactions are expected:\r\n    #Time and relatedness  (used in paper)\r\n    #Based on our common sense: Relatedness and groupsize  ?\r\n############################################################\r\n\r\n\r\n###########################################################################\r\n# Response variable: RankDifference  (difference between groomer and receiver)\r\n# Covariates: Time           (time since sunrise)\r\n#             Relatedness    (index specifying relatedness)\r\n#             GroupSize      (small vs large)  \r\n#             FocalGroomer   (monkey id who is doing the grooming)\r\n#             FocalHour      (focal hour)\r\n#             Receiver       (grooming partner identity)\r\n\r\n# We use this data set as an exercise in one of our courses. \r\n# During the exercise we carry out a full data exploration,\r\n# analysis, model validation and model visualisation. It takes\r\n# about a full hour to do this exercise, and the code is quite\r\n# extensive. In this R file we only provide the essential MCMC\r\n# code to run the model, assess mixing and produce Figure 7.\r\n# The code below does not contain code for data exploration and\r\n# model validation.\r\n\r\n# The beta distribution can be used if the response \r\n# variable Y is between 0 and 1; excluding the 0 and the 1.\r\n# In the Monkey data set, we have some 0s and 1s.\r\n# In that case transform the variable using: \r\n# (Y  (n - 1) + 0.5) / n, n is the sample size\r\n\r\n# See: Beta regression in R. Cibrari-Neto & Zeileis. \r\n#      Journal of Statistical Software. April 2010, \r\n#      Volume 34, Issue 2.\r\n\r\n# Transfor the response variable:\r\nN <- nrow(Monkeys.sub)\r\nMonkeys.sub$RD.scaled <- (Monkeys.sub$RankDifference * (N - 1) + 0.5) / N\r\n\r\n\r\n# In our \'Beginner\'s Guide to Zero-Inflated Models with R\' we\r\n# apply zero inflated beta models. A full explanation is provided\r\n# in there.\r\n\r\n# For MCMC it is essential to standardize continuous covariates\r\nMystd <- function(x) {(x - mean(x)) / sd(x)}\r\nMonkeys.sub$Time.std        <- Mystd(Monkeys.sub$Time)\r\nMonkeys.sub$Relatedness.std <- Mystd(Monkeys.sub$Relatedness)\r\n\r\n\r\n#JAGS coding\r\n#Create X matrix\r\nX <- model.matrix(~Time.std * Relatedness.std + Relatedness.std * GroupSize, \r\n                  data = Monkeys.sub)\r\nK <- ncol(X)\r\n\r\n#Random effects:\r\nreFocalGroomer  <- as.numeric(as.factor(Monkeys.sub$FocalGroomer))\r\nNumFocalGroomer <- length(unique(Monkeys.sub$FocalGroomer))\r\n\r\nreFocalHour  <- as.numeric(as.factor(Monkeys.sub$FocalHour))\r\nNumFocalHour <- length(unique(Monkeys.sub$FocalHour))\r\n\r\nreReceiver  <- as.numeric(as.factor(Monkeys.sub$Receiver))\r\nNumReceiver <- length(unique(Monkeys.sub$Receiver))\r\n\r\n#Put all data for JAGS in a list\r\nJAGS.data <- list(Y            = Monkeys.sub$RD.scaled,\r\n                  N            = nrow(Monkeys.sub),\r\n                  X            = X,\r\n                  K            = ncol(X),\r\n                  reFGroomer   = reFocalGroomer,\r\n                  NumFGroomer  = NumFocalGroomer,\r\n                  reFHour      = reFocalHour,\r\n                  NumFHour     = NumFocalHour,\r\n                  reRec        = reReceiver,\r\n                  NumReceiver  = NumReceiver\r\n                 )\r\nJAGS.data\r\n\r\n#JAGS code\r\nsink(""BetaRegmm.txt"")\r\ncat(""\r\nmodel{\r\n    #Priors regression parameters, theta\r\n    for (i in 1:K) { beta[i]  ~ dnorm(0, 0.0001) }  \r\n    theta ~ dunif(0, 20)\r\n    \r\n    #Priors random effects\r\n    for (i in 1: NumFGroomer) {a[i] ~ dnorm(0, tau.gr) }\r\n    for (i in 1: NumFHour)    {b[i] ~ dnorm(0, tau.fh) }\r\n    for (i in 1: NumReceiver) {c[i] ~ dnorm(0, tau.rc) }\r\n    \r\n    #Priors for the variances of the random effects \r\n    tau.gr <- 1 / (sigma.gr * sigma.gr)\r\n    tau.fh <- 1 / (sigma.fh * sigma.fh)\r\n    tau.rc <- 1 / (sigma.rc * sigma.rc)\r\n    \r\n    sigma.gr ~ dunif(0, 10)\r\n    sigma.fh ~ dunif(0, 10)\r\n    sigma.rc ~ dunif(0, 10)\r\n      \r\n    #######################\r\n    #Likelihood \r\n    for (i in 1:N){       \r\n      Y[i] ~ dbeta(shape1[i], shape2[i])\r\n      shape1[i] <- theta * pi[i]        \r\n      shape2[i] <- theta * (1 - pi[i])  \r\n      \r\n      logit(pi[i]) <- eta[i] + a[reFGroomer[i]] + b[reFHour[i]] + c[reRec[i]]\r\n      eta[i]       <- inprod(beta[], X[i,]) \r\n  } \r\n}\r\n"",fill = TRUE)\r\nsink()\r\n\r\n\r\n#Set the initial values for the betas and sigma\r\ninits <- function () {\r\n  list(\r\n    beta  = rnorm(ncol(X), 0, 0.1),\r\n    theta = runif(0, 20),\r\n    sigma.gr = runif(0, 10),\r\n    sigma.fh = runif(0, 10),     \r\n    sigma.rc = runif(0, 10)     \r\n    )  }\r\n\r\n#Parameters to estimate\r\nparams <- c(""beta"", ""theta"", \r\n            ""sigma.gr"", ""sigma.fh"", ""sigma.rc"")\r\n\r\nlibrary(R2jags)\r\nJ0 <- jags(data = JAGS.data,\r\n           inits = inits,\r\n           parameters = params,\r\n           model.file = ""BetaRegmm.txt"",\r\n           n.thin = 10,\r\n           n.chains = 3,\r\n           n.burnin = 4000,\r\n           n.iter   = 5000)\r\n\r\n#Fast computer:\r\nJ1  <- update(J0, n.iter = 10000, n.thin = 10)  \r\nJ2  <- update(J1, n.iter = 50000, n.thin = 10)  \r\nout <- J2$BUGSoutput\r\n\r\n\r\n# We present three functions to view the output.\r\n# These functions are explained and used in\r\n#  -Beginner\'s Guide to Zero-Inflated models with R\r\n#   Zuur and Ieno. www.highstat.com\r\n#  -Beginner\'s Guide to GLM and GLMM with R\r\n#   Zuur, Hilbe, Ieno. www.highstat.com\r\n\r\n# Copy and paste these three functions into R.\r\n# Don\'t bother trying to understand them.\r\n\r\n# Support files from \'Beginner\'s Guide to GLM and GLMM with R.\r\n# Zuur, Hilbe, Ieno (2013).\r\nMyBUGSChains <- function(xx, vars, PanelNames = NULL){\r\n#Small function to make an xyplot of the iterations per chain,\r\n#for each variable \r\n  x <- xx$sims.array\r\n  idchain.All <- NULL\r\n  x1.All <- NULL\r\n  ChainLength.All <- NULL\r\n  id.All <- NULL\r\n\r\n  NumBerChains <- ncol(x[,,vars[1]])\r\n \r\n  for (i in vars){\r\n\tx1          <- as.vector(x[,,i])\r\n\tid          <- rep(rep(i, length = nrow(x[,,i])),NumBerChains)\r\n\tidchain     <- rep(1:NumBerChains, each = nrow(x[,,i]))\r\n    ChainLength <- rep(1: nrow(x[,,i]), NumBerChains)\r\n\r\n    x1.All <- c(x1.All, x1)\r\n    ChainLength.All <- c(ChainLength.All, ChainLength)\r\n    id.All <- c(id.All, id)\r\n    idchain.All <- c(idchain.All, idchain)\r\n   }\r\n\r\n  if (!is.null(PanelNames)) { \r\n     if (length(unique(id.All)) != length(PanelNames)) {stop(""Wrong number of panel names"")}\r\n  \t  AllNames <- unique(id.All)\r\n  \t  for (i in 1:length(AllNames)) {\r\n  \t  \tid.All[id.All == AllNames[i]] <- PanelNames[i] \r\n  \t  }\r\n  \t  id.All <- factor(id.All, levels = PanelNames)\r\n  \t  }\r\n\r\n\r\n  Z <- xyplot(x1.All ~ ChainLength.All | factor(id.All) ,\r\n       type = ""l"",\r\n       strip = strip.custom(bg = \'white\',\r\n       par.strip.text = list(cex = 1.2)),\r\n       scales = list(x = list(relation = ""same"", draw = TRUE),\r\n                     y = list(relation = ""free"", draw = TRUE)),\r\n       groups = idchain.All,  col = 1:NumBerChains,\r\n       xlab = list(label = ""MCMC iterations"", cex = 1.5),\r\n       ylab = list(label = ""Sampled values"", cex = 1.5))\r\n  print(Z)\r\n}\r\n\r\n\r\n##################################################\r\nMyBUGSOutput <- function(Output  = Output, SelectedVar = SelectedVar, VarNames = NULL){\r\n\txx   <- Output\r\n\tvars <- SelectedVar\r\n\t\r\n\tif (is.null(VarNames)) { VarNames <- SelectedVar }\r\n\tif (length(SelectedVar) != length(VarNames)) {stop(""Wrong number of variable names"")}\r\n\r\n\tx <- xx$sims.matrix\r\n    OUT <- matrix(nrow = length(vars), ncol=4) \r\n    j<-1\r\n\tfor(i in vars){\r\n\t  xi <- x[,i]\t\r\n   \t  OUT[j,3:4] <- quantile(xi, probs = c(0.025, 0.975))\r\n   \t  OUT[j,1]   <- mean(xi)\r\n   \t  OUT[j,2]   <- sd(xi)\r\n   \t  j          <- j + 1\r\n\t}\r\n\tcolnames(OUT) <- c(""mean"", ""se"", ""2.5%"", ""97.5%"")\r\n\trownames(OUT) <- VarNames\r\n\tOUT\r\n}\r\n\r\nuNames <- function(k,Q){\r\n  #Function to make a string of variables names of the form:\r\n  #c(""u[1]"",""u[2]"", etc, ""u[50]"")\t\r\n  String<-NULL\r\n  for (j in 1:Q){String <- c(String, paste(k,""["",j,""]"",sep = """"))}\r\n  String\r\n}\r\n# End of support code from Zuur et al.(2013).\r\n##############################################\r\n\r\n\r\n#Assess mixing\r\nMyBUGSChains(out, \r\n             c(uNames(""beta"", K), ""theta"", ""sigma.gr"", ""sigma.fh"", ""sigma.rc""))\r\n\r\n\r\n##6. Present output\r\nOUT1 <- MyBUGSOutput(out, \r\n                     c(uNames(""beta"", K), ""sigma.gr"", ""sigma.fh"", ""sigma.rc"", ""theta""))\r\nrownames(OUT1)[1:K] <- colnames(X)\r\nprint(OUT1, digits = 5)  \r\n######################################################################\r\n\r\n\r\n######################################################################\r\n# Model validation:\r\n# Removed from this R file\r\n\r\n\r\n######################################################################\r\n#Sketch the fitted values\r\n\r\n#Get the realisations of the betas\r\nBeta.mcmc <- out$sims.list$beta \r\n\r\n\r\n# Create a grid of covariate values\r\nrange(Monkeys.sub$Time.std)\r\nrange(Monkeys.sub$Relatedness.std)\r\nMyData <- expand.grid(Time.std    = seq(-2.25, 1.86, length = 15),\r\n                      Relatedness.std = seq(-1.4, 1.67, length = 15),\r\n                      GroupSize = levels(Monkeys.sub$GroupSize))\r\n\r\n# Convert the grid into a X matrix, and calculate the fitted \r\n# values for each (!) MCMC iteration\r\nX          <- model.matrix(~Time.std * Relatedness.std + \r\n                            Relatedness.std * GroupSize, \r\n                            data = MyData)\r\nBetas      <- Beta.mcmc\r\neta        <- X %*% t(Betas)\r\nmu         <- exp(eta) / (1 + exp(eta))\r\n\r\n\r\n# Calculate the posterior mean and 95% CI for each value on the grid\r\n# See Zuur et al. (2013) and Zuur et al. (2016) for an explanation.\r\nMyLinesStuff <- function(x){\r\n   OUT <- matrix(nrow = nrow(x), ncol=4) \r\n\tfor(i in 1:nrow(x)){\r\n\t  xi <- x[i,]\t\r\n      OUT[i,3:4] <- quantile(xi, probs = c(0.025, 0.975))\r\n      OUT[i,1] <- mean(xi)\r\n      OUT[i,2] <- sd(xi)\r\n\t}\r\n\tcolnames(OUT) <- c(""mean"", ""se"", ""2.5%"", ""97.5%"")\r\n\tOUT\r\n}\r\nL <- MyLinesStuff(mu)  \r\n\r\nL #Posterior mean, se and 95% CI for each of these covariate values\r\n\r\n#Glue the results in L to the MyData object\r\nMyData <- cbind(MyData,L)\r\nMyData\r\n\r\n# Plot the results\r\n# Pick on value of i\r\n i <- 70\r\n p <- wireframe(mean ~ Time.std + Relatedness.std, \r\n                data = MyData,\r\n                group = GroupSize,\r\n                #zlim = c(0,1),\r\n                shade = TRUE,\r\n                scales = list(arrows = FALSE),\r\n                drape = TRUE, \r\n                colorkey = FALSE,\r\n                screen = list(z = i, x = -60 - i /5))\r\n print(p)\r\n# In the paper we use a slighly different angle. Change\r\n# the i value, or put a loop around this code in which i\r\n# runs form 1 to 1000.\r\n\r\n\r\n ']","Data from: A protocol for conducting and presenting results of regression-type analyses Scientific investigation is of value only insofar as relevant results are obtained and communicated, a task that requires organizing, evaluating, analysing and unambiguously communicating the significance of data. In this context, working with ecological data, reflecting the complexities and interactions of the natural world, can be a challenge. Recent innovations for statistical analysis of multifaceted interrelated data make obtaining more accurate and meaningful results possible, but key decisions of the analyses to use, and which components to present in a scientific paper or report, may be overwhelming. We offer a 10-step protocol to streamline analysis of data that will enhance understanding of the data, the statistical models and the results, and optimize communication with the reader with respect to both the procedure and the outcomes. The protocol takes the investigator from study design and organization of data (formulating relevant questions, visualizing data collection, data exploration, identifying dependency), through conducting analysis (presenting, fitting and validating the model) and presenting output (numerically and visually), to extending the model via simulation. Each step includes procedures to clarify aspects of the data that affect statistical analysis, as well as guidelines for written presentation. Steps are illustrated with examples using data from the literature. Following this protocol will reduce the organization, analysis and presentation of what may be an overwhelming information avalanche into sequential and, more to the point, manageable, steps. It provides guidelines for selecting optimal statistical tools to assess data relevance and significance, for choosing aspects of the analysis to include in a published report and for clearly communicating information.",3
Data from: Out of the tropics: Macroevolutionary size trends in an old insect order are shaped by temperature and predators,"Global body size distributions are shaped by selection pressures arising from biotic and abiotic factors such as temperature, predation and parasitism. Here, we investigated the ecological and evolutionary drivers of global latitudinal size gradients in an old insect order (Odonata; dragonflies and damselflies). Phylogenetic comparative analyses revealed that global size variation of extant taxa is negatively influenced by both regional avian diversity and temperature. Interestingly, fossil data show that the relationship between wing size and latitude has shifted: latitudinal size trends had initially negative slopes but became shallower or positive following the emergence of birds 150 MYA. These changing size-latitude trends over geological time were likely driven by bird predation and high dispersal ability of large dragonflies. Our results therefore suggest that latitudinal size gradients were shaped by temperature but also by predators driving the dispersal of large-sized clades out of the tropics and in to the temperate zone.","['#Set working directory\r\n\r\n#setwd()\r\n\r\ngetPerLat <- function(data, suborder) {\r\n  data = data[data$SubOrder == suborder, ]\r\n  \r\n  fit = lm(size_mm ~ age_cat * abs(lat), data = data)\r\n  return(summary(aov(fit)))\r\n}\r\n\r\n\r\n\r\ngetLat <- function(data, suborder, age) {\r\n  data = data[data$SubOrder == suborder, ]\r\n  data = data[data$age_cat == age, ]\r\n  \r\n  fit = lm(size_mm ~ abs(lat), data = data)\r\n  return(summary(fit))\r\n}\r\n\r\n\r\n################## Taken from Cardillo, M. & Skeels, A. 2016. PLoS ONE\r\n\r\nrun.Regress <- function(curr.predictors){\r\n  response <- names(D)[6]\r\n  predictors <- names(D)[c(9:11,7:8)]\r\n  fmla <- as.formula(paste(response, ""~"", paste(curr.predictors, collapse = ""+"")))\r\n  test <- regress(fmla, ~  S + V, data = df, start = c(0.001, 0.001, 0.1),\r\n                  identity = TRUE, pos = c(TRUE, TRUE, TRUE))\r\n  variables <- curr.predictors\r\n  estimates <- test$beta\r\n  standar_error <- test$beta.se\r\n  out <- data.frame(estimates, standar_error)\r\n  out$t.stat <- estimates/standar_error\r\n  out$p.val <- pt(abs(out$t.stat),test$rdf,lower.tail=F)\r\n  out$p.adj <- p.adjust(out$p.val, method = ""holm"")\r\n  out[,c(3:5)]<-round(out[,c(3:5)],4)\r\n  \r\n  aic <- 2*(length(test$Vnames)+length(test$Kcolnames))-2*(test$llik)\r\n  out$aic <- rep(aic, nrow(out))\r\n  LnLik <- test$llik\r\n  out$LnLik <- rep(LnLik, nrow(out))\r\n  return(out)\r\n}\r\n\r\n\r\n\r\nprepD <- function(C, B){\r\n  \r\n  B_hwl = B[c(""GenusSpecies"", ""Species"", ""Genus"", ""Family"", ""SubOrder"", ""hwl"")] # drop another variable so that is easy to work with\r\n  B_hwl = na.omit(B_hwl)\r\n  \r\n  B_tbl = B[c(""GenusSpecies"",""Species"",""Genus"",""Family"",""SubOrder"",""tbl"")] # drop another variable so that is easy to work with \r\n  B_tbl = na.omit(B_tbl) # remove missing \r\n  \r\n  AL = aggregate(C$decimalLatitude ~ GenusSpecies, data = C, median) # average latitude\r\n  colnames(AL) = c(""GenusSpecies"",""lat"")\r\n  AL$climate = ifelse(AL$lat < 23 & AL$lat > -23,""trop"",""temp"") # define tropical climates\r\n  \r\n  D_hwl = merge(B_hwl, AL, id = ""GenusSpecies"")\r\n  D_tbl = merge(B_tbl, AL, id = ""GenusSpecies"")\r\n  \r\n  # Zygoptera\r\n  D_hwlZ = D_hwl[D_hwl$SubOrder == ""Zygoptera"",]\r\n  D_tblZ = D_tbl[D_tbl$SubOrder == ""Zygoptera"",]\r\n  \r\n  # Ansioptera\r\n  D_hwlA = D_hwl[D_hwl$SubOrder == ""Anisoptera"",] \r\n  D_tblA = D_tbl[D_tbl$SubOrder == ""Anisoptera"",]\r\n  \r\n  \r\n  d1 = getD(D_hwl, D_tbl) # process data\r\n  d1$group = ""Odonata (both suborders)""\r\n  \r\n  # Zygoptera \r\n  d2 = getD(D_hwlZ, D_tblZ) # process data\r\n  d2$group = ""Zygoptera (damselflies)""\r\n  \r\n  d3 = getD(D_hwlA, D_tblA) # process data\r\n  d3$group = ""Anisoptera (dragonflies)""\r\n  \r\n  D = rbind(d1, d2, d3) # final plot data \r\n  D$pvalueRES = round(D$pvalue, 3)\r\n  D$group = factor(D$group, levels = c(""Odonata (both suborders)"", \r\n                                       ""Zygoptera (damselflies)"", \r\n                                       ""Anisoptera (dragonflies)"")) \r\n  return(D)\r\n}\r\n\r\ngetD = function(D_hwl,D_tbl) { \r\n  \r\n  fit_hwl = t.test(hwl ~ climate,data=D_hwl)\r\n  fit_tbl = t.test(tbl ~ climate,data=D_tbl)\r\n  \r\n  fitD_hwl = broom::tidy(fit_hwl) # fit data\r\n  fitD_tbl = broom::tidy(fit_tbl) # fit data\r\n  \r\n  pvalue_tbl = fitD_tbl$p.value\r\n  pvalue_hwl = fitD_hwl$p.value\r\n  \r\n  mean_temp_tbl = fitD_tbl$estimate1 \r\n  mean_trop_tbl = fitD_tbl$estimate2\r\n  tbl_means = c(mean_trop_tbl,mean_temp_tbl)\r\n  \r\n  mean_temp_hwl = fitD_hwl$estimate1\r\n  mean_trop_hwl = fitD_hwl$estimate2\r\n  hwl_means = c(mean_trop_hwl,mean_temp_hwl)\r\n  \r\n  hwl_se = sd(D_hwl$hwl)/sqrt(length(D_hwl$hwl))\r\n  tbl_se = sd(D_tbl$tbl)/sqrt(length(D_tbl$tbl))\r\n  \r\n  se = rep(c(hwl_se, tbl_se),each=2) # 95% ci\r\n  \r\n  climate = c(""Trop"", ""Temp"", ""Trop"", ""Temp"")\r\n  morph = c(""hwl"", ""hwl"", ""tbl"", ""tbl"")\r\n  means = c(hwl_means, tbl_means)\r\n  \r\n  pvalue = rep(c(pvalue_hwl, pvalue_tbl),each=2)\r\n  \r\n  return(data.frame(climate = climate, morph = morph,\r\n                    means = means, se = se, pvalue = pvalue))\r\n}\r\n\r\n\r\nprepD2 <- function(C, B, treee){\r\n  \r\n  B_hwl = B[c(""GenusSpecies"", ""Species"", ""Genus"", ""Family"", ""SubOrder"", ""hwl"")] # drop another variable so that is easy to work with\r\n  B_hwl = na.omit(B_hwl)\r\n  \r\n  B_tbl = B[c(""GenusSpecies"",""Species"",""Genus"",""Family"",""SubOrder"",""tbl"")] # drop another variable so that is easy to work with \r\n  B_tbl = na.omit(B_tbl) # remove missing \r\n  \r\n  AL = aggregate(C$decimalLatitude ~ GenusSpecies, data = C, median) # average latitude\r\n  colnames(AL) = c(""GenusSpecies"",""lat"")\r\n  AL$climate = ifelse(AL$lat < 23 & AL$lat > -23,""trop"",""temp"") # define tropical climates\r\n  \r\n  D_hwl = merge(B_hwl, AL, id = ""GenusSpecies"")\r\n  D_tbl = merge(B_tbl, AL, id = ""GenusSpecies"")\r\n  \r\n  # Zygoptera\r\n  D_hwlZ = D_hwl[D_hwl$SubOrder == ""Zygoptera"",]\r\n  D_tblZ = D_tbl[D_tbl$SubOrder == ""Zygoptera"",]\r\n  \r\n  # Ansioptera\r\n  D_hwlA = D_hwl[D_hwl$SubOrder == ""Anisoptera"",] \r\n  D_tblA = D_tbl[D_tbl$SubOrder == ""Anisoptera"",]\r\n  \r\n  # Odonata \r\n  d1 = treeD(tree, D_hwl, D_tbl) # process data\r\n  d1$group = ""Odonata (both suborders)""\r\n  \r\n  # Zygoptera \r\n  d2 = treeD(tree, D_hwlZ, D_tblZ) # process data\r\n  d2$group = ""Zygoptera (damselflies)""\r\n  \r\n  # Ansioptera\r\n  d3 = treeD(tree, D_hwlA, D_tblA) # process data\r\n  d3$group = ""Anisoptera (dragonflies)""\r\n  \r\n  D2 = rbind(d1,d2,d3) # final plot data \r\n  D2$pvalueRES = round(D2$pvalue,3)\r\n  D2$group = factor(D2$group, levels = c(""Odonata (both suborders)"", \r\n                                         ""Zygoptera (damselflies)"", \r\n                                         ""Anisoptera (dragonflies)""))\r\n  return(D2)\r\n}\r\n\r\n\r\ntreeD = function(tree, D_hwl, D_tbl) {\r\n  \r\n  D_hwl = D_hwl[D_hwl$GenusSpecies %in% tree$tip.label, ] # get only those with tip\r\n  D_tbl = D_tbl[D_tbl$GenusSpecies %in% tree$tip.label, ] # get only those with tip\r\n  \r\n  tip_hwl = tree$tip.label[!tree$tip.label %in% D_hwl$GenusSpecies] # drop tips without data\r\n  tip_tbl = tree$tip.label[!tree$tip.label %in% D_tbl$GenusSpecies] # drop tips without data\r\n  \r\n  D_hwl$Species <- D_hwl$GenusSpecies\r\n  D_tbl$Species <- D_tbl$GenusSpecies\r\n  \r\n  tree_hwl = drop.tip(tree, tip_hwl)\r\n  tree_tbl = drop.tip(tree, tip_tbl)\r\n  \r\n  rownames(D_hwl) = D_hwl$GenusSpecies\r\n  V_hwl = corBrownian(1, phy = tree_hwl, form = ~ Species)\r\n  fit_hwl = gls(hwl ~ climate, correlation = V_hwl, data = D_hwl)\r\n  \r\n  rownames(D_tbl) = D_tbl$GenusSpecies\r\n  V_tbl = corBrownian(1, phy = tree_tbl, form = ~ Species)\r\n  fit_tbl = gls(tbl ~ climate, correlation = V_tbl, data = D_tbl)\r\n  \r\n  tbl_se = summary(fit_tbl)$tTable[2, 2]\r\n  hwl_se = summary(fit_hwl)$tTable[2, 2]\r\n  \r\n  pvalue_tbl = summary(fit_tbl)$tTable[2, 4]\r\n  pvalue_hwl = summary(fit_hwl)$tTable[2, 4]\r\n  \r\n  mean_temp_tbl = coef(fit_tbl)[1]\r\n  mean_trop_tbl = coef(fit_tbl)[1] + coef(fit_tbl)[2]\r\n  tbl_means = c(mean_trop_tbl, mean_temp_tbl)\r\n  \r\n  mean_temp_hwl = coef(fit_hwl)[1]\r\n  mean_trop_hwl = coef(fit_hwl)[1] + coef(fit_hwl)[2]\r\n  hwl_means = c(mean_trop_hwl, mean_temp_hwl)\r\n  \r\n  climate = c(""Trop"", ""Temp"", ""Trop"", ""Temp"")\r\n  morph = c(""hwl"", ""hwl"", ""tbl"", ""tbl"")\r\n  means = c(hwl_means, tbl_means)\r\n  \r\n  se = rep(c(hwl_se, tbl_se), each = 2) # 95% ci\r\n  pvalue = rep(c(pvalue_hwl, pvalue_tbl), each = 2)\r\n  \r\n  return(data.frame(climate = climate, morph = morph,\r\n                    means = means, se = se, pvalue = pvalue))\r\n}\r\n\r\n', 'library(ape)\r\nlibrary(geiger)\r\nlibrary(mvMORPH)\r\nlibrary(tidyverse)\r\nlibrary(ggplot2)\r\nlibrary(nlme)\r\nlibrary(RColorBrewer)\r\nlibrary(fields)\r\nlibrary(maps)\r\nlibrary(phylopath)\r\nlibrary(caper)\r\nlibrary(fossil)\r\nlibrary(regress)\r\nlibrary(corrplot)\r\nlibrary(corHMM)\r\nlibrary(nlme)\r\nlibrary(gridExtra)\r\n\r\n########################\r\n####### SET WORKING DIRECTORY\r\n########################\r\n\r\n#setwd()\r\n\r\n########################\r\n###### FIGURE 1\r\n########################\r\n\r\nB = read.csv(""quickBodySize.csv"", sep="";"") # load cleaned body size data\r\n\r\nC <- read.csv(""climateLat.csv"") # load climate data\r\n\r\nC$GenusSpecies = gsub("" "", ""_"", C$GenusSpecies) \r\n\r\nD = prepD(C, B)\r\n\r\n### FIGURE 1A\r\np1 = ggplot(D, aes(x = morph, y = means, fill = factor(climate))) +\r\n  geom_bar(stat = ""identity"", position = ""dodge"") + facet_wrap( ~ group) +\r\n  geom_errorbar(aes(ymin = means - se * 2, ymax = means + se * 2),\r\n                width = .2, position = position_dodge(.9)) +\r\n  xlab(\'\') + ylab(\'Length (mm)\') +\r\n  theme_bw() + scale_fill_manual(values = c(""#0190b6"", ""#e13d14"")) +\r\n  scale_x_discrete(labels = c(\'h. wing\', \'body len.\')) +\r\n  guides(fill = guide_legend(title = ""Latitude"")) +\r\n  theme(strip.background = element_rect(fill = ""#f1e4d7"")) +\r\n  theme(legend.position = \'none\',\r\n        text = element_text(size = 14)) + ylim(0, 62)\r\n\r\n### FIGURE 1B\r\ntree <- read.tree(""tree.txt"") # load tree\r\n\r\nD2 <- prepD2(C, B, tree)\r\n\r\np2 = ggplot(D2, aes(x = morph, y = means, fill = factor(climate))) +\r\n  geom_bar(stat = ""identity"", position = ""dodge"") + facet_wrap( ~ group) +\r\n  geom_errorbar(aes(ymin = means - se, ymax = means + se),\r\n                width = .2, position = position_dodge(.9)) +\r\n  xlab(\'\') + ylab(\'Length (mm)\') +\r\n  scale_x_discrete(labels = c(\'h. wing\', \'body len.\')) +\r\n  theme_bw() + scale_fill_manual(values = c(""#0190b6"", ""#e13d14"")) +\r\n  guides(fill = guide_legend(title = ""Latitude"")) +\r\n  theme(strip.background = element_rect(fill = ""#f1e4d7"")) +\r\n  theme(legend.position = c(0.93, 0.12),\r\n        text = element_text(size = 14)) + ylim(0, 62)\r\n\r\npdf(\'fig1.pdf\', width = 14, height = 4) \r\ngrid.arrange(p1, p2, ncol = 2)\r\ndev.off()\r\n########################################################################\r\n\r\n\r\n########################\r\n##### paleolatitude\r\n########################\r\n\r\nB = read.csv(""quickBodySize.csv"",sep="";"") # load cleaned body size data\r\n\r\nB_hwl = B[c(""GenusSpecies"", ""Species"", ""Genus"", ""Family"", ""SubOrder"", ""hwl"")] # drop another variable so that is easy to work with\r\nB_hwl = na.omit(B_hwl)\r\n\r\nC <- read.csv(""climateLat.csv"") # load un-cleaned climate data\r\n\r\nC$GenusSpecies = gsub("" "", ""_"", C$GenusSpecies) \r\nAL = aggregate(decimalLatitude ~ GenusSpecies, data = C, mean) # average latitude\r\nD = merge(B, AL, id = ""GenusSpecies"") # final data.frame\r\n\r\n# make the data.frames match up for plotting\r\nD$age_ma = 0\r\nD$lat = D$decimalLatitude\r\nD = D[c(""GenusSpecies"", ""Genus"", ""Family"", ""SubOrder"", ""hwl"", ""age_ma"", ""lat"")]\r\ncolnames(D) = c(""GenusSpecies"", ""Genus"", ""Family"", ""SubOrder"", ""size_mm"", ""age_ma"", ""lat"")\r\n\r\nDF = read.csv(""FossilsLatTax.csv"", sep="";"") # load fossil data\r\n\r\nDF$lat = DF$paleolat\r\nDF = DF[c(""GenusSpecies"", ""Genus"", ""Family"", ""SubOrder"", ""size_mm"", ""age_ma"", ""lat"")]\r\n\r\nD$type = ""present""; DF$type = ""fossil""\r\n\r\nD = rbind(D, DF) # combine present and fossil datasets\r\nD$age_cat = D$age_ma\r\nD$age_cat = ""80-0""\r\nD$age_cat[D$age_ma > 80] = ""150-80""\r\nD$age_cat[D$age_ma > 149] = ""210-150""\r\nD$age_cat[D$type == ""present""] = ""0 extant""\r\n\r\nD$age_cat = factor(D$age_cat)\r\n\r\n# Suborders\r\nZ <- ""Zygoptera""\r\nA <- ""Anisoptera""\r\n\r\n# Time period\r\next <- ""0 extant""\r\nper80 <- ""80-0""\r\nper150 <- ""150-80""\r\nper210 <- ""210-150""\r\n\r\nDfos <- D[which(D$type == \'fossil\'),]\r\n\r\n###### \r\n\r\ngetPerLat(Dfos, Z)\r\ngetPerLat(Dfos, A) # Feed this formula with the data (D) and one suborder (Z or A)\r\n\r\n\r\n##### \r\ngetLat(D, Z, per150) # Feed this formula with the data (D), a suborder (Z or A) and a time period\r\ngetLat(D, A, per210)\r\n########################################################################\r\n\r\n\r\n########################\r\n##### FIGURE 2\r\n########################\r\n\r\nD$age_cat <- relevel(D$age_cat, ref = ""0 extant"")\r\nD$age_cat <- relevel(D$age_cat, ref = ""80-0"")\r\nD$age_cat <- relevel(D$age_cat, ref = ""150-80"")\r\nD$age_cat <- relevel(D$age_cat, ref = ""210-150"")\r\n\r\np3 <- ggplot(D, aes(abs(lat), size_mm)) + geom_point(alpha = 0.5, size = 2) +\r\n  geom_point(shape = 1, size = 2, colour = ""black"") +\r\n  geom_smooth(method = ""glm"", se = FALSE, colour = ""gray31"") +\r\n  ggtitle(""Fossils: wing length and latitude"") +\r\n  facet_grid(SubOrder ~ age_cat, scales = ""free_x"") +\r\n  xlab(""Paleo-latitude"") + theme_bw() + theme(legend.position = \'none\') +\r\n  theme(strip.background = element_rect(fill = ""#f1e4d7"")) +\r\n  ylab(""Wing length (mm)"") \r\n\r\npdf(\'fig2.pdf\', width = 8, height = 4) \r\np3\r\ndev.off()\r\n########################################################################\r\n\r\n\r\n########################\r\n##### FIGURE 3\r\n########################\r\nE = read.csv(""optima.csv"",sep="";"")\r\n\r\nE = E[E$tn == ""tip"",] # filter edge matrix\r\n\r\nexamples = c(""Calopteryx_exul"", ""Ictinogomphus_decoratus"", ""Aeshna_eremita"",\r\n             ""Macromia_taeniolata"", ""Megaloprepus_caerulatus"", ""Mecistogaster_linearis"",\r\n             ""Ischnura_aurora"") \r\nexample_optima = c(); for(i in 1:length(examples)) example_optima[i] = E[E$label == examples[i],]$optima_tip\r\n\r\nE$shifts = ifelse(E$optima_tip %in% example_optima, ""pos"", ""neg"")\r\nE = E[c(""label"", ""shifts"")]\r\ncolnames(E) = c(""GenusSpecies"", ""shifts"") \r\n\r\nC = read.csv(""climateLat.csv"") # load un-cleaned climate data\r\n\r\nC$GenusSpecies = gsub("" "", ""_"", C$GenusSpecies) \r\nAL = aggregate(C$decimalLatitude ~ GenusSpecies, data = C, mean) # average latitude\r\ncolnames(AL) = c(""GenusSpecies"", ""lat"")\r\nAL$climate = ifelse(AL$lat < 23 & AL$lat > -23, ""trop"", ""temp"") # define tropical climates\r\n\r\nD = merge(E, AL, id = ""GenusSpecies"")\r\n\r\nBS = read.csv(""quickBodySize.csv"",sep="";"")\r\n\r\nBS = BS[c(""GenusSpecies"", ""tbl"")]\r\nD = merge(D, BS, id = ""GenusSpecies"")\r\n\r\ntree = read.tree(""tree.txt"") # load tree for pgls\r\nD = D[D$GenusSpecies %in% tree$tip.label,] # get only those with tip\r\ntip = tree$tip.label[!tree$tip.label %in% D$GenusSpecies] # drop tips without data\r\ntree = drop.tip(tree, tip)\r\n\r\nshifts = setNames(D$shifts, D$GenusSpecies)\r\nclimate = setNames(D$climate, D$GenusSpecies)\r\n\r\ntree_map_shifts = make.simmap(tree, x = shifts, model = ""SYM"", nsim = 1)\r\ncols_shifts = setNames(c(""#ff7373"", ""#0190b6""), c(""neg"", ""pos""))\r\nbar_cols_shifts = setNames(ifelse(D$shifts == ""pos"", ""#0190b6"", ""#ff7373""), D$GenusSpecies)\r\nx_shifts = D$tbl\r\nnames(x_shifts) = D$GenusSpecies\r\n\r\ntree_map_climate = make.simmap(tree, x = climate, model = ""SYM"", nsim = 1)\r\ncols_climate = setNames(c(""#ff7373"", ""#0190b6""), c(""trop"", ""temp""))\r\nbar_cols_climate = setNames(ifelse(D$climate == ""pos"", ""#ff7373"", ""#0190b6""), D$GenusSpecies)\r\nx_climate = rep(1, length(climate))\r\nnames(x_climate) = D$GenusSpecies\r\n\r\npdf(\'fig3A.pdf\', width = 10, height = 10) \r\n\r\npar(mfrow = c(1, 2))\r\nplotTree.wBars(tree_map_shifts, x_shifts, scale = 1, width = NULL, type = ""phylogram"", \r\n               method = ""plotSimmap"", tip.labels = TRUE, col= bar_cols_shifts, \r\n               border = NA, fsize = 0.1, colors = cols_shifts)\r\n\r\nplotTree.wBars(tree_map_climate, x_climate, scale = 1, width = NULL, type = ""phylogram"",\r\n               method = ""plotSimmap"", tip.labels = TRUE, col = bar_cols_climate, \r\n               border = NA, fsize = 0.1, colors = cols_climate,\r\n               direction = ""leftwards"")\r\n\r\ndev.off()\r\n\r\n# Correlation plot\r\ntrait = data.frame(Genus_sp = D$GenusSpecies, T1 = D$climate, T2 = D$shifts)\r\ntrait$T1 = ifelse(trait$T1 == ""temp"", 1, 0)\r\ntrait$T2 = ifelse(trait$T2 == ""pos"", 1, 0)\r\n\r\n\r\nfit1 = corDISC(phy = tree, data = trait, ntraits = 2, model = ""ARD"",\r\n               node.states = ""marginal"", lewis.asc.bias = F, p = NULL, root.p = NULL, \r\n               ip = 1, lb = 0, ub = 100, diagn = FALSE)\r\n\r\nfit2 = corDISC(phy = tree, data = trait, ntraits = 2, model = ""ER"",\r\n               node.states = ""marginal"", lewis.asc.bias = F, p = NULL, root.p = NULL, \r\n               ip = 1, lb = 0, ub = 100, diagn = FALSE)\r\n\r\nfit3 = corDISC(phy = tree, data = trait, ntraits = 2, model = ""SYM"",\r\n               node.states = ""marginal"", lewis.asc.bias = F, p = NULL, root.p = NULL, \r\n               ip = 1, lb = 0, ub = 100, diagn = FALSE)\r\n\r\nc(fit1$AIC, fit2$AIC, fit3$AIC) # fit1 ARD has the lowest AIC value\r\n\r\nM = fit1$solution\r\nM[is.na(M)] = 0\r\nM = M*100\r\nrownames(M) = c(""trop & neutral"", ""trop & shift"", ""temp & neutral"", ""temp & shift"")\r\ncolnames(M) = c(""trop & neutral"", ""trop & shift"", ""temp & neutral"", ""temp & shift"")\r\n\r\npdf(\'fig3B.pdf\', width = 6, height = 6) \r\n\r\ncol1 = colorRampPalette(c(\'white\', \'darkblue\'), 10)\r\n\r\ncorrplot(M, method = ""circle"", is.corr = FALSE, tl.srt = 0, addgrid.col = \'red\',\r\n         tl.col = \'black\', col = col1(10))\r\ndev.off()\r\n########################################################################\r\n\r\n########################\r\n#### Environmental variables\r\n########################\r\n\r\nT = read.csv(""trees.csv"") # T for Tree Cover\r\n\r\nT$GenusSpecies = gsub("" "", ""_"", T$GenusSpecies)\r\nAT = aggregate(trees ~ GenusSpecies, data = T, mean) # AT average tree cover\r\ncolnames(AT) = c(""GenusSpecies"", ""cover"")\r\n\r\nC = read.csv(file = ""climateLat.csv"") # load un-cleaned climate data\r\n\r\nC$GenusSpecies = gsub("" "", ""_"", C$GenusSpecies) \r\nAC = aggregate(cbind(bio_max1, bio_max12) ~ GenusSpecies, data = C, mean) # average climate\r\ncolnames(AC) = c(""GenusSpecies"", ""Temp"", ""Prec"")\r\nAL = aggregate(decimalLatitude ~ GenusSpecies, data = C, mean) # average latitude\r\nALo = aggregate(decimalLongitude ~ GenusSpecies, data = C, mean) # average latitude\r\n\r\nBS = read.csv(file = ""quickBodySize.csv"",sep="";"") # load cleaned body size data\r\n\r\nBS = BS[c(""GenusSpecies"", ""Species"", ""Genus"", ""Family"", ""SubOrder"", ""tbl"", ""hwl"")] #\r\nBS = BS[c(""GenusSpecies"", ""Species"", ""Genus"", ""Family"", ""SubOrder"", ""hwl"")] # drop another variable so that is easy to work with \r\nBS = na.omit(BS) # remove missing \r\n\r\n# load biotic grids \r\nA = read.csv(""Mammals.csv"") # Mammals\r\n\r\nB = read.csv(""Birds.csv"") # load Birds\r\n\r\nB$div_brd = B$all # select Bird variable to analyze \r\nA$div_mam = A$all_spp # select Mammal variable to analyze \r\n\r\nAB = aggregate(div_brd ~ GenusSpecies, data = B, mean) # get average birds diversity \r\nAB$GenusSpecies = gsub("" "", ""_"", AB$GenusSpecies)\r\nD = merge(BS, AB, id = ""GenusSpecies"")\r\nAA = aggregate(div_mam ~ GenusSpecies, data = A, mean) # get average birds diversity \r\nAA$GenusSpecies = gsub("" "", ""_"", AA$GenusSpecies)\r\n\r\nD = merge(D, AA, id = ""GenusSpecies"")\r\nD = merge(D, AC, id = ""GenusSpecies"") # merge diversity with average climates\r\nD = merge(D, AT, id = ""GenusSpecies"") # merge diversity with average climates\r\nD = merge(D, AL, id = ""GenusSpecies"") # merge diversity with average lat\r\nD = merge(D, ALo, id = ""GenusSpecies"") # merge diversity with average long\r\n\r\n# Scale Data\r\nD$Temp = scale(D$Temp)/10\r\nD$Prec = scale(D$Prec)/10\r\nD$div_mam = scale(D$div_mam)\r\nD$div_brd = scale(D$div_brd)\r\nD$cover = scale(D$cover)\r\n\r\ncolnames(D) <- c(""GenusSpecies"", ""Species"",  ""Genus"", ""Family"", ""SubOrder"", ""hwl"",             \r\n                 ""div_brd"", ""div_mam"", ""Temp"", ""Prec"", ""cover"", ""LATITUDE"", ""LONGITUDE"")\r\n\r\nD$tip.label <- D$GenusSpecies\r\n\r\ntree = read.tree(""tree.txt"") # load tree and ape for pgls\r\n\r\nD = D[D$GenusSpecies %in% tree$tip.label, ] # get only those with tip\r\ntip = tree$tip.label[!tree$tip.label %in% D$GenusSpecies] # drop tips without data\r\ntree = drop.tip(tree, tip) # drop missing\r\n\r\nD$Species <- D$GenusSpecies\r\n\r\n########### Taken from Cardillo, M. & Skeels, A. 2016. PLoS ONE\r\n############ \r\n# create a comparative data object using caper:\r\nbanksiadat <- comparative.data(tree, D, na.omit = FALSE, names.col = ""tip.label"")\r\ndf <- banksiadat$data # this ensures a match between phylogeny & data \r\n\r\n############\r\n# create spatial distance matrix:\r\ncentroids <- data.frame(df$LATITUDE, df$LONGITUDE)\r\ncolnames(centroids) <- c(\'LATITUDE\', \'LONGITUDE\')\r\nS <- as.matrix(earth.dist(lats = cbind(centroids$LONGITUDE, \r\n                                       centroids$LATITUDE, dist = TRUE)))    \r\ncolnames(S) <- centroids$X\r\nrownames(S) <- centroids$X\r\nS <- S/max(S) # scale to 0-1\r\n\r\n###########\r\n# create phylogenetic VCV matrix:\r\nV <- vcv.phylo(banksiadat$phy, model = ""OU"")\r\nV <- V/max(V)   # scale to 0-1 \r\n\r\n###########\r\n###### FIT MODELS WITH SPATIAL + PHYLOGENETIC COVARIANCES ######\r\nmod1 <- run.Regress(""Temp"")\r\nmod2 <- run.Regress(""Prec"")\r\nmod3 <- run.Regress(""cover"")\r\nmod4 <- run.Regress(""div_brd"")\r\nmod5 <- run.Regress(""div_mam"")\r\nmod6 <- run.Regress(c(""Temp"", ""Prec""))\r\nmod7 <- run.Regress(c(""Temp"", ""cover""))\r\nmod8 <- run.Regress(c(""Temp"", ""div_brd""))\r\nmod9 <- run.Regress(c(""Temp"", ""div_mam""))\r\nmod10 <- run.Regress(c(""Prec"", ""cover""))\r\nmod11 <- run.Regress(c(""Prec"", ""div_brd""))\r\nmod12 <- run.Regress(c(""Prec"", ""div_mam""))\r\nmod13 <- run.Regress(c(""cover"", ""div_brd""))\r\nmod14 <- run.Regress(c(""cover"", ""div_mam""))\r\nmod15 <- run.Regress(c(""div_brd"", ""div_mam""))\r\nmod16 <- run.Regress(c(""Temp"", ""Prec"", ""cover""))\r\nmod17 <- run.Regress(c(""Temp"", ""Prec"", ""div_brd""))\r\nmod18 <- run.Regress(c(""Temp"", ""Prec"", ""div_mam""))\r\nmod19 <- run.Regress(c(""Temp"", ""cover"", ""div_brd""))\r\nmod20 <- run.Regress(c(""Temp"", ""cover"", ""div_mam""))\r\nmod21 <- run.Regress(c(""Temp"", ""div_brd"", ""div_mam""))\r\nmod22 <- run.Regress(c(""Prec"", ""cover"", ""div_brd""))\r\nmod23 <- run.Regress(c(""Prec"", ""cover"", ""div_mam""))\r\nmod24 <- run.Regress(c(""cover"", ""div_brd"", ""div_mam""))\r\nmod25 <- run.Regress(c(""Temp"", ""Prec"", ""cover"", ""div_brd""))\r\nmod26 <- run.Regress(c(""Temp"", ""Prec"", ""cover"", ""div_mam""))\r\nmod27 <- run.Regress(c(""Prec"", ""cover"", ""div_brd"", ""div_mam""))\r\nmod28 <- run.Regress(c(""Temp"", ""Prec"", ""cover"", ""div_brd"", ""div_mam""))\r\n\r\nmodels <- c(mod1$aic[1], mod2$aic[1], mod3$aic[1], mod4$aic[1], mod5$aic[1], \r\n            mod6$aic[1], mod7$aic[1], mod8$aic[1], mod9$aic[1], mod10$aic[1], \r\n            mod11$aic[1], mod12$aic[1], mod13$aic[1], mod14$aic[1], mod15$aic[1], \r\n            mod16$aic[1], mod17$aic[1], mod18$aic[1], mod19$aic[1], mod20$aic[1], \r\n            mod21$aic[1], mod22$aic[1], mod23$aic[1], mod24$aic[1], mod25$aic[1], \r\n            mod26$aic[1], mod27$aic[1], mod28$aic[1])\r\nmodels\r\n\r\nsort(models)\r\n\r\n########################################################################\r\n\r\n\r\n########################\r\n##### FIGURE 4\r\n########################\r\n\r\nT = read.csv(""trees.csv"") # T for Tree Cover\r\n\r\nT$GenusSpecies = gsub("" "", ""_"", T$GenusSpecies)\r\nAT = aggregate(trees ~ GenusSpecies, data = T, mean) # AT average tree cover\r\ncolnames(AT) = c(""GenusSpecies"", ""cover"")\r\n\r\nC = read.csv(file = ""climateLat.csv"") # load un-cleaned climate data\r\n\r\nC$GenusSpecies = gsub("" "", ""_"", C$GenusSpecies) \r\nAC = aggregate(cbind(bio_max1, bio_max12) ~ GenusSpecies, data = C, mean) # average climate\r\ncolnames(AC) = c(""GenusSpecies"", ""Temp"", ""Prec"")\r\nAL = aggregate(decimalLatitude ~ GenusSpecies, data = C, mean) # average latitude\r\nALo = aggregate(decimalLongitude ~ GenusSpecies, data = C, mean) # average latitude\r\n\r\nBS = read.csv(file = ""quickBodySize.csv"", sep="";"") # load cleaned body size data\r\n\r\nBS = BS[c(""GenusSpecies"", ""Species"", ""Genus"", ""Family"", ""SubOrder"", ""tbl"", ""hwl"")] # drop fwl since often NA\r\nBS = BS[c(""GenusSpecies"", ""Species"", ""Genus"", ""Family"", ""SubOrder"", ""hwl"")] # drop another variable so that is easy to work with \r\nBS = na.omit(BS) # remove missing \r\n\r\n# load biotic grids \r\nA = read.csv(""Mammals.csv"") # Mammals\r\n\r\nB = read.csv(""Birds.csv"") # load Birds\r\n\r\nB$div_brd = B$all # select Bird variable to analyze \r\nA$div_mam = A$all_spp # select Mammal variable to analyze \r\n\r\nAB = aggregate(div_brd ~ GenusSpecies, data = B, mean) # get average birds diversity \r\nAB$GenusSpecies = gsub("" "", ""_"", AB$GenusSpecies)\r\nD = merge(BS, AB, id = ""GenusSpecies"")\r\nAA = aggregate(div_mam ~ GenusSpecies, data = A, mean) # get average birds diversity \r\nAA$GenusSpecies = gsub("" "", ""_"", AA$GenusSpecies)\r\n\r\nD = merge(D, AA, id = ""GenusSpecies"")\r\nD = merge(D, AC, id = ""GenusSpecies"") # merge diversity with average climates\r\nD = merge(D, AT, id = ""GenusSpecies"") # merge diversity with average climates\r\nD = merge(D, AL, id = ""GenusSpecies"") # merge diversity with average lat\r\nD = merge(D, ALo, id = ""GenusSpecies"") # merge diversity with average long\r\n\r\ntree = read.tree(""tree.txt"") # load tree and ape for pgls\r\n\r\ncolnames(D) <- c(""GenusSpecies"", ""Species"",  ""Genus"", ""Family"", ""SubOrder"", ""hwl"",             \r\n                 ""div_brd"", ""div_mam"", ""Temp"", ""Prec"", ""cover"", ""Lat"", ""Lon"")\r\n\r\nD$tip.label <- D$GenusSpecies\r\n\r\nD = D[D$GenusSpecies %in% tree$tip.label, ] # get only those with tip\r\ntip = tree$tip.label[!tree$tip.label %in% D$GenusSpecies] # drop tips without data\r\ntree = drop.tip(tree, tip) # drop missing\r\n\r\nD$Temp = D$Temp/10\r\nD$Prec = D$Prec/10\r\nD$div_mam = scale(D$div_mam)\r\nD$div_brd = scale(D$div_brd)\r\nD$cover = scale(D$cover)\r\n\r\nworld <- map_data(""world"")\r\n\r\n# Temperature\r\npTemp <- ggplot() +\r\n  geom_map(data = world, map = world,\r\n           aes(x = long, y = lat, map_id = region),\r\n           color = ""lightgrey"", fill = ""white"", size = 0.5) +\r\n  geom_point(data = D, aes(x = Lon, y = Lat, color = Temp), \r\n             alpha = 0.5, size = 3) +\r\n  scale_colour_gradient2(high = ""#de2d26"", name = ""Temperature"") + \r\n  theme_void() +\r\n  theme(legend.position = \'top\')\r\n\r\n# Precipitation\r\npPrec <- ggplot() +\r\n  geom_map(data = world, map = world,\r\n           aes(x = long, y = lat, map_id = region),\r\n           color = ""lightgrey"", fill = ""white"", size = 0.5) +\r\n  geom_point(data = D, aes(x = Lon, y = Lat, color = Prec), \r\n             alpha = 0.5, size = 3) +\r\n  scale_colour_gradient2(high = ""#08519c"", name = ""Precipitation"") + \r\n  theme_void() +\r\n  theme(legend.position = \'top\')\r\n\r\n# Tree cover\r\npCover <- ggplot() +\r\n  geom_map(data = world, map = world,\r\n           aes(x = long, y = lat, map_id = region),\r\n           color = ""lightgrey"", fill = ""white"", size = 0.5) +\r\n  geom_point(data = D, aes(x = Lon, y = Lat, color = cover), \r\n             alpha = 0.5, size = 3) +\r\n  scale_colour_gradient2(low = ""white"", mid = \'#99d8c9\', high = ""#006d2c"", name = ""Tree cover"") + \r\n  theme_void() +\r\n  theme(legend.position = \'top\')\r\n\r\n# Birds\r\npBirds <- ggplot() +\r\n  geom_map(data = world, map = world,\r\n           aes(x = long, y = lat, map_id = region),\r\n           color = ""lightgrey"", fill = ""white"", size = 0.5) +\r\n  geom_point(data = D, aes(x = Lon, y = Lat, color = div_brd), \r\n             alpha = 0.5, size = 3) +\r\n  scale_colour_gradient2(low = \'white\', mid = \'#bcbddc\', high = ""#54278f"", name = ""Bird diversity"") + \r\n  theme_void() +\r\n  theme(legend.position = \'top\')\r\n\r\n# Mammals\r\npMammals <- ggplot() +\r\n  geom_map(data = world, map = world,\r\n           aes(x = long, y = lat, map_id = region),\r\n           color = ""lightgrey"", fill = ""white"", size = 0.5) +\r\n  geom_point(data = D, aes(x = Lon, y = Lat, color = div_mam), \r\n             alpha = 0.5, size = 3) +\r\n  scale_colour_gradient2(low = \'white\', mid = \'#cc4c02\', high = ""#993404"", name = ""Mammal diversity"") + \r\n  theme_void() +\r\n  theme(legend.position = \'top\')\r\n\r\n# Odonata\r\npOdo <- ggplot() +\r\n  geom_map(data = world, map = world,\r\n           aes(x = long, y = lat, map_id = region),\r\n           color = ""lightgrey"", fill = ""white"", size = 0.5) +\r\n  geom_point(data = D, aes(x = Lon, y = Lat, color = hwl), \r\n             alpha = 0.5, size = 3) +\r\n  scale_colour_gradient2(high = ""#fd8d3c"", name = ""Hind wing length"") + \r\n  theme_void() +\r\n  theme(legend.position = \'top\')\r\n\r\npdf(\'fig4.pdf\', width = 14, height = 14) \r\ngrid.arrange(pOdo, pTemp, pPrec, pCover, pBirds, pMammals, ncol = 2)\r\ndev.off()\r\n########################################################################\r\n\r\n\r\n######################\r\n##### FIGURE 5\r\n######################\r\nT = read.csv(""trees.csv"") # T for Tree Cover\r\n\r\nT$GenusSpecies = gsub("" "", ""_"", T$GenusSpecies)\r\nAT = aggregate(trees ~ GenusSpecies, data = T, mean) # AT average tree cover\r\ncolnames(AT) = c(""GenusSpecies"", ""cover"")\r\n\r\nC = read.csv(file = ""climateLat.csv"") # load un-cleaned climate data\r\n\r\nC$GenusSpecies = gsub("" "", ""_"", C$GenusSpecies) \r\nAC = aggregate(cbind(bio_max1, bio_max12) ~ GenusSpecies, data = C, mean) # average climate\r\ncolnames(AC) = c(""GenusSpecies"", ""Temp"", ""Prec"")\r\nAL = aggregate(decimalLatitude ~ GenusSpecies, data = C, mean) # average latitude\r\nALo = aggregate(decimalLongitude ~ GenusSpecies, data = C, mean) # average latitude\r\n\r\nBS = read.csv(file = ""quickBodySize.csv"",sep="";"") # load cleaned body size data\r\n\r\nBS = BS[c(""GenusSpecies"", ""Species"", ""Genus"", ""Family"", ""SubOrder"", ""tbl"", ""hwl"")] # drop fwl since often NA\r\nBS = BS[c(""GenusSpecies"", ""Species"", ""Genus"", ""Family"", ""SubOrder"", ""hwl"")] # drop another variable so that is easy to work with \r\nBS = na.omit(BS) # remove missing \r\n\r\n# load biotic grids \r\nA = read.csv(""Mammals.csv"") # Mammals\r\n\r\nB = read.csv(""Birds.csv"") # load Birds\r\n\r\nB$div_brd = B$all # select Bird variable to analyze \r\nA$div_mam = A$all_spp # select Mammal variable to analyze \r\n\r\nAB = aggregate(div_brd ~ GenusSpecies, data = B, mean) # get average birds diversity \r\nAB$GenusSpecies = gsub("" "", ""_"", AB$GenusSpecies)\r\nD = merge(BS, AB, id = ""GenusSpecies"")\r\nAA = aggregate(div_mam ~ GenusSpecies, data = A, mean) # get average birds diversity \r\nAA$GenusSpecies = gsub("" "", ""_"", AA$GenusSpecies)\r\n\r\nD = merge(D, AA, id = ""GenusSpecies"")\r\nD = merge(D, AC, id = ""GenusSpecies"") # merge diversity with average climates\r\nD = merge(D, AT, id = ""GenusSpecies"") # merge diversity with average climates\r\nD = merge(D, AL, id = ""GenusSpecies"") # merge diversity with average lat\r\nD = merge(D, ALo, id = ""GenusSpecies"") # merge diversity with average long\r\n\r\ntree = read.tree(""tree.txt"") # load tree and ape for pgls\r\n\r\ncolnames(D) <- c(""GenusSpecies"", ""Species"",  ""Genus"", ""Family"", ""SubOrder"", ""hwl"",             \r\n                 ""div_brd"", ""div_mam"", ""Temp"", ""Prec"", ""cover"", ""Lat"", ""Lon"")\r\n\r\nD$Temp = scale(D$Temp)\r\nD$Prec = scale(D$Prec)\r\nD$div_mam = scale(D$div_mam)\r\nD$div_brd = scale(D$div_brd)\r\nD$cover = scale(D$cover)\r\n\r\npT <- ggplot(D, aes(Temp, hwl)) +\r\n  geom_point(alpha = 0.5, col = ""#de2d26"") + geom_smooth(method = \'lm\', col = \'black\') +\r\n  theme_bw() + scale_fill_manual(values = c(""#0190b6"", ""#e13d14"")) +\r\n  theme(strip.background = element_rect(fill = ""#f1e4d7"")) +\r\n  xlab(""Temperature"") + ylab(""Hind wing length (mm)"") +\r\n  theme(legend.title = element_blank(),\r\n        text = element_text(size = 14)) + ylim(0, 62) +\r\n  annotate(\'text\', x = 1.9, y = 60, label = ""A"", size = 7)\r\n\r\npB <- ggplot(D, aes(div_brd, hwl)) +\r\n  geom_point(alpha = 0.5, col = ""#54278f"") + geom_smooth(method = \'lm\', col = \'black\') +\r\n  theme_bw() + scale_fill_manual(values = c(""#0190b6"", ""#e13d14"")) +\r\n  theme(strip.background = element_rect(fill = ""#f1e4d7"")) +\r\n  xlab(""Bird diversity"") + ylab(""Hind wing length (mm)"") +\r\n  theme(legend.title = element_blank(),\r\n        text = element_text(size = 14)) + ylim(0, 62) +\r\n  annotate(\'text\', x = 2.5, y = 60, label = ""B"", size = 7)\r\n\r\npdf(\'fig5.pdf\', width = 5, height = 10) \r\ngrid.arrange(pT, pB, ncol = 1)\r\ndev.off()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n']","Data from: Out of the tropics: Macroevolutionary size trends in an old insect order are shaped by temperature and predators Global body size distributions are shaped by selection pressures arising from biotic and abiotic factors such as temperature, predation and parasitism. Here, we investigated the ecological and evolutionary drivers of global latitudinal size gradients in an old insect order (Odonata; dragonflies and damselflies). Phylogenetic comparative analyses revealed that global size variation of extant taxa is negatively influenced by both regional avian diversity and temperature. Interestingly, fossil data show that the relationship between wing size and latitude has shifted: latitudinal size trends had initially negative slopes but became shallower or positive following the emergence of birds 150 MYA. These changing size-latitude trends over geological time were likely driven by bird predation and high dispersal ability of large dragonflies. Our results therefore suggest that latitudinal size gradients were shaped by temperature but also by predators driving the dispersal of large-sized clades out of the tropics and in to the temperate zone.",3
Data from: Testing potential mechanisms of conspecific sperm precedence in Drosophila pseudoobscura,"Drosophila pseudoobscura females that co-occur with sister species D. persimilis show elevated fertilization by conspecific sperm when they mate with both a heterospecific and a conspecific male. This phenomenon, known as conspecific sperm precedence (CSP), has evolved as a mechanism to avoid maladaptive hybridization with D. persimilis. In this study, we assessed pericopulatory (during mating) and postcopulatory (after mating) traits in crosses with sympatric or allopatric D. pseudoobscura females and conspecific or heterospecific males to evaluate potential mechanisms of CSP in this system. We observed shorter copulation duration in crosses with sympatric females, but found no difference in quantity of sperm transferred or female reproductive tract toxicity between sympatry and allopatry. Our data show some support for the hypothesis that parasperm, a short, sterile sperm morph, can protect fertile eusperm from the D. pseudoobscura female reproductive tract, though it is unclear how this might affect patterns of sperm use in sympatry vs. allopatry. Overall, these results suggest that copulation duration could potentially contribute to the elevated CSP observed in sympatry.","['library(lme4)\nlibrary(lmerTest)\nlibrary(aod)\nlibrary(beeswarm)\nlibrary(car)\nlibrary(influence.ME)\n\n\n## import data ##\n#all data combined (control, 30 min, & 2 hour exposure)\ndatatotal <- read.csv(""CSP_mechanisms_data.csv"")\n\n#all control crosses\ncontrol <- subset(datatotal, treatment == (""Control""))\n\n#only 2 hr exposure\ndata2 <- subset(datatotal, time == ""2hr"")\n\n#2 hr exposure divided into control (left in GIM) & treatment (left in female)\ncontrol2 <- subset(data2, treatment == (""Control""))\ntreatment2 <- subset(data2, treatment == (""Treatment""))\n\n#2 hr exposure divided into heterospecific and conspecific crosses\nhet2 <- subset(data2, species == (""Heterospecific""))\ncon2 <- subset(data2, species == (""Conspecific""))\n\n#only 30 min exposure\ndata30 <- subset(datatotal, time == ""30min"")\n\n#testes dissections + 2 hour controls\ntestes2 <- read.csv(""testes_2hr.csv"")\n\n#scored immediately after mating + 2 hour controls\naftermating2 <- read.csv(""aftermating_2hr.csv"")\n\n\n\n## set pathway for where to save figures ##\npath = ""~/Desktop""\n\n\n## functions to test for overdispersion & to use quasi-likelihood ##\noverdisp_fun <- function(model) {\n  rdf <- df.residual(model)\n  rp <- residuals(model,type=""pearson"")\n  Pearson.chisq <- sum(rp^2)\n  prat <- Pearson.chisq/rdf\n  pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)\n  c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)\n}\n\nquasi_table <- function(model,ctab=coef(summary(model)),\n                        phi=overdisp_fun(model)[""ratio""]) {\n  qctab <- within(as.data.frame(ctab),\n                  {   `Std. Error` <- `Std. Error`*sqrt(phi)\n                  `z value` <- Estimate/`Std. Error`\n                  `Pr(>|z|)` <- 2*pnorm(abs(`z value`), lower.tail=FALSE)\n                  })\n  return(qctab)\n}\n\n\n\n## figure 2 ##\n\n#code for figure 2: a) copulation duration, b) sperm transfer\n\nfemaletype = factor(datatotal$female_type, levels(as.factor(datatotal$female_type))[c(2,1)])\nprint(levels(femaletype))\n\nmale_species = factor(datatotal$species, levels(as.factor(datatotal$species))[c(2,1)])\nprint(levels(male_species))\n\nfemaletypec = factor(control$female_type, levels(as.factor(control$female_type))[c(2,1)])\nprint(levels(femaletypec))\n\nmale_speciesc = factor(control$species, levels(as.factor(control$species))[c(2,1)])\nprint(levels(male_speciesc))\n\nlabels1 <- c("""", """", """","""")\n\n\npng(filename=paste(path,""/figure2.png"",sep=""""), width=14, height=7,\n    units=""in"", res=300, pointsize=14)\n\npar(mfrow=c(1,2), mar = c(2, 5.5, 2, 2), oma = c(3,1,0,0))\nbeeswarm(duration~femaletype + male_species, data = datatotal,\n         cex = 1.1, pch = c(17,17,19,19), \n         labels = labels1, \n         xlab = """", ylab = """", \n         cex.axis = 1.4,\n         col = c(\'magenta4\', \'goldenrod1\'))\nbxplot(duration~femaletype + male_species, data = datatotal, add = T)\nlegend(""topright"", cex = 1.4, bty = ""n"", legend=c(""Sympatric"", ""Allopatric""), \n       inset = 0.03 ,fill=c(\'magenta4\', \'goldenrod1\'))\nmtext(text = ""Male type"", side = 1, line = 3.5, cex = 2)\nmtext(text = ""Copulation duration (min)"", side = 2, line = 4, cex = 2)\nmtext(text = ""Heterospecific       Conspecific"", side = 1, line = 1, cex = 1.6)\nmtext(text = ""A"", side = 1, line = -23.4, cex = 2, adj = -0.15, xpd = NA)\n\nbeeswarm(sperm_total ~ femaletypec + male_speciesc, data = control,\n         cex = 1.1, pch = c(17,17,19,19), \n         labels = labels1, ylim = c(10, 240),\n         xlab = """", ylab = """", \n         cex.axis = 1.4, \n         col = c(\'magenta4\', \'goldenrod1\'))\nbxplot(sperm_total ~ femaletypec + male_speciesc, data = control, add = T)\nmtext(text = ""Male type"", side = 1, line = 3.5, cex = 2)\nmtext(text = ""Sperm count"", side = 2, line = 4, cex = 2)\nmtext(text = ""Heterospecific       Conspecific"", side = 1, line = 1, cex = 1.6)\nmtext(text = ""B"", side = 1, line = -23.4, cex = 2, adj = -0.15, xpd = NA)\n\ndev.off()\n\n\n#statistical analysis for copulation duration (figure 2a)\n\nfemaletype = factor(datatotal$female_type, levels(as.factor(datatotal$female_type))[c(2,1)])\nprint(levels(femaletype))\n\nmalespecies = factor(datatotal$species, levels(as.factor(datatotal$species))[c(2,1)])\nprint(levels(malespecies))\n\nduration <- lmer(duration ~ femaletype + (1 | female_line) + malespecies + femaletype:malespecies, data = datatotal)\nsummary(duration)\n\n\n#statistical analysis for quantity of sperm transferred (figure 2b)\n\nspermtransfer <- lmer(sperm_total ~ female_type + (1 | female_line) + species + duration, data = control)\nsummary(spermtransfer)\n\nspermtransfer2 <- lmer(sperm_total ~ female_type + (1 | female_line) + species, data = control)\nsummary(spermtransfer2)\n\nspermcontrol <- subset(data2, treatment == (""Control""))\nspermtreatment <- subset(data2, treatment == (""Treatment""))\n\nt.test(spermcontrol$sperm_total, spermtreatment$sperm_total)\n\n\n\n\n## figure 3 ##\n\n#code for figure 3: eusperm viability, a) heterospecific, b) conspecific\n\nfemaletype2h = factor(het2$female_type, levels(as.factor(het2$female_type))[c(2,1)])\nprint(levels(femaletype2h))\n\nfemaletype2c = factor(con2$female_type, levels(as.factor(con2$female_type))[c(2,1)])\nprint(levels(femaletype2c))\n\nlabels1 <- c("""", """", """","""")\n\npng(filename=paste(path,""/figure3.png"",sep=""""), width=14, height=7,\n    units=""in"", res=300, pointsize=14)\n\npar(mfrow=c(1,2), mar = c(2, 5.5, 2, 2), oma = c(3,1,0,0))\nbeeswarm(percent_live_eu~treatment*femaletype2h, data = het2,\n         cex = 1.1, pch = c(2,17,2,17), \n         labels = labels1, \n         ylim = c(0.05, 0.95), \n         xlab = """", ylab = """", \n         cex.axis = 1.4, \n         col = c(\'magenta4\', \'magenta4\', \'goldenrod1\', \'goldenrod1\'))\nbxplot(percent_live_eu~treatment*femaletype2h, data = het2, add = T)\nlegend(x = 2, y = 0.35, cex = 1.4, bty = ""n"", legend=c(""No FRT exposure"", ""2 hr FRT exposure""), \n       fill=c(\'white\', \'black\'))\nlegend(x = 2, y = 0.2, cex = 1.4, bty = ""n"", legend=c(""Sympatric"", ""Allopatric""), \n       fill=c(\'magenta4\', \'goldenrod1\'))\nmtext(text = ""Heterospecific matings"", side = 1, line = 2, cex = 2)\nmtext(text = ""Proportion of live eusperm"", side = 2, line = 4, cex = 2)\nmtext(text = ""A"", side = 1, line = -23.2, cex = 2, adj = -0.15, xpd = NA)\n\nbeeswarm(percent_live_eu~treatment*femaletype2c, data = con2,\n         cex = 1.1, pch = c(1,19,1,19), \n         labels = labels1, \n         ylim = c(0.05, 0.95), \n         xlab = """", ylab = """", \n         cex.axis = 1.4, \n         col = c(\'magenta4\', \'magenta4\', \'goldenrod1\', \'goldenrod1\'))\nbxplot(percent_live_eu~treatment*femaletype2c, data = con2, add = T)\nlegend(x = 2, y = 0.35, cex = 1.4, bty = ""n"", legend=c(""No FRT exposure"", ""2 hr FRT exposure""), \n       fill=c(\'white\', \'black\'))\nlegend(x = 2, y = 0.2, cex = 1.4, bty = ""n"", legend=c(""Sympatric"", ""Allopatric""), \n       fill=c(\'magenta4\', \'goldenrod1\'))\nmtext(text = ""Conspecific matings"", side = 1, line = 2, cex = 2)\nmtext(text = ""B"", side = 1, line = -23.2, cex = 2, adj = -0.15, xpd = NA)\n\ndev.off()\n\n\n#statistical analysis for eusperm viability of 2 hour matings (figure 3)\n\neusperm2 <- data2[,27:28]\neusperm2 <- data.matrix(eusperm2)\n\neusperm <- glmer(eusperm2 ~ female_type + (1 | female_line) + species + treatment, data = data2, family = ""binomial"")\nsummary(eusperm)\noverdisp_fun(eusperm)\n\neuspermB<-betabin(formula=cbind(data2[,27],data2[,28])~female_type+species+treatment,random=~female_line,data=data2)\nsummary(euspermB)\n\n\n\n\n## figure 4 ##\n\n#code for figure 4: parasperm proportion & eusperm viability\n\npara_treatment <- lm(percent_live_eu ~ percent_parasperm, data = treatment2)\n\npng(filename=paste(path,""/figure4.png"",sep=""""), width=10, height=8,\n    units=""in"", res=300, pointsize=14)\n\npar(mfrow=c(2,1), mar = c(2, 2, 2.5, 2), oma = c(3,4,0,9))\nplot(percent_live_eu ~ percent_parasperm, data = treatment2, xlab = """", ylab = """",\n     cex = 1.1, cex.axis = 1.2, xlim = c(0, 0.9), ylim = c(0.43,0.95),\n     col = c(\'magenta4\', \'goldenrod1\')[as.factor(treatment2$female_type)], \n     pch = c(19,17)[as.factor(treatment2$species)])\nabline(para_treatment, col = ""black"")\nlegend(x = 0.95, y = 1, cex = 1.3, bty = ""n"", legend=c(""Heterospecific"", ""Conspecific""), \n       pch =c(17,19), xpd = NA)\nlegend(x = 0.94, y = 0.85, cex = 1.3, bty = ""n"", legend=c(""Sympatric"", ""Allopatric""), \n       fill=c(\'magenta4\', \'goldenrod1\'), xpd = NA)\nmtext(text = ""A. 2 hour FRT Exposure"", side = 1, line = -12.7, cex = 1.4, adj = 0.01)\n\nplot(percent_live_eu ~ percent_parasperm, data = control2, xlab = """", ylab = """",\n     cex = 1.1, cex.axis = 1.2, xlim = c(0, 0.9), ylim = c(0.43,0.95),\n     col = c(\'magenta4\', \'goldenrod1\')[as.factor(control2$female_type)], \n     pch = c(1,2)[as.factor(control2$species)])\nmtext(text = ""Parasperm proportion"", side = 1, line = 3.5, cex = 1.8)\nmtext(text = ""Proportion of live eusperm"", side = 2, line = 4, cex = 1.8, adj = -0.8)\nmtext(text = ""B. No FRT Exposure"", side = 1, line = -12.7, cex = 1.4, adj = 0.01)\n\ndev.off()\n\n\n#statistical analysis for effect of parasperm proportion on eusperm viability (figure 4)\n\neusperm2 <- data2[,27:28]\neusperm2 <- data.matrix(eusperm2)\n\neupara2 <- glmer(eusperm2 ~ treatment:percent_parasperm + (1|female_line), data = data2, family = ""binomial"")\nsummary(eupara2)\noverdisp_fun(eupara2)\n\neuparaB2<-betabin(formula=cbind(data2[,27],data2[,28]) ~ treatment:percent_parasperm, random=~female_line, data = data2)\nsummary(euparaB2)\n\n\n\n\n## figure S1 ##\n\n#code for figure S1: testes controls\n\nprint(levels(testes2$male_line))\nmaleline = factor(testes2$male_line, levels(as.factor(testes2$male_line))[c(2,4,3,1,5)])\nprint(levels(maleline))\n\nprint(levels(testes2$control_type))\ncontrol_type = factor(testes2$control_type, levels(as.factor(testes2$control_type))[c(2,1)])\nprint(levels(control_type))\n\nlabels3 = c("""","""","""","""","""","""","""","""","""","""")\n\npng(filename=paste(path,""/figureS1.png"",sep=""""), width=8, height=4,\n    units=""in"", res=300, pointsize=7)\n\npar(mfrow = c(1,1), mar = c(6, 6, 1, 2), oma = c(0,0,0,13.5))\nbeeswarm(testes2$prop_live_eu~control_type*maleline, \n         cex = 1.6, pch = c(17,2,19,1,19,1,19,1,19,1),\n         labels = labels3, corral = ""gutter"",\n         ylim = c(0.05, 0.95), \n         xlab = """", ylab = """", \n         cex.axis = 1.6, \n         col = c(\'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\'))\nbxplot(testes2$prop_live_eu~control_type*maleline, add = T)\nlegend(x = 11.1, y = 1, cex = 1.7, bty = ""n"", legend=c(""Heterospecific"", ""Conspecific""), \n       pch =c(17,19), xpd = NA)\nlegend(x = 11, y = 0.85, cex = 1.7, bty = ""n"", legend=c(""Testes"",""2 hour control""), \n       fill=c(\'black\', \'white\'), xpd = NA)\nlegend(x = 11, y = 0.7, cex = 1.7, bty = ""n"", legend=c(""Sympatric"", ""Allopatric""), fill = c(\'magenta4\', \'goldenrod1\'), xpd = NA)\nmtext(text = ""Proportion of live eusperm"", side = 2, line = 4, cex = 2.4)\nmtext(text = ""MSH         Sierra          MSH         Lamoille        Zion"", side = 1, line = 1, cex = 2)\nmtext(text = ""Male line"", side = 1, line = 4, cex = 2.4)\nabline(v = 2.5, lty = 2, col = ""gray"")\n\ndev.off()\n\n\n#statistical analysis for testes vs. 2 hour controls (figure S1)\n\ntestes <- testes2[,10:11]\ntestes <- data.matrix(testes)\n\ntestesm <- glmer(testes ~ species + control_type + (1|male_line), data = testes2, family = ""binomial"")\nsummary(testesm)\noverdisp_fun(testesm)\n\ntestesb<-betabin(formula=cbind(testes2[,10],testes2[,11]) ~ species + control_type, random=~male_line, data = testes2)\nsummary(testesb)\n\n\n\n\n## figure S2 ##\n\n#code for figure S2: after mating controls\nprint(levels(as.factor(aftermating2$cross_type)))\ncrosstype = factor(aftermating2$cross_type, levels(as.factor(aftermating2$cross_type))[c(4,3,2,1)])\nprint(levels(crosstype))\n\nprint(levels(aftermating2$control_type))\ncontrol_type2 = factor(aftermating2$control_type, levels(as.factor(aftermating2$control_type))[c(2,1)])\nprint(levels(control_type2))\n\nlabels4 = c("""","""","""","""","""","""","""","""")\n\npng(filename=paste(path,""/figureS2.png"",sep=""""), width=7, height=4,\n    units=""in"", res=300, pointsize=7)\n\npar(mfrow = c(1,1), mar = c(6, 6, 1, 2), oma = c(0,0,0,13.5))\nbeeswarm(aftermating2$prop_live_eu ~ control_type2*crosstype,\n         cex = 1.6, pch = c(17,2,19,1,17,2,19,1), \n         labels = labels4, \n         ylim = c(0.05, 0.95), \n         xlab = """", ylab = """", \n         cex.axis = 1.6, \n         col = c(\'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\'))\nbxplot(aftermating2$prop_live_eu ~ control_type2*crosstype, add = T)\nlegend(x = 9.1, y = 1, cex = 1.7, bty = ""n"", legend=c(""Heterospecific"", ""Conspecific""), \n       pch =c(17,19), xpd = NA)\nlegend(x = 9, y = 0.85, cex = 1.7, bty = ""n"", legend=c(""0 hour control"",""2 hour control""), \n       fill=c(\'black\', \'white\'), xpd = NA)\nlegend(x = 9, y = 0.7, cex = 1.7, bty = ""n"", legend=c(""Sympatric"", ""Allopatric""), fill = c(\'magenta4\', \'goldenrod1\'), xpd = NA)\nmtext(text = ""Proportion of live eusperm"", side = 2, line = 4, cex = 2.4)\nmtext(text = ""MSH                                Lamoille"", side = 1, line = 1, cex = 2)\nmtext(text = ""Female line"", side = 1, line = 4, cex = 2.4)\n\ndev.off()\n\n\n#statistical analysis for immediately after mating vs. 2 hour controls (figure S2)\n\naftermating <- aftermating2[,16:17]\naftermating <- data.matrix(aftermating)\n\naftermatingm <- glmer(aftermating ~ species + control_type + (1|female_line), data = aftermating2, family = ""binomial"")\nsummary(aftermatingm)\noverdisp_fun(aftermatingm)\n\naftermatingb<-betabin(formula=cbind(aftermating2[,16],aftermating2[,17]) ~ species + control_type, random=~female_line, data = aftermating2)\nsummary(aftermatingb)\n\n\n\n\n## figure S3 ##\n\n#code for figure S3: eusperm viability across lines (2 hour exposure)\n\nprint(levels(con2$female_line))\nfemalelineC = factor(con2$female_line, levels(as.factor(con2$female_line))[c(3,2,1,4)])\nprint(levels(femalelineC))\n\nprint(levels(het2$female_line))\nfemalelineH = factor(het2$female_line, levels(as.factor(het2$female_line))[c(3,2,1,4)])\nprint(levels(femalelineH))\n\nlabels1 <- c("""", """", """", """", """", """", """", """")\n\npng(filename=paste(path,""/figureS3.png"",sep=""""), width=10, height=8,\n    units=""in"", res=300, pointsize=14)\n\npar(mfrow=c(2,1), mar = c(2, 2, 2.5, 2), oma = c(3,4,0,11))\nbeeswarm(percent_live_eu~treatment*femalelineH, data = het2, \n         cex = 1.1, pch = c(2,17,2,17,2,17,2,17), \n         labels = labels1, ylim = c(0.15, 0.95), \n         xlab = """", ylab = """", cex.axis = 1.2, \n         col = c(\'magenta4\', \'magenta4\',\'magenta4\', \'magenta4\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\'))\nbxplot(percent_live_eu~treatment*femalelineH, data = het2, add=T)\nlegend(x = 9, y = 1, cex = 1.3, bty = ""n"", legend=c(""No FRT exposure"", ""2 hr FRT exposure""), \n       fill=c(\'white\', \'black\'), xpd = NA)\nlegend(x = 9, y = 0.75, cex = 1.3, bty = ""n"", legend=c(""Sympatric"", ""Allopatric""), \n       fill=c(\'magenta4\', \'goldenrod1\'), xpd = NA)\nmtext(text = ""A. Heterospecific"", side = 1, line = -12.7, cex = 1.4, adj = -0.05)\nmtext(text = ""Sierra         MSH        Lamoille       Zion"", side = 1, line = 0.9, cex = 1.4)\n\nbeeswarm(percent_live_eu~treatment*femalelineC, data = con2, \n         cex = 1.1, pch = c(1,19,1,19,1,19,1,19), \n         labels = labels1, ylim = c(0.15, 0.95), \n         xlab = """", ylab = """", cex.axis = 1.2, \n         col = c(\'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\'))\nbxplot(percent_live_eu~treatment*femalelineC, data = con2, add=T)\nmtext(text = ""B. Conspecific"", side = 1, line = -12.7, cex = 1.4, adj = -0.05)\nmtext(text = ""Sierra         MSH        Lamoille       Zion"", side = 1, line = 0.9, cex = 1.4)\nmtext(text = ""Female line"", side = 1, line = 3, cex = 1.8)\nmtext(text = ""Proportion of live eusperm"", adj = -0.8, side = 2, line = 4, cex = 1.8)\n\ndev.off()\n\n\n#statistical analysis for eusperm viability: variation among female lines (figure S3)\n\neusperm2 <- data2[,27:28]\neusperm2 <- data.matrix(eusperm2)\n\neu <- glm(eusperm2 ~0 + female_line + species + treatment, data = data2, family = ""binomial"")\n\noverdisp_fun(eu)\nquasi_table(eu)\n\nwald.test(b=coef(eu), Sigma=vcov(eu)*2.4935, Terms=1:4)\n\n\n\n\n## figure S4 ##\n\n#code for figure S4: all sperm viability (30 minute exposure)\n\nprint(levels(as.factor(data30$female_line)))\nfemaleline = factor(data30$female_line, levels(as.factor(data30$female_line))[c(6,5,3,4,1,2,7,8)])\nprint(levels(femaleline))\n\nlabels2 = c("""","""","""","""","""","""","""","""","""","""","""","""","""","""","""","""")\n\npng(filename=paste(path,""/figureS4.png"",sep=""""), width=8, height=4,\n    units=""in"", res=300, pointsize=7)\n\npar(mfrow = c(1,1), mar = c(8, 6, 1, 2), oma = c(0,0,0,15))\nbeeswarm(percent_live ~ treatment*femaleline, data = data30,\n         cex = 1.6, pch = c(2,17,2,17,2,17,2,17,2,17,2,17,2,17,2,17), \n         labels = labels2, corral = ""gutter"",\n         ylim = c(0.05, 0.95), \n         xlab = """", ylab = """", \n         cex.axis = 1.6, \n         col = c(\'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\'))\nbxplot(percent_live ~ treatment*femaleline, data = data30, add = T)\nlegend(x = 17, y = 1, cex = 1.7, bty = ""n"", legend=c(""No FRT exposure"", ""30 min FRT exposure""), \n       fill=c(\'white\', \'black\'), xpd = NA)\nlegend(x = 17, y = 0.85, cex = 1.7, bty = ""n"", legend=c(""Sympatric"", ""Allopatric""),\n       fill=c(\'magenta4\', \'goldenrod1\'), xpd = NA)\nmtext(text = ""Proportion of live sperm"", side = 2, line = 4, cex = 2.4)\nmtext(text = ""Line 1    Line 2    Line 3    Line 4    Line 5    Line 6    Line 7    Line 8"", side = 1, line = 1, cex = 1.7)\nmtext(text = ""Sierra               MSH             Lamoille             Zion"", side = 1, line = 3.5, cex = 2)\nmtext(text = ""Female line"", side = 1, line = 6, cex = 2.4)\nabline(v = c(2.5, 4.5, 6.5, 8.5, 10.5, 12.5, 14.5), lty = 2, col = ""gray"")\n\ndev.off()\n\n\n#statistical analysis for total sperm viability: 30 minutes (figure S4)\n\nviability30m <- glmer(formula=cbind(data30[,21],data30[,20]) ~ female_type + (1|female_line), data = data30, family = ""binomial"")\nsummary(viability30m)\noverdisp_fun(viability30m)\n\nviabilityb<-betabin(formula=cbind(data30[,21],data30[,20]) ~ female_type, random=~female_line, data = data30)\nsummary(viabilityb)\n\n\n\n\n## figure S5 ##\n\n#code for figure S5: parasperm proportion (spermblue data)\n\nprint(levels(as.factor(data30$female_line)))\nfemaleline = factor(data30$female_line, levels(as.factor(data30$female_line))[c(6,5,3,4,1,2,7,8)])\nprint(levels(femaleline))\n\nlabels2 = c("""","""","""","""","""","""","""","""","""","""","""","""","""","""","""","""")\n\npng(filename=paste(path,""/figureS5.png"",sep=""""), width=8, height=4,\n    units=""in"", res=300, pointsize=7)\n\npar(mfrow = c(1,1), mar = c(8, 6, 1, 2), oma = c(0,0,0,15))\nbeeswarm(percent_parasperm ~ treatment*femaleline, data = data30,\n         cex = 1.6, pch = c(2,17,2,17,2,17,2,17,2,17,2,17,2,17,2,17), \n         labels = labels2, corral = ""gutter"",\n         ylim = c(0.05, 0.95), \n         xlab = """", ylab = """", \n         cex.axis = 1.6, \n         col = c(\'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'magenta4\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\', \'goldenrod1\'))\nbxplot(percent_parasperm ~ treatment*femaleline, data = data30, add = T)\nlegend(x = 17, y = 1, cex = 1.7, bty = ""n"", legend=c(""No FRT exposure"", ""30 min FRT exposure""), \n       fill=c(\'white\', \'black\'), xpd = NA)\nlegend(x = 17, y = 0.85, cex = 1.7, bty = ""n"", legend=c(""Sympatric"", ""Allopatric""),\n       fill=c(\'magenta4\', \'goldenrod1\'), xpd = NA)\nmtext(text = ""Proportion of parasperm"", side = 2, line = 4, cex = 2.4)\nmtext(text = ""Line 1    Line 2    Line 3    Line 4    Line 5    Line 6    Line 7    Line 8"", side = 1, line = 1, cex = 1.7)\nmtext(text = ""Sierra               MSH             Lamoille             Zion"", side = 1, line = 3.5, cex = 2)\nmtext(text = ""Female line"", side = 1, line = 6, cex = 2.4)\nabline(v = c(2.5, 4.5, 6.5, 8.5, 10.5, 12.5, 14.5), lty = 2, col = ""gray"")\n\ndev.off()\n\n\n#statistical analysis for parasperm proportion across female lines (figure S5)\n\nparasperm30 <- data30[,23:24]\nparasperm30 <- data.matrix(parasperm30)\n\npara <- glmer(parasperm30 ~ treatment + (1|female_line), data = data30, family = ""binomial"")\nsummary(para)\noverdisp_fun(para)\n\nparab <- betabin(formula=cbind(data30[,23],data30[,24]) ~ treatment, random=~female_line, data = data30)\nsummary(parab)\n\nparabfemale <- betabin(formula=cbind(data30[,23],data30[,24]) ~ female_type, random=~female_line, data = data30)\nsummary(parabfemale)\n\n\n\n\n## figure S6 ##\n\n#code for figure S6: eusperm viability & parasperm proportion\n\neuspermt <- treatment2[,27:28]\neuspermt <- data.matrix(euspermt)\n\neuspermc <- control2[,27:28]\neuspermc <- data.matrix(euspermc)\n\neupara_treatment <- glmer(euspermt ~ female_type + (1 | female_line) + species + percent_parasperm, data = treatment2, family = ""binomial"")\neupara_control <- glmer(euspermc ~ female_type + (1 | female_line) + species + percent_parasperm, data = control2, family = ""binomial"")\n\npng(filename=paste(path,""/figureS6.png"",sep=""""), width=10, height=8,\n    units=""in"", res=300, pointsize=14)\n\npar(mfrow=c(2,1), mar = c(2, 2, 2.5, 2), oma = c(3,4,0,9))\nplot(x = treatment2$percent_parasperm, y = fitted(eupara_treatment), \n     xlab = """", ylab = """",\n     xlim = c(0.05,0.85), ylim = c(0.6,0.9), \n     cex = 1.1, cex.axis = 1.2, \n     col = c(\'goldenrod1\',\'magenta4\')[as.factor(treatment2$female_type)], \n     pch = c(19,17)[as.factor(treatment2$species)])\nmtext(text = ""A. 2 hour FRT Exposure"", side = 1, line = -12.7, cex = 1.4, adj = 0.01)\nlegend(x = 0.91, y = 0.93, cex = 1.3, bty = ""n"", legend=c(""Heterospecific"", ""Conspecific""), \n       pch =c(17,19), xpd = NA)\nlegend(x = 0.9, y = 0.85, cex = 1.3, bty = ""n"", legend=c(""Sympatric"", ""Allopatric""), \n       fill=c(\'magenta4\', \'goldenrod1\'), xpd = NA)\n\nplot(x = control2$percent_parasperm, y = fitted(eupara_control), \n     xlab = """", ylab = """",\n     xlim = c(0.05,0.85), ylim = c(0.6,0.9), \n     cex = 1.1, cex.axis = 1.2,\n     col = c(\'goldenrod1\',\'magenta4\')[as.factor(treatment2$female_type)], \n     pch = c(1,2)[as.factor(treatment2$species)])\nmtext(text = ""Proportion of live eusperm (model fitted values)"", adj = 0.12, side = 2, line = 4, cex = 1.8)\nmtext(text = ""Parasperm proportion"", side = 1, line = 3.5, cex = 1.8)\nmtext(text = ""B. No FRT Exposure"", side = 1, line = -12.7, cex = 1.4, adj = 0.01)\n\ndev.off()\n\n\n#statistical analysis for parasperm proportion vs. eusperm viability (figure S6; same analysis as figure 3)\n\neupara2 <- glmer(eusperm2 ~ treatment:percent_parasperm + (1|female_line), data = data2, family = ""binomial"")\nsummary(eupara2)\noverdisp_fun(eupara2)\n\neuparaB2<-betabin(formula=cbind(data2[,27],data2[,28]) ~ treatment:percent_parasperm, random=~female_line, data = data2)\nsummary(euparaB2)\n\n\n\n\n## influential outlier analysis ##\n\n#figure 2a: copulation duration\n\ndataf1a<-data.frame(datatotal$duration,datatotal$female_line,femaletype,malespecies)\ncolnames(dataf1a)<-c(""duration"", ""female_line"", ""femaletype"", ""malespecies"")\ndurationsub<-lmer(duration ~ femaletype + (1 | female_line) + malespecies + femaletype:malespecies, data = dataf1a)\n\n#effect of female line outliers\n\nfl<-influence(durationsub,""female_line"")\n\nplot(fl, which=""dfbetas"", parameters=c(2), xlab=""DFbetaS"",ylab=""Female Line"")\n2/sqrt(8)\ndfbetas(fl, parameters=c(2))\n\nplot(fl, which=""cook"", cutoff=4/8, sort=TRUE, xlab=""Cooks Distance"", ylab=""Female Line"")\nsigtest(fl,test=1.96,)\n\ndsMSH68<-exclude.influence(durationsub,""female_line"", ""MSH68"")\n\n#effect of individual observation level outliers\n\nflo<-influence(durationsub, obs=TRUE)\n\nplot(flo,which=""cook"",cutoff=4/160,sort=TRUE, xlab=""Cook\'s Distance"", ylab=""Individual Observation"")\ncooks.distance(flo,sort=T)\n\nsummary(durationsub)\nsigtest(flo,test=1.96)$femaletypeAllopatric[c(79,158,29,50),]\nsigtest(flo,test=1.96)$malespeciesConspecific[c(79,158,29,50),]\n\ndataf1a79<-dataf1a[-79,]\ndataf1a158<-dataf1a[-158,]\ndataf1a29<-dataf1a[-29,]\ndataf1a50<-dataf1a[-50,]\nsummary(durationsub)\nsummary(lmer(duration ~ femaletype + (1 | female_line) + malespecies + femaletype:malespecies, data = dataf1a79))\nsummary(lmer(duration ~ femaletype + (1 | female_line) + malespecies + femaletype:malespecies, data = dataf1a158))\nsummary(lmer(duration ~ femaletype + (1 | female_line) + malespecies + femaletype:malespecies, data = dataf1a29))\nsummary(lmer(duration ~ femaletype + (1 | female_line) + malespecies + femaletype:malespecies, data = dataf1a50))\n\n\n#figure 2b: sperm transfer\n\ndataf1b<-data.frame(control$sperm_total,control$female_line,control$female_type,control$species,control$duration)\ncolnames(dataf1b)<-c(""sperm_total"", ""female_line"", ""female_type"", ""species"", ""duration"")\nspermtotalsub<-lmer(sperm_total ~ female_type + (1 | female_line) + species + duration, data = dataf1b)\n\n#effect of female line outliers\n\nfl2<-influence(spermtotalsub,""female_line"")\n\nplot(fl2, which=""dfbetas"", parameters=c(2), xlab=""DFbetaS"",ylab=""Female Line"")\n2/sqrt(8)\ndfbetas(fl2, parameters=c(2))\n\nplot(fl2, which=""cook"", cutoff=4/8, sort=TRUE, xlab=""Cooks Distance"", ylab=""Female Line"")\nsigtest(fl2,test=1.96,)\n\ndsMVMS1<-exclude.influence(spermtotalsub,""female_line"", ""MVMS1-1"")\n\n#effect of individual observation level outliers\n\nflo2<-influence(spermtotalsub, obs=TRUE)\n\nplot(flo2,which=""cook"",cutoff=4/160,sort=TRUE, xlab=""Cook\'s Distance"", ylab=""Individual Observation"")\ncooks.distance(flo2,sort=T)\n\nsummary(spermtotalsub)\nsigtest(flo2,test=1.96)$female_typeSympatric[c(72,26,25,56,19,49,39,6),]\nsigtest(flo2,test=1.96)$speciesHeterospecific[c(72,26,25,56,19,49,39,6),]\nsigtest(flo2,test=1.96)$duration[c(72,26,25,56,19,49,39,6),]\n\ndataf1b72<-dataf1b[-72,]\ndataf1b26<-dataf1b[-26,]\ndataf1b25<-dataf1b[-25,]\ndataf1b56<-dataf1b[-56,]\ndataf1b19<-dataf1b[-19,]\ndataf1b49<-dataf1b[-49,]\ndataf1b39<-dataf1b[-39,]\ndataf1b6<-dataf1b[-6,]\nsummary(spermtotalsub)\nsummary(lmer(sperm_total ~ female_type + (1 | female_line) + species + duration, data = dataf1b72))\nsummary(lmer(sperm_total ~ female_type + (1 | female_line) + species + duration, data = dataf1b26))\nsummary(lmer(sperm_total ~ female_type + (1 | female_line) + species + duration, data = dataf1b25))\nsummary(lmer(sperm_total ~ female_type + (1 | female_line) + species + duration, data = dataf1b56))\nsummary(lmer(sperm_total ~ female_type + (1 | female_line) + species + duration, data = dataf1b19))\nsummary(lmer(sperm_total ~ female_type + (1 | female_line) + species + duration, data = dataf1b49))\nsummary(lmer(sperm_total ~ female_type + (1 | female_line) + species + duration, data = dataf1b39))\nsummary(lmer(sperm_total ~ female_type + (1 | female_line) + species + duration, data = dataf1b6))\n\n\n#figure 3: eusperm viability\n\neusub<-data.frame(data2[,27:28],as.factor(data2$female_type),data2$female_line,data2$species,data2$treatment)\ncolnames(eusub)<-c(""Live"",""Dead"",""female_type"",""female_line"",""species"",""treatment"")\n\n#effect of female line\n\nfeu<-influence(meusub,""female_line"")\nplot(feu, which=""cook"", cutoff=4/4, sort=TRUE, xlab=""Cooks Distance"", ylab=""Female Line"")\nmeusub<-glmer(cbind(Live,Dead)~ female_type + species + treatment +(1 | female_line) , data = eusub, family = ""binomial"")\n\n#effect of individual observation level outliers\n\nfeuo<-influence(meusub,obs=TRUE)\nplot(feuo, which=""cook"", cutoff=4/64, sort=TRUE, xlab=""Cooks Distance"", ylab=""Observation"")\ncooks.distance(feuo,sort=T)\neusub[c(50,22,6,61,26,33,20,24,48,29,51,42,38,1),]\n\neusub2<-eusub[-c(50,22,6,61,26,33,20,24,48,29,51,42,38,1),]\nmeusub2<-glmer(cbind(Live,Dead)~ female_type + species + treatment +(1 | female_line) , data = eusub2, family = ""binomial"")\nsummary(meusub2)\n\n\n#figure 4: eusperm viability and parasperm proportion\n\neuparasub<-data.frame(data2[,27:28],data2$female_line,data2$percent_parasperm,data2$treatment)\ncolnames(euparasub)<-c(""Live"",""Dead"",""female_line"",""percent_parasperm"",""treatment"")\nmeuparasub<-glmer(cbind(Live,Dead)~ treatment:percent_parasperm +(1 | female_line) , data = euparasub, family = ""binomial"")\n\n#effect of female line\n\nfeu2<-influence(meuparasub,""female_line"")\nplot(feu2, which=""cook"", cutoff=4/4, sort=TRUE, xlab=""Cooks Distance"", ylab=""Female Line"")\n\n#effect of individual observation level outliers\n\nfeuo2<-influence(meuparasub,obs=TRUE)\nplot(feuo2, which=""cook"", cutoff=4/64, sort=TRUE, xlab=""Cooks Distance"", ylab=""Observation"")\ncooks.distance(feuo2,sort=T)\neuparasub[c(1,38,20,42,51,2,48,36,26,54,33,4),]\n\neuparasub2<-euparasub[-c(1,38,20,42,51,2,48,36,26,54,33,4),]\nmeuparasub2<-glmer(cbind(Live,Dead)~ treatment:percent_parasperm +(1 | female_line) , data = euparasub2, family = ""binomial"")\nsummary(meuparasub2)\n\n\n\n']","Data from: Testing potential mechanisms of conspecific sperm precedence in Drosophila pseudoobscura Drosophila pseudoobscura females that co-occur with sister species D. persimilis show elevated fertilization by conspecific sperm when they mate with both a heterospecific and a conspecific male. This phenomenon, known as conspecific sperm precedence (CSP), has evolved as a mechanism to avoid maladaptive hybridization with D. persimilis. In this study, we assessed pericopulatory (during mating) and postcopulatory (after mating) traits in crosses with sympatric or allopatric D. pseudoobscura females and conspecific or heterospecific males to evaluate potential mechanisms of CSP in this system. We observed shorter copulation duration in crosses with sympatric females, but found no difference in quantity of sperm transferred or female reproductive tract toxicity between sympatry and allopatry. Our data show some support for the hypothesis that parasperm, a short, sterile sperm morph, can protect fertile eusperm from the D. pseudoobscura female reproductive tract, though it is unclear how this might affect patterns of sperm use in sympatry vs. allopatry. Overall, these results suggest that copulation duration could potentially contribute to the elevated CSP observed in sympatry.",3
Torrent frogs have fewer macroparasites but higher rates of chytrid infection in landscapes with smaller forest cover,data set and R script (statistical analysis),"['library(lme4)\r\nlibrary(scales)\r\nlibrary(cplm) # cpglmm function to perform GLMM with Tweedie distribution\r\n\r\n\r\nsetwd(""/home/pavel/Profissional/Pesquisa/MyPapers-Working/Frogs1"")\r\n\r\ndados <- read.table(""dados_revision.txt"", header=T, sep=""\\t"")\r\n\r\nstr(dados)\r\n\r\ndados$PresencaHeteroxenico <- as.numeric(dados$Heteroxenico > 0)\r\ndados$PresencaMonoxenico <- as.numeric(dados$Monoxenico > 0)\r\ndados$PresencaNematodes <- as.numeric(dados$Nematodes > 0)\r\ndados$PresencaAcanthocephala <- as.numeric(dados$Acanthocephala > 0)\r\ndados$PresencaPlatelmintos <- as.numeric(dados$Platelmintos > 0)\r\ndados$PresencaHelmintosTotal <- as.numeric(dados$HelmintosTotal > 0)\r\ndados$PresencaOriginalLoadBd <- as.numeric(dados$OriginalLoadBd > 0)\r\n\r\n\r\nstr(dados)\r\n\r\n### Calculating the proportion of organisms with each parasite group\r\n\r\napply(dados[,-c(1:14)], 2, sum, na.rm=T)\r\n\r\n### Acanthocephala only occurred in five individuals, so it makes little sense to analyze them. Other groups occurred in a sufficient number of individuals.\r\n\r\n### Calculate the number of individuals with each parasite group at each site\r\n\r\naggregate(PresencaHeteroxenico ~ Localidade, data=dados, FUN=sum, na.rm=T) # 6 sites\r\naggregate(PresencaMonoxenico ~ Localidade, data=dados, FUN=sum, na.rm=T) # 7 sites\r\naggregate(PresencaNematodes ~ Localidade, data=dados, FUN=sum, na.rm=T) # 7 sites\r\naggregate(PresencaAcanthocephala ~ Localidade, data=dados, FUN=sum, na.rm=T) # 2 sites\r\naggregate(PresencaPlatelmintos ~ Localidade, data=dados, FUN=sum, na.rm=T) # 4 sites\r\naggregate(PresencaHelmintosTotal ~ Localidade, data=dados, FUN=sum, na.rm=T) # 8 sites\r\naggregate(PresencaOriginalLoadBd ~ Localidade, data=dados, FUN=sum, na.rm=T) # 8 sites\r\n\r\n### Apart from Acanthocephala, all groups were found in at least 4 sites.\r\n\r\n\r\n### Make histograms os values to see distribution...\r\n\r\npar(mfrow=c(2,3), mar=c(3,3,2,2), oma=c(2,2,2,2))\r\n\r\nhist(dados$Heteroxenico)\r\nhist(dados$Monoxenico)\r\nhist(dados$Nematodes)\r\nhist(dados$Platelmintos)\r\nhist(dados$HelmintosTotal)\r\nhist(dados$OriginalLoadBd, breaks=20)\r\n\r\nround(sort(dados$OriginalLoadBd),2)\r\n\r\n\r\n### Macroparasites: the easiest option seems to use a negative binomial distribution.\r\n### Bd: should be used as logarithms...\r\n\r\ndados$LogLoadBd <- dados$OriginalLoadBd\r\ndados$LogLoadBd[dados$OriginalLoadBd < 1] <- 0\r\ndados$LogLoadBd[dados$OriginalLoadBd >= 1] <- log10(dados$OriginalLoadBd[dados$OriginalLoadBd>=1])\r\ndados$LogLoadBd2 <- dados$LogLoadBd\r\ndados$LogLoadBd2[dados$LogLoadBd2==0] <- 0.00001\r\n\r\ndev.off()\r\n\r\nhist(dados$LogLoadBd, breaks=20)\r\nsort(dados$LogLoadBd) # Can probably be modeled as a Tweedie... \r\n\r\n### Probably the most correct way would be to model as Tweedie. However, to make things easier, I will use a Gamma distribution, and add a really small value to zero-load individuals:\r\n\r\ndados$LogLoadBd2 <- dados$LogLoadBd\r\ndados$LogLoadBd2[dados$LogLoadBd2==0] <- 0.00001\r\n\r\n\r\n### Anlise para Presena de Helmintos\r\n\r\n### Criando conjuntos para bootstrap\r\n\r\n### 9999 conjuntos\r\n### Todos os conjuntos precisam conter todas as localidades.\r\n\r\nNboot <- 1E4\r\nNrow <- nrow(dados)\r\nrows.boot <- matrix(nrow=Nrow,ncol=Nboot)\r\nrows.boot[,1] <- 1:Nrow\r\n\r\nNlocalidades <- length(unique(dados$Localidade))\r\n\r\nk <- 2\r\n\r\nLocalidade <- as.character(dados$Localidade)\r\n\r\nwhile(k <= Nboot) {\r\n\trows.temp <- sample(1:Nrow, replace=T)\r\n\tfoo <- Localidade[rows.temp]\r\n\tif(length(unique(foo)) == Nlocalidades) {\r\n\t\trows.boot[,k] <- rows.temp\r\n\t\tk <- k+1\r\n\t\tprint(k)\r\n\t}\r\n}\r\n\r\n\r\n### Let\'s make a long loop and test all groups! Using glmer.nb for macroparasites, glmer(family=Gamma) for Bd, and glmer(family=binomial) for presence/absence data.\r\n\r\n### Distribui�o binomial\r\n\r\n### Nos interessa o beta\r\n\r\nbeta.Heteroxenico <- numeric(Nboot)\r\nbeta.Monoxenico <- numeric(Nboot)\r\nbeta.Nematodes <- numeric(Nboot)\r\nbeta.Platelmintos <- numeric(Nboot)\r\nbeta.HelmintosTotal <- numeric(Nboot)\r\nbeta.LogLoadBd2 <- numeric(Nboot)\r\nalpha.Heteroxenico <- numeric(Nboot)\r\nalpha.Monoxenico <- numeric(Nboot)\r\nalpha.Nematodes <- numeric(Nboot)\r\nalpha.Platelmintos <- numeric(Nboot)\r\nalpha.HelmintosTotal <- numeric(Nboot)\r\nalpha.LogLoadBd2 <- numeric(Nboot)\r\n\r\nbeta.PresencaHeteroxenico <- numeric(Nboot)\r\nbeta.PresencaMonoxenico <- numeric(Nboot)\r\nbeta.PresencaNematodes <- numeric(Nboot)\r\nbeta.PresencaPlatelmintos <- numeric(Nboot)\r\nbeta.PresencaHelmintosTotal <- numeric(Nboot)\r\nbeta.PresencaOriginalLoadBd <- numeric(Nboot)\r\nalpha.PresencaHeteroxenico <- numeric(Nboot)\r\nalpha.PresencaMonoxenico <- numeric(Nboot)\r\nalpha.PresencaNematodes <- numeric(Nboot)\r\nalpha.PresencaPlatelmintos <- numeric(Nboot)\r\nalpha.PresencaHelmintosTotal <- numeric(Nboot)\r\nalpha.PresencaOriginalLoadBd <- numeric(Nboot)\r\n\r\n\r\nfor(i in 1:Nboot) {\r\n\tdados.temp <- dados[rows.boot[,i],]\r\n\r\n\tmod.Heteroxenico.temp <- try(glmer.nb(Heteroxenico ~ CoberturaFlorestal + (1|Localidade), data=dados.temp))\r\n\tif(class(mod.Heteroxenico.temp) != ""try-error"") {\r\n\t\tbeta.Heteroxenico[i] <- coef(mod.Heteroxenico.temp)$Localidade[1,2]\r\n\t\talpha.Heteroxenico[i] <- mean(coef(mod.Heteroxenico.temp)$Localidade[,1])\r\n\t}\r\n\r\n\tmod.Monoxenico.temp <- try(glmer.nb(Monoxenico ~ CoberturaFlorestal + (1|Localidade), data=dados.temp))\r\n\tif(class(mod.Monoxenico.temp) != ""try-error"") {\r\n\t\tbeta.Monoxenico[i] <- coef(mod.Monoxenico.temp)$Localidade[1,2]\r\n\t\talpha.Monoxenico[i] <- mean(coef(mod.Monoxenico.temp)$Localidade[,1])\r\n\t}\r\n\t\r\n\tmod.Nematodes.temp <- try(glmer.nb(Nematodes ~ CoberturaFlorestal + (1|Localidade), data=dados.temp))\r\n\tif(class(mod.Nematodes.temp) != ""try-error"") {\r\n\t\tbeta.Nematodes[i] <- coef(mod.Nematodes.temp)$Localidade[1,2]\r\n\t\talpha.Nematodes[i] <- mean(coef(mod.Nematodes.temp)$Localidade[,1])\t\r\n\t}\r\n\r\n\tmod.Platelmintos.temp <- try(glmer.nb(Platelmintos ~ CoberturaFlorestal + (1|Localidade), data=dados.temp))\r\n\tif(class(mod.Platelmintos.temp) != ""try-error"") {\r\n\t\tbeta.Platelmintos[i] <- coef(mod.Platelmintos.temp)$Localidade[1,2]\r\n\t\talpha.Platelmintos[i] <- mean(coef(mod.Platelmintos.temp)$Localidade[,1])\t\r\n\t}\r\n\r\n\tmod.HelmintosTotal.temp <- try(glmer.nb(HelmintosTotal ~ CoberturaFlorestal + (1|Localidade), data=dados.temp))\r\n\tif(class(mod.HelmintosTotal.temp) != ""try-error"") {\r\n\t\tbeta.HelmintosTotal[i] <- coef(mod.HelmintosTotal.temp)$Localidade[1,2]\r\n\t\talpha.HelmintosTotal[i] <- mean(coef(mod.HelmintosTotal.temp)$Localidade[,1])\t\r\n\t}\r\n\r\n\tmod.LogLoadBd2.temp <- try(glmer(LogLoadBd2 ~ CoberturaFlorestal + (1|Localidade), data=dados.temp, family=Gamma)) # Keep in mind that Gamma uses an inverse function!\r\n\tif(class(mod.LogLoadBd2.temp) != ""try-error"") {\r\n\t\tbeta.LogLoadBd2[i] <- coef(mod.LogLoadBd2.temp)$Localidade[1,2]\r\n\t\talpha.LogLoadBd2[i] <- mean(coef(mod.LogLoadBd2.temp)$Localidade[,1])\r\n\t}\t\r\n\t\r\n\t### Presence/absence\r\n\r\n\tmod.PresencaHeteroxenico.temp <- glmer(PresencaHeteroxenico ~ CoberturaFlorestal + (1|Localidade), data=dados.temp, family=binomial)\r\n\tbeta.PresencaHeteroxenico[i] <- coef(mod.PresencaHeteroxenico.temp)$Localidade[1,2]\r\n\talpha.PresencaHeteroxenico[i] <- mean(coef(mod.PresencaHeteroxenico.temp)$Localidade[,1])\r\n\r\n\tmod.PresencaMonoxenico.temp <- glmer(PresencaMonoxenico ~ CoberturaFlorestal + (1|Localidade), data=dados.temp, family=binomial)\r\n\tbeta.PresencaMonoxenico[i] <- coef(mod.PresencaMonoxenico.temp)$Localidade[1,2]\r\n\talpha.PresencaMonoxenico[i] <- mean(coef(mod.PresencaMonoxenico.temp)$Localidade[,1])\t\r\n\t\r\n\tmod.PresencaNematodes.temp <- glmer(PresencaNematodes ~ CoberturaFlorestal + (1|Localidade), data=dados.temp, family=binomial)\r\n\tbeta.PresencaNematodes[i] <- coef(mod.PresencaNematodes.temp)$Localidade[1,2]\r\n\talpha.PresencaNematodes[i] <- mean(coef(mod.PresencaNematodes.temp)$Localidade[,1])\t\r\n\r\n\tmod.PresencaPlatelmintos.temp <- glmer(PresencaPlatelmintos ~ CoberturaFlorestal + (1|Localidade), data=dados.temp, family=binomial)\r\n\tbeta.PresencaPlatelmintos[i] <- coef(mod.PresencaPlatelmintos.temp)$Localidade[1,2]\r\n\talpha.PresencaPlatelmintos[i] <- mean(coef(mod.PresencaPlatelmintos.temp)$Localidade[,1])\t\r\n\r\n\tmod.PresencaHelmintosTotal.temp <- glmer(PresencaHelmintosTotal ~ CoberturaFlorestal + (1|Localidade), data=dados.temp, family=binomial)\r\n\tbeta.PresencaHelmintosTotal[i] <- coef(mod.PresencaHelmintosTotal.temp)$Localidade[1,2]\r\n\talpha.PresencaHelmintosTotal[i] <- mean(coef(mod.PresencaHelmintosTotal.temp)$Localidade[,1])\t\r\n\tmod.PresencaOriginalLoadBd.temp <- glmer(PresencaOriginalLoadBd ~ CoberturaFlorestal + (1|Localidade), data=dados.temp, family=binomial) # Keep in mind that Gamma uses an inverse function!\r\n\tbeta.PresencaOriginalLoadBd[i] <- coef(mod.PresencaOriginalLoadBd.temp)$Localidade[1,2]\r\n\talpha.PresencaOriginalLoadBd[i] <- mean(coef(mod.PresencaOriginalLoadBd.temp)$Localidade[,1])\t\r\n\tprint(i)\r\n}\r\n\r\n### Completed only 5472 boots\r\n\r\nsave.image(file=""frogs1.RData"")\r\n\r\nload(""frogs1.RData"")\r\n\r\n### remove NAs (non-convergence and extra boots)\r\n\r\nbeta.Heteroxenico.backup <- beta.Heteroxenico\r\nbeta.Monoxenico.backup <- beta.Monoxenico\r\nbeta.Nematodes.backup <- beta.Nematodes\r\nbeta.Platelmintos.backup <- beta.Platelmintos\r\nbeta.HelmintosTotal.backup <- beta.HelmintosTotal\r\nbeta.LogLoadBd2.backup <- beta.LogLoadBd2\r\nalpha.Heteroxenico.backup <- alpha.Heteroxenico\r\nalpha.Monoxenico.backup <- alpha.Monoxenico\r\nalpha.Nematodes.backup <- alpha.Nematodes\r\nalpha.Platelmintos.backup <- alpha.Platelmintos\r\nalpha.HelmintosTotal.backup <- alpha.HelmintosTotal\r\nalpha.LogLoadBd2.backup <- alpha.LogLoadBd2\r\nbeta.PresencaHeteroxenico.backup <- beta.PresencaHeteroxenico\r\nbeta.PresencaMonoxenico.backup <- beta.PresencaMonoxenico\r\nbeta.PresencaNematodes.backup <- beta.PresencaNematodes\r\nbeta.PresencaPlatelmintos.backup <- beta.PresencaPlatelmintos\r\nbeta.PresencaHelmintosTotal.backup <- beta.PresencaHelmintosTotal\r\nbeta.PresencaOriginalLoadBd.backup <- beta.PresencaOriginalLoadBd\r\nalpha.PresencaHeteroxenico.backup <- alpha.PresencaHeteroxenico\r\nalpha.PresencaMonoxenico.backup <- alpha.PresencaMonoxenico\r\nalpha.PresencaNematodes.backup <- alpha.PresencaNematodes\r\nalpha.PresencaPlatelmintos.backup <- alpha.PresencaPlatelmintos\r\nalpha.PresencaHelmintosTotal.backup <- alpha.PresencaHelmintosTotal\r\nalpha.PresencaOriginalLoadBd.backup <- alpha.PresencaOriginalLoadBd\r\n\r\n# Which rows to keep?\r\n\r\nkeep.Heteroxenico <- which(beta.Heteroxenico != 0 & alpha.Heteroxenico !=0)\r\nkeep.Monoxenico <- which(beta.Monoxenico != 0 & alpha.Monoxenico !=0)\r\nkeep.Nematodes <- which(beta.Nematodes != 0 & alpha.Nematodes !=0)\r\nkeep.Platelmintos <- which(beta.Platelmintos != 0 & alpha.Platelmintos !=0)\r\nkeep.HelmintosTotal <- which(beta.HelmintosTotal != 0 & alpha.HelmintosTotal !=0)\r\nkeep.LogLoadBd2 <- which(beta.LogLoadBd2 != 0 & alpha.LogLoadBd2 !=0)\r\nkeep.PresencaHeteroxenico <- which(beta.PresencaHeteroxenico != 0 & alpha.PresencaHeteroxenico !=0)\r\nkeep.PresencaMonoxenico <- which(beta.PresencaMonoxenico != 0 & alpha.PresencaMonoxenico !=0)\r\nkeep.PresencaNematodes <- which(beta.PresencaNematodes != 0 & alpha.PresencaNematodes !=0)\r\nkeep.PresencaPlatelmintos <- which(beta.PresencaPlatelmintos != 0 & alpha.PresencaPlatelmintos !=0)\r\nkeep.PresencaHelmintosTotal <- which(beta.PresencaHelmintosTotal != 0 & alpha.PresencaHelmintosTotal !=0)\r\nkeep.PresencaOriginalLoadBd <- which(beta.PresencaOriginalLoadBd != 0 & alpha.PresencaOriginalLoadBd !=0)\r\n\r\nbeta.Heteroxenico <- beta.Heteroxenico[keep.Heteroxenico]\r\nbeta.Monoxenico <- beta.Monoxenico[keep.Monoxenico]\r\nbeta.Nematodes <- beta.Nematodes[keep.Nematodes]\r\nbeta.Platelmintos <- beta.Platelmintos[keep.Platelmintos]\r\nbeta.HelmintosTotal <- beta.HelmintosTotal[keep.HelmintosTotal]\r\nbeta.LogLoadBd2 <- beta.LogLoadBd2[keep.LogLoadBd2]\r\nalpha.Heteroxenico <- alpha.Heteroxenico[keep.Heteroxenico]\r\nalpha.Monoxenico <- alpha.Monoxenico[keep.Monoxenico]\r\nalpha.Nematodes <- alpha.Nematodes[keep.Nematodes]\r\nalpha.Platelmintos <- alpha.Platelmintos[keep.Platelmintos]\r\nalpha.HelmintosTotal <- alpha.HelmintosTotal[keep.HelmintosTotal]\r\nalpha.LogLoadBd2 <- alpha.LogLoadBd2[keep.LogLoadBd2]\r\n\r\nbeta.PresencaHeteroxenico <- beta.PresencaHeteroxenico[keep.PresencaHeteroxenico]\r\nbeta.PresencaMonoxenico <- beta.PresencaMonoxenico[keep.PresencaMonoxenico]\r\nbeta.PresencaNematodes <- beta.PresencaNematodes[keep.PresencaNematodes]\r\nbeta.PresencaPlatelmintos <- beta.PresencaPlatelmintos[keep.PresencaPlatelmintos]\r\nbeta.PresencaHelmintosTotal <- beta.PresencaHelmintosTotal[keep.PresencaHelmintosTotal]\r\nbeta.PresencaOriginalLoadBd <- beta.PresencaOriginalLoadBd[keep.PresencaOriginalLoadBd]\r\nalpha.PresencaHeteroxenico <- alpha.PresencaHeteroxenico[keep.PresencaHeteroxenico]\r\nalpha.PresencaMonoxenico <- alpha.PresencaMonoxenico[keep.PresencaMonoxenico]\r\nalpha.PresencaNematodes <- alpha.PresencaNematodes[keep.PresencaNematodes]\r\nalpha.PresencaPlatelmintos <- alpha.PresencaPlatelmintos[keep.PresencaPlatelmintos]\r\nalpha.PresencaHelmintosTotal <- alpha.PresencaHelmintosTotal[keep.PresencaHelmintosTotal]\r\nalpha.PresencaOriginalLoadBd <- alpha.PresencaOriginalLoadBd[keep.PresencaOriginalLoadBd]\r\n\r\n\r\n### Keep the first 5000 values\r\n\r\nbeta.Heteroxenico <- beta.Heteroxenico[1:5000]\r\nbeta.Monoxenico <- beta.Monoxenico[1:5000]\r\nbeta.Nematodes <- beta.Nematodes[1:5000]\r\nbeta.Platelmintos <- beta.Platelmintos[1:5000]\r\nbeta.HelmintosTotal <- beta.HelmintosTotal[1:5000]\r\nbeta.LogLoadBd2 <- beta.LogLoadBd2[1:5000]\r\nalpha.Heteroxenico <- alpha.Heteroxenico[1:5000]\r\nalpha.Monoxenico <- alpha.Monoxenico[1:5000]\r\nalpha.Nematodes <- alpha.Nematodes[1:5000]\r\nalpha.Platelmintos <- alpha.Platelmintos[1:5000]\r\nalpha.HelmintosTotal <- alpha.HelmintosTotal[1:5000]\r\nalpha.LogLoadBd2 <- alpha.LogLoadBd2[1:5000]\r\n\r\nbeta.PresencaHeteroxenico <- beta.PresencaHeteroxenico[1:5000]\r\nbeta.PresencaMonoxenico <- beta.PresencaMonoxenico[1:5000]\r\nbeta.PresencaNematodes <- beta.PresencaNematodes[1:5000]\r\nbeta.PresencaPlatelmintos <- beta.PresencaPlatelmintos[1:5000]\r\nbeta.PresencaHelmintosTotal <- beta.PresencaHelmintosTotal[1:5000]\r\nbeta.PresencaOriginalLoadBd <- beta.PresencaOriginalLoadBd[1:5000]\r\nalpha.PresencaHeteroxenico <- alpha.PresencaHeteroxenico[1:5000]\r\nalpha.PresencaMonoxenico <- alpha.PresencaMonoxenico[1:5000]\r\nalpha.PresencaNematodes <- alpha.PresencaNematodes[1:5000]\r\nalpha.PresencaPlatelmintos <- alpha.PresencaPlatelmintos[1:5000]\r\nalpha.PresencaHelmintosTotal <- alpha.PresencaHelmintosTotal[1:5000]\r\nalpha.PresencaOriginalLoadBd <- alpha.PresencaOriginalLoadBd[1:5000]\r\n\r\n### Now perform the analyses...\r\n\r\n\r\n### 95% CIs e Propor�o de valores positivos\r\n\r\nCIs <- matrix(ncol=2, nrow=12)\r\nrow.names(CIs) <- c(""Heteroxenico"", ""Monoxenico"", ""Nematodes"", ""Platelmintos"", ""HelmintosTotal"", ""LogLoadBd2"", ""PresencaHeteroxenico"", ""PresencaMonoxenico"", ""PresencaNematodes"", ""PresencaPlatelmintos"", ""PresencaHelmintosTotal"", ""PresencaOriginalLoadBd"")\r\n\r\nCIs[1,] <- quantile(beta.Heteroxenico, c(0.025, 0.975))\r\nCIs[2,] <- quantile(beta.Monoxenico, c(0.025, 0.975))\r\nCIs[3,] <- quantile(beta.Nematodes, c(0.025, 0.975))\r\nCIs[4,] <- quantile(beta.Platelmintos, c(0.025, 0.975))\r\nCIs[5,] <- quantile(beta.HelmintosTotal, c(0.025, 0.975))\r\nCIs[6,] <- quantile(beta.LogLoadBd2, c(0.025, 0.975))\r\nCIs[7,] <- quantile(beta.PresencaHeteroxenico, c(0.025, 0.975))\r\nCIs[8,] <- quantile(beta.PresencaMonoxenico, c(0.025, 0.975))\r\nCIs[9,] <- quantile(beta.PresencaNematodes, c(0.025, 0.975))\r\nCIs[10,] <- quantile(beta.PresencaPlatelmintos, c(0.025, 0.975))\r\nCIs[11,] <- quantile(beta.PresencaHelmintosTotal, c(0.025, 0.975))\r\nCIs[12,] <- quantile(beta.PresencaOriginalLoadBd, c(0.025, 0.975))\r\n\r\n\r\nCIs\r\n\r\n\r\n### Making figures...\r\n\r\nstack.presabs <- function(x, y, spacing=0.011) {\r\n\tfoo <- numeric(length(x))\r\n\tfoo[1] <- x[1]\r\n\tfor(i in 2:length(x)) {\r\n\t\tNequal <- sum(y[i] == y[1:(i-1)] & x[i] == x[1:(i-1)])\r\n\t\tfoo[i] <- ifelse(x[i]==0, x[i]+Nequal*spacing, x[i]-Nequal*spacing)\r\n\t}\r\n\treturn(foo)\r\n}\r\n\r\nNboot <- 5000\r\n\r\npng(filename=""figura_funguinhos_rev_presenca.png"", height=15, width=21, unit=""cm"", res=300)\r\n\r\npar(mfrow=c(2,3), mar=c(4,4,2,2), oma=c(3,3,2,2))\r\n\r\nplot(PresencaHeteroxenico ~ CoberturaFlorestal, data=dados, type=""n"", xlab="""", ylab="""", yaxt=""n"", main=""a. Heteroxenous"")\r\nfor(i in 1:Nboot) {\r\n\tcurve(expr=exp(alpha.PresencaHeteroxenico[i] + beta.PresencaHeteroxenico[i]*x)/(1+exp(alpha.PresencaHeteroxenico[i] + beta.PresencaHeteroxenico[i]*x)), add=T, col=alpha(""gray30"",0.02))\r\n}\r\npoints(stack.presabs(PresencaHeteroxenico, CoberturaFlorestal, spacing=0.035) ~ CoberturaFlorestal, data=dados, pch=21, bg=alpha(""red"",0.5))\r\naxis(side=2,at=c(0,1))\r\n\r\nplot(PresencaMonoxenico ~ CoberturaFlorestal, data=dados, type=""n"", xlab="""", ylab="""", yaxt=""n"", main=""b. Monoxenous"")\r\nfor(i in 1:Nboot) {\r\n\tcurve(expr=exp(alpha.PresencaMonoxenico[i] + beta.PresencaMonoxenico[i]*x)/(1+exp(alpha.PresencaMonoxenico[i] + beta.PresencaMonoxenico[i]*x)), add=T, col=alpha(""gray30"",0.04))\r\n}\r\npoints(stack.presabs(PresencaMonoxenico, CoberturaFlorestal, spacing=0.035) ~ CoberturaFlorestal, data=dados, pch=21, bg=alpha(""red"",0.5))\r\naxis(side=2,at=c(0,1))\r\n\r\nplot(PresencaNematodes ~ CoberturaFlorestal, data=dados, type=""n"", xlab="""", ylab="""", yaxt=""n"", main=""c. Nematodes"")\r\nfor(i in 1:Nboot) {\r\n\tcurve(expr=exp(alpha.PresencaNematodes[i] + beta.PresencaNematodes[i]*x)/(1+exp(alpha.PresencaNematodes[i] + beta.PresencaNematodes[i]*x)), add=T, col=alpha(""gray30"",0.04))\r\n}\r\npoints(stack.presabs(PresencaNematodes, CoberturaFlorestal, spacing=0.035) ~ CoberturaFlorestal, data=dados, pch=21, bg=alpha(""red"",0.5))\r\naxis(side=2,at=c(0,1))\r\n\r\nplot(PresencaPlatelmintos ~ CoberturaFlorestal, data=dados, type=""n"", xlab="""", ylab="""", yaxt=""n"", main=""d. Platyhelminthes"")\r\nfor(i in 1:Nboot) {\r\n\tcurve(expr=exp(alpha.PresencaPlatelmintos[i] + beta.PresencaPlatelmintos[i]*x)/(1+exp(alpha.PresencaPlatelmintos[i] + beta.PresencaPlatelmintos[i]*x)), add=T, col=alpha(""gray30"",0.04))\r\n}\r\npoints(stack.presabs(PresencaPlatelmintos, CoberturaFlorestal, spacing=0.035) ~ CoberturaFlorestal, data=dados, pch=21, bg=alpha(""red"",0.5))\r\naxis(side=2,at=c(0,1))\r\n\r\nplot(PresencaHelmintosTotal ~ CoberturaFlorestal, data=dados, type=""n"", xlab="""", ylab="""", yaxt=""n"", main=""e. All helminths"")\r\nfor(i in 1:Nboot) {\r\n\tcurve(expr=exp(alpha.PresencaHelmintosTotal[i] + beta.PresencaHelmintosTotal[i]*x)/(1+exp(alpha.PresencaHelmintosTotal[i] + beta.PresencaHelmintosTotal[i]*x)), add=T, col=alpha(""gray30"",0.04))\r\n}\r\npoints(stack.presabs(PresencaHelmintosTotal, CoberturaFlorestal, spacing=0.035) ~ CoberturaFlorestal, data=dados, pch=21, bg=alpha(""red"",0.5))\r\naxis(side=2,at=c(0,1))\r\n\r\nplot(PresencaOriginalLoadBd ~ CoberturaFlorestal, data=dados, type=""n"", xlab="""", ylab="""", yaxt=""n"", main=""f. Chytrids (Bd)"")\r\nfor(i in 1:Nboot) {\r\n\tcurve(expr=exp(alpha.PresencaOriginalLoadBd[i] + beta.PresencaOriginalLoadBd[i]*x)/(1+exp(alpha.PresencaOriginalLoadBd[i] + beta.PresencaOriginalLoadBd[i]*x)), add=T, col=alpha(""gray30"",0.04))\r\n}\r\npoints(stack.presabs(PresencaOriginalLoadBd, CoberturaFlorestal, spacing=0.035) ~ CoberturaFlorestal, data=dados, pch=21, bg=alpha(""red"",0.5))\r\naxis(side=2,at=c(0,1))\r\n\r\nmtext(side=1, outer=T, text=""Forest cover (%)"")\r\nmtext(side=2, outer=T, text=""Presence/absence"")\r\n\r\ndev.off()\r\n\r\n\r\n### Figure - abundance\r\n\r\npng(filename=""figura_funguinhos_rev_load.png"", height=15, width=21, unit=""cm"", res=300)\r\n\r\npar(mfrow=c(2,3), mar=c(4,4,2,2), oma=c(3,3,2,2))\r\n\r\nplot(Heteroxenico ~ CoberturaFlorestal, data=dados, type=""n"", xlab="""", ylab="""", yaxt=""s"", main=""a. Heteroxenous"")\r\nfor(i in 1:Nboot) {\r\n\tcurve(expr=exp(alpha.Heteroxenico[i] + beta.Heteroxenico[i]*x), add=T, col=alpha(""gray30"",0.02))\r\n}\r\npoints(Heteroxenico ~ CoberturaFlorestal, data=dados, pch=21, bg=alpha(""red"",0.5))\r\naxis(side=2,at=c(0,1))\r\n\r\n\r\n\r\nplot(Monoxenico ~ CoberturaFlorestal, data=dados, type=""n"", xlab="""", ylab="""", yaxt=""s"", main=""b. Monoxenous"")\r\nfor(i in 1:Nboot) {\r\n\tcurve(expr=exp(alpha.Monoxenico[i] + beta.Monoxenico[i]*x), add=T, col=alpha(""gray30"",0.02))\r\n}\r\npoints(Monoxenico ~ CoberturaFlorestal, data=dados, pch=21, bg=alpha(""red"",0.5))\r\naxis(side=2,at=c(0,1))\r\n\r\n\r\n\r\nplot(Nematodes ~ CoberturaFlorestal, data=dados, type=""n"", xlab="""", ylab="""", yaxt=""s"", main=""d. Nematodes"")\r\nfor(i in 1:Nboot) {\r\n\tcurve(expr=exp(alpha.Nematodes[i] + beta.Nematodes[i]*x), add=T, col=alpha(""gray30"",0.02))\r\n}\r\npoints(Nematodes ~ CoberturaFlorestal, data=dados, pch=21, bg=alpha(""red"",0.5))\r\naxis(side=2,at=c(0,1))\r\n\r\n\r\nplot(Platelmintos ~ CoberturaFlorestal, data=dados, type=""n"", xlab="""", ylab="""", yaxt=""s"", main=""d. Platyhelminthes"")\r\nfor(i in 1:Nboot) {\r\n\tcurve(expr=exp(alpha.Platelmintos[i] + beta.Platelmintos[i]*x), add=T, col=alpha(""gray30"",0.02))\r\n}\r\npoints(Platelmintos ~ CoberturaFlorestal, data=dados, pch=21, bg=alpha(""red"",0.5))\r\naxis(side=2,at=c(0,1))\r\n\r\n\r\nplot(HelmintosTotal ~ CoberturaFlorestal, data=dados, type=""n"", xlab="""", ylab="""", yaxt=""s"", main=""e. All heminths"")\r\nfor(i in 1:Nboot) {\r\n\tcurve(expr=exp(alpha.HelmintosTotal[i] + beta.HelmintosTotal[i]*x), add=T, col=alpha(""gray30"",0.02))\r\n}\r\npoints(HelmintosTotal ~ CoberturaFlorestal, data=dados, pch=21, bg=alpha(""red"",0.5))\r\naxis(side=2,at=c(0,1))\r\n\r\n\r\nplot(LogLoadBd2 ~ CoberturaFlorestal, data=dados, type=""n"", xlab="""", ylab="""", yaxt=""s"", main=""f. Chytrids (Bd)"")\r\nfor(i in 1:Nboot) {\r\n\tcurve(expr=1/(alpha.LogLoadBd2[i] + beta.LogLoadBd2[i]*x), add=T, col=alpha(""gray30"",0.02))\r\n}\r\npoints(LogLoadBd2 ~ CoberturaFlorestal, data=dados, pch=21, bg=alpha(""red"",0.5))\r\naxis(side=2,at=c(0,1))\r\n\r\n\r\nmtext(side=1, outer=T, text=""Forest cover (%)"")\r\nmtext(side=2, outer=T, text=""Parasite load"")\r\n\r\ndev.off()\r\n\r\nround(CIs, 2)\r\n\r\n### Now... Are these results real, or an artefact of elevation and temperature?\r\n\r\n### Check the correlation between forest cover and these other variables.\r\n\r\npar(mfrow=c(1,2))\r\nplot(CoberturaFlorestal ~ TemperaturaMinima, data=dados)\r\nplot(CoberturaFlorestal ~ Altitude, data=dados)\r\n\r\npar(mfrow=c(1,2))\r\nplot(TemperaturaMinima ~ CoberturaFlorestal, data=dados)\r\nplot(Altitude ~ CoberturaFlorestal, data=dados)\r\n\r\n\r\n### Don\'t seem to be a strong correlation...\r\n\r\ncor(dados$CoberturaFlorestal, dados$TemperaturaMinima)^2\r\ncor(dados$CoberturaFlorestal, dados$Altitude)^2\r\n\r\n\r\n\r\n### Are helminths and fungi independent, within the landscape?\r\n\r\n### Restricted randomization for correlation (Pearson) and association (chi2).\r\n\r\npar(mfrow=c(1,2))\r\nplot(IntensidadeHelmintosLog ~ IntensidadeBd, data=dados, pch=20+as.numeric(Especie), bg=as.numeric(Localidade), main=""Intensidades"")\r\nplot(jitter(PresencaHelmintos) ~ jitter(PresencaBd), data=dados, main=""Intensidades"")\r\n\r\n# Correlation within landscape?\r\ncorrs <- numeric()\r\nfor(i in 1:length(unique(dados$Localidade))) {\r\n\tfoo <- subset(dados, Localidade==unique(dados$Localidade)[i])\r\n\tcorrs[i] <- cor(foo$IntensidadeHelmintosLog, foo$IntensidadeBd)\r\n}\r\n\r\nmean(corrs, na.rm=T)\r\n\r\n\r\n']",Torrent frogs have fewer macroparasites but higher rates of chytrid infection in landscapes with smaller forest cover data set and R script (statistical analysis),3
Data from: A meta-analysis of factors influencing the strength of mate choice copying in animals,"Davies et al., (2020) All data and R codeMate-choice copying is a form of social learning in which an individual's choice of mate is influenced by the apparent choices of other individuals of the same sex, and has been observed in more than 20 species across a broad taxonomic range. Though fitness benefits of copying have proven difficult to measure, theory suggests that copying should not be beneficial for all species or contexts. However, the factors influencing the evolution and expression of copying have proven difficult to resolve. We systematically searched the literature for studies of mate-choice copying in non-human animals, and then performed a phylogenetically-controlled meta-analysis to explore which factors influence the expression of copying across species. Across 58 published studies in 23 species, we find strong evidence that animals copy the mate choice of others. The strength of copying was significantly influenced by taxonomic group, however sample size limitations mean it is difficult to draw firm conclusions regarding copying in mammals and arthropods. The strength of copying was also influenced by experimental design: copying was stronger when choosers were tested before and after witnessing a conspecific's mate choice, compared to when choosers with social information were compared to choosers without. Importantly, we did not detect any difference in the strength of copying between males and females, or in relation to the rate of multiple mating. Our search also highlights that more empirical work is needed to investigate copying in a broader range of species, especially those with differing mating systems and levels of reproductive investment.","['###-----------------------------------------------------------------------------------------------###\r\n### Davies et al., 2020 Full code                                                                 ###\r\n### Reference: Davies, Lewis & Dougherty (2020) A meta-analysis of factors influencing the        ###\r\n### strength of mate choice copying in animals. Behavioural Ecology.                              ###\r\n### Author: Liam Dougherty, University of Liverpool [liam.dougherty@liv.ac.uk]                    ###\r\n### Date: 05/05/2020                                                                              ###\r\n###-----------------------------------------------------------------------------------------------###\r\n\r\n\r\n#-------------------------#\r\n# 1. Setup                #\r\n#-------------------------#\r\n\r\nlibrary(ape)\r\nlibrary(rotl)\r\nlibrary(metafor)\r\nlibrary(MCMCglmm)\r\n\r\ndata <- read.delim(""MCC_meta_analysis_final.txt"", h=T) \r\ndata$Study_no <- as.factor(data$Study_no) \r\ndata$species2 <- as.factor(data$Species_latin)\r\ndata$Species_latin <- gsub("" "", ""_"", data$Species_latin) # Remove spaces between binomial names\r\ndata$Species_latin <- as.factor(data$Species_latin) \r\ndata$animal <- as.factor(data$Species_latin) # MCMCglmm model needs column called \'animal\' for phylogeny\r\ndata$Effect_size_no <- as.factor(data$Effect_size_no)\r\ndata$obs <- as.factor(data$Effect_size_no)\r\nprecision <- sqrt(1/data$Variance) # Precision= inverse standard error\r\ndata[,""precision""] <- precision \r\n\r\ntree2 <- read.tree(""tree1_edited.tre"") # Import tree\r\ntree2_grafen <- compute.brlen(tree2, method=""Grafen"", power=1)\r\nmatrix <- vcv(tree2_grafen, cor=TRUE, model=""Brownian"")\r\n\r\nload(""meta2b.rda"") # Load MCMCglmm model to save time\r\n\r\n\r\n#----------------------------------#\r\n# 2. Creating phylogenetic tree    #\r\n#----------------------------------#\r\n\r\n# [Only need to do this once]\r\n\r\ntaxa1 <- levels(data$Species_latin) \r\nresolved_names1 <- tnrs_match_names(taxa1,context_name = ""Animals"") # Check OTL for species names\r\n\r\n# Plot tree using TOL data\r\ntree1 <- tol_induced_subtree(ott_ids = resolved_names1$ott_id) \r\ntree1$tip.label <- strip_ott_ids(tree1$tip.label) # Remove ott IDs for presentation\r\ntree1$node.label <- NULL # Remove node labels (these might be a problem later)\r\n\r\nwrite.nexus(tree1, file=""tree1.nex"")\r\n\r\n\r\n#-------------------------------------#\r\n# 3. Meta-analysis- overall models    #\r\n#-------------------------------------#\r\n\r\n# Simplest model- no random effects\r\nmeta1 <- rma.uni(Hedges_d_directional, Variance, data= data, method= ""REML"") \r\n\r\n# Funnel plot\r\nfunnel(meta1, yaxis=""seinv"", xlab=""Effect size (Hedges\' d)"", steps=11, digits=1, ylim=c(1, 11), xlim=c(-2.2, 5), \r\n       back=""white"", shade=""white"", hlines=""white"", pch=21, col=rgb(0,0,0, max=255), bg=rgb(255,255,255, max=255, alpha=150))\r\nabline(v=0, lty=2) \r\n\r\n### Overall model- including random effects\r\nmeta2 <- rma.mv(Hedges_d_directional, Variance, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nsummary(meta2)\r\n\r\n\r\n### Heteogeneity (I^2)\r\nW <- diag(1/data$Variance)\r\nX <- model.matrix(meta2)\r\nP <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W\r\n\r\n100 * sum(meta2$sigma2) / (sum(meta2$sigma2) + \r\n                             (meta2$k-meta2$p)/sum(diag(P))) # Total I^2\r\n100 * meta2$sigma2 / (sum(meta2$sigma2) + (meta2$k-meta2$p)/sum(diag(P))) # I^2 for each random factor\r\n\r\n\r\n### MCMCglmm model\r\nprior3 <- list(R=list(V=1,nu=0.002),G=list(G1=list(V=1,nu=0.002),G2=list(V=1,nu=0.002),G3=list(V=1,nu=0.002)))\r\nmeta2b <- MCMCglmm(Hedges_d_directional~1, random= ~Study_no + Species_latin + animal, mev=data$Variance, data=data,\r\n                   nitt=300000, thin=50, burnin=200000, pr=TRUE, prior=prior3, pedigree=tree2_grafen)\r\n\r\n\r\n### Meta-analytic residuals [already present in datafile]\r\nmeta2b$Random$formula<-update(meta2b$Random$formula, ~.+leg(mev, -1, FALSE):units)\r\nfitted <- predict(meta2b, marginal=~leg(mev, -1, FALSE):units) # Only works using R v3.2 or earlier\r\nresiduals <- data$Hedges_d_directional - fitted # raw data - predictions = meta-analytic residuals\r\nzresiduals <- residuals*precision\r\ndata$zresiduals <- zresiduals\r\nwrite.table(data, file= ""newdata.txt"", sep=""\\t"") # Write new data file\r\n\r\n# Funnel plot\r\nmeta_resid <- rma.uni(zresiduals, Variance, data= data, method= ""REML"") \r\nfunnel(meta_resid, yaxis=""seinv"", xlab=""Effect size (Hedges\' d)"", steps=11, digits=1, ylim=c(1, 11),\r\n       back=""white"", shade=""white"", hlines=""white"", pch=21, col=rgb(0,0,0, max=255), bg=rgb(255,255,255, max=255, alpha=150))\r\n\r\n\r\n#-----------------------------#\r\n# 4. Effect of moderators     #\r\n#-----------------------------#\r\n\r\n### Single-factor meta-regression models\r\nmeta_group <- rma.mv(Hedges_d_directional, Variance, mod= ~ Group, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                     R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_sex <- rma.mv(Hedges_d_directional, Variance, mod= ~ Choosing_sex, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                     R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_design <- rma.mv(Hedges_d_directional, Variance, mod= ~ Design_type, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                     R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_pairing <- rma.mv(Hedges_d_directional, Variance, mod= ~ Model_pairing, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                     R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_demo <- rma.mv(Hedges_d_directional, Variance, mod= ~ Demonstration_type, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                     R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_behaviour <- rma.mv(Hedges_d_directional, Variance, mod= ~ Behaviour_measured, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, \r\n                     ~ 1|obs), R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_copy_type <- rma.mv(Hedges_d_directional, Variance, mod= ~ Generalised_or_individual, random= list(~ 1|Species_latin, ~ 1|species2, \r\n                     ~ 1|Study_no, ~ 1|obs), R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_birth <- rma.mv(Hedges_d_directional, Variance, mod= ~ Animal_born, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                         R= list(Species_latin = matrix), data= data, method= ""REML"")\r\n\r\nsummary(meta_group)\r\nsummary(meta_sex)\r\nsummary(meta_design)\r\nsummary(meta_pairing)\r\nsummary(meta_demo)\r\nsummary(meta_behaviour)\r\nsummary(meta_copy_type)\r\nsummary(meta_birth)\r\n\r\n\r\n### To determine means and 95% CIs for each factor level- run same model as before but remove intercept\r\nmeta_group2 <- rma.mv(Hedges_d_directional, Variance, mod= ~ Group -1, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                     R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_sex2 <- rma.mv(Hedges_d_directional, Variance, mod= ~ Choosing_sex -1, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                     R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_design2 <- rma.mv(Hedges_d_directional, Variance, mod= ~ Design_type -1, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                     R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_pairing2 <- rma.mv(Hedges_d_directional, Variance, mod= ~ Model_pairing -1, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                     R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_demo2 <- rma.mv(Hedges_d_directional, Variance, mod= ~ Demonstration_type -1, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                     R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_behaviour2 <- rma.mv(Hedges_d_directional, Variance, mod= ~ Behaviour_measured -1, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, \r\n                     ~ 1|obs), R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_copy_type2 <- rma.mv(Hedges_d_directional, Variance, mod= ~ Generalised_or_individual -1, random= list(~ 1|Species_latin, ~ 1|species2, \r\n                   ~ 1|Study_no, ~ 1|obs), R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nmeta_birth2 <- rma.mv(Hedges_d_directional, Variance, mod= ~ Animal_born -1, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                     R= list(Species_latin = matrix), data= data, method= ""REML"")\r\n\r\nsummary(meta_group2)\r\nsummary(meta_sex2)\r\nsummary(meta_design2)\r\nsummary(meta_pairing2)\r\nsummary(meta_demo2)\r\nsummary(meta_behaviour2)\r\nsummary(meta_copy_type2)\r\nsummary(meta_birth2) \r\n\r\n\r\n### Mating rate- reduced dataset\r\nmating_data <- subset(data, Rate_of_multiple_mating==""High"" | Rate_of_multiple_mating==""Low"")\r\n\r\n# Getting pruned tree\r\nspecies_mating <- unique(mating_data$Species_latin) # List of species names\r\ntree_mating <- drop.tip(tree2_grafen, tree2_grafen$tip.label[-match(species_mating, tree2_grafen$tip.label)])\r\nmatrix2 <- vcv(tree_mating, cor=TRUE, model=""Brownian"")\r\n\r\n# Meta-regressions\r\nmeta_mating_rate <- rma.mv(Hedges_d_directional, Variance, mod= ~ Rate_of_multiple_mating, random= list(~ 1|Species_latin, ~ 1|species2, \r\n                    ~ 1|Study_no, ~ 1|obs), R= list(Species_latin = matrix), data= mating_data, method= ""REML"")\r\nsummary(meta_mating_rate)\r\n\r\nmeta_mating_rate2 <- rma.mv(Hedges_d_directional, Variance, mod= ~ Rate_of_multiple_mating -1, random= list(~ 1|Species_latin, ~ 1|species2, \r\n                     ~ 1|Study_no, ~ 1|obs), R= list(Species_latin = matrix), data= mating_data, method= ""REML"")\r\nsummary(meta_mating_rate2)\r\n\r\n\r\n#-------------------------#\r\n# 5. Publication bias     #\r\n#-------------------------#\r\n\r\n### Effect size over time\r\nmeta_year <- rma.mv(Hedges_d_directional, Variance, mod= ~ Year, random= list(~ 1|Species_latin, ~ 1|species2, ~ 1|Study_no, ~ 1|obs), \r\n                    R= list(Species_latin = matrix), data= data, method= ""REML"")\r\nsummary(meta_year)\r\n\r\n# Bubble plot\r\npreds <- predict(meta_year, newmods=c(1992:2018))\r\ndata$size <- (1 / sqrt(data$Variance))/10 \r\n\r\nplot(NA, NA, xlim=c(1992,2018), ylim=c(-2.6,4.5),\r\n     xlab=""Publication year"", ylab=""Effect size (Hedges\' d)"",\r\n     las=1, bty=""l"")\r\nsymbols(data$Year, data$Hedges_d_directional, circles=data$size, inches=FALSE, add=TRUE, \r\n        fg= rgb(0,0,0, max=255, alpha=150), pch=21, col=rgb(0,0,0, max=255), bg=rgb(255,255,255, max=255, alpha=150))\r\nlines(1992:2018, preds$pred)\r\nlines(1992:2018, preds$ci.lb, lty=""dashed"")\r\nlines(1992:2018, preds$ci.ub, lty=""dashed"")\r\nabline(h=0, lty=""dotted"")\r\n\r\n\r\n### Trim and fill\r\n\r\n# Raw data\r\ntrimfill_left <- trimfill(meta1, side=""left"") \r\nsummary(trimfill_left) \r\n\r\ntrimfill_right <- trimfill(meta1, side=""right"")\r\nsummary(trimfill_right)\r\nfunnel(trimfill_right, yaxis=""seinv"", xlab=""Effect size (Hedges\' d)"", steps=11, digits=1, ylim=c(1, 11), xlim=c(-2.2, 5), \r\n       back=""white"", shade=""white"", hlines=""white"", pch=21, col=rgb(0,0,0, max=255), bg=rgb(255,255,255, max=255, alpha=150), \r\n       pch.fill=16)\r\nabline(v=0, lty=2)\r\n\r\n# Residuals\r\ntrimfill_left2 <- trimfill(meta_resid, side=""left"") \r\nsummary(trimfill_left2) \r\n\r\ntrimfill_right2 <- trimfill(meta_resid, side=""right"")\r\nsummary(trimfill_right2)\r\nfunnel(trimfill_right2, yaxis=""seinv"", xlab=""Residual effect size"", steps=11, digits=1, ylim=c(1, 11),back=""white"", shade=""white"", \r\n       hlines=""white"", pch=21, col=rgb(0,0,0, max=255), bg=rgb(255,255,255, max=255, alpha=150), pch.fill=16)\r\nabline(v=0, lty=2)\r\n\r\n\r\n### Egger\'s regression\r\n\r\n# Raw data\r\negger <- lm(precision ~ Hedges_d_directional, data= data)\r\nsummary(egger)\r\nplot(data$Hedges_d_directional, precision, xlab=""Effect size (Hedges\' d)"", ylab=""Inverse standard error"", ylim=c(0,10),\r\n      pch=21, col=rgb(0,0,0, max=255), bg=rgb(255,255,255, max=255, alpha=150)) \r\nabline(egger)\r\n\r\n# Residuals\r\negger2 <- lm(precision ~ zresiduals, data= data)\r\nsummary(egger2)\r\nplot(data$zresiduals, data$precision, xlab=""Residual effect size"", ylab=""Inverse standard error"", ylim=c(0,10), \r\n     pch=21, col=rgb(0,0,0, max=255), bg=rgb(255,255,255, max=255, alpha=150)) \r\nabline(egger2)\r\n\r\n\r\n\r\n##########################################################################################################\r\n##########################################################################################################']","Data from: A meta-analysis of factors influencing the strength of mate choice copying in animals Davies et al., (2020) All data and R codeMate-choice copying is a form of social learning in which an individual's choice of mate is influenced by the apparent choices of other individuals of the same sex, and has been observed in more than 20 species across a broad taxonomic range. Though fitness benefits of copying have proven difficult to measure, theory suggests that copying should not be beneficial for all species or contexts. However, the factors influencing the evolution and expression of copying have proven difficult to resolve. We systematically searched the literature for studies of mate-choice copying in non-human animals, and then performed a phylogenetically-controlled meta-analysis to explore which factors influence the expression of copying across species. Across 58 published studies in 23 species, we find strong evidence that animals copy the mate choice of others. The strength of copying was significantly influenced by taxonomic group, however sample size limitations mean it is difficult to draw firm conclusions regarding copying in mammals and arthropods. The strength of copying was also influenced by experimental design: copying was stronger when choosers were tested before and after witnessing a conspecific's mate choice, compared to when choosers with social information were compared to choosers without. Importantly, we did not detect any difference in the strength of copying between males and females, or in relation to the rate of multiple mating. Our search also highlights that more empirical work is needed to investigate copying in a broader range of species, especially those with differing mating systems and levels of reproductive investment.",3
Data for: Forecasting shifts in habitat suitability of three marine predators suggests a rapid decline in inter-specific overlap under future climate change,"Aim: To estimate spatiotemporal changes in habitat suitability and inter-specific overlap among three marine predators: Baltic grey seals (Halichoerus grypus grypus), harbour seals (Phoca vitulina), and harbour porpoises (Phocoena phocoena) under contemporary and future conditions.Location: The southwestern region of the Baltic Sea, including the Danish Straits and the Kattegat, one of the fastest-warming semi-enclosed seas in the world.Methods: Location data (>200 tagged individuals) were analysed within the maximum entropy (MaxEnt) algorithm to estimate changes in total area size and overlap of species-specific habitat suitability between 1997-2020 and 2091-2100. A total of eleven candidate predictor variables were considered representing anthropogenic activity, environmental, and climate sensitive oceanographic conditions in the area. Sea surface temperature and salinity data were taken from representative concentration pathways [RCPs] scenarios 6.0 and 8.5 to forecast potential climate change effects.Results: Model output suggests that habitat suitability of Baltic grey seals will decline drastically over space and time, largely driven by changes in sea surface salinity and a loss of currently available haulout sites following sea level rise in the future. A similar though weaker response was observed for harbour seals, while suitability of habitat for harbour porpoises was predicted to remain fairly stable over space and time. Inter-specific overlap in highly suitable habitat was predicted to increase slightly under RCP scenario 6.0 when compared to contemporary conditions but to largely disappear under RCP scenario 8.5.Main conclusions: Marine predators in the southwestern Baltic Sea and adjacent waters may respond differently to future climatic conditions, leading to divergent shifts in habitat suitability that are likely to decrease inter-specific overlap. We, therefore, conclude that climate change can lead to a marked redistribution of area use by marine predators in the region, which may influence local food-web dynamics and ecosystem functioning.","['\n\n\n\n#----------------------------------------------------#\n# 0 LOAD REQUIRED LIBRARIES\n#----------------------------------------------------#\n\n\nlibrary(rJava)\nlibrary(dismo)\nlibrary(ENMeval)\nmaxent()\t\t\t# maxent version 3.4.1\nlibrary(SDMtune)\nlibrary(zeallot)\nlibrary(raster)\n\n\n\n#----------------------------------------------------#\n# 1 LOAD LOCATION DATA\n#----------------------------------------------------#\n\nsetwd(""............"")\ndat <- read.table(""locationData4Dryad.txt"",sep=""\\t"",dec=""."", header=T)\ntable(dat$species)\n\n\n#----------------------------------------------------#\n# 2 SELECT HARBOUR PORPOISE\n#----------------------------------------------------#\n\nhp <- subset(dat, species==""Harbour porpoise"")\n\n\n#-----------------------------------------------------#\n# 3 LOAD COVARIATE STACK 1997-2020\n#-----------------------------------------------------#\n\npredict_now <- stack(""covariateStack_1997-2020.grd"")\npredict_now <- dropLayer(predict_now, c(11:12))\n\n\n#-----------------------------------------------------#\n# 4 CLIP BY MESS LAYER\n#-----------------------------------------------------#\n# clip by MESS layer 1997-2020\nmess <- stack(""MESSStack_1997-2020.grd"")\nmess <- dropLayer(mess, c(1:2))\nmess \n\npredict_now <- crop(predict_now, extent(mess), snap=""in"")\npredict_now <- mask(predict_now, mess)\n\n\n#-----------------------------------------------------#\n# 5 GENERATE BACKGROUND POINTS\n#-----------------------------------------------------#\n\n# load sampling effort rasters\ndens.ras <- raster(""SamplingEffortStack_1997-2020.grd"")\ndens.ras <- dropLayer(dens.ras, c(1:2))\n\nset.seed(25)\nbg <- xyFromCell(dens.ras, sample(ncell(dens.ras), 20000, prob=values(dens.ras),replace=T))\nbg <- prepareSWD(species = ""Harbour porpoise"", a = bg, env = predict_now)\n\nplot(bg@coords, cex=0.1)\n\n\n#-----------------------------------------------------#\n# 6 MAXENT MODEL PRUNING\n#-----------------------------------------------------#\n\n#take background coord from SDM tune (these are without NAs in the background locs)\nbg <- as.matrix(bg@coords)\ncolnames(bg) <- c(""lon"", ""lat"")\n\n\nps <- list(aggregation.factor =2)\ntune.args <- list(fc = c(""L"", ""Q"", ""LQ"", ""H""), rm = seq(0.5,\xa05,\xa00.5))\nenmeval_results <- ENMevaluate(hp[,c(3:4)], predict_now, partitions =""checkerboard2"", bg =bg,categorical = ""sediment_latlon"",\n                                 algorithm=\'maxent.jar\',overlap=F, partition.settings = ps, tune.args = tune.args, doClamp=TRUE)\n\nenmeval_results@results\n\n\n# IDENTIFY SETTINGS OF BEST MODEL\nbestmod <- enmeval_results@results[3,]\nbestmod\n\n\n#-----------------------------------------------------#\n# 7 LOAD PREDICTION RASTER RCP SCENARIOS\n#-----------------------------------------------------#\n\n#---------------------------------------------#\n# future raster 2090-2100 RCP6.0\n#--------------------------------------------#\n\npredictors_RCP_60 <- stack(""covariateStack_2090-2100RCP60.grd"")\npredictors_RCP_60 <- dropLayer(predictors_RCP_60, c(11:12))\n\n# clip by MESS layer 2090-2100\nmess <- stack(""MESSStack_2090-2100.grd"")\nmess <- dropLayer(mess, c(1:2))\nmess \n\npredictors_RCP_60 <- crop(predictors_RCP_60, extent(mess), snap=""in"")\npredictors_RCP_60 <- mask(predictors_RCP_60, mess)\n\n#---------------------------------------------#\n# future raster 2090-2100 RCP8.5\n#--------------------------------------------#\n\npredictors_RCP_85 <- stack(""covariateStack_2090-2100RCP85.grd"")\npredictors_RCP_85 <- dropLayer(predictors_RCP_85, c(11:12))\n\n# clip by MESS layer 2090-2100\nmess <- stack(""MESSStack_2090-2100.grd"")\nmess <- dropLayer(mess, c(1:2))\nmess \n\npredictors_RCP_85 <- crop(predictors_RCP_85, extent(mess), snap=""in"")\npredictors_RCP_85 <- mask(predictors_RCP_85, mess)\n\n\n#-------------------------------------------------------------------#\n# 8 run loop of best model with different testing and training data\n#--------------------------------------------------------------------#\n\n#------------------------------------#\n# set up files to generate in loop\n#------------------------------------#\n\n\nrast.now<- stack()\nrast_2090_60<- stack()\nrast_2090_85<- stack()\nl.model <- list()\nthreshold_all <- data.frame()\nAUC  <- data.frame()\nresponse_dat  <- data.frame()\nVI_dat  <- data.frame()\n\n\n#------------------------------------#\n# run the loop 10 times\n#------------------------------------#\nrun = 10\nfor(i in 1:run){\n\xa0 \xa0\n\t\xa0# generate dataset for each run\n\tsmp_size <- floor(0.8 * nrow(hp))\n\t train_ind <- sample(seq_len(nrow(hp)), size = smp_size)\n\ttrain <- hp[train_ind, ]\n\ttest <- hp[-train_ind, ]\n\t\n\t#------------------------------------#\n\t# run the best ENMeval model\n\t#------------------------------------#\n\tps <- list(aggregation.factor =2)\n\ttune.args <- list(fc = c(""LQ""), rm = 0.5)\n\n\tbest.mod <- ENMevaluate(train[,c(3:4)], predict_now, partitions =""checkerboard2"", bg =bg, categorical = ""sediment_latlon"",\n                                 algorithm=\'maxent.jar\',overlap=F, partition.settings = ps, tune.args = tune.args, doClamp=TRUE)\n\tl.model <- list(l.model, best.mod)\n\t\n\t#------------------------------------#\n\t# Doing Model Evaluation\n\t#------------------------------------#\n\t\xa0 \xa0mod.evT1 <- evaluate(best.mod@models[[1]], p = test[,c(3:4)], a = bg, x = predict_now)\n\t\xa0 \xa0\n\t#------------------------------------#\n\t\xa0 \xa0# Capturing the threshold values\n\t#------------------------------------#\n\t  threshold <- threshold(mod.evT1)\n\t  threshold $model <- i\n\t  threshold$Species <- ""Harbour porpoise""\n\t  threshold_all <- rbind(threshold_all, threshold)\n\t\xa0\n\t#------------------------------------#\n\t# Capturing the AUC values\n\t#------------------------------------#\n\t\xa0 \xa0auc.T1 <- as.data.frame(best.mod@results$auc.train)\n\t\xa0 \xa0names(auc.T1)[1] <- ""AUC""\n\t   auc.T1$model <- i\n\t   auc.T1 $Species <- ""Harbour porpoise""\n\t   AUC <- rbind(AUC, auc.T1)\n\t\n\t#------------------------------------#\n\t# get response data\n\t#------------------------------------#\n\t\n\t\n\tsound <- as.data.frame(response(best.mod@models[[1]], var = ""Soundscape""))\n\tsound $var <- ""Soundscape""\n\tsound $species <- ""Harbour porpoise""\n\tsound $rep <- i\n\tresponse_dat <- rbind(response_dat, sound)\n\t\n\tcod <- as.data.frame(response(best.mod@models[[1]], var = ""Cod.landings""))\n\tcod $var <- ""Cod.landings""\n\tcod $species <- ""Harbour porpoise""\n\tcod $rep <- i\n\tresponse_dat <- rbind(response_dat, cod)\n\t\t\n\tsprat <- as.data.frame(response(best.mod@models[[1]], var = ""Sprat.landings""))\n\tsprat $var <- ""Sprat.landings""\n\tsprat $species <- ""Harbour porpoise""\n\tsprat $rep <- i\n\tresponse_dat <- rbind(response_dat, sprat)\n\n\therring <- as.data.frame(response(best.mod@models[[1]], var = ""Herring.landings""))\n\therring $var <- ""Herring.landings""\n\therring $species <- ""Harbour porpoise""\n\therring $rep <- i\n\tresponse_dat <- rbind(response_dat, herring)\n\n\tsediment <- as.data.frame(response(best.mod@models[[1]], var = ""sediment_latlon""))\n\tsediment $var <- ""sediment_latlon""\n\tsediment $species <- ""Harbour porpoise""\n\tsediment $rep <- i\n\tresponse_dat <- rbind(response_dat, sediment)\n\n\tbathy <- as.data.frame(response(best.mod@models[[1]], var = ""Bathymetry""))\n\tbathy $var <- ""Bathymetry""\n\tbathy $species <- ""Harbour porpoise""\n\tbathy $rep <- i\n\tresponse_dat <- rbind(response_dat, bathy)\n\n\tslope <- as.data.frame(response(best.mod@models[[1]], var = ""Slope""))\n\tslope $var <- ""Slope""\n\tslope $species <- ""Harbour porpoise""\n\tslope $rep <- i\n\tresponse_dat <- rbind(response_dat, slope)\n\t\n\tcurrent <- as.data.frame(response(best.mod@models[[1]], var = ""Current.Velocity""))\n\tcurrent $var <- ""Current.Velocity""\n\tcurrent $species <- ""Harbour porpoise""\n\tcurrent $rep <- i\n\tresponse_dat <- rbind(response_dat, current)\n\t\n\tsalt <- as.data.frame(response(best.mod@models[[1]], var = ""Salinity""))\n\tsalt $var <- ""Salinity""\n\tsalt $species <- ""Harbour porpoise""\n\tsalt $rep <- i\n\tresponse_dat <- rbind(response_dat, salt)\n\n\ttemp <- as.data.frame(response(best.mod@models[[1]], var = ""Temperature""))\n\ttemp $var <- ""Temperature""\n\ttemp $species <- ""Harbour porpoise""\n\ttemp $rep <- i\n\tresponse_dat <- rbind(response_dat, temp)\n\t\n\t#------------------------------------#\n\t#  get variable importance\n\t#------------------------------------#\n\t\n\tVI <- as.data.frame(best.mod@variable.importance)\n\tnames(VI)[1] <- ""Variable""\n\tnames(VI)[2] <- ""Percent.contribution""\n\tnames(VI)[3] <- ""Permutation importance""\n\tVI $species <- ""Harbour porpoise""\n\tVI $rep <- i\n\tVI_dat <- rbind(VI_dat, VI)\n\t\n\t#------------------------------------#\n\t#  predict current and stack\n\t#------------------------------------#\n\t\xa0\xa0predict.now <- predict(predict_now,best.mod@models[[1]], type = \'cloglog\')\n\t\xa0 rast.now <- stack(rast.now, predict.now)\n\n\xa0 \t#------------------------------------#\n\t#  predict 2090-2100 RCP 6.0 and stack\n\t#------------------------------------#\n\t\xa0\xa0predict.fut3 <- predict(predictors_RCP_60,best.mod@models[[1]], type = \'cloglog\')\n\t\xa0 rast_2090_60 <- stack(rast_2090_60, predict.fut3)\n\n\xa0\xa0 \t#------------------------------------#\n\t#  predict 2090-2100 RCP 8.5 and stack\n\t#------------------------------------#\n\t\xa0\xa0predict.fut4 <- predict(predictors_RCP_85,best.mod@models[[1]], type = \'cloglog\')\n\t\xa0 rast_2090_85 <- stack(rast_2090_85, predict.fut4)\n\n \xa0}\n\n\n#-----------------------------------------------------#\n# 9 SAVE OUTPUT\n#-----------------------------------------------------#\n\n\n# save response data\n write.table(response_dat, file = ""...../ResponseCurveData/HarbPorpoise.txt"", row.names = FALSE, append = FALSE, col.names = TRUE, sep=""\\t"")\n\n# save variable importance data\n write.table(VI_dat, file = ""..../VarImportanceData/HarbPorpoise.txt"", row.names = FALSE, append = FALSE, col.names = TRUE, sep=""\\t"")\n \n# save AUC data\n write.table(AUC, file = ""..../AUCdata/HarbPorpoise.txt"", row.names = FALSE, append = FALSE, col.names = TRUE, sep=""\\t"")\n\n# save Thresholds data\n write.table(threshold_all, file = ""....../Thresholds/HarbPorpoise.txt"", row.names = FALSE, append = FALSE, col.names = TRUE, sep=""\\t"")\n\n # save rasters\nrast_now_mean <- calc(rast.now,mean)\nwriteRaster(rast_now_mean, filename = ""...../SDMrasters/mean_SDM_stack_now_harb_porpoise.grd"", overwrite=TRUE)\n\nrast_2090_60_mean <- calc(rast_2090_60,mean)\n writeRaster(rast_2090_60_mean, filename = ""...../SDMrasters/mean_SDM_stack_2090_60_harb_porpoise.grd"", overwrite=TRUE)\n\nrast_2090_85_mean <- calc(rast_2090_85,mean)\n writeRaster(rast_2090_85_mean, filename = ""......./SDMrasters/mean_SDM_stack_2090_85_harb_porpoise.grd"", overwrite=TRUE)\n\n\n']","Data for: Forecasting shifts in habitat suitability of three marine predators suggests a rapid decline in inter-specific overlap under future climate change Aim: To estimate spatiotemporal changes in habitat suitability and inter-specific overlap among three marine predators: Baltic grey seals (Halichoerus grypus grypus), harbour seals (Phoca vitulina), and harbour porpoises (Phocoena phocoena) under contemporary and future conditions.Location: The southwestern region of the Baltic Sea, including the Danish Straits and the Kattegat, one of the fastest-warming semi-enclosed seas in the world.Methods: Location data (>200 tagged individuals) were analysed within the maximum entropy (MaxEnt) algorithm to estimate changes in total area size and overlap of species-specific habitat suitability between 1997-2020 and 2091-2100. A total of eleven candidate predictor variables were considered representing anthropogenic activity, environmental, and climate sensitive oceanographic conditions in the area. Sea surface temperature and salinity data were taken from representative concentration pathways [RCPs] scenarios 6.0 and 8.5 to forecast potential climate change effects.Results: Model output suggests that habitat suitability of Baltic grey seals will decline drastically over space and time, largely driven by changes in sea surface salinity and a loss of currently available haulout sites following sea level rise in the future. A similar though weaker response was observed for harbour seals, while suitability of habitat for harbour porpoises was predicted to remain fairly stable over space and time. Inter-specific overlap in highly suitable habitat was predicted to increase slightly under RCP scenario 6.0 when compared to contemporary conditions but to largely disappear under RCP scenario 8.5.Main conclusions: Marine predators in the southwestern Baltic Sea and adjacent waters may respond differently to future climatic conditions, leading to divergent shifts in habitat suitability that are likely to decrease inter-specific overlap. We, therefore, conclude that climate change can lead to a marked redistribution of area use by marine predators in the region, which may influence local food-web dynamics and ecosystem functioning.",3
"Data from: Early to rise, early to breed: a role for daily rhythms in seasonal reproduction","Vertebrates use environmental cues to time reproduction to optimal breeding conditions. Numerous laboratory studies have revealed that light experienced during a critical window of the circadian (daily) rhythm can influence reproductive physiology. However, whether these relationships observed in captivity hold true under natural conditions and how they relate to observed variation in timing of reproductive output remains largely unexplored. Here we test the hypothesis that individual variation in daily timing recorded in nature (i.e. chronotype) is linked with variation in timing of breeding. To address this hypothesis and its generality across species, we recorded incubation behavior data to identify individual patterns in daily onset of activity for 2 temperate-breeding songbird species, the dark-eyed junco (Junco hyemalis aikeni) and the great tit (Parus major). We found that females who first departed from their nest earlier in the morning (earlier chronotype) also initiated nests earlier in the year. Date of data collection and ambient temperature had no effect, but stage of incubation influenced daily onset of activity in great tits. Our findings suggest a role for daily rhythms as one mechanism underlying the observed variation in seasonal timing of breeding.","['#Read in GRTI Data\r\nGRTI <- read.csv(""C:/Users/Jesee/Dropbox/DEJU-GRTI.Wakeup/CSV.Files/GreatTitRData.csv"")\r\n\r\n#GRTI Repeatability with Confidence Intervals\r\nrep<-lmer(wakeup ~ inc.day2 + (1|nestid), data=GRTI)\r\nsummary(rep, correlation=T)\r\nconfint(rep)\r\n\r\n#Linear Mixed Effects Model-GRTI\r\nGRTIModel <-lmer(wakeup ~ egg1 + inc.day2 + apr.date + (1|nestid), data=GRTI)\r\nsummary(GRTIModel, correlation=T)\r\nanova(GRTIModel)\r\n\r\n#Read in WWJU Data\r\nWWJU <- read.csv(""C:/Users/Jesee/Dropbox/DEJU-GRTI.Wakeup/CSV.Files/JuncoRData.csv"")\r\n\r\n#WWJU Repeatability with Confidence Intervals\r\nrep2 <-lmer(wakeup ~ incdate2 + (1|nestid), data=WWJU)\r\nsummary(rep2, correlation=T)\r\nconfint(rep2)\r\n\r\n#Linear Mixed Effects Model With Year as RE\r\nWWJUModel <-lmer(wakeup ~ egg1 + incdate2 + date + (1|nestid) + (1|Year), data=WWJU)\r\nsummary(WWJUModel, correlation=T)\r\nanova(WWJUModel)\r\n\r\n#Light Intensity at sunrise\r\nWWJUIntensity<-lmer(wakeup ~ intensity + egg1 + incdate2 + date + (1|nestid), data=WWJU)\r\nsummary(WWJUIntensity, correlation=T)\r\nanova(WWJUIntensity)\r\n\r\n\r\n#Linear Mixed Effects Model w/Amb Temp & Year as RE\r\nWWJUModelTemp <-lmer(wakeup ~ egg1 + incdate2 + date + ambtemp + (1|nestid) + (1|Year), data=WWJU)\r\nsummary(WWJUModelTemp, correlation=T)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n#Read in WWJU Data\r\nWWJU2 <- read.csv(""C:/Users/Jesee/Dropbox/DEJU-GRTI.Wakeup/CSV.Files/JuncoBivariateData.csv"")\r\n\r\n#Linear Mixed Effects Model With Year as RE\r\nWWJUBi <-MCMCglmm(cbind(wakeup, egg1) ~ trait:date + trait:incdate - 1, random = ~units:nestid, rcov = ~us(trait):units, prior = prior, family = c(""gaussian"", ""gaussian""), data=WWJU2)\r\nsummary(WWJUBi, correlation=T)\r\n\r\n#Linear Mixed Effects Model With Year as RE\r\nWWJUBi <-lm(cbind(wakeup, egg1) ~ date + incdate + (1|nestid) - 1, data=WWJU2)\r\nsummary(WWJUBi, corr=TRUE)\r\n']","Data from: Early to rise, early to breed: a role for daily rhythms in seasonal reproduction Vertebrates use environmental cues to time reproduction to optimal breeding conditions. Numerous laboratory studies have revealed that light experienced during a critical window of the circadian (daily) rhythm can influence reproductive physiology. However, whether these relationships observed in captivity hold true under natural conditions and how they relate to observed variation in timing of reproductive output remains largely unexplored. Here we test the hypothesis that individual variation in daily timing recorded in nature (i.e. chronotype) is linked with variation in timing of breeding. To address this hypothesis and its generality across species, we recorded incubation behavior data to identify individual patterns in daily onset of activity for 2 temperate-breeding songbird species, the dark-eyed junco (Junco hyemalis aikeni) and the great tit (Parus major). We found that females who first departed from their nest earlier in the morning (earlier chronotype) also initiated nests earlier in the year. Date of data collection and ambient temperature had no effect, but stage of incubation influenced daily onset of activity in great tits. Our findings suggest a role for daily rhythms as one mechanism underlying the observed variation in seasonal timing of breeding.",3
Data from: Near infrared spectroscopy (NIRS) predicts non-structural carbohydrate concentrations in different tissue types of a broad range of tree species,"1. The allocation of non-structural carbohydrates (NSCs) to reserves constitutes an important physiological mechanism associated with tree growth and survival. However, procedures for measuring NSC in plant tissue are expensive and time-consuming. Near-infrared spectroscopy (NIRS) is a high-throughput technology that has the potential to infer the concentration of organic constituents for a large number of samples in a rapid and inexpensive way based on empirical calibrations with chemical analysis. 2. The main objectives of this study were (i) to develop a general NSC concentration calibration that integrates various forms of variation such as tree species and tissue types and (ii) to identify characteristic spectral regions associated with NSC molecules. In total, 180 samples from different tree organs (root, stem, branch, leaf) belonging to 73 tree species from tropical and temperate biomes were analysed. Statistical relationships between NSC concentration and NIRS spectra were assessed using partial least squares regression (PLSR) and a variable selection procedure (competitive adaptive reweighted sampling, CARS), in order to identify key wavelengths. 3. Parsimonious and accurate calibration models were obtained for total NSC (r2 of 091, RMSE of 134% in external validation), followed by starch (r2 = 085 and RMSE = 120%) and sugars (r2 = 082 and RMSE = 110%). Key wavelengths coincided among these models and were mainly located in the 17401800, 21002300 and 24102490 nm spectral regions. 4. This study demonstrates the ability of general calibration model to infer NSC concentrations across species and tissue types in a rapid and cost-effective way. The estimation of NSC in plants using NIRS therefore serves as a tool for functional biodiversity research, in particular for the study of the growthsurvival trade-off and its implications in response to changing environmental conditions, including growth limitation and mortality.","['#Instrcci�n para remover la memoria temporal de objetos \nrm(list=ls())\n\nsetwd("""")\n\n# load libraries\nlibrary(pls)\nlibrary(signal)\nlibrary(carspls)\nlibrary(soil.spec)\n\n\n# define functions for vector normalization\ncenter <- function (x) {x-mean(x)}\nvnorm <- function (v) {v/sqrt(sum(v^2))}  \n###########################################################################################\n\n\n### read and process data ##############################################\nload(""wavelength.RData"")\np<-read.csv(""spectra_data.csv"", row.names=1)\ns<-read.csv(""carbohydrates_data.csv"", row.names=1)\n\nrange(s$Sugars)\nrange(s$Starch)\nrange(s$NSC)\n\n# match samples between both datasets\nid<-which(is.element(rownames(p), rownames(s)))\n\n####### Complete spectral range\n# select spectral range\ncolnames(p) <-round(wavelength,1)\nx<-which(wavelength>1300 & wavelength<2650)\n\n# select samples and spectral bands from dataset\npred<-p[id,x]\n\n# plot(wavelength[x], pred[1,], type=""l"", ylim=c(0,1))\n# for (i in 1:nrow(pred)){lines(wavelength[x], pred[i,])}\n\nwavelengthmean <-apply(p,2,mean) #spectrum averaged from all samples\nplot(wavelength,wavelengthmean ,type=""l"",xlim=c(1300,2600), ylim=c(0,1), ylab="""")\n\n# sort according to sample name\npred<-pred[sort(rownames(pred)),]\nresp<-s[sort(rownames(s)),]\n\n# check if successful\nrownames(tail(pred))\nrownames(tail(resp))\n\n################################################################\n### preprocessing done                                         #\n################################################################\n\n### independent validation ######################################\n# 1. ramdom selection\n#    spl<-sample(1:180, 120) # random sample of 120 from 1 to 180\n\n# 2. Selection based on the Kennard Stone algorithm only\n# spl<-ken.sto(pred, per = ""F"", num=120)\n\n# 3. Selection based on the Kennard Stone algorithm with modification\n# select 4 samples with highest NCS content and put ""993R2"" and ""pipbog2R"" \n# in calibration dataset\n\nselspec<-which(resp$NSC>16.7)\nselnames<-rownames(pred)[selspec]\n\nspl<-ken.sto(pred, per = ""F"", num=120)\n\n# check line number of other two species \nwhich(spl[[4]]==selnames[2])\nwhich(spl[[4]]==selnames[3])\n\nspl<-spl[[5]]\nspl<-c(spl, 58, 156) # put ""993R2"" and ""pipbog2R"" into calibration set \nspl<-spl[-c(48,55)]  # take the other two out; for validation\n\n# construct validation dataset\n\npredVal<-pred[-spl,]\nnscVal<-resp$NSC[-spl]\n\n\n# construct calibration dataset\npredCal<-pred[spl,]\nnscCal<-resp$NSC[spl]\n\n\n### vectornormalization and 1st derivative #####################\n\nd1pred <- t(apply(pred, 1, sgolayfilt, m=1))\nd1predC <- as.data.frame(d1pred[spl,])\nd1predV <- as.data.frame(d1pred[-spl,])\n\n# also for the whole spectral dataset\nd1p <- t(apply(p[,x], 1, sgolayfilt, m=1))\n\n\n# vector normalization\n# vnpred<-apply(pred, 1, center)\n# vnpred<-apply(vnpred, 1, vnorm)\n# d1VNpred <- t(apply(vnpred, 1, sgolayfilt, m=1))\n# \n# d1predC <- as.data.frame(vnpred[spl,])\n# d1predV <- as.data.frame(vnpred[-spl,])\n\n\n# CHECK THE PERFORMANCE OF VECTORNORMALIZATION AND 1ST DERIVATIVE ON VECTOR\n# NORMALIZED DATA\n\n############################################################################\n### CARS PLS                                                               #\n############################################################################\n\n# run CARS PLS a 100 times because of Monte Carlo approach\n\nm<-matrix(NA, ncol=3, nrow=100) # collect results for each run\ncolnames(m)<-c(""RMSECV.cars.pls"", ""nLV"", ""npred"" )\nselVariables<-list()\n\n# (i in 1:100)\nfor (i in 1:100){\n\ncpls<-carsplsorigin(d1predC, nscCal, ratio=1, fold=120, scale.pretreat=0, nLV=12, iteration=50, PartitionType=""random"") \n## Settings: fold=120 -> full leave one out cross validation, ratio=1 -> keep all 120 samples in each iteration (this can be changed to base each iteration on a random  subsample of 120)\n\nj<-cpls$Optimal.iteration\nm[i,1]<-cpls$RMSECV[j]\nm[i,2]<-cpls$NumLV[j]\nm[i,3]<-length(cpls$selected.variables)\n\nselVariables[[i]]<-cpls$selected.variables\n}\n\n# write out results overview\n# write.csv(m, file=""results_CARS.csv"")\n\n# choose best model i=x\nBestMod<- which(m[,1]==min(m[,1], na.rm=T))\nBestMod\n\n\n# extract selected Variables and number of LVs\nsel<-selVariables[[BestMod]]\nnlv<-m[BestMod,2]\n\n\n# rerun PLS model under the same parameter as carlspls\npls<-plsr(nscCal~as.matrix(d1predC[,sel]), ncomp=nlv, validation=""LOO"", method=""simpls"", scale=FALSE)\n\n# get RMSE and R2\nrms <- RMSEP(pls, ""all"")\nr2<-R2(pls, ""all"")\nrmscal<-rms$val[1,,nlv+1]\nrms<-rms$val[2,,nlv+1]\nr2<-r2$val[2,,nlv+1]\nnPred<-length(sel)\n\n## extract predicted values and residuals from the calibration pls model \n\nfit.val <- pls$validation$pred[,,nlv]\ny.residuals <- residuals (pls)[,,nlv]\n\n## prepare output: table of response, predicted values, and residuals\n\nyres <- cbind (nscCal, fit.val, y.residuals)\ncolnames (yres) <- c (""observed"", ""predicted"", ""residuals"")\nrownames (yres) <- rownames(d1predC)   \n\n# VALIDATION: Do prediction for external data\nres<-predict(pls, newdata=as.matrix(d1predV[,sel]), ncomp=nlv, type = c(""response""))\nrmsVal<-sqrt(mean((res[,1,1] - nscVal)^2)) # RMSE by hand\n\n# for plotting key wavelengths vs spectrum averaged\nticks<-as.numeric(colnames(pred)[sel])\n# for plotting regression coefficients (in red)\ncoef<-pls$coefficients[,1,nlv]\n\n############################################################################\n### Plot results                                                           #\n############################################################################\n\npdf(""_NSC_CARS_XLV_Xpred_KSmanipulated.pdf"")  # change X according to your best model\n\npar(mfrow=c(2,2))\n# 1st plot\nplot(yres[,1], yres[,2], col=resp[spl,2], pch=16, xlab=""Observed NSC [%]"", ylab=""Predicted NSC [%]"", main=""calibration (n=120)"", cex=0.75)\nabline(0,1)\nabline(lm(yres[,2]~yres[,1]), lwd=1, col=""grey"")\nlegend(""topleft"", legend=c(paste(""R2="",round(r2,digits=2), sep=""""), paste(""RMSE cal="",round(rmscal,digits=2), sep="""") ,paste(""RMSE CV="",round(rms,digits=2), sep=""""),paste(""nLV="",nlv, sep=""""),paste(""nPred="",nPred, sep=""""), paste(""RPD="",round(sd(nscCal)/rms, digits=2), sep="""")), cex=0.75, bty=""n"")\nlegend(""bottomright"", legend=c(""branches"",""leaves"",""roots"",""stem"" ), pch=16,col=c(1,2,3,4), cex=0.75, bty=""n"")\n\n# 2nd plot\nplot(nscVal,res[,1,1], col=resp[-spl,2], pch=16, xlab=""Observed NSC [%]"", ylab=""Predicted NSC [%]"", main=""external validation (n=60)"", cex=0.75, xlim=c(1,20), ylim=c(1,20))\nabline(0,1)\nlmVal<-lm(res[,1,1]~nscVal)\nabline(lmVal, , lwd=1, col=""grey"")\n\nlegend(""topleft"", legend=c(paste(""R2="",round(summary(lmVal)$r.squared, digits=2), sep=""""),  paste(""RMSE="",round(rmsVal, digits=2), sep=""""), paste(""RPD="",round(sd(nscVal)/rmsVal, digits=2), sep="""") ), cex=0.75, bty=""n"")\nlegend(""bottomright"", legend=c(""branches"",""leaves"",""roots"",""stem"" ), pch=16,col=c(1,2,3,4), cex=0.75, bty=""n"")\nsummary(lmVal)\n\n# 3rd plot\n\nplot(wavelength,wavelengthmean ,type=""l"",xlim=c(1300,2600), ylim=c(0,0.75), xlab=""wavelength (nm)"", ylab=""log (1/R)"")\nabline(v=seq(1400,2600,100),col=""gray"")\npoints(as.numeric(colnames(pred)[sel]),rep(.7,length(as.numeric(colnames(pred)[sel]))), col=rgb(0.2,0.2,0.2,0.2), pch=15, cex=1.75)\nlines(ticks, 0.5*abs(coef)/max(abs(coef)), type=""h"", lwd=1, col=""red"")\n\ndev.off()\n\n######## \n\n#### Graph1\npng(\'1stPlot.png\')\nplot(yres[,1], yres[,2], col=resp[spl,2], pch=16, xlab=""Observed NSC [%]"", ylab=""Predicted NSC [%]"", main=""calibration (n=120)"", cex=0.75)\nabline(0,1)\nabline(lm(yres[,2]~yres[,1]), lwd=1, col=""grey"")\nlegend(""topleft"", legend=c(paste(""R2="",round(r2,digits=2), sep=""""), paste(""RMSE cal="",round(rmscal,digits=2), sep="""") ,paste(""RMSE CV="",round(rms,digits=2), sep=""""),paste(""nLV="",nlv, sep=""""),paste(""nPred="",nPred, sep=""""), paste(""RPD="",round(sd(nscCal)/rms, digits=2), sep="""")), cex=0.75, bty=""n"")\nlegend(""bottomright"", legend=c(""branches"",""leaves"",""roots"",""stem"" ), pch=16,col=c(1,2,3,4), cex=0.75, bty=""n"")\ndev.off()\n\n#### Graph1_mod\npng(\'NSC_calibration.png\', width = 4, height = 4, units = \'in\', res = 600)\nplot(yres[,1], yres[,2], col=resp[spl,2], pch=16, xlab="""", ylab="""", cex=0.75, xlim=c(0,20), ylim=c(0,20), xaxs=\'i\',yaxs=\'i\')\nabline(0,1)\nabline(lm(yres[,2]~yres[,1]), lwd=1, col=""grey"")\nlegend(""topleft"", ""NSC (cross-validation)"", cex=1, bty=""n"")\ndev.off()\n\n#### Graph2_mod\npng(\'NSC_validation.png\', width = 4, height = 4, units = \'in\', res = 600)\nplot(nscVal,res[,1,1], col=resp[-spl,2], pch=16, xlab="""", ylab="""", cex=0.75, xlim=c(0,20), ylim=c(0,20), xaxs=\'i\',yaxs=\'i\')\nabline(0,1)\nlmVal<-lm(res[,1,1]~nscVal)\n#abline(lmVal, , lwd=1, col=""grey"")\nlegend(""topleft"", ""NSC (validation)"", cex=1, bty=""n"")\nlegend(""bottomright"", legend=c(""branches"",""leaves"",""roots"",""stem"" ), pch=16,col=c(1,2,3,4), cex=0.75, bty=""n"")\nsummary(lmVal)\ndev.off()\n\n#### Graph3_mod\npng(\'1_NSC_xpectra.png\', width = 4, height = 4, units = \'in\', res = 600)\nplot(wavelength,wavelengthmean ,type=""l"",xlim=c(1300,2620), ylim=c(0,1.1), xlab="""", ylab="""", xaxs=\'i\',yaxs=\'i\')\npoints(as.numeric(colnames(pred)[sel]),rep(1.05,length(as.numeric(colnames(pred)[sel]))), col=rgb(0.5,0.5,0.5,0.5), pch=15, cex=1.75)\npearson<-abs(coef)/max(abs(coef))\nlines(ticks, pearson, type=""h"", lwd=1, col=""black"")\nlegend(y=1, x=1300, ""NSC"", cex=1, bty=""n"")\ndev.off()\n\nvalidation_table <-data.frame(nscVal,res[,1,1],nscVal-res[,1,1])\n']","Data from: Near infrared spectroscopy (NIRS) predicts non-structural carbohydrate concentrations in different tissue types of a broad range of tree species 1. The allocation of non-structural carbohydrates (NSCs) to reserves constitutes an important physiological mechanism associated with tree growth and survival. However, procedures for measuring NSC in plant tissue are expensive and time-consuming. Near-infrared spectroscopy (NIRS) is a high-throughput technology that has the potential to infer the concentration of organic constituents for a large number of samples in a rapid and inexpensive way based on empirical calibrations with chemical analysis. 2. The main objectives of this study were (i) to develop a general NSC concentration calibration that integrates various forms of variation such as tree species and tissue types and (ii) to identify characteristic spectral regions associated with NSC molecules. In total, 180 samples from different tree organs (root, stem, branch, leaf) belonging to 73 tree species from tropical and temperate biomes were analysed. Statistical relationships between NSC concentration and NIRS spectra were assessed using partial least squares regression (PLSR) and a variable selection procedure (competitive adaptive reweighted sampling, CARS), in order to identify key wavelengths. 3. Parsimonious and accurate calibration models were obtained for total NSC (r2 of 091, RMSE of 134% in external validation), followed by starch (r2 = 085 and RMSE = 120%) and sugars (r2 = 082 and RMSE = 110%). Key wavelengths coincided among these models and were mainly located in the 17401800, 21002300 and 24102490 nm spectral regions. 4. This study demonstrates the ability of general calibration model to infer NSC concentrations across species and tissue types in a rapid and cost-effective way. The estimation of NSC in plants using NIRS therefore serves as a tool for functional biodiversity research, in particular for the study of the growthsurvival trade-off and its implications in response to changing environmental conditions, including growth limitation and mortality.",3
"Data from: Larval density, sex and allocation hierarchy affect life-history trait covariances in a bean beetle","Life-history theory aims to understand how different environments result in differential investment in fitness-related traits. While trade-offs between traits are expected, many studies show positive or no correlation between pairs of costly traits. One hypothesis that may explain the inconsistency of trade-offs in the literature is that trait investment may occur in a dichotomous hierarchy (the tree model), that allows for differential trait investment weighted by the traits' respective positions within the hierarchy. Previous mathematical models predict different covariances between traits depending on their position on the allocation tree. While hierarchical differential investment is often used to discuss findings in life-history theory, the role of an allocation hierarchy on trait covariances has not been directly tested. In turn, this study aims to identify trait covariances between behavioral and morphological phenotypes on different branches of an allocation tree in the bean beetle, Callosobruchus maculatus. While trade-offs between copulatory behaviors and morphology were found for both males and females, only traits at the base and far from each other in the hierarchy negatively covaried. This study empirically shows that trade-offs may be the result of hierarchical investment.","['#Clear environment\r\nrm(list = ls())\r\n\r\n#Set working directory.\r\nsetwd(""C:/Users/SamGascoigne-G6Holdi/Desktop/Research stuff/Summer 2019"")\r\n\r\n#Import data frame.\r\ndata1<-read.csv(""Density Experiment Data Final.csv"")\r\n\r\n#Subset data1 to remove NA values for wing length. \r\n#This is required as principal component analysis requires a complete data set.\r\ndata1prime<- subset(data1, !is.na(Wing_Length))\r\ndata1primeMALE<-subset(data1prime, data1prime$Sex == ""M"")\r\ndata1primeFEMALE<-subset(data1prime, data1prime$Sex == ""F"")\r\n\r\ndata1MALE<-subset(data1, data1$Sex == ""M"")\r\ndata1FEMALE<-subset(data1, data1$Sex == ""F"")\r\n\r\n#Call packages for data manipulation (dplyr) and data visualization (ggplot2, ggbiplot).\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nlibrary(ggpubr)\r\nlibrary(ggbiplot)\r\nlibrary(car)\r\nlibrary(lsr)\r\n\r\n#Male graphs and analysis ----\r\n\r\nmale_bodymass <- ggplot(data1MALE, aes(x=Density_Level, y=log(Body_Mass_mg), fill=Density_Level)) +\r\n  geom_boxplot(alpha = 0.5, outlier.size = -1, lwd = 0.75, color = \'black\') +\r\n  geom_jitter(shape = 21, width = 0.25, size = 2) +\r\n  scale_x_discrete(limits = c(""L"", ""M"", ""H""), labels = c(""low"", ""medium"", ""high"")) +\r\n  labs(x=""density level"", y = ""ln(body mass (mg))"", fill = ""density level"") +\r\n  scale_fill_manual(values=c(\'#C10D0D\',\'#E69F00\',  \'#008000\'), breaks=c(""H"", ""M"", ""L""), labels = c(""high"", ""medium"", ""low"")) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(1.0,1.9)) +\r\n  theme_classic(base_size=15,base_family = ""serif"") +\r\n  theme(legend.position=""none"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,0,1,1), ""lines""))\r\nmale_bodymass\r\n\r\naov_male_bodymass <- aov(log(data1MALE$Body_Mass_mg) ~ data1MALE$Density_Level)\r\nsummary(aov_male_bodymass)\r\neta_male_bodymass <- etaSquared(aov_male_bodymass)\r\neta_male_bodymass\r\n\r\n\r\nmale_antennation_rate <- ggplot(data1MALE, aes(x=Density_Level, y=ant_rate_AVG, fill=Density_Level)) +\r\n  geom_boxplot(alpha = 0.5, outlier.size = -1, lwd = 0.75, color = \'black\') +\r\n  geom_jitter(shape = 21, width = 0.25, size = 2) +\r\n  scale_x_discrete(limits = c(""L"", ""M"", ""H""), labels = c(""low"", ""medium"", ""high"")) +\r\n  labs(x=""density level"", y = ""antennation rate (taps/second)"", fill = ""density level"") +\r\n  scale_fill_manual(values=c(\'#C10D0D\',\'#E69F00\',  \'#008000\'), breaks=c(""H"", ""M"", ""L""), labels = c(""high"", ""medium"", ""low"")) +\r\n  scale_y_continuous() +\r\n  theme_classic(base_size=15,base_family = ""serif"") +\r\n  theme(legend.position=""none"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nmale_antennation_rate\r\n\r\naov_male_antennation_rate <- aov(data1MALE$ant_rate_AVG ~ data1MALE$Density_Level)\r\nsummary(aov_male_antennation_rate)\r\neta_male_antennation_rate <- etaSquared(aov_male_antennation_rate)\r\neta_male_antennation_rate\r\nTukeyHSD(aov_male_antennation_rate)\r\n\r\nmale_copulation_duration <- ggplot(data1MALE, aes(x=Density_Level, y=Copulation_Duration, fill=Density_Level)) +\r\n  geom_boxplot(alpha = 0.5, outlier.size = -1, lwd = 0.75, color = \'black\') +\r\n  geom_jitter(shape = 21, width = 0.25, size = 2) +\r\n  scale_x_discrete(limits = c(""L"", ""M"", ""H""), labels = c(""low"", ""medium"", ""high"")) +\r\n  labs(x=""density level"", y = ""copulation duration (seconds)"", fill = ""density level"") +\r\n  scale_fill_manual(values=c(\'#C10D0D\',\'#E69F00\',  \'#008000\'), breaks=c(""H"", ""M"", ""L""), labels = c(""high"", ""medium"", ""low"")) +\r\n  scale_y_continuous() +\r\n  theme_classic(base_size=15,base_family = ""serif"") +\r\n  theme(legend.position=""none"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nmale_copulation_duration\r\n\r\naov_male_copulation_duration <- aov(data1MALE$Copulation_Duration ~ data1MALE$Density_Level)\r\nsummary(aov_male_copulation_duration)\r\neta_male_copulation_duration <- etaSquared(aov_male_copulation_duration)\r\neta_male_copulation_duration\r\nTukeyHSD(aov_male_copulation_duration)\r\n\r\nmale_ant_versus_cop <- ggplot(data1, aes(x=ant_rate_AVG, y=Copulation_Duration, fill=Density_Level, group=interaction(Density_Level,Sex))) +\r\n  scale_fill_manual(values=c(\'#008000\',\'#E69F00\', \'#C10D0D\'), breaks=c(""L"", ""M"", ""H""), labels=c(""low"",""medium"",""high"")) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  geom_point(shape = 21, size = 2) +\r\n  scale_x_continuous() +\r\n  scale_y_continuous(limits=c(150,800)) +\r\n  labs(x=""antennation rate (taps/second)"", y = ""copulation duration (seconds)"", fill = ""density level"")+\r\n  theme_classic(base_size=15, base_family = ""serif"") +\r\n  theme(legend.position = \'none\',\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""),\r\n        legend.text = element_text(size=15))\r\nmale_ant_versus_cop\r\n\r\naov_male_ant_versus_cop <- aov(data1$Copulation_Duration~data1$ant_rate_AVG*data1$Density_Level, data=data1)\r\nAnova(aov_male_ant_versus_cop, type = ""III"")\r\n\r\nmale_relative_testis <- ggplot(data1primeMALE, aes(x=Density_Level, y=(log(Testes)/(log((Body_Mass_mg)^2))), fill=Density_Level)) +\r\n  geom_boxplot(alpha = 0.5, outlier.size = -1, lwd = 0.75, color = \'black\') +\r\n  geom_jitter(shape = 21, width = 0.25, size = 2) +\r\n  scale_x_discrete(limits = c(""L"", ""M"", ""H""), labels = c(""low"", ""medium"", ""high"")) +\r\n  labs(x=""density level"", y = ""relative testis area"", fill = ""density level"") +\r\n  scale_fill_manual(values=c(\'#C10D0D\',\'#E69F00\',  \'#008000\'), breaks=c(""H"", ""M"", ""L""), labels = c(""high"", ""medium"", ""low"")) +\r\n  scale_y_continuous() +\r\n  theme_classic(base_size=15,base_family = ""serif"") +\r\n  theme(legend.position=""none"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,0,1,1), ""lines""))\r\nmale_relative_testis\r\n\r\naov_male_relative_testis <- aov((log(data1primeMALE$Testes)/log((data1primeMALE$Body_Mass_mg)^2)) ~ data1primeMALE$Density_Level)\r\nsummary(aov_male_relative_testis)\r\neta_male_relative_testis <- etaSquared(aov_male_relative_testis)\r\neta_male_relative_testis\r\nTukeyHSD(aov_male_relative_testis)\r\n\r\nmale_relative_wing <- ggplot(data1primeMALE, aes(x=Density_Level, y=(log(Wing_Length)/log(Body_Mass_mg)), fill=Density_Level)) +\r\n  geom_boxplot(alpha = 0.5, outlier.size = -1, lwd = 0.75, color = \'black\') +\r\n  geom_jitter(shape = 21, width = 0.25, size = 2) +\r\n  scale_x_discrete(limits = c(""L"", ""M"", ""H""), labels = c(""low"", ""medium"", ""high"")) +\r\n  labs(x=""density level"", y = ""relative wing length"", fill = ""density level"") +\r\n  scale_fill_manual(values=c(\'#C10D0D\',\'#E69F00\',  \'#008000\'), breaks=c(""H"", ""M"", ""L""), labels = c(""high"", ""medium"", ""low"")) +\r\n  scale_y_continuous() +\r\n  theme_classic(base_size=15,base_family = ""serif"") +\r\n  theme(legend.position=""none"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nmale_relative_wing\r\n\r\naov_male_relative_wing <- aov((log(data1primeMALE$Wing_Length)/log(data1primeMALE$Body_Mass_mg)) ~ data1primeMALE$Density_Level)\r\nsummary(aov_male_relative_wing)\r\neta_male_relative_wing <- etaSquared(aov_male_relative_wing)\r\neta_male_relative_wing\r\nTukeyHSD(aov_male_relative_wing)\r\n\r\nmale_relative_testis_versus_wing <- ggplot(data1MALE, aes(x=(log(Testes)/(log((Body_Mass_mg)^2))), y=log(Wing_Length)/log(Body_Mass_mg), fill=Density_Level, group=interaction(Density_Level,Sex))) +\r\n  scale_fill_manual(values=c(\'#008000\',\'#E69F00\', \'#C10D0D\'), breaks=c(""L"", ""M"", ""H""), labels=c(""low"",""medium"",""high"")) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  geom_point(shape = 21, size = 2) +\r\n  scale_x_continuous(limits = c(-1.1,-0.4)) +\r\n  scale_y_continuous() +\r\n  labs(x=""relative testis area"", y = ""relative wing length"", fill = ""density level"")+\r\n  theme_classic(base_size=15, base_family = ""serif"") +\r\n  theme(legend.position = \'none\', \r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nmale_relative_testis_versus_wing\r\n\r\naov_male_relative_testis_versus_wing <- aov(as.numeric((log(Wing_Length)/log(Body_Mass_mg))) ~ as.numeric((log(Testes)/(log(Body_Mass_mg)^2)))*Density_Level, data = data1MALE)\r\nAnova(aov_male_relative_testis_versus_wing, type = ""III"")\r\n\r\nmale_relative_testis_versus_ant_rate <- ggplot(data1MALE, aes(x=(log(Testes)/log((Body_Mass_mg)^2)), y=ant_rate_AVG, fill=Density_Level)) +\r\n  scale_fill_manual(values=c(\'#008000\',\'#E69F00\',  \'#C10D0D\'), breaks=c(""L"", ""M"", ""H""), labels=c(""low"",""medium"",""high"")) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  geom_point(shape = 21, size = 2) +\r\n  scale_x_continuous(limits = c(-1.1,-0.4)) +\r\n  scale_y_continuous() +\r\n  labs(x=""relative testis area"", y = ""antennation rate (taps/second)"", fill = ""density level"")+\r\n  theme_classic(base_size=15, base_family = ""serif"") +\r\n  theme(legend.position = ""bottom"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nmale_relative_testis_versus_ant_rate\r\n\r\naov_male_relative_testis_versus_ant_rate <- aov(ant_rate_AVG~log((Testes)/(log((Body_Mass_mg)^2)))*Density_Level, data = data1MALE)\r\nAnova(aov_male_relative_testis_versus_ant_rate, type = ""III"")\r\n\r\nmale_relative_wing_versus_ant_rate <- ggplot(data1MALE, aes(x=(log(Wing_Length)/log(Body_Mass_mg)), y=ant_rate_AVG, fill=Density_Level)) +\r\n  scale_fill_manual(values=c(\'#008000\',\'#E69F00\',  \'#C10D0D\'), breaks=c(""L"", ""M"", ""H""), labels=c(""low"",""medium"",""high"")) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  geom_point(shape = 21, size = 2) +\r\n  scale_x_continuous() +\r\n  scale_y_continuous() +\r\n  labs(x=""relative wing length"", y = ""antennation rate (taps/second)"", fill = ""density level"")+\r\n  theme_classic(base_size=15, base_family = ""serif"") +\r\n  theme(axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nmale_relative_wing_versus_ant_rate\r\n\r\naov_male_relative_wing_versus_ant_rate <- aov(ant_rate_AVG~log((Wing_Length)/(log(Body_Mass_mg)))*Density_Level, data = data1MALE)\r\nAnova(aov_male_relative_wing_versus_ant_rate, type = ""III"")\r\n\r\n\r\n\r\nmale_relative_testis_versus_cop <- ggplot(data1primeMALE, aes(x=(log(Testes)/log((Body_Mass_mg)^2)), y=Copulation_Duration, fill=Density_Level)) +\r\n  scale_fill_manual(values=c(\'#008000\',\'#E69F00\',  \'#C10D0D\'), breaks=c(""L"", ""M"", ""H""), labels=c(""low"",""medium"",""high"")) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  geom_point(shape = 21, size = 2) +\r\n  scale_x_continuous(limits = c(-1.1,-0.4)) +\r\n  scale_y_continuous() +\r\n  labs(x=""relative testis area"", y = ""copulation duration (seconds)"", fill = ""density level"")+\r\n  theme_classic(base_size=15, base_family = ""serif"") +\r\n  theme(axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nmale_relative_testis_versus_cop\r\n\r\naov_male_relative_testis_versus_cop <- aov(Copulation_Duration~log((Testes)/(log((Body_Mass_mg)^2)))*Density_Level, data1prime)\r\nAnova(aov_male_relative_testis_versus_cop, type = ""III"")\r\n\r\nmale_relative_wing_versus_cop <- ggplot(data1MALE, aes(x=(log(Wing_Length)/log(Body_Mass_mg)), y=Copulation_Duration, fill=Density_Level)) +\r\n  scale_fill_manual(values=c(\'#008000\',\'#E69F00\',  \'#C10D0D\'), breaks=c(""L"", ""M"", ""H""), labels=c(""low"",""medium"",""high"")) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  geom_point(shape = 21, size = 2) +\r\n  scale_x_continuous() +\r\n  scale_y_continuous() +\r\n  labs(x=""relative wing length"", y = ""copulation duration (seconds)"", fill = ""density level"")+\r\n  theme_classic(base_size=15, base_family = ""serif"") +\r\n  theme(axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nmale_relative_wing_versus_cop\r\n\r\naov_male_relative_wing_versus_cop <- aov(Copulation_Duration~as.numeric(log(Wing_Length)/(log(Body_Mass_mg)))*Density_Level, data=data1MALE)\r\nAnova(aov_male_relative_wing_versus_cop, type = ""III"")\r\n\r\n\r\n\r\nmale_g1 <- ggarrange(male_antennation_rate, male_copulation_duration, male_ant_versus_cop,\r\n                     male_relative_testis, male_relative_wing, male_relative_testis_versus_wing,\r\n                     nrow = 2, ncol = 3, widths = c(1,1,1), common.legend = FALSE, legend = NULL, align = c(\'hv\'))\r\nmale_g1\r\n\r\nmale_g2 <- ggarrange(male_relative_testis_versus_ant_rate, male_relative_wing_versus_ant_rate,\r\n                     male_relative_testis_versus_cop, male_relative_wing_versus_cop,\r\n                     nrow = 1, ncol = 4, common.legend = TRUE, legend = ""bottom"")\r\nmale_g2\r\n\r\nfig3 <- ggarrange(male_g1, male_g2, nrow = 2, ncol = 1, heights = c(1.5,1), align = c(\'hv\'))\r\nfig3\r\n\r\n\r\n#Female graphs and analysis ----\r\n\r\nfemale_bodymass <- ggplot(data1FEMALE, aes(x=Density_Level, y=log(Body_Mass_mg), fill=Density_Level)) +\r\n  geom_boxplot(alpha = 0.5, outlier.size = -1, lwd = 0.75, color = \'black\') +\r\n  geom_jitter(shape = 21, width = 0.25, size = 2) +\r\n  scale_x_discrete(limits = c(""L"", ""M"", ""H""), labels = c(""low"", ""medium"", ""high"")) +\r\n  labs(x=""density level"", y = ""ln(body mass (mg))"", fill = ""density level"") +\r\n  scale_fill_manual(values=c(\'#C10D0D\',\'#E69F00\',  \'#008000\'), breaks=c(""H"", ""M"", ""L""), labels = c(""high"", ""medium"", ""low"")) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(1.0,1.9)) +\r\n  theme_classic(base_size=15,base_family = ""serif"") +\r\n  theme(legend.position=""none"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfemale_bodymass\r\n\r\naov_female_bodymass <- aov(log(data1FEMALE$Body_Mass_mg) ~ data1FEMALE$Density_Level)\r\nsummary(aov_female_bodymass)\r\neta_female_bodymass <- etaSquared(aov_female_bodymass)\r\neta_female_bodymass\r\n\r\n\r\nfemale_lat_kick <- ggplot(data1, aes(x=Density_Level, y=log(Latency_to_Kicking), fill=Density_Level)) +\r\n  geom_boxplot(alpha = 0.5, outlier.size = -1, lwd = 0.75, color = \'black\') +\r\n  geom_jitter(shape = 21, width = 0.25, size = 2) +\r\n  scale_x_discrete(limits = c(""L"", ""M"", ""H""), labels = c(""low"", ""medium"", ""high"")) +\r\n  labs(x=""density level"", y = ""ln(latency to kicking (seconds))"", fill = ""density level"") +\r\n  scale_fill_manual(values=c(\'#C10D0D\',\'#E69F00\',\'#008000\'), breaks=c(""H"", ""M"", ""L""), labels = c(""high"", ""medium"", ""low"")) +\r\n  scale_y_continuous() +\r\n  theme_classic(base_size=15,base_family = ""serif"") +\r\n  theme(legend.position=""none"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfemale_lat_kick\r\n\r\naov_female_lat_kick <- aov(log(data1$Latency_to_Kicking) ~ data1$Density_Level)\r\nsummary(aov_female_lat_kick)\r\neta_female_lat_kick <- etaSquared(aov_female_lat_kick)\r\neta_female_lat_kick\r\nTukeyHSD(aov_female_lat_kick)\r\n\r\nfemale_clutch <- ggplot(data1, aes(x=Density_Level, y=(Clutch_Size/log(Body_Mass_mg)), fill=Density_Level)) +\r\n  geom_boxplot(alpha = 0.5, outlier.size = -1, lwd = 0.75, color = \'black\') +\r\n  geom_jitter(shape = 21, width = 0.25, size = 2) +\r\n  scale_x_discrete(limits = c(""L"", ""M"", ""H""), labels = c(""low"", ""medium"", ""high"")) +\r\n  labs(x=""density level"", y = ""relative initial clutch size"", fill = ""density level"") +\r\n  scale_fill_manual(values=c(\'#C10D0D\',\'#E69F00\',  \'#008000\'), \r\n                    breaks=c(""H"", ""M"", ""L""), \r\n                    labels = c(""high"", ""medium"", ""low"")) +\r\n  scale_y_continuous() +\r\n  theme_classic(base_size=15,base_family = ""serif"") +\r\n  theme(legend.position = ""none"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfemale_clutch\r\n\r\naov_female_clutch <- aov(data1$Clutch_Size/log(data1$Body_Mass_mg) ~ data1$Density_Level)\r\nsummary(aov_female_clutch)\r\neta_female_clutch <- etaSquared(aov_female_clutch)\r\neta_female_clutch\r\nTukeyHSD(aov_female_clutch)\r\n\r\nfemale_lat_kick_versus_clutch<- ggplot(data1, aes(x=log(Latency_to_Kicking), y=Clutch_Size/log(Body_Mass_mg), fill=Density_Level, group=interaction(Density_Level,Sex))) +\r\n  scale_fill_manual(values=c(\'#008000\',\'#E69F00\', \'#C10D0D\'), breaks=c(""L"", ""M"", ""H""), labels=c(""low"",""medium"",""high"")) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  geom_point(shape = 21, size = 2) +\r\n  scale_x_continuous() +\r\n  scale_y_continuous() +\r\n  labs(x=""ln(latency to kicking (seconds))"", y = ""clutch size"", fill = ""density level"")+\r\n  theme_classic(base_size=15, base_family = ""serif"") +\r\n  theme(legend.position = ""none"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfemale_lat_kick_versus_clutch\r\n\r\naov_female_lat_kick_versus_clutch <- aov((Clutch_Size/log(Body_Mass_mg))~log(Latency_to_Kicking)*Density_Level, data=data1)\r\nAnova(aov_female_lat_kick_versus_clutch, type = ""III"")\r\n\r\nfemale_relative_ovariole <- ggplot(data1primeFEMALE, aes(x=Density_Level, y=(log(Ovarioles)/log((Body_Mass_mg)^2)), fill=Density_Level)) +\r\n  geom_boxplot(alpha = 0.5, outlier.size = -1, lwd = 0.75, color = \'black\') +\r\n  geom_jitter(shape = 21, width = 0.25, size = 2) +\r\n  scale_x_discrete(limits = c(""L"", ""M"", ""H""), labels = c(""low"", ""medium"", ""high"")) +\r\n  labs(x=""density level"", y = ""relative ovariole area"", fill = ""density level"") +\r\n  scale_fill_manual(values=c(\'#C10D0D\',\'#E69F00\',  \'#008000\'), breaks=c(""H"", ""M"", ""L""), labels = c(""high"", ""medium"", ""low"")) +\r\n  scale_y_continuous() +\r\n  theme_classic(base_size=15,base_family = ""serif"") +\r\n  theme(legend.position=""none"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfemale_relative_ovariole\r\n\r\naov_female_relative_ovariole <- aov((log(data1primeFEMALE$Ovarioles)/log((data1primeFEMALE$Body_Mass_mg)^2)) ~ data1primeFEMALE$Density_Level)\r\nsummary(aov_female_relative_ovariole)\r\neta_female_relative_ovariole <- etaSquared(aov_female_relative_ovariole)\r\neta_female_relative_ovariole\r\nTukeyHSD(aov_female_relative_ovariole)\r\n\r\nfemale_relative_wing <- ggplot(data1primeFEMALE, aes(x=Density_Level, y=(log(Wing_Length)/log(Body_Mass_mg)), fill=Density_Level)) +\r\n  geom_boxplot(alpha = 0.5, outlier.size = -1, lwd = 0.75, color = \'black\') +\r\n  geom_jitter(shape = 21, width = 0.25, size = 2) +\r\n  scale_x_discrete(limits = c(""L"", ""M"", ""H""), labels = c(""low"", ""medium"", ""high"")) +\r\n  labs(x=""density level"", y = ""relative wing length"", fill = ""density level"") +\r\n  scale_fill_manual(values=c(\'#C10D0D\',\'#E69F00\',  \'#008000\'), breaks=c(""H"", ""M"", ""L""), labels = c(""high"", ""medium"", ""low"")) +\r\n  scale_y_continuous() +\r\n  theme_classic(base_size=15,base_family = ""serif"") +\r\n  theme(legend.position=""none"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfemale_relative_wing\r\n\r\naov_female_relative_wing <- aov((log(data1primeFEMALE$Wing_Length)/log((data1primeFEMALE$Body_Mass_mg)^2)) ~ data1primeFEMALE$Density_Level)\r\nsummary(aov_female_relative_wing)\r\neta_female_relative_wing <- etaSquared(aov_female_relative_wing)\r\neta_female_relative_wing\r\nTukeyHSD(aov_female_relative_ovariole)\r\n\r\nfemale_relative_ovariole_versus_relative_wing<- ggplot(data1primeFEMALE, aes(x=(log(Ovarioles)/log((Body_Mass_mg)^2)), y=(log(Wing_Length)/log(Body_Mass_mg)), fill=Density_Level, group=interaction(Density_Level,Sex))) +\r\n  scale_fill_manual(values=c(\'#008000\',\'#E69F00\', \'#C10D0D\'), breaks=c(""L"", ""M"", ""H""), labels=c(""low"",""medium"",""high"")) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  geom_point(shape = 21, size = 2) +\r\n  scale_x_continuous() +\r\n  scale_y_continuous() +\r\n  labs(x=""relative ovariole area"", y = ""relative wing length"", fill = ""density level"")+\r\n  theme_classic(base_size=15, base_family = ""serif"") +\r\n  theme(legend.position = ""none"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfemale_relative_ovariole_versus_relative_wing\r\n\r\naov_female_relative_ovariole_versus_relative_wing <- aov(as.numeric(log(Wing_Length)/log(Body_Mass_mg)) ~ as.numeric(log(Ovarioles)/(log(Body_Mass_mg)^2))*Density_Level, data = data1primeFEMALE)\r\nAnova(aov_female_relative_ovariole_versus_relative_wing, type = ""III"")\r\n\r\nfemale_relative_ovariole_versus_lat_kick <- ggplot(data1primeFEMALE, aes(x=(log(Ovarioles)/log((Body_Mass_mg)^2)), y=log(Latency_to_Kicking), fill=Density_Level)) +\r\n  scale_fill_manual(values=c(\'#008000\',\'#E69F00\', \'#C10D0D\'), breaks=c(""L"", ""M"", ""H""), labels=c(""low"",""medium"",""high"")) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  geom_point(shape = 21, size = 2) +\r\n  scale_x_continuous() +\r\n  scale_y_continuous() +\r\n  labs(x=""relative ovariole area"", y = ""ln(latency to kicking (seconds))"", fill = ""density level"")+\r\n  theme_classic(base_size=15, base_family = ""serif"") +\r\n  theme(axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfemale_relative_ovariole_versus_lat_kick\r\n\r\naov_female_relative_ovariole_versus_lat_kick <- aov(log(Latency_to_Kicking) ~ as.numeric(log(Ovarioles)/(log(Body_Mass_mg)^2))*Density_Level, data = data1primeFEMALE)\r\nAnova(aov_female_relative_ovariole_versus_lat_kick, type = ""III"")\r\n\r\nfemale_relative_wing_versus_lat_kick<- ggplot(data1primeFEMALE, aes(x=(log(Wing_Length)/log(Body_Mass_mg)), y=log(Latency_to_Kicking), fill=Density_Level)) +\r\n  scale_fill_manual(values=c(\'#008000\',\'#E69F00\', \'#C10D0D\'), breaks=c(""L"", ""M"", ""H""), labels=c(""low"",""medium"",""high"")) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  geom_point(shape = 21, size = 2) +\r\n  scale_x_continuous() +\r\n  scale_y_continuous() +\r\n  labs(x=""relative wing length"", y = ""ln(latency to kicking (seconds))"", fill = ""density level"")+\r\n  theme_classic(base_size=15, base_family = ""serif"") +\r\n  theme(axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfemale_relative_wing_versus_lat_kick\r\n\r\naov_female_relative_wing_versus_lat_kick <- aov(as.numeric(log(Wing_Length)/log(Body_Mass_mg)) ~ as.numeric(log(Ovarioles)/(log(Body_Mass_mg)^2))*Density_Level, data = data1primeFEMALE)\r\nAnova(aov_female_relative_wing_versus_lat_kick, type = ""III"")\r\n\r\n\r\nfemale_relative_ovariole_versus_relative_clutch<- ggplot(data1primeFEMALE, aes(x=(log(Ovarioles)/log((Body_Mass_mg)^2)), y=(Clutch_Size/log(Body_Mass_mg)), fill=Density_Level)) +\r\n  scale_fill_manual(values=c(\'#008000\',\'#E69F00\', \'#C10D0D\'), breaks=c(""L"", ""M"", ""H""), labels=c(""low"",""medium"",""high"")) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  geom_point(shape = 21, size = 2) +\r\n  scale_x_continuous() +\r\n  scale_y_continuous() +\r\n  labs(x=""relative ovariole area"", y = ""relative initial clutch size"", fill = ""density level"")+\r\n  theme_classic(base_size=15, base_family = ""serif"") +\r\n  theme(axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfemale_relative_ovariole_versus_relative_clutch\r\n\r\naov_female_relative_ovariole_versus_relative_clutch <- aov(as.numeric(Clutch_Size/log(Body_Mass_mg)) ~ as.numeric(log(Ovarioles)/(log(Body_Mass_mg)^2))*Density_Level, data = data1FEMALE)\r\nAnova(aov_female_relative_ovariole_versus_relative_clutch, type = ""III"")\r\n\r\nfemale_relative_wing_versus_relative_clutch<- ggplot(data1primeFEMALE, aes(x=(log(Wing_Length)/log(Body_Mass_mg)), y=(Clutch_Size/log(Body_Mass_mg)), fill=Density_Level)) +\r\n  scale_fill_manual(values=c(\'#008000\',\'#E69F00\', \'#C10D0D\'), breaks=c(""L"", ""M"", ""H""), labels=c(""low"",""medium"",""high"")) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  geom_point(shape = 21, size = 2) +\r\n  scale_x_continuous() +\r\n  scale_y_continuous() +\r\n  labs(x=""relative wing length"", y = ""relative initial clutch size"", fill = ""density level"")+\r\n  theme_classic(base_size=15, base_family = ""serif"") +\r\n  theme(axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfemale_relative_wing_versus_relative_clutch\r\n\r\naov_female_relative_wing_versus_relative_clutch <- aov((Clutch_Size/log(Body_Mass_mg)) ~ as.numeric(log(Wing_Length)/(log(Body_Mass_mg)))*Density_Level, data = data1primeFEMALE)\r\nAnova(aov_female_relative_wing_versus_relative_clutch, type = ""III"")\r\n\r\n\r\nfemale_g1 <- ggarrange(female_lat_kick, female_clutch, female_lat_kick_versus_clutch,\r\n                       female_relative_ovariole, female_relative_wing, female_relative_ovariole_versus_relative_wing,\r\n                       nrow = 2, ncol = 3, widths = c(1,1,1), common.legend = FALSE, legend = NULL, align = c(\'hv\'))\r\nfemale_g1\r\n\r\nfemale_g2 <- ggarrange(female_relative_ovariole_versus_lat_kick, female_relative_wing_versus_lat_kick,\r\n                       female_relative_ovariole_versus_relative_clutch, female_relative_wing_versus_relative_clutch,\r\n                       nrow = 1, ncol = 4, common.legend = TRUE, legend = ""bottom"", align = c(\'hv\'))\r\nfemale_g2\r\n\r\nfig4 <- ggarrange(female_g1, female_g2, nrow = 2, ncol = 1, heights = c(1.5,1), align = c(\'hv\'))\r\nfig4\r\n\r\n\r\n#Sexual dimorphism figures ----\r\n\r\nmeasurements.pca1 <- prcomp(log(data1prime[,c(24:29,35)]), center = TRUE,scale. = TRUE)\r\n\r\nfigS1a <- ggbiplot(measurements.pca1, choices=c(1,2), ellipse=TRUE, groups = data1prime$Sex, var.axes = F) +\r\n  geom_point(shape = 21, size = 2, alpha = 0.8) +\r\n  scale_color_manual(name=""sex"", \r\n                     values=c(\'#ADEFD1FF\', \'#00203FFF\'), \r\n                     breaks=c(""F"", ""M""), \r\n                     labels = c(""female"", ""male"")) +\r\n  labs(x = ""PC1 (42.7%)"", y = ""PC2 (17.1%)"", color = ""sex"") +\r\n  theme_classic(base_size=15,base_family = ""serif"") +\r\n  theme(legend.position=""right"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfigS1a\r\n\r\nfigS1b <- ggplot(data1prime, aes(x=log(Body_Mass_mg), y=log(AntSum), fill=Sex)) +\r\n  geom_point(shape = 21, size = 3, alpha = 0.8) +\r\n  geom_smooth(method=lm, color=\'black\') +\r\n  scale_x_continuous() +\r\n  scale_fill_manual(values=c(\'#ADEFD1FF\', \'#00203FFF\'), \r\n                    breaks=c(""F"", ""M""), \r\n                    labels = c(""female"", ""male"")) +\r\n  labs(x=""ln(body mass (mg))"", y = ""ln(antenna length)"", fill = ""sex"")+\r\n  theme_classic(base_size=15,base_family = ""serif"") +\r\n  theme(legend.position=""right"",\r\n        axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines""))\r\nfigS1b\r\n\r\nfigS1 <- ggarrange(figS1a, figS1b, nrow=1, ncol=2, \r\n                   common.legend = TRUE, legend = ""bottom"", align =""hv"")\r\nfigS1\r\n\r\n\r\n#effect size for male appendages that were not graphed ----\r\n\r\naov_male_relative_ana <- aov((log(data1MALE$Ana)/log(data1MALE$Body_Mass_mg)) ~ data1MALE$Density_Level)\r\nsummary(aov_male_relative_ana)\r\neta_male_relative_ana <- etaSquared(aov_male_relative_ana)\r\neta_male_relative_ana\r\n\r\naov_male_relative_antsum <- aov((log(data1MALE$AntSum)/log(data1MALE$Body_Mass_mg)) ~ data1MALE$Density_Level)\r\nsummary(aov_male_relative_antsum)\r\neta_male_relative_antsum <- etaSquared(aov_male_relative_antsum)\r\neta_male_relative_antsum\r\n\r\naov_male_relative_elytron <- aov((log(data1MALE$Elytra)/log(data1MALE$Body_Mass_mg)) ~ data1MALE$Density_Level)\r\nsummary(aov_male_relative_elytron)\r\neta_male_relative_elytron <- etaSquared(aov_male_relative_elytron)\r\neta_male_relative_elytron\r\n\r\naov_male_relative_max <- aov((log(data1MALE$MaxPalp)/log(data1MALE$Body_Mass_mg)) ~ data1MALE$Density_Level)\r\nsummary(aov_male_relative_max)\r\neta_male_relative_max <- etaSquared(aov_male_relative_max)\r\neta_male_relative_max\r\n\r\naov_male_relative_thorax <- aov((log(data1MALE$Thorax)/log(data1MALE$Body_Mass_mg)) ~ data1MALE$Density_Level)\r\nsummary(aov_male_relative_thorax)\r\neta_male_relative_thorax <- etaSquared(aov_male_relative_thorax)\r\neta_male_relative_thorax\r\n\r\n\r\n\r\n#effect size for female appendages that were not graphed ----\r\n\r\naov_female_relative_ana <- aov((log(data1FEMALE$Ana)/log(data1FEMALE$Body_Mass_mg)) ~ data1FEMALE$Density_Level)\r\nsummary(aov_female_relative_ana)\r\neta_female_relative_ana <- etaSquared(aov_female_relative_ana)\r\neta_female_relative_ana\r\n\r\naov_female_relative_antsum <- aov((log(data1FEMALE$AntSum)/log(data1FEMALE$Body_Mass_mg)) ~ data1FEMALE$Density_Level)\r\nsummary(aov_female_relative_antsum)\r\neta_female_relative_antsum <- etaSquared(aov_female_relative_antsum)\r\neta_female_relative_antsum\r\n\r\naov_female_relative_elytron <- aov((log(data1FEMALE$Elytra)/log(data1FEMALE$Body_Mass_mg)) ~ data1FEMALE$Density_Level)\r\nsummary(aov_female_relative_elytron)\r\neta_female_relative_elytron <- etaSquared(aov_female_relative_elytron)\r\neta_female_relative_elytron\r\n\r\naov_female_relative_max <- aov((log(data1FEMALE$MaxPalp)/log(data1FEMALE$Body_Mass_mg)) ~ data1FEMALE$Density_Level)\r\nsummary(aov_female_relative_max)\r\neta_female_relative_max <- etaSquared(aov_female_relative_max)\r\neta_female_relative_max\r\n\r\naov_female_relative_thorax <- aov((log(data1FEMALE$Thorax)/log(data1FEMALE$Body_Mass_mg)) ~ data1FEMALE$Density_Level)\r\nsummary(aov_female_relative_thorax)\r\neta_female_relative_thorax <- etaSquared(aov_female_relative_thorax)\r\neta_female_relative_thorax\r\n\r\n\r\n\r\n\r\n\r\n#effect size graphs ----\r\n\r\n\r\n\r\n\r\nphenotype <- c(""body mass"",\r\n               ""anal plate"", ""antenna"", ""elytron"", ""gonad"", \r\n               ""maxillary palp"", ""thorax"", ""wing"")\r\n\r\neta_male <- c(eta_male_bodymass[1],\r\n              eta_male_relative_ana[1],\r\n              eta_male_relative_antsum[1],\r\n              eta_male_relative_elytron[1],\r\n              eta_male_relative_testis[1], \r\n              eta_male_relative_max[1],\r\n              eta_male_relative_thorax[1],\r\n              eta_male_relative_wing[1])\r\n\r\nsig_male <- c(""not significant"", \r\n              ""significant"",\r\n              ""not significant"",\r\n              ""not significant"",\r\n              ""significant"",\r\n              ""not significant"",\r\n              ""not significant"",\r\n              ""significant"")\r\n\r\nmale_effects <- cbind(phenotype, eta_male, sig_male)\r\n\r\neta_female <- c(eta_female_bodymass[1],\r\n                eta_female_relative_ana[1],\r\n                eta_female_relative_antsum[1],\r\n                eta_female_relative_elytron[1],\r\n                eta_female_relative_ovariole[1], \r\n                eta_female_relative_max[1],\r\n                eta_female_relative_thorax[1],\r\n                eta_female_relative_wing[1])\r\n\r\nsig_female <- c(""not significant"",\r\n                ""not significant"",\r\n                ""not significant"",\r\n                ""not significant"",\r\n                ""significant"",\r\n                ""not significant"",\r\n                ""not significant"",\r\n                ""not significant"")\r\n\r\nfemale_effects <- cbind(phenotype, eta_female, sig_female)\r\n\r\neffect_df <- rbind(male_effects, female_effects)\r\n\r\n\r\nsex <- rep(c(""male"", ""female""), each = nrow(male_effects))\r\n\r\n\r\neffect_df <- cbind(sex, effect_df)\r\n\r\neffect_df <- as.data.frame(effect_df)\r\neffect_df[,3] <- as.numeric(as.character(effect_df[,3]))\r\n\r\n\r\n\r\nfigS2 <- ggplot(effect_df, aes(x=sex, y=eta_male, group = phenotype, color = phenotype, shape = sig_male)) + \r\n  geom_hline(yintercept = 0.01, linetype = ""dashed"", alpha = 0.25) +\r\n  geom_hline(yintercept = 0.06, linetype = ""dashed"", alpha = 0.25) +\r\n  geom_hline(yintercept = 0.14, linetype = ""dashed"", alpha = 0.25) +\r\n  geom_line(size = 1) +\r\n  geom_point(aes(color = factor(phenotype)), size = 4) +\r\n  scale_shape_manual(values = c(0,8)) +\r\n  scale_y_continuous() +\r\n  scale_x_discrete (limits = c(""male"", ""female"")) +\r\n  theme_classic(base_size=20, base_family = ""serif"") +\r\n  labs(x=""sex"", y = expression(paste(""effect of density ( "", eta^2, "")"")), shape = ""significance"", color = ""phenotype"") +\r\n  theme(axis.text.x = element_text(color=\'black\'),\r\n        axis.text.y = element_text(color=\'black\'),\r\n        axis.ticks = element_line(color=\'black\'),\r\n        plot.margin = unit(c(1,1,1,1), ""lines"")) \r\nfigS2\r\n\r\n']","Data from: Larval density, sex and allocation hierarchy affect life-history trait covariances in a bean beetle Life-history theory aims to understand how different environments result in differential investment in fitness-related traits. While trade-offs between traits are expected, many studies show positive or no correlation between pairs of costly traits. One hypothesis that may explain the inconsistency of trade-offs in the literature is that trait investment may occur in a dichotomous hierarchy (the tree model), that allows for differential trait investment weighted by the traits' respective positions within the hierarchy. Previous mathematical models predict different covariances between traits depending on their position on the allocation tree. While hierarchical differential investment is often used to discuss findings in life-history theory, the role of an allocation hierarchy on trait covariances has not been directly tested. In turn, this study aims to identify trait covariances between behavioral and morphological phenotypes on different branches of an allocation tree in the bean beetle, Callosobruchus maculatus. While trade-offs between copulatory behaviors and morphology were found for both males and females, only traits at the base and far from each other in the hierarchy negatively covaried. This study empirically shows that trade-offs may be the result of hierarchical investment.",3
Data from: Sharing detailed research data is associated with increased citation rate,"Sharing research data provides benefit to the general scientific community, but the benefit is less obvious for the investigator who makes his or her data available. We examined the citation history of 85 cancer microarray clinical trial publications with respect to the availability of their data. The 48% of trials with publicly available microarray data received 85% of the aggregate citations. Publicly available data was significantly (p = 0.006) associated with a 69% increase in citations, independently of journal impact factor, date of publication, and author country of origin using linear regression. This correlation between publicly available data and increased literature impact may further motivate investigators to share their detailed research data.","['\r\n# Read in data\r\nsetwd(""e:\\\\research\\\\PLoS ONE submission\\\\Revised\\\\"")\r\ndat = read.csv(""stats.csv"", header=TRUE, row.names=1)\r\n\r\ndim(dat)\r\nrownames(dat)\r\ncolnames(dat)\r\n\r\n# Calculate a few fields which will be useful later\r\ndat$cohortmonthsfromend =\r\n  max(dat$Number.of.months.between.1.99.and.trial.publication) -\r\n  dat$Number.of.months.between.1.99.and.trial.publication\r\ndat$Number.of.cases.in.trial.gt25 = dat$Number.of.cases.in.trial > 25\r\n\r\n\r\n# A quick summary of the data\r\nprint(""Number of papers"")\r\nprint(""Data not available, Data available"")\r\ntapply(dat$Number.of.Citations.during.2004.2005 > 1,\r\n       dat$Is.the.microarray.data.publicly.available,\r\n       sum)\r\n\r\nprint(""Number of citations"")\r\nprint(""Data not available, Data available"")\r\ntapply(dat$Number.of.Citations.during.2004.2005,\r\n       dat$Is.the.microarray.data.publicly.available,\r\n       sum)\r\n\r\n\r\n\r\n## Table 1\r\n\r\n# Note that the Fisher estimates of the odd\'s ratio are not exactly the same as a*d/b*c.  The paper actually reports the latter, but uses the fisher values for the confidence interval\r\n\r\nimpact.2x2 \t= table(dat$Impact.factor.of.journal < 25, !dat$Is.the.microarray.data.publicly.available)\r\nprint(impact.2x2)\r\nimpact.fisher = fisher.test(impact.2x2)\r\nprint(round(impact.fisher$estimate, 1))\r\nprint(round(impact.fisher$conf.int, 1))\r\n\r\nyear.2x2 \t= table(dat$Number.of.months.between.1.99.and.trial.publication > 24, !dat$Is.the.microarray.data.publicly.available)\r\nprint(year.2x2)\r\nyear.fisher = fisher.test(year.2x2)\r\nprint(round(year.fisher$estimate, 1))\r\nprint(round(year.fisher$conf.int, 1))\r\n\r\nusauth.2x2 \t= table(dat$Are.there.any.authors.from.the.US == 0, !dat$Is.the.microarray.data.publicly.available)\r\nprint(usauth.2x2)\r\nusauth.fisher = fisher.test(usauth.2x2)\r\nprint(round(usauth.fisher$estimate, 1))\r\nprint(round(usauth.fisher$conf.int, 1))\r\n\r\n\r\n\r\n\r\n\r\n### Table 2\r\n## Primary analysis\r\n\r\n\r\n# Some helper functions\r\ncalcCI.exp= function(res, param) {\r\n  coefs = summary(res)$coeff\r\n  coeff = coefs[param,]\r\n  x = coeff[1]\r\n  stderr = coeff[2]\r\n  p = coeff[4]\r\n  return(list(param = param,\r\n              est = round(exp(x), 2), \r\n\t      CI = c(round(exp(x - 1.96*stderr), 2),\r\n                     round(exp(x + 1.96*stderr), 2)), \r\n  \t      p = round(p, 3)))\r\n}\r\n\r\ncalcCI.noexp= function(res, param) {\r\n  coefs = summary(res)$coeff\r\n  coeff = coefs[param,]\r\n  x = coeff[1]\r\n  stderr = coeff[2]\r\n  p = coeff[4]\r\n  return(list(param = param,\r\n              est = round(x, 2), \r\n\t      CI = c(round(x - 1.96*stderr, 2),\r\n                     round(x + 1.96*stderr, 2)), \r\n  \t      p = round(p, 3)))\r\n}\r\n\r\nall.results = function(res) {\r\n  # give the results of the impact factor without exp because it is the\r\n  # log impact factor, so interpretation is easier if kept in the log domain\r\n  print(calcCI.noexp(res, ""lnimpact""))\r\n  print(calcCI.exp(res, ""Are.there.any.authors.from.the.US""))\r\n  print(calcCI.exp(res, ""Number.of.months.between.1.99.and.trial.publication""))\r\n  print(calcCI.exp(res, ""Is.the.microarray.data.publicly.available""))\r\n}\r\n\r\n\r\n# Take the log of the endpoints and impact factor\r\ndat$lnimpact = log(dat$Impact.factor.of.journal)\r\ndat$lncites0405 = log(dat$Number.of.Citations.during.2004.2005)\r\ndat$lncites24months = log(dat$Number.of.Citations.in.first.24.months.after.publication)\r\n\r\n# Define the lower-profile subset\r\nwhich.subset = which((dat$Impact.factor.of.journal < 25) & (dat$Number.of.months.between.1.99.and.trial.publication > 24))\r\ndat.subset = dat[which.subset,]\r\n\r\n# A quick summary of the lower-profile subset\r\nprint(""Number of papers in the lower-profile subset"")\r\nprint(""Data not available, Data available"")\r\ntapply(dat.subset$Number.of.Citations.during.2004.2005 > 1,\r\n       dat.subset$Is.the.microarray.data.publicly.available,\r\n       sum)\r\n\r\nprint(""Number of citations in the lower profile-subset"")\r\nprint(""Data not available, Data available"")\r\ntapply(dat.subset$Number.of.Citations.during.2004.2005,\r\n       dat.subset$Is.the.microarray.data.publicly.available,\r\n       sum)\r\n\r\n\r\n# Do the regressions\r\nresult.primary = lm(lncites0405\r\n  ~ lnimpact + Number.of.months.between.1.99.and.trial.publication + Are.there.any.authors.from.the.US + Is.the.microarray.data.publicly.available, dat)\r\nall.results(result.primary)\r\n\r\nresult.primary.24mo = lm(lncites24months\r\n  ~ lnimpact + Number.of.months.between.1.99.and.trial.publication + Are.there.any.authors.from.the.US + Is.the.microarray.data.publicly.available, dat)\r\nall.results(result.primary.24mo)\r\n\r\nresult.primary.subset = lm(lncites0405\r\n  ~ lnimpact + Number.of.months.between.1.99.and.trial.publication + Are.there.any.authors.from.the.US + Is.the.microarray.data.publicly.available, dat.subset)\r\nall.results(result.primary.subset)\r\n\r\n\r\n# Table 3\r\n# Exploratory results\r\n# Articles, No. is confirmed below\r\n# Citations, No. is therefore not confirmed; assumed to be correct based on prior computation\r\n# coef is raised to 10 and called ""fold increase"" since covariates are binary\r\n# note the new p-values\r\n\r\n# Use subset that makes data available\r\nwhich.Is.the.microarray.data.publicly.available = which(dat$Is.the.microarray.data.publicly.available == 1)\r\ndat.da = dat[which.Is.the.microarray.data.publicly.available,]\r\nn.Is.the.microarray.data.publicly.available = length(which.Is.the.microarray.data.publicly.available)\r\n\r\n# Do the calculations\r\nresult.expl.n \t\t\t= lm(lncites0405\r\n  ~ Number.of.cases.in.trial.gt25 + lnimpact + Are.there.any.authors.from.the.US + Number.of.months.between.1.99.and.trial.publication, \r\n  subset=which.Is.the.microarray.data.publicly.available, dat)\r\nprint(result.expl.n$call)\r\nprint(round(exp(summary(result.expl.n)$coeff[2,]), 2))\r\ntab = table(dat.da$Number.of.cases.in.trial.gt25); tab; round(tab/sum(tab), 2)\r\ntab = tapply(dat.da$Number.of.Citations.during.2004.2005, dat.da$Number.of.cases.in.trial.gt25, sum); tab; round(tab/sum(tab), 2)\r\n\r\nresult.expl.clinical\t\t= lm(lncites0405\r\n  ~ Trial.has.a.clinical.endpoint + lnimpact + Are.there.any.authors.from.the.US + Number.of.months.between.1.99.and.trial.publication, \r\n  subset=which.Is.the.microarray.data.publicly.available, dat)\r\nprint(result.expl.clinical$call)\r\nprint(round(exp(summary(result.expl.clinical)$coeff[2,]), 2))\r\ntab = table(dat.da$Trial.has.a.clinical.endpoint); tab; round(tab/sum(tab), 2)\r\ntab = tapply(dat.da$Number.of.Citations.during.2004.2005, dat.da$Trial.has.a.clinical.endpoint, sum); tab; round(tab/sum(tab), 2)\r\n\r\nresult.expl.affy\t\t= lm(lncites0405\r\n  ~ Uses.the.Affymetrix.microarray.platform + lnimpact + Are.there.any.authors.from.the.US + Number.of.months.between.1.99.and.trial.publication, \r\n  subset=which.Is.the.microarray.data.publicly.available, dat)\r\nprint(result.expl.affy$call)\r\nprint(round(exp(summary(result.expl.affy)$coeff[2,]), 2))\r\ntab = table(dat.da$Uses.the.Affymetrix.microarray.platform); tab; round(tab/sum(tab), 2)\r\ntab = tapply(dat.da$Number.of.Citations.during.2004.2005, dat.da$Uses.the.Affymetrix.microarray.platform, sum); tab; round(tab/sum(tab), 2)\r\n\r\nresult.expl.geo\t\t\t= lm(lncites0405\r\n  ~ In.the.GEO.database + lnimpact + Are.there.any.authors.from.the.US + Number.of.months.between.1.99.and.trial.publication, \r\n  subset=which.Is.the.microarray.data.publicly.available, dat)\r\nprint(result.expl.geo$call)\r\nprint(round(exp(summary(result.expl.geo)$coeff[2,]), 2))\r\ntab = table(dat.da$In.the.GEO.database); tab; round(tab/sum(tab), 2)\r\ntab = tapply(dat.da$Number.of.Citations.during.2004.2005, dat.da$In.the.GEO.database, sum); tab; round(tab/sum(tab), 2)\r\n\r\nresult.expl.smd\t\t\t= lm(lncites0405\r\n  ~ In.the.SMD.database + lnimpact + Are.there.any.authors.from.the.US + Number.of.months.between.1.99.and.trial.publication, \r\n  subset=which.Is.the.microarray.data.publicly.available, dat)\r\nprint(result.expl.smd$call)\r\nprint(round(exp(summary(result.expl.smd)$coeff[2,]), 2))\r\ntab = table(dat.da$In.the.SMD.database); tab; round(tab/sum(tab), 2)\r\ntab = tapply(dat.da$Number.of.Citations.during.2004.2005, dat.da$In.the.SMD.database, sum); tab; round(tab/sum(tab), 2)\r\n\r\nresult.expl.raw\t\t\t= lm(lncites0405\r\n  ~ Raw.data.such.as.CEL.files.are.available + lnimpact + Are.there.any.authors.from.the.US + Number.of.months.between.1.99.and.trial.publication,\r\n  subset=which.Is.the.microarray.data.publicly.available, dat)\r\nprint(result.expl.raw$call)\r\nprint(round(exp(summary(result.expl.raw)$coeff[2,]), 2))\r\ntab = table(dat.da$Raw.data.such.as.CEL.files.are.available); tab; round(tab/sum(tab), 2)\r\ntab = tapply(dat.da$Number.of.Citations.during.2004.2005, dat.da$Raw.data.such.as.CEL.files.are.available, sum); tab; round(tab/sum(tab), 2)\r\n\r\nresult.expl.suppl\t\t= lm(lncites0405\r\n  ~ Publication.mentions.Supplemental.data + lnimpact + Are.there.any.authors.from.the.US + Number.of.months.between.1.99.and.trial.publication,\r\n  subset=which.Is.the.microarray.data.publicly.available, dat)\r\nprint(result.expl.suppl$call)\r\nprint(round(exp(summary(result.expl.suppl)$coeff[2,]), 2))\r\ntab = table(dat.da$Publication.mentions.Supplemental.data); tab; round(tab/sum(tab), 2)\r\ntab = tapply(dat.da$Number.of.Citations.during.2004.2005, dat.da$Publication.mentions.Supplemental.data, sum); tab; round(tab/sum(tab), 2)\r\n\r\nresult.expl.oncomine\t\t= lm(lncites0405\r\n  ~ Publication.has.an.Oncomine.profile + lnimpact + Are.there.any.authors.from.the.US + Number.of.months.between.1.99.and.trial.publication,\r\n  subset=which.Is.the.microarray.data.publicly.available, dat)\r\nprint(result.expl.oncomine$call)\r\nprint(round(exp(summary(result.expl.oncomine)$coeff[2,]), 2))\r\ntab = table(dat.da$Publication.has.an.Oncomine.profile); tab; round(tab/sum(tab), 2)\r\ntab = tapply(dat.da$Number.of.Citations.during.2004.2005, dat.da$Publication.has.an.Oncomine.profile, sum); tab; round(tab/sum(tab), 2)\r\n\r\n\r\n### Figure 1\r\n\r\ntable(dat$Is.the.microarray.data.publicly.available)\r\nboxplot(Number.of.Citations.during.2004.2005 ~ Is.the.microarray.data.publicly.available,\r\n        data = dat,\r\n        boxwex = 0.5, \r\n        names=c(""Data Not Shared (n=44)"", ""Data Shared (n=41)""), \r\n        ylab = ""Number of Citations in 2004-2005"", outline=T, notch=F, log=""y"")\r\ndev.copy(postscript, file=""figure1.eps"", width=6, height=6, horizontal=F, onefile=F)\r\ndev.off()\r\n\r\ntable(dat.subset$Is.the.microarray.data.publicly.available)\r\nwindows()\r\nboxplot(Number.of.Citations.during.2004.2005 ~ Is.the.microarray.data.publicly.available,\r\n        data = dat.subset,\r\n        boxwex = 0.5, \r\n        names=c(""Data Not Shared (n=43)"", ""Data Shared (n=27)""), \r\n        ylab = ""Number of Citations in 2004-2005"", outline=T, notch=F, log=""y"")\r\ndev.copy(postscript, file=""figure2.eps"", width=6, height=6, horizontal=F, onefile=F)\r\ndev.off()\r\n\r\n\r\n']","Data from: Sharing detailed research data is associated with increased citation rate Sharing research data provides benefit to the general scientific community, but the benefit is less obvious for the investigator who makes his or her data available. We examined the citation history of 85 cancer microarray clinical trial publications with respect to the availability of their data. The 48% of trials with publicly available microarray data received 85% of the aggregate citations. Publicly available data was significantly (p = 0.006) associated with a 69% increase in citations, independently of journal impact factor, date of publication, and author country of origin using linear regression. This correlation between publicly available data and increased literature impact may further motivate investigators to share their detailed research data.",3
Data from: Transcriptome modulation during host shift is driven by secondary metabolites in desert Drosophila,"High-throughput transcriptome studies are breaking new ground to investigate the responses that organisms deploy in alternative environments. Nevertheless, much remains to be understood about the genetic basis of host plant adaptation. Here, we investigate genome-wide expression in the fly Drosophila buzzatii raised in different conditions. This species uses decaying tissues of cactus of the genus Opuntia as primary rearing substrate and secondarily, the necrotic tissues of the columnar cactus Trichocereus terscheckii. The latter constitutes a harmful host, rich in mescaline and other related phenylethylamine alkaloids. We assessed the transcriptomic responses of larvae reared in Opuntia sulphurea and T. terscheckii, with and without the addition of alkaloids extracted from the latter. Whole-genome expression profiles were massively modulated by the rearing environment, mainly by the presence of T. terscheckii alkaloids. Differentially expressed genes were mainly related to detoxification, oxidationreduction and stress response; however, we also found genes involved in development and neurobiological processes. In conclusion, our study contributes new data onto the role of transcriptional plasticity in response to alternative rearing environments.","['###   Transcriptome modulation during host shift is driven by secondary metabolites in desert Drosophila\n###   De Panis et al.\n###   Molecular Ecology, 2016\n\n\n###   Differential Expression analysis with NOISeq\n\nsource(""http://bioconductor.org/biocLite.R"")\nbiocLite(""NOISeq"")\nlibrary(NOISeq)\n\n\n###   Data: D. buzzatii release1 - FPKM normalized-counts with RSEM = 13657 genes\n\nmycounts <- read.table(""counts_rsem.fpkm"", header=TRUE, row.names = 1)\nhead(mycounts)\ndim(mycounts)\n\n\nmyfactors = data.frame(\n  cact.add_alk = c(""op_n"", ""op_y"", ""tr_n"", ""tr_y"", \n                   ""op_n"", ""op_y"", ""tr_n"", ""tr_y"", \n                   ""op_n"", ""op_y"", ""tr_n"", ""tr_y""),\n  add_alk = c(""n"", ""y"", ""n"", ""y"", \n              ""n"", ""y"", ""n"", ""y"", \n              ""n"", ""y"", ""n"", ""y"")\n  )  \n\n#treatment=cact.add_alk\n#op_n: Native O. sulphurea\n#op_y: O. sulphurea + alkaloids\n#tr_n: Native T. terscheckii\n#tr_y: T. terscheckii + alkaloids\n\n#added alkaloids condition=add_alk\n#n: no\n#y: yes\n\nmydata = readData(data = mycounts, factors = myfactors)\nclass(mydata)\n\n\n#\nOp.Nat_vs_Tr.Nat = noiseqbio(mydata, k = NULL, norm = ""n"", factor = ""cact.add_alk"", conditions = c(""op_n"", ""tr_n""), filter = 1)\nOp.Alk_vs_Tr.Alk = noiseqbio(mydata, k = NULL, norm = ""n"", factor = ""cact.add_alk"", conditions = c(""op_y"", ""tr_y""), filter = 1)\n#\n#\nOp.Nat_vs_Op.Alk = noiseqbio(mydata, k = NULL, norm = ""n"", factor = ""cact.add_alk"", conditions = c(""op_n"", ""op_y""), filter = 1)\nTr.Nat_vs_Tr.Alk = noiseqbio(mydata, k = NULL, norm = ""n"", factor = ""cact.add_alk"", conditions = c(""tr_n"", ""tr_y""), filter = 1)\n#\n#\nOp.Nat.AND.Tr.Nat_vs_Op.Alk.AND.Tr.Alk = noiseqbio(mydata, k = NULL, norm = ""n"", factor = ""add.alk"", conditions = c(""n"", ""y""), filter = 1)\n#\n\n\n#RESULTS\n\n#\nhead(degenes(Op.Nat_vs_Tr.Nat, q = 0.95, M = NULL))   # 3617 DEGs\nhead(degenes(Op.Nat_vs_Tr.Nat, q = 0.99, M = NULL))   # 41 DEGs\n\nhead(degenes(Op.Alk_vs_Tr.Alk, q = 0.95, M = NULL))   # 22 DEGs\nhead(degenes(Op.Alk_vs_Tr.Alk, q = 0.99, M = NULL))   # 19 DEGs\n#\n#\nhead(degenes(Op.Nat_vs_Op.Alk, q = 0.95, M = NULL))   # 1039 DEGs\nhead(degenes(Op.Nat_vs_Op.Alk, q = 0.99, M = NULL))   # 58 DEGs\n\nhead(degenes(Tr.Nat_vs_Tr.Alk, q = 0.95, M = NULL))   # 36 DEGs\nhead(degenes(Tr.Nat_vs_Tr.Alk, q = 0.99, M = NULL))   # 20 DEGs\n#\n#\nhead(degenes(Op.Nat.AND.Tr.Nat_vs_Op.Alk.AND.Tr.Alk, q = 0.95, M = NULL))   # 34 DEGs\nhead(degenes(Op.Nat.AND.Tr.Nat_vs_Op.Alk.AND.Tr.Alk, q = 0.99, M = NULL))   # 5 DEGs\n#\n\n', '###   Transcriptome modulation during host shift is driven by secondary metabolites in desert Drosophila\n###   De Panis et al.\n###   Molecular Ecology, 2016\n\n\n###   Developmental time analysis\n\nmatrix<-read.csv(""td_means.csv"")\nhead(matrix)\n\n\nlibrary(lme4)\n\n# Mixed model\n\n# Selection of the model using AIC\nm1<-lmer(TD_mean~Cactus*Treatment+(1|Genotype), data=matrix)\nm2<-lmer(TD_mean~Cactus*Treatment+(1|Genotype:Treatment), data=matrix)\nm3<-lmer(TD_mean~Cactus*Treatment+(1|Genotype:Cactus), data=matrix)\nm4<-lmer(TD_mean~Cactus*Treatment+(1|Genotype)+(1|Genotype:Cactus), data=matrix)\nm5<-lmer(TD_mean~Cactus*Treatment+(1|Genotype)+(1|Genotype:Treatment), data=matrix)\nm6<-lmer(TD_mean~Cactus*Treatment+(1|Genotype:Cactus)+(1|Genotype:Treatment), data=matrix)\nm7<-lmer(TD_mean~Cactus*Treatment+(Treatment|Genotype)+(Cactus|Genotype), data=matrix)\nm8<-lmer(TD_mean~Cactus*Treatment+(Treatment|Genotype), data=matrix)\nm9<-lmer(TD_mean~Cactus*Treatment+(Cactus|Genotype), data=matrix)\nm10<-lmer(TD_mean~Cactus*Treatment+(Treatment:Cactus|Genotype), data=matrix)\nm11<-lmer(TD_mean~Cactus*Treatment+(Treatment|Genotype:Cactus)+(Cactus|Genotype:Treatment), data=matrix)\n\nAIC(m1,m2,m3,m4,m5,m6,m7,m8,m9,m10,m11)\n\n#Best model: m6\nM<-m6\n\n\n# Tests of statistical assumptions\n\n# Homogeneity of variances\nres2<-resid(M) #res.\nfit2<-fitted(M) #pred.\nplot(fit2, res2) #res. vs. pred.\nrabs<-abs(res2) #abs. res.\nX<-as.factor(matrix$Cactus:matrix$Treatment)\nsummary(aov(rabs~X)) #Levene\n\n#Normal distribution\nqqnorm(res2) #quant. obs.\nqqline(res2) #quant.\nlibrary(car)\nqqPlot(res2) #qqplot\nshapiro.test(res2) #Shapiro\nhist(res2,breaks=10) #distribution has a slightly longer left tail (but ANOVA is robust to slight lack of normality)\n\n\n# Results\nsummary(M)\nAnova(M, type=""3"")\n\n# random intercepts significance:\nm12<-lm(TD_mean~Cactus*Treatment, data=matrix)\nanova(M,m12)\n\n']","Data from: Transcriptome modulation during host shift is driven by secondary metabolites in desert Drosophila High-throughput transcriptome studies are breaking new ground to investigate the responses that organisms deploy in alternative environments. Nevertheless, much remains to be understood about the genetic basis of host plant adaptation. Here, we investigate genome-wide expression in the fly Drosophila buzzatii raised in different conditions. This species uses decaying tissues of cactus of the genus Opuntia as primary rearing substrate and secondarily, the necrotic tissues of the columnar cactus Trichocereus terscheckii. The latter constitutes a harmful host, rich in mescaline and other related phenylethylamine alkaloids. We assessed the transcriptomic responses of larvae reared in Opuntia sulphurea and T. terscheckii, with and without the addition of alkaloids extracted from the latter. Whole-genome expression profiles were massively modulated by the rearing environment, mainly by the presence of T. terscheckii alkaloids. Differentially expressed genes were mainly related to detoxification, oxidationreduction and stress response; however, we also found genes involved in development and neurobiological processes. In conclusion, our study contributes new data onto the role of transcriptional plasticity in response to alternative rearing environments.",3
"Data from: Monitoring a Norwegian freshwater crayfish tragedy - eDNA snapshots of invasion, infection and extinction","1.The European Noble crayfish (Astacus astacus) is threatened by crayfish plague caused by the oomycete Aphanomyces astaci, which is spread by the invasive North American crayfish (e.g. signal crayfish, Pacifastacus leniusculus). Surveillance of crayfish plague status in Norway has traditionally relied on the monitoring survival of cageheld noble crayfish, a method of ethical concern. Additionally, trapping is used in crayfish population surveillance. Here we test whether environmental DNA (eDNA) monitoring could provide a suitable alternative to the cagemethod, and a supplement to trapping.2.We took advantage of an emerging crayfish plague outbreak in a Norwegian watercourse following illegal introduction of diseasecarrying signal crayfish, and initiated simultaneous eDNAmonitoring and cagebased surveillance, supplemented with trapping. A total of 304 water samples were filtered from several sampling stations over a four year period. eDNA data (speciesspecific qPCR) for the presence of A. astaci, noble and signal crayfish within the water samples were compared to cage mortality and trapping.3.This is the first study comparing eDNAmonitoring and cagesurveillance during a natural crayfish plague outbreak. We show that eDNAmonitoring corresponds well with the biological status measured in terms of crayfish mortality and trapping results. eDNA analysis also reveals the presence of A. astaci in the water up to 2.5 weeks in advance of the cagemethod. eDNA estimates of A. astaci concentration and noble crayfish numbers increased markedly during mortality, and vanished quickly thereafter. eDNA provides a snapshot of the presence, absence or disappearance of crayfish regardless of season, and constitutes a valuable supplement to the trappingmethod that relies on season and legislation.4.Synthesis and applications. Simultaneous eDNAmonitoring of Aphanomyces astaci (crayfish plague) and relevant native and invasive freshwater crayfish species is wellsuited for earlywarning of invasion or infection, risk assessments, habitat evaluation and surveillance regarding pathogen and invasive/native crayfish status. This noninvasive, animalwelfare friendly method excludes the need for cageheld susceptible crayfish in diseasemonitoring. Further, eDNAmonitoring is less likely to spread A. astaci than traditional methods. This study resulted in the implementation of eDNAmonitoring for Norwegian crayfish plague and crayfish surveillance programmes, and we believe other countries could improve management strategies for freshwater crayfish using a similar approach.","['setwd(""/path to folder/"")\r\n\r\ndE<-read.csv2(file=""Data_2015_eDNA_Haldenvassdraget.csv"")\r\n\r\n#Rdnessjen_Sr\r\nRSd<-dE[dE[,1]==""Rdnessjen_Sr"",]\r\nRSd\r\n\r\n#Rdnessjen_Joval\r\nRJd<-dE[dE[,1]==""Rdnessjen_Joval"",]\r\nRJd\r\n\r\n#Rdnessjen_Kroksund\r\nRKd<-dE[dE[,1]==""Rdnessjen_Kroksund"",]\r\nRKd\r\n\r\n#Skullerudsjen\r\nSd<-dE[dE[,1]==""Skullerudsjen"",]\r\nSd<-Sd[,!is.na(Sd[1,])]  # Remove 07.aug\r\nSd\r\n\r\n#Hlandselva\r\nHd<-dE[dE[,1]==""Hlandselva"",]\r\nHd<-Hd[,!is.na(Hd[1,])]  # Remove 07.aug\r\nHd\r\n\r\n#Use of r-package boot to estimate conf intervall around corr-coef\r\nlibrary(""boot"")\r\ncor.boot <- function(data, k) cor(data[k,])[1,2]\r\ncors.boot <- function(data, k) cor(data[k,],method=""spearman"")[1,2]\r\n\r\n#Excample Rsd\r\ndd<-RSd\r\nd_N<-apply(dd[dd$Agent==""Noble"",-c(1:4)],2,mean)\r\nd_S<-apply(dd[dd$Agent==""Signal"",-c(1:4)],2,mean)\r\nd_P<-apply(dd[dd$Agent==""Plague"",-c(1:4)],2,mean)\r\n\r\ndd1<-rbind(d_N,d_P,d_S)\r\n\r\nddtl0<-dd1[,-1]\r\nddtl1<-dd1[,-ncol(dd1)]\r\n\r\n\r\nddtl0\r\nddtl1\r\n\r\n####\r\n\r\n#Work with log10-data?\r\nldat<-t(cbind(log10(dd1+1)))   #turn table with the function t\r\nldat\r\n\r\n#First order difference (non-transformed data)\r\n#dd1D<-t(ddtl0-ddtl1)\r\n\r\n#First order difference  (log10 data)\r\nddlogD<-t(log10(ddtl0+1)-log10(ddtl1+1))\r\nddlogD\r\n\r\nRSlogD<-ddlogD\r\n\r\n################\r\n#Excample Rkd\r\ndd<-RKd\r\nd_N<-apply(dd[dd$Agent==""Noble"",-c(1:4)],2,mean)\r\nd_P<-apply(dd[dd$Agent==""Plague"",-c(1:4)],2,mean)\r\n\r\ndd1<-rbind(d_N,d_P)\r\n\r\nddtl0<-dd1[,-1]\r\nddtl1<-dd1[,-ncol(dd1)]\r\n\r\n\r\nddtl0\r\nddtl1\r\n\r\n####\r\n\r\n#Work with log10-data?\r\nldat<-t(cbind(log10(dd1+1)))   #turn table with the function t\r\nldat\r\n\r\n#First order difference (non-transformed data)\r\n#dd1D<-t(ddtl0-ddtl1)\r\n\r\n#First order difference  (log10 data)\r\nddlogD<-t(log10(ddtl0+1)-log10(ddtl1+1))\r\nddlogD\r\n\r\nRKlogD<-ddlogD\r\n\r\n##########################\r\n#Eksempel RJd\r\ndd<-RJd\r\n\r\nd_N<-apply(dd[dd$Agent==""Noble"",-c(1:4)],2,mean)\r\nd_P<-apply(dd[dd$Agent==""Plague"",-c(1:4)],2,mean)\r\n\r\ndd1<-rbind(d_N,d_P)\r\n\r\nddtl0<-dd1[,-1]\r\nddtl1<-dd1[,-ncol(dd1)]\r\n\r\n\r\nddtl0\r\nddtl1\r\n\r\n####\r\n\r\n#Work with log10-data?\r\nldat<-t(cbind(log10(dd1+1)))   #turn table with the function t\r\nldat\r\n\r\n#First order difference (log10 data)\r\nddlogD<-t(log10(ddtl0+1)-log10(ddtl1+1))\r\nddlogD\r\n\r\nRJlogD<-ddlogD\r\n\r\n##########################\r\n#Example Sd\r\ndd<-Sd\r\n\r\nd_N<-apply(dd[dd$Agent==""Noble"",-c(1:4)],2,mean)\r\nd_P<-apply(dd[dd$Agent==""Plague"",-c(1:4)],2,mean)\r\n\r\ndd1<-rbind(d_N,d_P)\r\n\r\nddtl0<-dd1[,-1]\r\nddtl1<-dd1[,-ncol(dd1)]\r\n\r\nddtl0\r\nddtl1\r\n\r\n#Work with log10-data?\r\nldat<-t(cbind(log10(dd1+1)))   #turn table with the function t\r\nldat\r\n\r\n#First order difference  (log10 data)\r\nddlogD<-t(log10(ddtl0+1)-log10(ddtl1+1))\r\nddlogD\r\n\r\nSdlogD<-ddlogD\r\n\r\n##########################\r\n#Example Sd\r\ndd<-Hd\r\n\r\nd_N<-apply(dd[dd$Agent==""Noble"",-c(1:4)],2,mean)\r\nd_P<-apply(dd[dd$Agent==""Plague"",-c(1:4)],2,mean)\r\n\r\ndd1<-rbind(d_N,d_P)\r\n\r\nddtl0<-dd1[,-1]\r\nddtl1<-dd1[,-ncol(dd1)]\r\n\r\n\r\nddtl0\r\nddtl1\r\n\r\n####\r\n\r\n#Work with log10-data?\r\nldat<-t(cbind(log10(dd1+1)))   #turn table with the function t\r\nldat\r\n\r\n#First order difference (non-transformed data)\r\ndd1D<-t(ddtl0-ddtl1)\r\n\r\n#First order difference  (log10 data)\r\nddlogD<-t(log10(ddtl0+1)-log10(ddtl1+1))\r\nddlogD\r\n\r\nHdlogD<-ddlogD\r\n\r\n#################\r\n#################\r\n#Spearman corr\r\n#All locations pooled  \r\nddlogD<-rbind(RSlogD[,-3],RKlogD,RJlogD,SdlogD,HdlogD)\r\n\r\ncor.test(ddlogD[,1],ddlogD[,2],method=""spearman"")\r\n\r\n#p=0.004272\r\n#rho=0.4845\r\n\r\n\r\n#Spearman corr\r\n#3 locations pooled (Excluding Hlandselva)\r\ndd3logD<-rbind(RSlogD[,-3],RKlogD,RJlogD)\r\n\r\ncor.test(dd3logD[,1],dd3logD[,2],method=""spearman"")\r\n\r\n#p=0.001339\r\n#rho=0.6527\r\n\r\n###################\r\n###################\r\n\r\n']","Data from: Monitoring a Norwegian freshwater crayfish tragedy - eDNA snapshots of invasion, infection and extinction 1.The European Noble crayfish (Astacus astacus) is threatened by crayfish plague caused by the oomycete Aphanomyces astaci, which is spread by the invasive North American crayfish (e.g. signal crayfish, Pacifastacus leniusculus). Surveillance of crayfish plague status in Norway has traditionally relied on the monitoring survival of cageheld noble crayfish, a method of ethical concern. Additionally, trapping is used in crayfish population surveillance. Here we test whether environmental DNA (eDNA) monitoring could provide a suitable alternative to the cagemethod, and a supplement to trapping.2.We took advantage of an emerging crayfish plague outbreak in a Norwegian watercourse following illegal introduction of diseasecarrying signal crayfish, and initiated simultaneous eDNAmonitoring and cagebased surveillance, supplemented with trapping. A total of 304 water samples were filtered from several sampling stations over a four year period. eDNA data (speciesspecific qPCR) for the presence of A. astaci, noble and signal crayfish within the water samples were compared to cage mortality and trapping.3.This is the first study comparing eDNAmonitoring and cagesurveillance during a natural crayfish plague outbreak. We show that eDNAmonitoring corresponds well with the biological status measured in terms of crayfish mortality and trapping results. eDNA analysis also reveals the presence of A. astaci in the water up to 2.5 weeks in advance of the cagemethod. eDNA estimates of A. astaci concentration and noble crayfish numbers increased markedly during mortality, and vanished quickly thereafter. eDNA provides a snapshot of the presence, absence or disappearance of crayfish regardless of season, and constitutes a valuable supplement to the trappingmethod that relies on season and legislation.4.Synthesis and applications. Simultaneous eDNAmonitoring of Aphanomyces astaci (crayfish plague) and relevant native and invasive freshwater crayfish species is wellsuited for earlywarning of invasion or infection, risk assessments, habitat evaluation and surveillance regarding pathogen and invasive/native crayfish status. This noninvasive, animalwelfare friendly method excludes the need for cageheld susceptible crayfish in diseasemonitoring. Further, eDNAmonitoring is less likely to spread A. astaci than traditional methods. This study resulted in the implementation of eDNAmonitoring for Norwegian crayfish plague and crayfish surveillance programmes, and we believe other countries could improve management strategies for freshwater crayfish using a similar approach.",3
Data from: Who shares? Who doesn't? Factors associated with openly archiving raw research data,"Many initiatives encourage investigators to share their raw datasets in hopes of increasing research efficiency and quality. Despite these investments of time and money, we do not have a firm grasp of who openly shares raw research data, who doesn't, and which initiatives are correlated with high rates of data sharing. In this analysis I use bibliometric methods to identify patterns in the frequency with which investigators openly archive their raw gene expression microarray datasets after study publication. Automated methods identified 11,603 articles published between 2000 and 2009 that describe the creation of gene expression microarray data. Associated datasets in best-practice repositories were found for 25% of these articles, increasing from less than 5% in 2001 to 30%-35% in 2007-2009. Accounting for sensitivity of the automated methods, approximately 45% of recent gene expression studies made their data publicly available. First-order factor analysis on 124 diverse bibliometric attributes of the data creation articles revealed 15 factors describing authorship, funding, institution, publication, and domain environments. In multivariate regression, authors were most likely to share data if they had prior experience sharing or reusing data, if their study was published in an open access journal or a journal with a relatively strong data sharing policy, or if the study was funded by a large number of NIH grants. Authors of studies on cancer and human subjects were least likely to make their datasets available. These results suggest research data sharing levels are still low and increasing only slowly, and data is least available in areas where it could make the biggest impact. Let's learn from those with high rates of sharing to embrace the full potential of our research output.",,"Data from: Who shares? Who doesn't? Factors associated with openly archiving raw research data Many initiatives encourage investigators to share their raw datasets in hopes of increasing research efficiency and quality. Despite these investments of time and money, we do not have a firm grasp of who openly shares raw research data, who doesn't, and which initiatives are correlated with high rates of data sharing. In this analysis I use bibliometric methods to identify patterns in the frequency with which investigators openly archive their raw gene expression microarray datasets after study publication. Automated methods identified 11,603 articles published between 2000 and 2009 that describe the creation of gene expression microarray data. Associated datasets in best-practice repositories were found for 25% of these articles, increasing from less than 5% in 2001 to 30%-35% in 2007-2009. Accounting for sensitivity of the automated methods, approximately 45% of recent gene expression studies made their data publicly available. First-order factor analysis on 124 diverse bibliometric attributes of the data creation articles revealed 15 factors describing authorship, funding, institution, publication, and domain environments. In multivariate regression, authors were most likely to share data if they had prior experience sharing or reusing data, if their study was published in an open access journal or a journal with a relatively strong data sharing policy, or if the study was funded by a large number of NIH grants. Authors of studies on cancer and human subjects were least likely to make their datasets available. These results suggest research data sharing levels are still low and increasing only slowly, and data is least available in areas where it could make the biggest impact. Let's learn from those with high rates of sharing to embrace the full potential of our research output.",3
Data and code accompanying: Predator-induced transgenerational plasticity in animals: a meta-analysis.,"These data and code were used to generate the publication ""Predator-induced transgenerational plasticity in animals: a meta-analysis"", accepted in Oecologia (Oct 2022).Contents:R code file (can be opened as txt) - all code used to generate statistical models and resultsDatafile - main datafile containing all meta-analysis data, and a metadata sheet explaining each column. See paper and associated supplementary material for more detail.","['#Code for ""Predator-induced transgenerational plasticity in animals: a meta-analysis""\n#Oecologia\n#Accepted Oct 2022\n\n#Code written by K MacLeod (k.macleod@bangor.ac.uk) except where specified \n# - sections written by others are specified below\n\n# PACKAGES ----------------------------------------------------------------\n\n\nlibrary(metafor)\nlibrary(diagram, tidyverse)\nlibrary(ape, curl)\nlibrary(fulltext, metafor)\nlibrary(treebase, devtools)\nlibrary(rotl)\nlibrary(MuMIn)\nlibrary(metaviz)\neval(metafor:::.MuMIn)\nlibrary(""patchwork"")\nlibrary(""R.rsp"")\ndevtools::install_github(""itchyshin/orchard_plot"", subdir = ""orchaRd"", force = TRUE, \n                         build_vignettes = TRUE)\nlibrary(orchaRd)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nrequire(MCMCglmm)\nrequire(plyr)\nrequire(car)\n\n\n# PHYLOGENY ---------------------------------------------------------------\n\n## 1. constructing a tree using rotl (accesses a synthetic super-tree from Open Tree of Life database (https:// opentreeoflife.org))\npred_prey_species <- c(""Acanthochromis polyacanthus"",\n                       ""Acyrthosiphon pisum"",\n                       ""Aedes albopictus"",\n                       ""Ambystoma maculatum"",\n                       ""Biomphalaria glabrata"",\n                       ""Cerorhinca monocerata"",\n                       ""Ctenomys talarum"",\n                       ""Daphnia ambigua"",\n                       ""Daphnia lumholtzi"",\n                       ""Daphnia magna"",\n                       ""Dendraster excentricus"",\n                       ""Drosophila melanogaster"",\n                       ""Dryophytes versicolor"",\n                       ""Ficedula hypoleuca"",\n                       ""Gambusia affinis"",\n                       ""Gasterosteus aculeatus"",  \n                       ""Gryllus pennsylvanicus"",\n                       ""Hydroporus pubescens"", \n                       ""Hyperolius spinigularis"",\n                       ""Ischnura elegans"",\n                       ""Larus michahellis"",\n                       ""Lasiopodomys brandtii"",\n                       ""Lepus americanus"",\n                       ""Lissotriton helveticus"",\n                       ""Microtus oeconomus"",\n                       ""Myodes glareolus"",\n                       ""Myzus persicae"",\n                       ""Nucella lapillus"",\n                       ""Palaemon argentinus"",\n                       ""Parus major"",\n                       ""Physa acuta"",\n                       ""Poecilia reticulata"",\n                       ""Pseudemoia pagenstecheri"",\n                       ""Pseudosimochromis pleurospilus"",\n                       ""Pseudacris regilla"",\n                       ""Rana dalmatina"",\n                       ""Rana sylvatica"", \n                       ""Rana temporaria"",\n                       ""Sepia officinalis"",\n                       ""Tetranychus urticae"",  \n                       ""Tritia obsoleta"",\n                       ""Zootoca vivipara"")\n\n\ntaxa <- tnrs_match_names(names = pred_prey_species)\ntaxa\n\npredpreytree <- tol_induced_subtree(ott_ids = taxa[[""ott_id""]], label_format = ""name"") #we can get a Warning meassage here about collapsing single nodes\n#The tree is returned as an ape::phylo object and it can be manipulated, printed and saved easily using functions from the ape package.\n#note this nomenclature (ott_id) means that species names must be written like this in the datafile\npredpreytree\n\nplot(predpreytree, \n     # type=""fan"",\n     cex=.8, adj=1, label.offset =.01, no.margin = FALSE)\n\npredpreytree$tip.label #see the current tree tip labels\n\n#check if the tree is really binary \nis.binary.tree(predpreytree)\n#[1] FALSE there are polytomies to resolve - will resolve at random (i.e. doesn\'t matter too much how much the Daphnia are related)\n\nset.seed(111) #making it replicable\ntree_random <- multi2di(predpreytree,random=TRUE)\nis.binary.tree(tree_random) #TRUE\n\nplot(tree_random, cex=.8, label.offset =.1, adj=1)\n\nwrite.tree(tree_random, file=""predpreytree.tre"")\ntree_random$tip.label\n\n# We can then make a correlation matrix (a relatedness matrix among species) to use in rma.mv objects.\ntree <- compute.brlen(tree_random)\ncor <- vcv(tree,cor=T)\n\n# Inverse of the phylogenetic correlation matrix. \nAinv <- inverseA(tree, nodes = ""ALL"", scale = TRUE)$Ainv\n\n# Data read in and prep ---------------------------------------------------\n\n##read in data file \npredprey_data<-read.csv(""analysis_data_predprey.csv"")    ##load the data\nnrow(predprey_data)\n##turn entry ID into a factor\npredprey_data$EntryID<-factor(as.character(predprey_data$EntryID))\n##turn paper ID into a factor\npredprey_data$PaperID<-factor(as.character(predprey_data$PaperID))\n\n#Calculate the effect size and variance using metafor\n\n#need to check that the means/sds/Ns are all numerical or escalc won\'t work\n#remove any rows with NA vals that are needed to calculate effect sizes\npredprey_data<-predprey_data [is.na(predprey_data $T_m)==FALSE,]\npredprey_data<-predprey_data [is.na(predprey_data $CorrectedC_N)==FALSE,]\npredprey_data<-predprey_data [is.na(predprey_data $Trait_simple)==FALSE,]\nnrow(predprey_data)\n\n##use the escalc function to make calculations - need to specify which columns correspond to n1i, n2i etc. Calculation requires Ns, means sds\n##n1i/n2i == sample size of treatment/control groups\n##m1i/m2i == mean response value in treatment/control groups\n##sd1i/sd2i == standard deviation of response value n treatment/control groups\n##measure == the type of effect size you want to calculate, I chose the standardized mean difference (hedge\'s d)\n\n#note - using corrected N (i.e. if control group was shared between two treatments, this corrects for that)\npreydat_SMD<-escalc(n1i = T_N, n2i = CorrectedC_N, m1i = T_m, m2i = C_m,\n                sd1i = T_sd, sd2i = C_sd, data = predprey_data, measure = ""SMD"",\n                append = TRUE)\nnrow(preydat_SMD) #577\n\n\n#changing sign directions\npreydat_SMD$SMD<-preydat_SMD$yi*preydat_SMD$Direction_factor\n#NB direction_factor switches the sign for the following traits: \n# tonic immobility, latency to move/return to normal behaviour, righting response time, \n# baseline CORT, CORT response\n#   i.e. assumes that increases in these things are negative in biological terms\n\n\n# 2. OVERALL MODEL  ---------------------------------------------\npreydat_SMD_TGP<-preydat_SMD[preydat_SMD$Who_exposed != ""embryo"",]\npreydat_SMD_WGP<-preydat_SMD[preydat_SMD$Who_exposed == ""embryo"",]\n\nnrow(preydat_SMD_TGP) #441\nlength(unique(preydat_SMD_TGP$PaperID))\nlength(unique(preydat_SMD_TGP$Species))\n\n#model with phylogeny\nphylo_SMD_TGP <- rma.mv(yi=SMD, V=vi, #effect sizes and variance\n                     random=list(~1|PaperID,~1|EntryID,~1|Species),\n                     R = list(Species=cor), \n                     method=""REML"", data=preydat_SMD_TGP, test=""t"")\nsummary(phylo_SMD_TGP)\n\nI2_overallTGP_SMD <- i2_ml(phylo_SMD_TGP)\nI2_overallTGP_SMD\n\nresults_table_TGP_SMD <- mod_results(phylo_SMD_TGP, mod=""Int"")\nprint(results_table_TGP_SMD)\n\noverallplot<- orchard_plot(results_table_TGP_SMD, mod = ""Int"", xlab = ""Hedge\'s g"") + \n  annotate(geom = ""text"", x = 14, y = 1.2, label = paste0(""italic(I)^{2} == "", \n       round(I2_overallTGP_SMD[1], 4) * 100), color = ""black"", parse = TRUE, size = 3.5) +\n  annotate(geom = ""text"", x = -7, y = 1.5, label = ""(a)"", color = ""black"", parse = TRUE, size = 4) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),axis.text.y = element_text(angle = 0))\n\n\n#second, WITH robust() i.e. robust variance estimation\nphylo_TGP_SMD_RVE<-robust(phylo_SMD_TGP, cluster=preydat_SMD_TGP$PaperID)\nsummary(phylo_TGP_SMD_RVE)\n\n\n\n# PUBLICATION BIAS TESTS --------------------------------------------------\n\nfunnel(phylo_SMD1,yaxis=""vi"")\nfunnel(phylo_SMD_TGP,yaxis=""vi"")\n\n# regtest(model) #can\'t do egger test on rma.mv object\n#can instead use a variance metric as a moderator in a regression - if this is significant, indicates bias - there is none in this case\n#using sqrt(inverse of combined sample sizes) as per Dan\'s suggestion\n\npreydat_SMD_TGP$ninv<-(1/preydat_SMD_TGP$CorrectedC_N)+(1/preydat_SMD_TGP$T_N)\negger.test.TGP.SMD <- rma.mv(yi = SMD, V = vi, mods=sqrt(1/ninv), \n                         random = list(~1|PaperID,~1|EntryID, ~1|Species),R = list(Species=cor),\n                         method = ""REML"", data = preydat_SMD_TGP, test=""t"")\nsummary(egger.test.TGP.SMD)\n\n\n\n# MAGNITUDE OF EFFECTS --------------------------------------------------------------\n\n#Functions for this analysis taken from:\n\n# Title: Clean Analysis Code\n# Reference: Noble et el. 2017. Incubation temperature and developmental plasticity in reptiles: A systematic review and meta-analysis. Biological Reviews.\n# Author: Daniel Noble @ daniel.wa.noble@gmail.com\n# Date: 5/11/2016 @ University of New South Wales (UNSW)\n#Available at osf.io/zbgss. \n\n#Parameters\niter = 510000\nburn = 10000\nthin = 1000\n\n# Prior. Inverse-Wishart priors on random effects.\nprior = list(R = list(V = 1, nu = 0.002), G = list(G1 = list(V = 1, nu = 0.002), G2 = list(V = 1, nu = 0.002), G3 = list(V = 1, nu = 0.002), G4 = list(V = 1, fix = 1)))\n\nmodel_random_pred <- MCMCglmm::MCMCglmm(SMD ~ 1, random = ~ tree+Species + PaperID + EntryID, \n                                        ginverse = list(tree = Ainv, EntryID = Sinv), \n                                        prior = prior, nitt = iter/2\n                                        # *3\n                                        , \n                                        burnin = burn*3, thin = thin, \n                                        data = preydat_SMD_TGP) \nsummary(model_random_pred)\n\npostVCV <- model_random_pred$VCV\npostSol    <- model_random_pred$Sol\n\n# Estimate the posterior distribution for folded normal by applying distribution estimated. \n#Here you want to use th residual variance for the distribution not the variance of the sampling distribution.\nest.fnorm <- folded(postSol, sqrt(rowSums(postVCV[,c(1:3,5)])))\nmu.fnorm(postSol, sqrt(rowSums(postVCV[,c(1:3,5)])))\n\n\n#-------------------------------------------------#\n#  Heterogeneity statistics \n#-------------------------------------------------#\n## I2 - Eqn. 22 Nakagawa and Santos (2012)\nwi <- 1/(preydat_SMD_TGP$vi)\npost <- model_random_pred$VCV\n\n# Calculate total sampling variability\nVw <- sum((wi) * (length(wi) - 1))  / (((sum(wi)^2) - sum((wi)^2)))\n\n# Calculate the proportion of heterogeneity explained by ""phylogeny"". Note we need to remove Vw.\nhet.phylo <- post[,""tree""]/(post[,""tree""]+post[,""Species""]+post[,""PaperID""]+post[,""units""])\n\n# Get the mode and credible interval for posterior distribution of het.phylo\nposterior.mode(het.phylo)\nHPDinterval(het.phylo)\nplot(density(het.phylo))\n\n# Total heterogeneity beyond sampling variability\nI2all<-(post[,""tree""]+post[,""Species""]+post[,""PaperID""]+post[,""units""])/(post[,""tree""]+post[,""Species""]+post[,""PaperID""]+post[,""units""]+Vw)\n\n# Get the mode and credible interval for posterior distribution of I2\nposterior.mode(I2all)\nHPDinterval(I2all)\n\n# Total between study heterogeneity\nI2paper<-(post[,""PaperID""])/(post[,""tree""]+post[,""Species""]+post[,""PaperID""]+post[,""units""]+Vw)\n\n# Get the mode and credible interval for posterior distribution of proportion of between study variance\nposterior.mode(I2paper)\nHPDinterval(I2paper)\n\n# Species level heterogeneity\nI2spp<-(post[,""Species""])/(post[,""tree""]+post[,""Species""]+post[,""PaperID""]+post[,""units""]+Vw)\n\n# Get the mode and credible interval for posterior distribution of proportion of between study variance\nposterior.mode(I2spp)\nHPDinterval(I2spp)\n\n# effect size level heterogeneity\nI2es<-(post[,""units""])/(post[,""tree""]+post[,""Species""]+post[,""PaperID""]+post[,""units""]+Vw)\n\n# Get the mode and credible interval for posterior distribution of proportion of between study variance\nposterior.mode(I2es)\nHPDinterval(I2es)\n\n# R-squared of model\nR2m<-(post[,""tree""]+post[,""Species""]+post[,""PaperID""])/(post[,""tree""]+post[,""Species""]+post[,""PaperID""]+post[,""units""])\n\n# Get the mode and CI for R2\nposterior.mode(R2m)\nHPDinterval(R2m)\n\n# Pooled effect size \nPool_es <- cbind(posterior.mode(model_random_pred$Sol), HPDinterval(model_random_pred$Sol))\n\n\n\n# TESTING DIFFERENCES IN VARIATION BETWEEN GROUPS -------------------------\n\n#### Created by A M Senior @ the University of Otago NZ 03/01/2014\n\n#### Below are funcitons for calculating effect sizes for meta-analysis of variance. \n#### Both functions take the mean, sd and n from the control and experimental groups.\n\n#### The first function, Cal.lnCVR, calculates the the log repsonse-ratio of the coefficient of variance (lnCVR) - see Nakagawa et al in prep.\n\n#### The second function calculates the measuremnt error variance for lnCVR. As well as the aforementioned parameters, this function also takes\n#### Equal.E.C.Corr (default = T), which must be True or False. If true, the funciton assumes that the correlaiton between mean and sd (Taylor\'s Law) \n#### is equal for the mean and control groups, and, thus these data are pooled. If False the mean-SD correlation for the experimental and control groups\n#### are calculated seperatley from one another.\n\n\nCalc.lnCVR<-function(CMean, CSD, CN, EMean, ESD, EN){\n  \n  ES<-log(ESD) - log(EMean) + 1 / (2*(EN - 1)) - (log(CSD) - log(CMean) + 1 / (2*(CN - 1)))\n  \n  return(ES)\n  \n}\n\n\n\nCalc.var.lnCVR<-function(CMean, CSD, CN, EMean, ESD, EN, Equal.E.C.Corr=T){\n  \n  if(Equal.E.C.Corr==T){\n    \n    mvcorr<-cor.test(log(c(CMean, EMean)), log(c(CSD, ESD)))$estimate\n    \n    S2<- CSD^2 / (CN * (CMean^2)) + 1 / (2 * (CN - 1)) - 2 * mvcorr * sqrt((CSD^2 / (CN * (CMean^2))) * (1 / (2 * (CN - 1)))) + ESD^2 / (EN * (EMean^2)) + 1 / (2 * (EN - 1)) - 2 * mvcorr * sqrt((ESD^2 / (EN * (EMean^2))) * (1 / (2 * (EN - 1))))\n    \n  }\n  else{\n    \n    Cmvcorr<-cor.test(log(CMean), log(CSD))$estimate\n    Emvcorr<-cor.test(log(EMean), (ESD))$estimate\n    \n    S2<- CSD^2 / (CN * (CMean^2)) + 1 / (2 * (CN - 1)) - 2 * Cmvcorr * sqrt((CSD^2 / (CN * (CMean^2))) * (1 / (2 * (CN - 1)))) + ESD^2 / (EN * (EMean^2)) + 1 / (2 * (EN - 1)) - 2 * Emvcorr * sqrt((ESD^2 / (EN * (EMean^2))) * (1 / (2 * (EN - 1))))\t\t\n    \n    \n  }\n  return(S2)\n  \n}\n\n\n#note that above function takes the log(mean) which is not possible if mean is negative\n#getting around this by making all means positive (this will not affect the variance)?\n\n#or just remove all means 0 or <0? (approx N=20)\n\n\npreydat_SMD_TGP_var<-preydat_SMD_TGP[preydat_SMD_TGP$T_m>0,]\nnrow(preydat_SMD_TGP_var) #438\npreydat_SMD_TGP_var<-preydat_SMD_TGP_var[preydat_SMD_TGP_var$C_m>0,]\nnrow(preydat_SMD_TGP_var) #435\n\nCmvcorr<-cor.test(log(preydat_SMD_TGP_var$C_m), log(preydat_SMD_TGP_var$C_sd))$estimate\nEmvcorr<-cor.test(log(preydat_SMD_TGP_var$T_m), (preydat_SMD_TGP_var$T_sd))$estimate\n\n# Calculate the lnCVR effect sizefor our data using the function Calc.lnCVR.\npreydat_SMD_TGP_var$CVR<-Calc.lnCVR(EMean = preydat_SMD_TGP_var$T_m, ESD = preydat_SMD_TGP_var$T_sd, EN = preydat_SMD_TGP_var$T_N, \n                            CMean = preydat_SMD_TGP_var$C_m, CSD = preydat_SMD_TGP_var$C_sd, CN = preydat_SMD_TGP_var$CorrectedC_N)\n\n# Calculate the sampling error for lnCVR using \n# the function Calc.var.lnCVR.\npreydat_SMD_TGP_var$V.CVR<-Calc.var.lnCVR(EMean = preydat_SMD_TGP_var$T_m, ESD = preydat_SMD_TGP_var$T_sd, EN = preydat_SMD_TGP_var$T_N, \n                                  CMean = preydat_SMD_TGP_var$C_m, CSD = preydat_SMD_TGP_var$C_sd, CN = preydat_SMD_TGP_var$CorrectedC_N, Equal.E.C.Corr=F)\n\n\nbasic_mod_var <- rma.mv(yi = CVR, V = V.CVR, random = list(~1|PaperID, ~1|EntryID, ~1|Species),\n                        method = ""REML"", data = preydat_SMD_TGP_var)\nsummary(basic_mod_var)\n\nI2_overallvar <- i2_ml(basic_mod_var)\nI2_overallvar\n\n#second, WITH robust() i.e. robust variance estimation\nbasic_mod_var_RVE<-robust(basic_mod_var, cluster=preydat_SMD_TGP_var$PaperID)\nsummary(basic_mod_var_RVE)\n\n# EFFECTS OF MODERATORS ---------------------------------------------------\n\n##WHAT PREDICTS STRENGTH/DIRECTION OF EFFECTS?\n####including moderators FOR WHICH WE HAVE A PRIORI PREDICTIONS\n\nTGP_SMD_mods<-rma.mv(SMD, vi,  \n                 mods = ~Trait_simple + Age_category +Repro_mode + Cue_type,\n                 random = list( ~1|PaperID, ~1|EntryID, ~1|Species),R = list(Species=cor), \n                 data = preydat_SMD_TGP, method = ""ML"",test=""t"")\nsummary(TGP_SMD_mods)\n\nI2_overallTGP_mods <- i2_ml(TGP_SMD_mods)\nI2_overallTGP_mods\n\n#second, WITH robust() i.e. robust variance estimation\nphylo_SMD_traits_RVE<-robust(TGP_traits_SMD, cluster=preydat_SMD_TGP$PaperID)\nsummary(phylo_SMD_traits_RVE)\n\n\n### Reviewers suggest significance testing the full model to make sure dredge is doing its job\n#switching to REML to get accurate coefficients\n\nTGP_SMD_mods_fullsig<-metafor::rma.mv(SMD, vi,  \n                     mods = ~Trait_simple + Age_category +Repro_mode + Cue_type,\n                     random = list( ~1|PaperID, ~1|EntryID, ~1|Species),R = list(Species=cor), \n                     data = preydat_SMD_TGP, method = ""REML"",test=""t"")\nsummary(TGP_SMD_mods_fullsig)\n\n#F tests of each coeff\nanova(TGP_SMD_mods_fullsig,btt=2:4)\nanova(TGP_SMD_mods_fullsig,btt=5:7)\nanova(TGP_SMD_mods_fullsig,btt=8)\nanova(TGP_SMD_mods_fullsig,btt=9:12)\n\npredict(TGP_SMD_mods_fullsig, mod = ""Trait_simple"")\n\nI2_TGP_SMD_mods_fullsig <- i2_ml(TGP_SMD_mods_fullsig)\nI2_TGP_SMD_mods_fullsig\n\n\n#robust variance est model\nTGP_SMD_mods_fullsig_RVE<-robust(TGP_SMD_mods_fullsig, cluster=preydat_SMD_TGP$PaperID)\nsummary(TGP_SMD_mods_fullsig_RVE)\n\nanova(TGP_SMD_mods_fullsig_RVE,btt=2:4)\nanova(TGP_SMD_mods_fullsig_RVE,btt=5:7)\nanova(TGP_SMD_mods_fullsig_RVE,btt=8)\nanova(TGP_SMD_mods_fullsig_RVE,btt=9:12)\n\nI2_TGP_SMD_mods_fullsig_RVE <- i2_ml(TGP_SMD_mods_fullsig_RVE)\nI2_TGP_SMD_mods_fullsig_RVE\n\n\n# DOES RISK ENVIRONMENT MATTER? -------------------------------------------\n\n\n#risk data - just studies and traits where high/low risk was directly compared, or antipredator behaviour tested\nriskdat<-preydat_SMD_TGP[preydat_SMD_TGP$Risk_compar==1,]\nnrow(riskdat) #149\nlength(unique(riskdat$PaperID))\nlength(unique(riskdat$Species))\nsummary(riskdat$Species)\n\nrisk_mod<-rma.mv(SMD, vi,  mods = ~Risk_environment, random = list( ~1|PaperID, ~1|EntryID, ~1|Species), R = list(Species=cor), \n                 data = riskdat, method = ""REML"",test=""t"")\nprint(risk_mod)\nanova(risk_mod)\n\n#sensitivity analysis\nrisk_modRVE<-robust(risk_mod, cluster=riskdat$PaperID)\nprint(risk_modRVE)\n\nI2_TGP_SMD_mods_risk <- i2_ml(risk_mod)\nI2_TGP_SMD_mods_risk\n\n']","Data and code accompanying: Predator-induced transgenerational plasticity in animals: a meta-analysis. These data and code were used to generate the publication ""Predator-induced transgenerational plasticity in animals: a meta-analysis"", accepted in Oecologia (Oct 2022).Contents:R code file (can be opened as txt) - all code used to generate statistical models and resultsDatafile - main datafile containing all meta-analysis data, and a metadata sheet explaining each column. See paper and associated supplementary material for more detail.",3
Siegel et al 2022 data and code,"Data and code for the analysis in the paper ""Accounting for snowpack and time-varying lags in statistical models of stream temperature"" Journal of Hydrology X, Volume 17, 1 December 2022, 100136",,"Siegel et al 2022 data and code Data and code for the analysis in the paper ""Accounting for snowpack and time-varying lags in statistical models of stream temperature"" Journal of Hydrology X, Volume 17, 1 December 2022, 100136",3
Differences in bee community composition between restored and remnant prairies are more strongly linked to forb community differences than landscape differences,"1. Grassland restoration is an important tool for conserving bee biodiversity within agricultural landscapes. Restorations foster increases in local bee abundance and -diversity, however, these measures are insufficient for understanding if remnant communities are being conserved. We compared native bee -diversity, -diversity, and community composition between restored and remnant prairies in Minnesota, USA. We then investigated two potential drivers of bee community dissimilarity between restored and remnant prairies: proportion of agricultural land surrounding a restoration and differences in floral community between restored and remnant prairies.2. We selected ten restored prairies that lie along a gradient of increasing agricultural land cover, ranging from 2085% of surrounding land in agricultural production. We paired each restoration with a nearby prairie remnant and sampled bee and floral communities concurrently in each restoration-remnant pair. We quantified bee and forb -diversity, community composition, -diversity, and levels of dissimilarity between restoration-remnant pairs along the gradient of agricultural development. Additionally, we quantified differences in the community weighted mean between restored and remnant prairies for two bee traits, oligolecty and tongue length, to investigate how differences in floral community between restored and remnant prairies may influence bee community composition.3. While bee -diversity between restored and remnant prairies was similar, bee composition between restorations and remnants were significantly different with restorations being more homogenous than remnants. Differences in bee community composition and -diversity were not statisticlly related to agricultural landscapes or floral community dissimilarity, however, we found a significantly higher proportion of oligolectic bees in remnant prairies. This difference in oligolectic bees was likely related to the absence of important host plants in restorations.4. 'Synthesis and application': Prairie restorations should seek to provide diverse floral resources that are of similar composition to remnant prairies. Specifically, providing important floral host plants for pollen specialist bees could improve restorations' ability to conserve prairie remnant bee communities.","['\r\n##### Loading in data and creating matrix for analysis #####\r\n\r\nspec<- read.csv(""Lane_et_al_2021_specimens.csv"") # loading in main specimen data. This data is cleaned and ready for analysis\r\nsite<- read.csv(""Lane_et_al_2021_sites.csv"") # site data\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\nspec.tally<- spec %>% # summarizing bee abundance by site\r\n  group_by(site,bee) %>%\r\n  tally()\r\n\r\nlibrary(reshape2)\r\n\r\ntally_matrix<- acast(spec.tally, site~bee, value.var=""n"") # creating a site~bee matrix\r\ntally_matrix[is.na(tally_matrix)] <- 0 # replacing NA values with 0\'s\r\n\r\n\r\n##### Bee Diversity #####\r\n\r\nlibrary(mobr)# package for calculating Probability of Specific Encounter (PIE) and its associated Effective Number of Species (ENS)\r\n\r\nens.1<- as.data.frame(calc_PIE(tally_matrix,ENS=TRUE)) # calculating ENS of PIE\r\n\r\nens.2<- tibble::rownames_to_column(ens.1, ""site"") # turning site into a distinct column\r\n\r\ncolnames(ens.2)[colnames(ens.2)==""calc_PIE(tally_matrix, ENS = TRUE)""] <- ""ens"" # changing column name to something more tractable\r\n\r\nens.3<- merge(ens.2,site,by=""site"") # merging site data with diversity data\r\n\r\nens.3$ens.r<- round(ens.3$ens,digits=0) # rounding ENS to nearest species for modeling\r\n\r\nlibrary(lme4)\r\n\r\ndiv.mod<- glmer(ens.r~type+(1|pair/site),data=ens.3,family=""poisson"")\r\n\r\nsummary(div.mod)\r\n\r\nlibrary(DHARMa) # package for checking model fit\r\n\r\nsimulationOutput<- simulateResiduals(fittedModel = div.mod, n = 250) # simulating residuals to test model fit\r\n\r\ntestResiduals(simulationOutput) # model fit tests - no test in the output is significant (p<0.05), which would indicate overdispersion etc.\r\n\r\n# Graph \r\n\r\ncols<- c(""remnant""=""goldenrod3"",""restoration""=""skyblue3"")\r\n\r\nstd <- function(x) sd(x)/sqrt(length(x)) # Function for standard error\r\n\r\ndiv.pred <- aggregate(ens.3[c(""ens.r"")], \r\n                      by = ens.3[c(""type"")], FUN=mean) # extracting mean from based on type\r\n\r\ndiv.pred$se<- aggregate(ens.3[c(""ens.r"")], \r\n                        by = ens.3[c(""type"")], FUN=std)[,2] # calculating standard error based on type\r\n\r\nggplot(div.pred, aes(x=type,y=ens.r)) + ## Figure 2A\r\n  geom_bar(stat=""identity"", position=""dodge"",fill=cols,color=""black"",size=1) +\r\n  geom_errorbar(aes(ymin=ens.r-se, ymax=ens.r+se), width=0.4, position=position_dodge(width=0.9), color=""black"",size=1) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(0,14)) + # Modifying the Y axis limit\r\n  #annotate(""text"", x = c(1), y = c( .45), label = c(""*""), size=10) + # adding letters for significance\r\n  theme_classic() +\r\n  theme(axis.line.x = element_line(colour = ""black"", size=0.5),axis.line.y = element_line(colour = ""black"", size=0.5)) + # Work around for classic theme being broke\r\n  theme(axis.title.x =element_blank()) + # modifying the Y axis label\r\n  ylab(bquote(\'Bee Community ENS\'[PIE])) +\r\n  xlab(NULL) +\r\n  labs(title=""A."") +\r\n  theme(plot.title = element_text(face = ""bold"",vjust = 8,size = 20)) +\r\n  theme(axis.text.y=element_text(size=14), axis.title.y=element_text(size=18)) +\r\n  theme(axis.text.x=element_text(size=18)) +\r\n  theme(plot.margin=unit(c(1.1,1.1,1.1,1.1), ""cm"")) +\r\n  theme(axis.title.y = element_text(margin=margin(0,15,0,0)), axis.title.x = element_text(margin=margin(15,0,0,0)) ) +\r\n  theme(legend.position = ""none"")\r\n\r\n\r\n\r\n\r\n##### Beta diversity ####\r\n\r\nlibrary(betapart)\r\n\r\nlibrary(vegan)\r\n\r\ndist<- vegdist(tally_matrix, method=""bray"") # creating distance matrix\r\n\r\nbetadiv <- with(site, betadisper(dist, type)) # running multivariate dispersion test\r\nanova(betadiv) # results\r\n\r\nbeta.at<- as.data.frame(betadiv$distances) # extracting distances and creating a dataframe for making a plot\r\nbeta.at1 <- tibble::rownames_to_column(beta.at, ""site"")\r\ncolnames(beta.at1)[colnames(beta.at1)==""betadiv$distances""] <- ""distances""\r\nbeta.af<- merge(site,beta.at1,by=""site"")\r\n\r\nbeta.ag<-aggregate(beta.af[c(""distances"")], \r\n                   by = beta.af[c(""type"")], FUN=mean)\r\n\r\nbeta.ag$se<- aggregate(beta.af[c(""distances"")], \r\n                       by = beta.af[c(""type"")], FUN=std)[,2]\r\n\r\nlibrary(ggpubr)\r\n\r\nggplot(beta.ag, aes(x=type,y=distances)) + # Fig 2B\r\n  geom_bar(stat=""identity"", position=""dodge"",fill=cols,color=""black"",size=1) +\r\n  geom_errorbar(aes(ymin=distances-se, ymax=distances+se), width=0.4, position=position_dodge(width=0.9), color=""black"",size=1) +\r\n  geom_bracket(xmin = ""remnant"", xmax = ""restoration"", y.position = .5,label = ""*"",size=1,label.size = 8) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(0,.6)) + # Modifying the Y axis limit\r\n  theme_classic() +\r\n  theme(axis.line.x = element_line(colour = ""black"", size=0.5),axis.line.y = element_line(colour = ""black"", size=0.5)) + # Work around for classic theme being broke\r\n  theme(axis.title.x =element_blank()) + # modifying the Y axis label\r\n  ylab(""Distance to Centroid"") +\r\n  labs(title = ""B."") +\r\n  theme(plot.title = element_text(face = ""bold"",vjust = 8,size = 20)) +\r\n  xlab(NULL) +\r\n  theme(axis.text.y=element_text(size=14), axis.title.y=element_text(size=18)) +\r\n  theme(axis.text.x=element_text(size=18)) +\r\n  theme(plot.margin=unit(c(1.1,1.1,1.1,1.1), ""cm"")) +\r\n  theme(axis.title.y = element_text(margin=margin(0,15,0,0)), axis.title.x = element_text(margin=margin(15,0,0,0)) ) +\r\n  theme(legend.position = ""none"")\r\n\r\n\r\n\r\n\r\n##### Community composition #####\r\n\r\n\r\nperm <- how(nperm = 1000) # setting permutation number\r\nsetBlocks(perm) <- with(site, pair) # setting up permutation strata\r\n\r\nadonis2(dist~type, data=site, permutations = perm) # Running PERMANOVA \r\n\r\n# Bee composition NMDS (abundance)\r\n\r\nspatial_plot1<- metaMDS (dist, k=3,trymax=100) # obtaining points for NMDS\r\n\r\nstressplot(spatial_plot1) ## NMDS stress test\r\n\r\nnmds.exp1<- site # copying site data to new object so we can add points\r\n\r\nnmds.exp1$NMDS1 = spatial_plot1$points[,1] # extracting first two dimensions\r\nnmds.exp1$NMDS2 = spatial_plot1$points[,2]\r\n\r\ngrp.res1 <- nmds.exp1[nmds.exp1$type == ""restoration"", ][chull(nmds.exp1[nmds.exp1$type == \r\n                                                                           ""restoration"", c(""NMDS1"", ""NMDS2"")]), ]  # hull values for grp A\r\ngrp.rem1 <- nmds.exp1[nmds.exp1$type == ""remnant"", ][chull(nmds.exp1[nmds.exp1$type == \r\n                                                                       ""remnant"", c(""NMDS1"", ""NMDS2"")]), ]  # hull values for grp B\r\n\r\nhull.data1 <- rbind(grp.res1, grp.rem1)  #combine grp.a and grp.b\r\n\r\nggplot() + # NMDS of bee communities, figure 2C\r\n  scale_colour_manual(values=c(""restoration"" = ""skyblue3"", ""remnant"" = ""goldenrod2"")) +\r\n  scale_fill_manual(values=c(""restoration"" = ""skyblue3"", ""remnant"" = ""goldenrod2"")) +\r\n  labs(colour=""Habitat Type"",fill=""Habitat Type"") +\r\n  geom_polygon(data=hull.data1,aes(x=NMDS1,y=NMDS2,fill=type,group=type),alpha=0.20)+\r\n  geom_point(data=nmds.exp1,aes(x=NMDS1,y=NMDS2,colour=type,stroke=2),size=5) + # add the point markers\r\n  coord_equal() +\r\n  theme_bw() + \r\n  theme(plot.margin=unit(c(1.1,1.1,1.1,1.1), ""cm"")) +\r\n  labs(title = ""C."") +\r\n  theme(plot.title = element_text(face = ""bold"",vjust = 8,size = 20)) +\r\n  theme(legend.title=element_text(size=12,face = ""bold""),legend.text=element_text(size=12)) +\r\n  theme(axis.text.x = element_blank(),  # remove x-axis text\r\n        axis.text.y = element_blank(), # remove y-axis text\r\n        axis.ticks = element_blank(),  # remove axis ticks\r\n        axis.title.x = element_text(size=18), # remove x-axis labels\r\n        axis.title.y = element_text(size=18), # remove y-axis labels\r\n        panel.background = element_blank(), \r\n        panel.grid.major = element_blank(),  #remove major-grid labels\r\n        panel.grid.minor = element_blank(),  #remove minor-grid labels\r\n        plot.background = element_blank())\r\n\r\n\r\n\r\n##### paired dissimilarity ####\r\n\r\npair.dis<- read.csv(""Lane_et_al_2021_PairedDis.csv"") # data frame with the dissimilarities for only site pairs extracted and combined with site data\r\n\r\nlibrary(nlme)\r\n\r\npairinga.mod<- lm(bee_dis~primary_ag_1500+forb_dis+distance,data=pair.dis) # model\r\n\r\nsummary(pairinga.mod) # no significant results\r\n\r\n# Paired Ag Plot\r\n\r\nggplot(pair.dis, aes(y=bee_dis, x=primary_ag_1500)) + # figure 3\r\n  theme_classic() +\r\n  geom_point(size=3) +\r\n  #geom_line(data=pred.1, aes(x=primary_ag_1500,y=bee_dis),linetype=""dashed"",size=1) +\r\n  ylab(""Bee Community Dissimilarity"") +\r\n  xlab(""Percent Surrounding Agriculture"") +\r\n  theme(axis.title.x = element_text(size=18),axis.text.x=element_text(size=14)) +\r\n  theme(plot.margin=unit(c(1.1,1.1,1.1,1.1), ""cm"")) +\r\n  theme(axis.title.y = element_text(margin=margin(0,15,0,0)), axis.title.x = element_text(margin=margin(15,0,0,0)) ) +\r\n  theme(legend.title=element_text(size=12,face = ""bold""),legend.text=element_text(size=12)) +\r\n  theme(axis.text.y=element_text(size=14), axis.title.y=element_text(size=18)) +\r\n  theme(axis.title.x = element_text(size=18), axis.text.x=element_text(size=14))\r\n\r\n\r\n##### Forb community ####\r\n\r\nforbs<- read.csv(""Lane_et_al_2021_forbs.csv"")\r\n\r\nforb.t<- forbs %>% # summing quadrat frequency for each species\r\n  group_by(site,plant) %>%\r\n  tally()\r\n\r\nforb.t<- na.omit(forb.t)\r\n\r\ntally_matrix.pl<- acast(forb.t, site~plant, value.var=""n"")\r\ntally_matrix.pl[is.na(tally_matrix.pl)] <- 0\r\n\r\n# Plant alpha diversity\r\n\r\np.ens.1<- as.data.frame(calc_PIE(tally_matrix.pl,ENS=TRUE)) # calculating ENS of PIE\r\n\r\np.ens.2<- tibble::rownames_to_column(p.ens.1, ""site"") # turning site/round into a distinct column\r\n\r\ncolnames(p.ens.2)[colnames(p.ens.2)==""calc_PIE(tally_matrix.pl, ENS = TRUE)""] <- ""p.ens"" # changing column name to something more tractable\r\n\r\np.ens.3<- merge(p.ens.2,site,by=""site"")\r\n\r\np.ens.3$ens.pr<- round(p.ens.3$p.ens,digits=0)\r\n\r\n# - model\r\n\r\np.div.mod<- glmer(ens.pr~type+(1|pair),data=p.ens.3,family=""poisson"")\r\n\r\nsummary(p.div.mod) # results\r\n\r\nsimulationOutput<- simulateResiduals(fittedModel = p.div.mod, n = 250) # simulating residuals to test model fit\r\n\r\ntestResiduals(simulationOutput) # model assumptions are reasonably met\r\n\r\n# - Plant diversity Plot\r\n\r\np.div.pred <- aggregate(p.ens.3[c(""ens.pr"")], # aggregating plant data for plotting\r\n                        by = p.ens.3[c(""type"")], FUN=mean)\r\n\r\np.div.pred$se<- aggregate(p.ens.3[c(""ens.pr"")], \r\n                          by = p.ens.3[c(""type"")], FUN=std)[,2]\r\n\r\nggplot(p.div.pred, aes(x=type,y=ens.pr)) + # figure 4A\r\n  geom_bar(stat=""identity"", position=""dodge"",fill=cols,color=""black"",size=1) +\r\n  geom_errorbar(aes(ymin=ens.pr-se, ymax=ens.pr+se), width=0.4, position=position_dodge(width=0.9), color=""black"",size=1) +\r\n  geom_bracket(xmin = ""remnant"", xmax = ""restoration"", y.position = 18,label = ""*"",size=1,label.size = 8) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(0,22)) + # Modifying the Y axis limit\r\n  #annotate(""text"", x = c(1), y = c( .45), label = c(""*""), size=10) + # adding letters for significance\r\n  theme_classic() +\r\n  theme(axis.line.x = element_line(colour = ""black"", size=0.5),axis.line.y = element_line(colour = ""black"", size=0.5)) + # Work around for classic theme being broke\r\n  theme(axis.title.x =element_blank()) + # modifying the Y axis label\r\n  ylab(bquote(\'Forb Community ENS\'[PIE])) +\r\n  xlab(NULL) +\r\n  labs(title = ""A."") +\r\n  theme(plot.title = element_text(face = ""bold"",vjust = 8,size = 20)) +\r\n  theme(axis.text.y=element_text(size=14), axis.title.y=element_text(size=18)) +\r\n  theme(axis.text.x=element_text(size=18)) +\r\n  theme(plot.margin=unit(c(1.1,1.1,1.1,1.1), ""cm"")) +\r\n  theme(axis.title.y = element_text(margin=margin(0,15,0,0)), axis.title.x = element_text(margin=margin(15,0,0,0)) ) +\r\n  theme(legend.position = ""none"")\r\n\r\n\r\n# forb composition plot\r\n\r\npdist<- vegdist(tally_matrix.pl, method=""bray"") # forb community pairwise distance matrix\r\n\r\nperm1 <- how(nperm = 1000) # setting up permutation blocking\r\nsetBlocks(perm1) <- with(site, pair)\r\n\r\nadonis2(pdist~type, data=site, permutations = perm1) # Plant Comp Differences\r\n\r\n# forb NMDS\r\n\r\nspatial_plot.pl2<- metaMDS (pdist, k=3,trymax=100)\r\nstressplot(spatial_plot.pl2)\r\n\r\nnmds.pl2<- site\r\n\r\n\r\n\r\nnmds.pl2$NMDS1 = spatial_plot.pl2$points[,1]\r\nnmds.pl2$NMDS2 = spatial_plot.pl2$points[,2]\r\n\r\ngrp.res.pl2 <- nmds.pl2[nmds.pl2$type == ""restoration"", ][chull(nmds.pl2[nmds.pl2$type == \r\n                                                                           ""restoration"", c(""NMDS1"", ""NMDS2"")]), ]  # hull values for grp A\r\ngrp.rem.pl2 <- nmds.pl2[nmds.pl2$type == ""remnant"", ][chull(nmds.pl2[nmds.pl2$type == \r\n                                                                       ""remnant"", c(""NMDS1"", ""NMDS2"")]), ]  # hull values for grp B\r\n\r\nhull.data.pl2 <- rbind(grp.res.pl2, grp.rem.pl2)  #combine grp.a and grp.b\r\n\r\nggplot() + # figure 4B\r\n  scale_colour_manual(values=c(""restoration"" = ""skyblue3"", ""remnant"" = ""goldenrod2"")) +\r\n  scale_fill_manual(values=c(""restoration"" = ""skyblue3"", ""remnant"" = ""goldenrod2"")) +\r\n  labs(colour=""Habitat Type"",fill=""Habitat Type"",shape=""Habitat Type"") +\r\n  geom_polygon(data=hull.data.pl2,aes(x=NMDS1,y=NMDS2,fill=type,group=type),alpha=0.30)+\r\n  geom_point(data=nmds.pl2,aes(x=NMDS1,y=NMDS2,shape=type,colour=type),size=4) + # add the point markers\r\n  coord_equal() +\r\n  theme_bw() + \r\n  theme(legend.title=element_text(size=12,face = ""bold""),legend.text=element_text(size=12)) +\r\n  labs(title = ""B."") +\r\n  theme(plot.title = element_text(face = ""bold"",vjust = 8,size = 20)) +\r\n  theme(axis.text.x = element_blank(),  # remove x-axis text\r\n        axis.text.y = element_blank(), # remove y-axis text\r\n        axis.ticks = element_blank(),  # remove axis ticks\r\n        axis.title.x = element_text(size=18), # remove x-axis labels\r\n        axis.title.y = element_text(size=18), # remove y-axis labels\r\n        panel.background = element_blank(), \r\n        panel.grid.major = element_blank(),  #remove major-grid labels\r\n        panel.grid.minor = element_blank(),  #remove minor-grid labels\r\n        plot.background = element_blank())\r\n\r\n\r\n\r\n##### Trait Analysis ####\r\n\r\ntraits<- read.csv(""Lane_et_al_2021_traits.csv"") # trait file where tongue length has already been calculated\r\n\r\ntrait.tally<- spec %>% \r\n  group_by(site,round,pair,type,bee) %>%\r\n  tally()\r\n\r\ntrait.tally$pair<- as.factor(trait.tally$pair)\r\n\r\ntrait.tally$round<- as.factor(trait.tally$round)\r\n\r\ntrait.tally$site_round<- paste(trait.tally$site,trait.tally$round,sep=""_"")\r\n\r\ntrait.sub<- merge(trait.tally,traits[,c(""bee"",""lecticity"",""tongue_length"")],by=""bee"",all.x=TRUE,all.y=FALSE)\r\n\r\ntrait.sub<- trait.sub[!is.na(trait.sub$lecticity),]\r\n\r\ncwm_abu<- trait.sub %>%\r\n  group_by(site,round,site_round,pair,type) %>%   # Groups the summary file by Plot number\r\n  summarize(           # Coding for how we want our CWMs summarized\r\n    lecticity_cwm = weighted.mean(lecticity, n),   # Actual calculation of CWMs\r\n    tl_cwm = weighted.mean(tongue_length, n)\r\n  )\r\n\r\n\r\n# Lecticity analysis\r\n\r\nlibrary(lmerTest)\r\n\r\ncw.mod<-lmer(lecticity_cwm~type+(1|round/pair),data=cwm_abu)\r\n\r\nsummary(cw.mod)\r\nplot(residuals(cw.mod)~fitted.values(cw.mod)) # homoscedasticity looks OK\r\n\r\nqqnorm(resid(cw.mod))\r\nqqline(resid(cw.mod)) # Normality assumption looks reasonably well met\r\n\r\n# plot for lecticity\r\n\r\nlec.pred <- aggregate(cwm_abu[c(""lecticity_cwm"")], \r\n                      by = cwm_abu[c(""type"")], FUN=mean)\r\n\r\nlec.pred$se<- aggregate(cwm_abu[c(""lecticity_cwm"")], \r\n                        by = cwm_abu[c(""type"")], FUN=std)[,2]\r\n\r\nggplot(lec.pred, aes(x=type, y=lecticity_cwm))+ #figure 5a\r\n  geom_bar(stat=""identity"", position=""dodge"",fill=cols,color=""black"",size=1) +\r\n  geom_errorbar(aes(ymin=lecticity_cwm-se, ymax=lecticity_cwm+se), width=0.4, position=position_dodge(width=0.9), color=""black"",size=1) +\r\n  geom_bracket(xmin = ""remnant"", xmax = ""restoration"", y.position = .45,label = ""*"",size=1,label.size = 8) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(0,.6)) + # Modifying the Y axis limit\r\n  #annotate(""text"", x = c(1), y = c( .45), label = c(""*""), size=10) + # adding letters for significance\r\n  theme_classic() +\r\n  theme(axis.line.x = element_line(colour = ""black"", size=0.5),axis.line.y = element_line(colour = ""black"", size=0.5)) + # Work around for classic theme being broke\r\n  ylab(""Lecticity CWM"") +\r\n  theme(axis.text.y=element_text(size=14), axis.title.y=element_text(size=18)) +\r\n  xlab(NULL) +\r\n  labs(title=""A."") +\r\n  theme(plot.title = element_text(face = ""bold"",vjust = 8,size = 20)) +\r\n  theme(axis.text.x=element_text(size=18)) +\r\n  theme(plot.margin=unit(c(1.1,1.1,1.1,1.1), ""cm"")) +\r\n  theme(axis.title.y = element_text(margin=margin(0,15,0,0)), axis.title.x = element_text(margin=margin(15,0,0,0)) ) +\r\n  theme(legend.position = ""none"")\r\n\r\n#tongue length analysis\r\n\r\n\r\ncw.mod1<-lmer(tl_cwm~type+(1|round/pair),data=cwm_abu)\r\n\r\nsummary(cw.mod1)\r\n\r\n# Trait graphs Tongue Length\r\n\r\nton.pred <- aggregate(cwm_abu[c(""tl_cwm"")], \r\n                      by = cwm_abu[c(""type"")], FUN=mean)\r\n\r\nton.pred$se<- aggregate(cwm_abu[c(""tl_cwm"")], \r\n                        by = cwm_abu[c(""type"")], FUN=std)[,2]\r\n\r\nggplot(ton.pred, aes(x=type, y=tl_cwm))+ # fig 5B\r\n  geom_bar(stat=""identity"", position=""dodge"",fill=cols,color=""black"",size=1) +\r\n  geom_errorbar(aes(ymin=tl_cwm-se, ymax=tl_cwm+se), width=0.4, position=position_dodge(width=0.9), color=""black"",size=1) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(0,6)) + # Modifying the Y axis limit\r\n  #annotate(""text"", x = c(1), y = c( .45), label = c(""*""), size=10) + # adding letters for significance\r\n  theme_classic() +\r\n  theme(axis.line.x = element_line(colour = ""black"", size=0.5),axis.line.y = element_line(colour = ""black"", size=0.5)) + # Work around for classic theme being broke\r\n  theme(axis.title.x =element_blank()) + # modifying the Y axis label\r\n  ylab(""Tongue Length CWM"") +\r\n  xlab(NULL) +\r\n  labs(title=""B."") +\r\n  theme(plot.title = element_text(face = ""bold"",vjust = 8,size = 20)) +\r\n  theme(axis.text.y=element_text(size=14), axis.title.y=element_text(size=18)) +\r\n  theme(axis.text.x=element_text(size=18)) +\r\n  theme(plot.margin=unit(c(1.1,1.1,1.1,1.1), ""cm"")) +\r\n  theme(axis.title.y = element_text(margin=margin(0,15,0,0)), axis.title.x = element_text(margin=margin(15,0,0,0)) ) +\r\n  theme(legend.position = ""none"")']","Differences in bee community composition between restored and remnant prairies are more strongly linked to forb community differences than landscape differences 1. Grassland restoration is an important tool for conserving bee biodiversity within agricultural landscapes. Restorations foster increases in local bee abundance and -diversity, however, these measures are insufficient for understanding if remnant communities are being conserved. We compared native bee -diversity, -diversity, and community composition between restored and remnant prairies in Minnesota, USA. We then investigated two potential drivers of bee community dissimilarity between restored and remnant prairies: proportion of agricultural land surrounding a restoration and differences in floral community between restored and remnant prairies.2. We selected ten restored prairies that lie along a gradient of increasing agricultural land cover, ranging from 2085% of surrounding land in agricultural production. We paired each restoration with a nearby prairie remnant and sampled bee and floral communities concurrently in each restoration-remnant pair. We quantified bee and forb -diversity, community composition, -diversity, and levels of dissimilarity between restoration-remnant pairs along the gradient of agricultural development. Additionally, we quantified differences in the community weighted mean between restored and remnant prairies for two bee traits, oligolecty and tongue length, to investigate how differences in floral community between restored and remnant prairies may influence bee community composition.3. While bee -diversity between restored and remnant prairies was similar, bee composition between restorations and remnants were significantly different with restorations being more homogenous than remnants. Differences in bee community composition and -diversity were not statisticlly related to agricultural landscapes or floral community dissimilarity, however, we found a significantly higher proportion of oligolectic bees in remnant prairies. This difference in oligolectic bees was likely related to the absence of important host plants in restorations.4. 'Synthesis and application': Prairie restorations should seek to provide diverse floral resources that are of similar composition to remnant prairies. Specifically, providing important floral host plants for pollen specialist bees could improve restorations' ability to conserve prairie remnant bee communities.",3
IPCC Climate Zones (from the 2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories),"DescriptionThese data (re)create spatial data for the 2019 IPCC Climate Zones, shown in Figure 3A.5.1 of Chapter 3: Consistent Representation of Lands in Volume 4: Agriculture, Forestry and Other Land Use of the 2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories. I recreated these data because I could not readily identify the data in a spatial format online, a problem which has previously been noted by ESDAC, who produced a spatial version of Figure 3A.5.1 from the original 2006 guidelines.Resolution: 0.5 arc degreeCRS: lon/lat WGS 84If you use these data please ensure you also cite the IPCC - Calvo Buendia, E et al. (2019). 2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories. IPCC, Switzerland. MethodsThe data were derived using the classification scheme shown in Figure 3A.5.2 based on the gridded Climate Research Unit (CRU) Time Series (TS) monthly climate data (Harris et al., 2014) for the period from 1985 to 2015 following the methods described in Annex 3A.5 Default climate and soil classifications of the above Chapter. All data were processed in R version 4.2.1, with the packages elevatr (v0.4.2), lubridate (v1.8.0), magrittr (v2.0.3), and terra (v1.6-7) attached. The full session info is included as a .txt file. As these methods are not exhaustively described in the Annex, the following assumptions were made:CRU TS3.25 was used as the most recently published data (published on 2017-09-22) that could have been incorporated into the Refinement. Other possibilities include CRU TS3.24 (which are the first data to include 2015), or CRU TS4.00 or CRU TS4.01 (both of which were published in parallel to 3.24 and 3.25). These data were all investigated, and CRU TS3.25 produced results that were the most visually similar to the published Figure 3A.5.1 (though non-identical).As the methods did not mention a preferred elevation data source, the elevatr R package was used to obtain data at zoom level 2 (approx resolution of 0.15 arc degree), that was then resampled to match the 0.5-degree resolution of the CRU data. These data originally come from the ETOPO1 global relief model. Known discrepanciesThe distribution of Tropical Wet and Tropical Moist in South America does not exactly match the original data.There are small discrepancies in Tropical Montane classifications (likely arising from the use of a different elevation layer). These are most noticeable in, but not restricted to, Africa.The classification of Boreal Dry, Polar Dry, and Polar Moist in northern Russia and (to a lesser extent) in northern Canada does not exactly match the original data.There are a small number of Cool Temperate Dry pixels in the UK, and Warm Temperate Dry pixels around Brittany which do not occur in the original data. DisclaimerI am not affiliated with the IPCC in any way, I just needed spatial data of the Climate Zones, and could not readily identify any online. This is a problem which has previously been noted by ESDAC, who produced a spatial version of Figure 3A.5.1 from the original 2006 guidelines. File descriptionREADME.html - ~this description file.IPCC_Climate_Zones_ts_3.25.tif - the output Climate Zones map at 0.5-arc degree resolution based on the CRU TS3.25 data.IPCC_Climate_Zones_colour_map.clr - a colour map file to render the output map with the same colours as in the IPCC 2019 Refinement figure.IPCC_Climate_Zones_ts_3.25.png - an image file of the output Climate Zones map.ipcc_climate_zones_2019.R - the script used to produce these data.session_info.txt - the R session info.","['\r\n# Script Purpose ----------------------------------------------------------\r\n\r\n# This script attempts to recreate:\r\n#   Fig 3A.5.1 Delineation of major climate zones, updated from the 2006 IPCC Guidelines.\r\n# Located in:\r\n#   Chapter 3: Consistent Representation of Lands\r\n#   Volume 4: Agriculture, Forestry and Other Land Uses\r\n#   2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories\r\n# Accessible at:\r\n#   https://www.ipcc-nggip.iges.or.jp/public/2019rf/pdf/4_Volume4/19R_V4_Ch03_Land%20Representation.pdf\r\n# Noting that there is a correction to the figure:\r\n#   https://www.ipcc-nggip.iges.or.jp/public/2019rf/corrigenda1.html\r\n\r\n# Script created: 2022-11-07\r\n# Author: Matt Lewis\r\n\r\n# Packages ----------------------------------------------------------------\r\n\r\npacks <-\r\n  c(\r\n    \'magrittr\',\r\n    \'terra\',\r\n    \'elevatr\',\r\n    \'lubridate\'\r\n  )\r\n\r\ninvisible(\r\n  lapply(\r\n    packs,\r\n    library,\r\n    character.only = T\r\n  )\r\n)\r\n\r\n# Data --------------------------------------------------------------------\r\n\r\n## Climate\r\nclim_ver <- \'ts_3.25\'\r\nclim_dir <- paste0(\'100_data/110_rawdata/ClimateResearchUnit/\', clim_ver,\'/\')\r\n\r\n# Temp\r\ntemp <-\r\n  paste0(clim_dir, \'temp_mean/\') %>%\r\n  list.files(full.names = T, recursive = T, pattern = "".nc"") %>%\r\n  lapply(terra::rast) %>%\r\n  terra::rast()\r\n\r\n# Precipitation\r\npre <-\r\n  paste0(clim_dir, \'precipitation/\') %>%\r\n  list.files(full.names = T, recursive = T, pattern = "".nc"") %>%\r\n  lapply(terra::rast) %>%\r\n  terra::rast()\r\n  \r\n# PET\r\npet <-\r\n  paste0(clim_dir, \'PET/\') %>%\r\n  list.files(full.names = T, recursive = T, pattern = "".nc"") %>%\r\n  lapply(terra::rast) %>%\r\n  terra::rast()\r\n\r\n# Frost\r\nfrost <-\r\n  paste0(clim_dir, \'frost/\') %>%\r\n  list.files(full.names = T, recursive = T, pattern = "".nc"") %>%\r\n  lapply(terra::rast) %>%\r\n  terra::rast()\r\n\r\n# elevation - commented out lines obtain data.\r\n# at a zoom level of 2, elevatr uses ETOPO1 data, see\r\n# https://github.com/tilezen/joerd/blob/master/docs/data-sources.md \r\n# and https://www.ngdc.noaa.gov/mgg/global/global.html\r\nelev <- \r\n  # elevatr::get_elev_raster(locations = raster::raster(frost[[1]]), z = 2) %>%\r\n  # terra::rast() %>%\r\n  # terra::resample(frost[[1]])\r\n  \'100_data/110_rawdata/elevatr/elevation_halfdeg.tif\' %>%\r\n  terra::rast()\r\n\r\n# Aggregate time ----------------------------------------------------------\r\n\r\ntime_window <- \r\n  seq(as.Date(\'1985-01-01\'), as.Date(\'2016-01-01\'), \'years\')\r\n\r\n# aggregate temperature for all months to get mean annual temperature\r\nagg_mean <-\r\n  function(x){\r\n    time_s <- terra::time(x)\r\n    lyrs <- which(time_s >= time_window[1] &\r\n                  time_s < tail(time_window, 1))\r\n    ret <-\r\n      x %>%\r\n      terra::subset(lyrs) %>%\r\n      mean()\r\n    \r\n    return(ret)\r\n  }\r\ntemp_agg <- temp %>% agg_mean()\r\n\r\n# aggregate temperature for each month across all years\r\nagg_mean_monthly <-\r\n  function(x){\r\n    time_s <- terra::time(x)\r\n    lyrs <- which(time_s >= time_window[1] &\r\n                    time_s < tail(time_window, 1))\r\n    lyrs_seq <- outer(X=c(1:12), Y=seq(0, length(lyrs), 12), FUN=""+"")\r\n    ret <- list()\r\n    for(i in 1:nrow(lyrs_seq)){\r\n      ret[[i]] <-\r\n        x %>%\r\n        terra::subset(lyrs_seq[i,]) %>%\r\n        mean()\r\n      \r\n      terra::set.names(ret[[i]], as.character(month.name[i]))\r\n    }\r\n    ret<-\r\n      ret %>%\r\n      terra::rast()\r\n    return(ret)\r\n  }\r\ntemp_agg_monthly <- temp %>% agg_mean_monthly()\r\n\r\n# aggregate by summing for one year then taking the mean across multiple\r\nagg_ann_sum <-\r\n  function(x, unit_denom = NA){\r\n    time_s <- terra::time(x)\r\n    # sum\r\n    ret<- list()\r\n    for(i in 1:(length(time_window)-1)){\r\n      lyrs <- which(time_s >= time_window[i] &\r\n                      time_s <= time_window[i+1])\r\n      if(length(lyrs) == 0L){next()}\r\n      tmp <-\r\n        x %>%\r\n        terra::subset(lyrs)\r\n      \r\n      # unit_denom here allows PET which is in mm/day to be converted to mm/month \r\n      # to match precipitation data\r\n      if(!is.na(unit_denom)){\r\n        # get number of days in months to multiply mm/day by for each month for PET\r\n        # first make each date the start of each month\r\n        yr <- lubridate::year(time_s[lyrs][1])\r\n        \r\n        lyrs_mths <- \r\n          paste(c(rep(yr, 12), yr+1), c(1:12, 1), 1, sep = \'-\') %>%\r\n          as.Date()\r\n        \r\n        # then get the number of days in each interval\r\n        lyrs_multi <-\r\n          (lyrs_mths[2:length(lyrs_mths)] - \r\n          lyrs_mths[1:length(lyrs_mths)-1]) %>%\r\n          as.numeric()\r\n        \r\n        # make sure we are multiplying by a vector of correct length\r\n        stopifnot(length(lyrs_multi) == terra::nlyr(tmp))\r\n        \r\n        # and multiply!\r\n        tmp <-\r\n          tmp * lyrs_multi\r\n      }\r\n      \r\n      ret[[i]] <-\r\n        tmp %>%\r\n        sum()\r\n    }\r\n    \r\n    # mean\r\n    ret <-\r\n      ret %>%\r\n      terra::rast() %>%\r\n      mean()\r\n    \r\n    return(ret)\r\n  }\r\n\r\n# NOTE: The decision tree published by the IPCC only makes sense if mean annual\r\n# precipitation (MAP) is in units of mm/month\r\npet_agg <- pet %>% agg_ann_sum(unit_denom = \'day\')\r\npre_agg <- pre %>% agg_ann_sum(unit_denom = NA)\r\n\r\nfrost_agg <- frost %>% agg_ann_sum(unit_denom = NA)\r\n\r\n# precipitation:PET\r\npre_pet <- pre_agg / pet_agg\r\n\r\n# Decision tree -----------------------------------------------------------\r\n\r\n# taken from https://www.ipcc-nggip.iges.or.jp/public/2019rf/corrigenda1.html\r\n\r\nipcc_clim_zones <- list()\r\n\r\ntropical <-\r\n  # mean temp > 18\r\n  temp_agg %>%\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(18, Inf, 1)), \r\n                  others = NA, include.lowest = F, right = T) %>%\r\n  terra::mask(\r\n    # <= 7 frost days\r\n    frost_agg %>%\r\n      terra::classify(matrix(nrow = 1, ncol = 3, data = c(0, 7, 1)),\r\n                      others = NA, include.lowest = T, right = T)\r\n  )\r\n\r\nipcc_clim_zones$tropical_montane <-\r\n  tropical %>%\r\n  terra::mask(\r\n    # elevation > 1000m\r\n    elev %>%\r\n      terra::classify(matrix(nrow = 1, ncol = 3, data = c(1000, Inf, 1)), \r\n                      others = NA, include.lowest = F, right = T)\r\n  )\r\n\r\nipcc_clim_zones$tropical_wet <-\r\n  # tropical - tropical montane\r\n  tropical %>%\r\n  terra::mask(ipcc_clim_zones$tropical_montane, inverse = T) %>%\r\n  terra::mask(\r\n    # precipitation > 2000mm/month\r\n    pre_agg %>%\r\n      terra::classify(matrix(nrow = 1, ncol = 3, data = c(2000, Inf, 1)), \r\n                      others = NA, include.lowest = F, right = T)\r\n  ) %>%\r\n  # give value of 2 (for flattening list to rast later)\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(-Inf, Inf, 2)),\r\n                  others = NA, include.lowest = F, right = F)\r\n\r\nipcc_clim_zones$tropical_moist <-\r\n  # tropical - tropical montane and tropical wet\r\n  tropical %>%\r\n  terra::mask(ipcc_clim_zones$tropical_montane, inverse = T) %>%\r\n  terra::mask(ipcc_clim_zones$tropical_wet, inverse = T) %>%\r\n  terra::mask(\r\n    # precipitation > 1000mm/month\r\n    pre_agg %>%\r\n      terra::classify(matrix(nrow = 1, ncol = 3, data = c(1000, Inf, 1)), \r\n                      others = NA, include.lowest = F, right = T)\r\n  ) %>%\r\n  # give value of 3 (for flattening list to rast later)\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(-Inf, Inf, 3)),\r\n                  others = NA, include.lowest = F, right = F)\r\n\r\nipcc_clim_zones$tropical_dry <-\r\n  # tropical - tropical montane, tropical wet, and tropical moist\r\n  tropical %>%\r\n  terra::mask(ipcc_clim_zones$tropical_montane, inverse = T) %>%\r\n  terra::mask(ipcc_clim_zones$tropical_wet, inverse = T) %>%\r\n  terra::mask(ipcc_clim_zones$tropical_moist, inverse = T) %>%\r\n  # give value of 4 (for flatting list to rast later)\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(-Inf, Inf, 4)),\r\n                  others = NA, include.lowest = F, right = F)\r\n\r\n## Warm temperate\r\n\r\nwarm_temperate <-\r\n  # mean temp > 10\r\n  temp_agg %>%\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(10, Inf, 1)), \r\n                  others = NA, include.lowest = F, right = T) %>%\r\n  # get rid of tropical\r\n  terra::mask(\r\n    tropical,\r\n    inverse = T\r\n  )\r\n\r\nipcc_clim_zones$warm_temperate_moist <-\r\n  warm_temperate %>%\r\n  terra::mask(\r\n    # pre:pet >1\r\n    pre_pet %>%\r\n      terra::classify(matrix(nrow = 1, ncol =3, data = c(1, Inf, 1)),\r\n                      others = NA, include.lowest = F, right = T)\r\n  ) %>%\r\n  # give value of 5 (for flatting list to rast later)\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(-Inf, Inf, 5)),\r\n                  others = NA, include.lowest = F, right = F)\r\n\r\nipcc_clim_zones$warm_temperate_dry <-\r\n  # warm temperate - warm temp moist\r\n  warm_temperate %>%\r\n  terra::mask(ipcc_clim_zones$warm_temperate_moist, inverse = T) %>%\r\n  # give value of 6 (for flatting list to rast later)\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(-Inf, Inf, 6)), \r\n                  others = NA, include.lowest = F, right = F)\r\n\r\n## Cool temperate\r\ncool_temperate <-\r\n  # temp > 0\r\n  temp_agg %>%\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(0, Inf, 1)), \r\n                  others = NA, include.lowest = F, right = T) %>%\r\n  # remove tropical and warm temperate\r\n  terra::mask(tropical, inverse = T) %>%\r\n  terra::mask(warm_temperate, inverse = T)\r\n\r\nipcc_clim_zones$cool_temperate_moist <-\r\n  cool_temperate %>%\r\n  terra::mask(\r\n    # pre:pet >1\r\n    pre_pet %>%\r\n      terra::classify(matrix(nrow = 1, ncol =3, data = c(1, Inf, 1)),\r\n                      others = NA, include.lowest = F, right = T)\r\n  ) %>%\r\n  # give value of 7 (for flatting list to rast later)\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(-Inf, Inf, 7)), \r\n                  others = NA, include.lowest = F, right = F)\r\n\r\nipcc_clim_zones$cool_temperate_dry <-\r\n  # cool temperate - cool temperate moist\r\n  cool_temperate %>%\r\n  terra::mask(ipcc_clim_zones$cool_temperate_moist, inverse = T) %>%\r\n  # give value of 8 (for flatting list to rast later)\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(-Inf, Inf, 8)), \r\n                  others = NA, include.lowest = F, right = F)\r\n\r\n## Boreal\r\nboreal_polar <-\r\n  # mean temp > 0\r\n  temp_agg %>%\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(-Inf, 0, 1)), \r\n                  others = NA, include.lowest = F, right = T)\r\n\r\nboreal <-\r\n  boreal_polar %>%\r\n  # at least 1 month\'s mean temperature >= 10\r\n  terra::mask(\r\n    temp_agg_monthly %>%\r\n      terra::classify(matrix(nrow = 1, ncol = 3, data = c(10, Inf, 1)), \r\n                      others = NA, include.lowest = T, right = T) %>%\r\n      mean(na.rm = T)\r\n  )\r\n\r\nipcc_clim_zones$boreal_moist <-\r\n  boreal %>%\r\n  terra::mask(\r\n    # pre:pet >1\r\n    pre_pet %>%\r\n      terra::classify(matrix(nrow = 1, ncol =3, data = c(1, Inf, 1)),\r\n                      others = NA, include.lowest = F, right = T)\r\n  ) %>%\r\n  # give value of 9 (for flatting list to rast later)\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(-Inf, Inf, 9)), \r\n                  others = NA, include.lowest = F, right = F)\r\n\r\nipcc_clim_zones$boreal_dry <-\r\n  # boreal - boreal moist\r\n  boreal %>%\r\n  terra::mask(ipcc_clim_zones$boreal_moist, inverse = T) %>%\r\n  # give value of 10 (for flatting list to rast later)\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(-Inf, Inf, 10)), \r\n                  others = NA, include.lowest = F, right = F)\r\n\r\n## Polar\r\npolar <-\r\n  # boreal/polar - boreal\r\n  boreal_polar %>%\r\n  terra::mask(boreal, inverse = T)\r\n\r\nipcc_clim_zones$polar_moist <-\r\n  polar %>%\r\n  terra::mask(\r\n    # pre:pet >1\r\n    pre_pet %>%\r\n      terra::classify(matrix(nrow = 1, ncol =3, data = c(1, Inf, 1)),\r\n                      others = NA, include.lowest = F, right = T)\r\n  ) %>%\r\n  # give value of 11 (for flatting list to rast later)\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(-Inf, Inf, 11)), \r\n                  others = NA, include.lowest = F, right = F)\r\n\r\nipcc_clim_zones$polar_dry <-\r\n  # polar - polar moist\r\n  polar %>%\r\n  terra::mask(ipcc_clim_zones$polar_moist, inverse = T) %>%\r\n  # give value of 12 (for flatting list to rast later)\r\n  terra::classify(matrix(nrow = 1, ncol = 3, data = c(-Inf, Inf, 12)), \r\n                  others = NA, include.lowest = F, right = F)\r\n\r\n# Flattening list to rast -------------------------------------------------\r\n\r\nipcc_clim_zones_rast <-\r\n  ipcc_clim_zones %>%\r\n  # flatten\r\n  terra::rast() %>%\r\n  # each cell should have one value so this could be sum, or any number of functions\r\n  mean(na.rm = T) %>%\r\n  # make categorical\r\n  as.factor()\r\n\r\n# add levels\r\nlevels(ipcc_clim_zones_rast) <-\r\n  data.frame(\r\n    ids = c(1:12),\r\n    label = c(\r\n      \'Tropical Montane\', \'Tropical Wet\', \'Tropical Moist\', \'Tropical Dry\',\r\n      \'Warm Temperate Moist\', \'Warm Temperate Dry\',\r\n      \'Cool Temperate Moist\', \'Cool Temperate Dry\',\r\n      \'Boreal Moist\', \'Boreal Dry\',\r\n      \'Polar Moist\', \'Polar Dry\'\r\n    )\r\n  )\r\n\r\n# Export ------------------------------------------------------------------\r\n\r\noutdir <- \'300_outputs/IPCC_Climate_Zones/\'\r\n\r\ndir.create(outdir, showWarnings = F, recursive = T)\r\n\r\nterra::writeRaster(ipcc_clim_zones_rast, \r\n                   filename = paste0(outdir, \'IPCC_Climate_Zones_\', clim_ver, \'.tif\'),\r\n                   overwrite = T)\r\n\r\npng(paste0(outdir, \'IPCC_Climate_Zones_\', clim_ver, \'.png\'), res = 300, \r\n    height = 3000, width = 6000)\r\nplot(ipcc_clim_zones_rast,\r\n     col = c(\'#6699cd\', \'#448970\', \'#89cd66\', \'#f5f57a\',\r\n             \'#73dfff\', \'#ffd37f\', \'#cdf57a\', \'#c29ed7\',\r\n             \'#9eaad7\', \'#d7d79e\', \'#d9ffe8\', \'#e1e1e1\'),\r\n     legend = \'bottom\')\r\ndev.off()\r\n']","IPCC Climate Zones (from the 2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories) DescriptionThese data (re)create spatial data for the 2019 IPCC Climate Zones, shown in Figure 3A.5.1 of Chapter 3: Consistent Representation of Lands in Volume 4: Agriculture, Forestry and Other Land Use of the 2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories. I recreated these data because I could not readily identify the data in a spatial format online, a problem which has previously been noted by ESDAC, who produced a spatial version of Figure 3A.5.1 from the original 2006 guidelines.Resolution: 0.5 arc degreeCRS: lon/lat WGS 84If you use these data please ensure you also cite the IPCC - Calvo Buendia, E et al. (2019). 2019 Refinement to the 2006 IPCC Guidelines for National Greenhouse Gas Inventories. IPCC, Switzerland. MethodsThe data were derived using the classification scheme shown in Figure 3A.5.2 based on the gridded Climate Research Unit (CRU) Time Series (TS) monthly climate data (Harris et al., 2014) for the period from 1985 to 2015 following the methods described in Annex 3A.5 Default climate and soil classifications of the above Chapter. All data were processed in R version 4.2.1, with the packages elevatr (v0.4.2), lubridate (v1.8.0), magrittr (v2.0.3), and terra (v1.6-7) attached. The full session info is included as a .txt file. As these methods are not exhaustively described in the Annex, the following assumptions were made:CRU TS3.25 was used as the most recently published data (published on 2017-09-22) that could have been incorporated into the Refinement. Other possibilities include CRU TS3.24 (which are the first data to include 2015), or CRU TS4.00 or CRU TS4.01 (both of which were published in parallel to 3.24 and 3.25). These data were all investigated, and CRU TS3.25 produced results that were the most visually similar to the published Figure 3A.5.1 (though non-identical).As the methods did not mention a preferred elevation data source, the elevatr R package was used to obtain data at zoom level 2 (approx resolution of 0.15 arc degree), that was then resampled to match the 0.5-degree resolution of the CRU data. These data originally come from the ETOPO1 global relief model. Known discrepanciesThe distribution of Tropical Wet and Tropical Moist in South America does not exactly match the original data.There are small discrepancies in Tropical Montane classifications (likely arising from the use of a different elevation layer). These are most noticeable in, but not restricted to, Africa.The classification of Boreal Dry, Polar Dry, and Polar Moist in northern Russia and (to a lesser extent) in northern Canada does not exactly match the original data.There are a small number of Cool Temperate Dry pixels in the UK, and Warm Temperate Dry pixels around Brittany which do not occur in the original data. DisclaimerI am not affiliated with the IPCC in any way, I just needed spatial data of the Climate Zones, and could not readily identify any online. This is a problem which has previously been noted by ESDAC, who produced a spatial version of Figure 3A.5.1 from the original 2006 guidelines. File descriptionREADME.html - ~this description file.IPCC_Climate_Zones_ts_3.25.tif - the output Climate Zones map at 0.5-arc degree resolution based on the CRU TS3.25 data.IPCC_Climate_Zones_colour_map.clr - a colour map file to render the output map with the same colours as in the IPCC 2019 Refinement figure.IPCC_Climate_Zones_ts_3.25.png - an image file of the output Climate Zones map.ipcc_climate_zones_2019.R - the script used to produce these data.session_info.txt - the R session info.",3
Data from: Positive severity feedback between consecutive fires in dry eucalypt forests of southern Australia,"Fire regimes have long-term effects on ecosystems which can be subtle, requiring study at a large spatial scale and temporal scale to fully appreciate. The way in which multiple fires interact to create a fire regime is poorly understood, and the relationship between the severities of consecutive fires has not been studied in Australia. By overlaying remotely sensed severity maps, our study investigated how the severity of a fire is influenced by previous fire severity. This was done by sampling points at 500-m spacing across 53 fires in dry eucalypt forests of southeast Australia, over a range of time since fire spanning every major fire season for 30 yr. Generalized additive models were used to determine the influence of previous severity on the probability of crown fire and understory fire, controlling for differences in time since fire, topography, and weather. We found that a crown fire is more than twice as likely after a previous crown fire than previous understory fire, and understory fire is more likely after previous understory fire. Our findings are in line with the results of studies from North America and suggest that severe fire promotes further fire. This may be evidence of a runaway positive feedback, which can drive ecological change, and lead to a mosaic of divergent vegetation, but research into more than two consecutive fires is needed to explore this. Our results also suggest that a low-severity prescribed fire may be a useful management option for breaking a cycle of crown fires.","['#### Positive severity feedback between consecutive fires in dry eucalypt forests of southern Australia ####\r\n\r\n# To fit random effects logistic regression\r\nlibrary(mgcv)\r\n# For data munging\r\nlibrary(tidyverse)\r\nlibrary(caret)\r\n\r\n# Path to data - edit as required\r\nPATH <- ""data/gis_data.csv""\r\n\r\n# Read in data\r\nDAT <- read.csv(PATH)\r\n\r\n#### Data manipulation ####\r\nDAT <- DAT %>%\r\n  # Add some derived and re-scaled variables\r\n  mutate(\r\n    lowsev = sev2 == 1,\r\n    highsev = sev2 == 3,\r\n    solrad = b1_solrad / 100) %>%\r\n  \r\n  # Subset the data\r\n  filter(\r\n    sev2 > 0,\r\n    vegclass==""Dry sclerophyll forests (Shrubby subformation)"",\r\n    sev1 > 0,\r\n    ffdi <= 75\r\n  )\r\n\r\n# Find fires with at least 10 data records\r\nfires10 <- DAT %>%\r\n  group_by(firename) %>%\r\n  summarize(count = n()) %>%\r\n  filter(count >= 10)\r\n\r\n# Subset data to these fires\r\nDAT <- filter(DAT, firename %in% fires10$firename)\r\n\r\n# Separate into test and training data\r\nset.seed(1234)\r\nTrain <- createDataPartition(DAT$sev2, p=0.7, list=FALSE)\r\ntraining <- DAT[ Train, ]\r\ntesting <- DAT[ -Train, ]\r\n\r\nattach(training)\r\n\r\n#### Analyses ####\r\n# Crown Fire\r\nhsbest<- gam(highsev ~ sev1 + s(tsf, k=4) + te(slope,solrad) + \r\n               topos + ffdi +\r\n               s(firename, bs=""re""),\r\n             family=binomial(),\r\n             data = training)\r\n\r\n# Understorey Fire\r\nlsbest<- gam(lowsev~sev1+s(slope)+\r\n               topos+ tsf + ffdi + \r\n               s(firename, bs=""re""),\r\n             family = binomial(),\r\n             data = training)\r\n\r\n# Test models\r\n# Crown Fire\r\nhstest<- gam(highsev ~ sev1 + s(tsf, k=4) + te(slope,solrad) + \r\n               topos + ffdi +\r\n               s(firename, bs=""re""),\r\n             family=binomial(),\r\n             data = testing)\r\n\r\n# Understorey Fire\r\nlstest<- gam(lowsev~sev1+s(slope)+\r\n               topos+ tsf + ffdi + \r\n               s(firename, bs=""re""),\r\n             family = binomial(),\r\n             data = testing)\r\n\r\n#### Accuracy ####\r\n# Crown fire\r\n# Training Data\r\ntraining$pred <- predict(hsbest, type = ""response"")\r\ntraining$predtrue <- training$pred>0.5\r\nwith(training,table(highsev,predtrue))\r\n\r\n# Testing Data\r\ntesting$pred <- predict(hstest, type = ""response"")\r\ntesting$predtrue <- testing$pred>0.5\r\nwith(testing,table(highsev,predtrue))\r\n\r\n# Understorey Fire\r\n# Training Data\r\ntraining$pred <- predict(lsbest, type = ""response"")\r\ntraining$predtrue <- training$pred>0.55\r\nwith(training,table(lowsev,predtrue))\r\n\r\n#Testing Data\r\ntesting$pred <- predict(lstest, type = ""response"")\r\ntesting$predtrue <- testing$pred>0.5\r\nwith(testing,table(lowsev,predtrue))']","Data from: Positive severity feedback between consecutive fires in dry eucalypt forests of southern Australia Fire regimes have long-term effects on ecosystems which can be subtle, requiring study at a large spatial scale and temporal scale to fully appreciate. The way in which multiple fires interact to create a fire regime is poorly understood, and the relationship between the severities of consecutive fires has not been studied in Australia. By overlaying remotely sensed severity maps, our study investigated how the severity of a fire is influenced by previous fire severity. This was done by sampling points at 500-m spacing across 53 fires in dry eucalypt forests of southeast Australia, over a range of time since fire spanning every major fire season for 30 yr. Generalized additive models were used to determine the influence of previous severity on the probability of crown fire and understory fire, controlling for differences in time since fire, topography, and weather. We found that a crown fire is more than twice as likely after a previous crown fire than previous understory fire, and understory fire is more likely after previous understory fire. Our findings are in line with the results of studies from North America and suggest that severe fire promotes further fire. This may be evidence of a runaway positive feedback, which can drive ecological change, and lead to a mosaic of divergent vegetation, but research into more than two consecutive fires is needed to explore this. Our results also suggest that a low-severity prescribed fire may be a useful management option for breaking a cycle of crown fires.",3
Data and code: Gut microbiota structure differs between honey bees in winter and summer,"This dataset contains data and code underlying the qPCR, amplicon sequencing, and statistical analysis of the research article ""Gut microbiota structure differs between honey bees in winter and summer. Short read datasets are available under NCBI Bioproject accession PRJNA578869.",,"Data and code: Gut microbiota structure differs between honey bees in winter and summer This dataset contains data and code underlying the qPCR, amplicon sequencing, and statistical analysis of the research article ""Gut microbiota structure differs between honey bees in winter and summer. Short read datasets are available under NCBI Bioproject accession PRJNA578869.",3
Data from: Maternal allocation of carotenoids increases tolerance to bacterial infection in brown trout,"Life-history theory predicts that iteroparous females allocate their resources differently among different breeding seasons depending on their residual reproductive value. In iteroparous salmonids there is typically much variation in egg size, egg number, and in the compounds that females allocate to their clutch. These compounds include various carotenoids whose functions are not sufficiently understood yet. We sampled 37 female and 35 male brown trout from natural streams, collected their gametes for in vitro fertilizations, experimentally produced 185 families in 7 full-factorial breeding blocks, raised the developing embryos singly (n = 2960), and either sham-treated or infected them with Pseudomonas fluorescens. We used female redness (as a measure of carotenoids stored in the skin) and their allocation of carotenoids to clutches to infer maternal strategies. Astaxanthin contents largely determined egg colour. Neither egg weight nor female size was correlated with the content of this carotenoid. However, astaxanthin content was positively correlated with larval growth and with tolerance against P. fluorescens. There was a negative correlation between female skin redness and the carotenoid content of their eggs. Although higher astaxanthin contents in the eggs were associated with an improvement of early fitness-related traits, some females appeared not to maximally support their current offspring as revealed by the negative correlation between female red skin colouration and egg carotenoid content. This correlation was not explained by female size and supports the prediction of a maternal trade-off between current and future reproduction.",,"Data from: Maternal allocation of carotenoids increases tolerance to bacterial infection in brown trout Life-history theory predicts that iteroparous females allocate their resources differently among different breeding seasons depending on their residual reproductive value. In iteroparous salmonids there is typically much variation in egg size, egg number, and in the compounds that females allocate to their clutch. These compounds include various carotenoids whose functions are not sufficiently understood yet. We sampled 37 female and 35 male brown trout from natural streams, collected their gametes for in vitro fertilizations, experimentally produced 185 families in 7 full-factorial breeding blocks, raised the developing embryos singly (n = 2960), and either sham-treated or infected them with Pseudomonas fluorescens. We used female redness (as a measure of carotenoids stored in the skin) and their allocation of carotenoids to clutches to infer maternal strategies. Astaxanthin contents largely determined egg colour. Neither egg weight nor female size was correlated with the content of this carotenoid. However, astaxanthin content was positively correlated with larval growth and with tolerance against P. fluorescens. There was a negative correlation between female skin redness and the carotenoid content of their eggs. Although higher astaxanthin contents in the eggs were associated with an improvement of early fitness-related traits, some females appeared not to maximally support their current offspring as revealed by the negative correlation between female red skin colouration and egg carotenoid content. This correlation was not explained by female size and supports the prediction of a maternal trade-off between current and future reproduction.",3
Slicing: a sustainable approach to structuring samples for analysis in long-term studies,"The longitudinal study of populations is a core tool for understanding ecological and evolutionary processes. Long-term studies typically collect samples repeatedly over individual lifetimes and across generations. These samples are then analysed in batches (e.g. qPCR-plates) and clusters (i.e. group of batches) over time in the laboratory. However, these analyses are constrained by cross-classified data structures introduced biologically or through experimental design. The separation of biological variation from the confounding among-batch and among-cluster variation is crucial, yet often ignored.The commonly used approaches to structuring samples for analysis, sequential and randomisation, generate bias due to the non-independence between time of collection and the batch and cluster they are analysed in. We propose a new sample structuring strategy, called slicing, designed to separate confounding among-batch and among-cluster variation from biological variation. Through simulations we tested the statistical power and precision to detect within-individual, between-individual, year and cohort effects of this novel approach.Our slicing approach, whereby recently and previously collected samples are sequentially analysed in clusters together, enables the statistical separation of collection time and cluster effects by bridging clusters together, for which we provide a case study. Our simulations show, with reasonable slicing width and angle, similar precision and similar or greater statistical power to detect year, cohort, within- and between-individual effects when samples are sliced across batches, compared with strategies that aggregate longitudinal samples or use randomised allocation.While the best approach to analysing long-term datasets depends on the structure of the data and questions of interest, it is vital to account for confounding among-cluster and batch variation. Our slicing approach is simple to apply and creates the necessary statistical independence of batch and cluster from environmental or biological variables of interest. Crucially, it allows sequential analysis of samples and flexible inclusion of current data in later analyses without completely confounding the analysis. Our approach maximises the scientific value of every sample, as each will optimally contribute to unbiased statistical inference from the data. Slicing thereby maximises the power of growing biobanks to address important ecological, epidemiological and evolutionary questions.","['###########################################################################################\r\n# Slicing: a sustainable approach to structuring samples for analysis in long-term studies\r\n# Original author: Mirre JP Simons (University of Sheffield)\r\n# Edited by: Sil HJ van Lieshout (University of Leeds)\r\n# Last updated: 04/11/2019\r\n###########################################################################################\r\nrm(list=ls())\r\n\r\n#install.packages(""lme4"")\r\n#install.packages(""reshape"")\r\n#install.packages(""lmerTest"")\r\nlibrary(lme4)\r\nlibrary(reshape)\r\nlibrary(lmerTest)\r\n###########################################################################################\r\nindividuals=200 # Becomes 400 in simulations with increased sample size\r\nSDd=1\r\nshortening=0.06*SDd \r\nmortalityrisk=0.25\r\nslopewithTL=-0.23 \r\nyears=5 \r\nmeasurementerror=1\r\nerrorplates2=c(1,2.5,5,10,20,40) # batch attributable error\r\nsamplesonplate2=c(12,24,36,48) # number of samples per batch\r\nruns=5000\r\n\r\n# fixed effects\r\ncohort_n=10 # number of cohorts\r\ncohort_fixed=0.9 # this is the max cohort effect from which a fraction is taken (drawn from uniform distribution see below) - in paper also changed to 0.45 and 1.35 to test variation in this parameter\r\n\r\n# Add year error\r\n#year_fixed = 0.7 # this is a fraction of the year (drawn from uniform distribution see below) - in paper also changed to 0.35 and 1.05 to test variation in this parameter\r\n\r\nresultstotal=matrix(NA,ncol=71,nrow=length(errorplates2)*length(samplesonplate2))\r\n\r\n# start loops\r\ni=1\r\nwhile(i<(length(errorplates2)+1))\r\n{\r\n# set parameter values as above\t\r\nerrorplates=errorplates2[i]\r\np=1\r\nwhile(p<(length(samplesonplate2))+1)\r\n{\r\n# set parameter values as above\r\nsamplesonplate=samplesonplate2[p]\r\n\r\n# make empty matrices to put results of simulation in\r\nresults_distributed=matrix(nrow=runs,ncol=7)          \r\nresults_sameplate=matrix(nrow=runs,ncol=7)\r\nresults_slicing=matrix(nrow=runs,ncol=7)\r\n\r\nq=1\r\nwhile(q<(runs+1))\r\n{\r\n\r\n# average TL distribution of individuals (at birth)\r\nTL=rnorm(individuals,mean=0,sd=SDd)\r\n\r\n# add cohort info\r\ncohortfix=cohort_fixed*runif(cohort_n)\r\nindivpercohort=round((length(TL)/cohort_n))\r\n#add cohort effect\r\nz=1\r\nwhile(z<(cohort_n+1))\r\n{\r\n  TL[(((z-1)*indivpercohort)+1):(z*indivpercohort)]=TL[(((z-1)*indivpercohort)+1):(z*indivpercohort)]+cohortfix[z]\r\n  z=z+1\r\n}\r\n\r\n### mortality induced by TL, using TL in early life as long term predictor, note shortening rate is constant in life thus using the first TL in early life.\r\nalive<-matrix(1,nrow=individuals,ncol=years+1)\r\nk=2\r\n# loop through each year of the study with all individuals alive at year 0\r\nwhile(k<(years+2))\r\n{\r\n  # draw a number from a uniform distribution for each individual \r\n  # and compare this to the mortality risk of that individual, \r\n  # which can be deterimned by TL, as by parameter slopewithTL\r\n  # mortality follows an exponential relationship with TL, minus TL means less risk to die with increasing TL.\r\n  # this is the chance to survive to the next year\r\n  \r\n  alive[which(runif(individuals)<mortalityrisk*exp(slopewithTL*TL)),k]=0\t\t\r\n\t\r\n  # when risk is incurred, keep a zero, do not ressurect from the dead..\r\n  alive[,k]=alive[,k-1]*alive[,k]\r\n\t\r\n\tk=k+1\r\n}\r\n\r\n### generate TL dataset to fit mixed effect model on ###\r\n\r\n# make empty matrix to put results in\r\nTLset=matrix(NA,nrow=individuals,ncol=years+1)\r\n\r\n# add measurement error to true TL, note that above true TL is used, because this is the biologically (""true"") relevant component ""causing death""\r\nTLset[,1]=TL+rnorm(individuals,sd=measurementerror)\r\n\r\n# add measurement error to each additional year and add shortening (which is a set value and linear)\r\nk=1\r\nwhile(k<(years+1))\r\n{\r\nTLset[,k+1]=TL+rnorm(individuals,sd=measurementerror)-shortening*k\r\n\tk=k+1\r\n}\r\n\r\n#\'kill\' individuals that died using alive matrix generated above\r\nTLset[which(alive==0)]=NA\r\n\r\n# assign an ID variable for each row in the matrix, which is one individual\r\nid=1:individuals\r\n# cohort\r\ncohortid=rep(1:cohort_n, each=indivpercohort)\r\n# bind individual ID together with dataframe\r\nTLset<-as.data.frame(cbind(id,cohortid,TLset))\r\n\r\n# name the dataframe\r\nnames(TLset)[3:(years+3)]=1:(years+1)                 \r\nnames(TLset)[2]=\'cohort\'\r\n# reshape the dataset into a long format\r\nlibrary(reshape)\r\nTLsetlong<-reshape(TLset,varying=list(names(TLset)[3:(years+3)]),idvar=c(""id""),direction=c(""long""))\r\nnames(TLsetlong)[4]=""TL""\r\n# delete NAs\r\nTLsetlong=TLsetlong[which(is.na(TLsetlong$TL)==F),]\r\n\r\n# generate delta and average age\r\n# mean age\r\navgageID<-aggregate(TLsetlong,list(TLsetlong$id),mean)[,c(2,4)]\r\n# merge avg age to the TL set\r\ndata2<-merge(TLsetlong,avgageID,by=""id"")\r\n# name the columns\r\ncolnames(data2)[2]<-c(""cohort"")\r\ncolnames(data2)[3]<-c(""age"")\r\ncolnames(data2)[5]<-c(""avgage"")\r\n# add deltaage\r\ndata3<-cbind(data2,data2$age-data2$avgage)\r\ncolnames(data3)[6]=c(""deltaage"")\r\n\r\n# Add year to dataframe\r\n#data3$year <- data3$cohort + data3$age - 1\r\n#data3$year <- as.factor(data3$year)\r\n\r\n#years_n <- length(unique(data3$year))\r\n\r\n#year_errordraw=year_fixed*runif(years_n)\r\n# add year specific error to each year\r\n#g=1\r\n#while(g<(years_n+1))\r\n#{\r\n  #data3$TL[which(data3$year==g)]=data3$TL[which(data3$year==g)]+year_errordraw[g]\r\n  #g=g+1\r\n#}\r\n\r\n######################################################################################\r\n# these lines can be changed to change different confounding variables, i.e. sort by cohort, sort by year..\r\n######################################################################################\r\n\r\n# To make sure the data is  organised by id, so plates are sequential by ID \r\ndata3<-data3[order(data3$id) ,]\r\n\r\n#Make data ordered by year\r\n#data3<-data3[order(data3$year) ,]\r\n\r\n######################################################################################\r\n# these lines can be changed to change different confounding variables, i.e. sort by cohort, sort by year..\r\n######################################################################################\r\n\r\n\r\n### plates and error of plates\r\n# how many sample do we have\r\nsamples=dim(data3)[1]\r\n# how many plates will we have depending on samples and samplesonplate\r\nplates=round(samples/samplesonplate)\r\n\r\n# rep maxes a string of plate numbers, [1:samples] is neccesary to index the repeated string to be able to bind it together nicely\r\ndata4=cbind(rep(1:plates, each=samplesonplate)[1:samples],data3)\r\n# add error per plate\r\nplate_errordraw=rnorm(plates,sd=errorplates)\r\nnames(data4)[1]=""plate""\r\n\r\n# add plate specific error to each plate\r\nk=1\r\nwhile(k<(plates+1))\r\n{\r\n\tdata4$TL[which(data4$plate==k)]=data4$TL[which(data4$plate==k)]+plate_errordraw[k]\r\n\tk=k+1\r\n}\r\n\r\n# dataset complete and we can analyse now using a mixed model\r\nfit0<-lmer(TL~(1|id)+(1|plate)+avgage+deltaage,data=data4)\r\nfit1<-lmer(TL~(1|id)+(1|plate)+avgage+deltaage+factor(cohort),data=data4)\r\nsummary(fit1)\r\nanova_cohort=anova(fit0,fit1)\r\n# put parameters of fit1 into the result matrixs of the simulation\r\nresults_sameplate[q,]=c(data.matrix(fixef(fit1))[1:3],sqrt(diag(vcov(fit1)))[1:3],anova_cohort[2,8])\r\n\r\n\r\n# fit the model but now as if samples where distributed randomly across plates.\r\ndata3=data3[sample(nrow(data3)),]  # to randomise all samples, not plates to easily keep the error, note this is the dataset generated above where no plate specific error had been added yet.\r\nsamples=dim(data3)[1]\r\nplates=round(samples/samplesonplate)\r\ndata5=cbind(rep(1:plates, each=samplesonplate)[1:samples],data3)\r\nnames(data5)[1]=""plate""\r\n\r\nk=1\r\nwhile(k<(plates+1))\r\n{\r\n\tdata5$TL[which(data5$plate==k)]=data5$TL[which(data5$plate==k)]+plate_errordraw[k]\r\n\tk=k+1\r\n}\r\n\r\n# dataset complete and we can analyse now using a mixed model\r\nfit2<-lmer(TL~(1|id)+(1|plate)+avgage+deltaage,data=data5)\r\nfit3<-lmer(TL~(1|id)+(1|plate)+avgage+deltaage+factor(cohort),data=data5)\r\nsummary(fit3)\r\nanova_cohort2=anova(fit2,fit3)\r\n# put parameters of fit1 into the result matrixs of the simulation\r\nresults_distributed[q,]=c(data.matrix(fixef(fit3))[1:3],sqrt(diag(vcov(fit3)))[1:3],anova_cohort2[2,8])\r\n\r\n\r\n############### slicing ############\r\n# slicing across three years; note sometimes little bits left of one year are added to another plate and hence sometimes spread across 4\r\n\r\ndata6=data3\r\ndata6$year=data6$cohort+data6$age\r\ndata6<-data6[order(data6$year) ,]\r\nyearz=unique(data6$year)[order(unique(data6$year))] # order important here as samples are sliced across subsequent years.\r\ndata6$plate=NA\r\n\r\n# first plate\r\nii=1\r\nindex=which(data6$year==yearz[ii]&is.na(data6$plate)==T)\r\ndata6$plate[index[1:((3/4)*samplesonplate)]]=1\r\n## this could give error if number of samples on plate is bigger than one year ##\r\nindex=which(data6$year==yearz[ii+1]&is.na(data6$plate)==T)\r\ndata6$plate[index[1:((1/4)*samplesonplate)]]=1\r\n\r\nnb_plate=ceiling(dim(data6)[1]/samplesonplate)\r\nii=2 # plate counter\r\nkk=1 # year counter\r\nwhile(ii<(nb_plate+1))\r\n{\r\n  # year t\r\n  index=which(data6$year==yearz[kk]&is.na(data6$plate)==T)\r\n  data6$plate[index[1:((1/4)*samplesonplate)]]=ii\r\n  # year t +1\r\n  index=which(data6$year==yearz[kk+1]&is.na(data6$plate)==T)\r\n  data6$plate[index[1:((1/2)*samplesonplate)]]=ii\r\n  # year t +2\r\n  index=which(data6$year==yearz[kk+2]&is.na(data6$plate)==T)\r\n  data6$plate[index[1:((1/4)*samplesonplate)]]=ii\r\n  \r\n  # if plate is full no need to move a year up. If plate is not full the first year was complete (or second or third) and hence year needs to move\r\n  \r\n  # AND need to add more samples to plate\r\n  while((length(which(data6$plate==ii))<samplesonplate)==T)\r\n  {\r\n    if(length(which(is.na(data6$plate)==T))<samplesonplate)  # check whether arrived at end of list of samples, if the case put all samples on last plate\r\n    {\r\n      data6$plate[which(is.na(data6$plate)==T)]=ii\r\n      break # toendloop\r\n      \r\n    } else {\r\n    \r\n    # add remaining samples of current year\r\n    index=which(data6$year==yearz[kk]&is.na(data6$plate)==T)\r\n    data6$plate[index[1:(samplesonplate-(length(which(data6$plate==ii))))]]=ii   \r\n    \r\n    # move year up if plate is not full  \r\n    # if not full yet add year\r\n    if((length(which(data6$plate==ii))<samplesonplate)==T)\r\n    {\r\n            kk=kk+1\r\n            index=which(data6$year==yearz[kk]&is.na(data6$plate)==T)\r\n            data6$plate[index[1:(samplesonplate-(length(which(data6$plate==ii))))]]=ii\r\n    }\r\n    \r\n        }\r\n  }\r\n  \r\nii=ii+1  \r\n}\r\n\r\n#add the plate error to each plate\r\nk=1\r\nwhile(k<(plates+1))\r\n{\r\n  data6$TL[which(data6$plate==k)]=data6$TL[which(data6$plate==k)]+plate_errordraw[k]\r\n  k=k+1\r\n}\r\n\r\n# dataset complete and we can analyse now using a mixed model\r\nfit4<-lmer(TL~(1|id)+(1|plate)+avgage+deltaage,data=data6)\r\nfit5<-lmer(TL~(1|id)+(1|plate)+avgage+deltaage+factor(cohort),data=data6)\r\nsummary(fit5)\r\nanova_cohort3=anova(fit4,fit5)\r\n# put parameters of fit1 into the result matrixs of the simulation\r\nresults_slicing[q,]=c(data.matrix(fixef(fit5))[1:3],sqrt(diag(vcov(fit5)))[1:3],anova_cohort3[2,8])\r\n\r\nq=q+1\r\n#print()\r\nprint(samplesonplate)\r\nprint(q)\r\n}\r\n\r\n# The output from the simulations into output file\r\nresultstotal[(i-1)*length(samplesonplate2)+p,]<-cbind(mean((results_sameplate[,2])/(results_sameplate[,5])),\r\nmean((results_distributed[,2])/(results_distributed[,5])),\r\nmean((results_slicing[,2])/(results_slicing[,5])),\r\nmean(((results_sameplate[,2])/(results_sameplate[,5]))-((results_distributed[,2])/(results_distributed[,5]))),\r\nmean(((results_slicing[,2])/(results_slicing[,5]))-((results_sameplate[,2])/(results_sameplate[,5]))),\r\nmean(((results_slicing[,2])/(results_slicing[,5]))-((results_distributed[,2])/(results_distributed[,5]))),\r\nmean(((results_sameplate[,2])/(results_sameplate[,5]))/((results_distributed[,2])/(results_distributed[,5]))),\r\nmean(((results_slicing[,2])/(results_slicing[,5]))/((results_sameplate[,2])/(results_sameplate[,5]))),\r\nmean(((results_slicing[,2])/(results_slicing[,5]))/((results_distributed[,2])/(results_distributed[,5]))),\r\nmean((results_sameplate[,3])/(results_sameplate[,6])),\r\nmean((results_distributed[,3])/(results_distributed[,6])),\r\nmean((results_slicing[,3])/(results_slicing[,6])),\r\nmean(((results_sameplate[,3])/(results_sameplate[,6]))-((results_distributed[,3])/(results_distributed[,6]))),\r\nmean(((results_slicing[,3])/(results_slicing[,6]))-((results_sameplate[,3])/(results_sameplate[,6]))),\r\nmean(((results_slicing[,3])/(results_slicing[,6]))-((results_distributed[,3])/(results_distributed[,6]))),\r\nmean(((results_sameplate[,3])/(results_sameplate[,6]))/((results_distributed[,3])/(results_distributed[,6]))),\r\nmean(((results_slicing[,3])/(results_slicing[,6]))/((results_sameplate[,3])/(results_sameplate[,6]))),\r\nmean(((results_slicing[,3])/(results_slicing[,6]))/((results_distributed[,3])/(results_distributed[,6]))),\r\nerrorplates,\r\nsamplesonplate,\r\nvar((results_sameplate[,2])/(results_sameplate[,5])),\r\nvar((results_distributed[,2])/(results_distributed[,5])),\r\nvar((results_slicing[,2])/(results_slicing[,5])),\r\nvar((results_sameplate[,3])/(results_sameplate[,6])),\r\nvar((results_distributed[,3])/(results_distributed[,6])),\r\nvar((results_slicing[,3])/(results_slicing[,6])),\r\n# Determining the number of times the null hpyothesis is rejected\r\nlength(which((results_sameplate[,3])/(results_sameplate[,6])<=-2)),\r\nlength(which((results_distributed[,3])/(results_distributed[,6])<=-2)),\r\nlength(which((results_slicing[,3])/(results_slicing[,6])<=-2)),\r\nlength(which((results_sameplate[,2])/(results_sameplate[,5])>=2)),\r\nlength(which((results_distributed[,2])/(results_distributed[,5])>=2)),\r\nlength(which((results_slicing[,2])/(results_slicing[,5])>=2)),\r\nlength(which(results_sameplate[,7]<0.05)),\r\nlength(which(results_distributed[,7]<0.05)),\r\nlength(which(results_slicing[,7]<0.05)),\r\n# Adding in precision output (using 75% and 25% quantile to determine width of distribution)\r\n(quantile(results_sameplate[,3],0.75)-quantile(results_sameplate[,3],0.25))/(median(results_sameplate[,3])),\r\n(quantile(results_distributed[,3],0.75)-quantile(results_distributed[,3],0.25))/(median(results_distributed[,3])),\r\n(quantile(results_slicing[,3],0.75)-quantile(results_slicing[,3],0.25))/(median(results_slicing[,3])),\r\n(quantile(results_sameplate[,2],0.75)-quantile(results_sameplate[,2],0.25))/(median(results_sameplate[,2])),\r\n(quantile(results_distributed[,2],0.75)-quantile(results_distributed[,2],0.25))/(median(results_distributed[,2])),\r\n(quantile(results_slicing[,2],0.75)-quantile(results_slicing[,2],0.25))/(median(results_slicing[,2])),\r\n(quantile(results_sameplate[,7],0.75)-quantile(results_sameplate[,7],0.25))/(median(results_sameplate[,7])),\r\n(quantile(results_distributed[,7],0.75)-quantile(results_distributed[,7],0.25))/(median(results_distributed[,7])),\r\n(quantile(results_slicing[,7],0.75)-quantile(results_slicing[,7],0.25))/(median(results_slicing[,7])),\r\nmedian(results_sameplate[,3]),\r\nmedian(results_sameplate[,2]),\r\nmedian(results_sameplate[,7]),\r\nmedian(results_distributed[,3]),\r\nmedian(results_distributed[,2]),\r\nmedian(results_distributed[,7]),\r\nmedian(results_slicing[,3]),\r\nmedian(results_slicing[,2]),\r\nmedian(results_slicing[,7]),\r\nmean(results_sameplate[,3]),\r\nmean(results_sameplate[,2]),\r\nmean(results_distributed[,3]),\r\nmean(results_distributed[,2]),\r\nmean(results_slicing[,3]),\r\nmean(results_slicing[,2]),\r\nmin(results_sameplate[,3]),\r\nmin(results_sameplate[,2]),\r\nmin(results_distributed[,3]),\r\nmin(results_distributed[,2]),\r\nmin(results_slicing[,3]),\r\nmin(results_slicing[,2]),\r\nmax(results_sameplate[,3]),\r\nmax(results_sameplate[,2]),\r\nmax(results_distributed[,3]),\r\nmax(results_distributed[,2]),\r\nmax(results_slicing[,3]),\r\nmax(results_slicing[,2]))\r\n\r\np=p+1\r\nprint(p)\r\n}\r\n\r\ni=i+1\r\nprint(i)\r\n}\r\nwrite.csv(resultstotal,""Output simulations cohort effect.csv"")\r\n', '###########################################################################################\r\n# Slicing: a sustainable approach to structuring samples for analysis in long-term studies\r\n# Original author: Mirre JP Simons (University of Sheffield)\r\n# Edited by: Sil HJ van Lieshout (University of Leeds)\r\n# Last updated: 04/11/2019\r\n###########################################################################################\r\nrm(list=ls())\r\n\r\n#install.packages(""lme4"")\r\n#install.packages(""reshape"")\r\n#install.packages(""lmerTest"")\r\nlibrary(lme4)\r\nlibrary(reshape)\r\nlibrary(lmerTest)\r\n###########################################################################################\r\nindividuals=200 # Becomes 400 in simulations with increased sample size\r\nSDd=1\r\nshortening=0.06*SDd \r\nmortalityrisk=0.25\r\nslopewithTL=-0.23 \r\nyears=5 \r\nmeasurementerror=1\r\nerrorplates2=c(1,2.5,5,10,20,40) # batch attributable error\r\nsamplesonplate2=c(12,24,36,48) # number of samples per batch\r\nruns=5000\r\n\r\n# fixed effects\r\ncohort_n=10 # number of cohorts\r\n#cohort_fixed=0.9 # this is a fraction of the cohort effect (drawn from uniform distribution see below) - in paper also changed to 0.45 and 1.35 to test variation in this parameter\r\n\r\n# Add year error\r\nyear_fixed = 0.7 # this is a fraction of the year (drawn from uniform distribution see below) - in paper also changed to 0.35 and 1.05 to test variation in this parameter\r\n\r\nresultstotal=matrix(NA,ncol=71,nrow=length(errorplates2)*length(samplesonplate2))\r\n\r\n# start loops\r\ni=1\r\nwhile(i<(length(errorplates2)+1))\r\n{\r\n# set parameter values as above\t\r\nerrorplates=errorplates2[i]\r\np=1\r\nwhile(p<(length(samplesonplate2))+1)\r\n{\r\n# set parameter values as above\r\nsamplesonplate=samplesonplate2[p]\r\n\r\n# make empty matrices to put results of simulation in\r\nresults_distributed=matrix(nrow=runs,ncol=7)          \r\nresults_sameplate=matrix(nrow=runs,ncol=7)\r\nresults_slicing=matrix(nrow=runs,ncol=7)\r\n\r\nq=1\r\nwhile(q<(runs+1))\r\n{\r\n\r\n# average TL distribution of individuals (at birth)\r\nTL=rnorm(individuals,mean=0,sd=SDd)\r\n\r\n### add cohort info, this section is not used when negating cohort effects\r\n#cohortfix=cohort_fixed*runif(cohort_n)\r\nindivpercohort=round((length(TL)/cohort_n))\r\n#add cohort effect\r\n#z=1\r\n#while(z<(cohort_n+1))\r\n#{\r\n#  TL[(((z-1)*indivpercohort)+1):(z*indivpercohort)]=TL[(((z-1)*indivpercohort)+1):(z*indivpercohort)]+cohortfix[z]\r\n  #z=z+1\r\n#}\r\n\r\n### mortality induced by TL, using TL in early life as long term predictor, note shortening rate is constant in life thus using the first TL in early life.\r\nalive<-matrix(1,nrow=individuals,ncol=years+1)\r\nk=2\r\n# loop through each year of the study with all individuals alive at year 0\r\nwhile(k<(years+2))\r\n{\r\n  # draw a number from a uniform distribution for each individual \r\n  # and compare this to the mortality risk of that individual, \r\n  # which can be deterimned by TL, as by parameter slopewithTL\r\n  # mortality follows an exponential relationship with TL, minus TL means less risk to die with increasing TL.\r\n  # this is the chance to survive to the next year\r\n  \r\n  alive[which(runif(individuals)<mortalityrisk*exp(slopewithTL*TL)),k]=0\t\t\r\n\t\r\n  # when risk is incurred, keep a zero, do not ressurect from the dead..\r\n  alive[,k]=alive[,k-1]*alive[,k]\r\n\t\r\n\tk=k+1\r\n}\r\n\r\n### generate TL dataset to fit mixed effect model on ###\r\n\r\n# make empty matrix to put results in\r\nTLset=matrix(NA,nrow=individuals,ncol=years+1)\r\n\r\n# add measurement error to true TL, note that above true TL is used, because this is the biologically (""true"") relevant component ""causing death""\r\nTLset[,1]=TL+rnorm(individuals,sd=measurementerror)\r\n\r\n# add measurement error to each additional year and add shortening (which is a set value and linear)\r\nk=1\r\nwhile(k<(years+1))\r\n{\r\n  TLset[,k+1]=TL+rnorm(individuals,sd=measurementerror)-shortening*k\r\n  k=k+1\r\n}\r\n\r\n#\'kill\' individuals that died using alive matrix generated above\r\nTLset[which(alive==0)]=NA\r\n\r\n# assign an ID variable for each row in the matrix, which is one individual\r\nid=1:individuals\r\n# cohort\r\ncohortid=rep(1:cohort_n, each=indivpercohort)\r\n# bind individual ID together with dataframe\r\nTLset<-as.data.frame(cbind(id,cohortid,TLset))\r\n\r\n# name the dataframe\r\nnames(TLset)[3:(years+3)]=1:(years+1)                 \r\nnames(TLset)[2]=\'cohort\'\r\n# reshape the dataset into a long format\r\nlibrary(reshape)\r\nTLsetlong<-reshape(TLset,varying=list(names(TLset)[3:(years+3)]),idvar=c(""id""),direction=c(""long""))\r\nnames(TLsetlong)[4]=""TL""\r\n# delete NAs\r\nTLsetlong=TLsetlong[which(is.na(TLsetlong$TL)==F),]\r\n\r\n# generate delta and average age\r\n# mean age\r\navgageID<-aggregate(TLsetlong,list(TLsetlong$id),mean)[,c(2,4)]\r\n# merge avg age to the TL set\r\ndata2<-merge(TLsetlong,avgageID,by=""id"")\r\n# name the columns\r\ncolnames(data2)[2]<-c(""cohort"")\r\ncolnames(data2)[3]<-c(""age"")\r\ncolnames(data2)[5]<-c(""avgage"")\r\n# add deltaage\r\ndata3<-cbind(data2,data2$age-data2$avgage)\r\ncolnames(data3)[6]=c(""deltaage"")\r\n\r\n# Add year to dataframe\r\ndata3$year <- data3$cohort + data3$age - 1\r\ndata3$year <- as.factor(data3$year)\r\n\r\nyears_n <- length(unique(data3$year))\r\n\r\nyear_errordraw=year_fixed*runif(years_n)\r\n# add year specific error to each year\r\ng=1\r\nwhile(g<(years_n+1))\r\n{\r\n  data3$TL[which(data3$year==g)]=data3$TL[which(data3$year==g)]+year_errordraw[g]\r\n  g=g+1\r\n}\r\n\r\n############################################################################################\r\n# these lines can be changed to change different confounding variables, i.e. sort by cohort, sort by year..\r\n############################################################################################\r\n\r\n# To make sure the data is  organised by id, so plates are sequential by ID \r\ndata3<-data3[order(data3$id) ,]\r\n\r\n###########################################################################################\r\n# these lines can be changed to change different confounding variables, i.e. sort by cohort, sort by year..\r\n############################################################################################\r\n\r\n\r\n### plates and error of plates\r\n# how many sample do we have\r\nsamples=dim(data3)[1]\r\n# how many plates will we have depending on samples and samplesonplate\r\nplates=round(samples/samplesonplate)\r\n\r\n# rep maxes a string of plate numbers, [1:samples] is neccesary to index the repeated string to be able to bind it together nicely\r\ndata4=cbind(rep(1:plates, each=samplesonplate)[1:samples],data3)\r\n# add error per plate\r\nplate_errordraw=rnorm(plates,sd=errorplates)\r\nnames(data4)[1]=""plate""\r\n\r\n# add plate specific error to each plate\r\nk=1\r\nwhile(k<(plates+1))\r\n{\r\n\tdata4$TL[which(data4$plate==k)]=data4$TL[which(data4$plate==k)]+plate_errordraw[k]\r\n\tk=k+1\r\n}\r\n\r\n# dataset complete and we can analyse now using a mixed model\r\nfit0<-lmer(TL~(1|id)+(1|plate)+avgage+deltaage,data=data4)\r\nfit1<-lmer(TL~(1|id)+(1|plate)+avgage+deltaage+factor(year),data=data4)\r\nsummary(fit1)\r\nanova_cohort=anova(fit0,fit1)\r\n# put parameters of fit1 into the result matrixs of the simulation\r\nresults_sameplate[q,]=c(data.matrix(fixef(fit1))[1:3],sqrt(diag(vcov(fit1)))[1:3],anova_cohort[2,8])\r\n\r\n\r\n# fit the model but now as if samples where distributed randomly across plates.\r\ndata3=data3[sample(nrow(data3)),]  # to randomise all samples, not plates to easily keep the error, note this is the dataset generated above where no plate specific error had been added yet.\r\nsamples=dim(data3)[1]\r\nplates=round(samples/samplesonplate)\r\ndata5=cbind(rep(1:plates, each=samplesonplate)[1:samples],data3)\r\nnames(data5)[1]=""plate""\r\n\r\nk=1\r\nwhile(k<(plates+1))\r\n{\r\n\tdata5$TL[which(data5$plate==k)]=data5$TL[which(data5$plate==k)]+plate_errordraw[k]\r\n\tk=k+1\r\n}\r\n\r\n# dataset complete and we can analyse now using a mixed model\r\nfit2<-lmer(TL~(1|id)+(1|plate)+avgage+deltaage,data=data5)\r\nfit3<-lmer(TL~(1|id)+(1|plate)+avgage+deltaage+factor(year),data=data5)\r\nsummary(fit3)\r\nanova_cohort2=anova(fit2,fit3)\r\n# put parameters of fit1 into the result matrixs of the simulation\r\nresults_distributed[q,]=c(data.matrix(fixef(fit3))[1:3],sqrt(diag(vcov(fit3)))[1:3],anova_cohort2[2,8])\r\n\r\n\r\n############### slicing ############\r\n# slicing across three years; note sometimes little bits left of one year are added to another plate and hence sometimes spread across 4\r\n\r\n# Make data ordered by year\r\ndata3<-data3[order(data3$year) ,]\r\n\r\ndata6=data3\r\n#data6$year=data6$cohort+data6$age\r\nyearz=unique(data6$year)[order(unique(data6$year))] # order important here as samples are sliced across subsequent years.\r\ndata6$plate=NA\r\n\r\n# first plate\r\nii=1\r\nindex=which(data6$year==yearz[ii]&is.na(data6$plate)==T)\r\ndata6$plate[index[1:((3/4)*samplesonplate)]]=1\r\n## this could give error if number of samples on plate is bigger than one year ##\r\nindex=which(data6$year==yearz[ii+1]&is.na(data6$plate)==T)\r\ndata6$plate[index[1:((1/4)*samplesonplate)]]=1\r\n\r\nnb_plate=ceiling(dim(data6)[1]/samplesonplate)\r\nii=2 # plate counter\r\nkk=1 # year counter\r\nwhile(ii<(nb_plate+1))\r\n{\r\n  # year t\r\n  index=which(data6$year==yearz[kk]&is.na(data6$plate)==T)\r\n  data6$plate[index[1:((1/4)*samplesonplate)]]=ii\r\n  # year t +1\r\n  index=which(data6$year==yearz[kk+1]&is.na(data6$plate)==T)\r\n  data6$plate[index[1:((1/2)*samplesonplate)]]=ii\r\n  # year t +2\r\n  index=which(data6$year==yearz[kk+2]&is.na(data6$plate)==T)\r\n  data6$plate[index[1:((1/4)*samplesonplate)]]=ii\r\n  \r\n  # if plate is full no need to move a year up. If plate is not full the first year was complete (or second or third) and hence year needs to move\r\n  \r\n  # AND need to add more samples to plate\r\n  while((length(which(data6$plate==ii))<samplesonplate)==T)\r\n  {\r\n    if(length(which(is.na(data6$plate)==T))<samplesonplate)  # check whether arrived at end of list of samples, if the case put all samples on last plate\r\n    {\r\n      data6$plate[which(is.na(data6$plate)==T)]=ii\r\n      break # toendloop\r\n      \r\n    } else {\r\n    \r\n    # add remaining samples of current year\r\n    index=which(data6$year==yearz[kk]&is.na(data6$plate)==T)\r\n    data6$plate[index[1:(samplesonplate-(length(which(data6$plate==ii))))]]=ii   \r\n    \r\n    # move year up if plate is not full  \r\n    # if not full yet add year\r\n    if((length(which(data6$plate==ii))<samplesonplate)==T)\r\n    {\r\n            kk=kk+1\r\n            index=which(data6$year==yearz[kk]&is.na(data6$plate)==T)\r\n            data6$plate[index[1:(samplesonplate-(length(which(data6$plate==ii))))]]=ii\r\n    }\r\n    \r\n        }\r\n  }\r\n  \r\nii=ii+1  \r\n}\r\n\r\n#add the plate error to each plate\r\nk=1\r\nwhile(k<(plates+1))\r\n{\r\n  data6$TL[which(data6$plate==k)]=data6$TL[which(data6$plate==k)]+plate_errordraw[k]\r\n  k=k+1\r\n}\r\n\r\n# dataset complete and we can analyse now using a mixed model\r\nfit4<-lmer(TL~(1|id)+(1|plate)+avgage+deltaage,data=data6)\r\nfit5<-lmer(TL~(1|id)+(1|plate)+avgage+deltaage+factor(year),data=data6)\r\nsummary(fit5)\r\nanova_cohort3=anova(fit4,fit5)\r\n# put parameters of fit1 into the result matrixs of the simulation\r\nresults_slicing[q,]=c(data.matrix(fixef(fit5))[1:3],sqrt(diag(vcov(fit5)))[1:3],anova_cohort3[2,8])\r\n\r\nq=q+1\r\n#print()\r\nprint(samplesonplate)\r\nprint(q)\r\n}\r\n\r\n# The output from the simulations into output file\r\nresultstotal[(i-1)*length(samplesonplate2)+p,]<-cbind(mean((results_sameplate[,2])/(results_sameplate[,5])),\r\nmean((results_distributed[,2])/(results_distributed[,5])),\r\nmean((results_slicing[,2])/(results_slicing[,5])),\r\nmean(((results_sameplate[,2])/(results_sameplate[,5]))-((results_distributed[,2])/(results_distributed[,5]))),\r\nmean(((results_slicing[,2])/(results_slicing[,5]))-((results_sameplate[,2])/(results_sameplate[,5]))),\r\nmean(((results_slicing[,2])/(results_slicing[,5]))-((results_distributed[,2])/(results_distributed[,5]))),\r\nmean(((results_sameplate[,2])/(results_sameplate[,5]))/((results_distributed[,2])/(results_distributed[,5]))),\r\nmean(((results_slicing[,2])/(results_slicing[,5]))/((results_sameplate[,2])/(results_sameplate[,5]))),\r\nmean(((results_slicing[,2])/(results_slicing[,5]))/((results_distributed[,2])/(results_distributed[,5]))),\r\nmean((results_sameplate[,3])/(results_sameplate[,6])),\r\nmean((results_distributed[,3])/(results_distributed[,6])),\r\nmean((results_slicing[,3])/(results_slicing[,6])),\r\nmean(((results_sameplate[,3])/(results_sameplate[,6]))-((results_distributed[,3])/(results_distributed[,6]))),\r\nmean(((results_slicing[,3])/(results_slicing[,6]))-((results_sameplate[,3])/(results_sameplate[,6]))),\r\nmean(((results_slicing[,3])/(results_slicing[,6]))-((results_distributed[,3])/(results_distributed[,6]))),\r\nmean(((results_sameplate[,3])/(results_sameplate[,6]))/((results_distributed[,3])/(results_distributed[,6]))),\r\nmean(((results_slicing[,3])/(results_slicing[,6]))/((results_sameplate[,3])/(results_sameplate[,6]))),\r\nmean(((results_slicing[,3])/(results_slicing[,6]))/((results_distributed[,3])/(results_distributed[,6]))),\r\nerrorplates,\r\nsamplesonplate,\r\nvar((results_sameplate[,2])/(results_sameplate[,5])),\r\nvar((results_distributed[,2])/(results_distributed[,5])),\r\nvar((results_slicing[,2])/(results_slicing[,5])),\r\nvar((results_sameplate[,3])/(results_sameplate[,6])),\r\nvar((results_distributed[,3])/(results_distributed[,6])),\r\nvar((results_slicing[,3])/(results_slicing[,6])),\r\n# Determining the number of times the null hpyothesis is rejected\r\nlength(which((results_sameplate[,3])/(results_sameplate[,6])<=-2)),\r\nlength(which((results_distributed[,3])/(results_distributed[,6])<=-2)),\r\nlength(which((results_slicing[,3])/(results_slicing[,6])<=-2)),\r\nlength(which((results_sameplate[,2])/(results_sameplate[,5])>=2)),\r\nlength(which((results_distributed[,2])/(results_distributed[,5])>=2)),\r\nlength(which((results_slicing[,2])/(results_slicing[,5])>=2)),\r\nlength(which(results_sameplate[,7]<0.05)),\r\nlength(which(results_distributed[,7]<0.05)),\r\nlength(which(results_slicing[,7]<0.05)),\r\n# Adding in precision output (using 75% and 25% quantile to determine width of distribution)\r\n(quantile(results_sameplate[,3],0.75)-quantile(results_sameplate[,3],0.25))/(median(results_sameplate[,3])),\r\n(quantile(results_distributed[,3],0.75)-quantile(results_distributed[,3],0.25))/(median(results_distributed[,3])),\r\n(quantile(results_slicing[,3],0.75)-quantile(results_slicing[,3],0.25))/(median(results_slicing[,3])),\r\n(quantile(results_sameplate[,2],0.75)-quantile(results_sameplate[,2],0.25))/(median(results_sameplate[,2])),\r\n(quantile(results_distributed[,2],0.75)-quantile(results_distributed[,2],0.25))/(median(results_distributed[,2])),\r\n(quantile(results_slicing[,2],0.75)-quantile(results_slicing[,2],0.25))/(median(results_slicing[,2])),\r\n(quantile(results_sameplate[,7],0.75)-quantile(results_sameplate[,7],0.25))/(median(results_sameplate[,7])),\r\n(quantile(results_distributed[,7],0.75)-quantile(results_distributed[,7],0.25))/(median(results_distributed[,7])),\r\n(quantile(results_slicing[,7],0.75)-quantile(results_slicing[,7],0.25))/(median(results_slicing[,7])),\r\nmedian(results_sameplate[,3]),\r\nmedian(results_sameplate[,2]),\r\nmedian(results_sameplate[,7]),\r\nmedian(results_distributed[,3]),\r\nmedian(results_distributed[,2]),\r\nmedian(results_distributed[,7]),\r\nmedian(results_slicing[,3]),\r\nmedian(results_slicing[,2]),\r\nmedian(results_slicing[,7]),\r\nmean(results_sameplate[,3]),\r\nmean(results_sameplate[,2]),\r\nmean(results_distributed[,3]),\r\nmean(results_distributed[,2]),\r\nmean(results_slicing[,3]),\r\nmean(results_slicing[,2]),\r\nmin(results_sameplate[,3]),\r\nmin(results_sameplate[,2]),\r\nmin(results_distributed[,3]),\r\nmin(results_distributed[,2]),\r\nmin(results_slicing[,3]),\r\nmin(results_slicing[,2]),\r\nmax(results_sameplate[,3]),\r\nmax(results_sameplate[,2]),\r\nmax(results_distributed[,3]),\r\nmax(results_distributed[,2]),\r\nmax(results_slicing[,3]),\r\nmax(results_slicing[,2]))\r\n\r\np=p+1\r\nprint(p)\r\n}\r\n\r\ni=i+1\r\nprint(i)\r\n}\r\nwrite.csv(resultstotal,""Output simulations year effect.csv"")\r\n']","Slicing: a sustainable approach to structuring samples for analysis in long-term studies The longitudinal study of populations is a core tool for understanding ecological and evolutionary processes. Long-term studies typically collect samples repeatedly over individual lifetimes and across generations. These samples are then analysed in batches (e.g. qPCR-plates) and clusters (i.e. group of batches) over time in the laboratory. However, these analyses are constrained by cross-classified data structures introduced biologically or through experimental design. The separation of biological variation from the confounding among-batch and among-cluster variation is crucial, yet often ignored.The commonly used approaches to structuring samples for analysis, sequential and randomisation, generate bias due to the non-independence between time of collection and the batch and cluster they are analysed in. We propose a new sample structuring strategy, called slicing, designed to separate confounding among-batch and among-cluster variation from biological variation. Through simulations we tested the statistical power and precision to detect within-individual, between-individual, year and cohort effects of this novel approach.Our slicing approach, whereby recently and previously collected samples are sequentially analysed in clusters together, enables the statistical separation of collection time and cluster effects by bridging clusters together, for which we provide a case study. Our simulations show, with reasonable slicing width and angle, similar precision and similar or greater statistical power to detect year, cohort, within- and between-individual effects when samples are sliced across batches, compared with strategies that aggregate longitudinal samples or use randomised allocation.While the best approach to analysing long-term datasets depends on the structure of the data and questions of interest, it is vital to account for confounding among-cluster and batch variation. Our slicing approach is simple to apply and creates the necessary statistical independence of batch and cluster from environmental or biological variables of interest. Crucially, it allows sequential analysis of samples and flexible inclusion of current data in later analyses without completely confounding the analysis. Our approach maximises the scientific value of every sample, as each will optimally contribute to unbiased statistical inference from the data. Slicing thereby maximises the power of growing biobanks to address important ecological, epidemiological and evolutionary questions.",3
Effects of Sinusoidal Vibrations on the Motion Response of Honeybees - Exemplary software,"# Motion Analysis of HoneybeesCode pieces for motion analysis of honey bees. For full reproducibility we include in this repository our collected data sets and code pieces. R session info is saved in the root as [SessionInfo.txt](SessionInfo.txt).For more details please refer to Stefanec, M., Oberreiter, H., Becher, M. A., Haase, G., & Schmickl, T. (2021). Effects of Sinusoidal Vibrations on the Motion Response of Honeybees. Frontiers in Physics, 9 - Script to calculate and extract PMI from movie recording, utilizating cv2 - R scripts to generate plots and statistical testsPlease also refer to https://github.com/martin-st/motion-analysis-of-honeybees ## Statistical AnalysisResults of pairwise of frequencies can be found in [pairwise_wilcox_holm.csv](pairwise_wilcox_holm.csv).## MIT LicenceCopyright (c) 2021 Martin Stefanec & Hannes Oberreiter","['# Libraries ---------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(here)\n\n# Constants\ncbb_Palette <- c(""#000000"", ""#E69F00"", ""#56B4E9"", ""#009E73"", ""#F0E442"", ""#0072B2"", ""#D55E00"", ""#CC79A7"")\n\n# Read Data ---------------------------------------------------------------\n# Read data and drop_na as cleanup, if we have empty rows appended by accident\ndf <- read_csv(""./datasets/amplitude_experiments.csv"") %>%\n  drop_na()\n\n# Normalize ---------------------------------------------------------------\nsource(paste0(here(), ""/data_analysis/utils.R""))\n\n# Plots -------------------------------------------------------------------\n\n# Control Plot\nplot_amp_control <- df %>%\n  filter(amplitude == 0) %>%\n  ggplot(aes(x = frequency_factor, y = norm_activity, color = dateTime)) +\n  geom_boxplot() +\n  ggtitle(""Control and Frequency at 0 Amplitude after Normalize"") +\n  ylab(""Normalized Activity"") + xlab(""Frequency"") +\n  scale_color_hue(l=40) +\n  theme_classic()\n\nggsave(""output/plot_amp_control.pdf"", width = 10, plot_amp_control)\n\n# Paper Plot\np1 <- df %>% \n  filter(frequency_factor != ""Control"") %>% \n  group_by(frequency_factor, amplitude) %>% \n  summarize(\n    mean = mean(norm_activity),\n    upper = mean + sd(norm_activity),\n    lower = mean - sd(norm_activity)\n  ) %>%\n  ggplot(aes(x = amplitude, y = mean, color = frequency_factor)) +\n  geom_hline(aes(yintercept = 1), color = ""black"", alpha = 0.5, show.legend = F) +\n  geom_pointrange(aes(ymin = lower, ymax = upper)) +\n  ylab(""Relative Pixel-Value Change"") +\n  xlab(""Amplitude"") +\n  labs(color = ""Frequency"") +\n  scale_color_manual(values = cbb_Palette) +\n  geom_line() +\n  scale_x_continuous(breaks = c(unique(df$amplitude))) +\n  theme_classic()\n\nggsave(paste0(""./data_analysis/output/"", ""amp_p1_lines.pdf""), width = 11, height = 5, p1)\n', '# Libs ----------------------------------------------------\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(here)\nlibrary(multcompView)\n# Following packages were not used for the paper\nlibrary(ggsignif)\nlibrary(coin)\nlibrary(broom)\n\n# colorblind friendly colours\ncbb_Palette <- c(""#000000"", ""#E69F00"", ""#56B4E9"", ""#009E73"", ""#F0E442"", ""#0072B2"", ""#D55E00"", ""#CC79A7"")\n\n# Read Data ---------------------------------------------------------------\n# Read data and drop_na as cleanup, if we have empty rows appended by accident\ndf <- read_csv(""./datasets/frequency_experiments.csv"") %>%\n  drop_na()\n\n# Activity Cols Description -----------------------------------------------\n# activity: four white points\n# new_activity_calculation: smaller ROI, symmetrical around the emitter\n# activity_calculation_frame: ROI over whole frame\n\n# Normalize ---------------------------------------------------------------\nsource(paste0(here(), ""/data_analysis/utils.R""))\n\n# Plotting Loop of Different ROIs -----------------------------------------------------------\n# in the final dataset we only have one ROI left\n# our different roi techniques\nrois <- c(""norm_activity"")\n\nfor(i in seq_along(rois)){\n  roi <- rois[i]\n  # col y will include our normalized value for given roi\n  df <- bind_cols(df, df[,roi] %>% rename(., ""y"" = roi))\n  \n  # dateTime and each ""block"" with sd\n  plot1 <- df %>% \n    group_by(date, dateTime, frequency_factor, subgroup) %>% \n    summarise(\n      m = mean(y),\n      sd = sd(y)\n    ) %>% \n    ggplot(aes(frequency_factor, m, color = dateTime)) +\n    geom_hline(aes(yintercept = 1), color = ""black"", alpha = 0.5) +\n    geom_pointrange(aes(ymin = m - sd, ymax = m + sd)) +\n    scale_color_hue(l = 40) +\n    ggtitle(paste(""Roi:"", roi), subtitle = ""Block Mean + SD"") +\n    ylab(""Normalized Activity"") + xlab(""Frequency"") +\n    theme_classic() +\n    theme(\n      axis.text.x = element_text(angle = 65, vjust = 0.5)\n    )\n  \n  plot2 <- df %>%\n    group_by(date, frequency_factor) %>%\n    summarise(\n      m = mean(y),\n      sd = sd(y)\n    ) %>%\n    ggplot(aes(frequency_factor, m, color = date)) +\n    geom_hline(aes(yintercept = 1), color = ""black"", alpha = 0.5)+\n    geom_pointrange(aes(ymin = m - sd, ymax = m + sd)) +\n    scale_color_manual(values = cbb_Palette) +\n    ggtitle(paste(""Roi:"", roi), subtitle = ""Day Mean + SD"") +\n    ylab(""Normalized Activity"") + xlab(""Frequency"") +\n    theme_classic() +\n    theme(\n      axis.text.x = element_text(angle = 65, vjust = 0.5)\n    )\n\n  plot3 <- df %>%\n    ggplot(aes(frequency_factor, y, color = date)) +\n    geom_hline(aes(yintercept = 1), color = ""black"", alpha = 0.5) +\n    geom_point(alpha = 0.7) +\n    scale_color_manual(values = cbb_Palette) +\n    ggtitle(paste(""Roi:"", roi), subtitle = ""All Datapoints colored by given date"") +\n    ylab(""Normalized Activity"") + xlab(""Frequency"") +\n    theme_classic() +\n    theme(\n      axis.text.x = element_text(angle = 65, vjust = 0.5)\n    )\n\n  ggsave(paste0(""./data_analysis/output/"", roi,""_plot1.pdf""), width = 10, plot1) \n  ggsave(paste0(""./data_analysis/output/"", roi,""_plot2.pdf""), width = 10, plot2) \n  ggsave(paste0(""./data_analysis/output/"", roi,""_plot3.pdf""), width = 10, plot3) \n\n  # drop y for next round\n  df <- df %>% select(-""y"")\n}\n\n# Plots Points -------------------------------------------------------------\n# Drop 440 Frequency, did not work\np1_df <- df %>%\n  filter(frequency_factor != ""WN"" & frequency_factor != ""440"") %>%\n  group_by(frequency) %>%\n  summarize(\n    mean = mean(norm_activity),\n    upper = mean + sd(norm_activity),\n    lower = mean - sd(norm_activity)\n  ) %>%\n  mutate(\n    colorb = ifelse(mean > 1, ""A"", ""B""),\n    colorb = ifelse(mean == 1, ""1"", colorb),\n    frequency = ifelse(frequency == -2, 0, frequency)\n    )\n\np1_df_group <- p1_df %>% \n  mutate(\n    colorb = ifelse(frequency >= 500, ""C"", colorb),\n    colorb = fct_recode(colorb, Control = ""1"", A = ""A"", B = ""B"")\n  ) %>% \n  group_by(colorb) %>% \n  summarise(\n    xmin = min(frequency),\n    xmax = max(frequency)\n  ) %>% \n  filter(colorb != ""Control"")\n\np1 <- p1_df %>% \n  ggplot(aes(x = frequency, y = mean, color = colorb)) +\n  geom_hline(\n    aes(yintercept = 1), color = ""black"", alpha = 0.5, show.legend = F\n    ) +\n  geom_line(\n    aes(color = NA), color = ""black"", show.legend = F\n    ) +\n  #geom_function(fun=f, colour=""red"") +\n  geom_rect(\n    data = p1_df_group,\n    aes(\n      xmin = xmin, xmax = xmax, \n      ymin = -Inf, ymax = Inf, fill = colorb, color = NA),\n    alpha = 0.2, show.legend = T, inherit.aes = FALSE\n    ) +\n  geom_pointrange(\n    aes(ymin = lower, ymax = upper), show.legend = F\n    ) +\n  scale_color_manual(values = cbb_Palette, na.translate = F) +\n  guides(color = FALSE) +\n  labs(fill = ""Group"") +\n  scale_fill_manual(values = cbb_Palette[-1]) +\n  ylim(0, 1.25) +\n  ylab(""Relative Pixel-Value Change"") +\n  xlab(""Frequency"") +\n  scale_x_continuous(breaks = c(unique(p1_df$frequency))) +\n  scale_y_continuous(breaks = seq(0,5,0.2)) +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 40, vjust = 0.5, hjust = 0.3))\n\nggsave(paste0(""./data_analysis/output/"", ""freq_p1_points.pdf""), width = 11, height = 5, p1)\n\n# Permutation Test  -------------------------------------------------------\n# Did not make it into the final Paper\n# preverable we would like to make permutation tests and we would only do white noise against control\n# Dataframe of only Control and White Noise\ndf_test <- df %>% \n  filter(frequency_factor == ""Control"" | frequency_factor == ""WN"") %>%\n  mutate(\n    dateTime = as_factor(dateTime),\n    subgroup = as_factor(subgroup)\n  )\n\n# we use permutation test -> distribution is no problem\n# repeated measurement, we need to take it into with dateTime\n(wt <- coin::oneway_test(\n  norm_activity ~ frequency_factor | dateTime, \n  data = df_test,\n  distribution = approximate(10000)\n))\n\np2_text = paste0(\n  ""Z = "", round(statistic(wt),1),\n  "", "",\n  ""p-value "", ifelse(pvalue(wt)<0.001, ""< 0.001"", pvalue(wt))\n)\n\np2 <- df_test %>% \n  ggplot(aes(x = frequency_factor, y = norm_activity)) +\n  geom_boxplot() +\n  scale_y_continuous(breaks = seq(0,5,0.2)) +\n  ylab(""Relative Pixel-Value Change"") +\n  xlab("""") +\n  labs(caption = ""Approximative Two-Sample Fisher-Pitman Permutation Test, 10,000 resamples"") +\n  scale_x_discrete(labels = c(""Control"" = ""Control"", ""WN"" = ""White Noise"")) +\n  ggsignif::geom_signif(\n    annotations = p2_text,\n    y_position = c(1.6), xmin=c(1), xmax=c(2)\n    ) +\n  theme_classic()\n\nggsave(paste0(""./data_analysis/output/"", ""freq_p2_test.pdf""), p2, width = 5, height = 5) \n\n\n# BoxPlots ----------------------------------------------------------------\np3_df <- df %>%\n  left_join(p1_df, by = c(""frequency"")) %>% \n  mutate(\n    colorb = ifelse(is.na(colorb), ""1"", colorb)\n  )\n\np3_df_group <-\n  tibble(\n    Group = p1_df_group$colorb,\n    xmin = c(2, 5, 11),\n    xmax = c(4, 10, 20)\n  )\n# See above, we did drop 440 because of data error\np3_df <- p3_df %>% \n  filter(frequency_factor != ""440"") %>%\n  mutate(\n    frequency_factor = fct_relevel(frequency_factor, ""WN"", after = Inf),\n    frequency_factor = fct_recode(frequency_factor, `White\\nNoise` = ""WN"")\n  )\n\np3 <- p3_df %>%\n  ggplot(aes(x = frequency_factor, y = norm_activity, color = colorb)) +\n  geom_hline(\n    aes(yintercept = 1), color = ""black"", alpha = 0.5, show.legend = F\n  ) +\n  geom_boxplot() +\n  theme_classic()  +\n  geom_rect(\n    data = p3_df_group,\n    aes(\n      xmin = xmin, xmax = xmax, \n      x = 0, y = 0,\n      ymin = -Inf, ymax = Inf, fill = Group, color = NA), \n    alpha = 0.2, show.legend = T, inherit.aes = T\n  ) +\n  geom_boxplot() +\n  scale_y_continuous(breaks = seq(0,5,0.2)) +\n  scale_color_manual(values = cbb_Palette, na.translate = F) +\n  guides(color = FALSE) +\n  labs(fill = ""Group"") +\n  scale_fill_manual(values = cbb_Palette[-1]) +\n  ylab(""Relative Pixel-Value Change"") +\n  xlab(""Frequency"")\n\nggsave(paste0(""./data_analysis/output/"", ""freq_p3_box.pdf""), width = 11, height = 5, p3) \n\n# Pairwise T-Test ---------------------------------------------------------\n# parametic and non-parametic give both more or less same results\n# we choose non-parametic as some assumptions for parametic are not met\nres     <- aov(norm_activity ~ frequency_factor, data = df)\nres_hsd <- TukeyHSD(res)\np.res   <- tidy(res_hsd)\nwrite_csv2(p.res, ""output/pairwise_HSD.csv"")\n\n# Same letters indiciate non-siginifant comparissons\nmultcompLetters(res_hsd$frequency_factor[,4])\nletters   <- multcompLetters4(res, res_hsd)\nlettersDf <- tibble(\n  frequency_factor = names(letters[[""frequency_factor""]][[""Letters""]]),\n  letters = letters[[""frequency_factor""]][[""Letters""]]\n  ) %>%\n  mutate(\n    frequency_factor = ifelse(frequency_factor == ""WN"", ""White\\nNoise"", frequency_factor)\n  )\n\np3_letters <- p3_df %>%\n  left_join(lettersDf, by = c(""frequency_factor"")) %>% \n  group_by(frequency_factor) %>% summarise(\n    y = max(norm_activity),\n    letter = unique(letters)\n  )\n\np4 <- p3_df %>% \n  ggplot(aes(x = frequency_factor, y = norm_activity, color = colorb)) +\n  geom_hline(\n    aes(yintercept = 1), color = ""black"", alpha = 0.5, show.legend = F\n  ) +\n  geom_boxplot() +\n  geom_text(\n    data = p3_letters, \n    aes(y = 0, label = letter, color = ""black""),\n    color = ""black"",\n    alpha = 0.5,\n    nudge_y = -0.05\n  ) +\n  theme_classic()  +\n  geom_rect(\n    data = p3_df_group,\n    aes(\n      xmin = xmin, xmax = xmax, \n      x = 0, y = 0,\n      ymin = -Inf, ymax = Inf, fill = Group, color = NA), \n    alpha = 0.2, show.legend = T, inherit.aes = T\n  ) +\n  geom_boxplot() +\n  scale_y_continuous(breaks = seq(0,5,0.2)) +\n  scale_color_manual(values = cbb_Palette, na.translate = F) +\n  guides(color = FALSE) +\n  labs(fill = ""Group"") +\n  scale_fill_manual(values = cbb_Palette[-1]) +\n  ylab(""Relative Pixel-Value Change"") +\n  xlab(""Frequency"")\n\nggsave(paste0(""./data_analysis/output/"", ""freq_p3_box_sign.pdf""), width = 11, height = 5, p4) \n\n# Pairwise T-Test - Non-Parametic -----------------------------------------\n# see above we choose to use non-parametic test, its not as powerful\n# but also takes less assumptions\n# res_k     <- kruskal.test(norm_activity ~ 0 + frequency_factor, data = df)\nres_hsd_w <- pairwise.wilcox.test(df$norm_activity, df$frequency_factor, p.adj = ""holm"")\np.res_w   <- tidy(res_hsd_w)\nwrite_csv2(p.res_w, ""./data_analysis/output/pairwise_wilcox_holm.csv"")\n# matrix from pairwise wilcox is not quatratic\n# generate named vector for multcompLetters, see help for multcompLetters\ntest_vector        <- p.res_w$p.value\nnames(test_vector) <- paste(p.res_w$group1, p.res_w$group2, sep = ""-"")\n(letters_w         <- multcompLetters(test_vector))\n# Generate Tibble for Joining with Main Table\nlettersDf_w <- tibble(\n  frequency_factor = names(letters_w$Letters),\n  letters = letters_w$Letters\n) %>% \n  mutate(\n    frequency_factor = ifelse(frequency_factor == ""WN"", ""White\\nNoise"", frequency_factor)\n  )\n# Plot df\np5_letters <- p3_df %>%\n  left_join(lettersDf_w, by = c(""frequency_factor"")) %>% \n  group_by(frequency_factor) %>% summarise(\n    y = max(norm_activity),\n    letter = unique(letters)\n  )\n\np5 <- p3_df %>% \n  ggplot(aes(x = frequency_factor, y = norm_activity, color = colorb)) +\n  geom_hline(\n    aes(yintercept = 1), color = ""black"", alpha = 0.5, show.legend = F\n  ) +\n  geom_boxplot() +\n  geom_text(\n    data = p5_letters, \n    aes(y = 0, label = letter, color = ""black""),\n    color = ""black"",\n    alpha = 0.5,\n    nudge_y = -0.05\n  ) +\n  theme_classic()  +\n  geom_rect(\n    data = p3_df_group,\n    aes(\n      xmin = xmin, xmax = xmax, \n      x = 0, y = 0,\n      ymin = -Inf, ymax = Inf, fill = Group, color = NA), \n    alpha = 0.2, show.legend = T, inherit.aes = T\n  ) +\n  geom_boxplot() +\n  scale_y_continuous(breaks = seq(0,5,0.2)) +\n  scale_color_manual(values = cbb_Palette, na.translate = F) +\n  guides(color = FALSE) +\n  labs(fill = ""Group"") +\n  scale_fill_manual(values = cbb_Palette[-1]) +\n  ylab(""Relative Pixel-Value Change"") +\n  xlab(""Frequency"")\n\nggsave(paste0(""./data_analysis/output/"", ""freq_p5_box_sign_wilcox.pdf""), width = 11, height = 5, p5)\n\n', '# Generate needed Cols ----------------------------------------------------\n# generate dateTime Field\ndf$dateTime      <- paste0(df$date, ""-"", df$time)\n# generate norm activity cols\ndf$norm_activity                     <- df$activity\n# save frequency original column as we do some mutating with it\ndf$frequency_org <- df$frequency\n\n# Generate Groups, to represent each Frequency Change (round) ---------------------\n# group each frequenz change\ndf$subgroup <- NA\nlastFreq    <- 0\ncurFreq     <- 0\nfreqGroup   <- 0\n\nfor(i in 1:nrow(df)){\n  curFreq <- df$frequency[i]\n  # check if current frequency is last frequency\n  # if not we change the group\n  if(curFreq != lastFreq){\n    freqGroup <- freqGroup + 1\n  }\n  df$subgroup[i] <- freqGroup\n  lastFreq <- curFreq\n}\n# Cleanup\nrm(lastFreq, curFreq, freqGroup, i)\n\n# Cumulative Sum ---------------------------------------------------------\n# generate a cumulative sum in our frequency breaks\n# this will help us to pick out the middle of the trial\ndf$csum <- 1\ndf <- df %>% \n  group_by(subgroup) %>%\n  mutate(\n    csum = cumsum(csum)\n  ) %>% \n  ungroup()\n\n# control are frames between breaks with 0 frequency from 15-20 cumsum (-2)\ndf <- df %>%\n  mutate(\n    frequency = ifelse(between(csum, 15, 20) & frequency_org == 0, -2, frequency_org)\n  )\n\n# Filter frames in our breaks, which are at the beginning < 3 and at the end > 10\ndf <- df %>% filter(between(csum, 3,10) | frequency == -2 )\n\n# Remove Zeroes\ndf <- df %>% filter(frequency != 0)\n\n# Add count, we could filter for it eg. that we only plot if we have at least 30 rounds\ndf <- df %>% add_count(frequency, name = ""count"")\n\n\n# Calculate Mean Difference of the Control -----------------------------------------------\n\n# Calculate mean difference of base -2 between rounds\ndfBaseSummary <- df %>%\n  filter(frequency == -2) %>%\n  group_by(dateTime) %>%\n  summarise(\n    m_activity = mean(activity)\n    #m_new_activity_calculation = mean(new_activity_calculation),\n    #m_activity_calculation_frame = mean(activity_calculation_frame)\n  ) %>% \n  mutate(\n    d_activity = m_activity - m_activity[[1]]\n    #d_new_activity_calculation = m_new_activity_calculation - m_new_activity_calculation[[1]],\n    #d_activity_calculation_frame = m_activity_calculation_frame - m_activity_calculation_frame[[1]]\n  )\n\n\n# Normalize based on Mean Base Activity -----------------------------------\ntestUnits <- unique(df$dateTime)\nfor(i in 1:length(testUnits)){\n  testName <- testUnits[[i]]\n  baseDiff <- dfBaseSummary %>% filter(dateTime == testName)\n  df <- df %>%\n    mutate(\n      norm_activity = ifelse(\n        dateTime == testName, \n        activity / baseDiff$m_activity, norm_activity # we could here change to median\n      )\n      #norm_new_activity_calculation = ifelse(\n      #  dateTime == testName, \n      #  new_activity_calculation / baseDiff$m_new_activity_calculation, norm_new_activity_calculation # we could here change to median\n      #),\n      #norm_activity_calculation_frame = ifelse(\n      #  dateTime == testName, \n      #  activity_calculation_frame / baseDiff$m_activity_calculation_frame, norm_activity_calculation_frame # we could here change to median\n      #),\n    )\n}\n\n# Cleanup\nrm(testUnits, i, testName)\n\n# Generate Factor Levels for Plots ----------------------------------------\n# generate factor frequency\ndf <- df %>% \n  mutate(\n    frequency_factor = factor(frequency) %>% fct_recode( Control = ""-2"", WN = ""-1"")\n  )', '# Libs ----------------------------------------------------\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(here)\n\n# colorblind friendly colours\ncbb_Palette <- c(""#000000"", ""#E69F00"", ""#56B4E9"", ""#009E73"", ""#F0E442"", ""#0072B2"", ""#D55E00"", ""#CC79A7"")\n\n# Read Data ---------------------------------------------------------------\n# Read data and drop_na as cleanup, if we have empty rows appended by accident\ndf_freq <- read_csv(""./datasets/frequency_experiments.csv"") %>%\n  drop_na() %>% \n  mutate(\n    amplitude = 1,\n    dataset = ""freq""\n  )\n\ndf_amp <- read_csv(""./datasets/amplitude_experiments.csv"") %>%\n  drop_na() %>% \n  mutate(\n    dataset = ""amp""\n  )\n\n\ndf <- bind_rows(\n  df_freq, df_amp\n)\n\n# Velocity ----------------------------------------------------------------\nvelo <- read_csv(""./datasets/velo_freq_amp.csv"") %>%\n  drop_na()\n\n# Joining Together --------------------------------------------------------\ndf <- df %>% \n  left_join(velo, by = c(""frequency"", ""amplitude""))\n\n# Normalize ---------------------------------------------------------------\nsource(paste0(here(), ""/data_analysis/utils.R""))\n\n# Velocity ----------------------------------------------------------------\n# -1 would be white noise and 440 is broken\ndf <- df %>% \n  filter(\n   frequency != -1 & frequency != 440 \n  )\n\np <- df %>% \n  mutate(\n    # set control to low number so we can plot with\n    velocity = if_else(frequency == -2, 0.04, velocity)\n  ) %>% \n  filter(\n    !is.na(velocity)\n    ) %>% \n  ggplot(\n    aes(\n      x = log10(velocity), \n      y = norm_activity, \n      group = velocity, \n      color = as.factor(frequency), \n      fill = as.factor(frequency)\n      )\n  ) +\n  geom_hline(aes(yintercept = 1), color = ""black"", alpha = 0.5, show.legend = F) +\n  geom_boxplot(\n    width = 0.01,\n    outlier.shape = NA, \n    ) +\n  scale_fill_viridis_d(\n    option = ""viridis"",\n    guide = guide_legend(""Frequency"")\n  ) + \n  scale_colour_viridis_d(\n    option = ""viridis"",\n    guide = guide_legend(""Frequency"")\n  ) + \n  ylab(""normalized pixel-based motion index"") +\n  xlab(""Velocity [log10(mm/s)]"") +\n  scale_y_continuous(breaks = seq(0, 2, 0.2)) +\n  scale_x_continuous(breaks = seq(-2, 2, 0.2)) +\n  theme_classic()\n\nggsave(paste0(""./data_analysis/output/"", ""velocity_vir.pdf""), width = 11, height = 5, p) \n\n\n\n\n\n\n\n']","Effects of Sinusoidal Vibrations on the Motion Response of Honeybees - Exemplary software # Motion Analysis of HoneybeesCode pieces for motion analysis of honey bees. For full reproducibility we include in this repository our collected data sets and code pieces. R session info is saved in the root as [SessionInfo.txt](SessionInfo.txt).For more details please refer to Stefanec, M., Oberreiter, H., Becher, M. A., Haase, G., & Schmickl, T. (2021). Effects of Sinusoidal Vibrations on the Motion Response of Honeybees. Frontiers in Physics, 9 - Script to calculate and extract PMI from movie recording, utilizating cv2 - R scripts to generate plots and statistical testsPlease also refer to https://github.com/martin-st/motion-analysis-of-honeybees ## Statistical AnalysisResults of pairwise of frequencies can be found in [pairwise_wilcox_holm.csv](pairwise_wilcox_holm.csv).## MIT LicenceCopyright (c) 2021 Martin Stefanec & Hannes Oberreiter",3
Data and code from: Thermal niche and habitat use by co-occurring lake trout (Salvelinus namaycush) and brook trout (S. fontinalis) in stratified lakes,"Realized thermal niche and habitat use are two conceptualizations of fish habitat based on organismal performance or lake-specific ecology, respectively. Both habitat types were compared for lake trout (Salvelinus namaycush) and brook trout (S. fontinalis) co-occurring in four large (> 500 ha) oligotrophic lakes. Lakes were partitioned into two morphological categories based on possession of a central or non-central deep basin with corresponding differences in adjoining shelf areas. Lake asymmetry in basin location has been shown to strongly influence food web connections based on isolation of basins from shelf areas.Generally, overlap between both habitat types occurred in several comparisons with lake trout, suggesting that thermal habitat is a reasonable proxy for habitat use boundaries though not a full replacement for insights gained from habitat use models.For brook trout, overlap was not as consistent, especially for lakes with non-central basins. In central basin lakes, there were closer proximity between the two species and overlap in both thermal niche and habitat use models. There was very limited overlap of either habitat type in lakes with non-central basins. Further, there were no shared areas of interspecific overlap between thermal niche and habitat use in non-central basins pointing to additional complexity governing habitat partitioning between lake trout and brook trout in these types of lakes. The shelf area effect on spatial structure of habitat, and likely food web connections, can occur in lakes regardless of basin centrality so long as shelf areas are large. In this lake set, lakes were sufficiently large to observe this phenomenon.","['#Read in the mean temp data\r\n\r\nlibrary(scales)\r\n\r\nsetwd(""~/Writing documents/Two Chars detection/Temp Selection Darren"")\r\nAVGtempdata81 <- read.csv(""lake_trout_adult_mean_temp_do.csv"",header=T,sep="","")\r\n\r\n#Calculate weighted mean and SD for lake trout\r\nxstar81=sum((AVGtempdata81$wi_temp*AVGtempdata81$mean_Temp))/sum(AVGtempdata81$wi_temp)\r\nout81 = (sum(AVGtempdata81$wi_temp*((AVGtempdata81$mean_Temp-xstar81)^2)))/((sum(AVGtempdata81$wi_temp))-(sum((AVGtempdata81$wi_temp)^2)/sum(AVGtempdata81$wi_temp)))                                \r\nSD81=sqrt(out81)\r\nplu2SD81 = (xstar81+(2*SD81))\r\nmin2SD81 =(xstar81-(2*SD81))\r\ndetails81 <-data.frame(c(xstar81,out81,SD81,plu2SD81,min2SD81), stringsAsFactors=F)\r\nrownames(details81)<-c(""Weighted Mean"",""Variance"",""SD"",""plu2SD"",""min2SD"")\r\n\r\ntiff(""081_080_Temp_Distributions_Green.tiff"",width=6,height=4 ,units = \'in\',res=600,compression=\'lzw\')\r\n\r\ncols<-""lightgreen""\r\npar(mar=c(3.5,3.5,0.2,0.2))\r\nset.seed(3000)\r\na=seq(2,20,0.001)\r\n#LAKE TROUT\r\nb=dnorm(a,xstar81,SD81) #probability density function across \'a\' x values, with mean and then sd\r\nplot(a,b,xlab="""",ylab=\'\',type=\'l\',lwd=1,xaxt=\'n\',ylim=c(0,0.3)) #plot the \'a\' sequence on the x and the probability density function on the y\r\naxis(side=1, at=seq(2,20,2))\r\naxis(side=1, at=seq(3,19,2),labels=FALSE,tck=-0.01)\r\nc=seq(min2SD81,plu2SD81,0.001) #set seq for the 2*SD polygon, where the seq begins and ends at the upper and lower 2*SD from the mean\r\nd=dnorm(c,xstar81,SD81) #make a density function across the above sequence, with same mean and SD\r\npolygon(c(min2SD81,c,plu2SD81),c(0,d,0),col=alpha(cols,alpha=0.5)) #plot the polygon with vertices at the lower bound, \'c\' sequence, and upper bound on the x, and then 0, density function, and 0 on the y,\r\n#polygon(c(min(b),a,max(b)),c(0,b,0),col=rgb(0,0,0,225,maxColorValue = 255)) #full distribution polygon if needed\r\ne=seq((xstar81-SD81),(xstar81+SD81),0.001) # same thing as above but for 1*SD from the mean\r\nf=dnorm(e,xstar81,SD81)\r\npolygon(c((xstar81-SD81),e,(xstar81+SD81)),c(0,f,0),col=cols)\r\n#segments(x0=xstar81, y0=0, x1=xstar81, y1=max(b-0.01), lty=\'66\') #dotted line for the mean\r\nmtext(""Temperature (C)"",side=1,line=2.2,cex=1.1)\r\nmtext(""Frequency"",side=2,line=2.5,cex=1.1)\r\npar(new=T)\r\n##For Brook trout values were calculated in excel and supplied to the dnorm function here.\r\nb=dnorm(a,13.31,1.404)\r\nplot(a,b,xlab="""",ylab=\'\',type=\'l\',lwd=1,xaxt=\'n\',ylim=c(0,0.3),yaxt=""n"")\r\nc=seq(10.502,16.118,0.001)\r\nd=dnorm(c,13.31,1.404)\r\npolygon(c(10.502,c,16.118),c(0,d,0),col=rgb(170,206,255,125,maxColorValue = 255))\r\ne=seq(11.906,14.714,0.001)\r\nf=dnorm(e,13.31,1.404)\r\npolygon(c(11.906,e,14.714),c(0,f,0),col=rgb(135,206,235,125,maxColorValue = 255))\r\npar(new=T)\r\nplot(a,b,xlab="""",ylab=\'\',type=\'l\',lwd=1,xaxt=\'n\',ylim=c(0,0.3),yaxt=""n"") #for black outline of dist\r\ndev.off()\r\n', 'setwd(""Q:/0_Requests/Ridgway"")\r\n#library(data.table)\r\nlibrary(ggplot2)\r\n#library(RODBC)\r\n#library(plyr)\r\n#library(xtable)\r\n#library(knitr)\r\n#library(Hmisc)\r\n#library(raster)\r\n#library(RColorBrewer)\r\n#library(rasterVis)\r\n#library(gridExtra)\r\n#library(Cairo)\r\n#library(fishmethods)\r\nlibrary(ggpubr) #for ggarrange\r\nlibrary(ggforce) #for theme_no_axes()\r\n#library(ggpubr)\r\n#library(rgdal)\r\nlibrary(sf)\r\nlibrary(cowplot) #for ggdraw() - drawing multiple ggplot objects\r\nlibrary(ggsn) #for scalebar() function\r\n\r\n\r\nsetwd(""X:/ARDS/Harkness/manuscripts for publishing/BT_LT_HabitatOverlap/DryadSubmission/HypsograpicPlots"")\r\n\r\n##Load Volume Data and calculate radii for radial hypsographic plots\r\nvol_sel<-read.csv(""lake_volumes.csv"")\r\nvol_sel$x<-1  #origin x for radial plot\r\nvol_sel$y=1   #origin y for radial plot\r\nvol_sel$vol_radius <- vol_sel$Volume/pi   #radius to draw dept layer based on volume below Depth\r\nvol_sel$area_radius <- vol_sel$Area_2D/pi #radius to draw depth layer based on area below Depth  \r\n\r\n##Load geopackage with spatial data \r\n##Replace filepath (fp) with path to where hypso.gpkg is located\r\nfp<- ""X:/ARDS/Harkness/manuscripts for publishing/BT_LT_HabitatOverlap/DryadSubmission/HypsograpicPlots/hypso.gpkg""\r\n\r\n##load lake outline, 1m contours, and 10m contours for each lake\r\nlam_outline <- st_read(fp, ""lam_outline"")\r\nlam_ctour<- st_read(fp, ""lam_ctour"")\r\nlam_ctour<-lam_ctour[lam_ctour$CONTOUR != 0,] #remove 0m contours\r\nlam_therm <- lam_ctour[lam_ctour$CONTOUR == -8,] #pick contour for thermocline\r\nlam_ctour10m<- st_read(fp, ""lam_ctour10m"")\r\nlam_ctour10m<-lam_ctour10m[lam_ctour10m$CONTOUR != 0,] #remove 0m contours\r\nlam_ctour10m<- lam_ctour10m[lam_ctour10m$CONTOUR == -20 | \r\n                              lam_ctour10m$CONTOUR == -40,] #select only 20m contour intervals so map not too cluttered\r\n\r\nhogan_outline<- st_read(fp, ""hogan_outline"")\r\nhogan_ctour<- st_read(fp, ""hogan_ctour"")\r\nhogan_ctour<-hogan_ctour[hogan_ctour$CONTOUR != 0,]\r\nhogan_therm<-hogan_ctour[hogan_ctour$CONTOUR == -8,]\r\nhogan_ctour10m<- st_read(fp, ""hogan_ctour10m"")\r\nhogan_ctour10m<-hogan_ctour10m[hogan_ctour10m$CONTOUR != 0,]\r\nhogan_ctour10m <- hogan_ctour10m[hogan_ctour10m$CONTOUR == -20, ]\r\n\r\nburnt_outline<- st_read(fp, ""burnt_outline"")\r\nburnt_ctour<- st_read(fp, ""burnt_ctour"")\r\nburnt_ctour<-burnt_ctour[burnt_ctour$CONTOUR != 0,]\r\nburnt_therm<-burnt_ctour[burnt_ctour$CONTOUR == -6,]\r\nburnt_ctour10m<- st_read(fp, ""burnt_ctour10m"")\r\nburnt_ctour10m<-burnt_ctour10m[burnt_ctour10m$CONTOUR != 0,]\r\nburnt_ctour10m <- burnt_ctour10m[burnt_ctour10m$CONTOUR == -20,]\r\n\r\nlav_outline<- st_read(fp, ""lav_outline"")\r\nlav_ctour<- st_read(fp, ""lav_ctour"")\r\nlav_ctour<-lav_ctour[lav_ctour$CONTOUR != 0,]\r\nlav_therm<-lav_ctour[lav_ctour$CONTOUR == -8,]\r\nlav_ctour10m<-st_read(fp, ""lav_ctour10m"")\r\nlav_ctour10m<-lav_ctour10m[lav_ctour10m$CONTOUR != 0,]\r\nlav_ctour10m <- lav_ctour10m[lav_ctour10m$CONTOUR == -20 |\r\n                               lav_ctour10m$CONTOUR == -40,]\r\n\r\n\r\n#Lavieille\r\n\r\nvol_l <- vol_sel[vol_sel$Lake == ""Lavieille"",] #select volume records for specific lake\r\n\r\n  ##ggplot\r\n  ##plot radial hypsographs based on surface area, red lines indicate depths below rough thermocline\r\n    lavieille<- ggplot() + geom_circle(aes(x0=x, y0=y, r=area_radius), data=vol_l[vol_l$Depth>=-7,]) +\r\n    geom_circle(aes(x0=x, y0=y, r=area_radius),  colour=""red"", data=vol_l[vol_l$Depth<=-8,]) + coord_fixed() +\r\n     theme_no_axes() + \r\n      theme(plot.title = element_text(hjust =0.5), rect = element_blank())\r\n    \r\n    ##plot lake outline, thermocline contour, and 20m contours, scalebar indicatres 1km\r\n    lav_bath<-ggplot() +  geom_sf(data=lav_outline, fill=""gray94"") +\r\n      geom_sf(data=lav_therm, colour =""red"", alpha=0.8) +\r\n      geom_sf(data=lav_ctour10m, colour =""gray40"") +\r\n      labs(title =""Lavieille"") + theme_no_axes() + \r\n      theme(plot.title = element_text(size=24,hjust =0.5), rect = element_blank()) +\r\n      theme(plot.subtitle=element_text( hjust=0.5, face=""italic"", color=""black"")) +\r\n      scalebar(lav_outline, dist =1, dist_unit = ""km"", transform = T, model = ""WGS84"", \r\n               location =""topleft"", box.fill=c(""black"", ""black""), height=.01, st.size= .4, st.dist = 0)\r\n    \r\n    ##combine above plots into one figure specifying location of radial plot\r\n    lav<-ggdraw() +\r\n      draw_plot(lav_bath, width = 1, height = 1)+\r\n      draw_plot(lavieille, x=.02, y=.0, width = 0.45, height = 0.45)\r\n\r\n    \r\n#Burntroot\r\n\r\n vol_b <- vol_sel[vol_sel$Lake == ""Burntroot"",]\r\n\r\n  ##ggplot\r\n  burnt<- ggplot() + geom_circle(aes(x0=x, y0=y, r=area_radius), data=vol_b[vol_b$Depth>=-7,]) +\r\n    geom_circle(aes(x0=x, y0=y, r=area_radius), alpha = 0.5, colour=""red"", data=vol_b[vol_b$Depth<=-8,]) + coord_fixed() +\r\n    theme_no_axes() + \r\n    theme(plot.title = element_text(hjust =0.5), rect = element_blank())\r\n  \r\n  burnt_bath<-ggplot() +  geom_sf(data=burnt_outline, fill=""gray94"") +\r\n    geom_sf(data=burnt_therm, colour =""red"", alpha=0.8) +\r\n    geom_sf(data=burnt_ctour10m, colour =""gray40"") + \r\n    labs(title =""Burntroot"") + theme_no_axes() + \r\n    theme(plot.title = element_text(size=24,hjust =0.5), rect = element_blank()) +\r\n    theme(plot.subtitle=element_text( hjust=0.5, face=""italic"", color=""black""))+\r\n    scalebar(burnt_outline, dist =1, dist_unit = ""km"", transform = T, model = ""WGS84"", \r\n             location =""bottomleft"", box.fill=c(""black"", ""black""), height=.01, st.size= .4, st.dist = -.2)\r\n\r\n  bu<- ggdraw() +\r\n    draw_plot(burnt_bath, width = 1, height = 1)+\r\n    draw_plot(burnt, x=.49, y=.43, width = 0.45, height = 0.45)\r\n\r\n##Hogan\r\n vol_h <- vol_sel[vol_sel$Lake == ""Hogan"",]\r\n\r\n  ##ggplot\r\n  hogan<- ggplot() + geom_circle(aes(x0=x, y0=y, r=area_radius), data=vol_h[vol_h$Depth>=-7,]) +\r\n    geom_circle(aes(x0=x, y0=y, r=area_radius), alpha = 0.5, colour=""red"", data=vol_h[vol_h$Depth<=-8,]) + coord_fixed() +\r\n    theme_no_axes() + \r\n    theme(plot.title = element_text(hjust =0.5), rect = element_blank())\r\n  \r\n  hogan_bath<-ggplot() +  geom_sf(data=hogan_outline, fill=""gray94"") +\r\n    geom_sf(data=hogan_therm, colour =""red"", alpha=0.8) +\r\n    geom_sf(data=hogan_ctour10m, colour =""gray40"") + \r\n    labs(title = ""Hogan"") + theme_no_axes() + \r\n    theme(plot.title = element_text(size=24,hjust =0.5), rect = element_blank()) + \r\n    theme(plot.subtitle=element_text( hjust=0.5, face=""italic"", color=""black""))+\r\n    scalebar(hogan_outline, dist =1, dist_unit = ""km"", transform = T, model = ""WGS84"", \r\n             location =""bottomright"", box.fill=c(""black"", ""black""), height=.01, st.size= .4, st.dist = -.2)\r\n  \r\n  ho<-ggdraw() +\r\n    draw_plot(hogan_bath, width = 1, height = 1)+\r\n    draw_plot(hogan, x=.50, y=.04, width = 0.45, height = 0.45)\r\n\r\n##La Muir\r\n vol_m <- vol_sel[vol_sel$Lake == ""La_Muir"",]\r\n\r\n  ##ggplot\r\n  la_muir<-ggplot() + geom_circle(aes(x0=x, y0=y, r=area_radius), data=vol_m[vol_m$Depth>=-7,]) +\r\n    geom_circle(aes(x0=x, y0=y, r=area_radius), alpha = 0.5, colour=""red"", data=vol_m[vol_m$Depth<=-8,]) + coord_fixed() +\r\n    theme_no_axes() +\r\n    theme(rect = element_blank())\r\n  \r\n  lam_bath<-ggplot() +  geom_sf(data=lam_outline, fill=""gray94"") +\r\n    geom_sf(data=lam_therm, colour =""red"", alpha=0.8) +\r\n    geom_sf(data=lam_ctour10m, colour =""gray40"") + \r\n    labs(title = ""La Muir"") + theme_no_axes() + \r\n    theme(plot.title = element_text(size=24, hjust =0.5), rect=element_blank())+ \r\n    theme(plot.subtitle=element_text(hjust=0.5, face=""italic"", color=""black""))+\r\n    scalebar(lam_outline, dist =1, dist_unit = ""km"", transform = T, model = ""WGS84"", \r\n             location =""bottomright"", box.fill=c(""black"", ""black""), height=.01, st.size= .4, st.dist = -.2)\r\n  \r\n  la <-ggdraw() +\r\n    draw_plot(lam_bath)+\r\n    draw_plot(la_muir, x=.02, y=.46, width = 0.45, height = 0.45)\r\n\r\n \r\n  ##combine plots into single figure and print in console\r\n  plt<-ggarrange(bu, ho,la, lav, nrow = 2, ncol = 2, widths = 1, heights = 1)\r\n\r\n  plt\r\n\r\n  ##export plot for publication\r\n  ggsave(""bath_radial.png"" ,plt, width=12, height=12, units= ""in"", dpi = 600)\r\n\r\n \r\n']","Data and code from: Thermal niche and habitat use by co-occurring lake trout (Salvelinus namaycush) and brook trout (S. fontinalis) in stratified lakes Realized thermal niche and habitat use are two conceptualizations of fish habitat based on organismal performance or lake-specific ecology, respectively. Both habitat types were compared for lake trout (Salvelinus namaycush) and brook trout (S. fontinalis) co-occurring in four large (> 500 ha) oligotrophic lakes. Lakes were partitioned into two morphological categories based on possession of a central or non-central deep basin with corresponding differences in adjoining shelf areas. Lake asymmetry in basin location has been shown to strongly influence food web connections based on isolation of basins from shelf areas.Generally, overlap between both habitat types occurred in several comparisons with lake trout, suggesting that thermal habitat is a reasonable proxy for habitat use boundaries though not a full replacement for insights gained from habitat use models.For brook trout, overlap was not as consistent, especially for lakes with non-central basins. In central basin lakes, there were closer proximity between the two species and overlap in both thermal niche and habitat use models. There was very limited overlap of either habitat type in lakes with non-central basins. Further, there were no shared areas of interspecific overlap between thermal niche and habitat use in non-central basins pointing to additional complexity governing habitat partitioning between lake trout and brook trout in these types of lakes. The shelf area effect on spatial structure of habitat, and likely food web connections, can occur in lakes regardless of basin centrality so long as shelf areas are large. In this lake set, lakes were sufficiently large to observe this phenomenon.",3
Statistical Analysis for Skinfaxi project,Software supplement for estimation of age-specific incidence from age-specific prevalence without differential mortality (Skinfaxi project). Source code for use with the free statistical software R (The R Foundation for Statistical Computing).,"['################################################################################\r\n# \r\n# Analysis for Skinfaxi project: estimation of age-specific incidence from\r\n#    age-specific prevalence without differential mortality\r\n# \r\n#   Ralph Brinks, May 2021\r\n#   German Diabetes Center and Department for Statistics, LMU Munich\r\n#   E-Mail: ralph.brinks@ddz.de\r\n#\r\n################################################################################\r\n\r\nrm(list=ls(all=TRUE))\r\n\r\n\r\n# aggregated current status data: breathlessness in British coal miners\r\n#--------------------------------\r\n# this is Table 14.2a from textbook Regina C. Elandt-Johnson, Norman L. Johnson: \r\n# Survival Models and Data Analysis, Wiley 2014\r\n# Elandt-Johnson & Johnson state that original data is from Ashford & Schowden, \r\n# Biometrics 26, 1970, this could not be checked\r\n\r\nageGrp <- c(""20-24"",""25-29"",""30-34"",""35-39"",""40-44"",""45-49"",""50-54"",""55-59"",""60-64"")\r\nN      <- c(   1952,   1791,   2113,   2783,   2274,   2393,   2090,   1750,   1136) # nr of coal miners in study\r\nC      <- c(     16,     32,     73,    169,    223,    357,    521,    558,    478) # nr of breathless miners\r\n\r\n# Incidence estimate from Elandt-Johnson & Johnson Table 14.2a\r\ninc    <- c( 0.0164, 0.0029, 0.0306, 0.0218, 0.0529, 0.0493, 0.1509,-0.0117, 0.2755)\r\n\r\n\r\n# prepare input data\r\ndat      <- data.frame(ageGrp = ageGrp, N = N, C = C, Inc.est = inc)\r\ndat$Prev <- with(dat, C/N)\r\ndat$se   <- with(dat, sqrt(Prev*(1-Prev)/N))\r\n\r\n#with(dat, barplot(Prev, names.arg = AgeGroup, las = 1, ylab = ""Prevalence""))\r\n\r\n# aux functions\r\nlogit <- function(x){\r\n  return(log(x/(1-x)))\r\n}\r\n\r\nexpit <- function(x){ # inverse of logit (expit is sometimes called logistic function)\r\n  return(exp(x)/(1+exp(x)))\r\n}\r\n\r\nages  <- 22.5+5*(0:8)\r\nmod.p <- lm(logit(Prev) ~ ages, data = dat)\r\nsummary(mod.p)\r\nmatplot(ages, logit(dat$Prev), type = ""p"", pch = 1, col = ""blue"")\r\nabline(mod.p)\r\n# -> approach p(a) = expit(beta0 + beta1 * a) seems ok\r\n\r\nfct_p <- function(a, x){\r\n  b0_ <- x[1]\r\n  b1_ <- x[2]\r\n  arg_ <- b0_ + b1_ * a\r\n  return(expit(arg_))\r\n}\r\n\r\n# with p(a) = expit(beta0 + beta1 * a) and the derivative of expit\r\n#        expit\' = expit*(1-expit) we have\r\n# dp/da = beta1 * [1-expit(beta0 + beta1 * a]*[expit(beta0 + beta1 * a)] \r\n#       = beta1 *[1-p(a)] * p(a)\r\n# as i = (dp/da)/(1-p) (Equation (2) from the maintext) we find\r\n# i(a) = beta1 * expit(beta0 + beta1 * a) = beta1 * p(a)\r\n\r\nx <- as.numeric(mod.p$coeff)\r\nb0_ <- x[1]\r\nb1_ <- x[2]\r\n\r\np_  <- fct_p(ages, x)\r\ni_  <- b1_*p_ \r\n\r\npng(""example1.png"", width = 1200, height = 1200, res=200)\r\nmatplot(ages, i_, las = 1, type = ""b"", pch = 3, ylim = c(-0.01, 0.06), ylab = ""Incidence i"", xlab = ""Age a (in years)"")\r\nabline(h=0)\r\n# inc aus Elandt-Johnson: note that 5y incidence is reported -> divide by 5\r\nmatplot(ages, dat$Inc.est/5, type = ""b"", pch = 3, col = ""blue"", add = TRUE)\r\ndev.off()\r\n\r\n# inc from ML-estimate\r\n#matplot(ages, exp(-7.8237 + 0.07559*ages), type = ""b"", pch = 2, col = ""red"", add = TRUE) \r\n\r\n\r\n\r\n# Ansatz i(a) = exp(gamma0 + gamma1 * a)\r\nfct_i <- function(a, x){\r\n  c0_ <- x[3]\r\n  c1_ <- x[4]\r\n  arg_ <- c0_ + c1_ * a\r\n  return(exp(arg_))\r\n}\r\n\r\nmod.i <- lm(log(i_) ~ ages)\r\nplot(ages, log(i_))\r\nabline(mod.i)\r\nsummary(mod.i)\r\n# -> approximation of inc seems ok\r\n\r\n\r\n################################################################################\r\n\r\n# ML estimation\r\n\r\n# binomial log-likelihood\r\nll <- function (prop, numerator, denominator) {\r\n  return(\r\n    lgamma(denominator + 1) - \r\n      lgamma(  numerator + 1) - \r\n      lgamma(denominator - numerator + 1) +\r\n      numerator * log(prop) + (denominator - numerator) * log(1 - prop)\r\n  )\r\n}\r\n\r\n# overall log likelihood for all age groups 20-24, ..., 60-64\r\ncompLogLike <- function(pvec){\r\n  return(sum(ll(pvec, dat$C, dat$N)))\r\n}\r\n\r\n# Ansatz i(a) = exp(c0 + c1*a) and Equation (3) leads to\r\nfct_p_ll <- function(a, c0, c1){\r\n  xa_ <- exp(c0 + c1*c(20, a))\r\n  return(1 - exp((xa_[1]-xa_[2])/c1))\r\n}\r\n\r\nfct_ll <- function(x){\r\n  pv <- sapply(22.5+5*(0:8), FUN = fct_p_ll, c0 = x[1], c1 = x[2])\r\n  return(compLogLike(pv))\r\n}\r\n\r\n# find ML estimator\r\nopt.res <- optim(par = c(-9.2, 0.07), fn = fct_ll, control = list(fnscale = -1.0), hessian = TRUE)\r\n\r\n# use standard ML theory for confidence bounds\r\nfisherInfo <-  solve(-opt.res$hessian)\r\npropSigma  <-  sqrt(diag(fisherInfo))\r\nupper      <-  opt.res$par+1.96*propSigma\r\nlower      <-  opt.res$par-1.96*propSigma\r\ninterval   <-  data.frame(val = opt.res$par, ci.low=lower, ci.up = upper)\r\n\r\n\r\n################################################################################\r\n\r\n# MCMC algorithm (see DOI 10.5281/zenodo.4770055) results stem from there\r\n\r\nres <- matrix(\r\n  data = c\r\n  (\r\n    1e+03,-11.395962, -8.612161, -7.246405, 0.06692346, 0.08831328, 0.13582897,\r\n    2e+03,-10.426125, -8.644811, -6.493999, 0.05225920, 0.09069918, 0.11965806,\r\n    5e+03, -8.889450, -7.816510, -6.925506, 0.06016502, 0.07592488, 0.09450954,\r\n    1e+04, -9.044156, -8.049865, -7.380215, 0.06828044, 0.08033019, 0.09722764,\r\n    2e+04, -8.588293, -7.955250, -7.502756, 0.07080393, 0.07860220, 0.08961482,\r\n    5e+04, -8.549381, -8.027441, -7.582884, 0.07224861, 0.07993875, 0.08916171,\r\n    1e+05, -8.383894, -7.961895, -7.639608, 0.07308477, 0.07870259, 0.08619884,\r\n    2e+05, -8.219660, -7.986505, -7.751834, 0.07481038, 0.07903302, 0.08319603,\r\n    5e+05, -8.081182, -7.944302, -7.768273, 0.07440854, 0.07805045, 0.08061308,\r\n    1e+06, -8.053960, -7.928882, -7.822193, 0.07516601, 0.07729412, 0.07962617,\r\n    2e+06, -8.010503, -7.924143, -7.790905, 0.07378004, 0.07652087, 0.07821109\r\n  ), \r\n  ncol = 7, byrow = TRUE) \r\n\r\npng(""gamma_relax.png"", width = 1500, height = 1200, res=200)\r\npar(mfrow = c(1, 2))\r\nmatplot(res[,1], res[,3], type = ""n"", log = ""x"", las = 1, ylim = c(-12, -6), \r\n        xlab = expression(lambda), ylab = expression(gamma[0]))\r\nrect(xleft   = 0.8*res[1,1], xright = 1.2*res[nrow(res),1],\r\n     ybottom = interval[1,]$ci.low, ytop = interval[1,]$ci.up, col = ""lightblue"", border = NA)\r\nabline(h = interval[1,]$value, col = ""blue"", lty = 2)\r\nmatplot(res[,1], res[,3], type = ""p"", pch = 3, add = TRUE)\r\nfor(rnr in 1:nrow(res)){\r\n  lines(x = rep(res[rnr,1],2), y = c(res[rnr,2], res[rnr,4]))\r\n}\r\n\r\nmatplot(res[,1], res[,6], type = ""n"", log = ""x"", las = 1, ylim = c(0.045, 0.15),\r\n        xlab = expression(lambda), ylab = expression(gamma[1]))\r\nrect(xleft   = 0.8*res[1,1], xright = 1.2*res[nrow(res),1],\r\n     ybottom = interval[2,]$ci.low, ytop = interval[2,]$ci.up, col = ""lightblue"", border = NA)\r\nabline(h = interval[2,]$value, col = ""blue"", lty = 2)\r\nmatplot(res[,1], res[,6], type = ""p"", pch = 3, add = TRUE)\r\nfor(rnr in 1:nrow(res)){\r\n  lines(x = rep(res[rnr,1],2), y = c(res[rnr,5], res[rnr,7]))\r\n}\r\ndev.off()\r\n']",Statistical Analysis for Skinfaxi project Software supplement for estimation of age-specific incidence from age-specific prevalence without differential mortality (Skinfaxi project). Source code for use with the free statistical software R (The R Foundation for Statistical Computing).,3
Pathogen dynamics across the diversity of ageing,"Reproduction, mortality and immune function often change with age, but do not invariably deteriorate. Across the tree of life, there is extensive variation in age-specific performance and changes to key life-history traits. These changes occur on a spectrum from classic senescence, where performance declines with age, to juvenescence, where performance improves with age. Reproduction, mortality and immune function are also important factors influencing the spread of infectious disease, yet there exists no comprehensive investigation into how the ageing spectrum of these traits impacts epidemics.We used a model laboratory infection system to compile an ageing profile of a single organism, including traits directly linked to pathogen resistance, and those that should indirectly alter pathogen transmission by influencing demography. We then developed generalizable epidemiological models demonstrating that different patterns of ageing produce dramatically different transmission landscapes: in many cases ageing can reduce the probability of epidemics, but it can also promote severity. This work provides context and tools for use across taxa by empiricists, demographers and epidemiologists, advancing our ability to accurately predict factors contributing to epidemics, or the potential repercussions of senescence manipulation.",,"Pathogen dynamics across the diversity of ageing Reproduction, mortality and immune function often change with age, but do not invariably deteriorate. Across the tree of life, there is extensive variation in age-specific performance and changes to key life-history traits. These changes occur on a spectrum from classic senescence, where performance declines with age, to juvenescence, where performance improves with age. Reproduction, mortality and immune function are also important factors influencing the spread of infectious disease, yet there exists no comprehensive investigation into how the ageing spectrum of these traits impacts epidemics.We used a model laboratory infection system to compile an ageing profile of a single organism, including traits directly linked to pathogen resistance, and those that should indirectly alter pathogen transmission by influencing demography. We then developed generalizable epidemiological models demonstrating that different patterns of ageing produce dramatically different transmission landscapes: in many cases ageing can reduce the probability of epidemics, but it can also promote severity. This work provides context and tools for use across taxa by empiricists, demographers and epidemiologists, advancing our ability to accurately predict factors contributing to epidemics, or the potential repercussions of senescence manipulation.",3
Data from: Hidden shift of the ionome of plants exposed to elevated CO2 depletes minerals at the base of human nutrition,"Mineral malnutrition stemming from undiversified plant-based diets is a top global challenge. In C3 plants (e.g., rice, wheat), elevated concentrations of atmospheric carbon dioxide (eCO2) reduce protein and nitrogen concentrations, and can increase the total nonstructural carbohydrates (TNC; mainly starch, sugars). However, contradictory findings have obscured the effect of eCO2 on the ionome - the mineral and trace-element composition - of plants. Consequently, CO2-induced shifts in plant quality have been ignored in the estimation of the impact of global change on humans. This study shows that eCO2 reduces the overall mineral concentrations (-8%, 95% confidence interval: -9.1 to -6.9, p carbon:minerals in C3 plants. The meta-analysis of 7,761 observations, including 2,264 observations at state of the art FACE centers, covers 130 species/cultivars. The attained statistical power reveals that the shift is systemic and global. Its potential to exacerbate the prevalence of 'hidden hunger' and obesity is discussed.","['################################################################################\n#\' This R script contains one functions ""pwr.boot,"" which consists of 11 lines \n#\' of R code. The rest is comments.\n#\' Description:\n#\' 1. calculates the statistical power for a nonparametric bootstrapping \n#\' procedure for the following test: Is the mean of sample x different \n#\' from 0? \n#\' 2. calculates a two-sided confidence interval (CI) around the mean(x) and\n#\'    returns it with the sample size, the SEM, Cohen\'s d and the power.\n#\' This version of the script together with possible updates can be found on\n#\' GitHub.\n#\' URL: http://github.com/loladze/co2.git\n#\' Author: Irakli Loladze <loladze@asu.edu>\n#\' Cite as: Loladze, I. (2014) eLife 10.7554/eLife.02245\n#\' License: CC0\n################################################################################\n#\' Arguments:\n#\' \n#\' @ x is the given sample for which the power is calculated for\n#\' \n#\' @ aplha is Type I error for which power is calculated for. (default is 0.05)\n#\' \n#\' @ delta is Effect size for which power is calculated for (default is 0.05) \n#\'      Note that the default value is chosen for analyzing the CO2 data; \n#\'      generally, 0.05 would not be suitable for other data sets.\n#\'      \n#\' @ rep is Number of bootstrap iterations with replacement (default is 10,000)\n#\' \n#\' @ sdp is A priori estimate for the population standard deviation. If no \n#\' estimate is available, then set ""sdp"" to 0 and it will be ignored (see the \n#\' explanation below)\n#\'\n#\' Explanation for ""sdp"":\n#\' If sample x is small, then its variance can be << the population variance. \n#\' This will result in the overestimation of Cohen\'s d and power. To avoid such\n#\' overestimations, an a priori estimate of the population variance can be used \n#\' if available. For the CO2 dataset, we do have such an estimate, namely\n#\' the standard deviation of the entire dataset for minerals = ~0.21.\n#\' What counts as a ""small sample is determined by parameter ""small"":\n#\' \n#\' @ small (default is 20)\n#\' \n#\' Usage examples: power.boot(c(1,-2,5,-6,0,5), sdp=0)\n#\'                 power.boot(rnorm(100), rep=1000, sdp=0)     \n################################################################################\n\npwr.boot <- function(x, alpha=0.05, rep=10000, delta=0.05, sdp=0.21, small=20)\n  {\n################################################################################ \n#\' Gives out warning about bootstrapping very small samples (size < 7)\n################################################################################\n  if ( length(x) < 7 ) print(""Small samples can yield inaccurate results!"")\n################################################################################   \n#\' 1) bootstrapping (sampling with replacement) vector x\n#\' 2) calculating the mean for each bootstrap sample\n#\' 3) Replicating R times steps 1-2 \n#\' 4) The end result of steps 1-3, is ""mb"" vector of length R.\n#\'    The following command accomplishes all of the above steps:\n################################################################################     \n    mb <- replicate(rep, mean(sample(x, replace=T)))\n################################################################################     \n#\' calculating the CI. For default alpha = 0.05, the CI is (2.5%, 97.5%).\n################################################################################ \n    ci <- quantile(mb, c(alpha/2, 1-alpha/2))\n################################################################################\n#\' for small samples, adjusting sd using the sdp estimate if necessary\n################################################################################ \n    if (length(x) < small)  sdx <- max(sd(x), sdp)   else sdx <- sd(x)\n################################################################################\n#\' shifts x by the effect size delta (adjusted if necessary for small samples)\n################################################################################    \n    x.shift <- x + delta*(sd(x)/sdx)\n################################################################################    \n#\' bootstrapping the shifted vector \'x.shift\'\n################################################################################ \n    mb.shift <- replicate(rep, mean(sample(x.shift, replace=T)))\n################################################################################    \n#\' returns the fraction of the elements of mb.shift that fall outside of the CI \n#\' for mb\n################################################################################ \n    rejects= sum((mb.shift<ci[1] | mb.shift>ci[2])==T)\n################################################################################    \n#\' The power of the test is the fraction of rejects among all the bootstraps\n################################################################################\n    pw <- rejects/rep\n################################################################################    \n#\' grouping all the results: mean, CI, power, sample size, SEM, Cohen\'s d\n################################################################################ \n    results <- c(mean=mean(x), ci, power=pw, ""sample size""=length(x), \n                 SEM=sdx/sqrt(length(x)-1), ""Cohen\'s d""=delta/sdx)\n################################################################################\n#\' outputting all the results\n################################################################################ \n    results \n}']","Data from: Hidden shift of the ionome of plants exposed to elevated CO2 depletes minerals at the base of human nutrition Mineral malnutrition stemming from undiversified plant-based diets is a top global challenge. In C3 plants (e.g., rice, wheat), elevated concentrations of atmospheric carbon dioxide (eCO2) reduce protein and nitrogen concentrations, and can increase the total nonstructural carbohydrates (TNC; mainly starch, sugars). However, contradictory findings have obscured the effect of eCO2 on the ionome - the mineral and trace-element composition - of plants. Consequently, CO2-induced shifts in plant quality have been ignored in the estimation of the impact of global change on humans. This study shows that eCO2 reduces the overall mineral concentrations (-8%, 95% confidence interval: -9.1 to -6.9, p carbon:minerals in C3 plants. The meta-analysis of 7,761 observations, including 2,264 observations at state of the art FACE centers, covers 130 species/cultivars. The attained statistical power reveals that the shift is systemic and global. Its potential to exacerbate the prevalence of 'hidden hunger' and obesity is discussed.",3
Data from: Fine-scale spatial ecology drives kin selection relatedness among cooperating amoebae,"Cooperation among microbes is important for traits as diverse as antibiotic resistance, pathogen virulence, and sporulation. The evolutionary stability of cooperation against ""cheater"" mutants depends critically on the extent to which microbes interact with genetically similar individuals. The causes of this genetic social structure in natural microbial systems, however, are unknown. Here we show that social structure among cooperative Dictyostelium amoebae is driven by the population ecology of colonization, growth, and dispersal acting at spatial scales as small as fruiting bodies themselves. Despite the fact that amoebae disperse while grazing, all it takes to create substantial genetic clonality within multicellular fruiting bodies is a few millimeters distance between the cells colonizing a feeding site. Even adjacent fruiting bodies can consist of different genotypes. Soil populations of amoebae are sparse and patchily distributed at millimeter scales. The fine-scale spatial structure of cells and genotypes can thus account for the otherwise unexplained high genetic uniformity of spores in fruiting bodies from natural substrates. These results show how a full understanding of microbial cooperation requires understanding ecology and social structure at the small spatial scales microbes themselves experience.",,"Data from: Fine-scale spatial ecology drives kin selection relatedness among cooperating amoebae Cooperation among microbes is important for traits as diverse as antibiotic resistance, pathogen virulence, and sporulation. The evolutionary stability of cooperation against ""cheater"" mutants depends critically on the extent to which microbes interact with genetically similar individuals. The causes of this genetic social structure in natural microbial systems, however, are unknown. Here we show that social structure among cooperative Dictyostelium amoebae is driven by the population ecology of colonization, growth, and dispersal acting at spatial scales as small as fruiting bodies themselves. Despite the fact that amoebae disperse while grazing, all it takes to create substantial genetic clonality within multicellular fruiting bodies is a few millimeters distance between the cells colonizing a feeding site. Even adjacent fruiting bodies can consist of different genotypes. Soil populations of amoebae are sparse and patchily distributed at millimeter scales. The fine-scale spatial structure of cells and genotypes can thus account for the otherwise unexplained high genetic uniformity of spores in fruiting bodies from natural substrates. These results show how a full understanding of microbial cooperation requires understanding ecology and social structure at the small spatial scales microbes themselves experience.",3
Data from: Background colour matching increases with risk of predation in a colour-changing grasshopper,"Cryptic colouration can be adjusted to the local environment by physiological (rapid) change, and/or by morphological (slow) change. The threat-sensitivity hypothesis predicts that the degree of crypsis should respond to the risk of predation (assuming some cost to crypsis). This has not been studied for morphological colour changers, so we manipulated the colour of the rearing substrate (black versus white) and the perceived risk of predation (higher versus lower) for the grasshopper Sphingonotus azurescens. Over a period of several weeks, both nymphs and adults greatly adjusted the brightness of their body towards that of the substrate. Moreover, when individuals were exposed to a greater simulated predation risk (disturbance by hand), they became even more similar in brightness to their substrates, apparently augmenting their degree of crypsis. This study on a morphological colour changer shows that the degree of cryptic colouration (body brightness) is under individual control and appears to change adaptively in response to increased predation risk. In addition, based on analyses of systematic differences in colour in lab-reared offspring, we found indications that even in colour changers there is genetic variation in colouration among individuals, and that populations have diverged adaptively. Such integration of factors determining the cryptic phenotype improves our understanding of the natural selection and constraints imposed on crypsis, which influence both its optimization and evolution.","['setwd(\'your location of data and code\')\r\n############################################################\r\n# analysis for adults data\r\n\r\ndata<-read.table(""data_adults_.txt"",header=T,sep=""\\t"")\r\nstr(data)\r\nView(data)\r\n\r\n# make sure all variables are classified correctly\r\ndata$individual <- as.factor(data$individual)  \r\ndata$clutch <- as.factor(data$clutch)\r\ndata$population <- as.factor(data$population)\r\ndata$colour <- as.factor(data$colour)\r\ndata$stones <- as.factor(data$stones)\r\ndata$disturbance <- as.factor(data$disturbance)\r\ndata$sex <- as.factor(data$Sex)\r\n\r\n# Full model\r\nlibrary(lme4)\r\nstr(data)\r\nm1 <- lmer(b_dif2 ~ colour + window \r\n           + stones + stones:colour\r\n           + sex + sex:colour \r\n           + disturbance + disturbance:colour\r\n           + population + population:colour\r\n           + disturbance:sex\r\n           + (1 | clutch)\r\n           + (1 | individual), data = data, REML=FALSE)\r\nsummary(m1) \r\nanova(m1)\r\ncoef(m1)\r\nplot(fitted(m1),residuals(m1))\r\nhist(residuals(m1))\r\nqqnorm(residuals(m1))  \r\na<-coef(m1)\r\nstr(a)\r\n\r\nlibrary(effects)\r\nef1<-allEffects(m1)\r\nef1\r\nplot(ef1) # effects\r\n\r\n\r\n# same model 1 with REML=TRUE\r\nm1b <- lmer(b_dif2 ~ colour + window \r\n            + stones + stones:colour\r\n            + sex + sex:colour \r\n            + disturbance + disturbance:colour\r\n            + population + population:colour\r\n            + disturbance:sex\r\n            + (1 | clutch)\r\n            + (1 | individual), data = data, REML=TRUE)\r\nsummary(m1b) \r\nanova(m1b)\r\n\r\nm2 <- update(m1, .~.-(1 | clutch))\r\nm2 <- update(m2, .~.+ (1+colour | clutch))  # means: make the effect of colour treatment a random factor based on clutch, i.e. the slopes are random\r\nsummary(m2) \r\nanova(m2)\r\nAIC(m2,m1)\r\nanova(m2,m1)  # there is very little support for a random slope for clutch, so this is not included\r\n\r\n# testing importance of all variables in main model:\r\n\r\nm3 <- update(m1, .~.-window) # without window\r\nsummary(m3) \r\nanova(m3)\r\nAIC(m1,m3)\r\nanova(m1,m3)\r\n\r\nm4 <- update(m1, .~.-disturbance:colour) # without disturbance:colour\r\nsummary(m4) \r\nanova(m4)\r\nAIC(m1,m4)\r\nanova(m1,m4)\r\n\r\nm5 <- update(m1, .~.-population:colour) # without population:colour\r\nsummary(m5) \r\nanova(m5)\r\nAIC(m1,m5)\r\nanova(m1,m5)           \r\n\r\nm6 <- update(m1, .~.-stones:colour) # without stones:colour\r\nsummary(m6) \r\nanova(m6)\r\nAIC(m1,m6)\r\nanova(m1,m6)  \r\n\r\nm7 <- update(m1, .~.- disturbance:sex) # without disturbance:sex\r\nsummary(m7) \r\nanova(m7)\r\nAIC(m1,m7)\r\nanova(m1,m7)  \r\n\r\n\r\nm9 <- update(m1, .~.-sex:colour) # without sex:colour\r\nsummary(m9) \r\nanova(m9)\r\nAIC(m1,m9)\r\nanova(m1,m9) \r\n\r\n\r\nm10b <- update(m1, .~.-sex:colour) \r\nm10b <- update(m10b, .~.-stones:colour)\r\nm10b <- update(m10b, .~.-disturbance:colour)\r\nm10b <- update(m10b, .~.-population:colour) # without interactions involving colour\r\nsummary(m10b)\r\n\r\nm11 <- update(m10b, .~.-colour) # without colour and without interactions involving colour\r\nsummary(m11) \r\nanova(m11)\r\nAIC(m10b,m11)\r\nanova(m10b,m11) \r\n\r\nm13a <- update(m9, .~.-sex:disturbance)\r\nm13 <- update(m13a, .~.-sex)# without sex and its interactions\r\nsummary(m13) \r\nanova(m13)\r\nanova(m13a,m13)   \r\nAIC(m13a,m13) \r\n\r\nm16 <- update(m6, .~.-stones) # without stones, stones:colour\r\nsummary(m16) \r\nanova(m16)\r\nanova(m6,m16)    \r\nAIC(m6,m16) \r\n\r\nm17a <- update(m4, .~.-sex:disturbance) # without disturbance and its interactions\r\nm17 <- update(m17a, .~.-disturbance)\r\nsummary(m17) \r\nanova(m17)\r\nanova(m17a,m17)   \r\nAIC(m17a,m17) \r\n\r\nm18 <- update(m5, .~.-population) # without population, population:colour\r\nsummary(m18) \r\nanova(m18)\r\nanova(m5,m18)    \r\nAIC(m5,m18) \r\n\r\nm19 <- update(m1b, .~.-(1 | clutch)) # without clutch\r\nsummary(m19) \r\nanova(m19)\r\nsummary(m1b)\r\nanova(m1b,m19)  \r\nAIC(m1b,m19) \r\n\r\n\r\n############################################################\r\n# analysis for nymphs data\r\n\r\ndatab<-read.table(""data_nymphs_.txt"",header=T,sep=""\\t"")\r\nstr(datab)\r\n\r\n# make sure all variables are classified correctly\r\ndatab$window <- as.numeric(datab$window)  \r\ndatab$clutch <- as.factor(datab$clutch)\r\ndatab$population <- as.factor(datab$population)\r\ndatab$colour <- as.factor(datab$colour)  \r\ndatab$stones <- as.factor(datab$stones)\r\ndatab$disturbance <- as.factor(datab$disturbance)\r\n\r\nlibrary(lme4)\r\nstr(datab)\r\ndatab<-na.omit(datab)\r\nView(datab)\r\n\r\n# full model\r\nm1n <- lmer(b_dif2 ~ colour + window\r\n            + stones + stones:colour \r\n            + disturbance + disturbance:colour\r\n            + population + population:colour \r\n            + (1 | clutch), data = datab, REML=FALSE) \r\nsummary(m1n) \r\nanova(m1n)\r\ncoef(m1n)\r\nplot(fitted(m1n),residuals(m1n))\r\nhist(residuals(m1n))\r\nqqnorm(residuals(m1n))\r\n\r\nlibrary(effects)\r\nef1n<-allEffects(m1n)\r\nef1n\r\nplot(ef1n) # effects\r\n\r\n# same model 1n with REML=TRUE\r\nm1bn <- lmer(b_dif2 ~ colour + window\r\n             + stones + stones:colour \r\n             + disturbance + disturbance:colour\r\n             + population + population:colour \r\n             + (1 | clutch), data = datab, REML=TRUE)\r\nsummary(m1bn) \r\nanova(m1bn)\r\n\r\nm4n <- update(m1n, .~.-disturbance:colour) # without disturbance:colour\r\nsummary(m4n) \r\nanova(m4n)\r\nAIC(m1n,m4n)\r\nanova(m1n,m4n)\r\n\r\nm5n <- update(m1n, .~.-population:colour) # without population:colour\r\nsummary(m5n) \r\nanova(m5n)\r\nAIC(m1n,m5n)\r\nanova(m1n,m5n)           \r\n\r\nm6n <- update(m1n, .~.-stones:colour) # without stones:colour\r\nsummary(m6n) \r\nanova(m6n)\r\nAIC(m1n,m6n)\r\nanova(m1n,m6n)          \r\n\r\nm10n <- update(m1n, .~.-stones:colour)  \r\nm10n <- update(m10n, .~.-disturbance:colour)\r\nm10n <- update(m10n, .~.-population:colour) # without interactions\r\nsummary(m10n)\r\n\r\nm11n <- update(m10n, .~.-colour) # without colour and its interactions\r\nsummary(m11n) \r\nanova(m11n)\r\nAIC(m10n,m11n)\r\nanova(m10n,m11n) \r\n\r\nm12n <- update(m1n, .~.-window) # without window\r\nsummary(m12n) \r\nanova(m12n)\r\nAIC(m1n,m12n)\r\nanova(m1n,m12n) \r\n\r\nm16n <- update(m6n, .~.-stones) # without stones, stones:colour\r\nsummary(m16n) \r\nanova(m16n)\r\nanova(m6n,m16n)    \r\nAIC(m6n,m16n) \r\n\r\nm17n <- update(m4n, .~.-disturbance) # without disturbance, disturbance:colour\r\nsummary(m17n) \r\nanova(m17n)\r\nanova(m4n,m17n)    \r\nAIC(m4n,m17n) \r\n\r\nm18n <- update(m5n, .~.-population) # without population, population:colour\r\nsummary(m18n) \r\nanova(m18n)\r\nanova(m5n,m18n) \r\nAIC(m5n,m18n) \r\n\r\nm1cn <- lmer(b_dif2  ~ colour + window  # adding a second, neutral random effect to be able to test for the random term ""clutch""\r\n             + stones + stones:colour \r\n             + disturbance + disturbance:colour\r\n             + population + population:colour\r\n             + (1 | random)\r\n             + (1 | clutch), data = datab, REML=T)\r\nm1dn <- lmer(b_dif2 ~ colour + window\r\n             + stones + stones:colour \r\n             + disturbance + disturbance:colour\r\n             + population + population:colour\r\n             + (1 | random), data = datab, REML=T)    # without clutch\r\n\r\nanova(m1cn,m1dn)   \r\nAIC(m1cn,m1dn)\r\n\r\nm1cn2 <- update(m1cn, .~.-(1 | random)) # effect of the neutral ""random"" variable\r\nanova(m1cn,m1cn2) \r\nAIC(m1cn,m1cn2) # no effect\r\n\r\n# To test for heritable, presumably genetic effects on grasshopper phenotypes, one can run the same code \r\n# but replacing the dependent variable ""b_dif2"" (the difference in brightness) for ""grey.value"" (brightness = percentage reflectance)\r\n\r\n\r\n\r\n\r\n']","Data from: Background colour matching increases with risk of predation in a colour-changing grasshopper Cryptic colouration can be adjusted to the local environment by physiological (rapid) change, and/or by morphological (slow) change. The threat-sensitivity hypothesis predicts that the degree of crypsis should respond to the risk of predation (assuming some cost to crypsis). This has not been studied for morphological colour changers, so we manipulated the colour of the rearing substrate (black versus white) and the perceived risk of predation (higher versus lower) for the grasshopper Sphingonotus azurescens. Over a period of several weeks, both nymphs and adults greatly adjusted the brightness of their body towards that of the substrate. Moreover, when individuals were exposed to a greater simulated predation risk (disturbance by hand), they became even more similar in brightness to their substrates, apparently augmenting their degree of crypsis. This study on a morphological colour changer shows that the degree of cryptic colouration (body brightness) is under individual control and appears to change adaptively in response to increased predation risk. In addition, based on analyses of systematic differences in colour in lab-reared offspring, we found indications that even in colour changers there is genetic variation in colouration among individuals, and that populations have diverged adaptively. Such integration of factors determining the cryptic phenotype improves our understanding of the natural selection and constraints imposed on crypsis, which influence both its optimization and evolution.",3
"Data from: Length, body depth, and gape relationships and inference on piscivory among North American centrarchids","Species of Centrarchidae are major components of inland fisheries in much of North America. Thus, information gained from the assessment of interspecies interactions and/or quantifying predator-prey relationships is a useful tool for fisheries managers. Using preserved fish specimens (n = 717) from 20 species of centrarchids, we made measurements of total length (TL), standard length (SL), horizontal gape, and body depth for each individual. We fitted mathematical models that included horizontal gape and body depth as functions of TL and SL, and TL as a function of SL. Linear-regression-model fits were generally good (r2 = 0.764-0.998) for all 20 species, with 61 of 78 possible models having r2 values exceeding 0.90. Horizontal gapeSL (F3,702 = 77.18, P < 0.001) and body depth-SL (F3,702 = 91.79, P < 0.001) ratios differed significantly along a gradient that reflected the species' likelihood of piscivory. Slopes of TLSL regressions did not vary by species, which enabled development of a generalized TLSL model for centrarchids. Supplemental analyses supported that morphometric measurements had not been influenced significantly by preservation. Results of this study are useful to fisheries managers involved with understanding species interactions within centrarchid-dominated food webs, which are of high priority in most fisheries-management plans.","['## read in data, modify path as appropriate if replicating\r\nd<-read.csv(file=""C:\\\\Users\\\\avfernando\\\\Dropbox\\\\Mike Revision\\\\transfer\\\\CentGape\\\\Data\\\\allfishes2.csv"",header=TRUE)\r\nattach(d)\r\n\r\n## Linear regression of TL~SL (general)\r\nl12<-lm(TL~SL)\r\ns2<-summary(l12)\r\n\r\n## ANCOVA for TL~SL by species\r\na12<-aov(TL~SL*Species)\r\nsummary(a12)\r\n\r\nl1<-lm(TL~SL*Species)\r\ns1<-summary(l1)\r\n\r\n## ANOVA for Hgape and Body Depth against Likelihood of Piscivory\r\n\r\nhrat<-Hgape/SL\r\ns3<-summary(aov(hrat~LikePisc))\r\nt3<-TukeyHSD(aov(hrat~LikePisc))\r\n\r\ndrat<-Depth/SL\r\ns4<-summary(aov(drat~LikePisc))\r\nt4<-TukeyHSD(aov(drat~LikePisc))\r\n\r\n## ANCOVA for preserved/live Bluegill\r\n## modify path if replicating\r\n\r\nd<-read.csv(file=""C:\\\\Users\\\\avfernando\\\\Dropbox\\\\Mike Revision\\\\transfer\\\\CentGape\\\\Analysis\\\\Bluegill_preservedvslive.csv"",header=TRUE)\r\nattach(d)\r\n\r\nhrat<-Gape/SL\r\ns5<-summary(aov(hrat~Source))\r\nt5<-TukeyHSD(aov(hrat~Source))\r\n\r\ndrat<-Depth/SL\r\ns6<-summary(aov(drat~Source))\r\nt6<-TukeyHSD(aov(drat~Source))\r\n']","Data from: Length, body depth, and gape relationships and inference on piscivory among North American centrarchids Species of Centrarchidae are major components of inland fisheries in much of North America. Thus, information gained from the assessment of interspecies interactions and/or quantifying predator-prey relationships is a useful tool for fisheries managers. Using preserved fish specimens (n = 717) from 20 species of centrarchids, we made measurements of total length (TL), standard length (SL), horizontal gape, and body depth for each individual. We fitted mathematical models that included horizontal gape and body depth as functions of TL and SL, and TL as a function of SL. Linear-regression-model fits were generally good (r2 = 0.764-0.998) for all 20 species, with 61 of 78 possible models having r2 values exceeding 0.90. Horizontal gapeSL (F3,702 = 77.18, P < 0.001) and body depth-SL (F3,702 = 91.79, P < 0.001) ratios differed significantly along a gradient that reflected the species' likelihood of piscivory. Slopes of TLSL regressions did not vary by species, which enabled development of a generalized TLSL model for centrarchids. Supplemental analyses supported that morphometric measurements had not been influenced significantly by preservation. Results of this study are useful to fisheries managers involved with understanding species interactions within centrarchid-dominated food webs, which are of high priority in most fisheries-management plans.",3
Plant defence to sequential attack is adapted to prevalent herbivores,"Plants have evolved plastic defence strategies to deal with uncertainty of when, by which species and in which order attack by herbivores will take place. However, the responses to current herbivore attack may come with a cost of compromising resistance to other, later arriving herbivores. Due to antagonistic cross-talk between physiological regulation of plant resistance to phloem-feeding and leaf-chewing herbivores, the feeding guild of the initial herbivore is considered to be the primary factor determining whether resistance to subsequent attack is compromised. We show that, by investigating 90 pair-wise insect-herbivore interactions among ten different herbivore species, resistance of the annual plant Brassica nigra to a later arriving herbivore species is not explained by feeding guild of the initial attacker. Instead, the prevalence of herbivore species that arrive on induced plants as approximated by three years of season-long insect community assessments in the field explained cross-resistance. Plants maintained resistance to prevalent herbivores in common patterns of herbivore arrival and compromises in resistance especially occurred for rare patterns of herbivore attack. We conclude that plants tailor induced defence strategies to deal with common patterns of sequential herbivore attack and anticipate arrival of the most prevalent herbivores.","['# Libraries directories ####\r\n#   Set general working directory\r\n\r\n# libraries\r\nlibrary(readxl)\r\nlibrary(ggplot2)\r\nlibrary(gridExtra)\r\nlibrary(emmeans)\r\nlibrary(multcomp)\r\nlibrary(multcompView)\r\nlibrary(car)\r\nlibrary(nlme)\r\nlibrary(lme4)\r\n\r\n# Expression analysis ----\r\n# Read Data\r\nM.Data <- read_excel(""Raw_data_Mertens_et_al_2021_NaturePlants.xlsx"", sheet = ""Expression"")\r\nSpec.Info <- read_excel(""Raw_data_Mertens_et_al_2021_NaturePlants.xlsx"", sheet = ""Supporting"")\r\n\r\n# Set data types\r\nM.Data[c(1:3)] <- lapply(M.Data[c(1:3)], factor)\r\nSpec.Info[c(1:4)] <- lapply(Spec.Info[c(1:4)], factor)\r\n\r\n# Merge data\r\ncolnames(M.Data)[2] <- ""Abbreviation""\r\nM.Data.Annot <- merge(M.Data,Spec.Info[,2:4], by = ""Abbreviation"")\r\nM.Data.Annot$FunctionalGroup <-  as.factor(paste0(M.Data.Annot$DietBreadth,M.Data.Annot$FeedingGuild))\r\n\r\n# Set order\r\nM.Data$Abbreviation <- factor(M.Data$Abbreviation, \r\n                              levels = c(""Ctrl"", ""Ag"", ""Mb"",""Ar"", ""Pb"", ""Pr"",""Px"",\r\n                                         ""Mp"",""Mpn"",""Bb"",""Le""))\r\n\r\nM.Data.Annot$FunctionalGroup <- factor(M.Data.Annot$FunctionalGroup, \r\n                                       levels = c(""GeneralistChewer"",""SpecialistChewer"",\r\n                                                  ""GeneralistSap_feeder"",""SpecialistSap_feeder""))\r\n\r\n# Model LOX2 Treatment\r\nM.Full <- glm(LOX2 ~ Abbreviation * TP,\r\n              data = M.Data, \r\n              family = Gamma (link = ""log"" ))\r\n\r\nM.Optimal <- glm(LOX2 ~ Abbreviation + TP,\r\n                 data = M.Data, \r\n                 family = Gamma (link = ""log"" ))\r\n\r\nM.Null <- glm(LOX2 ~ 1,\r\n              data = M.Data, \r\n              family = Gamma (link = ""log"" ))\r\n\r\ncbind(AIC(M.Full), AIC(M.Optimal), AIC(M.Null))\r\nAnova(M.Optimal)\r\nemmeans(ref_grid(M.Optimal, transform = ""response""), pairwise ~ Abbreviation | TP)\r\n\r\n# Model PR1 Treatment\r\nM.Full <- gls(PR1 ~ Abbreviation * TP,\r\n              method = ""ML"",\r\n              data = M.Data)\r\n\r\n\r\nM.Null <- gls(PR1 ~ 1,\r\n              method = ""ML"",\r\n              data = M.Data)\r\n\r\ncbind(AIC(M.Full), AIC(M.Null))\r\nAnova(M.Full)\r\n\r\n# Model LOX2 Functional Traits\r\n\r\nM.Full <- glmer(LOX2 ~ DietBreadth * FeedingGuild * TP +\r\n                  (1|Abbreviation),\r\n                data = M.Data.Annot,\r\n                family = Gamma  (link = ""log"" ))\r\n\r\n\r\nM.Optimal <- glmer(LOX2 ~ DietBreadth * FeedingGuild + TP +\r\n                     (1|Abbreviation),\r\n                   data = M.Data.Annot,\r\n                   family = Gamma  (link = ""log"" ))\r\n\r\nM.Null <- glmer(LOX2 ~ 1 +\r\n                  (1|Abbreviation),\r\n                data = M.Data.Annot,\r\n                family = Gamma  (link = ""log"" ))\r\n\r\ncbind(AIC(M.Full), AIC(M.Optimal), AIC(M.Null))\r\nAnova(M.Optimal)\r\nemmeans(ref_grid(M.Optimal, transform = ""response""), pairwise ~ Guild * Specialisation | TP)\r\n\r\n# Model PR1 Functional Traits\r\n\r\nM.Full <- glmer(PR1 ~ DietBreadth * FeedingGuild + TP +\r\n                  (1|Abbreviation),\r\n                data = M.Data.Annot,\r\n                family = Gamma(link = ""log"" ))\r\n\r\n\r\nM.Optimal <- glmer(PR1 ~ TP * FeedingGuild + DietBreadth +\r\n                     (1|Abbreviation),\r\n                   data = M.Data.Annot,\r\n                   family = Gamma(link = ""log"" ))\r\n\r\nM.Null <- glmer(PR1 ~  1 +\r\n                  (1|Abbreviation),\r\n                data = M.Data.Annot,\r\n                family = Gamma)\r\n\r\ncbind(AIC(M.Full), AIC(M.Optimal), AIC(M.Null))\r\nAnova(M.Optimal)\r\nemmeans(ref_grid(M.OPTIMAL, transform = ""response""), pairwise ~ Guild * Specialisation  | TP)\r\n\r\n\r\n# Plot data\r\nM.Data <- droplevels(subset(M.Data, TP == 96))\r\nplots <- vector(mode = ""list"", length = 4)\r\n\r\n# Ggplot custom theme \r\ncustomPlot = list(\r\n  theme(plot.title = element_text(face = ""italic"", hjust = 0.5),\r\n        axis.text= element_text( color=""black""),\r\n        axis.line = element_line(colour = ""black""),\r\n        panel.background = element_rect(fill = NA, colour = ""black""),\r\n        panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank(), \r\n        strip.background = element_rect(colour = ""black"", fill = NA),\r\n        strip.text = element_text(colour = ""black""),\r\n        legend.key = element_rect(fill = NA, color = NA))\r\n)\r\n# LOX2 Treatments\r\nLOX2_T <- ggplot(data = M.Data, aes(x = Abbreviation, y = LOX2)) +\r\n  geom_boxplot(aes(alpha = 0.3), outlier.shape = 0, outlier.colour = ""white"") + \r\n  geom_jitter (width = 0.05, height = 0) +\r\n  #scale_y_continuous(breaks = seq(0, 16, by = 2), limits = c(0, 16), expand = c(0, 0)) +\r\n  customPlot\r\nplots[[1]] <- LOX2_T\r\n\r\n# PR1 Treatments\r\nPR1_T <- ggplot(data = M.Data, aes(x = Abbreviation, y = PR1)) +\r\n  geom_boxplot(aes(alpha = 0.3), outlier.shape = 0, outlier.colour = ""white"") + \r\n  geom_jitter (width = 0.05, height = 0) +\r\n  #scale_y_continuous(breaks = seq(0, 7, by = 1), limits = c(0, 7.5), expand = c(0, 0)) +\r\n  customPlot\r\n\r\nplots[[2]] <- PR1_T\r\n\r\n# LOX2 Functional traits\r\nLOX2_F <- ggplot(data = M.Data.Annot, aes(x = FunctionalGroup, y = LOX2)) +\r\n  geom_boxplot(aes(alpha = 0.3), outlier.shape = 0, outlier.colour = ""white"") + \r\n  geom_jitter (width = 0.15, height = 0) +\r\n  #scale_y_continuous(breaks = seq(0, 16, by = 2), limits = c(0, 16), expand = c(0, 0)) +\r\n  customPlot\r\n\r\nplots[[3]] <- LOX2_F\r\n\r\n# PR1 Functional traits\r\nPR1_F <- ggplot(data = M.Data.Annot, aes(x = FunctionalGroup, y = PR1)) +\r\n  geom_boxplot(aes(alpha = 0.3), outlier.shape = 0, outlier.colour = ""white"") + \r\n  geom_jitter (width = 0.15, height = 0) +\r\n  #scale_y_continuous(breaks = seq(0, 7, by = 1), limits = c(0, 7.5), expand = c(0, 0)) +\r\n  customPlot\r\n\r\nplots[[4]] <- PR1_F\r\n\r\n# Aggregate plots\r\n\r\nExpressionData <- marrangeGrob(grobs = list (LOX2_T, PR1_T, LOX2_F, PR1_F), nrow = 2, ncol = 2)\r\nExpressionData\r\n\r\nrm(list = ls())\r\ngraphics.off()\r\n\r\n# Performance overview ----\r\nPref.Data <- read_excel(""Raw_data_Mertens_et_al_2021_NaturePlants.xlsx"", sheet = ""Performance"")\r\nSpec.Info <- read_excel(""Raw_data_Mertens_et_al_2021_NaturePlants.xlsx"", sheet = ""Supporting"")\r\nPref.Data[c(1:4)] <- lapply(Pref.Data[c(1:4)], factor)\r\nSpec.Info[c(2,4)] <- lapply(Spec.Info[c(2,4)], factor)\r\n\r\n# Merge data\r\ncolnames(Pref.Data)[1] <- ""Abbreviation""\r\nPref.Data.Annot <- merge(Pref.Data,Spec.Info[,2:4], by = ""Abbreviation"")\r\nCtrls <- Pref.Data[which(Pref.Data$Abbreviation == ""Ctrl""),]\r\nCtrls.Anot <- data.frame(Ctrls,\r\n                         ""DietBreadth"" = ""Ctrl"",\r\n                         ""FeedingGuild"" = ""Ctrl"")\r\n\r\nPreformance_All <- rbind(Pref.Data.Annot,Ctrls.Anot)\r\n\r\nall_blocks <- c(""B_Ag"", ""B_Mb"",""B_Ar"", ""B_Pb"", ""B_Pr"", ""B_Px"", ""B_Mp"", ""B_Mpn"", ""B_Bb"", ""B_Le"")\r\nall_blocks_names <- c(""Ag"", ""Mb"", ""Ar"", ""Pb"", ""Pr"", ""Px"", ""Mp"", ""Mpn"", ""Bb"", ""Le"")\r\n\r\n# Colors on feeding type\r\nColorInducer <- c(""#c2c2c2"",rep(""#00b050"",6),rep(""#ffbf00"",4))\r\n\r\n# Plotting\r\nNumber_measurements <- vector(mode = ""list"", length = length(all_blocks_names))\r\nplots <- vector(mode = ""list"", length = length(all_blocks_names))\r\n\r\n# For plotting in illustrator\r\nfor(i in 1:length(all_blocks_names)) {\r\n  \r\n  B_Loop <- subset(Preformance_All, Preformance_All$Receiver  == all_blocks_names[i] )\r\n  Counts <- as.data.frame(table(B_Loop$Abbreviation ))\r\n  Counts[,3] <- paste(Counts[,2])\r\n  Number_measurements[[i]] <- Counts[,3]\r\n  \r\n  p <- ggplot()+\r\n    geom_boxplot(data = B_Loop, aes(x = Abbreviation , \r\n                                    y = Response , \r\n                                    fill = Abbreviation ))+\r\n    scale_fill_manual(values = ColorInducer, labels = Number_measurements[[i]])+\r\n    labs(title = paste(all_blocks_names[i]))+\r\n    theme(panel.grid.major = element_blank(), \r\n          panel.grid.minor = element_blank(),\r\n          panel.background = element_blank(),\r\n          axis.title.x = element_blank(),\r\n          legend.key = element_rect(fill = ""white""),\r\n          axis.line = element_line(colour = ""black""),\r\n          axis.text.x = element_text(colour = "" black"", angle = 90),\r\n          axis.text.y = element_text(colour = "" black""))\r\n  \r\n  plots[[i]] <- p\r\n  \r\n}\r\n\r\nPerformance_Plots <- marrangeGrob(plots, nrow=2, ncol=5, top = ""Feeding type of Inducer"")\r\nPerformance_Plots\r\n\r\nrm(list = ls())\r\ngraphics.off()\r\n\r\n# Estimate Performance on non-treated ----\r\nPref.Data <- read_excel(""Raw_data_Mertens_et_al_2021_NaturePlants.xlsx"", sheet = ""Performance"")\r\nPref.Data[c(1:4)] <- lapply(Pref.Data[c(1:4)], factor)\r\n\r\n# Subset for modelling mean Ctrl Performance\r\n\r\nB_Ag <- subset(Pref.Data, Receiver == ""Ag"")\r\nB_Mb <- subset(Pref.Data, Receiver == ""Mb"")\r\nB_Ar <- subset(Pref.Data, Receiver == ""Ar"")\r\nB_Pb <- subset(Pref.Data, Receiver == ""Pb"")\r\nB_Pr <- subset(Pref.Data, Receiver == ""Pr"")\r\nB_Px <- subset(Pref.Data, Receiver == ""Px"")\r\nB_Mp <- subset(Pref.Data, Receiver == ""Mp"")\r\nB_Mpn <- subset(Pref.Data, Receiver == ""Mpn"")\r\nB_Bb <- subset(Pref.Data, Receiver == ""Bb"")\r\nB_Le <- subset(Pref.Data, Receiver == ""Le"")\r\n\r\n# Ag\r\nAg_Estim <- glmer(Response ~ Inducer+\r\n                    (1|Label ) + (1|TP),\r\n                  data = B_Ag,\r\n                  family = Gamma ( link = ""log""))\r\n\r\nAg_means <- data.frame(emmeans (ref_grid(Ag_Estim, transform = ""response""),  ~ Inducer))[,c(1:2)]\r\nAg_Ctrl <- Ag_means[which(Ag_means$Inducer == ""Ctrl""),2]\r\n\r\n# Mb\r\nMb_Estim <- glmer(Response ~ Inducer+\r\n                    (1|Label ) + (1|TP),\r\n                  data = B_Mb,\r\n                  family = Gamma ( link = ""log""))\r\n\r\nMb_means <- data.frame(emmeans (ref_grid(Mb_Estim, transform = ""response""),  ~ Inducer))[,c(1:2)]\r\nMb_Ctrl <- Mb_means[which(Mb_means$Inducer == ""Ctrl""),2]\r\n\r\n# Ar\r\nAr_Estim <- glmer(Response ~ Inducer+\r\n                    (1|Label ) + (1|TP),\r\n                  data = B_Ar,\r\n                  control = lmeControl(opt=\'optim\'),\r\n                  family = Gamma ( link = ""log""))\r\n\r\nAr_means <- data.frame(emmeans (ref_grid(Ar_Estim, transform = ""response""),  ~ Inducer))[,c(1:2)]\r\nAr_Ctrl <- Ar_means[which(Ar_means$Inducer == ""Ctrl""),2]\r\n\r\n# Pb\r\nPb_Estim <- lme(Response ~ Inducer,\r\n                random  = ~ 1| Label,\r\n                weights = varIdent(form= ~ 1 | Inducer),\r\n                data = B_Pb,\r\n                control = lmeControl(opt=\'optim\'))\r\n\r\nPb_means <- data.frame(emmeans (ref_grid(Pb_Estim, transform = ""response""),  ~ Inducer))[,c(1:2)]\r\nPb_Ctrl <- Pb_means[which(Pb_means$Inducer == ""Ctrl""),2]\r\n\r\n# Pr\r\nPr_Estim <- glmer(Response ~ Inducer+\r\n                    (1|Label ) + (1|TP),\r\n                  data = B_Pr,\r\n                  control = lmeControl(opt=\'optim\'),\r\n                  family = Gamma ( link = ""log""))\r\n\r\nPr_means <- data.frame(emmeans (ref_grid(Pr_Estim, transform = ""response""),  ~ Inducer))[,c(1:2)]\r\nPr_Ctrl <- Pr_means[which(Pr_means$Inducer == ""Ctrl""),2]\r\n\r\n# Px\r\nPx_Estim <- glmer(Response ~ Inducer+\r\n                    (1|Label ) + (1|TP),\r\n                  data = B_Px,\r\n                  control = lmeControl(opt=\'optim\'),\r\n                  family = Gamma ( link = ""log""))\r\n\r\nPx_means <- data.frame(emmeans (ref_grid(Px_Estim, transform = ""response""),  ~ Inducer))[,c(1:2)]\r\nPx_Ctrl <- Px_means[which(Px_means$Inducer == ""Ctrl""),2]\r\n\r\n# Mp\r\nMp_Estim <- lme(Response ~ Inducer,\r\n                random  = ~ 1| Label,\r\n                weights = varIdent(form= ~ 1 | Inducer),\r\n                data = B_Mp,\r\n                control = lmeControl(opt=\'optim\'))\r\n\r\nMp_means <- data.frame(emmeans (ref_grid(Mp_Estim, transform = ""response""),  ~ Inducer))[,c(1:2)]\r\nMp_Ctrl <- Mp_means[which(Mp_means$Inducer == ""Ctrl""),2]\r\n\r\n# Mpn\r\nMpn_Estim <- lme(Response ~ Inducer,\r\n                 random  = ~ 1| Label,\r\n                 weights = varIdent(form= ~ 1 | Inducer),\r\n                 data = B_Mpn,\r\n                 control = lmeControl(opt=\'optim\'))\r\n\r\nMpn_means <- data.frame(emmeans (ref_grid(Mpn_Estim, transform = ""response""),  ~ Inducer))[,c(1:2)]\r\nMpn_Ctrl <- Mpn_means[which(Mpn_means$Inducer == ""Ctrl""),2]\r\n\r\n# Bb\r\nBb_Estim <- lme(Response ~ Inducer,\r\n                random  = ~ 1| Label,\r\n                weights = varIdent(form= ~ 1 | Inducer),\r\n                data = B_Bb,\r\n                control = lmeControl(opt=\'optim\'))\r\n\r\nBb_means <- data.frame(emmeans (ref_grid(Bb_Estim, transform = ""response""),  ~ Inducer))[,c(1:2)]\r\nBb_Ctrl <- Bb_means[which(Bb_means$Inducer == ""Ctrl""),2]\r\n\r\n# Le\r\nLe_Estim <- lme(Response ~ Inducer,\r\n                random  = ~ 1| Label,\r\n                weights = varIdent(form= ~ 1 | Inducer),\r\n                data = B_Le,\r\n                control = lmeControl(opt=\'optim\'))\r\n\r\nLe_means <- data.frame(emmeans (ref_grid(Le_Estim, transform = ""response""),  ~ Inducer))[,c(1:2)]\r\nLe_Ctrl <- Le_means[which(Le_means$Inducer == ""Ctrl""),2]\r\n\r\nrm(list = ls())\r\ngraphics.off()\r\n\r\n# LnRR per Treatment ----\r\n\r\nLnRR_Data <- read_excel(""Raw_data_Mertens_et_al_2021_NaturePlants.xlsx"", sheet = ""Performance_LnRR"")\r\nLnRR_Data[c(1:4)] <- lapply(LnRR_Data[c(1:4)], factor)\r\n\r\nConspecific_measurements <- which(LnRR_Data[,1] == LnRR_Data[,2])\r\nLnRR_Data_F <- LnRR_Data[-( which(LnRR_Data[,1] == LnRR_Data[,2])),]\r\n\r\nctrl <- lmeControl(opt=\'optim\')\r\n\r\nRM.Full <- lme(LnRR ~ Receiver * Inducer, \r\n               random =  (~1|Label),\r\n               control = ctrl,\r\n               weights = varIdent(form= ~ 1 | Receiver),\r\n               method = ""ML"",\r\n               data = LnRR_Data_F)\r\n\r\nRM.Null <- lme(LnRR ~ 1, \r\n               random =  (~1|Label),\r\n               control = ctrl,\r\n               weights = varIdent(form= ~ 1 | Receiver),\r\n               method = ""ML"",\r\n               data = LnRR_Data)\r\n\r\ncbind(AIC(RM.Full), AIC(RM.Null))\r\nAnova(RM.Full)\r\nemmeans(ref_grid(RM.Full, transform = ""response""), pairwise ~ Receiver * Inducer)\r\n\r\nrm(list = ls())\r\ngraphics.off()\r\n\r\n# LnRR among functional traits ----\r\nLnRR_Data <- read_excel(""Raw_data_Mertens_et_al_2021_NaturePlants.xlsx"", sheet = ""Performance_LnRR"")\r\nSpec.Info <- read_excel(""Raw_data_Mertens_et_al_2021_NaturePlants.xlsx"", sheet = ""Supporting"")\r\nLnRR_Data[c(1:4)] <- lapply(LnRR_Data[c(1:4)], factor)\r\nSpec.Info[c(1:4)] <- lapply(Spec.Info[c(1:4)], factor)\r\n\r\nExp.Species <- Spec.Info[Spec.Info[,""Abbreviation""] != ""-"",]\r\n\r\nConspecific_measurements <- which(LnRR_Data[,1] == LnRR_Data[,2])\r\nMpn_Mp_measurements <- which(LnRR_Data[,1] == ""Mpn"" & LnRR_Data[,2] == ""Mp"" )\r\nMp_Mpn_measurements <- which(LnRR_Data[,1] == ""Mp"" & LnRR_Data[,2] == ""Mpn"" )\r\ndrop_measurements <- c(Conspecific_measurements,Mpn_Mp_measurements,Mp_Mpn_measurements)\r\n\r\nLnRR_Data_Funct <- LnRR_Data[-(drop_measurements),]\r\nLnRR_Data_Funct <- merge(LnRR_Data_Funct,Exp.Species[,c(""Abbreviation"", ""DietBreadth"",""FeedingGuild"")], \r\n                         by.x=c(""Inducer""), by.y=c(""Abbreviation""))\r\n\r\ncolnames(LnRR_Data_Funct)[c(6,7)] <- c(""SpecInd"",""GuildInd"")\r\n\r\nLnRR_Data_Funct <- merge(LnRR_Data_Funct,Exp.Species[,c(""Abbreviation"", ""DietBreadth"",""FeedingGuild"")], \r\n                        by.x=c(""Receiver""), by.y=c(""Abbreviation""))\r\n\r\ncolnames(LnRR_Data_Funct)[c(8,9)] <- c(""SpecRec"",""GuildRec"")\r\n\r\n# Compare fit random intercepts\r\nM1 <- lmer(LnRR ~ SpecRec  * GuildInd * SpecInd * GuildRec + \r\n                  (1|Label) + (1|Receiver) + (1|Inducer), \r\n                REML = F,\r\n                data = LnRR_Data_Funct)\r\n\r\nM2 <- lmer(LnRR ~ SpecRec  * GuildInd * SpecInd * GuildRec + \r\n             (1|Label) + (1|Inducer), \r\n           REML = F,\r\n           data = LnRR_Data_Funct)\r\n\r\nM3 <- lmer(LnRR ~ SpecRec  * GuildInd * SpecInd * GuildRec + \r\n             (1|Label) + (1|Receiver), \r\n           REML = F,\r\n           data = LnRR_Data_Funct)\r\n\r\n\r\nM4 <- lmer(LnRR ~ SpecRec  * GuildInd * SpecInd * GuildRec + \r\n                  (1|Label) , \r\n                REML = F,\r\n                data = LnRR_Data_Funct)\r\nM5 <- lmer(LnRR ~ 1 + \r\n             (1|Label) + (1|Receiver) + (1|Inducer), \r\n           REML = F,\r\n           data = LnRR_Data_Funct)\r\n\r\ncbind(AIC(M1),AIC(M2),AIC(M3),AIC(M4),AIC(M5))\r\n\r\n# Not adjusted for species identity \r\nRM.Full <- lmer(LnRR ~ SpecRec  * GuildInd * SpecInd * GuildRec + \r\n                  (1|Label) , \r\n                REML = F,\r\n                data = LnRR_Data_Funct)\r\n\r\nRM.Optimal <- lme(LnRR ~ GuildInd * SpecRec,\r\n                  random = ~1|Label,\r\n                  method = ""ML"",\r\n                  weights = varIdent(form= ~ 1 | GuildInd * SpecRec),\r\n                  data = LnRR_Data_Funct)\r\n\r\nRM.Null <- lmer(LnRR ~ 1 +\r\n                  (1|Label) , \r\n                REML = F,\r\n                data = LnRR_Data_Funct)\r\n\r\ncbind(AIC(RM.Full),AIC(RM.Optimal), AIC(RM.Null))\r\nAnova(RM.Optimal)\r\nemmeans(ref_grid(RM.Optimal, transform = ""response""), pairwise ~ GuildInd * SpecRec)\r\n\r\n# Adjusted for species identity \r\nRM.BothCovar <- lmer(LnRR ~ \r\n                       SpecRec  * GuildInd +\r\n                       (1|Label) + (1|Receiver) + (1|Inducer), \r\n                     REML = F,\r\n                     data = LnRR_Data_Funct)\r\n\r\nRM.IndCovar <- lmer(LnRR ~ \r\n                      SpecRec  * GuildInd +\r\n                      (1|Label) + (1|Inducer), \r\n                    REML = F,\r\n                    data = LnRR_Data_Funct)\r\n\r\nRM.RecCovar <- lmer(LnRR ~ \r\n                      SpecRec  * GuildInd +\r\n                      (1|Label) + (1|Receiver), \r\n                    REML = F,\r\n                    data = LnRR_Data_Funct)\r\n\r\ncbind(AIC(RM.IndCovar), AIC(RM.RecCovar), AIC(RM.BothCovar))\r\nemmeans(ref_grid(RM.BothCovar, transform = ""response""), pairwise ~ GuildInd * SpecRec)\r\n\r\n# Plot data\r\n# Ggplot custom theme \r\ncustomPlot = list(\r\n  theme(plot.title = element_text(face = ""italic"", hjust = 0.5),\r\n        axis.text= element_text( color=""black""),\r\n        axis.line = element_line(colour = ""black""),\r\n        panel.background = element_rect(fill = NA, colour = ""black""),\r\n        panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank(), \r\n        strip.background = element_rect(colour = ""black"", fill = NA),\r\n        strip.text = element_text(colour = ""black""),\r\n        legend.key = element_rect(fill = NA, color = NA))\r\n)\r\n\r\nLnRR_Data_Funct$IndRec_Functional <- paste0(LnRR_Data_Funct$SpecRec,\r\n                                                      LnRR_Data_Funct$GuildRec,\r\n                                                      LnRR_Data_Funct$SpecInd,\r\n                                                      LnRR_Data_Funct$GuildInd)\r\n\r\nlevels(LnRR_Data_Funct$IndRec_Functional)\r\nLnRR_Data_Funct$IndRec_Functional <- factor(LnRR_Data_Funct$IndRec_Functional, \r\n                              levels = c(""GeneralistChewerGeneralistChewer"", \r\n                                         ""GeneralistChewerSpecialistChewer"", \r\n                                         ""GeneralistChewerGeneralistSap_feeder"",\r\n                                         ""GeneralistChewerSpecialistSap_feeder"", \r\n                                         ""GeneralistSap_feederGeneralistChewer"", \r\n                                         ""GeneralistSap_feederSpecialistChewer"",\r\n                                         ""GeneralistSap_feederGeneralistSap_feeder"",\r\n                                         ""GeneralistSap_feederSpecialistSap_feeder"",\r\n                                         ""SpecialistChewerGeneralistChewer"",\r\n                                         ""SpecialistChewerSpecialistChewer"",\r\n                                         ""SpecialistChewerGeneralistSap_feeder"",\r\n                                         ""SpecialistChewerSpecialistSap_feeder"",\r\n                                         ""SpecialistSap_feederGeneralistChewer"",\r\n                                         ""SpecialistSap_feederSpecialistChewer"",\r\n                                         ""SpecialistSap_feederGeneralistSap_feeder"",\r\n                                         ""SpecialistSap_feederSpecialistSap_feeder""))\r\n\r\n\r\nggplot(data = LnRR_Data_Funct, aes(y = LnRR, x = IndRec_Functional)) +\r\n  geom_boxplot( outlier.shape = 0, outlier.colour = ""white"") + \r\n  geom_jitter (width = 0.05, height = 0) +\r\n  geom_hline(yintercept = 0) + \r\n  #scale_y_continuous(breaks = seq(0, 16, by = 2), limits = c(0, 16), expand = c(0, 0)) +\r\n  customPlot + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r\n\r\nrm(list = ls())\r\ngraphics.off()\r\n\r\n# Performance and prevalence in the field ----\r\n\r\nLnRR_Data <- read_excel(""Raw_data_Mertens_et_al_2021_NaturePlants.xlsx"", sheet = ""Performance_LnRR"")\r\nSpec.Info <- read_excel(""Raw_data_Mertens_et_al_2021_NaturePlants.xlsx"", sheet = ""Supporting"")\r\nPrevalence <- read_excel(""Raw_data_Mertens_et_al_2021_NaturePlants.xlsx"", sheet = ""Prevalence"")\r\n\r\nLnRR_Data[c(1:4)] <- lapply(LnRR_Data[c(1:4)], factor)\r\nSpec.Info[c(1:4)] <- lapply(Spec.Info[c(1:4)], factor)\r\nPrevalence[c(1:2)] <- lapply(Prevalence[c(1:2)], factor)\r\n\r\nExp.Species <- Spec.Info[Spec.Info[,""Abbreviation""] != ""-"",]\r\nExp.Prevalence <- Prevalence[Prevalence[,""Abbreviation""] != ""-"",]\r\n\r\n\r\nConspecific_measurements <- which(LnRR_Data[,1] == LnRR_Data[,2])\r\nMpn_Mp_measurements <- which(LnRR_Data[,1] == ""Mpn"" & LnRR_Data[,2] == ""Mp"" )\r\nMp_Mpn_measurements <- which(LnRR_Data[,1] == ""Mp"" & LnRR_Data[,2] == ""Mpn"" )\r\ndrop_measurements <- c(Conspecific_measurements,Mpn_Mp_measurements,Mp_Mpn_measurements)\r\n\r\nLnRR_Data_Funct <- LnRR_Data[-(drop_measurements),]\r\nLnRR_Data_Funct <- merge(LnRR_Data_Funct,Exp.Species[,c(""Abbreviation"", ""DietBreadth"",""FeedingGuild"")], \r\n                         by.x=c(""Inducer""), by.y=c(""Abbreviation""))\r\n\r\ncolnames(LnRR_Data_Funct)[c(6,7)] <- c(""SpecInd"",""GuildInd"")\r\n\r\nLnRR_Data_Funct <- merge(LnRR_Data_Funct,Exp.Species[,c(""Abbreviation"", ""DietBreadth"",""FeedingGuild"")], \r\n                         by.x=c(""Receiver""), by.y=c(""Abbreviation""))\r\n\r\ncolnames(LnRR_Data_Funct)[c(8,9)] <- c(""SpecRec"",""GuildRec"")\r\n\r\nLnRR_Data_Funct <- merge(LnRR_Data_Funct,Exp.Prevalence[,c(""Abbreviation"", ""Prevalence"")], \r\n                         by.x=c(""Inducer""), by.y=c(""Abbreviation""))\r\n\r\ncolnames(LnRR_Data_Funct)[10] <- ""PrevInd""\r\n\r\nLnRR_Data_Funct <- merge(LnRR_Data_Funct,Exp.Prevalence[,c(""Abbreviation"", ""Prevalence"")], \r\n                         by.x=c(""Receiver""), by.y=c(""Abbreviation""))\r\n\r\ncolnames(LnRR_Data_Funct)[11] <- ""PrevRec""\r\n\r\n# Model\r\nM1<- lme(LnRR ~ 1,\r\n         random = ~1|Label,\r\n         method = ""ML"",\r\n         data = LnRR_Data_Funct)\r\nAIC(M1)\r\n\r\nM2 <- lme(LnRR ~ GuildRec * PrevInd  + SpecRec  * GuildRec  * PrevRec + PrevRec * PrevInd  * SpecInd,\r\n          random = ~1|Label,\r\n          method = ""ML"",\r\n          weights = varComb(varIdent(form= ~ 1 | SpecInd),\r\n                            varIdent(form= ~ 1 | SpecRec),\r\n                            varIdent(form= ~ 1 | GuildRec),\r\n                            varExp(form= ~ PrevRec),\r\n                            varExp(form= ~ PrevInd)),\r\n          data = LnRR_Data_Funct)\r\nAIC(M2)\r\nAnova(M2)\r\n\r\ndrop1(M2)\r\n\r\n\r\n# Simulate\r\nSimData <- data.frame(PrevRec = seq(from = min(LnRR_Data_Funct$PrevRec), to = max(LnRR_Data_Funct$PrevRec), by = 0.07))\r\nSimData$Label <- sample(LnRR_Data_Funct$Label,length(SimData$PrevRec), replace = TRUE )\r\nSimData$PrevInd <- runif(length(SimData$PrevRec),0, 100)\r\nSimData$SpecRec <- sample(c(""Generalist"",""Specialist""),length(SimData$PrevRec), replace = TRUE )\r\nSimData$SpecInd <- sample(c(""Generalist"",""Specialist""),length(SimData$PrevRec), replace = TRUE )\r\nSimData$GuildRec <- sample(c(""Sap_feeder"",""Chewer""),length(SimData$PrevRec), replace = TRUE )\r\nSimData$Predict <- predict(M1, newdata = SimData, type = ""response"", se = TRUE)\r\nSimData$Inter <- as.factor(paste(SimData$SpecInd, SimData$SpecRec, sep = """"))\r\n\r\nfull.labs <- c(""Phloem-feeding receiver"", ""Leaf-chewing receiver"")\r\nnames(full.labs) <- c(""A"", ""C"")\r\n\r\nggplot(data = SimData, aes(x = PrevRec, y = Predict, shape = Inter, colour = Inter, fill = Inter)) +\r\n  geom_hline(yintercept = 0, linetype = 1, color = ""black"", size = 0.5) +\r\n  geom_point(size = 2) +\r\n  scale_x_continuous(name = ""Prevalence receiving herbivore (% of plants)"") +\r\n  scale_shape_manual(values=c(22,23,24,25), labels = c(""Generalist inducer - Generalist receiver"", \r\n                                                       ""Generalist inducer - Specialist receiver"", \r\n                                                       ""Specialist inducer - Generalist receiver"", \r\n                                                       ""Specialist inducer - Specialist receiver""))+\r\n  scale_y_continuous(name = ""Effect size"") +\r\n  scale_colour_manual(values = c(""#ff4040"",""#473f8e"",""#ff4040"",""#473f8e""), labels = c(""Generalist inducer - Generalist receiver"", \r\n                                                                                      ""Generalist inducer - Specialist receiver"", \r\n                                                                                      ""Specialist inducer - Generalist receiver"", \r\n                                                                                      ""Specialist inducer - Specialist receiver"")) +\r\n  scale_fill_manual(values = c(""#FFFFFF"",""#FFFFFF"",""#ff4040"",""#473f8e""), labels = c(""Generalist inducer - Generalist receiver"", \r\n                                                                                    ""Generalist inducer - Specialist receiver"", \r\n                                                                                    ""Specialist inducer - Generalist receiver"", \r\n                                                                                    ""Specialist inducer - Specialist receiver"")) +\r\n  theme(plot.title = element_text(face = ""italic"", hjust = 0.5),\r\n        axis.text= element_text( color=""black""),\r\n        axis.line = element_line(colour = ""black""),\r\n        panel.background = element_rect(fill = NA, colour = ""black""),\r\n        panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank(), \r\n        strip.background = element_rect(colour = ""black"", fill = NA),\r\n        strip.text = element_text(colour = ""black""),\r\n        legend.key = element_rect(fill = NA, color = NA)) +\r\n  facet_grid(~ GuildRec, labeller = labeller(GuildRec = full.labs))\r\n\r\n# constrain Inducer prevalence at 2.5 \r\n\r\nSimData <- data.frame(PrevRec = seq(from = min(LnRR_Data_Funct$PrevRec), to = max(LnRR_Data_Funct$PrevRec), by = 0.07))\r\nSimData$Label <- sample(LnRR_Data_Funct$Label,length(SimData$PrevRec), replace = TRUE )\r\nSimData$PrevInd <- rep(2.5,length(SimData$PrevRec) )\r\nSimData$SpecRec <- sample(c(""G"",""S""),length(SimData$PrevRec), replace = TRUE )\r\nSimData$SpecInd <- sample(c(""G"",""S""),length(SimData$PrevRec), replace = TRUE )\r\nSimData$GuildRec <- sample(c(""A"",""C""),length(SimData$PrevRec), replace = TRUE )\r\nSimData$Predict <- predict(M1, newdata = SimData, type = ""response"", se = TRUE)\r\nSimData$Inter <- as.factor(paste(SimData$SpecInd, SimData$SpecRec, sep = """"))\r\n\r\nfull.labs <- c(""Phloem-feeding receiver"", ""Leaf-chewing receiver"")\r\nnames(full.labs) <- c(""A"", ""C"")\r\nggplot(data = SimData, aes( x = PrevRec, y = Predict, shape = Inter, colour = Inter, fill = Inter)) +\r\n  geom_hline(yintercept = 0, linetype = 1, color = ""black"", size = 0.5) +\r\n  geom_point(size = 2) +\r\n  scale_x_continuous(name = ""Prevalence receiving herbivore (% of plants)"") +\r\n  scale_shape_manual(values=c(22,23,24,25), labels = c(""Generalist inducer - Generalist receiver"", \r\n                                                       ""Generalist inducer - Specialist receiver"", \r\n                                                       ""Specialist inducer - Generalist receiver"", \r\n                                                       ""Specialist inducer - Specialist receiver""))+\r\n  scale_y_continuous(name = ""Effect size"") +\r\n  scale_colour_manual(values = c(""#ff4040"",""#473f8e"",""#ff4040"",""#473f8e""), labels = c(""Generalist inducer - Generalist receiver"", \r\n                                                                                      ""Generalist inducer - Specialist receiver"", \r\n                                                                                      ""Specialist inducer - Generalist receiver"", \r\n                                                                                      ""Specialist inducer - Specialist receiver"")) +\r\n  scale_fill_manual(values = c(""#FFFFFF"",""#FFFFFF"",""#ff4040"",""#473f8e""), labels = c(""Generalist inducer - Generalist receiver"", \r\n                                                                                    ""Generalist inducer - Specialist receiver"", \r\n                                                                                    ""Specialist inducer - Generalist receiver"", \r\n                                                                                    ""Specialist inducer - Specialist receiver"")) +\r\n  theme(plot.title = element_text(face = ""italic"", hjust = 0.5),\r\n        axis.text= element_text( color=""black""),\r\n        axis.line = element_line(colour = ""black""),\r\n        panel.background = element_rect(fill = NA, colour = ""black""),\r\n        panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank(), \r\n        strip.background = element_rect(colour = ""black"", fill = NA),\r\n        strip.text = element_text(colour = ""black""),\r\n        legend.key = element_rect(fill = NA, color = NA)) +\r\n  facet_grid(~ GuildRec, labeller = labeller(GuildRec = full.labs))\r\n\r\n# constrain Inducer prevalence at 50 \r\n\r\nSimData <- data.frame(PrevRec = seq(from = min(LnRR_Data_Funct$PrevRec), to = max(LnRR_Data_Funct$PrevRec), by = 0.07))\r\nSimData$Label <- sample(LnRR_Data_Funct$Label,length(SimData$PrevRec), replace = TRUE )\r\nSimData$PrevInd <- rep(50,length(SimData$PrevRec) )\r\nSimData$SpecRec <- sample(c(""G"",""S""),length(SimData$PrevRec), replace = TRUE )\r\nSimData$SpecInd <- sample(c(""G"",""S""),length(SimData$PrevRec), replace = TRUE )\r\nSimData$GuildRec <- sample(c(""A"",""C""),length(SimData$PrevRec), replace = TRUE )\r\n\r\n\r\nSimData$Predict <- predict(M1, newdata = SimData, type = ""response"", se = TRUE)\r\nSimData$Inter <- as.factor(paste(SimData$SpecInd, SimData$SpecRec, sep = """"))\r\n\r\nfull.labs <- c(""Phloem-feeding receiver"", ""Leaf-chewing receiver"")\r\nnames(full.labs) <- c(""A"", ""C"")\r\n\r\n\r\nggplot(data = SimData, aes( x = PrevRec, y = Predict, shape = Inter, colour = Inter, fill = Inter)) +\r\n  geom_hline(yintercept = 0, linetype = 1, color = ""black"", size = 0.5) +\r\n  geom_point(size = 2) +\r\n  scale_x_continuous(name = ""Prevalence receiving herbivore (% of plants)"") +\r\n  scale_shape_manual(values=c(22,23,24,25), labels = c(""Generalist inducer - Generalist receiver"", \r\n                                                       ""Generalist inducer - Specialist receiver"", \r\n                                                       ""Specialist inducer - Generalist receiver"", \r\n                                                       ""Specialist inducer - Specialist receiver""))+\r\n  scale_y_continuous(name = ""Effect size"") +\r\n  scale_colour_manual(values = c(""#ff4040"",""#473f8e"",""#ff4040"",""#473f8e""), labels = c(""Generalist inducer - Generalist receiver"", \r\n                                                                                      ""Generalist inducer - Specialist receiver"", \r\n                                                                                      ""Specialist inducer - Generalist receiver"", \r\n                                                                                      ""Specialist inducer - Specialist receiver"")) +\r\n  scale_fill_manual(values = c(""#FFFFFF"",""#FFFFFF"",""#ff4040"",""#473f8e""), labels = c(""Generalist inducer - Generalist receiver"", \r\n                                                                                    ""Generalist inducer - Specialist receiver"", \r\n                                                                                    ""Specialist inducer - Generalist receiver"", \r\n                                                                                    ""Specialist inducer - Specialist receiver"")) +\r\n  theme(plot.title = element_text(face = ""italic"", hjust = 0.5),\r\n        axis.text= element_text( color=""black""),\r\n        axis.line = element_line(colour = ""black""),\r\n        panel.background = element_rect(fill = NA, colour = ""black""),\r\n        panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank(), \r\n        strip.background = element_rect(colour = ""black"", fill = NA),\r\n        strip.text = element_text(colour = ""black""),\r\n        legend.key = element_rect(fill = NA, color = NA)) +\r\n  facet_grid(~ GuildRec, labeller = labeller(GuildRec = full.labs))\r\n\r\nrm(list = ls())\r\ngraphics.off()\r\n\r\n# Permute commonality and variance ----\r\n# Obtain estimated performance data and variance from the analysis ""LnRR per Treatment""\r\n# Here called as the variable ""EffectSize"". Presented in Table S8\r\nInteraction_Data <- read_excel(""Raw_data_Mertens_et_al_2021_NaturePlants.xlsx"", sheet = ""Interaction_frequency"")\r\nThreshold <- c(50,40,30,25,20,15,10,9,8,7,6,5,4.5,4,3.5,3,2.5,2,1.5,1,0.5)\r\n\r\n# Generic functions\r\nbin.it <- function(to.bin, threshold){\r\n  Bins <- c()\r\n  for (a in 1:nrow(to.bin)){\r\n    if(to.bin$RelSum[a] < threshold){\r\n      bin <- ""Rare""\r\n    } else {\r\n      bin <- ""Frequent""\r\n    }\r\n    Bins <- c(Bins,bin)\r\n  }\r\n  out <- data.frame(to.bin, Bins)\r\n  return(out)\r\n} # Function to Bin Data into Frequent or Rare.\r\ntest.it <- function(to.test){\r\n  to.test <- data.frame(to.test, ""Rank"" = rank(to.test$EffectSize))\r\n  if (length(unique(to.test$Bins)) > 1 ) {\r\n    x <- ""OK""\r\n    wt <- wilcox.test(EffectSize ~ Bins, data = to.test, exact = T, paired = FALSE)\r\n    y <- wt$p.value\r\n    W <- wt$statistic\r\n    R_G1 <- mean(to.test[to.test$Bins %in% ""Rare"", ]$Rank)\r\n    R_G2 <- mean(to.test[to.test$Bins %in% ""Frequent"", ]$Rank)\r\n  } else {\r\n    x <- ""Not_OK""\r\n    y <- NA\r\n    W <- NA\r\n    R_G1 <- NA\r\n    R_G2 <- NA\r\n  }\r\n  out <- data.frame(""Succes"" = x, ""Rank_Rare"" = R_G1,""Rank_Frequent"" = R_G2, ""Statistic"" = W, ""Pvalue"" = y)\r\n  return(out)\r\n} # Function to test Man U and output results.\r\n\r\n# Permute mean\r\nGen_Data <- subset(InteractionData, SpecRec == ""Generalist"")\r\nResults <- list()\r\nfor (i in 1: length(Threshold)) {\r\n  to.test <- bin.it(Gen_Data,Threshold[i])\r\n  the.test <- test.it(to.test)\r\n  \r\n  Results[[i]] <- data.frame (""Threshold"" =  Threshold[i], the.test)\r\n}\r\nGen.M.Results <- do.call(rbind, Results)\r\n\r\nfor( i in 1:length(Threshold)){\r\n  x <- table(Gen_Data$RelSum > Threshold[i])\r\n  print(x)\r\n}\r\n\r\nSpec_Data <- subset(InteractionData, SpecRec == ""Specialist"")\r\nResults <- list()\r\nfor (i in 1: length(Threshold)) {\r\n  to.test <- bin.it(Spec_Data,Threshold[i])\r\n  the.test <- test.it(to.test)\r\n  \r\n  Results[[i]] <- data.frame (""Threshold"" =  Threshold[i], the.test)\r\n}\r\nSpec.M.Results <- do.call(rbind, Results)\r\n\r\ny <- data.frame(""False"" = 0, ""True"" = 0)\r\nfor( i in 1:length(Threshold)){\r\n  x <- table(Spec_Data$RelSum > Threshold[i])\r\n  y <- rbind(y,x)\r\n}\r\n\r\n# Permute variance\r\n# NOTE: Transform estimated effect sizes to absolute (i.e. magnitude of effect, not including sign)\r\nGen_Data <- subset(InteractionData, SpecRec == ""Generalist"")\r\nResults <- list()\r\nfor(i in 1: length(Threshold)){\r\n  p.data <- bin.it(Gen_Data,Threshold[i])\r\n  counts <- as.numeric((table(p.data$Bins)))\r\n  if (length(counts) < 2) {\r\n    counts[2] <- 0\r\n  }\r\n  Rare <- subset(p.data, Bins == ""Rare"")\r\n  Frequent <- subset(p.data, Bins == ""Frequent"")\r\n  \r\n  y <- c() \r\n  if (counts[1] > counts[2]) {\r\n    y <- c()\r\n    for(k in 1:10){\r\n      x <- c()\r\n      for(t in 1:1000){ \r\n        Row_numbers <- sample(1:counts[1], counts[2], replace=F)\r\n        \r\n        SetOfFrequentRows <- Frequent[c(Row_numbers),]\r\n        Variance <- var(SetOfFrequentRows$EffectSize)\r\n        \r\n        x <- c(x,Variance)\r\n      }\r\n      y <- c(y,sum(x < var(Rare$EffectSize)))\r\n      \r\n      MeanFrequentVariance <- mean(x)\r\n      FrequentVariance <- 0\r\n      MeanRareVariance <- 0\r\n      RareVariance <- var(Rare$EffectSize)\r\n    }\r\n  } else {\r\n    y <- c()\r\n    for(k in 1:10){\r\n      x <- c()\r\n      for(t in 1:1000){ \r\n        Row_numbers <- sample(1:counts[2], counts[1], replace=F)\r\n        \r\n        SetOfRareRows <- Rare[c(Row_numbers),]\r\n        Variance <- var(SetOfRareRows$EffectSize)\r\n        \r\n        x <- c(x,Variance)\r\n      }\r\n      y <- c(y,sum(x > var(Frequent$EffectSize)))\r\n      \r\n      MeanRareVariance <- mean(x)\r\n      RareVariance <- 0\r\n      MeanFrequentVariance <- 0\r\n      FrequentVariance <- var(Rare$EffectSize)\r\n    }\r\n  }\r\n  new_result <- data.frame(""Threshold"" = Threshold[i], \r\n                           ""N_Rare"" = counts[1],\r\n                           ""N_Frequent"" = counts[2],\r\n                           ""Var_Frequent"" = FrequentVariance,\r\n                           ""Mean_Var_Frequent"" = MeanFrequentVariance,\r\n                           ""Var_Rare"" = RareVariance,\r\n                           ""Mean_Var_Rare"" = MeanRareVariance,\r\n                           ""Mean_VarRare_Bigger"" = mean(y),\r\n                           ""Max_VarRare_Bigger"" = max(y),\r\n                           ""Min_VarRare_Bigger"" = min(y))\r\n  Results[[i]] <- new_result\r\n}\r\nGen.Var.Results <- do.call(rbind, Results)\r\n\r\n# Specialists\r\nSpec_Data <- subset(InteractionData, SpecRec == ""Specialist"")\r\nResults <- list()\r\nfor(i in 1: length(Threshold)){\r\n  p.data <- bin.it(Spec_Data,Threshold[i])\r\n  counts <- as.numeric((table(p.data$Bins)))\r\n  if (length(counts) < 2) {\r\n    counts[2] <- 0\r\n  }\r\n  Rare <- subset(p.data, Bins == ""Rare"")\r\n  Frequent <- subset(p.data, Bins == ""Frequent"")\r\n  \r\n  y <- c() \r\n  if (counts[1] > counts[2]) {\r\n    y <- c()\r\n    for(k in 1:10){\r\n      x <- c()\r\n      for(t in 1:1000){ \r\n        Row_numbers <- sample(1:counts[1], counts[2], replace=F)\r\n        \r\n        SetOfFrequentRows <- Frequent[c(Row_numbers),]\r\n        Variance <- var(SetOfFrequentRows$EffectSize)\r\n        \r\n        x <- c(x,Variance)\r\n      }\r\n      y <- c(y,sum(x < var(Rare$EffectSize)))\r\n      \r\n      MeanFrequentVariance <- mean(x)\r\n      FrequentVariance <- 0\r\n      MeanRareVariance <- 0\r\n      RareVariance <- var(Rare$EffectSize)\r\n    }\r\n  } else {\r\n    y <- c()\r\n    for(k in 1:10){\r\n      x <- c()\r\n      for(t in 1:1000){ \r\n        Row_numbers <- sample(1:counts[2], counts[1], replace=F)\r\n        \r\n        SetOfRareRows <- Rare[c(Row_numbers),]\r\n        Variance <- var(SetOfRareRows$EffectSize)\r\n        \r\n        x <- c(x,Variance)\r\n      }\r\n      y <- c(y,sum(x > var(Frequent$EffectSize)))\r\n      \r\n      MeanRareVariance <- mean(x)\r\n      RareVariance <- 0\r\n      MeanFrequentVariance <- 0\r\n      FrequentVariance <- var(Rare$EffectSize)\r\n    }\r\n  }\r\n  new_result <- data.frame(""Threshold"" = Threshold[i], \r\n                           ""N_Rare"" = counts[1],\r\n                           ""N_Frequent"" = counts[2],\r\n                           ""Var_Frequent"" = FrequentVariance,\r\n                           ""Mean_Var_Frequent"" = MeanFrequentVariance,\r\n                           ""Var_Rare"" = RareVariance,\r\n                           ""Mean_Var_Rare"" = MeanRareVariance,\r\n                           ""Mean_VarRare_Bigger"" = mean(y),\r\n                           ""Max_VarRare_Bigger"" = max(y),\r\n                           ""Min_VarRare_Bigger"" = min(y))\r\n  Results[[i]] <- new_result\r\n}\r\nSpec.Var.Results <- do.call(rbind, Results)']","Plant defence to sequential attack is adapted to prevalent herbivores Plants have evolved plastic defence strategies to deal with uncertainty of when, by which species and in which order attack by herbivores will take place. However, the responses to current herbivore attack may come with a cost of compromising resistance to other, later arriving herbivores. Due to antagonistic cross-talk between physiological regulation of plant resistance to phloem-feeding and leaf-chewing herbivores, the feeding guild of the initial herbivore is considered to be the primary factor determining whether resistance to subsequent attack is compromised. We show that, by investigating 90 pair-wise insect-herbivore interactions among ten different herbivore species, resistance of the annual plant Brassica nigra to a later arriving herbivore species is not explained by feeding guild of the initial attacker. Instead, the prevalence of herbivore species that arrive on induced plants as approximated by three years of season-long insect community assessments in the field explained cross-resistance. Plants maintained resistance to prevalent herbivores in common patterns of herbivore arrival and compromises in resistance especially occurred for rare patterns of herbivore attack. We conclude that plants tailor induced defence strategies to deal with common patterns of sequential herbivore attack and anticipate arrival of the most prevalent herbivores.",3
Fitting Dynamic Regression Models to Seshat Data - Supplemental Material,"This article presents a general statistical approach suitable for the analysis of time-resolved (time-series) cross-cultural data. The goal is to test theories about the evolutionary processes that generate cultural change. This approach allows us to investigate the effects of predictor variables (proxying for theory-suggested mechanisms), while controlling for spatial diffusion and autocorrelations due to shared cultural history (known as Galton's Problem). It also fits autoregressive terms to account for serial correlations in the data and tests for nonlinear effects. I illustrate these ideas and methods with an analysis of processes that may influence the evolution of one component of social complexity, information systems, using the Seshat: Global History Databank.","['############   !MI_SC  generates a multiple imputation dataset with seven CCs (sans the response CC), nrep = 20\r\n# \r\n########### The commented out scripts are not included in the release\r\n#    Run update_PolsVars.R after changing !PolsVars spreadsheet\r\n#load(""PolsVars.Rdata"")\r\n#Section <- ""Social Complexity variables""\r\n#PropCoded_threshold <- 30  #### Only use polities with better than 30% coded data\r\n#source(""precheck.R"")\r\n#########    generates SCDat.csv (prechecked, selected SC data), writes SCDat.csv\r\n#rm(errors)\r\n#==========================================   end of data checking and prep section\r\n\r\n##########################   Uses SCDat.csv. Construct ImpDatRepl\r\nload(""PolsVars.Rdata"")\r\nSection <- ""Social Complexity variables""\r\nPropCoded_threshold <- 30  #### Only use polities with better than 30% coded dataload(""PolsVars.Rdata"")\r\nVars <- variables[variables[,6]==Section,]\r\nNGAs <- unique(polities$NGA)\r\niResp <- 7   ##### Response Variable = Info   ALSO SET IN LINE 65\r\n\r\nnrep <- 20\r\nImpDatRepl <- matrix(NA, nrow=0, ncol=14)\r\nfor(irep in 1:nrep){\r\n  print(irep)\r\n  source(""ConstrMI.R"")\r\n  source(""fAggr.R"")\r\n  source(""ImputeMI.R"")\r\n  ones <- matrix(data=1,nrow=length(AggrDat[,1]),ncol=1)\r\n  colnames(ones) <- ""irep""\r\n  ImpDat <- cbind(AggrDat[,1:4],ImpDat,(ones*irep))\r\n  ImpDatRepl <- rbind(ImpDatRepl,ImpDat)\r\n}\r\n\r\n####### Remove polity-dates that didn\'t yield nrep repl\r\nload(""PolsVars.Rdata"")\r\ndat_temp <- ImpDatRepl\r\nfor(i in 1:nrow(polities)){\r\n  dat <- ImpDatRepl[as.character(ImpDatRepl[,2])==as.character(polities[i,2]),]\r\n  if(nrow(dat)!=0){\r\n    Time <- unique(dat$Time)\r\n    for(j in 1:length(Time)){\r\n      dt <- dat[dat$Time==Time[j],]\r\n      if(nrow(dt) != nrep){\r\n        print(nrow(dt))\r\n        print(dt[1,1:3])\r\n        dat_temp[as.character(dat_temp$PolID)==as.character(dat$PolID[1]) & dat_temp$Time==Time[j],length(dat_temp)] <- -99999\r\n      }\r\n    }\r\n  }\r\n}\r\nImpDatRepl <- dat_temp[dat_temp$irep!=-99999,]\r\nwrite.csv(ImpDatRepl, file=""ImpDatRepl.csv"",  row.names=FALSE)\r\n#============================================================== Multiple Imputation is done\r\n\r\n######################################################  Switching to averaged data\r\n#####     Uses the average of nrep CCs\r\nload(""PolsVars.Rdata"")\r\nSection <- ""Social Complexity variables""\r\nPropCoded_threshold <- 30  #### Only use polities with better than 30% coded data\r\nVars <- variables[variables[,6]==Section,]\r\nNGAs <- unique(polities$NGA)\r\nsource(""ConstrAvg.R"")\r\nConstrDat <- OutPolity\r\nsource(""fAggr.R"")\r\n\r\niResp <- 7     ##### Response Variable = Info   ALSO SET IN LINE 19\r\nload(""SC.Rdata"")\r\n\r\n#### Read CCs from file\r\nImpCCrepl <- read.table(\'ImpDatRepl.csv\', sep="","", header=TRUE)\r\nnrep <- ImpCCrepl$irep[nrow(ImpCCrepl)]\r\nfor(i in 1:2){ImpCCrepl[,i] <- as.character(ImpCCrepl[,i])}\r\nn <- nrow(ImpCCrepl)/nrep   #### Number of data points\r\nnCC <- length(ImpCCrepl) - 5\r\n\r\n######## Calculate average CCs\r\nCC <- matrix(0,n,nCC)\r\nfor(irep in 1:nrep){\r\n  dat <- ImpCCrepl[ImpCCrepl$irep==irep,5:(4+nCC)]\r\n  CC <- CC + dat\r\n}\r\nCC <- CC/nrep\r\nPredictors <- cbind(ImpCCrepl[ImpCCrepl$irep==1,1:3],CC)\r\nRespVar <- AggrDat[,c(1:3, 4+iResp)]\r\nRespVar <- RespVar[is.na(RespVar[,4]) == FALSE,]\r\n\r\n###   Merge Responses with Predictors\r\nRespPred <- data.frame()\r\nfor(i in 1:nrow(RespVar)){\r\n  record <- Predictors[RespVar[i,1] == Predictors[,1] & RespVar[i,2] == Predictors[,2] & RespVar[i,3] == Predictors[,3],]\r\n  record <- cbind(RespVar[i,], record[1,4:ncol(record)])\r\n  RespPred <- rbind(RespPred,record)\r\n}\r\n\r\n#####  Construct InterpDat: data with interpolated steps=100 y\r\n####### Interpolate centuries\r\nload(""PolsVars.Rdata"")\r\npolities <- polities[polities$Dupl == ""n"",]\r\nInterpDat <- data.frame()\r\nfor(iNGA in 1:length(NGAs)){\r\n  dat <- RespPred[RespPred$NGA==NGAs[iNGA],]\r\n  dt <- data.frame()\r\n  if(nrow(dat) > 1){\r\n    for(j in 2:nrow(dat)){\r\n      Tstart <- dat$Time[j-1]\r\n      PolID <- dat$PolID[j-1]\r\n      Tend <- dat$Time[j] - 100\r\n      for(t in seq(Tstart,Tend,100)){\r\n        dt <- rbind(dt,dat[j-1,])\r\n        dt$Time[nrow(dt)] <- t\r\n      }\r\n    }\r\n    dt <- rbind(dt,dat[j,])\r\n  }\r\n  InterpDat <- rbind(InterpDat,dt)\r\n}\r\n##### Take out centuries that are outside polity temporal bounds\r\nfor(i in 1:nrow(InterpDat)){\r\n  PolID <- InterpDat$PolID[i]\r\n  Time <- InterpDat$Time[i]\r\n  Start <- as.numeric(polities$Start[polities$PolID==PolID])\r\n  End <- as.numeric(polities$End[polities$PolID==PolID])\r\n  if(length(End)>1){print(i)}\r\n  if(Time < Start){InterpDat[i,3] <- -99999}\r\n  if(Time > End){InterpDat[i,3] <- -99999}\r\n}\r\nInterpDat <- InterpDat[InterpDat$Time!=-99999,]\r\n\r\n#dat <- InterpDat\r\n#for(i in 1:nrow(InterpDat)){if(is.nan(dat$Hier[i]) == TRUE){dat$Hier[i] <- NA}}  # Replace Nan with NA\r\n\r\n#### Save (or resave) everything in SC.Rdata\r\nPolityDat <- RespPred\r\nsave(polities, coords, NGAs, PolityDat,InterpDat, DistMatrix, file=""SC.Rdata"")\r\nrm(dat,dt,ImpCCrepl,Predictors,End,i,iNGA,irep,j,PolID,Start,t,Tend,Time,Tstart,RespPred)\r\nrm(n,nrep,nCC,NGAs,coords,DistMatrix,polities,record,CC, Section, PropCoded_threshold,variables)\r\n\r\n#######################################################################\r\n#######################################################################\r\n######### end of the new scrape and imputation \r\n\r\n########################################################\r\n##### Distribution of distances\r\n#dist <- vector()\r\n#mindist <- dist\r\n#for(i in 1:(nrow(DistMatrix)-1)){\r\n#  row <- DistMatrix[i,]\r\n#  row <- row[row != 0]\r\n#  mindist <- c(mindist,min(row))\r\n#  for(j in (i+1):ncol(DistMatrix)){dist <- c(dist,DistMatrix[i,j])}\r\n#}\r\n#hist(mindist, breaks=seq(0,6000,by=500))\r\n\r\n\r\n\r\n# cor(as.numeric(output[,8]),as.numeric(output[,9]), use=""complete.obs"")\r\n# cor(as.numeric(output[,8]),as.numeric(output[,11]), use=""complete.obs"")\r\n# plot(as.numeric(output[,8]),as.numeric(output[,11]))\r\n\r\n#plot(CC$PolTerr,CC$government)\r\n#res <- loess(CC$government ~ CC$PolTerr, span=0.5)\r\n#points(predict(res), x=CC$PolTerr, col=""red"")\r\n\r\n#plot(CC$PolPop,CC$government)\r\n#res <- loess(CC$government ~ CC$PolPop, span=0.5)\r\n#points(predict(res), x=CC$PolPop, col=""red"")\r\n\r\n#plot(CC$CapPop,CC$government)\r\n#res <- loess(CC$government ~ CC$CapPop, span=0.5)\r\n#points(predict(res), x=CC$CapPop, col=""red"")\r\n\r\n#plot(CC$levels,CC$government)\r\n#res <- loess(CC$government ~ CC$levels, span=0.5)\r\n#points(predict(res), x=CC$levels, col=""red"")\r\n\r\n\r\n', '#####  Regressions on SC data\r\n### Run !MI_SC.R after a new scrape. ImpDatRepl.csv has nrep imputations\r\nload(""SC.Rdata"")             ###  InterpDat is the average of nrep imputations\r\nRemPols <- c(""EsHabsb"",""InGaroL"",""CnHChin"",""PgOrokL"",""FmTrukL"",""InGupta"")   ### Polities to remove from analysis\r\n# RemPols <- c(""EsHabsb"",""InGaroL"",""CnHChin"",""PgOrokL"",""FmTrukL"",""InGupta"", ""IsCommw"",""NorKing"",""GhAshnE"",""GhAshnL"") ### eliminate spurious Infra\r\n\r\ndpar <- 1000   ### d parameter that determines how rapidly geographic influence declines with distance\r\n\r\n#### Construct RegrDat from InterpDat \r\nsource(""fRegrDat.R"")\r\n for(i in 1:length(RemPols)){NGARegrDat <- NGARegrDat[NGARegrDat$PolID != RemPols[i],] }\r\n#### Rename variables\r\ncolnames(NGARegrDat)[4:5] <- c(""Info"",""Lag1"")\r\ncolnames(NGARegrDat)[9:12] <- c(""Hier"",""Gov"",""Infra"",""Money"")\r\nRegrDat <- NGARegrDat[,4:ncol(NGARegrDat)]\r\nrm(coords,DelDat,DistMatrix, polities,dpar,i, NGAs,RemPols, InterpDat, PolityDat)\r\n#     write.csv(NGARegrDat, file=""NGARegrDat.csv"",  row.names=FALSE)\r\nRegrDat <- RegrDat[,c(1:9,11:12)]   ####  Drop Space as not significant\r\n\r\n####  Exhaustive regressions with linear terms\r\nprint(paste(""Response variable ="",colnames(RegrDat)[1]))\r\nPredictors <- 3:ncol(RegrDat)\r\noutput <- data.frame()\r\nfor (nPred in 1:length(Predictors)){ print(nPred)\r\n  Preds<- combn(Predictors, nPred)\r\n  for(j in 1:length(Preds[1,])){\r\n    fit <- lm(RegrDat[, c(1:2, Preds[,j])])\r\n    Pval <- summary(fit)$coefficients[,4]\r\n    tval <- summary(fit)$coefficients[,3]\r\n    out <- vector(""numeric"",length = length(RegrDat))\r\n    out[c(1:2,Preds[,j])] <- tval\r\n    out <- c(out,summary(fit)$r.sq)\r\n    fit <- glm(RegrDat[, c(1:2, Preds[,j])])\r\n    out <- c(out,summary(fit)$aic)\r\n    output <- rbind(output,out)\r\n  }\r\n}\r\ncolnames(output) <- c(colnames(RegrDat),""R-sq"",""delAIC"")\r\noutput <- output[order(output$delAIC),]\r\noutput$delAIC <- output$delAIC - min(output$delAIC)\r\nwrite.csv(output, file=""output.csv"",  row.names=FALSE)\r\n####\r\n\r\n#### Best model for Info by AIC, standardized coefficients  \r\n#RegrDat <- RegrDat[,1:(ncol(RegrDat)-1)]          ###  Omit Lag2 if not significant, \r\nRegrDat <- RegrDat[is.na(RegrDat[,ncol(RegrDat)]) == FALSE,]  ###  or omit missing values in Lag2 if significant\r\nfor(i in 1:ncol(RegrDat)){   RegrDat[,i] <- (RegrDat[,i] - mean(RegrDat[,i]))/sd(RegrDat[,i]) } # For standardized coefficients\r\n#options(scipen=999, digits = 5)\r\nsummary(fit <- glm(RegrDat[, c(1,2,3,7,8,9,10,11) ]))\r\n\r\n###  Diagnostics\r\nlayout(matrix(c(1,2,3,4),2,2)) # 4 graphs/page\r\nplot(fit)\r\n\r\n####################################################################################################\r\n#### Fixed-effects regression: NGAs\r\ndt <- NGARegrDat[,c(4,5,15,6,10,11,12,14,1)]\r\ndt <- dt[is.na(dt$Lag2) == FALSE,]  ###  or omit missing values in Lag2 if significant\r\nfor(i in 1:(ncol(dt)-1)){   dt[,i] <- (dt[,i] - mean(dt[,i]))/sd(dt[,i]) } # For standardized coefficients\r\nsummary(fit <- glm(dt))\r\n#summary(fit <- lm(dt$Info ~ dt$Lag1 + dt$Lag1.sq + dt$Lag2 +dt$PolPop +dt$Infra + dt$Money + dt$Phylogeny + factor(dt$NGA), data=dt))\r\n\r\n#### Take out Ghana and Iceland\r\ndt <- dt[(dt$NGA != ""Iceland"") & (dt$NGA != ""Ghanaian Coast""),]\r\nsummary(fit <- glm(dt))\r\n\r\n#### Best Linear Model without Ghana and Iceland\r\nsummary(fit <- glm(dt[,c(1:5,7:8)]))\r\n\r\n#### Absolute time as a covariate\r\ndt <- NGARegrDat[(NGARegrDat$NGA != ""Iceland"") & (NGARegrDat$NGA != ""Ghanaian Coast""),c(4,5,15,6,10,12,14,3)]\r\ndt <- dt[is.na(dt$Lag2) == FALSE,]  ###  omit missing values in Lag2\r\nfor(i in 1:(ncol(dt))){   dt[,i] <- (dt[,i] - mean(dt[,i]))/sd(dt[,i]) } # For standardized coefficients\r\nsummary(fit <- glm(dt))\r\n\r\n\r\n#### Tests for nonlin effects\r\niVar <- 2  ### adding Lag1 squared\r\nNLDat <- dt \r\nNLDat <- cbind(NLDat, NLDat[,iVar]^2)\r\ncolnames(NLDat)[9] <- paste0(colnames(NLDat)[iVar],"".sq"")\r\nNLDat <- NLDat[,c(1,2,9,3:8)]\r\nsummary(fit <- glm(NLDat))\r\n####\r\n\r\niVar <- 7  ### 4: Lag2, 5:PolPop, 6:Gov, 7:Money\r\nNLDat1 <- cbind(NLDat, NLDat[,iVar]^2)\r\ncolnames(NLDat1)[10] <- paste0(colnames(NLDat)[iVar],"".sq"")\r\nfor(i in 1:(ncol(NLDat1))){ NLDat1[,i] <- (NLDat1[,i] - mean(NLDat1[,i]))/sd(NLDat1[,i]) }\r\nsummary(fit <- glm(NLDat1))\r\n####\r\n\r\n#### The best overall model for Info\r\nsummary(fit <- glm(NLDat1[,c(1:4,6:7,10,8:9)]))\r\n###\r\n\r\n\r\n################################################################################################\r\n# Normality of Residuals\r\nlibrary(MASS)\r\nsresid <- studres(fit)\r\nhist(sresid, freq=FALSE, main=""Distribution of Studentized Residuals"")\r\nxfit<-seq(min(sresid),max(sresid),length=40)\r\nyfit<-dnorm(xfit)\r\nlines(xfit, yfit) \r\nrm(sresid,xfit,yfit)\r\n####\r\n\r\n\r\n############################################################################\r\n####     Estimate d parameter\r\n#load(""SC.Rdata"")             ###  InterpDat is the average of nrep imputations\r\n#out <- data.frame()\r\n#for(dpar in seq(100,2000, by=100)){\r\n#  source(""fRegrDat.R"")\r\n#  fit <- lm(RegrDat)\r\n#  print(summary(fit))\r\n#  out <- rbind(out,c(dpar,summary(fit)$r.sq))\r\n#}\r\n\r\n', '# Contructs Soc Complx data from SCdat.csv for PCA and regressions\r\n# Uses averages for ranges and disagreements\r\n\r\noutput <- matrix(nrow = 0, ncol = (5+nrow(Vars)))\r\nfor(iNGA in 1:length(NGAs)){\r\n   NGA <- NGAs[iNGA]\r\n   dat <- read.table(\'SCdat.csv\', sep="","", header=TRUE, colClasses = ""character"")\r\n   load(""PolsVars.Rdata"")\r\n   polities <- polities[polities[,1]==NGA,]      # Use only one NGA at a time\r\n   polities <-polities[polities[,8]==""n"",]       # Exclude duplicates\r\n   row.names(polities) <- NULL\r\n   dat <- dat[dat$NGA==NGA,]                     # Select data for the NGA\r\n   row.names(dat) <- NULL\r\n   dat_temp <- matrix(nrow = 0, ncol = 9)        # Make sure all data are for polities in PolVars\r\n   for(i in 1:nrow(polities)){\r\n      dat_temp <- rbind(dat_temp,dat[dat$Polity == polities[i,2],])\r\n      }\r\n   dat <-dat_temp\r\n\r\n   # If dat is empty, skip the rest and go to the next iNGA\r\n   if(length(dat[,1]!=0)){\r\n\r\n   # Reduce ranges to averages\r\n   for(i in 1:nrow(dat)){\r\n      if(dat[i,5]!="""") {\r\n         dat[i,4] <- mean(as.numeric(dat[i,4:5]))\r\n         dat$Value.Note[i] <- ""replaced""\r\n      }}\r\n\r\n# Substitute disputed and uncertain with averages, eliminate extra rows\r\n   dat_temp<-dat\r\n   for(i in 1:(nrow(dat)-1)){\r\n   if(dat[i,9]==""disputed"" | dat[i,9]==""uncertain""){value <- as.numeric(dat[i,4])\r\n      for(j in (i+1):nrow(dat)){\r\n         if(dat[i,1] == dat[j,1] & dat[i,3] == dat[j,3] & dat[i,6] == dat[j,6] & dat[i,7] == dat[j,7] & dat[i,9] == dat[j,9]){\r\n            value <- c(value,as.numeric(dat[j,4]))\r\n            dat[j,9] <- ""delete""}}\r\n            dat[i,4] <- mean(value)         \r\n   }  }   \r\n\r\n   datSC <- dat\r\n   datSC <- datSC[datSC[,9] != ""delete"",]\r\n   row.names(datSC) <- NULL\r\n   datSC <- datSC[,c(2,3,4,6,7)]\r\n   colnames(datSC) <- c(""PolName"",""Variable"",""Value"", ""Date"", ""DateTo"")\r\n\r\n# Construct output\r\n   tmin <- ceiling(0.01*min(polities$Start[polities$NGA==NGA]))\r\n   tmax <- floor(0.01*max(polities$End[polities$NGA==NGA]))\r\n   out <- matrix(nrow=c(length(100*tmin:tmax)),ncol=(4+nrow(Vars)))\r\n   colnames(out) <- c(""NGA"",""PolName"", ""PolID"", ""Date"", Vars[,3])  # Use short names for variables\r\n   out[,1] <- as.character(NGA)\r\n   out[,4] <- 100*tmin:tmax\r\n\r\nfor(i in 1:nrow(out)){ \r\n   for(j in 1:nrow(polities)){\r\n      if( (as.numeric(out[i,4]) <= as.numeric(polities[j,5])) & \r\n             (as.numeric(out[i,4]) >= as.numeric(polities[j,4])) ){ \r\n         out[i,2] <- as.character(polities[j,2]) \r\n         out[i,3] <- as.character(polities[j,3])\r\n      }}}\r\nout <- out[is.na(out[,2])==FALSE,]   # Eliminate centuries for which a polity is lacking\r\n\r\n# First populate \'out\' with data tied to polities, not dates\r\nfor(ivar in 1:nrow(Vars)){\r\n   datV <- datSC[(datSC[,2]==Vars[ivar,1]) & (datSC[,4]==""""),]\r\n   if(is.null(nrow(datV))){datV <- array(datV,c(1,4))}\r\n   for(i in 1:nrow(datV)){\r\n      for(j in 1:nrow(out)){\r\n         if(nrow(datV) != 0){\r\n            if(out[j,2] == datV[i,1]){out[j,ivar+4] <- datV[i,3]\r\n            }}}}}\r\n# Next populate \'out\' with data tied to a single date\r\nfor(ivar in 1:nrow(Vars)){\r\n   datV <- datSC[((datSC[,2]==Vars[ivar,1]) & (datSC[,4]!="""") & (datSC[,5]=="""")),]\r\n   if(is.null(nrow(datV))){datV <- array(datV,c(1,5))}\r\n   for(i in 1:nrow(datV)){\r\n      for(j in 1:nrow(out)){\r\n         if(nrow(datV) != 0){\r\n            century <- 100*round(as.numeric(datV[i,4])/100)\r\n            if(out[j,4] == as.character(century)){out[j,ivar+4] <- datV[i,3]\r\n            }}}}}\r\n# Finally populate \'out\' with data tied to a range of dates\r\nfor(ivar in 1:nrow(Vars)){\r\n   datV <- datSC[((datSC[,2]==Vars[ivar,1]) & (datSC[,4]!="""") & (datSC[,5]!="""")),]\r\n   if(is.null(nrow(datV))){datV <- array(datV,c(1,5))}\r\n   for(i in 1:nrow(datV)){\r\n      for(j in 1:nrow(out)){\r\n         if(nrow(datV) != 0){\r\n            century <- as.numeric(out[j,4])\r\n            tmin <- as.numeric(datV[i,4])\r\n            tmax <- as.numeric(datV[i,5])\r\n            if(century >= tmin & century <= tmax){out[j,ivar+4] <- datV[i,3]\r\n            }}}}}\r\n\r\n# Calculate the proportion of data coded by century\r\nPropCoded <- array(0,c(nrow(out),2))\r\nPropCoded[,1] <- out[,4]\r\ncolnames(PropCoded) <- c(""Date1"",""PropCoded"")\r\nfor(i in 1:nrow(out)){j <- 0  \r\n                      for(ivar in 1:nrow(Vars)){\r\n                         if(is.na(out[i,ivar+4])){j <- j+1} }\r\n                      PropCoded[i,2] <- 0.1*round((nrow(Vars) - j)/nrow(Vars)*1000) # Keep 3 sign digits\r\n}\r\nout <- cbind(out[,1:4],PropCoded[,2],out[,5:(nrow(Vars)+4)])\r\ncolnames(out) <- c(""NGA"",""PolName"", ""PolID"", ""Date"", ""PropCoded"", Vars[,3])  \r\n\r\noutput <- rbind(output,out)\r\n}  # Closing the if loop testing for empty dat\r\n   else{print(c(""No data for "", NGA))}\r\n}  # Closing the iNGA loop\r\n\r\noutput <- output[output[,5] != 0,]  # Remove rows of missing values\r\noutput <- output[,c(1,3:ncol(output))]   #### Remove redundant PolName \r\noutput <- as.data.frame(output, stringsAsFactors = FALSE)\r\nfor (i in 3:ncol(output)){output[,i] <- as.numeric(output[,i])}\r\nOutSeries <- output\r\n\r\noutNA <- output\r\noutNA[is.na(outNA)] <- -1\r\noutNA[,3] <- 0\r\nOutPolity <- data.frame()\r\nOutPolity <- rbind(OutPolity, output[1,])\r\nfor(i in 2:nrow(output)) {\r\n   if(all(outNA[i,] == outNA[i-1,]) == FALSE) OutPolity <- rbind(OutPolity,output[i,])\r\n}\r\n\r\n#write.csv(output, file=""output.csv"",  row.names=FALSE)\r\n#write.csv(OutPolity, file=""OutPolity.csv"",  row.names=FALSE)\r\nrm(i,j,tmin,tmax,datV,century,ivar,iNGA,NGA,PropCoded,out,value,dat,dat_temp,datSC, outNA, output)   # clean up the workspace\r\n\r\n', '# Multiple Imputation: sample from ranges, uncertainty, and disagreements  \r\n# Uses SCdat.csv from precheck.R\r\n#  \r\noutput <- matrix(nrow = 0, ncol = (4+nrow(Vars)))\r\n   \r\nfor(iNGA in 1:length(NGAs)){\r\n   NGA <- NGAs[iNGA]\r\n   dat <- read.table(\'SCdat.csv\', sep="","", header=TRUE, colClasses = ""character"")\r\n   load(""PolsVars.Rdata"")\r\n   polities <- polities[polities[,1]==NGA,]      # Use only one NGA at a time\r\n   polities <-polities[polities[,8]==""n"",]       # Exclude duplicates\r\n   row.names(polities) <- NULL\r\n   dat <- dat[dat$NGA==NGA,]                     # Select data for the NGA\r\n   row.names(dat) <- NULL\r\n   dat_temp <- matrix(nrow = 0, ncol = 9)        # Make sure all data are for polities in polities.csv\r\n   for(i in 1:nrow(polities)){\r\n      dat_temp <- rbind(dat_temp,dat[dat$Polity == polities[i,2],])\r\n      }\r\n   dat <-dat_temp\r\n# Randomly sample from ranges assuming 90% interval, Normal distribution\r\nfor(i in 1:nrow(dat)){\r\n   if(dat$Value.Note[i]==""range"") {\r\n      mean <- 0.5*(as.numeric(dat[i,4])+as.numeric(dat[i,5]))\r\n      sd <- abs((as.numeric(dat[i,5]) - as.numeric(dat[i,4]))/(2*1.645))\r\n      dat[i,4] <- rnorm(1,mean = mean, sd = sd)\r\n   }}\r\n# Randomly sample from disputed and uncertain, eliminate extra rows\r\nfor(i in 1:(nrow(dat)-1)){\r\n   if(dat$Value.Note[i]==""disputed"" | dat$Value.Note[i]==""uncertain""){value <- as.numeric(dat[i,4])\r\n      for(j in (i+1):nrow(dat)){\r\n         if(dat[i,1] == dat[j,1] & dat[i,2] == dat[j,2] & dat[i,3] == dat[j,3] & dat[i,6] == dat[j,6] & dat[i,7] == dat[j,7]){\r\n            value <- c(value,as.numeric(dat[j,4]))\r\n            dat[j,9] <- ""delete""}}\r\n            dat[i,4] <- sample(value, size=1)         \r\n   }  }   \r\ndat <- dat[dat[,9] != ""delete"",]\r\nrow.names(dat) <- NULL\r\ndatSC <- dat[,c(2,3,4,6,7)]\r\ncolnames(datSC) <- c(""PolID"",""Variable"",""Value"", ""Date"", ""DateTo"")\r\n\r\n# Construct output\r\ntmin <- ceiling(0.01*min(polities$Start[polities$NGA==NGA]))\r\ntmax <- floor(0.01*max(polities$End[polities$NGA==NGA]))\r\nout <- matrix(nrow=c(length(100*tmin:tmax)),ncol=(3+nrow(Vars)))\r\ncolnames(out) <- c(""NGA"",""PolID"", ""Date"", Vars[,3])  # Use short names for variables\r\nout[,1] <- as.character(NGA)\r\nout[,3] <- 100*tmin:tmax\r\n\r\nfor(i in 1:nrow(out)){ \r\n   for(j in 1:nrow(polities)){\r\n      if( (as.numeric(out[i,3]) <= as.numeric(polities[j,5])) & \r\n             (as.numeric(out[i,3]) >= as.numeric(polities[j,4])) ){ \r\n         out[i,2] <- as.character(polities[j,3]) \r\n      }}}\r\nout <- out[is.na(out[,2])==FALSE,]   # Eliminate centuries for which a polity is lacking\r\n\r\n# First populate \'out\' with data tied to polities, not dates\r\nfor(ivar in 1:nrow(Vars)){\r\n   datV <- datSC[(datSC$Variable==Vars[ivar,1]) & (datSC$Date==""""),]\r\n   if(is.null(nrow(datV))){datV <- array(datV,c(1,5))}\r\n   for(i in 1:nrow(datV)){\r\n      for(j in 1:nrow(out)){\r\n         if(nrow(datV) != 0){\r\n            if(out[j,2] == datV$PolID[i]){out[j,ivar+3] <- datV$Value[i]\r\n            }}}}}\r\n\r\n# Next populate \'out\' with data tied to a single date\r\nfor(ivar in 1:nrow(Vars)){\r\n   datV <- datSC[((datSC$Variable==Vars[ivar,1]) & (datSC$Date!="""") & (datSC$DateTo=="""")),]\r\n   if(is.null(nrow(datV))){datV <- array(datV,c(1,5))}\r\n   for(i in 1:nrow(datV)){\r\n      for(j in 1:nrow(out)){\r\n         if(nrow(datV) != 0){\r\n            century <- 100*round(as.numeric(datV[i,4])/100)\r\n            if(out[j,3] == as.character(century)){out[j,ivar+3] <- datV$Value[i]\r\n            }}}}}\r\n\r\n\r\n# Finally populate \'out\' with data tied to a range of dates\r\nfor(ivar in 1:nrow(Vars)){\r\n   datV <- datSC[((datSC[,2]==Vars[ivar,1]) & (datSC[,4]!="""") & (datSC[,5]!="""")),]\r\n   if(is.null(nrow(datV))){datV <- array(datV,c(1,5))}\r\n   for(i in 1:nrow(datV)){\r\n      for(j in 1:nrow(out)){\r\n         if(nrow(datV) != 0){\r\n            century <- as.numeric(out[j,3])\r\n            tmin <- as.numeric(datV[i,4])\r\n            tmax <- as.numeric(datV[i,5])\r\n            if(century >= tmin & century <= tmax){out[j,ivar+3] <- datV[i,3]\r\n            }}}}}\r\n\r\n# Calculate the proportion of data coded by century\r\nPropCoded <- array(0,c(nrow(out),2))\r\nPropCoded[,1] <- out[,3]\r\ncolnames(PropCoded) <- c(""Date1"",""PropCoded"")\r\nfor(i in 1:nrow(out)){\r\n  j <- 0  \r\n  for(ivar in 1:nrow(Vars)){\r\n       if(is.na(out[i,ivar+3])){j <- j+1} \r\n    }\r\n  PropCoded[i,2] <- 0.1*round((nrow(Vars) - j)/nrow(Vars)*1000) # Keep 3 sign digits\r\n}\r\nout <- cbind(out[,1:3],PropCoded[,2],out[,4:(nrow(Vars)+3)])\r\ncolnames(out) <- c(""NGA"",""PolID"", ""Date"", ""PropCoded"", Vars[,3])  \r\noutput <- rbind(output,out)\r\n}  # Closing the iNGA loop\r\n\r\noutput <- output[output[,4] != 0,]  # Remove rows of missing values\r\nConstrDat <- as.data.frame(output, stringsAsFactors = FALSE)\r\nfor(i in 3:ncol(ConstrDat)){ConstrDat[,i] <- as.numeric(ConstrDat[,i])}\r\n\r\nrm(dat_temp,dat,datSC,datV,out,PropCoded,century,i,iNGA,ivar,j,mean,NGA,sd,tmax,tmin,value,output)\r\n\r\n', '############  Averages uncertainty and disagreement\r\n#    Run update_PolsVars.R after changing !PolsVars spreadsheet\r\n#    source(""precheck.R"")   #### To run from !MI_SC.R after a new scrape. \r\nload(""PolsVars.Rdata"")\r\nSection <- ""Social Complexity variables""\r\nPropCoded_threshold <- 30  #### Only use polities with better than 30% coded data\r\n\r\nVars <- variables[variables[,6]==Section,]\r\nNGAs <- unique(polities$NGA)\r\nsource(""ConstrAvg.R"")\r\nConstrDat <- OutPolity\r\nsource(""fAggr.R"")\r\nload(""PolsVars.Rdata"")\r\nfor(i in 1:nrow(AggrDat)){\r\n  region <- polities$World.Region[polities$NGA == AggrDat$NGA[i]]\r\n  AggrDat[i,4] <- region[1]\r\n}\r\ncolnames(AggrDat)[4] <- ""Region""\r\n\r\n############################    Cross-Validation\r\niResp <- 7 #### Response variable: 1=PolPop, 2=PolTerr, ... 8=money\r\nObsPred <- AggrDat[,c(1:4, (iResp+4), 5)]\r\ncolnames(ObsPred)[6] <- ""Pred""\r\nindex <- 1:(ncol(AggrDat)-4)\r\nindex <- index[index != iResp]\r\ndata <- AggrDat[,4+c(iResp,index)]\r\nregions <- AggrDat$Region\r\n####   Omit NAs in the response variable\r\nregions <- regions[is.na(data[,1])==FALSE]\r\nObsPred <- ObsPred[is.na(data[,1])==FALSE,]\r\ndata <- data[is.na(data[,1])==FALSE,]\r\n\r\n##### Run cross-validation\r\nfor(i in 1:nrow(data)){print(i)\r\n    index <- c(1:ncol(data))[is.na(data[i,]) == FALSE]\r\n  dat <- data[regions[i] != regions,index]                   # Predict outside region\r\n  for(j in 1:ncol(dat)){dat <- dat[is.na(dat[,j])==FALSE,]}  # Omit rows with NAs\r\n  Predictors <- 2:ncol(dat)                                  # Exhaustive regressions\r\n  output <- data.frame()\r\n  for(nPred in 1:length(Predictors)){\r\n    Preds<- combn(Predictors, nPred)\r\n    for(j in 1:ncol(Preds)){\r\n      fit <- glm(dat[, c(1, Preds[,j])])\r\n      out <- head(c(Preds[,j], 0,0,0,0,0,0,0,0,0),length(Predictors))\r\n      out <- c(out,summary(fit)$aic)\r\n      output <- rbind(output,out)\r\n    }\r\n  }\r\n  output <- output[order(output[,ncol(output)]),]\r\n  Preds <- output[1,1:(ncol(output)-1)]\r\n  Preds <- Preds[Preds != 0]\r\n  fit <- glm(dat[, c(1, Preds)])\r\n  PredVar <- data[i,index[c(1,Preds)]]\r\n  PredVar[1] <- 1\r\n  ObsPred[i,6] <- sum(coefficients(fit)*PredVar)\r\n}\r\n\r\nRsq_out <- data.frame(NA, 10, 3)\r\nRegions <- unique(ObsPred$Region)\r\nfor(i in 1:length(Regions)){\r\n  X <- ObsPred$info[ObsPred$Region == Regions[i]]\r\n  Y <- ObsPred$Pred[ObsPred$Region == Regions[i]]\r\n  rsq <- 1 - sum((X-Y)^2)/sum((X-mean(X))^2)\r\n  Rsq_out[i,] <- c(Regions[i],rsq,length(X))\r\n}\r\ncolnames(Rsq_out) <- c(""Region"", ""R-sq"", ""n"")\r\nwrite.csv(Rsq_out, file=""output.csv"",  row.names=FALSE)\r\n####\r\n#############################################   Plot predicted - observed by region\r\nsummary(lm(ObsPred[,5:6]))\r\nX <- ObsPred$info\r\nY <- ObsPred$Pred\r\nrsq <- 1 - sum((X-Y)^2)/sum((X-mean(X))^2)\r\nrsq <- round(1000*rsq)/1000\r\n\r\ncolors <- c(""red"",""blue"",""darkgreen"",""purple"",""brown"",""tan"",""darkgrey"",""orange"",""cyan"",""black"")\r\npointshapes <-c(15:19,15:19)\r\nxcoord <-c(0,1)\r\nycoord <-c(0,1)\r\nplot(xcoord,ycoord,""n"",xaxp=c(0,1,10),yaxp=c(0,1,10),xlab=""Predicted"",ylab=""Observed"",\r\n     main=paste(""Info: observed vs. predicted.    Prediction R-sq ="",rsq))\r\nlines(x=c(0,1),y=c(0,1), lty = 2, lwd=2)\r\nxcoord <- 0\r\nycoord <- 1  ### To put labels in the upper left corner\r\nfor(i in 1:length(Regions)){\r\n  gdat <- ObsPred[ObsPred$Region == Regions[i],6:5]\r\n  points(gdat, col=colors[i], pch=pointshapes[i])\r\n  text(x=xcoord, y=(ycoord-0.05*(i-1)), Regions[i], col=colors[i], pos=4)\r\n  points(x=xcoord, y=(ycoord-0.05*(i-1)), col=colors[i], pch=pointshapes[i])\r\n}\r\n\r\n', '# fAggr.R  -- universal for averaged and MI data\r\n# Aggregate data into CCs: scale, hierarchy, government, infrastr, information, money\r\ndata <- ConstrDat[ConstrDat$PropCoded > PropCoded_threshold,]  ### From ConstrAvg.R, or ConstrMI.R; Omit sparsely coded polities\r\ndat <- data[,5:ncol(data)] \r\nAggrDat <- matrix(NA, 0, 0)\r\nfor(i in 1:nrow(dat)){\r\n   row <- log10(dat[i,1:3])                                         # scale variables, log-transformed based 10\r\n   dt <- dat[i,c(4:7)]\r\n     dt <- dt[is.na(dt)==FALSE]\r\n     row <- cbind(row,NA)\r\n     if(length(dt)!=0){row[length(row)] <- mean(dt) }               # hierarchy, averaging over non-missing values\r\n   dt <- dat[i,8:18]\r\n   if(is.na(dt$ExamSyst)){dt$ExamSyst <- 0}                         # Missing => absent (only secure presence counts)\r\n   if(is.na(dt$MeritProm)){dt$MeritProm <- 0}\r\n   row <- cbind(row,mean(dt[is.na(dt)==FALSE]))                     # government\r\n   dt <- dat[i,19:30]\r\n   row <- cbind(row,mean(dt[is.na(dt)==FALSE]))                     # infrastr\r\n   writing <- dat[i,c(31,32,34,33)]\r\n   writing[is.na(writing)] <- 0                                     # Missing => absent\r\n   writing[writing < 0.5] <- 0                                      # Inferred absent => absent\r\n   writing[writing > 0.5] <- 1                                      # Inferred present => present\r\n   writing <- max(writing * 1:4)                                    # Code writing on scale of 0 to 4\r\n   texts <- dat[i, 37:45]\r\n   texts[is.na(texts)] <- 0                                         # Count only securely ""present""\r\n   texts[texts < 1] <- 0\r\n   texts <- (sum(texts))\r\n   row <- cbind(row,(writing+texts)/13)                             # info = writing + texts, scaled between 0 and 1\r\n   dt <- dat[i,46:51]\r\n   money <- dt*1:6                                                  # money = coded on scale of 1 to 6\r\n      money <- money[is.na(money)==FALSE]\r\n      row <- cbind(row,NA)\r\n      if(length(money)!=0){row[length(row)] <- max(money) }\r\n   AggrDat <- rbind(AggrDat, row)\r\n} \r\nAggrDat <- cbind( data[,1:4] ,AggrDat )\r\ncolnames(AggrDat) <- c(""NGA"", ""PolID"",""Time"", ""PropCoded"", ""PolPop"",""PolTerr"",""CapPop"",""hierarchy"", ""government"", ""infrastr"", ""info"", ""money"")\r\nrow.names(AggrDat) <- NULL\r\n\r\nrm(data,dat,dt,row,i,money,writing, texts,polities)\r\n\r\n', '#### fRegrDat constructs RegrDat, using InterpDat from SC.Rdata\r\n##### Reconfigure InterpDat with response variable and lagged predictors\r\nRegrDat <- data.frame()\r\nfor(iNGA in 1:length(NGAs)){\r\n  dat <- InterpDat[InterpDat$NGA == NGAs[iNGA],]\r\n  n <- nrow(dat)\r\n  if(n != 0){rdat <- cbind(dat[2:n,1:4],dat[1:(n-1),4:length(dat)])\r\n  RegrDat <- rbind(RegrDat,rdat)\r\n  }\r\n}\r\nRegrDat$Time <- RegrDat$Time - 100 ### Set RegrDat$Time to t, not t+1\r\nnm <- colnames(RegrDat)\r\nnm[4] <- paste0(nm[4],""(t+1)"")\r\ncolnames(RegrDat) <- nm\r\n\r\n##### Calculate Space using estimated dpar (set in !SC_analyz)\r\nSpace <- RegrDat[,1:4]\r\nSpace[,4] <- 0\r\ncolnames(Space) <- c(""NGA"",""PolID"",""Time"",""Space"")\r\n\r\ncolMat <- colnames(DistMatrix)\r\nrowMat <- rownames(DistMatrix)\r\nfor(i in 1:nrow(RegrDat)){\r\n  t1 <- RegrDat$Time[i]\r\n  dat <- RegrDat[RegrDat$Time==t1,c(1:3,5)]\r\n  if(nrow(dat) > 1){\r\n    delta <- vector(length=nrow(dat))\r\n    for(j in 1:nrow(dat)){\r\n      dt <- DistMatrix[colMat==dat$NGA[j],]\r\n      delta[j] <- dt[rowMat==RegrDat$NGA[i]]\r\n    }\r\n    s <- exp(-delta/dpar)*dat[,4]\r\n    s <- s[delta != 0]  ### Exclude i=j\r\n    Space$Space[i] <- mean(s)\r\n  }\r\n}\r\nRegrDat <- cbind(RegrDat,Space$Space)\r\nnm <- colnames(RegrDat)\r\nnm[length(nm)] <- ""Space""\r\ncolnames(RegrDat) <- nm\r\n\r\n\r\n#### Calculate Language = matrix of linguistic distances\r\nPhylogeny <- RegrDat[,1:4]\r\nPhylogeny[,4] <- 0\r\ncolnames(Phylogeny) <- c(""NGA"",""PolID"",""Time"",""Phylogeny"")\r\n\r\nfor(i in 1:nrow(RegrDat)){\r\n  t1 <- RegrDat$Time[i]\r\n  dat <- RegrDat[RegrDat$Time==t1,c(1:3,5)]\r\n  dat <- dat[dat$NGA != RegrDat$NGA[i],]   ### Exclude i = j\r\n  PolID <- RegrDat$PolID[i]\r\n  PolLang <- polities[polities$PolID==PolID,9:11]\r\n  if(nrow(dat) > 1){\r\n    weight <- vector(length=nrow(dat)) * 0\r\n    for(j in 1:nrow(dat)){\r\n      dt <- dat[j,]\r\n      PolLang2 <- polities[polities$PolID==dt$PolID,9:11]\r\n      if(PolLang[1,3]==PolLang2[1,3]){weight[j] <- 0.25}\r\n      if(PolLang[1,2]==PolLang2[1,2]){weight[j] <- 0.5}\r\n      if(PolLang[1,1]==PolLang2[1,1]){weight[j] <- 1}\r\n    }\r\n    s <- weight*dat[,4]\r\n    Phylogeny$Phylogeny[i] <- mean(s)\r\n  }\r\n}\r\nRegrDat <- cbind(RegrDat,Phylogeny$Phylogeny)\r\nnm <- colnames(RegrDat)\r\nnm[length(nm)] <- ""Phylogeny""\r\ncolnames(RegrDat) <- nm\r\n\r\n#### Add time lag\r\nLag2 <- RegrDat[,1:4]\r\nLag2[,4] <- NA\r\nfor(i in 1:nrow(RegrDat)){\r\n  t2 <- RegrDat$Time[i] - 100\r\n  NGA <- RegrDat$NGA[i]\r\n  dat <- RegrDat[RegrDat$Time==t2 & RegrDat$NGA == NGA,]\r\n  if(nrow(dat)>1){print(""Error: more than one Lag2"")}\r\n  if(nrow(dat)==1){Lag2[i,4] <- dat[1,5]}\r\n}\r\nRegrDat <- cbind(RegrDat,Lag2[,4])\r\n\r\nnm <- colnames(RegrDat)\r\nnm[length(nm)] <- ""Lag2""\r\ncolnames(RegrDat) <- nm\r\nNGARegrDat <- RegrDat  ##### keep the NGA, PolID, Time\r\nRegrDat <- RegrDat[,4:length(RegrDat)]  #### Drop NGA, PolID, and Time\r\n\r\n#### Differenced response variable\r\nDelDat <- RegrDat\r\nDelDat[,1] <- DelDat[,1] - DelDat[,2]\r\nnm <- colnames(DelDat)\r\nnm[1] <- paste(""del"",nm[1])\r\ncolnames(DelDat) <- nm\r\n\r\n\r\n#### Save (or resave) everything in HS.Rdata\r\n#save(polities, coords, NGAs, RegrDat, DistMatrix, file=""HS.Rdata"")\r\nrm(dat,dt,Lag2,Phylogeny,PolLang,PolLang2,rdat,Space)\r\nrm(colMat,delta,i,iNGA,j,n,NGA,nm,PolID,rowMat,s,t1,t2,weight)\r\n\r\n\r\n\r\n\r\n\r\n\r\n', '# Single Imputation function using stochastic regression \r\n\r\ndat <- AggrDat[,5:length(AggrDat)]\r\n\r\n#### Omit the Response Variable\r\nindex <- 1:ncol(dat)\r\nindex <- index[index != iResp]\r\ndat <- dat[,index]\r\nImpDat <- dat\r\n\r\nfor(j in 1:length(dat[1,])){\r\n   for(i in 1:length(dat[,1])){\r\n      if(is.na(dat[i,j])==TRUE){\r\n         index <- c(1:length(dat[1,]))\r\n         index <- index[is.na(dat[i,])==FALSE]\r\n         RegrDat <- dat[,c(j,index)]\r\n         RegrDat <- RegrDat[is.na(RegrDat[,1])==FALSE,]\r\n         fit <- lm(RegrDat)\r\n         Pval <- summary(fit)$coefficients[,4]\r\n         Pval <- Pval[-1]\r\n         if (all((Pval < 0.05)==FALSE)){Pval[Pval==min(Pval)] <- 0.01}\r\n         index <- index[Pval < 0.05] \r\n         RegrDat <- dat[,c(j,index)]\r\n         RegrDat <- RegrDat[is.na(RegrDat[,1])==FALSE,]\r\n         fit <- lm(RegrDat)         \r\n         predictors <- dat[i,c(j,index)]\r\n         predictors[1] <- 1\r\n         coeff <- coefficients(fit)\r\n         ImpDat[i,j] <- sum(coeff*predictors) + sample(fit$residuals,1)\r\n      }\r\n   }\r\n}\r\n\r\nrm(dat,predictors,RegrDat,coeff,fit,i,index,j,Pval)\r\n', '#####     Runs dynamic regression analysis separately for each of nrep CCs\r\noutAll <- data.frame()\r\n\r\nfor(irep in 1:nrep){\r\n\r\nload(""PolsVars.Rdata"")\r\nSection <- ""Social Complexity variables""\r\nPropCoded_threshold <- 30  #### Only use polities with better than 30% coded data\r\nRemPols <- c(""EsHabsb"",""InGaroL"",""CnHChin"",""PgOrokL"",""FmTrukL"",""InGupta"", ""IsCommw"",""NorKing"",""GhAshnE"",""GhAshnL"") \r\ndpar <- 1000\r\nVars <- variables[variables[,6]==Section,]\r\nNGAs <- unique(polities$NGA)\r\nsource(""ConstrAvg.R"")\r\nConstrDat <- OutPolity\r\nsource(""fAggr.R"")\r\n\r\niResp <- 7     ##### Response Variable = Info   ALSO SET IN LINE 19\r\nload(""SC.Rdata"")\r\n\r\n#### Read CCs from file\r\nImpCCrepl <- read.table(\'ImpDatRepl.csv\', sep="","", header=TRUE)\r\nnrep <- ImpCCrepl$irep[nrow(ImpCCrepl)]\r\nfor(i in 1:2){ImpCCrepl[,i] <- as.character(ImpCCrepl[,i])}\r\nn <- nrow(ImpCCrepl)/nrep   #### Number of data points\r\nnCC <- length(ImpCCrepl) - 5\r\n\r\nCC <- ImpCCrepl[ImpCCrepl$irep==irep,5:(4+nCC)]\r\nPredictors <- cbind(ImpCCrepl[ImpCCrepl$irep==1,1:3],CC)\r\nRespVar <- AggrDat[,c(1:3, 4+iResp)]\r\nRespVar <- RespVar[is.na(RespVar[,4]) == FALSE,]\r\n\r\n###   Merge Responses with Predictors\r\nRespPred <- data.frame()\r\nfor(i in 1:nrow(RespVar)){\r\n  record <- Predictors[RespVar[i,1] == Predictors[,1] & RespVar[i,2] == Predictors[,2] & RespVar[i,3] == Predictors[,3],]\r\n  record <- cbind(RespVar[i,], record[1,4:ncol(record)])\r\n  RespPred <- rbind(RespPred,record)\r\n}\r\n\r\n#####  Construct InterpDat: data with interpolated steps=100 y\r\n####### Interpolate centuries\r\nload(""PolsVars.Rdata"")\r\npolities <- polities[polities$Dupl == ""n"",]\r\nInterpDat <- data.frame()\r\nfor(iNGA in 1:length(NGAs)){\r\n  dat <- RespPred[RespPred$NGA==NGAs[iNGA],]\r\n  dt <- data.frame()\r\n  if(nrow(dat) > 1){\r\n    for(j in 2:nrow(dat)){\r\n      Tstart <- dat$Time[j-1]\r\n      PolID <- dat$PolID[j-1]\r\n      Tend <- dat$Time[j] - 100\r\n      for(t in seq(Tstart,Tend,100)){\r\n        dt <- rbind(dt,dat[j-1,])\r\n        dt$Time[nrow(dt)] <- t\r\n      }\r\n    }\r\n    dt <- rbind(dt,dat[j,])\r\n  }\r\n  InterpDat <- rbind(InterpDat,dt)\r\n}\r\n##### Take out centuries that are outside polity temporal bounds\r\nfor(i in 1:nrow(InterpDat)){\r\n  PolID <- InterpDat$PolID[i]\r\n  Time <- InterpDat$Time[i]\r\n  Start <- as.numeric(polities$Start[polities$PolID==PolID])\r\n  End <- as.numeric(polities$End[polities$PolID==PolID])\r\n  if(length(End)>1){print(i)}\r\n  if(Time < Start){InterpDat[i,3] <- -99999}\r\n  if(Time > End){InterpDat[i,3] <- -99999}\r\n}\r\nInterpDat <- InterpDat[InterpDat$Time!=-99999,]\r\n\r\nsource(""fRegrDat.R"")\r\nfor(i in 1:length(RemPols)){NGARegrDat <- NGARegrDat[NGARegrDat$PolID != RemPols[i],] }\r\n#### Rename variables\r\ncolnames(NGARegrDat)[4:5] <- c(""Info"",""Lag1"")\r\ncolnames(NGARegrDat)[9:12] <- c(""Hier"",""Gov"",""Infra"",""Money"")\r\nRegrDat <- NGARegrDat[,4:ncol(NGARegrDat)]\r\nRegrDat <- RegrDat[,c(1:9,11:12)]   ####  Drop Space as not significant\r\n\r\n####  Exhaustive regressions with linear terms\r\nprint(paste(""Response variable ="",colnames(RegrDat)[1],""        irep ="",irep))\r\nPredictors <- 3:ncol(RegrDat)\r\noutput <- data.frame()\r\nfor (nPred in 1:length(Predictors)){ print(nPred)\r\n  Preds<- combn(Predictors, nPred)\r\n  for(j in 1:length(Preds[1,])){\r\n    fit <- lm(RegrDat[, c(1:2, Preds[,j])])\r\n    Pval <- summary(fit)$coefficients[,4]\r\n    tval <- summary(fit)$coefficients[,3]\r\n    out <- vector(""numeric"",length = length(RegrDat))\r\n    out[c(1:2,Preds[,j])] <- tval\r\n    out <- c(out,summary(fit)$r.sq)\r\n    fit <- glm(RegrDat[, c(1:2, Preds[,j])])\r\n    out <- c(out,summary(fit)$aic)\r\n    output <- rbind(output,out)\r\n  }\r\n}\r\ncolnames(output) <- c(colnames(RegrDat),""R-sq"",""delAIC"")\r\noutput <- output[order(output$delAIC),]\r\noutput$delAIC <- output$delAIC - min(output$delAIC)\r\noutAll <- rbind(outAll,output[1:10,])\r\n}\r\n\r\nwrite.csv(outAll, file=""output.csv"",  row.names=FALSE)\r\n####\r\n\r\n', '############  Summary statistics on CCs. Uses SCdat.csv\r\nload(""PolsVars.Rdata"")\r\nSection <- ""Social Complexity variables""\r\nPropCoded_threshold <- 30  #### Only use polities with better than 30% coded data\r\nVars <- variables[variables[,6]==Section,]\r\nNGAs <- unique(polities$NGA)\r\nsource(""ConstrAvg.R"")\r\nConstrDat <- OutPolity\r\nsource(""fAggr.R"")\r\n\r\n#### Plot frequency distributions for all CCs\r\nPlotNames <- c(""PolPop"",""PolTerr"",""CapPop"",""Hier"",""Gov"",""Infra"",""Info"",""Money"")\r\nlayout(matrix(c(1:8),4,2)) # 4 x 2 graphs/page\r\nfor(i in 1:8){\r\n  dat <- AggrDat[,4+i]\r\n  dat <- dat[is.na(dat)==FALSE]\r\n  PlotLabel <- paste0(PlotNames[i], "" (n = "",length(dat),"")"")\r\n  hist(dat, main = PlotLabel, xlab = """")\r\n  }\r\n\r\n# Number of complete rows\r\nCompletDat <- data.frame()\r\nfor(i in 1:nrow(AggrDat)){\r\n  if(all(is.na(AggrDat[i,]) == FALSE) ){  CompletDat <- rbind(CompletDat,AggrDat[i,]) }\r\n}\r\n\r\n\r\n\r\n\r\n']","Fitting Dynamic Regression Models to Seshat Data - Supplemental Material This article presents a general statistical approach suitable for the analysis of time-resolved (time-series) cross-cultural data. The goal is to test theories about the evolutionary processes that generate cultural change. This approach allows us to investigate the effects of predictor variables (proxying for theory-suggested mechanisms), while controlling for spatial diffusion and autocorrelations due to shared cultural history (known as Galton's Problem). It also fits autoregressive terms to account for serial correlations in the data and tests for nonlinear effects. I illustrate these ideas and methods with an analysis of processes that may influence the evolution of one component of social complexity, information systems, using the Seshat: Global History Databank.",3
Repeatable differences in exploratory behaviour predict tick infestation probability in wild great tits,"Ecological factors and individual-specific traits affect parasite infestation in wild animals. We studied various key ecological variables (breeding density, human disturbance) and phenotypic traits (exploratory behaviour, body condition) proposed to predict tick infestation probability and burden in great tits (Parus major). Our study spanned three years and 12 nest-box plots located in southern Germany. Adult breeders were assessed for exploration behaviour, body condition, and tick burden. Plots were open to human recreation; human disturbance was quantified in each plot as a recreation pressure index from biweekly nest box inspections. Infested individuals were repeatable in tick burden across years. These repeatable among-individual differences in tick burden were not attributable to exploration behaviour. However, faster explorers did have a higher infestation probability. Furthermore, body condition negatively correlated to tick burden. Recreation pressure also tended to increase infestation probability. Our study implies that avian infestation probability and tick burden are each governed by distinct phenotypic traits and ecological factors. Our findings highlight the importance of animal behaviour and human disturbance in understanding variation in tick burden among avian hosts.","['##Tick Load Analysis for Rollins et al, 2019##\r\n##Updated: 18.07.2019##\r\n#Directory on this Computer#\r\n#you will need to modify this to your personal computer#\r\nsetwd(""E:/"")\r\n\r\n#uncomment to install libraries before loading them#\r\n#install.packages(""lme4"")\r\n#install.packages(""arm"")\r\n#install.packages(""ggplot2"")\r\n\r\n#Loading Required Libraries#\r\nlibrary(lme4)\r\nlibrary(arm)\r\nlibrary(ggplot2)\r\ndispersion_glmer<- function(mod)#this is a function we coded to calcuate the dispersion of a model, needed for residual calculations later#\r\n{ n <- length(resid(mod))\r\nreturn( sqrt( sum(c(resid(mod), mod@u)^2) / n ) )}\r\n\r\n#Tick Data Fixing#\r\n#Loading Tick Data from 2017-2019#\r\ntickdata<-read.csv(""TickData_072019.csv"", dec = ""."", sep = "";"")\r\n#Adding variables to Tick Data#\r\n#these are both needed later in running the analysis#\r\ntickdata$PlotYear <- paste(tickdata$Plot, tickdata$TickYear, sep = """")\r\ntickdata$YearDay <- paste(tickdata$TickYear, tickdata$CatchAprilDay, sep = """")\r\n\r\n#Check Tick Data Structure#\r\n#this is the checking to make sure that all variables are in the proper from and removing null values which would complicate the analysis later#\r\nstr(tickdata)\r\ntickdata$Plot<- as.factor(tickdata$Plot)\r\ntickdata$RingNumber<- as.character(tickdata$RingNumber)\r\ntickdata<- tickdata[tickdata$BirdAge != 1,]\r\ntickdata$SexConclusion <- as.factor(tickdata$SexConclusion)\r\ntickdata <- tickdata[is.na(tickdata$SexConclusion) == F, ]\r\ntickdata<-tickdata[is.na(tickdata$Tarsus) ==F,]\r\ntickdata<- tickdata[is.na(tickdata$BodyMassField) == F,]\r\ntickdata$PlotYear<-as.factor(tickdata$PlotYear)\r\ntickdata$TickYear<- as.factor(tickdata$TickYear)\r\n\r\n#All Catches Data Fixing#\r\n#Loading All Catches from 2010-2019##\r\nallcatches<-read.csv(""Catches_072019.csv"", dec = ""."")\r\nallcatches <- allcatches[is.na(allcatches$CatchMonth) == F,] #Many 2019 data points do not actually have a bird attached to them, this removes these entries#\r\n\r\n#Addition of Variables to All Catches#\r\nallcatches$PlotYear <- paste(allcatches$Plot, allcatches$CatchYear, sep = """")\r\nallcatches$YearDay <- paste(allcatches$CatchYear, allcatches$CatchAprilDay, sep = """")\r\n#Check All Catches Structure#\r\nstr(allcatches)\r\nallcatches$Plot <- as.factor(allcatches$Plot)\r\nallcatches$PlotYear<- as.character(allcatches$PlotYear)\r\n\r\nbroods <- allcatches[duplicated(allcatches$BroodID) != F,]\r\n\r\n#Adding Density to the tick_data dataframe, number of birds recorded in PlotYear divided by area recorded by Alexia#\r\ntickdata$density<-NA\r\n\r\n#2017 Data#\r\ntickdata[tickdata$PlotYear ==102017,]$density <- length(broods[broods$PlotYear == 102017,]$BroodID)/8.76\r\ntickdata[tickdata$PlotYear ==112017,]$density <- length(broods[broods$PlotYear == 112017,]$BroodID)/9.1\r\ntickdata[tickdata$PlotYear ==122017,]$density <- length(broods[broods$PlotYear == 122017,]$BroodID)/9.71\r\ntickdata[tickdata$PlotYear ==132017,]$density <- length(broods[broods$PlotYear == 132017,]$BroodID)/8.31\r\ntickdata[tickdata$PlotYear ==142017,]$density <- length(broods[broods$PlotYear == 142017,]$BroodID)/8.98\r\ntickdata[tickdata$PlotYear ==152017,]$density <- length(broods[broods$PlotYear == 152017,]$BroodID)/8.38\r\ntickdata[tickdata$PlotYear ==162017,]$density <- length(broods[broods$PlotYear == 162017,]$BroodID)/10.66\r\ntickdata[tickdata$PlotYear ==172017,]$density <- length(broods[broods$PlotYear == 172017,]$BroodID)/9.06\r\ntickdata[tickdata$PlotYear ==182017,]$density <- length(broods[broods$PlotYear == 182017,]$BroodID)/9.19\r\ntickdata[tickdata$PlotYear ==192017,]$density <- length(broods[broods$PlotYear == 192017,]$BroodID)/7.57\r\ntickdata[tickdata$PlotYear ==202017,]$density <- length(broods[broods$PlotYear == 202017,]$BroodID)/8.57\r\ntickdata[tickdata$PlotYear ==212017,]$density <- length(broods[broods$PlotYear == 212017,]$BroodID)/9.42\r\n\r\n#2018 Data#\r\ntickdata[tickdata$PlotYear ==102018,]$density <- length(broods[broods$PlotYear == 102018,]$BroodID)/8.76\r\ntickdata[tickdata$PlotYear ==112018,]$density <- length(broods[broods$PlotYear == 112018,]$BroodID)/9.1\r\ntickdata[tickdata$PlotYear ==122018,]$density <- length(broods[broods$PlotYear == 122018,]$BroodID)/9.71\r\ntickdata[tickdata$PlotYear ==132018,]$density <- length(broods[broods$PlotYear == 132018,]$BroodID)/8.31\r\ntickdata[tickdata$PlotYear ==142018,]$density <- length(broods[broods$PlotYear == 142018,]$BroodID)/8.98\r\ntickdata[tickdata$PlotYear ==152018,]$density <- length(broods[broods$PlotYear == 152018,]$BroodID)/8.38\r\ntickdata[tickdata$PlotYear ==172018,]$density <- length(broods[broods$PlotYear == 172018,]$BroodID)/9.06\r\ntickdata[tickdata$PlotYear ==182018,]$density <- length(broods[broods$PlotYear == 182018,]$BroodID)/9.19\r\ntickdata[tickdata$PlotYear ==192018,]$density <- length(broods[broods$PlotYear == 192018,]$BroodID)/7.57\r\ntickdata[tickdata$PlotYear ==202018,]$density <- length(broods[broods$PlotYear == 202018,]$BroodID)/8.57\r\ntickdata[tickdata$PlotYear ==212018,]$density <- length(broods[broods$PlotYear == 212018,]$BroodID)/9.42\r\n\r\n#2019 Data#\r\ntickdata[tickdata$PlotYear ==102019,]$density <- length(broods[broods$PlotYear == 102019,]$BroodID)/8.76\r\ntickdata[tickdata$PlotYear ==112019,]$density <- length(broods[broods$PlotYear == 112019,]$BroodID)/9.1\r\ntickdata[tickdata$PlotYear ==122019,]$density <- length(broods[broods$PlotYear == 122019,]$BroodID)/9.71\r\ntickdata[tickdata$PlotYear ==132019,]$density <- length(broods[broods$PlotYear == 132019,]$BroodID)/8.31\r\ntickdata[tickdata$PlotYear ==142019,]$density <- length(broods[broods$PlotYear == 142019,]$BroodID)/8.98\r\ntickdata[tickdata$PlotYear ==152019,]$density <- length(broods[broods$PlotYear == 152019,]$BroodID)/8.38\r\ntickdata[tickdata$PlotYear ==172019,]$density <- length(broods[broods$PlotYear == 172019,]$BroodID)/9.06\r\ntickdata[tickdata$PlotYear ==182019,]$density <- length(broods[broods$PlotYear == 182019,]$BroodID)/9.19\r\ntickdata[tickdata$PlotYear ==192019,]$density <- length(broods[broods$PlotYear == 192019,]$BroodID)/7.57\r\ntickdata[tickdata$PlotYear ==202019,]$density <- length(broods[broods$PlotYear == 202019,]$BroodID)/8.57\r\ntickdata[tickdata$PlotYear ==212019,]$density <- length(broods[broods$PlotYear == 212019,]$BroodID)/9.42\r\n\r\n##Disturbance Data Set Up for BLUP Analysis##\r\n#Load Both Main and Sub Datasets#\r\nDistMain<-read.csv(""DisturbanceMain_072019.csv"", \r\n                   header = TRUE, \r\n                   sep = "","", \r\n                   quote=""\\"""", \r\n                   dec=""."", \r\n                   fill = TRUE, \r\n                   comment.char="""")\r\nDistSub<-read.csv(""DisturbanceSub_072019.csv"", \r\n                  header = TRUE, \r\n                  sep = "","", \r\n                  quote=""\\"""", \r\n                  dec=""."", \r\n                  fill = TRUE, \r\n                  comment.char="""")\r\n\r\n#Add PlotYear to Dist Main#\r\nDistMain$PlotYear <- paste(DistMain$DisturbancePlot, DistMain$DisturbanceYear, sep = """") \r\nDistMain$YearDay <- paste(DistMain$DisturbanceYear, DistMain$DisturbanceDay, sep = """")\r\n\r\n#Merge Distrubance Files#\r\nDist<-merge(DistMain, DistSub, by.x=c(""DisturbanceID""), by.y=c(""DisturbanceID""), all=TRUE)\r\n\r\n\r\n#Modify NA\'s to 0#\r\nfor(i in 1:length(Dist$NumberOfDisturbanceSubjects)){\r\n  if(is.na(Dist$NumberOfDisturbanceSubjects[i])== TRUE){\r\n    Dist$NumberOfDisturbanceSubjects[i] <- 0\r\n  }\r\n}\r\n\r\nAprilDay<-read.csv(""AprilDay.csv"", header = TRUE, sep = "","", quote=""\\"""", dec=""."",\r\n                   fill = TRUE, comment.char="""")\r\n\r\nDist<-merge(Dist, AprilDay, by.x=c(""YearDay""), by.y=c(""YearDay""), all = TRUE)\r\n\r\nDist<- Dist[is.na(Dist$DisturbanceID) == F,]\r\n#Calculate work times#\r\nDist$DisturbanceTimeStart <- as.POSIXct(Dist$DisturbanceTimeStart, format=\'%H:%M\')\r\nDist$DisturbanceTimeStop <- as.POSIXct(Dist$DisturbanceTimeStop, format=\'%H:%M\')\r\n\r\nDist[""WorkTime""]<-NA\r\nDist$WorkTime <- difftime(Dist$DisturbanceTimeStop, Dist$DisturbanceTimeStart, units = ""hours"")\r\n\r\nDist$Sunrise <- as.POSIXct(Dist$Sunrise,\r\n                           \r\n                           format=\'%H:%M:\')\r\n\r\nDist$Sunset <- as.POSIXct(Dist$Sunset,\r\n                          \r\n                          format=\'%H:%M\')\r\n\r\nDist[""TimefromSunrise""]<-NA\r\n\r\nDist$TimefromSunrise <- (Dist$DisturbanceTimeStart - Dist$Sunrise)\r\n\r\n#Presence of Humans#\r\nDist$humanpresence<-NA\r\nfor(k in 1:length(Dist$humanpresence)){\r\n  if(isTRUE(Dist$NumberOfDisturbanceSubjects[k] >= 1) == T){\r\n    Dist$humanpresence[k]<-1\r\n  } else {\r\n    Dist$humanpresence[k]<-0\r\n  }\r\n}\r\n\r\nstr(Dist)\r\nDist$PlotYear <- as.factor(Dist$PlotYear)\r\n\r\na#Should ask Alex if I have included all important factors, Disturbance Model for BLUPs#\r\ndist_mod<-glmer(humanpresence~1 +\r\n                  DisturbanceNumberOfObservers +\r\n                  WorkTime +\r\n                  TimefromSunrise +\r\n                  (1|PlotYear)+\r\n                  (1|DisturbanceDay),\r\n                Dist,\r\n                family = binomial,\r\n                glmerControl(optimizer=""bobyqa""))\r\n\r\n#Meed to check model fit and ask Alex how I remove the random effects\r\n#Model Checking \r\npar(mfrow = c(2,2))\r\nsummary(dist_mod)\r\nhist(resid(dist_mod), main = ""Distribution of Residuals"")\r\nacf(resid(dist_mod), main = ""ACF Plot"")\r\nscatter.smooth(fitted(dist_mod),resid(dist_mod), main = ""Tukey-Anscombe Plot"", xlab = ""Fitted Values"", ylab = ""Residual Values"")\r\nqqnorm(resid(dist_mod))\r\nqqline(resid(dist_mod))\r\n\r\ndist_sim<-sim(dist_mod, n.sim = 5000)\r\n\r\napply(dist_sim@fixef, 2, mean) \r\napply(dist_sim@fixef, 2, quantile, prob=c(0.025, 0.975))\r\nquantile (apply(dist_sim@ranef$PlotYear[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nquantile (apply(dist_sim@ranef$DisturbanceDay[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\ndist_residual<-dispersion_glmer(dist_mod)*((pi^2)/3)\r\n\r\n\r\ncorrecteddistval<-data.frame(ranef(dist_mod))\r\ncorrecteddistval<- correcteddistval[correcteddistval$grpvar == ""PlotYear"",]\r\n\r\n\r\n#Merging Corrected Disturbance Values and Data Fixing#\r\ntickdata<-merge(tickdata, correcteddistval, by.x=c(""PlotYear""), by.y=c(""grp""), all=TRUE)\r\n\r\ntickdata<-tickdata[is.na(tickdata$TickYear) == F,]\r\ntickdata$cor_dist<- tickdata$condval\r\ntickdata$grpvar<- NULL\r\ntickdata$term<- NULL\r\ntickdata$condval<- NULL\r\ntickdata$condsd<- NULL\r\n\r\ntickdata$cor_dist_mc <- NA\r\ntickdata$cor_dist_mc <- (tickdata$cor_dist - mean(tickdata$cor_dist))/sd(tickdata$cor_dist)\r\n\r\ntickdata$PlotYear<- paste(tickdata$Plot, tickdata$TickYear, sep = """")\r\ntickdata$PlotYear<- as.factor(tickdata$PlotYear)\r\nlevels(tickdata$PlotYear)\r\n\r\n#Adding tick presence#\r\ntickdata$TickPresence<-NA\r\nfor(i in 1:length(tickdata$NumberTicks)){\r\n  if(isTRUE(tickdata$NumberTicks[i] >= 1) == T){\r\n    tickdata$TickPresence[i] <- 1 \r\n  } else {\r\n    tickdata$TickPresence[i] <- 0\r\n  }\r\n}\r\n\r\n#Adding Exploratory Behavior#\r\nexpbehave<-read.csv(""Exploration_072019.csv"", dec = "","")\r\nstr(expbehave)\r\nexpbehave$PlotYear <- paste(expbehave$Plot, expbehave$ExpYear, sep = """")\r\nexpbehave$PlotYear <- as.factor(expbehave$PlotYear)\r\nexpbehave$RingNumber <- as.character(expbehave$RingNumber)\r\nexp_mod<- lmer(ExpScore ~ 1 +\r\n                   Sequence +\r\n                   (1|RingNumber) +\r\n                   (1|PlotYear),\r\n                   data = expbehave)\r\n\r\npar(mfrow = c(2,2))\r\nhist(resid(exp_mod), main = ""Distribution of Residuals"")\r\nacf(resid(exp_mod), main = ""ACF Plot"")\r\nscatter.smooth(fitted(exp_mod),resid(exp_mod), main = ""Tukey-Anscombe Plot"", xlab = ""Fitted Values"", ylab = ""Residual Values"")\r\nqqnorm(resid(exp_mod))\r\nqqline(resid(exp_mod))\r\nexp_sim<-sim(exp_mod, n.sim = 5000)\r\n\r\napply(exp_sim@fixef, 2, mean) \r\napply(exp_sim@fixef, 2, quantile, prob=c(0.025, 0.975))\r\nquantile(exp_sim@sigma, prob=c(0.025, 0.5, 0.975))\r\n\r\nquantile (apply(exp_sim@ranef$RingNumber[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nquantile (apply(exp_sim@ranef$PlotYear[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\n\r\n\r\ncor_exp_val<-data.frame(ranef(exp_mod))\r\ncor_exp_val<- cor_exp_val[cor_exp_val$grpvar == ""RingNumber"",]\r\npar(mfrow = c(1,1))\r\ntickdata<-merge(tickdata, cor_exp_val, by.x=c(""RingNumber""), by.y=c(""grp""), all=TRUE)\r\ntickdata<- tickdata[is.na(tickdata$PlotYear) == F,]\r\nexpbehave<-merge(expbehave, cor_exp_val, by.x=c(""RingNumber""), by.y=c(""grp""), all=TRUE)\r\n\r\n\r\ntickdata$exp_cor<- NA\r\ntickdata$exp_cor<- tickdata$condval\r\ntickdata$grpvar<- NULL\r\ntickdata$term<- NULL\r\ntickdata$condval<- NULL\r\ntickdata$condsd<- NULL\r\ntickdata<- tickdata[is.na(tickdata$exp_cor) == F,]\r\n\r\ntickdata$exp_cor_mc <- (tickdata$exp_cor - mean(tickdata[is.na(tickdata$exp_cor) == F,]$exp_cor))/sd(tickdata[is.na(tickdata$exp_cor) == F,]$exp_cor) \r\ntickdata$ExpScore_mc <- (tickdata$ExpScore - mean(tickdata[is.na(tickdata$ExpScore) == F,]$ExpScore))/sd(tickdata[is.na(tickdata$ExpScore) == F,]$ExpScore) \r\n\r\n\r\n#Mean Centering#\r\n\r\n#Calculating Body Condition#\r\npar(mfrow = c(2,1))\r\nallbirds <- allcatches\r\n#Remove Missing Data#\r\nstr(allbirds)\r\nallbirds <- allbirds[allbirds$Tarsus > 0,]\r\nallbirds <- allbirds[allbirds$BodyMassField > 0,]\r\nallbirds <- allbirds[is.na(allbirds$SexConclusion) == F,]\r\n\r\n#Plots of Log Mass to Log Tarsus#\r\nplot(log(allbirds[allbirds$SexConclusion == 1,]$Tarsus), log(allbirds[allbirds$SexConclusion == 1,]$BodyMassField))\r\nplot(log(allbirds[allbirds$SexConclusion == 2,]$Tarsus), log(allbirds[allbirds$SexConclusion == 2,]$BodyMassField))\r\n\r\n#Linear Model based on Sex#\r\ncondition_1<-lm(log(allbirds[allbirds$SexConclusion == 1,]$BodyMassField)~log(allbirds[allbirds$SexConclusion == 1,]$Tarsus))\r\ncondition_2<-lm(log(allbirds[allbirds$SexConclusion == 2,]$BodyMassField)~log(allbirds[allbirds$SexConclusion == 2,]$Tarsus))\r\n\r\nsummary(condition_1)\r\nsummary(condition_2)\r\n\r\ncondition_1\r\ncondition_2\r\n\r\n#Variables for Body Condition from Peig & Green, (2009)#\r\nbols_1<-  1.3109\r\nr_1<-sqrt(0.09635)\r\nbols_2<- 1.3065\r\nr_2<-sqrt(0.1118)\r\n\r\nbsma_1 <- bols_1/r_1 \r\nbsma_2 <- bols_2/r_2\r\n\r\n\r\n\r\nallbirds$scaledmass <- NA\r\nallbirds<- allbirds[is.na(allbirds$Tarsus) ==F,]\r\nallbirds[allbirds$SexConclusion == 1,]$scaledmass<-allbirds[allbirds$SexConclusion == 1,]$BodyMassField*((mean(allbirds[allbirds$SexConclusion == 1,]$Tarsus)/allbirds[allbirds$SexConclusion == 1,]$Tarsus)^bsma_1)\r\nallbirds[allbirds$SexConclusion == 2,]$scaledmass<-allbirds[allbirds$SexConclusion == 2,]$BodyMassField*((mean(allbirds[allbirds$SexConclusion == 2,]$Tarsus)/allbirds[allbirds$SexConclusion == 2,]$Tarsus)^bsma_2)\r\nallbirds<- allbirds[is.na(allbirds$scaledmass) ==F,]\r\nrownames(allbirds)<-1:4273\r\n\r\nallbirds$RingNumber <- as.character(allbirds$RingNumber)\r\nringnumbers<- allbirds$RingNumber\r\nallbirds$scaledmass_m <- NA\r\nn<-1\r\nfor(k in 1:length(ringnumbers)){\r\n  allbirds$scaledmass_m[n] <- mean(allbirds[allbirds$RingNumber == ringnumbers[k],]$scaledmass)\r\nn<- n+1\r\n  }\r\n\r\nringnumbers<- tickdata$RingNumber\r\ntickdata$scaledmass_m <- NA\r\nn<-1\r\nfor(r in 1:length(ringnumbers)){\r\n    tickdata$scaledmass_m[n] <- mean(allbirds[allbirds$RingNumber == ringnumbers[r],]$scaledmass_m)  \r\n  n<- n+1\r\n}\r\n\r\ntickdata$scaledmass_m_mc<- NA\r\ntickdata[tickdata$SexConclusion == 1,]$scaledmass_m_mc<- (tickdata[tickdata$SexConclusion == 1,]$scaledmass_m - mean(tickdata[tickdata$SexConclusion == 1,]$scaledmass_m))/sd(tickdata[tickdata$SexConclusion == 1,]$scaledmass_m)\r\ntickdata[tickdata$SexConclusion == 2,]$scaledmass_m_mc<- (tickdata[tickdata$SexConclusion == 2,]$scaledmass_m - mean(tickdata[tickdata$SexConclusion == 2,]$scaledmass_m))/sd(tickdata[tickdata$SexConclusion == 2,]$scaledmass_m)\r\n\r\ntickdata$scaledmass_dev_mc <- (tickdata$scaledmass_m - mean(tickdata$scaledmass_m))/sd(tickdata$scaledmass_m)\r\n\r\n#Density Centering#\r\ndensforcalc <- read.csv(""Density_AllPlotYears.csv"")\r\ndensforcalc$PlotYear <- paste(densforcalc$Plot, densforcalc$Year, sep = """")\r\ndensforcalc$N_Birds <- NA\r\ndensforcalc$Density <- NA\r\nplotyears<- densforcalc$PlotYear\r\n\r\n#Calculate Density for each Plot Year#\r\nfor(h in 1:length(plotyears)){\r\n  densforcalc$N_Birds[h] <- length(broods[broods$PlotYear == plotyears[h],]$BroodID)\r\n}\r\ndensforcalc$Density <- densforcalc$N_Birds/densforcalc$Area\r\n\r\n#Spatial Density (Average density of a plot over all years)#\r\ntickdata$sdensity<-NA\r\nplots <- c(10,11,12,13,14,15,16,17,18,19,20,21) \r\nfor(i in 1:length(plots)){\r\n  tickdata[tickdata$Plot == plots[i],]$sdensity <-  mean(densforcalc[densforcalc$Plot == plots[i],]$Density)\r\n}\r\n\r\ntickdata$sdensity_mc <- (tickdata$sdensity - mean(tickdata$sdensity))/sd(tickdata$sdensity)\r\n#Temporal Density (Difference in PlotYear Density to Plot Average)#\r\ntickdata$tdensity<-NA\r\ntickdata$tdensity <- tickdata$density - tickdata$sdensity\r\n\r\ntickdata$tdensity_mc <- (tickdata$tdensity - mean(tickdata$tdensity))/sd(tickdata$tdensity)\r\n\r\n#Age centering#\r\nringnumbers<- allbirds$RingNumber\r\nallbirds$meanage <- NA\r\nn<-1\r\nfor(k in 1:length(ringnumbers)){\r\n  allbirds$meanage[n] <- mean(allbirds[allbirds$RingNumber == ringnumbers[k],]$AgeObserved)\r\n  n<- n+1\r\n}\r\n\r\nringnumbers<- tickdata$RingNumber\r\ntickdata$meanage <- NA\r\nn<-1\r\nfor(r in 1:length(ringnumbers)){\r\n  tickdata$meanage[n] <- mean(allbirds[allbirds$RingNumber == ringnumbers[r],]$meanage)  \r\n  n<- n+1\r\n}\r\ntickdata$meanage_mc <- (tickdata$meanage - mean(tickdata$meanage))/sd(tickdata$meanage)\r\ntickdata$agedev<- tickdata$BirdAge - tickdata$meanage\r\ntickdata$agedev_mc <- (tickdata$agedev - mean(tickdata$agedev))/sd(tickdata$agedev)\r\nhist(tickdata$agedev)\r\n\r\n\r\n#adding unsplit values for test#\r\ntickdata$density_mc <- (tickdata$density - mean(tickdata$density))/sd(tickdata$density)\r\ntickdata$age_mc <- (tickdata$BirdAge - mean(tickdata$BirdAge))/sd(tickdata$BirdAge)\r\ntickdata$scaledmass <- NA\r\ntickdata[tickdata$SexConclusion == 1,]$scaledmass<-tickdata[tickdata$SexConclusion == 1,]$BodyMassField*((mean(tickdata[tickdata$SexConclusion == 1,]$Tarsus)/tickdata[tickdata$SexConclusion == 1,]$Tarsus)^bsma_1)\r\ntickdata[tickdata$SexConclusion == 2,]$scaledmass<-tickdata[tickdata$SexConclusion == 2,]$BodyMassField*((mean(tickdata[tickdata$SexConclusion == 2,]$Tarsus)/tickdata[tickdata$SexConclusion == 2,]$Tarsus)^bsma_2)\r\ntickdata$scaledmass_mc <- NA\r\ntickdata[tickdata$SexConclusion == 1,]$scaledmass_mc<- (tickdata[tickdata$SexConclusion == 1,]$scaledmass - mean(tickdata[tickdata$SexConclusion == 1,]$scaledmass))/sd(tickdata[tickdata$SexConclusion == 1,]$scaledmass)\r\ntickdata[tickdata$SexConclusion == 2,]$scaledmass_mc<- (tickdata[tickdata$SexConclusion == 2,]$scaledmass - mean(tickdata[tickdata$SexConclusion == 2,]$scaledmass))/sd(tickdata[tickdata$SexConclusion == 2,]$scaledmass)\r\nrownames(tickdata)<-1:783\r\ntickdata <- tickdata[1:783,]\r\n\r\n#Create Non-zero dataset#\r\ntickdata_nonzero<- tickdata[tickdata$NumberTicks != 0,]\r\ntickdata_nonzero<- tickdata_nonzero[is.na(tickdata_nonzero$NumberTicks) == F,]\r\nrownames(tickdata_nonzero)<- 1:505\r\nmean(tickdata_nonzero$NumberTicks)\r\nsd(tickdata_nonzero$NumberTicks)\r\n\r\n#Removing Un-needed columns#\r\nstr(tickdata)\r\ntickdata$grpvar.y <- NULL\r\ntickdata$term.y <- NULL\r\ntickdata$condval.y <- NULL\r\ntickdata$condsd.y <- NULL\r\ntickdata$grpvar.x <- NULL\r\ntickdata$term.x <- NULL\r\ntickdata$condval.x <- NULL\r\ntickdata$condsd.x <- NULL\r\nstr(tickdata)\r\n\r\nhist(tickdata$scaledmass_m)\r\n\r\n##BEGIN OF MODELS FOR PAPER##\r\n#Infestatino probability model including all base calculations#\r\n#ALL VARIABLES#\r\nfull_tickpre<- glmer(TickPresence ~ 1 +\r\n                         cor_dist_mc +\r\n                         ExpScore_mc + \r\n                         scaledmass_mc +\r\n                         SexConclusion +\r\n                         density_mc +\r\n                         age_mc +\r\n                         (1|TickYear) +\r\n                         (1|Plot.x)+\r\n                         (1|PlotYear)+\r\n                         (1|RingNumber)+\r\n                         (1|MeasurementObserver),\r\n                       data = tickdata,\r\n                       family = binomial,\r\n                       glmerControl(optimizer = c(""bobyqa""), optCtrl = list(maxfun = 1e5)))\r\npar(mfrow = c(2,2))\r\nsummary(full_tickpre)\r\nhist(resid(full_tickpre), main = ""Distribution of Residuals"")\r\nacf(resid(full_tickpre), main = ""ACF Plot"")\r\nscatter.smooth(fitted(full_tickpre),resid(full_tickpre), main = ""Tukey-Ascombe Plot"",\r\n               xlab = ""Fitted Values"",\r\n               ylab = ""Residual Values"")\r\nqqnorm(resid(full_tickpre))\r\nqqline(resid(full_tickpre))\r\n\r\nfull_tickpre_sim<-sim(full_tickpre, n.sim = 5000)\r\n\r\napply(full_tickpre_sim@fixef, 2, mean) \r\napply(full_tickpre_sim@fixef, 2, quantile, prob=c(0.025, 0.975)) \r\nquantile (apply(full_tickpre_sim@ranef$TickYear[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nquantile (apply(full_tickpre_sim@ranef$PlotYear[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nquantile (apply(full_tickpre_sim@ranef$Plot.x[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nquantile (apply(full_tickpre_sim@ranef$MeasurementObserver[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nquantile (apply(full_tickpre_sim@ranef$NestBox[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nquantile (apply(full_tickpre_sim@ranef$RingNumber[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nfull_tickpre_res<-dispersion_glmer(full_tickpre)*((pi^2)/3)\r\n#residuals in binomial models are fixed normally to (dispersion*pi^2)/3#\r\n\r\n#ALL VARIABLES#\r\nfull_nonzero<- glmer(NumberTicks ~ 1 +\r\n                       cor_dist_mc +\r\n                       exp_cor_mc + \r\n                       scaledmass_mc +\r\n                       SexConclusion +\r\n                       density_mc +\r\n                       age_mc +\r\n                       (1|TickYear)+\r\n                       (1|Plot.x)+\r\n                       (1|PlotYear)+\r\n                       (1|RingNumber)+\r\n                       (1|MeasurementObserver),\r\n                         data = tickdata_nonzero,\r\n                         family = ""poisson"",\r\n                         glmerControl(optimizer = c(""bobyqa"")))\r\nsummary(full_nonzero)\r\nhist(resid(full_nonzero), main = ""Distribution of Residuals"")\r\nacf(resid(full_nonzero), main = ""ACF Plot"")\r\nscatter.smooth(fitted(full_nonzero),resid(full_nonzero), main = ""Tukey-Ascombe Plot"",\r\n               xlab = ""Fitted Values"",\r\n               ylab = ""Residual Values"")\r\nqqnorm(resid(full_nonzero))\r\nqqline(resid(full_nonzero))\r\n\r\nfull_nonzero_sim<-sim(full_nonzero, n.sim = 5000)\r\n\r\napply(full_nonzero_sim@fixef, 2, mean) \r\napply(full_nonzero_sim@fixef, 2, quantile, prob=c(0.025, 0.975))\r\n\r\nquantile (apply(full_nonzero_sim@ranef$TickYear[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nquantile (apply(full_nonzero_sim@ranef$Plot.x[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nquantile (apply(full_nonzero_sim@ranef$PlotYear[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nquantile (apply(full_nonzero_sim@ranef$MeasurementObserver[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nquantile (apply(full_nonzero_sim@ranef$NestBox[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\nquantile (apply(full_nonzero_sim@ranef$RingNumber[ , , 1],1,var), prob=c(0.025, 0.5, 0.975))\r\n\r\n#to get resdiual range, extract intercept midpoint, low limit, and high limit from summmary() and past below. Then change from full_nonzero_intm/l/h#\r\nfull_nonzero_intm<-  1.161681175\r\nfull_nonzero_intl <-  0.6680645\r\nfull_nonzero_inth <-  1.6445485 \r\nfull_nonzero_res<-(dispersion_glmer(full_nonzero) + log(1/exp(full_nonzero_intm)+1))\r\nfull_nonzero_res']","Repeatable differences in exploratory behaviour predict tick infestation probability in wild great tits Ecological factors and individual-specific traits affect parasite infestation in wild animals. We studied various key ecological variables (breeding density, human disturbance) and phenotypic traits (exploratory behaviour, body condition) proposed to predict tick infestation probability and burden in great tits (Parus major). Our study spanned three years and 12 nest-box plots located in southern Germany. Adult breeders were assessed for exploration behaviour, body condition, and tick burden. Plots were open to human recreation; human disturbance was quantified in each plot as a recreation pressure index from biweekly nest box inspections. Infested individuals were repeatable in tick burden across years. These repeatable among-individual differences in tick burden were not attributable to exploration behaviour. However, faster explorers did have a higher infestation probability. Furthermore, body condition negatively correlated to tick burden. Recreation pressure also tended to increase infestation probability. Our study implies that avian infestation probability and tick burden are each governed by distinct phenotypic traits and ecological factors. Our findings highlight the importance of animal behaviour and human disturbance in understanding variation in tick burden among avian hosts.",3
Data from: Removal of an apex predator initiates a trophic cascade that extends from herbivores to vegetation and the soil nutrient pool,"It is widely assumed that organisms at low trophic levels, particularly microbes and plants, are essential to basic services in ecosystems, such as nutrient cycling. In theory, apex predators' effects on ecosystems could extend to nutrient cycling and the soil nutrient pool by influencing the intensity and spatial organization of herbivory. Here, we take advantage of a long-term manipulation of dingo abundance across Australia's dingo-proof fence in the Strzelecki Desert to investigate the effects that removal of an apex predator has on herbivore abundance, vegetation and the soil nutrient pool. Results showed that kangaroos were more abundant where dingoes were rare, and effects of kangaroo exclusion on vegetation, and total carbon, total nitrogen and available phosphorus in the soil were marked where dingoes were rare, but negligible where dingoes were common. By showing that a trophic cascade resulting from an apex predator's lethal effects on herbivores extends to the soil nutrient pool, we demonstrate a hitherto unappreciated pathway via which predators can influence nutrient dynamics. A key implication of our study is the vast spatial scale across which apex predators' effects on herbivore populations operate and, in turn, effects on the soil nutrient pool and ecosystem productivity could become manifest.","['# Analysing the cascading effects of dingoes\r\n# Author: Timothy Morris\r\n# Last edited: 1/04/2017\r\n# Contact: tim.morris.11@gmail.com\r\n\r\n\r\n# Load packages\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(magrittr)\r\nlibrary(reshape2)\r\nlibrary(MASS)\r\nlibrary(betareg)\r\nlibrary(rcompanion)\r\nlibrary(lme4)\r\nlibrary(nlme)\r\n\r\n\r\n# Analysing counts of dingoes and kangaroos\r\n\r\n# Importing the data\r\nsdat <- read.csv(""Morris_Letnic_Data_1_Fauna.csv"")\r\n\r\n# Preparing data\r\nsdat$date <- as.Date(sdat$date, format=""%d/%m/%Y"")\r\nsdat[c(""site"", ""run"", ""X"", ""X.1"")] <- NULL\r\nsdat$fence <- factor(sdat$fence, levels = c(""out"", ""in""))\r\n\r\nmsum <- sdat\r\nmsum$dingo <- msum$dingo/msum$km\r\nmsum$roo <- msum$roo/msum$km\r\nmsum$km <- NULL\r\n\r\n# Summarising data for plotting\r\nmdat <- melt(msum, id=c(""date"", ""fence""))\r\nssum <- mdat %>%\r\n  group_by(date, fence, variable) %>%\r\n  summarise(mean = mean(value),\r\n            sem = sd(value)/sqrt(length(value)))\r\n\r\n# Plotting data (Figure 2)\r\npar(mar = c(2.6, 4.2, 1, 1))\r\nm <- matrix(c(1:2), nrow=2,ncol=1,byrow = TRUE)\r\nlayout(mat = m)\r\nfor (i in unique(ssum$variable)) {\r\n  df <- subset(ssum, variable == i)\r\n  colors <- c(""black"", ""white""); names(colors) <- unique(ssum$fence)\r\n  tags <- c(""(a) dingo"", ""(b) kangaroo""); names(tags) <- unique(ssum$variable)\r\n  t <- with(df, (max(mean+sem)))\r\n  with(df, plot(date, mean, ylim=c(0, t), type=""n"", ylab=""Mean count / km"", xlab="""", las=1, cex.lab=1.2, cex.axis=1, bty=""n""))\r\n  mtext(tags[i], side = 3, adj = 0.2, line=-1.7, cex=1.4)\r\n  if(i==""dingo""){legend(""topright"", legend = c(""outside"", ""inside""), cex=1.2, pch=21, pt.bg=colors, pt.cex=1.4, lwd = 1.6,  bty=""n"")}\r\n  for (j in unique(df$fence)) {\r\n    dff <- subset(df, fence == j)\r\n    with(dff, arrows(date, mean-sem, date, mean+sem, lwd = 1.5, angle = 90, code = 3, length = 0.05))\r\n    with(dff, lines(date, mean, type=""o"", pch=21, bg=colors[j], cex=1.4, lwd=1.6))\r\n    box(bty=\'L\')\r\n  }\r\n}\r\ndev.off()\r\n\r\n# Preparing data for analysis\r\nsdat$trip <- factor(sdat$date, labels = 1:14)\r\n\r\n# Negative binomial models\r\nm1 <- glm.nb(dingo ~ fence*trip + offset(log(km)), data = sdat) # dingo\r\nanova(m1, test=""Chisq"")\r\n\r\nm2 <- glm.nb(roo ~ fence*trip + offset(log(km)), data = sdat) # kangaroo\r\nanova(m2, test=""Chisq"")\r\n\r\n\r\n\r\n# Analysing dingo scats\r\n\r\n# Importing the data\r\nddat <- read.csv(""Morris_Letnic_Data_2_Diet.csv"")\r\n\r\n# Preparing data\r\nddat[c(""X"", ""X.1"")] <- NULL\r\nddat$tr <- (ddat$rooindiet * (nrow(ddat) - 1) + 0.5) / nrow(ddat)\r\n\r\n# Beta regression model\r\nm3 <- betareg(tr ~ rooabundance, data = ddat, link=""logit"")\r\nsummary(m3)\r\n\r\n# Plotting beta regression model (Supplementary material, Figure S2)\r\npar(mar = c(5, 6, 2, 2))\r\nplotPredy(data  = ddat,\r\n          y     = tr,\r\n          x     = rooabundance,\r\n          model = m3,\r\n          col = ""black"",\r\n          xlab  = ""Kangaroos sighted per km"",\r\n          ylab  = ""Proportion of scats containing \\n kangaroo"",\r\n          cex = 2.2,\r\n          pch = ifelse(ddat$dingo == ""common"", 19, 21),\r\n          las=1, \r\n          cex.axis=1.4,\r\n          cex.lab=1.6,\r\n          bty = ""l""\r\n)\r\ndev.off()\r\n\r\n\r\n# Analysing vegetation cover\r\n\r\n# Importing the data\r\nvdat <- read.csv(""Morris_Letnic_Data_3_Vegetation.csv"", na.strings = """")\r\n\r\n# Preparing data\r\nvdat$date <- as.Date(vdat$date, format=""%d/%m/%Y"")\r\nvdat$treatloc <- factor(vdat$treatloc)\r\nvdat$quadno <- factor(vdat$quadno)\r\nvdat[c(""site"", ""X"", ""X.1"")] <- NULL\r\nvdat$fence <- factor(vdat$fence, levels = c(""out"", ""in""))\r\nvdat$treatment <- factor(vdat$treatment, levels = c(""exclosure"", ""control""))\r\n\r\n# Summarising data for plotting\r\nvsum <- vdat %>%\r\n  na.omit() %>%\r\n  group_by(date, fence, treatment) %>%\r\n  summarise(mean = 100*mean(totalveg),\r\n            sem = 100*sd(totalveg)/sqrt(length(totalveg)))\r\n\r\n# Plotting data (Figure 3)\r\npar(mar = c(2.6, 4.2, 1, 1))\r\nm <- matrix(c(1:2), nrow=2,ncol=1,byrow = TRUE)\r\nlayout(mat = m)\r\nfor (i in unique(vsum$fence)) {\r\n  df <- subset(vsum, fence == i)\r\n  colors <- c(""black"", ""white""); names(colors) <- unique(vsum$treatment)\r\n  tags <- c(""(a) dingoes common"", ""(b) dingoes rare""); names(tags) <- unique(vsum$fence)\r\n  t <- with(df, (max(mean+sem)))\r\n  b <- with(df, (min(mean-sem)))\r\n  with(df, plot(date, mean, ylim=c(b, t+5), type=""n"", ylab=""Mean vegetation cover (%)"", xlab="""", las=1, cex.lab=1.2, cex.axis=1, bty=""n""))\r\n  mtext(tags[i], side = 3, adj = 0.05, line=-1.7, cex=1.4)\r\n  if(i==""out""){legend(""bottomright"", legend = c(""ungrazed"", ""grazed""), cex=1.2, pch=21, pt.bg=colors, pt.cex=1.4, lwd = 1.6, bty=""n"")}\r\n  for (j in unique(df$treatment)) {\r\n    dff <- subset(df, treatment == j)\r\n    with(dff, arrows(date, mean-sem, date, mean+sem, lwd = 1.5, angle = 90, code = 3, length = 0.05))\r\n    with(dff, lines(date, mean, type=""o"", pch=21, bg=colors[j], cex=1.4, lwd=1.6))\r\n    box(bty=\'L\')\r\n  }\r\n}\r\ndev.off()\r\n\r\n# Preparing data\r\nvdat$trip <- factor(vdat$date, labels = 1:8)\r\nvdat$treatment <- relevel(vdat$treatment, ref=""control"")\r\nvdat$plotid <- with(vdat, factor(treatloc:treatment))\r\nvout <- subset(vdat, fence == ""out"")\r\nvin <- subset(vdat, fence == ""in"")\r\n\r\n# Linear mixed effect models\r\ni1 <- lmer(sqrt(totalveg)~1 + (trip|plotid), data=vin, REML = FALSE,lmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))\r\ni2 <- lmer(sqrt(totalveg)~treatment + (trip|plotid), data=vin, REML = FALSE, lmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))\r\ni3 <- lmer(sqrt(totalveg)~treatment + trip + (trip|plotid), data=vin, REML = FALSE,lmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))\r\ni4 <- lmer(sqrt(totalveg)~treatment * trip + (trip|plotid), data=vin, REML = FALSE,lmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))\r\nanova(i1,i2,i3,i4)\r\n\r\no1 <- lmer(sqrt(totalveg)~1 + (trip|plotid), data=vout, REML = FALSE,lmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))\r\no2 <- lmer(sqrt(totalveg)~treatment + (trip|plotid), data=vout, REML = FALSE,lmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))\r\no3 <- lmer(sqrt(totalveg)~treatment + trip + (trip|plotid), data=vout, REML = FALSE,lmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))\r\no4 <- lmer(sqrt(totalveg)~treatment * trip + (trip|plotid), data=vout, REML = FALSE,lmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))\r\nanova(o1,o2,o3,o4)\r\n\r\n\r\n# Analysing soil nutrients\r\n\r\n# Importing the data\r\nndat <- read.csv(""Morris_Letnic_Data_4_Soil.csv"")\r\n\r\n# Preparing the data\r\nndat$date <- as.Date(ndat$date, format=""%d/%m/%Y"")\r\nndat$treatloc <- factor(ndat$treatloc)\r\nndat[c(""site"", ""X"", ""X.1"")] <- NULL\r\nndat$c <- ndat$c*100\r\nndat$n <- ndat$n*100\r\nndat$fence <- factor(ndat$fence, levels = c(""out"", ""in""))\r\nndat$treatment <- factor(ndat$treatment, levels = c(""exclosure"", ""control""))\r\n\r\nnsum <- ndat\r\nnsum$treatloc <- NULL\r\n\r\n# Summarising the data for plotting\r\nmelt <- melt(nsum, id=c(""date"", ""fence"", ""treatment""))\r\nssum <- melt %>%\r\n  group_by(date, fence, treatment, variable) %>%\r\n  summarise(mean = mean(value),\r\n            sem = sd(value)/sqrt(length(value)))\r\n\r\nssum$date <- factor(ssum$date, labels = c(""Jul 2015"", ""Nov 2015"", ""Jun 2016""))\r\n\r\n# Plotting data (Figure 4)\r\npar(mar = c(3, 6, 2, 1.8))\r\nm <- matrix(c(1,4,2,5,3,6), nrow=3,ncol=2,byrow = TRUE)\r\nlayout(mat = m)\r\nfor (i in unique(ssum$fence)) {\r\n  sf <- subset(ssum, fence == i)\r\n  for (j in unique(sf$variable)) {\r\n    sff <- subset(sf, variable == j)\r\n    ylabels <- c(""Mean Available Phosphorus (mg / kg)"", ""Mean Total Carbon (%)"", ""Mean Total Nitrogen (%)""); names(ylabels) <- unique(ssum$variable)\r\n    ylabel <- if(i==""out""){ylabels[j]}\r\n    colors <- c(""gray"", ""white""); names(colors) <- unique(ssum$treatment)\r\n    olab <- c(""a"", ""c"", ""e""); ilab <- c(""b"", ""d"", ""f"")\r\n    hp <- paste0(""("",olab,"") dingoes common"");names(hp) <- unique(ssum$variable); lp <- paste0(""("",ilab,"") dingoes rare""); names(ilab) <- unique(ssum$variable)\r\n    tags <- cbind(hp, lp); colnames(tags) <- unique(ssum$fence)\r\n    t <- with(sff, (max(mean+sem)))\r\n    b <- with(sff, (min(mean-sem)))\r\n    mt <- matrix(sff$mean, nrow = 2, ncol = 3, byrow = FALSE, dimnames = list(c(unique(sf$treatment)), unique(sf$date)))\r\n    bp <- barplot(mt, beside = TRUE, ylab = ylabel, col = colors, ylim=c(0,t+t/6),las=1, cex.lab=1.8, cex.axis=1.6,\r\n                  axis.lty = 1, cex.names=1.6, mgp=c(4,1,0))\r\n    with(sff, arrows(bp, mean-sem, bp, mean+sem, lwd = 1.5, angle = 90, code = 3, length = 0.05))\r\n    mtext(tags[j, i], side = 3, adj = 0.1, line=-1.8, cex=1.4)\r\n    if(i==""out"" & j == ""p""){legend(""topright"", legend = c(""Ungrazed"", ""Grazed""), cex=1.9, fill = colors,  bty=""n"")}\r\n    bp\r\n    box(bty=\'L\')\r\n  }\r\n}\r\ndev.off()\r\n\r\n# Preparing data\r\nndat$trip <- factor(ndat$date, labels = 1:3)\r\nndat$treatment <- relevel(ndat$treatment, ref=""control"")\r\nndat$plotid <- with(ndat, factor(treatloc:treatment))\r\nnout <- subset(ndat, fence == ""out"")\r\nnin <- subset(ndat, fence == ""in"")\r\n\r\n# Linear mixed effect models\r\n\r\n# Phosphorus - outisde fence\r\npo1 <- lme(p~1, random=~trip|plotid, data=nout, method = ""ML"")\r\npo2 <- lme(p~treatment, random=~trip|plotid, data=nout, method = ""ML"")\r\npo3 <- lme(p~treatment+trip, random=~trip|plotid, data=nout, method = ""ML"")\r\npo4 <- lme(p~treatment*trip, random=~trip|plotid, data=nout, method = ""ML"")\r\nanova(po1,po2,po3,po4)\r\n\r\n# Phosphorus - inside fence\r\npi1 <- lme(p~1, random=~trip|plotid, data=nin, method = ""ML"")\r\npi2 <- lme(p~treatment, random=~trip|plotid, data=nin, method = ""ML"")\r\npi3 <- lme(p~treatment+trip, random=~trip|plotid, data=nin, method = ""ML"")\r\npi4 <- lme(p~treatment*trip, random=~trip|plotid, data=nin, method = ""ML"")\r\nanova(pi1,pi2,pi3,pi4)\r\n\r\n# Carbon - outside fence\r\nco1 <- lme(c~1, random=~trip|plotid, data=nout, method = ""ML"")\r\nco2 <- lme(c~treatment, random=~trip|plotid, data=nout, method = ""ML"")\r\nco3 <- lme(c~treatment+trip, random=~trip|plotid, data=nout, method = ""ML"")\r\nco4 <- lme(c~treatment*trip, random=~trip|plotid, data=nout, method = ""ML"")\r\nanova(co1,co2,co3,co4)\r\n\r\n# Carbon - inside fence\r\nci1 <- lme(c~1, random=~trip|plotid, data=nin, method = ""ML"")\r\nci2 <- lme(c~treatment, random=~trip|plotid, data=nin, method = ""ML"")\r\nci3 <- lme(c~treatment+trip, random=~trip|plotid, data=nin, method = ""ML"")\r\nci4 <- lme(c~treatment*trip, random=~trip|plotid, data=nin, method = ""ML"")\r\nanova(ci1,ci2,ci3,ci4)\r\n\r\n# Nitrogen - outside fence\r\nno1 <- lme(n~1, random=~trip|plotid, data=nout, method = ""ML"")\r\nno2 <- lme(n~treatment, random=~trip|plotid, data=nout, method = ""ML"")\r\nno3 <- lme(n~treatment+trip, random=~trip|plotid, data=nout, method = ""ML"")\r\nno4 <- lme(n~treatment*trip, random=~trip|plotid, data=nout, method = ""ML"")\r\nanova(no1,no2,no3,no4)\r\n\r\n# Nitrogen - inside fence\r\nni1 <- lme(n~1, random=~trip|plotid, data=nin, method = ""ML"")\r\nni2 <- lme(n~treatment, random=~trip|plotid, data=nin, method = ""ML"")\r\nni3 <- lme(n~treatment+trip, random=~trip|plotid, data=nin, method = ""ML"")\r\nni4 <- lme(n~treatment*trip, random=~trip|plotid, data=nin, method = ""ML"")\r\nanova(ni1,ni2,ni3,ni4)\r\n\r\n\r\n\r\n\r\n\r\n']","Data from: Removal of an apex predator initiates a trophic cascade that extends from herbivores to vegetation and the soil nutrient pool It is widely assumed that organisms at low trophic levels, particularly microbes and plants, are essential to basic services in ecosystems, such as nutrient cycling. In theory, apex predators' effects on ecosystems could extend to nutrient cycling and the soil nutrient pool by influencing the intensity and spatial organization of herbivory. Here, we take advantage of a long-term manipulation of dingo abundance across Australia's dingo-proof fence in the Strzelecki Desert to investigate the effects that removal of an apex predator has on herbivore abundance, vegetation and the soil nutrient pool. Results showed that kangaroos were more abundant where dingoes were rare, and effects of kangaroo exclusion on vegetation, and total carbon, total nitrogen and available phosphorus in the soil were marked where dingoes were rare, but negligible where dingoes were common. By showing that a trophic cascade resulting from an apex predator's lethal effects on herbivores extends to the soil nutrient pool, we demonstrate a hitherto unappreciated pathway via which predators can influence nutrient dynamics. A key implication of our study is the vast spatial scale across which apex predators' effects on herbivore populations operate and, in turn, effects on the soil nutrient pool and ecosystem productivity could become manifest.",3
Paradigm uniformity effects on French liaison,"This project includes the supplementary files accompanying the paper ""Paradigm uniformity effects on French liaison"" : study1b-data.csv: this file contains the dataset collected in Study 1. The variables are described in study1b-rscript.Rstudy1b-rscript.R: R script that was used to produce the figures in Study 2 and the corresponding statistical analysisstudy2-data.csv: this file contains the dataset collected in Study 2. The variables are described in study2-rscript.Rstudy2-rscript.R: R script that was used to produce the figures in Study 2 and the corresponding statistical analysis. prosodic-ambiguity-final-2.txt (tab-delimited text file): this file contains the constraint-based analysis of Encrev's (1988) data on liaison enchane and liaison non-enchane. The file can be run with OT-Soft 2.5 (https://linguistics.ucla.edu/people/hayes/otsoft/OTSoftManual_2.5_April_2021.pdf). phonetic-ambiguity-final-2.txt (tab-delimited text file): this file contains the constraint-based analysis of Ct's (2014) data on affrication of stable word-final consonants, stable word-initial initial, and liaison consonants. The file can be run with OT-Soft 2.5.COGETO-prosodic-ambiguity.zip: this folder contains the input file and output files of the CoGeTo analysis of the case study on prosodic ambiguity (see prosodic-ambiguity-final-2.txt). CoGeTo is a software developed by Giorgio Magri and Arto Anttila to explore the typological predictions of constraint-based grammars (https://cogeto.stanford.edu/home).COGETO-phonetic-ambiguity.zip: this folder contains the input file and output files of the CoGeTo analysis of the case study on prosodic ambiguity (see phonetic-ambiguity-final-2.txt). Files linked to the first version of the paper can also be found here (the first version of the paper is available on lingbuzz: https://ling.auf.net/lingbuzz/006457): prosodic-ambiguity.txt (tab-delimited text file): this file contains the constraint-based analysis of Encrev's (1988) data on liaison enchane and liaison non-enchane. The file can be run with OT-Soft 2.5 (https://linguistics.ucla.edu/people/hayes/otsoft/OTSoftManual_2.5_April_2021.pdf). phonetic-ambiguity.txt (tab-delimited text file): this file contains the constraint-based analysis of Ct's (2014) data on affrication of stable word-final consonants, stable word-initial initial, and liaison consonants. The file can be run with OT-Soft 2.5.study1-data.csv: this file contains the dataset collected in Study 1 (first version). The variables are described in study1-rscript.Rstudy1-rscript.R: R script that was used to produce the figures in Study 1 (first version) and the corresponding statistical analysis.","['#####################################################################################################################\n#Packages to be loaded for data manipulation/visualization and statistical analysis\n#####################################################################################################################\n\nif (!require(""plyr"")) {install.packages(""plyr""); require(""plyr"")}#to rename levels of a variable\nif (!require(""ggplot2"")) {install.packages(""ggplot2""); require(""ggplot2"")} #for visualization\nif (!require(""bayestestR"")) {install.packages(""bayestestR""); require(""bayestestR"")} #to calculate 95% credibility interval for Bayesian posterior samples \nif (!require(""utils"")) {install.packages(""utils""); require(""utils"")} #provides the expand.grid function \n#For Bayesian ordinal regression using brms (interface to Stan)\nif (!require(""brms"")) {install.packages(""brms""); require(""brms"")}\npackageVersion(""brms"")\n\n######################################################################################################################\n#Load data\n######################################################################################################################\n\n#Load the dataset\ndatafile = read.csv(""study1-data.csv"", sep ="","", header=TRUE)\nhead(datafile)\ndim(datafile) \n\n#Description of the variables: \n# * Participant: a number corresponding to each participant (23 participants)\n# * Gender: ""male""/""female"" (participant gender)\n# * Age: participant age\n# * Consonant: type of consonant, with 4 levels:  ""final"" (=stable word-final consonant, e.g. nor[m]) /\n#                                                 ""ep-liaison"" (=epenthetic liaison, e.g. gran[t]) /\n#                                                 ""sup-liaison"" (=suppletive liaison, e.g. be[l]) /\n#                                                 ""initial"" (=stable word-initial consonant, e.g. [s]ourire)\n# * Adjective: adjective used in the Adjective-Noun sequence (e.g. ""petit"")\n# * Noun: noun used in the Adjective-Noun sequence (e.g. ""ami"")\n# * AdjNoun: Adjective-Noun sequence (e.g. ""petit ami"")\n# * logFreqw1: log frequency of Word 1 (=Adj). This frequency was obtained using the corpus of movie/TV subtitles OpenSubtitles\n# * logFreqw2conditionedw1: log conditional frequency of Word 2 given Word 1. This frequency was obtained using the OpenSubtitles corpus.\n# * Response:   ""word-final"" (= participant prefers the pronunciation with the consonant prosodically attached at the end of Word 1)\n#               ""word-initial"" (= participant prefers the pronunciation with the consonant prosodically attached at the beginning of Word 2)\n# * ResponseTime (in seconds)\n\n#Turn the variables to be used in the analysis into factors\ndatafile$Participant <- as.factor(datafile$Participant)\ndatafile$Gender <- as.factor(datafile$Gender)\ndatafile$Consonant <- as.factor(datafile$Consonant)\ndatafile$AdjNoun <- as.factor(datafile$AdjNoun)\ndatafile$Response <- as.factor(datafile$Response)\n\n#Reorder the level of Consonant variable: \nlevels(datafile$Consonant)\ndatafile$Consonant <- factor(datafile$Consonant, levels = c(""final"", ""sup-liaison"", ""ep-liaison"", ""initial""))\nlevels(datafile$Consonant)\n\n###################################################################################################################\n#Statistical analysis\n###################################################################################################################\n\n#Dummy coding of Consonant, with final as baseline\ncontrasts(datafile$Consonant) <- contr.treatment(4,1)\n\n#Coding for the Response variable:\ncontrasts(datafile$Response)\n#word-initial is 1 and word-final is 0\n#--> the model will estimate the probability of word-initial attachment.\n\n#Model\nmodel1 <- brm(\n  formula = Response ~ 1 + Consonant + (Consonant|Participant) + (1|AdjNoun),\n  data = datafile,\n  family = bernoulli(""logit""),\n  control = list(adapt_delta = 0.99), #adapt_delta was increased above the default 0.8 value to avoid divergent transitions after warmup. \n  #See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup \n  set.seed(1702) #to be able to reproduce the analysis\n)\n\n#Save the model\n#save(model1, file = ""study1-model.RData"")\n#load(""study1-model.RData"")\n\n#Summary of the model\nsummary(model1)\n\n#Plot the posterior probability of the word-initial response as a function of Consonant\n#This plot shows the posterior mean and posterior 95% credibility interval (see documentation https://cran.r-project.org/web/packages/brms/brms.pdf)\nme <- conditional_effects(model1, effects=""Consonant"")\nresults.plot <- plot(me, plot = FALSE)[[1]] +\n  theme_bw() + theme(text = element_text(size=20)) + ylab(""Posterior probability of attachment to Word 2"")\nresults.plot\n\n#Save the plot as an eps file with dpi = 300\nggsave(file=""study1-Fig1.eps"", dpi = 300)\n\n###################################################################################################################\n#Exploratory analysis 1: check individual variation for ep-liaison\n###################################################################################################################\n\n#To get a better understanding of the variation observed for ep-liaison, we look at how different\n#participants differ in their treatment of ep-liaison. \n\n#Create a dataframe with the following columns: \nid <- levels(datafile$Participant) #id corresponds to the participant identity/number\nmeanLiaison1 <- rep(0, length(levels(datafile$Participant)))\nl95Liaison1 <- rep(0, length(levels(datafile$Participant)))\nu95Liaison1 <- rep(0, length(levels(datafile$Participant)))\nprobmeanLiaison1 <- rep(0, length(levels(datafile$Participant)))\nprobl95Liaison1 <- rep(0, length(levels(datafile$Participant)))\nprobu95Liaison1 <- rep(0, length(levels(datafile$Participant)))\n\ndatafile.individual <- data.frame(cbind(id, meanLiaison1, l95Liaison1, u95Liaison1, probmeanLiaison1, probl95Liaison1, probu95Liaison1),\n                                  stringsAsFactors = FALSE)\n\n#ep-liaison corresponds to Consonant3 in the dataframe with posterior samples\nfor (i in 1:length(datafile.individual$id)){#loop across the 23 participants\n  datafile.individual$meanLiaison1[i] <- mean(posterior_samples(model1)[[\'b_Intercept\']] + posterior_samples(model1)[[\'b_Consonant3\']] + \n                                                posterior_samples(model1)[[paste(""r_Participant["", as.numeric(as.character(datafile.individual$id[i])), "",Intercept]"", sep="""")]] +\n                                                posterior_samples(model1)[[paste(""r_Participant["", as.numeric(as.character(datafile.individual$id[i])), "",Consonant3]"", sep="""")]])\n  datafile.individual$l95Liaison1[i]  <- ci(posterior_samples(model1)[[\'b_Intercept\']] + posterior_samples(model1)[[\'b_Consonant3\']] + \n                                              posterior_samples(model1)[[paste(""r_Participant["", as.numeric(as.character(datafile.individual$id[i])), "",Intercept]"", sep="""")]] +\n                                              posterior_samples(model1)[[paste(""r_Participant["", as.numeric(as.character(datafile.individual$id[i])), "",Consonant3]"", sep="""")]], \n                                            method = ""HDI"", ci=0.95)[[2]]\n  datafile.individual$u95Liaison1[i]  <- ci(posterior_samples(model1)[[\'b_Intercept\']] + posterior_samples(model1)[[\'b_Consonant3\']] + \n                                              posterior_samples(model1)[[paste(""r_Participant["", as.numeric(as.character(datafile.individual$id[i])), "",Intercept]"", sep="""")]] +\n                                              posterior_samples(model1)[[paste(""r_Participant["", as.numeric(as.character(datafile.individual$id[i])), "",Consonant3]"", sep="""")]], \n                                            method = ""HDI"", ci=0.95)[[3]]}\n\n#taking the inverse of the logit: logit = log(p/(1-p)) where p is the probability of word-initial attachment\n#to get the inverse logit: exp(logit) = p/(1-p), so exp(logit) - p*exp(logit) - p = 0 \n#so exp(logit) - p*(exp(logit) + 1) = 0, so exp(logit) =  p*(exp(logit) + 1)\n#so p = exp(logit)/(exp(logit)+1)\n\ndatafile.individual$meanLiaison1 <- round(as.numeric(as.character(datafile.individual$meanLiaison1)), 2)\ndatafile.individual$l95Liaison1 <- round(as.numeric(as.character(datafile.individual$l95Liaison1)), 2)\ndatafile.individual$u95Liaison1 <- round(as.numeric(as.character(datafile.individual$u95Liaison1)), 2)\ntypeof(datafile.individual$id)\n\nfor (i in 1:length(datafile.individual$id)){#loop across the 23 participants\n  datafile.individual$probmeanLiaison1[i] <- exp(datafile.individual$meanLiaison1[i])/(exp(datafile.individual$meanLiaison1[i])+ 1)\n  datafile.individual$probl95Liaison1[i]  <- exp(datafile.individual$l95Liaison1[i])/(exp(datafile.individual$l95Liaison1[i])+ 1)\n  datafile.individual$probu95Liaison1[i]  <- exp(datafile.individual$u95Liaison1[i])/(exp(datafile.individual$u95Liaison1[i])+ 1)}\n\ndatafile.individual$probmeanLiaison1 <- round(as.numeric(as.character(datafile.individual$probmeanLiaison1)), 2)\ndatafile.individual$probl95Liaison1 <- round(as.numeric(as.character(datafile.individual$probl95Liaison1)), 2)\ndatafile.individual$probu95Liaison1 <- round(as.numeric(as.character(datafile.individual$probu95Liaison1)), 2)\n\n#Plot the results for individuals\nindividual.plot <- ggplot(datafile.individual, aes(x=reorder(id, -probmeanLiaison1), y=probmeanLiaison1)) + \n  geom_errorbar(aes(ymin=probl95Liaison1, ymax=probu95Liaison1), width=.1) +\n  geom_point(size=3) + theme_bw() + labs(y=""Posterior probability of attachment to Word 2"", x=""Participant"") + \n  theme(text = element_text(size=20))  + theme(axis.text.x = element_text(angle = 90)) \n\nindividual.plot \n\nggsave(file=""study1-Fig2.eps"", dpi = 300)\n\n###################################################################################################################\n#Exploratory analysis 2: check variation by AdjNoun sequence for ep-liaison\n###################################################################################################################\n\n#To get a better understanding of the variation observed for ep-liaison, we look at how different\n#AdjNoun differ in their treatment of ep-liaison. \n\n#create a dataframe with the following columns: \nAdjNoun <- levels(datafile$AdjNoun)\nmeanLiaison1 <- rep(0, length(levels(datafile$AdjNoun)))\nl95Liaison1 <- rep(0, length(levels(datafile$AdjNoun)))\nu95Liaison1 <- rep(0, length(levels(datafile$AdjNoun)))\nprobmeanLiaison1 <- rep(0, length(levels(datafile$AdjNoun)))\nprobl95Liaison1 <- rep(0, length(levels(datafile$AdjNoun)))\nprobu95Liaison1 <- rep(0, length(levels(datafile$AdjNoun)))\n\ndatafile.sequence <- data.frame(cbind(AdjNoun, meanLiaison1, l95Liaison1, u95Liaison1, probmeanLiaison1, probl95Liaison1, probu95Liaison1),\n                                stringsAsFactors = FALSE)\n\n#take a subset with only AdjNoun sequences featuring a liaison1 consonant\ndatafile.sequence <- droplevels(datafile.sequence[datafile.sequence$AdjNoun %in% c(""faux poux"", ""faux espoir"", ""grand hommage"", ""grand honneur"",    \n                                                                                   ""gros objet"",""gros orteil"", ""mauvais tat"", ""mauvais t"",   \n                                                                                   ""parfait imbcile"", ""parfait inconnu"", ""petit ami"",""petit anneau""),])\n\nfor (i in 1:length(datafile.sequence$AdjNoun)){#loop across the 48 AdjNoun sequences\n  datafile.sequence$meanLiaison1[i] <- mean(posterior_samples(model1)[[\'b_Intercept\']] + posterior_samples(model1)[[\'b_Consonant3\']] + \n                                              posterior_samples(model1)[[paste(""r_AdjNoun["", sub("" "", ""."", datafile.sequence$AdjNoun[i]), "",Intercept]"", sep="""")]])\n  datafile.sequence$l95Liaison1[i]  <- ci(posterior_samples(model1)[[\'b_Intercept\']] + posterior_samples(model1)[[\'b_Consonant3\']] + \n                                            posterior_samples(model1)[[paste(""r_AdjNoun["", sub("" "", ""."", datafile.sequence$AdjNoun[i]), "",Intercept]"", sep="""")]], \n                                          method = ""HDI"", ci=0.95)[[2]]\n  datafile.sequence$u95Liaison1[i]  <- ci(posterior_samples(model1)[[\'b_Intercept\']] + posterior_samples(model1)[[\'b_Consonant3\']] + \n                                            posterior_samples(model1)[[paste(""r_AdjNoun["", sub("" "", ""."", datafile.sequence$AdjNoun[i]), "",Intercept]"", sep="""")]], \n                                          method = ""HDI"", ci=0.95)[[3]]}\n\n#taking the inverse of the logit: logit = log(p/(1-p)) where p is the probability of word-initial attachment\n#to get the inverse logit: exp(logit) = p/(1-p), so exp(logit) - p*exp(logit) - p = 0 \n#so exp(logit) - p*(exp(logit) + 1) = 0, so exp(logit) =  p*(exp(logit) + 1)\n#so p = exp(logit)/(exp(logit)+1)\n\ndatafile.sequence$meanLiaison1 <- round(as.numeric(as.character(datafile.sequence$meanLiaison1)), 2)\ndatafile.sequence$l95Liaison1 <- round(as.numeric(as.character(datafile.sequence$l95Liaison1)), 2)\ndatafile.sequence$u95Liaison1 <- round(as.numeric(as.character(datafile.sequence$u95Liaison1)), 2)\ntypeof(datafile.sequence$id)\n\nfor (i in 1:length(datafile.sequence$AdjNoun)){#loop across the 12 AdjNoun sequences\n  datafile.sequence$probmeanLiaison1[i] <- exp(datafile.sequence$meanLiaison1[i])/(exp(datafile.sequence$meanLiaison1[i])+ 1)\n  datafile.sequence$probl95Liaison1[i]  <- exp(datafile.sequence$l95Liaison1[i])/(exp(datafile.sequence$l95Liaison1[i])+ 1)\n  datafile.sequence$probu95Liaison1[i]  <- exp(datafile.sequence$u95Liaison1[i])/(exp(datafile.sequence$u95Liaison1[i])+ 1)}\n\ndatafile.sequence$probmeanLiaison1 <- round(as.numeric(as.character(datafile.sequence$probmeanLiaison1)), 2)\ndatafile.sequence$probl95Liaison1 <- round(as.numeric(as.character(datafile.sequence$probl95Liaison1)), 2)\ndatafile.sequence$probu95Liaison1 <- round(as.numeric(as.character(datafile.sequence$probu95Liaison1)), 2)\n\n#Plot the results for sequences\nsequence.plot <- ggplot(datafile.sequence, aes(x=reorder(AdjNoun, -probmeanLiaison1), y=probmeanLiaison1)) + \n  geom_errorbar(aes(ymin=probl95Liaison1, ymax=probu95Liaison1), width=.1) +\n  geom_point(size=3) + theme_bw() + labs(y=""Posterior probability of attachment to Word 2"", x=""AdjNoun"") + \n  theme(text = element_text(size=20))  + theme(axis.text.x = element_text(angle = 90)) \n\nsequence.plot \n\nggsave(file=""study1-Fig3.eps"", dpi = 300)\n\n\n', '#####################################################################################################################\n#Packages to be loaded for data manipulation/visualization and statistical analysis\n#####################################################################################################################\n\nif (!require(""plyr"")) {install.packages(""plyr""); require(""plyr"")}#to rename levels of a variable\nif (!require(""ggplot2"")) {install.packages(""ggplot2""); require(""ggplot2"")} #for visualization\nif (!require(""bayestestR"")) {install.packages(""bayestestR""); require(""bayestestR"")} #to calculate 95% credibility interval for Bayesian posterior samples \nif (!require(""utils"")) {install.packages(""utils""); require(""utils"")} #provides the expand.grid function \n#For Bayesian logistic regression using brms (interface to Stan)\nif (!require(""brms"")) {install.packages(""brms""); require(""brms"")}\npackageVersion(""brms"")\n\n######################################################################################################################\n#Load data\n######################################################################################################################\n\n#Load the dataset\n#setwd(""/home/bstorme/Documents/Storme_Benjamin_18_04_2022/2022/LIAISON/LIAISON-ENCHAINE-ETUDE-FOLLOW-UP/Results/"")\ndatafile = read.csv(""study1b-data.csv"", sep ="","", header=TRUE, na.strings=c("""",""NA""))\nhead(datafile)\ndim(datafile) \nnrow(datafile)/(48*2)#Nb of participants\n\n#Description of the variables: \n# * Participant: a number corresponding to each participant (24 participants)\n# * Gender: ""male""/""female"" (participant gender)\n# * Age: participant age\n# * Speaker: ""Speaker1""/""Speaker2"" (Speaker1 is female, Speaker2 is male)\n# * Consonant: type of consonant, with 4 levels:  ""final"" (=stable word-final consonant, e.g. �nor[m]) /\n#                                                 ""ep-liaison"" (=epenthetic liaison, e.g. gran[t]) /\n#                                                 ""sup-liaison"" (=suppletive liaison, e.g. be[l]) /\n#                                                 ""initial"" (=stable word-initial consonant, e.g. [s]ourire)\n# * Adjective: adjective used in the Adjective-Noun sequence (e.g. ""petit"")\n# * Noun: noun used in the Adjective-Noun sequence (e.g. ""ami"")\n# * AdjNoun: Adjective-Noun sequence (e.g. ""petit ami"")\n# * logFreqw1: log frequency of Word 1 (=Adj). This frequency was obtained using the corpus of movie/TV subtitles OpenSubtitles\n# * logFreqw2conditionedw1: log conditional frequency of Word 2 given Word 1. This frequency was obtained using the OpenSubtitles corpus.\n# * Response:   ""word-final"" (= participant prefers the pronunciation with the consonant prosodically attached at the end of Word 1)\n#               ""word-initial"" (= participant prefers the pronunciation with the consonant prosodically attached at the beginning of Word 2)\n\n#Turn the variables to be used in the analysis into factors\ndatafile$Participant <- as.factor(datafile$Participant)\ndatafile$Gender <- as.factor(datafile$Gender)\ndatafile$Speaker <- as.factor(datafile$Speaker)\ndatafile$Consonant <- as.factor(datafile$Consonant)\ndatafile$AdjNoun <- as.factor(datafile$AdjNoun)\ndatafile$Response <- as.factor(datafile$Response)\n\n#Reorder the level of Consonant variable: \nlevels(datafile$Consonant)\ndatafile$Consonant <- factor(datafile$Consonant, levels = c(""final"", ""sup-liaison"", ""ep-liaison"", ""initial""))\nlevels(datafile$Consonant)\n\n#Remove rows with missing values\n#There are missing values because, for each Adj N sequence, there were two pronunciations (one by Speaker1 and the other by Speaker2)\n#But each participant was only exposed to one of the two pronunciations\ndatafile <- datafile[!is.na(datafile$Response),]\n\n###################################################################################################################\n#Statistical analysis\n###################################################################################################################\n\n#Dummy coding of Consonant, with final as baseline\ncontrasts(datafile$Consonant) <- contr.treatment(4,1)\n\n#Remove rows with missing values\n\n#Coding for the Response variable:\ncontrasts(datafile$Response)\n#word-initial is 1 and word-final is 0\n#--> the model will estimate the probability of word-initial attachment.\n\n#Model\nmodel1 <- brm(\n  #formula = Response ~ 1 + Consonant + (1|Participant) + (1|AdjNoun) + (1|Speaker),\n  #formula = Response ~ 1 + Consonant + logFreqw1 +logFreqw2conditionedw1, No effect of log probability\n formula = Response ~ 1 + Consonant + (Consonant|Participant) + (1|AdjNoun) + (Consonant|Speaker),\n  #formula = Response ~ 1 + Consonant*logFreqw2conditionedw1,\n #The big model with everything does not converge\n  data = datafile,\n  family = bernoulli(""logit""),\n  control = list(adapt_delta = 0.9999, max_treedepth=15), #adapt_delta was increased above the default 0.8 value to avoid divergent transitions after warmup. \n  #See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup \n  #Increase max_treedepth above 10 before 2000 tranditions after warmup exceeded the max treedepth\n  set.seed(1702) #to be able to reproduce the analysis\n)\n\n#Save the model\n#save(model1, file = ""study1b-model.RData"")\nload(""study1b-model.RData"")\n\n#Summary of the model\nsummary(model1)\n\n#Hypothesis testing\n# * sup-liaison C does not differ significantly from (stable word-)final C\nmean(posterior_samples(model1)[[\'b_Consonant2\']]) #mean = -0.19\nci(posterior_samples(model1)[[\'b_Consonant2\']], method = ""ETI"", ci=0.95)[[2]] #=-3.45\nci(posterior_samples(model1)[[\'b_Consonant2\']], method = ""ETI"", ci=0.95)[[3]] #=2.66\n\n# * ep-liaison C differs significantly from (stable word-)final and initial C, and also sup-liaison\n#diff with word-final\nmean(posterior_samples(model1)[[\'b_Consonant3\']]) #mean = 4.03\nci(posterior_samples(model1)[[\'b_Consonant3\']], method = ""ETI"", ci=0.95)[[2]] #=0.96\nci(posterior_samples(model1)[[\'b_Consonant3\']], method = ""ETI"", ci=0.95)[[3]] #=7.22\n#diff with sup-liaison\nmean(posterior_samples(model1)[[\'b_Consonant2\']] - posterior_samples(model1)[[\'b_Consonant3\']]) #mean = -4.23\nci(posterior_samples(model1)[[\'b_Consonant2\']] - posterior_samples(model1)[[\'b_Consonant3\']], method = ""ETI"", ci=0.95)[[2]] #=-8.67\nci(posterior_samples(model1)[[\'b_Consonant2\']] - posterior_samples(model1)[[\'b_Consonant3\']], method = ""ETI"", ci=0.95)[[3]] #=-0.04\n#diff with word-initial\nmean(posterior_samples(model1)[[\'b_Consonant3\']] - posterior_samples(model1)[[\'b_Consonant4\']]) #mean = -11.28\nci(posterior_samples(model1)[[\'b_Consonant3\']] - posterior_samples(model1)[[\'b_Consonant4\']], method = ""ETI"", ci=0.95)[[2]] #=-22.82\nci(posterior_samples(model1)[[\'b_Consonant3\']] - posterior_samples(model1)[[\'b_Consonant4\']], method = ""ETI"", ci=0.95)[[3]] #=-4.41\n\n#Plot the posterior probability of the word-initial response as a function of Consonant\n#This plot shows the posterior mean and posterior 95% credibility interval (see documentation https://cran.r-project.org/web/packages/brms/brms.pdf)\nme <- conditional_effects(model1, effects=""Consonant"")\nresults.plot <- plot(me, plot = FALSE)[[1]] +\n  theme_bw() + theme(text = element_text(size=16)) + ylab(""Posterior probability of attachment to Word 2"")\nresults.plot\n\n#Save the plot as an eps file with dpi = 300\nggsave(file=""study1b-Fig1.eps"", dpi = 300)', '#####################################################################################################################\n#Packages to be loaded for data manipulation/visualization and statistical analysis\n#####################################################################################################################\n\nif (!require(""Rcpp"")) {install.packages(""Rcpp""); require(""Rcpp"")}#necessary to install plyr\nif (!require(""plyr"")) {install.packages(""plyr""); require(""plyr"")}#to rename levels of a variable\nif (!require(""stringr"")) {install.packages(""stringr""); require(""stringr"")} #necessary to install reshape2\nif (!require(""reshape2"")) {install.packages(""reshape2""); require(""reshape2"")} #to reshape the dataset from wide to long format\nif (!require(""ggplot2"")) {install.packages(""ggplot2""); require(""ggplot2"")} #for visualization\nif (!require(""brms"")) {install.packages(""brms""); require(""brms"")} #For Bayesian logistic regression using brms (interface to Stan)\npackageVersion(""brms"")\n\n################################################################################################################\n#Prepare the data for analysis\n################################################################################################################\n\n#load the dataset\ndatafile = read.csv(""study2-data.csv"", sep ="","", header=TRUE)\nhead(datafile)\ndim(datafile) \n\n#Description of the variables: \n# * Filename: a number corresponding to each participant\n# * Consonant: final, liaison\n# * C:  nature of the consonant (d di ni noise t tz tzi unreleasedt z zi NA) --> only /t/ is relevant for our study\n# * Pause: sp (presence of a pause between /t/ and /i/), NA (no pause)\n# * schwa:  (presence of a schwa between /t/ and /i/), NA (no schwa)\n# * V: 1 (/i/ present), 0 (/i/ deleted)\n# * Cduration: consonant duration\n# * Pauseduration: duration of pause\n# * Vduration: duration of vowel (/i/)\n\n#Turn the variables that will be used in the analysis into factors\ndatafile$Consonant <- as.factor(datafile$Consonant)\ndatafile$C <- as.factor(datafile$C)\ndatafile$V <- as.factor(datafile$V)\n\n#Number of participants in the original datafile\nlength(levels(as.factor(datafile$Filename)))\n#394 participants\n\n#Remove the rows where liaison consonant is not produced (namely Consonant==liaison & C==""<NA>"")\ntotal_n <- dim(datafile)\ndatafile <- subset(datafile, !(Consonant==""liaison"" & is.na(C)))\ntotal_n - dim(datafile) #number of potential liaison consonants that were not realized\n#106 liaison consonants were not realized\n\n#Remove cases where there is a pause between C and V\n#Note: all cases that involve schwa between C and V also involve a pause\ntotal_n <- dim(datafile)\ndatafile <- subset(datafile, (is.na(Pause)))\ntotal_n - dim(datafile) #number of contexts witout enchanement (presence of a pause between C and V)\n#125 cases involving a pause\n\n#Remove cases of non-conventional liaison or final consonant\ntotal_n <- dim(datafile)\ndatafile <- subset(datafile, (C==""t""))\ndim(datafile)\ntotal_n - dim(datafile)\n#58 cases of non-conventional liaison or final consonant\n\n#Number of participants in the final datafile\nlength(levels(as.factor(datafile$Filename)))\n#322 participants\n\nxtabs(data=datafile, formula = ~ Consonant)\n#Number of occurrences of liaison and final consonants:\n# final liaison \n# 243     251 \n\n###################################################################################################################\n#Statistical analysis: logistic regression\n###################################################################################################################\n\n#How does the presence/absence of V varies as a function of Cduration in /ti/ and Consonant type (liaison/final)?\nlogistic.regression <- brm(\n  formula = V ~ 1 + Cduration*Consonant,\n  data = datafile,\n  family = bernoulli(""logit""),\n  set.seed(1702) #to be able to reproduce the analysis\n)\nlogistic.regression\n#save(logistic.regression, file = ""study2-model1.RData"")\n#load(""study2-model1.RData"")\n\n#Plot the results\nme <- conditional_effects(logistic.regression, effects=""Cduration:Consonant"")\nresults.plot <- plot(me, plot = FALSE)[[1]] +\n  theme_bw() + theme(text = element_text(size=16)) + ylab(""Probability of /i/-presence"") + \n  xlab(""/t/-duration (ms)"") + scale_fill_grey() + scale_colour_grey()\nresults.plot\n\n#Save the plot as an eps file with dpi = 300\nggsave(file=""study2-Fig1.eps"", dpi = 300, device=cairo_ps)\n\n###################################################################################################################\n#Statistical analysis: linear regression\n###################################################################################################################\n\n#How does Cduration vary for liaison vs final consonants?\nlinear.regression <- brm(\n  formula = Cduration ~ 1 + Consonant,\n  data = datafile,\n  set.seed(1702) #to be able to reproduce the analysis\n)\nlinear.regression\n\n#Plot the results\nme <- conditional_effects(linear.regression, effects=""Consonant"")\nresults.plot <- plot(me, plot = FALSE)[[1]] +\n  theme_bw() + theme(text = element_text(size=16)) + ylab(""/t/-duration (ms)"")\nresults.plot\n\n#Save the plot as an eps file with dpi = 300\nggsave(file=""study2-Fig2.eps"", dpi = 300, device=cairo_ps)\n\n\n']","Paradigm uniformity effects on French liaison This project includes the supplementary files accompanying the paper ""Paradigm uniformity effects on French liaison"" : study1b-data.csv: this file contains the dataset collected in Study 1. The variables are described in study1b-rscript.Rstudy1b-rscript.R: R script that was used to produce the figures in Study 2 and the corresponding statistical analysisstudy2-data.csv: this file contains the dataset collected in Study 2. The variables are described in study2-rscript.Rstudy2-rscript.R: R script that was used to produce the figures in Study 2 and the corresponding statistical analysis. prosodic-ambiguity-final-2.txt (tab-delimited text file): this file contains the constraint-based analysis of Encrev's (1988) data on liaison enchane and liaison non-enchane. The file can be run with OT-Soft 2.5 (https://linguistics.ucla.edu/people/hayes/otsoft/OTSoftManual_2.5_April_2021.pdf). phonetic-ambiguity-final-2.txt (tab-delimited text file): this file contains the constraint-based analysis of Ct's (2014) data on affrication of stable word-final consonants, stable word-initial initial, and liaison consonants. The file can be run with OT-Soft 2.5.COGETO-prosodic-ambiguity.zip: this folder contains the input file and output files of the CoGeTo analysis of the case study on prosodic ambiguity (see prosodic-ambiguity-final-2.txt). CoGeTo is a software developed by Giorgio Magri and Arto Anttila to explore the typological predictions of constraint-based grammars (https://cogeto.stanford.edu/home).COGETO-phonetic-ambiguity.zip: this folder contains the input file and output files of the CoGeTo analysis of the case study on prosodic ambiguity (see phonetic-ambiguity-final-2.txt). Files linked to the first version of the paper can also be found here (the first version of the paper is available on lingbuzz: https://ling.auf.net/lingbuzz/006457): prosodic-ambiguity.txt (tab-delimited text file): this file contains the constraint-based analysis of Encrev's (1988) data on liaison enchane and liaison non-enchane. The file can be run with OT-Soft 2.5 (https://linguistics.ucla.edu/people/hayes/otsoft/OTSoftManual_2.5_April_2021.pdf). phonetic-ambiguity.txt (tab-delimited text file): this file contains the constraint-based analysis of Ct's (2014) data on affrication of stable word-final consonants, stable word-initial initial, and liaison consonants. The file can be run with OT-Soft 2.5.study1-data.csv: this file contains the dataset collected in Study 1 (first version). The variables are described in study1-rscript.Rstudy1-rscript.R: R script that was used to produce the figures in Study 1 (first version) and the corresponding statistical analysis.",3
Nuc/Mito Ribosomal Ratio as an index of growth rate,"The dataset (NMRR.csv) consists of information regarding Daphnia magna in Nuc/Mito ribosomal ratio experiment. Information included treatments, ribosomal ratio, somatic growth rate, size, culture duration, rRNA counts and total RNA concentration. Total 166 samples recorded in this data set. R script file (NMRR.R) consists of R code for all the statistical analysis conducted in this study.","['#test of homoscedasticity\nbartlett.test(NMRR$RibosomalRatio, NMRR$Temperature)\nbartlett.test(NMRR$RibosomalRatio, NMRR$Food)\nbartlett.test(NMRR$SomaticGrowthRate, NMRR$Temperature)\nbartlett.test(NMRR$SomaticGrowthRate, NMRR$Food)\n\n#subsetting data\nNMRRhightemp <- subset(NMRR, Temperature==""High"")\nNMRRmediumtemp <- subset(NMRR, Temperature==""Medium"")\nNMRRlowtemp <- subset(NMRR, Temperature==""Low"")\nNMRRhighfood <- subset(NMRR, Food==""High"")\nNMRRmediumfood <- subset(NMRR, Food==""Medium"")\nNMRRlowfood <- subset(NMRR, Food==""Low"")\n\n#test of normality\nshapiro.test(NMRRhightemp$RibosomalRatio)\nshapiro.test(NMRRmediumtemp$RibosomalRatio)\nshapiro.test(NMRRlowtemp$RibosomalRatio)\nshapiro.test(NMRRhighfood$RibosomalRatio)\nshapiro.test(NMRRmediumfood$RibosomalRatio)\nshapiro.test(NMRRlowfood$RibosomalRatio)\nshapiro.test(NMRRhightemp$SomaticGrowthRate)\nshapiro.test(NMRRmediumtemp$SomaticGrowthRate)\nshapiro.test(NMRRlowtemp$SomaticGrowthRate)\nshapiro.test(NMRRhighfood$SomaticGrowthRate)\nshapiro.test(NMRRmediumfood$SomaticGrowthRate)\nshapiro.test(NMRRlowfood$SomaticGrowthRate)\n\n#use WRS2 package\ninstall.packages(""WRS2"")\nlibrary(WRS2)\n\n#Robust two-way ANOVA for somatic growth rate\nt2way(SomaticGrowthRate~Food*Temperature, data = NMRR)\nRobustANOVAgrowth <- mcp2atm(SomaticGrowthRate~Food*Temperature, data = NMRR) #post hoc test for two-way ANOVA\nRobustANOVAgrowth$contrasts #contrasts matrix\nt1way(SomaticGrowthRate~Temperature, data = NMRRlowfood) #confirmed that at low food concentration, temperature doesn\'t have significant effect on growth\n\n#1816S rRNA ratio ANOVA\nt2way(RibosomalRatio~Food*Temperature, data = NMRR)\n#correlation test for 1816S rRNA ratio and somatic growth rate\npbcor(NMRR$RibosomalRatio, NMRR$Growth)\n#further test of correlation for temperature and food concentration treatments\npbcor(NMRR$RibosomalRatio, NMRR$Cell) #convert food concentration treatments to concentration value (cell/ml) of microalgae first before correlation test \npbcor(NMRR$RibosomalRatio, NMRR$Celsius) #similar to food concentration, value of degree celsius are used instead\n#correlation test between ratio and growth rate without effect of temperature\npbcor(NMRRhightemp$RibosomalRatio, NMRRhightemp$SomaticGrowthRate)\npbcor(NMRRmediumtemp$RibosomalRatio, NMRRmediumtemp$SomaticGrowthRate)\npbcor(NMRRlowtemp$RibosomalRatio, NMRRlowtemp$SomaticGrowthRate)\n\n#linear regression model of 1816S rRNAratio and somatic growth rate under different temperature\nsummary(lm(SomaticGrowthRate~RibosomalRatio*Cell, data = NMRRhightemp))\nsummary(lm(SomaticGrowthRate~RibosomalRatio*Cell, data = NMRRmediumtemp))\nsummary(lm(SomaticGrowthRate~RibosomalRatio*Cell, data = NMRRlowtemp))\n\n\n#######################################################################################################################################################################################################################################\n#R script for figures plotting\n\n#Figure 1 - SGR against treatments \nggplot(NMRR, aes(y=SomaticGrowthRate, x=Treatment, color=Food))+\n  geom_boxplot(outlier.shape = NA)+\n  geom_jitter(size=2, width=0.1)+\n  ylab(bquote(\'Somatic growth rate (day\'^-1*\')\'))+\n  xlab(""Treatments (Temperature:Food concentration)"")+\n  theme_bw()+\n  theme(text=element_text(family = ""Times new roman"", size =26), legend.position = ""none"", axis.text = element_text(size = 18))\n\n\n#Figure 2 - rRNA ratio against SGR (boxplot and scatterplot)\ncbPalette <- c(""#999999"", ""#E69F00"", ""#56B4E9"", ""#66FF00"", ""#F0E442"", ""#0072B2"", ""#D55E00"", ""#CC79A7"", ""#006633"")\nplot1 <- ggplot(NMRR, aes(y=RibosomalRatio, x=SomaticGrowthRate, color=Treatment))+\n  geom_point(size = 4)+\n  scale_color_manual(values=cbPalette)+\n  ylab(""18S/16S rRNA ratio"")+\n  xlab(bquote(\'Somatic growth rate (day\'^-1*\')\'))+\n  theme_bw()+\n  theme(text=element_text(family = ""Times new roman""), axis.text = element_text(size = 20), axis.title = element_text(size=26), legend.title = element_text(size = 22), legend.text = element_text(size = 20))\n\n#medianplot dataset consist of median, upper quartile and lower quartile of each treatment in NMRR dataset\nplot1+\n  geom_point(data = medianplot, aes(y=MedRatio, x=MedSGR, fill=Treatment), size=5, shape=21, color = ""black"", stroke = 2, show.legend=FALSE)+\n  scale_fill_manual(values=cbPalette)+\n  geom_errorbar(data=medianplot, mapping=aes(y=MedRatio, x=MedSGR, xmin = Q1SGR, xmax = Q3SGR, ymin = Q1Ratio, ymax = Q3Ratio), width=0.0015, size=0.5, color=""black"")+\n  geom_errorbarh(data=medianplot, mapping=aes(y=MedRatio, x=MedSGR, xmin = Q1SGR, xmax = Q3SGR, ymin = Q1Ratio, ymax = Q3Ratio), width=0.005, size=0.5, color=""black"")+\n  geom_smooth(method = ""lm"", se=FALSE, aes(group=1), color=""black"")\n#medianplot dataset consist of median, upper quartile and lower quartile of each treatment in NMRR dataset\n\n#Figure 4a - rRNA ratio against treatments (boxplot with jitter)\nFig4a <- ggplot(NMRR, aes(y=RibosomalRatio, x=Treatment, color=Food))+\n  geom_boxplot(color=""black"", outlier.shape = NA)+\n  geom_text(x=0.70, y=140, label=""(a)"", color=""black"", size=6)+\n  geom_jitter(position = position_jitter(0.15))+\n  ylab(""18S/16S rRNA ratio"")+\n  xlab(""Treatments (Temperature:Food concentration)"")+\n  theme_bw()+\n  theme(legend.position=""none"", axis.text = element_text(size = 12), axis.title = element_text(size=24), text = element_text(family = ""Times new roman""))\n\n#Figure 4b -rRNA ratio against food concentration (boxplot with jitter)\nFig4b <- ggplot(NMRR, aes(y=RibosomalRatio, x=Food, color=Food))+\n  geom_boxplot(color=""black"", outlier.shape = NA)+\n  geom_text(x=0.65, y=140, label=""(b)"", color=""black"", size=6)+\n  geom_jitter(position = position_jitter(0.15))+\n  ylab(""18S/16S rRNA ratio"")+\n  xlab(""Food concentration"")+\n  theme_bw()+\n  theme(legend.position = ""none"", axis.title = element_text(size = 24), axis.text = element_text(size=12), text=element_text(family=""Times new roman""))\n\n#Figure 4c - rRNA ratio against temperature (boxplot with jitter)\nFig4c <- ggplot(NMRR, aes(y=RibosomalRatio, x=Temperature, color=Temperature))+\n  geom_boxplot(color=""black"", outlier.shape = NA)+\n  geom_text(x=0.65, y=140, label=""(c)"", color=""black"", size=6)+\n  geom_jitter(position = position_jitter(0.15))+\n  xlab(""Temperature"")+\n  theme_bw()+\n  theme(legend.position = ""none"", axis.title.y = element_blank(), axis.title.x = element_text(size = 24), axis.text = element_text(size=12), text=element_text(family=""Times new roman""))\n\n#Combine all the Figure 4\ngrid.arrange(Fig4a, arrangeGrob(Fig4b,Fig4c, ncol=2), ncol=1)\n\n\n# Figure 3(a) - 18S16S rRNA ratio & growth rate relationship under high temperature\nFig3a <- ggplot(NMRRhightemp, aes(x=SomaticGrowthRate, y=RibosomalRatio, color=Food))+\n  geom_point(size=4)+\n  geom_abline(intercept = 1.772, slope = 203.70)+\n  ylab(""18S/16S rRNA ratio"")+\n  annotate(""text"", label = ""(a)"", x = 0.01, y = 125, size = 7, color=""black"")+\n  annotate(""text"", label = ""Robust R-square = 0.70"", x = 0.06, y = 125, size = 4)+\n  annotate(""text"", label = ""P < 0.05"", x = 0.06, y = 110, size = 4)+\n  labs(title=""High temperature (25 C)"")+\n  theme_bw()+\n  theme(text=element_text(family=""Times new roman""), axis.title.x = element_blank(), plot.title = element_text(size = 16, face = ""bold""), axis.title = element_text(size = 16), axis.text = element_text(size=14), legend.title = element_text(size = 16, face = ""bold""), legend.text = element_text(size = 12), legend.key.height = unit(0.5, ""cm""))\n\n#Figure 3(b) - 18S16S rRNA ratio & growth rate relationship under medium temperature\nFig3b <- ggplot(NMRRmediumtemp, aes(x=SomaticGrowthRate, y=RibosomalRatio, color=Food))+\n  geom_point(size=4)+\n  geom_abline(intercept = 8.251, slope = 90.584)+\n  ylab(""18S/16S rRNA ratio"")+\n  annotate(""text"", label = ""(b)"", x = 0.01, y = 100, size = 7, color=""black"")+\n  annotate(""text"", label = ""Robust R-square = 0.29"", x = 0.06, y = 100, size = 4)+\n  annotate(""text"", label = ""P < 0.05"", x = 0.06, y = 90, size = 4)+\n  labs(title=""Medium temperature (20 C)"")+\n  theme_bw()+\n  theme(text=element_text(family=""Times new roman""), axis.title.x = element_blank(), plot.title = element_text(size = 16, face = ""bold""), axis.title = element_text(size = 16), axis.text = element_text(size=12), legend.position = ""none"")\n\n#Figure 3(c) - 18S16S rRNA ratio & growth rate relationship under low temperature\nFig3c <- ggplot(NMRRlowtemp, aes(x=SomaticGrowthRate, y=RibosomalRatio, color=Food))+\n  geom_point(size=4)+\n  geom_abline(intercept = 6.139, slope = 279.44)+\n  xlab(bquote(\'Somatic growth rate (day\'^-1*\')\'))+\n  ylab(""18S/16S rRNA ratio"")+\n  annotate(""text"", label = ""(c)"", x = 0.01, y = 138, size = 7, color=""black"")+\n  annotate(""text"", label = ""Robust R-square = 0.35"", x = 0.045, y = 138, size = 4)+\n  annotate(""text"", label = ""P < 0.05"", x = 0.045, y = 120, size = 4)+\n  labs(title=""Low temperature (15 C)"")+\n  theme_bw()+\n  theme(text=element_text(family=""Times new roman""), plot.title = element_text(size = 16, face = ""bold""), axis.title = element_text(size = 16), axis.text = element_text(size=12), legend.position = ""none"")\n\n#create a blankplot for the visual placement\nblankPlot <- ggplot()+geom_blank(aes(1,1))+\ntheme(plot.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.line = element_blank())\n\ngrid.arrange(Fig3a, Fig3b, blankPlot, Fig3c, blankPlot, layout_matrix = rbind(c(1,1), c(2,3), c(4,5)), widths=c(0.9,0.112))\n\n\n']","Nuc/Mito Ribosomal Ratio as an index of growth rate The dataset (NMRR.csv) consists of information regarding Daphnia magna in Nuc/Mito ribosomal ratio experiment. Information included treatments, ribosomal ratio, somatic growth rate, size, culture duration, rRNA counts and total RNA concentration. Total 166 samples recorded in this data set. R script file (NMRR.R) consists of R code for all the statistical analysis conducted in this study.",3
Data from: Alternative food sources interfere with removal of a fungal amphibian pathogen by zooplankton,"1. While the amphibian disease chytridiomycosis is causing ongoing population declines and biodiversity losses around the globe, efficient mitigation strategies are lacking. The free-living zoospores of the causative agents of this disease, the chytrid pathogens Batrachochytrium dendrobatidis (Bd) and Batrachochytrium salamandrivorans (Bsal), are a potential food source for filter-feeding micropredators as part of the aquatic food web. While consumption of zoospores can lower environmental pathogen loads, alternative food sources may interfere with pathogen removal rates.2. We compared the ability of three filter-feeding zooplankton species, i.e. the cladoceran Daphnia magna, the rotifer Brachionus calyciflorus and the ostracod Heterocypris incongruens, to remove Bd zoospores in water and investigated the effect of alternative food sources, i.e. the green algae Pseudokirchneriella subcapitata and Chlorella vulgaris, on zoospore ingestion by D. magna.3. D. magna was the only micropredator candidate that effectively removed Bd zoospores from its environment, with an average removal rate of 1,012  542 GE ind.-1 h-1 within our test system. High concentrations (1x105 cells/mL) of large and easily ingestible P. subcapitata reduced pathogen removal rates, whereas the small and less edible C. vulgaris did not interfere with pathogen removal.4. Synthesis and applications: We showed that Daphnia spp., which are keystone species in all sorts of aquatic habitats worldwide, are promising target agents for biologically mitigating chytridiomycosis infections and how natural food sources may interfere with this strategy. We also suggest potential management actions for biological disease mitigation, aiming to optimize environmental conditions for these target filter-feeders, thereby reducing pathogen densities and eventually infection pressure in amphibian hosts. Examples of such management actions include, but are not limited to, removal of planktivorous fish, habitat restoration, nutrient control or agrochemical regulation in the vicinity of amphibian breeding ponds. Further studies, including field trials, are needed to confirm the effects of pathogen consumption on infection dynamics in natural situations and investigate the impact of intervention actions.","['#setwd()\r\ndata <- read.csv(""Algtest.csv"")\r\nhead(data)\r\nsummary(data)\r\n\r\n#compute summary\r\nlibrary(dplyr)\r\ngroup_by(data, Treatment) %>%\r\n  summarise(\r\n    count = n(),\r\n    mean = mean(GE, na.rm = TRUE),\r\n    sd = sd(GE, na.rm = TRUE)\r\n  )\r\n\r\n#boxplot\r\nlibrary(""ggpubr"")\r\np <- ggboxplot(data, x = ""Treatment"", y = ""GE"", \r\n               color = ""black"", fill = c(""#0000CD"", ""#006600"", ""#006600""),\r\n               order = c(""Bd"", ""Bd + PS"", ""Bd + CV""),\r\n               ylab = ""GE"", xlab = ""Treatment"", legend = ""none"")\r\n\r\np + rotate_x_text(angle = 45) + font(""xy.text"", size = 9) + \r\n  font(""xylab"", face = ""bold"", size = 10) + border(size = 0.6)\r\n\r\n#compute the analysis of variance\r\nres.aov <- aov(GE ~ Treatment, data = data)\r\nsummary(res.aov)\r\n\r\npairwise.t.test(data$GE, data$Treatment,\r\n                p.adjust.method = ""BH"")\r\n\r\n#check for homogeneity\r\nplot(res.aov, 1)\r\n\r\nlibrary(car)\r\nleveneTest(GE ~ Treatment, data = data)\r\n\r\n#check for normality\r\nplot(res.aov, 2)\r\n\r\naov_residuals <- residuals(object = res.aov )\r\nshapiro.test(x = aov_residuals )\r\n\r\n', '#setwd()\r\n\r\ndata1 <- read.csv(""Predatietest DM repetition1.csv"")\r\ndata1 <- data1[-36,] #outlier\r\ndata1 <- data1[-35,] #outlier\r\ndata1 <- data1[-12,] #RSD >50%\r\ndata1$repetition=as.factor(""1"")\r\n#data1$GE=as.numeric(data1$GE)\r\ndata2 <- read.csv(""Predatietest DM repetition2.csv"")\r\ndata2 <- data2[-3,] #outlier\r\ndata2$repetition=as.factor(""2"")\r\ndata<-rbind(data1,data2)\r\nhead(data)\r\nsummary(data)\r\n\r\n#show a random sample\r\ndplyr::sample_n(data, 10)\r\n\r\n#compute summary\r\nlibrary(dplyr)\r\ngroup_by(data, Treatment) %>%\r\n  summarise(\r\n    count = n(),\r\n    mean = mean(GE, na.rm = TRUE),\r\n    sd = sd(GE, na.rm = TRUE)\r\n  )\r\n\r\n#boxplot\r\nlibrary(""ggpubr"")\r\np <- ggboxplot(data, x = ""Treatment"", y = ""GE"", \r\n               color = ""black"", fill = c(""#0000CD"", ""#FF8C00"", ""#66cc33"", ""#009900"",\r\n                                         ""#006600"", ""#66cc33"", ""#009900"",\r\n                                         ""#006600""),\r\n               order = c(""Bd"", ""Bd + DM"", ""Bd + DM + PS (low)"", ""Bd + DM + PS (medium)"", \r\n                         ""Bd + DM + PS (high)"", ""Bd + DM + CV (low)"", ""Bd + DM + CV (medium)"", \r\n                         ""Bd + DM + CV (high)""),\r\n               ylab = ""GE"", xlab = ""Treatment"", legend = ""none"")\r\n\r\np + rotate_x_text(angle = 45) + font(""xy.text"", size = 9) + \r\n  font(""xylab"", face = ""bold"", size = 10) + border(size = 0.6)\r\n\r\n#GLM\r\ndata$fDM=as.factor(data$DM)\r\ndata$fPS=as.factor(data$PS)\r\n\r\nres.aov <- lm(GE ~ PS * CV * fDM * repetition, data = data)\r\n#plot(data$PS[which(data$PS!=0)],data$GE[which(data$PS!=0)])\r\nsummary(res.aov)\r\n#plot(res.aov)\r\n\r\nres.aov2 <- lm(GE ~ fPS * fDM * repetition, data = data)\r\n#plot(data$PS[which(data$PS!=0)],data$GE[which(data$PS!=0)])\r\nsummary(res.aov2)\r\n#plot(res.aov2)\r\n\r\nx <- mean(data$GE[data$Treatment == ""Bd""])\r\ny <- mean(data$GE[data$Treatment == ""Bd + DM""])\r\nz <- mean(data$GE[data$Treatment == ""Bd + DM + PS (high)""])\r\nreductie <- x-y\r\nreductiePS <- x-z\r\npercentage <- 100-((y/x)*100)\r\npercentagePS <- 100-((z/x)*100)\r\npercentage\r\npercentagePS\r\n\r\nsdx <- sd(data$GE[data$Treatment == ""Bd""])\r\nsdy <- sd(data$GE[data$Treatment == ""Bd + DM""])\r\nsdz <- sd(data$GE[data$Treatment == ""Bd + DM + PS (high)""])\r\nsdreduction <- sqrt(sdx^2+sdy^2)\r\nsdreductionPS <- sqrt(sdx^2+sdz^2)\r\n\r\nrate <- reductie/20\r\nsdrate <- sdreduction/20\r\nvarrate <- 2*sdrate\r\nminconf <- rate-varrate\r\nmaxconf <- rate+varrate\r\nrate\r\nsdrate\r\n\r\nratePS <- reductiePS/20\r\nsdratePS <- sdreductionPS/20\r\nvarratePS <- 2*sdratePS\r\nminconfPS <- ratePS-varratePS\r\nmaxconfPS <- ratePS+varratePS\r\nratePS\r\nsdratePS', '#setwd()\r\n\r\ndataBC1 <- read.csv(""Predatietest BC repetition1.csv"")\r\ndataBC1$repetition=as.factor(""1"")\r\ndataBC1 <- subset(dataBC1, select = c(Sample, Treatment,GE, repetition))\r\ndataBC1$Treatment <- replace(dataBC1$Treatment, dataBC1$Treatment == ""Bd"", ""Control Bd + BC"")\r\ndataBC2 <- read.csv(""Predatietest BC repetition2.csv"")\r\ndataBC2$repetition=as.factor(""2"")\r\ndataBC2 <- subset(dataBC2, select = c(Sample, Treatment,GE, repetition))\r\ndataBC2$Treatment <- replace(dataBC2$Treatment, dataBC2$Treatment == ""Bd"", ""Control Bd + BC"")\r\ndataBC <- rbind(dataBC1, dataBC2)\r\n\r\ndataHI1 <- read.csv(""Predatietest HI repetition1.csv"")\r\ndataHI1 <- dataHI1[-7,] #RSD >50%\r\ndataHI1$repetition=as.factor(""1"")\r\ndataHI1 <- subset(dataHI1, select = c(Sample, Treatment,GE, repetition))\r\ndataHI1$Treatment <- replace(dataHI1$Treatment, dataHI1$Treatment == ""Bd"", ""Control Bd + HI"")\r\ndataHI2 <- read.csv(""Predatietest HI repetition2.csv"")\r\ndataHI2$repetition=as.factor(""2"")\r\ndataHI2 <- subset(dataHI2, select = c(Sample, Treatment,GE, repetition))\r\ndataHI2$Treatment <- replace(dataHI2$Treatment, dataHI2$Treatment == ""Bd"", ""Control Bd + HI"")\r\ndataHI <- rbind(dataHI1, dataHI2)\r\n\r\ndataDM1 <- read.csv(""Predatietest DM repetition1.csv"")\r\ndataDM1 <- subset(dataDM1, dataDM1$Treatment == ""Bd"" | dataDM1$Treatment == ""Bd + DM"")\r\ndataDM1$repetition=as.factor(""1"")\r\ndataDM1 <- dataDM1[-12,]\r\ndataDM1 <- subset(dataDM1, select = c(Sample, Treatment,GE, repetition))\r\ndataDM1$Treatment <- replace(dataDM1$Treatment, dataDM1$Treatment == ""Bd"", ""Control Bd + DM"")\r\ndataDM2 <- read.csv(""Predatietest DM repetition2.csv"")\r\ndataDM2 <- subset(dataDM2, dataDM2$Treatment == ""Bd"" | dataDM2$Treatment == ""Bd + DM"")\r\ndataDM2$repetition=as.factor(""2"")\r\ndataDM2 <- dataDM2[-3,] #outlier\r\ndataDM2 <- subset(dataDM2, select = c(Sample, Treatment,GE, repetition))\r\ndataDM2$Treatment <- replace(dataDM2$Treatment, dataDM2$Treatment == ""Bd"", ""Control Bd + DM"")\r\ndataDM <- rbind(dataDM1, dataDM2)\r\n\r\ndata <- rbind(dataBC, dataHI, dataDM)\r\n\r\n#boxplots\r\nlibrary(""ggpubr"")\r\ns <- ggboxplot(data, x = ""Treatment"", y = ""GE"", \r\n               color = ""black"", fill = c(""#0000CD"", ""#FF8C00"", ""#0000CD"", ""#FF8C00"",\r\n                                         ""#0000CD"", ""#FF8C00""),\r\n               order = c(""Control Bd + BC"", ""Bd + BC"", ""Control Bd + HI"", ""Bd + HI"", \r\n                         ""Control Bd + DM"", ""Bd + DM""),\r\n               ylab = ""GE"", xlab = ""Treatment"", legend = ""none"")\r\n\r\ns + rotate_x_text(angle = 45) + font(""xy.text"", size = 9) + \r\n  font(""xylab"", face = ""bold"", size = 10) + border(size = 0.6)\r\n\r\n#Welch\'s t-tests\r\nt.test(GE ~ Treatment, data = dataBC)\r\nt.test(GE ~ Treatment, data = dataHI)\r\nt.test(GE ~ Treatment, data = dataDM)\r\n\r\n#calculations\r\nx <- mean(dataDM$GE[dataDM$Treatment == ""Control Bd + DM""])\r\ny <- mean(dataDM$GE[dataDM$Treatment == ""Bd + DM""])\r\nreductie <- x-y\r\npercentage <- 100-((y/x)*100)\r\npercentage\r\n\r\nsdx <- sd(dataDM$GE[dataDM$Treatment == ""Control Bd + DM""])\r\nsdy <- sd(dataDM$GE[dataDM$Treatment == ""Bd + DM""])\r\nsdreduction <- sqrt(sdx^2+sdy^2)\r\n\r\nrate <- reductie/20\r\nsdrate <- sdreduction/20\r\nvarrate <- 2*sdrate\r\nminconf <- rate-varrate\r\nmaxconf <- rate+varrate\r\nrate\r\nsdrate\r\n\r\nx1 <- mean(dataDM1$GE[dataDM1$Treatment == ""Control Bd + DM""])\r\nx2 <- mean(dataDM2$GE[dataDM2$Treatment == ""Control Bd + DM""])\r\nx1/2\r\nx2/2\r\n']","Data from: Alternative food sources interfere with removal of a fungal amphibian pathogen by zooplankton 1. While the amphibian disease chytridiomycosis is causing ongoing population declines and biodiversity losses around the globe, efficient mitigation strategies are lacking. The free-living zoospores of the causative agents of this disease, the chytrid pathogens Batrachochytrium dendrobatidis (Bd) and Batrachochytrium salamandrivorans (Bsal), are a potential food source for filter-feeding micropredators as part of the aquatic food web. While consumption of zoospores can lower environmental pathogen loads, alternative food sources may interfere with pathogen removal rates.2. We compared the ability of three filter-feeding zooplankton species, i.e. the cladoceran Daphnia magna, the rotifer Brachionus calyciflorus and the ostracod Heterocypris incongruens, to remove Bd zoospores in water and investigated the effect of alternative food sources, i.e. the green algae Pseudokirchneriella subcapitata and Chlorella vulgaris, on zoospore ingestion by D. magna.3. D. magna was the only micropredator candidate that effectively removed Bd zoospores from its environment, with an average removal rate of 1,012  542 GE ind.-1 h-1 within our test system. High concentrations (1x105 cells/mL) of large and easily ingestible P. subcapitata reduced pathogen removal rates, whereas the small and less edible C. vulgaris did not interfere with pathogen removal.4. Synthesis and applications: We showed that Daphnia spp., which are keystone species in all sorts of aquatic habitats worldwide, are promising target agents for biologically mitigating chytridiomycosis infections and how natural food sources may interfere with this strategy. We also suggest potential management actions for biological disease mitigation, aiming to optimize environmental conditions for these target filter-feeders, thereby reducing pathogen densities and eventually infection pressure in amphibian hosts. Examples of such management actions include, but are not limited to, removal of planktivorous fish, habitat restoration, nutrient control or agrochemical regulation in the vicinity of amphibian breeding ponds. Further studies, including field trials, are needed to confirm the effects of pathogen consumption on infection dynamics in natural situations and investigate the impact of intervention actions.",3
Data and statistical analysis scripts for manuscript on pennycress roots & response to nitrate using 3D gel system,"Data and statistical analysis scripts for manuscript on pennycress roots & response to nitrate using 3Dgel systemA temporal atlas and response to nitrate availability of 3D root system architecture in diverse pennycress (Thlaspi arvense L.) accessionsThe following files contains:gel_data_preprocessing_20221024.R - R statistics script for pre-processing data files from 3Dgel system GIARoots & DynamicRoots raw outputgel_dataprocessing_20221229.R - R statistics script for data processing of pre-processed 3D gel dataTaGNS_N_Spring32.zip - CSV data files and R statistics script for Spring32 grown under high, low, trace and zero N treatments.TaGNE_N_Accessions.zip - CSV data files and R statistics script for 3 accessions under high and trace N treatments.TaGAA_N_Accessions.zip - CSV data files and R statistics script for 24 diverse pennycress lines grown under high N conditions.","['## Alex Liu, Marcus Griffiths (2022)\n## Data pre-processing for 3D gel imager\n## generalized version 6.21.2022\n\n## 1) Set working directory --------------------------------------------------------------------------------------------\ngetwd()\nsetwd(""C:/Users/USERNAME/FILEPATH"") #PC\nsetwd(""C:\\\\Users\\\\USERNAME\\\\FILEPATH"") #PC\nsetwd(""/Users/USERNAME/FILEPATH"") #macOS\n\n## 2) install & load following packages --------------------------------------------------------------------------------\nlibrary(tidyverse)      #includes packages ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats\n\n## 3) Load data files --------------------------------------------------------------------------------------------------\n## enter experiment parameter for filenames ----------------------------------------------------------------------------\nexp <- ""GAA""\nspecies <- ""Ta""\nexpname <- paste0(species,exp)\n\n## enter filenames for datafiles ---------------------------------------------------------------------------------------\nGia3D_csv <- ""TaGAA_Gia3D_10-11-2022_gia3d_v2.csv""\nDynamicRoots_csv <- ""TaGAA_DynamicRoots_2022-10-11_23-31-28.csv""\nBiomassdata_csv <- ""TaGAA_Biomass_data.csv""\nShootImagedata_csv <- ""TaGAA_PlantCVshootdata.csv""\n\n## 4) Load PlantID file for barcode mapping ----------------------------------------------------------------------------\nplant_treatment_mapping_csv <- ""TaGAA_metadata_12_7_2021.csv""\n\nplantid <- read_csv(plant_treatment_mapping_csv)\nplantid <- plantid %>% select(PlantID, Barcode)\n\n\n## delete?\n## 5) Get working directory\n# wd <- getwd()\n# setwd(paste0(wd,expname))\n\n\n## 6) Define parse functions -------------------------------------------------------------------------------------------\n## Pull out the Gia3D data that we want and make the proper plant ID ---------------------------------------------------\nparse_Gia3D <- function(csv){\n  data <- read_csv(csv,na = c(""NA"", ""na"", ""n.a."", """"))\n  # get barcode out of 4 columns: Species\tExperiment\tPlant\n  # ex. pennycress\tGAA\tp0422\n  # want TaGAAp0422\n  data <- data %>% mutate(Barcode = paste0(species,data$Experiment,data$Plant))\n  data <- data %>% select(Barcode\n                          ,ImagingDay\n                          ,\'ConvexHullVolume3D(mm^3)\' # RootConvexHullVolume3D_mm3,\n                          ,Solidity3D # RootSolidity3D,\n                          ,Bushiness3D # RootBushiness3D,\n                          ,\'Depth3D(mm)\' # RootDepth3D_mm,\n                          ,\'MaximumNetworkWidth3D(mm)\' # RootMaximumNetworkWidth3D_mm,\n                          ,\'LengthDistribution3D(mm)\' # RootLengthDistribution3D_mm,\n                          ,WidthDepthRatio3D # RootWidthDepth_Ratio\n                          )\n  return(data)\n}\n\n## Parse DynamicRoots data file ----------------------------------------------------------------------------------------\nparse_DynamicRoots <- function(csv){\n  data <- read_csv(csv) %>%\n  # get barcode out of FileNames\n  # ex. TaGAAp0101d05_aliu_2021-12-07_12-45-05_rootwork\n  # want TaGAAp0101\n  separate(FileNames, into = c(\'Barcode_ImagingDay\'), sep = \'_\') %>% \n  # remove d## from end\n  separate(Barcode_ImagingDay, into = c(\'Barcode\',\'ImagingDay\'), sep = -3)\n  # data <- data %>% select()\n  # data <- data %>% mutate(gsub(\'.{0,3}$\', \'\', barcode))\n  \n  ## delete??\n  # PlantID\tRootTotalCount\tRootVolume_mm3\tRootLength_mm\tRootMeanVolume_mm3\t\n  # RootMeanLength_mm\tRootMeanTortuosity\tRootMeanRadius_mm\tRootMeanSoilAngle_degrees\tRootMeanBranchingAngle_degrees\t\n  # RootMedianVolume_mm3\tRootMedianLength_mm\tRootMedianTortuosity\tRootMedianRadius_mm\tRootMedianSoilAngle_degrees\tRootMedianBranchingAngle_degrees\tRootCountLateral\tRootVolumeLateral_mm3\tRootLengthLateral_mm\tRootMeanVolumeLateral_mm3\tRootMeanLengthLateral_mm\tRootMeanTortuosityLateral\tRootMeanRadiusLateral_mm\tRootMeanSoilAngleLateral_degrees\tRootMeanBranchingAngleLateral_degrees\tRootMedianVolumeLateral_mm3\tRootMedianLengthLateral_mm\tRootMedianTortuosityLateral\tRootMedianRadiusLateral_mm\tRootMedianSoilAngleLateral_degrees\tRootMediaBranchingAngle_degrees\tRootCountFirstOrderLateral\tRootVolumeFirstOrderLateral_mm3\tRootLengthFirstOrderLateral_mm\tRootMeanVolumeFirstOrderLateral_mm3\tRootMeanLengthFirstOrderLateral_mm\tRootMeanTortuosityFirstOrderLateral\tRootMeanRadiusFirstOrderLateral_mm\tRootMeanSoilAngleFirstOrderLateral_degrees\tRootMeanBranchingAngleFirstOrderLateral_degrees\tRootMedianVolumeFirstOrderLateral_mm\tRootMedianLengthFirstOrderLateral_mm\tRootMedianTortuosityFirstOrderLateral\tRootMedianRadiusFirstOrderLateral_mm\tRootMedianSoilAngleFirstOrderLateral_degrees\tRootMedianBranchingAngleFirstOrderLateral_degrees\tRootDensityFirstOrderLateral_TL\tRootDensityFirstOrderLateral_BRTL\tInterbranchMeanDistance_mm\tInterbranchMedianDistance_mm\tRootVolumePrimary_mm3\tRootLengthPrimary_mm\tRootTortuosityPrimary\tRootRadiusPrimary_mm\tRootSoilAnglePrimary_degrees\n  \n  return(data)\n}\n\n## Parse biomass data file ---------------------------------------------------------------------------------------------\nparse_Biomassdata <- function(csv){\n  data <- read_csv(csv)\n  return(data)\n}\n\n## Parse shoot image data file -----------------------------------------------------------------------------------------\nparse_ShootImagedata <- function(csv){\n  data <- read_csv(csv)\n  return(data)\n}\n\n## 7) Load & prepare PlantID file for barcode mapping ------------------------------------------------------------------\nplantid <- read_csv(plant_treatment_mapping_csv)\n\nplant <- function(x) {\n  len <- 4-nchar(x)\n  paste0(""p"",paste0(integer(len), collapse=""""),x)\n}\n# format of plant should be p00xx\nplantid$Plant <- lapply(plantid$Plant,plant )\n\n## Make plantID column  ------------------------------------------------------------------------------------------------\n# ex 11122021_TaGAA_132_p0108_tHighN_r1\nplantid <- plantid %>% mutate(PlantID = paste(plantid$Date\n                                              ,plantid$ExperimentNumber\n                                              ,plantid$Geno\n                                              ,plantid$Plant\n                                              ,paste0(""t"",plantid$Treatment)\n                                              ,paste0(""r"",plantid$Block)\n                                              ,sep = \'_\'\n                                              ))\n\nplantid <- plantid %>% select(PlantID, Barcode)\n#metadata is weird, need to add a 4th digit otherwise rsa-Gia will fail\n#pattern: regex looking for \'p\' followed immediately by a numeric digit\n#replace: \\\\1 (group 1), 0 , \\\\2 (group 2)\n#plantid$PlantID <- gsub(\'(p)([0-9])\', \'\\\\10\\\\2\', plantid$PlantID)\n#plantid$Barcode <- gsub(\'(p)([0-9])\', \'\\\\10\\\\2\', plantid$Barcode)\n\nRootGIA3Ddata <- parse_Gia3D(Gia3D_csv)\nRootDynamicdata <- parse_DynamicRoots(DynamicRoots_csv)\nBiomassdata <- parse_Biomassdata(Biomassdata_csv)\nShootImagedata <- parse_ShootImagedata(ShootImagedata_csv)\n\n## 8) mapping of datafiles with PlantID  -------------------------------------------------------------------------------\n\n##Delete?\n# mapping \n#RootGIA3Ddata <- RootGIA3Ddata %>% mutate_at(c(\'Barcode\'), funs(ifelse(. %in% plantid$Barcode, plantid$PlantID[match(., plantid$Barcode)], .)))\n#RootDynamicdata <- RootDynamicdata %>% mutate_at(c(\'Barcode\'), funs(ifelse(. %in% plantid$Barcode, plantid$PlantID[match(., plantid$Barcode)], .)))\n\nRootGIA3Ddata <- RootGIA3Ddata %>% mutate(PlantID = ifelse(RootGIA3Ddata$Barcode %in% plantid$Barcode, plantid$PlantID[match(RootGIA3Ddata$Barcode, plantid$Barcode)], RootGIA3Ddata$Barcode))\nRootGIA3Ddata$PlantID <- paste(RootGIA3Ddata$PlantID, RootGIA3Ddata$ImagingDay, sep = \'_\')\nRootGIA3Ddata <- RootGIA3Ddata %>% select(PlantID\n                                          ,everything()\n                                          ,-Barcode\n                                          ,-ImagingDay\n                                          )\n\nRootDynamicdata <- RootDynamicdata %>% mutate(PlantID = ifelse(RootDynamicdata$Barcode %in% plantid$Barcode, plantid$PlantID[match(RootDynamicdata$Barcode, plantid$Barcode)], RootDynamicdata$Barcode))\nRootDynamicdata$PlantID <- paste(RootDynamicdata$PlantID, RootDynamicdata$ImagingDay, sep = \'_\')\nRootDynamicdata <- RootDynamicdata %>% select(PlantID\n                                              ,everything()\n                                              ,-Barcode\n                                              ,-ImagingDay\n                                              ,-\'Scale(mm)\'\n                                              ,-Resolution\n                                              ,-Threshold\n                                              )\n\nShootImagedata <- ShootImagedata %>% mutate(PlantID = ifelse(ShootImagedata$Barcode %in% plantid$Barcode, plantid$PlantID[match(ShootImagedata$Barcode, plantid$Barcode)], ShootImagedata$Barcode))\nShootImagedata <- ShootImagedata %>% select(PlantID\n                                            ,everything()\n                                            ,-Barcode\n                                            )\n\nBiomassdata <- Biomassdata %>% mutate(PlantID = ifelse(Biomassdata$Barcode %in% plantid$Barcode, plantid$PlantID[match(Biomassdata$Barcode, plantid$Barcode)], Biomassdata$Barcode))\nBiomassdata <- Biomassdata %>% select(PlantID\n                                      ,everything()\n                                      ,-Barcode\n                                      )\n\n## 9) Save preprocessed dataframes -------------------------------------------------------------------------------------\ndir.create(paste(expname,""_dataprocessing"", sep=""""), showWarnings = TRUE)\n\nwrite_csv(RootGIA3Ddata, paste0(expname,""_dataprocessing/"",expname,""_GIA3D_data.csv""))\nwrite_csv(RootDynamicdata, paste0(expname,""_dataprocessing/"",expname,""_DynamicRoots_data.csv""))\nwrite_csv(Biomassdata, paste0(expname,""_dataprocessing/"",expname,""_Biomass_data.csv""))\nwrite_csv(ShootImagedata, paste0(expname,""_dataprocessing/"",expname,""_ShootImage_data.csv""))\n', '## Marcus Griffiths, Alex Liu (2022)\r\n## Data processing for 3D gel imager\r\n\r\n#################\r\n## User setup  ## ------------------------------------------------------------------------------------------------------\r\n#################\r\n## 1) Set working directory --------------------------------------------------------------------------------------------\r\ngetwd()\r\nsetwd(""T:/FOR/for_AlexL/3DGel/TaGAA/10-26-22 sandbox/TaGAA_N_Accessions"") #PC\r\nsetwd(""C:\\\\Users\\\\USERNAME\\\\FILEPATH"") #PC\r\nsetwd(""/Users/USERNAME/FILEPATH"") #macOS\r\n\r\n# 2) Plantdata .csv files share the same PlantID column and identifier barcode eg. 011119_NAM1_B73_p1_t100_r1\r\n# 3) Install or load following packages\r\nlibrary(tidyverse)      #includes packages ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats\r\nlibrary(ggcorrplot)     #for correlation matrix plots\r\nlibrary(lmerTest)       #lme4, Matrix, lmer, step # for heritability & stat analysis\r\nlibrary(broom)          #for summarizing statistical model objects in tidy tibbles\r\nlibrary(agricolae)      #for tukeyhsd plots\r\nlibrary(ggpubr)         #for regression equations and r squared values\r\nlibrary(factoextra)     #for pca plots\r\nlibrary(FactoMineR)     #for pca plots\r\n\r\n##enter experiment parameter\r\nexp <- ""GAA""\r\nspecies <- ""Ta""\r\nexpname <- paste0(species,exp)       #experiment name for filenames\r\n\r\n## Import temporal plant measure .csv files\r\nRootGIA3Ddata <- read_csv(file=paste(expname,""_dataprocessing/"",expname,""_GIA3D_data.csv"", sep=""""), na = c(""NA"", ""na"", ""n.a."", """"))\r\nRootDynamicdata <- read_csv(file=paste(expname,""_dataprocessing/"",expname,""_DynamicRoots_data.csv"", sep=""""), na = c(""NA"", ""na"", ""n.a."", """"))\r\n## Import plant harvest measures .csv files\r\nBiomassdata <- read_csv(file=paste(expname,""_dataprocessing/"",expname,""_Biomass_data.csv"", sep=""""), na = c(""NA"", ""na"", ""n.a."", """"))\r\nShootImagedata <- read_csv(file=paste(expname,""_dataprocessing/"",expname,""_ShootImage_data.csv"", sep=""""), na = c(""NA"", ""na"", ""n.a."", """"))\r\nExtraRootdata <- read_csv(file=paste(expname,""_dataprocessing/"",expname,""_ExtraRoot_data.csv"", sep=""""), na = c(""NA"", ""na"", ""n.a."", """"))\r\n\r\n  \r\n## merge temporal measures\r\nPlantdata <- RootGIA3Ddata %>%\r\n  full_join(RootDynamicdata, by = ""PlantID"") %>% full_join(ExtraRootdata, by = ""PlantID"")\r\n\r\n\r\nPlantdata <-  Plantdata %>%\r\n  separate(PlantID,\r\n           into = c(""PlantID"", ""Day""),\r\n           sep = \'_d\')\r\n\r\nPlantdata <- Plantdata %>%\r\n  full_join(Biomassdata, by = ""PlantID"") %>%\r\n  full_join(ShootImagedata, by = ""PlantID"")\r\n\r\nrm(RootGIA3Ddata\r\n   ,RootDynamicdata\r\n   ,Biomassdata\r\n   ,ShootImagedata\r\n   ,ExtraRootdata\r\n   )\r\n\r\n##########################\r\n## Extract PlantID info ## ---------------------------------------------------------------------------------------------\r\n##########################\r\nname <- c(""Plant1""\r\n          ,""Treatment1""\r\n          ,""Rep1"")\r\n\r\nPlantdata <-  Plantdata %>%\r\n  separate(PlantID,\r\n           into = c(""Date"", ""Experiment"", ""Geno"", ""Plant"", ""Treatment"", ""Rep""),\r\n           # into = c(""Date"", ""Experiment"", ""Geno"", ""Plant"", ""Rep""),\r\n           sep = \'_\') %>%\r\n  separate(`Plant`,\r\n           into = c(""Plant1"",""Plant""),\r\n           sep = \'p\') %>%\r\n separate(`Treatment`,\r\n          into = c(""Treatment1"",""Treatment""),\r\n          sep = \'t\') %>%\r\n  separate(`Rep`,\r\n           into = c(""Rep1"",""Rep""),\r\n           sep = \'r\') %>%\r\n  select(-one_of(name))\r\n\r\nrm(name)\r\n\r\n###################\r\n## offset ## \r\n## if sample day goes to 21, those plants should be timeshifted back 4 days back to d17\r\n###################\r\n\r\n# concept: \r\n# if Day == 21:\r\n# Day of every entry with plant # x = Day-4\r\n# get list of rows to target\r\n# change the day to numeric\r\nPlantdata <- Plantdata %>% group_by(Plant) %>% mutate(Day = as.numeric(Day))\r\n\r\nshifted <- which(Plantdata$Day == 21)\r\nlate_plants <- Plantdata$Plant[shifted]\r\n#Plantdata$Plant %in% late_plants\r\nPlantdata <- Plantdata %>% \r\n  mutate(Day = replace(Day, Plant %in% late_plants, Day-4)) %>% \r\n  ungroup()\r\n\r\n###################\r\n## Sample counts ## ----------------------------------------------------------------------------------------------------\r\n###################\r\nSampleCount <- Plantdata %>% \r\n  group_by(Experiment\r\n           ,Geno\r\n           ,Treatment\r\n           ,Day\r\n           ) %>% \r\n  count(Geno)\r\n\r\ndir.create(paste0(expname,""_dataprocessing/output""),showWarnings = TRUE)\r\nwrite_csv(SampleCount, paste0(expname,""_dataprocessing/output/"",expname,""_samplecount.csv""))\r\n\r\n####################\r\n## Trait renaming ## ---------------------------------------------------------------------------------------------------\r\n####################\r\nPlantdata <- Plantdata %>% \r\n  rename(RootConvexHullVolume3D_mm3 = ""ConvexHullVolume3D(mm^3)"") %>%\r\n  rename(RootSolidity3D = Solidity3D) %>%\r\n  rename(RootBushiness3D = Bushiness3D) %>%\r\n  rename(RootDepth3D_mm = ""Depth3D(mm)"") %>%\r\n  rename(RootMaximumNetworkWidth3D_mm = ""MaximumNetworkWidth3D(mm)"") %>%\r\n  rename(RootLengthDistribution3D_mm = ""LengthDistribution3D(mm)"") %>%\r\n  rename(RootWidthDepth3D_Ratio = ""WidthDepthRatio3D"") %>%\r\n  rename(RootTotalCount = ""TotalRootNumber"") %>%\r\n  rename(RootTotalVolume_mm3 = ""TotalRootVolume(mm^3)"") %>%\r\n  rename(RootTotalLength_mm = ""TotalRootLength(mm)"") %>%\r\n  rename(RootTotalDepth_mm = ""TotalRootDepth(mm)"") %>%\r\n  rename(RootMeanVolume_mm3 = ""MeanRootVolume(mm^3)"") %>%\r\n  rename(RootMeanLength_mm = ""MeanRootLength(mm)"") %>%\r\n  rename(RootMeanDepth_mm = ""MeanRootDepth(mm)"") %>%\r\n  rename(RootMeanTortuosity = MeanRootTortuosity) %>%\r\n  rename(RootMeanRadius_mm = ""MeanRootRadius(mm)"") %>%\r\n  rename(RootMeanSoilAngle_degrees = MeanRootSoilAngle) %>%\r\n  rename(RootMeanBranchingAngle_degrees = MeanRootBranchingAngle) %>%\r\n  rename(RootMedianVolume_mm3 = ""MedianRootVolume(mm^3)"") %>%\r\n  rename(RootMedianLength_mm = ""MedianRootLength(mm)"") %>%\r\n  rename(RootMedianDepth_mm = ""MedianRootDepth(mm)"") %>%\r\n  rename(RootMedianTortuosity = MedianRootTortuosity) %>%\r\n  rename(RootMedianRadius_mm = ""MedianRootRadius(mm)"") %>%\r\n  rename(RootMedianSoilAngle_degrees = MedianRootSoilAngle) %>%\r\n  rename(RootMedianBranchingAngle_degrees = MedianRootBranchingAngle) %>%\r\n  rename(RootCountLateral = TotalLateralRootNumber) %>%\r\n  rename(RootVolumeLateral_mm3 = ""TotalLateralRootVolume(mm^3)"") %>%\r\n  rename(RootLengthLateral_mm = ""TotalLateralRootLength(mm)"") %>%\r\n  rename(RootTotalDepthLateral_mm = ""TotalLateralRootDepth(mm)"") %>%\r\n  rename(RootMeanVolumeLateral_mm3 = ""MeanLateralRootVolume(mm^3)"") %>%\r\n  rename(RootMeanLengthLateral_mm = ""MeanLateralRootLength(mm)"") %>%\r\n  rename(RootMeanDepthLateral_mm = ""MeanLateralRootDepth(mm)"") %>%\r\n  rename(RootMeanTortuosityLateral = MeanLateralRootTortuosity) %>%\r\n  rename(RootMeanRadiusLateral_mm = ""MeanLateralRootRadius(mm)"") %>%\r\n  rename(RootMeanSoilAngleLateral_degrees = MeanLateralRootSoilAngle) %>%\r\n  rename(RootMeanBranchingAngleLateral_degrees = MeanLateralRootBranchingAngle) %>%\r\n  rename(RootMedianVolumeLateral_mm3 = ""MedianLateralRootVolume(mm^3)"") %>%\r\n  rename(RootMedianLengthLateral_mm = ""MedianLateralRootLength(mm)"") %>%\r\n  rename(RootMedianDepthLateral_mm = ""MedianLateralRootDepth(mm)"") %>%\r\n  rename(RootMedianTortuosityLateral = MedianLateralRootTortuosity) %>%\r\n  rename(RootMedianRadiusLateral_mm = ""MedianLateralRootRadius(mm)"") %>%\r\n  rename(RootMedianSoilAngleLateral_degrees = MedianLateralRootSoilAngle) %>%\r\n  rename(RootMedianBranchingAngleLateral_degrees = MedianLateralRootBranchingAngle) %>%\r\n  rename(RootCountFirstOrderLateral = TotalFirstOrderLateralRootNumber) %>%\r\n  rename(RootVolumeFirstOrderLateral_mm3 = ""TotalFirstOrderLateralRootVolume(mm^3)"") %>%\r\n  rename(RootLengthFirstOrderLateral_mm = ""TotalFirstOrderLateralRootLength(mm)"") %>%\r\n  rename(RootTotalDepthFirstOrderLateral_mm = ""TotalFirstOrderLateralRootDepth(mm)"") %>%\r\n  rename(RootMeanVolumeFirstOrderLateral_mm3 = ""MeanFirstOrderLateralRootVolume(mm^3)"") %>%\r\n  rename(RootMeanLengthFirstOrderLateral_mm = ""MeanFirstOrderLateralRootLength(mm)"") %>%\r\n  rename(RootMeanDepthFirstOrderLateral_mm = ""MeanFirstOrderLateralRootDepth(mm)"") %>%\r\n  rename(RootMeanTortuosityFirstOrderLateral = MeanFirstOrderLateralRootTortuosity) %>%\r\n  rename(RootMeanRadiusFirstOrderLateral_mm = ""MeanFirstOrderLateralRootRadius(mm)"") %>%\r\n  rename(RootMeanSoilAngleFirstOrderLateral_degrees = MeanFirstOrderLateralRootSoilAngle) %>%\r\n  rename(RootMeanBranchingAngleFirstOrderLateral_degrees = MeanFirstOrderLateralRootBranchingAngle) %>%\r\n  rename(RootMedianVolumeFirstOrderLateral_mm = ""MedianFirstOrderLateralRootVolume(mm^3)"") %>%\r\n  rename(RootMedianLengthFirstOrderLateral_mm = ""MedianFirstOrderLateralRootLength(mm)"") %>%\r\n  rename(RootMedianDepthFirstOrderLateral_mm = ""MedianFirstOrderLateralRootDepth(mm)"") %>%\r\n  rename(RootMedianTortuosityFirstOrderLateral = ""MedianFirstOrderLateralRootTortuosity"") %>%\r\n  rename(RootMedianRadiusFirstOrderLateral_mm = ""MedianFirstOrderLateralRootRadius(mm)"") %>%\r\n  rename(RootMedianSoilAngleFirstOrderLateral_degrees = MedianFirstOrderLateralRootSoilAngle) %>%\r\n  rename(RootMedianBranchingAngleFirstOrderLateral_degrees = MedianFirstOrderLateralRootBranchingAngle) %>%\r\n  rename(RootDensityFirstOrderLateral_TL = DensityFirstOrderLateralRoot_TL) %>%\r\n  rename(RootDensityFirstOrderLateral_BRTL = DensityFirstOrderLateralRoot_BRTL) %>%\r\n  rename(InterbranchMeanDistance_mm = ""MeanInterbranchDistance(mm)"") %>%\r\n  rename(InterbranchMedianDistance_mm = ""MedianInterbranchDistance(mm)"") %>%\r\n  rename(RootVolumePrimary_mm3 = ""PrimaryRootVolume(mm^3)"") %>%\r\n  rename(RootLengthPrimary_mm = ""PrimaryRootLength(mm)"") %>%\r\n  rename(RootDepthPrimary_mm = ""PrimaryRootDepth(mm)"") %>%\r\n  rename(RootTortuosityPrimary = ""PrimaryRootTortuosity"") %>%\r\n  rename(RootRadiusPrimary = ""PrimaryRootRadius(mm)"") %>%\r\n  rename(RootSoilAnglePrimary_degrees = ""PrimaryRootSoilAngle"")\r\n\r\n########################\r\n## Trait calculations ## -----------------------------------------------------------------------------------------------\r\n########################\r\nPlantdata$Day <- as.numeric(Plantdata$Day)\r\n\r\n## arrange & calculate timeperiod between scans\r\nPlantdata <- Plantdata  %>%\r\n  arrange(Date\r\n          ,Experiment\r\n          ,Geno\r\n          ,Plant\r\n          ,Treatment\r\n          ,Rep\r\n          ,Day) %>%\r\n  mutate(Timeperiod = (Day-lag(Day))) ##order of rows is important for lag function\r\n\r\nPlantdata$Timeperiod[Plantdata$Timeperiod < 0 | is.na(Plantdata$Timeperiod)] <- 5\r\n\r\n\r\n# replace NAs in Extended_Root with 0s\r\nPlantdata$Extended_Root[is.na(Plantdata$Extended_Root)] <- 0\r\n\r\n\r\n## reorder columns\r\nPlantdata <- Plantdata %>% select(Experiment\r\n                                  ,Date\r\n                                  ,Geno\r\n                                  ,Plant\r\n                                  ,Treatment\r\n                                  ,Rep\r\n                                  ,Day\r\n                                  ,Timeperiod\r\n                                  ,RootDW_g\r\n                                  ,ShootDW_g\r\n                                  ,everything()\r\n                                  )\r\n# adding manually measured primary root lengths beyond camera FOV\r\n\r\n# root traits\r\nPlantdata <- Plantdata %>% ##add comments for traits\r\n  mutate(RootLengthSecondOrderLateral_mm = RootLengthLateral_mm-RootLengthFirstOrderLateral_mm\r\n         ,RootTotalGrowthRate_mm.h = (RootTotalLength_mm-lag(RootTotalLength_mm))/(Timeperiod*24) \r\n         ,RootLateralGrowthRate_mm.h = (RootLengthLateral_mm-lag(RootLengthLateral_mm))/(Timeperiod*24)\r\n         ,SpecRootLength_m.g = ((RootTotalLength_mm+Extended_Root)/1000)/(RootDW_g)\r\n         ## get correct units\r\n         ,SpecLeafArea_g.m2 = (ShootDW_g)/(PlantCVShootArea_unit2/10000)\r\n         ,TotalMass_g = (RootDW_g+ShootDW_g)\r\n         ,RootMassFract_g.g = (RootDW_g/TotalMass_g)\r\n         )\r\n\r\n# get the negatives of lag\r\n# there has to be a better way of doing this\r\nPlantdata$RootTotalGrowthRate_mm.h[Plantdata$RootTotalGrowthRate_mm.h < 0 |is.na(Plantdata$RootTotalGrowthRate_mm.h)] <- \r\n  Plantdata$RootTotalLength_mm[Plantdata$RootTotalGrowthRate_mm.h < 0 |is.na(Plantdata$RootTotalGrowthRate_mm.h)]/\r\n  Plantdata$Timeperiod[Plantdata$RootTotalGrowthRate_mm.h < 0 |is.na(Plantdata$RootTotalGrowthRate_mm.h)]/\r\n  24\r\nPlantdata$RootLateralGrowthRate_mm.h[Plantdata$RootLateralGrowthRate_mm.h < 0 | is.na(Plantdata$RootLateralGrowthRate_mm.h)] <- \r\n  Plantdata$RootLengthLateral_mm[Plantdata$RootLateralGrowthRate_mm.h < 0 | is.na(Plantdata$RootLateralGrowthRate_mm.h)]/\r\n  Plantdata$Timeperiod[Plantdata$RootLateralGrowthRate_mm.h < 0 | is.na(Plantdata$RootLateralGrowthRate_mm.h)]/\r\n  24\r\n\r\n## save it\r\nwrite_csv(Plantdata, paste0(expname,""_dataprocessing/output/"",expname,""_plantdata_unfiltered.csv""))\r\n\r\n###################\r\n## Data curation ## ----------------------------------------------------------------------------------------------------\r\n###################\r\n\r\nany(is.na(Plantdata))\r\n\r\n  ### Alex only throw out 1 reps per day rather than whole genotype?\r\n\r\nPlantdata <- Plantdata %>% replace(is.na(.), 0)\r\nPlantdata <- Plantdata %>% mutate_all(function(x) ifelse(is.infinite(x), 0, x))\r\n\r\ntemporaldata <- Plantdata %>% select(-Experiment\r\n                                     ,-Date\r\n                                     ,-Plant\r\n                                     ,-Timeperiod\r\n                                     ,-PlantCVShootArea_unit2\r\n                                     ,-PlantCVLeafCount\r\n                                     ,-SpecLeafArea_g.m2\r\n                                     ,-RootDW_g\r\n                                     ,-ShootDW_g\r\n                                     ,-SpecRootLength_m.g\r\n                                     ,-TotalMass_g\r\n                                     ,-RootMassFract_g.g\r\n                                     #,-RootDensityFirstOrderLateral_BRTL #inf value\r\n                                     )\r\n\r\n## remove plants with only one rep otherwise you\'ll break the ANOVA\r\n# one_reps <- SampleCount[which(SampleCount$n ==1), 2:3]\r\n\r\n# remove plants with any of the timepoints with one rep otherwise you\'ll break the ANOVA\r\nplants_rm <- SampleCount$Geno[which(SampleCount$n ==1)]\r\none_reps <- SampleCount[SampleCount$Geno %in% plants_rm, 2:3]\r\n\r\n# remove plants that are missing a day \r\ntemporaldata <- anti_join(temporaldata, one_reps)\r\nPlantdata <- anti_join(Plantdata, one_reps)\r\n\r\nwrite_csv(Plantdata, paste0(expname,""_dataprocessing/output/"",expname,""_plantdata.csv""))\r\n\r\n####################\r\n## Means & Errors ## ---------------------------------------------------------------------------------------------------\r\n####################\r\ndat <- Plantdata\r\ndat$Timeperiod <- dat$Timeperiod %>% as_factor()\r\ndat <- dat %>% mutate(across(where(is.numeric), ~na_if(., Inf)), across(where(is.numeric), ~na_if(., -Inf)))\r\n\r\n## define standard error function --------------------------------------------------------------------------------------\r\nstderror <- function(x) sd(na.omit(x))/sqrt(length(na.omit(x)))\r\n\r\n## calculate means & errors --------------------------------------------------------------------------------------------\r\ndat_mean <- dat %>% \r\n  group_by(Geno\r\n           ,Treatment\r\n           ,Day\r\n           ) %>% \r\n  summarise(\r\n    across(\r\n      .cols = where(is.numeric)\r\n      ,mean\r\n      ,.names = ""mean_{.col}""\r\n      ,na.rm = TRUE\r\n    )\r\n  ) %>% \r\n  ungroup()\r\n\r\ndat_se <- dat %>% \r\n  group_by(Geno\r\n           ,Treatment\r\n           ,Day\r\n           ) %>% \r\n  summarise(\r\n    across(\r\n      .cols = where(is.numeric)\r\n      ,stderror\r\n      ,.names = ""se_{.col}""\r\n    )\r\n  ) %>% \r\n  ungroup()\r\n\r\ndat_meanErrors <- merge(dat_mean, dat_se, by=c(""Geno""\r\n                                               ,""Treatment""\r\n                                               ,""Day""\r\n                                               ))\r\n\r\nwrite_csv(dat_meanErrors, paste0(expname,""_dataprocessing/output/"",expname,""_plantdata_MeansErrors.csv""))\r\n\r\nrm(dat\r\n   ,dat_mean\r\n   ,dat_se\r\n   )\r\n\r\n########################\r\n## correlation matrix ## -----------------------------------------------------------------------------------------------\r\n########################\r\ndir.create(paste0(expname,""_dataprocessing/output/corrplot""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/corrplot/png""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/corrplot/pdf""),showWarnings = TRUE)\r\n\r\n######################\r\n## correlation matrix all data\r\n## \r\n\r\n## keep variable columns only\r\n## Remove Inf value traits if present\r\n\r\n# dat <- Plantdata %>% \r\n#   filter(Day==17) %>%\r\n#   # filter(Treatment==""HighN"") %>%\r\n#   select(-Experiment\r\n#         ,-Date\r\n#         ,-Geno\r\n#         ,-Plant\r\n#         ,-Treatment\r\n#         ,-Day\r\n#         ,-Timeperiod\r\n#         ,-Rep\r\n#         )\r\n\r\ndat <- Plantdata %>%\r\n  filter(Day==17) %>%\r\n  filter(Treatment==""HighN"") %>%\r\n  #keep variable columns only\r\n  select(RootTotalLength_mm\r\n         ,RootTotalVolume_mm3\r\n         ,RootTotalCount\r\n         ,RootDepth3D_mm\r\n         ,RootBushiness3D\r\n         ,RootMaximumNetworkWidth3D_mm\r\n         ,RootConvexHullVolume3D_mm3\r\n         # ,RootWidthDepth3D_Ratio\r\n         ,RootLengthDistribution3D_mm\r\n         ,RootSolidity3D\r\n         # ,RootMeanRadius_mm\r\n         ,SpecRootLength_m.g\r\n         # ,RootTotalGrowthRate_mm.h\r\n         ,InterbranchMeanDistance_mm\r\n         ,RootMedianBranchingAngleLateral_degrees\r\n  )\r\n\r\ndat <- dat %>% mutate_all(function(x) ifelse(is.infinite(x), 0, x))\r\n\r\ncorr <- cor(dat, use = ""pairwise.complete.obs"")\r\npmat <- cor_pmat(dat) #compute matrix of correlation p-values\r\n\r\nggcorrplot(corr\r\n           ,method = ""circle""\r\n           ,hc.order = TRUE\r\n           #,type = ""lower""\r\n           #,lab = TRUE\r\n           ,p.mat = pmat\r\n           ,outline.col = ""white""\r\n           ) +\r\n  theme(legend.position=""top"")\r\nggsave(paste0(expname,""_dataprocessing/output/corrplot/png/"",expname,""_corrplot_plantdata_order_subset.png""), width=8, height=8, dpi=300)\r\nggsave(paste0(expname,""_dataprocessing/output/corrplot/pdf/"",expname,""_corrplot_plantdata_order_subset.pdf""), width=8, height=8, useDingbats=FALSE)\r\n# ggsave(paste0(expname,""_dataprocessing/output/corrplot/png/"",expname,""_corrplot_plantdata_order.png""), width=28, height=28, dpi=300)\r\n# ggsave(paste0(expname,""_dataprocessing/output/corrplot/pdf/"",expname,""_corrplot_plantdata_order.pdf""), width=28, height=28, useDingbats=FALSE)\r\n\r\ncorr %>% as_tibble() %>% write_csv(paste0(expname,""_dataprocessing/output/corrplot/"",expname,""_corr_plantdata_subset.csv"")) \r\npmat %>% as_tibble() %>% write_csv(paste0(expname,""_dataprocessing/output/corrplot/"",expname,""_pmat_plantdata_subset.csv""))\r\n\r\nrm(dat\r\n   ,corr\r\n   ,pmat\r\n   )\r\n\r\n##################\r\n## PCA analysis ## -----------------------------------------------------------------------------------------------------\r\n##################\r\ndir.create(paste0(expname,""_dataprocessing/output/pca""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/pca/png""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/pca/pdf""),showWarnings = TRUE)\r\n\r\n## keep variable columns only\r\n\r\n# dat <- Plantdata %>%\r\n#   filter(Day==17) %>%\r\n#   # filter(Treatment==""HighN"") %>%\r\n#   select(-Experiment\r\n#          ,-Date\r\n#          ,-Geno\r\n#          ,-Plant\r\n#          ,-Treatment\r\n#          ,-Day\r\n#          ,-Timeperiod\r\n#          ,-Rep\r\n#   )\r\n\r\ndat <- Plantdata %>%\r\n  filter(Day==17) %>%\r\n  filter(Treatment==""HighN"") %>%\r\n  #keep variable columns only\r\n  select(RootTotalLength_mm\r\n         ,RootTotalVolume_mm3\r\n         ,RootTotalCount\r\n         ,RootDepth3D_mm\r\n         ,RootBushiness3D\r\n         ,RootMaximumNetworkWidth3D_mm\r\n         ,RootConvexHullVolume3D_mm3\r\n         ,RootWidthDepth3D_Ratio\r\n         ,RootLengthDistribution3D_mm\r\n         ,RootSolidity3D\r\n         ,RootMeanRadius_mm\r\n         ,SpecRootLength_m.g\r\n         # ,RootTotalGrowthRate_mm.h\r\n         ,InterbranchMeanDistance_mm\r\n         ,RootMedianBranchingAngleLateral_degrees\r\n  )\r\n\r\n## Remove NA and Inf value traits if present\r\ndat <- dat %>% replace(is.na(.), 0)\r\ndat <- dat %>% mutate_all(function(x) ifelse(is.infinite(x), 0, x))\r\n\r\nres.pca <- PCA(dat, graph = FALSE)\r\nget_eig(res.pca)\r\neig_output <- get_eig(res.pca)\r\nwrite.csv(eig_output, paste0(expname,""_dataprocessing/output/pca/PCA_eig_plantdata_subset.csv""))\r\n\r\nfviz_screeplot(res.pca\r\n               ,addlabels = TRUE\r\n               ,ggtheme = theme_bw()\r\n               ,title = """"\r\n               )\r\nggsave(paste0(expname,""_dataprocessing/output/pca/png/"",expname,""_elbow_plantdata_subset.png""), width=7, height=5, dpi=300)\r\nggsave(paste0(expname,""_dataprocessing/output/pca/pdf/"",expname,""_elbow_plantdata_subset.pdf""), width=7, height=5)\r\n\r\nvar <- get_pca_var(res.pca)\r\nvar_contrib <- as.data.frame(var$contrib)\r\nwrite.csv(var_contrib, paste0(expname,""_dataprocessing/output/pca/PCA_contribution_plantdata_subset.csv""))\r\n\r\nfviz_pca_var(res.pca\r\n             ,col.var=""contrib""\r\n             ,gradient.cols = c(""blue"", ""red"")\r\n             ,ggtheme = theme_bw()\r\n             ,repel = FALSE # Avoid text overlapping\r\n             ,title = """"\r\n             )\r\nggsave(paste0(expname,""_dataprocessing/output/pca/png/"",expname,""_PCA_plantdata_subset.png""), width=7, height=5, dpi=300)\r\nggsave(paste0(expname,""_dataprocessing/output/pca/pdf/"",expname,""_PCA_plantdata_subset.pdf""), width=7, height=5)\r\n\r\nfviz_contrib(res.pca\r\n             ,choice = ""var""\r\n             ,axes = 1\r\n             ,top = 10\r\n             ,title = """"\r\n             )\r\nggsave(paste0(expname,""_dataprocessing/output/pca/png/"",expname,""_contrib_plantdata_subset_PC1.png""), width=7, height=5, dpi=300)\r\nggsave(paste0(expname,""_dataprocessing/output/pca/pdf/"",expname,""_contrib_plantdata_subset_PC1.pdf""), width=7, height=5)\r\n\r\nrm(dat\r\n   ,res.pca\r\n   ,eig_output\r\n   ,var\r\n   ,var_contrib\r\n   )\r\n\r\n###################\r\n## Stat analysis ## ----------------------------------------------------------------------------------------------------\r\n###################\r\ndir.create(paste0(expname,""_dataprocessing/output/stat_analysis""),showWarnings = TRUE)\r\n\r\n## define significance stars function\r\nmake_significance_stars <- function(pval) {    #make star function\r\n  stars = """"\r\n  if(pval <= 0.001)\r\n    stars = ""***""\r\n  if(pval > 0.001 & pval <= 0.01)\r\n    stars = ""**""\r\n  if(pval > 0.01 & pval <= 0.05)\r\n    stars = ""*""\r\n  if(pval > 0.05)\r\n    stars = ""ns""\r\n  stars\r\n}\r\n\r\n## define Day * Geno + (1|Rep) anova function\r\ncompute_aov_DayGeno <- function(x)\r\n{\r\n  dat <- x\r\n  dat[[1]] <- factor(dat[[1]])\r\n  dat[[2]] <- factor(dat[[2]])\r\n  dat[[3]] <- factor(dat[[3]])\r\n  \r\n  resultOutput <- data.frame()\r\n  \r\n  for (i in 4:length(colnames(dat)))\r\n  {\r\n    print(colnames(dat)[i])\r\n    # if(i != 31 && i != 32 && i != 36 && i != 37) #drop variables which are not present in both treatments\r\n    {\r\n      anova_output <- anova(lmer(dat[[i]] ~ Day * Geno + (1|Rep), data=dat))\r\n      anova_output <- broom::tidy(anova_output)\r\n      anova_output <- anova_output %>%\r\n        mutate(trait = colnames(dat)[i]) %>%\r\n        mutate(signif = sapply(p.value, function(x) make_significance_stars(x))) %>%\r\n        mutate(statistic_2dp = formatC(statistic, digits=2,format=""f"")) %>%\r\n        mutate(fvalue_significance = paste(statistic_2dp, signif))\r\n      resultOutput <- bind_rows(resultOutput, anova_output)\r\n    }\r\n  }\r\n  \r\n  return(resultOutput)\r\n}\r\n\r\n## Day * Genotype\r\ndir.create(paste0(expname,""_dataprocessing/output/stat_analysis/DayxGeno""),showWarnings = TRUE)\r\n\r\ndat <- temporaldata %>% select(everything()\r\n                               ,-Treatment\r\n                               )\r\n\r\ndat <- dat %>% replace(is.na(.), 0)\r\ndat <- dat %>% mutate_all(function(x) ifelse(is.infinite(x), 0, x))\r\n\r\nresultOutput <- compute_aov_DayGeno(dat)\r\nwrite_csv(resultOutput, paste0(expname,""_dataprocessing/output/stat_analysis/DayxGeno/"",expname,""_aov_dat_DayxGeno_raw.csv""))\r\nresultOutputSummaryTable <- resultOutput %>% select(term, fvalue_significance, trait) %>%\r\n  spread(term, fvalue_significance)\r\nwrite_csv(resultOutputSummaryTable, paste0(expname,""_dataprocessing/output/stat_analysis/DayxGeno/"",expname,""_aov_dat_DayxGeno_summary.csv""))\r\n\r\nrm(resultOutput\r\n   ,resultOutputSummaryTable\r\n   ,dat\r\n  )\r\n\r\n########################\r\n## TukeyHSD (posthoc) ## -----------------------------------------------------------------------------------------------\r\n########################\r\ndir.create(paste0(expname,""_dataprocessing/output/stat_analysis_posthoc/""),showWarnings = TRUE)\r\n\r\n## define Day * Geno + Rep tukeyHSD function\r\ncompute_aov_tukey_DayGeno <- function(x)\r\n{\r\n  dat <- x\r\n  dat[[1]] <- factor(dat[[1]])\r\n  dat[[2]] <- factor(dat[[2]])\r\n  # rep needs to be factored too\r\n  dat[[3]] <- factor(dat[[3]])\r\n  \r\n  for (i in 4:length(colnames(dat)))\r\n  {\r\n    print(colnames(dat)[i])\r\n    # if(i != 31 && i != 32 && i != 44 && i != 45) #drop variables which are not present in both treatments\r\n    {\r\n      print(i)\r\n      summarytable <- tidy(aov(dat[[i]] ~ Day * Geno + Rep, data = dat, na.action=na.omit))\r\n      write_csv(summarytable, paste0(expname,""_dataprocessing/output/stat_analysis_posthoc/DayxGeno/"",colnames(dat)[i],""_aov.csv""))\r\n      paste(\'made it\')\r\n      tempdf <- dat[c(1,2,3,i)];\r\n      thsd <- tidy(TukeyHSD(aov(tempdf[[4]] ~ Day * Geno + Rep, data=tempdf, na.action=na.omit)))\r\n      write_csv(thsd, paste0(expname,""_dataprocessing/output/stat_analysis_posthoc/DayxGeno/TukeyHSD/"",colnames(dat)[i],""_TukeyHSD.csv""))\r\n    }\r\n  }\r\n}\r\n\r\n## Day * Genotype\r\ndat <- temporaldata %>%\r\n  select(everything()\r\n         ,-Treatment\r\n  )\r\n\r\ndir.create(paste0(expname,""_dataprocessing/output/stat_analysis_posthoc/DayxGeno/""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/stat_analysis_posthoc/DayxGeno/TukeyHSD/""),showWarnings = TRUE)\r\n\r\ncompute_aov_tukey_DayGeno(dat)\r\n\r\nrm(dat)\r\n\r\n#########################\r\n## Interaction boxplots## ----------------------------------------------------------------------------------------------\r\n#########################\r\n# read in Plantdata if just starting here\r\nPlantdata <- read_csv(paste0(expname,""_dataprocessing/output/"",expname,""_Plantdata.csv""))\r\n\r\n## Day * Geno\r\nboxplot_fun_DayGeno = function(y) {\r\n  ggplot(dat, aes(x = interaction(dat[factors]), y=.data[[y]])) +\r\n    # ggplot(dat, aes(fct_reorder(Group, .data[[y]]), y=.data[[y]])) +\r\n    geom_boxplot(aes(\r\n      color=Geno\r\n    )) +\r\n    ylab(y) +\r\n    theme_bw() +\r\n    theme(\r\n      plot.background = element_blank()\r\n      ,panel.grid.major = element_blank()\r\n      ,panel.grid.minor = element_blank()\r\n      ,axis.text.x = element_text(angle=45,hjust=1)\r\n    )\r\n  ggsave(paste0(expname,""_dataprocessing/output/plant_boxplots_all/DayxGeno/png/"",expname,""_"",y,""_HighN.png""), width=6, height=3.1968, dpi=300)\r\n  ggsave(paste0(expname,""_dataprocessing/output/plant_boxplots_all/DayxGeno/pdf/"",expname,""_"",y,""_HighN.pdf""), width=6, height=3.1968, useDingbats=FALSE)\r\n}\r\n\r\ndat <- Plantdata %>% select(Geno,Day,everything())\r\ndat <- Plantdata %>% filter(Treatment==""HighN"") %>% select(Geno,Day,everything())\r\n# dat <- Plantdata %>% filter(Treatment==""TraceN"") %>% select(Day,Geno,everything())\r\n\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/DayxGeno/""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/DayxGeno/pdf""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/DayxGeno/png""),showWarnings = TRUE)\r\n\r\ncolnames(dat)\r\n\r\nfactors = names(dat[1:2])\r\nfactors = set_names(factors)\r\nfactors\r\n\r\ntraits = names(dat[9:length(dat)])\r\ntraits = set_names(traits)\r\ntraits\r\n\r\ndat <- unite(dat\r\n             ,all_of(factors)\r\n             ,col = ""Group""\r\n             ,sep = ""_""\r\n             ,remove = FALSE\r\n)\r\n\r\nall_crossed <- expand.grid(traits = traits,\r\n                           stringsAsFactors = FALSE\r\n)\r\n\r\nmap(all_crossed$traits, boxplot_fun_DayGeno)\r\n\r\nrm(dat\r\n   ,factors\r\n   ,traits\r\n   ,all_crossed\r\n)\r\n\r\n## Geno * Treatment\r\nd6data <- Plantdata %>% filter(Day==""6"")\r\nd9data <- Plantdata %>% filter(Day==""9"")\r\nd13data <- Plantdata %>% filter(Day==""13"")\r\nd17data <- Plantdata %>% filter(Day==""17"")\r\n\r\n## define Geno * Treatment boxplot function\r\nboxplot_fun_GenoTreatment = function(y) {\r\n  # ggplot(dat, aes(x = interaction(dat[factors]), y=.data[[y]])) +\r\n    ggplot(dat, aes(fct_reorder(Group, .data[[y]]), y=.data[[y]])) +\r\n    geom_boxplot(aes(color=Treatment)) +\r\n    ylab(y) +\r\n    theme_bw() +\r\n    theme(\r\n      plot.background = element_blank()\r\n      ,panel.grid.major = element_blank()\r\n      ,panel.grid.minor = element_blank()\r\n      ,axis.text.x = element_text(angle=45,hjust=1)\r\n    )\r\n  ggsave(paste0(expname,""_dataprocessing/output/plant_boxplots_all/GenoxTreatment/d17/png/"",expname,""_"",y,"".png""), width=6, height=3.1968, dpi=300)\r\n  ggsave(paste0(expname,""_dataprocessing/output/plant_boxplots_all/GenoxTreatment/d17/pdf/"",expname,""_"",y,"".pdf""), width=6, height=3.1968, useDingbats=FALSE)\r\n}\r\n\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/GenoxTreatment/""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/GenoxTreatment/d17/""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/GenoxTreatment/d17/pdf""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/GenoxTreatment/d17/png""),showWarnings = TRUE)\r\n\r\ndat <- d17data %>% select(Geno\r\n                          ,Treatment\r\n                          ,everything())\r\ncolnames(dat)\r\n\r\nfactors = names(dat[1:2])\r\nfactors = set_names(factors)\r\nfactors\r\n\r\ntraits = names(dat[9:length(dat)])\r\ntraits = set_names(traits)\r\ntraits\r\n\r\ndat <- unite(dat\r\n             ,all_of(factors)\r\n             ,col = ""Group""\r\n             ,sep = ""_""\r\n             ,remove = FALSE\r\n             )\r\n\r\nall_crossed <- expand.grid(traits = traits,\r\n                           stringsAsFactors = FALSE\r\n                           )\r\n\r\nmap(all_crossed$traits, boxplot_fun_GenoTreatment)\r\n\r\nrm(dat\r\n   ,d6data\r\n   ,d9data\r\n   ,d13data\r\n   ,d17data\r\n   ,traits\r\n   ,factors\r\n   ,all_crossed\r\n   )\r\n\r\n## Day * Treatment\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/DayxTreatment/""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/DayxTreatment/pdf""),showWarnings = TRUE)\r\ndir.create(paste0(expname,""_dataprocessing/output/plant_boxplots_all/DayxTreatment/png""),showWarnings = TRUE)\r\n\r\nboxplot_fun_DayTreatment = function(y) {\r\n  ggplot(dat, aes(x = interaction(dat[factors]), y=.data[[y]])) +\r\n  # ggplot(dat, aes(fct_reorder(Group, .data[[y]]), y=.data[[y]])) +\r\n    geom_boxplot(aes(\r\n      color=Treatment\r\n    )) +\r\n    ylab(y) +\r\n    theme_bw() +\r\n    theme(\r\n      plot.background = element_blank()\r\n      ,panel.grid.major = element_blank()\r\n      ,panel.grid.minor = element_blank()\r\n      ,axis.text.x = element_text(angle=45,hjust=1)\r\n    )\r\n  ggsave(paste0(expname,""_dataprocessing/output/plant_boxplots_all/DayxTreatment/png/"",expname,""_"",y,"".png""), width=6, height=3.1968, dpi=300)\r\n  ggsave(paste0(expname,""_dataprocessing/output/plant_boxplots_all/DayxTreatment/pdf/"",expname,""_"",y,""_.pdf""), width=6, height=3.1968, useDingbats=FALSE)\r\n}\r\n\r\ndat <- Plantdata %>% select(Day\r\n                            ,Treatment\r\n                            ,everything()\r\n                            )\r\ncolnames(dat)\r\n\r\nfactors = names(dat[1:2])\r\nfactors = set_names(factors)\r\nfactors\r\n\r\ntraits = names(dat[9:length(dat)])\r\ntraits = set_names(traits)\r\ntraits\r\n\r\ndat <- unite(dat\r\n             ,all_of(factors)\r\n             ,col = ""Group""\r\n             ,sep = ""_""\r\n             ,remove = FALSE\r\n             )\r\n\r\nall_crossed <- expand.grid(traits = traits,\r\n                           stringsAsFactors = FALSE\r\n                           )\r\n\r\nmap(all_crossed$traits, boxplot_fun_DayTreatment)\r\n\r\nrm(dat\r\n   ,factors\r\n   ,traits\r\n   ,all_crossed\r\n   )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n']","Data and statistical analysis scripts for manuscript on pennycress roots & response to nitrate using 3D gel system Data and statistical analysis scripts for manuscript on pennycress roots & response to nitrate using 3Dgel systemA temporal atlas and response to nitrate availability of 3D root system architecture in diverse pennycress (Thlaspi arvense L.) accessionsThe following files contains:gel_data_preprocessing_20221024.R - R statistics script for pre-processing data files from 3Dgel system GIARoots & DynamicRoots raw outputgel_dataprocessing_20221229.R - R statistics script for data processing of pre-processed 3D gel dataTaGNS_N_Spring32.zip - CSV data files and R statistics script for Spring32 grown under high, low, trace and zero N treatments.TaGNE_N_Accessions.zip - CSV data files and R statistics script for 3 accessions under high and trace N treatments.TaGAA_N_Accessions.zip - CSV data files and R statistics script for 24 diverse pennycress lines grown under high N conditions.",3
"Transmission, infectivity, and neutralization of a spike L452R SARS-CoV-2 variant","This dataset contains the following:genome fasta files from viral whole-genome sequencing of SARS-CoV-2 positive nasal/nasopharyngeal swabscodes used to analyze epidemiologic data (i.e. logistic growth curves, transmission rate, doubling time) and generate plots and figurescodes used to perform statistical analysis and visualization of viral loadsphylogenetic analyses description, trees, and logssupplementary tables with relevant sample information","['# This script summarizes stats and calculates Welch\'s t-test p value of Ct values \n# and generates corresponding box plots\n# Venice Servellita 2021-03-26\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(dplyr)\n\ndata_ct <- read_tsv(""Ct_values.txt"")\nset.seed(1234)\ndata_ct %>% sample_n_by(group,size=2)\n\nlevels(data_ct$group)\ndata_ct %>%\n  group_by(group) %>%\n  get_summary_stats(Ct, type = ""mean_sd"")\nsumm <- data_ct %>% \n  group_by(group) %>% \n  summarize(mean = mean(Ct), median = median(Ct), sd = sd(Ct))\nsumm\ndata_ct\n\ndata_ct %>% \n  group_by(group) %>%\n  identify_outliers(Ct)\n\n#Welch t-test\nstat.test <- data_ct %>%\n  t_test(Ct ~ group, detailed=TRUE) %>%  #var.equal = TRUE (Student\'s t test)\n  add_significance()\nstat.test\n\np <- ggboxplot(data_ct, x = ""group"", y = ""Ct"",\n               #color = ""group"", palette = c(""#00AFBB"", ""#E7B800""),\n               title=""Cycle Threshold (Ct) values"",size=0.5,\n               xlab=FALSE,bxp.errorbar=TRUE,bxp.errorbar.width=0.2,\n               width=0.5)\np + stat_pvalue_manual(stat.test, label = ""p"",y.position = 42, step.increase = 0.1)+\n  geom_text(data = summ, aes(x = group, y = mean, \n                             label = paste(""Mean: "", round(mean, 1), ""\\nMedian: "", median, ""\\nSD: "", round(sd, 1))))\nggsave(""~/work_backup/new_all_Ct_boxplot.pdf"", w= 5, h = 5)\nsessionInfo()\n\n', '# This code was modified from Kristian Andersen\'s Lab \n# by Venice Servellita 2021-03-26\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(zoo)\nlibrary(gridExtra)\nlibrary(sf)\nlibrary(dplyr)\n\n## b1427/b1429 frequency tables\nb1429 <- read_tsv(""~/work_backup/plot_rawdata_tab_plot.txt"") %>%\n  select(location, collection_date, n_seq, n_b1429, total_pct)\n\nmetadata_rolling_avg <- b1429 %>%\n  filter(location %in% c(""CA"")) %>%\n  group_by(location) %>%\n  group_modify(~{\n    tmp <- .x\n    last_date <- .x %>% filter(n_b1429 > 0) %>% tail(n = 1) %>% select(collection_date) %>% first()\n    first_date <- .x %>% filter(n_b1429 > 0) %>% head(n = 1) %>% select(collection_date) %>% first()\n    avg_last_5day_b1429_seq_pct = tmp %>% filter(n_b1429 > 0 & n_seq > 0) %>% filter(collection_date >= last_date - 5) %>% summarise(b1429_pct = mean(n_b1429/n_seq)) %>% select(b1429_pct) %>% last() #Averge of last 5 days of B1429\n    tmp <- tmp %>%\n      mutate(\n        n_b1429 = ifelse(collection_date > last_date, NA, n_b1429)\n      )\n    tmp <- tmp %>%\n      mutate(\n        type = ifelse(is.na(n_b1429), ""extrapolated"", ""data""),\n        b1429_est_pct_raw = ifelse(!is.na(n_b1429), (n_b1429/n_seq) , avg_last_5day_b1429_seq_pct)\n      )\n    tmp %>%\n      filter(!is.na(b1429_est_pct_raw)) %>%\n      group_by(collection_date) %>%\n      group_modify(~{\n        tmp_date <- .x\n        rolling_values <- tmp %>%\n          filter(collection_date >= .y - 2 & collection_date <= .y + 2) %>%\n          summarise(\n            b1429_est_pct = mean(b1429_est_pct_raw, na.rm=TRUE)\n          )\n        tmp_date %>%\n          mutate(\n            b1429_est_pct = rolling_values$b1429_est_pct\n          )\n      })\n  })\n\n## Plot rolling means\nrm_ca_p <- metadata_rolling_avg %>%\n  filter(location == ""CA"") %>%\n  ggplot() +\n  geom_col(aes(collection_date, b1429_est_pct_raw, fill = type)) +\n  geom_line(aes(collection_date, b1429_est_pct)) +\n  geom_point(aes(collection_date, b1429_est_pct)) +\n  theme_bw() +\n  scale_y_continuous(limits = c(-0.05, 1), name=""Proportion of B.1429 cases"") + xlab(""Days"") +\n  scale_x_date(limits=c(as.Date(""2020-09-01""), as.Date(""2021-05-01"")))\n\n\ngrid.arrange(rm_ca_p, ncol = 1, nrow =1)\nggsave(""~/work_backup/b1429_rolling_mean.pdf"", w= 10, h = 7.5)\n\ndev.off()\n\n## Fit NLS\nfit_plot_logistic_growth <- function(df, name){\n  tmp <- df %>%\n    arrange(collection_date)\n  first_date <- tmp %>%\n    filter(b1429_est_pct >= 0) %>% #changed to include zero values\n    arrange(collection_date) %>%\n    ungroup()  %>%\n    select(collection_date) %>%\n    head(n = 1) %>%\n    first()\n\n  tmp <- tmp %>%\n    filter(collection_date >= first_date)\n  tmp$ndays = (tmp$collection_date - tmp$collection_date[1]) %>% as.integer()\n  fit <- nls(b1429_est_pct ~ 1/(1+(((1/x0) - 1) * exp( -1 * r * ndays))), data = tmp, start=list(r = 0.08, x0 = 0.001))\n  tmp_newdata <- data.frame(ndays = seq(0,300), collection_date = first_date + seq(0,300))\n  tmp_newdata$predicted = predict(fit, newdata = tmp_newdata)\n  \n  above_50_date <- tmp_newdata %>% filter(predicted >= 0.5)  %>% select(collection_date) %>% first() %>% first()\n  serial_interval <- c(5,6.5)\n  print(coef(fit))\n  print (round(coef(fit)[1], 2))\n  print (round(serial_interval * coef(fit)[1] *100, 1))\n  print (round(log(2)/coef(fit)[1], 2))\n  \n  \n  ggplot() +\n    geom_col(data=tmp,\n             aes(x = collection_date,\n                 y = total_pct)) +\n    geom_point(data = tmp, aes(collection_date, b1429_est_pct,color = type)) +\n    geom_line(data = tmp_newdata, aes(collection_date, predicted)) +\n    #geom_point(aes(y=total_cases$total_pct,color=""blue""))+\n    geom_text(data = tmp, x = first_date, y = 0.1, label = paste0(""Logistic growth rate = "", round(coef(fit)[1], 2), "" per day""), hjust=-2) +\n    geom_text(data=tmp, x = first_date, y = 0.2, label = paste0(""Increase in transmission = "", paste(paste0(round(serial_interval * coef(fit)[1] *100, 1), ""%""), collapse = "" - "")), hjust=-1.5) +\n    ## geom_text(x = first_date, y = max(tmp$b117_est_pct, na.rm=T), label = paste0(""Doubling time ~ 1 week ""), hjust=0) +\n    geom_text(data = tmp, x = first_date, y = 0.6, label = paste0(""Rough doubling time = "", round(log(2)/coef(fit)[1], 2),"" days""), hjust=-0.5) +\n    geom_text(data = tmp, x = above_50_date, y = 0.75, label = paste0(""50% date = "", above_50_date), hjust=0) +\n    geom_vline(xintercept = above_50_date, color=""red"") +\n    theme_bw() + scale_x_date(limits=c(as.Date(""2020-10-01""), as.Date(""2021-05-01"")), date_breaks=""30 days"", date_labels=""%b %d"") + scale_y_continuous(limits = c(-0.05, 1), name=""Proportion of B.1429 cases"") + xlab(""Days"") + ggtitle(name)\n}\n\np4 <- metadata_rolling_avg %>%\n  filter(location == ""CA"") %>%\n  fit_plot_logistic_growth(""CA"")\n\ngrid.arrange(p4, ncol = 1, nrow =1)\nggsave(""~/work_backup/fit_b1429.pdf"", w= 10, h = 7.5)\n\n']","Transmission, infectivity, and neutralization of a spike L452R SARS-CoV-2 variant This dataset contains the following:genome fasta files from viral whole-genome sequencing of SARS-CoV-2 positive nasal/nasopharyngeal swabscodes used to analyze epidemiologic data (i.e. logistic growth curves, transmission rate, doubling time) and generate plots and figurescodes used to perform statistical analysis and visualization of viral loadsphylogenetic analyses description, trees, and logssupplementary tables with relevant sample information",3
Data from: Telomere length reveals cumulative individual and transgenerational inbreeding effects in a passerine bird,"Inbreeding results in more homozygous offspring that should suffer reduced fitness, but it can be difficult to quantify these costs for several reasons. First, inbreeding depression may vary with ecological or physiological stress and only be detectable over long time periods. Second, parental homozygosity may indirectly affect offspring fitness, thus confounding analyses that consider offspring homozygosity alone. Finally, measurement of inbreeding coefficients, survival and reproductive success may often be too crude to detect inbreeding costs in wild populations. Telomere length provides a more precise measure of somatic costs, predicts survival in many species and should reflect differences in somatic condition that result from varying ability to cope with environmental stressors. We studied relative telomere length in a wild population of Seychelles warblers (Acrocephalus sechellensis) to assess the lifelong relationship between individual homozygosity, which reflects genome-wide inbreeding in this species, and telomere length. In juveniles, individual homozygosity was negatively associated with telomere length in poor seasons. In adults, individual homozygosity was consistently negatively related to telomere length, suggesting the accumulation of inbreeding depression during life. Maternal homozygosity also negatively predicted offspring telomere length. Our results show that somatic inbreeding costs are environmentally dependent at certain life stages but may accumulate throughout life.","['rm(list=ls())\r\nAdults <- read.csv(""Adults Hz.csv"")\r\n###################################################\r\n#1) individual heterozygosity\r\nhist(Adults$RTL)\r\nAdults <- subset(Adults, PropCovered >0.49)\r\n\r\nlibrary(nlme)\r\n#minimal model with known predictors:\r\n\r\nnullmodelA<- lme(RTL~Age + MeanAdultFood + Sex, random=~1|BirdID, data=Adults, na.action=na.omit)\r\nsummary(nullmodelA)\r\n\r\n#all variables in the null model are significant\r\n\r\n#now create full model with all known predictors, plus homozygosity and interactions\r\n\r\nfullmodelA <- lme(RTL~Age +  Sex+ MeanAdultFood + Homozygosity+\r\n                    Homozygosity*Age  + Homozygosity*MeanAdultFood + Homozygosity*Sex,\r\n                  random=~1|BirdID, data=Adults, na.action=na.omit)\r\nsummary(fullmodelA)\r\n\r\n#remove nonsignificant interactions\r\n\r\nmod1 <- lme(RTL~Age + Homozygosity + Sex+ MeanAdultFood  + Homozygosity*MeanAdultFood + Homozygosity*Age,\r\n            random=~1|BirdID, data=Adults, na.action=na.omit)\r\nsummary(mod1)\r\nmod2<- lme(RTL~Age + Homozygosity + Sex+ MeanAdultFood + Homozygosity*MeanAdultFood ,\r\n           random=~1|BirdID, data=Adults, na.action=na.omit)\r\nsummary(mod2)\r\n\r\nmod3 <- lme(RTL~Age + Homozygosity + Sex+ MeanAdultFood ,\r\n            random=~1|BirdID, data=Adults, na.action=na.omit)\r\nsummary(mod3)\r\n\r\n#all variables in final model significant\r\n\r\n\r\nsummary(nullmodelA)\r\nsummary(mod3)\r\n\r\n\r\n#interactions added separately to get coefficients\r\nsummary(lme(RTL~Age + Homozygosity + Sex+ MeanAdultFood+Homozygosity*MeanAdultFood ,\r\n            random=~1|BirdID, data=Adults, na.action=na.omit))\r\n\r\n#get AICc of different models\r\nlibrary(AICcmodavg)\r\nAICc( nullmodelA)\r\nAICc(mod3)\r\n#and calculate delta AICc\r\nAICc(mod3)-AICc(nullmodelA)\r\n\r\n#calculate Rsquared for whole model and relationship between telomeres and homozygosity\r\nlibrary(MuMIn)\r\nr.squaredGLMM(mod3)\r\nr <- lm(RTL~Homozygosity, Adults)\r\nsummary(r)\r\n############################################################\r\n#2) parental heterozygosity\r\nAdults <- read.csv(""AdultsParentEffect.csv"")\r\nAdults <- subset(Adults, PropCovered>0.49)\r\nAdults$LayYearFood <- as.numeric(as.character(Adults$LayYearFood))\r\n\r\n\r\n#minimal model is the known predictors from the analysis of individual homozygosity above:\r\nnullmodelAp <- lme(RTL~Homozygosity+MeanAdultFood+Age+Sex, random=~1|BirdID,\r\n                   data=Adults, na.action=na.omit)\r\nsummary(nullmodelAp)\r\n\r\n#full model has the addition of maternal and paternal homozygosity plus interactions with food\r\n#in the lay year\r\n\r\nfullmodelAp <-lme(RTL ~ Homozygosity +MeanAdultFood + Age + Sex+MumHomo +DadHomo +LayYearFood\r\n                  + MumHomo*LayYearFood + DadHomo*LayYearFood,\r\n                  random=~1|BirdID,\r\n                  data=Adults, na.action=na.omit)\r\nsummary(fullmodelAp)\r\n\r\n#remove interactions\r\nmod1 <- lme(RTL ~ Homozygosity +MeanAdultFood + Age + LayYearFood+Sex+MumHomo +DadHomo \r\n            + DadHomo*LayYearFood,\r\n            random=~1|BirdID,\r\n            data=Adults, na.action=na.omit)\r\nsummary(mod1)\r\n\r\nmod2 <- lme(RTL ~ Homozygosity +MeanAdultFood + +LayYearFood +Age + Sex+MumHomo +DadHomo,\r\n            random=~1|BirdID,\r\n            data=Adults, na.action=na.omit)\r\nsummary(mod2)\r\nmod3 <- lme(RTL ~ Homozygosity +MeanAdultFood  +Age+ Sex + DadHomo+MumHomo,\r\n            random=~1|BirdID,\r\n            data=Adults, na.action=na.omit)\r\nsummary(mod3)\r\nmod4 <- lme(RTL ~ Age+Homozygosity +MeanAdultFood  +Sex +MumHomo,\r\n            random=~1|BirdID,\r\n            data=Adults, na.action=na.omit)\r\nsummary(mod4)\r\n\r\n#add in nonsignificant terms to report\r\nsummary(lme(RTL ~ Age+Homozygosity +MeanAdultFood  +Sex +MumHomo + DadHomo*LayYearFood,\r\n            random=~1|BirdID,\r\n            data=Adults, na.action=na.omit))\r\n\r\n#get Rsquared of final model and minimal\r\nr.squaredGLMM(mod4)\r\nr.squaredGLMM(nullmodelAp)\r\nAICc(mod4)-AICc(nullmodelAp)\r\n\r\nmumonlymodel <- lme(MeanTLKB ~ MeanAdultFood + Age +MumHz,\r\n                    random=~1|BirdID,\r\n                    data=Adults, na.action=na.omit)\r\nplot(MeanTLKB~MeanAdultFood, Adults)\r\nsummary(mumonlymodel)\r\nis.factor(Adults$MeanTLKB)\r\nAdults$MeanTLKB <- subset(Adults, MeanTLKB<11.71771)\r\nanova(nullmodelAp, finalmodelAp, test=T)\r\n\r\nAICc(nullmodelAp)\r\nAICc(finalmodelAp)\r\nAICc(fullmodelAp)\r\nAICc(mumonlymodel)\r\nfullmodelAp <-lme(MeanTLKB ~ Homozygosity +MeanAdultFood + Age + Sex+MumHomo +DadHomo \r\n                  + MumHomo*LayYearFood + DadHomo*LayYearFood,\r\n                  random=~1|BirdID,\r\n                  data=Adults, na.action=na.omit)\r\nfinalmodelAp <-lme(MeanTLKB ~ Homozygosity +MeanAdultFood + Age +MumHomo + Sex,\r\n                   random=~1|BirdID,\r\n                   data=Adults, na.action=na.omit)\r\nsummary(finalmodelAp)\r\nmoda <- lme(MeanTLKB ~ Homozygosity +MeanAdultFood + Age +MumHomo + Sex + LayYearFood,\r\n            random=~1|BirdID,\r\n            data=Adults, na.action=na.omit)\r\nsummary(moda)\r\n\r\n\r\n################################################################\r\n#juveniles\r\nrm(list=ls())\r\nJuvs <- read.csv(""Juvenile Hz.csv"")\r\n\r\n######################################################\r\n#1) individual heterozygosity\r\n#null model containing all known predictors\r\n\r\nnullmodelJ <-lm(RTL ~  Food+Sex,  data=Juvs)\r\nsummary(nullmodelJ)\r\n\r\n\r\nfullmodelJ <- lm(RTL ~ Food  + Homozygosity + Sex \r\n                 + Homozygosity*Food + Homozygosity*Sex , data=Juvs)\r\nsummary(fullmodelJ)\r\n\r\n\r\n\r\nmod1 <- lm(RTL ~ Food +  Homozygosity + Sex \r\n           + Homozygosity*Food  , data=Juvs)\r\nsummary(mod1)\r\n\r\n\r\nmod2 <- lm(RTL ~ Food  + Homozygosity \r\n           + Homozygosity*Food  , data=Juvs)\r\nsummary(mod2)\r\n\r\n#remove interaction to report main effects\r\nsummary(lm(RTL~Food+Homozygosity, Juvs))\r\n\r\n#add in nonsignificant terms to report\r\nsummary(lm(RTL~Homozygosity+Food+Homozygosity*Food+Homozygosity*Sex, Juvs))\r\n\r\n\r\n#get AICcs\r\nlibrary(AICcmodavg)\r\nAICc(nullmodelJ)\r\nAICc(mod2)\r\nAICc(mod2)-AICc(nullmodelJ)\r\n\r\n\r\n\r\n#######################################################\r\n#2) parental heterozygosity\r\n#use variables that are important in offspring TL\r\nrm(list=ls())\r\nJuvs <- read.csv(""JuvenilesParentEffect.csv"")\r\n\r\n####################\r\n#a) test effect of paternal hz of offspring hz\r\n#combine adults and juveniles\r\nhzcorr <- read.csv(""POff hz correlation.csv"")\r\n\r\nfullmodelJphz <-lm(Hz ~ MumHz + DadHz,data=hzcorr)\r\nhist(JuvsnoNA$MumHz)\r\nplot(fullmodelJphz)\r\nhist(fullmodelJphz$resid)\r\n\r\nsummary(fullmodelJphz)\r\nsummary(lm(MumHz~DadHz, hzcorr))\r\n#so only maternal heterozygosity has a positive effect on \r\n#offspring heterozygosity\r\n#what is the r2 of this relationship?\r\nsummary(lm(Hz~MumHz, JuvsnoNA))\r\n#0.07 - so should be able to include them in a model together\r\n\r\n\r\n#########################\r\n#parental effects on offspring TL\r\n#null model is significant predictors from invididual analysis (see above)\r\nnullmodelJp <- lm(RTL ~ Homozygosity+ Food + Homozygosity*Food,data=Juvs)\r\nsummary(nullmodelJp)\r\n\r\n#full model includes maternal and paternal homozygosity plus their interactions with food\r\nfullmodelJp <-lm(RTL ~ Homozygosity +Food  +MumHomo +  DadHomo \r\n                 + MumHomo*Food +DadHomo*Food + Homozygosity*Food,\r\n                 data=Juvs)\r\nsummary(fullmodelJp)\r\n\r\nmod1 <- lm(RTL ~ Homozygosity +Food  +MumHomo +  DadHomo \r\n           + MumHomo*Food  + Homozygosity*Food,\r\n           data=Juvs)\r\nsummary(mod1)\r\n\r\nmod2 <- lm(RTL ~ Homozygosity +Food  +MumHomo +  DadHomo \r\n           + Homozygosity*Food,\r\n           data=Juvs)\r\nsummary(mod2)\r\n\r\n\r\nmod3 <- lm(RTL ~ Homozygosity +Food   +  DadHomo \r\n           +Homozygosity*Food,\r\n           data=Juvs)\r\nsummary(mod3)\r\n\r\nmod4 <- lm(RTL ~ Homozygosity +Food  +Homozygosity*Food\r\n           ,\r\n           data=Juvs)\r\nsummary(mod4)\r\n\r\nmod4 <- lm(RTL ~ Food  +Homozygosity\r\n           ,\r\n           data=Juvs)\r\nsummary(mod4)\r\nhomo <- subset(Juvs, MumHomo>1)\r\nhetero <- subset(Juvs, MumHomo<1)\r\nsummary(lm(RTL~Food, homo))\r\nsummary(lm(RTL~Food, hetero))\r\n#add in nonsignificant terms\r\nsummary(lm(RTL~Homozygosity+Food+Homozygosity*Food+Food*DadHomo, Juvs))\r\n\r\n#AICc of null and parental models\r\nlibrary(AICcmodavg)\r\nAICc(nullmodelJp)\r\nAICc(mod2)\r\nAICc(nullmodelJp)-AICc(mod2)\r\n\r\n']","Data from: Telomere length reveals cumulative individual and transgenerational inbreeding effects in a passerine bird Inbreeding results in more homozygous offspring that should suffer reduced fitness, but it can be difficult to quantify these costs for several reasons. First, inbreeding depression may vary with ecological or physiological stress and only be detectable over long time periods. Second, parental homozygosity may indirectly affect offspring fitness, thus confounding analyses that consider offspring homozygosity alone. Finally, measurement of inbreeding coefficients, survival and reproductive success may often be too crude to detect inbreeding costs in wild populations. Telomere length provides a more precise measure of somatic costs, predicts survival in many species and should reflect differences in somatic condition that result from varying ability to cope with environmental stressors. We studied relative telomere length in a wild population of Seychelles warblers (Acrocephalus sechellensis) to assess the lifelong relationship between individual homozygosity, which reflects genome-wide inbreeding in this species, and telomere length. In juveniles, individual homozygosity was negatively associated with telomere length in poor seasons. In adults, individual homozygosity was consistently negatively related to telomere length, suggesting the accumulation of inbreeding depression during life. Maternal homozygosity also negatively predicted offspring telomere length. Our results show that somatic inbreeding costs are environmentally dependent at certain life stages but may accumulate throughout life.",3
MIA-Sig: multiplex chromatin interaction analysis by signal processing and statistical algorithms,"The single-molecule multiplex chromatin interaction data are generated by emerging 3D genome mapping technologies such as GAM, SPRITE, and ChIA-Drop. These datasets provide insights into high-dimensional chromatin organization, yet introduce new computational challenges. Thus, we developed MIA-Sig, an algorithmic solution based on signal processing and information theory. We demonstrate its ability to de-noise the multiplex data, assess the statistical significance of chromatin complexes, and identify topological domains and frequent inter-domain contacts. On chromatin immunoprecipitation (ChIP)-enriched data, MIA-Sig can clearly distinguish the protein-associated interactions from the non-specific topological domains. Together, MIA-Sig represents a novel algorithmic framework for multiplex chromatin interaction analysis.","['args = commandArgs(trailingOnly = TRUE)\n\ndir = toString(args[1])\nfile_in = toString(args[2])\nfile_out = toString(args[3])\n\nsetwd(dir)\n\nsaveRDS(read.table(file_in), file_out)\n\n']","MIA-Sig: multiplex chromatin interaction analysis by signal processing and statistical algorithms The single-molecule multiplex chromatin interaction data are generated by emerging 3D genome mapping technologies such as GAM, SPRITE, and ChIA-Drop. These datasets provide insights into high-dimensional chromatin organization, yet introduce new computational challenges. Thus, we developed MIA-Sig, an algorithmic solution based on signal processing and information theory. We demonstrate its ability to de-noise the multiplex data, assess the statistical significance of chromatin complexes, and identify topological domains and frequent inter-domain contacts. On chromatin immunoprecipitation (ChIP)-enriched data, MIA-Sig can clearly distinguish the protein-associated interactions from the non-specific topological domains. Together, MIA-Sig represents a novel algorithmic framework for multiplex chromatin interaction analysis.",3
Unexpected microbial metabolic responses to elevated temperatures and nitrogen addition in subarctic soils under different land-use,"This repository contains all necessary raw data as well as the R code used to conduct statistical analysis and create figures of the publication Unexpected microbial metabolic responses to elevated temperatures and nitrogen addition in subarctic soils under different land-useJulia Schroeder1, Tino Peplau1, Edward Gregorich2, Christoph C. Tebbe3, Christopher Poeplau11 Thnen Institute of Climate-Smart Agriculture, Bundesallee 68, 38116 Braunschweig, Germany2 Research and Development Centre, Central Experimental Farm, Agriculture and Agri-Food Canada, Ottawa, Canada3 Thnen Institute of Biodiversity, Bundesallee 65, 38116 Braunschweig, GermanyDOI:This study investigated how subarctic soils under different land use will respond to warming and increasing N availability to allow for better predictions of C cycling under global change. The short-term temperature sensitivity as well as N-input effects on microbial CUE, respiration, growth and turnover were assessed in a one-day incubation experiment according to the 18O-CUE approach. The warming and N response of SOM decomposition were assessed in a 50-days incubation experiment via measurement of cumulative respiration. Both experiments were conducted with the following three treatments: incubation at 10 C, incubation at 20 C, and incubation at 20 C plus N-fertiliser addition at an amendment rate of 100 kg N ha-1. The response to warming or N addition were expressed as response ratios RRT = 20C/10C and RRN = 20C+N/20C for warming and N response, respectively.The R code was developed under R v3.6.3 and adapted to work under version R v.4.1.2.The repository includes the following files:general_soil_parameters_per_sample.csv - general soil data for each field sample (n=27)general_soil_parameters_per_plot.csv - general soil data assessed on pooled replicated field samples (n=9)respiration_over_50d_incubation.csv - respiration rate and cumulative respiration for each time-point and laboratory sample over the 50-days incubationsample_data.csv - data measured for each laboratory sample (n=81) Warming_and_nitrogen_response_of_CUE_in_subarctic_soils.Rproj - Rproject (load project to work on provided scripts and data)load_data_script.R - loads required dataabsolute_values_script.R - summary of absolute ranges of parameters per land-use type and siteabsolute_linear_mixed_effects_model_script.R - run statistical analysiscorrelograms_absolute_soil_params_script.R - correlation analysis to identify what drives absolute valuesplot_correlations_absolute_soil_params_script.R - plot drivers of CUE and cumulative respirationRRT_RRN_calculation_script.R - calculates response ratiosplot_RRT_RRN_script.R - plot response ratiosRRT_RRN_linear_mixed_effects_models_script.R - run statistical analysiscorrelograms_RRT_RRN_soil_param_script.R - correlation analysis to identify drivers of response ratiosplot_correlations_RRT_RRN_soil_params_script.R - plot drivers of response ratiosRRT_RRN_resprate_cumulresp_over_time_50d_incubation_script.R - plot response ratios over time course",,"Unexpected microbial metabolic responses to elevated temperatures and nitrogen addition in subarctic soils under different land-use This repository contains all necessary raw data as well as the R code used to conduct statistical analysis and create figures of the publication Unexpected microbial metabolic responses to elevated temperatures and nitrogen addition in subarctic soils under different land-useJulia Schroeder1, Tino Peplau1, Edward Gregorich2, Christoph C. Tebbe3, Christopher Poeplau11 Thnen Institute of Climate-Smart Agriculture, Bundesallee 68, 38116 Braunschweig, Germany2 Research and Development Centre, Central Experimental Farm, Agriculture and Agri-Food Canada, Ottawa, Canada3 Thnen Institute of Biodiversity, Bundesallee 65, 38116 Braunschweig, GermanyDOI:This study investigated how subarctic soils under different land use will respond to warming and increasing N availability to allow for better predictions of C cycling under global change. The short-term temperature sensitivity as well as N-input effects on microbial CUE, respiration, growth and turnover were assessed in a one-day incubation experiment according to the 18O-CUE approach. The warming and N response of SOM decomposition were assessed in a 50-days incubation experiment via measurement of cumulative respiration. Both experiments were conducted with the following three treatments: incubation at 10 C, incubation at 20 C, and incubation at 20 C plus N-fertiliser addition at an amendment rate of 100 kg N ha-1. The response to warming or N addition were expressed as response ratios RRT = 20C/10C and RRN = 20C+N/20C for warming and N response, respectively.The R code was developed under R v3.6.3 and adapted to work under version R v.4.1.2.The repository includes the following files:general_soil_parameters_per_sample.csv - general soil data for each field sample (n=27)general_soil_parameters_per_plot.csv - general soil data assessed on pooled replicated field samples (n=9)respiration_over_50d_incubation.csv - respiration rate and cumulative respiration for each time-point and laboratory sample over the 50-days incubationsample_data.csv - data measured for each laboratory sample (n=81) Warming_and_nitrogen_response_of_CUE_in_subarctic_soils.Rproj - Rproject (load project to work on provided scripts and data)load_data_script.R - loads required dataabsolute_values_script.R - summary of absolute ranges of parameters per land-use type and siteabsolute_linear_mixed_effects_model_script.R - run statistical analysiscorrelograms_absolute_soil_params_script.R - correlation analysis to identify what drives absolute valuesplot_correlations_absolute_soil_params_script.R - plot drivers of CUE and cumulative respirationRRT_RRN_calculation_script.R - calculates response ratiosplot_RRT_RRN_script.R - plot response ratiosRRT_RRN_linear_mixed_effects_models_script.R - run statistical analysiscorrelograms_RRT_RRN_soil_param_script.R - correlation analysis to identify drivers of response ratiosplot_correlations_RRT_RRN_soil_params_script.R - plot drivers of response ratiosRRT_RRN_resprate_cumulresp_over_time_50d_incubation_script.R - plot response ratios over time course",3
Data from: Known mutator alleles do not markedly increase mutation rate in clinical Saccharomyces cerevisiae strains,"Natural selection has the potential to act on all phenotypes, including genomic mutation rate. Classic evolutionary theory predicts that in asexual populations, mutator alleles, which cause high mutation rates, can fix due to linkage with beneficial mutations. This phenomenon has been demonstrated experimentally and may explain the frequency of mutators found in bacterial pathogens. By contrast, in sexual populations, recombination decouples mutator alleles from beneficial mutations, preventing mutator fixation. In the facultatively sexual yeast Saccharomyces cerevisiae, segregating alleles of MLH1 and PMS1 have been shown to be incompatible, causing a high mutation rate when combined. These alleles had never been found together naturally, but were recently discovered in a cluster of clinical isolates. Here we report that the incompatible mutator allele combination only marginally elevates mutation rate in these clinical strains. Genomic and phylogenetic analyses provide no evidence of a historically elevated mutation rate. We conclude that the effect of the mutator alleles is dampened by background genetic modifiers. Thus, the relationship between mutation rate and microbial pathogenicity may be more complex than once thought. Our findings provide rare observational evidence that supports evolutionary theory suggesting that sexual organisms are unlikely to harbour alleles that increase their genomic mutation rate.","['library(dplyr)\nlibrary(ggplot2)\nlibrary(cowplot)\n\n# load the data and get it into a form ready for plotting:\ndat <- read.table(""mutators_supp_data.txt"", header=TRUE, sep=""\\t"", stringsAsFactors=FALSE)\ndat <- group_by(dat, assay) %>% mutate(rankOf=rank(ML)) %>% ungroup()\nmedians <- group_by(dat, strainBackground) %>% summarize(medianMutRate=median(log10(ML)))\ndat$strainF <- factor(dat$strainBackground, levels=medians$strainBackground[order(medians$medianMutRate)])\n# strains that are clinical but do not carry both incompatible mutator alleles\nbgStrain <- c(""YJM320"", ""YJM451"", ""YJM681"", ""YJM682"", ""YJM1083"", ""YJM1199"")\ndat$baseline_strain <- dat$strainBackground %in% bgStrain\nbottom.strains <- levels(dat$strainF)\n\n# Order strains by MEAN RELATIVE rate rather than MEDIAN ABSOLUTE rate.\nrel <- group_by(dat, assay) %>% \n\tmutate(baseline_mean=mean(ML[baseline_strain]), relrate=ML/baseline_mean)\nmeans <- group_by(rel, strainBackground) %>% summarize(meanRelRate=mean(relrate))\nrel$strainF <- factor(rel$strainBackground, levels=means$strainBackground[order(means$meanRelRate)])\ntopleft.dat <- droplevels(filter(rel, type != ""engineered mutator""))  # droplevels to remove unused factor levels\ntopright.dat <- droplevels(filter(rel, type != ""wild-type""))\ntop.left.strains <- levels(topleft.dat$strainF)\ntop.right.strains <- levels(topright.dat$strainF)\n\n# make the plots\ncolors <- c(""engineered mutator""=""magenta"", ""natural mutator""=""red"", ""wild-type""=""black"")\nshapes <- c(""engineered mutator""=4, ""natural mutator""=0, ""wild-type""=8)\nshapes <- c(""engineered mutator""=8, ""natural mutator""=17, ""wild-type""=16)\nalpha <- 0.5\nstroke <- 1.1\nptsz <- 2\n\t\ntopleft <- ggplot(topleft.dat, aes(x=strainF, y=relrate, color=type)) +\n\tgeom_point(aes(shape=type), size=ptsz, alpha=alpha, position=position_jitter(width=0.15), show.legend=FALSE) +\n\txlab("""") + ylab(""relative mutation rate"") +\n\tstat_summary(aes(color=type), fun.y=\'mean\', geom=\'point\', shape=23, size=3.6, show.legend=FALSE) +\n\tstat_summary(aes(fill=type), fun.y=\'mean\', geom=\'point\', shape=23, size=3.6, alpha=alpha, show.legend=FALSE) +\n\tscale_color_manual(values=colors) +\n\tscale_fill_manual(values=colors) +\n\tscale_shape_manual(values=shapes) +\n\ttheme(axis.text.x=element_text(angle = 90, hjust = 1, size=12),\n\t\taxis.text.y=element_text(size=12), axis.title.y=element_text(size=14, face=""bold"", hjust=1),\n\t\tplot.margin=grid::unit(c(0.1, 0.1, 0.2, 0.4), \'inches\')) +\n\tbackground_grid(major = \'xy\', minor=\'none\') +\n\tscale_x_discrete(labels=parse(text=top.left.strains))\n\ntopright <- ggplot(topright.dat, aes(x=strainF, y=relrate, color=type)) +\n\tgeom_point(aes(shape=type), size=ptsz, alpha=alpha, position=position_jitter(width=0.15), show.legend=FALSE) +\n\txlab("""") + ylab(""relative mutation rate"") +\n\tstat_summary(aes(color=type), fun.y=\'mean\', geom=\'point\', shape=23, size=3.6, show.legend=FALSE) +\n\tstat_summary(aes(fill=type), fun.y=\'mean\', geom=\'point\', shape=23, size=3.6, alpha=alpha, show.legend=FALSE) +\n\tscale_color_manual(values=colors) +\n\tscale_fill_manual(values=colors) +\n\tscale_shape_manual(values=shapes) +\n\ttheme(axis.text.x=element_text(angle = 90, hjust = 1, size=12),\n\t\taxis.text.y=element_text(size=12), axis.title.y=element_text(size=14, face=""bold"", hjust=1),\n\t\tplot.margin=grid::unit(c(0.1, 0.1, 0.2, 0.2), \'inches\')) +\n\tbackground_grid(major = \'xy\', minor=\'none\') +\n\tscale_x_discrete(labels=parse(text=top.right.strains))\n\nbottom <- ggplot(dat, aes(x=strainF, y=log10(ML), color=type)) +\n\tgeom_boxplot(mapping=aes(fill=type), outlier.colour = NA, coef=0, alpha=alpha, width=0.25, show.legend=FALSE) +\n\tgeom_point(aes(shape=type), position=position_jitter(width=0.1), size=ptsz, alpha=alpha) +\n\txlab(""strain"") + ylab(""log10(mutation rate)"") +\n\tscale_color_manual(values=colors) +\n\tscale_fill_manual(values=colors) +\n\tscale_shape_manual(values=shapes) +\n\tylim(-10.2, -6.8) + \n\ttheme(axis.text.x = element_text(angle = 90, hjust = 1, size=12),\n\t\taxis.text.y = element_text(size=12), legend.position=\'right\',\n\t\tlegend.margin=margin(0.1, 0.1, 0.1, 0.1, \'inches\'), legend.key.width=unit(0.1, \'inches\'),\n\t\tlegend.text=element_text(size=14), legend.title=element_blank(),\n\t\tlegend.key=element_rect(size=5), legend.key.size=unit(0.35, \'inches\'),\n\t\tplot.margin=grid::unit(c(0.1, 0.1, 0.1, 0.4), \'inches\'),\n\t\taxis.title.x=element_text(size=14, face=""bold""),\n\t\taxis.title.y=element_text(size=14, face=""bold"", hjust=1)) +\n\tguides(color=guide_legend(override.aes=list(size=4.5, alpha=1))) +\n\tbackground_grid(major = \'xy\', minor=\'none\') +\n\tscale_x_discrete(labels=parse(text=bottom.strains))\n\nwid <- 0.5 + 0.3 + length(top.left.strains)/2 + length(top.right.strains)/2\nhei <- 6\nleft.wid <- 0.5 + length(top.left.strains)/2\nright.wid <- 0.3 + length(top.right.strains)/2\nbottom.wid <- 0.5 + length(bottom.strains)/2 + 2\nbottom.xpos <- 0.5*(wid - bottom.wid)/wid\ng7 <- ggdraw() +\n\tdraw_plot(topleft, 0, 0.5, left.wid/wid, 0.5) +\n\tdraw_plot(topright, left.wid/wid, 0.5, right.wid/wid, 0.5) +\n\tdraw_plot(bottom, bottom.xpos, 0, bottom.wid/wid, 0.5) +\n\tdraw_plot_label(c(""A"", ""B""), c(0, 0), c(1, 0.5), size=16)\nsave_plot(""Figure2.png"", g7, base_width=wid, base_height=hei)\n', '# requires the lme4 package:\nlibrary(lme4)\n\ndat <- read.table(""mutators_supp_data.txt"", header=TRUE, sep=""\\t"", stringsAsFactors=FALSE)\ndat$mutator_relatives <- dat$strainBackground %in% c(""YJM320"", ""YJM681"")  # closest relatives to natural mutators\ndat$log10ML <- log10(dat$ML)\ndat$engineered_mutator <- dat$type == ""engineered mutator""\ndat$natural_mutator <- dat$type == ""natural mutator""\ndat$mutator <- dat$engineered_mutator | dat$natural_mutator\n\n# Test for the significance of the assay effect. \nnullmod <- lmer(log10ML ~ mutator + (1 | strainBackground),               data=dat, REML=FALSE)\nfullmod <- lmer(log10ML ~ mutator + (1 | strainBackground) + (1 | assay), data=dat, REML=FALSE)\nanova(nullmod, fullmod)\n\n# Test whether the engineered mutators are significantly higher than all other strains\nnullmod <- lmer(log10ML ~ \t\t\t\t\t   (1 | assay), data=dat, REML=FALSE)\nfullmod <- lmer(log10ML ~ engineered_mutator + (1 | assay), data=dat, REML=FALSE)\nanova(nullmod, fullmod)\n# estimate of engineered mutators effect:\n10^coef(summary(fullmod))[, ""Estimate""][""engineered_mutatorTRUE""]\n\n# test if the natural mutators are significantly higher than non-mutators\nnullmod <- lmer(log10ML ~ engineered_mutator + \t\t\t\t     (1 | assay), data=dat, REML=FALSE)\nfullmod <- lmer(log10ML ~ engineered_mutator + natural_mutator + (1 | assay), data=dat, REML=FALSE)\nanova(nullmod, fullmod)\n# estimate of natural mutators effect:\n10^coef(summary(fullmod))[, ""Estimate""][""natural_mutatorTRUE""]\n\n# test for a significant difference between natural mutators and their closest relatives\nnullmod <- lmer(log10ML ~ \t\t    (1 | assay), data=filter(dat, natural_mutator | mutator_relatives), REML=FALSE)\nfullmod <- lmer(log10ML ~ mutator + (1 | assay), data=filter(dat, natural_mutator | mutator_relatives), REML=FALSE)\nanova(nullmod, fullmod)\n10^coef(summary(fullmod))[, ""Estimate""][""mutatorTRUE""]\n']","Data from: Known mutator alleles do not markedly increase mutation rate in clinical Saccharomyces cerevisiae strains Natural selection has the potential to act on all phenotypes, including genomic mutation rate. Classic evolutionary theory predicts that in asexual populations, mutator alleles, which cause high mutation rates, can fix due to linkage with beneficial mutations. This phenomenon has been demonstrated experimentally and may explain the frequency of mutators found in bacterial pathogens. By contrast, in sexual populations, recombination decouples mutator alleles from beneficial mutations, preventing mutator fixation. In the facultatively sexual yeast Saccharomyces cerevisiae, segregating alleles of MLH1 and PMS1 have been shown to be incompatible, causing a high mutation rate when combined. These alleles had never been found together naturally, but were recently discovered in a cluster of clinical isolates. Here we report that the incompatible mutator allele combination only marginally elevates mutation rate in these clinical strains. Genomic and phylogenetic analyses provide no evidence of a historically elevated mutation rate. We conclude that the effect of the mutator alleles is dampened by background genetic modifiers. Thus, the relationship between mutation rate and microbial pathogenicity may be more complex than once thought. Our findings provide rare observational evidence that supports evolutionary theory suggesting that sexual organisms are unlikely to harbour alleles that increase their genomic mutation rate.",3
Co.Temp - Comparing series of Temperatures,"This program aims to offer useful information on temperature series (i.e.: identify the biases in the daily maximum and minimum temperature, compare manual and automatic weather stations, etc.) and works in three steps: statistical analysis and characterization of the daily series of maximum (Tx) and minimum temperature (Tn), computation of monthly-aggregated data for both Tx and Tn and comparison between temperature classes of events (like heat wave, cold wave and normal events). For details and the latest version, see https://github.com/UniToDSTGruppoClima/CoTemp","['###############################################################################\r\n############################################################################### \r\n#######################         Co.Temp         ###############################\r\n################### Comparing series of Temperatures ##########################\r\n###############################################################################\r\n###############################################################################\r\n###                                                                         ###\r\n### Copyright (C) 2018                                                      ###\r\n### Fiorella Acquaotta, Diego Guenzi, Diego Garzena and Simona Fratianni    ###\r\n###                                                                         ###\r\n### This program is free software: you can redistribute it and/or modify    ###\r\n### it under the terms of the GNU General Public License as published by    ###\r\n### the Free Software Foundation, either version 3 of the License, or       ###\r\n### (at your option) any later version.                                     ###\r\n###                                                                         ###\r\n### This program is distributed in the hope that it will be useful,         ###\r\n### but WITHOUT ANY WARRANTY; without even the implied warranty of          ###\r\n### MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the           ###\r\n### GNU General Public License for more details.                            ###\r\n###                                                                         ###\r\n### You should have received a copy of the GNU General Public License       ###\r\n### along with this program.  If not, see <http://www.gnu.org/licenses/>    ###\r\n###                                                                         ###\r\n###############################################################################\r\n###############################################################################\r\n\r\n\r\n###############################  Co.Temp  #####################################\r\n# Description of the software\r\n#\r\n# This program aims to offer useful information on temperature series (i.e.:\r\n# identify the biases in the daily maximum and minimum temperature, compare manual\r\n# and automatic weather stations, etc.) and works in three steps: statistical\r\n# analysis and characterization of the daily series of maximum (Tx) and minimum\r\n# temperature (Tn), computation of monthly-aggregated data for both Tx and Tn and\r\n# comparison between temperature classes of events (like heat wave, cold wave and\r\n# normal events).\r\n#\r\n# 1] Statistical analysis\r\n# In this first step, for each raw series (in example, a set of two Tx series and\r\n# another set of two Tn series that came from different instruments), a statistical\r\n# analysis is carried out. Mean, median, first and third quartiles, minimum and\r\n# maximum values are calculated and missing values are identified. A time series\r\n# plot and a density plot are created for each pair of series. Always for each pair\r\n# of series, the t test, the Kolmogorov-Smirnov test and the Wilcoxon\'s Rank Sum\r\n# test are carried out. Furthermore, the Root Mean Squared Error (RMSE), the Mean\r\n# Error (ME), and the correlation coefficient by Spearman\'s method are calculated\r\n# for every pair of series.\r\n# 2] Computation of monthly data\r\n# In the second step, any values that are missing in one series are also set to be\r\n# missing (as NA) in its counterpart. For each pair of series, the daily difference\r\n# is calculated in addition to the monthly difference series and the monthly\r\n# Percentage Relative Error (PRE). A values of PRE>0 shows an overestimation of the\r\n# second monthly Tx|Tn series (called candidate) over the first one (reference),\r\n# while PRE<0 highlights an underestimation of the candidate over the reference\r\n# series. On the monthly PRE series is also computed the trend whith its slope, to\r\n# identify if the shift between the pair of series increases, decreases or is\r\n# constant. \r\n# 3] Classification of events\r\n# In the third step, daily data is divided in 5 classes, that are extremely cold\r\n# events, cold events, mean events, warm events and extremely warm events, using\r\n# percentiles. The percentiles are estimated by combining the daily series pair\r\n# (candidate and reference). \r\n#\r\n# When you start the program, (if interactive mode is available) you will be \r\n# asked for an input text file and a folder where results will be stored. If\r\n# you are running in batch mode, please modify the code to use the correct \r\n# input file and create the right output folder.\r\n#\r\n# INPUT\r\n# The text file has to be formatted in five TAB-separated columns. The first\r\n# row of the file has to contain the headers (column names) while the first\r\n# column has to contain the dates (in DD/MM/YYYY format). Column two is the\r\n# Tx reference serie and column three is the Tn reference serie. Column four\r\n# and five should contain Tx and Tn candidate series. Missing values must be\r\n# marked as NA. It is very important to start the file from the first of\r\n# January (of any year) and end it at the 31st of December (of any year). See\r\n# the attached file called example_.txt (where _ is a number).\r\n#\r\n# OUTPUT\r\n# 01_TN_statistics_raw.csv - Main statistics on input Tn serie\r\n# 01_TN_summary_raw.csv - Summary of input Tn series and their difference\r\n# 01_TX_statistics_raw.csv - Main statistics on input Tx serie\r\n# 01_TX_summary_raw.csv - Summary of input Tx series and their difference\r\n# 01_TN_plot_raw.pdf - Plots of input Tn series\r\n# 01_TX_plot_raw.pdf - Plots of input Tx series\r\n# 02_TN_statistics_boxplot_monthly_diff.csv - Monthly differences of candidate and reference Tn\r\n# 02_TN_trend_relative error.csv - Informations on trend of Tn\r\n# 02_TN_boxplot_monthly_diff.pdf - Boxplot of monthly series, differences and PRE of Tn\r\n# 02_TN_plot_diff.pdf - Plot and distribution of difference in Tn series\r\n# 02_TN_relative_error.pdf - Plots of Tn PRE\r\n# 02_TX_statistics_boxplot_monthly_diff.csv - Monthly differences of candidate and reference Tx\r\n# 02_TX_trend_relative error.csv - Informations on trend of Tx\r\n# 02_TX_boxplot_monthly_diff.pdf - Boxplot of monthly series, differences and PRE of Tx\r\n# 02_TX_plot_diff.pdf - Plot and distribution of difference in Tx series\r\n# 02_TX_relative_error.pdf - Plots of Tx PRE\r\n# 03_TN_classes_candidate.csv - Statistics on the classes of candidate Tn\r\n# 03_TN_classes_reference.csv - Statistics on the classes of reference Tn\r\n# 03_TN_cold.csv - Statistics on Tn events classified as cold\r\n# 03_TN_extr_cold.csv - Statistics on Tn events classified as extremely cold\r\n# 03_TN_extr_warm.csv - Statistics on Tn events classified as extremely warm\r\n# 03_TN_frequencies_in_classes.csv - Frequencies of Tn events in every class\r\n# 03_TN_mean.csv - Statistics on Tn events classified as mean\r\n# 03_TN_warm.csv - Statistics on Tn events classified as warm\r\n# 03_TN_boxplot_diff.pdf - Boxplot of each class for Tn events\r\n# 03_TX_classes_candidate.csv - Statistics on the classes of candidate Tx\r\n# 03_TX_classes_reference.csv - Statistics on the classes of reference Tx\r\n# 03_TX_cold.csv - Statistics on Tx events classified as cold\r\n# 03_TX_extr_cold.csv - Statistics on Tx events classified as extremely cold\r\n# 03_TX_extr_warm.csv - Statistics on Tx events classified as extremely warm\r\n# 03_TX_frequencies_in_classes.csv - Frequencies of Tn events in every class\r\n# 03_TX_mean.csv - Statistics on Tx events classified as mean\r\n# 03_TX_warm.csv - Statistics on Tx events classified as warm\r\n# 03_TX_boxplot_diff.pdf - Boxplot of each class for Tx events\r\n# TN_Results.xlsx - Collection of previous results for Tn in a single Excel file\r\n# TX_Results.xlsx - Collection of previous results for Tx in a single Excel file\r\n#\r\n# Following R-packages have to be installed:\r\n#    ""MASS"", ""timeDate"", ""timeSeries"", ""fBasics"", ""zoo"", ""hydroGOF"", ""xts"",\r\n#    ""hydroTSM"", ""zyp"", ""xlsx""\r\n#\r\n# This code has been written under R-Version 3.3.0 and 3.4.3; for older or\r\n# newer versions problems might occur.\r\n#\r\n###############################################################################\r\n# Versions:\r\n# \r\n# v1.0 - 20180319: First public release after code cleaning and review\r\n# \r\n###############################################################################\r\n\r\n\r\n###############################################################################\r\n####                         INITIALIZATION                                ####\r\n###############################################################################\r\n\r\nstart.time = Sys.time()\r\nlibrary(MASS)\r\nlibrary(timeDate)\r\nlibrary(timeSeries)\r\nlibrary(fBasics)\r\nlibrary(zoo)\r\nlibrary(hydroGOF)\r\nlibrary(xts)\r\nlibrary(hydroTSM)\r\nlibrary(zyp)\r\nlibrary(xlsx)\r\n\r\n###############################################################################\r\n####                       FUNCTION DEFINITION                             ####\r\n###############################################################################\r\n\r\nclassification <- function(serie) {\r\n  ##########################################################\r\n  # Returns summary and length of the serie \r\n  #\r\n  # INPUT\r\n  # serie: a single temperature serie to analyze\r\n  #\r\n  # OUTPUT\r\n  # Summary and lenght of the serie\r\n  ##########################################################\r\n  \r\n  sum_serie = summary(serie[,4])\r\n  len_serie = length(serie[,4])\r\n  info_serie = c(sum_serie,len_serie)\r\n  \r\n  return(info_serie)\r\n} # end of function classification\r\n\r\n\r\nstatistics <- function(serie1, serie2) {\r\n  ##########################################################\r\n  # Compute RMSE, ME, cor and summary on common class events on both series\r\n  #\r\n  # INPUT\r\n  # serie1: a single candidate temperature serie to analyze\r\n  # serie2: a single reference temperature serie to analyze\r\n  #\r\n  # OUTPUT\r\n  # Summary, lenght, RMSE, ME and cor of the series\r\n  ##########################################################\r\n  \r\n  rmse_common = rmse(serie1, serie2)\r\n  me_common = me(serie1, serie2)\r\n  cor_common = cor(serie1, serie2)\r\n  sum_CAN = summary(serie1)\r\n  sum_REF = summary(serie2)\r\n  l_common = length(serie1)\r\n  info_common = cbind(sum_CAN, sum_REF, l_common, rmse_common, me_common, cor_common)\r\n  \r\n  return(info_common)\r\n} # end of function statistics\r\n\r\n\r\nanalyze <- function(name, Tserie) {\r\n  ##########################################################\r\n  # This function is the core of Co.Temp and does all the necessary\r\n  # computations to produce the output previously described in the general\r\n  # program description\r\n  #\r\n  # INPUT\r\n  # name: the name of the serie going to be analyzed\r\n  # Tserie: a single temperature serie to be analyzed\r\n  #\r\n  # OUTPUT\r\n  # No output, except 1 xlsx and 12 csv files produces as side effects\r\n  ##########################################################\r\n  \r\n  # Single series and differences\r\n  can_serie = Tserie[,1:4] \r\n  ref_serie = Tserie[,c(-4,-6,-7)]\r\n  dif_day = (can_serie[,4] - ref_serie[,4])\r\n  Tserie = cbind(Tserie, dif_day)\r\n  \r\n  # Remove NAs\r\n  Tserie1_na = cbind(can_serie, ref_serie[,4], dif_day)\r\n  Tserie1_na = na.omit(Tserie1_na)\r\n  Tserie_na = na.omit(Tserie)\r\n  can_na = Tserie_na[,4]\r\n  ref_na = Tserie_na[,5]\r\n  \r\n  # Create summary for original series, series without NAs and difference serie\r\n  sum_can = summary(can_serie[,4])\r\n  sum_ref = summary(ref_serie[,4])\r\n  sum_can_na = summary(Tserie1_na[,4])\r\n  sum_can_na[7] = 0\r\n  sum_ref_na = summary(Tserie1_na[,5])\r\n  sum_ref_na[7] = 0\r\n  sum_dif_day = summary(dif_day)\r\n  summ = c(""Min."",""1st Qu."",""Median"",""Mean"",""3rd Qu."",""Max."",""NA\'s"")\r\n  stat = suppressWarnings(cbind(summ, sum_can, sum_can_na, sum_ref, sum_ref_na, sum_dif_day))\r\n  stat = as.data.frame(stat[,-1])\r\n  for (i in c(1:5)) {\r\n    stat[,i] = as.numeric(as.character(stat[,i]))\r\n  }\r\n  write.csv(stat, file=paste(""01_"",name,""_summary_raw.csv"",sep=""""), row.names=T)\r\n  \r\n  # Compute Zooreg\r\n  raw_can = zooreg(can_serie[,4], start=as.Date(start.tab))\r\n  raw_ref = zooreg(ref_serie[,4], start=as.Date(start.tab))\r\n  raw_diff = zooreg(dif_day, start=as.Date(start.tab))\r\n  \r\n  # Save PDF plot\r\n  pdf(paste(""01_"",name,""_plot_raw.pdf"",sep=""""))\r\n  par(mfcol=c(2,2))\r\n  plot(raw_can, ylab=""Temperature (C)"", xlab=""Years"", \r\n       main=paste(name,""candidate station""), col=""blue"")\r\n  plot(raw_ref, ylab=""Temperature (C)"", xlab=""Years"", \r\n       main=paste(name,""reference station""), col=""red"")\r\n  plot(raw_can, ylab=""Temperature (C)"", xlab=""Years"", \r\n       main=""Candidate (blue) and reference (red)"", col=""blue"")\r\n  lines(raw_ref, col=""red"")\r\n  plot(density(ref_serie[,4],na=T), main=""Candidate (blue) and reference (red)"", col=""red"")\r\n  lines(density(can_serie[,4],na=T), col=""blue"")\r\n  dev.off()\r\n  \r\n  # Compute statistics: RMSE, ME, T, KS, Wilcox and Cor between the two series\r\n  rmse_er = round(rmse(ref_serie[,4],can_serie[,4],na.rm=T), digits=2)\r\n  mean_error = me(ref_serie[,4], can_serie[,4])\r\n  test_t = t.test(ref_serie[,4], can_serie[,4])\r\n  test_ks = suppressWarnings(ks.test(ref_serie[,4], can_serie[,4]))\r\n  test_wil = wilcox.test(ref_serie[,4], can_serie[,4])\r\n  test_cor = cor.test(ref_serie[,4], can_serie[,4], method=""spearman"", exact=FALSE)\r\n  test_names = c(""RMSE"", ""Mean error"", ""T-Test p-value"", ""Kolmogorov-Smirnov p-value"",\r\n                 ""Wilcoxon p-value"", ""Spearman rho"", ""Spearman p-value"")\r\n  test_res = c(rmse_er, mean_error, test_t$p.value, test_ks$p.value, test_wil$p.value, \r\n               as.numeric(test_cor$estimate), test_cor$p.value)\r\n  test_res = round(test_res, digits=2)\r\n  test_fin = cbind(test_names, test_res)\r\n  colnames(test_fin) = c(""Test name"", ""Test result"")\r\n  test_fin = as.data.frame(test_fin)\r\n  test_fin[,2] = as.numeric(as.character(test_fin[,2]))\r\n  write.csv(test_fin, file=paste(""01_"",name,""_statistics_raw.csv"",sep=""""), row.names=F)\r\n  \r\n  # Compute monthly data\r\n  dif_m = apply.monthly(raw_diff, mean, na.rm=T)\r\n  Tserie_can_m = apply.monthly(raw_can, mean, na.rm=T)\r\n  Tserie_ref_m = apply.monthly(raw_ref, mean, na.rm=T)\r\n  dif_mm = matrix(dif_m, ncol=12, byrow=T)\r\n  sum_dif_mon = summary(dif_mm)\r\n  Tserie_can_mm = matrix(Tserie_can_m, ncol=12, byrow=T)\r\n  Tserie_ref_mm = matrix(Tserie_ref_m, ncol=12, byrow=T)\r\n  \r\n  # Monthly percentage relative error\r\n  err = Tserie_can_mm - Tserie_ref_mm\r\n  err = err / Tserie_ref_mm\r\n  err_month = round(err*100, digits=2)\r\n  \r\n  # Save Monthly boxplots PDF\r\n  pdf(paste(""02_"",name,""_boxplot_monthly_diff.pdf"",sep=""""))\r\n  par(mfcol=c(2,2))\r\n  bp = boxplot(dif_mm, main=""Monthly difference"", ylab=""Temperature difference (C)"", xlab=""Months"")\r\n  boxplot(err_month, main=""Percentage relative error"", ylab=""(%)"", xlab=""Months"")\r\n  boxplot(Tserie_can_mm, main=paste(name,""monthly candidate""), ylab=""Temperature (C)"", xlab=""Months"")\r\n  boxplot(Tserie_ref_mm, main=paste(name,""monthly reference""), ylab=""Temperature (C)"", xlab=""Months"")\r\n  dev.off()\r\n  \r\n  # Compute statistics on boxplots\r\n  box_diff = round(bp$stats, digits=2)\r\n  mm_dif = matrix(box_diff, nrow=5, ncol=12,\r\n                  dimnames=list(c(""Max"",""3rd Qu."",""Median"",""1st Qu."",""Min""),\r\n                                c(""J"",""F"",""M"",""A"",""M\'"",""J\'"",""J\'\'"",""A\'"",""S"",""O"",""N"",""D"")))\r\n  mm_dif = as.data.frame(mm_dif)\r\n  write.csv(mm_dif, file=paste(""02_"",name,""_statistics_boxplot_monthly_diff.csv"",sep=""""), row.names=T)\r\n  \r\n  # Find abnormal values (>100) where one station has data but the other hasn\'t\r\n  pdf(paste(""02_"",name,""_relative_error.pdf"",sep=""""))\r\n  par(mfcol=c(2,1))\r\n  M = matrix(err_month, nrow=nrow(err_month), ncol=ncol(err_month), byrow=F)\r\n  threshold = 100\r\n  M[M<(-1*threshold)] = NA\r\n  M[M>threshold] = NA\r\n  boxplot(M, main=paste(name,""percentage relative error (range +/- 100)""), ylab=""(%)"", xlab=""Months"")\r\n  vect = as.vector(t(M))\r\n  raw_e = zooreg(vect, frequency=12, start=(tab[1,6]))\r\n  numb = c(1:length(vect))\r\n  plot(numb, raw_e, type=""l"", main=paste(name,""percentage relative error (range +/- 100)""),\r\n       ylab=""(%)"", xlab=""Index"")\r\n  trend = zyp.trend.vector(vect, conf.intervals=T, preserve.range.for.sig.test=T)\r\n  abline(trend[11], trend[2])\r\n  dev.off()\r\n  \r\n  # Save trends\r\n  trend = as.data.frame(trend)\r\n  write.csv(trend, file=paste(""02_"",name,""_trend_relative error.csv"",sep=""""), row.names=T)\r\n  \r\n  # Differences plot\r\n  pdf(paste(""02_"",name,""_plot_diff.pdf"",sep=""""))\r\n  par(mfcol=c(2,1))\r\n  plot(raw_diff, type=""l"", main=paste(name,""Difference (Candidate - Reference)""),\r\n       ylab=""Temperature difference (C)"", xlab=""Years"")\r\n  abline(0, 0, col=""red"")\r\n  hist(raw_diff, freq=T, main=""History"", xlab=""Temperature difference (C)"")\r\n  dev.off()\r\n  \r\n  # Classification\r\n  data = c(ref_serie[,4], can_serie[,4])\r\n  quant = quantile(data, c(.05,.20,.80,.95), na.rm=T)\r\n  q05 = quant[1]\r\n  q20 = quant[2]\r\n  q80 = quant[3]\r\n  q95 = quant[4]\r\n  \r\n  # Candidate Classes definition - Extremely cold\r\n  extr_cold_can = Tserie_na[can_na<=q05,]\r\n  Can_Extr_Cold = classification(extr_cold_can[,1:4])\r\n  \r\n  # Candidate Classes definition - Cold\r\n  cold_can = Tserie_na[can_na>q05 & can_na<=q20,] \r\n  Can_Cold = classification(cold_can[,1:4])\r\n  \r\n  # Candidate Classes definition - Mean\r\n  mean_can = Tserie_na[can_na>q20 & can_na<=q80,]\r\n  Can_Mean = classification(mean_can[,1:4])\r\n  \r\n  # Candidate Classes definition - Warm\r\n  warm_can = Tserie_na[can_na>q80 & can_na<=q95,]\r\n  Can_Warm = classification(warm_can[,1:4])\r\n  \r\n  # Candidate Classes definition - Extremely Warm\r\n  extr_warm_can = Tserie_na[can_na>q95,]\r\n  Can_Extr_Warm = classification(extr_warm_can[,1:4])\r\n  \r\n  # Final result for candidate serie\r\n  Candidate = c(""Min."",""1st Qu."",""Median"",""Mean"",""3rd Qu."",""Max."",""Length"")\r\n  mat_fin_can = cbind(Candidate, Can_Extr_Cold, Can_Cold, Can_Mean,Can_Warm, Can_Extr_Warm)\r\n  mat_fin_can = as.data.frame(mat_fin_can)\r\n  for (i in c(2:6)) {\r\n    mat_fin_can[,i] = as.numeric(as.character(mat_fin_can[,i]))\r\n  }\r\n  write.csv(mat_fin_can, file=paste(""03_"",name,""_classes_candidate.csv"",sep=""""), row.names=F)\r\n  \r\n  # Reference Classes definition - Extremely cold\r\n  extr_cold_ref = Tserie_na[ref_na<=q05,]\r\n  extr_cold_ref = cbind(extr_cold_ref[,1:3], extr_cold_ref[5])\r\n  Ref_Extr_Cold = classification(extr_cold_ref)\r\n  \r\n  # Reference Classes definition - Cold\r\n  cold_ref = Tserie_na[ref_na>q05 & ref_na<=q20,]\r\n  cold_ref = cbind(cold_ref[,1:3], cold_ref[5])\r\n  Ref_Cold = classification(cold_ref)\r\n  \r\n  # Reference Classes definition - Mean\r\n  mean_ref = Tserie_na[ref_na>q20 & ref_na<=q80,]\r\n  mean_ref = cbind(mean_ref[,1:3], mean_ref[5])\r\n  Ref_Mean = classification(mean_ref)\r\n  \r\n  # Reference Classes definition - Warm\r\n  warm_ref = Tserie_na[ref_na>q80 & ref_na<=q95,]\r\n  warm_ref = cbind(warm_ref[,1:3], warm_ref[5])\r\n  Ref_Warm = classification(warm_ref)\r\n  \r\n  # Reference Classes definition - Extremely Warm\r\n  extr_warm_ref = Tserie_na[ref_na>q95,]\r\n  extr_warm_ref = cbind(extr_warm_ref[,1:3], extr_warm_ref[5])\r\n  Ref_Extr_Warm = classification(extr_warm_ref)\r\n  \r\n  # Final result for reference serie\r\n  Reference = c(""Min."",""1st Qu."",""Median"",""Mean"",""3rd Qu."",""Max."",""Length"")\r\n  mat_fin_ref = cbind(Reference, Ref_Extr_Cold, Ref_Cold, Ref_Mean, Ref_Warm, Ref_Extr_Warm)\r\n  mat_fin_ref = as.data.frame(mat_fin_ref)\r\n  for (i in c(2:6)) {\r\n    mat_fin_ref[,i] = as.numeric(as.character(mat_fin_ref[,i]))\r\n  }\r\n  write.csv(mat_fin_ref, file=paste(""03_"",name,""_classes_reference.csv"",sep=""""), row.names=F)\r\n  \r\n  # Compute the difference in % in numbers of events between series in the same class\r\n  freq = rbind(as.numeric(mat_fin_can[7,2:6]), as.numeric(mat_fin_ref[7,2:6]))\r\n  perc = (freq[2,] - freq[1,]) / freq[2,]\r\n  freq = rbind(freq, perc)\r\n  colnames(freq) = c(""Extr_Cold"",""Cold"",""Mean"",""Warm"",""Extr_Warm"")\r\n  rownames(freq) = c(""Candidate events"",""Reference events"",""Increase in %"")\r\n  freq = round(freq, digits=4)\r\n  write.csv(freq, file=paste(""03_"",name,""_frequencies_in_classes.csv"",sep=""""), row.names=T)\r\n  \r\n  # Compute where both stations have events in the same classes (common class events)\r\n  extr_cold_common = Tserie_na[Tserie_na[,4]<=q05 & Tserie_na[,5]<=q05,]\r\n  cold_common = Tserie_na[Tserie_na[,4]>q05 & Tserie_na[,4]<=q20 & Tserie_na[,5]>q05 & Tserie_na[,5]<=q20,]\r\n  mean_common = Tserie_na[Tserie_na[,4]>q20 & Tserie_na[,4]<=q80 & Tserie_na[,5]>q20 & Tserie_na[,5]<=q80,]\r\n  warm_common = Tserie_na[Tserie_na[,4]>q80 & Tserie_na[,4]<=q95 & Tserie_na[,5]>q80 & Tserie_na[,5]<=q95,]\r\n  extr_warm_common = Tserie_na[Tserie_na[,4]>q95 & Tserie_na[,5]>q95,]\r\n  \r\n  # Compute RMSE, ME, cor and summary on common class events\r\n  extr_cold_common_stat = as.data.frame(statistics(extr_cold_common[,4], extr_cold_common[,5]))\r\n  cold_common_stat = as.data.frame(statistics(cold_common[,4],cold_common[,5]))\r\n  mean_common_stat = as.data.frame(statistics(mean_common[,4],mean_common[,5]))\r\n  warm_common_stat = as.data.frame(statistics(warm_common[,4],warm_common[,5]))\r\n  extr_warm_common_stat = as.data.frame(statistics(extr_warm_common[,4],extr_warm_common[,5]))\r\n  write.csv(extr_cold_common_stat, file=paste(""03_"",name,""_extr_cold.csv"",sep=""""), row.names=T)\r\n  write.csv(cold_common_stat, file=paste(""03_"",name,""_cold.csv"",sep=""""), row.names=T)\r\n  write.csv(mean_common_stat, file=paste(""03_"",name,""_mean.csv"",sep=""""), row.names=T)\r\n  write.csv(warm_common_stat, file=paste(""03_"",name,""_warm.csv"",sep=""""), row.names=T)\r\n  write.csv(extr_warm_common_stat, file=paste(""03_"",name,""_extr_warm.csv"",sep=""""), row.names=T)\r\n  \r\n  # Plot of differences on common class events\r\n  pdf(paste(""03_"",name,""_boxplot_diff.pdf"",sep=""""))\r\n  par(mfrow = c(3,2))\r\n  boxplot(extr_cold_common[,7], col=""darkblue"", xlab=""Extremely cold"",\r\n          ylab=""Temperature (C)"", main=paste(""Extremely cold"",name))\r\n  boxplot(cold_common[,7],col=""blue"", xlab=""Cold"", ylab=""Temperature (C)"", main=paste(""Cold"",name))\r\n  boxplot(mean_common[,7],col=""green"", xlab=""Mean"", ylab=""Temperature (C)"", main=paste(""Mean"",name))\r\n  boxplot(warm_common[,7],col=""red"", xlab=""Warm"", ylab=""Temperature (C)"", main=paste(""Warm"",name))\r\n  boxplot(extr_warm_common[,7],col=""darkred"", xlab=""Extremely warm"",\r\n          ylab=""Temperature (C)"", main=paste(""Extremely warm"",name))\r\n  dev.off()\r\n  \r\n  # Prepare single file output in xlsx\r\n  myexcel = paste(name,""_Results.xlsx"",sep="""")\r\n  write.xlsx(stat, myexcel, sheetName=""01 - Raw data"")\r\n  wb = loadWorkbook(myexcel)\r\n  sheets = getSheets(wb)\r\n  sheet = sheets[[""01 - Raw data""]]\r\n  addDataFrame(test_fin, sheet, startRow=11, startColumn=1, row.names=FALSE)\r\n  sheet = createSheet(wb, sheetName = ""02 - Monthly data"")\r\n  addDataFrame(mm_dif, sheet, startRow=1, startColumn=1)\r\n  addDataFrame(trend, sheet, startRow=9, startColumn=1, col.names=FALSE)\r\n  sheet = createSheet(wb, sheetName = ""03 - Classes"")\r\n  addDataFrame(mat_fin_can, sheet, startRow=1, startColumn=1, row.names=FALSE)\r\n  addDataFrame(mat_fin_ref, sheet, startRow=11, startColumn=1, row.names=FALSE)\r\n  addDataFrame(freq, sheet, startRow=21, startColumn=1)\r\n  addDataFrame(extr_cold_common_stat, sheet, startRow=27, startColumn=1)\r\n  addDataFrame(cold_common_stat, sheet, startRow=36, startColumn=1)\r\n  addDataFrame(mean_common_stat, sheet, startRow=45, startColumn=1)\r\n  addDataFrame(warm_common_stat, sheet, startRow=54, startColumn=1)\r\n  addDataFrame(extr_warm_common_stat, sheet, startRow=63, startColumn=1)\r\n  saveWorkbook(wb, myexcel)\r\n  \r\n  return(NULL)\r\n} # end of function analyze\r\n\r\n\r\n###############################################################################\r\n####                              MAIN PROGRAM                             ####\r\n###############################################################################\r\n\r\n# Read input data and output folder creation\r\nif (interactive()) { # User has to choose file in input and path of results\r\n  tab = read.table(file.choose(), header=T, na.strings=""NA"")\r\n  setwd(choose.dir(getwd(), ""Choose a folder to save your results:""))\r\n} else { # File in input and path of results are hardcoded\r\n  setwd(""/data/test"") # Path where input file is located\r\n  tab = read.table(""example1.txt"", header=T, na.strings=""NA"") # Input file name\r\n  dir.create(""./results/"", showWarnings = FALSE) # Results folder\r\n  setwd(""./results/"") # Results folder\r\n}\r\n\r\n# Clean input and prepare other variables\r\ntab$date = as.Date(tab[,1], format=\'%d/%m/%Y\', tz=""GMT"")\r\ntab = tab[complete.cases(tab[,1]), ]\r\ntab$Y = as.numeric(format(tab$date, ""%Y""))\r\ntab$M = as.numeric(format(tab$date, ""%m""))\r\ntab$D = as.numeric(format(tab$date, ""%d""))\r\nstart.tab = as.Date(tab$date[1])\r\n\r\n# Compute TX and TN on both series\r\nTX = data.frame(tab$Y, tab$M, tab$D, tab[4], tab[2])\r\ncolnames(TX) = c(""Y"", ""M"", ""D"", ""CAN"", ""REF"")\r\nTX$time = ISOdate(TX[[1]], TX[[2]], TX[[3]], 0)\r\nTN = data.frame(tab$Y, tab$M, tab$D, tab[5], tab[3])\r\ncolnames(TN) = c(""Y"", ""M"", ""D"", ""CAN"", ""REF"")\r\nTN$time = ISOdate(TN[[1]], TN[[2]], TN[[3]], 0)\r\n\r\n# Do analysis on TX and on TN\r\nanalyze(""TX"",TX)\r\nanalyze(""TN"",TN)\r\n\r\n# Print runtime stats and clean all the environment\r\ncat(""Elaboration completed. You can find your results in"", getwd(), ""\\n"")\r\nend.time = Sys.time()\r\ncat(capture.output(end.time - start.time))\r\nrm(list=ls())\r\n']","Co.Temp - Comparing series of Temperatures This program aims to offer useful information on temperature series (i.e.: identify the biases in the daily maximum and minimum temperature, compare manual and automatic weather stations, etc.) and works in three steps: statistical analysis and characterization of the daily series of maximum (Tx) and minimum temperature (Tn), computation of monthly-aggregated data for both Tx and Tn and comparison between temperature classes of events (like heat wave, cold wave and normal events). For details and the latest version, see https://github.com/UniToDSTGruppoClima/CoTemp",3
Cytokine response following perturbation of the cervicovaginal milieu during HPV genital infection,"Data and statistical analysis script to reproduce results from:Selinger et al. (2021): Cytokine response following perturbation of the cervicovaginal milieu during HPV genital infectionhttps://medrxiv.org/cgi/content/short/2021.02.10.21251486v1Accepted for publication in Immunological Research. This work was supported by the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation program, EVOLPROOF project, grant agreement No 648963 to Samuel Alizon (CNRS).","['###linear models for combinations of covariates related to HPV infection\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(parallel)\nlibrary(doParallel)\n\n###load the data\n##this data contains results from the papclear clinical trial, covariates, total protein, calibration dates, and concentrations for 20 cytokines from vaginal swabs of 92 study participants\nmerged<-data.table(read.csv(\'HPC_cytokines_data.csv\'))\n\n##these are the covariates we are interested in (partly in French)\ninfection_covariates<-c(\n  ""is.HPVpos"",\n  ""is.HPVposPattern"",\n  ""is.HPVpos.multiple"",\n  ""has.HPV.high.risk"",\n  ""chlamydia_recent"",\n  ""chlamydia_test"",\n  ""DIAG_VAGINOSE_ON"", ##Inc.Q1 (last 3months),\n  ""DIAG_INFECT_FONGIQUE_ON"", ##Inc.Q1 (last 3months),\n  ""DIAG_INFECT_URINAIRE_ON"", ##Inc.Q1 (last 3months),\n  ""DIAG_INFLAM_PELVIENNE_ON"", ##Inc.Q1 (last 3months),\n  ""DIAG_DOUCHE_VAG_ON"", ##Inc.Q1 (last 3months),\n  ""CONDYLOMES_ON"", ##Inc.Q1 (last 3 months)\n  ""DEMANGEAISONS_VAG_ON"",##last week\n  ""ODEUR_VAG_ON"", ##last week\n  ""BRULURES_VAG_ON"", ##last week\n  ""SECRETION_VAG_ON"", ##last week\n  ""INFECT_GENITALE_3DERMOIS_ON"", ##Inc.Q1 (last 3months),\n  ""HPV_ON""\n)\n\n\n\n##we control for calibration date and total protein\ncontrolling_for=\'Calibration+total_protein\'\n###error handling\nmyTryCatch <- function(expr) {\n  warn <- err <- NULL\n  value <- withCallingHandlers(\n    tryCatch(expr, error=function(e) {\n      err <<- e\n      NULL\n    }), warning=function(w) {\n      warn <<- w\n      invokeRestart(""muffleWarning"")\n    })\n  list(model=value, warning=warn, error=err)\n}\n\n###this function generates linear model formulas\nget.formula_vec<-function(dep_vars=cytokines,ind_vars,control=controlling_for){\n  \n  # create all combinations of ind_vars\n  ind_vars_comb <- \n    unlist( sapply( seq_len(length(ind_vars)), \n                    function(i) {\n                      apply( combn(ind_vars,i), 2, function(x) paste(x, collapse = ""+""))\n                    }))\n  \n  # pair with dep_vars:\n  var_comb <- expand.grid(dep_vars, paste0(ind_vars_comb,\'+\',control) ) \n  \n  \n  # formulas for all combinations\n  formula_vec <- sprintf(""log10(%s) ~ %s"", var_comb$Var1, var_comb$Var2)\n  out<-NULL\n  out$formula_vec<-formula_vec\n  return(out)\n}\n\n##generate model formulas; we we also store model results in this object\nlinear.model.formulas<-get.formula_vec(ind_vars =infection_covariates)\n\n\n###Parallelize\nno_cores <- detectCores() - 1  \nprint(paste(\'Using\',no_cores,\'cores\'))\ncl <- makeCluster(no_cores, type=""FORK"", outfile=\'Log_Cluster.txt\')  \nregisterDoParallel(cl)  \n\n#function to be parallelized, this is the linear model with error handling\nglm_res_f<-function(f){\n  \n  print(paste0(\'Evaluating formula \',f))\n  fit1 <- myTryCatch(glm(formula=f,family=gaussian(link=\'identity\'),data=merged))\n  \n  if (is.null(fit1$error) & is.null(fit1$warning)){\n    print(summary(fit1$model))\n    out=fit1$model\n    out$coefficients <- coef( summary(out))\n    out$confint<-confint(fit1$model)\n    out$shapiro.wilk.p.value<-shapiro.test(residuals(fit1$model))$p.value\n  }\n  \n  if (is.null(fit1$error)==F ){\n    print(\'Error\')\n    out=fit1$error\n  }\n  \n  if (is.null(fit1$warning)==F){\n    print(\'Warning\')\n    out=fit1$warning\n  }\n  \n  \n  return(out)\n}\n\n##set option for glm\noptions(na.action = ""na.omit"")\n#parallelize model evaluation, run on HPC with batches if possible; \n##if you want to test, set: linear.model.formulas$formula_vec<-linear.model.formulas$formula_vec[1:5]\nglm_res<-foreach(f=linear.model.formulas$formula_vec,.errorhandling = \'pass\') %dopar% glm_res_f(f)\nnames(glm_res) <- linear.model.formulas$formula_vec\nglm_res=glm_res[sapply(glm_res,function(x) is.null(coef(x))==F)]##keep only fits without error or warning\n\nif(length(glm_res)>0){\n  df<-lapply(glm_res, function(x) {data.table(melt(coef(x)))})\n  for (k in c(1:length(df))){\n    df[[k]][,formula:=names(df)[k]]\n  }\n  results<-do.call(rbind,df)\n  \n  aic<-lapply(glm_res, function(x) {data.table(melt(AIC(x)))})\n  for (k in c(1:length(aic))){\n    aic[[k]][,formula:=names(aic)[k]]\n  }\n  \n  aic<-data.table(do.call(rbind,aic)); setnames(aic,\'value\',\'AIC\')\n  \n  \n  CI<-lapply(glm_res, function(x) {data.table(melt(x$confint))})\n  for (k in c(1:length(CI))){\n    CI[[k]][,formula:=names(CI)[k]]\n  }\n  CI<-data.table(do.call(rbind,CI))\n  results<-rbind(results,CI)\n  \n  SW<-lapply(glm_res, function(x) {data.table(melt(x$shapiro.wilk.p.value))})\n  for (k in c(1:length(SW))){\n    SW[[k]][,formula:=names(SW)[k]]\n  }\n  SW<-data.table(do.call(rbind,SW))\n  setnames(SW,\'value\',\'Shapiro.Wilk.p.value.residuals\')\n  \n  \n  results<-merge(results,aic,by=\'formula\')\n  results<-merge(results,SW,by=\'formula\')\n  \n  results[,cytokine:=sapply(strsplit(formula,split=\'~\',fixed=T),""[["",1)]\n  results[,number_factors:=sapply(strsplit(formula,split=\'+\',fixed=T),length)]\n  results<-results[!apply(sapply(c(\'Intercept\',unlist(strsplit(controlling_for,split=""+"",fixed=T))),function(x) grepl(x,Var1)),1,any),]##remove estimates, pvalues for variables of control\n  results[,is.significant:=NA]\n  asdf<-na.omit(results[Var2==\'Pr(>|t|)\'&value<= 0.1,is.significant:=TRUE])\n  results=merge(results,asdf[,list(Var1,formula,is.significant)],by=c(\'Var1\',\'formula\'),all.x=T)\n  results[,is.significant.x:=NULL]\n  results[is.na(is.significant.y),is.significant.y:=FALSE]\n  setnames(results,\'is.significant.y\',\'is.significant\')\n  results[,cytokine:=gsub(\'log10(\',\'\',cytokine,fixed=T)]\n  results[,cytokine:=gsub(\') \',\'\',cytokine,fixed=T)]\n  results$CovariateGroup<-\'infections\'\n  \n  linear.model.formulas$glm_res<-glm_res\n  linear.model.formulas$results<-results\n}else\n{\n  linear.model.formulas$glm_res<-NULL\n  linear.model.formulas$results<-NULL      \n}\n\n\n\n']","Cytokine response following perturbation of the cervicovaginal milieu during HPV genital infection Data and statistical analysis script to reproduce results from:Selinger et al. (2021): Cytokine response following perturbation of the cervicovaginal milieu during HPV genital infectionhttps://medrxiv.org/cgi/content/short/2021.02.10.21251486v1Accepted for publication in Immunological Research. This work was supported by the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation program, EVOLPROOF project, grant agreement No 648963 to Samuel Alizon (CNRS).",3
Data science with R,"The purpose of this course is to present researchers and scientists with R implementation of Machine Learning methods. The first part of the course will consist of introductory lectures on popular Machine Learning algorithms including unsupervised methods (Clustering, Association Rules) and supervised ones (Decision Trees, Naive Bayes, Random Forests and Deep Neural Network). Basic Machine Learning concepts such as training set, test set, validation set, overfitting, bagging, boosting will be introduced as well as performance evaluation for supervised and unsupervised methods.The second part will consist of practical exercises such as reading data, using packages and building machine learning applications. Different options for parallel programming will be shown using specific R packages (parallel, h2o,). For Deep Learning applications the Keras package will be presented. The examples will cover the analysis of large datasets and images datasets. Participants will use R on Cineca HPC facilities for practical assignments.Skills:At the end of the course, the student will be expected to have acquired:  the ability to perform basic operations on matrices and dataframes  the ability to manage packages  the ability to navigate in the RStudio interface  a general knowledge of Machine and Deep Learning methods  a general knowledge of the most popular packages for Machine and Deep Learning  a basic knowledge of different parallel programming techniques  the ability to build machine learning applications with large datasets and images datasetsTarget audience:Students and researchers with different backgrounds, looking for technologies and methods to analyze a large amount of data.Pre-requisites:Participants must have a basic statistics knowledge. Participants must also be familiar with basic Linux and R language.","['library(caret)\r\nlibrary(dplyr)\r\nlibrary(lime)\r\nlibrary(doParallel)\r\ncluster <- makeCluster(detectCores()-1) # convention to leave 1 core for OS\r\nregisterDoParallel(cluster)\r\nsetwd(""C:/Users/g.pedrazzi/Documents/corsi/bbs/R_supervised"")\r\ncovtype = read.csv(""data/covtype.full.csv"",header=TRUE, sep="","", stringsAsFactors=FALSE)\r\ncovtype<-droplevels(covtype)\r\ntable(covtype$Cover_Type)\r\ncovtype=covtype %>% mutate_if(is.character,as.factor)\r\ndt = sort(sample(nrow(covtype), nrow(covtype)*.8))\r\ntrain<-covtype[dt,]\r\ntest<-covtype[-dt,]\r\ntrain <- train %>% group_by(Cover_Type) %>% sample_n(2000)\r\ntable(train$Cover_Type)\r\ntc <- trainControl(method = ""cv"", number = 5,verboseIter = FALSE,p = 0.8)\r\ngrid <- expand.grid(interaction.depth = seq(1,6, by = 2),\r\n                    n.trees = c(10,20,30),\r\n                    shrinkage = c(0.1),\r\n                    n.minobsinnode = c(3,6))\r\n\r\n############## run the model\r\n\r\ngbm1_cv <- train(Cover_Type~.,\r\n                 data = train, method = ""gbm"",\r\n                 trControl = tc , tuneGrid = grid,metric=\'Accuracy\') # here\r\n\r\ngbm1_cv\r\npred_gbm1<-predict(gbm1_cv, newdata=test)\r\nconfusionMatrix(pred_gbm1, test$Cover_Type)\r\nlibrary(gbm)\r\npar(mar = c(5, 8, 1, 1))\r\nsummary(\r\n  gbm1_cv, \r\n  cBars = 15,\r\n  method = relative.influence, # also can use permutation.test.gbm\r\n  las = 2\r\n)\r\n\r\nexplainer <- lime(train, gbm1_cv)\r\nexplanation <- explain(test[1,], explainer, n_features = 5,n_labels = 4)\r\nplot_features(explanation)\r\n\r\n\r\n#Challenge: use two more classification algorithm and discuss the results\r\n', '#### installation of required packages - first time execution ####\r\ninstall.packages(""devtools"")\r\nlibrary(""devtools"")\r\ndevtools::install_github(""rstudio/reticulate"")\r\ndevtools::install_github(""rstudio/tensorflow"")\r\ndevtools::install_github(""rstudio/keras"")\r\n#reticulate da devtools\r\n\r\n#install.packages(""base64enc"")\r\n\r\n#### set libraries and  variables #### \r\nlibrary(tensorflow)\r\nlibrary(keras)\r\ninstall_tensorflow()\r\ninstall_keras()\r\nsetwd(""C:/Users/g.pedrazzi/Documents/data"")\r\ntrain_directory <- ""train""\r\nvalidation_directory <- ""validation""\r\naugment_directory<-""dataaug/""\r\nimg_width <- 150\r\nimg_height <- 150\r\nbatch_size <- 32\r\nepochs <- 30\r\n#epochs <- 2\r\ntrain_samples = 2048\r\nvalidation_samples = 832\r\n\r\n\r\n#### read images from directories for train and validation ####\r\n# By applying random transformation to our train set, we artificially enhance our dataset with new unseen images.\r\n# This will hopefully reduce overfitting and allows better generalization capability for our network.\r\naugment <- image_data_generator(rescale=1./255,\r\n                                shear_range=0.2,\r\n                                zoom_range=0.2,\r\n                                horizontal_flip=TRUE)\r\ntrain_generator <- flow_images_from_directory(train_directory, generator = augment,\r\n                                              target_size = c(img_width, img_height), color_mode = ""rgb"",\r\n                                              class_mode = ""categorical"", batch_size = batch_size, shuffle = TRUE,\r\n                                              seed = 123)\r\n\r\nvalidation_generator <- flow_images_from_directory(validation_directory, generator = image_data_generator(rescale=1./255),\r\n                                                   target_size = c(img_width, img_height), color_mode = ""rgb"",\r\n                                                   classes=NULL,\r\n                                                   class_mode = ""categorical"", batch_size = batch_size, shuffle = TRUE,\r\n                                                   seed = 123)\r\n\r\n\r\n\r\n\r\n\r\n#### Model architecture definition ####\r\n\r\nmodel <- keras_model_sequential()\r\n\r\nmodel %>%\r\n  layer_conv_2d(filter = 32, kernel_size = c(3,3), input_shape = c(img_width, img_height, 3)) %>%\r\n  layer_activation(""relu"") %>%\r\n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \r\n  \r\n  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%\r\n  layer_activation(""relu"") %>%\r\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\r\n  \r\n  layer_conv_2d(filter = 64, kernel_size = c(3,3)) %>%\r\n  layer_activation(""relu"") %>%\r\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\r\n  \r\n  layer_flatten() %>%\r\n  layer_dense(64) %>%\r\n  layer_activation(""relu"") %>%\r\n  layer_dropout(0.5) %>%\r\n  layer_dense(2) %>%\r\n  layer_activation(""softmax"")\r\n\r\n#### model generation ####\r\nmodel %>% compile(\r\n  loss = ""categorical_crossentropy"",\r\n  optimizer = optimizer_rmsprop(lr = 0.0001, decay = 1e-6),\r\n  metrics = ""categorical_accuracy""\r\n)\r\n\r\n\r\n\r\nmodel %>% fit_generator(\r\n  train_generator,\r\n  steps_per_epoch = as.integer(train_samples/batch_size), \r\n  epochs = 5, \r\n  validation_data = validation_generator,\r\n  validation_steps = as.integer(validation_samples/batch_size),\r\n  verbose=2\r\n)\r\n\r\n#### model summary, saving and evaluation ####\r\nsummary(model)\r\nsave_model_hdf5(model, \'basic_cnn_30_epochsR_cat.h5\', overwrite = TRUE)\r\nmodel<-load_model_hdf5(\'basic_cnn_30_epochsR_cat.h5\')\r\n\r\n\r\n#### classification of images in a directory #####\r\nsetwd(""C:/Users/g.pedrazzi/Documents/data"")\r\ntrain_directory <- ""train""\r\nvalidation_directory <- ""validation""\r\ntest_generator <- flow_images_from_directory(validation_directory, generator = augment,\r\n                                             target_size = c(img_width, img_height), color_mode = ""rgb"",\r\n                                             classes=NULL,\r\n                                             class_mode = ""categorical"", batch_size = 832, shuffle = FALSE,\r\n                                             save_to_dir = augment_directory ,\r\n                                             save_prefix = ""aug_"",\r\n                                             save_format = ""png"", follow_links = TRUE,\r\n                                             seed = 123)\r\nreticulate::iter_next(test_generator)\r\n\r\n#### check accuracy results on validation_set ####\r\nresults_tot<-as.data.frame(predict_generator(model, test_generator,1,verbose=1))\r\nresults_tot$numrow <- as.integer(rownames(results_tot)) \r\nresults_tot$correct<-ifelse((results_tot$numrow<=416) & (results_tot$V1>=results_tot$V2),1,ifelse((results_tot$numrow>416) & (results_tot$V2>=results_tot$V1),1,0))\r\nsum(results_tot$correct)/832\r\n', 'library(keras)\r\nconv_base <- application_vgg16(\r\n  weights = ""imagenet"",\r\n  include_top = FALSE,\r\n  input_shape = c(150, 150, 3)\r\n)\r\nsummary(conv_base)\r\nsetwd(""C:/Users/g.pedrazzi/Documents/data2"")\r\ntrain_directory <- ""train""\r\nvalidation_directory <- ""validation""\r\ndatagen <- image_data_generator(rescale = 1/255)\r\nbatch_size <- 1\r\n###extract feature from vgg16\r\n# Feature extraction consists of using the representations learned\r\n# by a previous network to extract interesting features from new samples.\r\n# These features are then run through a new classifier, which is trained from scratch.\r\nextract_features <- function(directory, sample_count) {\r\n  \r\n  features <- array(0, dim = c(sample_count, 4, 4, 512))  \r\n  labels <- array(0, dim = c(sample_count))\r\n  \r\n  generator <- flow_images_from_directory(\r\n    directory = directory,\r\n    generator = datagen,\r\n    target_size = c(150, 150),\r\n    batch_size = batch_size,\r\n    class_mode = ""binary""\r\n  )\r\n  \r\n  i <- 0\r\n  while(TRUE) {\r\n    batch <- generator_next(generator)\r\n    inputs_batch <- batch[[1]]\r\n    labels_batch <- batch[[2]]\r\n    features_batch <- conv_base %>% predict(inputs_batch)\r\n    \r\n    index_range <- ((i * batch_size)+1):((i + 1) * batch_size)\r\n    features[index_range,,,] <- features_batch\r\n    labels[index_range] <- labels_batch\r\n    \r\n    i <- i + 1\r\n    if (i * batch_size >= sample_count)\r\n      # Note that because generators yield data indefinitely in a loop, \r\n      # you must break after every image has been seen once.\r\n      break\r\n  }\r\n  \r\n  list(\r\n    features = features, \r\n    labels = labels\r\n  )\r\n}\r\n\r\ntrain <- extract_features(train_directory, 28)\r\nvalidation <- extract_features(validation_directory, 12)\r\n\r\nreshape_features <- function(features) {\r\n  array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))\r\n}\r\ntrain$features <- reshape_features(train$features)\r\nvalidation$features <- reshape_features(validation$features)\r\n\r\n\r\nmodel <- keras_model_sequential() %>% \r\n  layer_dense(units = 256, activation = ""relu"", \r\n              input_shape = 4 * 4 * 512) %>% \r\n  layer_dropout(rate = 0.5) %>% \r\n  layer_dense(units = 1, activation = ""sigmoid"")\r\n\r\nmodel %>% compile(\r\n  optimizer = optimizer_rmsprop(lr = 2e-5),\r\n  loss = ""binary_crossentropy"",\r\n  metrics = c(""accuracy"")\r\n)\r\nhistory <- model %>% fit(\r\n  train$features, train$labels,\r\n  epochs = 100,\r\n  batch_size = 20,\r\n  validation_data = list(validation$features, validation$labels)\r\n)\r\nplot(history)', 'library(dplyr)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(h2o)\nlibrary(randomForest)\n#library(e1071)\nlibrary(mlbench)\n\n\ndata <- read.csv(""DIAB_ML_BBS2.csv"",sep="";"",header=TRUE)\ndim(data)\n\nnewdata <- data[ which(data$ANNO==2017 | data$ANNO==2018), ]\ndim(newdata)\n\n\n\nnewdata$SESSO <- as.character(newdata$SESSO)\n\nnewdata[newdata == ""M""] <- 1\nnewdata[newdata == ""F""] <- 2\n\n\n# excluding  variables that I don\'t need \n\nmyvars <- names(newdata) %in% c(""USA_FARMACI"",""ANNO"",""ID_PAZ"",""DIABETE"",""PEZ_FT"",""SPESA_FT"",""SPESA_SDO"",""SPESA_SPA"",""SPE_TOT"")\nnewdata <- newdata[!myvars]\n\ndim(newdata)\n\n\nfor (colname in colnames(newdata)) {\n  if (colname != ""ETAX"" & colname!=""CHARLSON_INDEX"") \n  {newdata[[colname]] <- as.factor(newdata[[colname]])\n  }}\n\n\ntraining.samples <- newdata$DIAB_RIC %>% \n  createDataPartition(p = 0.75, list = FALSE)\ntrainset  <- newdata[training.samples, ]\ntestset <- newdata[-training.samples, ]\ndim(testset)\ndim(trainset)\n\n# RANDOM FOREST 1\nset.seed(123)\nrf <- randomForest(DIAB_RIC ~ .,data=trainset, ntree=500)\nrf\n\n\n# predict new obs and confusion table\nrf1_pred <- predict(rf, newdata=testset,type=""class"")\ncm= confusionMatrix(rf1_pred, testset$DIAB_RIC, positive=""1"")\ncm\n\n\n\n\ncm = as.matrix(table(Actual = testset$DIAB_RIC, Predicted = rf1_pred)) # create the confusion matrix\ncm\n\nn = sum(cm) # number of instances\nnc = nrow(cm) # number of classes\ndiag = diag(cm) # number of correctly classified instances per class \nrowsums = apply(cm, 1, sum) # number of instances per class\ncolsums = apply(cm, 2, sum) # number of predictions per class\np = rowsums / n # distribution of instances over the actual classes\nq = colsums / n # distribution of instances over the predicted classes\n\naccuracy = sum(diag) / n \n\nprecision = diag / colsums \nrecall = diag / rowsums \nf1 = 2 * precision * recall / (precision + recall) \ndata.frame(precision, recall, f1,accuracy) \n\nmacroPrecision = mean(precision)\nmacroRecall = mean(recall)\nmacroF1 = mean(f1)\n\n\nrfimp <- varImp(rf, scale = FALSE)\nrfimp\n\n# RANDOM FOREST 2 - CARET\n\ncontrol <- trainControl(method=\'cv\', \n                        number=10)\n#Metric compare model is Accuracy\nmetric <- ""Accuracy""\nset.seed(123)\nmtry <- sqrt(ncol(testset))\n\ntunegrid <- expand.grid(.mtry=mtry)\nrf_caret <- train(DIAB_RIC~., \n                    data=trainset, \n                    method=\'rf\', \n                    metric=\'Accuracy\', \n                    tuneGrid=tunegrid, \n                    trControl=control)\nprint(rf_caret)\n\n\nrf2_pred <- predict(rf_caret, newdata=testset,type=""prob"")\n\n\ncm2 = as.matrix(table(Actual = testset$DIAB_RIC, Predicted = rf2_pred)) # create the confusion matrix\ncm2\n\nn = sum(cm2) # number of instances\nnc = nrow(cm2) # number of classes\ndiag = diag(cm2) # number of correctly classified instances per class \nrowsums = apply(cm2, 1, sum) # number of instances per class\ncolsums = apply(cm2, 2, sum) # number of predictions per class\np = rowsums / n # distribution of instances over the actual classes\nq = colsums / n # distribution of instances over the predicted classes\n\naccuracy = sum(diag) / n \n\nprecision = diag / colsums \nrecall = diag / rowsums \nf1 = 2 * precision * recall / (precision + recall) \ndata.frame(precision, recall, f1,accuracy) \n\nrfimp_caret <- varImp(rf, scale = FALSE)\nrfimp_caret\n\n# H20 AUTOML\n\nlibrary(h2o)\nh2o.init()\n\n\n#prova automl\n\nindexes <- sample(1:nrow(newdata), size=0.25*nrow(newdata))\n# Split data\ntest = as.h2o(newdata[indexes,])\ntrain = as.h2o(newdata[-indexes,])\n\ny <- names(newdata[3])\nx <- setdiff(names(train), y)\n\n# For binary classification, response should be a factor\ntrain[, y] <- as.factor(train[, y])\ntest[, y] <- as.factor(test[, y])\n\n# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\naml <- h2o.automl(x = x, y = y,\n                  training_frame = train,\n                  max_models = 10,\n                  seed = 1234)\n\n# View the AutoML Leaderboard\nlb <- aml@leaderboard\nprint(lb, n = nrow(lb))  # Print all rows instead of default (6 rows)\n\n# Best model variable importance \n\nm <- h2o.getModel(""GBM_2_AutoML_20210615"")\nh2o.varimp(m)\nh2o.varimp_plot(m)\n\n\n# to see all the varimp\n\nmodel_ids <- as.data.frame(lb$model_id)[,1]\n\nfor (model_id in model_ids) {\n  print(model_id)\n  m <- h2o.getModel(model_id)\n  h2o.varimp(m)\n  h2o.varimp_plot(m)\n}\n\n\n']","Data science with R The purpose of this course is to present researchers and scientists with R implementation of Machine Learning methods. The first part of the course will consist of introductory lectures on popular Machine Learning algorithms including unsupervised methods (Clustering, Association Rules) and supervised ones (Decision Trees, Naive Bayes, Random Forests and Deep Neural Network). Basic Machine Learning concepts such as training set, test set, validation set, overfitting, bagging, boosting will be introduced as well as performance evaluation for supervised and unsupervised methods.The second part will consist of practical exercises such as reading data, using packages and building machine learning applications. Different options for parallel programming will be shown using specific R packages (parallel, h2o,). For Deep Learning applications the Keras package will be presented. The examples will cover the analysis of large datasets and images datasets. Participants will use R on Cineca HPC facilities for practical assignments.Skills:At the end of the course, the student will be expected to have acquired:  the ability to perform basic operations on matrices and dataframes  the ability to manage packages  the ability to navigate in the RStudio interface  a general knowledge of Machine and Deep Learning methods  a general knowledge of the most popular packages for Machine and Deep Learning  a basic knowledge of different parallel programming techniques  the ability to build machine learning applications with large datasets and images datasetsTarget audience:Students and researchers with different backgrounds, looking for technologies and methods to analyze a large amount of data.Pre-requisites:Participants must have a basic statistics knowledge. Participants must also be familiar with basic Linux and R language.",4
Data Science in Biomedicine - R scripts,"R scripts for: 1) Google trends analysis regarding the terms ""Data Science"", ""Big Data"" and ""Cloud Computing"" and 2) Read and summarise the Web of Science search for the number of publications associated with the topics ""Data Science"", ""Big Data"" and ""Cloud Computing"" from 2004 to 2019 in nine different countries.","['#--------------------------------------------------------------\r\n# Chapter 2: Data science in biomedicine\r\n#--------------------------------------------------------------\r\n# Install and load the packages\r\n\r\n# install.packages(""gtrendsR"")\r\n# install.packages(""ggplot2"")\r\n# install.packages(""gridExtra"")\r\nlibrary(gtrendsR)\r\nlibrary(ggplot2)\r\nlibrary(gridExtra)\r\n\r\n#------------------------------------------------------------- \r\n# Figure 2.2: Google trends for the terms ""Data Science"" (red), \r\n# ""Big Data"" (green), and ""Cloud Computing"" (blue) for global \r\n# queries.\r\n#------------------------------------------------------------- \r\n\r\nres <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                 ""Data Science""),\r\n               time = ""2004-01-01 2019-12-31"")\r\nplot(res, main = """")\r\n\r\nwindows()\r\nworld <- res$interest_over_time\r\nworld$hits[world$hits == ""<1""] <- 0\r\nworld$hits<- as.numeric(as.character(world$hits))\r\nworld$date <- as.Date(world$date)\r\nworld$keyword <- factor(world$keyword, \r\n                        levels = c(""Data Science"", \r\n                                   ""Big Data"", \r\n                                   ""Cloud Computing""))\r\n\r\n#---\r\n\r\ngg_world <- ggplot(world, aes(x = date, y = hits, \r\n                              color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5)) +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"") +\r\n  theme(legend.position = ""none"")\r\nplot1 <- print(gg_world + ggtitle(""World"")) \r\n\r\n#------------------------------------------------------------- \r\n# Figure 2.3: Google trends for the terms ""Data Science"" (red), \r\n# ""Big Data"" (green), and ""Cloud Computing"" (blue) for some \r\n# countries of Europe\r\n#------------------------------------------------------------- \r\n\r\nres_spain <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                       ""Data Science""), \r\n                     time = ""2004-01-01 2019-12-31"", \r\n                     geo = ""ES"")\r\nres_germany <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                         ""Data Science""), \r\n                       time = ""2004-01-01 2019-12-31"",\r\n                       geo = ""DE"")\r\nres_unitedkingdom <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                               ""Data Science""), \r\n                             time = ""2004-01-01 2019-12-31"",\r\n                             geo = ""GB"")\r\nres_italy <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                       ""Data Science""), \r\n                     time = ""2004-01-01 2019-12-31"", \r\n                     geo = ""IT"")\r\nsp <- res_spain$interest_over_time\r\nsp$hits[sp$hits == ""<1""] <- 0\r\nsp$hits<- as.numeric(as.character(sp$hits))\r\nde <- res_germany$interest_over_time\r\nuk <- res_unitedkingdom$interest_over_time\r\nit <- res_italy$interest_over_time\r\nsp$date <- as.Date(sp$date)\r\nde$date <- as.Date(de$date)\r\nuk$date <- as.Date(uk$date)\r\nit$date <- as.Date(it$date)\r\n\r\nsp$keyword <- factor(sp$keyword, \r\n                     levels = c(""Data Science"", \r\n                                ""Big Data"", \r\n                                ""Cloud Computing""))\r\nde$keyword <- factor(de$keyword, \r\n                     levels = c(""Data Science"", \r\n                                ""Big Data"", \r\n                                ""Cloud Computing""))\r\nuk$keyword <- factor(uk$keyword, \r\n                     levels = c(""Data Science"", \r\n                                ""Big Data"", \r\n                                ""Cloud Computing""))\r\nit$keyword <- factor(it$keyword, \r\n                     levels = c(""Data Science"", \r\n                                ""Big Data"", \r\n                                ""Cloud Computing""))\r\n#---\r\n\r\ngg_sp <- ggplot(sp, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") + \r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_de <- ggplot(de, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_uk <- ggplot(uk, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_it <- ggplot(it, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\n\r\nplot1 <- gg_sp + ggtitle(""Spain"") \r\nplot2 <- gg_de + ggtitle(""Germany"")\r\nplot3 <- gg_uk + ggtitle(""United Kingdom"")\r\nplot4 <- gg_it + ggtitle(""Italy"")\r\nwindows()\r\ngrid.arrange(plot1, plot2, plot3, plot4, ncol = 2) \r\n\r\nplots <- list(x = plot1, y = plot2, z = plot3, \r\n              t = plot4)\r\n\r\n\r\n#------------------------------------------------------------- \r\n# Figure 2.4: Google trends for the terms ""Data Science"" (red), \r\n# ""Big Data"" (green), and ""Cloud Computing"" (blue) for United \r\n# States and some of its states\r\n#------------------------------------------------------------- \r\n\r\nres_usa <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                     ""Data Science""), \r\n                   time = ""2004-01-01 2019-12-31"",  \r\n                   geo = ""US"")\r\nres_usa_ma <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                        ""Data Science""), \r\n                      time = ""2004-01-01 2019-12-31"", \r\n                      geo = ""US-MA"")\r\nres_usa_ca <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                        ""Data Science""), \r\n                      time = ""2004-01-01 2019-12-31"", \r\n                      geo = ""US-CA"")\r\nres_usa_wa <- gtrends(c(""Cloud Computing"", ""Big Data"", \r\n                        ""Data Science""), \r\n                      time = ""2004-01-01 2019-12-31"", \r\n                      geo = ""US-WA"")\r\n\r\nusa <- res_usa$interest_over_time\r\nusa$hits[usa$hits == ""<1""] <- 0\r\nusa$hits<- as.numeric(as.character(usa$hits))\r\nusa_ma <- res_usa_ma$interest_over_time\r\nusa_ca <- res_usa_ca$interest_over_time\r\nusa_wa <- res_usa_wa$interest_over_time\r\nusa$date <- as.Date(usa$date)\r\nusa_ma$date <- as.Date(usa_ma$date)\r\nusa_ca$date <- as.Date(usa_ca$date)\r\nusa_wa$date <- as.Date(usa_wa$date)\r\n\r\nusa$keyword <- factor(usa$keyword, \r\n                      levels = c(""Data Science"",\r\n                                 ""Big Data"", \r\n                                 ""Cloud Computing""))\r\nusa_ma$keyword <- factor(usa_ma$keyword, \r\n                         levels = c(""Data Science"",\r\n                                    ""Big Data"", \r\n                                    ""Cloud Computing""))\r\nusa_ca$keyword <- factor(usa_ca$keyword, \r\n                         levels = c(""Data Science"", \r\n                                    ""Big Data"", \r\n                                    ""Cloud Computing""))\r\nusa_wa$keyword <- factor(usa_wa$keyword, \r\n                         levels = c(""Data Science"",\r\n                                    ""Big Data"", \r\n                                    ""Cloud Computing""))\r\n\r\n\r\ngg_usa <- ggplot(usa, aes(x = date, y = hits, \r\n                          color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_usa_ma <- ggplot(usa_ma, aes(x = date, y = hits, \r\n                                color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") + \r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_usa_ca <- ggplot(usa_ca, aes(x = date, y = hits, \r\n                                color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_usa_wa <- ggplot(usa_wa, aes(x = date, y = hits, \r\n                                color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\n\r\nplot5 <- gg_usa + ggtitle(""United States"")\r\nplot6 <- gg_usa_ma + ggtitle(""US - Massachusetts"")\r\nplot7 <- gg_usa_ca + ggtitle(""US - California"")\r\nplot8 <- gg_usa_wa + ggtitle(""US - Washington"")\r\nwindows()\r\ngrid.arrange(plot5, plot6, plot7, plot8, ncol = 2)\r\n\r\n#-------------------------------------------------------------\r\n# Figure 2.5: Google trends for the terms ""Data Science"" (red), \r\n# ""Big Data"" (green), and ""Cloud Computing"" (blue) in some \r\n# countries of Asia and in Australia\r\n#------------------------------------------------------------- \r\n\r\nres_china <- gtrends(c(""data science"", ""cloud computing"", \r\n                       ""big data""), \r\n                     time = ""2004-01-01 2019-12-31"", \r\n                     geo = ""CN"")\r\nres_japan <- gtrends(c(""data science"", ""cloud computing"", \r\n                       ""big data""), \r\n                     time = ""2004-01-01 2019-12-31"",\r\n                     geo = ""JP"")\r\nres_india <- gtrends(c(""data science"", ""cloud computing"", \r\n                       ""big data""), \r\n                     time = ""2004-01-01 2019-12-31"",\r\n                     geo = ""IN"")\r\nres_australia <- gtrends(c(""data science"", ""cloud computing"", \r\n                           ""big data""), \r\n                         time = ""2004-01-01 2019-12-31"",\r\n                         geo = ""AU"")\r\n\r\ncn <- res_china$interest_over_time\r\ncn$hits[cn$hits == ""<1""] <- 0\r\ncn$hits<- as.numeric(as.character(cn$hits))\r\nja <- res_japan$interest_over_time\r\nindia <- res_india$interest_over_time\r\nau <- res_australia$interest_over_time\r\ncn$date <- as.Date(cn$date)\r\nja$date <- as.Date(ja$date)\r\nindia$date <- as.Date(india$date)\r\nau$date <- as.Date(au$date)\r\n\r\ncn$keyword <- as.factor(cn$keyword)\r\nja$keyword <- as.factor(ja$keyword)\r\nindia$keyword <- as.factor(india$keyword)\r\nau$keyword <- as.factor(au$keyword)\r\nlevels(cn$keyword) <- c(""Data Science"", ""Big Data"", \r\n                        ""Cloud Computing"")\r\nlevels(ja$keyword) <- c(""Data Science"", ""Big Data"", \r\n                        ""Cloud Computing"")\r\nlevels(india$keyword) <- c(""Data Science"", ""Big Data"", \r\n                           ""Cloud Computing"")\r\nlevels(au$keyword) <- c(""Data Science"", ""Big Data"", \r\n                        ""Cloud Computing"")\r\n\r\ngg_cn <- ggplot(cn, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n\r\ngg_ja <- ggplot(ja, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"", \r\n       legend.position = ""none"")\r\n\r\ngg_india <- ggplot(india, aes(x = date, y = hits, \r\n                              color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"", \r\n       legend.position = ""none"")\r\n\r\ngg_au <- ggplot(au, aes(x = date, y = hits, \r\n                        color = keyword)) + \r\n  geom_line(size = 1) + geom_point() + theme_bw() +\r\n  theme(plot.title = element_text(hjust = 0.5), \r\n        legend.position = ""none"") +\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"")\r\n  labs(x = ""Date"", y = ""Interest"", color = ""Term"", \r\n       legend.position = ""none"")\r\n\r\nplot9 <- gg_cn + ggtitle(""China"")\r\nplot10 <- gg_ja + ggtitle(""Japan"")\r\nplot11 <- gg_india + ggtitle(""India"")\r\nplot12 <- gg_au + ggtitle(""Australia"")\r\nwindows()\r\ngrid.arrange(plot9, plot10, plot11, plot12, ncol = 2)\r\n\r\n', '#--------------------------------------------------------------\r\n# Chapter 2: Data science in biomedicine\r\n#--------------------------------------------------------------\r\n\r\n# Set working directory to source file location\r\n\r\n\r\n# Define the countries\r\ncountries <- c(""USA"", ""UK"", ""JAPAN"", ""GERMANY"", ""AUSTRALIA"",\r\n               ""SPAIN"", ""ITALY"", ""INDIA"", ""CHINA"")\r\nn_countries <- length(countries)\r\nyears <- 2004:2019\r\n\r\n# Creating empty datasets\r\ndata_bg_countries <- matrix(0, ncol = n_countries, \r\n                            nrow = length(years))\r\ndata_ds_countries <- matrix(0, ncol = n_countries, \r\n                            nrow = length(years))\r\ndata_cc_countries <- matrix(0, ncol = n_countries, \r\n                            nrow = length(years))\r\n\r\ncolnames(data_bg_countries) <- countries\r\ncolnames(data_ds_countries) <- countries\r\ncolnames(data_cc_countries) <- countries\r\n\r\n#--------------------------------------------------------------\r\n# Read ""DATA SCIENCE"" databases\r\n#--------------------------------------------------------------\r\nds_UK <- read.table(""DataScience_UK.txt"", header = TRUE)\r\nds_USA <- read.table(""DataScience_USA.txt"", header = TRUE)\r\nds_ITALY <- read.table(""DataScience_Italy.txt"", header = TRUE)\r\nds_SPAIN <- read.table(""DataScience_Spain.txt"", header = TRUE)\r\nds_JAPAN <- read.table(""DataScience_Japan.txt"", header = TRUE)\r\nds_CHINA <- read.table(""DataScience_China.txt"", header = TRUE)\r\nds_GER <- read.table(""DataScience_Germany.txt"", header = TRUE)\r\nds_INDIA <- read.table(""DataScience_India.txt"", header = TRUE)\r\nds_AUS <- read.table(""DataScience_Australia.txt"", header = TRUE)\r\n\r\ndata_ds_countries <- cbind(ds_USA[ds_USA[, 1]%in%years, 2],\r\n                           ds_UK[ds_UK[, 1]%in%years, 2],\r\n                           ds_JAPAN[ds_JAPAN[, 1]%in%years, 2],\r\n                           ds_GER[ds_GER[, 1]%in%years, 2],\r\n                           ds_AUS[ds_AUS[, 1]%in%years, 2], \r\n                           ds_SPAIN[ds_SPAIN[, 1]%in%years, 2],\r\n                           ds_ITALY[ds_ITALY[, 1]%in%years, 2],\r\n                           ds_INDIA[ds_INDIA[, 1]%in%years, 2],\r\n                           ds_CHINA[ds_CHINA[, 1]%in%years, 2])\r\ncolnames(data_ds_countries) <- countries\r\ndata_ds_countries <- data_ds_countries[length(years):1, ]\r\nrownames(data_ds_countries) <- years\r\ndata_ds_countries\r\n\r\n#--------------------------------------------------------------\r\n# Read ""BIG DATA"" databases\r\n#--------------------------------------------------------------\r\n\r\nbg_UK <- read.table(""BigData_UK.txt"", header = TRUE)\r\nbg_USA <- read.table(""BigData_USA.txt"", header = TRUE)\r\nbg_ITALY <- read.table(""BigData_Italy.txt"", header = TRUE)\r\nbg_SPAIN <- read.table(""BigData_Spain.txt"", header = TRUE)\r\nbg_JAPAN <- read.table(""BigData_Japan.txt"", header = TRUE)\r\nbg_CHINA <- read.table(""BigData_China.txt"", header = TRUE)\r\nbg_GER <- read.table(""BigData_Germany.txt"", header = TRUE)\r\nbg_INDIA <- read.table(""BigData_India.txt"", header = TRUE)\r\nbg_AUS <- read.table(""BigData_Australia.txt"", header = TRUE)\r\n\r\n\r\ndata_bg_countries <- cbind(bg_USA[bg_USA[, 1]%in%years, 2],\r\n                           bg_UK[bg_UK[, 1]%in%years, 2],\r\n                           bg_JAPAN[bg_JAPAN[, 1]%in%years, 2],\r\n                           bg_GER[bg_GER[, 1]%in%years, 2],\r\n                           bg_AUS[bg_AUS[, 1]%in%years, 2], \r\n                           bg_SPAIN[bg_SPAIN[, 1]%in%years, 2],\r\n                           bg_ITALY[bg_ITALY[, 1]%in%years, 2],\r\n                           bg_INDIA[bg_INDIA[, 1]%in%years, 2],\r\n                           bg_CHINA[bg_CHINA[, 1]%in%years, 2])\r\ncolnames(data_bg_countries) <- countries\r\ndata_bg_countries <- data_bg_countries[length(years):1, ]\r\nrownames(data_bg_countries) <- years\r\ndata_bg_countries\r\n\r\n#--------------------------------------------------------------\r\n# Read ""CLOUD COMPUTING"" databases\r\n#--------------------------------------------------------------\r\n\r\ncc_UK <- read.table(""CloudComputing_UK.txt"", header = TRUE)\r\ncc_USA <- read.table(""CloudComputing_USA.txt"", header = TRUE)\r\ncc_ITALY <- read.table(""CloudComputing_Italy.txt"", header = TRUE)\r\ncc_SPAIN <- read.table(""CloudComputing_Spain.txt"", header = TRUE)\r\ncc_JAPAN <- read.table(""CloudComputing_Japan.txt"", header = TRUE)\r\ncc_CHINA <- read.table(""CloudComputing_China.txt"", header = TRUE)\r\ncc_GER <- read.table(""CloudComputing_Germany.txt"", header = TRUE)\r\ncc_INDIA <- read.table(""CloudComputing_India.txt"", header = TRUE)\r\ncc_AUS <- read.table(""CloudComputing_Australia.txt"", header = TRUE)\r\n\r\ndata_cc_countries <- cbind(cc_USA[cc_USA[, 1]%in%years, 2],\r\n                           cc_UK[cc_UK[, 1]%in%years, 2],\r\n                           cc_JAPAN[cc_JAPAN[, 1]%in%years, 2],\r\n                           cc_GER[cc_GER[, 1]%in%years, 2],\r\n                           cc_AUS[cc_AUS[, 1]%in%years, 2], \r\n                           cc_SPAIN[cc_SPAIN[, 1]%in%years, 2],\r\n                           cc_ITALY[cc_ITALY[, 1]%in%years, 2],\r\n                           cc_INDIA[cc_INDIA[, 1]%in%years, 2],\r\n                           cc_CHINA[cc_CHINA[, 1]%in%years, 2])\r\ncolnames(data_cc_countries) <- countries\r\ndata_cc_countries <- data_cc_countries[length(years):1, ]\r\nrownames(data_cc_countries) <- years\r\ndata_cc_countries\r\n\r\n#--------------------------------------------------------------\r\n# Complete database and summary\r\n#--------------------------------------------------------------\r\n\r\ndata_countries_total <- matrix(NA, ncol = length(countries),\r\n                               nrow = length(years) * 3)\r\ncolnames(data_countries_total) <- countries\r\nrownames(data_countries_total) <- 1:(length(years) * 3)\r\ndata_countries_total\r\n\r\n#--\r\n\r\nfor(i in 1:length(years)){\r\n rownames(data_countries_total)[1 + (i - 1) * 3] <- paste(""ds"", \r\n                                                          years[i],\r\n                                                          sep = ""_"")\r\n data_countries_total[1 + (i - 1) * 3, ] <- data_ds_countries[i, ]\r\n}\r\ndata_countries_total\r\n\r\n']","Data Science in Biomedicine - R scripts R scripts for: 1) Google trends analysis regarding the terms ""Data Science"", ""Big Data"" and ""Cloud Computing"" and 2) Read and summarise the Web of Science search for the number of publications associated with the topics ""Data Science"", ""Big Data"" and ""Cloud Computing"" from 2004 to 2019 in nine different countries.",4
Distinct decision-making properties underlying the species specificity of group formation of flies,"Many animal species form groups. Group characteristics differ between species, suggesting that the decision-making of individuals for grouping varies across species. However, the actual decision-making properties that lead to interspecific differences in group characteristics remain unclear. Here, we compared the group formation processes of two Drosophilinae fly species, Colocasiomyia alocasiae and Drosophila melanogaster, which form dense and sparse groups, respectively. A high-throughput tracking system revealed that C. alocasiae flies formed groups faster than D. melanogaster flies, and the probability of C. alocasiae remaining in groups was far higher than that of D. melanogaster. C. alocasiae flies joined groups even when the group size was small, whereas D. melanogaster flies joined groups only when the group size was sufficiently large. C. alocasiae flies attenuated their walking speed when the inter-individual distance between flies became small, whereas such behavioural properties were not clearly observed in D. melanogaster. Furthermore, depriving C. alocasiae flies of visual input affected grouping behaviours, resulting in a severe reduction in group formation. These findings show that C. alocasiae decision-making regarding grouping, which greatly depends on vision, is significantly different from D. melanogaster, leading to species-specific group-formation properties.",,"Distinct decision-making properties underlying the species specificity of group formation of flies Many animal species form groups. Group characteristics differ between species, suggesting that the decision-making of individuals for grouping varies across species. However, the actual decision-making properties that lead to interspecific differences in group characteristics remain unclear. Here, we compared the group formation processes of two Drosophilinae fly species, Colocasiomyia alocasiae and Drosophila melanogaster, which form dense and sparse groups, respectively. A high-throughput tracking system revealed that C. alocasiae flies formed groups faster than D. melanogaster flies, and the probability of C. alocasiae remaining in groups was far higher than that of D. melanogaster. C. alocasiae flies joined groups even when the group size was small, whereas D. melanogaster flies joined groups only when the group size was sufficiently large. C. alocasiae flies attenuated their walking speed when the inter-individual distance between flies became small, whereas such behavioural properties were not clearly observed in D. melanogaster. Furthermore, depriving C. alocasiae flies of visual input affected grouping behaviours, resulting in a severe reduction in group formation. These findings show that C. alocasiae decision-making regarding grouping, which greatly depends on vision, is significantly different from D. melanogaster, leading to species-specific group-formation properties.",4
Data Science in Undergraduate Life Science Education: A Need for Instructor Skills Training - Associated data and code,"The following files are the data set and code for analysis and visualization associated with the manuscript ""Data Science in Undergraduate Life Science Education: A Need for Instructor Skills Training.""",,"Data Science in Undergraduate Life Science Education: A Need for Instructor Skills Training - Associated data and code The following files are the data set and code for analysis and visualization associated with the manuscript ""Data Science in Undergraduate Life Science Education: A Need for Instructor Skills Training.""",4
Key words related to public engagement with science by Society of Freshwater Science journals and conference sessions (1997-2019),"Data set that was used to determine the frequency each of 4 key words (public engagement, education, outreach, or science communication) in the title or abstract of published papers in Freshwater Science (formerly the Journal of the North American Benthological Society) and oral presentations (talks) at the annual Society for Freshwater Science meetings from 1997 to 2019. Does not include any data on talks for 2013-2014 because they were not published during those years.","['#Burdett et al. 2021\r\n#Figure 1 creation code\r\n# Dec 2019\r\n\r\nlibrary(tidyverse)\r\nlibrary(RColorBrewer)\r\nlibrary(wesanderson)\r\nlibrary(patchwork)\r\nlibrary(ggpubr)\r\nlibrary(ggpattern)\r\nlibrary(lemon)\r\n\r\n#To create Figure 1: Stacked bar chart\r\n#Load and tidy data\r\ndf<-read_csv(""Burdett_et_al_2021_Figure_1_Data.csv"")\r\ndf$Keyword <- factor(df$Keyword, levels=c(""Public engagement"", ""Education"", ""Outreach"", ""Science communication""))\r\ntalk<-subset(df, Type==""Talks"")\r\npaper<-subset(df, Type==""Papers"")\r\ndf$Year<-as.factor(df$Year)\r\ndf$Type<-factor(df$Type, levels=c(""Papers"", ""Talks""))\r\n\r\n## Generate plots with facet\r\n#Color version\r\nfig_col<-ggplot(df, aes(x = Count, y = reorder(Year, desc(Year)), fill = Keyword)) +\r\n  geom_col() +theme_classic()+\r\n  facet_rep_grid(. ~ Type) + \r\n  coord_capped_cart(bottom=\'both\', left=\'both\')+\r\n  ylab(""Year"")+\r\n  scale_fill_manual(values=c(""#fdae61"", ""#2c7bb6"", ""#d7191c"", ""#abd9e9""))+\r\n  labs(fill = ""Key word"")+\r\n  theme(axis.title.x=element_text(size=16), \r\n        axis.title.y = element_text(size=16), \r\n        axis.text.x = element_text(size=12, vjust=0.5), \r\n        axis.text.y=element_text(size=12),\r\n        strip.background = element_blank(),\r\n        strip.text.x = element_text(size=16, face=""bold"", hjust=-0.02),\r\n        legend.text=element_text(size=12),\r\n        legend.justification=c(1,1), legend.position=c(0.98,0.98),\r\n        legend.title = element_text(size=12, face=""bold""))\r\nfig_col\r\n\r\nggsave(""C:/Users/owner/Desktop/SFS/Fig1_Color.tiff"",\r\n       plot = fig_col,\r\n       width=7.5,\r\n       height=5,\r\n       units=c(""in""),\r\n       dpi = 600)\r\n\r\n#Greyscale version\r\nfig_gry<-ggplot(df, aes(x = Count, y = reorder(Year, desc(Year)), fill = Keyword)) +\r\n  geom_col() +theme_classic()+\r\n  facet_rep_grid(. ~ Type) + \r\n  coord_capped_cart(bottom=\'both\', left=\'both\')+\r\n  ylab(""Year"")+\r\n  scale_fill_grey()+\r\n  labs(fill = ""Key word"")+\r\n  theme(axis.title.x=element_text(size=16), \r\n        axis.title.y = element_text(size=16), \r\n        axis.text.x = element_text(size=12, vjust=0.5), \r\n        axis.text.y=element_text(size=12),\r\n        strip.background = element_blank(),\r\n        strip.text.x = element_text(size=16, face=""bold"", hjust=-0.02),\r\n        legend.text=element_text(size=12),\r\n        legend.justification=c(1,1), legend.position=c(0.98,0.98),\r\n        legend.title = element_text(size=12, face=""bold""))\r\nfig_gry\r\n\r\nggsave(""C:/Users/owner/Desktop/SFS/Fig1_Grey_12.12.tiff"",\r\n       plot = fig_gry,\r\n       width=7.5,\r\n       height=5,\r\n       units=c(""in""),\r\n       dpi = 600)\r\n']","Key words related to public engagement with science by Society of Freshwater Science journals and conference sessions (1997-2019) Data set that was used to determine the frequency each of 4 key words (public engagement, education, outreach, or science communication) in the title or abstract of published papers in Freshwater Science (formerly the Journal of the North American Benthological Society) and oral presentations (talks) at the annual Society for Freshwater Science meetings from 1997 to 2019. Does not include any data on talks for 2013-2014 because they were not published during those years.",4
Restart Data Workbench Motivation Survey,"The Restart Data Workbench motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting soil pollution through waste reduction/management in the Restart Data Workbench pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/restart-data-workbench/.The Restart Data Workbench motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation","['setwd(""restart-data-workbench"")\r\n\r\n## DATA PREPARATION\r\nraw.data<- read.csv(""restart-data-workbench-results.csv"", header=T)\r\n\r\n# Keep only completed surveys\r\nraw.data.unfinished<-raw.data[raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data.unfinished$user))\r\n\r\nraw.data<-raw.data[!raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data$user))\r\n\r\n# Keep only questions related to motivation \r\nmotivation.questions<-raw.data[raw.data$tag %in% c(""achievement"", ""conformity"", ""self-direction"", ""stimulation"", ""routine"", ""hedonism"", ""power"", ""belongingness"", ""benevolence"", ""universalism"", ""global motivation""),]\r\nunique(motivation.questions$tag)\r\n\r\n# Create unique id made usign tag+id.question to perform pivoting of the table  \r\nmotivation.questions<-cbind(motivation.questions, tag.question = paste0(motivation.questions$tag, motivation.questions$questionId))\r\n\r\n\r\n## --  pivoting table (one row for each user and one column for each question. As value the numerical value given as answer)\r\nlibrary(reshape2) \r\nlibrary(reshape) \r\n\r\nmatch.tag.question<-unique(motivation.questions[, c(""tag.question"", ""question"")])\r\n# Rows now represent users, each column is related to a tag.question. Value of each cell is taken from the ""value"" column of the original file (default behaviour of the function, if a ""value"" column doesn\'t exist in the original table you need to specify which column should be used).\r\npivot.motivation.questions<-cast(motivation.questions, user ~ tag.question , fun.aggregate = mean)\r\npivot.motivation.questions<-pivot.motivation.questions[ , !(names(pivot.motivation.questions) %in% c(""global motivation1492""))]\r\n\r\n\r\n# Modify question names to uniform analysis according to the survey-motivation-template\r\nnames(pivot.motivation.questions) <- c(""user"", ""achievement.1"", ""achievement.2"", ""belongingness.2"", ""belongingness.1"", \r\n  ""benevolence.1"", ""benevolence.2"",  ""conformity.2"", ""conformity.1"", ""global motivation"",\r\n  ""hedonism.2"", ""hedonism.1"", ""power.1"", ""power.2"", ""routine.2"", ""routine.1"",""self-direction.1"", \r\n  ""self-direction.2"", ""stimulation.1"", ""stimulation.2"", ""universalism.1"", ""universalism.2"")\r\n\r\nwrite.csv(pivot.motivation.questions, ""restart-data-workbench-pivot-questions.csv"", row.names = F)\r\n\r\n#--------------------------- ANALYSIS RESULTS BY QUESTION\r\n\r\n# Compute average and variance for each question \r\nans<- read.csv(""restart-data-workbench-pivot-questions.csv"", header=TRUE)\r\n\r\nmean.question<-round(sapply(ans[,c(2:22)], mean),2)\r\nvar.questions<-round(sapply(ans[,c(2:22)], var), 2)\r\n\r\ndf.questions<- data.frame(mean = mean.question, var = var.questions)\r\n\r\n\r\n#--------------------------- ANALYSIS RESULTS BY MOTIVATING FACTOR\r\n\r\n# Compute average and variance for each motivating factor \r\nans<- read.csv(""restart-data-workbench-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n\r\nfactors.summary= data.frame(factor= character(), mean=numeric(), var=numeric())\r\n\r\nall.factors<-c(""Achievement"", ""Belongingness"", ""Benevolence"", ""Conformity"", ""Hedonism"", ""Power"", ""Routine"", ""Self.direction"", ""Stimulation"", ""Universalism"", ""Global.motivation"")\r\n\r\nfor (k in all.factors){\r\n  f<-ans %>% select(starts_with(k)) \r\n  num.col<-ncol(f)\r\n  v.final=vector()\r\n  for(i in 1:num.col){\r\n    v<-as.vector(f[,i])\r\n    v.final<-c(v.final, v)\r\n  }\r\n  \r\n  f.mean<-round(mean(v.final), 2)\r\n  f.var<-round(var(v.final),2)\r\n  new.row<-data.frame(factor= k, mean=f.mean, var=f.var)\r\n  factors.summary<-rbind(factors.summary, new.row) # Add row to the dataframe\r\n  \r\n  rm(f, f.mean, f.var, new.row, v.final, v, num.col)\r\n  \r\n}\r\n\r\n\r\n#--------------------------- CORRELATION ANALYSIS\r\n\r\ncompleted<- read.csv(""restart-data-workbench-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n# average questions for each tag\r\n# average results for each tag\r\n\r\n#achievement\r\nach.subset<-completed %>% select(starts_with(""ach""))\r\ncompleted$ach <- rowMeans(ach.subset, na.rm = TRUE)\r\n\r\n#belongingness\r\nbel.subset<-completed %>% select(starts_with(""bel""))\r\ncompleted$bel <- rowMeans(bel.subset, na.rm = TRUE)\r\n\r\n#benevolence\r\nben.subset<-completed %>% select(starts_with(""ben""))\r\ncompleted$ben <- rowMeans(ben.subset, na.rm = TRUE)\r\n\r\n#conformity\r\nconf.subset<-completed %>% select(starts_with(""conf""))\r\ncompleted$conf <- rowMeans(conf.subset, na.rm = TRUE)\r\n\r\n#hedonism\r\nhed.subset<-completed %>% select(starts_with(""hed""))\r\ncompleted$hed <- rowMeans(hed.subset, na.rm = TRUE)\r\n\r\n#power\r\npwr.subset<-completed %>% select(starts_with(""pow""))\r\ncompleted$pwr <- rowMeans(pwr.subset, na.rm = TRUE)\r\n\r\n#routine\r\nrout.subset<-completed %>% select(starts_with(""rout""))\r\ncompleted$rout <- rowMeans(rout.subset, na.rm = TRUE)\r\n\r\n#self-direction\r\nself.subset<-completed %>% select(starts_with(""self""))\r\ncompleted$self <- rowMeans(self.subset, na.rm = TRUE)\r\n\r\n#stimulation\r\nstim.subset<-completed %>% select(starts_with(""stim""))\r\ncompleted$stim <- rowMeans(stim.subset, na.rm = TRUE)\r\n\r\n#universalism\r\nuniv.subset<-completed %>% select(starts_with(""univ""))\r\ncompleted$univ <- rowMeans(univ.subset, na.rm = TRUE)\r\n\r\n\r\n# Correlation between motivating factors and global motivation\r\nd.cor<-completed\r\nglobal.corr <- data.frame(var=c(),corr=c(),pv=c(),sign=c(), global.motivation= c())\r\nfor(i in 23:32){ # Change considering dataframe size\r\n  t <- cor.test(d.cor[,i], d.cor$global.motivation)\r\n  nome <- names(d.cor)[i]\r\n  corr <- t$estimate\r\n  pv <- t$p.value\r\n  sign <- ifelse(pv<0.001, ""***"", ifelse(pv<0.01, ""**"", ifelse(pv<0.05, ""*"", """")))\r\n  global.motivation<- paste0(round(corr,3),sign)\r\n  global.corr <- rbind(global.corr, data.frame(nome, corr, pv, sign, global.motivation))\r\n}']","Restart Data Workbench Motivation Survey The Restart Data Workbench motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting soil pollution through waste reduction/management in the Restart Data Workbench pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/restart-data-workbench/.The Restart Data Workbench motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation",4
Wow Nature Motivation Survey,"The Wow Nature motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting air pollution in the Wow Nature pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/wow-nature/.The Wow Nature motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation","['setwd(""wow-nature"")\r\n\r\n## DATA PREPARATION\r\nraw.data<- read.csv(""wow-nature-results.csv"", header=T)\r\n\r\n# Keep only completed surveys\r\nraw.data.unfinished<-raw.data[raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data.unfinished$user))\r\n\r\nraw.data<-raw.data[!raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data$user))\r\n\r\n# Keep only questions related to motivation \r\nmotivation.questions<-raw.data[raw.data$tag %in% c(""achievement"", ""conformity"", ""self-direction"", ""stimulation"", ""routine"", ""hedonism"", ""power"", ""belongingness"", ""benevolence"", ""universalism"", ""global motivation""),]\r\nunique(motivation.questions$tag)\r\n\r\n# Create unique id made usign tag+id.question to perform pivoting of the table  \r\nmotivation.questions<-cbind(motivation.questions, tag.question = paste0(motivation.questions$tag, motivation.questions$questionId))\r\n\r\n\r\n## --  pivoting table (one row for each user and one column for each question. As value the numerical value given as answer)\r\nlibrary(reshape2) \r\nlibrary(reshape) \r\n\r\nmatch.tag.question<-unique(motivation.questions[, c(""tag.question"", ""question"")])\r\n# Rows now represent users, each column is related to a tag.question. Value of each cell is taken from the ""value"" column of the original file (default behaviour of the function, if a ""value"" column doesn\'t exist in the original table you need to specify which column should be used).\r\npivot.motivation.questions<-cast(motivation.questions, user ~ tag.question , fun.aggregate = mean)\r\npivot.motivation.questions<-pivot.motivation.questions[ , !(names(pivot.motivation.questions) %in% c(""global motivation3906""))]\r\n\r\n\r\n# Modify question names to uniform analysis according to the survey-motivation-template\r\nnames(pivot.motivation.questions) <- c( ""user"", ""achievement.1"", ""achievement.2"", ""belongingness.1"", ""belongingness.2"", ""benevolence.1"", ""benevolence.2"", ""benevolence.3"", \r\n        ""conformity.2"", ""conformity.1"", ""global motivation"", ""hedonism.2"", ""hedonism.1"", ""power.1"", ""power.2"", ""routine.1"", ""routine.2"", \r\n        ""self-direction.1"", ""self-direction.2"", ""stimulation.1"", ""stimulation.2"", ""universalism.1"", ""universalism.2"" )\r\n\r\nwrite.csv(pivot.motivation.questions, ""wow-nature-pivot-questions.csv"", row.names = F)\r\n\r\n#--------------------------- ANALYSIS RESULTS BY QUESTION\r\n\r\n# Compute average and variance for each question \r\nans<- read.csv(""wow-nature-pivot-questions.csv"", header=TRUE)\r\n\r\nmean.question<-round(sapply(ans[,c(2:23)], mean), 2)\r\nvar.questions<-round(sapply(ans[,c(2:23)], var), 2)\r\n\r\ndf.questions<- data.frame(mean = mean.question, var = var.questions)\r\n\r\n\r\n#--------------------------- ANALYSIS RESULTS BY MOTIVATING FACTOR\r\n\r\n# Compute average and variance for each motivating factor \r\nans<- read.csv(""wow-nature-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n\r\nfactors.summary= data.frame(factor= character(), mean=numeric(), var=numeric())\r\n\r\nall.factors<-c(""Achievement"", ""Belongingness"", ""Benevolence"", ""Conformity"", ""Hedonism"", ""Power"", ""Routine"", ""Self.direction"", ""Stimulation"", ""Universalism"", ""Global.motivation"")\r\n\r\nfor (k in all.factors){\r\n  f<-ans %>% select(starts_with(k)) \r\n  num.col<-ncol(f)\r\n  v.final=vector()\r\n  for(i in 1:num.col){\r\n    v<-as.vector(f[,i])\r\n    v.final<-c(v.final, v)\r\n  }\r\n  \r\n  f.mean<-round(mean(v.final), 2)\r\n  f.var<-round(var(v.final),2)\r\n  new.row<-data.frame(factor= k, mean=f.mean, var=f.var)\r\n  factors.summary<-rbind(factors.summary, new.row) # Add row to the dataframe\r\n  \r\n  rm(f, f.mean, f.var, new.row, v.final, v, num.col)\r\n  \r\n}\r\n\r\n\r\n#--------------------------- CORRELATION ANALYSIS\r\n\r\ncompleted<- read.csv(""wow-nature-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n# average questions for each tag\r\n# average results for each tag\r\n\r\n#achievement\r\nach.subset<-completed %>% select(starts_with(""ach""))\r\ncompleted$ach <- rowMeans(ach.subset, na.rm = TRUE)\r\n\r\n#belongingness\r\nbel.subset<-completed %>% select(starts_with(""bel""))\r\ncompleted$bel <- rowMeans(bel.subset, na.rm = TRUE)\r\n\r\n#benevolence\r\nben.subset<-completed %>% select(starts_with(""ben""))\r\ncompleted$ben <- rowMeans(ben.subset, na.rm = TRUE)\r\n\r\n#conformity\r\nconf.subset<-completed %>% select(starts_with(""conf""))\r\ncompleted$conf <- rowMeans(conf.subset, na.rm = TRUE)\r\n\r\n#hedonism\r\nhed.subset<-completed %>% select(starts_with(""hed""))\r\ncompleted$hed <- rowMeans(hed.subset, na.rm = TRUE)\r\n\r\n#power\r\npwr.subset<-completed %>% select(starts_with(""pow""))\r\ncompleted$pwr <- rowMeans(pwr.subset, na.rm = TRUE)\r\n\r\n#routine\r\nrout.subset<-completed %>% select(starts_with(""rout""))\r\ncompleted$rout <- rowMeans(rout.subset, na.rm = TRUE)\r\n\r\n#self-direction\r\nself.subset<-completed %>% select(starts_with(""self""))\r\ncompleted$self <- rowMeans(self.subset, na.rm = TRUE)\r\n\r\n#stimulation\r\nstim.subset<-completed %>% select(starts_with(""stim""))\r\ncompleted$stim <- rowMeans(stim.subset, na.rm = TRUE)\r\n\r\n#universalism\r\nuniv.subset<-completed %>% select(starts_with(""univ""))\r\ncompleted$univ <- rowMeans(univ.subset, na.rm = TRUE)\r\n\r\n\r\n# Correlation between motivating factors and global motivation\r\nd.cor<-completed\r\nglobal.corr <- data.frame(var=c(),corr=c(),pv=c(),sign=c(), global.motivation= c())\r\nfor(i in 24:33){ # Change considering dataframe size\r\n  t <- cor.test(d.cor[,i], d.cor$global.motivation)\r\n  nome <- names(d.cor)[i]\r\n  corr <- t$estimate\r\n  pv <- t$p.value\r\n  sign <- ifelse(pv<0.001, ""***"", ifelse(pv<0.01, ""**"", ifelse(pv<0.05, ""*"", """")))\r\n  global.motivation<- paste0(round(corr,3),sign)\r\n  global.corr <- rbind(global.corr, data.frame(nome, corr, pv, sign, global.motivation))\r\n}']","Wow Nature Motivation Survey The Wow Nature motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting air pollution in the Wow Nature pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/wow-nature/.The Wow Nature motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation",4
Open Soil Atlas Motivation Survey,"The Open Soil Atlas motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting soil pollution in the Open Soil Atlas pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/open-soil-atlas/.The Open Soil Atlas motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation","['setwd(""open-soil-atlas"")\r\n\r\n## DATA PREPARATION\r\nraw.data<- read.csv(""open-soil-atlas-results.csv"", header=T)\r\n\r\n# Keep only completed surveys\r\nraw.data.unfinished<-raw.data[raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data.unfinished$user))\r\n\r\nraw.data<-raw.data[!raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data$user))\r\n\r\n# Keep only questions related to motivation \r\nmotivation.questions<-raw.data[raw.data$tag %in% c(""achievement"", ""conformity"", ""self-direction"", ""stimulation"", ""routine"", ""hedonism"", ""power"", ""belongingness"", ""benevolence"", ""universalism"", ""global motivation""),]\r\nunique(motivation.questions$tag)\r\n\r\n# Create unique id made usign tag+id.question to perform pivoting of the table  \r\nmotivation.questions<-cbind(motivation.questions, tag.question = paste0(motivation.questions$tag, motivation.questions$questionId))\r\n\r\n\r\n## --  pivoting table (one row for each user and one column for each question. As value the numerical value given as answer)\r\nlibrary(reshape2) \r\nlibrary(reshape) \r\n\r\nmatch.tag.question<-unique(motivation.questions[, c(""tag.question"", ""question"")])\r\n# Rows now represent users, each column is related to a tag.question. Value of each cell is taken from the ""value"" column of the original file (default behaviour of the function, if a ""value"" column doesn\'t exist in the original table you need to specify which column should be used).\r\npivot.motivation.questions<-cast(motivation.questions, user ~ tag.question , fun.aggregate = mean)\r\npivot.motivation.questions<-pivot.motivation.questions[ , !(names(pivot.motivation.questions) %in% c(""global motivation1188""))]\r\n\r\n\r\n# Modify question names to uniform analysis according to the survey-motivation-template\r\nnames(pivot.motivation.questions) <- c(""user"", ""achievement.2"", ""achievement.1"", ""belongingness.1"", ""benevolence.1"", ""benevolence.2"", ""conformity.1"", \r\n""global motivation"", ""hedonism.1"", ""power.2"", ""routine.1"", ""self-direction.1"", ""self-direction.2"", ""stimulation.1"", \r\n""stimulation.2"", ""universalism.1"", ""universalism.2"")\r\n\r\nwrite.csv(pivot.motivation.questions, ""open-soil-atlas-pivot-questions.csv"", row.names = F)\r\n\r\n#--------------------------- ANALYSIS RESULTS BY QUESTION\r\n\r\n# Compute average and variance for each question \r\nans<- read.csv(""open-soil-atlas-pivot-questions.csv"", header=TRUE)\r\n\r\nmean.question<-round(sapply(ans[,c(2:17)], mean),2)\r\nvar.questions<-round(sapply(ans[,c(2:17)], var), 2)\r\n\r\ndf.questions<- data.frame(mean = mean.question, var = var.questions)\r\n\r\n\r\n#--------------------------- ANALYSIS RESULTS BY MOTIVATING FACTOR\r\n\r\n# Compute average and variance for each motivating factor \r\nans<- read.csv(""open-soil-atlas-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n\r\nfactors.summary= data.frame(factor= character(), mean=numeric(), var=numeric())\r\n\r\nall.factors<-c(""Achievement"", ""Belongingness"", ""Benevolence"", ""Conformity"", ""Hedonism"", ""Power"", ""Routine"", ""Self.direction"", ""Stimulation"", ""Universalism"", ""Global.motivation"")\r\n\r\nfor (k in all.factors){\r\n  f<-ans %>% select(starts_with(k)) \r\n  num.col<-ncol(f)\r\n  v.final=vector()\r\n  for(i in 1:num.col){\r\n    v<-as.vector(f[,i])\r\n    v.final<-c(v.final, v)\r\n  }\r\n  \r\n  f.mean<-round(mean(v.final), 2)\r\n  f.var<-round(var(v.final),2)\r\n  new.row<-data.frame(factor= k, mean=f.mean, var=f.var)\r\n  factors.summary<-rbind(factors.summary, new.row) # Add row to the dataframe\r\n  \r\n  rm(f, f.mean, f.var, new.row, v.final, v, num.col)\r\n  \r\n}\r\n\r\n\r\n#--------------------------- CORRELATION ANALYSIS\r\n\r\ncompleted<- read.csv(""open-soil-atlas-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n# average questions for each tag\r\n# average results for each tag\r\n\r\n#achievement\r\nach.subset<-completed %>% select(starts_with(""ach""))\r\ncompleted$ach <- rowMeans(ach.subset, na.rm = TRUE)\r\n\r\n#belongingness\r\nbel.subset<-completed %>% select(starts_with(""bel""))\r\ncompleted$bel <- rowMeans(bel.subset, na.rm = TRUE)\r\n\r\n#benevolence\r\nben.subset<-completed %>% select(starts_with(""ben""))\r\ncompleted$ben <- rowMeans(ben.subset, na.rm = TRUE)\r\n\r\n#conformity\r\nconf.subset<-completed %>% select(starts_with(""conf""))\r\ncompleted$conf <- rowMeans(conf.subset, na.rm = TRUE)\r\n\r\n#hedonism\r\nhed.subset<-completed %>% select(starts_with(""hed""))\r\ncompleted$hed <- rowMeans(hed.subset, na.rm = TRUE)\r\n\r\n#power\r\npwr.subset<-completed %>% select(starts_with(""pow""))\r\ncompleted$pwr <- rowMeans(pwr.subset, na.rm = TRUE)\r\n\r\n#routine\r\nrout.subset<-completed %>% select(starts_with(""rout""))\r\ncompleted$rout <- rowMeans(rout.subset, na.rm = TRUE)\r\n\r\n#self-direction\r\nself.subset<-completed %>% select(starts_with(""self""))\r\ncompleted$self <- rowMeans(self.subset, na.rm = TRUE)\r\n\r\n#stimulation\r\nstim.subset<-completed %>% select(starts_with(""stim""))\r\ncompleted$stim <- rowMeans(stim.subset, na.rm = TRUE)\r\n\r\n#universalism\r\nuniv.subset<-completed %>% select(starts_with(""univ""))\r\ncompleted$univ <- rowMeans(univ.subset, na.rm = TRUE)\r\n\r\n\r\n# Correlation between motivating factors and global motivation\r\nd.cor<-completed\r\nglobal.corr <- data.frame(var=c(),corr=c(),pv=c(),sign=c(), global.motivation= c())\r\nfor(i in 18:27){ # Change considering dataframe size\r\n  t <- cor.test(d.cor[,i], d.cor$global.motivation)\r\n  nome <- names(d.cor)[i]\r\n  corr <- t$estimate\r\n  pv <- t$p.value\r\n  sign <- ifelse(pv<0.001, ""***"", ifelse(pv<0.01, ""**"", ifelse(pv<0.05, ""*"", """")))\r\n  global.motivation<- paste0(round(corr,3),sign)\r\n  global.corr <- rbind(global.corr, data.frame(nome, corr, pv, sign, global.motivation))\r\n}\r\n']","Open Soil Atlas Motivation Survey The Open Soil Atlas motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting soil pollution in the Open Soil Atlas pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/open-soil-atlas/.The Open Soil Atlas motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation",4
Walk Up Aniene Motivation Survey,"The Walk Up Aniene motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting soil and water pollution in the Walk Up Aniene pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/walk-up-aniene/.The Walk Up Aniene motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation","['setwd(""walk-up-aniene"")\r\n\r\n## DATA PREPARATION\r\nraw.data<- read.csv(""walk-up-aniene-results.csv"", header=T)\r\n\r\n# Keep only completed surveys\r\nraw.data.unfinished<-raw.data[raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data.unfinished$user))\r\n\r\nraw.data<-raw.data[!raw.data$totalDuration == \'unfinished\', ]\r\n# length(unique(raw.data$user))\r\n\r\n# Keep only questions related to motivation \r\nmotivation.questions<-raw.data[raw.data$tag %in% c(""achievement"", ""conformity"", ""self-direction"", ""stimulation"", ""routine"", ""hedonism"", ""power"", ""belongingness"", ""benevolence"", ""universalism"", ""global motivation""),]\r\nunique(motivation.questions$tag)\r\n\r\n# Create unique id made usign tag+id.question to perform pivoting of the table  \r\nmotivation.questions<-cbind(motivation.questions, tag.question = paste0(motivation.questions$tag, motivation.questions$questionId))\r\n\r\n\r\n## --  pivoting table (one row for each user and one column for each question. As value the numerical value given as answer)\r\nlibrary(reshape2) \r\nlibrary(reshape) \r\n\r\nmatch.tag.question<-unique(motivation.questions[, c(""tag.question"", ""question"")])\r\n# Rows now represent users, each column is related to a tag.question. Value of each cell is taken from the ""value"" column of the original file (default behaviour of the function, if a ""value"" column doesn\'t exist in the original table you need to specify which column should be used).\r\npivot.motivation.questions<-cast(motivation.questions, user ~ tag.question , fun.aggregate = mean)\r\npivot.motivation.questions<-pivot.motivation.questions[ , !(names(pivot.motivation.questions) %in% c(""global motivation739""))]\r\n\r\n\r\n# Modify question names to uniform analysis according to the survey-motivation-template\r\nnames(pivot.motivation.questions) <- c(""user"", ""achievement.1"", ""achievement.2"", ""achievement.3"", ""belongingness.1"", ""belongingness.2"", \r\n  ""benevolence.2"", ""benevolence.1"", ""conformity.2"", ""conformity.1"", ""global motivation"", ""hedonism.2"", ""hedonism.1"",\r\n  ""power.2"", ""power.1"", ""routine.1"", ""routine.2"", ""self-direction.2.1"", ""self-direction.2.2"", ""self-direction.1"", \r\n  ""self-direction.2.3"", ""self-direction.2.4"", ""stimulation.2"", ""stimulation.1"", ""universalism.1"", ""universalism.2"")\r\n\r\nwrite.csv(pivot.motivation.questions, ""walk-up-aniene-pivot-questions.csv"", row.names = F)\r\n\r\n#--------------------------- ANALYSIS RESULTS BY QUESTION\r\n\r\n# Compute average and variance for each question \r\nans<- read.csv(""walk-up-aniene-pivot-questions.csv"", header=TRUE)\r\n\r\nmean.question<-round(sapply(ans[,c(2:26)], mean),2)\r\nvar.questions<-round(sapply(ans[,c(2:26)], var), 2)\r\n\r\ndf.questions<- data.frame(mean = mean.question, var = var.questions)\r\n\r\n\r\n#--------------------------- ANALYSIS RESULTS BY MOTIVATING FACTOR\r\n\r\n# Compute average and variance for each motivating factor \r\nans<- read.csv(""walk-up-aniene-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n\r\nfactors.summary= data.frame(factor= character(), mean=numeric(), var=numeric())\r\n\r\nall.factors<-c(""Achievement"", ""Belongingness"", ""Benevolence"", ""Conformity"", ""Hedonism"", ""Power"", ""Routine"", ""Self.direction"", ""Stimulation"", ""Universalism"", ""Global.motivation"")\r\n\r\nfor (k in all.factors){\r\n  f<-ans %>% select(starts_with(k)) \r\n  num.col<-ncol(f)\r\n  v.final=vector()\r\n  for(i in 1:num.col){\r\n    v<-as.vector(f[,i])\r\n    v.final<-c(v.final, v)\r\n  }\r\n  \r\n  f.mean<-round(mean(v.final), 2)\r\n  f.var<-round(var(v.final),2)\r\n  new.row<-data.frame(factor= k, mean=f.mean, var=f.var)\r\n  factors.summary<-rbind(factors.summary, new.row) # Add row to the dataframe\r\n  \r\n  rm(f, f.mean, f.var, new.row, v.final, v, num.col)\r\n  \r\n}\r\n\r\n\r\n#--------------------------- CORRELATION ANALYSIS\r\n\r\ncompleted<- read.csv(""walk-up-aniene-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n# average questions for each tag\r\n# average results for each tag\r\n\r\n#achievement\r\nach.subset<-completed %>% select(starts_with(""ach""))\r\ncompleted$ach <- rowMeans(ach.subset, na.rm = TRUE)\r\n\r\n#belongingness\r\nbel.subset<-completed %>% select(starts_with(""bel""))\r\ncompleted$bel <- rowMeans(bel.subset, na.rm = TRUE)\r\n\r\n#benevolence\r\nben.subset<-completed %>% select(starts_with(""ben""))\r\ncompleted$ben <- rowMeans(ben.subset, na.rm = TRUE)\r\n\r\n#conformity\r\nconf.subset<-completed %>% select(starts_with(""conf""))\r\ncompleted$conf <- rowMeans(conf.subset, na.rm = TRUE)\r\n\r\n#hedonism\r\nhed.subset<-completed %>% select(starts_with(""hed""))\r\ncompleted$hed <- rowMeans(hed.subset, na.rm = TRUE)\r\n\r\n#power\r\npwr.subset<-completed %>% select(starts_with(""pow""))\r\ncompleted$pwr <- rowMeans(pwr.subset, na.rm = TRUE)\r\n\r\n#routine\r\nrout.subset<-completed %>% select(starts_with(""rout""))\r\ncompleted$rout <- rowMeans(rout.subset, na.rm = TRUE)\r\n\r\n#self-direction\r\nself.subset<-completed %>% select(starts_with(""self""))\r\ncompleted$self <- rowMeans(self.subset, na.rm = TRUE)\r\n\r\n#stimulation\r\nstim.subset<-completed %>% select(starts_with(""stim""))\r\ncompleted$stim <- rowMeans(stim.subset, na.rm = TRUE)\r\n\r\n#universalism\r\nuniv.subset<-completed %>% select(starts_with(""univ""))\r\ncompleted$univ <- rowMeans(univ.subset, na.rm = TRUE)\r\n\r\n\r\n# Correlation between motivating factors and global motivation\r\nd.cor<-completed\r\nglobal.corr <- data.frame(var=c(),corr=c(),pv=c(),sign=c(), global.motivation= c())\r\nfor(i in 27:36){ # Change considering dataframe size\r\n  t <- cor.test(d.cor[,i], d.cor$global.motivation)\r\n  nome <- names(d.cor)[i]\r\n  corr <- t$estimate\r\n  pv <- t$p.value\r\n  sign <- ifelse(pv<0.001, ""***"", ifelse(pv<0.01, ""**"", ifelse(pv<0.05, ""*"", """")))\r\n  global.motivation<- paste0(round(corr,3),sign)\r\n  global.corr <- rbind(global.corr, data.frame(nome, corr, pv, sign, global.motivation))\r\n}']","Walk Up Aniene Motivation Survey The Walk Up Aniene motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting soil and water pollution in the Walk Up Aniene pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/walk-up-aniene/.The Walk Up Aniene motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed and administered using the Coney toolkit.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation",4
"Past, present and future of chamois science","The chamois Rupicapra spp. is the most abundant mountain ungulate of Europe and the Near East, where it occurs as two species, the Northern chamois R. rupicapra and the Southern chamois R. pyrenaica. Here, we provide a state-of-the-art overview of research trends and the most challenging issues in chamois research and conservation, focusing on taxonomy and systematics, genetics, life history, ecology and behavior, physiology and disease, management, and conservation. Research on Rupicapra has a longstanding history and has contributed substantially to the biological and ecological knowledge of mountain ungulates. Although the number of publications on this genus has markedly increased over the past two decades, major differences persist with respect to knowledge of species and subspecies, with research mostly focusing on the Alpine chamois R. r. rupicapra and, to a lesser extent, the Pyrenean chamois R. p. pyrenaica. In addition, a scarcity of replicate studies of populations of different subspecies and/or geographic areas limits the advancement of chamois science. Since environmental heterogeneity impacts behavioral, physiological and life history traits, understanding the underlying processes would be of great value from both an evolutionary and conservation/management standpoint, especially in the light of ongoing climatic change. Substantial contributions to this challenge may derive from a quantitative assessment of reproductive success, investigation of fine-scale foraging patterns, and a mechanistic understanding of disease outbreak and resilience. Improving conservation status, resolving taxonomic disputes, identifying subspecies hybridization, assessing the impact of hunting and establishing reliable methods of abundance estimation are of primary concern. Despite being one of the most well-known mountain ungulates, substantial field efforts to collect paleontological, behavioral, ecological, morphological, physiological and genetic data on different populations and subspecies are still needed to ensure a successful future for chamois conservation and research.","['###############################################################################\n# ANALYSIS OF CHAMOIS SCIENCE\n# .R code for data analysis                                                   \n###############################################################################\n\n\ntrend_chamois <- read.csv2(file.choose())\ntrend_chamois\n\nlibrary(MuMIn)\nlibrary(DHARMa)\nlibrary(performance)\nlibrary(visreg)\n\nmod1 <- glm(Rupicapra ~ YEAR + Rupicapra_1, data=trend_chamois, family=poisson)\nmod2 <- glm(Rupicapra ~ poly(YEAR,2) + Rupicapra_1, data=trend_chamois, family=poisson)\nmod3 <- glm(Rupicapra ~ poly(YEAR,3) + Rupicapra_1, data=trend_chamois, family=poisson)\nmod4 <- glm(Rupicapra ~ poly(YEAR,4) + Rupicapra_1, data=trend_chamois, family=poisson)\nmod5 <- glm(Rupicapra ~ poly(YEAR,5) + Rupicapra_1, data=trend_chamois, family=poisson)\nmod6 <- glm(Rupicapra ~ poly(YEAR,6) + Rupicapra_1, data=trend_chamois, family=poisson)\nmodel.sel(mod2, mod3, mod4, mod5, mod6)\n\ncheck_autocorrelation(mod3) #OK!\n\nsim.mod3 <- simulateResiduals(mod3)\nplot(sim.mod3)\nsummary(mod3)\n\nvisreg::visreg(mod3, xvar=""YEAR"", scale=""response"")\n\npar(mfrow = c(1,1), mar=c(4.5,5,1,2), oma = c(0, 0, 0, 0))\n\nvisreg(mod3, xvar=""YEAR"", scale=""response"", # export 6x8 inches\n       rug=FALSE,\n       ylim = c(0,50),\n       overlay = F, \n       xlab=""Year"", \n       ylab=""Number of chamois publications"",\n       fill=list(col=grey(c(0.7), alpha=0.4)),\n       line=list(lty=1:3, col = ""black"", lwd = 1.5),\n       points=list(cex=1, pch=16, col = ""black""), # partial residuals\n       partial = FALSE,\n       cex.lab = 1.25)\nwith(trend_chamois, points(YEAR, Rupicapra, pch = 16, col = ""black"")) # real data\ntext(1990, 50, ""n = 160"", cex = 1, font = 2)\ntext(2010, 50, ""n = 596"", cex = 1, font = 2)\nabline(v=2000, col = ""black"", lty=2)\n\n\n# proportion of Caprinae papers\n\nmod0b <- lm(Prop ~ 1, data=trend_chamois)\nmod1b <- lm(Prop ~ YEAR, data=trend_chamois)\nmod2b <- lm(Prop ~ poly(YEAR,2), data=trend_chamois)\nmod3b <- lm(Prop ~ poly(YEAR,3), data=trend_chamois)\nmod4b <- lm(Prop ~ poly(YEAR,4), data=trend_chamois)\nmod5b <- lm(Prop ~ poly(YEAR,5), data=trend_chamois)\nmod6b <- lm(Prop ~ poly(YEAR,6), data=trend_chamois)\nmodel.sel(mod0b, mod1b, mod2b, mod3b, mod4b, mod5b, mod6b)\n\ncheck_autocorrelation(mod1b) #OK!\ncheck_heteroscedasticity(mod1b) #OK!\n\nsim.mod1b <- simulateResiduals(mod1b)\nplot(sim.mod1b)\nsummary(mod1b)\n\nvisreg::visreg(mod1b, scale=""response"")\n\npar(mfrow = c(1,1), mar=c(4.5,5,1,2), oma = c(0, 0, 0, 0))\n\nvisreg(mod1b, xvar=""YEAR"", scale=""response"", # export 6x8 inches\n       rug=FALSE,\n       ylim = c(0.00,0.30),\n       overlay = F, \n       xlab=""Year"", \n       ylab=""Proportion of chamois publications"",\n       fill=list(col=grey(c(0.7), alpha=0.4)),\n       line=list(lty=1:3, col = ""black"", lwd = 1.5),\n       points=list(cex=1, pch=16, col = ""black""), # partial residuals\n       partial = FALSE,\n       cex.lab = 1.25)\nwith(trend_chamois, points(YEAR, Prop, pch = 16, col = ""black"")) # real data\ntext(1990, 0.30, ""mean = 0.15"", cex = 1, font = 2)\ntext(2010, 0.30, ""mean = 0.17"", cex = 1, font = 2)\nabline(v=2000, col = ""black"", lty=2)\n']","Past, present and future of chamois science The chamois Rupicapra spp. is the most abundant mountain ungulate of Europe and the Near East, where it occurs as two species, the Northern chamois R. rupicapra and the Southern chamois R. pyrenaica. Here, we provide a state-of-the-art overview of research trends and the most challenging issues in chamois research and conservation, focusing on taxonomy and systematics, genetics, life history, ecology and behavior, physiology and disease, management, and conservation. Research on Rupicapra has a longstanding history and has contributed substantially to the biological and ecological knowledge of mountain ungulates. Although the number of publications on this genus has markedly increased over the past two decades, major differences persist with respect to knowledge of species and subspecies, with research mostly focusing on the Alpine chamois R. r. rupicapra and, to a lesser extent, the Pyrenean chamois R. p. pyrenaica. In addition, a scarcity of replicate studies of populations of different subspecies and/or geographic areas limits the advancement of chamois science. Since environmental heterogeneity impacts behavioral, physiological and life history traits, understanding the underlying processes would be of great value from both an evolutionary and conservation/management standpoint, especially in the light of ongoing climatic change. Substantial contributions to this challenge may derive from a quantitative assessment of reproductive success, investigation of fine-scale foraging patterns, and a mechanistic understanding of disease outbreak and resilience. Improving conservation status, resolving taxonomic disputes, identifying subspecies hybridization, assessing the impact of hunting and establishing reliable methods of abundance estimation are of primary concern. Despite being one of the most well-known mountain ungulates, substantial field efforts to collect paleontological, behavioral, ecological, morphological, physiological and genetic data on different populations and subspecies are still needed to ensure a successful future for chamois conservation and research.",4
Water Sentinels Motivation Survey,"The Water Sentinels motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting water pollution in the Water Sentinels pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/water-sentinels/.The Water Sentinels motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed using the Coney toolkit and administered using Google Forms.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-results-google-forms.csv contains the CSV of the collected answers exported from Google Forms*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation","['setwd(""water-sentinels"")\r\n\r\n\r\n##---------- PREPARAZIONE DATI\r\nraw.data<- read.csv(""water-sentinels-results.csv"", header=T)\r\nraw.data<-raw.data[1:4,8:28]\r\n\r\nraw.data.backup<-raw.data\r\n\r\nlibrary(stringr)\r\n# Keep only tag as header for the column\r\nnames(raw.data)<-str_replace(names(raw.data), ""\\\\.\\\\.\\\\..+"","""")\r\n\r\n# Substitute answers with their numeric value\r\nlibrary(plyr)\r\n#Stimulation.1\r\nlevels(raw.data$Stimulation.1)\r\nraw.data$Stimulation.1<- revalue(raw.data$Stimulation.1, c(""em parte""= 4, ""exactamente""= 5, ""no influenciou""=3) )\r\nraw.data$Stimulation.1<-as.numeric(as.character(raw.data$Stimulation.1))\r\n\r\n#Stimulation.2\r\nlevels(raw.data$Stimulation.2)\r\nraw.data$Stimulation.2<- revalue(raw.data$Stimulation.2, c(""em parte""= 4, ""exactamente""= 5) )\r\nraw.data$Stimulation.2<-as.numeric(as.character(raw.data$Stimulation.2))\r\n\r\n#Routine.1\r\nlevels(raw.data$Routine.1)\r\nraw.data$Routine.1<- revalue(raw.data$Routine.1, c(""nunca""= 1) )\r\nraw.data$Routine.1<-as.numeric(as.character(raw.data$Routine.1))\r\n\r\n#Achievement.1\r\nlevels(raw.data$Achievement.1)\r\nraw.data$Achievement.1<- revalue(raw.data$Achievement.1, c(""sim"" = 5, ""sim, um pouco""=4) )\r\nraw.data$Achievement.1<-as.numeric(as.character(raw.data$Achievement.1))\r\n\r\n#Achievement.2\r\nlevels(raw.data$Achievement.2)\r\nraw.data$Achievement.2<- revalue(raw.data$Achievement.2, c(""sim"" = 5) )\r\nraw.data$Achievement.2<-as.numeric(as.character(raw.data$Achievement.2))\r\n\r\n#Power.1\r\nlevels(raw.data$Power.1)\r\nraw.data$Power.1<- revalue(raw.data$Power.1, c(""no muito"" = 2,""no, de todo"" = 1, ""sim""= 5, ""sim, um pouco"" = 4 ) )\r\nraw.data$Power.1<-as.numeric(as.character(raw.data$Power.1))\r\n\r\n#Power.2\r\nlevels(raw.data$Power.2)\r\nraw.data$Power.2<- revalue(raw.data$Power.2, c(""nada"" = 1) )\r\nraw.data$Power.2<-as.numeric(as.character(raw.data$Power.2))\r\n\r\n#Belongingness.1\r\nlevels(raw.data$Belongingness.1)\r\nraw.data$Belongingness.1<- revalue(raw.data$Belongingness.1, c(""muito influenciada"" = 5 , ""no, de todo"" = 1, ""neutro"" = 3))\r\nraw.data$Belongingness.1<-as.numeric(as.character(raw.data$Belongingness.1))\r\n\r\n\r\n#Belongingness.2\r\nlevels(raw.data$Belongingness.2)\r\nraw.data$Belongingness.2<- revalue(raw.data$Belongingness.2, c(""sim"" = 5))\r\nraw.data$Belongingness.2<-as.numeric(as.character(raw.data$Belongingness.2))\r\n\r\n#Conformity.1\r\nlevels(raw.data$Conformity.1)\r\nraw.data$Conformity.1<- revalue(raw.data$Conformity.1, c(""algumas pessoas"" = 4, ""poucos participantes"" = 3))\r\nraw.data$Conformity.1<-as.numeric(as.character(raw.data$Conformity.1))\r\n\r\n#Benevolence.2\r\nlevels(raw.data$Benevolence.2)\r\nraw.data$Benevolence.2<- revalue(raw.data$Benevolence.2, c(""Sim, definitivamente"" = 5, ""Sim, principalmente por isso"" =4))\r\nraw.data$Benevolence.1<-as.numeric(as.character(raw.data$Benevolence.1))\r\n\r\n#Universalism.1\r\nlevels(raw.data$Universalism.1)\r\nraw.data$Universalism.1<- revalue(raw.data$Universalism.1, c(""Sim, definitivamente"" = 5))\r\nraw.data$Universalism.1<-as.numeric(as.character(raw.data$Universalism.1))\r\n\r\nwrite.csv(raw.data, ""water-sentinels-pivot-questions.csv"", row.names = F)\r\n\r\n\r\n#--------------------------- ANALYSIS RESULTS BY QUESTION\r\n\r\n# Compute average and variance for each question \r\nans<- read.csv(""water-sentinels-pivot-questions.csv"", header=TRUE)\r\n\r\nmean.question<-round(sapply(ans[,c(1:21)], mean),2)\r\nvar.questions<- round(sapply(ans[,c(1:21)], var), 2)\r\n\r\ndf.questions<- data.frame(mean= mean.question, var = var.questions)\r\n\r\n\r\n#--------------------------- ANALYSIS RESULTS BY MOTIVATING FACTOR\r\n\r\n# Compute average and variance for each motivating factor \r\nans<- read.csv(""water-sentinels-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n\r\nfactors.summary= data.frame(factor= character(), mean=numeric(), var=numeric())\r\n\r\nall.factors<-c(""Achievement"", ""Belongingness"", ""Benevolence"", ""Conformity"", ""Hedonism"", ""Power"", ""Routine"", ""Self.direction"", ""Stimulation"", ""Universalism"", ""Global.motivation"")\r\n\r\nfor (k in all.factors){\r\n  f<-ans %>% select(starts_with(k)) \r\n  num.col<-ncol(f)\r\n  v.final=vector()\r\n  for(i in 1:num.col){\r\n    v<-as.vector(f[,i])\r\n    v.final<-c(v.final, v)\r\n  }\r\n  \r\n  f.mean<-round(mean(v.final), 2)\r\n  f.var<-round(var(v.final),2)\r\n  new.row<-data.frame(factor= k, mean=f.mean, var=f.var)\r\n  factors.summary<-rbind(factors.summary, new.row) #aggiunta di riga al dataframe\r\n  \r\n  rm(f, f.mean, f.var, new.row, v.final, v, num.col)\r\n  \r\n}\r\n\r\n\r\n#--------------------------- CORRELATION ANALYSIS\r\n\r\ncompleted<- read.csv(""water-sentinels-pivot-questions.csv"", header=TRUE)\r\n\r\nlibrary(dplyr)\r\n# average questions for each tag\r\n# average results for each tag\r\n\r\n#achievement\r\nach.subset<-completed %>% select(starts_with(""ach""))\r\ncompleted$ach <- rowMeans(ach.subset, na.rm = TRUE)\r\n\r\n#belongingness\r\nbel.subset<-completed %>% select(starts_with(""bel""))\r\ncompleted$bel <- rowMeans(bel.subset, na.rm = TRUE)\r\n\r\n#benevolence\r\nben.subset<-completed %>% select(starts_with(""ben""))\r\ncompleted$ben <- rowMeans(ben.subset, na.rm = TRUE)\r\n\r\n#conformity\r\nconf.subset<-completed %>% select(starts_with(""conf""))\r\ncompleted$conf <- rowMeans(conf.subset, na.rm = TRUE)\r\n\r\n#hedonism\r\nhed.subset<-completed %>% select(starts_with(""hed""))\r\ncompleted$hed <- rowMeans(hed.subset, na.rm = TRUE)\r\n\r\n#power\r\npwr.subset<-completed %>% select(starts_with(""pow""))\r\ncompleted$pwr <- rowMeans(pwr.subset, na.rm = TRUE)\r\n\r\n#routine\r\nrout.subset<-completed %>% select(starts_with(""rout""))\r\ncompleted$rout <- rowMeans(rout.subset, na.rm = TRUE)\r\n\r\n#self-direction\r\nself.subset<-completed %>% select(starts_with(""self""))\r\ncompleted$self <- rowMeans(self.subset, na.rm = TRUE)\r\n\r\n#stimulation\r\nstim.subset<-completed %>% select(starts_with(""stim""))\r\ncompleted$stim <- rowMeans(stim.subset, na.rm = TRUE)\r\n\r\n#universalism\r\nuniv.subset<-completed %>% select(starts_with(""univ""))\r\ncompleted$univ <- rowMeans(univ.subset, na.rm = TRUE)\r\n\r\n\r\n# Correlation between motivating factors and global motivation\r\nd.cor<-completed\r\nglobal.corr <- data.frame(var=c(),corr=c(),pv=c(),sign=c(), global.motivation= c())\r\nfor(i in 22:31){ # Change considering dataframe size\r\n  t <- cor.test(d.cor[,i], d.cor$Global.motivation)\r\n  nome <- names(d.cor)[i]\r\n  corr <- t$estimate\r\n  pv <- t$p.value\r\n  sign <- ifelse(pv<0.001, ""***"", ifelse(pv<0.01, ""**"", ifelse(pv<0.05, ""*"", """")))\r\n  global.motivation<- paste0(round(corr,3),sign)\r\n  global.corr <- rbind(global.corr, data.frame(nome, corr, pv, sign, global.motivation))\r\n}\r\n### Warning! Standard Dev is zero because global motivation is 5 for each survey completion']","Water Sentinels Motivation Survey The Water Sentinels motivation study was conducted within the ongoing H2020 project named ACTION (pArticipatory sCience Toolkit agaInst pollutiON) on citizen science. Volunteers participate to citizen science initiatives for multiple reasons: personal enjoyment, desire for improvement or achievement, establishment of personal relationships, care for the environment, etc.Studying motivation and investigating the factors influencing people participation to citizen science projects is an essential aspect in the analysis of citizen science communities. Understanding the reasons that foster people to engage can support the successful design and implementation of effective participant involvement tasks, as well as pave the way for long-term engagement.The goal of the study is to analyse the motivation to participate of a specific citizen science community focused on fighting water pollution in the Water Sentinels pilot supported by the ACTION project. More info on the pilot available at https://actionproject.eu/citizen-science-pilots/water-sentinels/.The Water Sentinels motivation study is part of the study about motivation in citizen science projects conducted within the ACTION project (https://doi.org/10.5281/zenodo.5753092). The survey was designed using the Coney toolkit and administered using Google Forms.The research object adopts the RO-Crate specification. Files made available within the research object are:*-procedure.ttl contains the RDF representation of the structure of the conversational survey (questions, answers, etc.) using the Survey Ontology*-results.ttl contains the RDF representation of the answers collected using the Survey Ontology*-survey.tll contains a comprehensive RDF representation of the survey data using the Survey Ontology*-results.csv contains the CSV of the collected answers*-results-google-forms.csv contains the CSV of the collected answers exported from Google Forms*-script.R is the R script developed to analyse the collected answers*-mean-var-motivating-questions.csv contains the computed mean and average for each question considered (observable variables)*-mean-var-motivating-factor.csv contains the computed mean and average for each motivation factor considered (latent variables)*-correlation-factors-global-motivation.csv contains the correlation analysis between each motivation factor and the global motivation",4
Neural priming of adipose-derived stem cells by cell-imprinted substrates,Raw data and figure evaluation scripts for the manuscript Neural Priming of Adipose-Derived Stem Cells by Cell-Imprinted Substrates,,Neural priming of adipose-derived stem cells by cell-imprinted substrates Raw data and figure evaluation scripts for the manuscript Neural Priming of Adipose-Derived Stem Cells by Cell-Imprinted Substrates,4
Widespread reticulate evolution in an adaptive radiation,"A fundamental assumption of evolutionary biology is that phylogeny follows a bifurcating process. However, hybrid speciation and introgression are becoming more widely documented in many groups. Hybrid inference studies have been historically limited to small sets of taxa, while exploration of the prevalence and trends of reticulation at deep time scales remains unexplored. We study the evolutionary history of an adaptive radiation of 109 gemsnakes in Madagascar (Pseudoxyrhophiinae) to identify potential instances of introgression. Using several network inference methods, we find twelve reticulation events within the 22-million-year evolutionary history of gemsnakes, producing 28% of the diversity for the group, including one reticulation that resulted in the diversification of an 18 species radiation. These reticulations occur at nodes with high gene tree discordance. Hybridization events occurred between north-south distributed parentals that share similar ecologies. Younger hybrids occupy intermediate contact zones between the parentals, showing that post-speciation dispersal in this group has not eroded the spatial signatures of introgression. Reticulations accumulated consistently over time, despite drops in overall speciation rates during the Pleistocene. This suggests that while bifurcating speciation may decline as the result of species accumulation and environmental change, speciation by hybridization may be more robust to these processes.","['##Written by: Dylan DeBaun\nlibrary(\'MSCquartets\')\nlibrary(\'ape\')\nlibrary(\'gtools\')\nlibrary(\'stringr\')\n\nargs = commandArgs(trailingOnly=TRUE)\nout=args[1] #name of the group that we will be looking at\n\n#RUN NANUQ TO CALCULATE PROBABILITIES\n#check for if you already ran this part, if so, don\'t run it again\nif(!file.exists(paste0(out,""z"","".csv""))){\n#read in CF file (built in prep.R)\nu <- read.csv(paste0(out,""_fullindivCFs"","".csv""))\ncolnames(u)[dim(u)[2]-2] = ""12|34""\ncolnames(u)[dim(u)[2]-1] = ""13|24""\ncolnames(u)[dim(u)[2]] = ""14|23""\n}else{\n#read in previously made nanuq file\nu <- read.csv(paste0(out,""z"","".csv""))\ncolnames(u)[dim(u)[2]-4] = ""12|34""\ncolnames(u)[dim(u)[2]-3] = ""13|24""\ncolnames(u)[dim(u)[2]-2] = ""14|23""\n}\n\n#FOR PICKING CHOICE IN ALPHA/BETA\n#after running NANUQ for the first time, to make the output easier to read to choose alpha/beta, concatonate all individuals together (i.e. make it like species level matrix) \n#this assumes your indivdiuals are labelled as such GENUS_SPECIES_* or GENUS_SPECIES_CF_* (assuming the first 2 or 3 words are the species name)\nif(!file.exists(paste0(out,""zedited"","".csv""))){\n  newz <- c(rep(0,dim(z)[1]))\n  i=1\n  while(i != (dim(z)[2]-4)){\n    start = i\n    if(sapply(str_split(colnames(z)[i],""_""),length) == 5){\n      x = 3\n    }else{\n      x = 2\n    }\n    while(isTRUE(word(colnames(z)[i],x,x,sep=""_"")==word(colnames(z)[i+1],x,x,sep=""_""))){\n      i=i+1\n    }\n    end = i\n    sum = rep(0,dim(z)[1])\n    for(j in start:end){\n      sum = z[,j] + sum\n    }\n    newz<- cbind(newz,sum)\n    colnames(newz)[dim(newz)[2]] = word(colnames(z)[start],x,x,sep=""_"")\n    i=i+1\n  }\n  newz<- cbind(newz,z[,(dim(z)[2]-1):(dim(z)[2])])\n  write.csv(newz[,2:dim(newz)[2]],paste0(out,""zedited"","".csv""),row.names = F)\n}\n\n#TO RUN NANUQ W CHOSEN ALPHA/BETAS\n#if you already ran the NANUQ command, you will now have created a list of alpha and beta values you want to test. These should be in csv files with a column labelled either ""alphas"" or ""betas""\nif(file.exists(paste0(out,""z"","".csv""))){\nalpha <- read.csv(paste0(out,""alpha.csv""))\nbeta <- read.csv(paste0(out,""beta.csv""))\n#for every combo of alpha and beta, fun the NANUQ simulation to create the network\nfor(a in 1:dim(alpha)[1]){\n  for(b in 1:dim(beta)[1]){\n          cat(alpha[a,1])\n          z<- NANUQ(as.matrix(u), outfile = paste0(out), alpha =as.numeric(alpha[a,1]), beta =as.numeric(beta[b,1]), plot = TRUE)\n  }\n}\n}else{ #if first time running NANUQ, run it with arbitrary alpha and beta to get the matrix\n  z<- NANUQ(as.matrix(u), outfile = paste0(out), alpha =0.01, beta =0.01, plot = TRUE)\n  write.csv(z,paste0(out,""z"",uninf,"".csv""),row.names=F)\n}\n', '#Written by: Dylan DeBaun\n\n#load libraries\nlibrary(\'MSCquartets\')\nlibrary(\'ape\')\nlibrary(\'gtools\')\nlibrary(\'stringr\')\n\n#load species tree\nsnake_tree <-read.tree(""Tree_Dated_Point_PL_Astral_topology_130"")\n\n#load gene trees\ngenedata_ind = ""allindiv.treefile""\ngtrees_ind <- read.tree(genedata_ind)\n\n#INPUTS\nspecies = read.delim("""",header  = F) #list of species, with the last species being the outgroup species\noutgroup = species[length(species)]\nindivs = read.delim("""",header = F) # list of individuals\nsetwd("""") #where we want the outfiles to go\ngroup = """" #prefix for the clade\n\n#Create the quartet file for NANUQ\nb <- quartetTable(gtrees_ind,taxonnames=indivs,random = 0) #make quartet table at individual level\nb <- b[rowSums(b) !=4, colSums(b) !=0] #edit it to look the way we want for snaq\n# quartet file at the individual level\nwrite.csv(b, paste0(group,""_fullindivCFs.csv""), row.names = F) \n\n#Create the quartet file for SnaQ\nx<-b\ny<- matrix(rep(0,dim(x)[1]*8),ncol = 8,nrow=dim(x)[1])\ncolnames(y)<-c(""t1"",""t2"",""t3"",""t4"",""CF12_34"",""CF13_24"",""CF14_23"",""ngenes"")\nfor(i in 1:dim(x)[1]){\n  count=0\n  for(j in 1:dim(x)[2]){\n    if(x[i,j] == 1){\n      count = count+1\n      y[i,count] = colnames(x)[j]\n    }\n  }\n  y[i,""ngenes""] = x[i,dim(x)[2]-2] + x[i,dim(x)[2]-1]+x[i,dim(x)[2]]\n  y[i,""CF12_34""] = as.numeric(x[i,dim(x)[2]-2])/as.numeric(y[i,""ngenes""])\n  y[i,""CF13_24""] = as.numeric(x[i,dim(x)[2]-1])/as.numeric(y[i,""ngenes""])\n  y[i,""CF14_23""] = as.numeric(x[i,dim(x)[2]])/as.numeric(y[i,""ngenes""])\n}\nwrite.csv(y, paste0(group,""_individualsCFs.csv""), row.names = F)\n\n#create the clade\'s tree from the species tree for SnaQ\ntree_new = keep.tip(snake_tree, species)\ntree_new$edge.length<-NULL #we dont need branch length information/etc\ntree_new$node.label<-NULL\nwrite.tree(tree_new,paste0(group,"".tre""))\n\n\n', '#Written by: Dylan DeBaun\n\nlibrary(ape)\nlibrary(phytools)\nlibrary(ggplot2)\ntree<-read.tree()\ntree1<-force.ultrametric(tree)\ntree1<-drop.tip(tree1,c(1:14,124:130))\nx<-ltt(tree1,log.lineages = F)\n\nbt<-branching.times(tree1)\n\nltt1 <- as.data.frame(cbind(x[[""times""]]-22.17059,log(x[[""ltt""]])))\nltt15 <- format(ltt1[-c(1:11),],scientific=F)\nrtt <- read.csv(""~/Desktop/rtt1.csv"")\nltt15[1,] = as.numeric(c(-14.71835,2.639057))\nltt15[97,1] = as.numeric(0)\nrtt[13,] = as.numeric(c(0,109,12,0,0,0))\n\n#GEOM STEP Plot \np <- ggplot()+geom_step(aes(x=as.numeric(ltt15[,1]), y = as.numeric(ltt15[,2])-as.numeric(min(ltt15[,2]))), size= 1.5,direction=""vh"")+  geom_step(aes(x=-rtt$year..my.,y=log(rtt$num_retic)),color=""dark blue"",size=1.5,direction=""hv"")+ scale_y_continuous(name=""ln(Number of Lineages)"",breaks = seq(0,2.5,0.5), labels = seq(2.56,5.06,0.5),sec.axis = sec_axis(trans=~.*1, name=""ln(Number of Reticulations)""))\np\n#p <- ggplot() + geom_point(aes(x=rtt$year.my.,y=rtt$log.num.),color=""dark blue"",size=2) +scale_x_continuous(breaks = seq(-15,0,5),name = ""Time (mya)"")              \np <- p + geom_vline(xintercept=-2.588) + geom_vline(xintercept=-5.332)+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n                                                                              panel.background = element_blank(), axis.line = element_line(colour = ""black""),text = element_text(size=14),panel.border = element_rect(colour = ""black"", fill=NA, size=0.5))\np\np<- p +  geom_errorbarh(data = rtt, aes(xmin=-positive,xmax = -negative, y= log(num_retic)), alpha=0.4,linetype = ""dashed"",color=""dark blue"") +theme(legend.position = ""none"")  +xlab(""Time (mya)"") +scale_x_continuous(limits=c(-max(rtt$positive),0), expand = c(0.01, 0.02)) \np\npdf(""~/Desktop/Figure_LTTRTTLOGSCALE_VH.pdf"",height = 10, width = 12)\nplot(p)\ndev.off()\n\n#supplement figure\np <- ggplot()+  geom_step(aes(x=-rtt$year..my.,y=log(rtt$num_retic)),color=""dark blue"",size=1.5,direction=""hv"")+ scale_y_continuous(name=""ln(Number of Lineages)"",breaks = seq(0,2.5,0.5), labels = seq(2.56,5.06,0.5),sec.axis = sec_axis(trans=~.*1, name=""ln(Number of Reticulations)""))\np <- p +  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n                                                                              panel.background = element_blank(), axis.line = element_line(colour = ""black""),text = element_text(size=14),panel.border = element_rect(colour = ""black"", fill=NA, size=0.5))\np<- p +  geom_errorbarh(data = rtt, aes(xmin=-positive,xmax = -negative, y= log(num_retic)), alpha=0.4,linetype = ""dashed"",color=""dark blue"") +theme(legend.position = ""none"")  +xlab(""Time (mya)"") +scale_x_continuous(limits=c(-max(rtt$positive),0), expand = c(0.01, 0.02)) \np\np + geom_line(aes(x=c(-rtt$year..my.[1],0),y=c(log(1),log(12))),col=""red"")\n#calculation for the rate\nlog(12)/rtt$year..my.[1]\n#0.1688305\n']","Widespread reticulate evolution in an adaptive radiation A fundamental assumption of evolutionary biology is that phylogeny follows a bifurcating process. However, hybrid speciation and introgression are becoming more widely documented in many groups. Hybrid inference studies have been historically limited to small sets of taxa, while exploration of the prevalence and trends of reticulation at deep time scales remains unexplored. We study the evolutionary history of an adaptive radiation of 109 gemsnakes in Madagascar (Pseudoxyrhophiinae) to identify potential instances of introgression. Using several network inference methods, we find twelve reticulation events within the 22-million-year evolutionary history of gemsnakes, producing 28% of the diversity for the group, including one reticulation that resulted in the diversification of an 18 species radiation. These reticulations occur at nodes with high gene tree discordance. Hybridization events occurred between north-south distributed parentals that share similar ecologies. Younger hybrids occupy intermediate contact zones between the parentals, showing that post-speciation dispersal in this group has not eroded the spatial signatures of introgression. Reticulations accumulated consistently over time, despite drops in overall speciation rates during the Pleistocene. This suggests that while bifurcating speciation may decline as the result of species accumulation and environmental change, speciation by hybridization may be more robust to these processes.",4
Data from: The repeatable opportunity for selection differs between pre- and post-copulatory fitness components,"In species with multiple mating, intense sexual selection may occur both before and after copulation. However, comparing the strength of pre- and postcopulatory selection is challenging, because i) postcopulatory processes are generally difficult to observe and ii) the often-used opportunity for selection (I) metric contains both deterministic and stochastic components. Here, we quantified pre- and postcopulatory male fitness components of the simultaneously hermaphroditic flatworm, Macrostomum lignano. We did this by tracking fluorescent spermusing transgenicsthrough the transparent body of sperm recipients, enabling to observe postcopulatory processes in vivo. Moreover, we sequentially exposed focal worms to three independent mating groups, and in each assessed their mating success, sperm-transfer efficiency, sperm fertilising efficiency, and partner fecundity. Based on these multiple measures, we could, for each fitness component, combine the variance (I) with the repeatability (R) in individual success to assess the amount of repeatable variance in individual successa measure we call the repeatable opportunity for selection (IR). We found higher repeatable opportunity for selection in sperm-transfer efficiency and sperm fertilising efficiency compared to mating success, which clearly suggests that postcopulatory selection is stronger than precopulatory selection. Our study demonstrates that the opportunity for selection contains a repeatable deterministic component, which can be assessed and disentangled from the often large stochastic component, to provide a better estimate of the strength of selection.","['## Source R script associated to the manuscript entitled \r\n## ""The repeatable opportunity for selection differs between pre- and postcopulatory fitness components"" \r\n## by L. Marie-Orleach, N. Vellnow, and L. Schrer\r\n\r\n\r\n## Function to relativise a data column\r\nREL <- function (x) { x/mean(x, na.rm=TRUE) }\r\n\r\n\r\n## Function to compute the fitness components (as explained on Figure 1).\r\nFITCOMP <- function(dataset) {\r\n  dataset$mRS <- dataset$focal_offspring # mRS\r\n  dataset$F   <- dataset$total_offspring # F\r\n  dataset$MS  <- dataset$focal_matings / dataset$total_matings # MS\r\n  dataset$STE <- (dataset$focal_sperm / dataset$total_sperm) / (dataset$focal_matings / dataset$total_matings) # STE\r\n  dataset$SFE <- (dataset$focal_offspring / dataset$total_offspring) / (dataset$focal_sperm / dataset$total_sperm) # SFE\r\n  dataset$SFE[which(!is.finite(dataset$SFE))] <- NaN\r\n  \r\n  return(dataset)\r\n}\r\n\r\n\r\n## Function to compute the standard variance in a fitness component\r\nSTDVAR <- function(fit.comp, dataset) {\r\n  var(s(dataset[,fit.comp])/mean(s(dataset[,fit.comp]), na.rm=TRUE)) \r\n}\r\n\r\n\r\n## Function to estimate the binomial sampling error arising from STE and SFE\r\n## see Pelissie et al, Evolution, 2012 (Appendix A) and Marie-Orleach et al, Evolution, 2016 (Supp. Info. E) for more details\r\nBSE <- function(fit.comp, dataset) {\r\n  if(fit.comp==""STE""){\r\n    dataset$V_STS_exp <- ((dataset$focal_sperm/dataset$total_sperm) * (1 - (dataset$focal_sperm/dataset$total_sperm))) / (dataset$MS^2) # expected variance in each STS estimate\r\n    bse_STE_rel <- (sum(dataset$V_STS_exp) / (sum(dataset$total_sperm))) / (mean(dataset$STE)^2) # binomial sampling error in STE\r\n    return (bse_STE_rel)\r\n  }\r\n  \r\n  if(fit.comp==""SFE""){\r\n    dataset$STS       <- dataset$focal_sperm / dataset$total_sperm # sperm transfer success\r\n    dataset$V_PS_exp  <- (dataset$focal_offspring / dataset$total_offspring) * (1 - (dataset$focal_offspring /dataset$total_offspring)) # expected variance in each PS estimate\r\n    dataset$V_SFE_exp <- dataset$V_PS_exp / (dataset$STS^2) # expected variance in each SFE estimate\r\n    dataset_147 <- subset(dataset, focal_sperm!=0) # subset\r\n    bse_SFE_rel <- (sum(dataset_147$V_SFE_exp) / sum(dataset_147$total_offspring)) / (mean(dataset_147$SFE)^2) # binomial sampling error in SFE\r\n    return (bse_SFE_rel)\r\n  }\r\n}\r\n\r\n\r\n## Function to compute the standard covariance between two fitness components\r\nSTDCOV <- function (fit.comp1, fit.comp2, dataset){\r\n  if (fit.comp1==""SFE"" | fit.comp2==""SFE"") { dataset <- dataset[complete.cases(dataset),] }\r\n  CovArray <- (REL(dataset[,fit.comp1])-mean(REL(dataset[,fit.comp1]), na.rm=TRUE)) * (REL(dataset[,fit.comp2])-mean(REL(dataset[,fit.comp2]), na.rm=TRUE))\r\n  return (mean(CovArray))\r\n}\r\n\r\n\r\n## Function to compute the total variance explained by our model\r\nTOTVAR <- function(dataset){\r\n  stdvar_F   <- STDVAR (""F""  , dataset)\r\n  stdvar_MS  <- STDVAR (""MS"" , dataset)\r\n  stdvar_STE <- STDVAR (""STE"", dataset)\r\n  stdvar_SFE <- STDVAR (""SFE"", dataset)\r\n  \r\n  bse_STE <- BSE (""STE"", dataset)\r\n  bse_SFE <- BSE (""SFE"", dataset)\r\n  \r\n  stdcov_F.MS    <- STDCOV (""F"",   ""MS"",  dataset)\r\n  stdcov_F.STE   <- STDCOV (""F"",   ""STE"", dataset)\r\n  stdcov_F.SFE   <- STDCOV (""F"",   ""SFE"", dataset)\r\n  stdcov_MS.STE  <- STDCOV (""MS"",  ""STE"", dataset)\r\n  stdcov_MS.SFE  <- STDCOV (""MS"",  ""SFE"", dataset)\r\n  stdcov_STE.SFE <- STDCOV (""STE"", ""SFE"", dataset)\r\n  \r\n  return(stdvar_F + stdvar_MS + (stdvar_STE-bse_STE) + (stdvar_SFE-bse_SFE) +\r\n    (2*stdcov_F.MS) + (2*stdcov_F.STE) + (2*stdcov_F.SFE) + (2*stdcov_MS.STE) + (2*stdcov_MS.SFE) + (2*stdcov_STE.SFE) +\r\n    bse_STE + bse_SFE)\r\n}\r\n\r\n\r\n## Function to bootstrapp the variances and covariances to assess their 95 CI\r\n## Outcomes are here expressed as percentages of total variance\r\nBOOT_VARCOVAR <- function (dataset, iteration){\r\n  N <- NROW(dataset)\r\n  \r\n  stor.data <- data.frame(stdvar_mRS=numeric(iteration), stdvar_F=numeric(iteration), stdvar_MS=numeric(iteration), stdvar_STE=numeric(iteration), stdvar_SFE=numeric(iteration),\r\n                        bse_STE=numeric(iteration), bse_SFE=numeric(iteration),\r\n                        stdcov_F.MS=numeric(iteration), stdcov_F.STE=numeric(iteration), stdcov_F.SFE=numeric(iteration), stdcov_MS.STE=numeric(iteration), stdcov_MS.SFE=numeric(iteration), stdcov_STE.SFE=numeric(iteration))\r\n  \r\n  for(i in 1:iteration){\r\n    rand = sample(1:N, N, replace=TRUE)\r\n    dataset$nb <- 1:N\r\n    newdataset = dataset[match(rand, dataset$nb), ]\r\n    \r\n    stdvar_mRS <- STDVAR (""mRS"", newdataset)\r\n    stdvar_F   <- STDVAR (""F""  , newdataset)\r\n    stdvar_MS  <- STDVAR (""MS"" , newdataset)\r\n    stdvar_STE <- STDVAR (""STE"", newdataset)\r\n    stdvar_SFE <- STDVAR (""SFE"", newdataset)\r\n    \r\n    bse_STE <- BSE (""STE"", newdataset)\r\n    bse_SFE <- BSE (""SFE"", newdataset)\r\n    \r\n    stdcov_F.MS    <- STDCOV (""F"",   ""MS"",  newdataset)\r\n    stdcov_F.STE   <- STDCOV (""F"",   ""STE"", newdataset)\r\n    stdcov_F.SFE   <- STDCOV (""F"",   ""SFE"", newdataset)\r\n    stdcov_MS.STE  <- STDCOV (""MS"",  ""STE"", newdataset)\r\n    stdcov_MS.SFE  <- STDCOV (""MS"",  ""SFE"", newdataset)\r\n    stdcov_STE.SFE <- STDCOV (""STE"", ""SFE"", newdataset)\r\n    \r\n    totalvariance <- stdvar_F + stdvar_MS + (stdvar_STE-bse_STE) + (stdvar_SFE-bse_SFE) +\r\n      (2*stdcov_F.MS) + (2*stdcov_F.STE) + (2*stdcov_F.SFE) + (2*stdcov_MS.STE) + (2*stdcov_MS.SFE) + (2*stdcov_STE.SFE) +\r\n      bse_STE + bse_SFE\r\n    \r\n    stor.data[i,1]  = stdvar_mRS / totalvariance\r\n    stor.data[i,2]  = stdvar_F / totalvariance\r\n    stor.data[i,3]  = stdvar_MS / totalvariance\r\n    stor.data[i,4]  = (stdvar_STE-bse_STE) / totalvariance\r\n    stor.data[i,5]  = (stdvar_SFE-bse_SFE) / totalvariance\r\n     \r\n    stor.data[i,6]  = bse_STE / totalvariance\r\n    stor.data[i,7]  = bse_SFE / totalvariance\r\n     \r\n    stor.data[i,8]  = stdcov_F.MS / totalvariance\r\n    stor.data[i,9]  = stdcov_F.STE / totalvariance\r\n    stor.data[i,10] = stdcov_F.SFE / totalvariance\r\n    stor.data[i,11] = stdcov_MS.STE / totalvariance\r\n    stor.data[i,12] = stdcov_MS.SFE / totalvariance\r\n    stor.data[i,13] = stdcov_STE.SFE / totalvariance\r\n  }\r\n  output <- data.frame(source=numeric(13), mean=numeric(13), ""Q.025""=numeric(13), ""Q.500""=numeric(13), ""Q.975""=numeric(13))\r\n  output[,1]  <- names(stor.data)\r\n  \r\n  for (i in 1:13){\r\n    output[i,2] <- mean(stor.data[,i])\r\n    output[i,3] <- quantile(stor.data[,i], .025)\r\n    output[i,4] <- quantile(stor.data[,i], .500)\r\n    output[i,5] <- quantile(stor.data[,i], .975)\r\n  }\r\n  return(output)\r\n}\r\n\r\n\r\n## Function to bootstrapp the variances to do pairwise comparisons\r\nPWCOMP_VAR <- function (fit.comp1, fit.comp2, dataset, iteration){\r\n  dataset <- FITCOMP (dataset)\r\n  \r\n  ##use the n=150 dataset if SFE is not involved in the pairwise comparison\r\n  if (fit.comp1!=""SFE"" && fit.comp2!=""SFE""){  \r\n        \r\n    N <- NROW(dataset)\r\n    stor.data <- data.frame(var_fit.comp1=numeric(iteration), var_fit.comp2=numeric(iteration))\r\n\r\n    for(i in 1:iteration){\r\n      rand = sample(1:N, N, replace=TRUE)\r\n      dataset$nb <- 1:N\r\n      newdataset = dataset[match(rand, dataset$nb), ]\r\n\r\n      ##estimate binomial sampling error in STE\r\n      bse_STE <- BSE (""STE"", newdataset)\r\n      \r\n      ##estimate standardised variance\r\n      stdvar_fit.comp1 <- STDVAR (fit.comp1, newdataset)\r\n      stdvar_fit.comp2 <- STDVAR (fit.comp2, newdataset)\r\n      \r\n      if (fit.comp1==""STE"") { stdvar_fit.comp1 = stdvar_fit.comp1 - bse_STE }\r\n      if (fit.comp2==""STE"") { stdvar_fit.comp2 = stdvar_fit.comp2 - bse_STE }\r\n      stor.data[i,1] = stdvar_fit.comp1\r\n      stor.data[i,2] = stdvar_fit.comp2\r\n    }\r\n    \r\n  ##use the n=147 dataset if SFE is involved in the pairwise comparison\r\n  } else if(fit.comp1==""SFE"" | fit.comp2==""SFE""){\r\n    \r\n    dataset <- dataset[complete.cases(dataset),]\r\n    N <- NROW(dataset)\r\n    stor.data <- data.frame(var_fit.comp1=numeric(iteration), var_fit.comp2=numeric(iteration))\r\n\r\n    for(i in 1:iteration){\r\n      rand = sample(1:N, N, replace=TRUE)\r\n      dataset$nb <- 1:N\r\n      newdataset = dataset[match(rand, dataset$nb), ]\r\n        \r\n      ##estimate binomial sampling error in STE and SFE\r\n      bse_STE <- BSE (""STE"", newdataset)\r\n      bse_SFE <- BSE (""SFE"", newdataset)\r\n        \r\n      ##estimate standardised variance\r\n      stdvar_fit.comp1 <- STDVAR (fit.comp1, newdataset)\r\n      stdvar_fit.comp2 <- STDVAR (fit.comp2, newdataset)\r\n        \r\n      if (fit.comp1==""STE"") { stdvar_fit.comp1 = stdvar_fit.comp1 - bse_STE }\r\n      if (fit.comp2==""STE"") { stdvar_fit.comp2 = stdvar_fit.comp2 - bse_STE }\r\n      if (fit.comp1==""SFE"") { stdvar_fit.comp1 = stdvar_fit.comp1 - bse_SFE }\r\n      if (fit.comp2==""SFE"") { stdvar_fit.comp2 = stdvar_fit.comp2 - bse_SFE }\r\n      stor.data[i,1] = stdvar_fit.comp1\r\n      stor.data[i,2] = stdvar_fit.comp2\r\n    }\r\n  }\r\n  return (min((2*sum(stor.data[,1]>stor.data[,2]))/iteration, (2*sum(stor.data[,1]<stor.data[,2]))/iteration))\r\n}\r\n\r\n\r\n## Functions to transform the fitness data to limit data skewness\r\nTRANS_mRS <- function(x) { sqrt(x) }\r\nTRANS_F   <- function(x) { sqrt(x+0.5) }\r\nTRANS_MS  <- function(x) { x }\r\nTRANS_STE <- function(x) { sqrt(x) }\r\nTRANS_SFE <- function(x) { log10(x+1) }\r\n\r\n\r\n## Function to compute repeatability of individual success across the three mating groups\r\nRPT <- function (fit.comp, TRANS, dataset, iteration){\r\n  dataset$trans.rel.fit.comp <- REL(TRANS(dataset[,fit.comp]))\r\n  rpt_fit.comp <- rpt( trans.rel.fit.comp ~ (1|focal), grname=""focal"", CI=0.95, datatype=""Gaussian"", nboot=iteration, npermut=iteration, data=dataset)\r\n  return (rpt_fit.comp)\r\n}\r\n\r\n## Function to compute the repeatable variance in a fitness component\r\nRPTVAR <- function(fit.comp, TRANS, dataset_focal, dataset_group) {\r\n\r\n  #compute standard variance\r\n  dataset_focal <- dataset_focal[complete.cases(dataset_focal[,fit.comp]),]\r\n  v_fit.comp <- STDVAR (fit.comp, dataset_focal)\r\n  \r\n  #compute repeatability\r\n  rows <- sapply(dataset_focal$focal, function(x) which(x==dataset_group$focal))\r\n  dataset_group = dataset_group[as.vector(rows),]\r\n  rpt_fit.comp <- RPT (fit.comp, TRANS, dataset_group, 0)\r\n  \r\n  #compute repeatable variance\r\n  rptvar_fit.comp <- v_fit.comp * rpt_fit.comp$R\r\n\r\n  return(rptvar_fit.comp)\r\n}\r\n\r\n\r\n## Function to bootstrapp the repeatable variance to assess their 95 CI\r\n## Outcomes are here expressed as percentages of total variance\r\nBOOT_RPTVAR <- function(dataset_focal, dataset_group, iteration){\r\n  stor.data <- data.frame(stdrptvar_mRS=numeric(iteration), stdrptvar_F=numeric(iteration), stdrptvar_MS=numeric(iteration), stdrptvar_STE=numeric(iteration), stdrptvar_SFE=numeric(iteration))\r\n  \r\n  for(i in 1:iteration){\r\n    # estimate variance of mRS, F, MS and STE in boostrapped data (n=150)\r\n    N <- NROW(dataset_focal) \r\n    rand = sample(1:N,N,replace=TRUE)\r\n    dataset_focal$nb <- 1:N # assign new ID to each focal\r\n    newdataset_focal = dataset_focal[match(rand, dataset_focal$nb), ]\r\n    newdataset_focal$focal = rep(1:N)\r\n    dataset_group$nb <- rep(1:N, each=3)\r\n    rows <- sapply(rand, function(x) which(x==dataset_group$nb))\r\n    newdataset_group = dataset_group[as.vector(rows),]\r\n    newdataset_group$focal = rep(1:N,each=3)\r\n    \r\n    rptvar_mRS <- RPTVAR (""mRS"", TRANS_mRS, newdataset_focal, newdataset_group)\r\n    rptvar_F   <- RPTVAR (""F"",   TRANS_F,   newdataset_focal, newdataset_group)\r\n    rptvar_MS  <- RPTVAR (""MS"",  TRANS_MS,  newdataset_focal, newdataset_group)\r\n    rptvar_STE <- RPTVAR (""STE"", TRANS_STE, newdataset_focal, newdataset_group)\r\n    totvar150 <- TOTVAR (newdataset_focal)\r\n\r\n    # estimate variance of SFE other boostrapped data (n=147)\r\n    newdataset_focal <- dataset_focal[complete.cases(dataset_focal),]\r\n    N <- NROW(newdataset_focal) \r\n    rand = sample(1:N, N, replace=TRUE)\r\n    newdataset_focal$new.ID <- 1:N\r\n    newdataset_focal = newdataset_focal[match(rand, newdataset_focal$new.ID), ]\r\n    rows <- sapply(newdataset_focal$focal, function(x) which(x==dataset_group$focal))\r\n    newdataset_group = dataset_group[as.vector(rows),]\r\n    newdataset_focal$focal = rep(1:N)\r\n    newdataset_group$focal = rep(1:N,each=3)\r\n\r\n    rptvar_SFE <- RPTVAR (""SFE"", TRANS_SFE, newdataset_focal, newdataset_group)\r\n    totvar147  <- TOTVAR (newdataset_focal)\r\n    \r\n    stor.data[i,1] = rptvar_mRS\r\n    stor.data[i,2] = rptvar_F / totvar150\r\n    stor.data[i,3] = rptvar_MS / totvar150\r\n    stor.data[i,4] = rptvar_STE / totvar150\r\n    stor.data[i,5] = rptvar_SFE / totvar147\r\n  }\r\n  output <- data.frame(source=numeric(5), mean=numeric(5), ""Q.025""=numeric(5), ""Q.500""=numeric(5), ""Q.975""=numeric(5))\r\n  output[,1]  <- names(stor.data)\r\n  \r\n  for (i in 1:5){\r\n    output[i,2] <- mean(stor.data[,i])\r\n    output[i,3] <- quantile(stor.data[,i], .025)\r\n    output[i,4] <- quantile(stor.data[,i], .500)\r\n    output[i,5] <- quantile(stor.data[,i], .975)\r\n  }\r\n  return(output)\r\n}\r\n\r\n\r\nPWCOMP_RPTVAR <- function (fit.comp1, fit.comp2, TRANS1, TRANS2, dataset_focal, dataset_group, iteration){\r\n  \r\n  ##use the n=150 dataset if SFE is not involved in the pairwise comparison\r\n  if (fit.comp1!=""SFE"" && fit.comp2!=""SFE""){  \r\n    \r\n    N <- NROW(dataset_focal)\r\n    stor.data <- data.frame( rptvar_fit.comp1=numeric(iteration), rptvar_fit.comp2=numeric(iteration) )\r\n    \r\n    for(i in 1:iteration){\r\n      rand = sample(1:N, N, replace=TRUE)\r\n      dataset_focal$new.ID <- 1:N\r\n      newdataset_focal = dataset_focal[match(rand, dataset_focal$new.ID), ]\r\n      rows <- sapply(newdataset_focal$focal, function(x) which(x==dataset_group$focal))\r\n      newdataset_group = dataset_group[as.vector(rows),]\r\n      newdataset_focal$focal = rep(1:N)\r\n      newdataset_group$focal = rep(1:N,each=3)\r\n      \r\n      rptvar_fit.comp1 <- RPTVAR (fit.comp1, TRANS1, newdataset_focal, newdataset_group)\r\n      rptvar_fit.comp2 <- RPTVAR (fit.comp2, TRANS2, newdataset_focal, newdataset_group)\r\n\r\n      stor.data[i,1] = rptvar_fit.comp1\r\n      stor.data[i,2] = rptvar_fit.comp2\r\n    }\r\n    \r\n  ##use the n=147 dataset if SFE is involved in the pairwise comparison\r\n  } else if(fit.comp1==""SFE"" | fit.comp2==""SFE""){\r\n    dataset_focal <- dataset_focal[complete.cases(dataset_focal),]\r\n    N <- NROW(dataset_focal)\r\n    stor.data <- data.frame( rptvar_fit.comp1=numeric(iteration), rptvar_fit.comp2=numeric(iteration) )\r\n    \r\n    for(i in 1:iteration){\r\n      rand = sample(1:N, N, replace=TRUE)\r\n      dataset_focal$new.ID <- 1:N\r\n      newdataset_focal = dataset_focal[match(rand, dataset_focal$new.ID), ]\r\n      rows <- sapply(newdataset_focal$focal, function(x) which(x==dataset_group$focal))\r\n      newdataset_group = dataset_group[as.vector(rows),]\r\n      newdataset_focal$focal = rep(1:N)\r\n      newdataset_group$focal = rep(1:N,each=3)\r\n      \r\n      rptvar_fit.comp1 <- RPTVAR (fit.comp1, TRANS1, newdataset_focal, newdataset_group)\r\n      rptvar_fit.comp2 <- RPTVAR (fit.comp2, TRANS2, newdataset_focal, newdataset_group)\r\n\r\n      stor.data[i,1] = rptvar_fit.comp1\r\n      stor.data[i,2] = rptvar_fit.comp2\r\n    }\r\n  }\r\n  list <- list(""P value (repeatable variance)""=min((2*sum(stor.data[,1]>stor.data[,2]))/iteration, (2*sum(stor.data[,1]<stor.data[,2]))/iteration))\r\n  return(list)\r\n}', '## Main R script associated to the manuscript entitled \r\n## ""The repeatable opportunity for selection differs between pre- and postcopulatory fitness components"" \r\n## by L. Marie-Orleach, N. Vellnow, and L. Schrer\r\n\r\nlibrary(rptR) #for rptR::rpt\r\nlibrary(hablar) #for hablar::s\r\nlibrary(lmerTest) #for lmerTest::lmer\r\n\r\nsource("".../Marie-Orleach.et.al_R.SCRIPT_FUNCTIONS.R"")\r\niteration <- 10000\r\n\r\n#### 1. variance and covariance in mRS and all four fitness components ####\r\ndataset_focal <- read.delim("".../Marie-Orleach.et.al_DATASET.FOCAL.txt"")\r\ndataset_focal <- FITCOMP (dataset_focal)\r\n\r\n#### 1.1 standard variance in mRS and all four fitness components ####\r\nstdvar_mRS <- STDVAR (""mRS"", dataset_focal)\r\nstdvar_F   <- STDVAR (""F""  , dataset_focal)\r\nstdvar_MS  <- STDVAR (""MS"" , dataset_focal)\r\nstdvar_STE <- STDVAR (""STE"", dataset_focal)\r\nstdvar_SFE <- STDVAR (""SFE"", dataset_focal)\r\n \r\n#### 1.2 binomial sampling error in STE and SFE ####\r\nbse_STE <- BSE (""STE"", dataset_focal)\r\nbse_SFE <- BSE (""SFE"", dataset_focal)\r\n\r\n#### 1.3 covariances ####\r\nstdcov_F.MS    <- STDCOV (""F"",   ""MS"",  dataset_focal)\r\nstdcov_F.STE   <- STDCOV (""F"",   ""STE"", dataset_focal)\r\nstdcov_F.SFE   <- STDCOV (""F"",   ""SFE"", dataset_focal)\r\nstdcov_MS.STE  <- STDCOV (""MS"",  ""STE"", dataset_focal)\r\nstdcov_MS.SFE  <- STDCOV (""MS"",  ""SFE"", dataset_focal)\r\nstdcov_STE.SFE <- STDCOV (""STE"", ""SFE"", dataset_focal)\r\n\r\n#### 1.4 total variance ####\r\ntotvar <- TOTVAR (dataset_focal)\r\n\r\n#### 1.5 variance and covariance bootstrap (95% CIs) ####\r\nboot_varcovar_outcome <- BOOT_VARCOVAR (dataset_focal, iteration)\r\n\r\n#### 1.6 variance pairwise comparisons (P values) ####\r\npvar_F.MS    <- PWCOMP_VAR (""F"",   ""MS"",  dataset_focal, iteration)\r\npvar_F.STE   <- PWCOMP_VAR (""F"",   ""STE"", dataset_focal, iteration)\r\npvar_F.SFE   <- PWCOMP_VAR (""F"",   ""SFE"", dataset_focal, iteration)\r\npvar_MS.STE  <- PWCOMP_VAR (""MS"",  ""STE"", dataset_focal, iteration)\r\npvar_MS.SFE  <- PWCOMP_VAR (""MS"",  ""SFE"", dataset_focal, iteration)\r\npvar_STE.SFE <- PWCOMP_VAR (""STE"", ""SFE"", dataset_focal, iteration)\r\n\r\n\r\n\r\n#### 2. repeatability in mRS and all four fitness components ####\r\ndataset_group <- read.delim("".../Marie-Orleach.et.al_DATASET.MATING.GROUP.txt"")\r\ndataset_group <- FITCOMP (dataset_group) \r\n\r\n## /!\\ warnings \'boundary (singular) fit: see ?isSingular\' arise when the random effect (1|focal) explains 0 variance.\r\n## /!\\ These warnings arise for F, and for the permutation tests of all fitness components (which is expected).\r\n## /!\\ These warnings appear in all steps in which repeatability is assessed (3.2 & 3.3)\r\nrpt_mRS <- RPT (""mRS"", TRANS_mRS, dataset_group, iteration) \r\nrpt_F   <- RPT (""F"",   TRANS_F,   dataset_group, iteration)\r\nrpt_MS  <- RPT (""MS"",  TRANS_MS,  dataset_group, iteration)\r\nrpt_STE <- RPT (""STE"", TRANS_STE, dataset_group, iteration)\r\nrpt_SFE <- RPT (""SFE"", TRANS_SFE, dataset_group, iteration)\r\n\r\n\r\n\r\n#### 3. repeatable variance in mRS and all four fitness components ####\r\ndataset_focal <- read.delim("".../Marie-Orleach.et.al_DATASET.FOCAL.txt"")\r\ndataset_focal <- FITCOMP (dataset_focal)\r\ndataset_group <- read.delim("".../Marie-Orleach.et.al_DATASET.MATING.GROUP.txt"")\r\ndataset_group <- FITCOMP (dataset_group) \r\n\r\n#### 3.1 repeatable variance ####\r\n## /!\\ missing data are explained in the Methods of the article\r\nrptvar_mRS <- RPTVAR (""mRS"", TRANS_mRS, dataset_focal, dataset_group)\r\nrptvar_F   <- RPTVAR (""F"",   TRANS_F,   dataset_focal, dataset_group)\r\nrptvar_MS  <- RPTVAR (""MS"",  TRANS_MS,  dataset_focal, dataset_group)\r\nrptvar_STE <- RPTVAR (""STE"", TRANS_STE, dataset_focal, dataset_group)\r\nrptvar_SFE <- RPTVAR (""SFE"", TRANS_SFE, dataset_focal, dataset_group)\r\n\r\n#### 3.2 repeatable variance bootstrap (95% CIs) ####\r\nboot_rptvar_outcome <- BOOT_RPTVAR (dataset_focal, dataset_group, iteration)\r\n\r\n#### 3.3 pairwise comparisons in repeatable variance ####\r\nprptvar_F.MS    <- PWCOMP_RPTVAR (""F"",   ""MS"",  TRANS_F,   TRANS_MS,  dataset_focal, dataset_group, iteration)\r\nprptvar_F.STE   <- PWCOMP_RPTVAR (""F"",   ""STE"", TRANS_F,   TRANS_STE, dataset_focal, dataset_group, iteration)\r\nprptvar_F.SFE   <- PWCOMP_RPTVAR (""F"",   ""SFE"", TRANS_F,   TRANS_SFE, dataset_focal, dataset_group, iteration)\r\nprptvar_MS.STE  <- PWCOMP_RPTVAR (""MS"",  ""STE"", TRANS_MS,  TRANS_STE, dataset_focal, dataset_group, iteration)\r\nprptvar_MS.SFE  <- PWCOMP_RPTVAR (""MS"",  ""SFE"", TRANS_MS,  TRANS_SFE, dataset_focal, dataset_group, iteration)\r\nprptvar_STE.SFE <- PWCOMP_RPTVAR (""STE"", ""SFE"", TRANS_STE, TRANS_SFE, dataset_focal, dataset_group, iteration)\r\n\r\n\r\n\r\n#### 4. group and batch effects ####\r\ndataset_group <- read.delim("".../Marie-Orleach.et.al_DATASET.MATING.GROUP.txt"")\r\ndataset_group <- FITCOMP (dataset_group) \r\n\r\nm_mRS <- lmer(REL(TRANS_mRS(mRS)) ~  (1|focal) + as.factor(mating_group) + as.factor(batch) + as.factor(mating_group)*as.factor(batch), data=dataset_group)\r\nm_F   <- lmer(REL(TRANS_F(F))     ~  (1|focal) + as.factor(mating_group) + as.factor(batch) + as.factor(mating_group)*as.factor(batch), data=dataset_group)\r\nm_MS  <- lmer(REL(TRANS_MS(MS))   ~  (1|focal) + as.factor(mating_group) + as.factor(batch) + as.factor(mating_group)*as.factor(batch), data=dataset_group)\r\nm_STE <- lmer(REL(TRANS_STE(STE)) ~  (1|focal) + as.factor(mating_group) + as.factor(batch) + as.factor(mating_group)*as.factor(batch), data=dataset_group)\r\nm_SFE <- lmer(REL(TRANS_SFE(SFE)) ~  (1|focal) + as.factor(mating_group) + as.factor(batch) + as.factor(mating_group)*as.factor(batch), na.action=na.omit, data=dataset_group)\r\n\r\nanova(m_mRS)\r\nanova(m_F)\r\nanova(m_MS)\r\nanova(m_STE)\r\nanova(m_SFE)']","Data from: The repeatable opportunity for selection differs between pre- and post-copulatory fitness components In species with multiple mating, intense sexual selection may occur both before and after copulation. However, comparing the strength of pre- and postcopulatory selection is challenging, because i) postcopulatory processes are generally difficult to observe and ii) the often-used opportunity for selection (I) metric contains both deterministic and stochastic components. Here, we quantified pre- and postcopulatory male fitness components of the simultaneously hermaphroditic flatworm, Macrostomum lignano. We did this by tracking fluorescent spermusing transgenicsthrough the transparent body of sperm recipients, enabling to observe postcopulatory processes in vivo. Moreover, we sequentially exposed focal worms to three independent mating groups, and in each assessed their mating success, sperm-transfer efficiency, sperm fertilising efficiency, and partner fecundity. Based on these multiple measures, we could, for each fitness component, combine the variance (I) with the repeatability (R) in individual success to assess the amount of repeatable variance in individual successa measure we call the repeatable opportunity for selection (IR). We found higher repeatable opportunity for selection in sperm-transfer efficiency and sperm fertilising efficiency compared to mating success, which clearly suggests that postcopulatory selection is stronger than precopulatory selection. Our study demonstrates that the opportunity for selection contains a repeatable deterministic component, which can be assessed and disentangled from the often large stochastic component, to provide a better estimate of the strength of selection.",4
Evolutionary conservation and diversification of auditory neural circuits that process courtship songs in Drosophila,"Acoustic communication signals diversify even on short evolutionary time scales. To understand how the auditory system underlying acoustic communication could evolve, we conducted a systematic comparison of the early stages of the auditory neural circuit involved in song information processing between closely-related fruit-fly species. Male Drosophila melanogaster and D. simulans produce different sound signals during mating rituals, known as courtship songs. Female flies from these species selectively increase their receptivity when they hear songs with conspecific temporal patterns. Here, we first confirmed interspecific differences in temporal pattern preferences; D. simulans preferred pulse songs with longer intervals than D. melanogaster. Primary and secondary song-relay neurons, JO neurons and AMMC-B1 neurons, shared similar morphology and neurotransmitters between species. The temporal pattern preferences of AMMC-B1 neurons were also relatively similar between species, with slight but significant differences in their band-pass properties. Although the shift direction of the response property matched that of the behavior, these differences are not large enough to explain behavioral differences in song preferences. This study enhances our understanding of the conservation and diversification of the architecture of the early-stage neural circuit which processes acoustic communication signals.","['source (""Library.R"")\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\n\n\n#####melanogaster####\n\nd <- read.csv(""normmax_imaging_4C_melanogaster.csv"")\nd$x <- d$IPI\nd$y <- d$normmax\n\nobservation <- list(N = nrow(d), x = d$x, y = d$y, Ninds = length(unique(d$ID)), id = d$ID)\n\nparams = c(a0 = 0.1, b0 = 0.01, c0 = 0.1, d0 = 0.1, SD_a = 0.005, SD_b = 0.005, SD_c = 0.005, SD_d = 0.005)\ninit0 = c(as.list(params), shape = 50)\ninit0$a = rep(0.1, observation$Ninds)\ninit0$b = rep(0.01, observation$Ninds)\ninit0$c = rep(0.1, observation$Ninds)\ninit0$d = rep(0.1, observation$Ninds)\ninit = setNames(rep(list(init0), 4), LETTERS[1:4])\n\nexpo = function(x, y0, lambda) y0 * exp(- lambda * x)\n\ngenfunc = function(params) {\n  function(x) {\n    expo(x, params[""a0""] , params[""b0""]) - expo(x, params[""c0""], params[""d0""])\n  }\n}\n\nmod = rstan::stan_model(file = ""Bayesian_model.stan"")\nfit = rstan::sampling(mod, data = observation, init = init, iter = 10000, seed = 1)\nRhat = summary(fit)$summary[,""Rhat""]\ncoef = broom.mixed::tidyMCMC(fit) %>% {setNames(.$estimate, .$term)} %>% print()\n\nwrite.csv(coef, ""coef_melanogaster.csv"")\nwrite.csv(Rhat, ""Rhat_melanogaster.csv"")\n\n\n#####simulans####\n\nd <- read.csv(""normmax_imaging_4C_simulans.csv"")\nd$x <- d$IPI\nd$y <- d$normmax\n\nobservation <- list(N = nrow(d), x = d$x, y = d$y, Ninds = length(unique(d$ID)), id = d$ID)\n\nparams = c(a0 = 0.1, b0 = 0.01, c0 = 0.1, d0 = 0.1, SD_a = 0.005, SD_b = 0.005, SD_c = 0.005, SD_d = 0.005)\ninit0 = c(as.list(params), shape = 50)\ninit0$a = rep(0.1, observation$Ninds)\ninit0$b = rep(0.01, observation$Ninds)\ninit0$c = rep(0.1, observation$Ninds)\ninit0$d = rep(0.1, observation$Ninds)\ninit = setNames(rep(list(init0), 4), LETTERS[1:4])\n\nexpo = function(x, y0, lambda) y0 * exp(- lambda * x)\n\ngenfunc = function(params) {\n  function(x) {\n    expo(x, params[""a0""] , params[""b0""]) - expo(x, params[""c0""], params[""d0""])\n  }\n}\n\nmod = rstan::stan_model(file = ""Bayesian_model.stan"")\nfit = rstan::sampling(mod, data = observation, init = init, iter = 10000, seed = 1)\nRhat = summary(fit)$summary[,""Rhat""]\ncoef = broom.mixed::tidyMCMC(fit) %>% {setNames(.$estimate, .$term)} %>% print()\n\nwrite.csv(coef, ""coef_simulans.csv"")\nwrite.csv(Rhat, ""Rhat_simulans.csv"")\n\n\n', 'source (""Library.R"")\n\n#####melanogaster####\n\ndata= NULL\nfor (i in 1:8) {\n  sname <- paste(""Fly"",i, sep="""") \n  .data = read.xlsx(""raw_data_imaging_3E_melanogaster.xlsx"",sheetName= sname) %>%\n    dplyr::mutate(Fly = paste(i)) %>%\n    tidyr::gather(key = Frequency, value = response, -Frame, -Fly)\n  .data$Frequency = str_sub(.data$Frequency, start = 2)\n  background = .data %>%\n    dplyr::filter(Frame < 20) %>%\n    dplyr::group_by(Frequency) %>%\n    dplyr::summarise(background = mean(response))\n  ..data = full_join(.data, background) %>%\n    mutate(norm_response = (response-background)/background)\n  data = rbind (data, ..data) # bind the files\n}\nhead(data)\ndata $Fly = as.numeric(data$Fly)\n\n.data = data %>%\n  dplyr::filter(grepl(""Hz"", Frequency)) %>%\n  tidyr::separate(col = Frequency, into = c(""Frequency"", ""Hz"", ""rep""), sep = ""_"") %>%\n  dplyr::select(-Hz) \nhead(.data)\n\n.data$Fly = as.factor(.data$Fly)\nFrequency_order = as.character(c(40, seq(100, 300, by=100)))\n.data$Hz = factor(.data$Frequency, levels = Frequency_order)\n\nfnrollmean <- function (x) {\n  if (length(x) < 3) {\n    rep(NA,length(x)) \n  } else {\n    rollmean(x,3,align=""center"",na.pad=TRUE)\n  }\n}\n.data <- .data %>% group_by(Fly,Frequency) %>% \n  mutate(rollavg=fnrollmean(norm_response))\n\n\n.data.mean = .data %>% group_by(Frame, Fly, Frequency) %>%\n  dplyr::summarise(mean.response = mean(rollavg), sd.response = sd(rollavg))\nhead(.data.mean)\n\n\n.data.for.model = .data %>%\n  filter(Frequency != ""test"") %>% \n  mutate(norm_response.for.model = ifelse((Frame < 20 |Frame >130), rollavg, NA)) #select Frame 1~9, 41~50\nhead(.data.for.model)\n.data.for.model <- .data.for.model[!(.data.for.model$Frame==1),]\n\nmodel = .data.for.model %>% group_by(Fly, Frequency, rep) %>%\n  do(fit = nls(norm_response.for.model ~ a * Frame + b, data = ., start = list(a = 1, b = 0.1))) %>% #fit to lm\n  mutate(tidys = list(broom::tidy(fit))) %>%\n  unnest(tidys)\nhead(model)\n\nmodel.coef = model %>% \n  dplyr::select(Fly, Frequency, rep, term, estimate) %>%\n  tidyr::spread(key = term, value = estimate)\nhead(model.coef) \n\ndata.fitting = .data %>%\n  dplyr::filter(Frequency != ""test"") %>%\n  dplyr::select(-response, -background) %>% \n  left_join(., model.coef, by = c(""Fly"", ""Frequency"", ""rep"")) %>%\n  dplyr::mutate(fit_response = a * Frame + b) %>%\n  dplyr::mutate(norm_norm_response = rollavg - fit_response)\nhead(data.fitting)\n\n\ndata.fitting.mean = data.fitting %>% dplyr::filter(Frame > 1, Frame < 150) %>% group_by(Fly, Frame, Frequency) %>%\n  dplyr::summarise(average.response = mean(norm_norm_response), sd.response = sd(norm_norm_response))\ndata.fitting.mean =  dplyr::mutate(data.fitting.mean, time = Frame/10)\nhead(data.fitting.mean)\ndata.fitting.mean$Frequency = as.numeric(data.fitting.mean$Frequency)\ndata.fitting.mean[order(data.fitting.mean$Frequency, decreasing=F),]\n\nwrite.csv(data.fitting.mean, ""timetrace_imaging_3E_melanogaster.csv"")\n\n\nmax.data.fitting.mean = data.fitting.mean %>%\n  group_by(Fly, Frequency) %>%\n  summarize(max = max(average.response, na.rm = TRUE))\nintegral = max.data.fitting.mean %>% dplyr::group_by(Fly) %>%\n  dplyr::summarise(integral = sum(max))\nmax.data.fitting.mean = full_join(max.data.fitting.mean, integral) %>%\n  mutate(normmax = max/integral) %>%\n  group_by(Frequency) %>%\n  mutate(rank = row_number(Fly)) %>%\n  dplyr::select(ID = rank, dplyr::everything())\n\nwrite.csv(max.data.fitting.mean, ""normmax_imaging_3F_melanogaster.csv"")\n\n#####simulans####\n\ndata= NULL\nfor (i in 1:8) {\n  sname <- paste(""Fly"",i, sep="""") \n  .data = read.xlsx(""raw_data_imaging_3E_simulans.xlsx"",sheetName= sname) %>%\n    dplyr::mutate(Fly = paste(i)) %>%\n    tidyr::gather(key = Frequency, value = response, -Frame, -Fly)\n  .data$Frequency = str_sub(.data$Frequency, start = 2)\n  background = .data %>%\n    dplyr::filter(Frame < 20) %>%\n    dplyr::group_by(Frequency) %>%\n    dplyr::summarise(background = mean(response))\n  ..data = full_join(.data, background) %>%\n    mutate(norm_response = (response-background)/background)\n  data = rbind (data, ..data) # bind the files\n}\nhead(data)\ndata $Fly = as.numeric(data$Fly)\n\n.data = data %>%\n  dplyr::filter(grepl(""Hz"", Frequency)) %>%\n  tidyr::separate(col = Frequency, into = c(""Frequency"", ""Hz"", ""rep""), sep = ""_"") %>%\n  dplyr::select(-Hz) \nhead(.data)\n\n.data$Fly = as.factor(.data$Fly)\nFrequency_order = as.character(c(40, seq(100, 300, by=100)))\n.data$Hz = factor(.data$Frequency, levels = Frequency_order)\n\nfnrollmean <- function (x) {\n  if (length(x) < 3) {\n    rep(NA,length(x)) \n  } else {\n    rollmean(x,3,align=""center"",na.pad=TRUE)\n  }\n}\n.data <- .data %>% group_by(Fly,Frequency) %>% \n  mutate(rollavg=fnrollmean(norm_response))\n\n\n.data.mean = .data %>% group_by(Frame, Fly, Frequency) %>%\n  dplyr::summarise(mean.response = mean(rollavg), sd.response = sd(rollavg))\nhead(.data.mean)\n\n\n.data.for.model = .data %>%\n  filter(Frequency != ""test"") %>% \n  mutate(norm_response.for.model = ifelse((Frame < 20 |Frame >130), rollavg, NA)) #select Frame 1~9, 41~50\nhead(.data.for.model)\n.data.for.model <- .data.for.model[!(.data.for.model$Frame==1),]\n\nmodel = .data.for.model %>% group_by(Fly, Frequency, rep) %>%\n  do(fit = nls(norm_response.for.model ~ a * Frame + b, data = ., start = list(a = 1, b = 0.1))) %>% #fit to lm\n  mutate(tidys = list(broom::tidy(fit))) %>%\n  unnest(tidys)\nhead(model)\n\nmodel.coef = model %>% \n  dplyr::select(Fly, Frequency, rep, term, estimate) %>%\n  tidyr::spread(key = term, value = estimate)\nhead(model.coef) \n\ndata.fitting = .data %>%\n  dplyr::filter(Frequency != ""test"") %>%\n  dplyr::select(-response, -background) %>% \n  left_join(., model.coef, by = c(""Fly"", ""Frequency"", ""rep"")) %>%\n  dplyr::mutate(fit_response = a * Frame + b) %>%\n  dplyr::mutate(norm_norm_response = rollavg - fit_response)\nhead(data.fitting)\n\n\ndata.fitting.mean = data.fitting %>% dplyr::filter(Frame > 1, Frame < 150) %>% group_by(Fly, Frame, Frequency) %>%\n  dplyr::summarise(average.response = mean(norm_norm_response), sd.response = sd(norm_norm_response))\ndata.fitting.mean =  dplyr::mutate(data.fitting.mean, time = Frame/10)\nhead(data.fitting.mean)\ndata.fitting.mean$Frequency = as.numeric(data.fitting.mean$Frequency)\ndata.fitting.mean[order(data.fitting.mean$Frequency, decreasing=F),]\n\nwrite.csv(data.fitting.mean, ""timetrace_imaging_3E_simulans.csv"")\n\n\nmax.data.fitting.mean = data.fitting.mean %>%\n  group_by(Fly, Frequency) %>%\n  summarize(max = max(average.response, na.rm = TRUE))\nintegral = max.data.fitting.mean %>% dplyr::group_by(Fly) %>%\n  dplyr::summarise(integral = sum(max))\nmax.data.fitting.mean = full_join(max.data.fitting.mean, integral) %>%\n  mutate(normmax = max/integral) %>%\n  group_by(Frequency) %>%\n  mutate(rank = row_number(Fly)) %>%\n  dplyr::select(ID = rank, dplyr::everything())\n\nwrite.csv(max.data.fitting.mean, ""normmax_imaging_3E_simulans.csv"")\n\n\n\n\n\n', 'source (""Library.R"")\n\n# read data\ndata= NULL\nfor (i in 1:8) {\n  sname <- paste(""Fly"",i, sep="""") \n  .data = read.xlsx(""raw_data_imaging_S4B.xlsx"",sheetName= sname) %>%\n    dplyr::mutate(Fly = paste(i)) %>%\n    tidyr::gather(key = sound, value = response, -Frame, -Fly) \n  background = .data %>%\n    dplyr::filter(Frame < 10) %>%\n    dplyr::group_by(sound) %>%\n    dplyr::summarise(background = mean(response))\n  ..data = full_join(.data, background) %>%\n    mutate(norm_response = (response-background)/background)\n  data = rbind (data, ..data) \n}\nhead(data)\n\ndata $Fly = as.numeric(data$Fly)\n\n.data = data %>%\n  dplyr::filter(grepl(""Hz"", sound)) %>%\n  tidyr::separate(col = sound, into = c(""Frequency"", ""Hz"",""IPI"",""ms"", ""rep""), sep = ""_"") %>%\n  dplyr::select(-Hz, -ms) \nhead(.data)\n\n.data$Fly = as.factor(.data$Fly)\nFrequency_order = as.character(c(167, 167, 167, 333, 333, 333))\nIPI_order = as.character(c(15, 25, 35, 15, 25, 35))\ngroup_order = as.character(c(""167_15"",""167_25"",""167_35"", ""333_15"", ""333_25"", ""333_35"" ))\n.data <- .data %>% mutate(group = paste(!!!rlang::syms(c(""Frequency"", ""IPI"")), sep=""_""))\n\nfnrollmean <- function (x) {\n  if (length(x) < 3) {\n    rep(NA,length(x)) \n  } else {\n    rollmean(x,3,align=""center"",na.pad=TRUE)\n  }\n}\n\n.data <- .data %>% group_by(Fly,group) %>% \n  mutate(rollavg=fnrollmean(norm_response))\nhead(.data)\n\n.data.mean = .data %>% group_by(Frame, Fly, group) %>%\n  dplyr::summarise(mean.response = mean(rollavg), sd.response = sd(rollavg))\nhead(.data.mean)\n\n\n.data.for.model = .data %>%\n  mutate(norm_response.for.model = ifelse((Frame < 10 |Frame >40), rollavg, NA)) #select Frame 1~9, 41~50\nhead(.data.for.model)\n.data.for.model <- .data.for.model[!(.data.for.model$Frame==1),]\n\nmodel = .data.for.model %>% group_by(Fly, group, rep) %>%\n  do(fit = nls(norm_response.for.model ~ a * Frame + b, data = ., start = list(a = 1, b = 0.1))) %>% #fit to lm\n  mutate(tidys = list(broom::tidy(fit))) %>%\n  unnest(tidys)\nhead(model)\n\nmodel.coef = model %>% \n  dplyr::select(Fly, group, rep, term, estimate) %>%\n  tidyr::spread(key = term, value = estimate)\nhead(model.coef) \n\ndata.fitting = .data %>%\n  dplyr::filter(group != ""test"") %>%\n  dplyr::select(-response, -background) %>% \n  left_join(., model.coef, by = c(""Fly"", ""group"", ""rep"")) %>%\n  dplyr::mutate(fit_response = a * Frame + b) %>%\n  dplyr::mutate(norm_norm_response = rollavg - fit_response)\nhead(data.fitting)\n\n\ndata.fitting.mean = data.fitting %>% dplyr::filter(Frame > 1, Frame < 50) %>% group_by(Fly, Frame, IPI, Frequency, group) %>%\n  dplyr::summarise(average.response = mean(norm_norm_response), sd.response = sd(norm_norm_response))\ndata.fitting.mean =  dplyr::mutate(data.fitting.mean, time = Frame/10)\nhead(data.fitting.mean)\n\nwrite.csv(data.fitting.mean, ""timetrace_imaging_S4B.csv"")\n\n\nmax.data.fitting.mean = data.fitting.mean %>%\n  group_by(Fly, IPI, Frequency, group) %>%\n  summarize(max = max(average.response, na.rm = TRUE))\nintegral = max.data.fitting.mean %>% dplyr::group_by(Fly, Frequency) %>%\n  dplyr::summarise(integral = sum(max))\nmax.data.fitting.mean = full_join(max.data.fitting.mean, integral)\nmax.data.fitting.mean = max.data.fitting.mean %>% mutate(normmax = max/integral)\nmax.data.fitting.mean  <- max.data.fitting.mean %>%\n  group_by(IPI, Frequency) %>%\n  mutate(rank = row_number(Fly))\nmax.data.fitting.mean <- dplyr::select(max.data.fitting.mean, ID = rank, dplyr::everything())\n\nhead(max.data.fitting.mean)\n\nwrite.csv(max.data.fitting.mean, ""norm_max_mean_S4B.csv"")\n\n\n\nst_data <- read.csv(""norm_max_mean_S4B.csv"")\nst_data = st_data[, colnames(st_data) %in% c(""ID"",""group"",""Frequency"", ""IPI"", ""normmax"")]\nhead(st_data)\n\n\norder_f <- c(15, 35)\n\nst = st_data %>% filter (IPI == 25)\nnames(st)[ which( names(st)==""normmax"" ) ] <- ""max25""\n\nst_167 = st %>% filter(Frequency == 167)\nst_333 = st %>% filter(Frequency == 333)\n\nst_gp = st_data %>% filter (IPI == 25)\nnames(st_gp)[ which( names(st_gp)==""normmax"" ) ] <- ""max25""\nst_gp25 = st_gp[, colnames(st_gp) == ""max25""]\n\nst_gp = NULL\nfor(i in order_f){\n  st_i = st_data %>% filter(IPI == i)\n  st_i = st_i[, colnames(st_i) %in% c(""Frequency"",""ID"", ""IPI"",""group"", ""normmax"")]\n  st_ij = cbind(st_i, st_gp25) %>%\n    mutate(df = normmax - st_gp25) \n  st_ij = st_ij[, colnames(st_ij) %in% c(""Frequency"",""ID"", ""IPI"", ""group"", ""df"")]\n  st_gp = rbind(st_gp, st_ij)\n}\n\nwrite.csv(st_gp, ""delta_response_S4B.csv"")\n\n\n', 'source (""Library.R"")\n\n#####melanoagster#####\n\ndata= NULL\nfor (i in 1:12) {\n  sname <- paste(""Fly"",i, sep="""") \n  .data = read.xlsx(""raw_data_imaging_4B_melanogaster.xlsx"",sheetName= sname) %>%\n    dplyr::mutate(Fly = paste(i)) %>%\n    tidyr::gather(key = IPI, value = response, -Frame, -Fly)\n  background = .data %>%\n    dplyr::filter(Frame < 20) %>%\n    dplyr::group_by(IPI) %>%\n    dplyr::summarise(background = mean(response))\n  ..data = full_join(.data, background) %>%\n    mutate(norm_response = (response-background)/background)\n  data = rbind (data, ..data) # bind the files\n}\nhead(data)\ndata $Fly = as.numeric(data$Fly)\n\n\ndata $Fly = as.numeric(data$Fly)\n\n.data = data %>%\n  dplyr::filter(grepl(""IPI"", IPI)) %>%\n  tidyr::separate(col = IPI, into = c(""value"", ""IPI"", ""rep""), sep = ""_"") %>%\n  dplyr::select(-value) \nhead(.data)\n\n.data$Fly = as.factor(.data$Fly)\nIPI_order = as.character(c(seq(15, 105, by=10)))\n.data$IPI = factor(.data$IPI, levels = IPI_order)\n\nfnrollmean <- function (x) {\n  if (length(x) < 3) {\n    rep(NA,length(x)) \n  } else {\n    rollmean(x,3,align=""center"",na.pad=TRUE)\n  }\n}\n\n.data <- .data %>% group_by(Fly,IPI) %>% \n  mutate(rollavg=fnrollmean(norm_response))\nhead(.data)\n\n\n.data.mean = .data %>% group_by(Frame, Fly, IPI) %>%\n  dplyr::summarise(mean.response = mean(rollavg), sd.response = sd(rollavg))\nhead(.data.mean)\n\n.data.for.model = .data %>%\n  mutate(norm_response.for.model = ifelse((Frame < 10 |Frame >40), rollavg, NA)) #select Frame 1~9, 41~50\nhead(.data.for.model)\n.data.for.model <- .data.for.model[!(.data.for.model$Frame==1),]\n\nmodel = .data.for.model %>% group_by(Fly, IPI, rep) %>%\n  do(fit = nls(norm_response.for.model ~ a * Frame + b, data = ., start = list(a = 1, b = 0.1))) %>% #fit to lm\n  mutate(tidys = list(broom::tidy(fit))) %>%\n  unnest(tidys)\nhead(model)\n\nmodel.coef = model %>% \n  dplyr::select(Fly, IPI, rep, term, estimate) %>%\n  tidyr::spread(key = term, value = estimate)\nhead(model.coef) \n\ndata.fitting = .data %>%\n  dplyr::filter(IPI != ""test"") %>%\n  dplyr::select(-response, -background) %>% \n  left_join(., model.coef, by = c(""Fly"", ""IPI"", ""rep"")) %>%\n  dplyr::mutate(fit_response = a * Frame + b) %>%\n  dplyr::mutate(norm_norm_response = rollavg - fit_response)\nhead(data.fitting)\n\n\ndata.fitting.mean = data.fitting %>% dplyr::filter(Frame > 1, Frame < 50) %>% group_by(Frame, Fly, IPI) %>%\n  dplyr::summarise(average.response = mean(norm_norm_response), sd.response = sd(norm_norm_response))\ndata.fitting.mean =  dplyr::mutate(data.fitting.mean, time = Frame/10)\nhead(data.fitting.mean)\n\nwrite.csv(data.fitting.mean, ""timetrace_imaging_4B_melanogaster.csv"")\n\nmax.data.fitting.mean = data.fitting.mean %>%\n  group_by(Fly, IPI) %>%\n  summarize(max = max(average.response, na.rm = TRUE))\nintegral = max.data.fitting.mean %>% dplyr::group_by(Fly) %>%\n  dplyr::summarise(integral = sum(max))\nmax.data.fitting.mean = full_join(max.data.fitting.mean, integral) %>%\n  mutate(normmax = max/integral) %>%\n  group_by(IPI) %>%\n  mutate(rank = row_number(Fly)) %>%\n  dplyr::select(ID = rank, dplyr::everything())\n\nwrite.csv(max.data.fitting.mean, ""normmax_imaging_4C_melanogaster.csv"")\n\n#####simulans####\n\ndata= NULL\nfor (i in 1:13) {\n  sname <- paste(""Fly"",i, sep="""") \n  .data = read.xlsx(""raw_data_imaging_4B_simulans.xlsx"",sheetName= sname) %>%\n    dplyr::mutate(Fly = paste(i)) %>%\n    tidyr::gather(key = IPI, value = response, -Frame, -Fly)\n  background = .data %>%\n    dplyr::filter(Frame < 20) %>%\n    dplyr::group_by(IPI) %>%\n    dplyr::summarise(background = mean(response))\n  ..data = full_join(.data, background) %>%\n    mutate(norm_response = (response-background)/background)\n  data = rbind (data, ..data) # bind the files\n}\nhead(data)\ndata $Fly = as.numeric(data$Fly)\n\n\ndata $Fly = as.numeric(data$Fly)\n\n.data = data %>%\n  dplyr::filter(grepl(""IPI"", IPI)) %>%\n  tidyr::separate(col = IPI, into = c(""value"", ""IPI"", ""rep""), sep = ""_"") %>%\n  dplyr::select(-value) \nhead(.data)\n\n.data$Fly = as.factor(.data$Fly)\nIPI_order = as.character(c(seq(15, 105, by=10)))\n.data$IPI = factor(.data$IPI, levels = IPI_order)\n\nfnrollmean <- function (x) {\n  if (length(x) < 3) {\n    rep(NA,length(x)) \n  } else {\n    rollmean(x,3,align=""center"",na.pad=TRUE)\n  }\n}\n\n.data <- .data %>% group_by(Fly,IPI) %>% \n  mutate(rollavg=fnrollmean(norm_response))\nhead(.data)\n\n\n.data.mean = .data %>% group_by(Frame, Fly, IPI) %>%\n  dplyr::summarise(mean.response = mean(rollavg), sd.response = sd(rollavg))\nhead(.data.mean)\n\n.data.for.model = .data %>%\n  mutate(norm_response.for.model = ifelse((Frame < 10 |Frame >40), rollavg, NA)) #select Frame 1~9, 41~50\nhead(.data.for.model)\n.data.for.model <- .data.for.model[!(.data.for.model$Frame==1),]\n\nmodel = .data.for.model %>% group_by(Fly, IPI, rep) %>%\n  do(fit = nls(norm_response.for.model ~ a * Frame + b, data = ., start = list(a = 1, b = 0.1))) %>% #fit to lm\n  mutate(tidys = list(broom::tidy(fit))) %>%\n  unnest(tidys)\nhead(model)\n\nmodel.coef = model %>% \n  dplyr::select(Fly, IPI, rep, term, estimate) %>%\n  tidyr::spread(key = term, value = estimate)\nhead(model.coef) \n\ndata.fitting = .data %>%\n  dplyr::filter(IPI != ""test"") %>%\n  dplyr::select(-response, -background) %>% \n  left_join(., model.coef, by = c(""Fly"", ""IPI"", ""rep"")) %>%\n  dplyr::mutate(fit_response = a * Frame + b) %>%\n  dplyr::mutate(norm_norm_response = rollavg - fit_response)\nhead(data.fitting)\n\n\ndata.fitting.mean = data.fitting %>% dplyr::filter(Frame > 1, Frame < 50) %>% group_by(Frame, Fly, IPI) %>%\n  dplyr::summarise(average.response = mean(norm_norm_response), sd.response = sd(norm_norm_response))\ndata.fitting.mean =  dplyr::mutate(data.fitting.mean, time = Frame/10)\nhead(data.fitting.mean)\n\nwrite.csv(data.fitting.mean, ""timetrace_imaging_4B_simulans.csv"")\n\nmax.data.fitting.mean = data.fitting.mean %>%\n  group_by(Fly, IPI) %>%\n  summarize(max = max(average.response, na.rm = TRUE))\nintegral = max.data.fitting.mean %>% dplyr::group_by(Fly) %>%\n  dplyr::summarise(integral = sum(max))\nmax.data.fitting.mean = full_join(max.data.fitting.mean, integral) %>%\n  mutate(normmax = max/integral) %>%\n  group_by(IPI) %>%\n  mutate(rank = row_number(Fly)) %>%\n  dplyr::select(ID = rank, dplyr::everything())\n\nwrite.csv(max.data.fitting.mean, ""normmax_imaging_4C_simulans.csv"")\n\n#####delta response (Figure 4D)#####\n\n\nst_data <- read.csv(""normmax_imaging_4C_melanogaster.csv"") %>% mutate(species = ""mel"")\nst_data = st_data[, colnames(st_data) %in% c(""species"",""ID"", ""IPI"", ""normmax"")]\n.st_data <- read.csv(""normmax_imaging_4C_simulans.csv"") %>% mutate(species = ""sim"")\n.st_data = .st_data[, colnames(.st_data) %in% c(""species"",""ID"", ""IPI"", ""normmax"")]\nst_data <- rbind(st_data, .st_data)\nhead(st_data)\n\n\norder_f <- c(15, seq(35, 105, by = 10))\n\nst = st_data %>% filter (IPI == 25)\nnames(st)[ which( names(st)==""normmax"" ) ] <- ""max25""\n\nst_mel = st %>% filter(species == ""mel"")\nst_sim = st %>% filter(species == ""sim"")\n\nst_gp = st_data %>% filter (IPI == 25)\nnames(st_gp)[ which( names(st_gp)==""normmax"" ) ] <- ""max25""\nst_gp25 = st_gp[, colnames(st_gp) == ""max25""]\n\nst_gp = NULL\nfor(i in order_f){\n  st_i = st_data %>% filter(IPI == i)\n  st_i = st_i[, colnames(st_i) %in% c(""species"",""ID"", ""IPI"", ""normmax"")]\n  st_ij = cbind(st_i, st_gp25) %>%\n    mutate(df = normmax - st_gp25) \n  st_ij = st_ij[, colnames(st_ij) %in% c(""species"",""ID"", ""IPI"", ""df"")]\n  st_gp = rbind(st_gp, st_ij)\n}\n\nwrite.csv(st_gp, ""delta_response_4D.csv"")\n\n\n', 'source (""Library.R"")\n\n#####melanogaster####\n\ndata= NULL\nfor (i in 1:5) {\n  sname <- paste(""Fly"",i, sep="""") \n  .data = read.xlsx(""raw_data_imaging_2F.xlsx"",sheetName= sname) %>%\n    dplyr::mutate(Fly = paste(i)) %>%\n    tidyr::gather(key = Frequency, value = response, -Frame, -Fly)\n  .data$Frequency = str_sub(.data$Frequency, start = 2)\n  background = .data %>%\n    dplyr::filter(Frame < 20) %>%\n    dplyr::group_by(Frequency) %>%\n    dplyr::summarise(background = mean(response))\n  ..data = full_join(.data, background) %>%\n    mutate(norm_response = (response-background)/background)\n  data = rbind (data, ..data) # bind the files\n}\nhead(data)\ndata $Fly = as.numeric(data$Fly)\n\n.data = data %>%\n  dplyr::filter(grepl(""Hz"", Frequency)) %>%\n  tidyr::separate(col = Frequency, into = c(""Frequency"", ""Hz"", ""rep""), sep = ""_"") %>%\n  dplyr::select(-Hz) \nhead(.data)\n\n.data$Fly = as.factor(.data$Fly)\nFrequency_order = as.character(c(167, 333))\n.data$Hz = factor(.data$Frequency, levels = Frequency_order)\n\nfnrollmean <- function (x) {\n  if (length(x) < 3) {\n    rep(NA,length(x)) \n  } else {\n    rollmean(x,3,align=""center"",na.pad=TRUE)\n  }\n}\n.data <- .data %>% group_by(Fly,Frequency) %>% \n  mutate(rollavg=fnrollmean(norm_response))\n\n\n.data.mean = .data %>% group_by(Frame, Fly, Frequency) %>%\n  dplyr::summarise(mean.response = mean(rollavg), sd.response = sd(rollavg))\nhead(.data.mean)\n\n\n.data.for.model = .data %>%\n  filter(Frequency != ""test"") %>% \n  mutate(norm_response.for.model = ifelse((Frame < 20 |Frame >130), rollavg, NA)) #select Frame 1~9, 41~50\nhead(.data.for.model)\n.data.for.model <- .data.for.model[!(.data.for.model$Frame==1),]\n\nmodel = .data.for.model %>% group_by(Fly, Frequency, rep) %>%\n  do(fit = nls(norm_response.for.model ~ a * Frame + b, data = ., start = list(a = 1, b = 0.1))) %>% #fit to lm\n  mutate(tidys = list(broom::tidy(fit))) %>%\n  unnest(tidys)\nhead(model)\n\nmodel.coef = model %>% \n  dplyr::select(Fly, Frequency, rep, term, estimate) %>%\n  tidyr::spread(key = term, value = estimate)\nhead(model.coef) \n\ndata.fitting = .data %>%\n  dplyr::filter(Frequency != ""test"") %>%\n  dplyr::select(-response, -background) %>% \n  left_join(., model.coef, by = c(""Fly"", ""Frequency"", ""rep"")) %>%\n  dplyr::mutate(fit_response = a * Frame + b) %>%\n  dplyr::mutate(norm_norm_response = rollavg - fit_response)\nhead(data.fitting)\n\n\ndata.fitting.mean = data.fitting %>% dplyr::filter(Frame > 1, Frame < 150) %>% group_by(Fly, Frame, Frequency) %>%\n  dplyr::summarise(average.response = mean(norm_norm_response), sd.response = sd(norm_norm_response))\ndata.fitting.mean =  dplyr::mutate(data.fitting.mean, time = Frame/10)\nhead(data.fitting.mean)\ndata.fitting.mean$Frequency = as.numeric(data.fitting.mean$Frequency)\ndata.fitting.mean[order(data.fitting.mean$Frequency, decreasing=F),]\n\nwrite.csv(data.fitting.mean, ""timetrace_imaging_2F.csv"")\n', 'source (""Library.R"")\n\n\n####Figure 1 ####\n\na <- read_excel(""copulation_rate.xlsx"") %>%\n  dplyr::mutate(ctime=ctime/60) %>%\n  dplyr::mutate(event=ifelse(ctime>30, 0, 1)) %>%\n  dplyr::filter(ctime >0) %>%\n  dplyr::mutate(ID = row_number())\na$IPI <- as.factor(as.character(a$IPI))\na$species <- as.factor(a$species)\n\nIPI_order <- c(""35"", ""15"", ""55"", ""75"", ""95"")\nspecies_order <- c(""mel"", ""sim"")\na$IPI <- factor(a$IPI, levels = IPI_order)\na$species <- factor(a$species, levels = species_order)\n\n\nsurv_object <- Surv(time = a$ctime, event = a$event)\nfit.coxph <- coxph(Surv(ctime, event) ~ IPI * species,\n                   data = a)\n\nfit.zph <- cox.zph(fit.coxph)\nfit.zph\n\n\n#Divide time to keep hazard proportionality\n\na.split <- survSplit(Surv(ctime, event)~.,\n                     data = a, cut = c(7, 31),\n                     episode = ""tgroup"",\n                     id =""ID1"")\n\na.split0 <- subset(a.split, tstart == 0)#0-7\na.split7 <- subset(a.split, tstart == 7)#7-30\n\n\nfit.split0 <- coxph(Surv(ctime, event) ~ \n                      IPI*species,\n                    data = a.split0)\ncox.zph(fit.split0)\n\nfit.split7 <- coxph(Surv(ctime, event) ~ \n                       IPI * species,\n                     data = a.split7)\ncox.zph(fit.split7)\n\nsum_fit0 <- summary(fit.split0)\nsum_fit7 <- summary(fit.split7)\n\nHR_data = NULL\ngroup_order = c(""IPI(35-15ms)"", ""IPI(35-55ms)"",""IPI(35-75ms)"", ""IPI(35-95ms)"",\n                ""melanogaster-simulans"", ""(35-15)*(mel-sim)"", ""(35-55)*(mel-sim)"",\n                ""(35-75)*(mel-sim)"", ""(35-95)*(mel-sim)"")\n\n\n\n.HR_data = data_frame(Time = ""0-7 min"", \n                      group = group_order,\n                      HR = sum_fit0$conf.int[c(1:9)],\n                      lower.95 = sum_fit0$conf.int[c(19:27)],\n                      upper.95 = sum_fit0$conf.int[c(28:36)],\n                      p.value = sum_fit0$coefficients[c(37:45)])\n..HR_data = data_frame(Time = ""7-30 min"", \n                       group = group_order,\n                      HR = sum_fit7$conf.int[c(1:9)],\n                      lower.95 = sum_fit7$conf.int[c(19:27)],\n                      upper.95 = sum_fit7$conf.int[c(28:36)],\n                      p.value = sum_fit7$coefficients[c(37:45)])\nHR_data <- rbind(.HR_data, ..HR_data)\n\nwrite.csv(HR_data, ""Hazard_ratio.csv"")\n\n####Figure S1B ####\n\na <- read_excel(""copulation_rate_S1B.xlsx"") %>%\n  dplyr::mutate(ctime=ctime/60) %>%\n  dplyr::mutate(event=ifelse(ctime>30, 0, 1)) %>%\n  dplyr::filter(ctime >0) %>%\n  dplyr::mutate(ID = row_number())\na$IPI <- as.factor(as.character(a$IPI))\na$species <- as.factor(a$species)\n\nIPI_order <- c(""35"", ""15"", ""55"")\nspecies_order <- c(""mel"", ""sim"")\na$IPI <- factor(a$IPI, levels = IPI_order)\na$species <- factor(a$species, levels = species_order)\n\n\nsurv_object <- Surv(time = a$ctime, event = a$event)\nfit.coxph <- coxph(Surv(ctime, event) ~ IPI * species,\n                   data = a)\n\nfit.zph <- cox.zph(fit.coxph)\nfit.zph\n\n\n#Divide time to keep hazard proportionality\n\na.split <- survSplit(Surv(ctime, event)~.,\n                     data = a, cut = c(7, 31),\n                     episode = ""tgroup"",\n                     id =""ID1"")\n\na.split0 <- subset(a.split, tstart == 0)#0-7\na.split7 <- subset(a.split, tstart == 7)#7-30\n\n\nfit.split0 <- coxph(Surv(ctime, event) ~ \n                      IPI*species,\n                    data = a.split0)\ncox.zph(fit.split0)\n\nfit.split7 <- coxph(Surv(ctime, event) ~ \n                       IPI * species,\n                     data = a.split7)\ncox.zph(fit.split7)\n\nsum_fit0 <- summary(fit.split0)\nsum_fit7 <- summary(fit.split7)\n\nHR_data = NULL\ngroup_order = c(""IPI(35-15ms)"", ""IPI(35-55ms)"",\n                ""melanogaster-simulans"", ""(35-15)*(mel-sim)"", ""(35-55)*(mel-sim)"")\n\n\n\n.HR_data = data_frame(Time = ""0-7 min"", \n                      group = group_order,\n                      HR = sum_fit0$conf.int[c(1:5)],\n                      lower.95 = sum_fit0$conf.int[c(11:15)],\n                      upper.95 = sum_fit0$conf.int[c(16:20)],\n                      p.value = sum_fit0$coefficients[c(21:25)])\n..HR_data = data_frame(Time = ""7-30 min"", \n                       group = group_order,\n                       HR = sum_fit7$conf.int[c(1:5)],\n                       lower.95 = sum_fit7$conf.int[c(11:15)],\n                       upper.95 = sum_fit7$conf.int[c(16:20)],\n                       p.value = sum_fit7$coefficients[c(21:25)])\nHR_data <- rbind(.HR_data, ..HR_data)\n\nwrite.csv(HR_data, ""Hazard_ratio_S1B.csv"")\n\n\n\n\n', 'library(ggplot2)\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(zoo)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(xlsx)\nlibrary(stringr)\nlibrary(cmprsk)\nlibrary(survival)\nlibrary(survminer)\nlibrary(readxl)\nlibrary(data.table)\nlibrary(magrittr)\nlibrary(Greg)\nlibrary(timereg)\nlibrary(ggsci)\nlibrary(reshape2)\nlibrary(gt)\nlibrary(powerSurvEpi)\nlibrary(survRM2)\n', 'source (""Library.R"")\n\n#####Figure 1#####\n\nRMTL = NULL\ndata <- read_excel(""copulation_rate.xlsx"") %>%\n  dplyr::mutate(ctime=ctime/60) %>% \n  dplyr:: mutate(event=ifelse(ctime>30, 0, 1)) %>%\n  dplyr:: filter(ctime > 0)\n\nmel1535 <- data[(data$species == ""mel"" & (data$IPI ==15 | data$IPI ==35)),] %>% \n  dplyr:: mutate(sound=ifelse(IPI==15, 0, 1))\nrfit_mel1535 <- rmst2(mel1535$ctime, mel1535$event, mel1535$sound)\n\n.RMTL<- data.frame(species = ""mel"", IPI = 15, \n                   estimate = rfit_mel1535$RMST.arm0$rmtl[1],\n                   lower.95 = rfit_mel1535$RMST.arm0$rmtl[3],\n                   upper.95 = rfit_mel1535$RMST.arm0$rmtl[4],\n                   se = rfit_mel1535$RMST.arm0$rmtl[2])\n..RMTL <- data.frame(species = ""mel"", IPI = 35, \n                     estimate = rfit_mel1535$RMST.arm1$rmtl[1],\n                     lower.95 = rfit_mel1535$RMST.arm1$rmtl[3],\n                     upper.95 = rfit_mel1535$RMST.arm1$rmtl[4],\n                     se = rfit_mel1535$RMST.arm1$rmtl[2])\nRMTL <- rbind(.RMTL, ..RMTL)\n\nmel5575 <- data[(data$species == ""mel"" &(data$IPI ==55 | data$IPI ==75)),] %>% \n  dplyr:: mutate(sound=ifelse(IPI==55, 0, 1))\nrfit_mel5575 <- rmst2(mel5575$ctime, mel5575$event, mel5575$sound)\n\n.RMTL<- data.frame(species = ""mel"", IPI = 55, \n                   estimate = rfit_mel5575$RMST.arm0$rmtl[1],\n                   lower.95 = rfit_mel5575$RMST.arm0$rmtl[3],\n                   upper.95 = rfit_mel5575$RMST.arm0$rmtl[4],\n                   se = rfit_mel5575$RMST.arm0$rmtl[2])\n..RMTL <- data.frame(species = ""mel"", IPI = 75, \n                     estimate = rfit_mel5575$RMST.arm1$rmtl[1],\n                     lower.95 = rfit_mel5575$RMST.arm1$rmtl[3],\n                     upper.95 = rfit_mel5575$RMST.arm1$rmtl[4],\n                     se = rfit_mel5575$RMST.arm1$rmtl[2])\nRMTL <- rbind(RMTL, .RMTL, ..RMTL)\n\nmel95sim15 <- rbind(data[(data$species == ""mel"" &data$IPI ==95)|(data$species == ""sim"" & data$IPI ==15),]) %>% \n  dplyr:: mutate(sound=ifelse(IPI==95, 0, 1))\nrfit_mel95sim15 <- rmst2(mel95sim15$ctime, mel95sim15$event, mel95sim15$sound)\n\n.RMTL<- data.frame(species = ""mel"", IPI = 95, \n                   estimate = rfit_mel95sim15$RMST.arm0$rmtl[1],\n                   lower.95 = rfit_mel95sim15$RMST.arm0$rmtl[3],\n                   upper.95 = rfit_mel95sim15$RMST.arm0$rmtl[4],\n                   se = rfit_mel95sim15$RMST.arm0$rmtl[2])\n..RMTL <- data.frame(species = ""sim"", IPI = 15, \n                     estimate = rfit_mel95sim15$RMST.arm1$rmtl[1],\n                     lower.95 = rfit_mel95sim15$RMST.arm1$rmtl[3],\n                     upper.95 = rfit_mel95sim15$RMST.arm1$rmtl[4],\n                     se = rfit_mel95sim15$RMST.arm1$rmtl[2])\nRMTL <- rbind(RMTL, .RMTL, ..RMTL)\n\nsim3555 <- data[data$species == ""sim"" & (data$IPI ==35 | data$IPI ==55),] %>% \n  dplyr:: mutate(sound=ifelse(IPI==35, 0, 1))\nrfit_sim3555 <- rmst2(sim3555$ctime, sim3555$event, sim3555$sound)\n\n.RMTL<- data.frame(species = ""sim"", IPI = 35, \n                   estimate = rfit_sim3555$RMST.arm0$rmtl[1],\n                   lower.95 = rfit_sim3555$RMST.arm0$rmtl[3],\n                   upper.95 = rfit_sim3555$RMST.arm0$rmtl[4],\n                   se = rfit_sim3555$RMST.arm0$rmtl[2])\n..RMTL <- data.frame(species = ""sim"", IPI = 55, \n                     estimate = rfit_sim3555$RMST.arm1$rmtl[1],\n                     lower.95 = rfit_sim3555$RMST.arm1$rmtl[3],\n                     upper.95 = rfit_sim3555$RMST.arm1$rmtl[4],\n                     se = rfit_sim3555$RMST.arm1$rmtl[2])\nRMTL <- rbind(RMTL, .RMTL, ..RMTL)\n\nsim7595 <- data[data$species == ""sim"" & (data$IPI ==75 | data$IPI ==95),] %>% \n  dplyr:: mutate(sound=ifelse(IPI==75, 0, 1))\nrfit_sim7595 <- rmst2(sim7595$ctime, sim7595$event, sim7595$sound)\n\n.RMTL<- data.frame(species = ""sim"", IPI = 75, \n                   estimate = rfit_sim7595$RMST.arm0$rmtl[1],\n                   lower.95 = rfit_sim7595$RMST.arm0$rmtl[3],\n                   upper.95 = rfit_sim7595$RMST.arm0$rmtl[4],\n                   se = rfit_sim7595$RMST.arm0$rmtl[2])\n..RMTL <- data.frame(species = ""sim"", IPI = 95, \n                     estimate = rfit_sim7595$RMST.arm1$rmtl[1],\n                     lower.95 = rfit_sim7595$RMST.arm1$rmtl[3],\n                     upper.95 = rfit_sim7595$RMST.arm1$rmtl[4],\n                     se = rfit_sim7595$RMST.arm1$rmtl[2])\nRMTL <- rbind(RMTL, .RMTL, ..RMTL)\n\nwrite.csv(RMTL, ""RMTL.csv"",  row.names=FALSE)\n\n#####Figure S1B#####\n\n\nRMTL = NULL\ndata <- read_excel(""copulation_rate_S1B.xlsx"") %>%\n  dplyr::mutate(ctime=ctime/60) %>% \n  dplyr:: mutate(event=ifelse(ctime>30, 0, 1)) %>%\n  dplyr:: filter(ctime > 0)\n\nmel1535 <- data[(data$species == ""mel"" & (data$IPI ==15 | data$IPI ==35)),] %>% \n  dplyr:: mutate(sound=ifelse(IPI==15, 0, 1))\nrfit_mel1535 <- rmst2(mel1535$ctime, mel1535$event, mel1535$sound)\n\n.RMTL<- data.frame(species = ""mel"", IPI = 15, \n                   estimate = rfit_mel1535$RMST.arm0$rmtl[1],\n                   lower.95 = rfit_mel1535$RMST.arm0$rmtl[3],\n                   upper.95 = rfit_mel1535$RMST.arm0$rmtl[4],\n                   se = rfit_mel1535$RMST.arm0$rmtl[2])\n..RMTL <- data.frame(species = ""mel"", IPI = 35, \n                     estimate = rfit_mel1535$RMST.arm1$rmtl[1],\n                     lower.95 = rfit_mel1535$RMST.arm1$rmtl[3],\n                     upper.95 = rfit_mel1535$RMST.arm1$rmtl[4],\n                     se = rfit_mel1535$RMST.arm1$rmtl[2])\nRMTL <- rbind(.RMTL, ..RMTL)\n\n\nmel55sim15 <- rbind(data[(data$species == ""mel"" &data$IPI ==55)|(data$species == ""sim"" & data$IPI ==15),]) %>% \n  dplyr:: mutate(sound=ifelse(IPI==55, 0, 1))\nrfit_mel55sim15 <- rmst2(mel55sim15$ctime, mel55sim15$event, mel55sim15$sound)\n\n.RMTL<- data.frame(species = ""mel"", IPI = 55, \n                   estimate = rfit_mel55sim15$RMST.arm0$rmtl[1],\n                   lower.95 = rfit_mel55sim15$RMST.arm0$rmtl[3],\n                   upper.95 = rfit_mel55sim15$RMST.arm0$rmtl[4],\n                   se = rfit_mel55sim15$RMST.arm0$rmtl[2])\n..RMTL <- data.frame(species = ""sim"", IPI = 15, \n                     estimate = rfit_mel95sim15$RMST.arm1$rmtl[1],\n                     lower.95 = rfit_mel95sim15$RMST.arm1$rmtl[3],\n                     upper.95 = rfit_mel95sim15$RMST.arm1$rmtl[4],\n                     se = rfit_mel55sim15$RMST.arm1$rmtl[2])\nRMTL <- rbind(RMTL, .RMTL, ..RMTL)\n\nsim3555 <- data[data$species == ""sim"" & (data$IPI ==35 | data$IPI ==55),] %>% \n  dplyr:: mutate(sound=ifelse(IPI==35, 0, 1))\nrfit_sim3555 <- rmst2(sim3555$ctime, sim3555$event, sim3555$sound)\n\n.RMTL<- data.frame(species = ""sim"", IPI = 35, \n                   estimate = rfit_sim3555$RMST.arm0$rmtl[1],\n                   lower.95 = rfit_sim3555$RMST.arm0$rmtl[3],\n                   upper.95 = rfit_sim3555$RMST.arm0$rmtl[4],\n                   se = rfit_sim3555$RMST.arm0$rmtl[2])\n..RMTL <- data.frame(species = ""sim"", IPI = 55, \n                     estimate = rfit_sim3555$RMST.arm1$rmtl[1],\n                     lower.95 = rfit_sim3555$RMST.arm1$rmtl[3],\n                     upper.95 = rfit_sim3555$RMST.arm1$rmtl[4],\n                     se = rfit_sim3555$RMST.arm1$rmtl[2])\nRMTL <- rbind(RMTL, .RMTL, ..RMTL)\n\n\nwrite.csv(RMTL, ""RMTL_S1B.csv"",  row.names=FALSE)\n\n\n\n']","Evolutionary conservation and diversification of auditory neural circuits that process courtship songs in Drosophila Acoustic communication signals diversify even on short evolutionary time scales. To understand how the auditory system underlying acoustic communication could evolve, we conducted a systematic comparison of the early stages of the auditory neural circuit involved in song information processing between closely-related fruit-fly species. Male Drosophila melanogaster and D. simulans produce different sound signals during mating rituals, known as courtship songs. Female flies from these species selectively increase their receptivity when they hear songs with conspecific temporal patterns. Here, we first confirmed interspecific differences in temporal pattern preferences; D. simulans preferred pulse songs with longer intervals than D. melanogaster. Primary and secondary song-relay neurons, JO neurons and AMMC-B1 neurons, shared similar morphology and neurotransmitters between species. The temporal pattern preferences of AMMC-B1 neurons were also relatively similar between species, with slight but significant differences in their band-pass properties. Although the shift direction of the response property matched that of the behavior, these differences are not large enough to explain behavioral differences in song preferences. This study enhances our understanding of the conservation and diversification of the architecture of the early-stage neural circuit which processes acoustic communication signals.",4
Eco-evolutionary causes and consequences of rarity in plants: a meta-analysis,"Species differ dramatically in their prevalence in the natural world, with many species characterized as rare due to restricted geographic distribution, low local abundance, and/or habitat specialization.We investigated eco-evolutionary causes and consequences of rarity with phylogenetically-controlled meta-analyses of population genetic diversity, fitness, and functional traits in rare and common congeneric plant species. Our syntheses included 252 rare species and 267 common congeners reported in 153 peer-reviewed articles published from 1978-2020 and one manuscript in press.Rare species have reduced population genetic diversity, depressed fitness, and smaller reproductive structures than common congeners. Rare species also could suffer from inbreeding depression and reduced fertilization efficiency.By limiting their capacity to adapt and migrate, these characteristics could influence contemporary patterns of rarity and increase the susceptibility of rare species to rapid environmental change. We recommend that future studies present more nuanced data on the extent of rarity in focal species, expose rare and common species to ecologically-relevant treatments, including reciprocal transplants, and conduct quantitative genetic and population genomic analyses across a greater array of systems. This research could elucidate the processes that contribute to rarity and generate robust predictions of extinction risks under global change.","['\r\nrm(list=ls())\r\n\r\nlibrary(ape)\r\nlibrary(nlme)\r\nlibrary(metafor)\r\nlibrary(ggplot2)\r\nlibrary(forestplot)\r\nlibrary(clubSandwich)\r\nlibrary(car)\r\nlibrary(rotl)\r\nlibrary(ape)\r\nlibrary(phytools)\r\n\r\n\r\n##This file has continuous and binary data##\r\nFitTrait<-read.delim(""~/Documents/NSF rarity project/Dryad/Fitness_Traits_Data.txt"", header=TRUE)\r\n\r\n\r\nsapply(FitTrait,class)\r\nstr(FitTrait)\r\n#remove NA from species column\r\nFitTrait <- FitTrait[!is.na(FitTrait $Species_phylogeny), ]\r\nstr(FitTrait)\r\n#Remove lines that have no effect size listed\r\nFitTrait <-FitTrait[!is.na(FitTrait $yi),]\r\n\r\n\r\n\r\n#############################################################\r\n###Combined analysis of the continuous and binary datasets###\r\n#############################################################\r\n\r\n#Standardize year, elevation, lat and long\r\nFitTrait $year <- c(scale(FitTrait $YEAR,center=TRUE, scale=TRUE))\r\nFitTrait $abs_Lat <- abs(FitTrait $Latitude)\r\nFitTrait $abs_Long <- abs(FitTrait $Longitude)\r\nFitTrait $absLat <- c(scale(FitTrait $abs_Lat,center=TRUE, scale=TRUE))\r\nFitTrait $absLong <- c(scale(FitTrait $abs_Long,center=TRUE, scale=TRUE))\r\nFitTrait $elev <- c(scale(FitTrait $Elevation,center=TRUE, scale=TRUE))\r\n\r\n#Convert certain variables to factors\r\nFitTrait $treatment <- factor(FitTrait $treatment)\r\nFitTrait $trait <- factor(FitTrait $trait)\r\nFitTrait $Species_phylogeny <- factor(FitTrait $Species_phylogeny)\r\nFitTrait $PaperID <-as.factor(FitTrait $PaperID)\r\n\r\nlevels(FitTrait $trait)\r\n\r\nstr(FitTrait)\r\n\r\ngenus_list=unique(FitTrait$Species_phylogeny)\r\ngenus_list =data.frame(genus_list)\r\n\r\n#To pull the phylogeny\r\ngenus_list$genus_list<-as.character(genus_list$genus_list)\r\nsapply(genus_list,class)\r\ntaxon_search <- tnrs_match_names(names = genus_list$genus_list, context_name = ""Land plants"")\r\nknitr::kable(taxon_search)\r\ngenus_list $ott_name <- unique_name(taxon_search)\r\ngenus_list $ott_id <- taxon_search$ott_id\r\nhits <- lapply(genus_list $ott_id, studies_find_trees, property = ""ot:ottId"", detailed = FALSE)\r\nsapply(hits, function(x) sum(x[[""n_matched_trees""]]))\r\nott_in_tree <- ott_id(taxon_search)[is_in_tree(ott_id(taxon_search))]\r\ntr <- tol_induced_subtree(ott_ids = ott_in_tree)\r\nplot(tr)\r\ntr$tip.label <- strip_ott_ids(tr$tip.label, remove_underscores = TRUE)\r\ntr$tip.label %in% genus_list $ott_name\r\n\r\n## computes branch lengths (default method is ""Grafen"", but I don\'t even see alternatives in the help var.ggnette so I did not specify anything)\r\ntree2<-compute.brlen(tr)\r\n\r\ntree2\r\nplot(tree2,type=""fan"")\r\nplot(tree2)\r\n\r\n## plotting the tree\r\nplot(tree2, cex=0.7)\r\n\r\nplot(tree2,no.margin=TRUE,edge.width=2)\r\nplotTree(tree2,ftype=""i"",fsize=0.6,lwd=1)\r\n\r\nplot(tree2, cex=0.7)\r\n\r\n\r\ntree2$edge.length\r\ngenuslist_tree2<-tree2 $tip\r\n\r\n# generate the variance-covariance matrix for phylogenetic correction\r\nsigma<-vcv.phylo(tree2, cor=TRUE)\r\n\r\n##############################################################\r\n###Conducting the phylogenetically-corrected meta-analysis ###\r\n##############################################################\r\n\r\n\r\n##The negative coefficients indicate that rare species had lower fitness than common species. Positive values indicate that rare species overperformed compared with common species.\r\n\r\n##yi and vi represent Hedge\'s g effect sizes (yi) and variances (vi) after aggregating across similar traits, as described in the text of the manuscript. In addition, to combine binary and continuous outcomes, we first calcultaed the log of the odds ratio (yi_log_OR) and variances (vi_log_OR) as binary effect sizes and then converted them to Hedge\'s g as described in the manuscript.\r\n\r\n#The Rarity column indicates the type of rarity:\r\n##GD = geographically-restricted\r\n##LA = low local abundance\r\n##HS = Habitat specialist\r\n## GD_HS = geographically-restricted, habitat specialist\r\n## GD_LA = geographically-restricted, and low local abundance\r\n## GD_HS_LA = geographically-restricted, habitat specialist, with low local abundance\r\n\r\n#concatenations\r\nFitTrait $trait_trt<-interaction(FitTrait $trait, FitTrait $treatment,sep = ""_"")\r\nFitTrait $set_trait_trt<-interaction(FitTrait $setting, FitTrait $trait_trt,sep = ""_"")\r\n\r\n##Make a copy of the species variable\r\nFitTrait $Species<-FitTrait $Species_phylogeny\r\n##Create an effect size variable\r\nFitTrait $esid<-1:nrow(FitTrait)\r\n\r\n\r\n##Make a table of rarity type by fitness or trait category\r\naddmargins(table(FitTrait $Rarity, FitTrait $trait))\r\n\r\n##Run the model and extract the results\r\nmodB<-rma.mv(yi, vi, , mods=~ (trait)  -1,random=list(~1| Species_phylogeny,~1|Species,~1|esid, ~1|PaperID),R = list(Species_phylogeny = sigma),method=""REML"",data= FitTrait)\r\nsummary(modB)\r\n\r\n##plot the results\r\nestimated<-coef(summary(modB))\r\nestimated <-round(estimated, 3)\r\n\r\nis.data.frame(estimated)\r\nrownames(estimated)<-c(\r\n""Damage from natural enemies"",\r\n""Fitness"",\r\n""Growth"", \r\n""Leaf area allocation"",\r\n""Mutualists"", \r\n""Phenology"", \r\n""Physiology"",\r\n""Plasticity"",\r\n""Juvenile recruitment"",\r\n""Reproductive size"", \r\n""Shoot allocation"", \r\n""Plant size"", \r\n""Specific Root Length"", \r\n""Survival"")\r\nestimated <-estimated[ -c(2:4) ]\r\n\r\n\r\nestimated2<-estimated[c(\r\n""Juvenile recruitment"",\r\n""Survival"",\r\n""Growth"", \r\n""Fitness"",\r\n""Reproductive size"", \r\n\r\n""Plant size"", \r\n""Shoot allocation"", \r\n""Leaf area allocation"",\r\n""Phenology"", \r\n""Physiology"",\r\n""Damage from natural enemies"",\r\n""Mutualists"", \r\n""Plasticity"",\r\n""Specific Root Length"" ),]\r\n\r\n                    \r\nresulted <- rbind(rep(NA, 4), estimated2)\r\n\r\n## Add blank rows so that I can modify the figure more easily ###\r\nlibrary(berryFunctions)\r\nlibrary(dplyr)\r\nresultedB<-tibble::rownames_to_column(resulted, ""Cat"")\r\n\r\nresultedC <-insertRows(resultedB,r=c(6:7), new=NA)\r\n\r\nforestplot(labeltext = resultedC$Cat,\r\n           resultedC[,c(""estimate"", ""ci.lb"", ""ci.ub"")],is.summary=c(FALSE, FALSE, FALSE,FALSE),\r\n           zero = 0,\r\n           cex  = 1,\r\n           lineheight = ""auto"",\r\n           xlab = ""Effect size (Hedge\'s g)"", col=fpColors(box=""royalblue"",line=""darkblue"", summary=""royalblue""), ci.vertices=TRUE, boxsize   = c(0.2,0.2,0.2))\r\n\r\n\r\n#Calculate fail-safe number\r\nfsn(yi, vi, data= FitTrait, type=""Rosenthal"")\r\nN_tot <- unique(FitTrait$PaperID)\r\nlength(N_tot)\r\n3609/(10+(5*length(N_tot)))\r\n\r\n##visualize the funnel plot\r\nfunnel(modB)\r\n\r\n##Test  for asymmetry in the funnel plot\r\nfun<-rma(yi, vi,mods=~ trait  -1, data= FitTrait,method=""REML"")\r\nregtest(fun)\r\n\r\n##Fail safe numbers for each trait category and sample sizes of studies\r\n\r\ndamage<-subset(FitTrait, trait ==""damage"")\r\nfsn(yi, vi, data= damage, type=""Rosenthal"")\r\nN_dam <- unique(damage$PaperID)\r\nlength(N_dam)\r\n\r\nfitness<-subset(FitTrait, trait ==""fitness"")\r\nfsn(yi, vi, data= fitness, type=""Rosenthal"")\r\nN_fit <- unique(fitness$PaperID)\r\nlength(N_fit)\r\n\r\ngrowth<-subset(FitTrait, trait ==""growth"")\r\nfsn(yi, vi, data= growth, type=""Rosenthal"")\r\nN_gr <- unique(growth $PaperID)\r\nlength(N_gr)\r\n\r\nleaf_area_allocation<-subset(FitTrait, trait ==""leaf_area_allocation"")\r\nfsn(yi, vi, data= leaf_area_allocation, type=""Rosenthal"")\r\nN_leaf_area_allocation <- unique(leaf_area_allocation $PaperID)\r\nlength(N_leaf_area_allocation)\r\n\r\n\r\nmutualists<-subset(FitTrait, trait ==""mutualists"")\r\nfsn(yi, vi, data= mutualists, type=""Rosenthal"")\r\nN_mutualists <- unique(mutualists $PaperID)\r\nlength(N_mutualists)\r\n\r\nphenology<-subset(FitTrait, trait ==""phenology"")\r\nfsn(yi, vi, data= phenology, type=""Rosenthal"")\r\nN_phenology <- unique(phenology $PaperID)\r\nlength(N_phenology)\r\n\r\n\r\nPhysiology<-subset(FitTrait, trait ==""Physiology"")\r\nfsn(yi, vi, data= Physiology, type=""Rosenthal"")\r\nN_Physiology <- unique(Physiology $PaperID)\r\nlength(N_Physiology)\r\n\r\n\r\nplasticity<-subset(FitTrait, trait ==""plasticity"")\r\nfsn(yi, vi, data= plasticity, type=""Rosenthal"")\r\nN_plasticity <- unique(plasticity $PaperID)\r\nlength(N_plasticity)\r\n\r\n\r\nrecruit<-subset(FitTrait, trait ==""recruitment"")\r\nfsn(yi, vi, data= recruit, type=""Rosenthal"")\r\nN_rec <- unique(recruit $PaperID)\r\nlength(N_rec)\r\n\r\nrs<-subset(FitTrait, trait ==""repro_size"")\r\nfsn(yi, vi, data= rs, type=""Rosenthal"")\r\nN_rs <- unique(rs $PaperID)\r\nlength(N_rs)\r\n\r\n\r\nshoot_allocation<-subset(FitTrait, trait ==""shoot_allocation"")\r\nfsn(yi, vi, data= shoot_allocation, type=""Rosenthal"")\r\nN_shoot_allocation <- unique(shoot_allocation $PaperID)\r\nlength(N_shoot_allocation)\r\n\r\nSize<-subset(FitTrait, trait ==""Size"")\r\nfsn(yi, vi, data= Size, type=""Rosenthal"")\r\nN_Size <- unique(Size $PaperID)\r\nlength(N_Size)\r\n\r\nsurvival<-subset(FitTrait, trait ==""survival"")\r\nfsn(yi, vi, data= survival, type=""Rosenthal"")\r\nN_survival <- unique(survival $PaperID)\r\nlength(N_survival)\r\n\r\nsurvival<-subset(FitTrait, trait ==""survival"")\r\nfsn(OR_yi,OR_vi, data= survival, type=""Rosenthal"")\r\nN_survival <- unique(survival $PaperID)\r\nlength(N_survival)\r\n\r\nSRL<-subset(FitTrait, trait ==""SRL"")\r\nfsn(yi, vi, data= SRL, type=""Rosenthal"")\r\nN_SRL <- unique(SRL $PaperID)\r\nlength(N_SRL)\r\n\r\n\r\n\r\n', '\r\nrm(list=ls())\r\n\r\nlibrary(ape)\r\nlibrary(nlme)\r\nlibrary(metafor)\r\nlibrary(ggplot2)\r\nlibrary(forestplot)\r\nlibrary(clubSandwich)\r\nlibrary(car)\r\nlibrary(rotl)\r\nlibrary(curl)\r\n\r\ngenet<-read.delim(""~/Documents/NSF rarity project/Dryad/Genetics_Data.txt"", header=TRUE)\r\n\r\nsapply(genet,class)\r\nstr(genet)\r\n\r\n#remove NA from SD_rare and SD_common\r\ngene<-genet[!is.na(genet $SD_rare),]\r\ngenet_data <-gene[!is.na(gene $SD_common),]\r\n\r\n########################\r\n###Preliminary stages###\r\n########################\r\n#Convert certain variables to factors\r\ngenet_data $parameter <- factor(genet_data $parameter)\r\ngenet_data $life_history_Common <- factor(genet_data $life_history_Common)\r\ngenet_data $life_history_Rare <- factor(genet_data $life_history_Rare)\r\ngenet_data $breeding_Common <- factor(genet_data $breeding_Common)\r\ngenet_data $breeding_Rare <- factor(genet_data $breeding_Rare)\r\ngenet_data $ploidy_Common <- factor(genet_data $ploidy_Common)\r\ngenet_data $ploidy_Rare <- factor(genet_data $ploidy_Rare)\r\ngenet_data $Species <- factor(genet_data $Species)\r\ngenet_data $marker_type <- factor(genet_data $marker_type)\r\ngenet_data $PaperID <- factor(genet_data $PaperID)\r\n\r\n\r\ngenus_list=unique(genet_data$Species)\r\ngenus_list =data.frame(genus_list)\r\n\r\n#To pull the phylogeny\r\ngenus_list$genus_list<-as.character(genus_list$genus_list)\r\nsapply(genus_list,class)\r\ntaxon_search <- tnrs_match_names(names = genus_list$genus_list, context_name = ""Land plants"")\r\nknitr::kable(taxon_search)\r\ngenus_list $ott_name <- unique_name(taxon_search)\r\ngenus_list $ott_id <- taxon_search$ott_id\r\nhits <- lapply(genus_list $ott_id, studies_find_trees, property = ""ot:ottId"", detailed = FALSE)\r\nsapply(hits, function(x) sum(x[[""n_matched_trees""]]))\r\nott_in_tree <- ott_id(taxon_search)[is_in_tree(ott_id(taxon_search))]\r\ntr <- tol_induced_subtree(ott_ids = ott_in_tree)\r\nplot(tr)\r\ntr$tip.label <- strip_ott_ids(tr$tip.label, remove_underscores = TRUE)\r\ntr$tip.label %in% genus_list $ott_name\r\n\r\n## computes branch lengths \r\ntree2<-compute.brlen(tr)\r\n\r\n## plotting the tree\r\ntree2\r\nplot(tree2,type=""fan"")\r\nplot(tree2)\r\n\r\ntree2$edge.length\r\ngenuslist_tree2<-tree2 $tip\r\n\r\n# generate the variance-covariance matrix for phylogenetic correction\r\nsigma<-vcv.phylo(tree2, cor=TRUE)\r\n\r\n##############################################################\r\n###Conducting the phylogenetically-corrected meta-analysis ###\r\n##############################################################\r\n\r\n\r\n##In the model, negative coefficients indicate that rare species had lower fecundity than common species. Positive values indicate that rare species overperformed compared with common species.\r\n#This step calculates the effect sizes\r\ngenet_dat <-escalc(data= genet_data,measure=""SMD"",m2i= Mean_common,sd2i= SD_common,n2i= N_Common,m1i= Mean_rare,sd1i= SD_rare,n1i= N_Rare,append=T)\r\n\r\n#The Rarity column indicates the type of rarity:\r\n##GD = geographically-restricted\r\n##LA = low local abundance\r\n## GD_HS = geographically-restricted, habitat specialist\r\n## GD_LA = geographically-restricted, and low local abundance\r\n## GD_HS_LA = geographically-restricted, habitat specialist, with low local abundance\r\n\r\n#We also provide the breeding system of the rare and common species in three categories: \r\n## o = obligate outcrossing\r\n## m = mixed mating\r\n## s = self-pollinating\r\n\r\n#When known, we indicated whetther a species was diploid or polyploid.\r\n#Most species were perennials, but there were some annuals. Life history classificaitons are as follows:\r\n## p = perennial\r\n## a = annual\r\n\r\n\r\n#Parameter is the main moderator and it indicates which population genetic parameter was measured:\r\n## Polymorphic_loci = percentage of loci that were polymoprhic\r\n## Alleles_per_locus = the number of alleles per locus\r\n## Ho = observed heterozygosity\r\n## He = expected heterozygosity\r\n## Fst = population differentiation\r\n## Fis = inbreeding coefficient\r\n\r\n\r\n##Generates table with rarity type vs. genetic parameter\r\naddmargins(table(genet_data $parameter, genet_data $Rarity))\r\n\r\n##Make a copy of the species variable to include as a non-phylogenetic species random effect\r\ngenet_dat$Speciescat<-genet_dat$Species\r\n\r\n##Create an effect size variable to include as a random effect to account for multiple effect sizes per sutdy\r\ngenet_dat$esid<-1:nrow(genet_dat)\r\n\r\n\r\n#mixed-effects conditional model, corrected for phylogeny, with Hedge\'s g as the effect size\r\nmodA<-rma.mv(yi,vi, mods=~ (parameter)  -1,random=list(~1| Species,~1|Speciescat,~1|esid, ~1|PaperID),R = list(Species = sigma),data= genet_dat)\r\nsummary(modA)\r\n\r\n##testing parameter\r\nanova(modA,btt=1:6)\r\n\r\n\r\n##plot the effect sizes\r\noverall<-coef(summary(modA))\r\noverall <-round(overall, 3)\r\n\r\nis.data.frame(overall)\r\nrownames(overall)<-c(""Alleles per locus"",""Fis"",""Fst"", ""He"", ""Ho"", ""Percentage of loci that are polymorphic"")\r\noverall <-overall[ -c(2:4) ]\r\n\r\n\r\noverall2<-overall[c( ""Percentage of loci that are polymorphic"", ""Alleles per locus"",""Ho"",""He"", ""Fst"",""Fis""),]\r\n                   \r\nrow_names <- cbind(c( ""Percentage of loci that are polymorphic"", ""Alleles per locus"",""Observed heterozygosity"",""Expected heterozygosity"", ""Fst"", ""Fis""))                   \r\n    \r\nforestplot(labeltext = row_names,\r\n           overall2[,c(""estimate"", ""ci.lb"", ""ci.ub"")],is.summary=c(FALSE, FALSE, FALSE,FALSE),\r\n           zero = 0,\r\n           cex  = 2,\r\n           lineheight = ""auto"",\r\n           xlab = ""Effect size (Hedge\'s g)"", col=fpColors(box=""royalblue"",line=""darkblue"", summary=""royalblue""), ci.vertices=TRUE, boxsize   = c(0.2,0.2,0.2))\r\n\r\n##Calculate the fail safe number\r\nfsn(yi,vi, data= genet_dat, type=""Rosenthal"")\r\nfunnel(modA)\r\nN_tot <- unique(genet_dat$PaperID)\r\nlength(N_tot)\r\n5050/(10+(5*length(N_tot)))\r\n\r\n\r\n##Test  for asymmetry in the funnel plot\r\nfun<-rma(genet_dat,mods=~ parameter  -1, data= genet_dat,method=""REML"")\r\nregtest(fun)\r\n\r\n##visualize the funnel plot\r\nfunnel(modA)\r\n\r\n##Fail safe numbers for each genetic parameter\r\nAlleles<-subset(genet_dat, parameter==""Alleles_per_locus"")\r\nFis<-subset(genet_dat, parameter==""Fis"")\r\nFst<-subset(genet_dat, parameter==""Fst"")\r\nHe<-subset(genet_dat, parameter==""He"")\r\nHo <-subset(genet_dat, parameter==""Ho"")\r\nPolymorphic_loci <-subset(genet_dat, parameter==""Polymorphic_loci"")\r\n\r\nfsn(yi,vi, data= Alleles, type=""Rosenthal"")\r\nlength(unique(Alleles$PaperID))\r\n\r\nfsn(yi,vi, data= Fis, type=""Rosenthal"")\r\nlength(unique(Fis$PaperID))\r\n\r\nfsn(yi,vi, data= Fst, type=""Rosenthal"")\r\nlength(unique(Fst$PaperID))\r\n\r\nfsn(yi,vi, data= He, type=""Rosenthal"")\r\nlength(unique(He$PaperID))\r\n\r\nfsn(yi,vi, data= Ho, type=""Rosenthal"")\r\nlength(unique(Ho$PaperID))\r\n\r\nfsn(yi,vi, data= Polymorphic_loci, type=""Rosenthal"")\r\nlength(unique(Polymorphic_loci$PaperID))\r\n\r\n     \r\n', '\r\nrm(list=ls())\r\n\r\nlibrary(ape)\r\nlibrary(nlme)\r\nlibrary(metafor)\r\nlibrary(ggplot2)\r\nlibrary(forestplot)\r\nlibrary(clubSandwich)\r\nlibrary(car)\r\nlibrary(rotl)\r\nlibrary(ape)\r\npollination <-read.delim(""~/Documents/NSF rarity project/Dryad/Pollination_Data.txt"", header=TRUE)\r\nsapply(pollination,class)\r\nstr(pollination)\r\n\r\n#remove NA from species column\r\npollination_data <- pollination[!is.na(pollination $Species_phylogeny), ]\r\nstr(pollination_data)\r\n\r\n\r\n\r\n########################\r\n###Preliminary stages###\r\n########################\r\n#Standardize year, elevation, lat and long\r\npollination_data $year <- c(scale(pollination_data $YEAR,center=TRUE, scale=TRUE))\r\npollination_data $abs_Lat <- abs(pollination_data $Latitude)\r\npollination_data $abs_Long <- abs(pollination_data $Longitude)\r\npollination_data $absLat <- c(scale(pollination_data $abs_Lat,center=TRUE, scale=TRUE))\r\npollination_data $absLong <- c(scale(pollination_data $abs_Long,center=TRUE, scale=TRUE))\r\npollination_data $elev <- c(scale(pollination_data $elevation,center=TRUE, scale=TRUE))\r\n\r\n#Convert certain variables to factors\r\npollination_data $treatment <- factor(pollination_data $treatment)\r\npollination_data $trait <- factor(pollination_data $trait)\r\npollination_data $Species_phylogeny <- factor(pollination_data $Species_phylogeny)\r\nlevels(pollination_data $trait)\r\n\r\nstr(pollination_data)\r\npollination_data $PaperID <-as.factor(pollination_data $PaperID)\r\n\r\ngenus_list=unique(pollination_data$Species_phylogeny)\r\ngenus_list =data.frame(genus_list)\r\n\r\n#To pull the phylogeny\r\ngenus_list$genus_list<-as.character(genus_list$genus_list)\r\nsapply(genus_list,class)\r\ntaxon_search <- tnrs_match_names(names = genus_list$genus_list, context_name = ""Land plants"")\r\nknitr::kable(taxon_search)\r\ngenus_list $ott_name <- unique_name(taxon_search)\r\ngenus_list $ott_id <- taxon_search$ott_id\r\nhits <- lapply(genus_list $ott_id, studies_find_trees, property = ""ot:ottId"", detailed = FALSE)\r\nsapply(hits, function(x) sum(x[[""n_matched_trees""]]))\r\nott_in_tree <- ott_id(taxon_search)[is_in_tree(ott_id(taxon_search))]\r\ntr <- tol_induced_subtree(ott_ids = ott_in_tree)\r\nplot(tr)\r\ntr$tip.label <- strip_ott_ids(tr$tip.label, remove_underscores = TRUE)\r\ntr$tip.label %in% genus_list $ott_name\r\n\r\n## computes branch lengths (default method is ""Grafen"", but I don\'t even see alternatives in the help vignette so I did not specify anything)\r\ntree2<-compute.brlen(tr)\r\n\r\n## plotting the tree\r\ntree2\r\nplot(tree2,type=""fan"")\r\nplot(tree2)\r\n\r\ntree2$edge.length\r\ngenuslist_tree2<-tree2 $tip\r\n\r\n\r\n# generate the variance-covariance matrix and write out the file\r\nsigma<-vcv.phylo(tree2, cor=TRUE)\r\n\r\n\r\n##############################################################\r\n###Conducting the phylogenetically-corrected meta-analysis ###\r\n##############################################################\r\n\r\n##Make a copy of the species variable\r\npollination_data$Species<-pollination_data$Species_phylogeny\r\n##Create an effect size variable\r\npollination_data$esid<-1:nrow(pollination_data)\r\n\r\n\r\n##The negative coefficients indicate that rare species had lower fecundity than common species. Positive values indicate that rare species overperformed compared with common species.\r\n\r\n##yi and vi represent Hedge\'s g effect sizes (yi) and variances (vi) after aggregating across similar traits, as described in the text of the manuscript. In addition, to combine binary and continuous outcomes, we first calcultaed the log of the odds ratio (yi_log_OR) and variances (vi_log_OR) as binary effect sizes and then converted them to Hedge\'s g as described in the manuscript.\r\n\r\n#mixed-effects conditional model.\r\nfinal_model<-rma.mv(yi, vi ,mods=~ (treatment)   -1,random=list(~1| Species_phylogeny,~1|Species,~1|esid, ~1|PaperID),R = list(Species_phylogeny = sigma), control=list(optimizer=""optim"", optmethod=""Nelder-Mead""),data= pollination_data)\r\nsummary(final_model)\r\n\r\n##testing treatment\r\nanova(final_model,btt=1:4)\r\n\r\n##plot the results\r\noverall<-coef(summary(final_model))\r\noverall <-round(overall, 3)\r\n\r\nis.data.frame(overall)\r\nrownames(overall)<-c(""Hand pollination"",""Open pollination"", ""Self-pollination"",""Supplemental pollen"")\r\noverall <-overall[ -c(2) ]\r\n\r\n\r\noverall2<-overall[c( ""Open pollination"", ""Supplemental pollen"", ""Self-pollination"",""Hand pollination""),]\r\n                   \r\nrow_names <- cbind(c( ""Open pollination"",""Supplemental pollen"", ""Self-pollination"",""Hand pollination""))                   \r\n    \r\nforestplot(labeltext = row_names,\r\n           overall2[,c(""estimate"", ""ci.lb"", ""ci.ub"")],is.summary=c(FALSE, FALSE, FALSE,FALSE),\r\n           zero = 0,\r\n           cex  = 2,\r\n           lineheight = ""auto"",\r\n           xlab = ""Effect size (Hedge\'s g)"", col=fpColors(box=""royalblue"",line=""darkblue"", summary=""royalblue""), ci.vertices=TRUE, boxsize   = c(0.2,0.2,0.2))\r\n\r\n#Calculate fail-safe number\r\nfsn(yi, vi , data= pollination_data, type=""Rosenthal"")\r\nN_tot<-unique(pollination_data$PaperID)\r\nlength(N_tot)\r\n5490/(10+(5*length(N_tot)))\r\n\r\n#Visualize the funnel plot\r\nfunnel(final_model)\r\n\r\n##Test  for asymmetry in the funnel plot\r\nfun<-rma(yi, vi,mods=~ treatment  -1, data= pollination_data,method=""REML"")\r\nregtest(fun)\r\n\r\n##Fail safe numbers for each mating system treatment\r\n\r\n\r\nambient<-subset(pollination_data, treatment ==""open_pollination"")\r\nsupp<-subset(pollination_data, treatment ==""supplemental_pollen"")\r\nselfing<-subset(pollination_data, treatment ==""selfing"")\r\nhand<-subset(pollination_data, treatment==""hand_pollination"")\r\n\r\nfsn(yi, vi , data= ambient, type=""Rosenthal"")\r\nlength(unique(ambient$PaperID))\r\n\r\nfsn(yi, vi , data= selfing, type=""Rosenthal"")\r\nlength(unique(selfing$PaperID))\r\n\r\n\r\nfsn(yi, vi , data= supp, type=""Rosenthal"")\r\nlength(unique(supp$PaperID))\r\n\r\n\r\nfsn(yi, vi , data= hand, type=""Rosenthal"")\r\nlength(unique(hand$PaperID))\r\n\r\n\r\n\r\n']","Eco-evolutionary causes and consequences of rarity in plants: a meta-analysis Species differ dramatically in their prevalence in the natural world, with many species characterized as rare due to restricted geographic distribution, low local abundance, and/or habitat specialization.We investigated eco-evolutionary causes and consequences of rarity with phylogenetically-controlled meta-analyses of population genetic diversity, fitness, and functional traits in rare and common congeneric plant species. Our syntheses included 252 rare species and 267 common congeners reported in 153 peer-reviewed articles published from 1978-2020 and one manuscript in press.Rare species have reduced population genetic diversity, depressed fitness, and smaller reproductive structures than common congeners. Rare species also could suffer from inbreeding depression and reduced fertilization efficiency.By limiting their capacity to adapt and migrate, these characteristics could influence contemporary patterns of rarity and increase the susceptibility of rare species to rapid environmental change. We recommend that future studies present more nuanced data on the extent of rarity in focal species, expose rare and common species to ecologically-relevant treatments, including reciprocal transplants, and conduct quantitative genetic and population genomic analyses across a greater array of systems. This research could elucidate the processes that contribute to rarity and generate robust predictions of extinction risks under global change.",4
Ecologically diverse clades dominate the oceans via extinction resistance,"Ecological differentiation is correlated with taxonomic diversity in many clades, and ecological divergence is often assumed to be a cause and/or consequence of high speciation rate. However, an analysis of 30,074 genera of living marine animals and 19,992 genera of fossil marine animals indicates that greater ecological differentiation in the modern oceans is actually associated with lower rates of origination over evolutionary time. Ecologically differentiated clades became taxonomically diverse over time because they were better buffered against extinction, particularly during mass extinctions, which primarily affected genus-rich, ecologically homogeneous clades. The relationship between ecological differentiation and taxonomic richness was weak early in the evolution of animals but has strengthened over geological time as successive extinction events reshaped the marine fauna.",,"Ecologically diverse clades dominate the oceans via extinction resistance Ecological differentiation is correlated with taxonomic diversity in many clades, and ecological divergence is often assumed to be a cause and/or consequence of high speciation rate. However, an analysis of 30,074 genera of living marine animals and 19,992 genera of fossil marine animals indicates that greater ecological differentiation in the modern oceans is actually associated with lower rates of origination over evolutionary time. Ecologically differentiated clades became taxonomically diverse over time because they were better buffered against extinction, particularly during mass extinctions, which primarily affected genus-rich, ecologically homogeneous clades. The relationship between ecological differentiation and taxonomic richness was weak early in the evolution of animals but has strengthened over geological time as successive extinction events reshaped the marine fauna.",4
U.S. Science Journalists' Knowledge and Opinion of Open Access Articles as Sources,Survey data of U.S. science journalists and their knowledge and opinions of OA research as news sources.,"['###Analysis code for OAJournos_PublicData.csv dataset. \r\n\r\nlibrary(tidyverse)\r\nlibrary(here)\r\nlibrary(skimr) # install.packages(\'skimr\')\r\nlibrary(kableExtra) # install.packages(\'kableExtra\')\r\nlibrary(janitor) #install.packages(\'janitor\')\r\nlibrary(readxl) #install.packages(\'readxl\')\r\nlibrary(mice) #install.packages(\'mice\')\r\nlibrary(VIM) #install.packages(\'VIM\')\r\nlibrary(glmnet) #install.packages(\'glmnet\')\r\nlibrary(mlogit) #install.packages(\'mlogit\')\r\nlibrary(readr) #install.packages(\'readr\')\r\nlibrary(naniar) #install.packages(\'naniar\')\r\nlibrary(caret)   #install.packages(\'caret\')\r\nlibrary(foreign)      #install.packages(\'foreign\')\r\nlibrary(nnet)      #install.packages(\'nnet\')\r\nlibrary(reshape2)      #install.packages(\'reshape2\')\r\nlibrary(car)   #install.packages(\'car\')\r\n#install.packages(\'scales\')\r\n\r\n\r\n#Read in public data\r\nsurvey_results <- read_csv(here(""data"", ""OAJournos_PublicData.csv""))\r\n\r\n#Change values to shorter names and also collapse to help with analyzing - if do not want to collapse, see next section\r\nsurvey_results[survey_results == ""Yes, I am employed by a news organization""] <- ""Employed""\r\nsurvey_results[survey_results == ""Yes, I work as a freelancer for a news organization""] <- ""Freelance""\r\nsurvey_results[survey_results == ""Master\'s degree""] <- ""Grad degree""\r\nsurvey_results[survey_results == ""PhD degree""] <- ""Grad degree""\r\nsurvey_results[survey_results == ""I did not receive a master\'s or PhD degree""] <- ""NA""\r\nsurvey_results[survey_results == ""Extremely uncomfortable""] <-  ""Uncomfortable""\r\nsurvey_results[survey_results == ""Somewhat comfortable"" | survey_results == ""Extremely comfortable""] <- ""Comfortable""\r\nsurvey_results[survey_results == ""Science-focused magazine or other outlet""] <- ""ScienceOutlet""\r\nsurvey_results[survey_results == ""More than 75 percent"" | survey_results == ""More than 75 percent of my news stories cite a scientific study""] <- ""51+""\r\nsurvey_results[survey_results == ""26 to 50 percent"" | survey_results == ""26 to 50 percent of my news stories cite a scientific study""] <- ""-50""\r\nsurvey_results[survey_results == ""51 to 75 percent"" | survey_results == ""51 to 75 percent of my news stories cite a scientific study""] <- ""51+""\r\nsurvey_results[survey_results == ""25 percent or less"" | survey_results == ""25 percent or fewer of my news stories cite a scientific study""] <- ""-50""\r\nsurvey_results[survey_results == ""Very important - I will not reference a scholarly study if I cannot access the full-text version""] <- ""Important""\r\nsurvey_results[survey_results == ""Pretty important - I will take as many steps as I can to find a copy of the scientific study before relying on another source""] <- ""Important""\r\nsurvey_results[survey_results == ""Somewhat important - I try to find the scientific study, but if I can\'t get a copy of it or it\'s too expensive, I will use another source""] <- ""Somewhat important""\r\nsurvey_results[survey_results == ""It hasn\'t changed"" | survey_results == ""My impressions and views have stayed the same""] <- ""Same""\r\nsurvey_results[survey_results == ""It\'s gotten harder""] <- ""Harder""\r\nsurvey_results[survey_results == ""Yes, before COVID-19 research""] <- ""Yes pre Covid""\r\nsurvey_results[survey_results == ""Yes, but only in the past year during coverage of COVID-19 research""] <- ""Yes post Covid""\r\nsurvey_results[survey_results == ""I had heard of the term but did not know what it meant"" | survey_results == ""I had heard the term but did not know what it meant""] <- ""Heard of""\r\nsurvey_results[survey_results == ""I had never heard of the term""] <- ""No""\r\nsurvey_results[survey_results == ""I will cite or refer to them without concern to whether they have been peer reviewed or published""] <- ""Cite""\r\nsurvey_results[survey_results == ""I will cite or refer to them only if I know they have been peer reviewed""] <- ""Cautious/don\'t cite""\r\nsurvey_results[survey_results == ""I do not pay attention to if a study came from an open database""] <- ""Cite""\r\nsurvey_results[survey_results == ""I will cite or refer to them only if I know they have been peer reviewed and published""] <- ""Cautious/don\'t cite""\r\nsurvey_results[survey_results == ""I will not cite or refer to them""] <- ""Cautious/don\'t cite""\r\nsurvey_results[survey_results == ""I have no concern about using them""] <- ""Little/no concern""\r\nsurvey_results[survey_results == ""I am hesitant but will use them if I am familiar with the journal"" | survey_results == ""I am hesitant  but will use them from journals I trust""] <- ""Hesitant""\r\nsurvey_results[survey_results == ""I do not pay attention to the open access status of a journal"" | survey_results == ""I do not pay attention to the open access status of articles in paywalled journals""] <- ""Little/no concern""\r\nsurvey_results[survey_results == ""I have some concerns but will use them unless there are red flags""] <- ""Hesitant""\r\nsurvey_results[survey_results == ""My impressions and views have improved""] <- ""Improved""\r\nsurvey_results[survey_results == ""Not at all important""] <- ""Not important""\r\nsurvey_results[survey_results == ""Pretty important""] <- ""Important""\r\nsurvey_results[survey_results == ""Somewhat important""] <- ""Not important""\r\nsurvey_results[survey_results == ""Very important""] <- ""Important""\r\n\r\n\r\n####Change values to shorter names without collapsing values.\r\nsurvey_results[survey_results == ""Yes, I am employed by a news organization""] <- ""Employed""\r\nsurvey_results[survey_results == ""Yes, I work as a freelancer for a news organization""] <- ""Freelance""\r\nsurvey_results[survey_results == ""Master\'s degree""] <- ""Masters""\r\nsurvey_results[survey_results == ""PhD degree""] <- ""PhD""\r\nsurvey_results[survey_results == ""I did not receive a master\'s or PhD degree""] <- ""NA""\r\nsurvey_results[survey_results == ""Science-focused magazine or other outlet""] <- ""ScienceOutlet""\r\nsurvey_results[survey_results == ""More than 75 percent"" | survey_results == ""More than 75 percent of my news stories cite a scientific study""] <- ""75+""\r\nsurvey_results[survey_results == ""26 to 50 percent"" | survey_results == ""26 to 50 percent of my news stories cite a scientific study""] <- ""26-50""\r\nsurvey_results[survey_results == ""51 to 75 percent"" | survey_results == ""51 to 75 percent of my news stories cite a scientific study""] <- ""51-75""\r\nsurvey_results[survey_results == ""25 percent or less"" | survey_results == ""25 percent or fewer of my news stories cite a scientific study""] <- ""-25""\r\nsurvey_results[survey_results == ""Very important - I will not reference a scholarly study if I cannot access the full-text version""] <- ""Very""\r\nsurvey_results[survey_results == ""Pretty important - I will take as many steps as I can to find a copy of the scientific study before relying on another source""] <- ""Pretty""\r\nsurvey_results[survey_results == ""Somewhat important - I try to find the scientific study, but if I can\'t get a copy of it or it\'s too expensive, I will use another source""] <- ""Somewhat""\r\nsurvey_results[survey_results == ""It hasn\'t changed"" | survey_results == ""My impressions and views have stayed the same""] <- ""Same""\r\nsurvey_results[survey_results == ""It\'s gotten harder""] <- ""Harder""\r\nsurvey_results[survey_results == ""Yes, before COVID-19 research""] <- ""Yes pre Covid""\r\nsurvey_results[survey_results == ""Yes, but only in the past year during coverage of COVID-19 research""] <- ""Yes post Covid""\r\nsurvey_results[survey_results == ""I had heard of the term but did not know what it meant"" | survey_results == ""I had heard the term but did not know what it meant""] <- ""Heard of""\r\nsurvey_results[survey_results == ""I had never heard of the term""] <- ""No""\r\nsurvey_results[survey_results == ""I will cite or refer to them without concern to whether they have been peer reviewed or published""] <- ""Cite""\r\nsurvey_results[survey_results == ""I will cite or refer to them only if I know they have been peer reviewed""] <- ""Cite if peer reviewed""\r\nsurvey_results[survey_results == ""I do not pay attention to if a study came from an open database""] <- ""Don\'t pay attention""\r\nsurvey_results[survey_results == ""I will cite or refer to them only if I know they have been peer reviewed and published""] <- ""Cite if peer reviewed, published""\r\nsurvey_results[survey_results == ""I will not cite or refer to them""] <- ""Don\'t cite""\r\nsurvey_results[survey_results == ""I have no concern about using them""] <- ""No concern""\r\nsurvey_results[survey_results == ""I am hesitant but will use them if I am familiar with the journal"" | survey_results == ""I am hesitant  but will use them from journals I trust""] <- ""Hesitant""\r\nsurvey_results[survey_results == ""I do not pay attention to the open access status of a journal"" | survey_results == ""I do not pay attention to the open access status of articles in paywalled journals""] <- ""Don\'t pay attention""\r\nsurvey_results[survey_results == ""I have some concerns but will use them unless there are red flags""] <- ""Some concern""\r\nsurvey_results[survey_results == ""My impressions and views have improved""] <- ""Improved""\r\nsurvey_results[survey_results == ""Not at all important""] <- ""Not at all""\r\nsurvey_results[survey_results == ""Pretty important""] <- ""Pretty""\r\nsurvey_results[survey_results == ""Somewhat important""] <- ""Somewhat""\r\nsurvey_results[survey_results == ""Very important""] <- ""Very""\r\n\r\n\r\n#Setting factors and levels - based on collapsed values\r\nsurvey_results$audSize <- factor(survey_results$audSize, order = TRUE, levels =c(""Regional/local"", ""National""))\r\nsurvey_results$education <- factor(survey_results$education, order = TRUE, levels =c(""BS or lower"", ""Grad degree""))\r\nsurvey_results$comfortArts <- factor(survey_results$comfortArts, order = TRUE, levels = c(""Uncomfortable"", ""Comfortable""))\r\nsurvey_results$needFullText <- factor(survey_results$needFullText)\r\nsurvey_results$needFullText <- relevel(survey_results$needFullText, ""Not important"")\r\nsurvey_results$needFreeText <- factor(survey_results$needFreeText, order = TRUE, levels =c(""Not important"", ""Important""))\r\nsurvey_results$prepKnow <- factor(survey_results$prepKnow, order = TRUE, levels = c(""No"", ""Heard of"", ""Yes pre Covid"", ""Yes post Covid""))\r\nsurvey_results$postKnow <- factor(survey_results$postKnow, order = TRUE, levels = c(""No"", ""Heard of"", ""Yes pre Covid"", ""Yes post Covid""))\r\nsurvey_results$OAKnow <- factor(survey_results$OAKnow, order = TRUE, levels = c(""No"", ""Heard of"", ""Yes pre Covid"", ""Yes post Covid""))\r\nsurvey_results$greenOAKnow <- factor(survey_results$greenOAKnow, order = TRUE, levels = c(""Cautious/don\'t cite"", ""Cite""))\r\nsurvey_results$goldOAKnow <- factor(survey_results$goldOAKnow, order = TRUE, levels = c(""Hesitant"", ""Little/no concern""))\r\nsurvey_results$hybridOAKnow <- factor(survey_results$hybridOAKnow, order = TRUE, levels = c(""Hesitant"", ""Little/no concern""))\r\n\r\n####Set factors and levels - based on distinct values\r\nsurvey_results$audSize <- factor(survey_results$audSize, order = TRUE, levels =c(""Regional/local"", ""National""))\r\nsurvey_results$education <- factor(survey_results$education, order = TRUE, levels =c(""BS or lower"", ""Masters"", ""PhD""))\r\nsurvey_results$comfortArts <- factor(survey_results$comfortArts, order = TRUE, levels = c(""Extremely uncomfortable"", ""Somewhat comfortable"", ""Extremely comfortable""))\r\nsurvey_results$needFullText <- factor(survey_results$needFullText)\r\nsurvey_results$needFullText <- relevel(survey_results$needFullText, ""Somewhat"")\r\nsurvey_results$needFreeText <- factor(survey_results$needFreeText, order = TRUE, levels =c(""Not at all"", ""Somewhat"", ""Pretty"", ""Very""))\r\nsurvey_results$prepKnow <- factor(survey_results$prepKnow, order = TRUE, levels = c(""No"", ""Heard of"", ""Yes pre Covid"", ""Yes post Covid""))\r\nsurvey_results$postKnow <- factor(survey_results$postKnow, order = TRUE, levels = c(""No"", ""Heard of"", ""Yes pre Covid"", ""Yes post Covid""))\r\nsurvey_results$OAKnow <- factor(survey_results$OAKnow, order = TRUE, levels = c(""No"", ""Heard of"", ""Yes pre Covid"", ""Yes post Covid""))\r\nsurvey_results$greenOAKnow <- factor(survey_results$greenOAKnow, order = TRUE, levels = c(""Don\'t pay attention"", ""Don\'t cite"", ""Cite if peer reviewed, published"", ""Cite if peer reviewed"", ""Cite""))\r\nsurvey_results$goldOAKnow <- factor(survey_results$goldOAKnow, order = TRUE, levels = c(""Don\'t pay attention"", ""Hesitant"", ""Some concern"", ""No concern""))\r\nsurvey_results$hybridOAKnow <- factor(survey_results$hybridOAKnow, order = TRUE, levels = c(""Don\'t pay attention"", ""Hesitant"", ""Some concern"", ""No concern""))\r\n\r\n#Test that factor setting worked - should return TRUE. If FALSE, not factors\r\nprint(is.factor(survey_results$audSize))\r\n\r\n#Test that the levels were set correctly - should be Some BS or lower, Masters, PhD\r\nlevels(survey_results$education)\r\n\r\n#One way to get overview of my data\r\nskim(survey_results)\r\n\r\n#Look for missing values by column. Returns the numerical row for each missing value. \r\ncolSums(is.na(survey_results))\r\n\r\n\r\n\r\n\r\nsummary(survey_results)\r\n\r\ntable(survey_results$USJourno)\r\ntable(survey_results$education)\r\ntable(survey_results$degreeSubj)\r\ntable(survey_results$outletType)\r\ntable(survey_results$audSize)\r\ntable(survey_results$timeSciJ)\r\ntable(survey_results$percentCite)\r\ntable(survey_results$comfortArts)\r\ntable(survey_results$peerReview)\r\ntable(survey_results$needFreeText)\r\ntable(survey_results$needFullText)\r\ntable(survey_results$ideaSources)\r\ntable(survey_results$prepKnow)\r\ntable(survey_results$postKnow)\r\ntable(survey_results$OAKnow)\r\ntable(survey_results$greenOAKnow)\r\ntable(survey_results$goldOAKnow)\r\ntable(survey_results$hybridOAKnow)\r\ntable(survey_results$knowPredatory)\r\ntable(survey_results$predConcern)\r\n\r\n\r\n#Cross tab to compare IVs to DVs - plug and chug in the IVs and DVs as want to compare\r\nsurvey_results %>%\r\n  group_by(percentCite, comfortArts) %>% \r\n  summarize(count_by_needGold = n())\r\n\r\n#Run chi square test for knowledge of preprints and postprints with all different IVs\r\nchisq.test(table(survey_results$prepKnow, survey_results$education))\r\nchisq.test(table(survey_results$postKnow, survey_results$education))\r\nchisq.test(table(survey_results$prepKnow, survey_results$percentCite))\r\nchisq.test(table(survey_results$postKnow, survey_results$percentCite))\r\nchisq.test(table(survey_results$prepKnow, survey_results$audSize))\r\nchisq.test(table(survey_results$postKnow, survey_results$audSize))\r\nchisq.test(table(survey_results$prepKnow, survey_results$USJourno))\r\nchisq.test(table(survey_results$postKnow, survey_results$USJourno))\r\nchisq.test(table(survey_results$prepKnow, survey_results$timeSciJ))\r\nchisq.test(table(survey_results$postKnow, survey_results$timeSciJ))\r\nchisq.test(table(survey_results$prepKnow, survey_results$needFullText))\r\nchisq.test(table(survey_results$postKnow, survey_results$needFullText))\r\nchisq.test(table(survey_results$prepKnow, survey_results$needFreeText))\r\nchisq.test(table(survey_results$postKnow, survey_results$needFreeText))\r\nchisq.test(table(survey_results$prepKnow, survey_results$yearsWorkedGrouped))\r\nchisq.test(table(survey_results$postKnow, survey_results$yearsWorkedGrouped))\r\n#They all got errors - aka had at least one cell with too low frequency count. \r\n#so means need to use fisher\'s test on all of them\r\n\r\n#Fisher\'s exact test for preprint and postprint knowledge vs. all IVs\r\nfisher.test(table(survey_results$prepKnow, survey_results$education))\r\nfisher.test(table(survey_results$postKnow, survey_results$education))\r\nfisher.test(table(survey_results$prepKnow, survey_results$percentCite))\r\nfisher.test(table(survey_results$postKnow, survey_results$percentCite))\r\nfisher.test(table(survey_results$prepKnow, survey_results$audSize))\r\nfisher.test(table(survey_results$postKnow, survey_results$audSize))\r\nfisher.test(table(survey_results$prepKnow, survey_results$USJourno))\r\nfisher.test(table(survey_results$postKnow, survey_results$USJourno))\r\nfisher.test(table(survey_results$prepKnow, survey_results$timeSciJ))\r\nfisher.test(table(survey_results$postKnow, survey_results$timeSciJ))\r\nfisher.test(table(survey_results$prepKnow, survey_results$needFullText))\r\nfisher.test(table(survey_results$postKnow, survey_results$needFullText))\r\nfisher.test(table(survey_results$prepKnow, survey_results$needFreeText))\r\nfisher.test(table(survey_results$postKnow, survey_results$needFreeText))\r\n\r\n#IVs with p value < 0.05: \r\n  #For preprints, education, percentCite, audSize\r\n  #For postprints, percentCite, audSize, \r\n\r\n\r\nchisq.test(table(survey_results$greenOAKnow, survey_results$education))\r\nchisq.test(table(survey_results$goldOAKnow, survey_results$education))\r\nchisq.test(table(survey_results$hybridOAKnow, survey_results$education))\r\nchisq.test(table(survey_results$greenOAKnow, survey_results$percentCite))\r\nchisq.test(table(survey_results$goldOAKnow, survey_results$percentCite))\r\nchisq.test(table(survey_results$hybridOAKnow, survey_results$percentCite))\r\nchisq.test(table(survey_results$greenOAKnow, survey_results$audSize))\r\nchisq.test(table(survey_results$goldOAKnow, survey_results$audSize))\r\nchisq.test(table(survey_results$hybridOAKnow, survey_results$audSize))\r\nchisq.test(table(survey_results$greenOAKnow, survey_results$USJourno))\r\nchisq.test(table(survey_results$goldOAKnow, survey_results$USJourno))\r\nchisq.test(table(survey_results$hybridOAKnow, survey_results$USJourno))\r\nchisq.test(table(survey_results$greenOAKnow, survey_results$timeSciJ))\r\nchisq.test(table(survey_results$goldOAKnow, survey_results$timeSciJ))\r\nchisq.test(table(survey_results$hybridOAKnow, survey_results$timeSciJ))\r\nchisq.test(table(survey_results$greenOAKnow, survey_results$needFullText))\r\nchisq.test(table(survey_results$goldOAKnow, survey_results$needFullText))\r\nchisq.test(table(survey_results$hybridOAKnow, survey_results$needFullText))\r\nchisq.test(table(survey_results$greenOAKnow, survey_results$needFreeText))\r\nchisq.test(table(survey_results$goldOAKnow, survey_results$needFreeText))\r\nchisq.test(table(survey_results$hybridOAKnow, survey_results$needFreeText))\r\nchisq.test(table(survey_results$greenOAKnow, survey_results$outletType))\r\nchisq.test(table(survey_results$goldOAKnow, survey_results$outletType))\r\nchisq.test(table(survey_results$hybridOAKnow, survey_results$outletType))\r\n#IVs with too small frequency counts:\r\n  #For green - \r\n  #For gold - percentCite\r\n  #For hybrid - education, percent\r\n#IVs with significance:\r\n  #For green - percentCite, \r\n  #For gold - education, \r\n  #For hybrid - \r\n\r\nfisher.test(table(survey_results$greenOAKnow, survey_results$education))\r\nfisher.test(table(survey_results$goldOAKnow, survey_results$education))\r\nfisher.test(table(survey_results$hybridOAKnow, survey_results$education))\r\nfisher.test(table(survey_results$greenOAKnow, survey_results$percentCite))\r\nfisher.test(table(survey_results$goldOAKnow, survey_results$percentCite))\r\nfisher.test(table(survey_results$hybridOAKnow, survey_results$percentCite))\r\nfisher.test(table(survey_results$greenOAKnow, survey_results$audSize))\r\nfisher.test(table(survey_results$goldOAKnow, survey_results$audSize))\r\nfisher.test(table(survey_results$hybridOAKnow, survey_results$audSize))\r\nfisher.test(table(survey_results$greenOAKnow, survey_results$USJourno))\r\nfisher.test(table(survey_results$goldOAKnow, survey_results$USJourno))\r\nfisher.test(table(survey_results$hybridOAKnow, survey_results$USJourno))\r\nfisher.test(table(survey_results$greenOAKnow, survey_results$timeSciJ))\r\nfisher.test(table(survey_results$goldOAKnow, survey_results$timeSciJ))\r\nfisher.test(table(survey_results$hybridOAKnow, survey_results$timeSciJ))\r\nfisher.test(table(survey_results$greenOAKnow, survey_results$needFullText))\r\nfisher.test(table(survey_results$goldOAKnow, survey_results$needFullText))\r\nfisher.test(table(survey_results$hybridOAKnow, survey_results$needFullText))\r\nfisher.test(table(survey_results$greenOAKnow, survey_results$needFreeText))\r\nfisher.test(table(survey_results$goldOAKnow, survey_results$needFreeText))\r\nfisher.test(table(survey_results$hybridOAKnow, survey_results$needFreeText))\r\nfisher.test(table(survey_results$greenOAKnow, survey_results$outletType))\r\nfisher.test(table(survey_results$goldOAKnow, survey_results$outletType))\r\nfisher.test(table(survey_results$hybridOAKnow, survey_results$outletType))\r\nfisher.test(table(survey_results$greenOAKnow, survey_results$prepKnow))\r\nfisher.test(table(survey_results$goldOAKnow, survey_results$prepKnow))\r\nfisher.test(table(survey_results$hybridOAKnow, survey_results$prepKnow))\r\nfisher.test(table(survey_results$greenOAKnow, survey_results$postKnow))\r\nfisher.test(table(survey_results$goldOAKnow, survey_results$postKnow))\r\nfisher.test(table(survey_results$hybridOAKnow, survey_results$postKnow))\r\n\r\n#Determine how many people actually answered the question of how they check if journal is predatory\r\npredKnow <- subset(survey_results, knowPredatory == ""Yes"")\r\npredConcern <- subset(predKnow, predConcern != ""No"")\r\ntable(survey_results$knowPredatory)\r\ntable(predKnow$predConcern)\r\ntable(predConcern$predConcern)\r\nwhich(is.na(predConcern$predCheck))\r\nsum(predConcern$predCheck == """")\r\n\r\n#Same as above but for checking the websites\r\nwhich(is.na(predConcern$websiteCheck))\r\nsum(predConcern$websiteCheck == """")\r\n\r\n\r\n  table(survey_results$knowPredatory)\r\n\r\n\r\n###Data visuals - create a bunch of bar charts to visually explore data\r\n\r\nggplot(data = survey_results, aes(x = education)) +\r\n  geom_bar()\r\n\r\neducation_graph <- survey_results %>%\r\n  filter(!is.na(education)) %>%\r\n  ggplot(aes(x= education)) + \r\n  geom_bar()\r\n\r\neducation_graph\r\n\r\nyearsWorked_graph <- survey_results  %>%\r\n  filter(!is.na(yearsWorked)) %>%\r\n  ggplot(aes(x = yearsWorked)) +\r\n  geom_bar()\r\n\r\nyearsWorked_graph\r\n\r\nggplot(data = survey_results, aes(x = outletType)) +\r\n  geom_bar()\r\n\r\nggplot(data = survey_results, aes(x = audSize)) +\r\n  geom_bar()\r\n\r\nggplot(data = survey_results, aes(x = timeSciJ)) +\r\n  geom_bar()\r\n\r\nggplot(data = survey_results, aes(x = comfortArts)) +\r\n  geom_bar()\r\n\r\nggplot(data = survey_results, aes(x = peerReview)) +\r\n  geom_bar()\r\n\r\nggplot(data = survey_results, aes(x = percentCite)) +\r\n  geom_bar()  \r\n\r\nggplot(data = survey_results, aes(x = needFullText)) +\r\n  geom_bar()\r\n\r\nggplot(data = survey_results, aes(x = needFreeText)) +\r\n  geom_bar()\r\n\r\nggplot(data = survey_results, aes(x = easyFreeText)) +\r\n  geom_bar()\r\n\r\neasyFree_graph <- survey_results %>%\r\n  filter(!is.na(easyFreeText)) %>%\r\n  ggplot(aes(x= easyFreeText)) + \r\n  geom_bar()\r\n\r\neasyFree_graph\r\n\r\nggplot(data = survey_results, aes(x = prepKnow)) +\r\n  geom_bar()\r\n\r\nggplot(data = survey_results, aes(x = postKnow)) +\r\n  geom_bar()\r\n\r\nggplot(data = survey_results, aes(x = OAKnow)) +\r\n  geom_bar()\r\n\r\nggplot(data = survey_results, aes(x = greenOAKnow)) +\r\n  geom_bar() +\r\n  coord_flip()\r\n\r\ngoldOAKnow_graph <- survey_results %>%\r\n  filter(!is.na(goldOAKnow)) %>%\r\n  ggplot(aes(x= goldOAKnow)) + \r\n  geom_bar()\r\n\r\ngoldOAKnow_graph\r\n\r\nhybridOAKnow_graph <- survey_results %>%\r\n  filter(!is.na(hybridOAKnow)) %>%\r\n  ggplot(aes(x= hybridOAKnow)) + \r\n  geom_bar()\r\n\r\nhybridOAKnow_graph\r\n\r\nggplot(data = survey_results, aes(x = hybridOAKnow)) +\r\n  geom_bar() +\r\n  coord_flip()\r\n\r\nggplot(data = survey_results, aes(x = viewsChanged)) +\r\n  geom_bar()\r\n\r\nggplot(data = survey_results, aes(x = knowPredatory)) +\r\n  geom_bar()\r\n\r\npredConcern_graph <- survey_results %>%\r\n  filter(!is.na(predConcern)) %>%\r\n  ggplot(aes(x= predConcern)) + \r\n  geom_bar()\r\n\r\npredConcern_graph\r\n\r\nggplot(data = survey_results, aes(x = predConcern)) +\r\n  geom_bar()\r\n\r\ngreenKnowByEd_graph <- survey_results %>%\r\n  filter(!is.na(greenOAKnow)) %>%\r\n  filter(!is.na(education)) %>%\r\n  ggplot(aes(x= greenOAKnow, fill = education)) + \r\n  geom_bar()\r\ncoord_flip()\r\n\r\ngreenKnowByEd_graph\r\n\r\nggplot(survey_results, aes(x = greenOAKnow, fill = education)) + \r\n  geom_bar() +\r\n  coord_flip()\r\n\r\n\r\ntable(survey_results$needFreeText)']",U.S. Science Journalists' Knowledge and Opinion of Open Access Articles as Sources Survey data of U.S. science journalists and their knowledge and opinions of OA research as news sources.,4
Data From: Long-term winter food supplementation shows no significant impact on reproductive performance in Mountain Chickadees in the Sierra Nevada Mountains,"Supplemental feeding of wild animal populations is popular across many areas of the world and has long been considered beneficial, especially to avian taxa. Over four billion dollars are spent by hobby bird feeders in the United States each year alone. However, there is mixed evidence whether wildlife feeding is beneficial, including when it is implemented as a conservation management tool, a targeted experimental design, or an avocation. Much of the current evidence suggests that providing supplemental food is advantageous to the reproductive output and general survival of focal taxa. However, many of these studies are limited in scope and duration, leaving possible negative impacts unaddressed. This is particularly true regarding passive backyard feeding, which describes the majority of supplemental feeding, including the immense effort of millions of public enthusiasts. Here we show that winter supplemental feeding prior to reproduction had no significant impact on a range of reproductive parameters in a resident, montane passerine species, the Mountain Chickadee (Poecile gambeli). This population resides in an intact natural environment with no exposure to supplemental food beyond our experimental treatments, and individual birds were tracked across six years using radio frequency identification technology. Our results add to the growing evidence that supplemental feeding alone, isolated from the effects of urban environments, may have little to no impact on the population dynamics of some avian taxa.",,"Data From: Long-term winter food supplementation shows no significant impact on reproductive performance in Mountain Chickadees in the Sierra Nevada Mountains Supplemental feeding of wild animal populations is popular across many areas of the world and has long been considered beneficial, especially to avian taxa. Over four billion dollars are spent by hobby bird feeders in the United States each year alone. However, there is mixed evidence whether wildlife feeding is beneficial, including when it is implemented as a conservation management tool, a targeted experimental design, or an avocation. Much of the current evidence suggests that providing supplemental food is advantageous to the reproductive output and general survival of focal taxa. However, many of these studies are limited in scope and duration, leaving possible negative impacts unaddressed. This is particularly true regarding passive backyard feeding, which describes the majority of supplemental feeding, including the immense effort of millions of public enthusiasts. Here we show that winter supplemental feeding prior to reproduction had no significant impact on a range of reproductive parameters in a resident, montane passerine species, the Mountain Chickadee (Poecile gambeli). This population resides in an intact natural environment with no exposure to supplemental food beyond our experimental treatments, and individual birds were tracked across six years using radio frequency identification technology. Our results add to the growing evidence that supplemental feeding alone, isolated from the effects of urban environments, may have little to no impact on the population dynamics of some avian taxa.",4
Evaluating drivers of female dominance in the spotted hyena: R code and data,"Introduction: Dominance relationships in which females dominate males are rare among mammals. Mechanistic hypotheses explaining the occurrence of female dominance suggest that females dominate males because 1) they are intrinsically more aggressive or less submissive than males, and/or 2) they have access to more social support than males.Methods: Here, we examine the determinants of female dominance across ontogenetic development in spotted hyenas (Crocuta crocuta) using 30 years of detailed behavioral observations from the Mara Hyena Project to evaluate these two hypotheses.Results: Among adult hyenas, we find that females spontaneously aggress at higher rates than males, whereas males spontaneously submit at higher rates than females. Once an aggressive interaction has been initiated, adult females are more likely than immigrant males to elicit submission from members of the opposite sex, and both adult natal and immigrant males are more likely than adult females to offer submission in response to an aggressive act. We also find that adult male aggressors are more likely to receive social support than adult female aggressors, and that both adult natal and immigrant males are 2-3 times more likely to receive support when attacking a female than when attacking another male. Across all age classes, females are more likely than males to be targets of aggressive acts that occur with support. Further, receiving social support does slightly help immigrant males elicit submission from adult females compared to immigrant males acting alone, and it also helps females elicit submission from other females. However, adult females can dominate immigrant males with or without support far more often than immigrant males can dominate females, even when the immigrants are supported against females.Discussion: Overall, we find evidence for both mechanisms hypothesized to mediate female dominance in this species: 1) male and female hyenas clearly differ in their aggressive and submissive tendencies, and 2) realized social support plays an important role in shaping dominance relationships within a clan. Nevertheless, our results suggest that social support alone cannot explain sex-biased dominance in spotted hyenas. Although realized social support can certainly influence fight outcomes among females, adult females can easily dominate immigrant males without any support at all.",,"Evaluating drivers of female dominance in the spotted hyena: R code and data Introduction: Dominance relationships in which females dominate males are rare among mammals. Mechanistic hypotheses explaining the occurrence of female dominance suggest that females dominate males because 1) they are intrinsically more aggressive or less submissive than males, and/or 2) they have access to more social support than males.Methods: Here, we examine the determinants of female dominance across ontogenetic development in spotted hyenas (Crocuta crocuta) using 30 years of detailed behavioral observations from the Mara Hyena Project to evaluate these two hypotheses.Results: Among adult hyenas, we find that females spontaneously aggress at higher rates than males, whereas males spontaneously submit at higher rates than females. Once an aggressive interaction has been initiated, adult females are more likely than immigrant males to elicit submission from members of the opposite sex, and both adult natal and immigrant males are more likely than adult females to offer submission in response to an aggressive act. We also find that adult male aggressors are more likely to receive social support than adult female aggressors, and that both adult natal and immigrant males are 2-3 times more likely to receive support when attacking a female than when attacking another male. Across all age classes, females are more likely than males to be targets of aggressive acts that occur with support. Further, receiving social support does slightly help immigrant males elicit submission from adult females compared to immigrant males acting alone, and it also helps females elicit submission from other females. However, adult females can dominate immigrant males with or without support far more often than immigrant males can dominate females, even when the immigrants are supported against females.Discussion: Overall, we find evidence for both mechanisms hypothesized to mediate female dominance in this species: 1) male and female hyenas clearly differ in their aggressive and submissive tendencies, and 2) realized social support plays an important role in shaping dominance relationships within a clan. Nevertheless, our results suggest that social support alone cannot explain sex-biased dominance in spotted hyenas. Although realized social support can certainly influence fight outcomes among females, adult females can easily dominate immigrant males without any support at all.",4
Evolution and development at the origin of a phylum,"Quantifying morphological evolution is key to determining the patterns and processes underlying the origin of phyla. We constructed a hierarchical morphological character matrix to characterize the radiation and establishment of echinoderm body plans during the early Paleozoic. This showed that subphylum-level clades diverged gradually through the Cambrian, and the distinctiveness of the resulting body plans was amplified by the extinction of transitional forms and obscured by convergent evolution during the Ordovician. Higher-order characters that define these body plans were not fixed at the origin of the phylum, countering hypotheses regarding developmental processes governing the early evolution of animals. Instead, these burdened characters were flexible enabling continued evolutionary innovation throughout the clades' history.","['#Process: read in stratigraphic ranges, a tree, and tip character data.\r\n#1) Use strat data and tree to add branch lengths\r\n#2) Convert character data to prior probabilties\r\n#3) Check to see if there are NA for the character to establish the number of character states. \r\n#4) If character state is known- prior is 100%\r\n#5) If character state is unknown then all characters have equal probabilities  \r\n#6) Use make.simmap to performs the stocastic character mapping.\r\n#7) nsim= the number of iterations to perform\r\n#8) get the summary of the stocastic characters mapping to get the posterior probabilities\r\n#9) Use the character state with the highest posterior as the ancestral state. \r\n#10) z loop= number of characters, p loop=number of taxa, q loop= number of ancestral nodes. \r\n#11) The ancestral states are then calculated- you can then ordinate the entire data set of tips and nodes\r\n#12) Those tips, nodes, and tree can then be used to construct the phylomorphospace. \r\n\r\n#)In this case- 413 characters, 366 taxa, and 285 ancestral nodes\r\n\r\nlibrary(""ape"")\r\nlibrary(""phytools"")\r\nlibrary(""paleotree"")\r\n\r\nintervals<-read.table(""intervals.txt"", header=TRUE)\r\nbins<-read.table(""bins.txt"", header=TRUE, row.names=1)\r\nages<-list(intervals, bins)\r\ntree<-read.nexus(""echinodermtree.trees"")\r\ntree<-bin_timePaleoPhy(tree,ages,type=""equal"", vartime=1)\r\n\r\n\r\n\r\ntips<-read.table(""tips.txt"", row.names=1)\r\n\r\nancestralstates<-matrix(data=0,nrow=285, ncol=413)\r\n\r\nfor(z in 1:413){\r\n\ttip<-tips[,z]\r\n\tnames(tip)<-row.names(tips)\r\n\tprob<-matrix(data=0, nrow=366, ncol=(max(na.omit(tip))+1))\r\n\tif (\'0\' %in% tip){Navalue=1/ncol(prob)} else {Navalue=1/(ncol(prob)-1)}\r\n\r\n\tfor(p in 1:366){\r\n\t\tif (is.na(tip[p])){prob[p,]=Navalue}\r\n\t \telse {prob[p,(tip[p]+1)]=1}\r\n\t\tif (\'0\' %in% tip){} else{prob[,1]=0}\r\n\t\t}\r\n\trownames(prob)<-row.names(tips)\r\n\tcol<-c(0:max(na.omit(tip)))\r\n\tcolnames(prob)<-col\r\n\r\n\tx1<-make.simmap(tree,prob, nsim=1000, model= ""ER"")\r\n\tx2<-summary(x1, plot=FALSE)\r\n\r\n\tfor(q in 1:285){\r\n\t\tancestralstates[q,z]<-colnames(x2$ace)[which.max(x2$ace[q,])]\r\n\t\t}\r\n\t}\t\r\n\r\nrownames(ancestralstates)<-row.names(x2$ace)\r\nwrite.table(ancestralstates, file=""SCM.txt"") \r\n\r\n\r\n\r\n', '#Process: read in stratigraphic ranges, a tree, tip character data, and the possible states for each character.\r\n#1: Reduce dataset to only the characters being considered. \r\n#2: Model first set: Randomly select a character, then fill in non-applicable states, then apply missing data.\r\n#3: Model second set: Randomly select a character, then apply applicables. \r\n#4: Bind modeled data with ancestral states and tip data.\r\n#5: Run PCO\r\n#6: construct phylomorphospace. \r\n\r\nlibrary(ape)\r\nlibrary(cluster)\r\nlibrary(vegan)\r\nlibrary(phytools)\r\nlibrary(paleotree)\r\n\r\nchar<-read.table(""states.txt"", header=FALSE, fill=TRUE, col.names=paste(""V"",seq_len(10)))\r\nNAdist<-c(0:4)\r\n\r\nsubset<-c(1,4,21,22,36,55,76,78,79,81,102,111,120,127,144,160,173,181,185,191,201,221,234,245,249,255,274,276,306,325,340,347,352,373,386)\r\n\r\nrealchar<-read.table(""tips.txt"",row.names=1, header=FALSE)\r\nrealchar1<-unname(realchar)\r\nrealchar2<-data.matrix(realchar1)\r\nsubchar<-realchar2[,subset]\r\n}\r\nrealanc<-read.table(""SCM1000.txt"",row.names=1, header=FALSE)\r\nrealanc1<-unname(realanc)\r\nrealanc2<-data.matrix(realanc1)\r\nsubanc<-realanc2[,subset]\r\n\r\nnacount<-c()\r\nfor(g in 1:35){\r\nnacount[g]<-length(which(!is.na(subchar[,g])==""FALSE""))\r\n}\r\n\r\n\r\nsampmatrix<-matrix(data=0, nrow=10000, ncol=35)\r\n\r\n\r\n\r\n\r\nfor(m in 1:5000){\r\n\tspecies<-c()\r\n\tfor(q in 1:35){\r\n\t\tifelse(char[subset[q],1]==0\r\n\t\t\t,ifelse(sample(1:366,1)<length(which(subchar[,q]==0)),\r\n\t\t\t\tspecies[q]<-0,\r\n\t\t\t\tspecies[q]<-sample(which(char[subset[q],which(!is.na(char[subset[q],]))]>0),1)\r\n\t\t\t\t)\r\n\t\t\t,species[q]<-sample(which(char[subset[q],which(!is.na(char[subset[q],]))]>0),1)\r\n\t\t\t)\r\n\t\t}\r\n\t\r\n\textra<-c(28:31,34)\r\n\r\n\tif(species[4]==1){species[6]<-0}\r\n\tif(species[11]==1){species[12:13]<-0}\r\n\tif(species[18]==1){species[19:21]<-0}\r\n\tif(species[23]==1){species[24:25]<-0}\r\n\tif(species[27]==1){species[extra]<-0}\r\n\tif(species[32]==1){species[33]<-0}\r\n\r\n\tif(species[4]==0){species[6]<-0}\r\n\tif(species[11]==0){species[12:13]<-0}\r\n\tif(species[18]==0){species[19:21]<-0}\r\n\tif(species[23]==0){species[24:25]<-0}\r\n\tif(species[27]==0){species[extra]<-0}\r\n\tif(species[32]==0){species[33]<-0}\r\n\r\n\tfor(p in 1:35){\r\n\t\tifelse(sample(1:366,1)<nacount[p],species[p]<-NA,species[p]<-species[p])\r\n\t}\r\n\r\n\tif(is.na(species[4])){species[6]<-NA}\r\n\tif(is.na(species[11])){species[12:13]<-NA}\r\n\tif(is.na(species[18])){species[19:21]<-NA}\r\n\tif(is.na(species[23])){species[24:25]<-NA}\r\n\tif(is.na(species[27])){species[extra]<-NA}\r\n\tif(is.na(species[32])){species[33]<-NA}\r\n\r\nsampmatrix[m,]<-species\r\n}\r\n\r\n\r\nfor(m in 1:5000){\r\n\tspecies<-c()\r\n\tfor(q in 1:35){\r\n\t\tifelse(char[subset[q],1]==0\r\n\t\t\t,ifelse(sample(1:366,1)<length(which(subchar[,q]==0)),\r\n\t\t\t\tspecies[q]<-0,\r\n\t\t\t\tspecies[q]<-sample(which(char[subset[q],which(!is.na(char[subset[q],]))]>0),1)\r\n\t\t\t\t)\r\n\t\t\t,species[q]<-sample(which(char[subset[q],which(!is.na(char[subset[q],]))]>0),1)\r\n\t\t\t)\r\n\t\t}\r\n\r\n\tfor(p in 1:35){\r\n\t\tifelse(sample(1:366,1)<nacount[p],species[p]<-NA,species[p]<-species[p])\r\n\t}\r\n\r\n\tif(is.na(species[4])){species[6]<-NA}\r\n\tif(is.na(species[11])){species[12:13]<-NA}\r\n\tif(is.na(species[18])){species[19:21]<-NA}\r\n\tif(is.na(species[23])){species[24:25]<-NA}\r\n\tif(is.na(species[27])){species[extra]<-NA}\r\n\tif(is.na(species[32])){species[33]<-NA}\r\n\r\n\tmcount<-m+5000\t\r\n\tsampmatrix[mcount,]<-species\r\n\r\n}\r\n\r\n\r\nfull<-rbind(subchar,subanc,sampmatrix)\r\n\r\nx1<-daisy(full, metric = c(""gower""))\r\nx2<-pcoa(x1, correction=""cailliez"")\r\n#x2<-cmdscale(x1, k=2, add=TRUE)\r\nx3<-x2$vectors\r\n\r\nwrite.table(x3, file=""resultshuge.txt"")\r\n\r\n\r\ntree<-read.nexus(""echinodermtree.trees"")\r\nintervals<-read.table(""intervals.txt"", header=TRUE)\r\nbins<-read.table(""bins.txt"", header=TRUE, row.names=1)\r\nages<-list(intervals, bins)\r\ntree<-read.nexus(""echinodermtree.trees"")\r\ntree<-bin_timePaleoPhy(tree,ages,type=""equal"", vartime=1)\r\n\r\n\r\n\r\n\r\ntips<-matrix(data=0, nrow=366, ncol=2)\r\ntips[,1]<-x3[1:366,1]\r\ntips[,2]<-x3[1:366,2]\r\nrow.names(tips)<-row.names(realchar)\r\n\r\nanc<-matrix(data=0, nrow=285, ncol=2)\r\nanc[,1]<-x3[367:651,1]\r\nanc[,2]<-x3[367:651,2]\r\nrow.names(anc)<-row.names(realanc)\r\n\r\n\r\npdf(file=""theoretical5000both.pdf"", useDingbats=FALSE)\r\nplot(x=NULL,y=NULL,xlab=""PCO 1"", ylab=""PCO 2"", xlim=c(-0.3,0.3), ylim=c(-0.3,0.3))\r\n\r\npoints(x3[5652:10651,], col=""gray75"") #plot\r\npoints(x3[652:5651,], col=""gray40"") #plot\r\nphylomorphospace(tree,tips,A=anc, label=""off"", node.size=c(0.01,0.01), add=TRUE)\r\npoints(x3[c(1:18,20,177:180,200:234,271:283,334:336,363),], pch=21, col=""black"", bg=""red"", cex=1.0) #Radial Attached\r\npoints(x3[c(21:35,170:176,324:333,337:362),], pch=21, col=""black"",bg=""darkorange1"", cex=1.0) #nonradial\r\npoints(x3[c(37,39:73,75:111,113:162,164:169,183,265,364:365),], pch=21, col=""black"",bg=""blue"", cex=1.0) #crinoid\r\npoints(x3[c(19,36,38,74,112,163,181:182,184:199,235:264,266:270,284:323,366),], pch=21,col=""black"", bg=""springgreen4"", cex=1.0) #stalked radial\r\n\r\ndev.off()\r\n\r\n']","Evolution and development at the origin of a phylum Quantifying morphological evolution is key to determining the patterns and processes underlying the origin of phyla. We constructed a hierarchical morphological character matrix to characterize the radiation and establishment of echinoderm body plans during the early Paleozoic. This showed that subphylum-level clades diverged gradually through the Cambrian, and the distinctiveness of the resulting body plans was amplified by the extinction of transitional forms and obscured by convergent evolution during the Ordovician. Higher-order characters that define these body plans were not fixed at the origin of the phylum, countering hypotheses regarding developmental processes governing the early evolution of animals. Instead, these burdened characters were flexible enabling continued evolutionary innovation throughout the clades' history.",4
Infanticide by females is a leading source of juvenile mortality in a large social carnivore,"Social animals benefit from their group-mates, so why do they sometimes kill each other's offspring? Using 30 years of data from multiple groups of wild spotted hyenas, we address three critical aims for understanding infanticide in any species: (1) quantify the contribution of infanticide to overall mortality (2) describe the circumstances under which infanticide occurs and (3) evaluate hypotheses about the evolution of infanticide. We find that, although observed only rarely, infanticide is in fact a leading source of juvenile mortality. Infanticide accounted for 24% of juvenile mortality, and 1 in 10 hyenas born in our population perished due to infanticide. In all observed cases of infanticide, killers were adult females, but victims could be of both sexes. Of four hypotheses regarding the evolution of infanticide, we found the most support for the hypothesis that infanticide in spotted hyenas reflects competition over social status among matrilines.","['###############################################################################\n## Extract mortality and cub info\nrm(list = ls())\nhyenadata::update_tables(\'1.2.88\')\nlibrary(hyenadata)  ### Version 1.2.88\nlibrary(dplyr)\nlibrary(here)\nsource(\'000.functions.R\')\n\ndata(""tblHyenas"")\ndata(""tblLifeHistory.wide"")\ndata(""tblFemaleRanks"")\ndata(\'tblSessions\')\ndata(\'tblHyenasPerSession\')\ndata(\'tblPreyCensus\')\n\ninfanticide_notes <- read.csv(""Data/infanticide_notes.csv"")\ninfanticide_notes$Date <- as.Date(infanticide_notes$Date, format = \'%m/%d/%Y\')\n\n### Table of cleaned mortality data\ncleaned.mortality <- read.csv(here(\'Data/known_mortality.csv\'))\n\n## Combine rare mortality into an \'other\' category\ncleaned.mortality[cleaned.mortality$mortality %in% c(\'illness\', \'flooded den\'),]$mortality <- \'other\'\n\nnames(cleaned.mortality)\ncleaned.mortality$birthdate <- as.Date(cleaned.mortality$birthdate, format = \'%m/%d/%Y\')\ncleaned.mortality$disappeared <- as.Date(cleaned.mortality$disappeared, format = \'%m/%d/%Y\')\n\n###############################################################################\n\n### Table of all mortality\nall.mortality <- filter(tblLifeHistory.wide, disappeared <= (dob + 365), \n                        disappeared < \'2019-01-01\')\n\n## Restrict to study clans\nall.mortality <- filter(all.mortality, dob_event_data %in% c(\'talek\', \'pond\', \'kcm\', \'fig.tree\', \'mara.river\', \'happy.zebra\', \'serena.n\', \'serena.s\', \'oltukai\', \'airstrip\'))\n\n## How many cubs survived to 1 year old? \nall.cubs <- filter(tblLifeHistory.wide, \n       dob < \'2018-01-01\', dob >= \'1988-01-01\',\n       dob_event_data %in% c(\'talek\', \'pond\', \'kcm\', \'fig.tree\', \'mara.river\', \'happy.zebra\', \'serena.n\', \'serena.s\', \'oltukai\', \'airstrip\'))\n\nsurvive.to.1 <- tblHyenasPerSession %>%\n  filter(id %in% all.cubs$id) %>%\n  group_by(id) %>%\n  summarise(last.seen = max(date), .groups = \'drop_last\') %>%\n  left_join(all.cubs[,c(\'id\', \'dob\')], by = \'id\') %>%\n  filter(last.seen >= (dob+365))\n\n## 938 cubs who survived to 1 year old\nnum.survivors <- nrow(survive.to.1)\n\n## Number of sessions\n\n## Masai mara\nlength(unique(filter(tblSessions, clan  %in% c(\'talek\', \'pond\', \'kcm\', \'fig.tree\', \'mara.river\', \'happy.zebra\', \'serena.n\', \'serena.s\'),\n                     date <= \'2018-12-31\')$session))\n## Amboseli\nlength(unique(filter(tblSessions, clan  %in% c(\'amboseli\'))$session))\n\n## remove some individuals with incorrect dates or who are unverified\nall.mortality <- filter(all.mortality, !id %in% c(\'grig\', \'lb\', \'mc2\', \'mc3\', \'44\', \'aber\', \'mbrk\', \'sdc\'))\n\n## remove dummy cubs\nall.mortality <- filter(all.mortality, !grepl(\'dc[0-9]\', id))\n\nall.mortality <- left_join(all.mortality, tblHyenas[,c(\'id\', \'sex\', \'mom\')])\nall.mortality$age_at_death <- as.numeric((all.mortality$disappeared - all.mortality$dob)/30.4375)\nall.mortality$mom_disappeared <- left_join(all.mortality, tblLifeHistory.wide, by = c(\'mom\' = \'id\'))$disappeared.y\nall.mortality$mom_disappeared <- (all.mortality$mom_disappeared < all.mortality$disappeared) & !is.na(all.mortality$mom_disappeared)\n\nall.mortality <- all.mortality %>%\n  select(id, disappeared, dob, dob_event_data, disappeared_event_data, sex, mom, mom_disappeared, age_at_death) %>%\n  rename(mortality = disappeared_event_data, clan = dob_event_data, birthdate = dob)\n\n\nall.mortality <- all.mortality[,c(\'id\', \'sex\', \'disappeared\', \'mom\', \'birthdate\',\n                                  \'mortality\', \'clan\', \'mom_disappeared\', \'age_at_death\')]\n\n\nunknown.mortality <- filter(all.mortality, !id %in% cleaned.mortality$id)\n\n\n## Remove mc2 and mc3, who are unverifiable hyenas\nunknown.mortality <- filter(unknown.mortality, !id %in% c(\'mc2\', \'mc3\'))\nunique(unknown.mortality$mortality) ## Make sure these are all different versions of unknown.\n\nunknown.mortality$mortality <- \'unknown\'\n\n\n#### Finalize large dataset\nall.mortality <- rbind(unknown.mortality, cleaned.mortality)\nall.mortality[all.mortality$sex%in% c(\'u\', \'\') & !is.na(all.mortality$sex),\'sex\'] <- NA\n\n\n### Ensure clan names for tblPreyCensus match cub mortality table\ntblPreyCensus$clan[tblPreyCensus$clan == \'talek.w\'] <- \'talek\'\nall.mortality$prey_density <- get_prey_density(all.mortality$disappeared, all.mortality$clan, time.period = -(365.25/12))\n\n### Get number of cubs at den in month prior to death\nall.mortality$cub_associates <- get_cub_associates(all.mortality$id, all.mortality$disappeared, time.period = -(365.25/12))\n\nsave(num.survivors, all.mortality, tblFemaleRanks, infanticide_notes, file = here(\'Data/cub_data.RData\'))\n', '\r\n\r\n### Get prey data\r\n##### Function for calculating prey density - prey density = # of animals/km^2\r\n##### time.period = the number of days relative to the date for calculating\r\n##### prey density. \r\n##### Negative numbers = before the date, positive numbers = after the date.\r\n##### For example, to get the prey density for 110 days prior to the date, set ##### time.period = -110\r\nget_prey_density <- function(dates, clans, time.period, prey.animals = c(""thomsons"", ""impala"", ""zebra"", ""wildebeest"", ""topi"", ""warthog"", ""hartebeest"", ""grants"",""buffalo"", ""hippo"", ""giraffe"", ""ostrich"", ""eland"", ""elephant"", ""oribi"", ""reedbuck"",""waterbuck"", ""baboon"", ""bushbuck"")){\r\n  \r\n  ## Check to make sure supplied information is of same length\r\n  if(length(clans) == 1){\r\n    clans <- rep(clans, length(dates))\r\n    warning(\'only 1 clan provided. recycling clan for each date\')\r\n  }else if (length(clans) != length(dates)){\r\n    stop(\'clans not the same lenght as dates\')\r\n  }\r\n  \r\n  ## Store prey densities in here\r\n  preys <- rep(NA, length(dates))\r\n  \r\n  for(i in 1:length(dates)){\r\n    \r\n    ## Select prey censuses from the appropriate clan associated with 110 days prior to birth\r\n    prey.count.entries <- filter(tblPreyCensus, clan == clans[i] & date < dates[i] & date >= (dates[i] + time.period))[c(\'distance\', prey.animals)]\r\n    \r\n    ## If no prey census data, leave as NA\r\n    if(nrow(prey.count.entries) < 1)\r\n      next\r\n    \r\n    ## Convert entries to numeric\r\n    prey.count.entries[,c(\'distance\', prey.animals)] <- sapply(prey.count.entries[,c(\'distance\', prey.animals)], as.numeric)\r\n    \r\n    ## Count all species observed over the period (date + time.period), divided by distance. Also divide by 0.2 because prey censuses on 100m on either side of census route (i.e., 200m/1000m = 0.2)\r\n    preys[i] <- sum(prey.count.entries[,prey.animals], na.rm = TRUE)/sum(prey.count.entries[,\'distance\'] * 0.2)\r\n  }\r\n  return(preys)\r\n}\r\n\r\n### Get number of cubs\r\n#### This function extracts the number of cubs a hyena was in contact with\r\n#### in the past month. \r\n\r\nget_cub_associates <- function(ids, dates, time.period){\r\n  \r\n  cub.associates <- rep(NA, length(ids))\r\n  \r\n  for(i in 1:length(ids)){\r\n    if(!ids[i] %in% tblHyenas$id)\r\n      next\r\n    associates <- filter(tblHyenasPerSession, \r\n                         session %in% filter(tblHyenasPerSession, id %in% ids[i])$session,\r\n                         date <= dates[i],\r\n                         date >= dates[i] + time.period)\r\n    associates$age <- associates$date - left_join(associates, tblHyenas, by = \'id\')$birthdate\r\n    associates <- filter(associates, age <= 365)\r\n    cubs.per.session <- associates %>%\r\n      group_by(session) %>% \r\n      summarise(cubs.per.session = length(id), .groups = \'drop_last\')\r\n    \r\n    cub.associates[i] <- mean(cubs.per.session$cubs.per.session, na.rm = TRUE)\r\n  }\r\n  return(cub.associates)\r\n}\r\n\r\nmodel_summary <- function(fit){\r\n  \r\n  model.summary <- summary(fit)\r\n  df <- round(model.summary$fixed, 4)\r\n  rownames(df) <- gsub(rownames(df), pattern = \'mu\', replacement = \'\')\r\n  \r\n  df.output <- df[,c(\'Estimate\', \'Est.Error\', \'l-95% CI\', \'u-95% CI\')]\r\n  df.diagnostic <- df[,c(\'Rhat\', \'Bulk_ESS\', \'Tail_ESS\')]\r\n  \r\n  cat(\'MODEL SPECIFICATION:\\n\\n\')\r\n  cat(\'Family: \', model.summary$formula$family$family, \'\\n\')\r\n  cat(\'Formula: \', capture.output(model.summary$formula), \'\\n\')\r\n  cat(\'Number of observations: \', model.summary$nobs, \'\\n\')\r\n  cat(\'Samples: \', model.summary$chains, \'chains, each with iter = \', model.summary$iter, \'; warmup = \', model.summary$warmup, \'; thin = \', model.summary$thin, \'\\n\')\r\n  cat(\'\\nPRIORS:\\n\\n\')\r\n  print(prior_summary(fit, all = TRUE), show_df = FALSE)\r\n  cat(\'\\nMODEL OUTPUT:\\n\')\r\n  print(df.output)\r\n  cat(\'\\nMODEL DIAGNOSTICS:\\n\')\r\n  print(df.diagnostic)\r\n}\r\n', 'library(brms)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(here)\nlibrary(grid)\nlibrary(ggridges)\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nset.seed(1989)\noptions(stringsAsFactors = FALSE)\n################################################################################\n### Load data for analysis\nrm(list = ls())\nload(file = here(\'Data/cub_data.RData\'))\n\n##### Split into mortality with known and unknown sources\nknown.mortality <- filter(all.mortality, mortality != \'unknown\')\nunknown.mortality <- filter(all.mortality, mortality == \'unknown\')\n\n## For estimating frequency of different types of mortality, divide mortality\n## based on whether mother preceded offspring in death. Assign \'death of mother\' \n## to be it\'s own type of mortality, even superseding other causes. \n\nknown.mortality[is.na(known.mortality$mom_disappeared),\'mom_disappeared\'] <- FALSE\nknown.mortality[known.mortality$mom_disappeared,]$mortality <- \'death of mother\'\n\n## unknown mortality when mother precedes offspring in death\num.death.of.mother <- filter(unknown.mortality, mom_disappeared == TRUE)\num.death.of.mother$mortality <- \'death of mother\'\nunknown.mortality <- filter(unknown.mortality, mom_disappeared == FALSE)\n\nknown.mortality$infanticide <- as.numeric(known.mortality$mortality == \'infanticide\')\nknown.mortality$starvation <- as.numeric(known.mortality$mortality == \'starvation\')\nknown.mortality$lion <- as.numeric(known.mortality$mortality == \'lion\')\nknown.mortality$human <- as.numeric(known.mortality$mortality == \'human\')\nknown.mortality$siblicide <- as.numeric(known.mortality$mortality == \'siblicide\')\nknown.mortality$other <- as.numeric(known.mortality$mortality == \'other\')\n\nknown.mortality$y <- as.matrix(known.mortality[,c(\'infanticide\', \'starvation\', \'lion\', \'siblicide\',\n                                                  \'human\',\'other\')])\n\n\nknown.mortality.mom.alive <- filter(known.mortality, mom_disappeared == FALSE)\nnrow(known.mortality.mom.alive)\n\nsave(known.mortality, file = \'Data/known_mortality_cleaned.RData\')\n\n################################################################################\n### Descriptives\n\n## Total births\nnrow(all.mortality) + num.survivors\n\n## Total juvenile mortality\nnrow(unknown.mortality) + nrow(um.death.of.mother) + nrow(known.mortality)\nnrow(all.mortality)\n\n## Proportion of juveniles suffering mortality\nnrow(all.mortality)/(nrow(all.mortality)+num.survivors)\n\n## Proportion of mortality known\nnrow(known.mortality)/nrow(all.mortality)\n\n## Raw counts of known mortality sources after death of mother recoding\ntable(known.mortality$mortality)\n\n## Proportion of known mortality due to different causes\ntable(known.mortality$mortality)/nrow(known.mortality)\n\n## representations of different clans\ntable(all.mortality$clan)\n\n## Number of mortality sources recoded as \'death of mother\'\ntable(filter(all.mortality, mom_disappeared == TRUE)$mortality)\n\n## Proportion of cubs dying due to orphaning?\nsum(table(filter(all.mortality, mom_disappeared == TRUE)$mortality))/nrow(all.mortality)\n\n\n################################################################################\n### Modeling\n\n## Model\nfit <- brm(data = known.mortality.mom.alive, formula = bf(y|trials(1) ~ 1 + age_at_death), family = multinomial(), \n           chains = 4, iter = 30000, warmup = 15000, seed = 1989, cores = 4, inits = 0)\nsave(fit, file = \'Data/age_model.RData\')\n\nnull <- brm(data = known.mortality.mom.alive, formula = bf(y|trials(1) ~ 1), family = multinomial(), chains = 4, iter = 30000, warmup = 15000, seed = 1989, cores = 4, inits = 0)\n\nloo(fit, null)\n\n\n################################################################################\n### Predictions\n\n### Predict mortality source for each individual with unknown mortality source\npred.fit <- posterior_predict(fit, newdata = unknown.mortality, nsamples = 200)\n\n## Take mean of predictions to get mean and CI for number of inferred mortality\n#  events for each mortality source\nposterior.means <- apply(pred.fit, 3, function(x)(sum(x)/200))\nprobs <- apply(pred.fit, c(2,3), function(x)(sum(x)/200))\nposterior.draws <- apply(pred.fit, c(1,3), sum)\nposterior.cred.int <- apply(posterior.draws, 2, quantile, c(0.95, 0.05))\n\npost.ci <- data.frame(mortality = rownames(t(posterior.cred.int)),\n                      t(posterior.cred.int))\npost.ci[,2:3] <- post.ci[,2:3] + table(known.mortality.mom.alive$mortality)[post.ci$mortality]\nnames(post.ci) <- c(\'mortality\', \'high\', \'low\')\n                      \n\n\n## Sample posterior for predictions of probability of different mortality\n#  sources based on age at death. \nsmooth.pred <- posterior_epred(fit, newdata = data.frame(age_at_death = seq(from = 0, to = 12, by = 0.1)),\n                               nsamples = 200)\nsmooth.probs <- data.frame(apply(smooth.pred, c(2,3), mean))\nsmooth.probs$age <- seq(from = 0, to = 12, by = 0.1)\nsmooth.probs.high <- data.frame(apply(smooth.pred, c(2,3), quantile, 0.95))\nsmooth.probs.high$age <- seq(from = 0, to = 12, by = 0.1)\nsmooth.probs.low <- data.frame(apply(smooth.pred, c(2,3), quantile, 0.05))\nsmooth.probs.low$age <- seq(from = 0, to = 12, by = 0.1)\n\nmost.likely.death.by.age <- data.frame(age = smooth.probs$age, most.likely.death = NA)\n\nfor(i in 1:nrow(smooth.probs)){\n  most.likely.death.by.age[i,]$most.likely.death <- names(smooth.probs[,1:6])[which.max(smooth.probs[i,1:6])]\n}\nmost.likely.death.by.age\n\n\n################################################################################\n### Prepare data for plotting\nsummarized.mortality <- data.frame(mortality = names(table(known.mortality$mortality)),\n                                   frequency = as.numeric(table(known.mortality$mortality)),\n                                   obs.inf = \'observed\')\n\nsummarized.mortality$mortality <- factor(summarized.mortality$mortality,\n                                         levels = c( \'other\', \'human\',\n                                                     \'siblicide\', \'starvation\', \'lion\',\'infanticide\', \'death of mother\',\n                                                     \'unknown\'))\n\nsummarized.mortality <- rbind(summarized.mortality,\n                              data.frame(mortality = c(names(posterior.means), \'death of mother\', \'unknown\'),\n                                         frequency = c(posterior.means, nrow(um.death.of.mother), NA),\n                                         obs.inf = \'inferred\'))\n\nage.by.mortality <- rbind(known.mortality[,c(\'mortality\', \'age_at_death\')],\n                          unknown.mortality[,c(\'mortality\', \'age_at_death\')],\n                          data.frame(mortality = \'unknown\', age_at_death = um.death.of.mother$age_at_death))\n\nage.by.mortality$mortality <- factor(age.by.mortality$mortality, \n                                     levels = c( \'other\', \'human\',\n                                                 \'siblicide\', \'starvation\', \'lion\',\'infanticide\', \'death of mother\',\n                                                 \'unknown\'))\n\nlevs = c( \'other\', \'human\',\n          \'siblicide\', \'starvation\', \'lion\', \'infanticide\', \'death of mother\',  \'unknown\')\nlevs.ss <- paste0(levs, \'\\n(n = \', table(age.by.mortality$mortality), \')\')\n\nage.by.mortality$mortality <- factor(age.by.mortality$mortality, \n                                     levels = c( \'other\', \'human\',\n                                                 \'siblicide\', \'starvation\', \'lion\',\'infanticide\', \'death of mother\',\n                                                 \'unknown\'),\n                                     labels = levs.ss)\n\n\n## CI around infanticide and lion freqeuncy\npost.ci[""infanticide"",2]/nrow(all.mortality)\npost.ci[""infanticide"",3]/nrow(all.mortality)\nsum(summarized.mortality[summarized.mortality$mortality == \'infanticide\',]$frequency)/nrow(all.mortality)\n\npost.ci[""lion"",2]/nrow(all.mortality)\npost.ci[""lion"",3]/nrow(all.mortality)\nsum(summarized.mortality[summarized.mortality$mortality == \'lion\',]$frequency)/nrow(all.mortality)\n\n## Proportion of all cubs born dying due to infanticide\npost.ci[""infanticide"",2]/(nrow(all.mortality)+num.survivors)\npost.ci[""infanticide"",3]/(nrow(all.mortality)+num.survivors)\nsum(summarized.mortality[summarized.mortality$mortality == \'infanticide\',]$frequency)/(nrow(all.mortality)+num.survivors)\n\n\n### Plotting\n################################################################################\npdf(\'Plots/prob_mortality_source.pdf\', width = 7, height = 5, useDingbats = F)\npar(mfrow = c(2,3),\n    mar = c(0,0,0,0),\n    oma = c(4,4,1,1), family = \'sans\')\n\n## Infanticide \nplot(x = smooth.probs$age, \n     y = smooth.probs$infanticide, \n     col = \'black\', \n     type = \'l\', \n     lwd = 2,\n     ylim = c(0,1.1),\n     xlab = \'Age\',\n     ylab = \'\',\n     yaxt = \'n\',\n     xaxt = \'n\')\n\nlines(x = smooth.probs.high$age, \n     y = smooth.probs.high$infanticide, \n     col = \'black\', \n     type = \'l\', \n     lty = 2,\n     lwd = 1,\n     ylim = c(0,1))\n\nlines(x = smooth.probs.low$age, \n      y = smooth.probs.low$infanticide, \n      col = \'black\', \n      type = \'l\', \n      lty = 2,\n      lwd = 1,\n      ylim = c(0,1))\ntitle(main = \'infanticide\', line = -2)\naxis(side = 2, at = c(0,0.5, 1), labels = TRUE, outer = TRUE)\n\n## Lions \nplot(x = smooth.probs$age, \n     y = smooth.probs$lion, \n     col = \'black\', \n     type = \'l\', \n     lwd = 2,\n     ylim = c(0,1.1),\n     xlab = \'Age\',\n     ylab = \'\',\n     yaxt = \'n\',\n     xaxt = \'n\')\ntitle(main = \'lions\', line = -2)\n\nlines(x = smooth.probs.high$age, \n      y = smooth.probs.high$lion, \n      col = \'black\', \n      type = \'l\', \n      lty = 2,\n      lwd = 1,\n      ylim = c(0,1))\n\nlines(x = smooth.probs.low$age, \n      y = smooth.probs.low$lion, \n      col = \'black\', \n      type = \'l\', \n      lty = 2,\n      lwd = 1,\n      ylim = c(0,1))\n\n## Starvation \nplot(x = smooth.probs$age, \n     y = smooth.probs$starvation, \n     col = \'black\', \n     type = \'l\', \n     lwd = 2,\n     ylim = c(0,1.1),\n     xlab = \'Age\',\n     ylab = \'\',\n     yaxt = \'n\',\n     xaxt = \'n\')\n\nlines(x = smooth.probs.high$age, \n      y = smooth.probs.high$starvation, \n      col = \'black\', \n      type = \'l\', \n      lty = 2,\n      lwd = 1,\n      ylim = c(0,1))\n\nlines(x = smooth.probs.low$age, \n      y = smooth.probs.low$starvation, \n      col = \'black\', \n      type = \'l\', \n      lty = 2,\n      lwd = 1,\n      ylim = c(0,1))\ntitle(main = \'starvation\', line = -2)\n\n\n\n## Humans \nplot(x = smooth.probs$age, \n     y = smooth.probs$human, \n     col = \'black\', \n     type = \'l\', \n     lwd = 2,\n     ylim = c(0,1.1),\n     xlab = \'Age\',\n     ylab = \'\',\n     yaxt = \'n\',\n     xaxt = \'n\')\n\nlines(x = smooth.probs.high$age, \n      y = smooth.probs.high$human, \n      col = \'black\', \n      type = \'l\', \n      lty = 2,\n      lwd = 1,\n      ylim = c(0,1))\n\nlines(x = smooth.probs.low$age, \n      y = smooth.probs.low$human, \n      col = \'black\', \n      type = \'l\', \n      lty = 2,\n      lwd = 1,\n      ylim = c(0,1))\ntitle(main = \'humans\', line = -2)\naxis(side = 1, at = c(0,6,12), labels = TRUE, outer = TRUE)\nmtext(\'Probability of mortality source\', side = 2, line = 2,at = 1)\naxis(side = 2, at = c(0,0.5, 1), labels = TRUE, outer = TRUE)\n\n## Siblicide \nplot(x = smooth.probs$age, \n     y = smooth.probs$siblicide, \n     col = \'black\', \n     type = \'l\', \n     lwd = 2,\n     ylim = c(0,1.1),\n     ylab = \'\',\n     yaxt = \'n\',\n     xaxt = \'n\')\n\nlines(x = smooth.probs.high$age, \n      y = smooth.probs.high$siblicide, \n      col = \'black\', \n      type = \'l\', \n      lty = 2,\n      lwd = 1,\n      ylim = c(0,1))\n\nlines(x = smooth.probs.low$age, \n      y = smooth.probs.low$siblicide, \n      col = \'black\', \n      type = \'l\', \n      lty = 2,\n      lwd = 1,\n      ylim = c(0,1))\ntitle(main = \'siblicide\', line = -2)\naxis(side = 1, at = c(0,6,12), labels = TRUE, outer = TRUE)\nmtext(\'Age at death (months)\', side = 1, line = 2)\n\n## Other \nplot(x = smooth.probs$age, \n     y = smooth.probs$other, \n     col = \'black\', \n     type = \'l\', \n     lwd = 2,\n     ylim = c(0,1.1),\n     xlab = \'Age\',\n     ylab = \'\',\n     yaxt = \'n\',\n     xaxt = \'n\')\ntitle(main = \'other\', line = -2)\n\nlines(x = smooth.probs.high$age, \n      y = smooth.probs.high$other, \n      col = \'black\', \n      type = \'l\', \n      lty = 2,\n      lwd = 1,\n      ylim = c(0,1))\n\nlines(x = smooth.probs.low$age, \n      y = smooth.probs.low$other, \n      col = \'black\', \n      type = \'l\', \n      lty = 2,\n      lwd = 1,\n      ylim = c(0,1))\naxis(side = 1, at = c(0,6,12), labels = TRUE, outer = TRUE)\n\ndev.off()\n\n\n\nages <- ggplot(age.by.mortality, aes(x = age_at_death, y = mortality, fill = mortality))+\n  geom_density_ridges(scale = 1.6, panel_scaling = FALSE)+\n  theme_classic(base_size = 14)+\n  theme(legend.position = \'none\')+\n  xlab(\'Age at death (months)\')+\n  ylab(\'Mortality source\')+\n  scale_fill_manual(values = c(rep(\'grey30\', 7), \'grey85\'))+\n  xlim(-1,12.5)\n\n\n\n\ncounts <- ggplot(data=summarized.mortality,aes(x=mortality, y = frequency, width=0.8, color = obs.inf,\n                                                              fill = obs.inf))+\n  geom_bar(stat = \'identity\')+\n  theme_classic(base_size = 14)+\n  xlab(""Source of Mortality"")+\n  ylab(""Count"")+\n  scale_fill_manual(values = c(\'gray85\', \'grey30\'))+\n  scale_color_manual(values = c(\'gray85\', \'grey30\'))+\n  theme(legend.position = c(0.7,0.95),\n        legend.title = element_blank(),\n        axis.text.y = element_blank(),\n        axis.line.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(),\n        plot.margin = unit(c(4,0,6,0), units = \'pt\'))+\n  geom_errorbar(data = post.ci, aes(x = mortality, ymin = low, ymax = high), inherit.aes = F, width = 0.2) + \n  coord_flip()\n\n\ngroblist <- list(ggplotGrob(ages), ggplotGrob(counts))\n\npdf(\'Plots/mortality_source_and_age.pdf\', width = 7, height = 5, useDingbats = F)\ngrid.arrange(grobs = groblist,\n             layout_matrix = matrix(data = c(1,1,1,2,2,\n                                             1,1,1,2,2,\n                                             1,1,1,2,2,\n                                             1,1,1,2,2,\n                                             1,1,1,2,2,\n                                             1,1,1,2,2,\n                                             1,1,1,2,2,\n                                             1,1,1,2,2,\n                                             1,1,1,2,2,\n                                             1,1,1,2,2),\n                                    nrow = 10, ncol = 5, byrow = TRUE))\ndev.off()\n################################################################################\n', '\nlibrary(brms)\nlibrary(dplyr)\nlibrary(here)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nset.seed(1989)\noptions(stringsAsFactors = FALSE)\n################################################################################\n### Load data for analysis\nrm(list = ls())\nload(file = here(\'Data/cub_data.RData\'))\nload(file = here(\'Data/known_mortality_cleaned.RData\'))\n\n\nhypothesis.test.dataset <- filter(known.mortality, clan %in% tblFemaleRanks$clan)\n\n### Set infanticide as intercept for modeling\nhypothesis.test.dataset$mortality.model <- factor(hypothesis.test.dataset$mortality, levels = c(\'infanticide\', \'lion\', \'other\', \'siblicide\', \'death of mother\', \'starvation\', \'human\'))\nhypothesis.test.dataset$death_of_mother <- as.numeric(hypothesis.test.dataset$mortality == \'death of mother\')\n\nprey.test.data <- hypothesis.test.dataset[!is.na(hypothesis.test.dataset$prey_density),]\nprey.test.data$y <- as.matrix(prey.test.data[,c(\'infanticide\', \'starvation\', \'lion\', \'siblicide\',\'death_of_mother\',\n                                                  \'human\',\'other\')])\n\ncub.density.test.data <- hypothesis.test.dataset[!is.na(hypothesis.test.dataset$cub_associates),]\ncub.density.test.data$y <- as.matrix(cub.density.test.data[,c(\'infanticide\', \'starvation\', \'lion\', \'siblicide\',\'death_of_mother\',\n                                                \'human\',\'other\')])\n\n\n################################################################################\n### Does prey density vary by mortality source?\nprey_mod <- brm(data = prey.test.data, bf(y|trials(1) ~ 1 + prey_density),\n                family = multinomial(), chains = 4, iter = 30000, warmup = 15000, cores = 4, seed= 1989, inits = 0)\nprey_null_mod <- brm(data = prey.test.data, bf(y|trials(1) ~ 1),\n                     family = multinomial(), chains = 4, iter = 30000, warmup = 15000, cores = 4, seed= 1989, inits = 0)\n\n\n\n## Model comparison\nloo(prey_mod, prey_null_mod)\n\n################################################################################\n### Does the number of cubs vary by mortality source?\ncub_density_mod <- brm(data = cub.density.test.data, bf(y|trials(1) ~ 1 + cub_associates),\n                       family = multinomial(), chains = 4, iter = 30000, warmup = 15000, cores = 4, seed= 1989, inits = 0)\ncub_null_mod <- brm(data = cub.density.test.data, bf(y|trials(1) ~ 1),\n                    family = multinomial(), chains = 4, iter = 30000, warmup = 15000, cores = 4, seed= 1989, inits = 0)\n\n## Model comparison\nloo(cub_density_mod, cub_null_mod)\n\nsave(prey_mod, prey_null_mod, cub_density_mod, cub_null_mod, file = \'Data/prey_and_cub_models.RData\')\n\n################################################################################\n### Do the ranks of killers and mothers of victims differ?\n\n### Examine ranks of killers and victims. Use table of qualitative data on\n#   infanticide events\n\n### add year to infanticide data\ntblFemaleRanks$year <- as.numeric(tblFemaleRanks$year)\ntblFemaleRanks$stan_rank <- as.numeric(tblFemaleRanks$stan_rank)\ninfanticide_notes$Mom_rank <- NA\ninfanticide_notes$Killer_rank <- NA\n\n\n### Add killer and mother ranks\nfor(i in 1:nrow(infanticide_notes)){\n  mom <- infanticide_notes$Mom[i]\n  killer <- infanticide_notes$Killers[i]\n  date <- infanticide_notes$Date[i]\n  ###mom rank\n  if(mom %in% filter(tblFemaleRanks, year == infanticide_notes[i,\'Year\'])$id){\n    infanticide_notes$Mom_rank[i] <- filter(tblFemaleRanks, year == infanticide_notes[i,\'Year\'],\n                                            id == mom)$stan_rank\n  }\n  \n  ##killer rank\n  if(killer %in% filter(tblFemaleRanks, year == infanticide_notes[i,\'Year\'])$id){\n    infanticide_notes$Killer_rank[i] <- filter(tblFemaleRanks, year == infanticide_notes[i,\'Year\'],\n                                               id == killer)$stan_rank\n  }\n  \n}\n\n### Statistical test comparing ranks of killers and mothers\nt.test(infanticide_notes$Mom_rank[!is.na(infanticide_notes$Mom_rank)], \n       infanticide_notes$Killer_rank[!is.na(infanticide_notes$Killer_rank)])\n\n### Prep data for plotting\nmom.killer.rank <- rbind(data.frame(whose = rep(\'mom\'), \n                                    rank = infanticide_notes$Mom_rank),\n                         data.frame(whose = rep(\'killer\'),\n                                    rank = infanticide_notes$Killer_rank))\nmom.killer.rank$whose <-factor(mom.killer.rank$whose, levels = c(\'killer\', \'mom\'))\n\n\nlabs = c(\'Killer\', \'Mother of victim\')\nlabs = paste0(labs, \'\\n(n = \', table(na.omit(mom.killer.rank)$whose), \')\')\nmom.killer.rank$whose <-factor(mom.killer.rank$whose, levels = c(\'killer\', \'mom\'),\n                               labels = labs)\n\n################################################################################\n### Plotting\npdf(file = \'Plots/Killer_rank.pdf\',width = 3.5, height = 3.5, useDingbats = F)\nggplot(mom.killer.rank, aes(x = whose, y= rank))+\n  geom_boxplot(color = \'grey30\', size= 1, fill = \'grey85\')+\n  theme_classic(base_size = 14)+\n  xlab("""")+\n  ylab(""Standardized Rank\\n(Low)                                 (High)"")+\n  scale_x_discrete()\ndev.off()\n\n\n#### Plotting\n\nhypothesis.test.dataset$mortality.plot <- factor(hypothesis.test.dataset$mortality, levels = c(\'death of mother\', \'infanticide\', \'lion\', \'starvation\', \'siblicide\', \'human\', \'other\'),\n                                         labels = c(\'death of\\nmother\', \'infanticide\', \'lion\', \'starvation\', \'siblicide\', \'human\', \'other\'))\n\nprey <- ggplot(data = hypothesis.test.dataset, aes(x = mortality.plot, y = prey_density))+\n  geom_boxplot(outlier.color = NA, color = \'grey30\', size= 1, fill = \'grey85\')+\n  labs(tags = \'a) \')+\n  theme_classic(base_size = 14) +\n  coord_cartesian(ylim= c(0,800))+\n  ylab(\'Average prey density\')+\n  xlab(\'\')\n\n\ncubs <- ggplot(data = hypothesis.test.dataset, aes(x = mortality.plot, y = cub_associates))+\n  geom_boxplot(outlier.color = NA, color = \'grey30\', size= 1, fill = \'grey85\')+\n  labs(tags = \'b) \')+\n  theme_classic(base_size = 14)+\n  ylab(\'Average cub density\')+\n  xlab(\'Mortality source\')\n\n\npdf(file = \'Plots/prey_and_cub_mortality.pdf\', width = 7, height = 6, useDingbats = F)\nprey + cubs + plot_layout(ncol = 1)\ndev.off()\n']","Infanticide by females is a leading source of juvenile mortality in a large social carnivore Social animals benefit from their group-mates, so why do they sometimes kill each other's offspring? Using 30 years of data from multiple groups of wild spotted hyenas, we address three critical aims for understanding infanticide in any species: (1) quantify the contribution of infanticide to overall mortality (2) describe the circumstances under which infanticide occurs and (3) evaluate hypotheses about the evolution of infanticide. We find that, although observed only rarely, infanticide is in fact a leading source of juvenile mortality. Infanticide accounted for 24% of juvenile mortality, and 1 in 10 hyenas born in our population perished due to infanticide. In all observed cases of infanticide, killers were adult females, but victims could be of both sexes. Of four hypotheses regarding the evolution of infanticide, we found the most support for the hypothesis that infanticide in spotted hyenas reflects competition over social status among matrilines.",4
Data for: Decoupling of sexual signals and their underlying morphology facilitates rapid phenotypic diversification,"How novel phenotypes evolve is challenging to imagine because traits are often underlain by numerous integrated phenotypic components, and changes to any one form can disrupt the function of the entire module. Yet novel phenotypes do emerge, and research on adaptive phenotypic evolution suggests that complex traits can diverge while either maintaining existing form-function relationships or through innovations that alter form-function relationships. How these alternate routes contribute to sexual signal evolution is poorly understood, despite the role of sexual signals in generating biodiversity. In Hawaiian populations of the Pacific field cricket, male song attracts both female crickets and a deadly acoustically orienting parasitoid fly. In response to this conflict between natural and sexual selection, male crickets have evolved altered wing morphologies multiple times, resulting in loss and dramatic alteration of sexual signals. More recently, we and others have observed a radical increase in sexual signal variation and the underlying morphological structures that produce song. We conducted the first combined analysis of form (wing morphology), function (emergent signal), and receiver responses to characterize novel variation, test alternative hypotheses about form-function relationships (Form-Function Continuity vs. Form-Function Decoupling) and investigate underlying mechanistic changes and fitness consequences of novel signals. We identified three sound-producing male morphs (one previously undescribed, named ""rattling"") and found that relationships between morphology and signals have been rewired (Form-Function Decoupling), rapidly and repeatedly, through the gain, loss, and alteration of morphological structures, facilitating the production of signals that exist in novel phenotypic space. By integrating across a hierarchy of phenotypes, we uncovered divergent morphs with unique solutions to the challenge of attracting mates while evading fatal parasitism.","['library(tidyverse)\nlibrary(ggcorrplot)\nlibrary(GGally)\nlibrary(Hmisc)\nlibrary(corrplot)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(emmeans)\nlibrary(logistf)\nlibrary(factoextra)\n\n# Read in data on wing morphology and song\nsw <- read.csv(""~/Desktop/SuppData_songwing.csv"", header = T) # Change \'...\' to your path to data/script\n\n#### Plotting Form-Function relationships of all individuals in our data set (Figure 2C)\n# First choose which variables to plot\nkeep <- c(""wingPC1"", ""harp_width"",""Scraper"",""Mirror"",""calling_PC1"")\n\n# Copy dataframe\nfig2_pairs <- sw\n\n# Remove individuals with NA\'s for wing structures, convert mirror & scraper to factor \nfig2_pairs <- fig2_pairs %>% filter(Scraper %in% c(0,1))\nfig2_pairs <- fig2_pairs %>% filter(Mirror %in% c(0,1))\nfig2_pairs$Mirror <- as.factor(as.character(fig2_pairs$Mirror))\nfig2_pairs$Scraper <- as.factor(as.character(fig2_pairs$Scraper))\n\n# Pairwise plots \ng <- ggpairs(fig2_pairs[,keep], \n             lower = list(continuous = ""points"", combo = ""box""), switch = ""y"",\n             upper = list(continuous = ""cor"", combo = ""box_no_facet""))\ng + theme_bw()\n\nrm(fig2_pairs, keep, g)\n\n#### Hierarchical clustering, using individuals with complete data (Figure 3A)\n# Choose which variables to include in clustering\nkeep <- c(""maleID"", ""Peak_Frequency"",""Amplitude"", ""rangeA"", ""rangeB"", ""rangeC"", ""rangeD"", ""rangeE"",""rangeF"", ""Frequency.Evenness"", \n          ""Peak.Frequency_chirp"",""Amplitude_chirp"", ""rangeA_court_chirp"", ""rangeB_court_chirp"", ""rangeC_court_chirp"", ""rangeD_court_chirp"", ""rangeE_court_chirp"",""rangeF_court_chirp"", ""freq_even_chirp"", \n          ""Peak.Frequency_trill"",""Amplitude_trill"", ""rangeA_court_trill"", ""rangeB_court_trill"", ""rangeC_court_trill"", ""rangeD_court_trill"", ""rangeE_court_trill"",""rangeF_court_trill"", ""freq_even_trill"",\n          ""wingPC1"",""wingPC2"",""wingPC3"", ""harp_width"",""Scraper"",""Mirror"", ""calling_PC1"",""court_PC1"", ""mirror_cs"")\nkeep <- which(colnames(sw) %in% keep)\n\n# Remove individuals who are missing any data (left with males with wing data, plus calling and courtship song measures)\nfig3_dendro <- na.omit(sw[,keep])\n\n# Perform hierarchical clustering using 33 measures of song and wing variation\nrownames(fig3_dendro) <- fig3_dendro$maleID\nfviz_nbclust(fig3_dendro[,c(2:35)], hcut, method = ""gap_stat"") # include everything except maleID & song PCA composites (because raw variables already included)\nres <- hcut(fig3_dendro[,c(2:35)], k = 3, hc_func = ""hclust"", hc_method = ""ward.D2"", stand = T)\n\n# Visualize the dendrogram, with clusters colored\nfviz_dend(res, rect = TRUE, k_colors = c(""#1B9E77"", ""#D95F02"", ""#7570B3""), lower_rect = -0.1)\n\nrm(fig3_dendro, keep, res) # clean up environment\n\n### Within-morph analyses, and wing x song correlations\n# While hierarchical clustering was restricted to males with ALL data, we created a larger data set with less restrictive criteria for inclusion.\n# We included individuals that had wing morphology data, plus measures from at least one song (calling and/or courtship). \n# Using diagnostic phenotypic characteristics from the above cluster analysis, we manually assigned these individuals to morphs. \n# Run the two lines below to filter data to these individuals\nmanual_morph <- sw %>% filter(data_types >1 & wingData == 1)\nmanual_morph <- manual_morph %>% filter(Song_Type  %in% c(""Typical"",""Rattle"",""Purr""))\n\n# This results in 69 purring males, 13 rattling males, and 23 ancestral males\nmanual_morph %>% group_by(Song_Type) %>% tally()\n\n# Run MANOVAs on wing, calling & courtship song, by morph\ncalling_manova <- manova(cbind(calling_PC1, calling_PC2, calling_PC3) ~ Song_Type, data = manual_morph)\nsummary(calling_manova)\n\ncourt_manova <- manova(cbind(court_PC1, court_PC2, court_PC3) ~ Song_Type, data = manual_morph)\nsummary(court_manova)\n\nwing_manova <- manova(cbind(wingPC1, wingPC2, wingPC3) ~ Song_Type, data = manual_morph)\nsummary(wing_manova)\n\n# Calculate summary statistics \nmeansZ <- manual_morph %>% group_by(Song_Type) %>% summarise(across(.cols = everything(), ~mean(.x, na.rm = TRUE)))\nmeansZ <- meansZ[c(3,2,1),]\nSDsZ <- manual_morph %>% group_by(Song_Type) %>% summarise(across(.cols = everything(), ~sd(.x, na.rm = TRUE)))\nSDsZ <- SDsZ[c(3,2,1),]\n\n# Simple t-test of peak frequencies of purring males with and without scrapers\nt.test(manual_morph[manual_morph$Song_Type == ""Purr"" & manual_morph$Scraper == 1,]$Peak_Frequency, manual_morph[manual_morph$Song_Type == ""Purr"" & manual_morph$Scraper == 0,]$Peak_Frequency)\n\nrm(calling_manova, court_manova, meansZ, SDsZ, wing_manova) #clean up\n\n#### Within the three morphs (Purring, Rattling, Ancestral) calculate Pearson\'s correlations between a subset of wing morphology and song characteristic variables (Figure 3D)\n# Make a vector of variables that we want to include\nkeep <- c(""wingPC1"",""Scraper"",""mirror_cs"",""harp_width"",""calling_PC1"",""Peak_Frequency"",""Amplitude"",""Frequency.Evenness"")\nkeep <- which(colnames(manual_morph) %in% keep)\n\n# Calculate correlations\ncorrelations <- cor(manual_morph[,keep], use = ""pairwise.complete.obs"")\n\n# Significance testing of correlations\ncorrelations_sig <- rcorr(as.matrix(manual_morph[,keep]))\n\n# Plot correlations between wing morphological variable and signal characteristics for ALL individuals\ncorrplot(correlations[c(8,5:7), c(1:2,4,3)], p.mat = correlations_sig$P[c(8,5:7), c(1:2,4,3)], sig.level = 0.05)\n\n# Do the same, but this time only for purring males\ncorrelations_purr <- cor(manual_morph[manual_morph$Song_Type == ""Purr"", c(keep)], use = ""pairwise.complete.obs"")\ncorrelations_sig_purr <- rcorr(as.matrix(manual_morph[manual_morph$Song_Type == ""Purr"", c(keep)]))\ncorrplot(correlations_purr[c(8,5:7), c(1:2,4,3)], p.mat = correlations_sig_purr$P[c(8,5:7), c(1:2,4,3)]) #, sig.level = 0.05, insig = ""blank"")\n\n# Form-Function correlations within rattling males\ncorrelations_rattle <- cor(manual_morph[manual_morph$Song_Type == ""Rattle"", c(keep)], use = ""pairwise.complete.obs"")\ncorrelations_sig_rattle <- rcorr(as.matrix(manual_morph[manual_morph$Song_Type == ""Rattle"", c(keep)]))\ncorrplot(correlations_rattle[c(8,5:7), c(1:2,4,3)], p.mat = correlations_sig_rattle$P[c(8,5:7), c(1:2,4,3)]) #, sig.level = 0.05, insig = ""blank"")\n\n# Form-Function correlations within ancestral males\ncorrelations_typical <- cor(manual_morph[manual_morph$Song_Type == ""Typical"", c(keep)], use = ""pairwise.complete.obs"")\ncorrelations_sig_typical <- rcorr(as.matrix(manual_morph[manual_morph$Song_Type == ""Typical"", c(keep)]))\ncorrplot(correlations_typical[c(8,5:7), c(1:2,4,3)], p.mat = correlations_sig_typical$P[c(8,5:7), c(1:2,4,3)]) #, sig.level = 0.05, insig = ""blank"")\n\nrm(correlations, correlations_purr, correlations_rattle, correlations_typical,correlations_sig, correlations_sig_purr, correlations_sig_rattle, correlations_sig_typical, keep)\n\n#### Fly and female cricket phonotaxis analyses (Figure 4B)\n# Read in cricket phonotaxis data\nphono <- read.csv(""~/Desktop/SuppData_cricket_phono.csv"") # Change \'...\' to your path to data\n\n# Calculate proportions of females phonotactic to each stimulus\nphono_summary <- phono %>% group_by(song_simple, Phonotaxis) %>% tally()\nphono_summary <- phono_summary %>% group_by( song_simple ) %>% mutate(Sum = sum(n))\nphono_summary <- phono_summary %>% mutate(proportion = n/Sum) \n\n# Plot\nphono_summary$song_simple <- fct_relevel(phono_summary$song_simple, ""WN"",""Purr"", ""Rattle"",""Typical"")\nggplot(phono_summary, aes(fill=as.character(Phonotaxis), y=n, x=song_simple)) + \n  geom_bar(position=""fill"", stat=""identity"") + theme_bw()\n\n# Generalized linear mixed model testing effect of song on phonotactic behavior, with individual ID as random effect\nmixe <- glmer(Phonotaxis ~ song_simple +    \n                (1 | uniqueID), family = ""binomial"", data = phono,\n              control = glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1000000)))\nsummary(mixe)\ncar::Anova(mixe, type = ""III"")\n\n# Pairwise comparisons between stimuli using estimated marginal means\nmixe.em <- emmeans(mixe, ""song_simple"")\npairs(mixe.em)\n\n# Calculate proportion of females that contacted playback speaker to each stimulus\ncontact_summary <- phono %>% group_by(song_simple, contact_yn) %>% tally()\ncontact_summary <- contact_summary %>% group_by( song_simple ) %>% mutate(Sum = sum(n))\ncontact_summary <- contact_summary %>% mutate(proportion = n/Sum) \n\n#Plot\ncontact_summary$song_simple <- fct_relevel(contact_summary$song_simple, ""WN"",""Purr"", ""Rattle"",""Typical"")\nggplot(contact_summary, aes(fill=as.character(contact_yn), y=n, x=song_simple)) + \n  geom_bar(position=""fill"", stat=""identity"") + theme_bw()\n\n# Due to problem of separation in this analysis (no observations of contact with White Noise), run a Firth\'s Bias-reduced logistic regression \n#(note that this does not allow us to include individual as a random effect, as we did with the glmm above)\nphono$song_simple <- relevel(phono$song_simple, ref = ""Typical"")\nfit <- logistf(contact_yn ~ song_simple, data = phono)\nsummary(fit)\n\n# Pull in fly phonotaxis data (Figure 3B, cont\'d)\nfly_phono <- read.csv(""~/Desktop/SuppData_fly_phono.csv"") # Change \'...\' to your path to data\n\n# Calculate proportions of flies phonotactic to each stimulus\nflycontact <- fly_phono %>% group_by(song_simple, contact_yn) %>% tally()\nflycontact <- flycontact %>% group_by( song_simple ) %>% mutate(Sum = sum(n))\nflycontact <- flycontact %>% mutate(proportion = n/Sum) \nflycontact$song_simple <- fct_relevel(flycontact$song_simple, ""WN"",""purr"", ""rattle"",""typical"")\n\n#Plot\nggplot(flycontact, aes(fill=as.character(contact_yn), y=n, x=song_simple)) + \n  geom_bar(position=""fill"", stat=""identity"") + theme_bw()\n']","Data for: Decoupling of sexual signals and their underlying morphology facilitates rapid phenotypic diversification How novel phenotypes evolve is challenging to imagine because traits are often underlain by numerous integrated phenotypic components, and changes to any one form can disrupt the function of the entire module. Yet novel phenotypes do emerge, and research on adaptive phenotypic evolution suggests that complex traits can diverge while either maintaining existing form-function relationships or through innovations that alter form-function relationships. How these alternate routes contribute to sexual signal evolution is poorly understood, despite the role of sexual signals in generating biodiversity. In Hawaiian populations of the Pacific field cricket, male song attracts both female crickets and a deadly acoustically orienting parasitoid fly. In response to this conflict between natural and sexual selection, male crickets have evolved altered wing morphologies multiple times, resulting in loss and dramatic alteration of sexual signals. More recently, we and others have observed a radical increase in sexual signal variation and the underlying morphological structures that produce song. We conducted the first combined analysis of form (wing morphology), function (emergent signal), and receiver responses to characterize novel variation, test alternative hypotheses about form-function relationships (Form-Function Continuity vs. Form-Function Decoupling) and investigate underlying mechanistic changes and fitness consequences of novel signals. We identified three sound-producing male morphs (one previously undescribed, named ""rattling"") and found that relationships between morphology and signals have been rewired (Form-Function Decoupling), rapidly and repeatedly, through the gain, loss, and alteration of morphological structures, facilitating the production of signals that exist in novel phenotypic space. By integrating across a hierarchy of phenotypes, we uncovered divergent morphs with unique solutions to the challenge of attracting mates while evading fatal parasitism.",4
In the face of climate change and exhaustive exercise: The physiological response of an important recreational fish species,"Cobia (Rachycentron canadum) support recreational fisheries along the U.S. mid- and south-Atlantic states and have been recently subjected to increased fishing effort, primarily during their spawning season in coastal habitats where increasing temperatures and expanding hypoxic zones are occurring due to climate change. We therefore undertook a study to quantify the physiological abilities of cobia to withstand increases in temperature and hypoxia, including their ability to recover from exhaustive exercise. Respirometry was conducted on cobia from Chesapeake Bay to determine aerobic scope, critical oxygen saturation, ventilation volume, and the time to recover from exhaustive exercise under temperature and oxygen conditions projected to be more common in inshore areas by the middle and end of this century. Cobia physiologically tolerated predicted mid- and end-of-century temperatures (28-32C) and oxygen concentrations as low as 1.7-2.4 mg l-1 suggesting that they may be a climate change winner. Our results indicated cobia can withstand environmental fluctuations that occur in coastal habitats and the broad environmental conditions their prey items can tolerate. However, at these high temperatures some cobia did suffer post exercise mortality. It appears cobia will be able to withstand near future climate impacts in coastal habitats like Chesapeake Bay, but as conditions worsen, catch-and-release fishing may result in higher mortality than under present conditions.",,"In the face of climate change and exhaustive exercise: The physiological response of an important recreational fish species Cobia (Rachycentron canadum) support recreational fisheries along the U.S. mid- and south-Atlantic states and have been recently subjected to increased fishing effort, primarily during their spawning season in coastal habitats where increasing temperatures and expanding hypoxic zones are occurring due to climate change. We therefore undertook a study to quantify the physiological abilities of cobia to withstand increases in temperature and hypoxia, including their ability to recover from exhaustive exercise. Respirometry was conducted on cobia from Chesapeake Bay to determine aerobic scope, critical oxygen saturation, ventilation volume, and the time to recover from exhaustive exercise under temperature and oxygen conditions projected to be more common in inshore areas by the middle and end of this century. Cobia physiologically tolerated predicted mid- and end-of-century temperatures (28-32C) and oxygen concentrations as low as 1.7-2.4 mg l-1 suggesting that they may be a climate change winner. Our results indicated cobia can withstand environmental fluctuations that occur in coastal habitats and the broad environmental conditions their prey items can tolerate. However, at these high temperatures some cobia did suffer post exercise mortality. It appears cobia will be able to withstand near future climate impacts in coastal habitats like Chesapeake Bay, but as conditions worsen, catch-and-release fishing may result in higher mortality than under present conditions.",4
Topics in Research on International Relations as Clusters of Citation Links,"Data, scripts, and results of a memetic topic clustering of citation links in papers published 2011-2015 in the specialty of political science that is dealing with international relations Supplementary Information to the paper about ""Topics as clusters of citation links to highly cited sources: The case of research on international relation"" by Frank Havemann (published 2021 in the OA-journal Quantitative Science Studies 2 (1): 204223). https://doi.org/10.1162/qss_a_00108",,"Topics in Research on International Relations as Clusters of Citation Links Data, scripts, and results of a memetic topic clustering of citation links in papers published 2011-2015 in the specialty of political science that is dealing with international relations Supplementary Information to the paper about ""Topics as clusters of citation links to highly cited sources: The case of research on international relation"" by Frank Havemann (published 2021 in the OA-journal Quantitative Science Studies 2 (1): 204223). https://doi.org/10.1162/qss_a_00108",4
Data from: Nitrogen enrichment alters multiple dimensions of grassland functional stability via changing compositional stability,"Anthropogenic nutrient enrichment is known to alter the composition and functioning of plant communities. However, how nutrient enrichment influences multiple dimensions of community- and ecosystem-level stability remains poorly understood. Using data from a nitrogen (N) and phosphorus (P) addition experiment in a temperate semi-arid grassland that experienced a natural drought, we show that N enrichment, not P enrichment, decreased grassland functional and compositional temporal stability, resistance, and recovery, but increased functional and compositional resilience. Compositional stability and species asynchrony, rather than species diversity, were identified as key determinants of all dimensions of grassland functional stability, except for recovery. Whereas grassland functional recovery was decoupled from compositional recovery, N enrichment altered other dimensions of functional stability primarily through changing their corresponding compositional stability dimensions. Our findings highlight the need to examine ecological stability at the community level for a more mechanistic understanding of ecosystem dynamics in the face of environmental change.","['# Clear working environment\r\nrm(list = ls())\r\n\r\n# load packages\r\nlibrary(plyr)\r\nlibrary(ggplot2)\r\nlibrary(agricolae)\r\nlibrary(ggpubr)\r\nlibrary(cowplot)\r\nlibrary(nlme)\r\nlibrary(piecewiseSEM)\r\n\r\n# Import data\r\nNPdata <- read.csv(""Duolun_grassland_experiment_data.csv"", header = T)\r\n\r\n############### Figure 2 ###############\r\n#Panel a\r\nFunction_ts.aov1 <- aov(Function_ts ~ Treatment, data = NPdata)\r\nhsd.Function_ts <- HSD.test(Function_ts.aov1, ""Treatment"", group = T)\r\nTS_func <- ddply(NPdata, ""Treatment"", summarise,\r\n                 N = length(Function_ts), mean = mean(Function_ts), sd = sd(Function_ts), se = sd/sqrt(N))\r\nTS_func$Treatment <- factor(TS_func$Treatment, levels = c(""Control"",""P"",""N"",""N+P"" ))\r\np0 <- ggplot(TS_func, aes(x=Treatment, y=mean))+\r\n  geom_bar(stat = ""identity"", fill=""darkgray"", width = 0.6) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(0, 3)) +\r\n  geom_errorbar(aes(ymin=mean-se, ymax=mean+se),width=0.1, size=0.3)+\r\n  xlab("""") +\r\n  ylab(""Functional temporal stability"") +\r\n  labs(tag = ""(a)"") +\r\n  theme_bw()+ \r\n  theme(aspect.ratio = 1, panel.grid.major =element_blank(), panel.grid.minor = element_blank()) +\r\n  theme(axis.title = element_text(size=15), axis.text = element_text(size=12)) +\r\n  annotate(""text"", x = 1:4, y = c(2.7,2.85,1.85,1.7), label=c(""a"", ""a"",""b"",""b""), size=4)\r\n\r\n#Panel c\r\nResistance.aov1 <- aov(Function_Resistance ~ Treatment, data = NPdata)\r\nhsd.Resistance <- HSD.test(Resistance.aov1, ""Treatment"",group = T)\r\nResistance <- ddply(NPdata,""Treatment"",summarise,\r\n                N = length(Function_Resistance),\r\n                mean = mean(Function_Resistance),\r\n                sd = sd(Function_Resistance),\r\n                se = sd/sqrt(N))\r\nResistance$Treatment <- factor(Resistance$Treatment, levels = c(""Control"",""P"",""N"",""N+P"" ))\r\np1 <- ggplot(Resistance, aes(x=Treatment, y=mean))+\r\n  geom_bar(stat = ""identity"", fill=""darkgrey"", width = 0.6) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(-2.2, 0)) +\r\n  geom_errorbar(aes(ymin=mean-se, ymax=mean+se),width=0.1, size=0.3)+\r\n  xlab("""") +\r\n  ylab(""Functional resistance"") +\r\n  labs(tag = ""(c)"")+\r\n  theme_bw()+ \r\n  theme(aspect.ratio = 1, panel.grid.major =element_blank(), panel.grid.minor = element_blank()) +\r\n  theme(axis.title = element_text(size=15), axis.text = element_text(size=12))+\r\n  annotate(""text"", x = 1:4, y = c(-0.85,-1.05,-1.78,-2.1), label=c(""a"", ""a"",""b"",""b""), size=4)\r\n\r\n#Panel e\r\nresil.aov1 <- aov(Function_Resilience ~ Treatment, data = NPdata)\r\nhsd.resil <- HSD.test(resil.aov1, ""Treatment"",group = T)\r\nresil <- ddply(NPdata,""Treatment"",summarise,\r\n               N = length(Function_Resilience),\r\n               mean = mean(Function_Resilience),\r\n               sd = sd(Function_Resilience),\r\n               se = sd/sqrt(N))\r\nresil$Treatment <- factor(resil$Treatment, levels = c(""Control"",""P"",""N"",""N+P"" ))\r\np2 <- ggplot(resil, aes(x=Treatment, y=mean))+\r\n  geom_bar(stat = ""identity"", fill=""darkgrey"", width = 0.6) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(0, 2)) +\r\n  geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=0.1, size=0.3)+\r\n  xlab("""") +\r\n  ylab(""Functional resilience"") +\r\n  labs(tag = ""(e)"")+\r\n  theme_bw()+ \r\n  theme(aspect.ratio = 1, panel.grid.major =element_blank(), panel.grid.minor = element_blank()) +\r\n  theme(axis.title = element_text(size=15), axis.text = element_text(size=12)) +\r\n  annotate(""text"", x = 1:4, y = c(0.95,1.15,1.6,1.9), label=c(""b"", ""b"",""a"",""a""), size=4)\r\n\r\n#Panel g\r\nrcv.aov1 <- aov(Function_Recovery ~ Treatment, data = NPdata)\r\nhsd.rcv <- HSD.test(rcv.aov1, ""Treatment"",group = T)\r\nrecover <- ddply(NPdata,""Treatment"",summarise,\r\n                 N = length(Function_Recovery),\r\n                 mean = mean(Function_Recovery),\r\n                 sd = sd(Function_Recovery),\r\n                 se = sd/sqrt(N))\r\nrecover$Treatment <- factor(recover$Treatment, levels = c(""Control"",""P"",""N"",""N+P"" ))\r\np3 <- ggplot(recover, aes(x=Treatment, y=mean))+\r\n  geom_bar(stat = ""identity"", fill=""darkgrey"", width = 0.6) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(-1, 1)) +\r\n  geom_hline(yintercept = 0) +\r\n  geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=0.1, size=0.3)+\r\n  xlab("""") +\r\n  ylab(""Functional recovery"") +\r\n  labs(tag = ""(g)"")+\r\n  theme_bw()+ \r\n  theme(aspect.ratio = 1, panel.grid.major =element_blank(), panel.grid.minor = element_blank()) +\r\n  theme(axis.title = element_text(size=15), axis.text = element_text(size=12))+\r\n  annotate(""text"", x = 1:4, y = c(0.4,0.25,-0.35,-0.4), label=c(""a"", ""ab"",""ab"",""b""), size=4)\r\n\r\n#Panel b\r\nvar.aov1 <- aov(Composition_ts ~ Treatment, data = NPdata)\r\nhsd.var <- HSD.test(var.aov1, ""Treatment"", group = T)\r\nTS_comp <- ddply(NPdata, ""Treatment"", summarise,\r\n                 N = length(Composition_ts), mean = mean(Composition_ts), sd = sd(Composition_ts), se = sd/sqrt(N))\r\nTS_comp$Treatment <- factor(TS_comp$Treatment, levels = c(""Control"",""P"",""N"",""N+P"" ))\r\np00 <- ggplot(TS_comp, aes(x=Treatment, y=mean))+\r\n  geom_bar(stat = ""identity"", fill=""darkgray"", width = 0.6) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(0, 1)) +\r\n  geom_errorbar(aes(ymin=mean-se, ymax=mean+se),width=0.1, size=0.3)+\r\n  xlab("""") +\r\n  ylab(""Compositional temporal stability"") +\r\n  labs(tag = ""(b)"")+\r\n  theme_bw()+ \r\n  theme(aspect.ratio = 1, panel.grid.major =element_blank(), panel.grid.minor = element_blank())+\r\n  theme(axis.title = element_text(size=15), axis.text = element_text(size=12)) +\r\n  annotate(""text"", x = 1:4, y = c(0.85,0.8,0.7,0.7), label=c(""a"", ""a"",""b"",""b""), size=4)\r\n\r\n#Panel d\r\nResistance.comp1 <- aov(Composition_Resistance ~ Treatment, data = NPdata)\r\nhsd.Resistance.comp <- HSD.test(Resistance.comp1, ""Treatment"",group = T)\r\nResistance.comp <- ddply(NPdata,""Treatment"",summarise,\r\n                     N = length(Composition_Resistance),\r\n                     mean = mean(Composition_Resistance),\r\n                     sd = sd(Composition_Resistance),\r\n                     se = sd/sqrt(N))\r\nResistance.comp$Treatment <- factor(Resistance.comp$Treatment, levels = c(""Control"",""P"",""N"",""N+P""))\r\np4 <- ggplot(Resistance.comp, aes(x=Treatment, y=mean))+\r\n  geom_bar(stat = ""identity"", fill=""darkgrey"", width = 0.6) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(0, 1)) +\r\n  geom_errorbar(aes(ymin=mean-se, ymax=mean+se),width=0.1, size=0.3)+\r\n  xlab("""") +\r\n  ylab(""Compositional resistance"") +\r\n  labs(tag = ""(d)"")+\r\n  theme_bw()+ \r\n  theme(aspect.ratio = 1, panel.grid.major =element_blank(), panel.grid.minor = element_blank())+\r\n  theme(axis.title = element_text(size=15), axis.text = element_text(size=12)) +\r\n  annotate(""text"", x = 1:4, y = c(0.62,0.55,0.35,0.3), label=c(""a"", ""a"",""b"",""b""), size=4)\r\n\r\n#Panel f\r\nresil.comp1 <- aov(Composition_Resilience ~ Treatment, data = NPdata)\r\nhsd.resil.comp <- HSD.test(resil.comp1, ""Treatment"",group = T)\r\nresil.comp <- ddply(NPdata,""Treatment"",summarise,\r\n                    N = length(Composition_Resilience),\r\n                    mean = mean(Composition_Resilience),\r\n                    sd = sd(Composition_Resilience),\r\n                    se = sd/sqrt(N))\r\nresil.comp$Treatment <- factor(resil.comp$Treatment, levels = c(""Control"",""P"",""N"",""N+P""))\r\np5 <- ggplot(resil.comp, aes(x=Treatment, y=mean))+\r\n  geom_bar(stat = ""identity"", fill=""darkgrey"", width = 0.6) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(0, 1)) +\r\n  geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=0.1, size = 0.3)+\r\n  xlab("""") +\r\n  ylab(""Compositional resilience"") +\r\n  labs(tag = ""(f)"")+\r\n  theme_bw()+ \r\n  theme(aspect.ratio = 1, panel.grid.major =element_blank(), panel.grid.minor = element_blank()) +\r\n  theme(axis.title = element_text(size=15), axis.text = element_text(size=12)) +\r\n  annotate(""text"", x = 1:4, y = c(0.5,0.6,0.8,0.82), label=c(""b"", ""b"",""a"",""a""), size=4) \r\n\r\n# Panel h\r\nrcv.comp1 <- aov(Composition_Recovery ~ Treatment, data = NPdata)\r\nhsd.rcv.comp <- HSD.test(rcv.comp1, ""Treatment"",group = T)\r\nrcv.comp <- ddply(NPdata,""Treatment"",summarise,\r\n                  N = length(Composition_Recovery),\r\n                  mean = mean(Composition_Recovery),\r\n                  sd = sd(Composition_Recovery),\r\n                  se = sd/sqrt(N))\r\nrcv.comp$Treatment <- factor(rcv.comp$Treatment, levels = c(""Control"",""P"",""N"",""N+P""))\r\np6 <- ggplot(rcv.comp, aes(x=Treatment, y=mean))+\r\n  geom_bar(stat = ""identity"", fill=""darkgrey"", width = 0.6) +\r\n  scale_y_continuous(expand = c(0,0), limits = c(0, 1)) +\r\n  geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=0.1, size = 0.3)+\r\n  xlab("""") +\r\n  ylab(""Compsitional recovery"") +\r\n  labs(tag = ""(h)"")+\r\n  theme_bw()+ \r\n  theme(aspect.ratio = 1, panel.grid.major =element_blank(), panel.grid.minor = element_blank()) +\r\n  theme(axis.title = element_text(size=15), axis.text = element_text(size=12)) +\r\n  annotate(""text"", x = 1:4, y = c(0.8,0.7,0.6,0.6), label=c(""a"", ""a"",""a"",""a""), size=4) \r\n\r\nplot_grid(p0, p00, p1, p4, p2, p5, p3, p6, ncol = 2, nrow = 4, align = ""v"")\r\n\r\n############### Figure 3 ###############\r\n#Panel a\r\nTS_SEM <- psem(\r\n  lme(Richness ~ N, random = ~ 1|Block, data = NPdata),\r\n  lme(DominantFunction_ts ~ DominantComposition_ts + Richness, random = ~ 1|Block, data = NPdata),\r\n  lme(DominantComposition_ts ~ asynchrony_3yrs + N + P, random = ~ 1|Block, data = NPdata),\r\n  lme(Function_ts ~  asynchrony_3yrs + Composition_ts, random = ~ 1|Block, data = NPdata),\r\n  lme(Composition_ts ~ N + asynchrony_3yrs + DominantComposition_ts, random = ~ 1|Block, data = NPdata)\r\n)\r\n#Panel b\r\nRst_SEM <- psem(\r\n  lme(Richness ~ N, random = ~ 1|Block, data = NPdata),\r\n  lme(DominantFunction_Resistance ~ asynchrony_resistance + P + DominantComposition_Resistance, random = ~ 1|Block, data = NPdata),\r\n  lme(DominantComposition_Resistance ~ N + P + Richness, random = ~ 1|Block, data = NPdata),\r\n  lme(Function_Resistance ~  Richness + asynchrony_resistance + DominantFunction_Resistance + Composition_Resistance, random = ~ 1|Block, data = NPdata),\r\n  lme(Composition_Resistance ~ N + asynchrony_resistance + DominantComposition_Resistance, random = ~ 1|Block, data = NPdata)\r\n)\r\n#Panel c\r\nRsl_SEM <- psem(\r\n  lme(Richness ~ N, random = ~ 1|Block, data = NPdata),\r\n  lme(DominantFunction_Resilience ~ N + P + DominantComposition_Resilience, random = ~ 1|Block, data = NPdata),\r\n  lme(DominantComposition_Resilience ~ N + P + Richness, random = ~ 1|Block, data = NPdata),\r\n  lme(Function_Resilience ~  asynchrony_resilience + DominantFunction_Resilience + Composition_Resilience, random = ~ 1|Block, data = NPdata),\r\n  lme(Composition_Resilience ~ N + asynchrony_resilience + DominantComposition_Resilience, random = ~ 1|Block, data = NPdata)\r\n)\r\n\r\n#Panel d\r\nRcv_SEM <- psem(\r\n  lme(Richness ~ N, random = ~ 1|Block, data = NPdata),\r\n  lme(DominantFunction_Recovery ~ N + Richness + DominantComposition_Recovery, random = ~ 1|Block, data = NPdata),\r\n  lme(DominantComposition_Recovery ~ N, random = ~ 1|Block, data = NPdata),\r\n  lme(Function_Recovery ~  DominantFunction_Recovery + Composition_Recovery, random = ~ 1|Block, data = NPdata),\r\n  lme(Composition_Recovery ~ Richness + asynchrony_recovery + DominantComposition_Recovery, random = ~ 1|Block, data = NPdata)\r\n)\r\n\r\n############### Figure 4 ###############\r\ncbp1 <- c(""#999999"", ""#E69F00"", ""#56B4E9"", ""#009E73"")\r\n#Panel a\r\np0 <- ggplot(NPdata, aes(Composition_ts, Function_ts)) +\r\n  geom_point(aes(color=Treatment), size=3) +\r\n  geom_smooth(method = lm, color= ""Black"") +\r\n  stat_cor(label.x = 0.65, label.y = 1.2) +\r\n  xlab(""Compositional"") + ylab(""Functional"") +\r\n  labs(tag = ""(a)"") +\r\n  scale_color_manual(values = cbp1)+\r\n  theme_bw()+\r\n  theme(aspect.ratio = 1, panel.grid.major =element_blank(), panel.grid.minor = element_blank()) + \r\n  theme(legend.position = c(0.16, 0.8), legend.title = element_blank(), legend.background = element_blank()) +  \r\n  theme(axis.title = element_text(size = 13), plot.title = element_text(size=15)) + \r\n  ggtitle(""Temporal stability"")\r\n\r\n#Panel b\r\np1 <- ggplot(NPdata, aes(Composition_Resistance, Function_Resistance)) +\r\n  geom_point(aes(color=factor(Treatment)), size = 3) +\r\n  geom_smooth(method = lm, color= ""Black"") + \r\n  stat_cor(label.x = 0.3, label.y = -2) +\r\n  xlab(""Compositional"") + ylab(""Functional"") + labs(tag = ""(b)"") +\r\n  scale_color_manual(values = cbp1)+\r\n  theme_bw()+\r\n  theme(aspect.ratio = 1, panel.grid.major =element_blank(), panel.grid.minor = element_blank()) + \r\n  theme(legend.position = ""none"")  +\r\n  theme(axis.title = element_text(size = 13)) +\r\n  ggtitle(""Resistance"") + theme(plot.title = element_text(size=15))\r\n\r\n#Panel c\r\np2 <- ggplot(NPdata, aes(Composition_Resilience, Function_Resilience)) +\r\n  geom_point(aes(color=factor(Treatment)), size=3) + \r\n  geom_smooth(method = lm, color= ""Black"") +  \r\n  stat_cor(label.x = 0.3, label.y = 2) +\r\n  xlab(""Compositional"") + ylab(""Functional"") + labs(tag = ""(c)"") +\r\n  scale_color_manual(values = cbp1)+\r\n  theme_bw()+\r\n  theme(aspect.ratio = 1, panel.grid.major =element_blank(), panel.grid.minor = element_blank()) + \r\n  theme(axis.title = element_text(size = 13)) +\r\n  theme(legend.position = ""none"") +\r\n  ggtitle(""Resilience"") + theme(plot.title = element_text(size=15))\r\n\r\n#Panel d\r\np3 <- ggplot(NPdata, aes(Composition_Recovery, Function_Recovery)) +\r\n  geom_point(aes(color=factor(Treatment)), size=3) + \r\n  geom_smooth(method = lm, color= ""Black"", linetype = ""dashed"") +   \r\n  stat_cor(label.x = 0.25, label.y = 0.6) +\r\n  xlab(""Compositional"") + ylab(""Functional"") + labs(tag = ""(d)"") +\r\n  scale_color_manual(values = cbp1)+\r\n  theme_bw()+\r\n  theme(aspect.ratio = 1, panel.grid.major =element_blank(), panel.grid.minor = element_blank()) + \r\n  theme(axis.title = element_text(size = 13)) +\r\n  theme(legend.position = ""none"") +\r\n  ggtitle(""Recovery"") + theme(plot.title = element_text(size=15))\r\n\r\nplot_grid(p0,p1,p2,p3, ncol = 2, nrow = 2, align=""v"")\r\n\r\n']","Data from: Nitrogen enrichment alters multiple dimensions of grassland functional stability via changing compositional stability Anthropogenic nutrient enrichment is known to alter the composition and functioning of plant communities. However, how nutrient enrichment influences multiple dimensions of community- and ecosystem-level stability remains poorly understood. Using data from a nitrogen (N) and phosphorus (P) addition experiment in a temperate semi-arid grassland that experienced a natural drought, we show that N enrichment, not P enrichment, decreased grassland functional and compositional temporal stability, resistance, and recovery, but increased functional and compositional resilience. Compositional stability and species asynchrony, rather than species diversity, were identified as key determinants of all dimensions of grassland functional stability, except for recovery. Whereas grassland functional recovery was decoupled from compositional recovery, N enrichment altered other dimensions of functional stability primarily through changing their corresponding compositional stability dimensions. Our findings highlight the need to examine ecological stability at the community level for a more mechanistic understanding of ecosystem dynamics in the face of environmental change.",4
Habitat specialization by wildlife reduces pathogen spread in urbanizing landscapes,"Urban areas are expanding globally, with far-reaching ecological consequences, including for wildlife-pathogen interactions. Wildlife show tremendous variation in their responses to urbanization; even within a single population, some individuals can specialize on urban or natural habitat types. This specialization could alter pathogen impacts on host populations via changes to wildlife movement and aggregation. Here, we build a mechanistic model to explore how habitat specialization in urban landscapes affects interactions between a mobile host population and a density-dependent specialist pathogen that confers no immunity. We model movement on a network of resource-stable urban sites and resource-fluctuating natural sites, where hosts are either urban specialists, natural specialists, or generalists that use both patch types. We find that, for generalists, natural and partially urban landscapes produce the highest infection prevalence and mortality, driven by high movement rates at natural sites and high densities at urban sites. However, habitat specialization protects hosts from these negative effects of partially urban landscapes by limiting movement between patch types. These findings suggest that habitat specialization can benefit populations by reducing infectious disease transmission, but by reducing movement between habitat types could also carry the cost of reducing other movement-related ecosystem functions such as seed dispersal and pollination.","['#This script creates functions to model infection dynamics & population sizes\n#in a population of moving animals\n#Claire Teitelbaum\n#claire.teitelbaum@gmail.com\n\n\n#FUNCTION FOR SIMULATION\nmove_infect = function(pars,pops,A,\n                       depart_fun = depart_informed,\n                       n_times = 30){\n  library(data.table)\n  library(dplyr)\n  \n  #inputs:\n  #pars: parameters needed in functions below. Required parameters:\n      #beta_w: contact rate at natural patches\n      #beta_diff: difference in contact rate between urban and natural patches (0=identical)\n      #gamma: recovery rate\n      #v: maximum infection-induced mortality. see text.\n      #mu_0: density-independent mortality rate\n      #mu_1: density-dependent mortality rate\n  #pops: initial populatinon sizes. Must include N_0, p, q, infect_status as columns\n  #A: matrix of resource dynamics. rows are timesteps, columns are sites.\n  #n_times: how many timesteps of output of diff eq to include? 2 will be just beginning and end (and is minimum value)\n  \n  #RUN SIMS\n  \n  #create vector of urban/natural site types in order of sites in pops\n  site_types = list(pops[!duplicated(pops$site),""p""])\n  \n  #initiate matrices to save outputs\n  moves = pops[,-which(colnames(pops)==""N_0"")] #movement in each time step\n  site_dynamics = pops[,-which(colnames(pops)==""N_0"")] #at-site compartment changes (mortality and infection)\n  \n  for(t in 1:pars$tmax){\n    pops$N = pops[,paste(""N"" , t-1 , sep = ""_"")]\n    \n    #extract resource availablity at timestep\n    pops$A = A[t,pops$site]\n    As = A[t,] ; names(As) = paste0(""A_"",1:ncol(A))\n    \n    #recalculate total individuals at site across all infection classes and types\n    #N_i = pops %>% group_by(site) %>% summarize(N = sum(N)) %>% pull(N) #old (equivalent) way using dplyr is slower\n    N_i = as.data.table(pops)\n    N_i = N_i[, .(\n      N = sum(N)\n    ), by = site]\n    N_i = N_i[,N]\n    \n    pops$N_i = N_i[pops$site]\n    \n    #departure\n    M = depart_fun(A = pops$A, N = pops$N, N_i = pops$N_i)\n    \n    \n    pops$M = M\n    pops$N = pops$N - pops$M\n    #sum(pops$N) + sum(pops$M)  #keep track of total population size, make sure it hasn\'t changed\n    \n    #recalculate total individuals at site across all infection classes and types\n    N_i = as.data.table(pops)\n    N_i = N_i[, .(\n      N = sum(N)\n    ), by = site]\n    N_i = N_i[,N]\n    \n    pops$N_i = N_i[pops$site]\n    \n    #destinations\n    M_qij = destination_fun(M = pops$M , A_all = A[t,] , pops = pops)\n    \n    #aggregate destinations per destination site and impose mortality cost of movement\n    dt = as.data.table(M_qij)\n    N_in = dt[, .(\n      site = j,\n      N_in = sum(N)\n    ), by = .(infect_status,q,j)] %>% as.data.frame()\n    \n    pops = left_join(pops , N_in , by = c(""site"", ""q"", ""infect_status"")) %>% mutate(N = N + N_in) %>% dplyr::select(-N_in , -j)\n    #sum(pops$N)\n    \n    #total individuals at site across all infection classes and types\n    N_i = as.data.table(pops)\n    N_i = N_i[, .(\n      N = sum(N)\n    ), by = site]\n    N_i = N_i[,N]\n    \n    pops$N_i = N_i[pops$site]\n    \n    \n    #infection and demography occurs at each site\n    initial_pops = pops$N\n    pops = mutate(pops , full_name = paste(infect_status ,q ,site , sep = ""_""))\n    names(initial_pops) = pops$full_name\n    #run infection function through desolve\n    out = deSolve::ode(y = initial_pops, \n                       times = seq(0,pars$w,length.out=n_times), \n                       func = infect_dem, \n                       parms = c(pars,As,site_types=site_types),\n                       method = ""lsode"", mf = 10)\n    \n    N_new = out[nrow(out),-1]\n    N_new = N_new[names(initial_pops)]\n    id_result = initial_pops - N_new\n    \n    \n    pops$N = N_new\n    #sum(pops$N) #keep track of total population size\n    \n    \n    #save the end population sizes for this timestep\n    pops[,paste(""N"",t,sep=""_"")] = pops$N\n    #save number moving out (for movement rate calculation)\n    moves[,paste(""M"",t,sep=""_"")] = pops$M\n    #save infection and demography output\n    site_dynamics[,paste(""ID"",t,sep=""_"")] = id_result\n    \n  }\n  \n  #convert to matrix for saving (remove non-numeric info)\n  info = select(pops , site, p , q , infect_status) %>% as.matrix() #character matrix giving info for each row in other matrices\n  pops$N_final = pops[,ncol(pops)]\n  pops = select(pops , -N , -A , -M , -N_i , -full_name, -p,-q,-infect_status,-site) %>% \n    as.matrix()\n  \n  moves = select_at(moves, vars(contains(""M_""))) %>% as.matrix()\n  site_dynamics = select_at(site_dynamics , vars(contains(""ID_""))) %>% as.matrix()\n  \n  return(list(pops = pops , moves = moves , site_dynamics = site_dynamics, info = info))\n}\n\n#FUNCTIONS\n#infection and mortality function in continuous time\n#s-i-s\ninfect_dem = function(time , x , parms){\n  \n  class = stringr::str_split_fixed(names(x) , ""_"" , 3)[,1]\n  types = stringr::str_split_fixed(names(x) , ""_"" , 3)[,2]\n  sites = stringr::str_split_fixed(names(x) , ""_"" , 3)[,3]\n  \n  out = NULL\n  for(i in unique(sites)){ #go through each site\n    \n    #generalist = r , urban = u , natural = w\n    S_ri = x[class == ""S"" & types == ""r"" & sites == i]\n    I_ri = x[class == ""I"" & types == ""r"" & sites == i]\n    N_ri = sum(x[types == ""r"" & sites == i])\n    \n    S_wi = x[class == ""S"" & types == ""w"" & sites == i]\n    I_wi = x[class == ""I"" & types == ""w"" & sites == i]\n    N_wi = sum(x[types == ""w"" & sites == i])\n    \n    S_ui = x[class == ""S"" & types == ""u"" & sites == i]\n    I_ui = x[class == ""I"" & types == ""u"" & sites == i]\n    N_ui = sum(x[types == ""u"" & sites == i])\n    \n    I_i = sum(x[class == ""I"" & sites == i])\n    N_i = sum(x[sites == i])\n    A_i = parms[[(paste0(""A_"",i))]]\n    \n    if(""beta_u"" %in% names(parms)){ #assign beta value depending on habitat type. makes it possible to have beta differ across site types.\n      beta = ifelse(parms$site_types[as.numeric(i)]==""u"" , parms$beta_u , parms$beta_w)\n    }\n    \n    out <- with(as.list(parms),{\n      mu_1 = ifelse(N_i < A_i , 0 , mu_1)\n      dSr = (-1)*beta*S_ri*I_i + gamma*I_ri - S_ri*(mu_0 + mu_1*(N_i/A_i)) \n      names(dSr) = paste(""S_r"",i,sep=""_"")\n      dSw = (-1)*beta*S_wi*I_i + gamma*I_wi - S_wi*(mu_0 + mu_1*(N_i/A_i)) \n      names(dSw) = paste(""S_w"",i,sep=""_"")\n      dSu = (-1)*beta*S_ui*I_i + gamma*I_ui - S_ui*(mu_0 + mu_1*(N_i/A_i)) \n      names(dSu) = paste(""S_u"",i,sep=""_"")\n      \n      dIr = beta*S_ri*I_i - gamma*I_ri - v*I_ri*(1-A_i) - I_ri*(mu_0 + mu_1*(N_i/A_i))\n      names(dIr) = paste(""I_r"",i,sep=""_"")\n      dIw = beta*S_wi*I_i - gamma*I_wi - v*I_wi*(1-A_i) - I_wi*(mu_0 + mu_1*(N_i/A_i))\n      names(dIw) = paste(""I_w"",i,sep=""_"")\n      dIu = beta*S_ui*I_i - gamma*I_ui - v*I_ui*(1-A_i) - I_ui*(mu_0 + mu_1*(N_i/A_i))\n      names(dIu) = paste(""I_u"",i,sep=""_"")\n      \n      c(out,dSr,dSw,dSu,dIr,dIw,dIu)\n    })\n    \n  }\n  out = out[names(x)] #reorder output so that it is in the same order as initial conditions\n  names(out) = paste0(""d"",names(out)) #change names\n  list(out)\n}\n\n\n\n#movement (competition rule)\ndepart_informed = function(A , N , N_i){\n  M = ifelse(N_i > A ,  #move if N>carrying capacity\n             N * (1 - (A / N_i)),\n             0\n  )\n  return(M)\n}\n\n\n#destination function: distribute equally to all sites, scaled by phi\ndestination_fun = function(M , A_all ,pops = NULL , p = NULL , q = NULL, i = NULL , class = NULL){\n  \n  #M: populations departing from all sites (M_qi)\n  #f: preference parameter\n  #A_all: A values at current timestep, same order as p\n  \n  #either provide these variables or they will be extracted from pops if it is not provided\n  #q: individual types (n=#of site-class combos, same length as M)\n  #i: site numbers\n  #class: infection status\n  #p: site types (n=#of sites)\n  \n  if(!is.null(pops)){\n    p = pops$p[!duplicated(pops$site)]\n    q = pops$q\n    i = pops$site\n    class = pops$infect_status\n  }\n  \n  \n  n_sites = length(p) - 1\n  n_u = sum(p == ""u"")\n  n_w = sum(p == ""w"")\n  \n  p_mat = matrix(p , nrow = length(q) , ncol = length(p) , byrow = T)\n  q_mat = matrix(rep(q,length(p)) , nrow = length(q) , ncol = length(p))\n  phis = matrix(1/n_sites , nrow = length(q) , ncol = length(p)) #set up matrix: rows are from (i), columns are to (j)\n  phis[q_mat == ""r""] = 1/n_sites #go equally to all other patches if generalist\n  phis[q_mat == ""u"" & p_mat == ""w""] = 0 #apply specialization parameter if specialized\n  phis[q_mat == ""w"" & p_mat == ""u""] = 0 #apply specialization parameter if specialized\n  different_site = !sapply(unique(pops$site) , function(x) pops$site == x) #cannot go back to the site you left from\n  if(n_u==1){ #you can go back to the site you left from if you are a specialist and there in only one site of that type\n    different_site[which(pops$p==""u"" & pops$q==""u""),which(p==""u"")] = 1\n  } else if(n_w==1){\n    different_site[which(pops$p==""w"" & pops$q==""w""),which(p==""w"")] = 1\n  }\n  phis = phis*different_site \n  phis = phis/rowSums(phis) #calculate proportion going to each site\n  #impute zeroes if there are no urban or natural hosts; otherwise these turn into NAs\n  if(n_u==0){\n    phis[q_mat==""u""] = 0\n  }\n  if(n_w==0){\n    phis[q_mat==""w""] = 0\n  }\n  \n  #multiply by number departing to get number going to each site\n  M_ijq = M*phis\n  \n  #transform to long form\n  colnames(M_ijq) = NULL\n  out = cbind.data.frame(M_ijq , i = as.numeric(i) , q = as.character(q) , infect_status = as.character(class) , stringsAsFactors = F) %>%\n    reshape2::melt(id.vars = c(""i"",""q"",""infect_status"") , \n                   measure.vars = 1:length(p) , variable.name = ""j"" , value.name = ""N"") %>%\n    mutate(j = as.numeric(j))\n  \n  \n  return(out)\n  \n}\n', '#This script creates functions to simulate resource dynamics\n#in a population of moving animals\n#Claire Teitelbaum\n#Date created: 29 April 2020\n\n#uniform distribution, random at each point in time\n#bounded by 0 and 1\nsim_A_unif = function(tmax,min=0,max=1){\n  A = runif(tmax , min = min , max = max)\n  return(A)\n}\n\n#normal distribution, random at each point in time\nsim_A_norm = function(tmax,mean=0.5,sd=0.5){\n  A = rnorm(tmax , mean = mean , sd = sd)\n  A[A<0] = 0\n  A[A>1] = 1\n  return(A)\n}\n\n#random walk\n#range is the amount that A can change in a single timestep\nsim_A_rw = function(tmax,range=0.1){\n  A = vector(mod = ""numeric"" , length = tmax)\n  A[1] = runif(1 , 0 , 1)\n  for(i in 2:length(A)){\n    #if A is very close to 1 or 0, implement reflecting boundary\n    if(A[i-1] < range){\n      min = -1*A[i]\n      max = range + A[i]\n    } else if(A[i-1] > (1-range)){\n      max = 1 - A[i-1]\n      min = -1*range - (1 - A[i-1])\n    } else{\n      min = -1*range\n      max = range\n    }\n    A[i] = A[i-1] + runif(1,min = min , max = max)\n    if(A[i]<0) A[i] = 0\n    if(A[i]>1) A[i] = 1\n  }\n  return(A)\n}\n\n#functions for linear interpolation of resource availability \n#to create some autocorrelation at natural sites\n#b is the time frame at which resources fluctuate randomly\n#linear interpolation between these points\nsim_A_linear_interp = function(tmax,b){\n  vals = runif(ceiling(tmax/b)+1 , 0 , 1)\n  A = c(sapply(1:(length(vals)-1) , function(x){\n    seq(vals[x],vals[x+1],length.out = pars$b)\n  }))\n  return(A)\n}', '#This script sets up parameter combinations and runs all simulations\n#Claire Teitelbaum\n#claire.teitelbaum@gmail.com\n\nlibrary(dplyr); library(data.table); library(stringr)\nlibrary(doParallel)\n#load in functions that run model\nsource(""move_infect_fun.R"")\nsource(""resource_sim_fun.R"")\noptions(stringsAsFactors = FALSE)\n\n\n\n#### 1. SET PARAMETERS ####\nn_sims = 20 #number of repeated simulations for each prop_urban\n\n#""baseline"" parameter values: use these when holding values constant\npars_base = cbind(P=10, urbval = 0.5, urbvar = 0.1, \n                  mu_0 = 0.001 , mu_1 = (0.001*2),\n                  beta_w = 0.15, beta_diff = 0,  gamma = 0.03, v = 0.01,\n                  prop_generalist = 1, \n                  w = 7, tmax = 52, prop_infected = 0.01)\npars_base = as.data.frame(pars_base)\n\n#set up parameter combinations to test\nparams_list = list(\n  pars_base,\n  #mortality rates\n  pars_mort = expand.grid(mu_0=c(0, 0.001, 0.002) , mu_1=c(0, 0.001*2, 0.002*2),\n                          beta_w = 0, gamma = 0, v = 0, urbvar = c(0, 0.1)) \n  ,\n  #infection parameters\n  pars_infect = expand.grid(beta_w = c(0, 0.06, 0.15, 0.17), gamma = c(0, pars_base$gamma, 0.06), v = c(0, 0.001, pars_base$v, 0.1),\n                            urbvar = c(0, 0.1)) \n  ,\n  #specialization\n  pars_spec = expand.grid(prop_generalist = seq(0,1,0.1), beta_w = 0, gamma = 0, v = 0, urbvar = c(0, 0.1))\n  ,\n  #specialization with infection and urbval\n  pars_spec_infect = expand.grid(prop_generalist = seq(0,1,0.1), urbval = c(0.3, pars_base$urbval), urbvar = c(0, 0.1))\n  ,\n  #number of patches\n  pars_P = expand.grid(P = c(10,50), \n                       beta_w = c(0, pars_base$beta_w), gamma = c(0, pars_base$gamma), v = c(0, pars_base$v)) \n  ,\n  #urban resource level\n  pars_urbval = expand.grid(urbval = c(0.3,0.5,0.7), \n                            beta_w = c(0, pars_base$beta_w), gamma = c(0, pars_base$gamma), v = c(0, pars_base$v))\n  ,\n  #resource variability at urban patches\n  pars_urbvar = expand.grid(urbvar = c(0,0.01,0.1,0.2,0.5,0.8),\n                            beta_w = c(0, pars_base$beta_w), gamma = c(0, pars_base$gamma), v = c(0, pars_base$v)) \n  ,\n  #beta_diff (difference in beta between urban and natural patches)\n  pars_deltab = expand.grid(beta_diff = c(0, 0.01, 0.06, 0.09), prop_generalist = c(0,0.1,0.5,1)) \n)\n\nparams_df = bind_rows(params_list) \nparams_df = sapply(names(params_df), function(x)  tidyr::replace_na(params_df[,x], pars_base[[x]])) \n\nprop_urban = c(0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0) #proportion of urban and natural patches\n\nprop_urban_add = expand.grid(row = 1:nrow(params_df), prop_urban = prop_urban)\n\nparams_df = cbind(params_df[prop_urban_add$row,], prop_urban = prop_urban_add$prop_urban)\nparams_df = data.frame(params_df)\n\nparams_df$n_sims = ifelse(params_df$prop_urban==1 & params_df$urbvar==0, 1, n_sims)\nparams_df$prop_infected = with(params_df, ifelse(beta_w==0, 0, prop_infected))\nparams_df$beta_u = with(params_df, beta_w + beta_diff)\nparams_df$beta_avg = with(params_df, (beta_w+beta_u)/2)\n\nparams_df = params_df[!duplicated(params_df),]\n\n#simulate resource availability and set initial population sizes for each parameter set\npars_list = lapply(1:nrow(params_df),function(i){\n  out_list = lapply(1:params_df[i,""n_sims""], function(j){\n    #extract parameter values\n    out = as.list(params_df[i,])\n    \n    #simulate resource availability\n    P_u = out$P * out$prop_urban\n    if(P_u>0){\n      A_u = sapply(1:P_u , function(x) sim_A_unif(out$tmax, min=out$urbval-(out$urbvar/2), max=out$urbval+(out$urbvar/2)))\n    } else(A_u = NULL)\n    P_w = out$P - P_u\n    if(P_w>0){\n      A_w = sapply(1:P_w , function(x) sim_A_unif(out$tmax,min=0,max=1))\n    } else(A_w = NULL)\n    #combine all site resource availability\n    #rows are time-steps, columns are sites\n    A_all = cbind(A_w , A_u)\n    \n    #save resource availability to master list\n    out$A_all = A_all\n    out$A_num = j\n    \n    #simulation population starting conditions\n    #back-calculate proportion urban patches\n    p_urb = out$prop_urban\n    #extract which patches are urban\n    urb = c(rep(F, P_w) , rep(T, P_u)) #urban patches are always appended to the end of the series\n    \n    \n    #vector of site types and number of sites of each type\n    site_types = ifelse(urb , ""u"" , ""w"")\n    sites_u = length(which(urb))\n    sites_w = length(which(!urb))\n    \n    #set initial population sizes average carrying capacity (i.e. equal to mean A over time across all patches)\n    #separately for urban and natural sites\n    #assign population sizes by site ID\n    pops = expand.grid(site = 1:length(site_types),\n                       q = c(""w"",""u"",""r"")) %>%\n      mutate(p = site_types[site],\n             N_site = ifelse(p==""u"" , out$urbval , 0.5),\n             N_total = ifelse(q==""r"" , N_site*out$prop_generalist ,\n                              ifelse(p==q , N_site-N_site*out$prop_generalist, 0))) %>%\n      select(-N_site)\n    \n    #seed infection\n    #start infection at each site type depending on j value: first site is natural and last site is urban when both types are present\n    infection_start_loc = ifelse(j <= out$n_sims/2, 1 , out$P)\n    out$infection_start_loc = infection_start_loc\n    \n    p_infect = out$prop_infected\n    #proportion infected at each site of each type with infection\n    N_total = sum(c(rep(0.5, sites_w), rep(out$urbval, sites_u))) #total initial starting population\n    pops = mutate(pops , I = ifelse(site %in% infection_start_loc , N_total*p_infect , 0),\n                  S = N_total-I)\n    \n    #translate to long form\n    pops = select(pops , -N_total) %>%\n      reshape2::melt(id.vars = c(""site"",""q"",""p"") , measure.vars = c(""I"",""S""),\n                     value.name = ""N_0"" , variable.name = ""infect_status"")\n    \n    out$pops = pops\n    \n    return(out)\n  })\n  #save output\n  return(out_list)\n})\n\npars_list = unlist(pars_list, recursive = F)\n\n\n##### 2. RUN SIMULATIONS ####\n#run simulations in parallel\nno_cores = ifelse(detectCores() > 30 , 30 , detectCores()-1) #how many cores to use depends on what computer you\'re using\n\nsimFun = function(x){\n  out = move_infect(pars = x, pops = x$pops, A=x$A_all)\n  gc()\n  return(out)\n}\n\nout_list = vector(mode = ""list"" , length = length(pars_list))\n#run sub-lists of simulations each to save memory (sets of 10 simulations per core)\n#create data frame of indices to separate blocks of simulations\nstart_end = data.frame(start = seq(1,length(pars_list), no_cores*10) ,\n                       end = c(seq(no_cores*10, length(pars_list),no_cores*10), length(pars_list)))\nstartTime = Sys.time()\nfor(i in 1:nrow(start_end)){\n  start = start_end[i,""start""] ; end = start_end[i,""end""]\n  out_list[start:end] <- mclapply(pars_list[start:end] , simFun , mc.cores = no_cores)\n  gc()\n  cat(round(i/nrow(start_end) , 2)*100) ; cat(""%: "") ; cat(Sys.time()-startTime); cat(""\\n"")\n}\n\n\n#option to run not in parallel\n# out_list = vector(mode = ""list"" , length = length(pars_list))\n# startTime = Sys.time()\n# for(i in 1:length(pars_list)){\n#   out_list[[i]] = simFun(pars_list[[i]])\n#   cat(round(i/length(pars_list) , 3)*100) ; cat(""%: "") ; cat(difftime(Sys.time(),startTime,units=""hours"")); cat("" hours \\n"")\n# }\n\n\n\nsaveRDS(list(out_list = out_list, pars = pars_list) , ""sim_outputs/landscape_sim_output.Rds"")\n\n\npars_df = do.call(rbind , pars_list)\npars_df = pars_df[,!colnames(pars_df) %in% c(""A_all"",""A_num"",""pops"")] %>% \n  as.data.frame() %>% mutate_all(unlist)\npars_df$sim_num = ifelse(!duplicated(pars_df), 1, 2)\n\n#### 3. SUMMARIZE SIMULATION RESULTS ####\n#calculate summary prevalence metrics for each sim\n#this is the function to calculate them\ncalc_prev_metrics = function(out, time_final=52){\n  #inputs: \n  #out: data from simulation\n  #time: time step from which to calculate metrics\n  \n  #reorganize data\n  pops = cbind.data.frame(out[[""info""]] , out[[""pops""]])\n  pops = mutate(pops , p = factor(p , levels = c(""u"",""w""))) #set factor levels so that summaries will be calculated even when no urban or natural patches are present\n  N_0 = sum(pops$N_0)\n  S_0 = sum(pops$N_0[pops$infect_status==""S""])\n  \n  pops = select(pops , 1:paste0(""N_"",time_final)) %>% rename(""N_final""=paste0(""N_"",time_final))\n  \n  prev = sum(pops[pops$infect_status == ""I"",""N_final""])/sum(pops[,""N_final""])\n  #difference in prevalence in hosts (natural-urban)\n  hostprevs = pops %>% group_by(q) %>% \n    summarize(prev = sum(N_final[infect_status==""I""])/n())\n  host_diff = hostprevs$prev[hostprevs$q==""w""] - hostprevs$prev[hostprevs$q==""u""] \n  #generalist prevalence\n  gen_prev = hostprevs$prev[hostprevs$q==""r""]\n  \n  #difference in prevalence at sites (natural-urban)\n  siteprevs = pops %>% group_by(p , .drop = F) %>% \n    summarize(prev = sum(N_final[infect_status==""I""])/n()) \n  site_diff = siteprevs$prev[siteprevs$p==""w""] - siteprevs$prev[siteprevs$p==""u""]\n  \n  #number of sites at which prevalence is >0 (with some tolerance)\n  prop_sites_positive = sum(siteprevs$prev>0.01)/length(siteprevs$prev)\n  \n  #varaiation in prevalence across sites\n  end_prev_bysite = pops %>% group_by(site) %>% \n    summarize(prev = sum(N_final[infect_status==""I""])/n()) \n  spat_var = sd(end_prev_bysite$prev)\n  \n  #maximum infection prevalence\n  max_prev = pops %>% select(N_1:N_final) %>% \n    apply(2 , function(x) sum(x[pops$infect_status==""I""])/sum(x)) %>% max()\n  \n  #time of maximum infection prevalence\n  max_prev_time = pops %>% select(N_1:N_final) %>% \n    apply(2 , function(x) sum(x[pops$infect_status==""I""])/sum(x)) %>% which.max()\n  \n  #linear interpolation of spread rate until peak\n  prev_init = sum(pops %>% filter(infect_status==""I"") %>% pull(N_0))/sum(pops %>% pull(N_0))\n  spread_rate = (max_prev-prev_init)/max_prev_time\n  \n  #translate to long form\n  pops_long = select(pops , -N_final) %>%\n    reshape2::melt(id.vars = c(""site"",""p"",""q"",""infect_status""),\n                   variable.name = ""time"",\n                   value.name = ""N"") %>%\n    mutate(time = str_split_fixed(time,""_"",2)[,2] %>% as.numeric,\n           N = as.numeric(N))\n  #time it takes until all sites reach prevalence of 0.1 (starting prevalence of index site)\n  spread_threshold = pops_long %>% group_by(site,time) %>%\n    summarize(prev = sum(N[infect_status==""I""])/sum(N)) %>% \n    group_by(time) %>%\n    summarize(metric=all(prev>0.1)) %>%\n    filter(metric) %>%\n    slice(1) %>% pull(time)\n  spread_threshold = ifelse(length(spread_threshold)==0,NA,spread_threshold)\n  \n  #proportion of time prevalence at urban sites is greater than at wild sites\n  if(any(pops$p==""u"") & any(pops$p==""w"")){\n    u_greater = pops_long %>% group_by(time,p) %>%\n      summarize(prev = sum(N[infect_status==""I""])/sum(N)) %>% \n      group_by(time) %>%\n      summarize(uw = as.numeric(prev[p==""u""]>prev[p==""w""]), .groups = ""drop"") %>%\n      pull(uw) %>% mean()\n  } else{\n    u_greater = NA\n  }\n  \n  \n  #overall movement rate across all sites and time (mean number moving per timestep)\n  move_rate = out[[""moves""]] %>% as.data.frame() %>% select_at(vars(contains(""M_""))) %>% colSums() %>%\n    mean()\n  \n  #overall movement rate across all sites and 1st 8 timesteps\n  move_rate_init = out[[""moves""]] %>% as.data.frame() %>% select(M_1:M_8) %>% colSums() %>%\n    mean()\n  \n  #proportion of individuals moving per timestep\n  move_rates_all = out[[""moves""]][,1:time_final] %>% as.data.frame() %>% select_at(vars(contains(""M_""))) %>% as.matrix() #number moving\n  n_start = pops %>% select_at(vars(contains(""N_""))) %>% select(-ncol(.)) %>% as.matrix() #total number initially\n  n_pre_move = n_start - out[[""site_dynamics""]][,1:time_final] #total number after infection and mortality\n  move_rate_prop = mean(move_rates_all/n_pre_move , na.rm = T) #proportion of those surviving that move\n  \n  #population size\n  end_pop = sum(pops$N_final)\n  \n  #annual survival\n  annual_surv = sum(pops$N_final)/sum(pops$N_0)\n  \n  #difference in survival in hosts (natural-urban)\n  hostsurvs = pops %>% group_by(q) %>% \n    summarize(surv = sum(N_final)/sum(N_0))\n  surv_u = hostsurvs$surv[hostsurvs$q==""u""]\n  surv_w = hostsurvs$surv[hostsurvs$q==""w""]\n  surv_r = hostsurvs$surv[hostsurvs$q==""r""]\n  \n  \n  return(cbind(end_prev = prev , host_diff  = host_diff, site_diff = site_diff, prop_sites_positive , annual_surv,\n               spat_var , gen_prev , end_pop , max_prev , N_0 , S_0 , max_prev_time,\n               spread_rate, spread_threshold , u_greater,\n               move_rate , move_rate_init, move_rate_prop,\n               surv_u, surv_w , surv_r))\n} \n\ngc()\n\n#CALCULATE in parallel\nprev_metrics = mclapply(out_list , function(x) calc_prev_metrics(x,time_final=40), mc.cores = no_cores) \nprev_metrics_20w = mclapply(out_list , function(x) calc_prev_metrics(x,time_final=20), mc.cores = no_cores) \nprev_metrics = do.call(rbind,prev_metrics) \nprev_metrics_20w = do.call(rbind,prev_metrics_20w) \n\n#for infection start loc, also add type of site where it started\npars_df = pars_df %>% mutate(max_w = round(P*(1-prop_urban)) , #need to round to deal with R\'s invisible decimals\n                             infection_w = as.numeric(infection_start_loc)<=max_w,\n                             infection_start_type = ifelse(infection_w , ""w"" , ""u""),\n                             infection_start_type = tidyr::replace_na(infection_start_type,""all"")) %>%\n  select(-max_w,-infection_w)\n\nresults = cbind(pars_df, prev_metrics) \nresults_20w = cbind(pars_df, prev_metrics_20w) \n\n#save results summary\nsaveRDS(results , ""sim_outputs/results_summary.Rds"")\nsaveRDS(results_20w , ""sim_outputs/results_summary_20w.Rds"")\n\n#save subsets of one representative simulation from out_list (for faster download/loading)\nexample_sims = out_list[pars_df$sim_num==1]\nsaveRDS(list(pars = pars_df[pars_df$sim_num==1,] , out = example_sims),\n        ""sim_outputs/example_sims.Rds"")\n']","Habitat specialization by wildlife reduces pathogen spread in urbanizing landscapes Urban areas are expanding globally, with far-reaching ecological consequences, including for wildlife-pathogen interactions. Wildlife show tremendous variation in their responses to urbanization; even within a single population, some individuals can specialize on urban or natural habitat types. This specialization could alter pathogen impacts on host populations via changes to wildlife movement and aggregation. Here, we build a mechanistic model to explore how habitat specialization in urban landscapes affects interactions between a mobile host population and a density-dependent specialist pathogen that confers no immunity. We model movement on a network of resource-stable urban sites and resource-fluctuating natural sites, where hosts are either urban specialists, natural specialists, or generalists that use both patch types. We find that, for generalists, natural and partially urban landscapes produce the highest infection prevalence and mortality, driven by high movement rates at natural sites and high densities at urban sites. However, habitat specialization protects hosts from these negative effects of partially urban landscapes by limiting movement between patch types. These findings suggest that habitat specialization can benefit populations by reducing infectious disease transmission, but by reducing movement between habitat types could also carry the cost of reducing other movement-related ecosystem functions such as seed dispersal and pollination.",4
"Data for: Genomic vulnerability to climate change in Quercus acutissima, a dominant tree species in East Asian deciduous forests","Understanding the evolutionary processes that shape the landscape of genetic variation and influence the response of species to future climate change is critical for biodiversity conservation. Here, we sampled 27 populations across the distribution range of a dominant forest tree, Quercus acutissima, in East Asia, and applied genome-wide analyses to track the evolutionary history and predict the fate of populations under future climate. We found two genetic groups (East and West) in Q. acutissima that diverged during the Pliocene. We also found a heterogeneous landscape of genomic variation in this species, which may have been shaped by population demography and linked selections. Using genotype-environment association analyses, we identified climate-associated SNPs in a diverse set of genes and functional categories, indicating a model of polygenic adaptation in Q. acutissima. We further estimated three genetic offset metrics to quantify genomic vulnerability of this species to climate change due to the complex interplay between local adaptation and migration. We found that marginal populations are under higher risk of local extinction because of future climate change, and may not be able to track suitable habitats to maintain the gene-environment relationships observed under the current climate. We also detected higher reverse genetic offsets in northern China, indicating that genetic variation currently present in the whole range of Q. acutissima may not adapt to future climate conditions in this area. Overall, this study illustrates how evolutionary processes have shaped the landscape of genomic variation, and provides a comprehensive genome-wide view of climate maladaptation in Q. acutissima.","['setwd(""G:/data_analysis/GradientForest20210517/genetic_diversity_rank_correlation"")\r\nlibrary(corrplot)\r\n\r\nrawdata <- read.table(""QaNS_intra_paraeter.txt6"", head=T)\r\nhead(rawdata)\r\n\r\ndata <- subset(rawdata, s12>= 1000 & N12 >= 10 & fst12 != ""NaN"")#& CDS_len != ""NaN""\r\ndim(data)\r\n\r\n\r\nmean(data$sum_rho1)\r\nsd(data$sum_rho1)/42499^0.5\r\n\r\nmean(data$sum_rho2)\r\nsd(data$sum_rho2)/42499^0.5\r\n\r\ndata$Dxy <- data$dxy12/data$s12\r\ndata$pi1 <- data$tP1/data$s1\r\ndata$pi2 <- data$tP2/data$s2\r\n\r\nhead(data)\r\n\r\n\r\nintradata <- data[c(""Dxy"",""fst12"",""pi1"",""pi2"",""sum_rho1"",""sum_rho2"")]\r\n\r\nhead(intradata)\r\ncorr <- cor(intradata)\r\n\r\ncor.mtest <- function(mat, ...) {\r\n  mat <- as.matrix(mat)\r\n  n <- ncol(mat)\r\n  p.mat<- matrix(NA, n, n)\r\n  diag(p.mat) <- 0\r\n  for (i in 1:(n - 1)) {\r\n    for (j in (i + 1):n) {\r\n      tmp <- cor.test(mat[, i], mat[, j], ...)\r\n      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value\r\n    }\r\n  }\r\n  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)\r\n  p.mat\r\n}\r\np.mat <- cor.mtest(mtcars)\r\n\r\n\r\npdf(""inter_intrapop_data_correlation_20211104.pdf"")\r\ncol<- colorRampPalette(c(""#0000FF"", ""white"", ""#FF0000""))(20)\r\ncorrplot(corr, method = ""circle"", type=""lower"",col=col, diag=F, p.mat = p.mat, sig.level = 0.01, addCoef.col = ""black"", title = """")\r\ndev.off()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n', 'library(LEA)\n###\n\n###\nfor (k in c(1:200)){\nvcf2lfmm(paste(""Qa"",k,""lfmm"",""vcf"", sep="".""))}\n\n\nfor (i in c(1:200)){\n\nproject2 = NULL\nproject2 = lfmm(paste(""Qa"",i,""lfmm"",""lfmm"", sep="".""),""pc1.load.env"",K = 2,repetitions = 5,project = ""new"")\n###\np = lfmm.pvalues(project2, K = 2)\npvalues = p$pvalues\nprint (pvalues)   ### total snp ,each snp correspond a pvalue\npdf(paste(""K2"",""hist"",i,""pdf"", sep="".""))\nhist(pvalues, col = ""lightblue"")\ndev.off()\nwrite.table(x=pvalues,file=paste(""K2pc1"",i,""pvalues"",sep="".""),quote=F, sep=""\\t"", row.names=F)\n###\nlibrary(qvalue)\nfor (alpha in c(0.01, 0.05)) {\n# expected FDR\nprint(paste(""Expected FDR:"", alpha))\nqval <- qvalue(pvalues)$qvalue\noutliers <- which(qval < alpha)\nwrite.table(x=outliers,file=paste(""K2pc1"",alpha,""outlier"",i,""txt"",sep="".""),quote=F, sep=""\\t"", row.names=F)\n}\n}', '\r\nsetwd(""F:/data_analysis/GradientForest20210517/environment_m5/14GDM_model_each_env/env1_outlier"")\r\n\r\nrequire(raster)\r\nrequire(gdm)\r\nrequire(foreach)\r\nrequire(parallel)\r\nrequire(doParallel)\r\nrequire(fields)\r\n\r\n\r\n\r\nrequire(geosphere)\r\n#function to remove intercept from gdm predictions\r\nremoveIntercept <- function(mod,pred){\r\n  adjust <- 0 - log(1-pred) - mod$intercept\r\n  adjustDissim <- 1-exp(0-adjust)\r\n  return(adjustDissim)\r\n}\r\n\r\n##############\r\n#Load and prep FST data\r\n##############\r\nadptMat <- read.csv(""Qa_22pop_lfmm_each_env_total.csv"")\r\n\r\n#Read in population locations\r\npops <- read.csv(""Qa_22pop_info.csv"")\r\n\r\n#make sure the two are in the same order\r\nall(adptMat$pop == pops$code)\r\n\r\n##############\r\n#Load and prep shapefile and climate data\r\n##############\r\n#load shapefile\r\nshp <- shapefile(""RasterT_Int_Rec1.shp"")\r\n\r\n#choose predictors\r\npredNames <- c(""bio1"",""bio2"",""bio3"",""bio4"",""bio8"",""bio13"",""bio15"",""bio19"")\r\n\r\nraster1 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10current_climate/bio1.tif"")\r\nraster2 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10current_climate/bio2.tif"")\r\nraster3 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10current_climate/bio3.tif"")\r\nraster4 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10current_climate/bio4.tif"")\r\nraster5 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10current_climate/bio8.tif"")\r\nraster6 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10current_climate/bio13.tif"")\r\nraster7 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10current_climate/bio15.tif"")\r\nraster8 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10current_climate/bio19.tif"")\r\n\r\npresClim <- stack(raster1,raster2,raster3,raster4,raster5,raster6,raster7,raster8)\r\npresClim <- presClim[[predNames]]\r\n\r\n#Creates pred data for gdm (cols = population name, long, lat, climate data)\r\npred <- data.frame(pop=pops$code,long=pops$long, lat=pops$lat, extract(presClim, y=pops[,c(""long"",""lat"")]),\r\n                   stringsAsFactors=FALSE)\r\n\r\n######################\r\n#GDM model\r\n######################\r\n#Create site pair table\r\nsitePair <- formatsitepair(bioDat = adptMat, bioFormat=3, siteColumn=""pop"", XColumn=""long"",YColumn=""lat"",predData=pred)\r\n\r\n#Create and plot gdm\r\nmod <- gdm(na.omit(sitePair), geo=FALSE)\r\n\r\n#load future climate data\r\nraster1 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10future_cilmate_RCP85/bio1.tif"")\r\nraster2 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10future_cilmate_RCP85/bio2.tif"")\r\nraster3 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10future_cilmate_RCP85/bio3.tif"")\r\nraster4 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10future_cilmate_RCP85/bio4.tif"")\r\nraster5 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10future_cilmate_RCP85/bio8.tif"")\r\nraster6 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10future_cilmate_RCP85/bio13.tif"")\r\nraster7 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10future_cilmate_RCP85/bio15.tif"")\r\nraster8 <- raster(""F:/data_analysis/GradientForest20210517/environment_m5/06GDM_model_PC1_BF25/10future_cilmate_RCP85/bio19.tif"")\r\n\r\nfutClims <- stack(raster1,raster2,raster3,raster4,raster5,raster6,raster7,raster8)\r\nfutClims <- futClims[[predNames]]\r\nfutClimDat <- as.data.frame(futClims, xy=TRUE, na.rm=TRUE)\r\n\r\n#Getting all coordinates in range map\r\npopDat <- na.omit(as.data.frame(mask(presClim, shp), xy=TRUE))\r\npopDat <- data.frame(distance=1, weight=1, popDat)\r\npopDat <- split(popDat, seq(nrow(popDat)))\r\n\r\n\r\n###############\r\n#Forward offset calculation\r\n##############\r\ncl <- makeCluster(4) #ideally should be run in parallel to reduce computing time\r\nregisterDoParallel(cl)\r\nforwardOffsetGDM <- foreach(i = 1:length(popDat), .packages=c(""fields"",""gdm"",""geosphere"")) %dopar%{\r\n  \r\n  #get the focal population\r\n  onePop <- popDat[[i]]\r\n  \r\n  #set up a dataframe where the first site is the focal population, and the second population\r\n  #are sites across North America\r\n  setUp <- cbind(onePop,futClimDat)\r\n  colnames(setUp) <- c(""distance"",""weights"",\r\n                       ""s1.xCoord"", ""s1.yCoord"",paste(""s1."", predNames, sep=""""), \r\n                       ""s2.xCoord"", ""s2.yCoord"",paste(""s2."", predNames, sep=""""))\r\n  \r\n  #rearrange the colums for the gdm prediction\r\n  dat <- setUp[,c(""distance"",""weights"",""s1.xCoord"", ""s1.yCoord"",""s2.xCoord"", ""s2.yCoord"",\r\n                  paste(""s1."", predNames, sep=""""), \r\n                  paste(""s2."", predNames, sep=""""))]\r\n  \r\n  #do the prediction and set up a dataframe with second sites x/y and predicted Fst\r\n  combinedDat <- predict(object=mod, dat, time=FALSE)\r\n  combinedDat <- data.frame(dat[,c(""s2.xCoord"",""s2.yCoord"")], predFst=removeIntercept(mod, combinedDat))\r\n  \r\n  ##Get metrics for the focal population\r\n  #coordinate of focal population\r\n  coord <- onePop[,c(""x"",""y"")]\r\n  \r\n  #choose the pixels with the minimum fst\r\n  minCoords <- combinedDat[which(combinedDat$predFst == min(combinedDat$predFst)),]\r\n  \r\n  #calculate the distance to the sites with minimum fst, and selct the one with the shortest distance\r\n  minCoords[""dists""] <- distGeo(p1=coord, p2=minCoords[,1:2])\r\n  minCoords <- minCoords[which(minCoords$dists == min(minCoords$dists)),]\r\n  \r\n  #if multiple sites have the same fst, and same distance, one is randomly chosen\r\n  minCoords <- minCoords[sample(1:nrow(minCoords),1),]\r\n  \r\n  #get local offset\r\n  offset <- combinedDat[which(combinedDat$s2.xCoord == coord$x & combinedDat$s2.yCoord == coord$y),""predFst""]\r\n  \r\n  #get the minimum predicted fst - forward offset in this case\r\n  minVal <- minCoords$predFst\r\n  \r\n  #get distance and coordinates of site that minimizes fst\r\n  toGo <- minCoords$dists\r\n  minPt <- minCoords[,c(""s2.xCoord"", ""s2.yCoord"")]\r\n  \r\n  #get bearing to the site that minimizes fst\r\n  bear <- bearing(coord, minPt)\r\n  \r\n  #write out\r\n  out <- c(x1=coord[[1]], y1=coord[[2]],local=offset,forwardFst=minVal, predDist=toGo, bearing=bear,x2=minPt[[1]],y2=minPt[[2]])\r\n  \r\n}\r\n\r\nstopCluster(cl)\r\n\r\n#in this resultant dataframe the columns are:\r\n#x1/y1: focal coordinates\r\n#local: local offset\r\n#forwardFst: forward offset\r\n#predDist: distance to site of forward offset\r\n#bearing: bearing to site of forward offset\r\n#x2/y2: coordinate of site of forward offset\r\nforwardOffsetGDM <- do.call(rbind, forwardOffsetGDM)\r\n\r\nwrite.csv(forwardOffsetGDM,paste0(""./Qa_each_env_lfmm_total_rcp85_forwardOffsetGDM.csv""), row.names=FALSE)\r\n\r\n\r\n###############\r\n#Reverse offset calculation\r\n##############\r\n#Getting all coordinates in the range in current climate\r\npopDat <- na.omit(as.data.frame(mask(presClim, shp), xy=TRUE))\r\npopDat <- data.frame(popDat)\r\n\r\n#Gets climate data from the range in future climate\r\nfutClimMask <- mask(x=futClims, mask=shp)\r\nfutClimDat <- as.data.frame(futClimMask, xy=TRUE, na.rm=TRUE)\r\n\r\n#set up for prediction\r\nfutClimDat <- data.frame(distance=1, weight=1, futClimDat)\r\n\r\n###############\r\n#Reverse offset calculation\r\n##############\r\ncl <- makeCluster(4)\r\nregisterDoParallel(cl)\r\nreverseOffsetGDM <- foreach(i = 1:nrow(futClimDat), .packages=c(""fields"",""gdm"",""geosphere"")) %dopar%{\r\n  \r\n  #get the focal population\r\n  onePop <- futClimDat[i,]\r\n  \r\n  #set up a dataframe where the first site is the focal population, and the second population\r\n  #are sites across the range\r\n  setUp <- cbind(onePop,popDat)\r\n  colnames(setUp) <- c(""distance"",""weights"",\r\n                       ""s1.xCoord"", ""s1.yCoord"",paste(""s1."", predNames, sep=""""), \r\n                       ""s2.xCoord"", ""s2.yCoord"",paste(""s2."", predNames, sep=""""))\r\n  \r\n  #rearrange the colums for the gdm prediction\r\n  dat <- setUp[,c(""distance"",""weights"",""s1.xCoord"", ""s1.yCoord"",""s2.xCoord"", ""s2.yCoord"",\r\n                  paste(""s1."", predNames, sep=""""), \r\n                  paste(""s2."", predNames, sep=""""))]\r\n  \r\n  #do the prediction and set up a dataframe with second sites x/y and predicted Fst\r\n  combinedDat <- predict(object=mod, dat, time=FALSE)\r\n  combinedDat <- data.frame(dat[,c(""s2.xCoord"",""s2.yCoord"")], predFst=removeIntercept(mod, combinedDat))\r\n  \r\n  ##Get metrics for the focal population\r\n  #coordinate of focal population\r\n  coord <- onePop[,c(""x"",""y"")]\r\n  \r\n  #choose the pixels with the minimum fst\r\n  minCoords <- combinedDat[which(combinedDat$predFst == min(combinedDat$predFst)),]\r\n  \r\n  #calculate the distance to the sites with minimum fst, and selct the one with the shortest distance\r\n  minCoords[""dists""] <- distGeo(p1=coord, p2=minCoords[,1:2])\r\n  minCoords <- minCoords[which(minCoords$dists == min(minCoords$dists)),]\r\n  \r\n  #if multiple sites have the same fst, and same distance, one is randomly chosen\r\n  minCoords <- minCoords[sample(1:nrow(minCoords),1),]\r\n  \r\n  #get local offset\r\n  offset <- combinedDat[which(combinedDat$s2.xCoord == coord$x & combinedDat$s2.yCoord == coord$y),""predFst""]\r\n  \r\n  #get the minimum predicted fst - reverse offset in this case\r\n  minVal <- minCoords$predFst\r\n  \r\n  #get distance and coordinates of site that minimizes fst\r\n  toGo <- minCoords$dists\r\n  minPt <- minCoords[,c(""s2.xCoord"", ""s2.yCoord"")]\r\n  \r\n  #get bearing to the site that minimizes fst\r\n  bear <- bearing(coord, minPt)\r\n  \r\n  #write out\r\n  out <- c(x1=coord[[1]], y1=coord[[2]],local=offset,reverseFst=minVal, predDist=toGo, bearing=bear,x2=minPt[[1]],y2=minPt[[2]])\r\n  \r\n}\r\n\r\nstopCluster(cl)\r\n\r\n#in this resultant dataframe the columns are:\r\n#x1/y1: focal coordinates\r\n#local: local offset - included as sanity check - should be identical to \'offset\' from the calculation of forward offset above\r\n#reverseFst: reverse offset\r\n#predDist: distance to site of reverse offset\r\n#bearing: bearing to site of reverse offset\r\n#x2/y2: coordinate of site of reverse offset\r\nreverseOffsetGDM <- do.call(rbind, reverseOffsetGDM)\r\n\r\nwrite.csv(reverseOffsetGDM,paste0(""./Qa_each_env_lfmm_total_rcp85_reverseOffsetGDM.csv""), row.names=FALSE)\r\n']","Data for: Genomic vulnerability to climate change in Quercus acutissima, a dominant tree species in East Asian deciduous forests Understanding the evolutionary processes that shape the landscape of genetic variation and influence the response of species to future climate change is critical for biodiversity conservation. Here, we sampled 27 populations across the distribution range of a dominant forest tree, Quercus acutissima, in East Asia, and applied genome-wide analyses to track the evolutionary history and predict the fate of populations under future climate. We found two genetic groups (East and West) in Q. acutissima that diverged during the Pliocene. We also found a heterogeneous landscape of genomic variation in this species, which may have been shaped by population demography and linked selections. Using genotype-environment association analyses, we identified climate-associated SNPs in a diverse set of genes and functional categories, indicating a model of polygenic adaptation in Q. acutissima. We further estimated three genetic offset metrics to quantify genomic vulnerability of this species to climate change due to the complex interplay between local adaptation and migration. We found that marginal populations are under higher risk of local extinction because of future climate change, and may not be able to track suitable habitats to maintain the gene-environment relationships observed under the current climate. We also detected higher reverse genetic offsets in northern China, indicating that genetic variation currently present in the whole range of Q. acutissima may not adapt to future climate conditions in this area. Overall, this study illustrates how evolutionary processes have shaped the landscape of genomic variation, and provides a comprehensive genome-wide view of climate maladaptation in Q. acutissima.",4
Data from: A well-studied parasitoid fly of field crickets uses multiple alternative hosts in its introduced range,"Organisms and their natural enemies can have dynamic coevolutionary trajectories, but anthropogenic effects like species introductions interrupt existing coevolutionary relationships. For parasites in particular, if they are introduced to a location without their hosts, they can only persist in the new environment if alternative hosts are 1) present, 2) detectable to parasites, and 3) capable of sustaining parasites. The circumstances surrounding the addition of alternative hosts to a parasite's repertoire are rarely observed. The parasitoid fly Ormia ochracea locates its field cricket hosts by orienting acoustically to their conspicuous mating songs. In Hawaii, O. ochracea is only known to parasitize one species, Teleogryllus oceanicus, but rapid evolution of T. oceanicus mating song over the past 20 years has led to several prevalent morphs of the cricket that produce no song or novel songs that the flies cannot detect. Yet flies persist in populations that lack ancestral singing T. oceanicus, prompting us to investigate the possibility of alternative hosts in Hawaii. We demonstrate first that three potential alternative hosts (Gryllodes sigillatus, Gryllus bimaculatus, and Modicogryllus pacificus) are present. Second, O. ochracea exhibits a positive phonotactic response to all three species' songs in the field and in the lab. And third, O. ochracea can successfully develop to pupae and emerge as adults in all three species. Our discovery of alternative hosts for O. ochracea in Hawaii infuses the system with intriguing complexity and offers extensive opportunities for future work.","['## packages\nlibrary(emmeans)\nlibrary(car)\nlibrary(logistf)\nlibrary(tidyverse)\n\n#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n## loading in data\n#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n## butterfly trap phonotaxis data from Kalaupapa\nbutterfly_kalaupapa <- read.csv(""butterfly_kalaupapa_data.csv"")\n\n## fly trapping data from Kalaupapa\ntrapping_kalaupapa <- read.csv(""trapping_kalaupapa_data.csv"")\n\n## butterfly trap phonotaxis data from Wailua summer 2022\nbutterfly_wailua <- read.csv(""butterfly_wailua_data.csv"")\n\n## infestation data from Wailua\ninfestation_wailua <- read.csv(""infestation_wailua_data.csv"")\n\n## ## fly trapping data from Wailua\ntrapping_wailua <- read.csv(""trapping_wailua_data.csv"")\n\n## converting columns to factors\nbutterfly_kalaupapa$song_type <- as.factor(butterfly_kalaupapa$song_type)\ntrapping_kalaupapa$song_type <- as.factor(trapping_kalaupapa$song_type)\ninfestation_wailua$Species <- as.factor(infestation_wailua$Species)\ntrapping_wailua$song_type <- as.factor(trapping_wailua$song_type)\nbutterfly_wailua$song_type <- as.factor(butterfly_wailua$song_type)\njanuary_butterfly_both_pops$song_type_2 <- as.factor(january_butterfly_both_pops$song_type)\njanuary_butterfly_both_pops$population <- as.factor(january_butterfly_both_pops$population)\nwailua_january$song_type_2 <-as.factor(wailua_january$song_type_2)\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n# Field fly trapping models\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n## glm of Wailua flies caught by songtype, includes all stimuli--------------------------------------------------------------------------------------------------------\nglm_traps_wailua <- glm(flies_count ~ song_type, family = poisson, data = trapping_wailua)\n\nsummary(glm_traps_wailua)\n\nAnova(glm_traps_wailua, type = ""II"", test.statistic = ""LR"")\n\npairs(emmeans(glm_traps_wailua, ~ song_type))\n\n## removing purring/toceaincus---------------------------------------------------------------------------------------------------------------------------------------------------------\nwailua_traps_no_oceanicus <- trapping_wailua %>% filter(!song_type %in% c(""purrs""))\n\n## glm of Wailua flies caught by songtype, excluding purring/toceanicus--------------------------------------------------------------------------------------------------------\nwailua_traps_no_oceanicus_glm <- glm(flies_count ~ song_type,\n                                     family = poisson, data = wailua_traps_no_oceanicus)\n\nsummary(wailua_traps_no_oceanicus_glm)\n\nAnova(wailua_traps_no_oceanicus_glm, type = ""II"", test.statistic = ""LR"")\n\npairs(emmeans(wailua_traps_no_oceanicus_glm, ~ song_type))\n\n## glm of Kalaupapa flies caught by songtype, includes all stimuli----------------------------------------------------------------------------------------------------------------------------------------------------------------\nglm_traps_kalaupapa <- glm(flies ~ song_type, family = poisson, data = trapping_kalaupapa)\n\nsummary(glm_traps_kalaupapa)\n\nAnova(glm_traps_kalaupapa, type = ""II"", test.statistic = ""LR"")\n\npairs(emmeans(glm_traps_kalaupapa, ~ song_type))\n\n## removing purring--------------------------------------------------------------------------------------------------------\nkalaupapa_traps_no_purring <- trapping_kalaupapa %>% filter(!song_type %in% c(""purring""))\n\n## glm of Kalaupapa flies caught by songtype, excluding purring/toceanicus--------------------------------------------------------------------------------------------------------\nglm_traps_kalaupapa_no_purring <- glm(flies ~ song_type, family = poisson, data = kalaupapa_traps_no_purring)\n\npairs(emmeans(glm_traps_kalaupapa_no_purring, ~ song_type))\n\nsummary(glm_traps_kalaupapa_no_purring)\n\nAnova(glm_traps_kalaupapa_no_purring, type = ""II"", test.statistic = ""LR"")\n\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n# Laboratory Phonotaxis models\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n## glm of contact by songtype for Wailua flies\nwailua_contact_glm <- glm(contact ~ song_type,\n                            family = binomial, na.action = na.omit, data = butterfly_wailua)\n\nsummary(wailua_contact_glmer)\n\nAnova(wailua_contact_glmer, type = ""II"", test.statistic = ""LR"")\n\npairs(emmeans(wailua_contact_glmer, ~song_type))\n\n# glm of contact by songtype for Kalaupapa flies\nkalaupapa_contact_glm <- glm(contact ~ song_type,\n                                family = binomial, na.action = na.omit, data = butterfly_kalaupapa)\n\nsummary(kalaupapa_contact_glm)\n\nAnova(kalaupapa_contact_glm, type = ""II"", test.statistic = ""LR"")\n\npairs(emmeans(kalaupapa_contact_glm, ~ song_type))\n\n\n## logistf Firth\'s penalized regression of contact by songtype for Wailua flies, all songtypes\n\n## relevel to silence\nbutterfly_wailua$song_type<-relevel(butterfly_wailua$song_type,""silence"")\n\nwailua_contact_logistf <- logistf(formula = contact ~ song_type, data = butterfly_wailua)\n\nsummary(wailua_contact_logistf)\n\n\n## glm contact by songtype for Wailua flies, silence removed\ncontact_wailua_nosilence_orWN_dat <- butterfly_wailua %>% filter(!song_type %in% c(""silence""))\n\ncontact_glmer_no_wn <- glm(contact ~ song_type,\n                           family = binomial, na.action = na.omit, data = contact_wailua_nosilence_orWN_dat)\n\npairs(emmeans(contact_glmer_no_wn, ~song_type))\n\n\n\n### logistf Firth\'s penalized regression of contact by songtype for Kalaupapa flies, all songtypes\nbutterfly_kalaupapa$song_type<-relevel(butterfly_kalaupapa$song_type,""silent"")\n\nkalaupapa_contact_logistf <- logistf(contact ~ song_type, data = butterfly_kalaupapa)\n\nsummary(kalaupapa_contact_logistf)\n\n## glm contact by songtype for Kalaupapa flies, silence removed\ncontact_kalaupapa_nosilence <- butterfly_kalaupapa %>% filter(!song_type %in% c(""silent""))\n\nkalaupapa_contact_glmer_no_silence <- glm(contact ~ song_type,\n                                          family = binomial, na.action = na.omit, data = contact_kalaupapa_nosilence)\n\nAnova(kalaupapa_contact_glmer_no_silence, type = ""II"", test.statistic = ""LR"")\n\n\npairs(emmeans(kalaupapa_contact_glmer_no_silence, ~ song_type))\n\n\n## lm of distance by songtype for Wailua flies\nlm_distance_wailua <- lm(distance ~ song_type, data = butterfly_wailua)\n\nAnova(lm_distance_wailua, type = ""II"")\n\npairs(emmeans(lm_distance_wailua, ~ song_type))\n\n\n## lm of distance by songtype for Kalaupapa flies\nlm_distance_kalaupapa <- lm(distance ~ song_type, data = butterfly_kalaupapa)\n\nAnova(lm_distance_kalaupapa, type = ""II"")\n\npairs(emmeans(lm_distance_kalaupapa, ~ song_type))\n\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n# Artificial infestation models\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n## logistf Firth\'s penalized regression of pupae count by host species\n\n## relevel to t oceanicus\ninfestation_wailua$Species<-relevel(infestation_wailua$Species,""toceanicus"")\n\ninfest_logistf <- logistf(pupae_yn ~ Species, data = infestation_wailua, pl = TRUE)\n\nsummary(infest_logistf)\n\n\n# glm of pupae count by host species with t. oceanicus removed\ninfestation_pupae_no_toceanicus <- infestation_wailua %>% filter(!Species %in% c(""toceanicus""))\n\ninfestation_pupae_no_toceanicus$Species <- as.factor(infestation_pupae_no_toceanicus$Species)\n\nglm_infestation_pupae_no_oceanicus <- glm(pupae_yn ~ Species,\n                                          family = poisson, data = infestation_pupae_no_toceanicus)\n\nAnova(glm_infestation_pupae_no_oceanicus, type = ""II"", test.statistic = ""LR"")\n\npairs(emmeans(glm_infestation_pupae_no_oceanicus, ~ Species))\n\n\n## logistf Firth\'s penalized regression of pupae that developed into adults by host species\ninfestation_adults$Species<-relevel(infestation_adults$Species,""gryllodes"")\n\ninfest_logistf_adults <- logistf(adult_fly_yn ~ Species, data = infestation_adults, pl = TRUE)\n\nsummary(infest_logistf_adults)']","Data from: A well-studied parasitoid fly of field crickets uses multiple alternative hosts in its introduced range Organisms and their natural enemies can have dynamic coevolutionary trajectories, but anthropogenic effects like species introductions interrupt existing coevolutionary relationships. For parasites in particular, if they are introduced to a location without their hosts, they can only persist in the new environment if alternative hosts are 1) present, 2) detectable to parasites, and 3) capable of sustaining parasites. The circumstances surrounding the addition of alternative hosts to a parasite's repertoire are rarely observed. The parasitoid fly Ormia ochracea locates its field cricket hosts by orienting acoustically to their conspicuous mating songs. In Hawaii, O. ochracea is only known to parasitize one species, Teleogryllus oceanicus, but rapid evolution of T. oceanicus mating song over the past 20 years has led to several prevalent morphs of the cricket that produce no song or novel songs that the flies cannot detect. Yet flies persist in populations that lack ancestral singing T. oceanicus, prompting us to investigate the possibility of alternative hosts in Hawaii. We demonstrate first that three potential alternative hosts (Gryllodes sigillatus, Gryllus bimaculatus, and Modicogryllus pacificus) are present. Second, O. ochracea exhibits a positive phonotactic response to all three species' songs in the field and in the lab. And third, O. ochracea can successfully develop to pupae and emerge as adults in all three species. Our discovery of alternative hosts for O. ochracea in Hawaii infuses the system with intriguing complexity and offers extensive opportunities for future work.",4
Dispersal increases spatial synchrony of populations but has weak effects on population variability: a meta-analysis,"The effects of dispersal on spatial synchrony and population variability have been well documented in theoretical research, and a growing number of empirical tests have been performed. Yet a synthesis is still lacking. Here, we conducted a meta-analysis of relevant experiments and examined how dispersal affected spatial synchrony and temporal population variability across scales. Our analyses showed that dispersal generally promoted spatial synchrony, and such effects increased with dispersal rate and decreased with environmental correlation among patches. The synchronizing effect of dispersal, however, was only detected when spatial synchrony was measured using the correlation-based index, but not for the covariance-based index. In contrast to theoretical predictions, the effect of dispersal on local population variability was generally non-significant, except when environment correlation among patch was negative and/or experimental period was long. At the regional scale, while low dispersal stabilized metapopulation dynamics, high dispersal led to destabilization. Overall, the sign and strength of dispersal effects on spatial synchrony and population variability were modulated by taxa, environmental heterogeneity, type of perturbations, patch number, and experimental length. Our synthesis demonstrates that dispersal can substantially affect the dynamics of spatially distributed populations, but its effects are context dependent on abiotic and biotic factors.","['\r\nlibrary(cowplot)\r\nlibrary(ggplot2)\r\nlibrary(lemon)\r\nlibrary(ggpubr)\r\nlibrary(ggsci)\r\nlibrary(metafor)\r\nlibrary(multcomp)\r\nlibrary(extrafont)\r\n\r\n##set the theme for plot\r\ntheme_set(theme_cowplot())\r\n\r\n########################################################################################################################\r\n##set work path\r\nsetwd(""~"")\r\n\r\n# Ensure the following data files are in the working directory to enable the code to run smoothly\r\n\r\n##synchrony.csv-101 individual effect sizes and covariates information for dispersal-synchrony relationship \r\n##local.csv-91 individual effect sizes and covariates information for dispersal-local-variability relationship \r\n##regional.csv-75 individual effect sizes and covariates information for dispersal-regional-variability relationship \r\n##All.csv-Estimated mean effect sizes for different categories of discrete covariates\r\n##synchrony_index.csv: 4*45 individual effect sizes for different synchrony measure and effect size indices\r\n##local_index.csv 2*45 individual effect sizes for different local variability measure \r\n\r\n# Explanation of key variables & data  ####################################################################################\r\n\r\n##sample information ######################################################################################################\r\n# \r\n# Article_ID:Unique identifier for article\r\n# Factor.Level:Treatment level of manipulated variable(s) besides dispersal rate\r\n# Sample.Level: A unique numeric ID number for each different combination within multi-factorial experiment design (e.g. dispersal*PH)\r\n# Dispersal.Level: Treatment level of dispersal within each factor level\r\n# Treat.rate: Dispersal rate in treatment group\r\n# Treat. mean/var/n:synchrony/local/regional variability value, corresponding variance across replicates and number of replicates for each treatment group\r\n# Con. mean/var/n:synchrony/local/regional variability value, corresponding variance across replicates and number of replicates for each control group\r\n#As We mentioned in manuscript that multiple treatment groups could share a common control\r\n\r\n##moderators(see Table S2 for detailed definition & categorization) ########################################################\r\n \r\n# taxa: terrestrial animals(Animal_T), aquatic animals(Animal_A), aquatic bacteria (Bacteria_A), aquatic plant (Plant_A), and aquatic protist (Protist_A)\r\n# diversity: single-species, single-trophic, multi-trophic\r\n# EH(environmental heterogeneity): HOM: totally homogeneous; SPA: only spatially heterogeneity\r\n# TEM0: uncorrelated temporal pulse; TEM-: asynchronous temporal pulse; TEM+: synchronous temporal pulse\r\n# Patch_num: number of patches for each experimental unit. 1: two patches;0 : >2 patches\r\n# perturbations; A: aboitic perturbation; B: boitic perturbation; N: no perturbation\r\n# time:short, 2.5 - 45 generations; long,120 - 1000 generations.\r\n\r\n##synchrony index (see Statistical analysis for detailed definition & categorization)########################################\r\n# \r\n# D.r:  raw mean difference in the mean correlation coefficient of population growth rates\r\n# D.x:  raw mean difference in the mean correlation coefficient of population sizes\r\n# D.phi:  raw mean difference in the covariance-based index\r\n# LRR.phi: log response ratio for the covariance-based index\r\n\r\n##Local variability index (see Figure S3 legend for detailed definition & categorization)#####################################\r\n# \r\n# CV.m: local variability measured by the simple arithmetic mean of coefficient of CV across patches\r\n# CV.w:local variability measured by an average of CV weighted by the population biomass of each patch\r\n\r\n##individual effect sizes and corresponding variance-covariance\r\n\r\n# yi: individual effect size (raw mean different for synchrony; log response ratio for variability)\r\n# vi: variance for effect szie\r\n# v1-v.....: the variance-covariance matrix constructed to account for the non-independence of sharing a common control\r\n\r\n# read effect sizes and co-variates ####################################################################################\r\n\r\nsynchrony=read.csv(""synchrony.csv"")\r\nlocal=read.csv(""local.csv"")\r\nregional=read.csv(""regional.csv"")\r\n\r\n\r\n########################################################################################################################\r\n## main text#############################################################################################\r\n########################################################################################################################\r\n\r\n##random-effect only model for overall effect\r\n\r\n##synchrony \r\nsyn.all=rma.mv(yi,synchrony[,c(23:123)],random=~1|Article_ID,data=synchrony)#synchrony[,c(22:122)] the variance-covariance matrix\r\nsummary(syn.all)\r\n\r\n##local \r\nloc.all=rma.mv(yi,local[,c(23:113)],random=~1|Article_ID,data=local)\r\nsummary(loc.all)\r\n\r\n##regional \r\nreg.all=rma.mv(yi,regional[,c(23:97)],random=~1|Article_ID,data=regional)\r\nsummary(reg.all)\r\n\r\n##Figure 1 Forest plot for overall effect ##########################################################################################\r\n\r\n##order by effect size values\r\nsynchrony=synchrony[with(synchrony, order(yi)),]\r\nsynchrony$es_order=rev(c(1:101))\r\nsynchrony$lb=synchrony$yi-1.96*sqrt(synchrony$vi)\r\nsynchrony$ub=synchrony$yi+1.96*sqrt(synchrony$vi)\r\n\r\nlwd=.pt*72.27/96##convert line size unit from R pixels to points\r\n\r\nSyn_all=ggplot(synchrony,aes(x=yi,y=es_order))+\r\n  geom_errorbar( aes( xmin=lb, xmax=ub), width=0,color=""gray"",size=1/lwd)+\r\n  geom_point(color = \'black\',size=3)+\r\n  geom_point(aes(x=syn.all$b, y=-3), shape=9, size=3,show.legend = FALSE)+\r\n  geom_errorbar( aes( xmin=syn.all$ci.lb, xmax=syn.all$ci.ub,y=-3), width=0,color=""gray"",size=0.2)+\r\n  geom_vline(xintercept = 0,linetype=""longdash"")+\r\n  geom_vline(xintercept = syn.all$b,linetype=""dotted"")+\r\n  scale_x_continuous(limits = c(-2,2))+\r\n  scale_y_continuous(limits = c(-7,103),expand =expansion(add = c(0,0)))+\r\n  coord_capped_cart(bottom=\'both\')+\r\n  xlab(""D"")+\r\n  theme(\r\n    axis.title.y =element_blank(),\r\n    axis.line.y = element_blank(),\r\n    axis.ticks.y=element_blank(),\r\n    axis.text.x =element_text(size=10),\r\n    axis.title = element_text(size=11),\r\n    axis.text.y = element_blank())\r\n\r\n\r\nlocal=local[with(local, order(yi)),]\r\nlocal$es_order=rev(c(1:91))\r\nlocal$lb=local$yi-1.96*sqrt(local$vi)\r\nlocal$ub=local$yi+1.96*sqrt(local$vi)\r\n\r\nLoc_all=ggplot(local,aes(x=yi,y=es_order))+\r\n  geom_errorbar( aes( xmin=lb, xmax=ub), width=0,color=""gray"",size=1/lwd)+\r\n  geom_point(color = \'black\',size=3)+\r\n  geom_point(aes(x=loc.all$b, y=-3), shape=9, size=3,show.legend = FALSE)+\r\n  geom_errorbar( aes(xmin=loc.all$ci.lb, xmax=loc.all$ci.ub,y=-3), width=0,color=""gray"",size=0.2)+\r\n  geom_vline(xintercept = 0,linetype=""longdash"")+\r\n  geom_vline(xintercept = loc.all$b,linetype=""dotted"")+\r\n  scale_x_continuous(limits = c(-4,2))+\r\n  scale_y_continuous(limits = c(-6,93),expand =expansion(add = c(0,0)))+\r\n  coord_capped_cart(bottom=\'both\')+\r\n  xlab(""LRR"")+\r\n  theme(\r\n    axis.title.y =element_blank(),\r\n    axis.line.y = element_blank(),\r\n    axis.ticks.y=element_blank(),\r\n    axis.text.y = element_blank(),\r\n    axis.text.x =element_text(size=10),\r\n    axis.title = element_text(size=11,hjust=0.68))\r\n\r\nregional=regional[with(regional, order(yi)),]\r\nregional$es_order=rev(c(1:75))\r\nregional$lb=regional$yi-1.96*sqrt(regional$vi)\r\nregional$ub=regional$yi+1.96*sqrt(regional$vi)\r\n\r\nReg_all=ggplot(regional,aes(x=yi,y=es_order))+\r\n  geom_errorbar( aes( xmin=lb, xmax=ub), width=0,color=""gray"",size=1/lwd)+\r\n  geom_point(color = \'black\',size=3)+\r\n  geom_point(aes(x=reg.all$b, y=-3), shape=9, size=3,show.legend = FALSE)+\r\n  geom_errorbar( aes( xmin=reg.all$ci.lb, xmax=reg.all$ci.ub,y=-3), width=0,color=""gray"",size=0.2)+\r\n  geom_vline(xintercept = 0,linetype=""longdash"")+\r\n  geom_vline(xintercept =reg.all$b,linetype=""dotted"")+\r\n  scale_x_continuous(limits = c(-2,2))+\r\n  scale_y_continuous(limits = c(-5.5,77),expand =expansion(add = c(0,0)))+\r\n  coord_capped_cart(bottom=\'both\')+\r\n  xlab(""LRR"")+\r\n  \r\n  theme(\r\n    axis.title.y =element_blank(),\r\n    axis.line.y = element_blank(),\r\n    axis.ticks.y=element_blank(),\r\n    axis.text.x =element_text(size=10),\r\n    axis.title = element_text(size=11),\r\n    axis.text.y = element_blank())\r\n\r\n\r\n#pdf(""Figure 1.pdf"",width = 10, height =4, useDingbats=FALSE,bg = ""white"", colormodel = ""cmyk"") \r\nggarrange(Syn_all,Loc_all,Reg_all,labels=c(""A"",""B"",""C""),ncol=3,nrow=1,font.label = list(size=12))\r\n#dev.off()\r\n#embedFonts(""Figure 1.pdf"")\r\n\r\n##mixed-effects meta-regresson: Dispersal rate effect ######################################################################################################\r\n\r\n#For spatial synchrony\r\nsynchrony=synchrony[with(synchrony, order(Article_ID,Factor.Level,Sample.Level)),]\r\nsyn.rate=rma.mv(yi,synchrony[,c(23:123)],mods=~Treat.rate,random = ~1|Article_ID, data=synchrony)\r\nsummary(syn.rate)\r\n\r\n##For local variability\r\nlocal=local[with(local, order(Article_ID,Factor.Level,Sample.Level)),]\r\nloc.rate.poly=rma.mv(yi,local[,c(23:113)],mods=~poly(Treat.rate, degree=2, raw=TRUE),random = ~1|Article_ID, data=local)\r\nloc.rate.linear=rma.mv(yi,local[,c(23:113)],mods=~Treat.rate,random = ~1|Article_ID, data=local)\r\nsummary(loc.rate.poly)#AICc: 427.1453   \r\nsummary(loc.rate.linear)#AICc: 452.2753\r\n\r\n##For regional variability\r\nregional=regional[with(regional, order(Article_ID,Factor.Level,Sample.Level)),]\r\nreg.rate.linear=rma.mv(yi,regional[,c(23:97)],mods=~Treat.rate,random = ~1|Article_ID, data=regional)\r\nreg.rate.poly=rma.mv(yi,regional[,c(23:97)],mods=~poly(Treat.rate, degree=2, raw=TRUE),random = ~1|Article_ID, data=regional)\r\nsummary(reg.rate.poly)#AICc:  295.8079   \r\nsummary(reg.rate.linear)# AICc: 293.7956  \r\n\r\n\r\n##model fit diagnostics:  marginal R2, AICc, and I2\r\nmodels <- list(syn.all,loc.all,reg.all,syn.rate,loc.rate.linear,loc.rate.poly,reg.rate.linear,reg.rate.poly)\r\nmarg.r2<- c()##for dispersal rate\r\naicc <- c()\r\nI2 <- c()\r\n\r\nfor (i in 1:length(models)) {\r\n  \r\n  # marginal and conditional R2 for linear mixed-models \r\n  # variance from fixed effects\r\n    x= model.matrix(models[[i]])\r\n  fe.total = x %*% models[[i]]$b\r\n  v.fix <- var(fe.total)\r\n  # variance from random effects\r\n  v.rand <- sum(models[[i]]$sigma2)\r\n  # variance from residuals\r\n  v.resid <- var(residuals(models[[i]]))\r\n  # marginal R2 - total variance explained from fixed effects\r\n  marg.r2[i] <- v.fix / (v.fix + v.rand + v.resid)\r\n  \r\n    # I2 statistic - amount of heterogeneity relative to the total amount of variance in observed effects \r\n  W <- solve(models[[i]]$V)\r\n  X <- model.matrix(models[[i]])\r\n  P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W\r\n  I2[i] <- 100 * sum(models[[i]]$sigma2) / (sum(models[[i]]$sigma2) + (models[[i]]$k - models[[i]]$p) / sum(diag(P)))\r\n  aicc[i] <- fitstats(models[[i]])[5]\r\n}\r\n\r\n##Figure 2 dispersal rate~LRR scatter plot\r\n\r\n##only keep samples with dispersal rate information\r\nsynchrony.2=synchrony[!is.na(synchrony$Treat.rate),]\r\nsynchrony.2$weight=diag(weights(syn.rate, type=""matrix""))[!is.na(diag(weights(syn.rate, type=""matrix"")))]\r\n\r\n##predicted values\r\ny=predict(syn.rate,newmods=seq(0.001,0.3,0.001))\r\ny.syn=data.frame(raw=(seq(0.001,0.3,0.001)),pre=y$pred,lb=y$ci.lb,ub=y$ci.ub) \r\nsyn.raw=ggplot(synchrony.2,aes(x=Treat.rate)) +\r\n  geom_hline(yintercept = 0,linetype=""dashed"",color=""gray"",size=1/lwd)+\r\n  geom_point(aes( y =yi,size=weight),shape=1,alpha=0.7,show.legend = FALSE) +\r\n  scale_size(range=c(2.5,5))+\r\n  geom_ribbon(data=y.syn,aes(x=raw,ymin = lb, ymax = ub), fill = ""grey70"",alpha=0.6) + \r\n  geom_line(data=y.syn,aes(x=raw,y=pre),size=2/lwd)+\r\n  ylab(""D"")+xlab(""Dispersal Rate"")+\r\n  scale_x_continuous(limits=c(0,0.3),breaks=c(0.001,0.1,0.2,0.3),labels=c(""0.001"",""0.1"",""0.2"",""0.3""),expand=expansion(add = c(0.01,0.01)) )+\r\n  scale_y_continuous(limits=c(-1,2),breaks=c(-1,0,1,2),expand=expansion(add = c(0,0)))+\r\n  theme(\r\n    panel.border = element_rect(color=""black""),\r\n    axis.text =element_text(size=10),\r\n    axis.title = element_text(size=11)\r\n    \r\n  )\r\n\r\nlocal.2=local[!is.na(local$Treat.rate),]\r\nlocal.2$weight=diag(weights(loc.rate.poly, type=""matrix""))[!is.na(diag(weights(loc.rate.poly, type=""matrix"")))]\r\n\r\ny=predict(loc.rate.poly, newmods=unname(poly(seq(0.01,0.3,0.001), degree=2, raw=TRUE)))\r\ny.loc=data.frame(raw=(seq(0.01,0.3,0.001)),pre=y$pred,lb=y$ci.lb,ub=y$ci.ub)\r\nloc.raw=ggplot(local.2,aes(x=Treat.rate)) +\r\n  geom_hline(yintercept = 0,linetype=""dashed"",color=""gray"",size=1/lwd)+\r\n  geom_point(aes( y =yi,size=weight),shape=1,alpha=0.7,show.legend = FALSE) +\r\n  scale_size(range=c(2.5,5))+\r\n  geom_ribbon(data=y.loc,aes(x=raw,ymin = lb, ymax = ub), fill = ""grey70"",alpha=0.6) + \r\n  geom_line(data=y.loc,aes(x=raw,y=pre),size=2/lwd)+\r\n  ylab(""LRR"")+xlab(""Dispersal Rate"")+\r\n  scale_x_continuous(limits=c(0,0.3),breaks=c(0.01,0.1,0.2,0.3),labels=c(""0.01"",""0.1"",""0.2"",""0.3""),expand=expansion(add = c(0,0.01)) )+\r\n  scale_y_continuous(limits=c(-1.05,1),expand=expansion(add = c(0,0)))+\r\n  theme(\r\n    panel.border = element_rect(color=""black""),\r\n    axis.text =element_text(size=10),\r\n    axis.title = element_text(size=11)\r\n  )\r\n\r\n\r\nregional.2=regional[!is.na(regional$Treat.rate),]\r\nregional.2$weight=diag(weights(reg.rate.linear, type=""matrix""))[!is.na(diag(weights(reg.rate.linear, type=""matrix"")))]\r\ny=predict(reg.rate.linear,newmods=seq(0.01,0.3,0.001))\r\ny.reg=data.frame(raw=(seq(0.01,0.3,0.001)),pre=y$pred,lb=y$ci.lb,ub=y$ci.ub) \r\n\r\nreg.raw=ggplot(regional.2,aes(x=Treat.rate)) +\r\n  geom_hline(yintercept = 0,linetype=""dashed"",color=""gray"",size=1/lwd)+\r\n  geom_point(aes( y =yi,size=weight),shape=1,alpha=0.7\r\n             ,show.legend = FALSE) +\r\n  scale_size(range=c(2.5,5))+\r\n  geom_ribbon(data=y.reg,aes(x=raw,ymin = lb, ymax = ub), fill = ""grey70"",alpha=0.3) + \r\n  geom_line(data=y.reg,aes(x=raw,y=pre),size=2/lwd)+\r\n  ylab(""LRR"")+xlab(""Dispersal Rate"")+\r\n  scale_x_continuous(limits=c(0,0.3),breaks=c(0.01,0.1,0.2,0.3),labels=c(""0.01"",""0.1"",""0.2"",""0.3""),expand=expansion(add = c(0,0.01)) )+\r\n  scale_y_continuous(limits=c(-1,1),expand=expansion(add = c(0,0)))+\r\n  theme(\r\n    panel.border = element_rect(color=""black""),\r\n    legend.position = c(0,0.91),\r\n    axis.text =element_text(size=10),\r\n    axis.title = element_text(size=11)\r\n  )\r\n\r\n# pdf(""Figure 2.pdf"",width = 12, height =4, useDingbats=FALSE,bg = ""white"", colormodel = ""cmyk"") \r\ngarrange(syn.raw,loc.raw,reg.raw,labels=c(""A"",""B"",""C""),ncol=3,nrow=1,font.label = list(size=12))\r\n# dev.off()\r\n# embedFonts(""Figure 2.pdf"")\r\n\r\n##Figure 3 Covariates effect ######################################################################################################\r\n\r\n##estimated mean effect sizes for categorical variables and overall effect\r\n\r\n#for vategorical variable, mean effect size for each category was acquired from:\r\n#rma.mv(yi,variance-covariance-matrix,mods=~covariate-1,random = ~1|Article_ID, data=data)\r\n#between-group heterogeneity (Qm) was estimated by:\r\n#rma.mv(yi,variance-covariance-matrix,mods=~covariate,random = ~1|Article_ID, data=data)\r\n\r\n##Full model and best model\r\n\r\n#Full model including all covariates except  experimental length was fitted as :\r\n#rma.mv(yi,variance-covariance-matrix,mods=~all covariate,random = ~1|Article_ID, data=data)\r\n#Best model with lowest AICc selected by:\r\n#glmulti(yi~Treat.rate+relevel(homo,ref=""-2"").....,level=1,data=data,crit=""aicc"",confsetsize = 64)\r\n\r\nES=read.csv(""All.csv"")\r\n\r\nES$Variable[ES$Variable==""Environmental Heterogeneity""]=""Environmental\\n Heterogeneity""\r\nES$Variable[ES$Variable==""Experimental Length""]=""Experimental\\n Length"" \r\n\r\n##forest plot-categorical variables\r\n\r\n##labeling significance\r\n\r\nES$Sig.syn[ES$pval.syn>0.1]="" ""\r\nES$Sig.syn[ES$pval.syn<0.1&ES$pval.syn>0.05]=""\\U00B7""\r\nES$Sig.syn[ES$pval.syn<0.05&ES$pval.syn>0.01]=""*""\r\nES$Sig.syn[ES$pval.syn<0.01&ES$pval.syn>0.001]=""**""\r\nES$Sig.syn[ES$pval.syn<0.001]=""***""\r\n\r\nES$Sig.loc[ES$pval.loc>0.1]="" ""\r\nES$Sig.loc[ES$pval.loc<0.1&ES$pval.loc>0.05]=""\\U00B7""\r\nES$Sig.loc[ES$pval.loc<0.05&ES$pval.loc>0.01]=""*""\r\nES$Sig.loc[ES$pval.loc<0.01&ES$pval.loc>0.001]=""**""\r\nES$Sig.loc[ES$pval.loc<0.001]=""***""\r\n\r\nES$Sig.reg[ES$pval.reg>0.1]="" ""\r\nES$Sig.reg[ES$pval.reg<0.1&ES$pval.reg>0.05]=""\\U00B7""\r\nES$Sig.reg[ES$pval.reg<0.05&ES$pval.reg>0.01]=""*""\r\nES$Sig.reg[ES$pval.reg<0.01&ES$pval.reg>0.001]=""**""\r\nES$Sig.reg[ES$pval.reg<0.001]=""***""\r\n\r\n##labeling sample size\r\n\r\nES$lab.syn=paste(""("",ES$N.syn,"")"",sep="""")\r\nES$lab.loc=paste(""("",ES$N.loc,"")"",sep="""")\r\nES$lab.reg=paste(""("",ES$N.reg,"")"",sep="""")\r\n\r\n\r\n##coding y for plot\r\n\r\nES$group=ES$Variable\r\nES$group[duplicated(ES$group)]="" ""\r\n\r\nES$pos[c(21)]=1\r\nES$h[c(21)]=-1\r\nfor(i in 1:20){\r\n  ES$pos[21-i]=ifelse(ES$Variable[21-i]==ES$Variable[22-i],ES$pos[22-i]+1,ES$pos[22-i]+2)\r\n  ES$h[21-i]=ifelse(ES$Variable[21-i]==ES$Variable[22-i],-1,mean(ES$pos[c(21-i,22-i)]))\r\n  \r\n}\r\n\r\nES$h2[ES$group=="" ""]=-1\r\nES$h2[is.na(ES$h2)]=c(25,20,15,10,6.5,3.5,1)\r\n\r\n\r\n##left part of forest plot-covariates and categories\r\n\r\nALL.var=ggplot(data=ES,aes(y =pos ))+\r\n  \r\n  geom_text(aes(label = Cat,x=8), hjust=0.5,vjust = 0.5,size=12/.pt) +\r\n  geom_text(aes(y=h2,x=3,label =group), vjust = 0.5,hjust=0.6,size=13/.pt,lineheight=1)+\r\n  geom_hline(aes(yintercept =h),color=""gray"",size=1.1/lwd) +\r\n  \r\n  scale_x_continuous(limits = c(-1,11),expand =expansion(add = c(0,0)))+\r\n  scale_y_continuous(limits=c(0,28),expand =expansion(add = c(0,0)))+\r\n  theme(axis.title.y = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        axis.line.y = element_blank(),\r\n        axis.ticks.y = element_blank(),\r\n        axis.ticks.x = element_blank(),\r\n        axis.title.x = element_blank(),\r\n        axis.text.x = element_blank(),\r\n        plot.margin = margin(5, -5, 5, 5, ""pt""))\r\n\r\n##synchrony\r\n\r\nALL.syn=ggplot(data=ES,aes(y =pos,x =b.syn ))+\r\n  \r\n  geom_errorbar(aes(xmin=ci.lb.syn, xmax=ci.ub.syn,y=pos ),width=0,cex=3,col=""gray55"",alpha=0.3)+ \r\n  geom_point(aes(x=b.syn,y=pos ),cex=3,shape=19)+\r\n  geom_text(aes(x=ci.ub.syn+0.25,y=pos ,label=Sig.syn),size=12/.pt,hjust=1,vjust=0.7)+\r\n  geom_text(aes(x=ci.ub.syn+0.25,y=pos ,label=lab.syn),size=11/.pt,hjust=0,vjust=0.3)+\r\n  geom_text(aes(x=2,y=pos ,label=ano.syn),size=11/.pt,hjust=0,vjust=0.3)+\r\n  geom_hline(aes(yintercept =h),color=""gray"",size=1.1/lwd) +\r\n  geom_segment(x=0,y=0,xend=0,yend=28,linetype=""longdash"",col=""gray60"",size=1.3/lwd)+\r\n  \r\n  scale_x_continuous(limits = c(-0.6,2.1),breaks=c(-0.5,0,0.5,1,1.5,2),expand =expansion(add = c(0,0.1)))+\r\n  scale_y_continuous(limits=c(0,28),expand =expansion(add = c(0,0)))+\r\n  xlab(""Synchrony"")+\r\n  \r\n  theme(axis.title.y = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        axis.line.y = element_blank(),\r\n        axis.ticks.y = element_blank(),\r\n        axis.text.x =element_text(size=11),\r\n        axis.title = element_text(hjust=0.15,size=12),\r\n        plot.margin = margin(5, 5, 5, -5, ""pt""))\r\n\r\nAll.loc=ggplot(data=ES,aes(y =pos,x =b.loc ))+\r\n  \r\n  geom_errorbar(aes(xmin=ci.lb.loc, xmax=ci.ub.loc,y=pos),col=""red"",width=0,cex=3,alpha=0.3)+ \r\n  geom_point(cex=3,shape=19,col=""red"")+\r\n  geom_text(aes(x=ci.ub.loc+0.2,label=Sig.loc),col=""red"",size=12/.pt,hjust=1,vjust=0.7)+\r\n  geom_text(aes(x=ci.ub.loc+0.2,label=lab.loc),col=""red"",size=11/.pt,hjust=0,vjust=0.3)+ \r\n  geom_text(aes(x=1.2,label=ano.loc),col=""red"",size=11/.pt,hjust=0,vjust=0.3)+ \r\n  geom_hline(aes(yintercept =h),color=""gray"",size=1.1/lwd) +\r\n  geom_segment(x=0,y=0,xend=0,yend=28,linetype=""longdash"",col=""gray60"",size=1.3/lwd)+\r\n  \r\n  scale_x_continuous(limits = c(-0.7,1.4),expand =expansion(add = c(0,0)))+\r\n  scale_y_continuous(limits=c(0,28),expand =expansion(add = c(0,0)))+\r\n  \r\n  xlab(""Local Variability"")+\r\n  theme(axis.title.y = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        axis.ticks.y = element_blank(),\r\n        axis.line.y = element_blank(),\r\n        axis.text.x =element_text(size=11),\r\n        axis.title = element_text(hjust=0.25,size=12),\r\n        plot.margin = margin(5, 5, 5, 5, ""pt""))\r\n\r\nAll.reg=ggplot(data=ES,aes(y =pos,x=b.reg))+\r\n  \r\n  geom_errorbar(aes(xmin=ci.lb.reg, xmax=ci.ub.reg,y=pos),col=""blue4"",width=0,cex=4,alpha=0.3)+ \r\n  geom_point(cex=3,shape=19,col=""blue4"")+\r\n  geom_text(aes(x=ci.ub.reg+0.2,label=Sig.reg),size=12/.pt,hjust=1,vjust=0.7,col=""blue4"")+\r\n  geom_text(aes(x=ci.ub.reg+0.2,label=lab.reg),size=11/.pt,hjust=0,vjust=0.3,col=""blue4"")+ \r\n  geom_text(aes(x=1.85,y=pos+0.25,label=ano.reg),size=11/.pt,col=""blue4"",hjust=0.5,vjust=0.3)+\r\n  geom_hline(aes(yintercept =h),color=""gray"",size=1.2/lwd) +\r\n  geom_segment(x=0,y=0,xend=0,yend=28,linetype=""longdash"",col=""gray60"",size=1.3/lwd)+\r\n  scale_x_continuous(limits = c(-0.7,2),expand =expansion(add = c(0,0.1)))+\r\n  scale_y_continuous(limits=c(0,28),expand =expansion(add = c(0,0)))+\r\n  xlab(""Regional Variability"")+\r\n  \r\n  theme(axis.title.y = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        axis.ticks.y = element_blank(),\r\n        axis.line.y = element_blank(),\r\n        axis.text.x =element_text(size=11),\r\n        axis.title = element_text(hjust=0.12,size=12),\r\n        plot.margin = margin(5, 5, 5, 5, ""pt""))\r\n\r\n# pdf(""Figure 3.pdf"",width = 12, height =7.2, useDingbats=FALSE,bg = ""white"", colormodel = ""cmyk"") \r\n plot_grid(ALL.var,ALL.syn,All.loc,All.reg,nrow = 1, \r\n           rel_widths = c(1.4,1,1,1.1),align = ""h"")\r\n# dev.off()\r\n# embedFonts(""Figure 3.pdf"")\r\n\r\n##Figure 4 #######################################################################################################################\r\n\r\n##individual effect sizes based on different synchrony measure and effect size metrics\r\n## 4*45 effect sizes calculated from raw data (8 studies)\r\n\r\nsynchrony.index=read.csv(""synchrony_index.csv"")\r\n#for mean effect size\r\nsyn.ind=rma.mv(yi,synchrony.index[,c(11:190)],mods=~Metrics-1,random =~ 1|Article_ID, data=synchrony.index)\r\n#for between-group heterogeneity (Qm)\r\nsummary(rma.mv(yi,synchrony.index[,c(11:190)],mods=~Metrics,random =~ 1|Article_ID, data=synchrony.index))\r\n#for pairwise comparison\r\nsummary(glht(syn.ind, linfct=cbind(contrMat(c(""D.phi""=1,""D.r""=1,""D.x""=1,\r\n                                              ""LRR.phi""=1), type=""Tukey""))))\r\n\r\ndat.syn=data.frame(b=syn.ind$b,lb=syn.ind$ci.lb,ub=syn.ind$ci.ub,\r\n                  cat=unique(synchrony.index$Metrics)[c(3,2,1,4)],ano=c(""c"",""b"",""a"",""c""),\r\n                  p=aggregate(.~Metrics,data=synchrony.index[,c(8,9)], FUN= max)[,2])\r\n\r\nsyn.comp=ggplot()+\r\n  geom_violin(synchrony.index,mapping=aes(Metrics,yi,color=Metrics))+ ## violin plot\r\n  geom_jitter(synchrony.index,mapping=aes(Metrics,yi,color=Metrics),width=0.2,size=2,alpha=0.2,show.legend = FALSE)+ ## jitter plot for raw data\r\n  scale_color_manual(values=c(""#008B45FF"" ,""#00A087FF"", ""#3C5488FF"",""#4DBBD5FF""))+ ## set the color\r\n  geom_linerange(dat.syn,mapping=aes(x=cat,ymin=lb,ymax=ub))+ ## plot the error bar\r\n  geom_point(dat.syn,mapping=aes(cat,b,color=cat),size=3)+ ## plot the mean\r\n  geom_text(dat.syn,mapping=aes(x=cat,y=p+0.1,label=ano),vjust=0,size=12/.pt)+ ## plot the text\r\n  geom_text(aes(x=dat.syn$cat,y=-.9,label=c(""p = 0.52"",""p = 0.04"",""p < 0.01"",""p = 0.53"")),size=12/.pt,vjust=0)+ ## plot the text\r\n  scale_y_continuous(limits = c(-0.9,1.6),breaks = c(-0.5,0,0.5,1,1.5))+ ## set the limitation of y and its break\r\n  scale_x_discrete(limits=c( ""D.r"" ,""D.x"", ""D.phi"",""LRR.phi"" ))+\r\n  geom_hline(yintercept=0,col=""gray50"",linetype=""longdash"")+\r\n  ylab(""Effect Size"")+\r\n  theme(legend.position = ""none""\r\n        ,panel.border = element_rect(color=""black"")\r\n        ,axis.title.x = element_blank()\r\n        ,axis.title.y = element_text(size=12)\r\n        ,axis.text.x = element_text(size=12,vjust=-2)\r\n        ,axis.ticks.length.y = unit(-0.15, \'cm\')\r\n        ,axis.ticks.length.x = unit(-0.15, \'cm\')\r\n        ,axis.text.y = element_text(size=11) ,\r\n         plot.margin = margin(0.5, 0.5, 1, 1, ""cm""))#\r\n\r\n# pdf(""Figure 4.pdf"",width =6, height =4, useDingbats=FALSE,bg = ""white"", colormodel = ""cmyk"") \r\n syn.comp\r\n# dev.off()\r\n# embedFonts(""Figure 4.pdf"")\r\n \r\n########################################################################################################################\r\n## supplemental information#############################################################################################\r\n########################################################################################################################\r\n\r\n## modified egger\'s test:residuals of random-effects-only model ~ inverse se ###########################################\r\n\r\n##synchrony\r\nsynchrony=synchrony[with(synchrony, order(Article_ID,Factor.Level,Sample.Level)),]\r\nsynchrony$resi.f<-residuals.rma(syn.all)\r\nsynchrony$inv<-1/sqrt(synchrony$vi)\r\nsummary(rma.mv(resi.f~inv,vi,data=synchrony,random=~1|Article_ID))\r\n\r\n##local\r\nlocal=local[with(local, order(Article_ID,Factor.Level,Sample.Level)),]\r\nlocal$resi.f<-residuals.rma(loc.all)\r\nlocal$inv<-1/sqrt(local$vi)\r\nsummary(rma.mv(resi.f~inv,vi,data=local,random=~1|Article_ID))\r\n\r\n##regional\r\nregional=regional[with(regional, order(Article_ID,Factor.Level,Sample.Level)),]\r\nregional$resi.f<-residuals.rma(reg.all)\r\nregional$inv<-1/sqrt(regional$vi)\r\nsummary(rma.mv(resi.f~inv,vi,data=regional,random=~1|Article_ID))\r\n\r\n##Figure S2 funnel plot#####################################################################################################\r\n\r\n#pdf(""Figure S2.pdf"",width = 10, height =4, useDingbats=FALSE,bg = ""white"", colormodel = ""cmyk"") \r\n\r\npar(font.main=1, font.lab=1,\r\n    cex.main=1.3, cex.lab=1,mfrow=c(1,3),mar=c(4,4,2,1),mgp=c(1.6,0.6,0),adj=0.5\r\n)\r\n\r\n\r\nfunnel(rma.mv(resi.f~inv,vi,data=synchrony,random=~1|Article_ID),\r\n       xlab=""Residual Value"", back=""white"", yaxis=""seinv"",\r\n       level=c(90, 95, 99), shade=c(""white"", ""gray45"", ""gray75""),\r\n       refline=0, legend=FALSE, hlines=""white"",xlim=c(-3.5,3.5)\r\n       \r\n)\r\nmtext(side=3,adj=-0.16,line=-1,""A"",cex=1)\r\ntext(-3.6,3.849,labels=""Egger\'s p = 0.496"",adj=c(0,0))\r\n\r\n##local\r\nfunnel(rma.mv(resi.f~inv,vi,data=local,random=~1|Article_ID),\r\n       yaxis=""seinv"",xlab=""Residual Value"", back=""white"", \r\n       level=c(90, 95, 99), shade=c(""white"", ""gray45"", ""gray75""),\r\n       refline=0, legend=FALSE, hlines=""white"",xlim=c(-4,4)\r\n)\r\nmtext(side=3,adj=-0.16,line=-1,""B"",cex=1)\r\ntext(-4,4.479,labels=""Egger\'s p = 0.129"",adj=c(0,0))\r\n\r\n##regional\r\nfunnel(rma.mv(resi.f~inv,vi,data=regional,random=~1|Article_ID),\r\n       yaxis=""seinv"",xlab=""Residual Value"", back=""white"", \r\n       level=c(90, 95, 99), shade=c(""white"", ""gray45"", ""gray75""),\r\n       refline=0, legend=FALSE, hlines=""white"",xlim=c(-3,3))\r\nmtext(side=3,adj=-0.16,line=-1,""C"",cex=1)\r\ntext(-3,5.658,labels=""Egger\'s p = 0.504"",adj=c(0,0))\r\n\r\n# dev.off()\r\n# embedFonts(""Figure S2.pdf"")\r\n\r\n##Figure S3############################################################################################################\r\n\r\n##individual effect sizes based on different local variability measures \r\n## 2*45 effect sizes calculated from raw data (8 studies)\r\n\r\nlocal.index=read.csv(""local_index.csv"")\r\nloc.ind=rma.mv(yi,local.index[,c(11:100)],mods=~Metrics-1,random =~ 1|Article_ID, data=local.index)\r\n\r\ndat.loc=data.frame(b=loc.ind$b,lb=loc.ind$ci.lb,ub=loc.ind$ci.ub,\r\n                   cat=unique(local.index$Metrics))\r\n\r\nloc.comp=ggplot()+\r\n  geom_violin(local.index,mapping=aes(Metrics,yi,color=Metrics))+ ## violin plot\r\n  geom_jitter(local.index,mapping=aes(Metrics,yi,color=Metrics),width=0.2,size=2,alpha=0.2,show.legend = FALSE)+ ## jitter plot for raw data\r\n  scale_color_manual(values=c(""#3B4992FF"",""#BB0021FF""))+ ## set the color\r\n  geom_linerange(dat.loc,mapping=aes(x=cat,ymin=lb,ymax=ub))+ ## plot the error bar\r\n  geom_point(dat.loc,mapping=aes(cat,b,color=cat),size=3)+ ## plot the mean\r\n  geom_text(aes(x=dat.loc$cat,y=-1.2,label=c(""p = 0.97"",""p = 0.97"")),vjust=0,size=12/.pt)+ ## plot the text\r\n  scale_y_continuous(limits = c(-1.2,1),breaks = c(-1,-0.5,0,0.5,1),expand=expansion(add = c(0.1,0)))+ ## set the limitation of y and its break\r\n  scale_x_discrete(labels=c(expression(CV[L.m]),expression(CV[L.w])))+\r\n  geom_hline(yintercept=0,col=""gray50"",linetype=""longdash"")+\r\n  ylab(""LRR"")+\r\n  theme(legend.position = ""none""\r\n        ,panel.border = element_rect(color=""black"")\r\n        ,axis.title.x = element_blank()\r\n        ,axis.title.y = element_text(size=12)\r\n        ,axis.text.x = element_text(size=12,vjust = -2)\r\n        ,axis.text.y = element_text(size=11)\r\n        ,axis.ticks.length.y = unit(-0.15, \'cm\')\r\n        ,axis.ticks.length.x = unit(-0.15, \'cm\')\r\n        ,plot.margin = margin(0.2, 0.2, 0.5, 0.5, ""cm""))#\r\n\r\n# pdf(""Figure S3.pdf"",width =5, height =4, useDingbats=FALSE,bg = ""white"", colormodel = ""cmyk"") \r\nloc.comp\r\n# dev.off()\r\n# embedFonts(""Figure S3.pdf"")\r\n\r\n##tracking session info\r\n# sink(""sessionInfo.txt"")\r\n# sessionInfo()\r\n# sink()']","Dispersal increases spatial synchrony of populations but has weak effects on population variability: a meta-analysis The effects of dispersal on spatial synchrony and population variability have been well documented in theoretical research, and a growing number of empirical tests have been performed. Yet a synthesis is still lacking. Here, we conducted a meta-analysis of relevant experiments and examined how dispersal affected spatial synchrony and temporal population variability across scales. Our analyses showed that dispersal generally promoted spatial synchrony, and such effects increased with dispersal rate and decreased with environmental correlation among patches. The synchronizing effect of dispersal, however, was only detected when spatial synchrony was measured using the correlation-based index, but not for the covariance-based index. In contrast to theoretical predictions, the effect of dispersal on local population variability was generally non-significant, except when environment correlation among patch was negative and/or experimental period was long. At the regional scale, while low dispersal stabilized metapopulation dynamics, high dispersal led to destabilization. Overall, the sign and strength of dispersal effects on spatial synchrony and population variability were modulated by taxa, environmental heterogeneity, type of perturbations, patch number, and experimental length. Our synthesis demonstrates that dispersal can substantially affect the dynamics of spatially distributed populations, but its effects are context dependent on abiotic and biotic factors.",4
Life history data for longer-lived tropical songbirds reduce breeding activity as they buffer impacts of drought,"Droughts are expected to increase in frequency and severity with climate change. Population impacts of such harsh environmental events are theorized to vary with life history strategies among species. However, existing demographic models generally do not consider behavioral plasticity that may modify the impact of harsh events. Here we show that tropical songbirds in the New and Old World reduced reproduction during drought, with greater reductions in species with higher average long-term survival. Large reductions in reproduction by longer-lived species were associated with higher survival during drought than pre-drought years in Malaysia, whereas shorter-lived species maintained reproduction and survival decreased. Behavioral strategies of longer-lived, but not shorter-lived, species mitigated the effect of increasing drought frequency on long-term population growth. Behavioral plasticity can buffer the impact of climate change on populations of some species, and differences in plasticity among species related to their life histories are critical for predicting population trajectories.","['\r\n#################################################################################################\r\n#####   Load Packages   #########################################################################\r\n#################################################################################################\r\nlibrary(plyr)\r\nlibrary(tidyr)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nlibrary(pbapply)\r\nlibrary(emmeans)\r\nlibrary(MSwM)\r\nlibrary(MuMIn)\r\nlibrary(popbio)\r\nlibrary(lmerTest)\r\n\r\n#################################################################################################\r\n#####   Create Functions   ######################################################################\r\n#################################################################################################\r\n\r\n# Function to summary data for plots\r\ndata_summary <- function(data, varname, groupnames){\r\n  require(plyr)\r\n  summary_func <- function(x, col){\r\n    c(mean = mean(x[[col]], na.rm=TRUE),\r\n      sd = sd(x[[col]], na.rm=TRUE))\r\n  }\r\n  data_sum<-ddply(data, groupnames, .fun=summary_func,\r\n                  varname)\r\n  data_sum <- rename(data_sum, c(""mean"" = varname))\r\n  return(data_sum)\r\n}\r\n\r\n### Create geometric mean function:  #######\r\ngmean=function(x){(prod(x))^(1/length(x))}\r\n\r\n\r\n### Create function to estimate parameters for Beta distribution given mean and variance:  #######\r\nestBetaParams = function(mu, var) {\r\n  alpha = (mu^2 - mu^3 - mu*var)/var\r\n  beta = (mu - 2*mu^2 +mu^3 - var +mu*var)/var\r\n  return(params = as.numeric(c(alpha,beta)))\r\n}\r\n\r\n\r\n#################################################################################################\r\n#####   Create Simulation Functions   ###########################################################\r\n#################################################################################################\r\n\r\nPopSim = function(N0, ElNino, Norm, Dry){\r\n  \r\n  nyrs=ncol(ElNino)\r\n  \r\n  final.list=vector(""list"", nrow(ElNino))  # Create list to store all output\r\n  \r\n  for(esim in 1:nrow(ElNino)){   # For each simulated sequence of El Ninos:\r\n    \r\n    out=matrix(nrow=length(Norm), ncol=nyrs+1)  #Create a matrix to save summed population sizes for each popualation run\r\n    \r\n    for(msim in 1:length(Norm)){    # For each simulation population realization:\r\n      \r\n      # Create empty list of stage structured population matrices:\r\n      Nstr=vector(""list"", nyrs+1)\r\n      \r\n      # set initial population to a stable age distribution for the first matrix\r\n      if(ElNino[esim, 1]==0){  # Draw first matrix\r\n        mat1=Norm[[runif(1, 1, length(Norm))]]   # Draw a Normal matrix if it\'s NOT an Elnino in the first year\r\n      }else{\r\n        mat1=Dry[[runif(1, 1, length(Norm))]]}   # Draw a Dry matrix if it IS an Elnino in the first year\r\n      \r\n      # Determine the stable age distribution for the first year\'s matrix\r\n      eigen.mat1=eigen(mat1)\r\n      sad=eigen.mat1$vectors[,1]/sum(eigen.mat1$vectors[,1])\r\n      \r\n      # Set initial age distribution\r\n      Nstr[[1]]=matrix( c(N0*sad[1], N0*sad[2]), nrow=2, ncol=1, byrow=TRUE) \r\n      \r\n      # Project the population across years:\r\n      for(t in 2:(nyrs+1)){\r\n        if(t==2){\r\n          Nstr[[t]] = as.matrix(mat1 %*% Nstr[[t-1]])\r\n        }else{\r\n          mat.draw=runif(1, 1, length(Norm))\r\n          if(ElNino[esim, t-1]==0){ \r\n            Nstr[[t]] =as.matrix(Norm[[mat.draw]] %*% Nstr[[t-1]])\r\n          }else{ \r\n            Nstr[[t]] = as.matrix(Dry[[mat.draw]] %*% Nstr[[t-1]])\r\n          } #else\r\n        } #else\r\n      } # t\r\n      \r\n      # Sum across age classes for total population size at each year and save to output matrix\r\n      out[msim, ] = unlist(lapply(Nstr, sum))  #(each row of out is a sequence of nyrs+1 total population sizes):   \r\n    } # msim\r\n    \r\n    outdf=as.data.frame(out)\r\n    outdf$ElNinoSim=esim\r\n    outdf$NumDroughts=sum(ElNino[esim,])\r\n    outdf$PopSim=1:length(Norm)\r\n    final.list[[esim]] = outdf\r\n  } # esim\r\n  \r\n  final=ldply(final.list)\r\n  final$MeanLambda=rep(NA, times=nrow(final))\r\n  final$AMeanLambda=rep(NA, times=nrow(final))\r\n  final$VarLambda=rep(NA, times=nrow(final))\r\n  \r\n  for(i in 1:nrow(final)){\r\n    lambda=numeric(length = nyrs)\r\n    \r\n    for(t in 1:(nyrs)){\r\n      lambda[t] = final[i,t+1]/final[i,t]\r\n    } #t\r\n    \r\n    final$MeanLambda[i] = gmean(lambda)\r\n    final$AMeanLambda[i] = mean(lambda)\r\n    final$VarLambda[i] = var(lambda)\r\n    final$DryMeanLambda[i] = gmean(lambda[which(ElNino[final$ElNinoSim[i],]==1)])\r\n    final$DryAMeanLambda[i] = mean(lambda[which(ElNino[final$ElNinoSim[i],]==1)])\r\n    final$DryVarLambda[i] = var(lambda[which(ElNino[final$ElNinoSim[i],]==1)])\r\n    final$NormMeanLambda[i] = gmean(lambda[which(ElNino[final$ElNinoSim[i],]==0)])\r\n    final$NormAMeanLambda[i] = mean(lambda[which(ElNino[final$ElNinoSim[i],]==0)])\r\n    final$NormVarLambda[i] = var(lambda[which(ElNino[final$ElNinoSim[i],]==0)])\r\n  } #i\r\n  \r\n  return(final)\r\n} # End Function\r\n\r\n\r\n#################################################################################################\r\n#####   Load Demographic Data   #################################################################\r\n#################################################################################################\r\n\r\n### Species Demogrphic data\r\ndata=data.frame(SPP=c(""OCBU"", ""INFL"", ""TESU"", ""MOTA"", ""MLWA"", ""WTFA"", ""WBSH"", ""BOFO""), \r\n                asurv = c(0.818, 0.760, 0.830, 0.663, 0.696, 0.662, 0.837, 0.822),\r\n                asurv.se = c(0.037, 0.085, 0.079, 0.082, 0.063, 0.043, 0.044, 0.065),\r\n                cs = c(1.812, 1.926, 1.0, 2.676, 2.273, 1.98, 1.951, 2.057),\r\n                dpr = c(0.0473, 0.0277, 0.034, 0.05132, 0.05303, 0.047,0.044, 0.047),\r\n                totper = c(28.2, 36.05, 35.17, 31.83, 33.02, 30.15, 36.701, 34.53),\r\n                num_att = c(1.5, 1.25, 2.0, 1.5, 1.5, 2.0, 1.25, 1.5)\r\n)\r\ndata$jsurv=data$asurv-0.24\r\ndata$pbreed=rep(0.95, times=nrow(data)) # assumes XX% of individuals breed in a given season\r\ndata$pbreed.se=rep(0.05, times=nrow(data)) # \r\ndata$fsurv = (sin(0.249 + 0.692*asin(sqrt(data$asurv)) ))^2  # Based on data from Lloyd et al. 2017\r\ndata$annfec = data$cs*data$num_att*((1-data$dpr)^data$totper)*data$fsurv\r\ndata$annfec.se = data$annfec * 0.1\r\n\r\n\r\ndr_effect=data.frame(d_asurv = c( 0.1965, -0.245, -0.245,  -0.318, 0.1827, 0.2103, 0.2052, -0.2488, -0.011647, -0.1868, -0.426, -0.21),\r\n                     d_p.breed = c( -0.57, 0.035, -0.57, -0.7115, -0.4286, -0.600, -0.6471, 0.0909, 0.035714, -0.0769, -0.756, -0.667))\r\nrownames(dr_effect)=c(""HighAs"", ""LowAS"", ""LowAS_Alt"", ""Wet"", ""dOCBU"", ""dINFL"", ""dTESU"", ""dMOTA"", ""dMLWA"", ""dWTFA"", ""dWBSH"", ""dBOFO"" )\r\n\r\n\r\n#################################################################################################\r\n#####  Simulate Future ElNino sequences given changing temperatures   ###########################\r\n#################################################################################################\r\n\r\n\r\n### Switching-Markov model to predict El Nino from SOI data: ###########################\r\n\r\n# Load global mean temperature predictions from climate models:\r\ntemp26=read.table(""WGIAR5_FD_AnnexI_series_tas_modelmean_rcp26_world_annual.txt"")\r\ntemp60=read.table(""WGIAR5_FD_AnnexI_series_tas_modelmean_rcp60_world_annual.txt"")\r\ntemp85=read.table(""WGIAR5_FD_AnnexI_series_tas_modelmean_rcp85_world_annual.txt"")\r\n\r\ntemp=list(temp26, temp60, temp85)\r\n\r\n### Load Southern Oscillation Index data: #####################################################\r\n\r\nsoi=read.csv(""SOI_data.csv"")\r\nsoi=soi[-154,]  # Remove year with incomplete data\r\n\r\n# Calculate annual mean SOI\r\nsoi$Mean=apply(soi[,-1], 1, FUN=mean)\r\n\r\n\r\n### Predict SOI and Severe Elninos (SOI<-1) from temperature predictions #######################\r\nElnino=list() # Create list for future Elnino predictions\r\nSOI=list() # Create list for SOI predictions\r\nHist=list() # Create list for historic Elnino data\r\n\r\nfor(i in 1:length(temp)){   # For each CO2 concentration scenario\r\n  colnames(temp[[i]])=c(""Year"", ""Mean"") # Rename columns\r\n  temp[[i]]=temp[[i]][-c(1:5),-1]  # Remove the years 1881-1885 from the temp data to match the soi data\r\n  \r\n  clim=data.frame(soi=soi$Mean, temp=temp[[i]][1:nrow(soi)]) # Create data frame with temp and soi data for historic data only\r\n  \r\n  slr=lm(soi ~ temp, data=clim) # Run linear model\r\n  \r\n  ### Run  switching-markov models:\r\n  \r\n  msmod2.3=msmFit(slr, k=2, p=3, sw=rep(TRUE, 6))\r\n  # summary(msmod2.3) #AIC = 318.7\r\n\r\n  \r\n  #################################################\r\n  ### Predict into future  ########################\r\n  #################################################\r\n  \r\n  pmat=msmod2.3@transMat # Regime transition matrix from switching markov model\r\n  \r\n  n.yrs=length(temp[[i]]) # Number of years from 1886-2100\r\n  \r\n  soimat=matrix(nrow=1000, ncol=n.yrs) # Create matrix to store soi predictions\r\n  elnino=matrix(nrow=1000, ncol=n.yrs) # Create matrix to store El Nino predictions\r\n  for(t in 1:nrow(soi)){\r\n    soimat[,t]=soi$Mean[t] # Fill soi matrix with real soi values\r\n    elnino[,t]=ifelse(soi$Mean[t] < -1, 1, 0) # Fill El Nino matrix with real soi values\r\n  }\r\n  \r\n  regime=numeric(n.yrs) # Create vector for regime data:\r\n  regime[1:153]=2 # Set regime to 2 for the end of historic period (based on model results)\r\n  \r\n  for(r in 1:nrow(soimat)){ # For each simulation:\r\n    for(x in 153:(n.yrs-1)){ # For each year after the historic period (2019-2100)\r\n      \r\n      # Make predictions based on the switching markov model   \r\n      soimat[r,x+1] = ifelse(regime[x]==1, # Store SOI predictions\r\n                             msmod2.3@Coef[1,1]+msmod2.3@Coef[1,2]*temp[[i]][x+1] + msmod2.3@Coef[1,3]*soimat[x] + msmod2.3@Coef[1,4]*soimat[x-1]+ msmod2.3@Coef[1,5]*soimat[x-2] + rnorm(1,0,msmod2.3@std[1]),\r\n                             msmod2.3@Coef[2,1]+msmod2.3@Coef[2,2]*temp[[i]][x+1] + msmod2.3@Coef[2,3]*soimat[x] + msmod2.3@Coef[2,4]*soimat[x-1]+ msmod2.3@Coef[2,5]*soimat[x-2] + rnorm(1,0,msmod2.3@std[2]))\r\n      regime[x+1] = rbinom(1,1,pmat[regime[x],2])+1 # Store regime sequence\r\n      elnino[r,x+1] = ifelse(soimat[r,x+1] < -1, 1, 0) #Store El Nino prediction\r\n    } #x\r\n  }  #r\r\n  \r\n  SOI[[i]]=soimat  # Save SOI predictions\r\n  Elnino[[i]]=elnino[,154:n.yrs] # Save El Nino predicitons\r\n  Hist[[i]]=elnino[,1:153] # Save historic El Nino data\r\n}\r\n\r\nHist=Hist[[1]] # Remove multiple copies of historic El Nino data\r\n\r\n# write.csv(ldply(SOI), paste(Sys.Date(), ""SOIsims.csv"", sep=""_""))\r\n# write.csv(ldply(Elnino), paste(Sys.Date(), ""ElNinosims.csv"", sep=""_""))\r\n\r\n# d2=apply(Elnino[[1]], 1, mean)\r\n# hist(d2)\r\n# summary(d2)\r\n# \r\n# \r\n# d2=apply(Elnino[[2]], 1, mean)\r\n# hist(d2)\r\n# summary(d2)\r\n# \r\n# d2=apply(Elnino[[3]], 1, mean)\r\n# hist(d2)\r\n# summary(d2)\r\n\r\n#################################################################################################\r\n#####   Set Parameters and Run Simulations   ####################################################\r\n#################################################################################################\r\n\r\n### Set required parameters:  ###########################################\r\nn.mats=1000\r\nN0=5000\r\n\r\n### Run Simulations:  ##################################################\r\n\r\nt1 = Sys.time()\r\nSppList=list()\r\nElasList=list()\r\nHistList=list()\r\nfor(spp in 1:nrow(data)){\r\n  \r\n  print(paste(""Species"",spp, ""of"", nrow(data), sep="" ""))\r\n  s1 = Sys.time()\r\n  \r\n  ### Draw vital rates for each matrix:  \r\n  AS=rbeta(n.mats, estBetaParams(data$asurv[spp], data$asurv.se[spp]^2)[1], estBetaParams(data$asurv[spp], data$asurv.se[spp]^2)[2])\t\t\t\r\n  Annfec=log(rlnorm(n.mats, data$annfec[spp], data$annfec.se[spp]^2))\r\n  pBreed=rbeta(n.mats, estBetaParams(data$pbreed[spp], data$pbreed.se[spp]^2)[1], estBetaParams(data$pbreed[spp], data$pbreed.se[spp]^2)[2]) \r\n  JS=rbeta(n.mats, estBetaParams(data$jsurv[spp], 1.39*data$asurv.se[spp]^2)[1], estBetaParams(data$jsurv[spp], 1.39*data$asurv.se[spp]^2)[2])\t    \r\n  AFec=Annfec*pBreed\r\n  JFec=Annfec*pBreed*0.9\r\n  \r\n  ### Create lists of population matrices: \r\n  Norm=list(length=n.mats)\r\n  Dry_HighAS=list(length=n.mats)\r\n  Dry_LowAS=list(length=n.mats)\r\n  Dry_LowAS_Alt=list(length=n.mats)\r\n  Dry_Wet=list(length=n.mats)\r\n  Dry_dOCBU=list(length=n.mats)\r\n  Dry_dINFL=list(length=n.mats)\r\n  Dry_dTESU=list(length=n.mats)\r\n  Dry_dMOTA=list(length=n.mats)\r\n  Dry_dMLWA=list(length=n.mats)\r\n  Dry_dWTFA=list(length=n.mats)\r\n  Dry_dWBSH=list(length=n.mats)\r\n  Dry_dBOFO=list(length=n.mats)\r\n  \r\n  for(m in 1:n.mats){\r\n    Norm[[m]]=matrix(c(JFec[m], AFec[m], JS[m], AS[m]), nrow=2, byrow=TRUE)\r\n    \r\n    for(x in 1:nrow(dr_effect)){\r\n      Dry_HighAS[[m]]=matrix(c( (JFec[m]+JFec[m]*dr_effect[1,2])*JS[m] , (AFec[m]+AFec[m]*dr_effect[1,2])*(AS[m]+AS[m]*dr_effect[1,1]), JS[m], AS[m]+AS[m]*dr_effect[1,1]), nrow=2, byrow=TRUE)\r\n      Dry_LowAS[[m]]=matrix(c( (JFec[m]+JFec[m]*dr_effect[2,2])*JS[m], (AFec[m]+AFec[m]*dr_effect[2,2])*(AS[m]+AS[m]*dr_effect[2,1]), JS[m], AS[m]+AS[m]*dr_effect[2,1]), nrow=2, byrow=TRUE)\r\n      Dry_LowAS_Alt[[m]]=matrix(c( (JFec[m]+JFec[m]*dr_effect[3,2])*JS[m], (AFec[m]+AFec[m]*dr_effect[3,2])*(AS[m]+AS[m]*dr_effect[3,1]), JS[m], AS[m]+AS[m]*dr_effect[3,1]), nrow=2, byrow=TRUE)\r\n      Dry_Wet[[m]]=matrix(c( (JFec[m]+JFec[m]*dr_effect[4,2])*JS[m], (AFec[m]+AFec[m]*dr_effect[4,2])*(AS[m]+AS[m]*dr_effect[4,1]), JS[m], AS[m]+AS[m]*dr_effect[4,1]), nrow=2, byrow=TRUE)\r\n      Dry_dOCBU[[m]]=matrix(c( (JFec[m]+JFec[m]*dr_effect[5,2])*JS[m], (AFec[m]+AFec[m]*dr_effect[5,2])*(AS[m]+AS[m]*dr_effect[5,1]), JS[m], AS[m]+AS[m]*dr_effect[5,1]), nrow=2, byrow=TRUE)\r\n      Dry_dINFL[[m]]=matrix(c( (JFec[m]+JFec[m]*dr_effect[6,2])*JS[m], (AFec[m]+AFec[m]*dr_effect[6,2])*(AS[m]+AS[m]*dr_effect[6,1]), JS[m], AS[m]+AS[m]*dr_effect[6,1]), nrow=2, byrow=TRUE)\r\n      Dry_dTESU[[m]]=matrix(c( (JFec[m]+JFec[m]*dr_effect[7,2])*JS[m], (AFec[m]+AFec[m]*dr_effect[7,2])*(AS[m]+AS[m]*dr_effect[7,1]), JS[m], AS[m]+AS[m]*dr_effect[7,1]), nrow=2, byrow=TRUE)\r\n      Dry_dMOTA[[m]]=matrix(c( (JFec[m]+JFec[m]*dr_effect[8,2])*JS[m], (AFec[m]+AFec[m]*dr_effect[8,2])*(AS[m]+AS[m]*dr_effect[8,1]), JS[m], AS[m]+AS[m]*dr_effect[8,1]), nrow=2, byrow=TRUE)\r\n      Dry_dMLWA[[m]]=matrix(c( (JFec[m]+JFec[m]*dr_effect[9,2])*JS[m], (AFec[m]+AFec[m]*dr_effect[9,2])*(AS[m]+AS[m]*dr_effect[9,1]), JS[m], AS[m]+AS[m]*dr_effect[9,1]), nrow=2, byrow=TRUE)\r\n      Dry_dWTFA[[m]]=matrix(c( (JFec[m]+JFec[m]*dr_effect[10,2])*JS[m], (AFec[m]+AFec[m]*dr_effect[10,2])*(AS[m]+AS[m]*dr_effect[10,1]), JS[m], AS[m]+AS[m]*dr_effect[10,1]), nrow=2, byrow=TRUE)\r\n      Dry_dWBSH[[m]]=matrix(c( (JFec[m]+JFec[m]*dr_effect[11,2])*JS[m], (AFec[m]+AFec[m]*dr_effect[11,2])*(AS[m]+AS[m]*dr_effect[11,1]), JS[m], AS[m]+AS[m]*dr_effect[11,1]), nrow=2, byrow=TRUE)\r\n      Dry_dBOFO[[m]]=matrix(c( (JFec[m]+JFec[m]*dr_effect[12,2])*JS[m], (AFec[m]+AFec[m]*dr_effect[12,2])*(AS[m]+AS[m]*dr_effect[12,1]), JS[m], AS[m]+AS[m]*dr_effect[12,1]), nrow=2, byrow=TRUE)\r\n      \r\n    } #x\r\n  } #m\r\n  \r\n  ### Calculate elasticity for every normal matrix\r\n  Elas=list()\r\n  for(x in 1:n.mats){\r\n    Elas[[x]]=elasticity(Norm[[x]])\r\n  } #x\r\n  ElasDF=ldply(lapply(Elas, as.numeric))\r\n  colnames(ElasDF) = c(""JFec"", ""Jsurv"", ""Afec"", ""Asurv"")\r\n  ElasDF$Fec=ElasDF$Afec+ElasDF$JFec\r\n  ElasDF$SPP=data$SPP[spp]\r\n  \r\n  \r\n  ### Historic data:\r\n  ### Simulate populations assuming HighAS strategy\r\n  HighASHist=PopSim(N0, Hist, Norm, Dry_HighAS)\r\n  HighASHist$Exp=""HighAS""\r\n  HighASHist$Rcp=""Hist""\r\n  HighASHist$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming LowAS strategy\r\n  LowASHist=PopSim(N0, Hist, Norm, Dry_LowAS)\r\n  LowASHist$Exp=""LowAS""\r\n  LowASHist$Rcp=""Hist""\r\n  LowASHist$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming LowAS_Alt strategy\r\n  LowASaltHist=PopSim(N0, Hist, Norm, Dry_LowAS_Alt)\r\n  LowASaltHist$Exp=""LowAS_Alt""\r\n  LowASaltHist$Rcp=""Hist""\r\n  LowASaltHist$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming Wet strategy\r\n  WetHist=PopSim(N0, Hist, Norm, Dry_Wet)\r\n  WetHist$Exp=""Wet""\r\n  WetHist$Rcp=""Hist""\r\n  WetHist$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming the species specific strategy\r\n  dOCBUHist=PopSim(N0, Hist, Norm, Dry_dOCBU)\r\n  dOCBUHist$Exp=""dOCBU""\r\n  dOCBUHist$Rcp=""Hist""\r\n  dOCBUHist$SPP=data$SPP[spp]\r\n  \r\n  dINFLHist=PopSim(N0, Hist, Norm, Dry_dINFL)\r\n  dINFLHist$Exp=""dINFL""\r\n  dINFLHist$Rcp=""Hist""\r\n  dINFLHist$SPP=data$SPP[spp]\r\n  \r\n  dTESUHist=PopSim(N0, Hist, Norm, Dry_dTESU)\r\n  dTESUHist$Exp=""dTESU""\r\n  dTESUHist$Rcp=""Hist""\r\n  dTESUHist$SPP=data$SPP[spp]\r\n  \r\n  dMOTAHist=PopSim(N0, Hist, Norm, Dry_dMOTA)\r\n  dMOTAHist$Exp=""dMOTA""\r\n  dMOTAHist$Rcp=""Hist""\r\n  dMOTAHist$SPP=data$SPP[spp]\r\n  \r\n  dMLWAHist=PopSim(N0, Hist, Norm, Dry_dMLWA)\r\n  dMLWAHist$Exp=""dMLWA""\r\n  dMLWAHist$Rcp=""Hist""\r\n  dMLWAHist$SPP=data$SPP[spp]\r\n  \r\n  dWTFAHist=PopSim(N0, Hist, Norm, Dry_dWTFA)\r\n  dWTFAHist$Exp=""dWTFA""\r\n  dWTFAHist$Rcp=""Hist""\r\n  dWTFAHist$SPP=data$SPP[spp]\r\n  \r\n  dWBSHHist=PopSim(N0, Hist, Norm, Dry_dWBSH)\r\n  dWBSHHist$Exp=""dWBSH""\r\n  dWBSHHist$Rcp=""Hist""\r\n  dWBSHHist$SPP=data$SPP[spp]\r\n  \r\n  dBOFOHist=PopSim(N0, Hist, Norm, Dry_dBOFO)\r\n  dBOFOHist$Exp=""dBOFO""\r\n  dBOFOHist$Rcp=""Hist""\r\n  dBOFOHist$SPP=data$SPP[spp]\r\n  \r\n  \r\n  ### RCP 26:\r\n  ### Simulate populations assuming HighAS strategy\r\n  HighAS26=PopSim(N0, Elnino[[1]], Norm, Dry_HighAS)\r\n  HighAS26$Exp=""HighAS""\r\n  HighAS26$Rcp=""Rcp26""\r\n  HighAS26$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming LowAS strategy\r\n  LowAS26=PopSim(N0, Elnino[[1]], Norm, Dry_LowAS)\r\n  LowAS26$Exp=""LowAS""\r\n  LowAS26$Rcp=""Rcp26""\r\n  LowAS26$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming Wet strategy\r\n  Wet26=PopSim(N0, Elnino[[1]], Norm, Dry_Wet)\r\n  Wet26$Exp=""Wet""\r\n  Wet26$Rcp=""Rcp26""\r\n  Wet26$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming the species specific strategy\r\n  dOCBU26=PopSim(N0, Elnino[[1]], Norm, Dry_dOCBU)\r\n  dOCBU26$Exp=""dOCBU""\r\n  dOCBU26$Rcp=""Rcp26""\r\n  dOCBU26$SPP=data$SPP[spp]\r\n  \r\n  dINFL26=PopSim(N0, Elnino[[1]], Norm, Dry_dINFL)\r\n  dINFL26$Exp=""dINFL""\r\n  dINFL26$Rcp=""Rcp26""\r\n  dINFL26$SPP=data$SPP[spp]\r\n  \r\n  dTESU26=PopSim(N0, Elnino[[1]], Norm, Dry_dTESU)\r\n  dTESU26$Exp=""dTESU""\r\n  dTESU26$Rcp=""Rcp26""\r\n  dTESU26$SPP=data$SPP[spp]\r\n  \r\n  dMOTA26=PopSim(N0, Elnino[[1]], Norm, Dry_dMOTA)\r\n  dMOTA26$Exp=""dMOTA""\r\n  dMOTA26$Rcp=""Rcp26""\r\n  dMOTA26$SPP=data$SPP[spp]\r\n  \r\n  dMLWA26=PopSim(N0, Elnino[[1]], Norm, Dry_dMLWA)\r\n  dMLWA26$Exp=""dMLWA""\r\n  dMLWA26$Rcp=""Rcp26""\r\n  dMLWA26$SPP=data$SPP[spp]\r\n  \r\n  dWTFA26=PopSim(N0, Elnino[[1]], Norm, Dry_dWTFA)\r\n  dWTFA26$Exp=""dWTFA""\r\n  dWTFA26$Rcp=""Rcp26""\r\n  dWTFA26$SPP=data$SPP[spp]\r\n  \r\n  dWBSH26=PopSim(N0, Elnino[[1]], Norm, Dry_dWBSH)\r\n  dWBSH26$Exp=""dWBSH""\r\n  dWBSH26$Rcp=""Rcp26""\r\n  dWBSH26$SPP=data$SPP[spp]\r\n  \r\n  dBOFO26=PopSim(N0, Elnino[[1]], Norm, Dry_dBOFO)\r\n  dBOFO26$Exp=""dBOFO""\r\n  dBOFO26$Rcp=""Rcp26""\r\n  dBOFO26$SPP=data$SPP[spp]\r\n  \r\n  \r\n  ### RCP 60:\r\n  ### Simulate populations assuming HighAS strategy\r\n  HighAS60=PopSim(N0, Elnino[[2]], Norm, Dry_HighAS)\r\n  HighAS60$Exp=""HighAS""\r\n  HighAS60$Rcp=""Rcp60""\r\n  HighAS60$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming LowAS strategy\r\n  LowAS60=PopSim(N0, Elnino[[2]], Norm, Dry_LowAS)\r\n  LowAS60$Exp=""LowAS""\r\n  LowAS60$Rcp=""Rcp60""\r\n  LowAS60$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming Wet strategy\r\n  Wet60=PopSim(N0, Elnino[[2]], Norm, Dry_Wet)\r\n  Wet60$Exp=""Wet""\r\n  Wet60$Rcp=""Rcp60""\r\n  Wet60$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming the species specific strategy\r\n  dOCBU60=PopSim(N0, Elnino[[2]], Norm, Dry_dOCBU)\r\n  dOCBU60$Exp=""dOCBU""\r\n  dOCBU60$Rcp=""Rcp60""\r\n  dOCBU60$SPP=data$SPP[spp]\r\n  \r\n  dINFL60=PopSim(N0, Elnino[[2]], Norm, Dry_dINFL)\r\n  dINFL60$Exp=""dINFL""\r\n  dINFL60$Rcp=""Rcp60""\r\n  dINFL60$SPP=data$SPP[spp]\r\n  \r\n  dTESU60=PopSim(N0, Elnino[[2]], Norm, Dry_dTESU)\r\n  dTESU60$Exp=""dTESU""\r\n  dTESU60$Rcp=""Rcp60""\r\n  dTESU60$SPP=data$SPP[spp]\r\n  \r\n  dMOTA60=PopSim(N0, Elnino[[2]], Norm, Dry_dMOTA)\r\n  dMOTA60$Exp=""dMOTA""\r\n  dMOTA60$Rcp=""Rcp60""\r\n  dMOTA60$SPP=data$SPP[spp]\r\n  \r\n  dMLWA60=PopSim(N0, Elnino[[2]], Norm, Dry_dMLWA)\r\n  dMLWA60$Exp=""dMLWA""\r\n  dMLWA60$Rcp=""Rcp60""\r\n  dMLWA60$SPP=data$SPP[spp]\r\n  \r\n  dWTFA60=PopSim(N0, Elnino[[2]], Norm, Dry_dWTFA)\r\n  dWTFA60$Exp=""dWTFA""\r\n  dWTFA60$Rcp=""Rcp60""\r\n  dWTFA60$SPP=data$SPP[spp]\r\n  \r\n  dWBSH60=PopSim(N0, Elnino[[2]], Norm, Dry_dWBSH)\r\n  dWBSH60$Exp=""dWBSH""\r\n  dWBSH60$Rcp=""Rcp60""\r\n  dWBSH60$SPP=data$SPP[spp]\r\n  \r\n  dBOFO60=PopSim(N0, Elnino[[2]], Norm, Dry_dBOFO)\r\n  dBOFO60$Exp=""dBOFO""\r\n  dBOFO60$Rcp=""Rcp60""\r\n  dBOFO60$SPP=data$SPP[spp]\r\n  \r\n  \r\n  ### RCP 85:\r\n  ### Simulate populations assuming HighAS strategy\r\n  HighAS85=PopSim(N0, Elnino[[3]], Norm, Dry_HighAS)\r\n  HighAS85$Exp=""HighAS""\r\n  HighAS85$Rcp=""Rcp85""\r\n  HighAS85$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming LowAS strategy\r\n  LowAS85=PopSim(N0, Elnino[[3]], Norm, Dry_LowAS)\r\n  LowAS85$Exp=""LowAS""\r\n  LowAS85$Rcp=""Rcp85""\r\n  LowAS85$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming Wet strategy\r\n  Wet85=PopSim(N0, Elnino[[3]], Norm, Dry_Wet)\r\n  Wet85$Exp=""Wet""\r\n  Wet85$Rcp=""Rcp85""\r\n  Wet85$SPP=data$SPP[spp]\r\n  \r\n  ### Simulate populations assuming the species specific strategy\r\n  dOCBU85=PopSim(N0, Elnino[[3]], Norm, Dry_dOCBU)\r\n  dOCBU85$Exp=""dOCBU""\r\n  dOCBU85$Rcp=""Rcp85""\r\n  dOCBU85$SPP=data$SPP[spp]\r\n  \r\n  dINFL85=PopSim(N0, Elnino[[3]], Norm, Dry_dINFL)\r\n  dINFL85$Exp=""dINFL""\r\n  dINFL85$Rcp=""Rcp85""\r\n  dINFL85$SPP=data$SPP[spp]\r\n  \r\n  dTESU85=PopSim(N0, Elnino[[3]], Norm, Dry_dTESU)\r\n  dTESU85$Exp=""dTESU""\r\n  dTESU85$Rcp=""Rcp85""\r\n  dTESU85$SPP=data$SPP[spp]\r\n  \r\n  dMOTA85=PopSim(N0, Elnino[[3]], Norm, Dry_dMOTA)\r\n  dMOTA85$Exp=""dMOTA""\r\n  dMOTA85$Rcp=""Rcp85""\r\n  dMOTA85$SPP=data$SPP[spp]\r\n  \r\n  dMLWA85=PopSim(N0, Elnino[[3]], Norm, Dry_dMLWA)\r\n  dMLWA85$Exp=""dMLWA""\r\n  dMLWA85$Rcp=""Rcp85""\r\n  dMLWA85$SPP=data$SPP[spp]\r\n  \r\n  dWTFA85=PopSim(N0, Elnino[[3]], Norm, Dry_dWTFA)\r\n  dWTFA85$Exp=""dWTFA""\r\n  dWTFA85$Rcp=""Rcp85""\r\n  dWTFA85$SPP=data$SPP[spp]\r\n  \r\n  dWBSH85=PopSim(N0, Elnino[[3]], Norm, Dry_dWBSH)\r\n  dWBSH85$Exp=""dWBSH""\r\n  dWBSH85$Rcp=""Rcp85""\r\n  dWBSH85$SPP=data$SPP[spp]\r\n  \r\n  dBOFO85=PopSim(N0, Elnino[[3]], Norm, Dry_dBOFO)\r\n  dBOFO85$Exp=""dBOFO""\r\n  dBOFO85$Rcp=""Rcp85""\r\n  dBOFO85$SPP=data$SPP[spp]\r\n  \r\n  \r\n  ### Create output\r\n  SppHist=rbind(HighASHist, LowASHist, LowASaltHist, WetHist, dOCBUHist, dINFLHist, dTESUHist, dMOTAHist, dMLWAHist, dWTFAHist, dWBSHHist, dBOFOHist)\r\n  SppDF=rbind(HighAS26, LowAS26, Wet26, dOCBU26, dINFL26, dTESU26, dMOTA26, dMLWA26, dWTFA26, dWBSH26, dBOFO26, \r\n              HighAS60, LowAS60, Wet60, dOCBU60, dINFL60, dTESU60, dMOTA60, dMLWA60, dWTFA60, dWBSH60, dBOFO60, \r\n              HighAS85, LowAS85, Wet85, dOCBU85, dINFL85, dTESU85, dMOTA85, dMLWA85, dWTFA85, dWBSH85, dBOFO85 )\r\n  HistList[[spp]] = SppHist\r\n  SppList[[spp]] = SppDF\r\n  ElasList[[spp]] = ElasDF\r\n  s2 = Sys.time()\r\n  print(s2-s1)\r\n} # spp\r\n\r\nHistData=ldply(HistList)\r\nSimData=ldply(SppList)\r\nElasData=ldply(ElasList)\r\nt2=Sys.time()\r\nprint(t2-t1)\r\n\r\nSimData$SPP=factor(SimData$SPP, c(""OCBU"", ""INFL"", ""TESU"", ""MOTA"", ""MLWA"", ""WTFA"", ""WBSH"", ""BOFO""))\r\nSimData$Exp=factor(SimData$Exp, c(""HighAS"", ""Wet"", ""LowAS"", ""LowAS_Alt"", ""dOCBU"", ""dINFL"", ""dTESU"", ""dMOTA"", ""dMLWA"", ""dWTFA"", ""dWBSH"", ""dBOFO""))\r\nSimData$Rcp=factor(SimData$Rcp, c(""Rcp26"", ""Rcp60"", ""Rcp85""))\r\nSimData$Diff=SimData$DryMeanLambda-SimData$NormMeanLambda\r\nSimData$DrFreq=SimData$NumDroughts/83\r\n\r\nHistData$SPP=factor(HistData$SPP, c(""OCBU"", ""INFL"", ""TESU"", ""MOTA"", ""MLWA"", ""WTFA"", ""WBSH"", ""BOFO""))\r\nHistData$Exp=factor(HistData$Exp, c(""HighAS"", ""Wet"", ""LowAS"", ""LowAS_Alt"", ""dOCBU"", ""dINFL"", ""dTESU"", ""dMOTA"", ""dMLWA"", ""dWTFA"", ""dWBSH"", ""dBOFO""))\r\nHistData$Rcp=factor(HistData$Rcp, c(""Rcp26"", ""Rcp60"", ""Rcp85""))\r\nHistData$Diff=HistData$DryMeanLambda-HistData$NormMeanLambda\r\nHistData$DrFreq=HistData$NumDroughts/83\r\n\r\n']","Life history data for longer-lived tropical songbirds reduce breeding activity as they buffer impacts of drought Droughts are expected to increase in frequency and severity with climate change. Population impacts of such harsh environmental events are theorized to vary with life history strategies among species. However, existing demographic models generally do not consider behavioral plasticity that may modify the impact of harsh events. Here we show that tropical songbirds in the New and Old World reduced reproduction during drought, with greater reductions in species with higher average long-term survival. Large reductions in reproduction by longer-lived species were associated with higher survival during drought than pre-drought years in Malaysia, whereas shorter-lived species maintained reproduction and survival decreased. Behavioral strategies of longer-lived, but not shorter-lived, species mitigated the effect of increasing drought frequency on long-term population growth. Behavioral plasticity can buffer the impact of climate change on populations of some species, and differences in plasticity among species related to their life histories are critical for predicting population trajectories.",4
"R code, spatial and tabular data to fully reproduce STEPS simulations of population change for common brushtail possum, grassland melomys and northern brown bandicoot in northern Australia","The development of effective fire management for biodiversity conservation is a global challenge. The highly dynamic nature of fire, the difficulty in replicating 'real-world' fire experiments, and the need to understand population changes at large spatiotemporal scales make computer simulations particularly useful for identifying optimal fire management regimes for biodiversity conservation. We aimed to develop a flexible modelling approach with which to investigate how the spatiotemporal application of fire (i.e. management scenarios) influences savanna biodiversity. We used existing data from a landscape-scale fire experiment to develop population simulations for the common brushtail possum (Trichosurus vulpecula), grassland melomys (Melomys burtoni) and northern brown bandicoot (Isoodon macrourus) across the Kapalga area of Kakadu National Park in northern Australia. We simulated how populations were expected to change between 1995 and 2015 in response to the fire patterns observed at Kapalga over this period, and under a hypothetical management scenario of extensive prescribed burning.Our models predicted a substantial decline in all three species in response to the observed fire regime at Kapalga, suggesting that the fire patterns observed at Kapalga, with the associated mechanisms and interactions with other ecological processes, were not conducive with the persistence of native mammal populations. Our prescribed burning scenario had little effect on the predicted population trajectory of the common brushtail possum and grassland melomys, but markedly improved the population trajectory of the northern brown bandicoot. These inconsistencies highlight the need for a nuanced approach to fire management across northern Australian savannas, that is tailored to local conditions and management objectives. Synthesis and applications. The modelling approach outlined here, provides a basis for identifying fire patterns that are beneficial for conserving biodiversity, thereby increasing our capacity to establish clear targets for prescribed fire management. Importantly, this approach is flexible and can be easily adapted to other taxa and fire-prone ecosystems.",,"R code, spatial and tabular data to fully reproduce STEPS simulations of population change for common brushtail possum, grassland melomys and northern brown bandicoot in northern Australia The development of effective fire management for biodiversity conservation is a global challenge. The highly dynamic nature of fire, the difficulty in replicating 'real-world' fire experiments, and the need to understand population changes at large spatiotemporal scales make computer simulations particularly useful for identifying optimal fire management regimes for biodiversity conservation. We aimed to develop a flexible modelling approach with which to investigate how the spatiotemporal application of fire (i.e. management scenarios) influences savanna biodiversity. We used existing data from a landscape-scale fire experiment to develop population simulations for the common brushtail possum (Trichosurus vulpecula), grassland melomys (Melomys burtoni) and northern brown bandicoot (Isoodon macrourus) across the Kapalga area of Kakadu National Park in northern Australia. We simulated how populations were expected to change between 1995 and 2015 in response to the fire patterns observed at Kapalga over this period, and under a hypothetical management scenario of extensive prescribed burning.Our models predicted a substantial decline in all three species in response to the observed fire regime at Kapalga, suggesting that the fire patterns observed at Kapalga, with the associated mechanisms and interactions with other ecological processes, were not conducive with the persistence of native mammal populations. Our prescribed burning scenario had little effect on the predicted population trajectory of the common brushtail possum and grassland melomys, but markedly improved the population trajectory of the northern brown bandicoot. These inconsistencies highlight the need for a nuanced approach to fire management across northern Australian savannas, that is tailored to local conditions and management objectives. Synthesis and applications. The modelling approach outlined here, provides a basis for identifying fire patterns that are beneficial for conserving biodiversity, thereby increasing our capacity to establish clear targets for prescribed fire management. Importantly, this approach is flexible and can be easily adapted to other taxa and fire-prone ecosystems.",4
Supplementary material: Thunderstorm Environments in Europe,"Supplementary material for the paper ""Thunderstorm Environments in Europe"".- R code to reproduce the core analysis (reproducing_calculations.R)- Data for each domain (data_domain*.csv)- csv-files of the two tables in the paper (means_clusters.csv and means_domains.csv).- Domain definitions (domains.csv).- A precise variable description (era5_vars.csv)- README for more information.Summary of the corresponding paper: Two thunderstorm environments are described for Europe: Mass-field thunderstorms, which occur mostly in summer, over land, and under similar meteorological conditions, and wind-field thunderstorms, which occur mostly in winter, over the sea, and under more diverse meteorological conditions. Our descriptions are independent of static thresholds and help to understand why thunderstorms in unfavorable seasons for lightning pose a particular risk to tall infrastructure such as wind turbines.Link to the preprint: https://egusphere.copernicus.org/preprints/2023/egusphere-2022-1453/","['#!/usr/bin/env Rscript\n#\' ----------------------------------------------------------------------------\n#\' - NAME:          reproducing_calculations.R\n#\' - AUTHOR:        Deborah Morgenstern\n#\' - DATE:          2023-03-03\n#\' ----------------------------------------------------------------------------\n#\' - OVERVIEW:      Reproducing the core analysis for the paper\n#\'                  ""Thunderstorm Environments in Europe""\n#\'                  (Morgenstern et al. 2023)\n#\' \n#\' - DESCRIPTION:   - Load and inspect the data.\n#\'                  - Do a regional comparison using a principal component\n#\'                    analysis (PCA).\n#\'                  - Find thunderstorm environments using k-means clustering.\n#\'                  - Label the found clusters as thunderstorm environments\n#\'                    using a decision tree.\n#\'                  - Order the found thunderstorm environments.\n#\'                  - Plot figures 2, 3, 5, 6, and 8.\n#\'\n#\' - DEMANDS:       Takes about half a minute to run.\n#\'                  If you have problems with this script, inspect my\n#\'                  sessionInfo() at the end.\n#\' ----------------------------------------------------------------------------\n\n###############################################################################\n#\' \n###############################################################################\n\nlibrary(data.table)\nlibrary(colorspace)\n\n# You might want to set the working directory manually\n# setwd(""path/to/the/data"")\n\n# For simplicity the algorithm is demonstrated at one domain and the plots\n# consider only k = 3. To look at other domains or other k change the values\n# here.\nid <- ""A""  # A - L are available\nk  <- 3    # k = 2 - 6 is what we used (3 is presented in the paper).\n\n##############################################################################\n# Introduction to the data\n##############################################################################\n\n# *************\n# Get an overview over the data by looking at one domain\n\n# Open data\none_domain <- read.csv(sprintf(""data_domain%s.csv"", id),\n                       header = TRUE, row.names = 1,\n                       strip.white = TRUE)\n\n# Get an overview\nhead(one_domain)\n\n# This is the data for one representative sample in domain A\n# Each line represents one lightning observation, i.e. ERA5 cell-hours where\n# at least one flash was observed. \n# The first columns are meta-data (lon, lat, time, season).\n# The last columns are the results from k-means clustering for k = 2-6. These\n# values are reproduced later in this script (thunderstorm environments).\n# The ERA5 values for each observation are the 25 columns in the middle. \n# The column names refer to the short names of each variable.\n\n# Which ERA5 variables are in the data set?\nera5_vars <- names(one_domain[5:29])\nera5_vars\n\n# Details on the era5-variables can be found in era5_vars.csv\nera5_details <- read.csv(""era5_vars.csv"",\n                         header = TRUE, row.names = 1,\n                         strip.white = TRUE)\n\n# These are the more speaking names of the variables\nera5_details$niceName\n\n# Quick check if the ERA5 short names in the data and in era5_vars.csv\n# are the same\nstopifnot(era5_details$shortName == era5_vars)\n\n# If you want to use more speaking names for the era5 variables in the plots,\n# then use the other line.\n# era5_lbl <- era5_details$niceName\nera5_lbl <- era5_vars\n\n# era5_vars.csv provides even more information\nnames(era5_details)\n\n# Back to our data: each season is equally represented\ntable(one_domain$season)\n\n\n##############################################################################\n# Part 1: Reproducing Sec. 4.1: Regional analysis\n##############################################################################\n\n# In the regional analysis, the domain means are compared. \n# The domain means are provided in Table 1 of the paper but also in the\n# supplements as means_domains.csv.\n# This section reproduces means_domains.csv, computes the PCA, and reproduces\n# Figures 2 and 3.\n\n# *************\n# Load domain means (original)\ndomain_means_original <- read.csv(""means_domains.csv"",\n                                  header = TRUE, row.names = 1,\n                                  strip.white = TRUE)\n\n# *************\n# Start reproduction (rep) by initializing an empty data.frame\ndomain_means_rep <- data.frame(matrix(nrow = 25, ncol = 12),\n                               row.names = era5_vars)\ncolnames(domain_means_rep) <- LETTERS[1:12]\n\n# Get the mean value for each domain\n# This takes a few moments because the files for each domain are opened.\nfor (idx in LETTERS[1:12]) {\n  # Open data\n  dat <- read.csv(paste0(""data_domain"", idx, "".csv""),\n                  header = TRUE, row.names = 1,\n                  strip.white = TRUE)\n  # Calculate domain means\n  domain_means_rep[[idx]] <- colMeans(dat[, 5:29])\n}\n\n# *************\n# Compare the reproduction with the original\n\n# There might be minor differences in precision. Omit them by rounding.\ndomMean_original_round     <- round(domain_means_original, digits = 7)\ndomMean_reproduction_round <- round(domain_means_rep, digits = 7)\n\nfor (idx in LETTERS[1:12]) {\n  stopifnot(identical(domMean_original_round[[idx]],\n                      domMean_reproduction_round[[idx]]))\n}\n\n# *****************************************************************************\n# Transform and scale data (apply equations 1 and 2)\n    \ndomMean_t          <- t(domain_means_original)                # transpose\ndomMean_trafo_t    <- sign(domMean_t) * sqrt(abs(domMean_t))  # transform\ndomMean_scaled_t   <- scale(domMean_trafo_t)                  # scale\ndomMean_scaled     <- t(domMean_scaled_t)                     # transpose back\n\n# *****************************************************************************\n# Calculate PCA\npca_domains <- prcomp(domMean_scaled_t)\n\n# Get loadings\nloadings <- pca_domains$rotation[, c(1:2)]\n\n# How much variance is explained by the first two PC?\npc1_domains <- round(summary(pca_domains)$importance[2,1] * 100, digits = 2)\npc2_domains <- round(summary(pca_domains)$importance[2,2] * 100, digits = 2)\n\n\n# *****************************************************************************\n# Prepare plotting: Define colors\n# *****************************************************************************\n\ncols_region <- colorspace::qualitative_hcl(n = 4, h = c(-260, 190),\n                                           c = 60, l = 70)\ncols_region_named <- c(""Mediterranean""  = cols_region[4],\n                       ""Alpine-central"" = cols_region[3],\n                       ""Coastal""        = cols_region[2],\n                       ""Continental""    = cols_region[1])\n\n\n# *****************************************************************************\n# Plotting: PCA of domain means (Fig. 2)\n# *****************************************************************************\n\n# Scaling of loadings to have loadings and PC1 & 2 at the same axis\n# Formula is based on getAnywhere(biplot.prcomp)\nscale_factor <- 1.4\nloadingsS <- loadings\nlambda1 <- pca_domains$sdev[1] * sqrt(length(pca_domains$x[,1])) ^ scale_factor\nlambda2 <- pca_domains$sdev[2] * sqrt(length(pca_domains$x[,2])) ^ scale_factor\nloadingsS[,1] <- loadings[,1] * lambda1\nloadingsS[,2] <- loadings[,2] * lambda2\n\n# *************\n# Plot\n\n# Save figure as pdf\npdf(""rep_figure_02.pdf"", width = 11, height = 6)\npar(mar = c(3, 3, 3, 3) + 2, xpd = FALSE)\n\n\n# Empty plot\nplot(pca_domains$x[,1], pca_domains$x[,2],\n     type = ""n"", xlim = c(-8, 10), ylim = c(-6, 6), las = 1,\n     xlab = paste(""PC 1 ("", pc1_domains, ""%)""),\n     ylab = paste(""PC 2 ("", pc2_domains, ""%)""),\n     main = ""Principal component analysis of domain means"")\n\n# Lines at x = 0 and y = 0\nabline(v = 0, col = ""darkgray"", lty = 2)\nabline(h = 0, col = ""darkgray"", lty = 2)\n\n# Add biplot / loadings\narrows(0, 0, x1 = loadingsS[,1], y1 = loadingsS[,2], length = .1,\n       col = ""lightsteelblue3"")\n\n# Add colored symbols\npoints(pca_domains$x[,1], pca_domains$x[,2],\n       col = c(rep(cols_region[2], 4),   # coastal:        A, B, C, D\n               rep(cols_region[1], 3),   # continental:    E, F, G\n               rep(cols_region[3], 2),   # alpine-central: H, I\n               rep(cols_region[4], 3)),  # mediterranean:  J, K, L\n       pch = c(rep(17, 4), rep(18, 3), rep(16, 2), rep(15, 3)),\n       cex = c(rep(5, 4), rep(6.5, 3), rep(5.5, 2), rep(5, 3)),\n       cex.lab = 1.7, cex.axis = 1.3, las = 1, lwd = 2)\n\n# Add labels to colored symbols\ntext(x = pca_domains$x[,1], y = pca_domains$x[,2],\n     labels = rownames(pca_domains$x), offset = 0.2, col = ""black"", xpd = TRUE)\n\n# Add labels to arrows\n# Many labels are overlapping. For the paper the position of the labels are\n# manually tweaked.\ntext(loadingsS, labels = era5_lbl, cex = .9,\n     pos = 4, offset = 0.3, col = ""lightsteelblue4"", xpd = TRUE)\n\n# Legend\nlegend(""bottomleft"", bty = ""n"",\n       pch = c(17, 18, 15, 16), pt.cex = c(2, 2.5, 1.8, 2),\n       legend = c(""Coastal"", ""Continental"", ""Mediterranean"", ""Alpine-central""),\n       col = cols_region_named[c(""Coastal"", ""Continental"",\n                                 ""Mediterranean"", ""Alpine-central"")])\ndev.off()\n\n\n# *****************************************************************************\n# Plotting: Parallel coordinate plot of domain means (Fig. 3)\n# *****************************************************************************\n\n# Calculate means for each region\nregion_means <- data.frame(\n  ""Coastal""       = rowMeans(domMean_scaled[ , c(""A"", ""B"", ""C"", ""D"")]),\n  ""Continental""   = rowMeans(domMean_scaled[ , c(""E"", ""F"", ""G"")]),\n  ""AlpineCentral"" = rowMeans(domMean_scaled[ , c(""H"", ""I"")]),\n  ""Mediterranean"" = rowMeans(domMean_scaled[ , c(""J"", ""K"", ""L"")])\n  )\n\n# *************\n# Plot\n\n# Save figure as pdf\npdf(""rep_figure_03.pdf"", width = 12, height = 7)\npar(mar = c(9, 5, 7, 2), xpd = TRUE)\n\n# Empty plot\nmatplot(region_means, type = ""n"", ylim = c(-1.9, 2.5), xaxt = ""n"", las = 1,\n        cex.lab = 1.3, cex.axis = 1.3, ylab = ""Scaled value"")\ntitle(main = ""Parallel coordinate plot: Region means"", line = 5)\n\n# xlab\ntext(y = -2.2, x = seq(1, 25), xpd = TRUE, labels = era5_lbl,\n     las = 1, srt = 45, adj = 1, cex = 1.2)\n\n# Gray vertical lines\nabline(v = seq(1, 25), col = ""gray"", lty = 1, xpd = FALSE)\n\n# Horizontal 0 line\nlines(x = c(par(\'usr\')[1], par(\'usr\')[2]), y = c(0, 0), lty = 1, lwd = 2)\n\n# Vertical lines to separate categories\nlines(x = c(7.5, 7.5),\n      y = c(par(\'usr\')[3], par(\'usr\')[4]),\n      lty = 1, lwd = 3, col = ""lightsteelblue4"")\nlines(x = c(11.5, 11.5),\n      y = c(par(\'usr\')[3], par(\'usr\')[4]),\n      lty = 1, lwd = 3, col = ""lightsteelblue4"")\nlines(x = c(16.5, 16.5),\n      y = c(par(\'usr\')[3], par(\'usr\')[4]),\n      lty = 1, lwd = 3, col = ""lightsteelblue4"")\nlines(x = c(23.5, 23.5),\n      y = c(par(\'usr\')[3], par(\'usr\')[4]),\n      lty = 1, lwd = 3, col = ""lightsteelblue4"")\n\n# Category annotations\ntext(.5, par(\'usr\')[4] + .5 / 2, ""Mass field"",\n     pos = 4, cex = 1.5, col = ""lightsteelblue4"")\ntext(7.5, par(\'usr\')[4] + .5, ""Surface\nExchange"", pos = 4, cex = 1.5, col = ""lightsteelblue4"")\ntext(11.5, par(\'usr\')[4] + .5 / 2, ""Wind field"",\n     pos = 4, cex = 1.5, col = ""lightsteelblue4"")\ntext(16.5, par(\'usr\')[4] + .5 / 2, ""Cloud physics"",\n     pos = 4, cex = 1.5, col = ""lightsteelblue4"")\ntext(23.3, par(\'usr\')[4] + .5 / 2, ""Topo"",\n     pos = 4, cex = 1.5, col = ""lightsteelblue4"")\n\n# Plot\nmatplot(region_means, col = cols_region_named, type = ""o"",\n        cex = c(1.7, 1.7, 1.7, 2.2), pch = seq(15, 18), lty = 1, lwd = 2.5,\n        xaxt = ""n"", add = TRUE)\n\n# legend\nlegend(x = .3, y = -3.3,\n       inset = c(0, -.2),\n       horiz = TRUE,\n       legend = names(cols_region_named),\n       col = cols_region_named,\n       text.col = cols_region_named,\n       text.width = c(6, 6, 4, 2),\n       pch = seq(15, 18),\n       cex = 1.5,\n       pt.cex = c(1.5, 1.5, 1.5, 2),\n       bty = ""n"")\ntext(x = c(3, 10.2, 16.4, 22.1),\n     y = rep(-4.05, 4),\n     labels = c(""J, K, L"", ""H, I"", ""A, B, C, D"", ""E, F, G""),\n     col = cols_region_named,\n     cex = 1.5)\n\ndev.off()\n\n\n##############################################################################\n# Part 2: Reproducing Sec. 4.2, 4.3, and 4.4: Thunderstorm environments\n##############################################################################\n\n# Find thunderstorm environments using k-means clustering.\n# This results in a table of cluster means as provided in Table 2 in the \n# paper and in the supplements as means_clusters.csv\n# This section computes k-means clustering, labels the found clusters according \n# to the decision tree, reproduces means_clusters.csv, and reproduces\n# Figures 5, 6, and 8.\n\n# *************\n# Load data (Table 2, original)\ncluster_means_original <- read.csv(""means_clusters.csv"",\n                                   header = TRUE, row.names = 1,\n                                   strip.white = TRUE)\n\n# Have a look at the data. Each domain is represented by the cluster means (rows)\n# of the three thunderstorm environments found there (columns). \ncluster_means_original\n\n# In the following, we only reproduce the results for one domain.\n# Subset the respective columns.\ncm_original <- cluster_means_original[, grep(id, names(cluster_means_original))]\n\n# Get the names of the thunderstorm environments we want to find in the reproduction.\nlabels_original <- substr(colnames(cm_original), 2, 20)\nlabels_original\n\n# *************\n# Load data for one domain\nobs <- read.csv(paste0(""data_domain"", id, "".csv""),\n                header = TRUE, row.names = 1,\n                strip.white = TRUE)\n\n# Have a look at the data. Each row is one lightning observation, i.e. an ERA5\n# cell-hour where at least one lightning stroke was observed. Columns are\n# meta-data or names of ERA5 variables. The last six columns are the results\n# from the cluster analysis presented in the paper. In the following, we\n# reproduce one of these cluster analyses.\nhead(obs)\n\n# Each season is equally represented in the data\ntable(obs$season)\n\n# For the reproduction, only ERA5 variables are required\n# (remove meta-data and results)\nobs_era5 <- as.data.table(obs[, 5:29])\n\n# *****************************************************************************\n# Transformation and scaling\n\nobs_trafo  <- sign(obs_era5) * sqrt(abs(obs_era5))  # transform\nobs_scaled <- as.data.table(scale(obs_trafo))       # scale\n\n\n# In domains without sea, the land-sea mask has the same value everywhere,\n# which results in problems during scaling. When this occurs, the scaled\n# value for land-sea mask is manually set to zero, i.e. an average value.\nif (length(unique(obs_scaled$lsm_bin)) == 1) {\n  obs_scaled$lsm_bin <- 0\n}\n\n# There should be no NAs in the data.\nif (any(is.na(obs_scaled))) {\n  warning(""  * Careful! NA in data. \\n"")\n}\n\n# *****************************************************************************\n# Cluster analysis\n\n# Calculate k-means clustering\nset.seed(1)\nclK <- kmeans(obs_scaled, centers = k, iter.max = 100,\n              nstart = 150, algorithm = ""MacQueen"")\n\n# *************\n# Save results\n\n# Get cluster means\ncm_scaled   <- clK$centers\ncm_unscaled <- setorder(obs_era5[, lapply(.SD, mean), by = clK$cluster], clK)\n# Add cluster id to obs data\ncl_name <- sprintf(""rep_cluster_k%02d"", k)\nobs[[cl_name]] <- clK$cluster\n\n\n# *****************************************************************************\n# Labeling the found clusters with the corresponding thunderstorm type.\n# i.e. describe the clusters meteorologically.\n# *****************************************************************************\n\n# To label the clusters, first the mean in each category is calculated. \n# Based on the resulting values, the thunderstorm environments are assigned\n# using the decision tree (Fig. 4). Then the clusters are ordered to have well\n# organized legends and to have always the same order within the barplots later.\n# After all this, the reproduction is compared to the original values.\n\n# Get the era5 variable categories\ncategories <- era5_details$meteorologicalCategory\nnames(categories) <- era5_vars\ncategories\n\n# *************\nget_increased_categories <- function(cl_mns, ctgr = categories,\n                                     cls = k, thresh = 0.3) {\n  # cls_mns = cluster means as returned from kmeans()\n  # ctgr = categories; named vector of category-variable pairs\n  # cls = number of clusters\n  # thresh = threshold for considering a category for naming\n  \n  # Calculate category means per cluster, i.e.,\n  # meteorological characterization of clusters.\n  # Figure out which category means are above or below the threshold.\n  # This is the basis for labeling with the decision tree later.\n  # Therefore, 1) compute category means for each cluster.\n  # 2) label each cluster with all the increased categories.\n  # e.g. if wind field and surface exchange are both above threshold the \n  # cluster is labeled wf_sfxexH.\n  # If none is above it results in noLabelX (x is a number)\n  \n  # Take absolute values in tSfc1000. This variable is just a difference.\n  cl_mns[,""tSfc1000""] <- abs(cl_mns[,""tSfc1000""])\n  \n  # Calculate means for all values in each category\n  lbl        <- data.frame(matrix(ncol = 0, nrow = cls))\n  lbl$mfH    <- rowMeans(cl_mns[, names(ctgr[ctgr == ""mass field""])])\n  lbl$mfL    <- lbl$mfH * -1\n  lbl$wf     <- rowMeans(cl_mns[, names(ctgr[ctgr == ""wind field""])])\n  lbl$cp     <- rowMeans(cl_mns[, names(ctgr[ctgr == ""cloud physics""])])\n  lbl$sfcexH <- rowMeans(cl_mns[, names(ctgr[ctgr == ""surface exchange""])])\n  lbl$sfcexL <- lbl$sfcexH * -1\n  \n  # If the mean in a category is above threshold,\n  # include the category name in the label\n  # (assign label names given threshold).\n  lbl_labeled <- lbl  # Prepare results\n  nlab_id <- 1        # Number, in case its a noLabel\n  for (cl in seq(1, cls)) {  # Loop over each cluster\n    suppressWarnings(cl_sort <- sort(lbl[cl, ], decreasing = TRUE))\n    idx <- which(cl_sort > thresh)\n    nm <- names(cl_sort)[idx]\n    if (length(nm) == 0) {\n      nm <- paste0(""noLabel"", nlab_id)\n      nlab_id <- nlab_id + 1\n    }\n    lbl_labeled$label[cl] <- paste(nm, collapse = \'_\')\n  }\n  \n  # Create return vector\n  lbs <- lbl_labeled$label\n  names(lbs) <- as.character(seq(1, length(lbs)))\n  return(lbs)\n}\n\n# *************\ndecision_tree <- function(incr_cat) {\n  # incr_cat = result of get_increased_categories()\n  # Apply decision tree to name the clusters properly\n  \n  named <- incr_cat\n  \n  for (i in seq(1, length(named))) {\n    if (grepl(""noLabel"", named[i])) {\n      next   \n    }\n    \n    if (grepl(""wf"", named[i])) {\n      if (grepl(""cp"", named[i])) {\n        named[i] <- ""wind_field_cp""\n      } else {\n        named[i] <- ""wind_field""\n      }\n    } else if (grepl(""mfL"", named[i])) {\n      named[i] <- ""wind_field_noMF""\n    } else if (grepl(""sfcexH"", named[i])) {\n      named[i] <- ""mass_field_sx""\n    } else {\n      named[i] <- ""mass_field""\n    }\n  }\n  \n  return(named)\n}\n\n# *************\n# Apply functions\nincreased_categories <- get_increased_categories(cl_mns = cm_scaled)\nclusters_labeled <- decision_tree(incr_cat = increased_categories)\n\n# The clusters have two names / labels now: Their original number obtained\n# from kmeans() and a label obtained from decision_tree (the label from\n# get_increased_categories() is no longer required).\n# Both names (number and label) are stored in the variable clusters_labeled\nclusters_labeled\n\n# *****************************************************************************\n# Ordering the clusters as follows:\n# Mass field SX, mass field, wind_field noMF, wind_field, wind_field CP\n# Purpose: Have always the same color ordering in the barplots and in\n# the legends.\n\n# Difficulty:\n# Not every cluster is present in every domain. \n# At higher k, some cluster labels occur twice.\n\n# *************\n# First, order the available cluster labels.\n# If there were always all five thunderstorm environments present,\n# we could easily use match().\nclbl <- clusters_labeled\n\ntmpBlueDark <- clbl[which(clbl == ""wind_field_cp"")]\nclbl <- clbl[!clbl %in% tmpBlueDark]\ntmpBlueMiddle <- clbl[which(clbl == ""wind_field"")]\nclbl <- clbl[!clbl %in% tmpBlueMiddle]\ntmpBlueLight <- clbl[which(clbl == ""wind_field_noMF"")]\nclbl <- clbl[!clbl %in% tmpBlueLight]\ntmpRedDark <- clbl[which(clbl == ""mass_field_sx"")]\ntmpRedLight <- clbl[!clbl %in% tmpRedDark]\n\nclusters_labeled_ord <- unique(c(tmpRedDark, tmpRedLight,\n                               tmpBlueLight,\n                               tmpBlueMiddle, tmpBlueDark))\n\n# Have a look at the ordered cluster labels\nclusters_labeled_ord\n\n# *************\n# Second, order the original cluster names (numbers).\n# If k >= 4 there are sometimes two clusters in one domain that have the \n# same label according to the decision tree. If there were no duplicates,\n# we could easily use match(). But now we need this loop.\nclusters_ord <- c()\n\nfor (i in seq(1, length(clusters_labeled))) {\n  pos <- which(clusters_labeled_ord[i] == unname(clusters_labeled))\n  if (length(pos) == 1) {\n    clusters_ord <- c(clusters_ord, pos)\n  } else {\n    for (j in seq(1, length(clusters_labeled))) {\n      # if value is not yet in clusters_ord, add it!\n      if (!pos[j] %in% clusters_ord) {\n        if (is.na(pos[j])) {next}\n        clusters_ord <- c(clusters_ord, pos[j])\n      }\n    }\n  }\n}\n\n# This are the ordered cluster labels\nlabels_ord <- clusters_labeled[clusters_ord]\nlabels_ord\n\n# *************\n# Rename and order the cluster-means data (scaled and unscaled)\ncm_scaled_ord           <- cm_scaled[clusters_ord,]\nrownames(cm_scaled_ord) <- labels_ord\n\ncm_unscaled_ord         <- cm_unscaled[clusters_ord,]\ncm_unscaled_ord$clK     <- labels_ord\n\n# *************\n# Check if the reproduction is the same as the original\n\n# Double check, if there are the same ERA5 variables\nstopifnot(rownames(cm_original) == colnames(cm_scaled_ord))\nstopifnot(era5_vars == colnames(cm_scaled_ord))\n\n# Table 2 / cm_original contains only data for k = 3\nif ( k == 3 ) {\n  # Same thunderstorm environments?\n  stopifnot(labels_original == labels_ord)\n  \n  # Same mean values?\n  # Round to 5 digits as there are sometimes difficulties with the precision\n  cm_original_round <- as.matrix(round(cm_original, digits = 5))\n  cm_unscaled_round <- round(t(cm_unscaled_ord[, 2:26]), digits = 5)\n  # no column / row names\n  colnames(cm_original_round) <- NULL\n  rownames(cm_unscaled_round) <- NULL\n  stopifnot(cm_original_round == cm_unscaled_round)\n}\n\n\n# *****************************************************************************\n# Prepare plotting: Define colors\n# *****************************************************************************\n\nmake_cols_type <- function(lbs) {\n  # Returns a named vector of the hcl-colors for thunderstorm environments\n  \n  all_col_h <- c()\n  all_col_c <- c()\n  all_col_l <- c()\n  \n  # -------------\n  # define noLabel colors\n  for (i in seq(1, length(lbs))) {\n    \n    if (grepl(""noLabel"", lbs[i])) {\n      all_col_h <- c(all_col_h, 120)\n      all_col_c <- c(all_col_c, 30)\n      all_col_l <- c(all_col_l, 90)\n      next\n    }\n    \n    # -------------\n    if (lbs[i] == ""wind_field_cp"") {\n      # dark blue\n      col_h <- 250\n      col_c <- 45\n      col_l <- 40\n    } else if (lbs[i] == ""wind_field"") {\n      # middle blue\n      col_h <- 250\n      col_c <- 45\n      col_l <- 60\n    } else if (lbs[i] == ""wind_field_noMF"") {\n      # light blue\n      col_h <- 250\n      col_c <- 65\n      col_l <- 80\n    } else if (lbs[i] == ""mass_field"") {\n      # light red\n      col_h <- 10\n      col_c <- 45\n      col_l <- 70\n    } else if (lbs[i] == ""mass_field_sx"") {\n      # darkred\n      col_h <- 10\n      col_c <- 45\n      col_l <- 40\n    } else {\n      stop(""  * Error: Cluster name does not exist \\n"")\n    }\n    \n    all_col_h <- c(all_col_h, col_h)\n    all_col_c <- c(all_col_c, col_c)\n    all_col_l <- c(all_col_l, col_l)\n  }\n  \n  cols <- c(hcl(all_col_h, all_col_c, all_col_l))\n  names(cols) <- lbs\n  \n  return(cols)\n}\n\n# same color but a little darker (better for parallel coordinate plots).\nmake_cols_type_darker <- function(lbs) {\n  # Returns a named vector of the hcl-colors for thunderstorm environments\n  \n  all_col_h <- c()\n  all_col_c <- c()\n  all_col_l <- c()\n  \n  # -------------\n  # define noLabel colors\n  for (i in seq(1, length(lbs))) {\n    \n    if (grepl(""noLabel"", lbs[i])) {\n      all_col_h <- c(all_col_h, 120)\n      all_col_c <- c(all_col_c, 30)\n      all_col_l <- c(all_col_l, 90)\n      next\n    }\n    \n    # -------------\n    if (lbs[i] == ""wind_field_cp"") {\n      # dark blue\n      col_h <- 250\n      col_c <- 80\n      col_l <- 30\n    } else if (lbs[i] == ""wind_field"") {\n      # middle blue\n      col_h <- 250\n      col_c <- 45\n      col_l <- 60\n    } else if (lbs[i] == ""wind_field_noMF"") {\n      # light blue\n      col_h <- 250\n      col_c <- 65\n      col_l <- 70\n    } else if (lbs[i] == ""mass_field"") {\n      # light red\n      col_h <- 10\n      col_c <- 60\n      col_l <- 65\n    } else if (lbs[i] == ""mass_field_sx"") {\n      # darkred\n      col_h <- 10\n      col_c <- 60\n      col_l <- 40\n    } else {\n      stop(""  * Error: Cluster name does not exist \\n"")\n    }\n    \n    all_col_h <- c(all_col_h, col_h)\n    all_col_c <- c(all_col_c, col_c)\n    all_col_l <- c(all_col_l, col_l)\n  }\n  \n  cols <- c(hcl(all_col_h, all_col_c, all_col_l))\n  names(cols) <- lbs\n  \n  return(cols)\n}\n\n# *************\n# Define symbols\n\nsym_environments <- c(""wind_field_cp"" = 1,\n               ""wind_field"" = 2,\n               ""wind_field_noMF"" = 3,\n               ""mass_field"" = 4,\n               ""mass_field_sx"" = 5)\n\n\n# *****************************************************************************\n# Plotting: Parallel coordinates plot of cluster means (Fig. 5)\n# *****************************************************************************\n\npdf(""rep_figure_05.pdf"", width = 7, height = 3.7)\npar(mar = c(.5, .5, .5, .5), oma = c(5, 4, 4, 2),\n    mgp = c(2, .6, 0), xpd = TRUE)\n\n# Empty plot\nmatplot(t(cm_scaled_ord), type = ""n"", ylim = c(-1.5, 2), xaxt = ""n"", las = 1,\n        ylab = """")\n\n# Gray vertical lines\nabline(v = seq(1, 25), col = ""gray"", lty = 1, xpd = FALSE)\n\n# Horizontal 0 line\nlines(x = c(par(\'usr\')[1], par(\'usr\')[2]), y = c(0, 0), lty = 1, lwd = 2)\n\n# Horizontal 0.3 lines\nlines(x = c(par(\'usr\')[1], par(\'usr\')[2]),\n      y = c(0.3, 0.3), lty = 2, lwd = 2, col = ""gray"")\nlines(x = c(par(\'usr\')[1], par(\'usr\')[2]),\n      y = c(-0.3, -0.3), lty = 2, lwd = 2, col = ""gray"")\n\n# Add curves\nmatplot(t(cm_scaled_ord), lwd = 2,\n        col = make_cols_type_darker(labels_ord),\n        type = ""o"", pch = sym_environments[labels_ord],\n        cex = 1, lty = 1, xaxt = ""n"", ylab = """", add = TRUE)\n\n# ylab\ntext(x = -3, y = 0, srt = 90, xpd = NA, label = ""Scaled Value"")\n\n# xlab\ntext(y = -2, x = seq(1, 25), labels = era5_lbl,\n     las = 1, srt = 45, adj = 1, xpd = NA)\n\n# Legend\nlegend(x = 0, y = 3,text.width = c(4.6, 3.3, 5.1, 3.3),\n  xpd = NA, horiz = TRUE, bty = ""n"", cex = .8,\n  legend = c(""Wind-field CP"", ""Wind-field"", ""Wind-field noMF"",\n             ""Mass-field"", ""Mass-field SX""),\n  pch = sym_environments, pt.cex = 1.3,  pt.lwd = 3,\n  col = make_cols_type_darker(names(sym_environments)))\n\n# Title\ntitle(sprintf(""Parallel coordinate plot: Cluster means (Domain %s)"", id),\n      line = 1.8, outer = TRUE)\n\ndev.off()\n\n# For simplicity, the category annotations are omitted.\n# They can be added analogue to Figure 3.\n\n\n# *****************************************************************************\n# Plotting: Barplot, Seasonal variation of thunderstorm environments (Fig. 6)\n# *****************************************************************************\n\n# *************\n# Preparation\n\n# Calculations\ncluster_name <- sprintf(""cluster_k%02d_label"", k)\ntab <- table(obs$season, obs[[cl_name]])\ntab <- tab[c(""winter"", ""spring"", ""summer"", ""fall""), clusters_ord]\n\n# Look at the table\ntab\n\n# Remove column names and row names because we make our own labels and legend\ncolnames(tab) <- rep(\'\', length(colnames(tab)))\nrownames(tab) <- rep(\'\', length(rownames(tab)))\n\n# *************\n# Plot\n\npdf(""rep_figure_06.pdf"", width = 6, height = 5)\npar(mar = c(5, 2, 4, 9), xpd = TRUE, lheight = 0.8)\n\n# Bars\nmosaicplot(tab, color = make_cols_type(labels_ord),\n    xlab = """", ylab = """", cex = 1.4, off = c(8, 0), las = 2, main = """")\n\n# x labels\ntext(x = c(0.2, 0.45, 0.7, 0.95), y = -0.1,\n     labels = c(""Winter"", ""Spring"", ""Summer"", ""Fall""),\n     srt = 45, cex = 1.2, pos = 2)\n\n# y annotation\ntext(x = 0.05, y = c(0, .25, .5, .75, 1, 1.11) - .045,\n     labels = c(paste(as.character(seq(0, 100, 25)), "" -""), ""%   ""),\n     cex = 1, pos = 2, xpd = NA)\n\n# Legend\nlegend(x = 1.04, y = .7, xpd = NA, pch = 15, pt.cex = 2, bty = ""n"",\n       col = make_cols_type(rev(names(sym_environments))), \n       legend = c(""Mass-field SX"", ""Mass-field"", ""Wind-field noMF"",\n                  ""Wind-field"", ""Wind-field CP""))\n\n# Title\ntitle(sprintf(""Seasonal variation of thunderstorm environments (Domain %s)"", id ),\n      adj = 0.01, cex.main = 1.1)\n\ndev.off()\n\n\n# *****************************************************************************\n# Plotting: PCA of cluster means (Fig. 8)\n# *****************************************************************************\n\n# *************\n# Preparation\n\n# Transform and scale\ncm_all_t      <- t(cluster_means_original)             # transpose\ncm_all_trafo  <- sign(cm_all_t) * sqrt(abs(cm_all_t))  # transform\ncm_all_scaled <- scale(cm_all_trafo)                   # scale\n\n# Get colors\ncol_cm_all <- make_cols_type(sub(\'.\',\'\', colnames(cluster_means_original)))\n\n# Calculate PCA and get loadings\npca_cluMeans <- prcomp(cm_all_scaled)\nloadings <- pca_cluMeans$rotation[, c(1:2)]\n\n# Scale loadings to have them on the same axis as PC1 & 2\n# Formula based on getAnywhere(biplot.prcomp)\nscale_factor <- 1.2\nloadingsS <- loadings\nlambda1 <- pca_cluMeans$sdev[1] * sqrt(length(pca_cluMeans$x[,1])) ^ scale_factor\nlambda2 <- pca_cluMeans$sdev[2] * sqrt(length(pca_cluMeans$x[,2])) ^ scale_factor\nloadingsS[,1] <- loadings[,1] * lambda1\nloadingsS[,2] <- loadings[,2] * lambda2\n\n# How much variance is explained by the first two PC?\npc1_cluMeans <- round(summary(pca_cluMeans)$importance[2, 1] * 100, digits = 2)\npc2_cluMeans <- round(summary(pca_cluMeans)$importance[2, 2] * 100, digits = 2)\n\n# *************\n# Plot\n\npdf(""rep_figure_08.pdf"", width = 11, height = 6)\npar(mar = c(3, 3, 3, 3) + 2, xpd = FALSE)\n\n# Empty plot\nplot(pca_cluMeans$x[,1], pca_cluMeans$x[,2],\n     type = ""n"", xlim = c(-13, 15), ylim = c(-7, 8), las = 1,\n     xlab = paste(""PC 1 ("", pc1_cluMeans, ""%)""),\n     ylab = paste(""PC 2 ("", pc2_cluMeans, ""%)""),\n     main = ""Principal component analysis on all cluster means""\n)\n\n# Add loadings (arrows)\narrows(0, 0, x1 = loadingsS[,1], y1 = loadingsS[,2], length = .1,\n       col = ""darkgray"")\n\n# Add lines (x = 0 , y = 0)\nabline(v = 0, col = ""darkgray"", lty = 2)\nabline(h = 0, col = ""darkgray"", lty = 2)\n\n# Add colored circles\npoints(pca_cluMeans$x[,1], pca_cluMeans$x[,2],\n       col = adjustcolor(col_cm_all, alpha.f = 0.9),\n       pch = 16, cex = 5, cex.lab = 1.7, cex.axis = 1.3, las = 1, lwd = 2)\n\n# Label colored circles\n# Many labels are overlapping. For the paper the position of the labels are\n# manually tweaked.\ntext(x = pca_cluMeans$x[,1], y = pca_cluMeans$x[,2],\n     labels = rep(LETTERS[1:12], each = 3), col = ""black"", xpd = TRUE)\n\n# Label loadings (arrows)\n# Many labels are overlapping. For the paper the position of the labels are\n# manually tweaked.\ntext(loadingsS, labels = era5_lbl,\n     cex = .9, pos = 4, offset = 0.3, col = ""lightsteelblue4"", xpd = TRUE)\n\n# Legend\nlegend(""bottomleft"", bty = ""n"", pch = 16, pt.cex = 2,\n       legend = c(""Wind-field CP"", ""Wind-field"", ""Wind-field noMF"",\n                  ""Mass-field"", ""Mass-field SX""),\n       col = make_cols_type(c(""wind_field_cp"", ""wind_field"", ""wind_field_noMF"",\n                              ""mass_field"", ""mass_field_sx"")))\n\ndev.off()\n\n\n##############################################################################\n# Troubleshooting\n##############################################################################\n\n# In case you have problems with this script, compare your sessionInfo() with\n# the one in which this script was written:\n\n# R version 4.2.2 (2022-10-31)\n# Platform: x86_64-pc-linux-gnu (64-bit)\n# Running under: Debian GNU/Linux 11 (bullseye)\n# \n# Matrix products: default\n# BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3\n# LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.13.so\n# \n# locale:\n#   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n# [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n# [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n# [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n# [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n# [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n# \n# attached base packages:\n#   [1] stats     graphics  grDevices utils     datasets  methods   base     \n# \n# other attached packages:\n#   [1] colorspace_2.0-3  data.table_1.14.2\n# \n# loaded via a namespace (and not attached):\n#   [1] Rcpp_1.0.8.3        pillar_1.8.1        compiler_4.2.2     \n# [4] class_7.3-20        tools_4.2.2         digest_0.6.29      \n# [7] evaluate_0.16       lifecycle_1.0.3     tibble_3.1.8       \n# [10] lattice_0.20-45     pkgconfig_2.0.3     rlang_1.0.6        \n# [13] DBI_1.1.2           cli_3.4.1           rstudioapi_0.13    \n# [16] yaml_2.3.5          xfun_0.31           fastmap_1.1.0      \n# [19] e1071_1.7-9         dplyr_1.0.9         knitr_1.39         \n# [22] generics_0.1.2      vctrs_0.5.1         classInt_0.4-3     \n# [25] grid_4.2.2          tidyselect_1.1.2    glue_1.6.2         \n# [28] sf_1.0-9            R6_2.5.1            rnaturalearth_0.1.0\n# [31] fansi_1.0.3         rmarkdown_2.14      sp_1.4-7           \n# [34] purrr_0.3.4         magrittr_2.0.3      htmltools_0.5.2    \n# [37] units_0.8-0         assertthat_0.2.1    utf8_1.2.2         \n# [40] KernSmooth_2.23-20  proxy_0.4-26\n\n# *************\n# Uncomment to get your sessionInfo.\n# sessionInfo()\n\n']","Supplementary material: Thunderstorm Environments in Europe Supplementary material for the paper ""Thunderstorm Environments in Europe"".- R code to reproduce the core analysis (reproducing_calculations.R)- Data for each domain (data_domain*.csv)- csv-files of the two tables in the paper (means_clusters.csv and means_domains.csv).- Domain definitions (domains.csv).- A precise variable description (era5_vars.csv)- README for more information.Summary of the corresponding paper: Two thunderstorm environments are described for Europe: Mass-field thunderstorms, which occur mostly in summer, over land, and under similar meteorological conditions, and wind-field thunderstorms, which occur mostly in winter, over the sea, and under more diverse meteorological conditions. Our descriptions are independent of static thresholds and help to understand why thunderstorms in unfavorable seasons for lightning pose a particular risk to tall infrastructure such as wind turbines.Link to the preprint: https://egusphere.copernicus.org/preprints/2023/egusphere-2022-1453/",4
Gaps and complex structurally variant loci in phased genome assemblies,Supplementary data including code for an science article 'Gaps and complex structurally variant loci in phased genome assemblies'.,"['#\' Load a de novo assembly aligned to the reference in PAF format into a \\code{\\link{GRanges-class}} object.\n#\' \n#\' This function will take a PAF file of contig alignments to a reference genome and converts them \n#\' into a \\code{\\link{GRanges-class}} object.\n#\'\n#\' @param paf.file A BED file of contig alignments to a reference genome.\n#\' @param min.ctg.size A minimum length a final contig after gaps are collapsed.\n#\' @param report.ctg.ends Set to \\code{TRUE} if aligned position of each contig ends should be reported\n#\' @param min.ctg.ends A minimum length of alignment to be considered when reporting contig end alignments.\n#\' @return A \\code{\\link{GRanges-class}} object.\n#\' @import GenomicRanges\n#\' @import GenomeInfoDb\n#\' @author David Porubsky\n#\' @export\n#\' \npaf2ranges <- function(paf.file=NULL, index=NULL, min.mapq=10, min.aln.width=10000, min.ctg.size=500000, report.ctg.ends=FALSE, min.ctg.ends=50000) {\n  ## Get total processing time\n  #ptm <- proc.time()\n  \n  if (file.exists(paf.file)) {\n    ptm <- startTimedMessage(""\\nLoading PAF file: "", paf.file)\n    paf <- utils::read.table(paf.file, stringsAsFactors = FALSE, comment.char = \'&\')\n    ## Keep only first 12 columns\n    paf <- paf[,c(1:12)]\n    ## Add header\n    header <- c(\'q.name\', \'q.len\', \'q.start\', \'q.end\', \'strand\', \'t.name\', \'t.len\', \'t.start\', \'t.end\', \'n.match\', \'aln.len\', \'mapq\') \n    colnames(paf) <- header\n    stopTimedMessage(ptm)\n  } else {\n    stop(paste0(""PAF file "", bedfile, "" doesn\'t exists !!!""))\n  }  \n  ## Filter by mapping quality\n  if (min.mapq > 0) {\n    ptm <- startTimedMessage(""    Keeping alignments of min.mapq: "", min.mapq)\n    paf <- paf[paf$mapq >= min.mapq,]\n    stopTimedMessage(ptm)\n  }\n  if (nrow(paf) == 0) {\n    stop(""None of the PAF alignments reach user defined mapping quality (min.mapq) !!!"")\n  }\n  ## Convert data.frame to GRanges object\n  #paf.gr <- GenomicRanges::makeGRangesFromDataFrame(paf, keep.extra.columns = FALSE, seqnames.field = \'q.name\', start.field = \'q.start\', end.field = \'q.end\')\n  paf.gr <- GenomicRanges::makeGRangesFromDataFrame(paf, keep.extra.columns = FALSE, seqnames.field = \'t.name\', start.field = \'t.start\', end.field = \'t.end\')\n  mcols(paf.gr) <- paf[,c(\'q.len\', \'t.len\', \'n.match\', \'aln.len\', \'mapq\', \'q.name\')]\n  names(paf.gr) <- NULL\n  #paf.gr$target.gr <- GenomicRanges::GRanges(seqnames=paf$t.name, ranges=IRanges::IRanges(start=paf$t.start, end=paf$t.end), \'q.name\'= paf$q.name)\n  paf.gr$query.gr <- GenomicRanges::GRanges(seqnames=paf$q.name, ranges=IRanges::IRanges(start=paf$q.start, end=paf$q.end), \'t.name\'= paf$t.name)\n  ## Add index if defined\n  if (!is.null(index) & is.character(index)) {\n    paf.gr$ID <- index\n  }\n  ## Ignore strand\n  #GenomicRanges::strand(paf.gr) <- \'*\'\n  ## Filter out small alignments\n  if (min.aln.width > 0) {\n    ptm <- startTimedMessage(""    Keeping alignments of min width: "", min.aln.width, \'bp\')\n    #paf.gr <- paf.gr[width(paf.gr) >= min.aln.width]\n    paf.gr <- paf.gr[paf.gr$aln.len >= min.aln.width]\n    stopTimedMessage(ptm)\n  }\n  if (length(paf.gr) == 0) {\n    stop(""None of the PAF alignments reach user defined alignment size (min.aln.width !!!"")\n  }\n  ## Filter out small contig sizes\n  if (min.ctg.size > 0) {\n    ptm <- startTimedMessage(""    Keeping contigs of min size: "", min.ctg.size, \'bp\')\n    paf.gr <- paf.gr[paf.gr$q.len >= min.ctg.size]\n    stopTimedMessage(ptm)\n  }\n  ## Keep only seqlevels that remained after data filtering\n  if (length(paf.gr) > 0) {\n    paf.gr <- GenomeInfoDb::keepSeqlevels(paf.gr, value = unique(as.character(GenomeInfoDb::seqnames(paf.gr))))\n    paf.gr$query.gr <- GenomeInfoDb::keepSeqlevels(paf.gr$query.gr, value = unique(as.character(GenomeInfoDb::seqnames(paf.gr$query.gr))))\n  } else {\n    stop(""None of the PAF alignments reach user defined contig size (min.ctg.size) !!!"")\n  }\n  \n  if (report.ctg.ends == TRUE) {\n    ptm <- startTimedMessage(""    Reporting end positions of each contig"")\n    #paf.grl <- split(paf.gr, GenomeInfoDb::seqnames(paf.gr))\n    paf.grl <- split(paf.gr, GenomeInfoDb::seqnames(paf.gr$query.gr))\n    to.collapse <- which(lengths(paf.grl) > 1)\n    #ctg.ends.grl <- S4Vectors::endoapply( paf.grl[to.collapse], function(x) range(sort(x, ignore.strand=TRUE)$target.gr) )\n    \n    ## Helper function\n    gr2ranges <- function(gr, min.ctg.ends=0, allowed.ctg.length=0.05) {\n      gr <- GenomeInfoDb::keepSeqlevels(gr, value = unique(as.character(seqnames(gr))))\n      q.name <- unique(gr$q.name)\n      if (length(q.name) > 1) {\n        stop(""Submitted GRanges object contains more than one \'q.name\', not allowed !!!"")\n      }\n      if (min.ctg.ends > 0) {\n        gr <- gr[gr$aln.len >= min.ctg.ends]\n      }\n      if (length(gr) > 0) {\n        ## Order by contig alignments\n        gr <- gr[order(gr$query.gr)]\n        if (length(gr) > 1) {\n          ## Report target ranges corresponding to the contig ends\n          gr.ends <- gr[c(1, length(gr))]\n          ## Report genomic range for contig ends mapping\n          gr.range <- range(gr.ends, ignore.strand=TRUE)\n          ## Get best contiguous alignment ##\n          ## Report target sequence with the highest number of continuously aligned bases\n          target2keep <- names(which.max(sum(split(width(gr), seqnames(gr)))))\n          gr.contig <- gr[seqnames(gr) == target2keep]\n          gr.contig <- gr.contig[,0]\n          strand(gr.contig) <- \'*\'\n          gr.gaps <- gaps(gr.contig, start = min(start(gr.contig)))\n          if (length(gr.gaps) > 0) {\n            ## Remove gaps longer than query(contig) length\n            q.len <- unique(gr$q.len)\n            ## Remove gaps longer than total alignment length\n            #aln.len <- sum(gr$aln.len)\n            ## Adjust allowed contig length based on fraction allowed to be added to the contigs size\n            if (!is.null(allowed.ctg.length)) {\n              if (allowed.ctg.length > 0) {\n                q.len <- q.len + (q.len * allowed.ctg.length)\n                #aln.len <- aln.len + (aln.len * allowed.ctg.length)\n              }\n            }\n            gr.gaps <- gr.gaps[width(gr.gaps) < q.len]\n            #gr.gaps <- gr.gaps[width(gr.gaps) < aln.len]\n            ## Keep only gaps that together with contig length are no longer than query(contig) length\n            ctg.size <- sum(width(reduce(gr.contig)))\n            gap.size <- sum(width(reduce(gr.gaps)))  \n            while ((ctg.size + gap.size) > q.len & length(gr.gaps) > 0) {\n              #while ((ctg.size + gap.size) > aln.len & length(gr.gaps) > 0) {  \n              ## Remove longest gap\n              gr.gaps <- gr.gaps[-which.max(width(gr.gaps))]\n              gap.size <- sum(width(gr.gaps))\n            }\n          }  \n          ## Allow only gaps that are no longer than smallest alignment\n          #gr.gaps <- gr.gaps[width(gr.gaps) <= min(gr$aln.len)]\n          ## Collapse contiguous ranges\n          gr.contig <- reduce(c(gr.contig, gr.gaps))\n          ## Add contig names and split alignment ids\n          if (length(gr.contig) > 1) {\n            gr.contig$q.name <- q.name\n            aln <- paste0(\'p\', 1:length(gr.contig))\n            gr.contig$aln <- aln\n          } else {\n            gr.contig$q.name <- q.name\n            gr.contig$aln <- \'single\'\n          }  \n          #gr.contig <- gr.contig[which.max(width(gr.contig))]\n          # gr.ranges <- range(gr[order(gr$query.gr)], ignore.strand=TRUE)\n          # if (report.longest.range) {\n          #   return(gr.ranges[which.max(width(gr.ranges))])\n          # } else {\n          #   return(gr.ranges)\n          # }\n          gr.contig$ends <- paste(as.character(gr.range), collapse = \';\')\n          #gr.range$longest.aln <- gr.contig\n        } else {\n          #gr.range <- gr[,0]\n          #strand(gr.range) <- \'*\'\n          #gr.range$longest.aln <- gr.range\n          gr.contig <- gr[,0]\n          strand(gr.contig) <- \'*\'\n          gr.contig$q.name <- q.name\n          gr.contig$aln <- \'single\'\n          gr.contig$ends <- paste(as.character(gr.contig), collapse = \';\')\n        }  \n      } else {\n        #gr.range <- GRanges()\n        gr.contig <- GRanges()\n      }  \n      #return(gr.range)\n      return(gr.contig)\n    }\n    \n    #ctg.ends.grl <- S4Vectors::endoapply( paf.grl[to.collapse], function(x) range(x[order(x$query.gr)], ignore.strand=TRUE) )\n    \n    ## Report contigs alignment ranges only for alignments of a certain size (default: 50kb)\n    if (min.ctg.ends > 0) {\n      ctg.ends.grl <- S4Vectors::endoapply( paf.grl[to.collapse], \n                                            function(x) gr2ranges(gr = x, min.ctg.ends = min.ctg.ends) )  \n    } else {\n      ctg.ends.grl <- S4Vectors::endoapply( paf.grl[to.collapse], \n                                            function(x) gr2ranges(gr = x) )\n    }\n    \n    simple.ends.gr <- unlist(paf.grl[-to.collapse], use.names = FALSE)\n    #simple.ends <- as.character(simple.ends.gr$target.gr)\n    simple.ends <- as.character(simple.ends.gr)\n    #names(simple.ends) <- unique(as.character(GenomeInfoDb::seqnames(simple.ends.gr)))\n    names(simple.ends) <- unique(as.character(GenomeInfoDb::seqnames(simple.ends.gr$query.gr)))\n    \n    simple.gr <- simple.ends.gr[,0]\n    strand(simple.gr) <- \'*\'\n    #simple.gr$longest.aln <- simple.gr\n    simple.gr$q.name <- as.character(seqnames(simple.ends.gr$query.gr))\n    simple.gr$aln <- \'single\'\n    simple.gr$ends <- as.character(simple.gr)\n    names(simple.gr) <- NULL\n    \n    split.ends <- as.character(unlist(ctg.ends.grl))\n    split.ends.l <- split(split.ends, names(split.ends))\n    split.ends <- sapply(split.ends.l, function(x) paste(x, collapse = \';\'))\n    \n    split.ends.gr <- unlist(ctg.ends.grl)\n    #split.ends.gr$q.name <- names(split.ends.gr)\n    names(split.ends.gr) <- NULL\n    \n    #ctg.ends <- c(simple.ends, split.ends)\n    #paf.gr$ctg.end.pos <- ctg.ends[match(as.character(seqnames(paf.gr)), names(ctg.ends))]\n    #paf.gr$ctg.end.pos <- ctg.ends[match(as.character(seqnames(paf.gr$query.gr)), names(ctg.ends))]\n    \n    ends.gr <- sort(c(simple.gr, split.ends.gr))\n    ends.gr$ID <- unique(paf.gr$ID)\n    \n    stopTimedMessage(ptm)\n    return(list(\'ctg.aln\'=paf.gr, \'ctg.ends\'=ends.gr))\n  } else {\n    return(list(\'ctg.aln\'=paf.gr, \'ctg.ends\'=NULL))\n  }\n  \n  ## Report total processing time\n  #time <- proc.time() - ptm\n  #message(""Total time: "", round(time[3],2), ""s"")\n  \n  #return(paf.gr)\n} \n\n#\' This function will take in a \\code{\\link{GRanges-class}} or \\code{\\link{GRangesList-class}} object of alignments of a \n#\' single contig to a reference and collapses alignment gaps based user specified maximum allowed gap.\n#\'\n#\' @param ranges A \\code{\\link{GRanges-class}} or \\code{\\link{GRangesList-class}} object of regions of a single or multiple contigs aligned to a reference.\n#\' @param max.gap A maximum length of a gap within a single contig alignments to be collapsed.\n#\' @return A \\code{\\link{GRanges-class}} object.\n#\' @author David Porubsky\n#\' @export\n#\' \ncollapseGaps <- function(ranges, max.gap=100000) {\n  ## Helper function definition\n  fillGaps <- function(gr, max.gap=100000) {\n    gr <- GenomeInfoDb::keepSeqlevels(gr, value = as.character(unique(GenomeInfoDb::seqnames(gr))), pruning.mode = \'coarse\')\n    gr <- GenomicRanges::sort(gr)\n    gap.gr <- GenomicRanges::gaps(gr, start = min(start(gr)))\n    gap.gr <- gap.gr[width(gap.gr) <= max.gap]\n    if (length(gap.gr) > 0) {\n      red.gr <- GenomicRanges::reduce(c(gr[,0], gap.gr))\n      GenomicRanges::mcols(red.gr) <- GenomicRanges::mcols(gr)[length(gr),]\n    } else {\n      red.gr <- GenomicRanges::reduce(gr)\n      GenomicRanges::mcols(red.gr) <- GenomicRanges::mcols(gr)[length(gr),]\n    }\n    return(red.gr)\n  }\n  \n  if (class(ranges) == ""CompressedGRangesList"") {\n    ## Process only contigs with split alignments\n    to.fill <- which(lengths(ranges) > 1)\n    red.ranges <-  suppressWarnings( S4Vectors::endoapply(ranges[to.fill], function(gr) fillGaps(gr=gr, max.gap=max.gap)) )\n    red.ranges <- unlist(red.ranges, use.names = FALSE)\n    ## Add ranges with no split alignment\n    red.ranges <- c(unlist(ranges[-to.fill], use.names = FALSE), red.ranges)\n  } else if (class(ranges) == ""GRanges"") {\n    red.ranges <- fillGaps(gr=ranges, max.gap = max.gap)\n  } else {\n    stop(""Only objects of class \'GRanges\' or \'GRangesList\' are allowed as input for parameter \'ranges\' !!!"")\n  }\n  return(red.ranges)\n}\n\n#\' This function will take in a \\code{\\link{GRanges-class}} or \\code{\\link{GRangesList-class}} object of genomic ranges of a \n#\' single a contig aligned to a reference and reports all alignment gaps.\n#\'\n#\' @param id.col A column number from the original \\code{\\link{GRanges-class}} object to be reported as an unique ID.\n#\' @inheritParams collapseGaps\n#\' @return A \\code{\\link{GRanges-class}} object.\n#\' @author David Porubsky\n#\' @export\n#\' \nreportGaps <- function(ranges, id.col=NULL) {\n  ## Helper function definitions\n  getGaps <- function(gr=NULL) {\n    gap.gr <- GenomicRanges::gaps(gr, start = min(start(gr)))\n    gap.gr <- gap.gr[GenomicRanges::strand(gap.gr) == \'*\']\n    return(gap.gr)\n  }  \n  \n  processGaps <- function(gr, id.col=NULL) {\n    ## Make sure only seqlevels present in the submitted ranges are kept\n    gr <- GenomeInfoDb::keepSeqlevels(gr, value = as.character(unique(GenomeInfoDb::seqnames(gr))), pruning.mode = \'coarse\')\n    ## Sort ranges by position\n    GenomicRanges::strand(gr) <- \'*\'\n    gr <- GenomicRanges::sort(gr)\n    ## Make sure rows are not named\n    names(gr) <- NULL\n    ## Keep only ranges with the same seqnames\n    if (length(GenomeInfoDb::seqlevels(gr)) > 1) {\n      max.seqname <- GenomeInfoDb::seqlevels(gr)[which.max(S4Vectors::runLength(GenomeInfoDb::seqnames(gr)))]\n      gr <- gr[GenomeInfoDb::seqnames(gr) == max.seqname]\n      warning(""Multiple \'seqlevels\' present in submitted ranges, keeping only ranges for: "", max.seqname)\n    }\n    ## TODO for alignment landing on different chromosomes report gap == 0 and both alignments\n    if (length(gr) > 1) {\n      ## Calculate gaps\n      #gap.gr <- GenomicRanges::gaps(gr, start = min(start(gr)))\n      #gap.gr <- gap.gr[strand(gap.gr) == \'*\']\n      gap.gr <- getGaps(gr)\n      \n      ## Add ID column from the original gr object if defined\n      if (!is.null(id.col)) {\n        if (id.col > 0 & ncol(GenomicRanges::mcols(gr)) >= id.col) {\n          GenomicRanges::mcols(gap.gr) <- rep(unique(GenomicRanges::mcols(gr)[id.col]), length(gap.gr))\n        } else {\n          warning(""User defined \'id.col\' number is larger the then total number of columns in input \'gr\', skipping adding id column ..."")\n        }\n      }\n            \n      ## Report alignment on each side of the gap\n      if (length(gap.gr) > 0) {\n        ## Report upstream and downstream target ranges\n        ## Upstream\n        up.idx <- IRanges::follow(gap.gr, gr)\n        up.idx.keep <- !is.na(up.idx)\n        gap.gr$up.gr <- GenomicRanges::GRanges(seqnames=GenomeInfoDb::seqnames(gap.gr), \n                                               ranges=IRanges::IRanges(start=GenomicRanges::start(gap.gr), end=GenomicRanges::start(gap.gr)))\n        gap.gr$up.gr[up.idx.keep] <- gr[up.idx[up.idx.keep]][,0]\n        ## Downstream\n        down.idx <- IRanges::precede(gap.gr, gr)\n        down.idx.keep <- !is.na(down.idx)\n        gap.gr$down.gr <- GenomicRanges::GRanges(seqnames=GenomeInfoDb::seqnames(gap.gr), \n                                                 ranges=IRanges::IRanges(start=GenomicRanges::end(gap.gr), end=GenomicRanges::end(gap.gr)))\n        gap.gr$down.gr[down.idx.keep] <- gr[down.idx[down.idx.keep]][,0]\n        \n        ## Report upstream and downstream query ranges and gaps if defined\n        if (\'query.gr\' %in% names(mcols(gr)) & class(gr$query.gr) == \'GRanges\') {\n          ## Upstream\n          gap.gr$query.up.gr <- GenomicRanges::GRanges(seqnames=GenomeInfoDb::seqnames(gap.gr), \n                                                       ranges=IRanges::IRanges(start=GenomicRanges::start(gap.gr), end=GenomicRanges::start(gap.gr)))\n          suppressWarnings( gap.gr$query.up.gr[up.idx.keep] <- gr$query.gr[up.idx[up.idx.keep]][,0] )\n          ## Downstream\n          gap.gr$query.down.gr <- GenomicRanges::GRanges(seqnames=GenomeInfoDb::seqnames(gap.gr), \n                                                         ranges=IRanges(start=GenomicRanges::end(gap.gr), end=GenomicRanges::end(gap.gr)))\n          suppressWarnings( gap.gr$query.down.gr[down.idx.keep] <- gr$query.gr[down.idx[down.idx.keep]][,0] )\n          ## Calculate gaps\n          gap.grl <- split(gap.gr, 1:length(gap.gr))\n          query.gap.grl <- S4Vectors::endoapply(gap.grl, function(x) getGaps(c(x$query.up.gr, x$query.down.gr)))\n          query.gap.gr <- unlist(query.gap.grl, use.names = FALSE)\n          ## Initialize gap ranges\n          gap.gr$query.gap.gr <- GenomicRanges::GRanges(seqnames=rep(\'unknown\', length(gap.gr)), \n                                                        ranges=IRanges::IRanges(start=1, end=1))\n          if (length(query.gap.gr) > 0) {\n            gap.idx <- which(lengths(query.gap.grl) > 0)\n            gap.gr$query.gap.gr[gap.idx] <- query.gap.gr\n          }\n        }\n        return(gap.gr)\n      } else {\n        ## Report upstream and downstream query ranges and gaps if defined\n        if (\'query.gr\' %in% names(mcols(gr)) & class(gr$query.gr) == \'GRanges\') {\n          dummy.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1), ID=\'dummy\')\n          dummy.gr$up.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n          dummy.gr$down.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n          dummy.gr$query.up.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n          dummy.gr$query.down.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n          dummy.gr$query.gap.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n        } else {\n          dummy.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1), ID=\'dummy\')\n          dummy.gr$up.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n          dummy.gr$down.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n        }  \n        return(dummy.gr)\n      }\n    } else {\n      ## Report upstream and downstream query ranges and gaps if defined\n      if (\'query.gr\' %in% names(mcols(gr)) & class(gr$query.gr) == \'GRanges\') {\n        dummy.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1), ID=\'dummy\')\n        dummy.gr$up.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n        dummy.gr$down.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n        dummy.gr$query.up.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n        dummy.gr$query.down.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n        dummy.gr$query.gap.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n      } else {\n        dummy.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1), ID=\'dummy\')\n        dummy.gr$up.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n        dummy.gr$down.gr <- GRanges(seqnames = \'dummy\', ranges = IRanges(start=1, end=1))\n      }  \n      return(dummy.gr)\n      #return(GRanges())\n    }\n  }  \n  \n  if (class(ranges) == ""CompressedGRangesList"") {\n    ## Keep only contigs with split alignments\n    ptm <- startTimedMessage(""Reporting gaps"")\n    \n    ranges <- ranges[lengths(ranges) > 1]\n    gaps <-  suppressWarnings( S4Vectors::endoapply(ranges, function(gr) processGaps(gr=gr, id.col=id.col)) )\n    gaps <- unlist(gaps, use.names = FALSE)\n    ## Remove empty ranges\n    gaps <- gaps[seqnames(gaps) != \'dummy\']\n    #gaps <-  suppressWarnings( S4Vectors::lapply(ranges, function(gr) processGaps(gr=gr, id.col=id.col)) )\n    #do.call(c, gaps)\n    \n    stopTimedMessage(ptm)\n  } else if (class(ranges) == ""GRanges"") {\n    ptm <- startTimedMessage(""Reporting gaps"")\n    \n    gaps <- processGaps(gr=ranges, id.col=id.col)\n    \n    stopTimedMessage(ptm)\n  } else {\n    stop(""Only objescts of class \'GRanges\' or \'GRangesList\' are allowed as input for parameter \'ranges\' !!!"")\n  }\n  return(gaps)\n}\n\n\n#\' This function will take in a \\code{\\link{GRanges-class}} or \\code{\\link{GRangesList-class}} object of genomic ranges of a \n#\' single a contig aligned to a reference and reports coverage of regions that overlaps each other. \n#\'\n#\' @inheritParams collapseGaps\n#\' @inheritParams reportGaps\n#\' @return A \\code{\\link{GRanges-class}} object.\n#\' @author David Porubsky\n#\' @export\n#\' \nreportContigCoverage <- function(ranges, id.col=NULL) {\n  ## Helper function definitions\n  processContigCoverage <- function(gr=NULL, id.col=NULL) {\n    ## Get disjoin ranges\n    gr.disjoint <- GenomicRanges::disjoin(gr, ignore.strand=TRUE)\n    ## Add ID column from the original gr object if defined\n    if (!is.null(id.col)) {\n      if (id.col > 0 & ncol(GenomicRanges::mcols(gr)) >= id.col) {\n        GenomicRanges::mcols(gr.disjoint) <- rep(unique(GenomicRanges::mcols(gr)[id.col]), length(gr.disjoint))\n      } else {\n        warning(""User defined \'id.col\' number is larger the then total number of columns in input \'gr\', skipping adding id column ..."")\n      }\n    }\n    ## Report coverage of each disjoint range\n    gr.disjoint$cov <- IRanges::countOverlaps(gr.disjoint, gr)\n    ## Export final ranges with coverage\n    return(gr.disjoint)\n  }\n  \n  if (class(ranges) == ""CompressedGRangesList"") {\n    covs <-  suppressWarnings( S4Vectors::endoapply(ranges, function(gr) processContigCoverage(gr=gr, id.col=id.col)) )\n    covs <- unlist(covs, use.names = FALSE)\n  } else if (class(ranges) == ""GRanges"") {\n    covs <- processContigCoverage(gr=ranges, id.col=id.col)\n  } else {\n    stop(""Only objects of class \'GRanges\' or \'GRangesList\' are allowed as input for parameter \'ranges\' !!!"")\n  }\n  return(covs)\n}\n\n#\' This function will take in a \\code{\\link{GRanges-class}} object of genomic ranges and enumerate the number of overlapping bases\n#\' with other set of user defined \'query\' genomic ranges.\n#\'\n#\' @param ranges A \\code{\\link{GRanges-class}} object of genomic regions to get the number of overlapping bases with \'query.ranges\'.\n#\' @param query.ranges A \\code{\\link{GRanges-class}} object of genomic regions for which one want to report overlaps. \n#\' @param index A user defined name of a column where the number of overlapping bases between \'ranges\' and \'query.ranges\' will be reported.\n#\' @return A \\code{\\link{GRanges-class}} object.\n#\' @author David Porubsky\n#\' @export\n#\'\nreportOverlapBases <- function(ranges=NULL, query.ranges=NULL, index=NULL) {\n  if (!is.null(query.ranges) & !is.null(ranges)) {\n    if (class(ranges) == ""GRanges"" & class(query.ranges) == ""GRanges"") {\n      ## Get disjoint ranges between query and user.defined set of ranges\n      disj.gr <- suppressWarnings( GenomicRanges::disjoin(c(ranges[,0], query.ranges[,0])) )\n      ## Get disjoint ranges that are overlapping with query.ranges\n      disj.query.gr <- IRanges::subsetByOverlaps(disj.gr, query.ranges)\n      ## Get disjoint ranges overlapping with ranges of interest\n      disj.query.roi.gr <- IRanges::subsetByOverlaps(disj.query.gr, ranges)\n      ## Split by regions of interest\n      hits <- IRanges::findOverlaps(ranges, disj.query.roi.gr)\n      disj.query.roi.grl <- split(disj.query.roi.gr[S4Vectors::subjectHits(hits)], S4Vectors::queryHits(hits))\n      query.bases <- sapply(disj.query.roi.grl, function(gr) sum(width(reduce(gr))))\n      ## Add overlapping bases counts\n      if (!is.null(index) & nchar(index) > 0) {\n        new.col.idx <- ncol(GenomicRanges::mcols(ranges)) + 1\n        GenomicRanges::mcols(ranges)[new.col.idx] <- 0\n        colnames(GenomicRanges::mcols(ranges))[new.col.idx] <- index\n        GenomicRanges::mcols(ranges)[new.col.idx][unique(S4Vectors::queryHits(hits)),] <- query.bases\n      } else {\n        ranges$query.bases <- 0\n        ranges$query.bases[unique(S4Vectors::queryHits(hits))] <- query.bases\n      } \n    }\n  }\n  return(ranges)\n}  \n      \n']",Gaps and complex structurally variant loci in phased genome assemblies Supplementary data including code for an science article 'Gaps and complex structurally variant loci in phased genome assemblies'.,4
Extracting terms concerning AI based on Web of Science data,This is the accompanied code to extract terms connected to AI through titles and abstracts from Web of Science data.,"['library(readr)\nlibrary(stringi)\nlibrary(stringr)\nlibrary(textstem)\nlibrary(koRpus)\nlibrary(qdapDictionaries)\nlibrary(tokenizers)\nlibrary(stringdist)\n\n#### 1 Data collection\n# data collected: 12/7/2017\n# 29,195 papers downloaded from WoS by searching ""artificial intelligence"" without any other criteria\n# Read data and merge into a single file\n\nfile <- list.files(path = ""./ai_data"",\n                   pattern = ""\\\\.csv"")\ndata <- data.frame(matrix(vector(), 0, 66),\n                   stringsAsFactors=F)\n\nfor (i in 1:length(file)) {\n  data.1 <- read.csv(file = paste(""./ai_data/"",\n                                  file[i],\n                                  sep = """"),\n                     stringsAsFactors = F)\n  data <- rbind(data, data.1)\n}\n\ndata <- data[which(duplicated(data$UT) == F),] # 29,184\ndata <- data[which(data$LA == ""English""),] # 28,322\ndata$PY[which(data$PY == 31)] <- 2017\ndata <- data[data$PY > 1994,] # 24,396\ndata <- data[data$DT %in% c(""Article"", ""Proceedings Paper"", ""Article; Proceedings Paper"", ""Review""),]\n# 22743\n\nhist(data$PY, xlim = c(1950, 2018), breaks = 3000)\nfor (i in 1:nrow(data)) {\n  if (is.na(data$AB[i]) == T) {\n    data$AB.1[i] = ""N""\n  } else if (nchar(data$AB[i]) < 1) {\n    data$AB.1[i] = ""N""\n  } else {\n    data$AB.1[i] = ""Y""\n  }\n}\ndata.1 <- data[data$AB.1 == ""Y"",] # 22,339\n\nwrite.csv(data.1, ""revised_data.csv"", row.names = F)\nrm(list = ls())\n\n#### 2 NLP preprocessing\n\n#### 2.1 Extract DE terms (Run the whole section)\n\ndata <- read.csv(""revised_data.csv"", stringsAsFactors = F) # 22339\nkeywords.table.1 <- data.frame(keywords = character(0),\n                               keywords.standard = character(0))\nis.word  <- function(x) x %in% GradyAugmented\nword.standardization <- function(x) {\n  x <- as.character(x)\n  x <- gsub(""(-|/|\\\\[|\\\\]){1,}"", "" "", x, perl = F)\n  x <- trimws(x)\n  for (i in 1:length(x)) {\n    x.1 <- x[i]\n    if (str_count(x.1, ""\\\\("") < 2) {\n      x[i] <- gsub(""\\\\(.*\\\\)"", """", x.1, perl = T)\n    } else {\n      x[i] <- paste(substring(x.1, \n                              c(1, \n                                stri_locate_first_regex(x.1, ""\\\\)"")[1] + 1), \n                              c(stri_locate_first_regex(x.1, ""\\\\("")[1]-1, \n                                stri_locate_last_regex(x.1, ""\\\\("")[1]-1)), \n                    collapse = """")\n    }\n    if (str_count(x.1, ""\\\\,"") ==1 ) {\n      part <- trimws(tail(strsplit(x.1, "","")[[1]], n = 1))\n      part.1 <- trimws(head(strsplit(x.1, "","")[[1]], n = 1))\n      if (is.word(part) == F | nchar(part) < 6) {\n        x[i] <- ifelse(abbreviate(part.1, nchar(part)) == part,\n                    part.1,\n                    x.1)\n      }\n    }\n  }\n  x <- gsub(""[[:punct:]]$"", """", x)\n  x <- gsub(""\\"""", """", x)\n  x <- trimws(x)\n  x <- ifelse(str_count(x, "" "") > 0, \n              stem_words(x), \n              x)\n  x <- gsub("" "", """", x)\n}\n\nfor (i in 1:nrow(data)) {\n  keywords.1 <- strsplit(tolower(data$DE[i]), ""; "")[[1]]\n  keywords.1 <- gsub(""^the "", """", keywords.1)\n  keywords.1 <- gsub(""^([[:digit:]]\\\\.){1,}[[:digit:]]*"", """", keywords.1)\n  keywords.1 <- gsub(""\\\\, artificial intelligence|\\\\: artificial intelligence|artificial intelligence and |artificial intelligence & |ai and |\\\\(ai and\\\\)|\\\\(artificial intelligence\\\\)"", """", keywords.1)\n  keywords.1 <- gsub(""^ai | ai |\\\\-ai "", ""artificial intelligence "", keywords.1)\n  keywords.1 <- trimws(keywords.1)\n  keywords.1 <- keywords.1[which(str_count(keywords.1, "" |/|-"") > 0)]\n\n  if (length(keywords.1) > 0) {\n    keywords.2 <- word.standardization(keywords.1)\n    keywords.df.1 <- data.frame(keywords = keywords.1,\n                                keywords.standard = keywords.2,\n                                stringsAsFactors = F)\n    keywords.table.1 <- rbind(keywords.table.1,\n                              keywords.df.1)\n  }\n}\n\nkeywords.summary <- data.frame(table(keywords.table.1$keywords.standard))\nkeywords.summary <- keywords.summary[keywords.summary$Freq > 1,]\nkeywords.table.2 <- keywords.table.1[keywords.table.1$keywords.standard %in% keywords.summary$Var1,]\nkeywords.table.2 <- keywords.table.2[keywords.table.2$keywords.standard != """",]\nkeywords.table.2 <- keywords.table.2[duplicated(keywords.table.2) == F,]\nkeywords.table.2 <- keywords.table.2[order(nchar(keywords.table.2$keywords), \n                                           decreasing = T),]\nwrite.csv(keywords.table.2, ""keywords.table.freq2.csv"", row.names = F) # 23296\nrm(list = ls())\n\n#### 2.2 Preprocessing\n\ndata <- read.csv(""revised_data.csv"", stringsAsFactors = F) # 22339\nkeywords <- read.csv(""keywords.table.freq2.csv"",\n                     colClasses = c(""character"", ""character"")) # 4945\nabstract.table <- data.frame(id = character(0),\n                             abstract = character(0))\n\nfor (i in 1:nrow(data)) {\n  title <- as.character(data$TI[i])\n  sen <- as.character(data$AB[i])\n  sen <- paste(title, sen, sep = "". "")\n  sen <- gsub(""\\\\[.*?\\\\]"", """", sen)\n  sen <- gsub(""\\\\(\\\\)|\\\\(s\\\\)"", """", sen)\n  #### think about multi-parenthesis scenario\n  par <- regmatches(sen, \n                    gregexpr(""\\\\(.*?(\\\\)|\\\\)\\\\))"", sen, perl = T))[[1]]\n  countchar <- function(x) {nchar(substr(par[x], 2, nchar(par[x])-1))}\n  if (length(par) > 0) {\n    for (j in 1:length(par)) {\n      if (grepl(""[[:digit:]]{4}"", par[j]) == T | grepl(""[[:digit:]]{2}\\\\([[:digit:]]{1,}.*?\\\\)"", par[j]) == T) {\n        sen <- gsub(gsub(""([[:punct:]]){1}"", ""\\\\\\\\\\\\1"", par[j], perl = T), \n                    """", \n                    sen)\n        next\n      } else if (grepl(""[[:digit:]]"", par[j]) == F & countchar(j) < 7 & countchar(j) > 2) {\n        sen <- gsub(gsub(""([[:punct:]]){1}"", ""\\\\\\\\\\\\1"", par[j], perl = T), \n                    """", \n                    sen)\n        next\n      } else if (grepl("" ai$|ai\\\\-|\\\\-ai$"", par[j]) == T) {\n        sen <- gsub(gsub(""([[:punct:]]){1}"", ""\\\\\\\\\\\\1"", par[j], perl = T), \n                    """", \n                    sen)\n        next\n      } else if (countchar(j) == 2 & substr(par[j], 1, 1) != substr(par[j], 2, 2)) {\n        sen <- gsub(gsub(""([[:punct:]]){1}"", ""\\\\\\\\\\\\1"", par[j], perl = T), \n                    """",\n                    sen)\n      } else if (grepl(""[[:digit:]]{1, }\\\\-[[:digit:]]{1, }|[[:digit:]]{1, }\\\\.[[:digit:]]{1, }|[[:digit:]]{1, }//%"", par[j]) == T) {\n        sen <- gsub(gsub(""([[:punct:]]){1}"", ""\\\\\\\\\\\\1"", par[j], perl = T), \n                    """", \n                    sen)\n        next\n      } else if (grepl(""[[:alpha:]]+(\\\\\'|\\\\-)[[:digit:]]{2}"", par[j]) == T) {\n        sen <- gsub(gsub(""([[:punct:]]){1}"", ""\\\\\\\\\\\\1"", par[j], perl = T), \n                    """", \n                    sen)\n        next\n      } else if (grepl(""n( {0,1})\\\\=( {0,1})[[:digit:]]{1,}"", par[j]) == T) {\n        sen <- gsub(gsub(""([[:punct:]]){1}"", ""\\\\\\\\\\\\1"", par[j], perl = T), \n                    """", \n                    sen)\n        next\n      }\n    }\n  }\n  \n  sen <- tolower(tokenize_sentences(sen)[[1]])\n  sen <- subset(sen, grepl(""artificial( |\\\\-)intelligence"", sen) | grepl(""( |\\\\(|^)ai( |\\\\)|$)"", sen, perl = T))\n  sen <- gsub(""international joint conference on artificial intelligence"", ""JCArtificialIntelligence"", sen)\n  sen <- gsub(""joint conference on artificial intelligence"", ""JCArtificialIntelligence"", sen)\n  sen <- gsub(""mexican international conference on artificial intelligence"", ""MICArtificialIntelligence"", sen)\n  sen <- gsub(""electronic transactions on artificial intelligence"", ""ETArtificialIntelligence"", sen)\n  sen <- gsub(""computers and artificial intelligence journal"", ""CArtificialIntelligenceJ"", sen)\n  sen <- gsub(""international conference on artificial intelligence and statistics"", ""ICArtificialIntelligenceS"", sen)\n  sen <- gsub(""annals of mathematics and artificial intelligence\\\\, mathematics and informatic"", ""AMArtificialIntelligenceMI"", sen)\n  sen <- gsub(""conference on uncertainty in artificial intelligence"", ""CUArtificialIntelligence"", sen)\n  sen <- gsub(""aaai conference on artificial intelligence and digital entertainment"", ""AAAIArtificialIntelligence"", sen)\n  sen <- gsub(""aaai conference on artificial intelligence"", ""AAAIArtificialIntelligence"", sen)\n  sen <- gsub(""international conference on knowledge representation and reasoning"", ""ICKRR"", sen)\n  sen <- gsub(""indian international conference on artificial intelligence"", ""IICArtificialIntelligence"", sen)\n  sen <- gsub(""international conference on artificial intelligence planning and scheduling"", ""ICArtificialIntelligencePS"", sen)\n  sen <- gsub(""international conference on artificial intelligence planning systems"", ""ICArtificialIntelligencePS"", sen)\n  sen <- gsub(""ieee conference on artificial intelligence for applications"", ""IEEECArtificialIntelligenceA"", sen)\n  sen <- gsub(""international conference on artificial intelligence applications and innovations"", ""ICArtificialIntelligenceAI"", sen)\n  sen <- gsub(""international conference on artificial intelligence and law"", ""ICArtificialIntelligenceL"", sen)\n  sen <- gsub(""innovative applications of artificial intelligence conference"", ""IArtificialIntelligenceAC"", sen)\n  sen <- gsub(""international journal of artificial intelligence in education"", ""IJArtificialIntelligenceE"", sen)\n  sen <- gsub(""international journal artificial intelligence in education"", ""IJArtificialIntelligenceE"", sen)\n  sen <- gsub(""european conference on artificial intelligence"", ""ECArtificialIntelligence"", sen)\n  sen <- gsub(""international conference on the application of artificial intelligence"", ""ICAartificialIntelligence"", sen)\n  sen <- gsub(""national conference on artificial intelligence"", ""AAAIArtificialIntelligence"", sen)\n  sen <- gsub(""international conference on artificial intelligence in medicine"", ""ArtificialIntelligenceME"", sen)\n  sen <- gsub(""artificial intelligence in medicine europe"", ""ArtificialIntelligenceME"", sen)\n  sen <- gsub(""conference on artificial intelligence and interactive digital entertainment"", ""CArtificialIntelligenceIDE"", sen)\n  sen <- gsub(""international conference on artificial intelligence"", ""ICArtificialIntelligence"", sen)\n  sen <- gsub(""association for the advancement of artificial intelligence"", ""AssoAArtificialIntelligence"", sen)\n  sen <- gsub(""american association for artificial intelligence"", ""AmAArtificialIntelligence"", sen)\n  sen <- gsub(""israeli association for artificial intelligence"", ""ISArtificialIntelligence"", sen)\n  sen <- gsub(""international association for artificial intelligence and law"", ""ISArtificialIntelligenceL"", sen)\n  sen <- gsub("" aips"", ""ICArtificialIntelligencePS"", sen)\n  sen <- gsub("" aiide|aiide conference"", ""CArtificialIntelligenceIDE"", sen)\n  sen <- gsub("" ncai|ncai conference"", ""NCArtificialIntelligence"", sen)\n  sen <- gsub("" aaai conference"", ""AAAIArtificialIntelligence"", sen)\n  sen <- gsub(\'artificial( |\\\\-)intelligence series\', ""ArtificialIntelligence"", sen)\n  sen <- gsub(\'artificial( |\\\\-)intelligence (\\\\(ai\\\\)|\\\\(a\\\\.i\\\\.\\\\)|\\\\(al\\\\)|\\\\(at\\\\))\', ""ArtificialIntelligence"", sen)\n  sen <- gsub(\'artificial( |\\\\-)intelligence(, | )(ai|a\\\\.i\\\\.|al|at)($|[[:punct:]])\', ""ArtificialIntelligence "", sen)\n  sen <- gsub(\'(ai|a\\\\.i\\\\.|al|at) \\\\(artificial( |\\\\-)intelligence\\\\)\', ""ArtificialIntelligence"", sen)\n  sen <- gsub(\'artificial( |\\\\-)intelligence\', ""ArtificialIntelligence"", sen, perl = T)\n  sen <- gsub(\'( |^)(ai|a.i.)( |$|\\\\-|\\\\/)\', "" ArtificialIntelligence "", sen)\n  sen <- gsub(\'[[:punct:]]ArtificialIntelligence [[:punct:]]\', "" ArtificialIntelligence "", sen, perl = F)\n  \n  if (length(sen) > 0) {\n    for (j in 1:length(sen)) {\n      sen.1 <- sen[j]\n      for (k in 1:nrow(keywords)) {\n        old <- gsub(""([[:punct:]])"", ""\\\\\\\\\\\\1"", keywords$keywords[k])\n        new <- keywords$keywords.standard[k]\n        sen.1 <- gsub(old,\n                        new,\n                        sen.1)\n      }\n      abstract.df <- data.frame(id = as.character(data$UT[i]),\n                                abstract = sen.1,\n                                stringsAsFactors = F)\n      abstract.table <- rbind(abstract.table,\n                              abstract.df)\n    }\n  }\n}\n\nwrite.csv(abstract.table, ""abstract.sample.new.csv"", row.names = F)\nrm(list = ls())\n\n\n#### 2.2 Add sentence number\n\nabstract <- read.csv(""abstract.sample.new.csv"", \n                     stringsAsFactors = F)\nabstract.1 <- data.frame(id = character(0),\n                         sentence.no = numeric(0),\n                         abstract = character(0))\nid.list <- as.character(unique(abstract$id))\nfor (i in 1:length(id.list)) {\n  abstract.df <- abstract[which(abstract$id == id.list[i]),]\n  abstract.df$sentence.no <- 1:nrow(abstract.df)\n  abstract.df <- abstract.df[,c(1, 3, 2)]\n  abstract.1 <- rbind(abstract.1,\n                      abstract.df)\n}\n\n# write.csv(abstract.1, ""abstract.sample.csv"", row.names = F)\nwrite.csv(abstract.1, ""abstract.sample.new.csv"", row.names = F)\nrm(list = ls())\n\n#### 2.3 NLP processing\n\nlibrary(coreNLP)\ninitCoreNLP()\n\nabstract <- read.csv(""abstract.sample.new.csv"",\n                 stringsAsFactors = F) # 22896 sentences from 17184 papers\noutput.final <- data.frame(id = character(0),\n                           sentence = numeric(0),\n                           term = character(0),\n                           lemma = character(0),\n                           govenor = character(0),\n                           relation = character(0),\n                           target = character(0),\n                           POS = character(0),\n                           NER = character(0),\n                           stringsAsFactors = F)\n\nai.list <- c(""ArtificialIntelligence"", ""DisArtificialIntelligence"", ""GamArtificialIntelligence"")\nfor (i in 1:nrow(abstract)) {\n  output <- data.frame(getDependency(annotateString(abstract$abstract[i])))\n  token <- data.frame(getToken(annotateString(abstract$abstract[i])))\n  ###### check this line later\n  id.ai <- token$id[token$token %in% ai.list]\n  output.1 <- output[(output$governorIdx %in% id.ai | output$dependentIdx %in% id.ai),]\n  if (nrow(output.1) > 0) {\n    for (m in 1:nrow(output.1)) {\n      id <- abstract$id[i]\n      order <- ifelse(sum(grepl(""ArtificialIntelligence"", output.1[m, 2:3]) == F) == 0,\n                      1,\n                      which(grepl(""ArtificialIntelligence"", output.1[m, 2:3]) == F))\n      sentence <- abstract$sentence.no[i]\n      term <- c(output.1[m, 2], output.1[m,3])[order]\n      lemma <- token$lemma[which(token$sentence == output.1$sentence[m] & token$token == term)][1]\n      governor <- ifelse(order == 1,\n                         ""governor"", ""dependent"")\n      target <- c(output.1[m, 2], output.1[m,3])[3-order]\n      relation <- as.character(output.1$type[m])\n      POS <- token$POS[which(token$sentence == output.1$sentence[m] & token$token == term)][1]\n      NER <- token$NER[which(token$sentence == output.1$sentence[m] & token$token == term)][1]\n      output.final <- rbind(output.final,\n                            data.frame(id = id,\n                                       sentence = sentence,\n                                       term = term,\n                                       lemma = lemma,\n                                       governor = governor,\n                                       relation = relation,\n                                       target = target,\n                                       POS = POS,\n                                       NER = NER,\n                                       stringsAsFactors = F))\n    }\n  }\n}\n\nwrite.csv(output.final, ""nlp_result_final.csv"", row.names = F) # 45440\n\n# Depulicate and summarize\nnlp_output <- read.csv(""nlp_result_final.csv"", stringsAsFactors = F) # 45440\nsummary <- data.frame(table(nlp_output$lemma))\nnlp_output <- nlp_output[which(duplicated(nlp_output) == F),]\nsummary <- summary[order(summary$Freq, decreasing = T),]\n\nwrite.csv(nlp_output, ""nlp_result_final_dedup.csv"", row.names = F) # 44676\nwrite.csv(summary, ""nlp_summary.csv"", row.names = F)\nrm(list = ls())\n', 'library(ggplot2)\nlibrary(pander)\nlibrary(dplyr)\nlibrary(qdapDictionaries)\n\n# Retrieve data\ndata <- read.csv(""revised_data.csv"",\n                 stringsAsFactors = F) # 22339\ndata$text <- paste(data$TI,\n                   data$AB,\n                   sep = "". "")\nwrite.csv(data, \n          ""revised_data.csv"",\n          row.names = F)\nrm(list = ls())\n\n# Subject\ndata <- read.csv(""revised_data.csv"",\n                 stringsAsFactors = F)\nsubject.table <- data.frame(id = character(0),\n                            subject = character(0))\nfor (i in 1:nrow(data)) {\n  subject <- strsplit(data$WC[i], ""; "")[[1]]\n  subject.df <- data.frame(id = rep(data$UT[i], length(subject)),\n                           subject = subject,\n                           stringsAsFactors = F)\n  subject.table <- rbind(subject.table, subject.df)\n}\nwrite.csv(subject.table, ""subject.table.csv"", row.names = F) # 46556\nrm(list = ls())\n\n# Abstract\nabstract <- read.csv(""abstract.sample.new.csv"",\n                     stringsAsFactors = F) # 22896\ndata <- read.csv(""revised_data.csv"",\n                 stringsAsFactors = F)\ndata <- data[data$UT %in% abstract$id,] # 17184\nwrite.csv(data, ""data_final.csv"", row.names = F)\nrm(list = ls())\n\n# Term table\ndata <- read.csv(""data_final.csv"",\n                 stringsAsFactors = F)\nterm <- read.csv(""nlp_result_final_dedup.csv"",\n                 stringsAsFactors = F) # 44485\nfor (i in 1:nrow(term)) {\n  term$relation.1[i] <- strsplit(term$relation[i], "":"")[[1]][1]\n  term$POS.1[i] <- substr(term$POS[i], 1, 2)\n}\n\nterm.1 <- data.frame(id = character(0),\n                     sentence = numeric(0),\n                     term = character(0),\n                     lemma = character(0),\n                     governor = character(0),\n                     relation = character(0),\n                     target = character(0),\n                     POS = character(0),\n                     NER = character(0),\n                     direct = numeric(0),\n                     relation.1 = character(0),\n                     POS.1 = character(0))\nfor (i in 1:nrow(term)) {\n  if (grepl(""\\\\/"", term$target[i])) {\n    target = strsplit(term$target[i], ""/"")[[1]]\n    target = target[target != ""ArtificialIntelligence""]\n    term$target[i] <- ""ArtificialIntelligence""\n    term.df <- rbind(term[i,],\n                     data.frame(id = rep(term$id[i], length(target)),\n                                sentence = rep(term$sentence[i], length(target)),\n                                term = target,\n                                lemma = target,\n                                governor = rep(""governor"", length(target)),\n                                relation = rep(""conj:and"", length(target)),\n                                target = rep(""ArtificialIntelligence"", length(target)),\n                                POS = rep(""NN"", length(target)),\n                                NER = rep(""O"", length(target)),\n                                direct = rep(term$direct[i], length(target)),\n                                relation.1 = rep(""conj"", length(target)),\n                                POS.1 =rep(""NN"", length(target))))\n    term.1 <- rbind(term.1, term.df)\n  } else {\n    term.1 <- rbind(term.1, term[i,])\n  }\n}\nrow.names(term.1) <- 1:nrow(term.1)\n\nterm.2 <- data.frame(id = character(0),\n                     sentence = numeric(0),\n                     term = character(0),\n                     lemma = character(0),\n                     governor = character(0),\n                     relation = character(0),\n                     target = character(0),\n                     POS = character(0),\n                     NER = character(0),\n                     relation.1 = character(0),\n                     POS.1 = character(0))\nfor (i in 1:nrow(term.1)) {\n  if (grepl(""\\\\/"", term.1$lemma[i])) {\n    target = strsplit(term.1$lemma[i], ""/"")[[1]]\n    if (sum(nchar(target) > 2) ==  length(target)) {\n      term.1$term[i] <- target[1]\n      term.1$lemma[i] <- target[1]\n      target <- target[-1]\n      term.df <- rbind(term.1[i,],\n                       data.frame(id = rep(term.1$id[i], length(target)),\n                                  sentence = rep(term.1$sentence[i], length(target)),\n                                  term = target,\n                                  lemma = target,\n                                  governor = rep(""governor"", length(target)),\n                                  relation = rep(term.1$relation[i], length(target)),\n                                  target = rep(term.1$target[i], length(target)),\n                                  POS = rep(term.1$POS[i], length(target)),\n                                  NER = rep(""O"", length(target)),\n                                  relation.1 = rep(term.1$relation.1[i], length(target)),\n                                  POS.1 =rep(term.1$POS.1[i], length(target))))\n      term.2 <- rbind(term.2, term.df)\n    } else {\n      term.2 <- rbind(term.2, term.1[i,])\n    }\n  } else {\n    term.2 <- rbind(term.2, term.1[i,])\n  }\n}\nrow.names(term.2) <- 1:nrow(term.2)\n\nterm.3 <- term.2[,-2]\nterm.3 <- term.3[duplicated(term.3) == F,]\nterm.4 <- term.3[term.3$POS.1 %in% c(""JJ"", ""RB"", ""NN"", ""VB""),] # 31208 words\nterm.4 <- term.4[term.4$id %in% data$UT,] # 31208\nfor (i in 1:nrow(term.4)) {\n  if (term.4$lemma[i] == ""computerscus"") {\n    term.4$lemma[i] <- ""computersci""\n  }\n  if (term.4$lemma[i] == ""cognitivescus"") {\n    term.4$lemma[i] <- ""cognitivesci""\n  }\n}\n\nwrite.csv(term.4, ""nlp_result_final_dedup_deselect.csv"", row.names = F)\nrm(list = ls())\n\n# Term\nterm <- read.csv(""nlp_result_final_dedup_deselect.csv"",\n                 stringsAsFactors = F) # 31081\nsubject <- read.csv(""subject.table.csv"",\n                    stringsAsFactors = F) # 46556\ndata <- read.csv(""data_final.csv"",\n                 stringsAsFactors = F)\nsubject <- subject[subject$id %in% data$UT,] # 15,524\nwrite.csv(subject, ""subject.table.csv"", row.names = F) # 35753\nsubject.table <- data.frame(table(subject$subject))\nsubject.table <- subject.table[order(subject.table$Freq,\n                                     decreasing = T),]\ncolnames(subject.table) <- c(""Subject"", ""Count"")\nrow.names(subject.table) <- 1:nrow(subject.table)\nwrite.csv(subject.table, ""subject_table.csv"", row.names = F)\nrm(list = ls())\n\n# Total frequency of words\n\ndata <- read.csv(""data_final.csv"",\n                 stringsAsFactors = F)\n# x = ""a"" (all), or ""conj"" (conjucture), or ""verb""\nget.term <- function(x) {\n  term <- read.csv(""nlp_result_final_dedup_deselect.csv"",\n                   stringsAsFactors = F) # 18,733\n  if (x == ""a"") {\n    term <- term\n  } else if (x == ""conj"") {\n    term <- term[term$relation.1 == ""conj"",]\n  } else if (x == ""verb"") {\n    term <- term[term$POS.1 == ""VB"",]\n  } else if (x == ""noun"") {\n    term <- term[term$POS.1 == ""NN"",]\n  }\n  \n  return(term)\n}\n\n# x = ""a"" (all), or ""conj"" (conjucture), or ""verb""\nget.table <- function(x) {\n  term <- get.term(x)\n  term.table <- data.frame(table(term$lemma), stringsAsFactors = F)\n  term.table <- term.table[order(term.table$Freq, decreasing = T),]\n  colnames(term.table) <- c(""Term"", ""Count"")\n  row.names(term.table) <- 1:nrow(term.table)\n  term.table$Percentage <- term.table$Count / sum(term.table$Count)\n  term.table$Term <- as.character(term.table$Term)\n  \n  return(term.table)\n}\n\n# x = ""a"" (all), or ""conj"" (conjucture), or ""verb""\nget.slope <- function(x) {\n  term <- get.term(x)\n  term.table <- get.table(x)\n\n  term.table.1 <- term.table[,1:2]\n  min.list <- c(1995, 2000, 2005, 2010, 2015)\n  max.list <- c(1999, 2004, 2009, 2014, 2017)\n  time.table <- data.frame(time = numeric(0),\n                          count = numeric(0),\n                          stringsAsFactors = F)\n\n  for (i in 1:length(min.list)) {\n    data.sub <- data$UT[data$PY > min.list[i]-1 & data$PY < max.list[i] +1]\n    term.sub <- term[term$id %in% data.sub,]\n    time.table[i, 1] <- i\n    time.table[i, 2] <- nrow(term.sub)\n    term.table.sub <- data.frame(table(term.sub$lemma), stringsAsFactors = F)\n    term.table.sub <- term.table.sub[order(term.table.sub$Freq, decreasing = T),]\n    colnames(term.table.sub) <- c(""Term"", ""Frequency"")\n    rownames(term.table.sub) <- 1:nrow(term.table.sub)\n    term.table.1 <- merge(x = term.table.1,\n                          y = term.table.sub,\n                          by.x = ""Term"",\n                          by.y = ""Term"",\n                          all.x = T,\n                          all.y = T)\n  }\n  time.table[,3] <- time.table[,2] / time.table[4,2]\n\n  for (i in 1:nrow(term.table.1)) {\n    for (j in 1:ncol(term.table.1)) {\n      if (is.na(term.table.1[i, j]) == T) {\n        term.table.1[i, j] <- 0\n      }\n    }\n    term.table.1[i, 2] <- sum(term.table.1[i, 3:7])\n  }\n  term.table.1 <- term.table.1[term.table.1[,2] >= 18,]\n\n  for (i in 1:nrow(term.table.1)) {\n    if (term.table.1[i, 2] > 0) {\n      for (j in 1:nrow(time.table)) {\n        term.table.1[i, j + 2] <- term.table.1[i, j + 2] / time.table[j, 3]\n      }\n      new.total = sum(term.table.1[i, 3:(nrow(time.table) + 2)])\n      for (j in 1:nrow(time.table)) {\n        term.table.1[i, j + 2] <- term.table.1[i, j + 2] / new.total\n      }\n      model <- lm(as.numeric(term.table.1[i, 3:(nrow(time.table) + 2)]) ~ c(1:nrow(time.table)))\n      term.table.1[i, ""slope""] <- model$coefficients[2]\n    }\n  }\n\n  term.table.1[,""slope""] <- round(term.table.1[,""slope""], digits = 4)\n  colnames(term.table.1)[2:8] <- c(""total"",\n                                   ""95-99"",\n                                   ""00-04"",\n                                   ""05-09"",\n                                   ""10-14"",\n                                   ""15-17"")\n  return(term.table.1)\n}\n\nterm.table <- get.slope(""a"")\ncolnames(term.table)[2:8] <- c(""total"", ""95-99"", ""00-04"", ""05-09"", ""10-14"", ""15-17"", ""Slope"")\nwrite.csv(term.table, ""term_slope_all.csv"", row.names = F)\nterm.table.conj <- get.slope(""conj"")\ncolnames(term.table.conj)[2:8] <- c(""total"", ""95-99"", ""00-04"", ""05-09"", ""10-14"", ""15-17"", ""Slope"")\nwrite.csv(term.table.conj, ""term_slope_conj.csv"", row.names = F)\n# term.table.noun <- get.slope(""noun"")\n# colnames(term.table.noun)[2:8] <- c(""total"", ""95-99"", ""00-04"", ""05-09"", ""10-14"", ""15-17"", ""Slope"")\n# write.csv(term.table.noun, ""term_slope_noun.csv"", row.names = F)\n# rm(list = ls())\n\n# Domain\ndomain.1 <- read.csv(""domain_mapping.csv"",\n                     stringsAsFactors = F)\ndomain.2 <- read.csv(""domain.csv"",\n                   stringsAsFactors = F)\ndomain <- merge(x = domain.1[, 3:4],\n                y = domain.2,\n                by.x = ""field"",\n                by.y = ""domain"",\n                all = T)\nfor (i in 1:nrow(domain)) {\n  if (is.na(domain$new_domain[i]) == T) {\n    if (domain$field[i] == ""Arts & Humanities, general"") {\n      domain$new_domain[i] <- ""Social science and humanities""\n    } else {\n      domain$new_domain[i] <- ""Science""\n    }\n  }\n}\n\ndomain <- read.csv(""domain_all.csv"",\n                   stringsAsFactors = F)\ndomain$term <- tolower(domain$term)\nsubject <- read.csv(""subject.table.csv"",\n                    stringsAsFactors = F)\nsubject$subject <- tolower(subject$subject)\nsubject <- merge(x = subject[,1:2],\n                 y = domain[,2:3],\n                 by.x = ""subject"",\n                 by.y = ""term"",\n                 all.x = T,\n                 all.y = F)\nfor (i in 1:nrow(subject)) {\n  if (is.na(subject$new_domain[i]) == T) {\n    subject$new_domain[i] <- ""Social science and humanities""\n  }\n}\n\nwrite.csv(subject, ""subject.table.csv"",\n          row.names = F)\nrm(list = ls())\n\n# Summarize subject\nsubject <- read.csv(""subject.table.csv"",\n                    stringsAsFactors = F)\nsubject.1 <- subject[,2:3]\nsubject.1 <- subject.1[duplicated(subject.1) == 0,] # 11,308\nsubject.table.1 <- data.frame(table(subject.1$new_domain))\nsubject.table.1 <- subject.table.1[order(subject.table.1$Freq, decreasing = T),]\nrow.names(subject.table.1) <- 1:nrow(subject.table.1)\ncolnames(subject.table.1) <- c(""Domain"", ""Count"")\nwrite.csv(subject.table.1, ""subject.summary.table.csv"",\n          row.names = F)\nrm(list = ls())\n\n# Calculate publication slopes\n\nwos.term <- read.csv(""term_wos_publication.csv"",\n                     stringsAsFactors = F)\nwos.pub <- read.csv(""wos_publication.csv"",\n                    stringsAsFactors = F)\nmin.list <- c(1995, 2000, 2005, 2010, 2015)\nmax.list <- c(1999, 2004, 2009, 2014, 2017)\nterm.list <- as.character(unique(wos.term$Term))\nterm.publication.freq <- data.frame(term = character(0),\n                                    frequency.1 = numeric(0),\n                                    frequency.2 = numeric(0),\n                                    frequency.3 = numeric(0),\n                                    frequency.4 = numeric(0),\n                                    frequency.5 = numeric(0),\n                                    stringsAsFactors = F)\n\nfor (i in 1:8) {\n  term.publication.freq[i, 1] <- term.list[i]\n  for (j in 1:5) {\n    year.min <- min.list[j]\n    year.max <- max.list[j]\n    term.publication.freq[i, j+1] <- sum(wos.term$Count[which(wos.term$Term == term.list[i] &\n                                                                wos.term$Year > year.min -1 &\n                                                                wos.term$Year < year.max + 1)])\n  }\n  term.publication.freq[i, 7] <- sum(term.publication.freq[i, 2:6])\n}\n\nyear.publication.freq <- data.frame(Year = numeric(0),\n                                    count = numeric(0),\n                                    stringsAsFactors = F)\nfor (i in 1:5) {\n  year.publication.freq[i, 1] <- paste(""period."", i, sep = """")\n  year.min <- min.list[i]\n  year.max <- max.list[i]\n  year.publication.freq[i, 2] <- sum(wos.pub$Publication[wos.pub$Year > year.min -1 & \n                                                           wos.pub$Year < year.max +1])\n}\n\nyear.publication.freq[,3] <- year.publication.freq[,2] / year.publication.freq[3, 2]\n\nfor (i in 1:5) {\n  term.publication.freq[, i + 1] <- term.publication.freq[, i + 1] / year.publication.freq[i, 3]\n}\n\nfor (i in 1:8) {\n  term.publication.freq[i, 7] <- sum(term.publication.freq[i, 2:6])\n  for (j in 1:5) {\n    term.publication.freq[i, j + 1] <- term.publication.freq[i, j + 1] / term.publication.freq[i, 7]\n  }\n  term.publication.freq[i, 7] <- sum(term.publication.freq[i, 2:6])\n  model <- lm(as.numeric(term.publication.freq[i, 2:6]) ~ c(1:5))\n  term.publication.freq[i, ""slope""] <- model$coefficients[2]\n}\nwrite.csv(term.publication.freq, ""term_pub_freq_all.csv"", row.names = F)\nrm(list = ls())\n\n# Form a table\n\nterm.publication.freq <- read.csv(""term_pub_freq_all.csv"", stringsAsFactors = F)\n# term.publication.freq <- term.publication.freq[! term.publication.freq$term %in% c(""cybernetics"", ""machine learning""),]\nterm.table.conj <- read.csv(""term_slope_conj.csv"", stringsAsFactors = F)\n\nterm.frequency.visualization <- data.frame(term = character(0),\n                                           group = character(0),\n                                           period = numeric(0),\n                                           ratio = numeric(0),\n                                           stringsAsFactors = F)\nterm.1 <- c(""Human-computer interaction"", ""Robot"", ""Natural language processing"", ""Artificial life"", ""Expert system"",\n            ""Cognitive psychology"", ""cybernetics"", ""machine learning"")\nterm.2 <- c(""humancomputerinteract"", ""robot"", ""naturallanguageprocess"", ""artificiallif"", ""expertsystem"", ""cognitivepsychologi"", ""cybernetics"", ""machinelearn"")\n\nfor (i in 1:8) {\n  term.a <- term.1[i]\n  term.b <- term.2[i]\n  for (j in 1:5) {\n    period = j\n    ratio.1 <- term.publication.freq[which(term.publication.freq$term == term.a), j + 1]\n    ratio.2 <- term.table.conj[which(term.table.conj$Term == term.b), j + 2]\n    term.frequency.visualization.df.1 <- data.frame(term = term.a,\n                                                    group = ""P"",\n                                                    period = period,\n                                                    ratio = ratio.1,\n                                                    stringsAsFactors = F)\n    term.frequency.visualization.df.2 <- data.frame(term = term.a,\n                                                    group = ""T"",\n                                                    period = period,\n                                                    ratio = ratio.2,\n                                                    stringsAsFactors = F)\n    term.frequency.visualization <- rbind(term.frequency.visualization,\n                                          term.frequency.visualization.df.1)\n    term.frequency.visualization <- rbind(term.frequency.visualization,\n                                          term.frequency.visualization.df.2)\n  }\n}\n\nwrite.csv(term.frequency.visualization, ""term.visualization.csv"", row.names = F)\n\n']",Extracting terms concerning AI based on Web of Science data This is the accompanied code to extract terms connected to AI through titles and abstracts from Web of Science data.,4
A highly virulent variant of HIV-1 circulating in the Netherlands,"Analysis code for ""A highly virulent variant of HIV-1 circulating in the Netherlands"" by Wymant et al., Science","['# Author: Chris Wymant\n# Acknowledgment: I wrote this while funded by ERC Advanced Grant PBDR-339251\n# and a Li Ka Shing Foundation grant, both awarded to Christophe Fraser.\n# This script analyses a whole-genome nucleotide alignment with the VB variant\n# consensus, the consensus of other Dutch subtype-B in BEEHIVE, and HXB2. It\n# splits the genome into equally sized segments, calculates how many positions\n# in each segment are SNPs (single nucleotide polymorphisms) between the VB variant and the\n# consensus of other Dutch subtype-B in BEEHIVE, and compares this to a null\n# model. The null distribution is constructed as follows. Conditioning on the \n# total number of nucleotide changes, the number in each segment is taken to be\n# multinomially distributed, with the probability for each bin being the mean\n# evolutionary rate over positions inside that segment, normalised to 1. \n\nlibrary(tidyverse)\nlibrary(seqinr)\n\n# WARNING: the command below deletes all objects in memory in your current R\n# session. Convenient when repeatedly re-running this script.\nrm(list = ls())\n\n# INPUT ------------------------------------------------------------------------\n\n# Set seed for random number generation, for reproducibility \nset.seed(12345)\n\n# Set the working directory to where this code lives on your machine\n# setwd()\n# TODO: remove private path\nsetwd(""~/hiv_hypervirulent_lineage_code/"")\n\n# The whole-genome nucleotide alignment with the VB variant consensus, the \n# consensus of other Dutch subtype-B in BEEHIVE, and HXB2, provided as\n# supplementary data with the associated paper.\nfile_in_aln <- ""private_data_seqs/nucleotide_VB_vs_DutchB_wHXB2.fasta""\n\n# The \'normalisation\' file, measuring between-host evolutionary rate as a\n# function of position in the HIV genome (HXB2 coordinates).\nfile_in_normalisation <- ""data_external/HIV_DistanceNormalisationOverGenome.csv""\n\n# Files we\'ll create\nfile_out_snps <- ""SNPs_genome_distribution.pdf""\nfile_out_snps_raw <- ""SNPs_genome_distribution_raw.pdf""\n\nnum_segments <- 10L\n\nnum_multinomial_draws <- 200000L\n\nplot <- FALSE\n\n# MAIN -------------------------------------------------------------------------\n\nstopifnot(file.exists(file_in_aln))\nstopifnot(file.exists(file_in_normalisation))\n\n# positions (wrt HXB2) between which we have sequence for both the lineage and\n# the Dutch subtype B consensus (defined by the Gall et al 2012 primers)\npos_start <- 497L\npos_end <- 9496L\n\nsegment_boundaries <- seq(pos_start, pos_end, length.out = num_segments + 1L)\nsegment_boundaries[[1]] <- pos_start - 1L # Since bins are defined as (min,max]\n\n# Percentiles of the null distribution that we\'ll calculate. Subsequent code\n# expects these specific values.\npercentiles <- c(0.5, 2.5, 25, 50, 75, 97.5, 99.5) \n\n# Read in the aln, convert bases to upper case\naln <- read.fasta(file_in_aln, seqtype = ""DNA"") %>%\n  map(toupper)\n\n# Check the three expected seq names are present\nname_hxb2 <- ""B.FR.83.HXB2_LAI_IIIB_BRU.K03455""\nname_dutch_b <- ""consensus_Dutch_subtypeB_inBEEHIVE""\nname_lineage <- ""consensus_VB_variant""\nstopifnot(name_hxb2 %in% names(aln))\nstopifnot(name_dutch_b %in% names(aln))\nstopifnot(name_lineage %in% names(aln))\n\nseq_hxb2    <- aln[[name_hxb2]]\nseq_dutch_b <- aln[[name_dutch_b]]\nseq_lineage <- aln[[name_lineage]]\n\n# Check all seqs have the same length\nlength_aln <- length(seq_hxb2)\nstopifnot(length_aln == length(seq_dutch_b))\nstopifnot(length_aln == length(seq_lineage))\n\n# Get the HXB2 position of aln positions where the lineage and dutch B seqs\n# disagree. Check the bases are what we expect.\npos_snps <- integer()\nbases_ok <- c(""A"", ""C"", ""G"", ""T"")\nhxb2_pos_current <- 0L\nfor (pos in 1:length_aln) {\n  if (seq_hxb2[[pos]] != ""-"") hxb2_pos_current <- hxb2_pos_current + 1L\n  if (seq_dutch_b[[pos]] == seq_lineage[[pos]] ||\n      seq_dutch_b[[pos]] == ""N"" ||\n      seq_lineage[[pos]] == ""N"" ||\n      seq_dutch_b[[pos]] == ""-"" ||\n      seq_lineage[[pos]] == ""-"") {\n    next\n  }\n  \n  # Manual hack: R means A or G so that\'s not a SNP\n  if (seq_dutch_b[[pos]] == ""R"" && seq_lineage[[pos]] == ""A"") next\n\n  if (! seq_dutch_b[[pos]] %in% bases_ok) {\n    stop(paste(""Unexpected base"", seq_dutch_b[[pos]], ""in seq"", name_dutch_b, \n               ""at position"", pos))\n  }\n  if (! seq_lineage[[pos]] %in% bases_ok) {\n    # Manual hack, Y means C or T so that\'s a SNP\n    if (seq_lineage[[pos]] != ""Y"" && seq_dutch_b[[pos]] != ""A"") {\n    stop(paste(""Unexpected base"", seq_lineage[[pos]], ""in seq"", name_lineage, \n               ""at position"", pos))\n    }\n  }\n  pos_snps[[length(pos_snps) + 1L]] <- hxb2_pos_current\n}\nstopifnot(all(pos_snps >= pos_start))\nstopifnot(all(pos_snps <= pos_end))\nnum_snps <- length(pos_snps)\n\n# Read in the normalisation file\ndf_normalisation <- read_csv(file_in_normalisation, col_types = cols(\n  POSITION_WRT_HXB2 = col_integer(),\n  MEDIAN_PAIRWISE_DISTANCE_BETWEEN_STANDARD_REFS = col_double()\n  )) %>%\n  rename(pos = POSITION_WRT_HXB2,\n         distance = MEDIAN_PAIRWISE_DISTANCE_BETWEEN_STANDARD_REFS)\n\n# Add extra rows before the start and after the end of the normalisation file,\n# which are within the [pos_start, pos_end] range we consider but where\n# normalisation values are missing. Then check we have all the positions needed.\ndf_normalisation <- df_normalisation %>%\n  bind_rows(tibble(pos = c(seq(pos_start, min(df_normalisation$pos) - 1L),\n                           seq(max(df_normalisation$pos) + 1L, pos_end)),\n                   distance = NA)) %>%\n  arrange(pos)\nstopifnot(identical(df_normalisation$pos,\n                    pos_start:pos_end))\n\n# Group the normalisation file into segments; take the mean normalisation in\n# each segment, and then re-normalise so the segment values sum to 1.\ndf_normalisation <- df_normalisation %>%\n  mutate(segment = as.character(cut(pos, breaks = segment_boundaries)))\ndf_segments <- df_normalisation %>%\n  group_by(segment) %>%\n  summarise(distance = mean(distance, na.rm = TRUE),\n            segment_start = min(pos),\n            segment_end = max(pos),\n            .groups = ""drop"") %>%\n  arrange(segment_start) %>%\n  mutate(fraction_snps_expected = distance / sum(distance))\n\n# Make a set of multinomial draws, with probabilities given by the normalisation\n# value for each segment. i.e. simulate how many SNPs expected in each segment\n# by chance: our null model.\nsnps_over_segments_simulation <- rmultinom(\n  n = num_multinomial_draws,\n  size = num_snps,\n  prob = df_segments$fraction_snps_expected\n)\n\n# Calculate the desired percentiles for the null model.\ndf_segments[paste0(""num_snps_percentile_"", percentiles)] <-\n  apply(snps_over_segments_simulation, 1, function(snp_counts) {\n  quantile(snp_counts, probs = percentiles / 100)\n  }) %>%\n  t()\n\n# Merge the actual SNP counts with the null model percentiles\nsnp_segment_counts <- table(cut(pos_snps, segment_boundaries)) \nstopifnot(identical(names(snp_segment_counts),\n                    df_segments$segment))\ndf_segments <- full_join(df_segments,\n                         tibble(segment = names(snp_segment_counts),\n                                snp_count = snp_segment_counts),\n                         by = ""segment"")\n\n# Plot!\nif (plot) {\np <- ggplot(df_segments) +\n  theme_classic() + \n  scale_y_continuous(limits = c(0, NA), expand = c(0, 0)) +\n  scale_x_continuous(limits = c(pos_start, pos_end), breaks = 1:9 * 1000, expand = c(0, 0)) +\n  geom_rect(aes(xmin = segment_start,\n                xmax = segment_end,\n                ymin = num_snps_percentile_25,\n                ymax = num_snps_percentile_75), fill = ""grey"", alpha = 1) +\n  geom_rect(aes(xmin = segment_start,\n                xmax = segment_end,\n                ymin = num_snps_percentile_2.5,\n                ymax = num_snps_percentile_97.5), fill = ""grey"", alpha = 0.7) +\n  geom_rect(aes(xmin = segment_start,\n                xmax = segment_end,\n                ymin = num_snps_percentile_0.5,\n                ymax = num_snps_percentile_99.5), fill = ""grey"", alpha = 0.4) +\n  geom_segment(aes(x = segment_start, xend = segment_end,\n                   y = snp_count, yend = snp_count), colour = ""black"") +\n  labs(x = ""genome position (HXB2)"",\n       y = ""number of nucleotide changes"")\nggsave(file_out_snps,\n       p, height = 4.5, width = 5.5)\n\np <- ggplot() +\n  theme_classic() + \n  scale_y_continuous(limits = c(0, NA), expand = c(0, 0)) +\n  scale_x_continuous(limits = c(pos_start, pos_end), breaks = 1:9 * 1000, expand = c(0, 0)) +\n  geom_point(data = tibble(x = pos_snps, y = 0), aes(x, y), shape = ""|"", size = 1.5) +\n  geom_line(data = df_normalisation, aes(x = pos, y = distance)) +\n  labs(x = ""genome position (HXB2)"",\n       y = ""evolutionary rate (substitutions per site)"")\nggsave(file_out_snps_raw,\n       p, height = 4.5, width = 5.5)\n}\n\n# After visual inspection, when 10 segments are used, segment 5 (with 32 snps) \n# deviates most from the null. Calculate its two-tailed p value, unadjusted for\n# multiple testing:\n2 * sum(snps_over_segments_simulation[5,] >= 32) / num_multinomial_draws\n# 2 * sum(snps_over_segments_simulation[9,] >= 21) / num_multinomial_draws # <- for when 20 segments are used\n', '# Author: Chris Wymant\n# Acknowledgment: I wrote this while funded by ERC Advanced Grant PBDR-339251\n# and a Li Ka Shing Foundation grant, both awarded to Christophe Fraser.\n# This script collects codon-level annotations for the HXB2 HIV genome and\n# transfers them to those positions where two other amino acid sequences (for\n# example in the present study, the consensus of the VB variant sequences and\n# the consensus of other Dutch subtype-B sequences in BEEHIVE) disagree with\n# each other. \n\n# Abbreviations:\n# AA = amino acid\n# df = dataframe\n# seq = sequence\n# aln = alignment\n\nlibrary(tidyverse)\nlibrary(seqinr)\n\n# WARNING: the command below deletes all objects in memory in your current R\n# session. Convenient when repeatedly re-running this script.\nrm(list = ls())\n\n# INPUT ------------------------------------------------------------------------\n\n# Set the working directory to where this code lives on your machine\n# setwd()\nsetwd(""~/hiv_hypervirulent_lineage_code/"")\n\n# The per-gene amino acid alignments with the VB variant consensus, the \n# consensus of other Dutch subtype-B in BEEHIVE, and HXB2, provided as\n# supplementary data with the associated paper.\nfiles_in_aln <- Sys.glob(""private_data_seqs/VB_vs_DutchB_*.fasta"")\n\nfile_in_hxb2_annotated_codons <- ""data_external/hxb2_annotated_SpecialSites_ByCodon.csv""\n\nfile_out_vb_differences <- ""VB_vs_Dutch_B_subsitutions_annotated.csv""\n\n# MAIN -------------------------------------------------------------------------\n\nstopifnot(file.exists(file_in_hxb2_annotated_codons))\nstopifnot(length(files_in_aln) > 0L)\n\n\n# Label alns by their gene\ngenes <- str_match(files_in_aln, ""VB_vs_DutchB_([a-z]{3}).fasta"")[, 2]\nnames(files_in_aln) <- genes\n\n# Read in the HXB2 annotation file. Check that each gene-codon is unique.\ndf_hxb2 <- read_csv(file_in_hxb2_annotated_codons, col_types = cols(\n  codon_in_that_gene = col_integer()\n)) %>%\n  rename(codon = codon_in_that_gene)\nstopifnot(!anyDuplicated(df_hxb2 %>% select(gene, codon)))\n\n# Iterate through genes, reading in that alignment, recording codons which\n# differ between the VB variant and the consensus of other Dutch subtype B seqs\n# in BEEHIVE, and transferring HXB2 annotations to those positions. Merge the\n# results from all genes into a single df.\ndf_differences <- map(genes, function(gene_) {\n  \n  cat(""Analysing alignment for"", gene_, ""\\n"")\n  \n  # Read in the aln, convert seqs to upper case\n  aln <- read.fasta(files_in_aln[[gene_]], seqtype = ""AA"") %>%\n    map(toupper)\n\n  # Check the three expected seq names are present\n  name_hxb2 <- ""HXB2""\n  name_dutch_b <- ""DutchSubtypeB""\n  name_lineage <- ""VB""\n  stopifnot(name_hxb2 %in% names(aln))\n  stopifnot(name_dutch_b %in% names(aln))\n  stopifnot(name_lineage %in% names(aln))\n  \n  seq_hxb2    <- aln[[name_hxb2]]\n  seq_dutch_b <- aln[[name_dutch_b]]\n  seq_lineage <- aln[[name_lineage]]\n  \n  # Check all seqs have the same length\n  length_aln <- length(seq_hxb2)\n  stopifnot(length_aln == length(seq_dutch_b))\n  stopifnot(length_aln == length(seq_lineage))\n  \n  # Check the HXB2 seq in the aln is what we expected\n  seq_hxb2_ungapped <- seq_hxb2[seq_hxb2 != ""-""]\n  seq_hxb2_expected <- df_hxb2 %>%\n    filter(gene == gene_) %>%\n    arrange(codon) %>%\n    pull(HXB2_amino_acid)\n  stopifnot(identical(seq_hxb2_ungapped,\n                      seq_hxb2_expected))\n  \n  # Get the HXB2 position of aln positions where the lineage and dutch B seqs\n  # disagree\n  aln_pos_differences <- integer()\n  hxb2_pos_differences <- integer()\n  hxb2_pos_current <- 0L\n  for (pos in 1:length_aln) {\n    if (seq_hxb2[[pos]] != ""-"") hxb2_pos_current <- hxb2_pos_current + 1L\n    if (seq_dutch_b[[pos]] == seq_lineage[[pos]] ||\n        seq_dutch_b[[pos]] == ""X"" ||\n        seq_lineage[[pos]] == ""X"") {\n      next\n    }\n    aln_pos_differences[[length(aln_pos_differences) + 1L]] <- pos\n    hxb2_pos_differences[[length(hxb2_pos_differences) + 1L]] <- hxb2_pos_current\n  }\n\n  # Record the AAs and positions of disagreements\n  df_gene <- tibble(gene = gene_,\n                    gene_position_hxb2 = hxb2_pos_differences,\n                    aa_hxb2 = seq_hxb2[aln_pos_differences],\n                    aa_DutchB_inBEEHIVE = seq_dutch_b[aln_pos_differences],\n                    aa_vb = seq_lineage[aln_pos_differences])\n  stopifnot(all(! df_gene$aa_DutchB_inBEEHIVE == df_gene$aa_vb))\n  \n  # Merge in annotation details \n  df_gene <- left_join(df_gene,\n                       df_hxb2 %>% filter(gene == gene_) %>% select(-gene),\n                       by = c(""gene_position_hxb2"" = ""codon""))\n  stopifnot(all(df_gene$aa_hxb2 == df_gene$HXB2_amino_acid |\n                  df_gene$aa_hxb2 == ""-""))\n  \n  df_gene %>% select(-HXB2_amino_acid)\n  \n}) %>%\n  bind_rows() %>%\n  arrange(gene)\n\n# Categorise differences either substitutions or indels\ndf_differences <- df_differences %>% \n  mutate(difference_type = if_else(aa_DutchB_inBEEHIVE != ""-"" & aa_vb != ""-"", \n                                   ""substitution"", ""indel""))\ntable(df_differences$difference_type)\n\n# Restrict the set of CTL escapes to amino acids that the VB variant has, i.e.\n# not just any amino acid at a site where the variant has some mutation.\nmax_num_escapes_per_codon <- df_differences %>% \n  pull(CTL_escape_details_for_q_less_than_1percent) %>%\n  str_count("";"") %>%\n  max(na.rm = TRUE) + 1\ndf_differences_specific_escapes <- df_differences %>% \n  select(gene, gene_position_hxb2, aa_vb, CTL_escape_details_for_q_less_than_1percent) %>%\n  separate(CTL_escape_details_for_q_less_than_1percent,\n           into = paste0(""Escape_"", 1:max_num_escapes_per_codon),\n           sep = ""; "", fill = ""right"") %>%\n  pivot_longer(starts_with(""Escape_""), values_to = ""escape"") %>%\n  filter(!is.na(escape)) %>%\n  select(-name) %>%\n  filter(substr(escape, 1, 1) == aa_vb) \n\n# Separate the mutations into adaptive (the amino acid is enriched in the \n# presence of the noted HLA type, indicated by a positive log odds for escape)\n# vs nonadaptive (the amino acid is depleted in the presence of the noted HLA\n# type, indicated by a negative log odds for escape).\ndf_differences_specific_escapes <- df_differences_specific_escapes %>%\n  mutate(adaptive =\n           str_detect(escape, ""and escape log odds [0-9]*\\\\.{0,1}[0-9]+ for HLA"")) %>%\n  group_by(gene, gene_position_hxb2, aa_vb, adaptive) %>%\n  summarise(escapes = paste(escape, collapse = ""; ""), .groups = ""drop"") %>%\n  mutate(escapes_adaptive    = if_else( adaptive, escapes, NA_character_),\n         escapes_nonadaptive = if_else(!adaptive, escapes, NA_character_)) %>%\n  select(-c(""adaptive"", ""escapes"")) %>%\n  group_by(gene, gene_position_hxb2, aa_vb) %>%\n  summarise(escapes_adaptive = paste(na.omit(escapes_adaptive), collapse = """"),\n            escapes_nonadaptive = paste(na.omit(escapes_nonadaptive), collapse = """"),\n            .groups = ""drop"") %>%\n  mutate(escapes_adaptive = if_else(escapes_adaptive == """", NA_character_,\n                                    escapes_adaptive),\n         escapes_nonadaptive = if_else(escapes_nonadaptive == """", NA_character_,\n                                    escapes_nonadaptive))\n\n# Merge the VB-specific CTL mutations back into the table with all mutations\nnrow_df_differences_before_merge <- nrow(df_differences)\ndf_differences <- left_join(df_differences, df_differences_specific_escapes,\n                            by = c(""gene"", ""gene_position_hxb2"", ""aa_vb""))\nif (nrow(df_differences) != nrow_df_differences_before_merge) {\n  stop(paste0(""Problem merging VB-specific CTL escape info into the df with all info""))\n}\n\n# Write output\nwrite_csv(df_differences %>% select(gene,\n                                    gene_position_hxb2,\n                                    HXB2_base_positions,\n                                    aa_hxb2,\n                                    aa_DutchB_inBEEHIVE,\n                                    aa_vb,\n                                    difference_type,\n                                    DRM_Los_Alamos,\n                                    DRM_IAS,\n                                    Other_features,\n                                    CTL_escape_details_for_q_less_than_1percent,\n                                    escapes_adaptive,\n                                    escapes_nonadaptive) %>%\n            replace_na(list(DRM_Los_Alamos = """",\n                            DRM_IAS = """",\n                            Other_features = """",\n                            CTL_escape_details_for_q_less_than_1percent = """",\n                            escapes_adaptive = """",\n                            escapes_nonadaptive = """")) %>%\n            rename(CTL_escape_all = CTL_escape_details_for_q_less_than_1percent,\n                   CTL_escape_vb_adaptive = escapes_adaptive,\n                   CTL_escape_vb_nonadaptive = escapes_nonadaptive),\n          file_out_vb_differences)\n\n# Print some numbers of mutations of interest\nnum_escapes <- df_differences %>% \n  mutate(escapes_either = (!is.na(escapes_adaptive)) | (!is.na(escapes_nonadaptive))) %>%\n  pull(escapes_either) %>%\n  sum()\nnum_escapes\nnum_escapes_both_directions <- df_differences %>% \n  mutate(escapes_both = (!is.na(escapes_adaptive)) & (!is.na(escapes_nonadaptive))) %>%\n  pull(escapes_both) %>%\n  sum()\nnum_escapes_both_directions\nnum_escapes_adaptive <- sum(!is.na(df_differences$escapes_adaptive))\nnum_escapes_adaptive\nnum_escapes_nonadaptive <- sum(!is.na(df_differences$escapes_nonadaptive))\nnum_escapes_nonadaptive\nnum_mutations_at_ctl_site <- sum(!is.na(df_differences$CTL_escape_details_for_q_less_than_1percent))\nnum_mutations_at_ctl_site\n\n# What are the HLA types for which the VB variant has adaptations?\nescapes_adaptive_hlas <- df_differences$escapes_adaptive %>%\n  na.omit() %>%\n  str_split(""; "") %>%\n  unlist()\nescapes_adaptive_hlas <- str_match(escapes_adaptive_hlas, pattern = ""for HLA (.+)"")[,2]\nescapes_adaptive_hlas\ntable_ <- table(escapes_adaptive_hlas)\ntibble(hla = names(table_),\n       count = table_) \n', '# Author: Chris Wymant\n# Acknowledgment: I wrote this while funded by ERC Advanced Grant PBDR-339251\n# and a Li Ka Shing Foundation grant, both awarded to Christophe Fraser.\n# Abbreviations: VL = viral load (log10 copies per ml), LMM = linear mixed model,\n# df = dataframe\n\nlibrary(tidyverse)\nlibrary(lme4)\n\n# WARNING: deletes all objects in your R session\'s memory\nrm(list = ls()) \n\n# Change directory appropriately:\nsetwd(""replace this bit by the path to your directory with the data"")\n\n# Read in the input csvs. For you, lucky reader, this is after much data\n# wrangling.\n# ""individual"" characteristics (one row per individual):\ndf_ind <- read_csv(""individual_summary.csv"", col_types = cols(\n  id_paper = col_factor(),\n  in_lineage = col_factor(),\n  diagnosis_period = col_factor(),\n  sex = col_factor(),\n  age_diagnosed = col_factor(),\n  death_at_censoring = col_logical()\n))\n# And CD4 counts (many per individual):\ndf_cd4_decline <- read_csv(""cd4_counts_pretreatment.csv"", col_types = cols(\n  id_paper = col_factor()\n))\n\n# Inspect the mean VL in each time period for lineage and not lineage (what we\n# plot in Figure 1a).\ndf_ind %>%\n  group_by(diagnosis_period, in_lineage) %>%\n  summarise(vl_group_mean = mean(vl_mean, na.rm = TRUE),\n            number_of_vl_measurements = sum(!is.na(vl_mean)))\n\n# Merge individual-level data into the CD4 counts.\nstopifnot(all(df_cd4_decline$id_paper %in% df_ind$id_paper))\nstopifnot(! anyNA(df_ind$in_lineage))\ndf_cd4_decline <- left_join(df_cd4_decline, df_ind, by = ""id_paper"") \n\n# Calculate the mean VL of the reference category: not-lineage males diagnosed \n# in their thirties. We\'ll measure VLs offset by this value, i.e. relative to \n# it, for the VL-adjusted LMM, for more easily interpretable regression\n# coefficients. \nmean_vl_reference_category <- df_ind %>%\n  filter(sex %in% ""male"",\n         age_diagnosed %in% ""[30, 40)"",\n         in_lineage %in% ""not.lineage"") %>%\n  pull(vl_mean) %>%\n  mean(na.rm = TRUE)\ndf_cd4_decline <- df_cd4_decline %>%\n  mutate(vl_ref_cat_diff = vl_mean - mean_vl_reference_category)\n\n# Define the reference category\ndf_cd4_decline <- df_cd4_decline %>%\n  mutate(sex = relevel(sex, ref = ""male""),\n         age_diagnosed = relevel(age_diagnosed, ref = ""[30, 40)""),\n         in_lineage = relevel(df_cd4_decline$in_lineage, ref = ""not.lineage""))\n\nlmm <- lmer(data = df_cd4_decline, \n            # Model CD4 counts as a linear function of time,\n            cd4_count ~ years_since_diagnosis +  \n              # with a fixed effect of age on the intercept,\n              age_diagnosed + \n              # a fixed effect of sex on both intercept and slope,\n              years_since_diagnosis * sex + \n              # a fixed effect of the lineage on both intercept and slope,\n              years_since_diagnosis * in_lineage + \n              # and a random effect of the individual on both intercept and slope.\n              (years_since_diagnosis | id_paper))  \nsummary(lmm)\n\n# Three ""simple/crude"" measures of the fraction of variance explained by the lmm.\n# Thank you Ben Bolker https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#how-do-i-compute-a-coefficient-of-determination-r2-or-an-analogue-for-glmms\n# Also note this on-point tweet https://twitter.com/GinaMCalabrese/status/1364591840754880515\nr2.corr.mer <- function(m) {\n  lmfit <-  lm(model.response(model.frame(m)) ~ fitted(m))\n  summary(lmfit)$r.squared\n}\nr2.corr.mer(lmm)\n1 - var(residuals(lmm)) / var(model.response(model.frame(lmm)))\ncor(model.response(model.frame(lmm)), predict(lmm, type = ""response""))^2\n\nlmm_adjusted_for_vl <- lmer(data = df_cd4_decline, \n                            # Model CD4 counts as a linear function of time,\n                            cd4_count ~ years_since_diagnosis +  \n                              # with a fixed effect of age on the intercept,\n                              age_diagnosed + \n                              # a fixed effect of sex on both intercept and slope,\n                              years_since_diagnosis * sex + \n                              # a fixed effect of the lineage on both intercept and slope,\n                              years_since_diagnosis * in_lineage + \n                              # a fixed effect of VL on both intercept and slope,\n                              years_since_diagnosis * vl_ref_cat_diff + \n                              # and a random effect of the individual on both intercept and slope.\n                              (years_since_diagnosis | id_paper))  \nsummary(lmm_adjusted_for_vl)\n\n# Calculate LMM confidence intervals. Takes a few minutes each.\nconfints <- confint(lmm)\nconfints_adjusted_for_vl <- confint(lmm_adjusted_for_vl)\nconfints\nconfints_adjusted_for_vl\n\n# Cox model for the survival analysis.\n# Test association between the hazard for death and age, sex and the lineage.\nfit.coxph <- coxph(data = df_survival,\n                   Surv(time = years_to_censoring, event = death_at_censoring) ~\n                      age_diagnosed + sex + in_lineage)\nsummary(fit.coxph)\n', '# Authors: Chris Wymant and Francois Blanquart \n# Acknowledgment: Chris wrote this while funded by ERC Advanced Grant PBDR-339251\n# and a Li Ka Shing Foundation grant, both awarded to Christophe Fraser.\n# Abbreviations: VL = viral load (log10 copies per ml), PC = principal component,\n# df = dataframe\n\nlibrary(ape)\nlibrary(phytools)\nlibrary(tidyverse)\n\n# WARNING: deletes all objects in your R session\'s memory\nrm(list = ls()) \n\n# Change directory appropriately:\nsetwd(""replace this bit by the path to your directory with the data"")\n\n# Dataframe with VLs and PCs. Loading takes a few seconds - 50MB \ndf <- read_csv(""BEEHIVE_VLs.csv"")\n\ntree <- read.tree(""BEEHIVE_tree.tree"")\n\n# Test association with VL for the first N PCs, with N chosen to taste\nnumber_pcs_to_test <- 50\ndf.pcs <- purrr::map(1:number_pcs_to_test, function(i) {\n  lm_ <- lm(df[[""BEEHIVE_LVL""]] ~ df[[paste0(""PC"", i)]])\n  c(summary(lm_)[[4]][2, ], confint(lm_)[2,])\n}) %>%\n  bind_rows() %>%\n  mutate(pc = factor(1:number_pcs_to_test)) %>%\n  mutate(pc = fct_reorder(pc, `Pr(>|t|)`)) %>%\n  mutate(is_lineage_pc = if_else(pc == ""5"", ""lineage"", ""other""))\n\nView(df.pcs)\n\n# Plot PC effect sizes on VL, with 95% confidence intervals\np <- ggplot(df.pcs %>% \n              mutate(`2.5 %` = if_else(Estimate < 0, -1 * `2.5 %`, `2.5 %`),\n                     `97.5 %` = if_else(Estimate < 0, -1 * `97.5 %`, `97.5 %`)) %>%\n              mutate(Estimate = abs(Estimate))) +\n  geom_point(aes(x = pc, y = Estimate, col = is_lineage_pc)) +\n  geom_errorbar(aes(x = pc, ymin = `2.5 %`, ymax = `97.5 %`, col = is_lineage_pc)) +\n  theme_classic() +\n  geom_hline(yintercept = 0, col = ""black"", linetype = ""dashed"") +\n  labs(x = ""Principal component (ordered by p value)"",\n       y = ""Size of association with VL"",\n       col = ""PC:"")\np\n\n# Choose a PC to plot on the tree. PC5 is the lineage.\nPC <- ""PC5""\n\n# Multiply the PC by minus one, purely for consistent red/blue aesthetics with\n# the paper. Set colours for the tree.\ndf[[PC]] <- -df[[PC]]\ncols <- df[[PC]]\nnames(cols) <- df$id_paper\n\n# Show the PC on the tree. We see PC5 is associated with a single long branch:\n# this branch defines the lineage.\nplotBranchbyTrait(ladderize(tree),\n                  cols, \n                  mode = ""tips"", \n                  edge.width = 0.4, \n                  show.tip.label = F, \n                  title = PC)\n']","A highly virulent variant of HIV-1 circulating in the Netherlands Analysis code for ""A highly virulent variant of HIV-1 circulating in the Netherlands"" by Wymant et al., Science",4
Data for: Frequent fire slows microbial decomposition of newly deposited fine fuels in a pyrophilic ecosystem,"Fire-plant feedbacks engineer recurrent fires in pyrophilic ecosystems like savannas. The mechanisms sustaining these feedbacks may be related to plant adaptations that trigger rapid responses to fire's effects on soil. Plants adapted for high fire frequencies should quickly regrow, flower, and produce seeds that mature rapidly and disperse post-fire. We hypothesized that offspring of such plants would germinate and grow rapidly, responding to fire-generated changes in soil nutrients and biota. We conducted an experiment using longleaf pine savanna plants that were paired based on differences in reproduction and survival under annual (""more"" pyrophilic) vs. less frequent (""less"" pyrophilic) fire regimes. Seeds were planted in different soil inoculations from experimental fires of varying severity. The ""more"" pyrophilic species displayed high germination rates followed by species-specific, rapid growth responses to soil location and fire severity effects on soils. In contrast, the ""less"" pyrophilic species had lower germination rates that were not responsive to soil treatments. This suggests that rapid germination and growth constitute adaptations to frequent fires and that plants respond differently to fire severity effects on soil abiotic factors and microbes. Further, variable plant responses to post-fire soils may influence plant community diversity and fire-fuel feedbacks in pyrophilic ecosystems.",,"Data for: Frequent fire slows microbial decomposition of newly deposited fine fuels in a pyrophilic ecosystem Fire-plant feedbacks engineer recurrent fires in pyrophilic ecosystems like savannas. The mechanisms sustaining these feedbacks may be related to plant adaptations that trigger rapid responses to fire's effects on soil. Plants adapted for high fire frequencies should quickly regrow, flower, and produce seeds that mature rapidly and disperse post-fire. We hypothesized that offspring of such plants would germinate and grow rapidly, responding to fire-generated changes in soil nutrients and biota. We conducted an experiment using longleaf pine savanna plants that were paired based on differences in reproduction and survival under annual (""more"" pyrophilic) vs. less frequent (""less"" pyrophilic) fire regimes. Seeds were planted in different soil inoculations from experimental fires of varying severity. The ""more"" pyrophilic species displayed high germination rates followed by species-specific, rapid growth responses to soil location and fire severity effects on soils. In contrast, the ""less"" pyrophilic species had lower germination rates that were not responsive to soil treatments. This suggests that rapid germination and growth constitute adaptations to frequent fires and that plants respond differently to fire severity effects on soil abiotic factors and microbes. Further, variable plant responses to post-fire soils may influence plant community diversity and fire-fuel feedbacks in pyrophilic ecosystems.",4
Testing for adaptive radiation: a new approach applied to Madagascar frogs,"Adaptive radiation is a key topic at the intersection of ecology and evolutionary biology. Yet the definition and identification of adaptive radiation both remain contentious. Here, we introduce a new approach for identifying adaptive radiations which combines key aspects of two widely used definitions. Our approach compares evolutionary rates in morphology, performance, and diversification between the candidate radiation and other clades. We then apply this approach to a putative adaptive radiation of frogs from Madagascar (Mantellidae). We collect new data on morphology and performance from mantellid frogs and compare rates of diversification and multivariate evolution of size, shape, and performance between mantellids and other frogs. We find that mantellids potentially pass our test for accelerated rates of evolution for shape, but not for size, performance, or diversification. Our results demonstrate that clades can have accelerated phenotypic evolution without rapid diversification (dubbed ""adaptive non-radiation""). We also highlight general issues in testing for adaptive radiation, including taxon sampling and the problem of including another adaptive radiation in the comparison clades. Finally, we suggest that similar tests should be conducted on other putative adaptive radiations on Madagascar, comparing their evolutionary rates to those of related clades outside Madagascar. Based on our results, we speculate that older Madagascar clades may show evolutionary patterns more similar to those on a continent than an island.",,"Testing for adaptive radiation: a new approach applied to Madagascar frogs Adaptive radiation is a key topic at the intersection of ecology and evolutionary biology. Yet the definition and identification of adaptive radiation both remain contentious. Here, we introduce a new approach for identifying adaptive radiations which combines key aspects of two widely used definitions. Our approach compares evolutionary rates in morphology, performance, and diversification between the candidate radiation and other clades. We then apply this approach to a putative adaptive radiation of frogs from Madagascar (Mantellidae). We collect new data on morphology and performance from mantellid frogs and compare rates of diversification and multivariate evolution of size, shape, and performance between mantellids and other frogs. We find that mantellids potentially pass our test for accelerated rates of evolution for shape, but not for size, performance, or diversification. Our results demonstrate that clades can have accelerated phenotypic evolution without rapid diversification (dubbed ""adaptive non-radiation""). We also highlight general issues in testing for adaptive radiation, including taxon sampling and the problem of including another adaptive radiation in the comparison clades. Finally, we suggest that similar tests should be conducted on other putative adaptive radiations on Madagascar, comparing their evolutionary rates to those of related clades outside Madagascar. Based on our results, we speculate that older Madagascar clades may show evolutionary patterns more similar to those on a continent than an island.",4
Resolving the consequences of gradual phenotypic plasticity for populations in variable environments,"Phenotypic adjustments following environmental change are ubiquitous and trait changes arising through phenotypic plasticity often lag behind their environmental stimuli. Evolutionary biologists seeking to understand how adaptive plasticity can evolve have extensively studied this phenomenon. However, the ecological consequences of common features of plastic responses to environmental variability, including gradual phenotypic change (i.e., slower than the pace of environmental change), are underappreciated. We present a framework based on the unifying concept of phenotype x environment performance landscapes that encompasses gradual plasticity. Then, we experimentally investigate the environmental contexts where gradual plasticity is important, using freshwater phytoplankton populations exposed to thermal variation. Finally, based on our conceptual framework, we develop a mathematical model of gradual plasticity that explains population dynamics in variable environments better than common alternative models. Understanding and accounting for the ecological effects of plasticity in variable environments is critical to making vital predictions and advancing ecology.",,"Resolving the consequences of gradual phenotypic plasticity for populations in variable environments Phenotypic adjustments following environmental change are ubiquitous and trait changes arising through phenotypic plasticity often lag behind their environmental stimuli. Evolutionary biologists seeking to understand how adaptive plasticity can evolve have extensively studied this phenomenon. However, the ecological consequences of common features of plastic responses to environmental variability, including gradual phenotypic change (i.e., slower than the pace of environmental change), are underappreciated. We present a framework based on the unifying concept of phenotype x environment performance landscapes that encompasses gradual plasticity. Then, we experimentally investigate the environmental contexts where gradual plasticity is important, using freshwater phytoplankton populations exposed to thermal variation. Finally, based on our conceptual framework, we develop a mathematical model of gradual plasticity that explains population dynamics in variable environments better than common alternative models. Understanding and accounting for the ecological effects of plasticity in variable environments is critical to making vital predictions and advancing ecology.",4
Coalescent-based species delimitation is sensitive to geographic sampling and isolation by distance,"Species are a fundamental unit of biodiversity that are delimited via genetic data and coalescent-based methods with increasing frequency. Despite the widespread use of coalescent-based species delimitation, we do not fully understand the sensitivity of these methods to potential sources of bias and violations of their underlying assumptions. One implicit assumption of coalescent-based species delimitation is that geographic sampling is adequate and representative of genetic variation among populations within the lineage of interest. Yet exhaustive geographic sampling is logistically difficult, if not impossible, for many taxa that span large geographic expanses or occupy remote regions. Here, we examine the impact of geographic sampling on the output of Bayes-factor delimitation with SNAPP, a popular coalescent-based species delimitation pipeline. First, we demonstrate the problematic nature of sparse geographic sampling and isolation by distance for species delimitation using simulated data sets of populations connected by different levels of gene flow. We then examine whether similar trends are present in an empirical dataset of Andesiops mayflies (Ephemeroptera: Baetidae) from a high elevation transect in the Ecuadorian Andes. In both the simulated and empirical analyses, we systematically exclude geographically intermediate sites to quantify the impact of geographic sampling and isolation by distance on coalescent-based species delimitation. We find that removing intermediate sites with genetically admixed individuals incorrectly favors multi-species delimitation scenarios. Oversplitting is especially pronounced when isolation by distance is strong, but exists even when gene flow among neighboring populations is relatively high. These findings highlight the importance of adequate geographic sampling in species delimitation and urge caution in interpreting the output of such methods when species' distributions are sparsely sampled and in systems characterized by strong patterns of isolation by distance.","['### Set up SNAPP files for testing sensitivity to geographic sampling in APE_ECP drainage ###\n### Libraries ###\nrequire(""adegenet"")\n\n### remove scientific notation ###\noptions(scipen=999)\n\n### Read in bigger GenePop flie\nog<-read.genepop(""~/Dropbox/InsectTaxa/genepop/Ape.gen"")\nv1<-which(sapply(strsplit(rownames(og@tab),""_""),function(x) x[3]) ==""ECA4248"")\nv2<-grep(""ECP"",sapply(strsplit(rownames(og@tab),""_""),function(x) x[3]))\ngp<-og[c(v1,v2),]\n\n### Read in CLUMPP K 2 file to get Q scores for taxon assignment ###\necp<-read.genepop(""~/Dropbox/InsectTaxa/genepop/ApeECP.gen"")\necp_k2<-readLines(file(""~/Dropbox/InsectTaxa/dapc_and_structure/Ape_ECP_Struc/StructureHarvesterOutput 2/K2_CLUMPP.OUT""))\n\nq<-sapply(strsplit(ecp_k2,"":""),function(x) x[length(x)])\nq_df<-data.frame(lapply(strsplit(q,"" ""),function(x) x[3:4]),stringsAsFactors=F)\ncolnames(q_df)<-NULL\necp_k2_q_df<-as.matrix(q_df)\nQ_vec<-as.numeric(ecp_k2_q_df[1,])\nnames(Q_vec)<-row.names(ecp@tab)\n\n### SET UP THIS DATA FRAME TO INCLUDE THE OUTGRUP !!!!\nass<-data.frame(row.names=rownames(ecp@tab),taxon=paste(""taxon"",as.numeric(Q_vec>0.5)+1,sep=""""))\nass<-rbind(ass,data.frame(row.names=rownames(og@tab)[v1],taxon=rep(""taxon3"",length(v1))) )\nass<-cbind(ass,site=sapply(strsplit(row.names(ass),""_""),function(x) x[3]))\n\n## Remove outermost 4 populations to facilitate running SNAPP ###\nass<-ass[!ass[,2] %in% levels(ass[,2])[c(2,3,13,14)],]\nass[,2]<-factor(ass[,2])\n\n### Set up basic parameters\nn_ingroup<-36\nn_per_pop<-4\nn_loci<-100\n\n### Create list for subsampling scenarios ###\nscen_list<-list()\nscen_list[[1]]<-levels(ass[,2])[-1][rep(T,9)]\nscen_list[[2]]<-levels(ass[,2])[-1][c(rep(T,4),F,rep(T,4))]\nscen_list[[3]]<-levels(ass[,2])[-1][c(rep(T,3),rep(F,3),rep(T,3))]\nscen_list[[4]]<-levels(ass[,2])[-1][c(rep(T,2),rep(F,5),rep(T,2))]\n\n#GenePop2SNAPP<-function(inFile,outFile,assignmentFile,n_per_pop,n_loci,writeFile=T){\n#gp<-read.genepop(inFile)\n\ngenotypes<-gp@tab\ngenotypes[is.na(genotypes)]<-""?""\n\t\nnpop<-length(unique(ass[,2]))\n\n### Remove individuals with > 50% missing data, reexamine sample sizes per site \nmissing_data<-vector()\nfor(i in 1:nrow(genotypes)){\n\tmissing_data[i]<-table(genotypes[i,])[""?""]/sum(table(genotypes[i,]))\n}\n\nmissing_data\n\ngp_red<-gp[which(missing_data<0.5),]\n\ntable(ass[which(missing_data<0.5),2]) ### Still enough to get random samples \n\n### Create n_rep Replicates of Random samples with n_per_pop for each population and all of outgroup ###\nog_samp<-sample(rownames(ass)[ass$site==""ECA4248""],8) #Keep same OGs for all sampling scenarios\n\n### SET UP PARENTAL POPS TO BE PURE PARENTALS ###\nparental_list<-list()\nfor(i in 1:length(scen_list[[4]])){\n\tfoo<-Q_vec[sapply(strsplit(names(Q_vec),""_""),function(x) x[3]) %in% scen_list[[4]][i]]\n\tfoo<-foo[names(foo) %in% row.names(gp_red@tab)]\n\tparental_list[[i]]<-names(foo[foo < 0.025 | foo > 0.975])\n}\nnames(parental_list)<-scen_list[[4]]\n\n### Set up admixed indivuals to have at least some level of admixture ###\nadmixed_list<-list()\nadmixed_pops<-scen_list[[1]][!scen_list[[1]]%in%scen_list[[4]]]\nfor(i in 1:length(admixed_pops)){\n\tfoo<-Q_vec[sapply(strsplit(names(Q_vec),""_""),function(x) x[3]) %in% admixed_pops[i]]\n\tadmixed_list[[i]]<-names(sort(abs(foo-0.5)))[names(sort(abs(foo-0.5))) %in% row.names(gp_red@tab)]\n}\nnames(admixed_list)<-admixed_pops\n\nscen_list\n\nsamp_list <-list()\nsamp_list[[1]]<-list()\nsamp_list[[1]][[1]]<-og_samp\nfor(j in 2:length(levels(ass[,2]))){\n\tif(levels(ass[,2])[j] %in% names(parental_list)){\n\t\tsamp_list[[1]][[j]]<-sample(parental_list[[levels(ass[,2])[j]]], 9) \n\t\t}else{\n\t\t\tif(levels(ass[,2])[j] %in% names(admixed_list)){\n\t\t\t\tsamp_list[[1]][[j]]<-admixed_list[[levels(ass[,2])[j]]][1:9]\n\t\t\t}\n\t\t}\t\n\t}\n\t\t\t\t\nnames(samp_list[[1]])<-levels(ass[,2])\n\nfor(i in 2:4){\n\tsamp_list[[i]]<-list()\n\tsamp_list[[i]]<-samp_list[[1]]\n}\n\nfor(i in 1:length(scen_list)){\n\tnperpop<-floor(n_ingroup/length(which(names(samp_list[[i]])[-1] %in% c(scen_list[[i]]))))\n\tfor(j in 2:length(levels(ass[,2]))){\n\t\tif(names(samp_list[[i]])[j] %in% scen_list[[i]]){\n\t\t\tsamp_list[[i]][[names(samp_list[[i]])[j]]]<-samp_list[[i]][[names(samp_list[[i]])[j]]][1:nperpop]\n\t\t\tsamp_list[[i]][[names(samp_list[[i]])[j]]]<-sample(samp_list[[i]][[names(samp_list[[i]])[j]]],length(samp_list[[i]][[names(samp_list[[i]])[j]]]))\n\t\t}else{\n\t\t\tsamp_list[[i]][[names(samp_list[[i]])[j]]]<-NA\n\t\t}\t\n\t}\n\tsamp_list[[i]]<-samp_list[[i]][!sapply(samp_list[[i]],function(x) is.na(x[1]))]\n}\t\nsapply(samp_list,function(x) length(unlist(x)))\n\ngp_list<-list()\nfor(i in 1:length(samp_list)){\n\tgp_list[[i]]<-gp_red[unlist(samp_list[[i]]),]\n}\n\n### Subsample loci to maximize data availability and minor allele frequencies ###\ngp_red2<-gp_red[unlist(samp_list)[!duplicated(unlist(samp_list))],]\n\nmissing_data<-list()\nmaf<-vector()\nfor(j in 1:length(levels(gp_red2@loc.fac))){\n\tmissing_data[[j]]<-vector()\n\tfor(k in 1:length(levels(gp_red2@pop))){\n\t\tfoo<-gp_red2@tab[which(gp_red2@pop ==levels(gp_red2@pop)[k]),which(gp_red2@loc.fac == levels(gp_red@loc.fac)[j])]\n\t\tmissing_data[[j]][k]<-length(which(is.na(foo[,1])))/nrow(foo)\n\t}\n\tfoo<-gp_red2@tab[,which(gp_red2@loc.fac == levels(gp_red@loc.fac)[j])]\n\tbar<-apply(foo,2,function(x) sum(x,na.rm=T))\n\tmaf[j]<-min(bar)/sum(bar)\n}\n\n\nloci_filt<-sample(levels(gp_list[[i]]@loc.fac)[which(!sapply(missing_data,function(x) any(x==1)) & maf > 0.05)],100)\n\n### Filter by missing data and MAF for whole group ###\ngp_locisel_list<-list()\nfor(i in 1:length(gp_list)){\n\t#Constrain to biallelic loci#\n\tgp_locisel_list[[i]]<-gp_list[[i]][,gp_list[[i]]@loc.fac %in% names(gp_list[[i]]@loc.n.all)[which(gp_list[[i]]@loc.n.all == 2)]]\n\t\n\t#Filter to those loci included above#\n\tgp_locisel_list[[i]]<-gp_list[[i]][,gp_list[[i]]@loc.fac %in% loci_filt]\n\t}\n\t\t\t\nrownames(gp_locisel_list[[1]]@tab) == \tunlist(samp_list[[1]]) #ALL TRUE ALL GOOD\n\t\n### We\'ve subsampled loci for each replicate ### Now to create xml output files for each replicate, each geosampling scenario, and each speciesdelim scenario\n\t\n# ### Create subsample data sets ###\n# ### Create subsampling figure ###\n# pdf(file=""~/Desktop/Manuscripts/SpeciesDelimitationSensitivity/Figures/sampscheme_v2.pdf"",width=3.25,height=2)\n# #quartz()\n# par(mfrow=c(4,1))\n# par(mar=c(0,0,0,0))\n# plot(1:length(levels(ass[,2])[-1]), rep(9,length(levels(ass[,2])[-1])),cex=3,axes=F,xlab="""",ylab="""",pch=21,bg=""black"",col=""black"")\n# mtext(""Sampling Scenario 1"",side=1,cex=0.7,line=-1)\n# plot(1:length(levels(ass[,2])[-1]), rep(9,length(levels(ass[,2])[-1])),cex=3,axes=F,xlab="""",ylab="""",pch=21,bg=c(rep(""black"",4),""white"",rep(""black"",4)), col=""black"")\n# mtext(""Sampling Scenario 2"",side=1,cex=0.7,line=-1)\n# plot(1:length(levels(ass[,2])[-1]), rep(9,length(levels(ass[,2])[-1])),cex=3,axes=F,xlab="""",ylab="""",pch=21,bg=c(rep(""black"",3),rep(""white"",3),rep(""black"",3)), col=""black"")\n# mtext(""Sampling Scenario 3"",side=1,cex=0.7,line=-1)\n# plot(1:length(levels(ass[,2])[-1]), rep(9,length(levels(ass[,2])[-1])),cex=3,axes=F,xlab="""",ylab="""",pch=21,bg=c(rep(""black"",2),rep(""white"",5),rep(""black"",2)), col=""black"")\n# mtext(""Sampling Scenario 4"",side=1,cex=0.7,line=-1)\n# dev.off()\n### Create nexus file to generate appropriate priors ###\n\nfor(m in 1:length(gp_locisel_list)){\n\tnexus_loci_list<-list()\n\tfor(i in 1:length(levels(gp_locisel_list[[m]]@loc.fac))){\n\t\tfoo<-gp_locisel_list[[m]]@tab[,gp_locisel_list[[m]]@loc.fac==levels(gp_locisel_list[[m]]@loc.fac)[i]][,1]\n\t\tfoo[is.na(foo)]<-""?""\n\t\tnexus_loci_list[[i]]<-foo\n\t}\n\t\n\tnexus_df<-data.frame(nexus_loci_list,stringsAsFactors=F)\n\tcolnames(nexus_df)<-names(gp_locisel_list[[m]]@all.names)\n\t\n\tsink(paste0(""~/Desktop/SNAPP_SamplingScenario_"",m,"".nex""))\n\tcat(""#NEXUS\\n"")\n\tcat(""\\n"")\n\tcat(""BEGIN DATA;"")\n\tcat(""\\n"")\n\tcat(paste0(""\\tDIMENSIONS NTAX="",nrow(nexus_df),"" NCHAR="",ncol(nexus_df),"";\\n""))\n\tcat(""\\tFORMAT DATATYPE=INTEGER SYMBOLS=\\""012\\"" MISSING=? GAP=- INTERLEAVE=NO;\\n"")\n\tcat(""\\tMATRIX\\n"")\n\t\n\tfor(i in 1:nrow(nexus_df)){\n\t\t\tcat(paste0(""\\t\\t"",rownames(nexus_df)[i],""\\t\\t\\t\\t"",paste(nexus_df[i,],collapse=""""),""\\n""))\t\n\t}\n\tcat(""\\t\\t;\\n"")\n\tcat(""END;"")\n\tsink()\n}\n\ngp_locisel_list[[1]]\n\n### For each replicate, each sampling scenario, and each species delimitation scenario, generate an xml file for path sampling analysis ###\n### Read template xml ###\ntemplate<-readLines(file(""~/Desktop/Manuscripts/SpeciesDelimitationSensitivity/SNAPP/Empirical/Input/Rep_1_sampscen_4_spDscen_2_100loci_v13.xml""))\n\n#### Set up variables ###\nid_name<-""test""\nn_steps_bfd<-24\nalpha<-0.3\nchainLength<-500000\nburnInPercentage<-20\npreBurnin<-50000\n\ntemplate<-gsub(""test"",id_name,template)\n\n\n#for(k in 1:length(gp_locisel_list)){\n\tk<-1\n\t\nfor(m in 1:length(gp_locisel_list)){\n\n### Create separate species assignment runs\nfor(n in 1:2){\n\nsink(file=paste0(""~/Desktop/Manuscripts/SpeciesDelimitationSensitivity/SNAPP/Rep_"",k,""_sampscen_"",m,""_spDscen_"",n,""_100loci_v15.xml""))\n\ncat(template[1:4],sep=""\\n"")\n\n### print sequence data for all individuals included ###\nfor(i in 1:nrow(gp_locisel_list[[m]]@tab)){\n\tseqfoo<-vector()\n\tfor(j in 1:n_loci){\n\t\tseqfoo <-c(seqfoo,gp_locisel_list[[m]]@tab[i,gp_locisel_list[[m]]@loc.fac==names(gp_locisel_list[[m]]@all.names)[j]][1])\n\t}\n\t\n\tseqfoo[is.na(seqfoo)]<-""?""\t\n\tseqfoo<-paste(seqfoo,collapse="""")\n\tgrep(""[?]"",strsplit(seqfoo,"""")[[1]])\n\tcat(""\\t\\t\\t"")\n\tcat(""<sequence id=\\"""")\n\tcat(paste(""seq"", rownames(gp_locisel_list[[m]]@tab)[i], ass[rownames(gp_locisel_list[[m]]@tab)[i],1],sep=""_""))\n\tcat(""\\"" taxon=\\"""")\n\tcat(paste(rownames(gp_locisel_list[[m]]@tab)[i], ass[rownames(gp_locisel_list[[m]]@tab)[i],1],sep=""_""))\n\tcat(""\\"" totalcount=\\""3\\"" value=\\"""")\n\tcat(seqfoo)\n\tcat(""\\""/>"")\n\tcat(""\\n"")\n\t\n}\n\ncat(""\\n"")\ncat(""\\t</data>"")\ncat(""\\n\\n"")\n\ncat(template[grep(""<map name"",template)[1]:grep(""<map name"",template)[length(grep(""<map name"",template))]],sep=""\\n"")\n\n### Set up BFD Parameters ###\ncat(""\\n"")\n\ncat(""<run spec=\\""beast.inference.PathSampler\\""\\n"")\ncat(paste0(""\\tchainLength=\\"""",as.character(chainLength),""\\""\\n""))\ncat(paste0(""\\talpha=\\"""",alpha,""\\""\\n""))\ncat(paste0(""\\trootdir=\\""""))\ncat(paste0(""/workdir/nam232/Rep_"",k,""_sampscen_"",m,""_spDscen_"",n,""\\""""))\ncat(""\\n"")\ncat(paste0(""\\tburnInPercentage=\\"""", burnInPercentage,""\\""\\n""))\ncat(paste0(""\\tpreBurnin=\\"""", preBurnin,""\\""\\n""))\ncat(""\\tdeleteOldLogs=\\""true\\""\\n"")\ncat(paste0(""\\tnrOfSteps=\\"""", n_steps_bfd,""\\"">\\n""))\ncat(""\\n"")\n\ncat(template[grep(""cd "",template):grep(""<rawdata"",template)],sep=""\\n"")\n\nif(n==1){\n\tcat(""\\t\\t\\t\\t<taxonset id=\\""taxon_three\\"" spec=\\""TaxonSet\\"">\\n"")\n\tog<-grep(""ECA"",rownames(gp_locisel_list[[m]]@tab),value=T)\n\t\tfor(i in 1:length(og)){\n\t\t\tcat(""\\t\\t\\t\\t\\t<taxon id=\\"""")\n\t\t\tcat(paste(og[i],ass[og[i],1],sep=""_""))\n\t\t\tcat(""\\"" spec=\\""Taxon\\""/>"")\n\t\t\tcat(""\\n"")\n\t\t}\n\tcat(""\\t\\t\\t\\t</taxonset>\\n"")\n\tcat(""\\t\\t\\t\\t<taxonset id=\\""taxon_onetwocomb\\"" spec=\\""TaxonSet\\"">\\n"")\n\t\n\tig<-rownames(gp_locisel_list[[m]]@tab)[!rownames(gp_locisel_list[[m]]@tab) %in% og]\n\t\n\tfor(i in 1:length(ig)){\n\t\t\tcat(""\\t\\t\\t\\t\\t<taxon id=\\"""")\n\t\t\tcat(paste(ig[i],ass[ig[i],1],sep=""_""))\n\t\t\tcat(""\\"" spec=\\""Taxon\\""/>"")\n\t\t\tcat(""\\n"")\n\t\t}\n\t\t\t\n\tcat(""\\t\\t\\t\\t</taxonset>\\n"")\n\t}else{\n\t\tcat(""\\t\\t\\t\\t<taxonset id=\\""taxon_three\\"" spec=\\""TaxonSet\\"">\\n"")\n\t\tog<-grep(""ECA"",rownames(gp_locisel_list[[m]]@tab),value=T)\n\t\t\tfor(i in 1:length(og)){\n\t\t\t\tcat(""\\t\\t\\t\\t\\t<taxon id=\\"""")\n\t\t\t\tcat(paste(og[i],ass[og[i],1],sep=""_""))\n\t\t\t\tcat(""\\"" spec=\\""Taxon\\""/>"")\n\t\t\t\tcat(""\\n"")\n\t\t\t}\n\t\tcat(""\\t\\t\\t\\t</taxonset>\\n"")\n\t\tcat(""\\t\\t\\t\\t<taxonset id=\\""taxon_one\\"" spec=\\""TaxonSet\\"">\\n"")\n\t\n\t\tt1<-rownames(ass)[ass[,1]==""taxon1""] [rownames(ass)[ass[,1]==""taxon1""] %in% rownames(gp_locisel_list[[m]]@tab)]\n\t\t\tfor(i in 1:length(t1)){\n\t\t\t\tcat(""\\t\\t\\t\\t\\t<taxon id=\\"""")\n\t\t\t\tcat(paste(t1[i],ass[t1[i],1],sep=""_""))\n\t\t\t\tcat(""\\"" spec=\\""Taxon\\""/>"")\n\t\t\t\tcat(""\\n"")\n\t\t\t}\n\t\t\n\t\tcat(""\\t\\t\\t\\t</taxonset>\\n"")\n\t\tcat(""\\t\\t\\t\\t<taxonset id=\\""taxon_two\\"" spec=\\""TaxonSet\\"">\\n"")\n\t\t\n\t\tt2<-rownames(ass)[ass[,1]==""taxon2""] [rownames(ass)[ass[,1]==""taxon2""] %in% rownames(gp_locisel_list[[m]]@tab)]\n\t\t\tfor(i in 1:length(t2)){\n\t\t\t\tcat(""\\t\\t\\t\\t\\t<taxon id=\\"""")\n\t\t\t\tcat(paste(t2[i],ass[t2[i],1],sep=""_""))\n\t\t\t\tcat(""\\"" spec=\\""Taxon\\""/>"")\n\t\t\t\tcat(""\\n"")\n\t\t\t}\n\t\t\tcat(""\\t\\t\\t\\t</taxonset>\\n"")\n\n\t\t\t\t\t\t\n\t}\n\t\n\tcat(template[grep(""</taxa>"",template):length(template)],sep=""\\n"")\n\tsink()\n}\t\n\n}\n\n### Create batch running script ###\n### Medium memory has 40 cores, use 32 / 8 = 4 per run ###\n\nsink(""~/Desktop/bfd_v6.sh"")\nfor(m in 1:length(gp_locisel_list)){\n\tfor(n in 1:2){\n\t\tcat(paste0(""nohup java -jar /programs/beast/lib/beast.jar -threads 4 /workdir/nam232/Rep_"",k,""_sampscen_"",m,""_spDscen_"",n,""_100loci_v15.xml > nohup_"",m,""_"",n,"".out &\\n""))\n\t}\n}\nsink()\n\n### Examine Q Scores for individuals in this run ###\nfoo<-na.omit(Q_vec[row.names(gp_locisel_list[[1]]@tab)])\nbg_vec<-c(rep(""red"",8),rep(""purple"",20),rep(""blue"",8))\n\nseq(1,length(foo),4)\n\n### Heuristic figure ###\npdf(file=""~/Desktop/Manuscripts/SpeciesDelimitationSensitivity/Figures/sampscheme_v4.pdf"",width=6.5,height=4)\nquartz(width=6.5,height=4)\nlayout(matrix(c(1,1,1,1,2,3,4,5),ncol=2))\npar(mar=c(4,4,0.5,0.5))\nplot(1:length(foo),foo,pch=21,bg=bg_vec,cex=2,xlab=""Sampling Site"",ylab=""Q score"")\n\npar(mar=c(0,2,0,2))\npar(xpd=NA)\nplot(1:length(levels(ass[,2])[-1]), rep(9,length(levels(ass[,2])[-1])),cex=3,axes=F,xlab="""",ylab="""",pch=21,bg=c(rep(""red"",2),rep(""purple"",5),rep(""blue"",2)), col=""black"")\nmtext(""Sampling Scenario 1"",side=1,cex=0.7,line=-1)\nplot(1:length(levels(ass[,2])[-1]), rep(9,length(levels(ass[,2])[-1])),cex=3,axes=F,xlab="""",ylab="""",pch=21,bg=c(rep(""red"",2),""purple"",""purple"",""white"",""purple"",""purple"",rep(""blue"",2)), col=""black"")\nmtext(""Sampling Scenario 2"",side=1,cex=0.7,line=-1)\n\nplot(1:length(levels(ass[,2])[-1]), rep(9,length(levels(ass[,2])[-1])),cex=3,axes=F,xlab="""",ylab="""",pch=21,bg=c(rep(""red"",2),""purple"",""white"",""white"",""white"",""purple"",rep(""blue"",2)), col=""black"")\nmtext(""Sampling Scenario 3"",side=1,cex=0.7,line=-1)\nplot(1:length(levels(ass[,2])[-1]), rep(9,length(levels(ass[,2])[-1])),cex=3,axes=F,xlab="""",ylab="""",pch=21,bg=c(rep(""red"",2),""white"",""white"",""white"",""white"",""white"",rep(""blue"",2)), col=""black"")\nmtext(""Sampling Scenario 4"",side=1,cex=0.7,line=-1)\n\n\ndev.off()\n\n\nadmixed_list[1]\nrep_list[[1]]\n', 'require(RColorBrewer)\nrequire(plotrix)\n\n### Read in Empirical Data ###\nempirical_out<-list.files(""~/Desktop/Manuscripts/SpeciesDelimitationSensitivity/SNAPP/Empirical/Output_v2"",pattern="".out"",full.names=T)\n\nempirical_mL<-vector()\nempirical_bf<-vector()\nfor(i in 1:length(empirical_out)){\n\tfoo<-readLines(file(empirical_out[i]))\n\tbar<-strsplit(grep(""marginal L"",foo,value=T),"" "")[[1]]\n\tempirical_mL[i]<-as.numeric(bar[length(bar)])\n}\nngss<-length(empirical_mL)/2 #ngs = Number of Geographic Sampling Schemes\n\nfor(i in 1: ngss){\n\tfoo<-empirical_mL[(2*((1:ngss)-1)+1)[i]:(2*((1:ngss)-1)+2)[i]]\n\tempirical_bf[i]<-2*(foo[1]-foo[2])\n}\nempirical_mL\nempirical_bf\n\n### Analyze SNAPP simulated output for BFD and generate figure ###\nm_vec<-c(0.5,1,5,10,50,100)\n\n### Read in output files ###\noutput_list<-list()\nfor(i in 1:length(m_vec)){\noutput_list[[i]]<-list.files(""~/Desktop/Manuscripts/SpeciesDelimitationSensitivity/SNAPP/Simulated/Output"",pattern=""SpDelim[12].out"",full.names=T)\noutput_list[[i]]<-output_list[[i]][grep(paste(""_"",m_vec[i],""_"",sep=""""),output_list[[i]])]\n}\noutput_list\n\nmL_list<-list()\nbf_list<-list()\n\nfor(j in 1:length(output_list)){\t\n\tmL_list[[j]]<-vector()\n\tbf_list[[j]]<-vector()\n\tfor(i in 1:length(output_list[[j]])){\n\t\tfoo<-readLines(file(output_list[[j]][i]))\n\t\tbar<-strsplit(grep(""marginal L"",foo,value=T),"" "")[[1]]\n\t\tmL_list[[j]][i]<-as.numeric(bar[length(bar)])\n\t}\n\tngss<-length(mL_list[[j]])/2 #ngs = Number of Geographic Sampling Schemes\n\n\tfor(i in 1: ngss){\n\t\tfoo<-mL_list[[j]][(2*((1:ngss)-1)+1)[i]:(2*((1:ngss)-1)+2)[i]]\n\t\tbf_list[[j]][i]<-2*(foo[1]-foo[2])\n\t}\n\t\n}\n\nmL_list\nbf_list\n\n### Make plot with all levels of m investigated and included here ###\npng(file=""~/Desktop/Manuscripts/SpeciesDelimitationSensitivity/Figures/BFD_results_v5.png"",width=3.25,height=8,units=""in"",res=500)\n#quartz(width=3.25,height=8)\n\nlayout(matrix(c(1,2,3),nrow=3),heights=c(0.35,0.35,0.3))\n\n### Simulated data scatterplot ###\npar(mar=c(1,4,1,1))\nplot(0,0,type=""n"",ylim=c(-810,810),xlim=c(1,4),xlab="" "",ylab="" "",axes=F)\naxis(1,at=1:4,labels=F,cex=0.8, tck=-0.025)\naxis(2,at=seq(-800,800,100),cex.axis=0.8,mgp=c(3,0.5,0),tck=-0.025)\nmtext(""Bayes Factor"",2,line=2.5,cex=1)\npar(xpd=NA)\ntext(""One Species"",srt=90,cex=1,x=0.55,y=500,col=""forest green"")\ntext(""Two Species"",srt=90,cex=1,x=0.55,y=-500,col=""orange"")\n\nbox()\n\npar(xpd=F)\nabline(h=0,lty=2)\n\nfor(i in 1:length(bf_list)){\n\tpoints(bf_list[[i]],type=""l"",col=brewer.pal(9,""Blues"")[-c(1,2,3)][i],lwd=0.75)\n\tpoints(bf_list[[i]],bg=paste(brewer.pal(9,""Blues"")[-c(1,2,3)][i],""90"",sep=""""),col=""black"",cex=2,lwd=0.5,pch=c(21:25,21:25,21:25)[i])\n}\nlegend(""topright"",legend=rev(paste(""m = "",m_vec,sep="""")),pch=rev(c(21:25,21:25,21:25)[1:length(bf_list)]),pt.bg=paste(rev(brewer.pal(9,""Blues"")[-c(1,2,3)][1:6]),""90"",sep=""""),pt.cex=1.5,pt.lwd=0.5,cex=0.8,y.intersp=1.2)\n\npar(xpd=NA)\ntext(x=par()$usr[1]*0.45,y=par()$usr[4]*1.0,label=""A"",cex=2,font=2)\ntext(x=par()$usr[1],y=par()$usr[4]-(par()$usr[4]-par()$usr[3])*0.025,label="" Simulated"",cex=1,font=1,adj=c(0,0.5))\n\n\n### Empirical Scatterplot ###\npar(mar=c(1,4,1,1))\nplot(0,0,type=""n"",ylim=c(-2500,2500),xlim=c(1,4),xlab="" "",ylab="" "",axes=F)\naxis(1,at=1:4,labels=F,cex=0.8, tck=-0.025)\naxis(2,at=seq(-2500,2500,250),cex.axis=0.5,mgp=c(3,0.5,0),tck=-0.025,labels=F)\naxis(2,at=seq(-2500,2500,500),cex.axis=0.6,mgp=c(3,0.5,0),tck=-0.025,labels=seq(-2500,2500,500))\n\nmtext(""Bayes Factor"",2,line=2.5,cex=1)\npar(xpd=NA)\ntext(""One Species"",srt=90,cex=1,x=0.55,y=1250,col=""forest green"")\ntext(""Two Species"",srt=90,cex=1,x=0.55,y=-1250,col=""orange"")\n\nbox()\n\npar(xpd=F)\nabline(h=0,lty=2)\n\npoints(empirical_bf, type=""l"",lwd=0.75)\npoints(empirical_bf, pch=21,lwd=0.75,cex=2)\n\npar(xpd=NA)\ntext(x=par()$usr[1]*0.45,y=par()$usr[4]*1.0,label=""B"",cex=2,font=2)\ntext(x=par()$usr[1],y=par()$usr[4]-(par()$usr[4]-par()$usr[3])*0.025,label="" Empirical"",cex=1,font=1,adj=c(0,0.5))\n\n### Geographic sampling scenario panels ###\nplot(0,0,type=""n"",ylim=c(-10,10),xlim=c(1,4),xlab="" "",ylab="" "",axes=F)\n\ncol1<-colorRampPalette(colors=c(""red"",""blue""))(9)\ncol4<-col3<-col2<-col1\ncol2[5]<-""white""\ncol3[4:6]<-""white""\ncol4[3:7]<-""white""\n\npoints(rep(1,9),seq(10,-5,length.out=9),cex=3,pch=21,bg=col1, col=""black"")\npoints(rep(2,9), seq(10,-5,length.out=9),cex=3,pch=21,bg=col2,col=""black"")\npoints(rep(3,9), seq(10,-5,length.out=9),cex=3,pch=21,bg=col3, col=""black"")\npoints(rep(4,9), seq(10,-5,length.out=9),cex=3,pch=21,bg=col4, col=""black"")\n\n\npar(xpd=NA)\ntext(x=par()$usr[1]*0.45,y=par()$usr[4]*1.0,label=""C"",cex=2,font=2)\n\narrows(x0=0.8,y0=-9,x1=4.2,y1=-9,code=3,angle=15)\ntext(x=2.5,y= -8,label=""Admixed Populations"",font=1,cex=1)\ntext(x=1,y= -11,label=""More"",font=1,cex=0.8)\ntext(x=4,y= -11,label=""Fewer"",font=1,cex=0.8)\n\ndev.off()\n', '#### Create sampling map for ECP species delimitation project #####\n### Libraries ###\nrequire(raster)\nrequire(rgdal)\nrequire(sp)\nrequire(xlsx)\nrequire(maps)\nrequire(rgeos)\nrequire(adegenet)\nrequire(plotrix)\n\n### Read in meta data for sampling sites ###\nmetadata<-read.xlsx(""~/Dropbox/InsectTaxa/metadata/insecttaxa_metadata.xlsx"",sheetName=""Andesiops peruvianus (Ape)"",stringsAsFactors=F)\nhead(metadata)\nECP_data<-metadata[grep(""ECP"",metadata$FULL),]\nECA_data<-metadata[grep(""ECA"",metadata$FULL),]\n\nECP_df<-data.frame(lat=ECP_data$LAT,lon=ECP_data$LON,row.names=ECP_data$FULL,stringsAsFactors=F)\nECP_df<-ECP_df[!duplicated(ECP_df[,1]),]\nrownames(ECP_df)<-sapply(strsplit(rownames(ECP_df),""_""),function(x) x[3])\n\nECA_df<-data.frame(lat= ECA_data $LAT,lon= ECA_data $LON,row.names= ECA_data $FULL,stringsAsFactors=F)\nECA_df<-ECA_df[!duplicated(ECA_df[,1]),]\nrownames(ECA_df)<-sapply(strsplit(rownames(ECA_df),""_""),function(x) x[3])\n\ncoords_df<-rbind(ECP_df,ECA_df[1,])\n#coords_df<-coords_df[-c(1,2,12,13),]\n\n### Read in K2 structure q values ###\necp<-read.genepop(""~/Dropbox/InsectTaxa/genepop/ApeECP.gen"")\necp_k2<-readLines(file(""~/Dropbox/InsectTaxa/dapc_and_structure/Ape_ECP_Struc/StructureHarvesterOutput 2/K2_CLUMPP.OUT""))\n\nq<-sapply(strsplit(ecp_k2,"":""),function(x) x[length(x)])\nq_df<-data.frame(lapply(strsplit(q,"" ""),function(x) x[3:4]),stringsAsFactors=F)\ncolnames(q_df)<-NULL\necp_k2_q_df<-as.matrix(q_df)\nQ_vec<-as.numeric(ecp_k2_q_df[1,])\nnames(Q_vec)<-row.names(ecp@tab)\n\nq_df<-data.frame(q=Q_vec,site=sapply(strsplit(names(Q_vec),""_""),function(x) x[3]),stringsAsFactors=F)\nq_list<-split(q_df,q_df$site)\n\nmean_q<-sapply(q_list,function(x) mean(x$q))\nmean_q2<-data.frame(q=mean_q,q2=1-mean_q,row.names=names(mean_q),stringsAsFactors=F)\nmean_q2<-mean_q2[-c(1,2,12,13),]\n\n### Download elevational data ###\necu_ele1<-raster(""~/Desktop/Manuscripts/SpeciesDelimitationSensitivity/Data/GIS/ASTGTM2_S01W078/ASTGTM2_S01W078_dem.tif"")\necu_ele2<-raster(""~/Desktop/Manuscripts/SpeciesDelimitationSensitivity/Data/GIS/ASTGTM2_S01W079/ASTGTM2_S01W079_dem.tif"")\necu_ele<-merge(ecu_ele1,ecu_ele2)\n\n### Read in water shapefile ### LARGE FILE TAKES A LONG TIME TO READ IN\necu_wat<-shapefile(""~/Desktop/Manuscripts/SpeciesDelimitationSensitivity/Data/GIS/hydrosheds-f2bbc7371b8c08308119/sa_riv_15s/sa_riv_15s.shp"")\n\nsite_bbox<-as(extent(bbox(as.matrix(coords_df[c(2,1)]))),""SpatialPolygons"")\nproj4string(site_bbox)<-proj4string(ecu_wat)\n\necu_wat_crop<-crop(ecu_wat,site_bbox)\n\n### Create map ###\n#quartz(width=6.5,height=6.5*0.7)\npdf(file=""~/Desktop/Manuscripts/SpeciesDelimitationSensitivity/Figures/samplingmap_v1.pdf"")\n#layout(matrix(c(1:2),nrow=1),widths=c(0.7,0.3))\n#par(mar=c(4,4,1,1))\nplot(coords_df[,2],coords_df[,1],xlab=""Latitude"",ylab=""Longitude"")\nplot(ecu_ele,add=T,col=colorRampPalette(c(""black"",""white""))(100))\nplot(ecu_wat_crop,col=""blue"",add=T,lwd=2)\n\nfor(i in 1:nrow(mean_q2)){\n\tfloating.pie(xpos=coords_df[rownames(mean_q2),2][i],ypos= coords_df[rownames(mean_q2),1][i],x=as.numeric(mean_q2[i,]),radius=0.01,col=c(""red"",""blue""))\n}\n\npoints(coords_df[nrow(coords_df),2],coords_df[nrow(coords_df),1],pch=21,bg=""orange"",cex=4)\n\ndev.off()\n\n0.008998329 #degrees to km at this latitude\n\nmap(xlim=c(-84,-35),ylim=c(-60,15))\n\npng(file=""~/Desktop/PeruWithBox.png"",res=500,width=4,height=4,units=""in"")\npar(xpd=NA)\nmap(regions=""Ecuador"",exact=T,mar=c(0,0,0,0))\nrect(bbox(as.matrix(coords_df[c(2,1)]))[1],bbox(as.matrix(coords_df[c(2,1)]))[2],bbox(as.matrix(coords_df[c(2,1)]))[3],bbox(as.matrix(coords_df[c(2,1)]))[4],border=""red"",lwd=1,col=""transparent"")\ndev.off()\n\nsa_bbox<-locator(2)\n\n### Create south america map with lil square for region of interest ###\nmap(xlim=c(-90,-20),ylim=c())']","Coalescent-based species delimitation is sensitive to geographic sampling and isolation by distance Species are a fundamental unit of biodiversity that are delimited via genetic data and coalescent-based methods with increasing frequency. Despite the widespread use of coalescent-based species delimitation, we do not fully understand the sensitivity of these methods to potential sources of bias and violations of their underlying assumptions. One implicit assumption of coalescent-based species delimitation is that geographic sampling is adequate and representative of genetic variation among populations within the lineage of interest. Yet exhaustive geographic sampling is logistically difficult, if not impossible, for many taxa that span large geographic expanses or occupy remote regions. Here, we examine the impact of geographic sampling on the output of Bayes-factor delimitation with SNAPP, a popular coalescent-based species delimitation pipeline. First, we demonstrate the problematic nature of sparse geographic sampling and isolation by distance for species delimitation using simulated data sets of populations connected by different levels of gene flow. We then examine whether similar trends are present in an empirical dataset of Andesiops mayflies (Ephemeroptera: Baetidae) from a high elevation transect in the Ecuadorian Andes. In both the simulated and empirical analyses, we systematically exclude geographically intermediate sites to quantify the impact of geographic sampling and isolation by distance on coalescent-based species delimitation. We find that removing intermediate sites with genetically admixed individuals incorrectly favors multi-species delimitation scenarios. Oversplitting is especially pronounced when isolation by distance is strong, but exists even when gene flow among neighboring populations is relatively high. These findings highlight the importance of adequate geographic sampling in species delimitation and urge caution in interpreting the output of such methods when species' distributions are sparsely sampled and in systems characterized by strong patterns of isolation by distance.",4
Environmentally associated variation in dispersal distance affects inbreeding risk in a stream salamander,"Avoiding inbreeding is considered a key driver of dispersal evolution, and dispersal distances should be especially important in mediating inbreeding risk because the likelihood of mating with relatives decreases with dispersal distance. However, a lack of direct data on dispersal distances has limited empirical tests of this prediction, particularly in the context of the multiple selective forces that can influence dispersal. Using a headwater salamander system, we tested whether spatial variation in environmental conditions leads to differences in dispersal distances, resulting in spatial variation in the effect of dispersal on inbreeding risk. Using capture-recapture and population genomic data from 5 streams, we found that dispersal distances were greater in downstream reaches than upstream reaches. Inbreeding risk was lower for dispersers than non-dispersers in downstream reaches, but not in upstream reaches. Furthermore, stream reaches did not differ in spatial patterns of individual relatedness, indicating that variation in inbreeding risk was in fact due to differences in dispersal distances. These results demonstrate that environmentally associated variation in dispersal distances can cause the inbreeding consequences of dispersal to vary at fine spatial scales. They also show that selective pressures other than inbreeding avoidance maintain phenotypic variation in dispersal, underscoring the importance of addressing alternative hypotheses in dispersal research.","['#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# analyses for Addis & Lowe ""Environmentally associated variation in dispersal distance affects inbreeding risk in a stream salamander."" The American Naturalist.\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nlibrary(lme4)\r\nlibrary(nlme)\r\nlibrary(plotrix)\r\nlibrary(moments)\r\nlibrary(psych)\r\nlibrary(emmeans)\r\nlibrary(plyr)\r\nlibrary(ggplot2)\r\nlibrary(tidyr)\r\nlibrary(effects)\r\nlibrary(adegenet)\r\nlibrary(ecodist)\r\nlibrary(h2o)\r\nlibrary(ggpubr)\r\nlibrary(ggpattern)  ## \r\n\r\n#install \'related\' package (not on CRAN)\r\ninstall.packages(""related"", repos=""http://R-Forge.R-project.org"")\r\nlibrary(related)\r\n\r\n######################################################################\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n#dispersal distance analyses, using all recaptured individuals\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ndata=read.table(""Dispersal.csv"", sep="","", header=T)\r\nnrow(data) #663 individuals\r\n\r\n#subset to movements within reaches (< 500m), then by dispersal status and stream reach\r\ndisp=data[which(data$absolute_distance<500 & data$absolute_distance >=10),]\r\nnodisp=data[which(data$absolute_distance<500 & data$absolute_distance < 10),]\r\nfishdisp=disp[which(disp$reach==""downstream""),] \r\nfishnodisp=nodisp[which(nodisp$reach==""downstream""),]\r\nfishlessdisp=disp[which(disp$reach==""upstream""),]\r\nfishlessnodisp=nodisp[which(nodisp$reach==""upstream""),]\r\nnrow(fishdisp) #43 individuals\r\nnrow(fishlessdisp) #50 individuals\r\n\r\n#test for difference in proportion dispersing between reaches\r\nprop.test(x=c(43,50), n=c(43+184,50+384), conf.level=0.95)\r\n43/184 #raw proportion dispersing downstream = 23.4%\r\n50/384 #raw proportion dispersing upstream= 13%\r\n\r\n# compute mean distances of dispersers in upstream and downstream reaches\r\nmean(fishdisp$absolute_distance)\r\nstd.error(fishdisp$absolute_distance)\r\nmean(fishlessdisp$absolute_distance)\r\nstd.error(fishlessdisp$absolute_distance)\r\n\r\n# test for diffs in dispersal between upstream and downstream reaches\r\nt.test(fishdisp$absolute_distance, fishlessdisp$absolute_distance)\r\nwilcox.test(fishdisp$absolute_distance, fishlessdisp$absolute_distance)\r\n\r\n#use log-transformed distance\r\nlog.fishdisp<-log(fishdisp$absolute_distance)\r\nlog.fishlessdisp<-log(fishlessdisp$absolute_distance)\r\nt.test(log.fishdisp, log.fishlessdisp)\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# test for sex-biased dispersal\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n#subset by sex and dispersal status\r\nmales<-data[which(data$sex==""M"" & data$absolute_distance <=500),]\r\nnrow(males)#45\r\nmale.disp<-males[which(males$absolute_distance>=10 & males$absolute_distance<=500),]\r\nnrow(male.disp)#11\r\nfemales<-data[which(data$sex==""F"" & data$absolute_distance <=500),]\r\nnrow(females)#80\r\nfemale.disp<-females[which(females$absolute_distance>=10 & females$absolute_distance<=500),]\r\nnrow(female.disp)#16\r\n\r\n#subset by sex and reach\r\nmale.up<-data[which(data$sex==""M"" & data$reach==""upstream""),]\r\nnrow(male.up)#33\r\nmale.down<-data[which(data$sex==""M"" & data$reach==""downstream""),]\r\nnrow(male.down)#13\r\nfemale.up<-data[which(data$sex==""F"" & data$reach==""upstream""),]\r\nnrow(female.up)#55\r\nfemale.down<-data[which(data$sex==""F"" & data$reach==""downstream""),]\r\nnrow(female.down)#25\r\n\r\n#test for difference in proportion dispersing >= 10m\r\nprop.test(x=c(11,16), n=c(11+34,16+64), conf.level = 0.95)\r\n\r\n#means\r\nmean(males$absolute_distance)\r\nstd.error(males$absolute_distance)\r\nmean(females$absolute_distance)\r\nstd.error(females$absolute_distance)\r\n\r\n#log-transformed distances\r\nlog.male<-log(males$absolute_distance+1)\r\nlog.female<-log(females$absolute_distance+1)\r\nt.test(log.male, log.female)\r\nwilcox.test(log.male, log.female)\r\n\r\n#test for differences in proportion dispersing between reaches\r\n#subset males by dispersal status and reach\r\nmale.disp.up<-male.up[which(male.up$absolute_distance>9),]\r\nnrow(male.disp.up)#9\r\nmale.nodisp.up<-male.up[which(male.up$absolute_distance<=9),]\r\nnrow(male.nodisp.up)#24\r\nmale.disp.down<-male.down[which(male.down$absolute_distance>9),]\r\nnrow(male.disp.down)#3\r\nmale.nodisp.down<-male.down[which(male.down$absolute_distance<=9),]\r\nnrow(male.nodisp.down)#10\r\nprop.test(x=c(9,3), n =c(9+24, 3+10),conf.level=0.95)\r\n\r\n#subset females by dispersal status and reach\r\nfemale.disp.up<-female.up[which(female.up$absolute_distance>9),]\r\nnrow(female.disp.up)#9\r\nfemale.nodisp.up<-female.up[which(female.up$absolute_distance<=9),]\r\nnrow(female.nodisp.up)#46\r\nfemale.disp.down<-female.down[which(female.down$absolute_distance>9),]\r\nnrow(female.disp.down)#7\r\nfemale.nodisp.down<-female.down[which(female.down$absolute_distance<=9),]\r\nnrow(female.nodisp.down)#18\r\nprop.test(x=c(9,7), n =c(9+46, 7+18),conf.level=0.95)\r\n\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n#plots of dispersal distance\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\n#calculate group means\r\nmu<-ddply(disp, ""reach"", summarise, grp.mean=mean(absolute_distance))\r\n\r\nggplot(disp, aes(x=absolute_distance,color=reach, fill=reach))+\r\n  geom_histogram(alpha=0.7)+\r\n  facet_grid(reach ~ .)+\r\n  geom_vline(data=mu,aes(xintercept=grp.mean, color=reach),linetype=""dashed"")+\r\n  geom_density(alpha=0.6)+\r\n  scale_color_manual(values=c(""#000000"",""#000000""))+\r\n  scale_fill_manual(values=c(""#000000"",""#999999""))+\r\n  labs(x=""Dispersal distance (m)"", y =""Number of individuals"")+\r\n  theme_classic()+\r\n  theme(axis.title=element_text(size=16),axis.text.y=element_text(size=14), \r\n        axis.text.x=element_text(size=14),strip.background = element_blank(),\r\n        strip.text.y = element_blank(),legend.title=element_blank(),\r\n        legend.position=""top"")\r\n\r\n\r\n#####################################################################3\r\n#Mantel test\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# calculate geographic distances\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nxycoords=read.table(""XYcoords.csv"", sep="","", header=T)\r\nstr(xycoords)\r\n\r\n#subset by stream\r\nparxy=xycoords[which(xycoords$stream==""paradise""),]\r\nbearxy=xycoords[which(xycoords$stream==""bear""),]\r\nzigxy=xycoords[which(xycoords$stream==""zigzag""),]\r\ncasxy=xycoords[which(xycoords$stream==""cascade""),]\r\ncanxy=xycoords[which(xycoords$stream==""canyon""),]\r\n\r\n#remove extra columns\r\nparxy$ID<-NULL\r\nparxy$stream<-NULL\r\nbearxy$ID<-NULL\r\nbearxy$stream<-NULL\r\nzigxy$ID<-NULL\r\nzigxy$stream<-NULL\r\ncasxy$ID<-NULL\r\ncasxy$stream<-NULL\r\ncanxy$ID<-NULL\r\ncanxy$stream<-NULL\r\n\r\n\r\nas.matrix(parxy)\r\nas.matrix(bearxy)\r\nas.matrix(zigxy)\r\nas.matrix(casxy)\r\nas.matrix(canxy)\r\n\r\npar.geo<-dist(parxy) \r\nbear.geo<-dist(bearxy)\r\nzig.geo<-dist(zigxy)\r\ncas.geo<-dist(casxy)\r\ncan.geo<-dist(canxy)\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# calculate genetic distances\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# read in structure files for each stream\r\nbear.str<-read.structure(""bear_297loci_structure.str"") #150 genotypes, 297 loci, 1,2,none,1,n\r\ncascade.str<-read.structure(""cascade_297loci_structure.str"")#22 genotypes, 297 loci, 1,2,none,1,n\r\ncanyon.str<-read.structure(""canyon_297loci_structure.str"")#36 genotypes, 297 loci, 1,2,none,1,n\r\nparadise.str<-read.structure(""paradise_297loci_structure.str"")#112 genotypes, 297 loci, 1,2,none,1,n\r\nzigzag.str<-read.structure(""zigzag_297loci_structure.str"")#62 genotypes, 297 loci, 1,2,none,1,n\r\n\r\n\r\n#calculate genetic distances for each stream\r\nbear.distgenEUCL <- dist(bear.str, method = ""euclidean"", diag = FALSE, upper = FALSE, p = 2)\r\ncascade.distgenEUCL <- dist(cascade.str, method = ""euclidean"", diag = FALSE, upper = FALSE, p = 2)\r\ncanyon.distgenEUCL <- dist(canyon.str, method = ""euclidean"", diag = FALSE, upper = FALSE, p = 2)\r\nparadise.distgenEUCL <- dist(paradise.str, method = ""euclidean"", diag = FALSE, upper = FALSE, p = 2)\r\nzigzag.distgenEUCL <- dist(zigzag.str, method = ""euclidean"", diag = FALSE, upper = FALSE, p = 2)\r\n\r\n#mantel Rs\r\nmantel.bear<-mantel(bear.distgenEUCL~bear.geo,  nperm = 999,nboot = 500, pboot = 0.9, cboot = 0.95)\r\nmantel.cascade<-mantel(cascade.distgenEUCL~cas.geo,  nperm = 999,nboot = 500, pboot = 0.9, cboot = 0.95)\r\nmantel.canyon<-mantel(canyon.distgenEUCL~can.geo,  nperm = 999,nboot = 500, pboot = 0.9, cboot = 0.95)\r\nmantel.paradise<-mantel(paradise.distgenEUCL~par.geo,  nperm = 999,nboot = 500, pboot = 0.9, cboot = 0.95)\r\nmantel.zigzag<-mantel(zigzag.distgenEUCL~zig.geo,  nperm = 999,nboot = 500, pboot = 0.9, cboot = 0.95)\r\n\r\n#mantel plots for each stream\r\nbreaks<-c(-50,50,150,250,350,450,550,650,750,850,950,1050,1150,1250,1350,1450)\r\npar.gram <- mgram(paradise.distgenEUCL, par.geo,breaks=breaks,nperm=999,nboot = 500, pboot = 0.9, cboot = 0.95)\r\nplot(par.gram)\r\nbear.gram<-mgram(bear.distgenEUCL,bear.geo,breaks=breaks,nperm=999,nboot = 500, pboot = 0.9, cboot = 0.95)\r\nplot(bear.gram)\r\nzig.gram<-mgram(zigzag.distgenEUCL,zig.geo,breaks=breaks,nperm=999,nboot = 500, pboot = 0.9, cboot = 0.95)\r\nplot(zig.gram)\r\ncan.gram<-mgram(canyon.distgenEUCL,can.geo, breaks=breaks,nperm=999,nboot = 500, pboot = 0.9, cboot = 0.95)\r\nplot(can.gram)\r\ncas.gram<-mgram(cascade.distgenEUCL,cas.geo, breaks=breaks,nperm=999,nboot = 500, pboot = 0.9, cboot = 0.95)\r\nplot(cas.gram)\r\n\r\n#save data for paradise and bear for plotting (because these streams have significant IBD)\r\npar.gram<-as.data.frame(par.gram$mgram)\r\npar.gram<-par.gram[par.gram$ngroup != 0,] # remove rows at distances with no data\r\nbear.gram<-as.data.frame(bear.gram$mgram)\r\n\r\n# combine data frames and add identifiers for stream and significance level\r\nbear.par<-rbind(par.gram, bear.gram)\r\nbear.par$stream<-c(rep(""paradise"", 13), rep(""bear"", 15))\r\nbear.par$sig<-c(rep(""y"",1), rep(""n"",1), rep(""y"", 1), rep(""n"",10), \r\n                rep(""y"",1), rep(""n"",1), rep(""y"", 1), rep(""n"", 6), rep(""y"", 2), rep(""n"",4 ))\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# plot mantel correlograms for bear and paradise\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nggplot(bear.par, aes(x=lag, y=mantelr, group=stream)) +\r\n  geom_hline(aes(yintercept=0), colour=""grey"")+\r\n  geom_line(aes(linetype=stream), size=1,show.legend = FALSE)+\r\n  geom_point(aes(shape=sig),size=3,show.legend = FALSE)+\r\n  scale_shape_manual(values=c(1,16))+\r\n  ylim(-.15,.15)+\r\n  xlab(""Distance (m)"")+\r\n  ylab(""Mantel r correlation"")+\r\n  scale_x_continuous(breaks=seq(0,1400,200))+\r\n  theme_classic()+\r\n  theme(axis.title=element_text(size=16),axis.text.y=element_text(size=14), axis.text.x=element_text(size=14))\r\n\r\n\r\n#############################################################################\r\n### get estimates of relatedness using coancestry############\r\ninput <- readgenotypedata(""297loci_genotypes_related.txt"")\r\n\r\n#look at data frames\r\ninput$gdata\r\ninput$nloci\r\ninput$nalleles\r\ninput$ninds\r\ninput$freqs\r\n\r\n#compare relatedness estimators (only moments based estimators (but not ritland), not likelihood)\r\nout<-compareestimators(input, 100)\r\n\r\n#Correlation Coefficients Between Observed & Expected Values:\r\n#wang\t\t0.955558\r\n#lynchli\t\t0.955932\r\n#lynchrd\t\t0.962566  # so this is best\r\n#quellergt\t0.961201\r\n\r\n# now do custom comparisons including two likelihood methods and lynchrd estimator\r\n\r\n##simulations to determine which simulator is best\r\nsim<-familysim(input$freqs, 100)\r\noutputsim<-coancestry(sim, dyadml=1, trioml=1,lynchrd=1) ## this takes a long time\r\nsimrel<-cleanuprvals(outputsim$relatedness, 100)\r\nwrite.table(as.data.frame(simrel),file=""simrel.csv"",quote=F,sep="","")\r\nsimrel=read.table(""simrel297.csv"", sep="","",header=T)\r\n\r\n#parse data based on relatedness type and estimator\r\ntriomlpo<-simrel[1:100, 5]\r\ntriomlfs<-simrel[(100+1):(2*100), 5]\r\ntriomlhs<-simrel[((2*100)+1):(3*100),5]\r\ntriomlur<-simrel[((3*100)+1):(4*100),5]\r\n\r\ndyadmlpo<-simrel[1:100, 11]\r\ndyadmlfs<-simrel[(100+1):(2*100), 11]\r\ndyadmlhs<-simrel[((2*100)+1):(3*100),11]\r\ndyadmlur<-simrel[((3*100)+1):(4*100),11]\r\n\r\nlynchrdpo<-simrel[1:100, 8]\r\nlynchrdfs<-simrel[(100+1):(2*100), 8]\r\nlynchrdhs<-simrel[((2*100)+1):(3*100),8]\r\nlynchrdur<-simrel[((3*100)+1):(4*100),8]\r\n\r\n#create labels for the different estimators\r\ntrioml<- rep(\'tri\', 100)\r\ndyadml<- rep(\'di\', 100)\r\nlynchrd<- rep(\'LR\', 100)\r\nestimator2<-c(trioml,dyadml,lynchrd)\r\nEstimator<- rep(estimator2, 4)\r\n\r\n#labels for relatedness types\r\npo<-rep(""Parent-Offspring"", (3*100))\r\nfs<-rep(""Full-Sibs"", (3*100))\r\nhs<-rep(""Half-Sibs"", (3*100))\r\nur<-rep(""Unrelated"", (3*100))\r\nrelationship<-c(po,fs,hs,ur)\r\n\r\n#combine different values for each estimator based on relatedness type, as lists\r\nrelatednesspo<-c(triomlpo, dyadmlpo, lynchrdpo)\r\nrelatednessfs<-c(triomlfs, dyadmlfs, lynchrdfs)\r\nrelatednesshs<-c(triomlhs, dyadmlhs, lynchrdhs)\r\nrelatednessur<-c(triomlur, dyadmlur, lynchrdur)\r\nRelatedness_Value<-c(relatednesspo, relatednessfs, relatednesshs, relatednessur)\r\n\r\n#combine the data\r\ncombineddata<-as.data.frame(cbind(Estimator, relationship, Relatedness_Value))\r\ncombineddata$Relatedness_Value<-as.numeric(as.character(combineddata$Relatedness_Value))\r\n\r\n#plot the data\r\nggplot(combineddata, aes(x=Estimator, y=Relatedness_Value), ylim=c(-0.5,1.0))+\r\n  geom_boxplot() +\r\n  facet_wrap (~ relationship)\r\n\r\n#calculate correlation coefficient between the observed values and expected values\r\nurval<-rep(0, 100)\r\nhsval<-rep(0.25, 100)\r\nfsval<-rep(0.5, 100)\r\npoval<-rep(0.5, 100)\r\nrelvals<-c(poval,fsval,hsval,urval)\r\n\r\ncor(relvals, simrel[,5])\r\n# 0.9760151, highest is trioml\r\ncor(relvals, simrel[,8])\r\n# 0.9685344\r\ncor(relvals, simrel[,11])\r\n# 0.9754507\r\n\r\n# determine cutoffs for relatedness categories for trioml estimator\r\nrelvalues<-simrel[, 5]\r\nlabel1<-rep(""PO"", 100)\r\nlabel2<-rep(""Full"",100)\r\nlabel3<-rep(""Half"",100)\r\nlabel4<-rep(""Unrelated"",100)\r\nlabels<-c(label1,label2,label3,label4)\r\nplot(as.factor(labels), relvalues, ylab=""Relatedness Value"", xlab=""Relatedness"")\r\n\r\npdf(""FigS1.pdf"", useDingbats = FALSE)\r\nqplot(as.factor(labels), relvalues, geom =""boxplot"", ylab =""Relatedness Values"", xlab =""Relatedness"")\r\ndev.off()\r\n\r\nPO=simrel[which(simrel$group==""POPO""),]\r\nSB=simrel[which(simrel$group==""SBSB""),]\r\nHS=simrel[which(simrel$group==""HSHS""),]\r\nUR=simrel[which(simrel$group==""URUR""),]\r\n\r\n#these are the cutoffs for relationship categories\r\nquantile(PO$trioml,probs=c(.025,.975)) #0.500-0.609\r\nquantile(SB$trioml,probs=c(.025,.975)) #0.422-0.613\r\nquantile(HS$trioml,probs=c(.025,.975))#0.131-0.384\r\nquantile(UR$trioml,probs=c(.025,.975))#0-0.123\r\n\r\nhist(PO$trioml, breaks=seq(0,1,.01), freq=T, xlim=c(0,1),ylim=c(0,20),col=""grey"", border = ""black"", xlab=""Relatedness"", ylab=""Frequency"", cex.axis=1.8, cex.lab=1.8,main = ""Parent-offspring"")\r\nqts<-quantile(PO$trioml,probs=c(.025,.975))\r\nabline(v=qts[1],col=""red"")\r\nabline(v=qts[2],col=""red"")\r\nhist(SB$trioml, breaks=seq(0,1,.01), freq=T, xlim=c(0,1),ylim=c(0,20),col=""grey"", border = ""black"", xlab=""Relatedness"", ylab=""Frequency"", cex.axis=1.8, cex.lab=1.8,main = ""Full-siblings"")\r\nqts<-quantile(SB$trioml,probs=c(.025,.975))\r\nabline(v=qts[1],col=""red"")\r\nabline(v=qts[2],col=""red"")\r\nhist(HS$trioml, breaks=seq(0,1,.01), freq=T, xlim=c(0,1),ylim=c(0,20),col=""grey"", border = ""black"", xlab=""Relatedness"", ylab=""Frequency"", cex.axis=1.8, cex.lab=1.8,main = ""Half-siblings"")\r\nqts<-quantile(HS$trioml,probs=c(.025,.975))\r\nabline(v=qts[1],col=""red"")\r\nabline(v=qts[2],col=""red"")\r\nhist(UR$trioml, breaks=seq(-.2,1,.01), freq=T, xlim=c(-.2,1),ylim=c(0,40),col=""grey"", border = ""black"", xlab=""Relatedness"", ylab=""Frequency"", cex.axis=1.8, cex.lab=1.8,main = ""Unrelated"")\r\nqts<-quantile(UR$trioml,probs=c(.025,.975))\r\nabline(v=qts[1],col=""red"")\r\nabline(v=qts[2],col=""red"")\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n## now calculate relatedness using 297 snps and trioml estimator\r\n# multithread the process to take less time\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nh2o.init(nthreads=8)\r\noutput<-coancestry(""297loci_genotypes_related.txt"", trioml=2)\r\nwrite.table(as.data.frame(output$relatedness),file=""5pops_relatedness_trioml.csv"",quote=F,sep="","")\r\nwrite.table(as.data.frame(output$relatedness.ci95),file=""5pops_relatedness_ci95_trioml.csv"",quote=F,sep="","")\r\n# this yields 72,771 estimates of pairwise relatedness (pairwise for 382 individuals)\r\n\r\n\r\n###################################################################################\r\n# calculate proportion of individuals within 50m that are relatives\r\nrelatedness<-read.csv(""5pops_relatedness_trioml.csv"", stringsAsFactors=FALSE, strip.white=TRUE)\r\nstr(relatedness)\r\nnrow(relatedness)\r\n\r\n#delete extra columns\r\nrelatedness$lynchli<-NULL\r\nrelatedness$lynchrd<-NULL\r\nrelatedness$ritland<-NULL\r\nrelatedness$quellergt<-NULL\r\nrelatedness$dyadml<-NULL\r\nrelatedness$wang<-NULL\r\n\r\n# only keep relatedness estimates among individuals within same stream\r\n#add column designating within-stream comparisons\r\nrelatedness$stream<-relatedness$group\r\n\r\nrelatedness$stream[relatedness$stream==1111]=""bear""\r\nrelatedness$stream[relatedness$stream==2222]=""cascade""\r\nrelatedness$stream[relatedness$stream==3333]=""canyon""\r\nrelatedness$stream[relatedness$stream==4444]=""paradise""\r\nrelatedness$stream[relatedness$stream==5555]=""zigzag""\r\n\r\n#new dataframe subset to only include within stream relatedness estimates\r\ninstream<-relatedness[relatedness$stream==""bear"" | relatedness$stream==""cascade"" | relatedness$stream==""canyon""\r\n                      | relatedness$stream==""paradise"" | relatedness$stream==""zigzag"",]\r\n\r\nstr(instream)\r\nnrow(instream) #20143 pairwise comparisons\r\nunique(instream$stream) #check to make sure only within-stream comparisons included\r\n\r\n# need to make IDs match dispersal dataframe\r\ninstream<-separate(instream, ind1.id, c(""group2"", ""ind1""), sep=""_"", remove=FALSE)\r\ninstream<-separate(instream, ind2.id, c(""group3"", ""ind2""), sep=""_"", remove=FALSE)\r\n\r\n#get rid of extra columns\r\ninstream$group2<-NULL\r\ninstream$group3<-NULL\r\ninstream$ind1.id<-NULL\r\ninstream$ind2.id<-NULL\r\n\r\n# add location data for each individual from dispersal dataframe to figure out how far apart individuals are\r\n# bring in dispersal data\r\ndispdata=read.table(""Dispersal.csv"", sep="","", header=T)\r\nstr(dispdata)\r\n\r\n#need to make ids character to match with instream df\r\ndispdata$id<-as.character(dispdata$id)\r\nstr(dispdata)\r\nstr(instream)\r\n\r\n#add final location data for each individual\r\ninstream$ind1.loc <- dispdata$final_location[match(instream$ind1, dispdata$id)]\r\ninstream$ind2.loc <- dispdata$final_location[match(instream$ind2, dispdata$id)]\r\nstr(instream)\r\ntail(instream)\r\n\r\n# calculate distance apart between individuals\r\ninstream$dist.apart<-abs(instream$ind1.loc-instream$ind2.loc)\r\n\r\n#new dataframe with individuals 50m apart or less\r\ninstream.50<-instream[instream$dist.apart < 51,]\r\ninstream.50\r\ntail(instream.50)\r\n\r\n# get rid of rows with NAs\r\ninstream.50<-na.omit(instream.50)\r\nnrow(instream.50)\r\ntable(instream.50$dist.apart)\r\n\r\n# count number of individuals within 50m of focal individual\r\nind1.count<-as.data.frame(table(instream.50$ind1))\r\nind1.count\r\nstr(ind1.count)\r\nnames(ind1.count)\r\n\r\n#rename columns\r\nnames(ind1.count)[names(ind1.count)==""Var1""] <-""Id""\r\nnames(ind1.count)[names(ind1.count)==""Freq""] <-""Ind1.Count""\r\nind2.count<-as.data.frame(table(instream.50$ind2))\r\nind2.count\r\n\r\n#rename columns\r\nnames(ind2.count)[names(ind2.count)==""Var1""] <-""Id""\r\nnames(ind2.count)[names(ind2.count)==""Freq""] <-""Ind2.Count""\r\n\r\nsetdiff(ind1.count$Id, ind2.count$Id) # 40 individuals in ind1 but not in ind2\r\nsetdiff(ind2.count$Id, ind1.count$Id) # 38 individuals in ind2 but not in ind1\r\n\r\n\r\n#merge count dataframes to make mastercount dataframe\r\nmastercount<-merge(ind1.count, ind2.count, \r\n                   by = ""Id"",\r\n                   all = TRUE)\r\nnrow(mastercount)\r\n\r\n#replace NAs with zero\r\nmastercount$Ind1.Count[is.na(mastercount$Ind1.Count)] = 0\r\nmastercount$Ind2.Count[is.na(mastercount$Ind2.Count)] = 0\r\n\r\n# sum counts to get total number of individuals within 50m for each focal individual\r\nmastercount$total.count<-mastercount$Ind1.Count+mastercount$Ind2.Count\r\nmax(mastercount$total.count) #30\r\nmin(mastercount$total.count) #1\r\n\r\n##new dataframe with individuals 50m apartt or less AND relatedness higher than .132\r\ninstream.related<-instream.50[instream.50$trioml > 0.132,]\r\ninstream.related\r\ntail(instream.related)\r\nnrow(instream.related)\r\nstr(instream.related)\r\n\r\n# count number of individuals that are related\r\nind1.count<-as.data.frame(table(instream.related$ind1))\r\nind1.count\r\nstr(ind1.count)\r\nnames(ind1.count)\r\n\r\n#rename columns\r\nnames(ind1.count)[names(ind1.count)==""Var1""] <-""Id""\r\nnames(ind1.count)[names(ind1.count)==""Freq""] <-""Ind1.Count""\r\nind2.count<-as.data.frame(table(instream.related$ind2))\r\nind2.count\r\n\r\n#rename columns\r\nnames(ind2.count)[names(ind2.count)==""Var1""] <-""Id""\r\nnames(ind2.count)[names(ind2.count)==""Freq""] <-""Ind2.Count""\r\n\r\n#merge count dataframes to make mastercount dataframe\r\nrelatedcount<-merge(ind1.count, ind2.count, \r\n                    by = ""Id"",\r\n                    all = TRUE)\r\nnrow(relatedcount)\r\n\r\n#replace NAs with zero\r\nrelatedcount$Ind1.Count[is.na(relatedcount$Ind1.Count)] = 0\r\nrelatedcount$Ind2.Count[is.na(relatedcount$Ind2.Count)] = 0\r\n\r\n# sum counts to get total number of individuals within 50m for each focal individual\r\nrelatedcount$related<-relatedcount$Ind1.Count+relatedcount$Ind2.Count\r\nmax(relatedcount$related) #10\r\nmin(relatedcount$related) #1\r\n\r\n#now merged related counts with mastercounts\r\nall.counts<-merge(mastercount, relatedcount, \r\n                  by = ""Id"",\r\n                  all = TRUE)\r\n\r\n#replace NAs with zero\r\nall.counts$related[is.na(all.counts$related)] = 0\r\nall.counts\r\n\r\n#calculate proportion relatives\r\nall.counts$prop.related<-all.counts$related/all.counts$total.count\r\nall.counts\r\nmax(all.counts$prop.related) #1\r\nmin(all.counts$prop.related) #0\r\nmedian(all.counts$prop.related)# 0.111\r\n\r\n#get rid of unnecessary columns\r\nall.counts$Ind1.Count.x<-NULL\r\nall.counts$Ind2.Count.x<-NULL\r\nall.counts$Ind1.Count.y<-NULL\r\nall.counts$Ind2.Count.y<-NULL\r\n\r\n# add dispersal data to all.counts df\r\nstr(dispdata) # add final_location,absolute_distance, stream, reach\r\nall.counts$stream <- dispdata$stream[match(all.counts$Id, dispdata$id)]\r\nall.counts$final_location <- dispdata$final_location[match(all.counts$Id, dispdata$id)]\r\nall.counts$absolute_distance <- dispdata$absolute_distance[match(all.counts$Id, dispdata$id)]\r\nall.counts$reach <- dispdata$reach[match(all.counts$Id, dispdata$id)]\r\n\r\n# add column designating disperser status\r\nall.counts$disperser10<- ""no""\r\nall.counts$disperser10[all.counts$absolute_distance >=10]<- ""yes""\r\nstr(all.counts)\r\nunique(all.counts$stream)\r\nunique(all.counts$reach)\r\nunique(all.counts$disperser10)\r\ntable(all.counts$disperser10)\r\n\r\n# need to exclude GBO4 and GZY1-7-8 because they dispersed between stream reaches\r\nall.counts<-all.counts[all.counts$Id!=""GBO4"",]\r\nall.counts<-all.counts[all.counts$Id!=""GZY1-7-8"",]\r\nnrow(all.counts)\r\n\r\n# test for correlation between salamander density and proportion related\r\nranges<-cbind(all.counts$total.count, all.counts$prop.related)\r\ncor(ranges, use=""pairwise.complete.obs"", method=""pearson"")\r\ncorr.test(ranges)\r\n\r\n\r\n#################################################################################\r\n#### glmm testing for effects of dispersal on inbreeding risk\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# logistic regression with binomial distribution, dispersal treated categorically\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nmod<-glmer(cbind(related, total.count)~reach+disperser10+reach*disperser10 +(1|stream),na.action = na.exclude,data=all.counts, family=""binomial"")\r\n\r\nsummary(mod)\r\nprint(mod)\r\nfixef(mod) #get fixed effects estimates\r\nranef(mod) #get random effects estimates\r\n\r\n#function from Ben Bolker to test for overdispersion\r\noverdisp_fun <- function(model) {\r\n  rdf <- df.residual(model)\r\n  rp <- residuals(model,type=""pearson"")\r\n  Pearson.chisq <- sum(rp^2)\r\n  prat <- Pearson.chisq/rdf\r\n  pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)\r\n  c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)\r\n}\r\noverdisp_fun(mod)\r\n\r\n\r\n# post-hoc tukey tests for dispersal x reach groups\r\nem<-emmeans(mod, specs=pairwise~reach:disperser10, adjust=""tukey"")\r\nem.response<-summary(em, type=""response"")\r\n\r\n\r\n#save estimates for plotting\r\nem.estimates<-as.data.frame(em.response$emmeans)\r\nem.estimates$upper.se<-em.estimates$prob + em.estimates$SE\r\nem.estimates$lower.se<-em.estimates$prob - em.estimates$SE\r\n\r\n#rename column \r\nnames(em.estimates)[names(em.estimates)==""disperser10""] <-""Dispersal""\r\nem.estimates\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n### plot estimated marginal means\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\n#greyscale plot\r\na<-ggplot(em.estimates, aes(x=reach, y=prob))+\r\n  geom_point(size=4, aes(color=reach, shape=Dispersal),position=position_dodge(width=0.4))+\r\n  geom_errorbar(aes(ymin=lower.se, ymax=upper.se, color=reach, group=Dispersal),width = .1,position=position_dodge(width=0.4))+\r\n  scale_shape_manual(values=c(17,16), guide=guide_legend(override.aes=list(shape=c(2,1), size=4)))+\r\n  scale_color_manual(values=c(\'#000000\', \'#999999\'), guide=""none"")+\r\n  scale_fill_discrete()+\r\n  labs(x=""Reach"", y= ""Proportion relatives within 50m (EMM \\u00B1 SE)"")+\r\n  theme_classic()+\r\n  theme(axis.title=element_text(size=9),axis.text.y=element_text(size=8), axis.text.x=element_text(size=8),\r\n        legend.position=""top"", legend.text=element_text(size=8),legend.title=element_text(size=8))\r\n \r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# logistic regression with binomial distribution, dispersal treated continuously\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nmod2<-glmer(cbind(related, total.count)~reach*absolute_distance +(1|stream),na.action = na.exclude,data=all.counts, family=""binomial"")\r\nsummary(mod2)\r\n\r\nem<-as.data.frame(effect(""reach*absolute_distance"", mod2, xlevels=list(absolute_distance=seq(0,351,1))))\r\nstr(em)\r\n\r\n#greyscale\r\nb<-ggplot(em, aes(x=absolute_distance, y=fit, fill=reach, color=reach))+\r\n  geom_line(size=1)+\r\n  geom_ribbon_pattern(aes(ymin=lower, ymax=upper, pattern=reach), \r\n                      alpha=0.3, size=0.1, \r\n                      pattern_fill=""black"",\r\n                      pattern_alpha=0.4)+\r\n  scale_x_continuous(name=""Dispersal distance (m)"")+\r\n  scale_y_continuous(name=""Proportion relatives within 50m"")+\r\n  scale_color_manual(values=c(""#000000"",""#999999""))+\r\n  scale_fill_manual(values=c(""#000000"",""#999999""))+\r\n  theme_bw()+\r\n  theme(legend.position=""top"",legend.text=element_text(size=8),\r\n        legend.title=element_blank(), panel.border=element_blank(), panel.grid.major=element_blank(),\r\n        panel.grid.minor=element_blank(), axis.line=element_line(colour=""black""),\r\n        axis.title=element_text(size=9),axis.text.y=element_text(size=8), axis.text.x=element_text(size=8))\r\n\r\n\r\n##################################################################################################\r\n#calculate number of conspecifics within 50m each individual -- test of intraspecific competition\r\n\r\nalldata<-read.table(""CaptureData.csv"", sep="","", header=T)\r\n\r\n#subset data by stream\r\nbear<-alldata[which(alldata$Stream==""Bear""),]\r\ncascade<-alldata[which(alldata$Stream==""Cascade""),]\r\ncanyon<-alldata[which(alldata$Stream==""Canyon""),]\r\nparadise<-alldata[which(alldata$Stream==""Paradise""),]\r\nzigzag<-alldata[which(alldata$Stream==""Zig Zag""),]\r\n\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n#calculate pairwise distances apart for zigzag\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nzigzag.pair<-data.frame()\r\nfor (i in 1:nrow(zigzag)){\r\n  for (j in 1:nrow(zigzag)){\r\n    output=c(zigzag$ID[i], zigzag$ID[j], zigzag$EndLoc[i], zigzag$EndLoc[j])\r\n    zigzag.pair=rbind(zigzag.pair,output)\r\n  }\r\n}\r\nzigzag.pair\r\ncolnames(zigzag.pair)<-c(""Ind1"", ""Ind2"", ""EndLoc1"", ""EndLoc2"")\r\nnrow(zigzag.pair)#306916, =554*554. so need to get rid of duplicate comparisons\r\n\r\n#get rid of rows that compare individuals to themselves\r\nzigzag.pair<-zigzag.pair[which(zigzag.pair$Ind1 != zigzag.pair$Ind2),]\r\nnrow(zigzag.pair) #306362, = 306916-554, good\r\n\r\n#find distance between individuals\r\nzigzag.pair$EndLoc1<-as.numeric(zigzag.pair$EndLoc1)\r\nzigzag.pair$EndLoc2<-as.numeric(zigzag.pair$EndLoc2)\r\nzigzag.pair$diff<-abs(zigzag.pair$EndLoc1-zigzag.pair$EndLoc2)\r\n\r\n#delete end locs\r\nzigzag.pair$EndLoc1<-NULL\r\nzigzag.pair$EndLoc2<-NULL\r\nzigzag.pair\r\n\r\n#sort to identify duplicate comparisons\r\nzigzag.pair<-data.frame(t(apply(zigzag.pair,1,sort)))\r\nzigzag.pair\r\n\r\nzigzag.pair<-unique(zigzag.pair)\r\nnrow(zigzag.pair)# 153181, yay!\r\n\r\n#rename columns\r\ncolnames(zigzag.pair)<-c(""DistApart"", ""Ind1"", ""Ind2"")\r\n\r\n#slim down to only individuals that are 50m apart or less\r\nzigzag.pair$DistApart<-as.numeric(zigzag.pair$DistApart)\r\nzigzag.50<-zigzag.pair[which(zigzag.pair$DistApart < 51),]\r\nmax(zigzag.50$DistApart)# good\r\ntable(zigzag.50$DistApart)\r\n\r\n# count number of individuals within 50m of focal individual\r\nind1.count<-as.data.frame(table(zigzag.50$Ind1))\r\nind1.count\r\nstr(ind1.count)\r\nnames(ind1.count)\r\n\r\n#rename columns\r\nnames(ind1.count)[names(ind1.count)==""Var1""] <-""Id""\r\nnames(ind1.count)[names(ind1.count)==""Freq""] <-""Ind1.Count""\r\n\r\nind2.count<-as.data.frame(table(zigzag.50$Ind2))\r\nind2.count\r\n\r\n#rename columns\r\nnames(ind2.count)[names(ind2.count)==""Var1""] <-""Id""\r\nnames(ind2.count)[names(ind2.count)==""Freq""] <-""Ind2.Count""\r\n\r\n#merge count dataframes to make mastercount dataframe\r\nmastercount<-merge(ind1.count, ind2.count, \r\n                   by = ""Id"",\r\n                   all = TRUE)\r\nnrow(mastercount)\r\n\r\n#replace NAs with zero\r\nmastercount$Ind1.Count[is.na(mastercount$Ind1.Count)] = 0\r\nmastercount$Ind2.Count[is.na(mastercount$Ind2.Count)] = 0\r\n\r\n#sum counts to get total number of individuals within 50m for each focal individual\r\nmastercount$total.count<-mastercount$Ind1.Count+mastercount$Ind2.Count\r\nmax(mastercount$total.count) #117\r\nmin(mastercount$total.count) #1\r\n\r\n#add # of inds within 50m  to Dispersal dataframe\r\ndispersal=read.table(""Dispersal.csv"", sep="","", header=T)\r\ndispersal\r\nnames(dispersal)\r\nnames(mastercount)\r\n\r\ndispersal$NumIndsWithin50m.zigzag<- mastercount$total.count[match(dispersal$id, mastercount$Id)]\r\ndispersal\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n#calculate pairwise distances apart for canyon\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncanyon.pair<-data.frame()\r\nfor (i in 1:nrow(canyon)){\r\n  for (j in 1:nrow(canyon)){\r\n    output=c(canyon$ID[i], canyon$ID[j], canyon$EndLoc[i], canyon$EndLoc[j])\r\n    canyon.pair=rbind(canyon.pair,output)\r\n  }\r\n}\r\ncanyon.pair\r\ncolnames(canyon.pair)<-c(""Ind1"", ""Ind2"", ""EndLoc1"", ""EndLoc2"")\r\nnrow(canyon.pair)#152100, =390*390. so need to get rid of duplicate comparisons\r\n\r\n#get rid of rows that compare individuals to themselves\r\ncanyon.pair<-canyon.pair[which(canyon.pair$Ind1 != canyon.pair$Ind2),]\r\nnrow(canyon.pair) #151710, = 152100-390, good\r\n\r\n#find distance between individuals\r\ncanyon.pair$EndLoc1<-as.numeric(canyon.pair$EndLoc1)\r\ncanyon.pair$EndLoc2<-as.numeric(canyon.pair$EndLoc2)\r\ncanyon.pair$diff<-abs(canyon.pair$EndLoc1-canyon.pair$EndLoc2)\r\n\r\n#delete end locs\r\ncanyon.pair$EndLoc1<-NULL\r\ncanyon.pair$EndLoc2<-NULL\r\ncanyon.pair\r\n\r\n#sort to identify duplicate comparisons\r\ncanyon.pair<-data.frame(t(apply(canyon.pair,1,sort)))\r\ncanyon.pair\r\n\r\ncanyon.pair<-unique(canyon.pair)\r\nnrow(canyon.pair)# 75855, yay!\r\n\r\n#rename columns \r\ncolnames(canyon.pair)<-c(""DistApart"", ""Ind1"", ""Ind2"")\r\n\r\n#slim down to only inds that are 50m apart or less\r\ncanyon.pair$DistApart<-as.numeric(canyon.pair$DistApart)\r\ncanyon.50<-canyon.pair[which(canyon.pair$DistApart < 51),]\r\nmax(canyon.50$DistApart)# good\r\ntable(canyon.50$DistApart)\r\n\r\n# count number of individuals within 50m of focal individual\r\nind1.count<-as.data.frame(table(canyon.50$Ind1))\r\nind1.count\r\nstr(ind1.count)\r\nnames(ind1.count)\r\n\r\n#rename columns\r\nnames(ind1.count)[names(ind1.count)==""Var1""] <-""Id""\r\nnames(ind1.count)[names(ind1.count)==""Freq""] <-""Ind1.Count""\r\n\r\nind2.count<-as.data.frame(table(canyon.50$Ind2))\r\nind2.count\r\n#rename columns\r\nnames(ind2.count)[names(ind2.count)==""Var1""] <-""Id""\r\nnames(ind2.count)[names(ind2.count)==""Freq""] <-""Ind2.Count""\r\n\r\n#merge count dataframes to make mastercount dataframe\r\nmastercount<-merge(ind1.count, ind2.count, \r\n                   by = ""Id"",\r\n                   all = TRUE)\r\nnrow(mastercount)\r\n\r\n#replace NAs with zero\r\nmastercount$Ind1.Count[is.na(mastercount$Ind1.Count)] = 0\r\nmastercount$Ind2.Count[is.na(mastercount$Ind2.Count)] = 0\r\n\r\n# sum counts to get total number of individuals within 50m for each focal individual\r\nmastercount$total.count<-mastercount$Ind1.Count+mastercount$Ind2.Count\r\nmax(mastercount$total.count) #100\r\nmin(mastercount$total.count) #21\r\n\r\n#add # of inds within 50m  to Dispersal dataframe\r\nsetwd(""C:/Users/addis/OneDrive/manuscripts/genomics/to submit"")\r\ndispersal=read.table(""Dispersal.csv"", sep="","", header=T)\r\ndispersal\r\nnames(dispersal)\r\nnames(mastercount)\r\n\r\ndispersal$NumIndsWithin50m.canyon<- mastercount$total.count[match(dispersal$id, mastercount$Id)]\r\ndispersal\r\n\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n#calculate pairwise distances apart for paradise\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nparadise.pair<-data.frame()\r\nfor (i in 1:nrow(paradise)){\r\n  for (j in 1:nrow(paradise)){\r\n    output=c(paradise$ID[i], paradise$ID[j], paradise$EndLoc[i], paradise$EndLoc[j])\r\n    paradise.pair=rbind(paradise.pair,output)\r\n  }\r\n}\r\nparadise.pair\r\ncolnames(paradise.pair)<-c(""Ind1"", ""Ind2"", ""EndLoc1"", ""EndLoc2"")\r\nnrow(paradise.pair)#753424, =868*868. so need to get rid of duplicate comparisons\r\n\r\n#get rid of rows that compare individuals to themselves\r\nparadise.pair<-paradise.pair[which(paradise.pair$Ind1 != paradise.pair$Ind2),]\r\nnrow(paradise.pair) #752556, = 753424-868, good\r\n\r\n#find distance between individuals\r\nparadise.pair$EndLoc1<-as.numeric(paradise.pair$EndLoc1)\r\nparadise.pair$EndLoc2<-as.numeric(paradise.pair$EndLoc2)\r\nparadise.pair$diff<-abs(paradise.pair$EndLoc1-paradise.pair$EndLoc2)\r\n\r\n#delete end locs\r\nparadise.pair$EndLoc1<-NULL\r\nparadise.pair$EndLoc2<-NULL\r\nparadise.pair\r\n\r\n#sort to identify duplicate comparisons\r\nparadise.pair<-data.frame(t(apply(paradise.pair,1,sort)))\r\nparadise.pair\r\n\r\nparadise.pair<-unique(paradise.pair)\r\nnrow(paradise.pair)# 376278, yay!\r\n\r\n#rename columns \r\ncolnames(paradise.pair)<-c(""DistApart"", ""Ind1"", ""Ind2"")\r\n\r\n#slim down to only inds that are 50m apart or less\r\nparadise.pair$DistApart<-as.numeric(paradise.pair$DistApart)\r\nparadise.50<-paradise.pair[which(paradise.pair$DistApart < 51),]\r\nmax(paradise.50$DistApart)# good\r\ntable(paradise.50$DistApart)\r\n\r\n# count number of individuals within 50m of focal individual\r\nind1.count<-as.data.frame(table(paradise.50$Ind1))\r\nind1.count\r\nstr(ind1.count)\r\nnames(ind1.count)\r\n\r\n#rename columns\r\nnames(ind1.count)[names(ind1.count)==""Var1""] <-""Id""\r\nnames(ind1.count)[names(ind1.count)==""Freq""] <-""Ind1.Count""\r\n\r\nind2.count<-as.data.frame(table(paradise.50$Ind2))\r\nind2.count\r\n\r\n#rename columns\r\nnames(ind2.count)[names(ind2.count)==""Var1""] <-""Id""\r\nnames(ind2.count)[names(ind2.count)==""Freq""] <-""Ind2.Count""\r\n\r\n#merge count dataframes to make mastercount dataframe\r\nmastercount<-merge(ind1.count, ind2.count, \r\n                   by = ""Id"",\r\n                   all = TRUE)\r\nnrow(mastercount)\r\n\r\n#replace NAs with zero\r\nmastercount$Ind1.Count[is.na(mastercount$Ind1.Count)] = 0\r\nmastercount$Ind2.Count[is.na(mastercount$Ind2.Count)] = 0\r\n\r\n# sum counts to get total number of individuals within 50m for each focal individual\r\nmastercount$total.count<-mastercount$Ind1.Count+mastercount$Ind2.Count\r\nmax(mastercount$total.count) #158\r\nmin(mastercount$total.count) #22\r\n\r\n#add # of inds within 50m  to Dispersal dataframe\r\nsetwd(""C:/Users/addis/OneDrive/manuscripts/genomics/to submit"")\r\ndispersal=read.table(""Dispersal.csv"", sep="","", header=T)\r\ndispersal\r\nnames(dispersal)\r\nnames(mastercount)\r\n\r\ndispersal$NumIndsWithin50m.paradise<- mastercount$total.count[match(dispersal$id, mastercount$Id)]\r\ndispersal\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n#calculate pairwise distances apart for bear\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nbear.pair<-data.frame()\r\nfor (i in 1:nrow(bear)){\r\n  for (j in 1:nrow(bear)){\r\n    output=c(bear$ID[i], bear$ID[j], bear$EndLoc[i], bear$EndLoc[j])\r\n    bear.pair=rbind(bear.pair,output)\r\n  }\r\n}\r\nbear.pair\r\ncolnames(bear.pair)<-c(""Ind1"", ""Ind2"", ""EndLoc1"", ""EndLoc2"")\r\nnrow(bear.pair)#863041, =929*929. so need to get rid of duplicate comparisons\r\n\r\n#get rid of rows that compare individuals to themselves\r\nbear.pair<-bear.pair[which(bear.pair$Ind1 != bear.pair$Ind2),]\r\nnrow(bear.pair) #862110, = 863041-929, good\r\n\r\n#find distance between individuals\r\nbear.pair$EndLoc1<-as.numeric(bear.pair$EndLoc1)\r\nbear.pair$EndLoc2<-as.numeric(bear.pair$EndLoc2)\r\nbear.pair$diff<-abs(bear.pair$EndLoc1-bear.pair$EndLoc2)\r\n\r\n#delete end locs\r\nbear.pair$EndLoc1<-NULL\r\nbear.pair$EndLoc2<-NULL\r\nbear.pair\r\n\r\n#sort to identify duplicate comparisons\r\nbear.pair<-data.frame(t(apply(bear.pair,1,sort)))\r\nbear.pair\r\n\r\nbear.pair<-unique(bear.pair)\r\nnrow(bear.pair)# 431055, yay!\r\n\r\n#rename columns \r\ncolnames(bear.pair)<-c(""DistApart"", ""Ind1"", ""Ind2"")\r\n\r\n#slim down to only inds that are 50m apart or less\r\nbear.pair$DistApart<-as.numeric(bear.pair$DistApart)\r\nbear.50<-bear.pair[which(bear.pair$DistApart < 51),]\r\nmax(bear.50$DistApart)# good\r\ntable(bear.50$DistApart)\r\n\r\n# count number of individuals within 50m of focal individual\r\nind1.count<-as.data.frame(table(bear.50$Ind1))\r\nind1.count\r\nstr(ind1.count)\r\nnames(ind1.count)\r\n\r\n#rename columns\r\nnames(ind1.count)[names(ind1.count)==""Var1""] <-""Id""\r\nnames(ind1.count)[names(ind1.count)==""Freq""] <-""Ind1.Count""\r\n\r\nind2.count<-as.data.frame(table(bear.50$Ind2))\r\nind2.count\r\n\r\n#rename columns\r\nnames(ind2.count)[names(ind2.count)==""Var1""] <-""Id""\r\nnames(ind2.count)[names(ind2.count)==""Freq""] <-""Ind2.Count""\r\n\r\n#merge count dataframes to make mastercount dataframe\r\nmastercount<-merge(ind1.count, ind2.count, \r\n                   by = ""Id"",\r\n                   all = TRUE)\r\nnrow(mastercount)\r\n\r\n#replace NAs with zero\r\nmastercount$Ind1.Count[is.na(mastercount$Ind1.Count)] = 0\r\nmastercount$Ind2.Count[is.na(mastercount$Ind2.Count)] = 0\r\n\r\n# sum counts to get total number of individuals within 50m for each focal individual\r\nmastercount$total.count<-mastercount$Ind1.Count+mastercount$Ind2.Count\r\nmax(mastercount$total.count) #221\r\nmin(mastercount$total.count) #17\r\n\r\n#add # of inds within 50m  to Dispersal dataframe\r\nsetwd(""C:/Users/addis/OneDrive/manuscripts/genomics/to submit"")\r\ndispersal=read.table(""Dispersal.csv"", sep="","", header=T)\r\ndispersal\r\nnames(dispersal)\r\nnames(mastercount)\r\n\r\ndispersal$NumIndsWithin50m.bear<- mastercount$total.count[match(dispersal$id, mastercount$Id)]\r\ndispersal\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n#calculate pairwise distances apart for cascade\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncas.pair<-data.frame()\r\nfor (i in 1:nrow(cascade)){\r\n  for (j in 1:nrow(cascade)){\r\n    output=c(cascade$ID[i], cascade$ID[j], cascade$EndLoc[i], cascade$EndLoc[j])\r\n    cas.pair=rbind(cas.pair,output)\r\n  }\r\n}\r\ncas.pair\r\ncolnames(cas.pair)<-c(""Ind1"", ""Ind2"", ""EndLoc1"", ""EndLoc2"")\r\nnrow(cas.pair)#57121, =239*239. so need to get rid of duplicate comparisons\r\n\r\n#get rid of rows that compare individuals to themselves\r\ncas.pair<-cas.pair[which(cas.pair$Ind1 != cas.pair$Ind2),]\r\nnrow(cas.pair) #56882, = 57121-239, good\r\n\r\n#find distance between individuals\r\ncas.pair$EndLoc1<-as.numeric(cas.pair$EndLoc1)\r\ncas.pair$EndLoc2<-as.numeric(cas.pair$EndLoc2)\r\ncas.pair$diff<-abs(cas.pair$EndLoc1-cas.pair$EndLoc2)\r\n\r\n#delete end locs\r\ncas.pair$EndLoc1<-NULL\r\ncas.pair$EndLoc2<-NULL\r\ncas.pair\r\n\r\n#sort to identify duplicate comparisions\r\ncas.pair<-data.frame(t(apply(cas.pair,1,sort)))\r\ncas.pair\r\n\r\ncas.pair<-unique(cas.pair)\r\nnrow(cas.pair)# 28441, yay!\r\n\r\n#rename columns \r\ncolnames(cas.pair)<-c(""DistApart"", ""Ind1"", ""Ind2"")\r\n\r\n#slim down to only inds that are 50m apart or less\r\ncas.pair$DistApart<-as.numeric(cas.pair$DistApart)\r\ncas.50<-cas.pair[which(cas.pair$DistApart < 51),]\r\nmax(cas.50$DistApart)# good\r\ntable(cas.50$DistApart)\r\n\r\n# count number of individuals within 50m of focal individual\r\nind1.count<-as.data.frame(table(cas.50$Ind1))\r\nind1.count\r\nstr(ind1.count)\r\nnames(ind1.count)\r\n\r\n#rename columns\r\nnames(ind1.count)[names(ind1.count)==""Var1""] <-""Id""\r\nnames(ind1.count)[names(ind1.count)==""Freq""] <-""Ind1.Count""\r\n\r\nind2.count<-as.data.frame(table(cas.50$Ind2))\r\nind2.count\r\n\r\n#rename columns\r\nnames(ind2.count)[names(ind2.count)==""Var1""] <-""Id""\r\nnames(ind2.count)[names(ind2.count)==""Freq""] <-""Ind2.Count""\r\n\r\n#merge count dataframes to make mastercount dataframe\r\nmastercount<-merge(ind1.count, ind2.count, \r\n                   by = ""Id"",\r\n                   all = TRUE)\r\nnrow(mastercount)\r\n\r\n#replace NAs with zero\r\nmastercount$Ind1.Count[is.na(mastercount$Ind1.Count)] = 0\r\nmastercount$Ind2.Count[is.na(mastercount$Ind2.Count)] = 0\r\n\r\n# sum counts to get total number of individuals within 50m for each focal individual\r\nmastercount$total.count<-mastercount$Ind1.Count+mastercount$Ind2.Count\r\nmax(mastercount$total.count) #61\r\nmin(mastercount$total.count) #3\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n#add # of inds within 50m  to Dispersal dataframe\r\n\r\ndispersal=read.table(""Dispersal.csv"", sep="","", header=T)\r\ndispersal\r\nnames(dispersal)\r\nnames(mastercount)\r\n\r\ndispersal$NumIndsWithin50m.cascade<- mastercount$total.count[match(dispersal$id, mastercount$Id)]\r\ndispersal\r\n\r\n\r\n#replace NAs with 0\r\ndispersal$NumIndsWithin50m.bear[is.na(dispersal$NumIndsWithin50m.bear)]<-0\r\ndispersal$NumIndsWithin50m.canyon[is.na(dispersal$NumIndsWithin50m.canyon)]<-0\r\ndispersal$NumIndsWithin50m.cascade[is.na(dispersal$NumIndsWithin50m.cascade)]<-0\r\ndispersal$NumIndsWithin50m.paradise[is.na(dispersal$NumIndsWithin50m.paradise)]<-0\r\ndispersal$NumIndsWithin50m.zigzag[is.na(dispersal$NumIndsWithin50m.zigzag)]<-0\r\n\r\n#create one column\r\ndispersal$NumIndsWithin50m<-dispersal$NumIndsWithin50m.bear+dispersal$NumIndsWithin50m.canyon+\r\n  dispersal$NumIndsWithin50m.cascade+dispersal$NumIndsWithin50m.paradise+dispersal$NumIndsWithin50m.zigzag\r\n\r\n#delete extra columns\r\ndispersal$NumIndsWithin50m.bear<-NULL\r\ndispersal$NumIndsWithin50m.canyon<-NULL\r\ndispersal$NumIndsWithin50m.cascade<-NULL\r\ndispersal$NumIndsWithin50m.paradise<-NULL\r\ndispersal$NumIndsWithin50m.zigzag<-NULL\r\ndispersal\r\n\r\n#calculate mean number of conspecifics within 50m\r\ndispersal<-dispersal[!is.na(dispersal$NumIndsWithin50m),]# get rid of rows with NA\r\nmin(dispersal$NumIndsWithin50m)#6\r\nmax(dispersal$NumIndsWithin50m)#221\r\nmedian(dispersal$NumIndsWithin50m)#89\r\n\r\n#add dispersal key\r\ndispersal$disp<-""NULL""\r\ndispersal$disp[dispersal$absolute_distance>=10]<-""yes""\r\ndispersal$disp[dispersal$absolute_distance<10]<-""no""\r\ndispersal[,c(""id"", ""absolute_distance"", ""disp"")]\r\ntable(dispersal$disp)\r\n\r\n# test for correlation between salamander density and dispersal distance\r\n#exclude distance > 500m\r\n\r\ndispersal2<-dispersal[which(dispersal$absolute_distance<501),]\r\ndispersal2$log.dist<-log(dispersal2$absolute_distance+1)\r\nstr(dispersal2)\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n#linear mixed model\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nLMM<-lme(NumIndsWithin50m~disp*reach, random = ~1|stream, \r\n          data=dispersal2, na.action=na.exclude)\r\n\r\nsummary(LMM)\r\n\r\n#get estimated marginal means\r\nem.lmm<-emmeans(LMM, specs=pairwise~reach:disp, adjust=""tukey"")\r\nem.response<-summary(em.lmm, type=""response"")\r\nem.response\r\n\r\n#save estimates for plotting\r\nem.estimates<-as.data.frame(em.response$emmeans)\r\nem.estimates$upper.se<-em.estimates$emmean + em.estimates$SE\r\nem.estimates$lower.se<-em.estimates$emmean - em.estimates$SE\r\n\r\n#rename column \r\nnames(em.estimates)[names(em.estimates)==""disp""] <-""Dispersal""\r\nem.estimates\r\n\r\n\r\n#greyscale\r\nc<-ggplot(em.estimates, aes(x=reach, y=emmean))+\r\n  geom_point(size=4, aes(color=reach, shape=Dispersal),position=position_dodge(width=0.4))+\r\n  geom_errorbar(aes(ymin=lower.se, ymax=upper.se, color=reach, group=Dispersal),width = .1,position=position_dodge(width=0.4))+\r\n  scale_shape_manual(values=c(17,16), guide=guide_legend(override.aes=list(shape=c(2,1), size=4)))+\r\n  scale_color_manual(values=c(\'#000000\', \'#999999\'), guide=""none"")+\r\n  scale_fill_discrete()+\r\n  labs(x=""Reach"", y= ""Number of conspecifics within 50m (EMM \\u00B1 SE)"")+\r\n  theme_classic()+\r\n  theme(axis.title=element_text(size=9),axis.text.y=element_text(size=8), axis.text.x=element_text(size=8),\r\n        legend.position=""top"",legend.text=element_text(size=8), legend.title=element_text(size=8))\r\n\r\n\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# create multiplanel plot\r\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nggarrange(a,b,c, nrow=2, ncol=3)\r\n\r\n\r\n\r\n']","Environmentally associated variation in dispersal distance affects inbreeding risk in a stream salamander Avoiding inbreeding is considered a key driver of dispersal evolution, and dispersal distances should be especially important in mediating inbreeding risk because the likelihood of mating with relatives decreases with dispersal distance. However, a lack of direct data on dispersal distances has limited empirical tests of this prediction, particularly in the context of the multiple selective forces that can influence dispersal. Using a headwater salamander system, we tested whether spatial variation in environmental conditions leads to differences in dispersal distances, resulting in spatial variation in the effect of dispersal on inbreeding risk. Using capture-recapture and population genomic data from 5 streams, we found that dispersal distances were greater in downstream reaches than upstream reaches. Inbreeding risk was lower for dispersers than non-dispersers in downstream reaches, but not in upstream reaches. Furthermore, stream reaches did not differ in spatial patterns of individual relatedness, indicating that variation in inbreeding risk was in fact due to differences in dispersal distances. These results demonstrate that environmentally associated variation in dispersal distances can cause the inbreeding consequences of dispersal to vary at fine spatial scales. They also show that selective pressures other than inbreeding avoidance maintain phenotypic variation in dispersal, underscoring the importance of addressing alternative hypotheses in dispersal research.",4
Do Synthesis Centers Synthesize? A Semantic Analysis of Topical Diversity in Research,"Synthesis centers are a form of scientific organization that catalyzes and supports research that integrates diverse theories, methods and data across spatial or temporal scales to increase the generality, parsimony, applicability, or empirical soundness of scientific explanations. Synthesis working groups are a distinctive form of scientific collaboration that produce consequential, high-impact publications. But no one has asked if synthesis working groups synthesize: are their publications substantially more diverse than others, and if so, in what ways and with what effect? We investigate these questions by using Latent Dirichlet Analysis to compare the topical diversity of papers published by synthesis center collaborations with that of papers in a reference corpus. Topical diversity was operationalized and measured in several ways, both to reflect aggregate diversity and to emphasize particular aspects of diversity (such as variety, evenness, and balance). Synthesis center publications have greater topical variety and evenness, but less disparity, than do papers in the reference corpus. The influence of synthesis center origins on aspects of diversity is only partly mediated by the size and heterogeneity of collaborations: when taking into account the numbers of authors, distinct institutions, and references, synthesis center origins retain a significant direct effect on diversity measures. Controlling for the size and heterogeneity of collaborative groups, synthesis center origins and diversity measures significantly influence the visibility of publications, as indicated by citation measures. We conclude by suggesting social processes within collaborations that might account for the observed effects, by inviting further exploration of what this novel textual analysis approach might reveal about interdisciplinary research, and by offering some practical implications of our results.","['#Script: percentiles.R\r\n#Author: Diego Chavarro (dchavarro@gmail.com), Based on the paper Do Synthesis Centers Synthesize? Research Policy, forthcoming.\r\n#Version: 0.3\r\n#Note: this script is only provided as a guidance that needs to be adapted to your specific computing environment. It computes median and top 10% diversity adapted from Uzzi\'s paper: Uzzi, B., Mukherjee, S., Stringer, M., & Jones, B. (2013). Atypical Combinations and Scientific Impact. Science, 342(6157), 468-472. https://doi.org/10.1126/science.1240474\r\n\r\n\r\n\r\n\r\n\r\n#require library tidyr (needed for this script to work)\r\n library(tidyr)\r\n\r\n#set work-space\r\n setwd(""PATH\\\\TO\\\\YOUR-WORKSPACE"")\r\n\r\n#Load topic distance matrix\r\n distData <-read.table(""PATH\\\\TO\\\\distMatrix1.csv"", header=TRUE, sep="","")\r\n#convert distance matrix into long format for calculations \r\n distDataLong <- gather(distData, topic, weight, T1:T152)\r\n#apply column names distance matrix. First column is topic 1, second column topic 2, and third column is the distance between the 2.\r\n colnames(distDataLong) <- c(""topic1"",""topic2"",""distance"")\r\n#remove the ""T"" and leave only the numbers of the topics 1 to 152\r\n distDataLong$topic1 <- gsub(""T"", """", distDataLong$topic1)\r\n distDataLong$topic2 <- gsub(""T"", """", distDataLong$topic2)\r\n#ensure cells are numeric\r\n distDataLong$topic2 <- as.numeric(distDataLong$topic2)\r\n distDataLong$topic1 <- as.numeric(distDataLong$topic1)\r\n#sort out matrix\r\n distDataLong <- distDataLong[order(distDataLong$topic2,distDataLong$topic1),]\r\n#read papers database. In the repository there is one file per batch of approx. 50,000 papers. change file to analyze a different batch, or implement a loop.\r\n art <- read.table(""PATH#\\\\TO\\\\Table 3 .csv"", header=TRUE, sep="","", row.names=""id"")\r\n#calculate proportions of each paper each of the 152 topics\r\n prop <- art / rowSums(art, na.rm=TRUE)\r\n#count number of papers in batch\r\n count <- nrow(prop)\r\n\r\n#calculations for each paper in batch\r\n\tfor(i in 1:count){\r\n\t\t#paperTopicMatrix combinations\r\n\t\t PaperTopicX <- as.numeric(prop[i,]) %*% t(as.numeric(prop[i,])) \r\n\t\t#convert to data frame\r\n\t\t PaperTopicX <- as.data.frame(PaperTopicX)\r\n\t\t#add topic numbers to columns\r\n\t\t PaperTopicX <- cbind(X = seq(1:152),PaperTopicX)\r\n\t\t#convert to Long format\r\n\t\t PaperTopicXLong <- gather(PaperTopicX,topic, weight, V1:V152)\r\n\t\t#name the columns of the long format table for papers and topics\r\n\t\t colnames(PaperTopicXLong) <- c(""topic1"",""topic2"",""weight"")\r\n\t\t#remove V from variable names, so that the topics match those of the distance matrix\r\n\t\t PaperTopicXLong$topic2 <- gsub(""V"", """", distDataLong$topic2)\r\n\t\t#ensure cells are numeric\r\n\t\t PaperTopicXLong$topic2 <- as.numeric(PaperTopicXLong$topic2)\r\n\t\t PaperTopicXLong$topic1 <- as.numeric(PaperTopicXLong$topic1)\r\n\t\t#merge topic distance and paper distribution of topics\r\n\t\t paper <- merge(PaperTopicXLong,distDataLong,by=c(""topic1"",""topic2""))\r\n\t\t#add identifiers to papers\r\n\t\t paper <- cbind(Id = rownames(prop[i,]),paper)\r\n\t\t#sort papers \r\n\t\t OrderedPaper <- paper[order(paper$distance,paper$topic1),]\r\n\t\t#calculate the cumulative sum of sum of each paper\'s weights\r\n\t\t OrderedPaper <- cbind(OrderedPaper, cumul = cumsum(OrderedPaper$weight))\r\n\t\t#find the values at which the cumulative sum reaches 0.5\r\n\t\t paperPercentil50 <-head (OrderedPaper[OrderedPaper$cumul >=0.5,],1)\r\n\t\t#find the values at which the cumulative sum reaches 0.9\r\n\t\t paperPercentil90 <-head (OrderedPaper[OrderedPaper$cumul >=0.9,],1)\r\n\t\t#merge results\r\n\t\t paperPercentil <- cbind(paperPercentil50, paperPercentil90)\r\n\t\t#write results\r\n\t\t write.table(paperPercentil, ""percentil.txt"", append=TRUE, quote = TRUE, sep = ""\\t"", na = ""0"", row.names=TRUE, col.names=FALSE)\r\n\r\n\t\t\r\n\r\n\t}\r\n']","Do Synthesis Centers Synthesize? A Semantic Analysis of Topical Diversity in Research Synthesis centers are a form of scientific organization that catalyzes and supports research that integrates diverse theories, methods and data across spatial or temporal scales to increase the generality, parsimony, applicability, or empirical soundness of scientific explanations. Synthesis working groups are a distinctive form of scientific collaboration that produce consequential, high-impact publications. But no one has asked if synthesis working groups synthesize: are their publications substantially more diverse than others, and if so, in what ways and with what effect? We investigate these questions by using Latent Dirichlet Analysis to compare the topical diversity of papers published by synthesis center collaborations with that of papers in a reference corpus. Topical diversity was operationalized and measured in several ways, both to reflect aggregate diversity and to emphasize particular aspects of diversity (such as variety, evenness, and balance). Synthesis center publications have greater topical variety and evenness, but less disparity, than do papers in the reference corpus. The influence of synthesis center origins on aspects of diversity is only partly mediated by the size and heterogeneity of collaborations: when taking into account the numbers of authors, distinct institutions, and references, synthesis center origins retain a significant direct effect on diversity measures. Controlling for the size and heterogeneity of collaborative groups, synthesis center origins and diversity measures significantly influence the visibility of publications, as indicated by citation measures. We conclude by suggesting social processes within collaborations that might account for the observed effects, by inviting further exploration of what this novel textual analysis approach might reveal about interdisciplinary research, and by offering some practical implications of our results.",4
Conspicuous coloration of toxin-resistant predators implicates additional trophic interactions in a predator-prey arms race,"Antagonistic coevolution between natural enemies can produce highly exaggerated traits, such as prey toxins and predator resistance. This reciprocal process of adaptation and counter-adaptation may also open doors to other evolutionary novelties not directly involved in the phenotypic interface of coevolution. We tested the hypothesis that predator-prey coevolution coincided with the evolution of conspicuous coloration on resistant predators that retain prey toxins. In western North America, common garter snakes (Thamnophis sirtalis) have evolved extreme resistance to tetrodotoxin (TTX) in the coevolutionary arms race with their deadly prey, Pacific newts (Taricha spp.). TTX-resistant snakes can retain large amounts of ingested TTX, which could serve as a deterrent against the snakes' own predators if TTX toxicity and resistance are coupled with a conspicuous warning signal. We evaluated whether arms race escalation co-varies with bright red coloration in snake populations across the geographic mosaic of coevolution. Snake color variation departs from the neutral expectations of population genetic structure and co-varies with escalating clines of newt TTX and snake resistance at two coevolutionary hotspots. In the Pacific Northwest, bright red coloration fits an expected pattern of an aposematic warning to avian predators: TTX-resistant snakes that consume highly toxic newts also have relatively large, reddish-orange dorsal blotches. Snake coloration also seems to have evolved with the arms race in California, but overall patterns are less intuitively consistent with aposematism. These results suggest that interactions with additional trophic levels can generate novel traits as a cascading consequence of arms race coevolution across the geographic mosaic.","['library(dplyr)\nlibrary(lsmeans)\nlibrary(lme4)\nlibrary(ggplot2); theme_set(theme_classic())\nlibrary(hzar)\nlibrary(ecodist)\nlibrary(cowplot)\nlibrary(biotools)\nlibrary(pwr)\n\n#--------User Input----------------------------------------\n# Set working directory\nsetwd(""~/Desktop/Aposematism_ms_Dryad/"")\n\n# Load file with summary data\nrawData <- read.csv(file = ""Thamnophis_color.csv"")\n\n#-------Load Data------------------------------------------\n# Load regional phenotypic/DIV data from Hague disseration\nallData <- read.csv(""CA_TTXdata.csv"")\n\n#-----Subset regional data-------------------------------------\nfocalPops <- c(""Dry"", ""Ang"", ""Will"", ""Hop"", ""Knox"")\nrawData$Pop <- gsub(""_.*$"", """", rawData$Tube.Label.ID)\nfocalData <- rawData[rawData$Pop %in% focalPops,]\nfocalData$Pop <- factor(focalData$Pop, levels=c(""Dry"", ""Ang"", ""Will"", ""Hop"", ""Knox""))\n\n# Variables to analyze\n# major head scales\n# 1. Hue of major head scales = adjustedHue_head (values centered on 1 based on uncorrected measurements)\n\n# upper temporal and post-temporals\n# 1. Hue of temporals = adjustedHue_corner (values centered on 1 based on uncorrected measurements)\n\n# three uppermost labials\n# 1. Hue of labials = adjustedHue_Labial (values centered on 1 based on uncorrected measurements)\n# 2. Proportion of red on labial = proportion.red\n\n# dorsal blotches\n# 1. Hue of dorsal splotches = adjustedHue_splotch (values centered on 1 based on uncorrected measurements)\n# 2. Area of splotches = average.area.splotch (need to add SVL or mass as covariate)\n\n#-----calculate population means for distance analyses--------\n\n#calculate SVL-adjusted splotch area\naverage.area.splotch.RESID <- lm(average.area.splotch ~ SVL..mm., focalData)\naverage.area.splotch.RESID <- as.data.frame(resid(average.area.splotch.RESID))\nfocalData <- merge(focalData, average.area.splotch.RESID, by=""row.names"", all=TRUE)\nnames(focalData)[names(focalData) == \'resid(average.area.splotch.RESID)\'] <- ""average.area.splotch.RESID""\n\nsumData_colors <- as.data.frame(focalData %>% \n                                   group_by(Pop) %>%\n                                   dplyr::summarise(n=n(),\n                                                    head_mean_hue = mean(adjustedHue_head, na.rm=TRUE),\n                                                    \n                                                    temp_mean_hue = mean(adjustedHue_corner, na.rm=TRUE),\n                                                    \n                                                    labial_mean_hue = mean(adjustedHue_Labial, na.rm=TRUE),\n                                                    labial_mean_prop = mean(proportion.red, na.rm=TRUE),\n                                                    \n                                                    splotch_mean_hue = mean(adjustedHue_splotch, na.rm=TRUE),\n                                                    splotch_mean_area = mean(average.area.splotch.RESID, na.rm=TRUE))) \nsumData_colors[sapply(sumData_colors, is.nan)] <- NA\nsumData_colors\n\nallData <- left_join(allData, sumData_colors, by = c(""Population"" = ""Pop""))\nallData <- allData[allData$Population %in% focalPops,]\n\n#-----Plot TTX and TTX resistance-----------------------------\nnewts.TTX <- read.csv(""CA_Taricha_TTX.csv"")\nnewts.TTX$Pop <- ordered(newts.TTX$Pop, levels = c(""Dry"", ""Ang"", ""Will"", ""Hop"", ""Knox""))\n\nas.data.frame(newts.TTX %>% group_by(Pop, Species) %>% dplyr::summarise(n=n()))\nas.data.frame(newts.TTX %>% group_by(Pop) %>% dplyr::summarise(n=n(),\n                                                               meanTTX=mean(TTXcm2ug),\n                                                               se=sd(TTXcm2ug)/sqrt(n())))\n\np1 <- ggplot(newts.TTX, aes(x=Pop, y=TTXcm2ug)) +\n  geom_boxplot(width=0.4, outlier.shape = NA) +\n  geom_point(aes(x=Pop, y=TTXcm2ug, color=Species),  position=position_jitter(width=0.15), size=2) +\n  scale_y_continuous(trans=""log10"") +\n  annotation_logticks(sides = ""l"") +\n  labs(x=""Population"", y=""TTX (ug/cm2)"") +\n  theme(legend.position = ""none"") \np1\n\nnewts.TTX$logTTX <- log10(newts.TTX$TTXcm2ug + 0.1)\n\n#One-way ANOVA if TTX differs among localities\nplot(density(newts.TTX$logTTX))\nlm_newts.TTX <- lm(logTTX ~ Pop, data = newts.TTX)\n# Type I SS\nanova(lm_newts.TTX)\n# Type III SS\ncar::Anova(lm_newts.TTX, type=""III"")\n\n#One-way ANOVA if TTX differs among species at Willits\nplot(density(subset(newts.TTX, Pop == ""Will"")$logTTX))\nlm_newts.TTX_Will <- lm(logTTX ~ Species, data = subset(newts.TTX, Pop == ""Will""))\n# Type I SS\nanova(lm_newts.TTX_Will)\n# Type III SS\ncar::Anova(lm_newts.TTX_Will, type=""III"")\n\n#One-way ANOVA if TTX differs among species at Knoxville\nplot(density(subset(newts.TTX, Pop == ""Knox"")$logTTX))\nlm_newts.TTX_Knox <- lm(logTTX ~ Species, data = subset(newts.TTX, Pop == ""Knox""))\n# Type I SS\nanova(lm_newts.TTX_Knox)\n# Type III SS\ncar::Anova(lm_newts.TTX_Knox, type=""III"")\n\nallData$Population <- ordered(allData$Population, levels = c(""Dry"", ""Ang"", ""Will"", ""Hop"", ""Knox""))\np2 <- ggplot(allData, aes(x=Population, y=MAMU)) +\n  geom_point(size=2) +\n  geom_errorbar(aes(ymax=MAMUupperCI, ymin=MAMUlowerCI, width=0.3), size=1) +\n  labs(x=""Distance (km)"", y=""50% MAMU dose"") +\n  scale_y_continuous(trans=""log10"") + \n  annotation_logticks(sides = ""l"") +\n  labs(x=""Population"", y=""TTX Resistance (50% MAMU dose)"")\np2\n\n#------------Analysis------------------------\n#---------major head scales------------------\n###One-way ANOVA for hue before color correction\nsum(!is.na(focalData$adjustedHue_head))\nplot(density(focalData$adjustedHue_head, na.rm=TRUE))\nplot(density(log10(focalData$adjustedHue_head), na.rm=TRUE))\np4 <- ggplot(subset(focalData, Pop != ""Warr"" & Pop != ""Bent""), aes(x=Pop, y=adjustedHue_head)) +\n        geom_boxplot(width=0.4, outlier.shape = NA) +\n        geom_point(aes(x=Pop, y=adjustedHue_head),  position=position_jitter(width=.15), size=2) +\n        labs(x=""Population"", y=""Major Head Scales Hue"")\np4\n\noptions(contrasts = c(""contr.sum"", ""contr.poly"")) # for type III SSS\n# options(contrasts = c(""contr.treatment"", ""contr.poly"")) # Default\n\nlm_head_hue <- lm(adjustedHue_head ~ Pop, data = focalData) \n\n# Check model assumptions\n#1. Homogeneity of variances\nplot(lm_head_hue, 1)\n#2. Normality\nplot(lm_head_hue, 2)\n\n# Type I SS\nanova(lm_head_hue)\n# Type III SS\ncar::Anova(lm_head_hue, type=""III"")\n\n# Pearson\'s correlation\ncor.test(allData$head_mean_hue, allData$lnMAMU, method=""pearson"")\ncor.test(allData$head_mean_hue, allData$logTTXcm2ug.mean, method=""pearson"")\n\np4.3 <- ggplot(allData, aes(x=lnMAMU, y=head_mean_hue, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Dark2"") +\n  labs(x=""mean lnMAMU"", y=""mean head hue"")\np4.3\n\np4.4 <- ggplot(allData, aes(x=logTTXcm2ug.mean, y=head_mean_hue, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Dark2"") +\n  labs(x=""mean logTTX"", y=""mean head hue"")\np4.4\n\n#----------temporal scales scales----------\n###One-way ANOVA for hue before color correction\nsum(!is.na(focalData$adjustedHue_corner))\nplot(density(focalData$adjustedHue_corner, na.rm=TRUE))\nplot(density(log10(focalData$adjustedHue_corner), na.rm=TRUE))\np6 <- ggplot(subset(focalData, Pop != ""Warr"" & Pop != ""Bent""), aes(x=Pop, y=adjustedHue_corner)) +\n        geom_boxplot(width=0.4, outlier.shape = NA) +\n        geom_point(aes(x=Pop, y=adjustedHue_corner),  position=position_jitter(width=.15), size=2) +\n        labs(x=""Population"", y=""Temporal Scales Hue"")\np6\n\noptions(contrasts = c(""contr.sum"", ""contr.poly"")) # for type III SSS\n# options(contrasts = c(""contr.treatment"", ""contr.poly"")) # Default\n\nlm_temporals_hue <- lm(adjustedHue_corner ~ Pop, data = focalData) \n\n# Check model assumptions\n#1. Homogeneity of variances\nplot(lm_temporals_hue, 1)\n#2. Normality\nplot(lm_temporals_hue, 2)\n\n# Type I SS\nanova(lm_temporals_hue)\n# Type III SS\ncar::Anova(lm_temporals_hue, type=""III"")\n\n# Pearson\'s correlation\ncor.test(allData$temp_mean_hue, allData$lnMAMU, method=""pearson"")\ncor.test(allData$temp_mean_hue, allData$logTTXcm2ug.mean, method=""pearson"")\n\np6.3 <- ggplot(allData, aes(x=lnMAMU, y=temp_mean_hue, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Dark2"") +\n  labs(x=""mean lnMAMU"", y=""mean temp hue"")\np6.3\n\np6.4 <- ggplot(allData, aes(x=logTTXcm2ug.mean, y=temp_mean_hue, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Dark2"") +\n  labs(x=""mean logTTX"", y=""mean temp hue"")\np6.4\n\n#-----------labial scales----------\n###One-way ANOVA for hue before color correction\nsum(!is.na(focalData$adjustedHue_Labial))\nplot(density(focalData$adjustedHue_Labial, na.rm=TRUE))\nplot(density(log10(focalData$adjustedHue_Labial), na.rm=TRUE))\np8 <- ggplot(subset(focalData, Pop != ""Warr"" & Pop != ""Bent""), aes(x=Pop, y=adjustedHue_Labial)) +\n        geom_boxplot(width=0.4, outlier.shape = NA) +\n        geom_point(aes(x=Pop, y=adjustedHue_Labial),  position=position_jitter(width=.15), size=2) +\n        labs(x=""Population"", y=""Labial Scales Hue"")\np8\n\noptions(contrasts = c(""contr.sum"", ""contr.poly"")) # for type III SSS\n# options(contrasts = c(""contr.treatment"", ""contr.poly"")) # Default\n\nlm_labial_hue <- lm(adjustedHue_Labial ~ Pop, data = focalData) \n\n# Check model assumptions\n#1. Homogeneity of variances\nplot(lm_labial_hue, 1)\n#2. Normality\nplot(lm_labial_hue, 2)\n\n# Type I SS\nanova(lm_labial_hue)\n# Type III SS\ncar::Anova(lm_labial_hue, type=""III"")\n\n# Pearson\'s correlation\ncor.test(allData$labial_mean_hue, allData$lnMAMU, method=""pearson"")\ncor.test(allData$labial_mean_hue, allData$logTTXcm2ug.mean, method=""pearson"")\n\np8.3 <- ggplot(allData, aes(x=lnMAMU, y=labial_mean_hue, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Dark2"") +\n  labs(x=""mean lnMAMU"", y=""mean labial hue"")\np8.3\n\np8.4 <- ggplot(allData, aes(x=logTTXcm2ug.mean, y=labial_mean_hue, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Dark2"") +\n  labs(x=""mean logTTX"", y=""mean labial hue"")\np8.4\n\n###Kruskal Wallis Rank Sum Test for proportion of red on labial\nsum(!is.na(focalData$proportion.red))\nplot(density(focalData$proportion.red, na.rm=TRUE))\np9 <- ggplot(subset(focalData, Pop != ""Warr"" & Pop != ""Bent""), aes(x=Pop, y=proportion.red)) +\n        geom_boxplot(width=0.4, outlier.shape = NA) +\n        geom_point(aes(x=Pop, y=proportion.red),  position=position_jitter(width=.15), size=2) +\n        labs(x=""Population"", y=""Proportion Labial Scales Red"")\np9\n\nkruskal.test(proportion.red ~ Pop, data = focalData)\n\n# Pearson\'s correlation\ncor.test(allData$labial_mean_prop, allData$lnMAMU, method=""pearson"")\ncor.test(allData$labial_mean_prop, allData$logTTXcm2ug.mean, method=""pearson"")\n\np9.3 <- ggplot(allData, aes(x=lnMAMU, y=labial_mean_prop, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Dark2"") +\n  labs(x=""mean lnMAMU"", y=""mean labial prop"")\np9.3\n\np9.4 <- ggplot(allData, aes(x=logTTXcm2ug.mean, y=labial_mean_prop, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Dark2"") +\n  labs(x=""mean logTTX"", y=""mean labial prop"")\np9.4\n\n#---------dorsal blotches----------\n###One-way ANOVA for hue before color correction\nsum(!is.na(focalData$adjustedHue_splotch))\nplot(density(focalData$adjustedHue_splotch, na.rm=TRUE))\nplot(density(log10(focalData$adjustedHue_splotch), na.rm=TRUE))\np11 <- ggplot(subset(focalData, Pop != ""Warr"" & Pop != ""Bent""), aes(x=Pop, y=adjustedHue_splotch)) +\n          geom_boxplot(width=0.4, outlier.shape = NA) +\n          geom_point(aes(x=Pop, y=adjustedHue_splotch),  position=position_jitter(width=.15), size=2) +\n          labs(x=""Population"", y=""Dorsal Splotches Hue"")\np11\n\noptions(contrasts = c(""contr.sum"", ""contr.poly"")) # for type III SSS\n# options(contrasts = c(""contr.treatment"", ""contr.poly"")) # Default\n\nlm_splotch_hue <- lm(adjustedHue_splotch ~ Pop, data = focalData) \n\n# Check model assumptions\n#1. Homogeneity of variances\nplot(lm_splotch_hue, 1)\n#2. Normality\nplot(lm_splotch_hue, 2)\n\n# Type I SS\nanova(lm_splotch_hue)\n# Type III SS\ncar::Anova(lm_splotch_hue, type=""III"")\n\n# Pearson\'s correlation\ncor.test(allData$splotch_mean_hue, allData$lnMAMU, method=""pearson"")\ncor.test(allData$splotch_mean_hue, allData$logTTXcm2ug.mean, method=""pearson"")\n\np11.3 <- ggplot(allData, aes(x=lnMAMU, y=splotch_mean_hue, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Dark2"") +\n  labs(x=""mean lnMAMU"", y=""mean splotch hue"")\np11.3\n\np11.4 <- ggplot(allData, aes(x=logTTXcm2ug.mean, y=splotch_mean_hue, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Dark2"") +\n  labs(x=""mean logTTX"", y=""mean splotch hue"")\np11.4\n\n###One-way ANOVA on residual red area of blotches (after accounting for SVL)\nsum(!is.na(focalData$average.area.splotch.RESID))\nplot(density(focalData$average.area.splotch.RESID, na.rm=TRUE))\nplot(density(log10(focalData$average.area.splotch.RESID), na.rm=TRUE))\np12 <- ggplot(subset(focalData, Pop != ""Warr"" & Pop != ""Bent""), aes(x=Pop, y=average.area.splotch.RESID)) +\n          geom_boxplot(width=0.4, outlier.shape = NA) +\n          geom_point(aes(x=Pop, y=average.area.splotch.RESID),  position=position_jitter(width=.15), size=2) +\n          labs(x=""Population"", y=""Dorsal Splotches Area (SVL-adjusted)"")\np12\n\noptions(contrasts = c(""contr.sum"", ""contr.poly"")) # for type III SSS\n# options(contrasts = c(""contr.treatment"", ""contr.poly"")) # Default\n\nlm_splotch_area.RESID <- lm(average.area.splotch.RESID ~ Pop, data = focalData) \n\n# Check model assumptions\n#1. Homogeneity of variances\nplot(lm_splotch_area.RESID, 1)\n#2. Normality\nplot(lm_splotch_area.RESID, 2)\n\n# Type I SS\nanova(lm_splotch_area.RESID)\n# Type III SS\ncar::Anova(lm_splotch_area.RESID, type=""III"")\n\n# Pearson\'s correlation\ncor.test(allData$splotch_mean_area, allData$lnMAMU, method=""pearson"")\ncor.test(allData$splotch_mean_area, allData$logTTXcm2ug.mean, method=""pearson"")\n\np12.3 <- ggplot(allData, aes(x=lnMAMU, y=splotch_mean_area, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Dark2"") +\n  labs(x=""mean lnMAMU"", y=""mean splotch area"")\np12.3\n\np12.4 <- ggplot(allData, aes(x=logTTXcm2ug.mean, y=splotch_mean_area, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Dark2"") +\n  labs(x=""mean logTTX"", y=""mean splotch area"")\np12.4\n', 'library(dplyr)\nlibrary(lsmeans)\nlibrary(lme4)\nlibrary(ggplot2); theme_set(theme_classic())\nlibrary(hzar)\nlibrary(ecodist)\nlibrary(cowplot)\nlibrary(pwr)\n\n#--------User Input----------------------------------------\n# Set working directory\nsetwd(""~/Desktop/Aposematism_ms_Dryad/"")\n\n# Load file with summary data\nrawData <- read.csv(file = ""Thamnophis_color.csv"")\n\n#-------Load Data------------------------------------------\n# Load regional phenotypic/DIV data from Hague dissertation\nallData <- read.csv(""PNW_TTXData.csv"")\n\n#-----Subset regional data-------------------------------------\nfocalPops <- c(""Clal"", ""Cook"", ""Pot"", ""Warr"", ""Hebo"", ""Bent"", ""Ten"", ""Tahk"", ""Elk"")\nrawData$Pop <- gsub(""_.*$"", """", rawData$Tube.Label.ID)\nrawData$Pop <- sub(""Pott"", ""Pot"", rawData$Pop)\nfocalData <- rawData[rawData$Pop %in% focalPops,]\n\nfocalData$Pop <- factor(focalData$Pop, levels=c(""Clal"", ""Cook"", ""Pot"", ""Warr"", ""Hebo"", ""Bent"", ""Ten"", ""Tahk"", ""Elk""))\n\n# Variables to analyze\n# major head scales\n# 1. Hue of major head scales = adjustedHue_head (values centered on 1 based on uncorrected measurements)\n\n# upper temporal and post-temporals\n# 1. Hue of temporals = adjustedHue_corner (values centered on 1 based on uncorrected measurements)\n\n# three uppermost labials\n# 1. Hue of labials = adjustedHue_Labial (values centered on 1 based on uncorrected measurements)\n# 2. Proportion of red on labial = proportion.red\n\n# dorsal blotches\n# 1. Hue of dorsal splotches = adjustedHue_splotch (values centered on 1 based on uncorrected measurements)\n# 2. Area of splotches = average.area.splotch (need to add SVL or mass as covariate)\n\n#-----calculate population means for distance analyses--------\n\n#calculate SVL-adjusted splotch area\naverage.area.splotch.RESID <- lm(average.area.splotch ~ SVL..mm., focalData)\naverage.area.splotch.RESID <- as.data.frame(resid(average.area.splotch.RESID))\nfocalData <- merge(focalData, average.area.splotch.RESID, by=""row.names"", all=TRUE)\nnames(focalData)[names(focalData) == \'resid(average.area.splotch.RESID)\'] <- ""average.area.splotch.RESID""\n\nsumData_colors <- as.data.frame(focalData %>% \n                                   group_by(Pop) %>%\n                                   dplyr::summarise(n=n(),\n                                                    head_mean_hue = mean(adjustedHue_head, na.rm=TRUE),\n                                                    \n                                                    temp_mean_hue = mean(adjustedHue_corner, na.rm=TRUE),\n                                                    \n                                                    labial_mean_hue = mean(adjustedHue_Labial, na.rm=TRUE),\n                                                    labial_mean_prop = mean(proportion.red, na.rm=TRUE),\n                                                    \n                                                    splotch_mean_hue = mean(adjustedHue_splotch, na.rm=TRUE),\n                                                    splotch_mean_area = mean(average.area.splotch.RESID, na.rm=TRUE))) \nsumData_colors[sapply(sumData_colors, is.nan)] <- NA\nsumData_colors\n\nallData <- left_join(allData, sumData_colors, by = c(""Population"" = ""Pop""))\nallData <- allData[allData$Population %in% focalPops,]\n\n#-----Plot TTX and TTX resistance-----------------------------\nnewts.TTX <- read.csv(""PNW_Taricha_TTX.csv"")\nnewts.TTX$Pop <- sub(""Pott"", ""Pot"", newts.TTX$Pop)\nnewts.TTX$Pop <- ordered(newts.TTX$Pop, levels = c(""Clal"", ""Cook"", ""Pot"", ""Warr"", ""Hebo"", ""Bent"", ""Ten"", ""Tahk"", ""Elk""))\n\np1 <- ggplot(newts.TTX, aes(x=Pop, y=TTXcm2ug)) +\n  geom_boxplot(width=0.4, outlier.shape = NA) +\n  geom_point(aes(x=Pop, y=TTXcm2ug),  position=position_jitter(width=.15), size=2) +\n  scale_y_continuous(trans=""log10"") +\n  annotation_logticks(sides = ""l"") +\n  labs(x=""Population"", y=""TTX (ug/cm2)"")\np1\n\nallData$Population <- ordered(allData$Population, levels = c(""Clal"", ""Cook"", ""Pot"", ""Warr"", ""Hebo"", ""Bent"", ""Ten"", ""Tahk"", ""Elk""))\np2 <- ggplot(allData, aes(x=Population, y=MAMU)) +\n  geom_point(size=2) +\n  geom_errorbar(data=allData, aes(ymax=MAMUupperCI, ymin=MAMUlowerCI, width=0.3), size=1) +\n  labs(x=""Distance (km)"", y=""50% MAMU dose"") +\n  scale_y_continuous(trans=""log10"") + \n  annotation_logticks(sides = ""l"") +\n  labs(x=""Population"", y=""TTX Resistance (50% MAMU dose)"")\n\np2\n\n#------------Analysis------------------------\n#---------major head scales------------------\n###One-way ANOVA for hue before color correction\nsum(!is.na(focalData$adjustedHue_head))\nplot(density(focalData$adjustedHue_head, na.rm=TRUE))\nplot(density(log10(focalData$adjustedHue_head), na.rm=TRUE))\np4 <- ggplot(subset(focalData, Pop != ""Warr"" & Pop != ""Bent""), aes(x=Pop, y=adjustedHue_head)) +\n        geom_boxplot(width=0.4, outlier.shape = NA) +\n        geom_point(aes(x=Pop, y=adjustedHue_head),  position=position_jitter(width=.15), size=2) +\n        labs(x=""Population"", y=""Major Head Scales Hue"")\np4\n\noptions(contrasts = c(""contr.sum"", ""contr.poly"")) # for type III SSS\n# options(contrasts = c(""contr.treatment"", ""contr.poly"")) # Default\n\nlm_head_hue <- lm(adjustedHue_head ~ Pop, data = focalData) \n\n# Check model assumptions\n#1. Homogeneity of variances\nplot(lm_head_hue, 1)\n#2. Normality\nplot(lm_head_hue, 2)\n\n# Type I SS\nanova(lm_head_hue)\n# Type III SS\ncar::Anova(lm_head_hue, type=""III"")\n\n# Pearson\'s correlation\ncor.test(allData$head_mean_hue, allData$lnMAMU, method=""pearson"")\ncor.test(allData$head_mean_hue, allData$logTTXcm2ug.mean, method=""pearson"")\n\np4.3 <- ggplot(allData, aes(x=lnMAMU, y=head_mean_hue, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Set1"") +\n  labs(x=""mean lnMAMU"", y=""mean head hue"")\np4.3\n\np4.4 <- ggplot(allData, aes(x=logTTXcm2ug.mean, y=head_mean_hue, color=Population)) +\n  geom_point(size=2) + \n  scale_color_brewer(palette=""Set1"") +\n  labs(x=""mean logTTX"", y=""mean head hue"")\np4.4\n\n#---------temporal scales scales----------\n###One-way ANOVA for hue before color correction\nsum(!is.na(focalData$adjustedHue_corner))\nplot(density(focalData$adjustedHue_corner, na.rm=TRUE))\nplot(density(log10(focalData$adjustedHue_corner), na.rm=TRUE))\np6 <- ggplot(subset(focalData, Pop != ""Warr"" & Pop != ""Bent""), aes(x=Pop, y=adjustedHue_corner)) +\n        geom_boxplot(width=0.4, outlier.shape = NA) +\n        geom_point(aes(x=Pop, y=adjustedHue_corner),  position=position_jitter(width=.15), size=2) +\n        labs(x=""Population"", y=""Temporal Scales Hue"")\np6\n\noptions(contrasts = c(""contr.sum"", ""contr.poly"")) # for type III SSS\n# options(contrasts = c(""contr.treatment"", ""contr.poly"")) # Default\n\nlm_temporals_hue <- lm(adjustedHue_corner ~ Pop, data = focalData) \n\n# Check model assumptions\n#1. Homogeneity of variances\nplot(lm_temporals_hue, 1)\n#2. Normality\nplot(lm_temporals_hue, 2)\n\n# Type I SS\nanova(lm_temporals_hue)\n# Type III SS\ncar::Anova(lm_temporals_hue, type=""III"")\n\n# Pearson\'s correlation\ncor.test(allData$temp_mean_hue, allData$lnMAMU, method=""pearson"")\ncor.test(allData$temp_mean_hue, allData$logTTXcm2ug.mean, method=""pearson"")\n\np6.3 <- ggplot(allData, aes(x=lnMAMU, y=temp_mean_hue, color=Population)) +\n  geom_point() + \n  scale_color_brewer(palette=""Set1"") +\n  labs(x=""mean lnMAMU"", y=""mean temp hue"")\np6.3\n\np6.4 <- ggplot(allData, aes(x=logTTXcm2ug.mean, y=temp_mean_hue, color=Population)) +\n  geom_point() + \n  scale_color_brewer(palette=""Set1"") +\n  labs(x=""mean logTTX"", y=""mean temp hue"")\np6.4\n\n#----------labial scales----------\n###One-way ANOVA for hue before color correction\nsum(!is.na(focalData$adjustedHue_Labial))\nplot(density(focalData$adjustedHue_Labial, na.rm=TRUE))\nplot(density(log10(focalData$adjustedHue_Labial), na.rm=TRUE))\np8 <- ggplot(subset(focalData, Pop != ""Warr"" & Pop != ""Bent""), aes(x=Pop, y=adjustedHue_Labial)) +\n        geom_boxplot(width=0.4, outlier.shape = NA) +\n        geom_point(aes(x=Pop, y=adjustedHue_Labial),  position=position_jitter(width=.15), size=2) +\n        labs(x=""Population"", y=""Labial Scales Hue"")\np8\n\noptions(contrasts = c(""contr.sum"", ""contr.poly"")) # for type III SSS\n# options(contrasts = c(""contr.treatment"", ""contr.poly"")) # Default\n\nlm_labial_hue <- lm(adjustedHue_Labial ~ Pop, data = focalData) \n\n# Check model assumptions\n#1. Homogeneity of variances\nplot(lm_labial_hue, 1)\n#2. Normality\nplot(lm_labial_hue, 2)\n\n# Type I SS\nanova(lm_labial_hue)\n# Type III SS\ncar::Anova(lm_labial_hue, type=""III"")\n\n# Pearson\'s correlation\ncor.test(allData$labial_mean_hue, allData$lnMAMU, method=""pearson"")\ncor.test(allData$labial_mean_hue, allData$logTTXcm2ug.mean, method=""pearson"")\n\np8.3 <- ggplot(allData, aes(x=lnMAMU, y=labial_mean_hue, color=Population)) +\n  geom_point() + \n  scale_color_brewer(palette=""Set1"") +\n  labs(x=""mean lnMAMU"", y=""mean labial hue"")\np8.3\n\np8.4 <- ggplot(allData, aes(x=logTTXcm2ug.mean, y=labial_mean_hue, color=Population)) +\n  geom_point() + \n  scale_color_brewer(palette=""Set1"") +\n  labs(x=""mean logTTX"", y=""mean labial hue"")\np8.4\n\n###Kruskal Wallis Rank Sum Test for proportion of red on labial\nsum(!is.na(focalData$proportion.red))\nplot(density(focalData$proportion.red, na.rm=TRUE))\np9 <- ggplot(subset(focalData, Pop != ""Warr"" & Pop != ""Bent""), aes(x=Pop, y=proportion.red)) +\n        geom_boxplot(width=0.4, outlier.shape = NA) +\n        geom_point(aes(x=Pop, y=proportion.red),  position=position_jitter(width=.15), size=2) +\n        labs(x=""Population"", y=""Proportion Labial Scales Red"")\np9\n\nkruskal.test(proportion.red ~ Pop, data = focalData)\n\n# Pearson\'s correlation\ncor.test(allData$labial_mean_prop, allData$lnMAMU, method=""pearson"")\ncor.test(allData$labial_mean_prop, allData$logTTXcm2ug.mean, method=""pearson"")\n\np9.3 <- ggplot(allData, aes(x=lnMAMU, y=labial_mean_prop, color=Population)) +\n  geom_point() + \n  scale_color_brewer(palette=""Set1"") +\n  labs(x=""mean lnMAMU"", y=""mean labial prop"")\np9.3\n\np9.4 <- ggplot(allData, aes(x=logTTXcm2ug.mean, y=labial_mean_prop, color=Population)) +\n  geom_point() + \n  scale_color_brewer(palette=""Set1"") +\n  labs(x=""mean logTTX"", y=""mean labial prop"")\np9.4\n\n#----------dorsal blotches----------\n###One-way ANOVA for hue before color correction\nsum(!is.na(focalData$adjustedHue_splotch))\nplot(density(focalData$adjustedHue_splotch, na.rm=TRUE))\nplot(density(log10(focalData$adjustedHue_splotch), na.rm=TRUE))\np11 <- ggplot(subset(focalData, Pop != ""Warr"" & Pop != ""Bent""), aes(x=Pop, y=adjustedHue_splotch)) +\n          geom_boxplot(width=0.4, outlier.shape = NA) +\n          geom_point(aes(x=Pop, y=adjustedHue_splotch),  position=position_jitter(width=.15), size=2) +\n          labs(x=""Population"", y=""Dorsal Splotches Hue"")\np11\n\noptions(contrasts = c(""contr.sum"", ""contr.poly"")) # for type III SSS\n# options(contrasts = c(""contr.treatment"", ""contr.poly"")) # Default\n\nlm_splotch_hue <- lm(adjustedHue_splotch ~ Pop, data = focalData) \n\n# Check model assumptions\n#1. Homogeneity of variances\nplot(lm_splotch_hue, 1)\n#2. Normality\nplot(lm_splotch_hue, 2)\n\n# Type I SS\nanova(lm_splotch_hue)\n# Type III SS\ncar::Anova(lm_splotch_hue, type=""III"")\n\n# Pearson\'s correlation\ncor.test(allData$splotch_mean_hue, allData$lnMAMU, method=""pearson"")\ncor.test(allData$splotch_mean_hue, allData$logTTXcm2ug.mean, method=""pearson"")\n\np11.3 <- ggplot(allData, aes(x=lnMAMU, y=splotch_mean_hue, color=Population)) +\n  geom_point() + \n  scale_color_brewer(palette=""Set1"") +\n  labs(x=""mean lnMAMU"", y=""mean splotch hue"")\np11.3\n\np11.4 <- ggplot(allData, aes(x=logTTXcm2ug.mean, y=splotch_mean_hue, color=Population)) +\n  geom_point() + \n  scale_color_brewer(palette=""Set1"") +\n  labs(x=""mean logTTX"", y=""mean splotch hue"")\np11.4\n\n###One-way ANOVA on residual red area of blotches (after accounting for SVL)\nsum(!is.na(focalData$average.area.splotch.RESID))\nplot(density(focalData$average.area.splotch.RESID, na.rm=TRUE))\nplot(density(log10(focalData$average.area.splotch.RESID), na.rm=TRUE))\np12 <- ggplot(subset(focalData, Pop != ""Warr"" & Pop != ""Bent""), aes(x=Pop, y=average.area.splotch.RESID)) +\n          geom_boxplot(width=0.4, outlier.shape = NA) +\n          geom_point(aes(x=Pop, y=average.area.splotch.RESID),  position=position_jitter(width=.15), size=2) +\n          labs(x=""Population"", y=""Dorsal Splotches Area (SVL-adjusted)"")\np12\n\noptions(contrasts = c(""contr.sum"", ""contr.poly"")) # for type III SSS\n# options(contrasts = c(""contr.treatment"", ""contr.poly"")) # Default\n\nlm_splotch_area.RESID <- lm(average.area.splotch.RESID ~ Pop, data = focalData) \n\n# Check model assumptions\n#1. Homogeneity of variances\nplot(lm_splotch_area.RESID, 1)\n#2. Normality\nplot(lm_splotch_area.RESID, 2)\n\n# Type I SS\nanova(lm_splotch_area.RESID)\n# Type III SS\ncar::Anova(lm_splotch_area.RESID, type=""III"")\n\n# Pearson\'s correlation\ncor.test(allData$splotch_mean_area, allData$lnMAMU, method=""pearson"")\ncor.test(allData$splotch_mean_area, allData$logTTXcm2ug.mean, method=""pearson"")\n\np12.3 <- ggplot(allData, aes(x=lnMAMU, y=splotch_mean_area, color=Population)) +\n  geom_point() + \n  scale_color_brewer(palette=""Set1"") +\n  labs(x=""mean lnMAMU"", y=""mean splotch area"")\np12.3\n\np12.4 <- ggplot(allData, aes(x=logTTXcm2ug.mean, y=splotch_mean_area, color=Population)) +\n  geom_point() + \n  scale_color_brewer(palette=""Set1"") +\n  labs(x=""mean logTTX"", y=""mean splotch area"")\np12.4\n', 'library(""adegenet"")\nlibrary(""hierfstat"")\nlibrary(""pegas"")\nlibrary(""reshape2"")\nlibrary(""dartR"")\nlibrary(""StAMPP"")\nlibrary(""ggplot2"")\nlibrary(""tidyverse"")\nlibrary(""plotly"")\nlibrary(""poppr"")\nlibrary(""lattice"")\nlibrary(""fossil"")\nlibrary(""vcfR"")\nlibrary(""vegan"")\nlibrary(""ecodist"")\nlibrary(""matrixStats"")\nlibrary(""lme4"")\nlibrary(""car"")\nlibrary(""MuMIn"")\nlibrary(""gridExtra"")\nlibrary(""raster"")\nlibrary(""sp"")\nlibrary(""psych"")\nlibrary(""Hmisc"")\nlibrary(""corrplot"")\n\n#Set working directory\nsetwd(""~/Desktop/"")\nworkDir <- getwd()\n\n#Load filter genind object\nload(""Thamnophis_reduc.CA.gi.Rda"")\ninds <- as.data.frame(indNames(snakes.reduc.gi))\ninds <- cbind(inds, pop(snakes.reduc.gi))\ncolnames(inds) <- c(""ind"", ""pop"")\n\nlength(snakes.reduc.gi$all.names)\nlength(indNames(snakes.reduc.gi))\n\nsumData <- summary(snakes.reduc.gi)\nas.data.frame(sumData$n.by.pop)\n\n#---------Calculate population summary statistics----------------------------------\n#First count # of indivuals, plus other diversity stats in poppr (may want later)\ndiv <- poppr(snakes.reduc.gi, sample=0, plot=FALSE)\ndiv <- div[,names(div)%in%c(""Pop"", ""N"")]\n#Calculate mean observed heterozygosity (Ho), Nei\'s gene diversity (Hs, but sometimes called expected heterozygosity), \n#and Fis in hierfstat\nHo <- colMeans(basic.stats(snakes.reduc.gi)$Ho, na.rm=T) #Equation 7.38 on pg. 164 of Nei (1987) -- proportion of heterozygosity observed\nHoSD <- colSds(basic.stats(snakes.reduc.gi)$Ho, na.rm=T)\nHs <- colMeans(basic.stats(snakes.reduc.gi)$Hs, na.rm=T) #Equation 7.39 on pg. 164 of Nei (1987) -- Nei\'s gene diversity, proportion of heterozygosity expected\nHsSD <- colSds(basic.stats(snakes.reduc.gi)$Hs, na.rm=T)\nFis <- colMeans(basic.stats(snakes.reduc.gi)$Fis, na.rm=T)\ndiv2 <- data.frame(Ho=Ho, HoSD=HoSD, Hs=Hs, HsSD=HsSD, Fis=Fis)\ntots <- colMeans(div2)\ndiv2 <- rbind(div2, tots)\ndiv <- cbind(div, div2)\n#Count number of private alleles in popper\nPA <- rowSums(private_alleles(snakes.reduc.gi, count.alleles=F))\nPA <- data.frame(PA)\nPA <- rbind(PA, Total=colSums(PA))\ndiv <- cbind(div, PA)\nrow.names(div) <- NULL\nis.num <- sapply(div, is.numeric)\ndiv[is.num] <- lapply(div[is.num], round, 3)\ndiv\n\n#----------Calculate WC pairwise Fst---------------------------------------------------\nsnakes.reduc.gl <- gi2gl(snakes.reduc.gi)\ngenDist <- stamppFst(snakes.reduc.gl, nboots=1000, percent=95, nclusters=1)\nFstDist <- as.dist(genDist$Fsts)\nFstDist\n\nsnakes.reduc.hier <- genind2hierfstat(snakes.reduc.gi)\nloci <- snakes.reduc.hier[,-1]\n\nvarcomp.glob(levels=data.frame(inds$pop), loci, diploid=TRUE)\n# Calculate bootstrap CI\nFST.CI <- boot.vc(loci=loci, levels=data.frame(inds$pop), diploid=TRUE, nboot=1000)\nFST.CI$ci\n\n#---------------------------PCA-------------------------------------------------\nsnakes.reduc.gl <- gi2gl(snakes.reduc.gi)\npcoa.snakes <- gl.pcoa(snakes.reduc.gl, nfactors=5)\n\n#Save as .eps\ngl.pcoa.plot(pcoa.snakes, snakes.reduc.gl, pop.labels=""pop"", xaxis=1, yaxis=2)\n\n#Export PCs to file for use in HZAR\nsnakesPCs <- as.data.frame(pcoa.snakes$scores)\nsnakesPCs$Pop <- gsub(""_.*$"", """", rownames(snakesPCs))\nsnakesPCs$Individual <- rownames(snakesPCs)\nsnakesPCs\n']","Conspicuous coloration of toxin-resistant predators implicates additional trophic interactions in a predator-prey arms race Antagonistic coevolution between natural enemies can produce highly exaggerated traits, such as prey toxins and predator resistance. This reciprocal process of adaptation and counter-adaptation may also open doors to other evolutionary novelties not directly involved in the phenotypic interface of coevolution. We tested the hypothesis that predator-prey coevolution coincided with the evolution of conspicuous coloration on resistant predators that retain prey toxins. In western North America, common garter snakes (Thamnophis sirtalis) have evolved extreme resistance to tetrodotoxin (TTX) in the coevolutionary arms race with their deadly prey, Pacific newts (Taricha spp.). TTX-resistant snakes can retain large amounts of ingested TTX, which could serve as a deterrent against the snakes' own predators if TTX toxicity and resistance are coupled with a conspicuous warning signal. We evaluated whether arms race escalation co-varies with bright red coloration in snake populations across the geographic mosaic of coevolution. Snake color variation departs from the neutral expectations of population genetic structure and co-varies with escalating clines of newt TTX and snake resistance at two coevolutionary hotspots. In the Pacific Northwest, bright red coloration fits an expected pattern of an aposematic warning to avian predators: TTX-resistant snakes that consume highly toxic newts also have relatively large, reddish-orange dorsal blotches. Snake coloration also seems to have evolved with the arms race in California, but overall patterns are less intuitively consistent with aposematism. These results suggest that interactions with additional trophic levels can generate novel traits as a cascading consequence of arms race coevolution across the geographic mosaic.",4
"OpenBUGS code for ""all floors"" spatial temporal analysis",Code to run all floors model in OpenBUGS.Refer to publication: https://www.sciencedirect.com/science/article/pii/S0195670122000123Code also in supplementary material,"['# load libraries\r\nlibrary(R2OpenBUGS)\r\nlibrary(coda)\r\nlibrary(data.table)\r\nlibrary(matlab)\r\n\r\n\r\nmymodel <- function(){\r\n  for(i in 1:N) # rooms x floors loop\r\n  {\r\n    for (t in 1:Tmax){ # weeks loop\r\n      # ZIP model  not executed\r\n      #w[i, t] ~ dbern(psi) \r\n      #Y[i, t] ~ dpois(eff.mu[i, t]) \r\n      #eff.mu[i, t]<-w[i, t]*mu[i, t] \r\n      \r\n      Y[i, t] ~ dpois(mu[i, t]) # mean prevalence for week t in room i of floor j\r\n      \r\n      log(mu[i, t]) <- log(PAR[i, t]) # offset\r\n      +beta[floor[i]]   # intercept per floor k\r\n      +U[i] # unstructured spatial random effect, irrespective of floor\r\n      +V[floor[i]] # structured spatial random effect on floor k\r\n      +theta[t] # structured temporal effect RW(1) for *all* floors\r\n      #+delta[t] # unstructured temporal effect\r\n      \r\n      #risk ratio\r\n      RR[i, t] <- exp(#beta0\r\n        beta[floor[i]]\r\n        +U[i] \r\n        +V[i] \r\n        +theta[t]\r\n        #+delta[t]\r\n      )\r\n    } \r\n    U[i] ~ dnorm(0, tau.U) # unstructured spatial random effect for each room\r\n  } \r\n  \r\n  ## PRIORS\r\n  \r\n  # SPATIAL\r\n  for(i in 1:Nfloor)\r\n  {\r\n    V[(((i-1)*22)+1):(22*i)] ~ car.normal(adj[]\r\n                                          , weights[] # Adj: ID numbers of the rooms which are adjacent to room i\r\n                                          #\tCHECK: A room cannot be specified as its own neighbor\r\n                                          # CHECK symmetric so w_ij=w_ji\r\n                                          , num[] # vector of length N, number of neighbors of room i, i.e. count the polygons in adj for each room\r\n                                          , tauomega.V # precision parameter of the Gaussian prior\r\n    )\r\n  }\r\n  \r\n  for(k in 1:sumNumNeigh) {\r\n    weights[k] <- 1\r\n  }\r\n  \r\n  #------------------------------\r\n  # NEW: prior for floor random intercept: mean = 0\r\n  for(j in 1:Nfloor) {\r\n    beta[j]~dnorm(beta0, prec.tau2) \r\n  }\r\n  \r\n  prec.tau2~dgamma(0.001, 0.001) # for floors loop\r\n  \r\n  #---------------------\r\n  # U+V\r\n  # weakly informative prior on total tolerance T = 1 / (total variance U + V)\r\n  tau.T ~ dgamma(0.5, 0.005) # tolerance of the total random effects variance U + V\r\n  \r\n  # prior to make sure U+V=T\r\n  p ~ dbeta(1,1) \r\n  sigma.Z <- sqrt(p/tau.T) \r\n  omega.V <- sigma.Z\r\n  sigma.U <- sqrt((1-p)/tau.T)\r\n  \r\n  # definitions for tolerance\r\n  tauomega.V <- 1/(omega.V*omega.V) # in ICAR: tolerance for the structured part of the variance: square of the estimate of sd(omega)\r\n  tau.U <- 1/(sigma.U*sigma.U) \r\n  \r\n  # TEMPORAL\r\n  delta[1]<-0 # to start RW\r\n  theta[1]<-0 # always starts at 0, which isn\'t totally correct but works\r\n  \r\n  for(t in 2:Tmax){\r\n    delta[t]~dnorm(0, tau.delta)\r\n    theta[t]~dnorm(theta[t-1], tau.theta) # recursive definition of RW - why we need theta[1] defined above\r\n  }\r\n  \r\n  #--------------------------------------------------------\r\n  \r\n  #delta0 ~ dnorm(0, 0.001)\r\n  tau.delta~dgamma(0.1, 0.1)\r\n  tau.theta~dgamma(0.1, 0.1)\r\n  #tau.theta1~dgamma(0.1, 0.1)\r\n  \r\n  \r\n  # FIXED EFFECTS\r\n  beta1 ~ dnorm(0, 0.0001)\r\n  \r\n  # Functions of interest\r\n  #sigma.V <- sqrt(1/tau.V) # standard deviation of non-spatial\r\n  #RRl95 <- exp(-1.96*sigma.V)\r\n  #RRu95 <- exp(1.96*sigma.V) \r\n  # V and U variance components\r\n  mean.V<-mean(V[1:N])\r\n  sd.V <- sd(V[1:N])       \r\n  mean.U<-mean(U[1:N])\r\n  sd.U<-sd(U[1:N])\r\n  vratio <- sd.V*sd.V/(sd.V*sd.V+sigma.U*sigma.U) \r\n  mean.theta<-mean(theta[1:Tmax])\r\n  sd.theta<-sd(theta[1:Tmax])\r\n  mean.delta<-mean(delta[1:Tmax])\r\n  sd.delta<-mean(delta[1:Tmax])\r\n  #mean.eps<-mean(eps[1:N, 1:Tmax])\r\n  #sd.eps<-sd(eps[1:N, 1:Tmax])\r\n}\r\n\r\n\r\n# data\r\nmydata <-list(Tmax = nweeks # weeks\r\n              ,N   = nroom*nfloor\r\n              ,Nfloor=nfloor\r\n              ,Nroom=nroom\r\n              ,Y   = structure(.Data=    Ylong,      .Dim=c(nroom*nfloor, nweeks)) \r\n              ,PAR = structure(.Data=    PARlong,    .Dim=c(nroom*nfloor, nweeks)) \r\n              ,gender=structure(.Data=   genderlong, .Dim=c(nroom*nfloor, nweeks))\r\n              ,m_age_ind=structure(.Data= m_age_indlong,.Dim=c(nroom*nfloor, nweeks))\r\n              ,icu_ind=structure(.Data=icu_indlong, .Dim=c(nroom*nfloor, nweeks))\r\n              ,floor=floor # nested indexing in model\r\n              # ICAR\r\n              ,num = adjdata$num # number of neighbors for each area\r\n              ,adj =  adjdata$adj # adjacency matrix \r\n              ,sumNumNeigh = adjdata$sumNumNeigh # sum of num\r\n)\r\n\r\nlineinits <- function() {\r\n  list(  beta0 = 0 # fixed\r\n         #,beta1=0\r\n         #,beta2=0 \r\n         #,beta3=0 \r\n         , tau.T = 1 \r\n         , p=0.5 # split U:V starting value\r\n         , U=rep(0,nroom*nfloor) # rooms and floors\r\n         #, V=rep(0,nroom) # rooms on a floor have spatial dependence structure\r\n         #, theta=rep(0.1, 104) # time structured part of model\r\n         ,prec.tau2=0.001\r\n         #,alpha0=0 # imputation\r\n         #,tau.PAR=0.1 # imputation\r\n  )\r\n}\r\n\r\nparams <- c(\r\n  ""beta""\r\n  ,""RR""\r\n  ,""mean.U""\r\n  ,""sd.U""\r\n  ,""mean.V""\r\n  ,""sd.V""\r\n  ,""vratio""\r\n  ,""mean.theta""\r\n  ,""mean.delta""\r\n  ,""sd.delta""\r\n)\r\n\r\ntic()\r\nlineout <- bugs(data = mydata\r\n                , inits = lineinits\r\n                , parameters.to.save = params\r\n                , model.file = mymodel\r\n                , codaPkg = T\r\n                , n.chains = 1\r\n                , n.iter =  10000 # 10000\r\n                , n.burnin =4000 # 4000\r\n                , n.thin=1\r\n                , debug=T\r\n)\r\ntoc() \r\n\r\n#--------------------------------------------------------------------------------------------\r\n# summarise results\r\n# using coda\r\nline.coda <- read.bugs(lineout)\r\nsummary(line.coda)\r\n\r\n']","OpenBUGS code for ""all floors"" spatial temporal analysis Code to run all floors model in OpenBUGS.Refer to publication: https://www.sciencedirect.com/science/article/pii/S0195670122000123Code also in supplementary material",4
"Rapid Creation of a Data Product for the World's Specimens of Horseshoe Bats and Relatives, a Known Reservoir for Coronaviruses","This repository is associated with NSF DBI 2033973, RAPID Grant: Rapid Creation of a Data Product for the World's Specimens of Horseshoe Bats and Relatives, a Known Reservoir for Coronaviruses (https://www.nsf.gov/awardsearch/showAward?AWD_ID=2033973). Specifically, this repository contains (1) raw data from iDigBio (http://portal.idigbio.org) and GBIF (https://www.gbif.org), (2) R code for reproducible data wrangling and improvement, (3) protocols associated with data enhancements, and (4) enhanced versions of the dataset published at various project milestones. Additional code associated with this grant can be found in the BIOSPEX repository (https://github.com/iDigBio/Biospex). Long-term data management of the enhanced specimen data created by this project is expected to be accomplished by the natural history collections curating the physical specimens, a list of which can be found in this Zenodo resource.Grant abstract: ""The award to Florida State University will support research contributing to the development of georeferenced, vetted, and versioned data products of the world's specimens of horseshoe bats and their relatives for use by researchers studying the origins and spread of SARS-like coronaviruses, including the causative agent of COVID-19. Horseshoe bats and other closely related species are reported to be reservoirs of several SARS-like coronaviruses. Species of these bats are primarily distributed in regions where these viruses have been introduced to populations of humans. Currently, data associated with specimens of these bats are housed in natural history collections that are widely distributed both nationally and globally. Additionally, information tying these specimens to localities are mostly vague, or in many instances missing. This decreases the utility of the specimens for understanding the source, emergence, and distribution of SARS-COV-2 and similar viruses. This project will provide quality georeferenced data products through the consolidation of ancillary information linked to each bat specimen, using the extended specimen model. The resulting product will serve as a model of how data in biodiversity collections might be used to address emerging diseases of zoonotic origin. Results from the project will be disseminated widely in opensource journals, at scientific meetings, and via websites associated with the participating organizations and institutions. Support of this project provides a quality resource optimized to inform research relevant to improving our understanding of the biology and spread of SARS-CoV-2. The overall objectives are to deliver versioned data products, in formats used by the wider research and biodiversity collections communities, through an open-access repository; project protocols and code via GitHub and described in a peer-reviewed paper, and; sustained engagement with biodiversity collections throughout the project for reintegration of improved data into their local specimen data management systems improving long-term curation.This RAPID award will produce and deliver a georeferenced, vetted and consolidated data product for horseshoe bats and related species to facilitate understanding of the sources, distribution, and spread of SARS-CoV-2 and related viruses, a timely response to the ongoing global pandemic caused by SARS-CoV-2 and an important contribution to the global effort to consolidate and provide quality data that are relevant to understanding emergent and other properties the current pandemic. This RAPID award is made by the Division of Biological Infrastructure (DBI) using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.""Files included in this resource9d4b9069-48c4-4212-90d8-4dd6f4b7f2a5.zip: Raw data from iDigBio, DwC-A format0067804-200613084148143.zip: Raw data from GBIF, DwC-A format0067806-200613084148143.zip: Raw data from GBIF, DwC-A format1623690110.zip: Full export of this project's data (enhanced and raw) from BIOSPEX, CSV formatbionomia-datasets-attributions.zip: Directory containing 103 Frictionless Data packages for datasets that have attributions made containing Rhinolophids or Hipposiderids, each package also containing a CSV file for mismatches in person date of birth/death and specimen eventDate. File bionomia-datasets-attributions-key_2021-02-25.csv included in this directory provides a key between dataset identifier (how the Frictionless Data package files are named) and dataset name.bionomia-problem-dates-all-datasets_2021-02-25.csv: List of 21 Hipposiderid or Rhinolophid records whose eventDate or dateIdentified mismatches a wikidata recipients date of birth or death across all datasets.flagEventDate.txt: file containing term definition to reference in DwC-AflagExclude.txt: file containing term definition to reference in DwC-AflagGeoreference.txt: file containing term definition to reference in DwC-AflagTaxonomy.txt: file containing term definition to reference in DwC-AgeoreferencedByID.txt: file containing term definition to reference in DwC-AidentifiedByNames.txt: file containing term definition to reference in DwC-Ainstructions-to-get-people-data-from-bionomia-via-datasetKey: instructions given to data providersRAPID-code_collection-date.R: code associated with enhancing collection datesRAPID-code_compile-deduplicate.R: code associated with compiling and deduplicating raw dataRAPID-code_external-linkages-bold.R: code associated with enhancing external linkagesRAPID-code_external-linkages-genbank.R: code associated with enhancing external linkagesRAPID-code_external-linkages-standardize.R: code associated with enhancing external linkagesRAPID-code_people.R: code associated with enhancing data about peopleRAPID-code_standardize-country.R: code associated with standardizing country dataRAPID-data-dictionary.pdf: metadata about terms included in this projects data, in PDF formatRAPID-data-dictionary.xlsx: metadata about terms included in this projects data, in spreadsheet formatrapid-data-providers_2021-05-03.csv: list of data providers and number of records provided to rapid-joined-records_country-cleanup_2020-09-23.csvrapid-final-data-product_2021-06-29.zip: Enhanced data from BIOSPEX, DwC-A formatrapid-final-gazetteer.zip: Gazetteer providing georeference data and metadata for 10,341 localities assessed as part of this projectrapid-joined-records_country-cleanup_2020-09-23.csv: data product initial version where raw data has been compiled and deduplicated, and country data has been standardizedRAPID-protocol_collection-date.pdf: protocol associated with enhancing collection datesRAPID-protocol_compile-deduplicate.pdf: protocol associated with compiling and deduplicating raw dataRAPID-protocol_external-linkages.pdf: protocol associated with enhancing external linkagesRAPID-protocol_georeference.pdf: protocol associated with georeferencingRAPID-protocol_people.pdf: protocol associated with enhancing data about peopleRAPID-protocol_standardize-country.pdf: protocol associated with standardizing country dataRAPID-protocol_taxonomic-names.pdf: protocol associated with enhancing taxonomic name dataRAPIDAgentStrings1_archivedCopy_30March2021.ods: resource used in conjunction with RAPID people protocolrecordedByNames.txt: file containing term definition to reference in DwC-ARhinolophid-HipposideridAgentStrings_and_People2_archivedCopy_30March2021.ods: resource used in conjunction with RAPID people protocolwikidata-notes-for-bat-collectors_leachman_2020: please see https://zenodo.org/record/4724139 for this resource","['# RAPID code for enhancing collection dates\n# This code accompanies document \'RAPID-protocol_collection-dates.pdf\'\n# Code written by Katelin Pearson, 2021-01-19.\n# Latest update 2021-04-02.\n\n# Load core libraries; install these packages if you have not already\nlibrary(tidyverse)\nlibrary(splitstackshape)\n\n# Read into R.....\nspecs <- read.csv(""C:/Users/Katie/Desktop/agents_gbifIDs_combined_NEW.csv"")\ncols <- read.csv(""C:/Users/Katie/Desktop/Horseshoe_bat_people_combined.csv"")\n\n# PART 1\n# Join agent birth and death years, when available from Bionomia, to specimen\n# data\n\n#Convert columns to character type, because R is finicky\nspecs$recordedBy_gbifR <- as.character(specs$recordedBy_gbifR)\nspecs$identifiedBy_gbifR <- as.character(specs$identifiedBy_gbifR)\ncols$birth_year <- as.character(cols$birth_year)\ncols$death_year <- as.character(cols$death_year)\nspecs$recordedByBirthYear <- NA\nspecs$recordedByDeathYear <- NA\nspecs$identifiedByBirthYear <- NA\nspecs$identifiedByDeathYear <- NA\n\n#separate cases of multiple collectors into 4 new columns\nspecs$recordedByID_split <- specs$recordedByID\nspecs <- cSplit(specs,""recordedByID_split"",sep=""|"",type.convert = FALSE)\n\n#More character conversions\nspecs$recordedByID_split_1 <- as.character(specs$recordedByID_split_1)\nspecs$recordedByID_split_2 <- as.character(specs$recordedByID_split_2)\nspecs$recordedByID_split_3 <- as.character(specs$recordedByID_split_3)\nspecs$recordedByID_split_4 <- as.character(specs$recordedByID_split_4)\n\n#Assign birth and death years of collectors of each specimen\n#In cases of multiple collectors, the birth dates and death dates are concatenated\nfor(i in 1:dim(specs)[1]){\n  print(i)\n  if(is.na(specs$recordedByID[i])){\n    next\n  } else {\n    m <- match(specs$recordedByID_split_1[i],cols$bionomia_url)\n    if(is.na(m)){\n      next\n    }else{\n      p <- match(specs$recordedByID_split_2[i],cols$bionomia_url)\n      if(is.na(p)){\n        specs$recordedByBirthYear[i] <- as.character(cols$birth_year[m])\n        specs$recordedByDeathYear[i] <- as.character(cols$death_year[m])\n      } else {\n        q <- match(specs$recordedByID_split_3[i],cols$bionomia_url)\n        if(is.na(q)){\n          specs$recordedByBirthYear[i] <- as.character(paste(cols$birth_year[m],cols$birth_year[p],sep=""|""))\n          specs$recordedByDeathYear[i] <- as.character(paste(cols$death_year[m],cols$death_year[p],sep=""|""))\n        } else {\n          r <- match(specs$recordedByID_split_4[i],cols$bionomia_url)\n          if(is.na(r)){\n            specs$recordedByBirthYear[i] <- as.character(paste(cols$birth_year[m],cols$birth_year[p],cols$birth_year[q], sep=""|""))\n            specs$recordedByDeathYear[i] <- as.character(paste(cols$death_year[m],cols$death_year[p],cols$death_year[q], sep=""|""))\n          } else {\n            specs$recordedByBirthYear[i] <- as.character(paste(cols$birth_year[m],cols$birth_year[p],cols$birth_year[q],cols$birth_year[r], sep=""|""))\n            specs$recordedByDeathYear[i] <- as.character(paste(cols$death_year[m],cols$death_year[p],cols$death_year[q],cols$death_year[r], sep=""|""))\n          }\n        }\n      }\n    }\n  }\n}\n\n#Combine identifier birth and death years with specimen data\n#NOTE: this loop does not handle instances of multiple identifiers\nfor(i in 1:dim(specs)[1]){\n  if(is.na(specs$identifiedByID[i])){\n    next\n  } else {\n    n <- match(specs$identifiedByID[i],cols$bionomia_url)\n    #if no duplicate found, go to the next record\n    if(is.na(n)){\n      next\n    }\n    else {\n      specs$identifiedByBirthYear[i] <- cols$birth_year[n]\n      specs$identifiedByDeathYear[i] <- cols$death_year[n]\n    }\n  }\n}\n\nwrite.csv(specs, ""C:/Users/Katie/Desktop/specs_after_part2.csv"")\n\n####PART 2###############\n#In PART 2, the collection data are augmented with collection date data (this was not present\n#in the initial download, and this part could be skipped if the initial download\n#contains all the collection date information.)\n####Combine specimen date data with specimen recorded data\n#load the old dataset\nall_data <- read.csv(""C:/Users/Katie/Desktop/rapid-joined-records_country-cleanup_2020-09-23.csv"")\n#isolate only time/date data from old dataset\ntime_data <- all_data %>%\n  select(gbifID_gbifR,eventDate_gbifP,eventDate_gbifR,eventDate_idbP,eventDate_idbR,verbatimEventDate_gbifR,verbatimEventDate_gbifP,verbatimEventDate_idbP,verbatimEventDate_idbR,year_gbifP,year_gbifR,year_idbR,month_gbifR,month_gbifP,month_idbR,day_gbifP,day_gbifR,day_idbR,issue_gbifP)\n#remove unnecessary dataset (for memory)\nrm(all_data)\n\ntime_data$eventDate_gbifP <- as.character(time_data$eventDate_gbifP)\ntime_data$eventDate_gbifR <- as.character(time_data$eventDate_gbifR)\ntime_data$eventDate_idbP <- as.character(time_data$eventDate_idbP)\ntime_data$eventDate_idbR <- as.character(time_data$eventDate_idbR)\ntime_data$verbatimEventDate_gbifR <- as.character(time_data$verbatimEventDate_gbifR)\ntime_data$verbatimEventDate_gbifP <- as.character(time_data$verbatimEventDate_gbifP)\ntime_data$verbatimEventDate_idbP <- as.character(time_data$verbatimEventDate_idbP)\ntime_data$verbatimEventDate_idbR <-as.character(time_data$verbatimEventDate_idbR)\ntime_data$year_gbifP <- as.character(time_data$year_gbifP)\ntime_data$year_gbifR <- as.character(time_data$year_gbifR)\ntime_data$year_idbR <- as.character(time_data$year_idbR)\ntime_data$month_gbifR <- as.character(time_data$month_gbifR)\ntime_data$month_gbifP <- as.character(time_data$month_gbifP)\ntime_data$month_idbR <- as.character(time_data$month_idbR)\ntime_data$day_gbifP <- as.character(time_data$day_gbifP)\ntime_data$day_gbifR <- as.character(time_data$day_gbifR)\ntime_data$day_idbR <- as.character(time_data$day_idbR)\ntime_data$issue_gbifP <- as.character(time_data$issue_gbifP)\n\nspecs$eventDate_gbifP <- NA\nspecs$eventDate_gbifR <- NA\nspecs$eventDate_idbP <- NA\nspecs$eventDate_idbR <-NA\nspecs$verbatimEventDate_gbifR <- NA\nspecs$verbatimEventDate_gbifP <- NA\nspecs$verbatimEventDate_idbP <- NA\nspecs$verbatimEventDate_idbR <-NA\nspecs$year_gbifP <- NA\nspecs$year_gbifR <- NA\nspecs$year_idbR <- NA\nspecs$month_gbifR <- NA\nspecs$month_gbifP <- NA\nspecs$month_idbR <- NA\nspecs$day_gbifP <- NA \nspecs$day_gbifR <- NA\nspecs$day_idbR <- NA\nspecs$issue_gbif <- NA\n\nfor(k in 1:dim(specs)[1]){\n  print(k)\n  o <- match(specs$gbifID_gbifR[k],time_data$gbifID_gbifR)\n  #if no duplicate found, go to the next record\n  if(is.na(o)){\n    print(paste(""That\'s weird; "",specs$gbifID_gbifR[k],"" doesn\'t have a match.""))\n    next\n  }  \n  else {\n    specs$eventDate_gbifP[k] <- as.character(time_data$eventDate_gbifP[o])\n    specs$eventDate_gbifR[k] <- as.character(time_data$eventDate_gbifR[o])\n    specs$eventDate_idbP[k] <- as.character(time_data$eventDate_idbP[o])\n    specs$eventDate_idbR[k] <- as.character(time_data$eventDate_idbR[o])\n    specs$verbatimEventDate_gbifR[k] <- as.character(time_data$verbatimEventDate_gbifR[o])\n    specs$verbatimEventDate_gbifP[k] <- as.character(time_data$verbatimEventDate_gbifP[o])\n    specs$verbatimEventDate_idbP[k] <- as.character(time_data$verbatimEventDate_idbP[o])\n    specs$verbatimEventDate_idbR[k] <- as.character(time_data$verbatimEventDate_idbR[o])\n    specs$year_gbifP[k] <- as.character(time_data$year_gbifP[o])\n    specs$year_gbifR[k] <- as.character(time_data$year_gbifR[o])\n    specs$year_idbR[k] <- as.character(time_data$year_idbR[o])\n    specs$month_gbifR[k] <- as.character(time_data$month_gbifR[o])\n    specs$month_gbifP[k] <- as.character(time_data$month_gbifP[o])\n    specs$month_idbR[k] <- as.character(time_data$month_idbR[o])\n    specs$day_gbifP[k] <- as.character(time_data$day_gbifP[o])\n    specs$day_gbifR[k] <- as.character(time_data$day_gbifR[o])\n    specs$day_idbR[k] <- as.character(time_data$day_idbR[o])\n    specs$issue_gbif[k] <- as.character(time_data$issue_gbif[o])\n  }\n}\n\nwrite.csv(specs,""C:/Users/Katie/Desktop/HipRhi_combined_20200701.csv"")\n\n\n####PART 3####################\n#In PART 3, date information is coalesced into a new eventDate_rapid field. Some date parsing\n#steps are included in this step based on issues viewed in the Hipposideridae/Rhinolophidae dataset.\n#load combined dataset\nspecs <- read.csv(""C:/Users/Katie/Desktop/specs_after_part2.csv"",na.strings=c("""","" "",""NA""))\n\n#make new column to hold the RAPID-enhanced date information\nspecs$year_rapid <- NA\nspecs$eventDate_rapid <- NA\nspecs$dateSource <- NA\n\n#when eventDate exists, put it in the eventDate_rapid column\nfor(i in 1:dim(specs)[1]){\n  print(i)\n  if(!is.na(specs$eventDate_gbifP[i])){\n    specs$eventDate_rapid[i] <- as.character(specs$eventDate_gbifP[i])\n    specs$dateSource[i] <- ""eventDate_gbifP""\n  }else{\n    if(!is.na(specs$eventDate_gbifR[i])){\n      specs$eventDate_rapid[i] <- as.character(specs$eventDate_gbifR[i])\n      specs$dateSource[i] <- ""eventDate_gbifR""\n    }else{\n      if(!is.na(specs$eventDate_idbP[i])){\n        specs$eventDate_rapid[i] <- as.character(specs$eventDate_idbP[i])\n        specs$dateSource[i] <- ""eventDate_idbP""\n      }else{\n        if(!is.na(specs$eventDate_idbR[i])){\n          specs$eventDate_rapid[i] <- as.character(specs$eventDate_idbR[i])\n          specs$dateSource[i] <- ""eventDate_idbR""\n        }else{\n          if(!is.na(specs$year_idbR[i]) & !is.na(specs$month_idbR[i]) & !is.na(specs$day_idbR[i])){\n            specs$eventDate_rapid[i] <- as.character(ymd(paste(specs$year_idbR[i],""-"",specs$month_idbR[i],""-"",specs$day_idbR[i])))\n            specs$dateSource[i] <- ""ymd_idbR""\n          }\n        }\n      }\n    }\n  }\n}\n\n#make a column that assumes all dates formatted like ""29-Oct-62"" from verbatimEventDate_gbifP are in the 20th century\nspecs$interpretedVerbatimDate <- specs$verbatimEventDate_gbifP %>%\n  str_replace(""n-"",""n-19"") %>%\n  str_replace(""b-"",""b-19"") %>%\n  str_replace(""r-"",""r-19"") %>%\n  str_replace(""y-"",""y-19"") %>%\n  str_replace(""l-"",""l-19"") %>%\n  str_replace(""g-"",""g-19"") %>%\n  str_replace(""p-"",""p-19"") %>%\n  str_replace(""t-"",""t-19"") %>%\n  str_replace(""v-"",""v-19"") %>%\n  str_replace(""c-"",""c-19"")\n\n#if the collector was born after 1880, use this 20th century date\nfor(i in 1:dim(specs)[1]){\n  if(!is.na(specs$eventDate_rapid[i])){\n    next\n  }else{\n    if(!is.na(specs$recordedByBirthYear[i])){\n      if(as.numeric(specs$recordedByBirthYear[i]) > 1880){\n        specs$eventDate_rapid[i] <- as.character(as.Date(specs$interpretedVerbatimDate[i],format=""%d-%b-%Y""))\n      }\n    }\n  }\n}\n\nspecs$needsReview <- NA\n\n#make date flags\nfor(i in 1:dim(specs)[1]){\n  if(!is.na(specs$eventDate_rapid[i])){\n    next\n  } else {\n    #if there is no date or collector information, don\'t even try\n    if(is.na(specs$eventDate_gbifP[i]) & is.na(specs$eventDate_gbifR[i]) & is.na(specs$eventDate_idbP[i]) & is.na(specs$eventDate_idbR[i]) & is.na(specs$verbatimEventDate_gbifR[i]) & is.na(specs$verbatimEventDate_gbifP[i]) & is.na(specs$verbatimEventDate_idbP[i]) & is.na(specs$verbatimEventDate_idbR[i]) & is.na(specs$year_gbifP[i]) & is.na(specs$year_gbifR[i]) & is.na(specs$year_idbR[i]) & is.na(specs$recordedBy_gbifP[i]) & is.na(specs$recordedBy_gbifR[i]) & is.na(specs$recordedBy_idbP[i]) & is.na(specs$recordedBy_idbR[i])){\n      specs$needsReview[i] <- ""abandonHope""\n    } else {\n      #if there is ONLY collector information, flag this for programmatic assignment of collector life dates\n      if(!is.na(specs$recordedByID[i]) & is.na(specs$eventDate_gbifP[i]) & is.na(specs$eventDate_gbifR[i]) & is.na(specs$eventDate_idbP[i]) & is.na(specs$eventDate_idbR[i]) & is.na(specs$verbatimEventDate_gbifR[i]) & is.na(specs$verbatimEventDate_gbifP[i]) & is.na(specs$verbatimEventDate_idbP[i]) & is.na(specs$verbatimEventDate_idbR[i]) & is.na(specs$year_gbifP[i]) & is.na(specs$year_gbifR[i]) & is.na(specs$year_idbR[i])){\n        specs$needsReview[i] <- ""collectorOnly""\n      } else {\n        specs$needsReview[i] <- ""forHumanReview""\n      }\n    }\n  }\n}\n\n#To get an idea of how big this dataset would be:\nno_dates <- specs %>%\n  filter(needsReview == ""forHumanReview"")\n\nwrite.csv(specs,""C:/Users/Katie/Desktop/HipRhi_coalesced_20200107.csv"")\n\n####PART 4###############\n#Change the dates of collection of disambiguiated collectors\n#This step was conducted after human review of the ""forHumanReview"" specimens\n#Note that this does not appropriately assign collection dates for living collectors\n#because they don\'t have birth and death dates in Bionomia.\n\n#In PART 4, eventDate_rapid is populated with the range of the collector\'s birth and death years,\n#drawn from the attributed recordedByID in Bionomia.\nthedat <- read.csv(""C:/Users/Katie/Documents/FSU_RAPID/HipRhi_coalesced_20200107_kdp.csv"")\n\nthedat$eventDate_rapid <- as.character(thedat$eventDate_rapid)\nthedat$notes <- as.character(thedat$notes)\n\nfor(i in 1:dim(thedat)[1]){\n  if(is.na(thedat$needsReview[i])){\n    next\n  } else {\n    if(thedat$needsReview[i]==""collectorOnly""){\n      thedat$eventDate_rapid[i] <- paste(thedat$recordedByBirthYear[i],""-00-00 / "",thedat$recordedByDeathYear[i],""-00-00"",sep="""")\n      thedat$notes[i] <- ""collector birth and death years""\n      }\n  }\n  print(i)\n}\n  \nwrite.csv(thedat, ""C:/Users/Katie/Documents/FSU_RAPID/HipRhi_coalesced_20200119.csv"")', '# RAPID code for compiling raw data from multiple sources and deduplicating records\n# This code accompanies document \'RAPID-protocol_compile-deduplicate.pdf\'\n# Code written by Erica Krimmel, 2020-07-31. Updated 2020-09-23.\n\n# Load core libraries; install this package if you have not already\nlibrary(tidyverse)\n\n# Load data GBIF R package for data quality control further down; install\n# this package if you have not already\nlibrary(rgbif)\n\n# PREPARE DATA FROM IDIGBIO\n\n# Read into R the raw occurrence data from iDigBio, which should be whatever was\n# published by the data provider (e.g. the collection)\nidb_raw <- read_csv(""9d4b9069-48c4-4212-90d8-4dd6f4b7f2a5/occurrence_raw.csv"", \n                    na = c("""", ""NA""),\n                    col_types = cols(.default = col_character()))\n\n# Rename columns to reflect provenance and remove colon characters\nidb_raw <- idb_raw %>% \n  rename_all(function(x){paste0(x, ""_idbR"")}) %>% \n  rename_all(funs(str_replace_all(., ""dwc:"", """"))) %>% \n  rename_all(funs(str_replace_all(., "":"", ""_"")))\n\n# Read into R the version of occurrence data processed by iDigBio\nidb_processed <- read_csv(""9d4b9069-48c4-4212-90d8-4dd6f4b7f2a5/occurrence.csv"", \n                          na = c("""", ""NA""),\n                          col_types = cols(.default = col_character()))\n\n# Rename columns to reflect provenance and remove illegal characters\nidb_processed <- idb_processed %>% \n  rename_all(function(x){paste0(x, ""_idbP"")}) %>% \n  rename_all(funs(str_replace_all(., ""dwc:"", """"))) %>% \n  rename_all(funs(str_replace_all(., "":"", ""_"")))\n\n# Join raw and processed iDigBio data together\nidb_joined <- idb_raw %>% \n  left_join(idb_processed, by = c(""coreid_idbR"" = ""coreid_idbP"")) %>% \n  # Subset by families of interest\n  mutate(family_idbR = tolower(family_idbR)) %>% \n  filter(family_idbR %in% c(""rhinolophidae"", \n                            ""hipposideridae"", \n                            ""rhinonycteridae"") |\n           family_idbP %in% c(""rhinolophidae"", \n                              ""hipposideridae"", \n                              ""rhinonycteridae"")) %>% \n  # Expand the idbP field `geopoint` into two fields to reflect the same\n  # structure as the other datasets here\n  separate(idigbio_geoPoint_idbP, \n           c(""decimalLatitude_idbP"", ""decimalLongitude_idbP""), \n           sep = "","") %>% \n  mutate(decimalLatitude_idbP = parse_number(decimalLatitude_idbP)) %>% \n  mutate(decimalLongitude_idbP = parse_number(decimalLongitude_idbP)) %>% \n  # Create a new column to identify duplicate records between iDigBio and GBIF\n  # in future steps\n  unite(matchDuplicates, \n        c(""institutionCode_idbR"", \n          ""collectionCode_idbR"", \n          ""catalogNumber_idbR"",\n          ""occurrenceID_idbR""), \n        sep = "" "", \n        remove = FALSE) %>% \n  # Normalize text differences by lowercasing\n  mutate(matchDuplicates = str_squish(tolower(matchDuplicates)))\n\n# PREPARE DATA FROM GBIF\n\n# Read into R the raw occurrence data from GBIF, which should be whatever was\n# published by the data provider (e.g. the collection); this should ostensibly\n# be the same as what is in `idb_raw`\ngbif_subset_1R <- read_tsv(""0067804-200613084148143/verbatim.txt"", \n                           na = c("""", ""NA""),\n                           col_types = cols(.default = col_character()))\n\n# Rename columns to reflect provenance\ngbif_subset_1R <- gbif_subset_1R %>% \n  rename_all(function(x){paste0(x, ""_gbifR"")})\n\n# Read into R the version of occurrence data processed by GBIF\ngbif_subset_1P <- read_tsv(""0067804-200613084148143/occurrence.txt"", \n                           na = c("""", ""NA""),\n                           col_types = cols(acceptedTaxonKey = col_integer(),\n                                            classKey = col_integer(),\n                                            familyKey = col_integer(),\n                                            genusKey = col_integer(),\n                                            kingdomKey = col_integer(),\n                                            orderKey = col_integer(),\n                                            phylumKey = col_integer(),\n                                            speciesKey = col_integer(),\n                                            .default = col_character()))\n\n# Rename columns to reflect provenance\ngbif_subset_1P <- gbif_subset_1P %>% \n  rename_all(function(x){paste0(x, ""_gbifP"")})\n\n# Join raw and processed GBIF subset data together\ngbif_subset_1 <- gbif_subset_1R %>% \n  left_join(gbif_subset_1P, by = c(""gbifID_gbifR"" = ""gbifID_gbifP"")) %>% \n  # Get rid of these columns because I can\'t figure out how to coerce them into\n  # the same data class to later join GBIF subsets\n  select(-organismQuantity_gbifR, -organismQuantity_gbifP)\n\n# Read into R the raw occurrence data from GBIF, which should be whatever was\n# published by the data provider (e.g. the collection); this should ostensibly\n# be the same as what is in `idb_raw`\ngbif_subset_2R <- read_tsv(""0067806-200613084148143/verbatim.txt"", \n                           #  na = c("""", ""NA""),\n                           col_types = cols(.default = col_character()))\n\n# Rename columns to reflect provenance\ngbif_subset_2R <- gbif_subset_2R %>% \n  rename_all(function(x){paste0(x, ""_gbifR"")})\n\n# Read into R the version of occurrence data processed by GBIF\ngbif_subset_2P <- read_tsv(""0067806-200613084148143/occurrence.txt"", \n                           # na = c("""", ""NA""),\n                           col_types = cols(acceptedTaxonKey = col_integer(),\n                                            classKey = col_integer(),\n                                            familyKey = col_integer(),\n                                            genusKey = col_integer(),\n                                            kingdomKey = col_integer(),\n                                            orderKey = col_integer(),\n                                            phylumKey = col_integer(),\n                                            speciesKey = col_integer(),\n                                            .default = col_character()))\n\n# Rename columns to reflect provenance\ngbif_subset_2P <- gbif_subset_2P %>% \n  rename_all(function(x){paste0(x, ""_gbifP"")})\n\n# Join raw and processed GBIF subset data together\ngbif_subset_2 <- gbif_subset_2R %>% \n  left_join(gbif_subset_2P, by = c(""gbifID_gbifR"" = ""gbifID_gbifP"")) %>% \n  # Get rid of these columns because I can\'t figure out how to coerce them into\n  # the same data class to later join GBIF subsets\n  select(-organismQuantity_gbifR, -organismQuantity_gbifP)\n\n# Bind `gbif_subset_1` and `gbif_subset_2` together\ngbif_joined <- bind_rows(list(gbif_subset_1, gbif_subset_2), .id = ""id"") %>%\n  # Create a new column to identify duplicate records between iDigBio and GBIF\n  # in future steps\n  unite(matchDuplicates, \n        c(""institutionCode_gbifR"", \n          ""collectionCode_gbifR"", \n          ""catalogNumber_gbifR"",\n          ""occurrenceID_gbifR""), \n        sep = "" "", \n        remove = FALSE) %>%\n  # Normalize text differences by lowercasing\n  mutate(matchDuplicates = str_squish(tolower(matchDuplicates)))\n\n# Define function that will fetch publisher information for a GBIF dataset\ngetPublisher_gbif <- function(datasetKey) {\n  publisher <- rgbif::datasets(uuid = datasetKey)\n  publisher <- paste(publisher$data$publishingOrganizationKey,\n                     publisher$data$title,\n                     publisher$data$type,\n                     sep = ""xxxxx"")\n  publisher\n}\n\n# Compile list of all datasets and publishers contributing GBIF records\ngbif_datasets <- gbif_joined %>% \n  select(datasetKey_gbifP) %>% \n  distinct() %>% \n  mutate(publisher = map_chr(datasetKey_gbifP, getPublisher_gbif)) %>% \n  separate(publisher, \n           into = c(""publisherKey_gbifP"", \n                    ""publisherTitle_gbifP"",\n                    ""publisherType_gbifP""),\n           sep = ""xxxxx"")\n\n# Add publisher information to `records` data so that we can exclude checklist\n# datasets, which are are linked directly to physical specimens\ngbif_joined <- gbif_joined %>% \n  left_join(gbif_datasets, by = ""datasetKey_gbifP"") %>% \n  filter(publisherType_gbifP != ""CHECKLIST"")\n\n# JOIN IDIGBIO AND GBIF DATA TOGETHER\n\n# Join `idb_joined` and `gbif_joined` based on unique combinations of\n# institution code, collection code, catalog number, and occurrence ID\nrecords <- gbif_joined %>% \n  full_join(idb_joined, by = ""matchDuplicates"") %>%\n  # Record which records were present in iDigBio data\n  mutate(idigbio = case_when(!is.na(idigbio_uuid_idbP) ~ 1,\n                             is.na(idigbio_uuid_idbP) ~ 0)) %>% \n  # Record which records were present in GBIF data\n  mutate(gbif = case_when(!is.na(gbifID_gbifR) ~ 1,\n                          is.na(gbifID_gbifR) ~ 0)) %>% \n  # Rearrange columns by provenance, unique IDs, and then alphabetically\n  select(gbif, idigbio, gbifID_gbifR, idigbio_uuid_idbP, \n         sort(tidyselect::peek_vars()),\n         # Remove column used to match duplicate records between iDigBio and GBIF\n         -starts_with(""matchDuplicates""))\n\n# Save `records` as CSV file\nwrite_csv(records, \n          paste(""rapid-joined-records_"", Sys.Date(), "".csv"", sep = """"),\n          na = """")', '##############\r\n###Title: RAPID Code to Find and Link Barcode of Life External Linkages\r\n###Author: Katelin D. Pearson\r\n###Date: February 12, 2021\r\n#############\r\n\r\nlibrary(DataCombine)\r\n\r\n#load in all Hipposideridae and Rhinolophidae sequences from BOLD\r\nboldbats <- read.csv(""C:/Users/Katie/Documents/FSU_RAPID/bold_hippo_rhino.csv"")\r\n\r\n#exclude records that were already mined from NCBI\r\nboldbats <- subset(boldbats,boldbats$institutionCode!=""Mined from GenBank, NCBI"")\r\n\r\n#standardize the ROM and MZB catalog numbers\r\n#no other catalog numbers were identified as needing standardization\r\ntofrom <- matrix(ncol = 2, nrow = 2)\r\ntofrom <- as.data.frame(tofrom)\r\ncolnames(tofrom) <- c(""from"",""to"")\r\ntofrom$from[1] <- ""ROM:MAM:""\r\ntofrom$to[1] <- ""ROM ""\r\ntofrom$from[2] <- ""MZB_""\r\ntofrom$to[2] <- ""MZB ""\r\nboldbats <- FindReplace(boldbats, ""catalogNumber"", tofrom, from = ""from"", to = ""to"", exact = FALSE)\r\nboldbats$catalogNumber <- as.character(boldbats$catalogNumber)\r\n\r\n#need to add ROM when belongs to Royal Ontario Museum (some were missing acronym in catalog number)\r\nfor(i in 1:dim(boldbats)[1]){\r\n  if(grepl(""Royal Ontario"",boldbats$institutionCode[i])){\r\n    if(is.na(boldbats$catalogNumber[i])){\r\n      next\r\n    } else {\r\n      if(grepl(""ROM"",boldbats$catalogNumber[i])==FALSE){\r\n        boldbats$catalogNumber[i] <- paste(""ROM"",boldbats$catalogNumber[i])\r\n      }\r\n    }\r\n  } else {\r\n    next\r\n  }\r\n}\r\n\r\n#load the previous specimen data\r\nspecs <- read.csv(""C:/Users/Katie/Documents/FSU_RAPID/FinalParsedFiles/final_standardized_assocSeq.csv"")\r\n\r\n#add a flag for specimens that already have sequences from NCBI\r\n#these are expected to be dupliates, not new sequences\r\nspecs$inBOLD <- NA\r\nspecs$associatedSequences_rapid <- as.character(specs$associatedSequences_rapid)\r\n\r\n#find BOLD sequences corresponding to the specimens in our dataset\r\nfor(i in 1:dim(specs)[1]){\r\n  matches <- which(boldbats$catalogNumber %in% specs$unifiedCatNum[i])\r\n  if(length(matches)==0){\r\n    next\r\n  } else {\r\n    if(!is.na(specs$associatedSequences_rapid[i])){\r\n     specs$inBOLD[i] <- TRUE\r\n    } else {\r\n      specs$associatedSequences_rapid[i] <- paste(paste(""http://www.barcodinglife.org/index.php/Public_RecordView?processid="",as.character(boldbats$occurrenceID[matches]),sep=""""), collapse = ""|"")\r\n    }\r\n  }\r\n}\r\n\r\n#how many specimens were from BOLD?\r\nboldIDs <- subset(specs,grepl(""barcoding"",specs$associatedSequences_rapid))\r\n\r\n#how many specimens were both in NCBI and BOLD?\r\nboldAndNCBI <- subset(specs, specs$inBOLD==TRUE)\r\n\r\n#output data file\r\nwrite.csv(specs,""C:/Users/Katie/Documents/FSU_RAPID/associatedSequences_after_bold.csv"")\r\n', '##############\r\n###Title: RAPID Code to Find GenBank Sequences\r\n###Author: Katelin D. Pearson\r\n###Date: February 4, 2021\r\n#############\r\n\r\nlibrary(splitstackshape)\r\nlibrary(tidyverse)\r\nlibrary(DataCombine)\r\n\r\n##Part 1: Extract Catalog Numbers from GenBank Results\r\n\r\n#Read in dataset (a gff3 file converted to csv from NCBI Nuccore)\r\nsample <- read_csv(""PATH"",\r\n  col_types = cols(\r\n  text = col_character()\r\n))\r\n\r\n#Make a dataframe to hold each line that contains a catalog number\r\npositives <- matrix(NA,nrow=100000)\r\npositives <- as.data.frame(positives)\r\ncolnames(positives) <- c(""text"")\r\n\r\n#Go through the dataset and extract only lines with catalog numbers\r\n#In GenBank, these are values in the ""specimen-voucher"" field\r\nk = 1\r\n#ptm <- proc.time() #for if you want to time the loop\r\nfor(i in 1:dim(sample)[1]){\r\n  if(sample$text[i]==""""){\r\n    next\r\n  } else {\r\n    if(is.na(sample$text[i])){\r\n      next\r\n    } else {\r\n      if(grepl(""specimen-voucher="",sample$text[i])){\r\n        positives$text[k] <- as.character(sample$text[i])\r\n        k <- k + 1\r\n      }\r\n    }\r\n  }\r\n}\r\n#ptm - proc.time() #for if you want to time the loop\r\n\r\n#Remove any blank rows\r\npositives <- positives %>% filter_all(any_vars(!is.na(.)))\r\n\r\n#Split the results into two columns\r\npositives <- cSplit(positives, ""text"", sep = ""+"")\r\n\r\n#Because the first column always has the GenBankID at the beginning followed by a tab\r\n#We can extract this value using cSplit and put it into its own column\r\npositives <- cSplit(positives, ""text_1"", sep = ""/t"")\r\ncolnames(positives)[2] <- ""GenBankID""\r\n\r\n#Make a new catalogNumber field\r\npositives$catalogNumber <- NA\r\n\r\n#Extract the catalogNumbers from each specimen record\r\nfor(i in 1:dim(positives)[1]){\r\n  cn <- sub("".*specimen-voucher="","""", positives$text_2[i])\r\n  positives$catalogNumber[i] <- cn\r\n}\r\n\r\n#Remove garbage on the other side of the catalog number\r\npositives <- cSplit(positives,""catalogNumber"", sep = "";"")\r\n\r\n#Make a cleaned dataframe of only catalog number and GenBankID\r\ncleaned <- cbind(catalogNumber = as.character(positives$catalogNumber_1), GenBankID = as.character(positives$GenBankID))\r\ncleaned <- as.data.frame(cleaned)\r\n\r\n#standardize ROM catalog numbers that say ROM MAM or ROM: into just ROM\r\ntofrom <- matrix(ncol = 2, nrow = 1)\r\ntofrom <- as.data.frame(tofrom)\r\ncolnames(tofrom) <- c(""from"",""to"")\r\ntofrom$from[1] <- ""ROM MAM""\r\ntofrom$to[1] <- ""ROM""\r\ncleaned <- FindReplace(cleaned, ""catalogNumber"", tofrom, from = ""from"", to = ""to"", exact = FALSE)\r\ntofrom$from[1] <- ""ROM:""\r\ntofrom$to[1] <- ""ROM ""\r\ncleaned <- FindReplace(cleaned, ""catalogNumber"", tofrom, from = ""from"", to = ""to"", exact = FALSE)\r\n\r\n#standardize Western Australian Museum records\r\nfor(i in 1:dim(cleaned)[1]){\r\n  if(grepl(""Western Australian"", cleaned$catalogNumber[i])){\r\n    this1 <- sub("" Western Australian Museum.*"", """", as.character(cleaned$catalogNumber[i]))\r\n    this2 <- paste(""WAM "",this1)\r\n    cleaned$catalogNumber[i] <- this2\r\n  } else {\r\n    next\r\n  }\r\n}\r\n\r\nwrite.csv(cleaned, ""PATH"")\r\n\r\n##Part 2: Unify Catalog Number in Specimen Dataset\r\n#The GenBank specimen voucher numbers are generally the institution code followed by the number\r\n#Often the catalog number in the specimen dataset is only the catalog number\r\n#We need to make a catalog number field that matches the GenBank format for matching purposes\r\n\r\n#load GenBank dataset if need be \r\n#cleaned <- read.csv(""PATH"")\r\n\r\nspecs <- read.csv(""PATH"")\r\n\r\nspecs$unifiedCatNum <- paste(as.character(specs$institutionCode_gbifP),as.character(specs$catalogNumber_gbifP),sep = "" "")\r\n\r\n#look at what else needs to be cleaned or standardized:\r\n\r\n#look at values in the catalogNumber field for the GenBank data\r\n# table(substr(as.character(cleaned$catalogNumber), 1, 4))\r\n#look at values in the unifiedCatNum field for the specimen data\r\n# table(substr(specs$institutionCode_gbifP, 1, 4))\r\n\r\n##Part 3: Compare Catalog Number in Specimen Dataset to Catalog Numbers for GenBank sequences\r\n\r\n#only uncomment the first time you are making this column\r\n# specs$associatedSequences_rapid <- NA\r\n\r\nfor(i in 1:dim(specs)[1]){\r\n  matches <- which(cleaned$catalogNumber %in% specs$unifiedCatNum[i])\r\n  if(length(matches)==0){\r\n    next\r\n  } else {\r\n    specs$associatedSequences_rapid[i] <- paste(paste(""http://www.ncbi.nlm.nih.gov/nuccore/"",as.character(cleaned$GenBankID[matches]),sep=""""), collapse = ""|"")\r\n  }\r\n}\r\n\r\nView(specs$associatedSequences_rapid)\r\nt <- table(specs$associatedSequences_rapid)\r\nView(t)\r\n\r\nwrite.csv(specs, ""PATH"")', '##############\n###Title: RAPID Code to Standardize External Linkages\n###Author: Katelin D. Pearson\n###Date: February 4, 2021\n#############\n\nlibrary(splitstackshape)\n\n#load the dataset of specimen records\n#the default associatedSequences field in our dataset is named associatedSequences_gbifP\nbats <- read.csv(""PATH"")\n\n#make a subset of the dataset for which the associatedSequences field is not empty\n#(to get a simple count)\nassocSeq <- subset(bats, bats$associatedSequences_gbifP!="""")\n\n#make a new column for the re-formatted sequences\n#DO NOT RUN THIS if you have already run the GenBank-Specimen Linkage code on your dataset\n#bats$associatedSequences_rapid <- NA\n\nbats$associatedSequences_rapid <- as.character(bats$associatedSequences_rapid)\n\n#populate the new (or previously existing) column\n#we are using the NCBI URL as recommended by the Darwin Core\nfor(i in 1:dim(bats)[1]){\n  if(bats$associatedSequences_gbifP[i]==""""){\n    next\n  } else {\n    if(is.na(bats$associatedSequences_gbifP[i])){\n      next\n    } else {\n      #transfer over the sequences that are already correctly formatted with the nuccure URL\n      if(grepl(""nuccore"",as.character(bats$associatedSequences_gbifP[i]))){\n        bats$associatedSequences_rapid[i] <- as.character(bats$associatedSequences_gbifP[i])\n      } else {\n        #reformat the records that are only an NCBI number\n        if(nchar(as.character(bats$associatedSequences_gbifP[i]))<10){\n          bats$associatedSequences_rapid[i] <- as.character(paste(""http://www.ncbi.nlm.nih.gov/nuccore/"",as.character(bats$associatedSequences_gbifP[i]),sep=""""))\n        }\n      }\n    }\n  }\n}\n\n#make sure it works\nt <- table(bats$associatedSequences_rapid)\nt <- as.data.frame(t)\n\n#now to deal with columns that have two sequences\n#preserve the original column\nbats$associatedSequences_gbifP_orig <- bats$associatedSequences_gbifP\n\n#split the column into two columns, assuming the sequences are separated by a |\nbats <- cSplit(bats,""associatedSequences_gbifP"",sep=""|"",type.convert = FALSE)\n\n#isolate each incorrect URL and apply the correct URL, then join them together\n#into the associatedSequences_rapid column\nfor(i in 1:dim(bats)[1]){\n  if(!is.na(bats$associatedSequences_gbifP_2[i])){\n    first <- sub("".*term="", """", as.character(bats$associatedSequences_gbifP_1[i]))\n    second <- sub("".*term="", """", as.character(bats$associatedSequences_gbifP_2[i]))\n    bats$associatedSequences_rapid[i] <- paste(paste(""http://www.ncbi.nlm.nih.gov/nuccore/"",first, sep = """"), paste(""http://www.ncbi.nlm.nih.gov/nuccore/"",second,sep = """"),sep = ""|"")\n    print(i)\n  }\n}\n\n#remove erroneous associatedSequences\nfor(i in 1:dim(bats)[1]){\n  if(is.na(bats$associatedSequences_rapid[i])){\n    next\n  } else {\n    if(nchar(as.character(bats$associatedSequences_rapid[i]))<10){\n      bats$associatedSequences_rapid[i] <- NA\n    }\n  }\n}', '# RAPID code for fetching data from Bionomia to reintegrate with data in BIOSPEX\n# This code accompanies document \'RAPID-protocol_people.pdf\'\n# Code written by Katelin Pearson, 2020-12-16. Updated by Erica Krimmel on\n# 2021-02-02.\n\n# Load core libraries; install these packages if you have not already\nlibrary(tidyverse)\nlibrary(jsonlite)\n\n# Read into R the data exported from Bionomia and specify column types (doing\n# this is recommended when dataset is large and columns may be sparsely\n# populated). Change filename for reuse.\nspecs <- read_csv(""biospex_people_2020-12-15.csv"",\n                  col_types = cols(\'_id\' = col_character(),\n                                   gbifID = col_double(),\n                                   recordedBy = col_character(),\n                                   recordedByID = col_character(),\n                                   identifiedBy = col_character(),\n                                   identifiedByID = col_character()))\n\n# Modify `specs` for data enhancement process\nspecs <- specs %>% \n  # Create column to use when fetching data from Bionomia\n  mutate(gbifIDurl = paste0(""https://bionomia.net/occurrence/"",\n                            specs$gbifID,\n                            "".json"")) %>% \n  # Create columns for the enhanced people data from Bionomia\n  mutate(recordedBy_rapid = NA) %>% \n  mutate(recordedByID_rapid = NA) %>% \n  mutate(identifiedBy_rapid = NA) %>% \n  mutate(identifiedByID_rapid = NA)\n\n# Define function to apply a NULL value to any JSON queries returning an error\nfromJSON_possibly <- possibly(fromJSON,\n                              otherwise = NULL)\n\n# Loop through records in \'spec\' to fetch person data from Bionomia\nfor(i in 1:dim(specs)[1]){\n  print(i)\n  # Skip record if it lacks a value for `gbifID`\n  if(is.na(specs$gbifID[i])){\n    next\n  }\n  # Use record to fetch data from Bionomia\n  thisSpec <- fromJSON_possibly(specs$gbifIDurl[i],\n                                simplifyDataFrame = TRUE,\n                                flatten = TRUE)\n  # Skip record if it lacks Bionomia data\n  if(is.null(thisSpec)){\n    next\n  }else{\n    # Check for collector person data from Bionomia\n    if(length(thisSpec$recorded) > 0){\n      # Create table of collector person data\n      recby <- as_tibble(thisSpec$recorded)\n      \n      # Check for multiple collectors. If there are multiple collectors,\n      # concatenate values that will be transferred to `recordedBy_rapid` and\n      # `recordedByID_rapid`\n      if(dim(recby)[1]>1){\n        # Make placeholders for concatenated values\n        allids <- c()\n        allnam <- c()\n        # Concatenate values\n        for(k in 1:dim(recby)[1]){\n          allids <- paste(allids,\n                          recby$`@id`[k],\n                          sep="" | "")\n          allnam <- paste(allnam,\n                          paste0(recby$givenName[k],\n                                "" "",\n                                recby$familyName[k]),\n                          sep="" | "")\n        }\n        \n        # Remove extra pipes from strings\n        allids <- sub(\' \\\\| \', \'\', allids)\n        allnam <- sub(\' \\\\| \', \'\', allnam)\n        \n        # Put concatenated values for `id` into `recordedByID_rapid`\n        specs$recordedByID_rapid[i] <- allids\n       \n        # Put concatenated values for `recorded` into `recordedBy_rapid`\n        specs$recordedBy_rapid[i] <- allnam\n        \n      # If there is a single collectors, transfer values directly into\n      # `recordedBy_rapid` and `recordedByID_rapid`\n      }else{\n        # Put value for `id` in `recordedByID_rapid`\n        specs$recordedByID_rapid[i] <- recby$`@id`\n        # Put value for `recorded` into `recordedBy_rapid`\n        specs$recordedBy_rapid[i] <- paste0(recby$givenName,\n                                           "" "",\n                                           recby$familyName)\n      }\n    }\n    \n    # Check for identifier person data from Bionomia\n    if(length(thisSpec$identified) > 0){\n      # Create table of identifier person data\n      idby <- as_tibble(thisSpec$identified)\n      \n      # Check for multiple identifiers. If there are multiple identifiers,\n      # concatenate values that will be transferred to `identifiedBy_rapid` and\n      # `identifiedByID_rapid`\n      if(dim(idby)[1]>1){\n        # Make placeholders for concatenated values\n        allids <- c()\n        allnam <- c()\n        # Concatenate values\n        for(k in 1:dim(idby)[1]){\n          allids <- paste(allids,\n                          idby$`@id`[k],\n                          sep="" | "")\n          allnam <- paste(allnam,\n                          paste0(idby$givenName[k],\n                                "" "",\n                                idby$familyName[k]),\n                          sep="" | "")\n        }\n\n        # Remove the extra pipes from strings\n        allids <- sub(\' \\\\| \', \'\', allids)\n        allnam <- sub(\' \\\\| \', \'\', allnam)\n        \n        # Put concatenated values for `id` into `identifiedByID_rapid`\n        specs$identifiedByID_rapid[i] <- allids\n        \n        # Put concatenated values for `recorded` into `identifiedBy_rapid`\n        specs$identifiedBy_rapid[i] <- allnam\n      }else{\n        # Put value for `id` in `identifiedByID_rapid`\n        specs$identifiedByID_rapid[i] <- idby$`@id`\n        # Put value for `recorded` in `identifiedBy_rapid`\n        specs$identifiedBy_rapid[i] <- paste0(idby$givenName,\n                                              "" "",\n                                              idby$familyName)\n      }\n    }\n  }\n}\n\n# Save data as a CSV file to import back into BIOSPEX\nwrite_csv(specs,\n          paste0(""RAPID-people_"",Sys.Date(),"".csv""),\n          na = """")\n', '# RAPID code for standardizing country data\n# This code accompanies document \'RAPID-protocol_standardize-country.pdf\'\n# Code written by Erica Krimmel, 2020-07-31. Updated 2020-09-23.\n\n# Load core libraries; install this package if you have not already\nlibrary(tidyverse)\n\n# Read into R the primary records dataset\nrecords <- read_csv(""rapid-joined-records_2020-09-23.csv"", \n                    na = c("""", ""NA""),\n                    col_types = cols(.default = col_character()))\n\n# Extract distinct values for country information\ncountryCleanup <- records %>% \n  select(starts_with(""country""), idigbio_isoCountryCode_idbP) %>% \n  distinct() %>% \n  unite(countryKey, everything(), remove = FALSE) %>% \n  mutate_at(vars(starts_with(""country_"")), tolower) %>% \n  mutate_at(vars(contains(""countryCode"")), toupper) %>% \n  mutate(country_rapid = coalesce(country_idbP,\n                                  country_gbifR,\n                                  country_idbR)) %>% \n  mutate(countryCode_rapid = coalesce(countryCode_gbifP,\n                                      idigbio_isoCountryCode_idbP,\n                                      countryCode_gbifR,\n                                      countryCode_idbR))\n\n# Save `countryCleanup` as file to do cleanup work in OpenRefine\nwrite_csv(countryCleanup,\n          paste(""rapid-country-cleanup_"", Sys.Date(), "".csv"", sep = """"),\n          na = """")\n\n# Read in cleaned up country data file saved from OpenRefine\ncountryCleanup_done <- read_csv(""rapid-country-cleanup_2020-09-23.csv"",\n                                na = c("""", ""NA""),\n                                col_types = cols(.default = col_character()))\n\n# Join cleaned up country data back into `records`\nrecords_countryCleanup <- records %>% \n  unite(countryKey,\n        country_gbifR,\n        country_idbP,\n        country_idbR,\n        countryCode_gbifP,\n        countryCode_gbifR,\n        countryCode_idbR,\n        idigbio_isoCountryCode_idbP,\n        remove = FALSE) %>% \n  left_join(select(countryCleanup_done, \n                   countryKey, country_rapid, countryCode_rapid),\n            by = ""countryKey"") %>% \n  select(-countryKey)\n\n# Save `records` as CSV file\nwrite_csv(records_countryCleanup, \n          paste(""rapid-joined-records_country-cleanup_"", Sys.Date(), "".csv"", sep = """"),\n          na = """")']","Rapid Creation of a Data Product for the World's Specimens of Horseshoe Bats and Relatives, a Known Reservoir for Coronaviruses This repository is associated with NSF DBI 2033973, RAPID Grant: Rapid Creation of a Data Product for the World's Specimens of Horseshoe Bats and Relatives, a Known Reservoir for Coronaviruses (https://www.nsf.gov/awardsearch/showAward?AWD_ID=2033973). Specifically, this repository contains (1) raw data from iDigBio (http://portal.idigbio.org) and GBIF (https://www.gbif.org), (2) R code for reproducible data wrangling and improvement, (3) protocols associated with data enhancements, and (4) enhanced versions of the dataset published at various project milestones. Additional code associated with this grant can be found in the BIOSPEX repository (https://github.com/iDigBio/Biospex). Long-term data management of the enhanced specimen data created by this project is expected to be accomplished by the natural history collections curating the physical specimens, a list of which can be found in this Zenodo resource.Grant abstract: ""The award to Florida State University will support research contributing to the development of georeferenced, vetted, and versioned data products of the world's specimens of horseshoe bats and their relatives for use by researchers studying the origins and spread of SARS-like coronaviruses, including the causative agent of COVID-19. Horseshoe bats and other closely related species are reported to be reservoirs of several SARS-like coronaviruses. Species of these bats are primarily distributed in regions where these viruses have been introduced to populations of humans. Currently, data associated with specimens of these bats are housed in natural history collections that are widely distributed both nationally and globally. Additionally, information tying these specimens to localities are mostly vague, or in many instances missing. This decreases the utility of the specimens for understanding the source, emergence, and distribution of SARS-COV-2 and similar viruses. This project will provide quality georeferenced data products through the consolidation of ancillary information linked to each bat specimen, using the extended specimen model. The resulting product will serve as a model of how data in biodiversity collections might be used to address emerging diseases of zoonotic origin. Results from the project will be disseminated widely in opensource journals, at scientific meetings, and via websites associated with the participating organizations and institutions. Support of this project provides a quality resource optimized to inform research relevant to improving our understanding of the biology and spread of SARS-CoV-2. The overall objectives are to deliver versioned data products, in formats used by the wider research and biodiversity collections communities, through an open-access repository; project protocols and code via GitHub and described in a peer-reviewed paper, and; sustained engagement with biodiversity collections throughout the project for reintegration of improved data into their local specimen data management systems improving long-term curation.This RAPID award will produce and deliver a georeferenced, vetted and consolidated data product for horseshoe bats and related species to facilitate understanding of the sources, distribution, and spread of SARS-CoV-2 and related viruses, a timely response to the ongoing global pandemic caused by SARS-CoV-2 and an important contribution to the global effort to consolidate and provide quality data that are relevant to understanding emergent and other properties the current pandemic. This RAPID award is made by the Division of Biological Infrastructure (DBI) using funds from the Coronavirus Aid, Relief, and Economic Security (CARES) Act.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.""Files included in this resource9d4b9069-48c4-4212-90d8-4dd6f4b7f2a5.zip: Raw data from iDigBio, DwC-A format0067804-200613084148143.zip: Raw data from GBIF, DwC-A format0067806-200613084148143.zip: Raw data from GBIF, DwC-A format1623690110.zip: Full export of this project's data (enhanced and raw) from BIOSPEX, CSV formatbionomia-datasets-attributions.zip: Directory containing 103 Frictionless Data packages for datasets that have attributions made containing Rhinolophids or Hipposiderids, each package also containing a CSV file for mismatches in person date of birth/death and specimen eventDate. File bionomia-datasets-attributions-key_2021-02-25.csv included in this directory provides a key between dataset identifier (how the Frictionless Data package files are named) and dataset name.bionomia-problem-dates-all-datasets_2021-02-25.csv: List of 21 Hipposiderid or Rhinolophid records whose eventDate or dateIdentified mismatches a wikidata recipients date of birth or death across all datasets.flagEventDate.txt: file containing term definition to reference in DwC-AflagExclude.txt: file containing term definition to reference in DwC-AflagGeoreference.txt: file containing term definition to reference in DwC-AflagTaxonomy.txt: file containing term definition to reference in DwC-AgeoreferencedByID.txt: file containing term definition to reference in DwC-AidentifiedByNames.txt: file containing term definition to reference in DwC-Ainstructions-to-get-people-data-from-bionomia-via-datasetKey: instructions given to data providersRAPID-code_collection-date.R: code associated with enhancing collection datesRAPID-code_compile-deduplicate.R: code associated with compiling and deduplicating raw dataRAPID-code_external-linkages-bold.R: code associated with enhancing external linkagesRAPID-code_external-linkages-genbank.R: code associated with enhancing external linkagesRAPID-code_external-linkages-standardize.R: code associated with enhancing external linkagesRAPID-code_people.R: code associated with enhancing data about peopleRAPID-code_standardize-country.R: code associated with standardizing country dataRAPID-data-dictionary.pdf: metadata about terms included in this projects data, in PDF formatRAPID-data-dictionary.xlsx: metadata about terms included in this projects data, in spreadsheet formatrapid-data-providers_2021-05-03.csv: list of data providers and number of records provided to rapid-joined-records_country-cleanup_2020-09-23.csvrapid-final-data-product_2021-06-29.zip: Enhanced data from BIOSPEX, DwC-A formatrapid-final-gazetteer.zip: Gazetteer providing georeference data and metadata for 10,341 localities assessed as part of this projectrapid-joined-records_country-cleanup_2020-09-23.csv: data product initial version where raw data has been compiled and deduplicated, and country data has been standardizedRAPID-protocol_collection-date.pdf: protocol associated with enhancing collection datesRAPID-protocol_compile-deduplicate.pdf: protocol associated with compiling and deduplicating raw dataRAPID-protocol_external-linkages.pdf: protocol associated with enhancing external linkagesRAPID-protocol_georeference.pdf: protocol associated with georeferencingRAPID-protocol_people.pdf: protocol associated with enhancing data about peopleRAPID-protocol_standardize-country.pdf: protocol associated with standardizing country dataRAPID-protocol_taxonomic-names.pdf: protocol associated with enhancing taxonomic name dataRAPIDAgentStrings1_archivedCopy_30March2021.ods: resource used in conjunction with RAPID people protocolrecordedByNames.txt: file containing term definition to reference in DwC-ARhinolophid-HipposideridAgentStrings_and_People2_archivedCopy_30March2021.ods: resource used in conjunction with RAPID people protocolwikidata-notes-for-bat-collectors_leachman_2020: please see https://zenodo.org/record/4724139 for this resource",4
Data from: The contribution of road-based citizen science efforts to the conservation of pond-breeding amphibians,"1. Road-side amphibian citizen science programs bring together volunteers focused on collecting scientific data while working to mitigate population declines by directly reducing road mortality of pond-breeding amphibians. Despite the international popularity of these movement-based road-side conservation efforts (i.e., 'big nights', 'bucket brigades' and 'toad patrols'), direct benefits to conservation have rarely been quantified or evaluated. 2. As a case study, we used a population simulation approach to evaluate how volunteer intensity, frequency and distribution influence three conservation outcomes (minimum population size, population growth rate, and years to extinction) of the spotted salamander (Ambystoma maculatum)  a common, focal pond-breeding amphibian of citizen science and conservation programs in the United States. 3. Sensitivity analysis supported the expectation that populations were primarily recruitment-driven. Thus, conservation outcomes were highest when volunteers focused on out-migration of metamorphs as opposed to in-migration of adults contrary to the typical timing of such volunteer events. 4. Almost every volunteer strategy resulted in increased conservation outcomes compared to a no-volunteer strategy. Specifically, volunteer frequency during metamorph migration increased outcomes more than the same increases in volunteer effort during adult migration. Small population sizes resulted in a negligible effect of volunteer intensity. Volunteers during the first adult in-migration had a relatively small effect compared to most other strategies. 5. Synthesis and applications. Although citizen science focused conservation actions could directly benefit declining populations, other conservation measures are additionally needed to halt or reverse local amphibian declines. This study demonstrates a need to evaluate the effectiveness of focusing citizen science mitigation efforts on the metamorph stage, as opposed the adult stage, which may be challenging, compared to other management actions such as road-crossing infrastructure. Current amphibian citizen science programs will be challenged to balance implementing evidence-based conservation measures on the most limiting life stage while retaining the social and community benefits to volunteers.","[""# bprob.r: function that uses breeding probability to split population into breeders and nonbreeders at start of breeding season\n# input: population size, breeding probability (1 - temporary emigration; estimated from multistate mark-recapture model)\n# processing: determine whether each adult breeds or doesn't breed\n# output: return vector with 1/0 for breeding/nonbreeding\nbprob <- function(pop, prob){\n out <- rbinom(pop, 1, prob)\n return(out)\n# could modify function to deal with Markovian temporary emigration\n}\n\n\n# not run:\n# adults <- 100\n# bprob(adults,.5) -> breeders\n\n"", 'fecund <- function(pop, fec){\n  # function that creates number of individuals produced by adults with Poisson distribution\n  # could focus on overall number of metamorphs relative to number of adults, skipping larval dynamics\n  return(rpois(pop, fec))\n}\n', 'migrate <- function(num, night_prob, rmort, num_guards, guard_limit){   #,fec, bmort){\n#   input: number of crossing adults susceptible to road mortality, proportion of adults migrating each night, fecunedity, road mortality, number xing guards each night,\n#       maximum number of animals saved per guard per night # drop breeding mortality\n#   processing:\n#       mortality of adults crossing road per night\n#       #drop: fecundity of surviving adults\n#       # drop: mortality of adults that survive inbound migration\n#   output:\n#       adults that make it into the pond to breed on each night\n#       # drop: number of adults that will make outbound migration\n  #print(flag) debugging parameter, set unique integer to flag different function calls\n  if(length(num)>1) print(""Check input - pop vector rather than pop total"")\n\n  # create object to store migrants each night, should fix to get whole numbers\n  num * night_prob -> nxing\n  round(nxing) -> nxing\n\n  # calculate potential mortality\n  pmort <- rep(NA, length(nxing))\n  for(i in 1:length(nxing)){\n     pmort[i] <- sum(rbinom(nxing[i], 1, rmort))\n  }\n  # number of salamanders saved from cars\n  num_guards*guard_limit -> numsave\n  # \'save\' salamanders\n  pmort - numsave -> mort\n  # set mortality to zero if guards can save more salamanders than those that could be killed\n  mort[mort<0] <- 0\n  # subtract killed individuals from individuals that crossed road\n  nxing - mort -> numcrossed\n  return(numcrossed)\n\n}\n\n\n', ""# scenario simulations - first run\n#out <- array(dim=c(50,2,100), data=NA)\n\n#system.time(for(i in 1:100){\n#out[,,i] <- popsim(nyears=50, init_pop=c(100,100), dphi_sa = .9986, pr_b= .4230, nb_dphi = .9992, a_in_night_prob = c(0.333,0.333,0.334), \n#       a_rmort = 0.0407, a_in_num_guards = c(0,0,0), guard_limit = 10, a_p_dphi = 0.9994, p_days = 14, fec = 165, lmort = 0.0523, \n#       a_out_night_prob = c(0.333,0.333,0.334), rmort2 = 0.0407, a_out_nguard = c(0,0,0), met_night_prob = c(0.333,0.333,0.334), \n#       rmort_met = 0.0407, ng_met = c(0,0,0))\n#})  \n\n# load data\nsource('./R/popsim.r')\nsource('./R/bprob.r')\nsource('./R/migrate.r')\nsource('./R/fecund.r')\nsource('./R/surv.r')\n# fecund, migrate\nload('./data/scenarios.Rdata')\n\n# create object for simulation output\nmy_fun <- function(){\n  list(d=array(dim=c(50,2,500), data=NA))\n}\nreplicate(361, my_fun(), simplify=TRUE)->out\n\n# list option\n# loop through first 100 scenarios, going to do 500 iterations\nsystem.time(for(i in 1:361){\n  \n   # pull in road xing guard data from scenarios object\n  gds_ai <- c(scenarios$night1_adult_in_migtation[i],scenarios$night2_adult_inout_migtation[i],scenarios$night3_adult_inout_migtation[i])\n  gds_ao <- c(scenarios$night2_adult_inout_migtation[i],scenarios$night3_adult_inout_migtation[i],scenarios$night4_adult_out_migtation[i])\n  gd_m <- c(scenarios$night5_meta_out_migration[i], scenarios$night6_meta_out_migration[i], scenarios$night7_meta_out_migration[i])\n  # loop through for 500 iterations\n    for(j in 1:500){\n    out[[i]][,,j] <- popsim(nyears=50, init_pop=c(100,100), dphi_sa = .9986, pr_b= .4230, nb_dphi = .9992, a_in_night_prob = c(0.333,0.333,0.334), \n            a_rmort = 0.0407, a_in_num_guards = gds_ai, guard_limit = 10, a_p_dphi = 0.9994, p_days = 14, fec = 165, lmort = 0.0523, \n            a_out_night_prob = c(0.333,0.333,0.334), rmort2 = 0.0407, a_out_nguard = gds_ao, met_night_prob = c(0.333,0.333,0.334), \n            rmort_met = 0.0407, ng_met = gd_m)\n    \n      } #j\n    if(i%%1==0) {save(out, file='./data/outdata.Rdata')}\n  }) #i\n\n"", 'surv <- function(pop, phi, days){\n  # function to calculate fate of POP individuals with daily survival PHI after DAYS\n  # input: pop, phi, days\n  # processing\n  pop <- rep(1,pop)\n  #tmp <- rbinom(pop, 1, phi)\n  pop\n  for(i in 1:days){\n    pop <- pop*(rbinom(pop,1,phi))\n    #print(i)\n    #print(pop)\n  }\n  return(pop)\n}\n\n']","Data from: The contribution of road-based citizen science efforts to the conservation of pond-breeding amphibians 1. Road-side amphibian citizen science programs bring together volunteers focused on collecting scientific data while working to mitigate population declines by directly reducing road mortality of pond-breeding amphibians. Despite the international popularity of these movement-based road-side conservation efforts (i.e., 'big nights', 'bucket brigades' and 'toad patrols'), direct benefits to conservation have rarely been quantified or evaluated. 2. As a case study, we used a population simulation approach to evaluate how volunteer intensity, frequency and distribution influence three conservation outcomes (minimum population size, population growth rate, and years to extinction) of the spotted salamander (Ambystoma maculatum)  a common, focal pond-breeding amphibian of citizen science and conservation programs in the United States. 3. Sensitivity analysis supported the expectation that populations were primarily recruitment-driven. Thus, conservation outcomes were highest when volunteers focused on out-migration of metamorphs as opposed to in-migration of adults contrary to the typical timing of such volunteer events. 4. Almost every volunteer strategy resulted in increased conservation outcomes compared to a no-volunteer strategy. Specifically, volunteer frequency during metamorph migration increased outcomes more than the same increases in volunteer effort during adult migration. Small population sizes resulted in a negligible effect of volunteer intensity. Volunteers during the first adult in-migration had a relatively small effect compared to most other strategies. 5. Synthesis and applications. Although citizen science focused conservation actions could directly benefit declining populations, other conservation measures are additionally needed to halt or reverse local amphibian declines. This study demonstrates a need to evaluate the effectiveness of focusing citizen science mitigation efforts on the metamorph stage, as opposed the adult stage, which may be challenging, compared to other management actions such as road-crossing infrastructure. Current amphibian citizen science programs will be challenged to balance implementing evidence-based conservation measures on the most limiting life stage while retaining the social and community benefits to volunteers.",4
Biodiversity on public lands: how citizen science can help,"These are the code and data files for ""Biodiversity on public lands: how citizen science can help"". Included in this are the following:1. taxataxi_opt_multiple_reports.R which is code for running multiple iNaturalist Bioblitz datasets with built in re-setting mechanisms for the various reports run.2. iNat_NPS_comparison_code.R which is code for running a single iNaturalist Bioblitz dataset. This is identical to (1) except for the re-setting mechanisms and some re-organization of parts of the code.3. synonym_links & taxonomic_units which are used in conjunction with (1) & (2) for the synonym identification.4. All_iNat.csv is all of the iNaturalist entries used in the initial study named above.5. all_parks.csv contains all of the filtered data from the park units through (1) for the initial study named above.6. invasives_for_all_table1.R is code used in the initial study to identify introduced species.7. migration.R is code used in the initial study to identify previously unrecorded species that were recorded less than 50 times in BISON in the USA state that the park unit is located in.8. table3_all_parks.csv is the data used in the migration.R code for identifying these species.","['#Contents:\r\n#1: Working directory & package library\r\n#2: Data import (NPSpecies & iNaturalist)\r\n#3: Direct Matches to NPS\r\n#4: Matches to NPS synonyms\r\n#5: Matches to ITIS\r\n#6: Make dataframe with raw results\r\n#7: Take raw results dataframe and pull out all of the unique entries\r\n#8: Make table 1\r\n#9: Make table 2\r\n#10: Make table 3\r\n#11: Make table 4\r\n#12: Make graph\r\n\r\n#1----------------------------------------------------------------------------------------\r\nsetwd(""C:/Users/Amanda/Desktop/VSFS/compare_iNat_NPS/"")\r\nlibrary(rinat) \r\n#library(taxize) only needed if downloading from ITIS database\r\nlibrary(stringr)\r\nlibrary(tibble)\r\nlibrary(""rbison"")\r\n#devtools::install_github(""ropensci/rbison"")\r\nlibrary(tidyr)\r\nlibrary(dplyr)\r\nlibrary(data.table)\r\n#2----------------------------------------------------------------------------------------\r\n#Specify park:\r\npark <- c(""LEWI"")\r\noptions(stringsAsFactors = FALSE)\r\n\r\ndir.create(paste(""./"", park, sep=""""))\r\n\r\n#First, import data\r\nNPS_data <- read.csv(""./NPSpecies-Lists/NPSpecies_Checklist_BOHA_20190917094232.csv"", header=T, skipNul = T, stringsAsFactors=F)\r\nNPS_data2 <- word(NPS_data$Scientific.name, start=1, end=2, sep="" "")\r\n\r\n#Get iNaturalist data through an API\r\n#get_inat_obs_project is from the rinat package\r\n\r\niNat_data <- get_inat_obs_project(""2016-national-parks-bioblitz-lewis-and-clark"", type=""observations"")\r\nas.character(iNat_data$Scientific.name) #this prints the data?\r\n\r\n#Create new data frame with only the RG & iconic taxa\r\nRG_iNat_data <- iNat_data[iNat_data$Quality.grade == ""research"",]\r\n\r\nRG_iNat_data_iconic <- RG_iNat_data[RG_iNat_data$Iconic.taxon.name == ""Plantae"" | RG_iNat_data$Iconic.taxon.name == ""Aves"" | RG_iNat_data$Iconic.taxon.name == ""Mammalia"" | RG_iNat_data$Iconic.taxon.name == ""Reptilia"" | RG_iNat_data$Iconic.taxon.name == ""Amphibia"",]\r\n\r\n#Chop off subspecies names. Genus only names are assigned NA. Returns a vector, not a dataframe\r\nRG_iNat_data2 <- word(RG_iNat_data_iconic$Scientific.name, start=1, end=2, sep="" "") \r\n\r\n#Make new dataframe with only the RG, non-iconic taxa will be stored. This will be used in table 4.\r\nRG_iNat_not_iconic <- RG_iNat_data[RG_iNat_data$Iconic.taxon.name != ""Plantae"" & RG_iNat_data$Iconic.taxon.name != ""Aves"" & RG_iNat_data$Iconic.taxon.name != ""Mammalia"" & RG_iNat_data$Iconic.taxon.name != ""Reptilia"" & RG_iNat_data$Iconic.taxon.name != ""Amphibia"",]\r\n\r\n#3----------------------------------------------------------------------------------------\r\n#Use a loop to compare the iNat entries to the NPS entries.\r\niNat_NPS_matches <- vector(mode=""logical"", length=0)\r\n\r\nfor (i in 1:length(RG_iNat_data2)){\r\n  iNat_NPS_matches[i] <- RG_iNat_data2[i] %in% NPS_data2\r\n}\r\n\r\n#4----------------------------------------------------------------------------------------\r\n#Use a loop to compare the iNat entries to the NPS synonym entries (1st in every entry)\r\nNPS_data3 <- word(NPS_data$Synonyms, start=1, end=2, sep= "" "")\r\niNat_NPSsynonym_matches <- vector(mode=""logical"", length=0)\r\n\r\nfor (i in 1:length(RG_iNat_data2)){\r\n  iNat_NPSsynonym_matches[i] <- RG_iNat_data2[i] %in% NPS_data3\r\n}\r\n\r\n#5----------------------------------------------------------------------------------------\r\n#Get synonyms for all of the NPSpecies entries from ITIS\r\n\r\n#input table so we do not have to download the synonyms: code from Kelsey Cooper (Indiana University)\r\nsyn_nums <- read.table(\'synonym_links\', sep =""|"", header = FALSE, dec =""."", stringsAsFactors = FALSE)\r\nsci_names <- read.table(\'taxonomic_units\', sep =""|"", header = FALSE, dec =""."", fill=T, stringsAsFactors = FALSE)\r\nsci_names <- sci_names[c(\'V1\',\'V26\')]\r\nsetnames(sci_names, old=c(""V1"",""V26""), new=c(""TSN"", ""Scientific.Name""))\r\nsetnames(syn_nums, old=c(""V1"",""V2"", ""V3""), new=c(""TSN"", ""SYN.TSN"", ""Date""))\r\nNPS_ITIS_synonyms_df <- merge(syn_nums, sci_names, by=\'TSN\', all.x=T)\r\nNPS_ITIS_synonyms_df <- merge(NPS_ITIS_synonyms_df, sci_names, by.x=\'SYN.TSN\', by.y=\'TSN\', all.x=T)\r\nNPS_ITIS_synonyms_df <- NPS_ITIS_synonyms_df[-3]\r\nNPS_ITIS_synonyms_df <- NPS_ITIS_synonyms_df[,c(1, 4, 2, 3)]\r\nsetnames(NPS_ITIS_synonyms_df, old=c(""SYN.TSN"",""TSN"", ""Scientific.Name.x"", ""Scientific.Name.y""), new=c(""TSN"", ""SYN.TSN"", ""SYN.Scientific.Name"", ""Scientific.Name""))\r\n\r\nNPS_synonyms <- data.frame(Scientific.name=character(), Synonym=character(), stringsAsFactors = FALSE)\r\n\r\nfor (i in 1:length(NPS_data$Scientific.name)) {\r\n    placeholderdf <- NPS_ITIS_synonyms_df[grepl(NPS_data$Scientific.name[i], NPS_ITIS_synonyms_df$Scientific.Name),] \r\n    placeholderdf2 <- NPS_ITIS_synonyms_df[grepl(NPS_data$Scientific.name[i], NPS_ITIS_synonyms_df$SYN.Scientific.Name),]\r\n    NPS_synonyms <- rbind(NPS_synonyms, placeholderdf, placeholderdf2)\r\n}\r\n\r\n\r\n#use the df of synonyms\r\niNat_ITIS_synonym_matches <- vector(mode=""logical"", length=0)\r\niNat_ITIS_nonsyn_matches <- vector(mode=""logical"", length=0)\r\n\r\nfor (i in 1:length(RG_iNat_data2)) {\r\n  iNat_ITIS_synonym_matches[i] <- RG_iNat_data2[i] %in% NPS_synonyms$SYN.Scientific.Name\r\n  iNat_ITIS_nonsyn_matches[i] <- RG_iNat_data2[i] %in% NPS_synonyms$Scientific.Name\r\n}\r\n\r\n#6----------------------------------------------------------------------------------------\r\n#Make a new dataframe with the raw results\r\ndata <- cbind(RG_iNat_data2, RG_iNat_data_iconic$Iconic.taxon.name, iNat_NPS_matches, iNat_NPSsynonym_matches, iNat_ITIS_synonym_matches, iNat_ITIS_nonsyn_matches, RG_iNat_data_iconic$Url)\r\ndata <- as.data.frame(na.omit(data))\r\ncolnames(data)[1] <- c(""Scientific_name"")\r\ncolnames(data)[2] <- c(""Iconic_taxa"")\r\ncolnames(data)[7] <- c(""iNaturalist_URL"")\r\ndata <- data[order(data$Iconic_taxa, data$Scientific_name),]\r\n\r\n#OUtput the raw data into a csv file. \r\nrownames(data) <- c()\r\nwrite.csv(data, file=paste(""./"", park, ""/"", park,""_data.csv"", sep=""""))\r\ndata <- data[,-7]\r\n#7----------------------------------------------------------------------------------------\r\n#For all the tables, need to remove duplicate species entries\r\ndata2 <- unique(data)\r\n\r\n#8----------------------------------------------------------------------------------------\r\n#Table 1: All data; tells which falses are synonyms (to NPS or itis) and which are new\r\n\r\ndata2[,""Match NPSpecies""] <- c("""")\r\nfor (i in 1:length(data2$Scientific_name)){\r\n  if (data2[i,3]==TRUE){\r\n    data2[i,7] <- c(""Match"")\r\n  } else if (data2[i,4]==TRUE & data2[i,3]==FALSE) {\r\n  data2[i,7] <- c(""NPSpecies synonym match"")\r\n  } else if (data2[i,5] == TRUE & data2[i,3] == FALSE & data2[i,4] == FALSE) {\r\n    data2[i,7] <- c(""ITIS Match"")\r\n  } else if (data2[i,6] ==TRUE & data2[i,3] == FALSE & data2[i,4] == FALSE){\r\n    data2[i,7] <- c(""ITIS Match"")\r\n  } else {\r\n    data2[i,7] <- c(""No Match"")\r\n  }\r\n}\r\n\r\ntable1 <- data2[,c(1,2,7)]\r\n\r\ntable1 <- add_row(.data=table1, Scientific_name=RG_iNat_not_iconic$Scientific.name, Iconic_taxa=RG_iNat_not_iconic$Iconic.taxon.name)\r\ntable1[\'iNaturalist_Entry_count\'] <- c("""")\r\n\r\nfor (i in 1:length(table1$Scientific_name)){\r\n  if (table1[i, 2] != ""Amphibia"" & table1[i, 2] != ""Aves"" & table1[i, 2] != ""Mammalia"" &\r\n      table1[i, 2] != ""Plantae"" & table1[i, 2] != ""Reptilia""){\r\n    table1[i,3] <- c(""Table 4"")\r\n  } \r\n}\r\n\r\n#table1 <- table1[order(table1$Iconic_taxa, table1$Scientific_name),]\r\ntable1 <- unique(table1)\r\n\r\n\r\n#Add in the counts for entries (number of entries per species)\r\nfor (i in 1:length(table1$Scientific_name)) {\r\n  if (table1$Scientific_name[i] %in% data2$Scientific_name){\r\n  table1$iNaturalist_Entry_count[i] <- length(which(data$Scientific_name == table1$Scientific_name[i]))\r\n  } else {\r\n    table1$iNaturalist_Entry_count[i] <- length(which(RG_iNat_not_iconic ==table1$Scientific_name[i]))\r\n  }\r\n}\r\n\r\nrownames(table1) <- c()\r\nwrite.csv(table1, paste(""./"", park, ""/"", park, ""_table1.csv"", sep=""""))\r\n\r\n#9----------------------------------------------------------------------------------------\r\n#Table 2: All Synonym data and what the taxonomic update is (itis only)\r\n\r\n#1. set up table to have 4 columns\r\ntable2 <- table1[table1$`Match NPSpecies` ==""ITIS Match"",]\r\ntable2 <- table2[,c(2,1)]\r\ncolnames(table2)[colnames(table2)==""Scientific_name""] <- ""iNat_entry"" #Giving some weird warning message that is annoying\r\ntable2$iNat_entry <- as.character(table2$iNat_entry)\r\ntable2[,""NPSpecies_name""] <- c("""") \r\ntable2[,""Accepted_TSN""] <- c("""")\r\ntable2[,""NPS_entry_TSN""] <- c("""")\r\n\r\nfor (i in 1:length(table2$iNat_entry)) {\r\n  if (table2$iNat_entry[i] %in% NPS_synonyms$SYN.Scientific.Name == TRUE){\r\n    table2$Accepted_TSN[i] <- NPS_synonyms$TSN[match(table2$iNat_entry[i], NPS_synonyms$SYN.Scientific.Name)]\r\n    table2$NPS_entry_TSN[i] <- NPS_synonyms$SYN.TSN[match(table2$iNat_entry[i], NPS_synonyms$SYN.Scientific.Name)]\r\n    table2$NPSpecies_name[i] <- NPS_synonyms$Scientific.Name[match(table2$iNat_entry[i], NPS_synonyms$SYN.Scientific.Name)]\r\n  } else if (table2$iNat_entry[i] %in% NPS_synonyms$Scientific.Name == TRUE) {\r\n    table2$Accepted_TSN[i] <- NPS_synonyms$TSN[match(table2$iNat_entry[i], NPS_synonyms$Scientific.Name)]\r\n    table2$NPS_entry_TSN[i] <- NPS_synonyms$SYN.TSN[match(table2$iNat_entry[i], NPS_synonyms$Scientific.Name)]\r\n    table2$NPSpecies_name[i] <- NPS_synonyms$SYN.Scientific.Name[match(table2$iNat_entry[i], NPS_synonyms$Scientific.Name)]\r\n    }\r\n  }\r\n\r\nrownames(table2) <- c()\r\nwrite.csv(table2, paste(""./"", park, ""/"", park,""_table2.csv"", sep=""""))\r\n\r\n#10----------------------------------------------------------------------------------------\r\n#Table 3: All potentially new species\r\n#columns: taxa, scientific name, common name, BISON occurrences, invasive\r\ntable3 <- table1[table1$`Match NPSpecies`==""No Match"",]\r\ntable3 <- table3[,c(2:1)]\r\ntable3[,""Occurences in BISON""] <- c("""")\r\ntable3[,""Invasive.Record""] <- c("""")\r\n\r\n#Invasive record \r\ninv_list <- read.csv(""https://raw.githubusercontent.com/KelseyDCooper/USGS-NPS-App/master/invasives_list.csv"")\r\nfor (i in 1:length(table3$Scientific_name)){\r\n  if (table3$Scientific_name[i] %in% inv_list$Scientific.Name){\r\n    table3$Invasive.Record[i] <- c(""Yes"")\r\n  } else {\r\n    table3$Invasive.Record[i] <- c(""No"")\r\n  }\r\n}\r\n\r\n#Add in BISON entries. Best to do this last due to the vast amounts of data (list of states) that isn\'t easily visible in R dataframe.\r\n#Takes a bit of time.\r\n\r\nlong.name <- c(state.name, ""Alberta Canada"", ""American Samoa"", ""British Columbia Canada"", ""Commonwealth of the Northern Mariana Islands"",\r\n               ""District of Columbia"", ""Guam"", ""Manitoba Canada"", ""New Brunswick Canada"", \r\n               ""Newfoundland and Labrador Canada"", ""Northwest Territories Canada"", \r\n               ""Nova Scotia Canada"", ""Nunavut Canada"", ""Ontario Canada"", ""Prince Edward Island Canada"",\r\n               ""Puerto Rico"", ""Quebec Canada"", ""Saskatchewan Canada"", ""United States Virgin Islands"", \r\n               ""Yukon Canada"", ""Alaska EEZ"", ""American Samoa EEZ"", ""Hawaii EEZ"", ""Howland and Baker Island EEZ"",\r\n               ""Jarvis Island EEZ"", ""Johnston Atoll EEZ"", ""Northern Mariana Islands and Guam EEZ"", \r\n               ""Palmyra Atoll EEZ"", ""Puerto Rico EEZ"", ""US Atlantic EEZ"", ""US Pacific EEZ"",\r\n               ""US Virgin Islands EEZ"", ""Wake Island EEZ"")\r\nshort.name <- c(state.abb, ""AB CAN"", ""AS"", ""BC CAN"", ""CNMI"", ""DC"", ""GU"", ""MB CAN"", ""NB CAN"",\r\n                ""NL CAN"", ""NT CAN"", ""NS CAN"", ""NU CAN"", ""ON CAN"", ""PE CAN"", ""PR"", ""QC CAN"",\r\n                ""SK CAN"", ""USVI"", ""YT CAN"", ""AK EEZ"", ""AS EEZ"", ""HI EEZ"", ""Howland-Baker EEZ"",\r\n                ""Jarvis EEZ"", ""Johnston Atoll EEZ"", ""N MP-GU EEZ"", ""Palmyra Atoll EEZ"", ""PR EEZ"", ""US Atlantic EEZ"", ""US Pacific EEZ"",\r\n                ""USVI EEZ"", ""Wake EEZ"")\r\nall.states <- as.data.frame(cbind(long.name, short.name), stringsAsFactors = FALSE)\r\n\r\nfor (i in 1:length(table3$Scientific_name)){\r\n  out <- bison(species=table3$Scientific_name[i], count=1)\r\n  if (is.null(out$states) == FALSE){\r\n    state.counts <- as.data.frame(cbind(out$states[,""record_id""]))\r\n    for (k in 1:length(state.counts$V1)){\r\n      state.counts$V2[k] <- all.states$short.name[match(state.counts$V1[k], all.states$long.name)]\r\n    }\r\n    states <- paste(state.counts$V2, out$states[,\'total\'])\r\n    states <- paste(states, collapse=""; "")\r\n    table3[i, \'Occurences in BISON\'] <- states\r\n  } else {\r\n    table3[i, \'Occurences in BISON\'] <- ""No data available""\r\n  }\r\n}\r\n\r\n#export table\r\nrownames(table3) <- c()\r\nwrite.csv(table3, paste(""./"", park, ""/"", park, ""_table3.csv"", sep=""""))\r\n#11----------------------------------------------------------------------------------------\r\n#Table 4\r\ntable4 <- RG_iNat_not_iconic[,c(5,3,4)]\r\ntable4 <- table4[order(table4$Iconic.taxon.name, table4$Scientific.name),]\r\ntable4 <- unique(table4)\r\n\r\nrownames(table4) <- c()\r\nwrite.csv(table4, paste(""./"", park, ""/"", park, ""_table4.csv"", sep=""""))\r\n\r\n#12----------------------------------------------------------------------------------------\r\n#Graph\r\n\r\ntable1a <- data2[,c(1,2,7)]\r\n\r\ncounts2 <- table(table1a$`Match NPSpecies`, table1a$Iconic_taxa) \r\ncounts3 <- counts2[,-4] #delete the plants column from the table\r\ncolnames(counts3) <- c(""Amph"", ""Aves"", ""Mamm"", ""Rept"")\r\n#give us y axis maximums\r\ndfcounts <- as.data.frame(counts2)\r\namphibiasum <- sum(dfcounts$Freq[dfcounts$Var2 == ""Amphibia""])\r\navessum <- sum(dfcounts$Freq[dfcounts$Var2 == ""Aves""])\r\nmammaliasum <- sum(dfcounts$Freq[dfcounts$Var2 == ""Mammalia""])\r\nplantaesum <- sum(dfcounts$Freq[dfcounts$Var2 == ""Plantae""])\r\nreptiliasum <- sum(dfcounts$Freq[dfcounts$Var2 == ""Reptilia""])\r\n\r\nall_high <- max(rbind(amphibiasum, avessum, mammaliasum, plantaesum, reptiliasum)) +10\r\nsub_high <- max(rbind(amphibiasum, avessum, mammaliasum, reptiliasum)) +5\r\n\r\ncounts2 <- counts2[,c(1,2,3,5,4)]\r\n\r\n#Time to graph!\r\npdf(paste(""./"", park, ""/"", park,""_figure1.pdf"", sep=""""), width=7, height=8)\r\n\r\npar(fig=c(0,1,0,1))\r\n#par(mar=c(5.1, 4.1, 4.1, 8.1), xpd=TRUE)\r\n\r\nbarplot(counts2, ylim=c(0,all_high), xlab=""Taxa"", col=c(""cadetblue1"", ""seashell"",""lightsalmon"",""mediumorchid4""), \r\n        ylab=""Number of Species"")\r\n\r\nlegend(""top"", inset=0.02, legend = rownames(counts2) , fill = c(""cadetblue1"", ""seashell"",""lightsalmon"",""mediumorchid4""))\r\n\r\n#par(fig=c(0.75,1,0.75,1), mar=c(1,1,1,1), new=TRUE)\r\npar(fig=c(0.1,0.5,0.5,1), new=TRUE)\r\nbarplot(counts3, ylim=c(0,sub_high),col=c(""cadetblue1"", ""seashell"",""lightsalmon"", ""mediumorchid4""), cex.names=0.45)\r\n\r\n\r\n\r\ndev.off()\r\n\r\n#no inset\r\npdf(paste(""./"", park, ""/"", park,""_figure1_no_inset.pdf"", sep=""""), width=7, height=8)\r\n\r\npar(fig=c(0,1,0,1))\r\n\r\nbarplot(counts2, ylim=c(0,all_high), xlab=""Taxa"", col=c(""cadetblue1"", ""seashell"",""lightsalmon"",""mediumorchid4""), \r\n        ylab=""Number of Species"")\r\n\r\nlegend(""topleft"", inset=c(0.02), legend = rownames(counts2) , fill = c(""cadetblue1"", ""seashell"",""lightsalmon"",""mediumorchid4""))\r\n\r\ndev.off()\r\n\r\n\r\n\r\n\r\n\r\n\r\n', 'table1 <- read.csv(""./table1_all_reports/all_parks.csv"", stringsAsFactors = F)\r\ntable1[,""Invasives""] <- c("""")\r\n\r\ninv_list <- read.csv(""https://raw.githubusercontent.com/KelseyDCooper/USGS-NPS-App/master/invasives_list.csv"")\r\ninv_list <- inv_list[,""Scientific.Name""]\r\ninv_list <- as.character(inv_list)\r\ninv_list2 <- read.csv(""./table1_all_reports/InvasivePlantsList.csv"", stringsAsFactors = F)\r\ninv_list2 <- inv_list2[,""ScientificName2""]\r\ninv_list2 <- as.character(inv_list2)\r\n\r\ninv_list3 <- c(inv_list, inv_list2, ""Plantago coronopus"", ""Senna artemisioides"", ""Soliva sessilis"", ""Veronica persica"")\r\n\r\n\r\nfor (i in 1:length(table1$..Scientific_name)){\r\n  if (table1$..Scientific_name[i] %in% inv_list3){\r\n    table1$Invasives[i] <- c(""Yes"")\r\n  } else {\r\n    table1$Invasives[i] <- c(""No"")\r\n  }\r\n}\r\n\r\nwrite.csv(table1, file=""20210128_invasives_all_RG_entries.csv"")\r\n', '#this code is meant to identify the species not previously identified in the parks where there were less than 50 (or other specified number) in the park\r\nsetwd(""C:/Users/amkat/Desktop/VSFS/compare_iNat_NPS"")\r\n\r\nlibrary(stringr) #use str_extract from this package\r\n\r\n#table3 <- read.csv(""./ACAD/ACAD_table3.csv"", stringsAsFactors = FALSE)\r\n\r\nparkname <- ""ZION""\r\ntable3 <- read.csv(paste(""./"", parkname, ""/"", parkname, ""_table3.csv"", sep=""""), stringsAsFactors = FALSE)\r\ntable3 <- table3[table3$Occurences.in.BISON != \'No data available\',]\r\n\r\npark_states <- read.csv(""./20200928_migration/nps_boundary.csv"", stringsAsFactors = FALSE)\r\n\r\nBISON_in_state <- rep(NA, length(table3$Scientific_name))\r\nindivdparkstate <- rep(NA, length(table3$Scientific_name))\r\nspeciesundercutoff <- data.frame(table3$Scientific_name, table3$Invasive.Record, BISON_in_state, \r\n                                 indivdparkstate, table3$Occurences.in.BISON, stringsAsFactors = FALSE)\r\n\r\ncutoff <- 50 #put in the cutoff amount that you want for entries in the state\r\n\r\nfor (i in 1:length(table3$Scientific_name)){\r\n  numberinBISON <- unlist(strsplit(table3$Occurences.in.BISON[i], split="";"")) #splits up the BISON occurrences from table 3\r\n  numberinBISON <- gsub(""[[:space:]]"", """", numberinBISON) #remove white spaces\r\n  parkstate <- park_states$STATE[park_states$UNIT_CODE == parkname] #get park state from the database\r\n  \r\n  if (length(grep(parkstate, numberinBISON)) > 0){\r\n  statedata <- as.integer(str_extract(numberinBISON[pmatch(parkstate, numberinBISON)], ""[0-9]+$"")) \r\n   if (statedata <= cutoff) {\r\n      speciesundercutoff$BISON_in_state[i] <- statedata\r\n      speciesundercutoff$indivdparkstate[i] <- parkstate\r\n   }\r\n  }\r\n  \r\n  if (length(grep(parkstate, numberinBISON)) <= 0){\r\n    speciesundercutoff$BISON_in_state[i] <- \'No records\'\r\n    speciesundercutoff$indivdparkstate[i] <- parkstate\r\n  }\r\n}\r\n\r\nspeciesundercutoff <- na.omit(speciesundercutoff[speciesundercutoff$BISON_in_state != ""NA"",])\r\nspeciesundercutoff\r\n\r\nwrite.csv(speciesundercutoff, paste(""./20200928_migration/"", parkname, ""_migration_under_50.csv"", sep=""""))\r\n\r\nrm(BISON_in_state, indivdparkstate, speciesundercutoff)\r\n', '#Contents:\r\n#1: Working directory & package library\r\n#2: Data import (NPSpecies & iNaturalist)\r\n#3: Direct Matches to NPS\r\n#4: Matches to NPS synonyms\r\n#5: Matches to ITIS\r\n#6: Make dataframe with raw results\r\n#7: Take raw results dataframe and pull out all of the unique entries\r\n#8: Make table 1\r\n#9: Make table 2\r\n#10: Make table 3\r\n#11: Make table 4\r\n#12: Make graph\r\n\r\n#1----------------------------------------------------------------------------------------\r\n#If running multiple reports. this code only needs to be run once per session. This helps cut down on time.\r\nsetwd(""C:/Users/amkat/Desktop/VSFS/compare_iNat_NPS/"")\r\nlibrary(rinat)\r\nlibrary(taxize)\r\nlibrary(stringr)\r\nlibrary(tibble)\r\nlibrary(""rbison"")\r\n#devtools::install_github(""ropensci/rbison"")\r\n#library(tidyr)\r\nlibrary(stringr)\r\n#library(dplyr)\r\nlibrary(data.table)\r\n\r\n#input table so we do not have to download the synonyms: code from Kelsey Cooper (Indiana State)\r\nsyn_nums <- read.table(\'synonym_links\', sep =""|"", header = FALSE, dec =""."", stringsAsFactors = FALSE)\r\nsci_names <- read.table(\'taxonomic_units\', sep =""|"", header = FALSE, dec =""."", fill=T, stringsAsFactors = FALSE)\r\nsci_names <- sci_names[c(\'V1\',\'V26\')]\r\nsetnames(sci_names, old=c(""V1"",""V26""), new=c(""TSN"", ""Scientific.Name""))\r\nsetnames(syn_nums, old=c(""V1"",""V2"", ""V3""), new=c(""TSN"", ""SYN.TSN"", ""Date""))\r\nNPS_ITIS_synonyms_df <- merge(syn_nums, sci_names, by=\'TSN\', all.x=T)\r\nNPS_ITIS_synonyms_df <- merge(NPS_ITIS_synonyms_df, sci_names, by.x=\'SYN.TSN\', by.y=\'TSN\', all.x=T)\r\nNPS_ITIS_synonyms_df <- NPS_ITIS_synonyms_df[-3]\r\nNPS_ITIS_synonyms_df <- NPS_ITIS_synonyms_df[,c(1, 4, 2, 3)]\r\nsetnames(NPS_ITIS_synonyms_df, old=c(""SYN.TSN"",""TSN"", ""Scientific.Name.x"", ""Scientific.Name.y""), new=c(""TSN"", ""SYN.TSN"", ""SYN.Scientific.Name"", ""Scientific.Name""))\r\n\r\n#invaisves list (from Kelsey Cooper)\r\ninv_list <- read.csv(""https://raw.githubusercontent.com/KelseyDCooper/USGS-NPS-App/master/invasives_list.csv"")\r\n\r\n#abbreviations of state names for BISON entries\r\nlong.name <- c(state.name, ""Alberta Canada"", ""American Samoa"", ""British Columbia Canada"", ""Commonwealth of the Northern Mariana Islands"",\r\n               ""District of Columbia"", ""Guam"", ""Manitoba Canada"", ""New Brunswick Canada"", \r\n               ""Newfoundland and Labrador Canada"", ""Northwest Territories Canada"", \r\n               ""Nova Scotia Canada"", ""Nunavut Canada"", ""Ontario Canada"", ""Prince Edward Island Canada"",\r\n               ""Puerto Rico"", ""Quebec Canada"", ""Saskatchewan Canada"", ""United States Virgin Islands"", \r\n               ""Yukon Canada"", ""Alaska EEZ"", ""American Samoa EEZ"", ""Hawaii EEZ"", ""Howland and Baker Island EEZ"",\r\n               ""Jarvis Island EEZ"", ""Johnston Atoll EEZ"", ""Northern Mariana Islands and Guam EEZ"", \r\n               ""Palmyra Atoll EEZ"", ""Puerto Rico EEZ"", ""US Atlantic EEZ"", ""US Pacific EEZ"",\r\n               ""US Virgin Islands EEZ"", ""Wake Island EEZ"")\r\nshort.name <- c(state.abb, ""AB CAN"", ""AS"", ""BC CAN"", ""CNMI"", ""DC"", ""GU"", ""MB CAN"", ""NB CAN"",\r\n                ""NL CAN"", ""NT CAN"", ""NS CAN"", ""NU CAN"", ""ON CAN"", ""PE CAN"", ""PR"", ""QC CAN"",\r\n                ""SK CAN"", ""USVI"", ""YT CAN"", ""AK EEZ"", ""AS EEZ"", ""HI EEZ"", ""Howland-Baker EEZ"",\r\n                ""Jarvis EEZ"", ""Johnston Atoll EEZ"", ""N MP-GU EEZ"", ""Palmyra Atoll EEZ"", ""PR EEZ"", ""US Atlantic EEZ"", ""US Pacific EEZ"",\r\n                ""USVI EEZ"", ""Wake EEZ"")\r\nall.states <- as.data.frame(cbind(long.name, short.name), stringsAsFactors = FALSE)\r\n\r\n\r\n#2----------------------------------------------------------------------------------------\r\n#Specify park:\r\npark <- c(""MORA"")\r\noptions(stringsAsFactors = FALSE)\r\n\r\ndir.create(paste(""./"", park, sep=""""))\r\n\r\n#First, import data\r\nNPS_data <- read.csv(""./NPSpecies-Lists/NPSpecies_Checklist_MORA_20190917123525.csv"", header=T, skipNul = T, stringsAsFactors=F)\r\nNPS_data2 <- word(NPS_data$Scientific.name, start=1, end=2, sep="" "")\r\n\r\n#Get iNaturalist data through an API\r\n#get_inat_obs_project is from the rinat package\r\n#iNat_data <- get_inat_obs_project(""2016-national-parks-bioblitz-lewis-and-clark"", type=""observations"") \r\n\r\n#iNat entries from a csv:\r\niNat_data <- read.csv(""./2016-iNat-BioBlitz-Data/2016-national-parks-bioblitz-mount-rainier.csv"", header=T, stringsAsFactors = F)\r\nas.character(iNat_data$Scientific.name) #this prints the data?\r\n\r\n#Create new data frame with only the RG & iconic taxa\r\nRG_iNat_data <- iNat_data[iNat_data$Quality.grade == ""research"",]\r\n\r\nRG_iNat_data_iconic <- RG_iNat_data[RG_iNat_data$Iconic.taxon.name == ""Plantae"" | RG_iNat_data$Iconic.taxon.name == ""Aves"" | RG_iNat_data$Iconic.taxon.name == ""Mammalia"" | RG_iNat_data$Iconic.taxon.name == ""Reptilia"" | RG_iNat_data$Iconic.taxon.name == ""Amphibia"",]\r\n\r\n#Chop off subspecies names. Genus only names are assigned NA. Returns a vector, not a dataframe\r\nRG_iNat_data2 <- word(RG_iNat_data_iconic$Scientific.name, start=1, end=2, sep="" "") \r\n\r\n#Make new dataframe with only the RG, non-iconic taxa will be stored. This will be used in table 4.\r\nRG_iNat_not_iconic <- RG_iNat_data[RG_iNat_data$Iconic.taxon.name != ""Plantae"" & RG_iNat_data$Iconic.taxon.name != ""Aves"" & RG_iNat_data$Iconic.taxon.name != ""Mammalia"" & RG_iNat_data$Iconic.taxon.name != ""Reptilia"" & RG_iNat_data$Iconic.taxon.name != ""Amphibia"",]\r\n\r\n#3----------------------------------------------------------------------------------------\r\n#Use a loop to compare the iNat entries to the NPS entries.\r\niNat_NPS_matches <- vector(mode=""logical"", length=0)\r\n\r\nfor (i in 1:length(RG_iNat_data2)){\r\n  iNat_NPS_matches[i] <- RG_iNat_data2[i] %in% NPS_data2\r\n}\r\n\r\n#4----------------------------------------------------------------------------------------\r\n#Use a loop to compare the iNat entries to the NPS synonym entries (1st in every entry)\r\nNPS_data3 <- word(NPS_data$Synonyms, start=1, end=2, sep= "" "")\r\niNat_NPSsynonym_matches <- vector(mode=""logical"", length=0)\r\n\r\nfor (i in 1:length(RG_iNat_data2)){\r\n  iNat_NPSsynonym_matches[i] <- RG_iNat_data2[i] %in% NPS_data3\r\n}\r\n\r\n#5----------------------------------------------------------------------------------------\r\n#Get synonyms for all of the NPSpecies entries from ITIS\r\n\r\n\r\n\r\nNPS_synonyms <- data.frame(Scientific.name=character(), Synonym=character(), stringsAsFactors = FALSE)\r\n\r\nfor (i in 1:length(NPS_data$Scientific.name)) {\r\n  placeholderdf <- NPS_ITIS_synonyms_df[grepl(NPS_data$Scientific.name[i], NPS_ITIS_synonyms_df$Scientific.Name),] \r\n  placeholderdf2 <- NPS_ITIS_synonyms_df[grepl(NPS_data$Scientific.name[i], NPS_ITIS_synonyms_df$SYN.Scientific.Name),]\r\n  NPS_synonyms <- rbind(NPS_synonyms, placeholderdf, placeholderdf2)\r\n}\r\n\r\n\r\n#use the df of synonyms\r\niNat_ITIS_synonym_matches <- vector(mode=""logical"", length=0)\r\niNat_ITIS_nonsyn_matches <- vector(mode=""logical"", length=0)\r\n\r\nfor (i in 1:length(RG_iNat_data2)) {\r\n  iNat_ITIS_synonym_matches[i] <- RG_iNat_data2[i] %in% NPS_synonyms$SYN.Scientific.Name\r\n  iNat_ITIS_nonsyn_matches[i] <- RG_iNat_data2[i] %in% NPS_synonyms$Scientific.Name\r\n}\r\n\r\n#6----------------------------------------------------------------------------------------\r\n#Make a new dataframe with the raw results\r\ndata <- cbind(RG_iNat_data2, RG_iNat_data_iconic$Iconic.taxon.name, iNat_NPS_matches, iNat_NPSsynonym_matches, iNat_ITIS_synonym_matches, iNat_ITIS_nonsyn_matches, RG_iNat_data_iconic$Url)\r\ndata <- as.data.frame(na.omit(data))\r\ncolnames(data)[1] <- c(""Scientific_name"")\r\ncolnames(data)[2] <- c(""Iconic_taxa"")\r\ncolnames(data)[7] <- c(""iNaturalist_URL"")\r\ndata <- data[order(data$Iconic_taxa, data$Scientific_name),]\r\n\r\n#OUtput the raw data into a csv file. \r\nrownames(data) <- c()\r\nwrite.csv(data, file=paste(""./"", park, ""/"", park,""_data.csv"", sep=""""))\r\ndata <- data[,-7]\r\n#7----------------------------------------------------------------------------------------\r\n#For all the tables, need to remove duplicate species entries\r\ndata2 <- unique(data)\r\n\r\n#8----------------------------------------------------------------------------------------\r\n#Table 1: All data; tells which falses are synonyms (to NPS or itis) and which are new\r\n\r\ndata2[,""Match NPSpecies""] <- c("""")\r\nfor (i in 1:length(data2$Scientific_name)){\r\n  if (data2[i,3]==TRUE){\r\n    data2[i,7] <- c(""Match"")\r\n  } else if (data2[i,4]==TRUE & data2[i,3]==FALSE) {\r\n    data2[i,7] <- c(""NPSpecies synonym match"")\r\n  } else if (data2[i,5] == TRUE & data2[i,3] == FALSE & data2[i,4] == FALSE) {\r\n    data2[i,7] <- c(""ITIS Match"")\r\n  } else if (data2[i,6] ==TRUE & data2[i,3] == FALSE & data2[i,4] == FALSE){\r\n    data2[i,7] <- c(""ITIS Match"")\r\n  } else {\r\n    data2[i,7] <- c(""No Match"")\r\n  }\r\n}\r\n\r\ntable1 <- data2[,c(1,2,7)]\r\n\r\ntable1 <- add_row(.data=table1, Scientific_name=RG_iNat_not_iconic$Scientific.name, Iconic_taxa=RG_iNat_not_iconic$Iconic.taxon.name)\r\ntable1[\'iNaturalist_Entry_count\'] <- c("""")\r\n\r\nfor (i in 1:length(table1$Scientific_name)){\r\n  if (table1[i, 2] != ""Amphibia"" & table1[i, 2] != ""Aves"" & table1[i, 2] != ""Mammalia"" &\r\n      table1[i, 2] != ""Plantae"" & table1[i, 2] != ""Reptilia""){\r\n    table1[i,3] <- c(""Table 4"")\r\n  } \r\n}\r\n\r\n#table1 <- table1[order(table1$Iconic_taxa, table1$Scientific_name),]\r\ntable1 <- unique(table1)\r\n\r\n\r\n#Add in the counts for entries (number of entries per species)\r\nfor (i in 1:length(table1$Scientific_name)) {\r\n  if (table1$Scientific_name[i] %in% data2$Scientific_name){\r\n    table1$iNaturalist_Entry_count[i] <- length(which(data$Scientific_name == table1$Scientific_name[i]))\r\n  } else {\r\n    table1$iNaturalist_Entry_count[i] <- length(which(RG_iNat_not_iconic ==table1$Scientific_name[i]))\r\n  }\r\n}\r\n\r\nrownames(table1) <- c()\r\nwrite.csv(table1, paste(""./"", park, ""/"", park, ""_table1.csv"", sep=""""))\r\n\r\n#9----------------------------------------------------------------------------------------\r\n#Table 2: All Synonym data and what the taxonomic update is (itis only)\r\n\r\n#1. set up table to have 4 columns\r\ntable2 <- table1[table1$`Match NPSpecies` ==""ITIS Match"",]\r\ntable2 <- table2[,c(2,1)]\r\ncolnames(table2)[colnames(table2)==""Scientific_name""] <- ""iNat_entry"" \r\ntable2$iNat_entry <- as.character(table2$iNat_entry)\r\ntable2[,""NPSpecies_name""] <- c("""") \r\ntable2[,""Accepted_TSN""] <- c("""")\r\ntable2[,""NPS_entry_TSN""] <- c("""")\r\n\r\nfor (i in 1:length(table2$iNat_entry)) {\r\n  if (table2$iNat_entry[i] %in% NPS_synonyms$SYN.Scientific.Name == TRUE){\r\n    table2$Accepted_TSN[i] <- NPS_synonyms$TSN[match(table2$iNat_entry[i], NPS_synonyms$SYN.Scientific.Name)]\r\n    table2$NPS_entry_TSN[i] <- NPS_synonyms$SYN.TSN[match(table2$iNat_entry[i], NPS_synonyms$SYN.Scientific.Name)]\r\n    table2$NPSpecies_name[i] <- NPS_synonyms$Scientific.Name[match(table2$iNat_entry[i], NPS_synonyms$SYN.Scientific.Name)]\r\n  } else if (table2$iNat_entry[i] %in% NPS_synonyms$Scientific.Name == TRUE) {\r\n    table2$Accepted_TSN[i] <- NPS_synonyms$TSN[match(table2$iNat_entry[i], NPS_synonyms$Scientific.Name)]\r\n    table2$NPS_entry_TSN[i] <- NPS_synonyms$SYN.TSN[match(table2$iNat_entry[i], NPS_synonyms$Scientific.Name)]\r\n    table2$NPSpecies_name[i] <- NPS_synonyms$SYN.Scientific.Name[match(table2$iNat_entry[i], NPS_synonyms$Scientific.Name)]\r\n  }\r\n}\r\n\r\nrownames(table2) <- c()\r\nwrite.csv(table2, paste(""./"", park, ""/"", park,""_table2.csv"", sep=""""))\r\n\r\n#10----------------------------------------------------------------------------------------\r\n#Table 3: All potentially new species\r\n#columns: taxa, scientific name, common name, BISON occurrences, invasive\r\ntable3 <- table1[table1$`Match NPSpecies`==""No Match"",]\r\ntable3 <- table3[,c(2:1)]\r\ntable3[,""Occurences in BISON""] <- c("""")\r\ntable3[,""Invasive.Record""] <- c("""")\r\n\r\n#Invasive record \r\n\r\nfor (i in 1:length(table3$Scientific_name)){\r\n  if (table3$Scientific_name[i] %in% inv_list$Scientific.Name){\r\n    table3$Invasive.Record[i] <- c(""Yes"")\r\n  } else {\r\n    table3$Invasive.Record[i] <- c(""No"")\r\n  }\r\n}\r\n\r\n#Add in BISON entries. Best to do this last due to the vast amounts of data (list of states) that isn\'t easily visible in R dataframe.\r\n#Takes a bit of time.\r\n\r\n\r\nfor (i in 1:length(table3$Scientific_name)){\r\n  out <- bison(species=table3$Scientific_name[i], count=1)\r\n  if (is.null(out$states) == FALSE){\r\n    state.counts <- as.data.frame(cbind(out$states[,""record_id""]))\r\n    for (k in 1:length(state.counts$V1)){\r\n      state.counts$V2[k] <- all.states$short.name[match(state.counts$V1[k], all.states$long.name)]\r\n    }\r\n    states <- paste(state.counts$V2, out$states[,\'total\'])\r\n    states <- paste(states, collapse=""; "")\r\n    table3[i, \'Occurences in BISON\'] <- states\r\n  } else {\r\n    table3[i, \'Occurences in BISON\'] <- ""No data available""\r\n  }\r\n}\r\n\r\n#export table\r\nrownames(table3) <- c()\r\nwrite.csv(table3, paste(""./"", park, ""/"", park, ""_table3.csv"", sep=""""))\r\n#11----------------------------------------------------------------------------------------\r\n#Table 4\r\ntable4 <- RG_iNat_not_iconic[,c(5,3,4)]\r\ntable4 <- table4[order(table4$Iconic.taxon.name, table4$Scientific.name),]\r\ntable4 <- unique(table4)\r\n\r\nrownames(table4) <- c()\r\nwrite.csv(table4, paste(""./"", park, ""/"", park, ""_table4.csv"", sep=""""))\r\n\r\n#12----------------------------------------------------------------------------------------\r\n#Graph\r\n\r\ntable1a <- data2[,c(1,2,7)]\r\n\r\ncounts2 <- table(table1a$`Match NPSpecies`, table1a$Iconic_taxa) \r\ncounts3 <- counts2[,-4] #delete the plants column from the table\r\ncolnames(counts3) <- c(""Amph"", ""Aves"", ""Mamm"", ""Rept"")\r\n#give us y axis maximums\r\ndfcounts <- as.data.frame(counts2)\r\namphibiasum <- sum(dfcounts$Freq[dfcounts$Var2 == ""Amphibia""])\r\navessum <- sum(dfcounts$Freq[dfcounts$Var2 == ""Aves""])\r\nmammaliasum <- sum(dfcounts$Freq[dfcounts$Var2 == ""Mammalia""])\r\nplantaesum <- sum(dfcounts$Freq[dfcounts$Var2 == ""Plantae""])\r\nreptiliasum <- sum(dfcounts$Freq[dfcounts$Var2 == ""Reptilia""])\r\n\r\nall_high <- max(rbind(amphibiasum, avessum, mammaliasum, plantaesum, reptiliasum)) +5\r\nsub_high <- max(rbind(amphibiasum, avessum, mammaliasum, reptiliasum)) +5\r\n\r\ncounts2 <- counts2[,c(1,2,3,5,4)]\r\n\r\n#Time to graph!\r\npdf(paste(""./"", park, ""/"", park,""_figure1.pdf"", sep=""""), width=7, height=8)\r\n\r\npar(fig=c(0,1,0,1))\r\n#par(mar=c(5.1, 4.1, 4.1, 8.1), xpd=TRUE)\r\n\r\nbarplot(counts2, ylim=c(0,all_high), xlab=""Taxa"", col=c(""cadetblue1"", ""seashell"",""lightsalmon"",""mediumorchid4""), \r\n        ylab=""Number of Species"")\r\n\r\nlegend(""top"", inset=0.02, legend = rownames(counts2) , fill = c(""cadetblue1"", ""seashell"",""lightsalmon"",""mediumorchid4""))\r\n\r\n#par(fig=c(0.75,1,0.75,1), mar=c(1,1,1,1), new=TRUE)\r\npar(fig=c(0.1,0.5,0.5,1), new=TRUE)\r\nbarplot(counts3, ylim=c(0,sub_high),col=c(""cadetblue1"", ""seashell"",""lightsalmon"", ""mediumorchid4""), cex.names=0.45)\r\n\r\n\r\n\r\ndev.off()\r\n\r\n#no inset\r\npdf(paste(""./"", park, ""/"", park,""_figure1_no_inset.pdf"", sep=""""), width=7, height=8)\r\n\r\npar(fig=c(0,1,0,1))\r\n\r\nbarplot(counts2, ylim=c(0,all_high), xlab=""Taxa"", col=c(""cadetblue1"", ""seashell"",""lightsalmon"", ""mediumorchid4""), \r\n        ylab=""Number of Species"")\r\n\r\nlegend(""topleft"", inset=c(0.02), legend = rownames(counts2) , fill = c(""cadetblue1"", ""seashell"",""lightsalmon"",""mediumorchid4""))\r\n\r\ndev.off()\r\n\r\n\r\n#13---------------------------------------------------------------------------------\r\n#Remove tables/variables when running multiple reports in a row\r\nrm(data, data2, dfcounts, iNat_data, NPS_data, NPS_synonyms, placeholderdf, placeholderdf2, \r\n   RG_iNat_data, RG_iNat_data_iconic, RG_iNat_not_iconic, table1, table1a, table2, table3,\r\n   table4, all_high, amphibiasum, avessum, counts2, counts3, i, iNat_ITIS_nonsyn_matches, \r\n   iNat_NPSsynonym_matches, iNat_NPS_matches, iNat_NPSsynonym_matches, mammaliasum,\r\n   NPS_data2, NPS_data3, park, plantaesum, reptiliasum, RG_iNat_data2, sub_high, \r\n   iNat_ITIS_synonym_matches, out, state.counts, k, states)\r\n\r\n\r\n\r\n\r\n']","Biodiversity on public lands: how citizen science can help These are the code and data files for ""Biodiversity on public lands: how citizen science can help"". Included in this are the following:1. taxataxi_opt_multiple_reports.R which is code for running multiple iNaturalist Bioblitz datasets with built in re-setting mechanisms for the various reports run.2. iNat_NPS_comparison_code.R which is code for running a single iNaturalist Bioblitz dataset. This is identical to (1) except for the re-setting mechanisms and some re-organization of parts of the code.3. synonym_links & taxonomic_units which are used in conjunction with (1) & (2) for the synonym identification.4. All_iNat.csv is all of the iNaturalist entries used in the initial study named above.5. all_parks.csv contains all of the filtered data from the park units through (1) for the initial study named above.6. invasives_for_all_table1.R is code used in the initial study to identify introduced species.7. migration.R is code used in the initial study to identify previously unrecorded species that were recorded less than 50 times in BISON in the USA state that the park unit is located in.8. table3_all_parks.csv is the data used in the migration.R code for identifying these species.",4
"From nature reserve to mosaic management: improving matrix survival, not permeability, benefits regional populations under habitat loss and fragmentation","Although matrix improvement in fragmented landscapes is a promising conservation measure, matrix permeability (willingness of an organism to enter the matrix) and movement survival in the matrix are usually aggregated. Consequently, it is unknown which matrix property needs to be improved. It also remains unclear whether matrix upgrading from dispersal passage to providing reproduction opportunities has large conservation benefits and whether there are interactive effects between habitat and matrix management.We examined matrix effects on regional populations across a gradient of habitat loss and fragmentation using simulation experiments that integrated demographic processes and movement modeling based on circuit theory. We separately modified the levels of matrix permeability and movement survival to evaluate their individual effects. We also altered the amount and configuration of not only habitat but also improved matrix to assess their effects on population vital rates (size, survival and density).In binary landscapes comprising habitat and unimproved matrix, matrix movement survival had larger effects on population vital rates than matrix permeability. Increasing movement survival increased vital rates, yet, increasing matrix permeability decreased vital rates. Increased permeability required corresponding increased movement survival to offset potential negative population outcomes.When subsets of the matrix functioning as dispersal passage only (where no reproduction opportunities existed) were improved, increasing matrix permeability but holding movement survival constant reduced all vital rates, especially with increasing habitat fragmentation. In contrast, when movement survival increased, vital rates increased given strong habitat fragmentation. The benefits of upgrading dispersal passage to provide reproduction opportunities for population survival were greatest when habitat amount was moderate. We also found synergetic effects between amounts of habitat and improved matrix, and the benefits of matrix improvement were promoted when improvement was achieved in a spatially aggregated manner.Synthesis and applications: Matrix improvement and connectivity modeling aimed at increasing movement survival will likely bring larger conservation benefits than those for improving permeability alone. Buffering and connecting habitat remnants with improved matrix could provide benefits as long as movement survival is increased. Simultaneous implementation of habitat management and matrix improvement would yield synergistic conservation benefits.","['# our developed model is executed by running the following function ""simcode"".\r\n# in our study, parameters held constant were specified within the function,\r\n# but the varied parameters were specified and supplied to the function like: simcode(cover=0.1,...,). \r\n\r\nlibrary(raster)\r\nsimcode <- function(\r\n  # landscape stuff               \r\n  cover, # habitat amount (or proportion: HP)\r\n  frag,  # habitat fragmentation (FRAG)\r\n  # background (low-quality) matrix stuff\r\n  matrix.move.prob, # movement probability (permeability) of low-quality matrix\r\n  matrix.move.survival, # movement survival of low-quality matrix\r\n  # high quality matrix stuff  \r\n  hqm_cover, # proportion of high-quality matrix\r\n  hqm_frag,  # fragmentation of high-quality matrix\r\n  improv.growth.rate, # productivity of high-quality matrix\r\n  improv.move.prob,   # permeability of high-quality matrix\r\n  improv.move.survival.unsettle     # when high-quality (improved) matrix works as \'dispersal passage\' and \'population sink\', this variable dictates movement survival and partial settlement into improved matrix, respectively\r\n){\r\n  # set-up parameters\r\n  nyear              = 2000         # number of simulation years\r\n  size               = 50           # length of one side (in cell number)\r\n  # demographic & movement stuff of habitat\r\n  growth.rate        = 0.1          # reproductive rate of habitat\r\n  mortal.rate        = 0.05         # annual mortality of habitat\r\n  emigr.rate         = 0.1          # emigration rate (whether individual depart from the habitat cell) \r\n  habita.move.prob   = 1            # movement probability (permeability) of habitat cell\r\n  # demographic & movement stuff of high quality matrix\r\n  improv.mortal.rate = 0.1\t\t\t# annual mortality of high-quality matrix when it supports population sink\r\n  improv.emigr.rate  = 0.5\t\t\t# emigration rate of high-quality matrix when it supports population sink\r\n  # converting survival to mortality\r\n  matrix.move.mortality        <- 1 - matrix.move.survival\r\n  improv.move.mortality.settle <- 1 - improv.move.survival.unsettle\r\n  \r\n  # other parameters of focal landscape\r\n  ncol  <- nrow <- size             # landscape size\r\n  nhabi <- cover*ncol*nrow          # number of habitat cell\r\n  nhqm  <- hqm_cover*ncol*nrow      # number of high-quality matrix cell\r\n  \r\n  # landscape generation (for habitat)\r\n  # cover (habitat) type code\r\n  hcode <- c(0,10,100) # background low-quality matrix, high-quality matrix, and (original) habitat\r\n  \r\n  # landscape: one additional row and column are added for a practical reason (will be deleted finally)\r\n  land <- matrix(0,ncol+1,nrow+1)\r\n  \r\n  while(sum(land)<nhabi*hcode[3]){ # original habitat will continue to be generated until its total area is met\r\n    # select random cell by referring their coordinates\r\n    # coordinates are designed to be between 1 to ncol or nrow (\'zero\' is not included)\r\n    # note: since as.integer make non-integral values truncated towards zero, \'+1\' is required: check -> table(as.integer(runif(1000,min=1,max=(10+1))))\r\n    x.land <- as.integer(runif(1,min=1,max=(ncol+1))) \r\n    y.land <- as.integer(runif(1,min=1,max=(nrow+1)))\r\n    if(land[x.land,y.land]==hcode[1]){ # if the selected cell is already designated as habitat, do nothing (= else{})\r\n      if(runif(1) < frag){ # habitat configuration: following Fahrig\'s methodology\r\n        land[x.land,y.land] <- hcode[3]\r\n      } else{\r\n        # examine surrounding cells: \'zero\' row and column numbers can be specified here (but row/column+1 cannot be specified)\r\n        surround <- land[c(x.land-1,x.land,x.land+1),c(y.land-1,y.land,y.land+1)]\r\n        if(sum(surround) >= hcode[3]){\r\n          land[x.land,y.land] <- hcode[3]\r\n        } else{}\r\n      }\r\n      # print(sum(land)/hcode[3])\r\n    } else{}\r\n  }\r\n  \r\n  land <- land[-c(nrow+1),-c(ncol+1)]\r\n  land <- raster(land)\r\n  # plot(land,las=1) # check the status visually if needed\r\n  \r\n  # write & read\r\n  writeRaster(land,paste0(""land_"",cover,""_"",frag,""_.asc""),overwrite=TRUE)\r\n  land <- raster(paste0(""land_"",cover,""_"",frag,""_.asc""))\r\n  # plot(land,las=1,main=paste0(""HABITAT = "",cover,"", FRAG = "",frag))\r\n  land <- as.matrix(land)\r\n  \r\n  #############################\r\n  # landscape generation (for high-quality [improved] matrix)\r\n  # adding one row and column with \'zero\' values for the practical reason\r\n  land <- rbind(land,rep(0,dim(land)[1]))\r\n  land <- cbind(land,rep(0,dim(land)[1]))\r\n  \r\n  while(sum(land)<(nhabi*hcode[3]+nhqm*hcode[2])){ # high quality matrix will continue to be generated until its total area is met\r\n    x.land <- as.integer(runif(1,min=1,max=(ncol+1))) \r\n    y.land <- as.integer(runif(1,min=1,max=(nrow+1)))\r\n    if(land[x.land,y.land]==hcode[1]){\r\n      if(runif(1) < hqm_frag){\r\n        land[x.land,y.land] <- hcode[2]\r\n      } else{\r\n        surround <- land[c(x.land-1,x.land,x.land+1),c(y.land-1,y.land,y.land+1)]\r\n        if(sum(surround) >= hcode[2]){ # the case when original habitat (hcode[3] = 100) exists nearby is included\r\n          land[x.land,y.land] <- hcode[2]\r\n        } else{}\r\n      }\r\n      # print((sum(land)-nhabi*hcode[3])/hcode[2])\r\n    } else{}\r\n  }\r\n  \r\n  land <- land[-c(nrow+1),-c(ncol+1)]\r\n  table(land) # check whether the number of every habitat type is equal to the intended number\r\n  land <- raster(land)\r\n  #  plot(land,las=1,main=paste0(""HP="",cover,"", FRAG="",frag,"", HQM="",hqm_cover,"", HQMF="",hqm_frag))\r\n  # write (& read)\r\n  writeRaster(land,paste0(""land_"",cover,""_"",frag,""_"",hqm_cover,""_"",hqm_frag,""_.asc""), overwrite=TRUE)\r\n  ############################# end of landscape generation  \r\n  \r\n  #############################\r\n  # dispersal & population dynamics\r\n  # given parameters\r\n  ini.pop     <- 10     # initial population size for every habitat cell\r\n  Ceiling     <- 10     # ceiling of local population size (per cell)\r\n  # reading landscape\r\n  land <- raster(paste0(""land_"",cover,""_"",frag,""_"",hqm_cover,""_"",hqm_frag,""_.asc""))\r\n  # plot(land,las=1,main=paste0(""HABITAT = "",cover,"", FRAG = "",frag,"", HQM = "",hqm_cover,"", HQMFRAG = "",hqm_frag))\r\n  land <- as.matrix(land)\r\n  \r\n  #############################\r\n  # dispersal\r\n  # input var.\r\n  matrix.move.resi <- 1/matrix.move.prob\r\n  habita.move.resi <- 1/habita.move.prob\r\n  improv.move.resi <- 1/improv.move.prob\r\n  inj.curr  <- 100 # 100 ampere is injected to every habitat cell to assess possible movement destination\r\n\r\n  # coordinates of original habitat & high-quality matrix\r\n  table(land)\r\n  x.coord    <- which(land==hcode[3],arr.ind=TRUE)[,""row""]\r\n  y.coord    <- which(land==hcode[3],arr.ind=TRUE)[,""col""]\r\n  hab.coord  <- rbind(x.coord,y.coord)\r\n  im.x.coord <- which(land==hcode[2],arr.ind=TRUE)[,""row""]\r\n  im.y.coord <- which(land==hcode[2],arr.ind=TRUE)[,""col""]\r\n  im.coord   <- rbind(im.x.coord,im.y.coord)\r\n  hab.im.x.coord <- which((land==hcode[3]|land==hcode[2]),arr.ind=TRUE)[,""row""]\r\n  hab.im.y.coord <- which((land==hcode[3]|land==hcode[2]),arr.ind=TRUE)[,""col""]\r\n  hab.im.coord <- rbind(hab.im.x.coord,hab.im.y.coord)\r\n  im.mat.x.coord <- which((land==hcode[1]|land==hcode[2]),arr.ind=TRUE)[,""row""]\r\n  im.mat.y.coord <- which((land==hcode[1]|land==hcode[2]),arr.ind=TRUE)[,""col""]\r\n  im.mat.coord <- rbind(im.mat.x.coord,im.mat.y.coord)\r\n  \r\n  # movement resistance\r\n  resist <- matrix(matrix.move.resi,nrow=nrow,ncol=ncol)\r\n  resist[land==hcode[3]] <- habita.move.resi # original habitat\r\n  resist[land==hcode[2]] <- improv.move.resi # high-quality matrix\r\n  Resist <- resist\r\n  resist <- raster(resist,xmn=1,ymn=1,xmx=ncol,ymx=nrow)\r\n  #  plot(resist,las=1,main=""Resistance"")\r\n\r\n  # ground resistor is used to designate possible final destination (habitat cells other than source cell), movement mortality and partial settlement\r\n  # a common ground resistor grid is created; specific ground resistor for every iteration of the Circuitscape would be newly created by modifying the common one (in the for loop)\r\n  common.ground <- matrix(NA,nrow,ncol)\r\n  common.ground[Resist==habita.move.resi]            <- 0                # assign zero movement mortality into source habitat, which means direct ground connection (R = 0) (i.e., all individuals reaching the cells settle into the cells and reproduce there)\r\n  # specify the values of exiting resistance by looking at surrounding movement resistance (since exiting flow depends not only on exiting resistance of the exiting cells but also movement resistance of the surrounding cells)\r\n  # since the values of Resist[,51] and Resist[51,] (i.e., at margin) cannot be assessed, resistance matrix is expanded (but Resist[0,0] can be assessed)\r\n  ExpandResist <- cbind(Resist,rep(NA,dim(Resist)[2]))\r\n  ExpandResist <- rbind(ExpandResist,rep(NA,dim(ExpandResist)[2]))\r\n  for(i in 1:dim(im.mat.coord)[2]){\r\n    if(land[im.mat.coord[1,i],im.mat.coord[2,i]]==hcode[1]){ # movement mortality depends on whether the cell is the background low-quality matrix or improved high-quality matrix\r\n      move.mortality <- matrix.move.mortality\r\n    } else{\r\n      move.mortality <- improv.move.mortality.settle\r\n    }\r\n    common.ground[im.mat.coord[1,i],im.mat.coord[2,i]] <- ((1-move.mortality)/move.mortality)/(\r\n      # four cardinals\r\n      sum(\r\n          1/ExpandResist[im.mat.coord[1,i]-1,im.mat.coord[2,i]],\r\n          1/ExpandResist[im.mat.coord[1,i],  im.mat.coord[2,i]-1],\r\n          1/ExpandResist[im.mat.coord[1,i]+1,im.mat.coord[2,i]],\r\n          1/ExpandResist[im.mat.coord[1,i],  im.mat.coord[2,i]+1],na.rm=T\r\n      ) + \r\n        # then, four diagonals\r\n        (1/sqrt(2))*(\r\n          sum(\r\n              1/ExpandResist[im.mat.coord[1,i]-1,im.mat.coord[2,i]-1],\r\n              1/ExpandResist[im.mat.coord[1,i]+1,im.mat.coord[2,i]-1],\r\n              1/ExpandResist[im.mat.coord[1,i]+1,im.mat.coord[2,i]+1],\r\n              1/ExpandResist[im.mat.coord[1,i]-1,im.mat.coord[2,i]+1],na.rm=T\r\n          )\r\n        )\r\n    )\r\n  }\r\n  # plot(raster(common.ground))\r\n\r\n  if(improv.growth.rate==0){  # when high-quality matrix does not allow the reproduction, only dispersals from original habitat cells are considered below\r\n    hab.im.coord <- hab.coord\r\n  } else{}\r\n\r\n  # from making source & ground rasters to running Circuitscape, and retrieve dispersal success\r\n  # scenario 10 produces large numbers of output files, which can be larger than the upper limit of the number of files in the single folder\r\n  # therefore, the series of the following procedure is to be divided into five parts to lower the number of files\r\n  # first, divide the reproductive cells into five groups\r\n  group  <- seq(1,dim(hab.im.coord)[2])\r\n  group1 <- seq(1,c(length(group)/5))    # denominator 5 should work in typical cases since the cell size is 50 \r\n  group2 <- seq(tail(group1,1)+1,c(length(group)/5*2))\r\n  group3 <- seq(tail(group2,1)+1,c(length(group)/5*3))\r\n  group4 <- seq(tail(group3,1)+1,c(length(group)/5*4))\r\n  group5 <- seq(tail(group4,1)+1,c(length(group)/5*5))\r\n  all(group==c(group1,group2,group3,group4,group5)) # check the division\r\n  \r\n  nsite <- dim(hab.im.coord)[2] # number of possible reproduction cells (including source habitat and population sink)\r\n  # dispersal success (number of immigrants)\r\n  disp.succ <- matrix(NA,nsite,nsite) # row = source (departure or injected) cell; column = recipient cell\r\n  \r\n  # since the same procedure would be repeated five times, the procedure is to be wrapped as a function\r\n  RasterCircuitDisSucc <- function(GROUP){\r\n    for(i in GROUP){ # when high-quality matrix does allow the reproduction, this includes high-quality matrix since it can support potential emigrants\r\n      # current is injected into every reproductive cell to assess possible movement from it (injected cell is called \'source\')\r\n      source <- matrix(NA,nrow,ncol)\r\n      source[hab.im.coord[1,i],hab.im.coord[2,i]] <- inj.curr\r\n      source <- raster(source,xmn=1,ymn=1,xmx=ncol,ymx=nrow)\r\n      # plot(source,las=1)\r\n      # common ground is customized to consider that injected cell should have \'NA\' ground resistance\r\n      ground <- common.ground\r\n      ground[hab.im.coord[1,i],hab.im.coord[2,i]] <- NA  # injected cell from which dispersers come (can comprise source & population sink) should have \'NA\' resistance\r\n      ground <- raster(ground,xmn=1,ymn=1,xmx=ncol,ymx=nrow)\r\n      # plot(ground,las=1)\r\n      # set projection (Circuitscape requires rasters to be projected)\r\n      projection(resist) <- projection(source) <- projection(ground)  <- ""+init=epsg:3100""\r\n      # save raster files\r\n      writeRaster(resist,""resistance.asc"",overwrite=TRUE)\r\n      writeRaster(source,paste0(""source_"",i,""_.asc""),overwrite=TRUE)\r\n      writeRaster(ground,paste0(""ground_"",i,""_.asc""),overwrite=TRUE)\r\n    } # produce the bunch of raster files for each group\r\n    \r\n    ##################\r\n    # run Circuitscape\r\n    for(i in GROUP){ # when high-quality matrix does allow the reproduction, this includes high-quality matrix since it can support potential emigrants\r\n      # settings and options\r\n      cs_ini <- c(""[circuitscape options]"",""data_type = raster"",""scenario = advanced"",""write_cur_maps = 1"",""write_volt_maps = 1"",\r\n                  paste(c(""ground_file ="",""source_file ="",""habitat_file ="",""output_file =""),\r\n                        paste(getwd(),c(\r\n                          paste0(""ground_"",i,""_.asc""),\r\n                          paste0(""source_"",i,""_.asc""),\r\n                          ""resistance.asc"",\r\n                          paste0(""flow_"",i,"".out"")\r\n                        ),sep=""/"")))\r\n      writeLines(cs_ini,paste0(""circuitini_"",i,"".ini""))\r\n    }\r\n    # Circuitscape is executed: specific commands depend on the computation environment\r\n    # for Windows environment, something like as follows:\r\n    cs_exe <- \'C:/""Program Files""/Circuitscape/cs_run.exe\'\r\n    for(i in GROUP){\r\n      wd <- getwd()\r\n      cs_run <- paste(cs_exe,paste(getwd(),paste0(""circuitini_"",i,"".ini""),sep=""/""))\r\n      system(cs_run)\r\n    }\r\n    # end of Circuitscape\r\n \r\n    # retrieve dispersal success\r\n    # note: dispersal success of source habitat (with \'zero\' ground resistance) and population sink (with \'nonzero\' ground resistance) is assessed by current and voltage map, respectively \r\n    for(i in GROUP){\r\n      cur <- raster(paste0(""flow_"",i,""_curmap.asc""))  # current will always take values >=0\r\n      # plot(cur,las=1,main=paste0(""Current (from "",i,"")""))\r\n      # text(cur)\r\n      cur <- as.matrix(cur)\r\n      vol <- raster(paste0(""flow_"",i,""_voltmap.asc"")) # voltage will always take values >=0\r\n      # plot(vol,las=1,main=paste0(""Voltage (from "",i,"")""))\r\n      # text(vol)\r\n      vol <- as.matrix(vol)\r\n      grnd <- raster(paste0(""ground_"",i,""_.asc""))\r\n      # plot(grnd,las=1,main=paste0(""Ground (from "",i,"")"")) # ground will take values of NA (injected cell), 0 (possible recipient cell [source habitat]) or >=0 (low- or high-quality matrix)\r\n      # text(grnd)\r\n      grnd <- as.matrix(grnd)\r\n      exitcur <- vol/grnd     # exiting current from ground resistor with \'non-zero\' resistance\r\n      # exitcur <- raster(exitcur) # NA (exiting resistance = NA), NaN (exiting resistance = 0 - original habitat)\r\n      # plot(exitcur,las=1,main=paste0(""Exiting current (from "",i,"")""))\r\n      # text(exitcur)\r\n      \r\n      for(j in 1:nsite){\r\n        # if target cell is original habitat (hcode[3]) and high-quality matrix (hcode[2]), retrieve the values from entering current (\'current\' output file) and exiting current from ground resistor (voltage/ground), respectively \r\n        if(land[hab.im.coord[1,j],hab.im.coord[2,j]]==hcode[3]){\r\n          disp.succ[i,j] <<- cur[hab.im.coord[1,j],hab.im.coord[2,j]]\r\n        } else{\r\n          if(land[hab.im.coord[1,j],hab.im.coord[2,j]]==hcode[2]){ # when HQ matrix does not and does allow reproduction, exiting current represents movement mortality and proportion of settling individuals, respectively\r\n            disp.succ[i,j] <<- exitcur[hab.im.coord[1,j],hab.im.coord[2,j]]\r\n          } else{}\r\n        } \r\n      }\r\n      disp.succ[i,i] <<- cur[hab.im.coord[1,i],hab.im.coord[2,i]] # diagonal element should be injected current (exiting current would yield \'NA\' since ground target cell has \'NA\' resistance)\r\n    }  \r\n    \r\n    # here, source, ground, and flow files are to be deleted to reduce the number of output files\r\n    # list-up all files\r\n    files <- list.files()\r\n    # list-up files with \'_.asc\' (ground & source asc files), \'curmap.asc\', \'voltmap.asc\', \'.ini\' or \'.out\'\r\n    gs.files     <- grep(""\\\\_.asc$"",       files)\r\n    cur.files    <- grep(""\\\\curmap.asc$"",  files)\r\n    volt.files   <- grep(""\\\\voltmap.asc$"", files)\r\n    ini.files    <- grep(""\\\\.ini$"",        files)\r\n    out.files    <- grep(""\\\\.out$"",        files)\r\n    # remove these files\r\n    file.remove(files[gs.files])\r\n    file.remove(files[cur.files])\r\n    file.remove(files[volt.files])\r\n    file.remove(files[ini.files])\r\n    file.remove(files[out.files])\r\n  }\r\n  \r\n  # repeat the above procedure\r\n  RasterCircuitDisSucc(group1)\r\n  RasterCircuitDisSucc(group2)\r\n  RasterCircuitDisSucc(group3)\r\n  RasterCircuitDisSucc(group4)\r\n  RasterCircuitDisSucc(group5)\r\n\r\n  # # making plots to check individual movements\r\n  #  pdf(file=""flow.pdf"")\r\n  #  plot(raster(land),las=1,main=paste0(""HABITAT = "",cover,"", FRAG = "",frag,"", HQM = "",hqm_cover,"", HQMFRAG = "",hqm_frag))\r\n  #  plot(resist,las=1,main=""Resistance"")\r\n  #  for(i in 1:nsite){\r\n  #    cur <- raster(paste0(""flow_"",i,""_curmap.asc""));  cur <- as.matrix(cur); flow <- cur\r\n  #    vol <- raster(paste0(""flow_"",i,""_voltmap.asc"")); vol <- as.matrix(vol)\r\n  #    grnd <- raster(paste0(""ground_"",i,""_.asc""));     grnd <- as.matrix(grnd)\r\n  #    exitcur <- vol/grnd; exitcur <- as.matrix(exitcur)\r\n  #    flow[Resist==matrix.move.resi] <- exitcur[Resist==matrix.move.resi]\r\n  #    flow[Resist==improv.move.resi] <- exitcur[Resist==improv.move.resi]\r\n  #    flow[hab.im.coord[1,i],hab.im.coord[2,i]] <- cur[hab.im.coord[1,i],hab.im.coord[2,i]] # because HQ matrix can yield NA value\r\n  #    Nflow <- sum(flow,na.rm=T)\r\n  #    plot(raster(flow),las=1,main=paste0(""Current flow (from "",i,"": sum = "",round(Nflow,0),"")""))\r\n  #    if(improv.growth.rate==0){ # when HQ matrix does not allow reproduction, settlement only occurs in original habitat\r\n  #      settle <- flow; settle[Resist!=habita.move.resi] <- NA # excluding low & high quality matrix\r\n  #      text(raster(settle))\r\n  #      text(raster(exitcur),col=""red"")\r\n  #    } else{ # when HQ matrix does allow reproduction, settlement occurs in original habitat as well as HQ matrix\r\n  #      settle <- flow; settle[Resist==matrix.move.resi] <- NA # excluding low quality matrix\r\n  #      text(raster(settle))\r\n  #      exitcur[Resist==improv.move.resi] <- NA\r\n  #      text(raster(exitcur),col=""red"")\r\n  #    }\r\n  #  }\r\n  #  dev.off()\r\n  \r\n  # mathematical matrix dictating dispersal success from each OR habitat & HQ matrix cell to the other OR habitat & HQ matrix cells\r\n  disp.mort <- 1 - (apply(disp.succ,1,sum)-inj.curr)/inj.curr\r\n  disp.mort <- ifelse(disp.mort<0,0,disp.mort) # when improv.move.mortality.settle = 0, minus mortality (but ~0) can occur and the multinomial distribution fails. minus mortality is replaced by zero\r\n  disp.succ <- disp.succ/inj.curr; diag(disp.succ) <- disp.mort\r\n  disp.succ <- ifelse(disp.succ<0,0,disp.succ) # sink to sink can also yield minus mortality\r\n  \r\n  #############################\r\n  # population dynamics\r\n  # habitat type of each original habitat or high quality matrix cells\r\n  hab.type <- rep(NA,nsite) \r\n  for(i in 1:nsite){\r\n    hab.type[i] <- land[hab.im.coord[1,i],hab.im.coord[2,i]]\r\n  }\r\n  \r\n  # number of individuals in each OR habitat & HQ matrix cell\r\n  N <- matrix(NA,nsite,nyear) \r\n  N[hab.type==hcode[3],1] <- ini.pop # original habitat has individuals\r\n  N[hab.type==hcode[2],1] <- ini.pop # high-quality matrix also supports individuals\r\n  \r\n  # organize demographic parameters\r\n  hab.im.growth.rate <- rep(NA,nsite)\r\n  hab.im.growth.rate[hab.type==hcode[3]] <- growth.rate        \r\n  hab.im.growth.rate[hab.type==hcode[2]] <- improv.growth.rate\r\n  hab.im.emigr.rate <- rep(NA,nsite)\r\n  hab.im.emigr.rate[hab.type==hcode[3]] <- emigr.rate\r\n  hab.im.emigr.rate[hab.type==hcode[2]] <- improv.emigr.rate\r\n  hab.im.mortal.rate <- rep(NA,nsite)\r\n  hab.im.mortal.rate[hab.type==hcode[3]] <- mortal.rate\r\n  hab.im.mortal.rate[hab.type==hcode[2]] <- improv.mortal.rate\r\n  \r\n  for(i in 1:(nyear-1)){\r\n    \r\n    # reproduction\r\n    expect_grow <- N[,i]*hab.im.growth.rate\r\n    recruit <- rpois(nsite,expect_grow)\r\n    Ncombine <- N[,i] + recruit\r\n    \r\n    # natural death (winter/annual mortality)\r\n    Nsurvive <- rbinom(n=nsite,size=Ncombine,prob=c(1-hab.im.mortal.rate))\r\n    \r\n    # dispersal\r\n    Nceiling <- Nsurvive\r\n    Nceiling[Nceiling>Ceiling] <- Ceiling  # ceiling of each habitat cell\r\n    Nsurplus <- Nsurvive - Nceiling        # density-dependent/active dispersal fraction (surplus)\r\n    \r\n    Ndisp <- rbinom(n=nsite,size=Nceiling,hab.im.emigr.rate)  # intrinsic/density-independent/passive dispersal\r\n    Nremain <- Nceiling - Ndisp\r\n    Ndisp <- Ndisp + Nsurplus  # total dispersal fraction\r\n    \r\n    # dispersal\r\n    NsuccD <- matrix(NA,nsite,nsite)\r\n    for(j in 1:nsite){\r\n      NsuccD[j,] <- rmultinom(n=1,size=Ndisp[j],prob=disp.succ[j,])\r\n    }\r\n    NdeadD <- diag(NsuccD)\r\n    diag(NsuccD) <- 0\r\n    N[,i+1] <- Nremain + apply(NsuccD,2,sum)\r\n    \r\n    # overabundance mortality\r\n    N[N[,i+1]>Ceiling,i+1] <- Ceiling\r\n  }\r\n  \r\n  # pdf(file=""dynamics.pdf"")\r\n  # plot(N[1,]~seq(1,nyear),ylim=c(0,max(N)),\r\n  #      type=""l"",las=1,xlab=""Year"",ylab=""N. of individuals in each patch"",\r\n  #      main=""Population dynamics"")\r\n  # for(i in 2:nsite){\r\n  #   lines(N[i,]~seq(1,nyear),col=i)\r\n  # }\r\n  # SumN <- apply(N,2,sum); SumN <- (max(N)/max(SumN))*SumN\r\n  # lines(SumN~seq(1,nyear),lwd=3,col=""grey"")\r\n  # legend(""topright"",lwd=c(1,3),lty=1,col=c(""black"",""grey""),legend=c(""Each"",""Total""))\r\n  # dev.off()\r\n  \r\n  # removing raster and Circuitscape files to save the memory\r\n  # list-up all files\r\n  files <- list.files()\r\n  # list-up files with \'.asc\'\r\n  asc.files <- grep(""\\\\.asc$"", files)\r\n  # remove these files\r\n  file.remove(files[asc.files])\r\n  \r\n  # saving parameters\r\n  # background parameters\r\n  write.table(size,                          ""size.csv"",                          row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(cover,                         ""cover.csv"",                         row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(frag,                          ""frag.csv"",                          row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(hqm_cover,                     ""hqm_cover.csv"",                     row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(hqm_frag,                      ""hqm_frag.csv"",                      row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(matrix.move.prob,              ""matrix.move.prob.csv"",              row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(habita.move.prob,              ""habita.move.prob.csv"",              row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(improv.move.prob,              ""improv.move.prob.csv"",              row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(matrix.move.survival,          ""matrix.move.survival.csv"",          row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(improv.move.survival.unsettle, ""improv.move.survival.unsettle.csv"", row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(growth.rate,                   ""growth.rate.csv"",                   row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(emigr.rate,                    ""emigr.rate.csv"",                    row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(mortal.rate,                   ""mortal.rate.csv"",                   row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(improv.growth.rate,            ""improv.growth.rate.csv"",            row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(improv.emigr.rate,             ""improv.emigr.rate.csv"",             row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(improv.mortal.rate,            ""improv.mortal.rate.csv"",            row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  write.table(l,                             ""repl.csv"",                          row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  \r\n  # outputs\r\n  SumN <- apply(N,2,sum) # total population size\r\n  write.table(t(SumN),             ""SumN.csv"",              row.names=FALSE,col.names=FALSE,sep="","",append=TRUE)\r\n  \r\n}\r\n']","From nature reserve to mosaic management: improving matrix survival, not permeability, benefits regional populations under habitat loss and fragmentation Although matrix improvement in fragmented landscapes is a promising conservation measure, matrix permeability (willingness of an organism to enter the matrix) and movement survival in the matrix are usually aggregated. Consequently, it is unknown which matrix property needs to be improved. It also remains unclear whether matrix upgrading from dispersal passage to providing reproduction opportunities has large conservation benefits and whether there are interactive effects between habitat and matrix management.We examined matrix effects on regional populations across a gradient of habitat loss and fragmentation using simulation experiments that integrated demographic processes and movement modeling based on circuit theory. We separately modified the levels of matrix permeability and movement survival to evaluate their individual effects. We also altered the amount and configuration of not only habitat but also improved matrix to assess their effects on population vital rates (size, survival and density).In binary landscapes comprising habitat and unimproved matrix, matrix movement survival had larger effects on population vital rates than matrix permeability. Increasing movement survival increased vital rates, yet, increasing matrix permeability decreased vital rates. Increased permeability required corresponding increased movement survival to offset potential negative population outcomes.When subsets of the matrix functioning as dispersal passage only (where no reproduction opportunities existed) were improved, increasing matrix permeability but holding movement survival constant reduced all vital rates, especially with increasing habitat fragmentation. In contrast, when movement survival increased, vital rates increased given strong habitat fragmentation. The benefits of upgrading dispersal passage to provide reproduction opportunities for population survival were greatest when habitat amount was moderate. We also found synergetic effects between amounts of habitat and improved matrix, and the benefits of matrix improvement were promoted when improvement was achieved in a spatially aggregated manner.Synthesis and applications: Matrix improvement and connectivity modeling aimed at increasing movement survival will likely bring larger conservation benefits than those for improving permeability alone. Buffering and connecting habitat remnants with improved matrix could provide benefits as long as movement survival is increased. Simultaneous implementation of habitat management and matrix improvement would yield synergistic conservation benefits.",4
Batch Convert TRS to Lat/Long via Lookup Table,"This code uses a PLSS lookup table provided by Nelson Rios (Yale Unviersity; GEOLocate) to assign a latitude, longitude, and error radius (according to section) to TRS coordinates extracted from herbarium specimen records. Several patterns in the PLSS lookup table were added from Earth Point. Note that each version of the code is customized for a certain state.The script is not able to address the following circumstances:any information below the rank of section (it scrubs all quarter and half section data, etc. before converting to coordinates)when section is listed before the township and rangewhen multiple sections are listedwhen TRS and UTM data are concatenated into a single fieldwhen TRS data don't exist in the reference file (the reference file we used largely excludes sections that are not the standard size)when no section is provided","['####\r\n# Title: TRS Batch Conversion\r\n# Author: Katie Pearson\r\n# Date: February 12, 2021\r\n# Purpose: This script cleans the verbatimCoordinates field into a standardized TRS value\r\n# Then it compares the TRS value (with county) to a reference file and imports the\r\n# centroid of the section.\r\n# This version of the script is customized for California TRS data.\r\n\r\n### Several critical exceptions apply ###\r\n# The script is not able to address the following circumstances:\r\n# - any information below the rank of section (it scrubs all quarter, half, etc. data before converting to coordinates)\r\n# - when section is listed before the township and range\r\n# - when multiple sections are listed\r\n# - when TRS and UTM data are concatenated into a single field\r\n# - when TRS data don\'t exist in the reference file (the reference file we used largely excludes sections that are not the standard size)\r\n# - when no section is provided\r\n\r\n#load packages\r\nlibrary(DataCombine)\r\nlibrary(gdata)\r\nlibrary(stringr)\r\n\r\n#load your dataset that needs coordinates\r\ndat <- read.csv(""PATH"")\r\n\r\n#in my dataset, I first opened the file in Excel and used the function\r\n#=IF(ISERROR(SEARCH(""T"",[CELL])),"""",""pickme"")\r\n#to flag all the rows that could potentially contain TRS data\r\n#I then sorted the dataset to look at all the cells with ""pickme"" in that column.\r\n#From there, I manually flagged all data that had TRS data first\r\n#(not at the end of a cell also containing UTM values, due to the difficulty\r\n#of parsing digits from digits). These I flagged as ""x"" in a TRS column.\r\n#This line will limit my dataset to just those potentially ""easy"" targets.\r\nfdat <- subset(dat,dat$TRS==""x"")\r\n\r\n#make a new column to hold the scrubbed data\r\nfdat$trs_only <- fdat$verbatimCoordinates\r\n\r\n#make everything uppercase to avoid case redundancy\r\nfdat$trs_only <- toupper(fdat$trs_only)\r\n\r\n#scrub excess information from field and standardize\r\n#to township (T), range (R), and section (X)\r\ntofrom <- matrix(ncol = 2, nrow = 76)\r\ntofrom <- as.data.frame(tofrom)\r\ncolnames(tofrom) <- c(""from"",""to"")\r\ntofrom$from[1] <- ""\\\\.""\r\ntofrom$to[1] <- """"\r\ntofrom$from[2] <- ""SECTION ""\r\ntofrom$from[3] <- ""SEC ""\r\ntofrom$from[4] <- ""SECT ""\r\ntofrom$from[5] <- ""SEC""\r\ntofrom$to[2:5] <- ""X""\r\ntofrom$from[6] <- "" ""\r\ntofrom$from[7] <- ""M-CA""\r\ntofrom$from[8] <- ""MERIDIAN:MD""\r\ntofrom$from[9] <- "",""\r\ntofrom$from[10] <- "";""\r\ntofrom$from[11] <- ""NW1/4""\r\ntofrom$from[12] <- ""NE1/4""\r\ntofrom$from[13] <- ""SW1/4""\r\ntofrom$from[14] <- ""SE1/4""\r\ntofrom$from[15] <- ""N1/2""\r\ntofrom$from[16] <- ""S1/2""\r\ntofrom$from[17] <- ""OF""\r\ntofrom$to[6:17] <- """"\r\ntofrom$from[18] <- ""TWP""\r\ntofrom$to[18] <- ""T""\r\ntofrom$from[19] <- ""MT.DIABLOMERIDIAN""\r\ntofrom$from[20] <- ""NW/4""\r\ntofrom$from[21] <- ""NE/4""\r\ntofrom$from[22] <- ""SW/4""\r\ntofrom$from[23] <- ""SE/4""\r\ntofrom$from[24] <- ""N/2""\r\ntofrom$from[25] <- ""S/2""\r\ntofrom$from[26] <- ""TRS:""\r\ntofrom$from[27] <- ""NW1/16""\r\ntofrom$from[28] <- ""NE1/16""\r\ntofrom$from[29] <- ""SW1/16""\r\ntofrom$from[30] <- ""SE1/16""\r\ntofrom$from[31] <- ""NENE""\r\ntofrom$from[32] <- ""NENW""\r\ntofrom$from[33] <- ""NWNE""\r\ntofrom$from[34] <- ""NWNW""\r\ntofrom$from[35] <- ""SESE""\r\ntofrom$from[36] <- ""SWSW""\r\ntofrom$from[37] <- ""SESW""\r\ntofrom$from[38] <- ""SWSE""\r\ntofrom$from[39] <- ""NESE""\r\ntofrom$from[40] <- ""NWSW""\r\ntofrom$from[41] <- ""NESW""\r\ntofrom$from[42] <- ""NWSE""\r\ntofrom$from[43] <- ""SENE""\r\ntofrom$from[44] <- ""SWNW""\r\ntofrom$from[45] <- ""SENW""\r\ntofrom$from[46] <- ""SWNE""\r\ntofrom$from[47] <- ""W1/2""\r\ntofrom$from[48] <- ""W/2""\r\ntofrom$from[49] <- ""E1/2""\r\ntofrom$from[50] <- ""E/2""\r\ntofrom$from[51] <- ""W1/4""\r\ntofrom$from[52] <- ""N1/4""\r\ntofrom$from[53] <- ""E1/4""\r\ntofrom$from[54] <- ""S1/4""\r\ntofrom$to[19:54] <- """"\r\ntofrom$from[55] <- ""R ""\r\ntofrom$to[55] <- ""R""\r\ntofrom$from[56] <- ""T ""\r\ntofrom$to[56] <- ""T""\r\ntofrom$from[57] <- ""S ""\r\ntofrom$to[57] <- ""S""\r\ntofrom$from[58] <- "" N""\r\ntofrom$to[58] <- ""N""\r\ntofrom$from[59] <- "" S ""\r\ntofrom$to[59] <- ""S ""\r\ntofrom$from[60] <- ""SANBERNARDINOMERIDIAN ""\r\ntofrom$to[60] <- """"\r\ntofrom$from[61] <- ""NR""\r\ntofrom$to[61] <- ""N""\r\ntofrom$from[62] <- ""SR""\r\ntofrom$to[62] <- ""S""\r\ntofrom$from[63] <- ""WS""\r\ntofrom$to[63] <- ""W""\r\ntofrom$from[64] <- ""ES""\r\ntofrom$to[64] <- ""E""\r\ntofrom$from[65] <- ""NW""\r\ntofrom$from[66] <- ""SW""\r\ntofrom$from[67] <- ""NE""\r\ntofrom$from[68] <- ""SE""\r\ntofrom$from[69] <- ""/16""\r\ntofrom$from[70] <- ""/4""\r\ntofrom$from[71] <- ""/2""\r\ntofrom$from[72] <- ""AND""\r\ntofrom$from[73] <- ""EEDGE""\r\ntofrom$from[74] <- ""SEDGE""\r\ntofrom$from[75] <- ""NEDGE""\r\ntofrom$from[76] <- ""WEDGE""\r\ntofrom$to[65:76] <- """"\r\nfdat <- FindReplace(fdat, ""trs_only"", tofrom, from = ""from"", to = ""to"", exact = FALSE)\r\n\r\n# if the range is listed before the township, switch the order\r\nfor(i in 1:dim(fdat)[1]){\r\n  if(startsWith(fdat$trs_only[i],""R"")){\r\n    twp <- str_match(fdat$trs_only[i],""T(.*?)[NS]"")\r\n    twp <- twp[,1]\r\n    rng <- str_match(fdat$trs_only[i],""R(.*?)[WE]"")\r\n    rng <- rng[,1]\r\n    sec <- sub("".*X"","""",fdat$trs_only[i])\r\n    fdat$trs_only[i] <- paste(twp,rng,sec, sep = """")\r\n  }\r\n}\r\n\r\n# remove township (T), range (R), and section (X) delimiters\r\n# also remove leading zeros in front of section and range\r\ntofromfinal <- matrix(ncol = 2, nrow = 7)\r\ntofromfinal <- as.data.frame(tofromfinal)\r\ncolnames(tofromfinal) <- c(""from"",""to"")\r\ntofromfinal$from[1] <- ""T""\r\ntofromfinal$from[2] <- ""R""\r\ntofromfinal$from[3] <- ""X""\r\ntofromfinal$to[1:3] <- """"\r\ntofromfinal$from[4] <- ""N0""\r\ntofromfinal$to[4] <- ""N""\r\ntofromfinal$from[5] <- ""S0""\r\ntofromfinal$to[5] <- ""S""\r\ntofromfinal$from[6] <- ""E0""\r\ntofromfinal$to[6] <- ""E""\r\ntofromfinal$from[7] <- ""W0""\r\ntofromfinal$to[7] <- ""W""\r\nfdat <- FindReplace(fdat, ""trs_only"", tofromfinal, from = ""from"", to = ""to"", exact = FALSE)\r\n\r\n#remove some other last-minute issues (California-specific)\r\ntofrommer <- matrix(ncol = 2, nrow = 3)\r\ntofrommer <- as.data.frame(tofrommer)\r\ncolnames(tofrommer) <- c(""from"",""to"")\r\ntofrommer$from[1] <- ""MDIABLOMEIDIAN""\r\ntofrommer$from[2] <- ""SANBENADINOMEIDIAN""\r\ntofrommer$from[3] <- ""HUMBOLDMEIDIAN""\r\ntofrommer$to[1:3] <- """"\r\nfdat <- FindReplace(fdat, ""trs_only"", tofrommer, from = ""from"", to = ""to"", exact = FALSE)\r\n\r\n#remove leading zeros\r\nfor(i in 1:dim(fdat)[1]){\r\n  if(startsWith(fdat$trs_only[i],""0"")){\r\n    fdat$trs_only[i] <- sub(""."", """", fdat$trs_only[i])\r\n  }\r\n}\r\n\r\n###Part 2: Match TRS values to lat/longs\r\n\r\n#load TRS data\r\n#these must include a latitude, longitude, TRS pattern (e.g., 1N13E12), and county field\r\ncentroids <- read.csv(""PATH"")\r\n\r\n#make TRS + county columns\r\ncentroids$trs_county <- as.character(paste(centroids$pattern,centroids$county, sep = "" ""))\r\ncentroids$trs_county <- toupper(centroids$trs_county)\r\nfdat$trs_county <- as.character(paste(fdat$trs_only,fdat$county, sep = "" ""))\r\nfdat$trs_county <- toupper(fdat$trs_county)\r\n\r\n#remove instances of ""county"" and ""co."" from the county field\r\ntofromco <- matrix(ncol = 2, nrow = 2)\r\ntofromco <- as.data.frame(tofromco)\r\ncolnames(tofromco) <- c(""from"",""to"")\r\ntofromco$from[1] <- "" COUNTY""\r\ntofromco$from[2] <- "" CO\\\\.""\r\ntofromco$to[1:2] <- """"\r\nfdat <- FindReplace(fdat, ""trs_county"", tofromco, from = ""from"", to = ""to"", exact = FALSE)\r\n\r\n#add columns for the interpreted (int) data\r\nfdat$decimalLatitude_int <- NA\r\nfdat$decimalLongitude_int <- NA\r\nfdat$coordinateUncertaintyInMeters_int <- NA\r\nfdat$georeferencedBy_int <- NA\r\nfdat$georeferenceRemarks_int <- NA\r\nfdat$georeferenceProtocol_int <- NA\r\nfdat$georeferenceSources_int <- NA\r\nfdat$flag <- NA\r\n\r\n#find the coordinate data!\r\n#you will want to fill in the appropriate information for fields listed below\r\n#specifically, georeferencedBy and georeferenceRemarks\r\nfor(i in 1:dim(fdat)[1]){\r\n  #find the cell in the centroids file that matches the TRS + county pattern\r\n  m <- match(fdat$trs_county[i],centroids$trs_county)\r\n  #if no match, flag and move on\r\n  if(is.na(m)){\r\n    fdat$flag[i] <- ""could not interpret""\r\n    next\r\n  } else {\r\n    #if there is a match, assign the following information to the fields\r\n    fdat$decimalLatitude_int[i] <- as.character(centroids$latitude[m])\r\n    fdat$decimalLongitude_int[i] <- as.character(centroids$longitude[m])\r\n    fdat$coordinateUncertaintyInMeters_int[i] <- ""969""\r\n    fdat$georeferencedBy_int[i] <- ""kdpearso (via TRS conversion script)""\r\n    fdat$georeferenceRemarks_int[i] <- ""auto-generated from TRS by California Phenology Network; February 2021""\r\n    fdat$georeferenceSources_int[i] <- ""TRS from label""\r\n    fdat$georeferenceProtocol_int[i] <- ""DOI:10.5281/zenodo.4507032""\r\n    #optional progress report\r\n    print(i)\r\n  }\r\n}\r\n\r\n#get a count of how many could not be interpreted\r\nnot_interpreted <- subset(fdat,fdat$flag==""could not interpret"")\r\ndim(not_interpreted)[1]\r\n#calculate a percentage that COULD be interpreted\r\ninterpreted <- dim(fdat)[1] - dim(not_interpreted)[1]\r\nfiltered <- dim(fdat)[1]\r\nunfiltered <- dim(dat)[1]\r\nprint(paste(round((interpreted/filtered)*100, digits = 1),""% successfully interpreted!"",sep=""""))\r\nprint(paste(""This is "",round((interpreted/unfiltered)*100, digits = 1),""% of your total starting dataset."",sep=""""))\r\n\r\n#save this for posterity\r\nwrite.csv(fdat, ""C:/Users/Katie/Desktop/CA_interpreted_coords_v12.csv"")\r\n\r\nfat <- subset(fdat,is.na(fdat$flag))\r\nthistab <- table(as.character(fat$collid))\r\nthistab <- as.data.frame(thistab)\r\n']","Batch Convert TRS to Lat/Long via Lookup Table This code uses a PLSS lookup table provided by Nelson Rios (Yale Unviersity; GEOLocate) to assign a latitude, longitude, and error radius (according to section) to TRS coordinates extracted from herbarium specimen records. Several patterns in the PLSS lookup table were added from Earth Point. Note that each version of the code is customized for a certain state.The script is not able to address the following circumstances:any information below the rank of section (it scrubs all quarter and half section data, etc. before converting to coordinates)when section is listed before the township and rangewhen multiple sections are listedwhen TRS and UTM data are concatenated into a single fieldwhen TRS data don't exist in the reference file (the reference file we used largely excludes sections that are not the standard size)when no section is provided",4
Morning glory species co-occurrence is associated with asymmetrically decreased and cascading reproductive isolation,"Hybridization between species can affect the strength of the reproductive barriers that separate those species. Two extensions of this effect are: (1) the expectation that asymmetric hybridization or gene flow will have asymmetric effects on reproductive barrier strength and (2) the expectation that local hybridization will affect only local reproductive barrier strength and could therefore alter within-species compatibility. We tested these hypotheses in a pair of morning glory species that exhibit asymmetric gene flow from highly selfing Ipomoea lacunosa into mixed-mating I. cordatotriloba in regions where they co-occur. Because of the direction of this gene flow, we predicted that reproductive barrier strength would be more strongly affected in I. cordatotriloba than I. lacunosa. We also predicted that changes to reproductive barriers in sympatric I. cordatotriloba populations would affect compatibility with allopatric populations of that species. We tested these predictions by measuring the strength of a reproductive barrier to seed set across the species' ranges. Consistent with our first prediction, we found that sympatric and allopatric I. lacunosa produce the same number of seeds in crosses with I. cordatotriloba, whereas crosses between sympatric I. cordatotriloba and I. lacunosa are more successful than crosses between allopatric I. cordatotriloba and I. lacunosa. This difference in compatibility appears to reflect an asymmetric decrease in the strength of the barrier to seed set in sympatric I. cordatotriloba, which could be caused by I. lacunosa alleles that have introgressed into I. cordatotriloba. We further demonstrated that changes to sympatric I. cordatotriloba have decreased its ability to produce seeds with allopatric populations of the same species, in line with our second prediction. Thus, in a manner analogous to cascade reinforcement, we suggest that introgression associated with hybridization not only influences between-species isolation but can also contribute to isolation within a species.","['library(""tidyverse"")\nlibrary(""gplots"") # for plotCI\nlibrary(""scales"") # for transparency\nlibrary(""lme4"") # for linear mixed-effect models\nlibrary(""glmmTMB"") # for zero-inflation and hurdle models\nlibrary(""DHARMa"") # for model fits\nlibrary(""emmeans"") # for model comparisons\nlibrary(""mapdata"") # for map data \nlibrary(""geosphere"") # for distances between points\npar(mar=c(5, 4, 4, 2), oma=c(2, 2, 2, 2))\noptions(scipen = 999)\n\n# define a function that runs multiple model fitting tests \ntest_model_fit <- function(model) {\n  print(summary(model))\n  sim_res <- simulateResiduals(model)\n  testZeroInflation(sim_res)\n  testOutliers(sim_res)\n  testDispersion(sim_res) # simulation test for over/under dispersion\n  plot(sim_res)\n  hist(resid(model))\n}\n\n\n#############################################\n# --- Make maps of the populations used --- #\n#############################################\n\n# load population data\nmdata <- read.csv(file=""data/Ipo_RI_survey_pops.csv"", stringsAsFactors = TRUE, na.strings = c(""NA"", """"))\n\npar(mar=c(1, 1, 1, 1), oma=c(1, 1, 1, 1))\nmap(""worldHires"",""usa"", xlim=c(-105,-75),ylim=c(15,40), col=""gray90"", fill=TRUE)\nmap(""worldHires"",""mexico"", xlim=c(-105,-75),ylim=c(15,40), col=""gray95"", fill=TRUE, add=TRUE)\npoints(mdata$long, mdata$lat, pch=19, cex=1.5, col=as.numeric(mdata$type))\ntext(mdata$long, mdata$lat+1, mdata$name)\n\nmap(""worldHires"", ""usa"", xlim=c(-86,-75), ylim=c(33,38), col=""gray90"", fill=TRUE)\npoints(mdata$long, mdata$lat, pch=19, cex=1.5, col=as.numeric(mdata$type))\ntext(mdata$long, mdata$lat + 0.25, mdata$name)\n\npar(mar=c(5, 4, 4, 2), oma=c(2, 2, 2, 2))\n\n##############################\n# --- Analyze cross data --- #\n##############################\n\n# load data \ncross_data <- read.csv(file=""data/Ipo_RI_survey.csv"", stringsAsFactors = TRUE, na.strings = c(""NA"", """"))\ncross_data$r.cross.type <- ordered(cross_data$r.cross.type, levels = c(""ACxAC"",""ACxSC"",""SCxAC"",""SCxSC"",""ACxAL"",""ACxSL"",""ALxAC"",""SLxAC"",""SCxAL"",""SCxSL"",""ALxSC"",""SLxSC"",""ALxAL"",""SLxAL"",""ALxSL"",""SLxSL""))\n\n# filter out the population ""Ocean""\ncross_data <- cross_data %>% filter(mom.loc != ""Ocean"", dad.loc != ""Ocean"") %>% droplevels()\n\n# look at seed weights\nhist(c(cross_data$seed1, cross_data$seed1, cross_data$seed1, cross_data$seed1), breaks = 30, xlab = ""Seed weight (mg)"", main = NULL)\nabline(v = 10, col = ""blue"", lty = 2)\n\n# make means data\nmeans_dat <- cross_data %>% group_by(focal.plant, mom, mom.pop, mom.type, dad, dad.pop, dad.type, r.sp, cross.type, r.cross.type, cross.loc, intraspecific) %>%\n  summarise(mean = mean(n.seeds)) %>%\n  as.data.frame %>% filter(., complete.cases(.))\nmeans_dat$r.cross.type <- ordered(means_dat$r.cross.type, levels = c(""ACxAC"",""ACxSC"",""SCxAC"",""SCxSC"",""ACxAL"",""ACxSL"",""ALxAC"",""SLxAC"",""SCxAL"",""SCxSL"",""ALxSC"",""SLxSC"",""ALxAL"",""SLxAL"",""ALxSL"",""SLxSL""))\n\n# look at population averages (use data with Ocean)\nlist1 <- cross_data %>% group_by(cross.ind) %>% \n  summarise(cross.loc = first(cross.loc), cross.type = first(cross.type), prop = mean(min1seed)) %>%\n  group_by(cross.loc, cross.type) %>% summarise(prop.mean = mean(prop))\nlist2 <- means_dat %>% group_by(cross.loc, cross.type) %>% summarise(mean = mean(mean))\npop_averages <- full_join(list1, list2, by = ""cross.loc"")\n\n\n#######################################################################\n### --- Test for differences between different types of crosses --- ###\n#######################################################################\n\n# look at data\nstripchart(mean ~ r.cross.type, data=means_dat, las = 2, vertical = TRUE, method = ""jitter"", ylab = ""Mean number of seeds produced per pod"", \n           pch = 16, cex = 1.5, lty = 1, col = alpha(c(rep(""#AD3C8C"", 4), rep(""#4259A7"", 8), rep(""#38B14A"", 4)), 0.6))\nmosaicplot(table(cross_data$r.cross.type, cross_data$n.seeds), las=2, main=NULL, ylab = ""Number of seeds per pod"", xlab = ""Cross type"")\nmosaicplot(table(cross_data$r.sp, cross_data$n.seeds), las=2, main=NULL)\n\n# test models of fruit set\nfull_B_model1 <- glmer(min1seed ~ r.sp + (1 | mom.pop/mom/cross.ind) + (1 | dad.pop/dad), family = ""binomial"", data = cross_data)\nfull_B_model2 <- glmer(min1seed ~ r.sp + (1 | mom.pop/mom/cross.ind), family = ""binomial"", data = cross_data)\nfull_B_model3 <- glmer(min1seed ~ (1 | mom.pop/mom/cross.ind), family = ""binomial"", data = cross_data)\nanova(full_B_model1, full_B_model2, full_B_model3)\ntest_model_fit(full_B_model2)\nlsmeans(full_B_model2, pairwise ~ r.sp)\ncontrast(lsmeans(full_B_model2, ""r.sp""), list(CvL=c(1,0,0,-1), CvH=c(1,-1,-1,0), LvH=c(0,-1,-1,1), H1vH2=c(0,1,-1,0)))\n#test_model_fit(full_B_model1)\n#anova(full_B_model1, glmer(min1seed ~ (1 | mom.pop/mom/cross.ind) + (1 | dad.pop/dad), family = ""binomial"", data = cross_data))\n#lsmeans(full_B_model1, pairwise ~ r.sp)\n#contrast(lsmeans(full_B_model1, ""r.sp""), list(CvL=c(1,0,0,-1), CvH=c(1,-1,-1,0), LvH=c(0,-1,-1,1), H1vH2=c(0,1,-1,0)))\n\n# test models of mean seed number\nfull_M_model1 <- glmmTMB(mean ~ r.sp + (1 | mom.pop/mom) + (1 | dad.pop/dad),  ziformula=~r.sp, data = means_dat)\nfull_M_model2 <- glmmTMB(mean ~ r.sp + (1 | mom.pop/mom),  ziformula=~r.sp, data = means_dat)\nfull_M_model3 <- glmmTMB(mean ~ (1 | mom.pop/mom),  ziformula=~r.sp, data = means_dat)\nanova(full_M_model1, full_M_model2, full_M_model3)\ntest_model_fit(full_M_model2)\nlsmeans(full_M_model2, pairwise ~ r.sp)\ncontrast(lsmeans(full_M_model2, ""r.sp""), list(CvL=c(1,0,0,-1), CvH=c(1,-1,-1,0), LvH=c(0,-1,-1,1), H1vH2=c(0,1,-1,0)))\n#test_model_fit(full_M_model1)\n#anova(full_M_model1, glmmTMB(mean ~ (1 | mom.pop/mom) + (1 | dad.pop/dad),  ziformula=~r.sp, data = means_dat))\n#lsmeans(full_M_model1, pairwise ~ r.sp)\n#contrast(lsmeans(full_M_model1, ""r.sp""), list(CvL=c(1,0,0,-1), CvH=c(1,-1,-1,0), LvH=c(0,-1,-1,1), H1vH2=c(0,1,-1,0)))\n\n\n#####################################################################################\n### --- Test for differences between different types of interspecific crosses --- ###\n#####################################################################################\n\n# subset data\ninter_means <- means_dat %>% filter(intraspecific == FALSE) %>% droplevels()\ninter_cross <- cross_data %>% filter(intraspecific == FALSE) %>% droplevels()\n\n# look at data\nmosaicplot(table(inter_cross$r.cross.type, inter_cross$n.seeds), las=2, main=NULL)\nt <- table(inter_cross$cross.type, inter_cross$min1seed) %>% as.data.frame.matrix()\nnames(t) <- c(""fail"", ""success"")\nt %>% mutate(sum = fail + success, percent = fail/sum)\npar(mfrow=c(2,2))\npar(mar=c(2,2,2,2))\npie(c(t[1,1],t[1,2])); pie(c(t[3,1],t[3,2])); pie(c(t[2,1],t[2,2])); pie(c(t[4,1],t[4,2]))\npar(mfrow=c(1,1))\n\n# test models of fruit set\ninter_B_model1 <- glmer(min1seed ~ cross.type + (1 | mom.pop/mom/cross.ind) + (1 | dad.pop/dad), family = ""binomial"", data = inter_cross)\ninter_B_model2 <- glmer(min1seed ~ cross.type + (1 | mom.pop/mom/cross.ind), family = ""binomial"", data = inter_cross)\ninter_B_model3 <- glmer(min1seed ~ (1 | mom.pop/mom/cross.ind), family = ""binomial"", data = inter_cross)\nanova(inter_B_model1, inter_B_model2, inter_B_model3)\ntest_model_fit(inter_B_model2)\nlsmeans(inter_B_model2, pairwise ~ cross.type)\ncontrast(lsmeans(inter_B_model2, ""cross.type""), list(AvS=c(1,0,0,-1), ACvSC=c(1,1,-1,-1), ALvSL=c(1,-1,1,-1)))\n#test_model_fit(inter_B_model1)\n#anova(inter_B_model1, glmer(min1seed ~ (1 | mom.pop/mom/cross.ind) + (1 | dad.pop/dad), family = ""binomial"", data = inter_cross))\n#lsmeans(inter_B_model1, pairwise ~ cross.type)\n#contrast(lsmeans(inter_B_model1, ""cross.type""), list(AvS=c(1,0,0,-1), ACvSC=c(1,1,-1,-1), ALvSL=c(1,-1,1,-1)))\n\n# test models of mean seed number\ninter_M_model1 <- glmmTMB(mean ~ cross.type + (1 | mom.pop/mom) + (1 | dad.pop/dad), ziformula=~1, family = tweedie(link = ""log""), data = inter_means)\ninter_M_model2 <- glmmTMB(mean ~ cross.type + (1 | mom.pop/mom), ziformula=~1, family = tweedie(link = ""log""), data = inter_means)\ninter_M_model3 <- glmmTMB(mean ~ (1 | mom.pop/mom), ziformula=~1, family = tweedie(link = ""log""), data = inter_means)\nanova(inter_M_model1, inter_M_model2, inter_M_model3)\ntest_model_fit(inter_M_model2)\nlsmeans(inter_M_model2, pairwise ~ cross.type)\ncontrast(lsmeans(inter_M_model2, ""cross.type""), list(AvS=c(1,0,0,-1), ACvSC=c(1,1,-1,-1), ALvSL=c(1,-1,1,-1)))\n#test_model_fit(inter_M_model1)\n#anova(inter_M_model1, glmmTMB(mean ~ (1 | mom.pop/mom) + (1 | dad.pop/dad), ziformula=~1, family = tweedie(link = ""log""), data = inter_means))\n#lsmeans(inter_M_model1, pairwise ~ cross.type)\n#contrast(lsmeans(inter_M_model1, ""cross.type""), list(AvS=c(1,0,0,-1), ACvSC=c(1,1,-1,-1), ALvSL=c(1,-1,1,-1)))\n\n\n##################################################################################################\n### find and plot mean and 95% CI\'s for each ""type"" of interspecific cross using bootstrapping ###\n##################################################################################################\n\n# make new categories\ninter_means <- inter_means %>% mutate(A = (mom.type == ""AC"" & dad.type == ""AL"") | ((mom.type == ""AL"" & dad.type == ""AC"")),\n                                      S = (mom.type == ""SC"" & dad.type == ""SL"") | ((mom.type == ""SL"" & dad.type == ""SC"")),\n                                      AC = mom.type == ""AC"" | dad.type == ""AC"",\n                                      SC = mom.type == ""SC"" | dad.type == ""SC"",\n                                      AL = mom.type == ""AL"" | dad.type == ""AL"",\n                                      SL = mom.type == ""SL"" | dad.type == ""SL"")\n\n# find mean and 95% bootstrap confidence interval for each cross type\ncross_type_summary <- setNames(data.frame(matrix(ncol = 5, nrow = 0)), c(""cross.type"", ""mean"", ""LCI"", ""UCI"", ""N""))\ncross_type_list <- c(""A"", ""S"", ""AC"", ""SC"", ""AL"", ""SL"")\nset.seed(1)\nfor (i in 1:6){\n  cross_type_summary[i,1] <- cross_type_list[[i]]\n  # subset data\n  temp <- inter_means %>% filter(inter_means[,13+i] == TRUE)\n  # find actual mean\n  cross_type_summary[i,2] <- mean(temp$mean)\n  # find 95% bootstrap confidence interval using the percentile method\n  bstrap <- c()\n  for (j in 1:10000){ bstrap <- c(bstrap, mean(sample(temp$mean,length(temp$mean),replace=T))) }\n  cross_type_summary[i,3] <- quantile(bstrap,.025)\n  cross_type_summary[i,4] <- quantile(bstrap,.975)\n  cross_type_summary[i,5] <- nrow(temp)\n}\n\n# plot means and 95% confidence intervals for all comparisons\nplotCI(c(1.1,1.9, 3.1,3.9,5.1,5.9), cross_type_summary$mean, ui = cross_type_summary$UCI, li = cross_type_summary$LCI,\n       err=""y"", pch = 1, gap = 0, lwd = 2, cex = 1.5, sfrac = 0, las = 1, ylim = c(-0.02, 0.26), xlim = c(0.5, 6.5), xaxt = ""n"",\n       ylab=""Mean number of seeds per pod"", xlab=""Interspecific cross types"")\naxis(1, at = c(1.1,1.9,3.1,3.9,5.1,5.9), labels = cross_type_list)\nsegments(c(1.1, 3.1, 5.1), rep(0.23, 3), c(1.9, 3.9, 5.9), rep(0.23, 3))\ntext(1.5, 0.225, labels = ""*"", pos = 3)\ntext(3.5, 0.225, labels = ""***"", pos = 3)\ntext(5.5, 0.225, labels = ""NS"", pos = 3, cex = 0.75)\ntext(c(0.6,1.1,1.9,3.1,3.9,5.1,5.9), rep(0, 7), labels = c(""N ="", cross_type_summary$N), pos = 1, cex = 0.75)\n\n\n######################################################################################\n### --- Test for differences between different types of cordatotriloba crosses --- ###\n######################################################################################\n\n# subset data\ncor_means <- means_dat %>% filter((mom.type == ""AC"" | mom.type == ""SC"") & (dad.type == ""AC"" | dad.type == ""SC"")) %>% droplevels()\ncor_cross <- cross_data %>% filter((mom.type == ""AC"" | mom.type == ""SC"") & (dad.type == ""AC"" | dad.type == ""SC"")) %>% droplevels()\n\n# look at data\nstripchart(mean ~ cross.type, data=cor_means, las = 2, vertical = TRUE, method = ""jitter"", ylab = ""Mean number of seeds produced per pod"", pch = 16, cex = 1, lty = 1, col = alpha(""blue"", 0.6))\nplot(table(cor_cross$cross.type, cor_cross$n.seeds))\nt2 <- table(cor_cross$cross.type, cor_cross$min1seed) %>% as.data.frame.matrix()\nnames(t2) <- c(""fail"", ""success"")\nt2 %>% mutate(sum = fail + success, percent = fail/sum)\npar(mfrow=c(1,3))\npar(mar=c(2,2,2,2))\npie(c(t2[1,1],t2[1,2])); pie(c(t2[2,1],t2[2,2])); pie(c(t2[3,1],t2[3,2]))\npar(mfrow=c(1,1))\n\n# test models of fruit set\ncor_B_model1 <- glmer(min1seed ~ cross.type + (1 | mom.pop/mom/cross.ind) + (1 | dad.pop/dad), family = ""binomial"", data = cor_cross)\ncor_B_model2 <- glmer(min1seed ~ cross.type + (1 | mom.pop/mom/cross.ind), family = ""binomial"", data = cor_cross)\ncor_B_model3 <- glmer(min1seed ~ (1 | mom.pop/mom/cross.ind), family = ""binomial"", data = cor_cross)\nanova(cor_B_model1, cor_B_model2, cor_B_model3)\ntest_model_fit(cor_B_model2)\nlsmeans(cor_B_model2, pairwise ~ cross.type)\n#test_model_fit(cor_B_model1)\n#anova(cor_B_model1, glmer(min1seed ~ (1 | mom.pop/mom/cross.ind) + (1 | dad.pop/dad), family = ""binomial"", data = cor_cross))\n#lsmeans(cor_B_model1, pairwise ~ cross.type)\n\n# test models of mean seed number\ncor_M_model1 <- lmer(mean ~ cross.type + (1 | mom.pop/mom) + (1 | dad.pop/dad), data = cor_means)\ncor_M_model2 <- lmer(mean ~ cross.type + (1 | mom.pop/mom), data = cor_means)\ncor_M_model3 <- lmer(mean ~ (1 | mom.pop/mom), data = cor_means)\nanova(cor_M_model1, cor_M_model2, cor_M_model3)\ntest_model_fit(cor_M_model2)\nlsmeans(cor_M_model2, pairwise ~ cross.type)\n#test_model_fit(cor_M_model1)\n#anova(cor_M_model1, lmer(mean ~ (1 | mom.pop/mom) + (1 | dad.pop/dad), data = cor_means))\n#lsmeans(cor_M_model1, pairwise ~ cross.type)\n\n\n################################################################################\n### --- Test for differences between different types of lacunosa crosses --- ###\n################################################################################\n\n# subset data\nlac_means <- means_dat %>% filter((mom.type == ""AL"" | mom.type == ""SL"") & (dad.type == ""AL"" | dad.type == ""SL"")) %>% droplevels()\nlac_cross <- cross_data %>% filter((mom.type == ""AL"" | mom.type == ""SL"") & (dad.type == ""AL"" | dad.type == ""SL"")) %>% droplevels()\n\n# look at data\nstripchart(mean ~ cross.type, data=lac_means, las = 2, vertical = TRUE, method = ""jitter"", ylab = ""Mean number of seeds produced per pod"", pch = 16, cex = 1, lty = 1, col = alpha(""blue"", 0.6))\nplot(table(lac_cross$cross.type, lac_cross$n.seeds))\nt3 <- table(lac_cross$cross.type, lac_cross$min1seed) %>% as.data.frame.matrix()\nnames(t3) <- c(""fail"", ""success"")\nt3 %>% mutate(sum = fail + success, percent = fail/sum)\npar(mfrow=c(1,3))\npar(mar=c(2,2,2,2))\npie(c(27,91)); pie(c(67,176)); pie(c(35,86))\npie(c(t3[1,1],t3[1,2])); pie(c(t3[2,1],t3[2,2])); pie(c(t3[3,1],t3[3,2]))\npar(mfrow=c(1,1))\n\n# test models of fruit set\nlac_B_model1 <- glmer(min1seed ~ cross.type + (1 | mom.pop/mom/cross.ind) + (1 | dad.pop/dad), family = ""binomial"", data = lac_cross)\nlac_B_model2 <- glmer(min1seed ~ cross.type + (1 | mom.pop/mom/cross.ind), family = ""binomial"", data = lac_cross)\nlac_B_model3 <- glmer(min1seed ~ (1 | mom.pop/mom/cross.ind), family = ""binomial"", data = lac_cross)\nanova(lac_B_model1, lac_B_model2, lac_B_model3)\ntest_model_fit(lac_B_model2)\nlsmeans(lac_B_model2, pairwise ~ cross.type)\n#test_model_fit(lac_B_model1)\n#anova(lac_B_model1, glmer(min1seed ~ (1 | mom.pop/mom/cross.ind) + (1 | dad.pop/dad), family = ""binomial"", data = lac_cross))\n#lsmeans(lac_B_model1, pairwise ~ cross.type)\n\n# test models of mean seed number\nlac_M_model1 <- lmer(mean ~ cross.type + (1 | mom.pop/mom) + (1 | dad.pop/dad), data = lac_means)\nlac_M_model2 <- lmer(mean ~ cross.type + (1 | mom.pop/mom), data = lac_means)\nlac_M_model3 <- lmer(mean ~ (1 | mom.pop/mom), data = lac_means)\nanova(lac_M_model1, lac_M_model2, lac_M_model3)\ntest_model_fit(lac_M_model2)\nlsmeans(lac_M_model2, pairwise ~ cross.type)\n#test_model_fit(lac_M_model1)\n#anova(lac_M_model1, lmer(mean ~ (1 | mom.pop/mom) + (1 | dad.pop/dad), data = lac_means))\n#lsmeans(lac_M_model1, pairwise ~ cross.type)\n\n\n#############################################################\n### --- Figure for both sets of intraspecific crosses --- ###\n#############################################################\n\npar(mfrow=c(1,2))\nestimates1 <- lsmeans(cor_M_model2, pairwise ~ cross.type)[[1]] %>% data.frame()\nstripchart(mean ~ cross.type, data=cor_means, las = 2, vertical = TRUE, method = ""jitter"", \n           ylab = ""Mean number of seeds produced per pod"", ylim = c(0, 4), xlim = c(0.5, 3.5),\n           pch = 16, cex = 1.5, lty = 1, col = alpha(""#AD3C8C"", 0.6))\nplotCI(c(1.2, 2.2, 3.2), estimates1$lsmean, ui=estimates1$upper.CL, li=estimates1$lower.CL, add=TRUE, \n       err=""y"", pch = 1, gap = 0, lwd = 2, cex = 1.5, sfrac = 0, las = 1)\nestimates2 <- lsmeans(lac_M_model2, pairwise ~ cross.type)[[1]] %>% data.frame()\nstripchart(mean ~ cross.type, data=lac_means, las = 2, vertical = TRUE, method = ""jitter"", \n           ylab = ""Mean number of seeds produced per pod"", ylim = c(0, 4), xlim = c(0.5, 3.5),\n           pch = 16, cex = 1.5, lty = 1, col = alpha(""#38B14A"", 0.6))\nplotCI(c(1.2, 2.2, 3.2), estimates2$lsmean, ui=estimates2$upper.CL, li=estimates2$lower.CL, add=TRUE, \n       err=""y"", pch = 1, gap = 0, lwd = 2, cex = 1.5, sfrac = 0, las = 1)\npar(mfrow=c(1,1))\n\n\n####################################################################\n### --- Test for an effect of geographic distance on crosses --- ###\n####################################################################\n\n# load population locations \npop_data <- read.csv(file=""data/Ipo_RI_survey_pops.csv"", stringsAsFactors = TRUE, na.strings = c(""NA"", """")) %>%\n  select(name, lat, long)\n\n# add locations to means data\nintra_means <- means_dat %>% filter(intraspecific == TRUE) %>%\n  mutate(mom_pop = gsub(\'.{2}$\', \'\', .$mom.pop)) %>%\n  left_join(., pop_data, by = c(""mom_pop"" = ""name"")) %>%\n  mutate(dad_pop = gsub(\'.{2}$\', \'\', .$dad.pop)) %>%\n  left_join(., pop_data, by = c(""dad_pop"" = ""name"")) %>% \n  mutate(pch = case_when(cross.type == ""ACxxAC"" | cross.type == ""ALxxAL"" ~ 15,\n                         cross.type == ""ACxxSC"" | cross.type == ""ALxxSL"" ~ 16,\n                         TRUE ~ 17)) %>% droplevels()\n\n# add locations to cross data\nintra_crosses <- cross_data %>% filter(intraspecific == TRUE) %>%\n  mutate(mom_pop = gsub(\'.{2}$\', \'\', .$mom.pop)) %>%\n  left_join(., pop_data, by = c(""mom_pop"" = ""name"")) %>%\n  mutate(dad_pop = gsub(\'.{2}$\', \'\', .$dad.pop)) %>%\n  left_join(., pop_data, by = c(""dad_pop"" = ""name"")) %>% \n  mutate(pch = case_when(cross.type == ""ACxxAC"" | cross.type == ""ALxxAL"" ~ 15,\n                         cross.type == ""ACxxSC"" | cross.type == ""ALxxSL"" ~ 16,\n                         TRUE ~ 17)) %>% droplevels()\n\n# calculate distances\nfor (i in 1:nrow(intra_means)) {\n  intra_means[i,""dist""] <- distm(c(intra_means[i,""long.x""], intra_means[i,""lat.x""]), c(intra_means[i,""long.y""], intra_means[i,""lat.y""]))/1000\n}\nfor (i in 1:nrow(intra_crosses)) {\n  intra_crosses[i,""dist""] <- distm(c(intra_crosses[i,""long.x""], intra_crosses[i,""lat.x""]), c(intra_crosses[i,""long.y""], intra_crosses[i,""lat.y""]))/1000\n}\n\n# test for effect on fruit set in I cordatotriloba\nintra_cross_C <- intra_crosses %>% filter(r.sp == ""CxC"")\ncor_dist_B_model1 <- glmer(min1seed ~ dist + (1 | mom.pop/mom/cross.ind) + (1 | dad.pop/dad), family = ""binomial"", data = intra_cross_C)\ncor_dist_B_model2 <- glmer(min1seed ~ dist + (1 | mom.pop/mom/cross.ind), family = ""binomial"", data = intra_cross_C)\ncor_dist_B_model3 <- glmer(min1seed ~ (1 | mom.pop/mom/cross.ind), family = ""binomial"", data = intra_cross_C)\nanova(cor_dist_B_model1, cor_dist_B_model2, cor_dist_B_model3)\ntest_model_fit(cor_dist_B_model2)\n#anova(cor_dist_B_model1, glmer(min1seed ~ (1 | mom.pop/mom/cross.ind) + (1 | dad.pop/dad), family = ""binomial"", data = intra_cross_C))\n#test_model_fit(cor_dist_B_model1)\n\n# test for effect on mean seeds in I cordatotriloba\nintra_means_C <- intra_means %>% filter(r.sp == ""CxC"")\ncor_dist_M_model1 <- lmer(mean ~ dist + (1 | mom.pop/mom) + (1 | dad.pop/dad), data = intra_means_C)\ncor_dist_M_model2 <- lmer(mean ~ dist + (1 | mom.pop/mom), data = intra_means_C)\ncor_dist_M_model3 <- lmer(mean ~ (1 | mom.pop/mom), data = intra_means_C)\nanova(cor_dist_M_model1, cor_dist_M_model2, cor_dist_M_model3)\ntest_model_fit(cor_dist_M_model2)\n#anova(cor_dist_M_model1, lmer(mean ~ (1 | mom.pop/mom) + (1 | dad.pop/dad), data = intra_means_C))\n#test_model_fit(cor_dist_M_model1)\n\n# test for effect on fruit set in I lacunosa\nintra_cross_L <- intra_crosses %>% filter(r.sp == ""LxL"")\nlac_dist_B_model1 <- glmer(min1seed ~ dist + (1 | mom.pop/mom/cross.ind) + (1 | dad.pop/dad), family = ""binomial"", data = intra_cross_L)\nlac_dist_B_model2 <- glmer(min1seed ~ dist + (1 | mom.pop/mom/cross.ind), family = ""binomial"", data = intra_cross_L)\nlac_dist_B_model3 <- glmer(min1seed ~ (1 | mom.pop/mom/cross.ind), family = ""binomial"", data = intra_cross_L)\nanova(lac_dist_B_model1, lac_dist_B_model2, lac_dist_B_model3)\ntest_model_fit(lac_dist_B_model2)\n#anova(lac_dist_B_model1, glmer(min1seed ~ (1 | mom.pop/mom/cross.ind) + (1 | dad.pop/dad), family = ""binomial"", data = intra_cross_L))\n#test_model_fit(lac_dist_B_model1)\n\n# test for effect on mean seeeds in I lacunosa\nintra_means_L <- intra_means %>% filter(r.sp == ""LxL"")\nlac_dist_M_model1 <- lmer(mean ~ dist + (1 | mom/mom.pop) + (1 | dad.pop/dad), data = intra_means_L)\nlac_dist_M_model2 <- lmer(mean ~ dist + (1 | mom/mom.pop), data = intra_means_L)\nlac_dist_M_model3 <- lmer(mean ~ (1 | mom/mom.pop), data = intra_means_L)\nanova(lac_dist_M_model1, lac_dist_M_model2, lac_dist_M_model3)\ntest_model_fit(lac_dist_M_model2)\n#anova(lac_dist_M_model1, lmer(mean ~ (1 | mom/mom.pop) + (1 | dad.pop/dad), data = intra_means_L))\n#test_model_fit(lac_dist_M_model1)\n\n# plot relationahip between number of seeds and distance\npar(mfrow=c(2,1))\nplot(intra_means_C$mean ~ (intra_means_C$dist), xlim = c(0, 2500), ylim = c(0,4), xlab = ""Distance (km)"", ylab = ""Mean number of seeds"",\n     pch = intra_means_C$pch, cex = 1.5, lty = 1, col = alpha(""#AD3C8C"", 0.6))\nabline(summary(cor_dist_M_model2)[[10]][1,1], summary(cor_dist_M_model2)[[10]][2,1])\nplot(intra_means_L$mean ~ intra_means_L$dist, xlim = c(0, 2500), ylim = c(0,4), xlab = ""Distance (km)"", ylab = ""Mean number of seeds"",\n     pch = intra_means_C$pch, cex = 1.5, lty = 1, col = alpha(""#38B14A"", 0.6))\nabline(summary(lac_dist_M_model2)[[10]][1,1], summary(lac_dist_M_model2)[[10]][2,1])\nlegend(2000, 4, legend = c(""allo"", ""cross"", ""sym""), pch = 15:17, col = alpha(""black"", 0.6))\npar(mfrow=c(1,1))\n\n\n#########################################\n### --- Test for F1 cross success --- ###\n#########################################\n\n# load data \nF1_data <- read.csv(file=""data/Ipo_RI_F1_crosses.csv"", stringsAsFactors = TRUE, na.strings = c(""NA"", """")) \nF1_data$type2 <- ordered(F1_data$type2, levels = c(""CC"", ""BCC"", ""WF1"", ""BCL"", ""LL"", ""BS""))\nmosaicplot(table(F1_data$type2, F1_data$n.seeds), las=2, main=NULL, ylab = ""Number of seeds per pod"", xlab = ""Cross type"")\n\n# test models of fruit set\nF1_B_model1 <- glmer(min1seed ~ type2 + (1 | mom) + (1 | dad), family = ""binomial"", data = F1_data)\nF1_B_model2 <- glmer(min1seed ~ (1 | mom) + (1 | dad), family = ""binomial"", data = F1_data)\nanova(F1_B_model1, F1_B_model2)\ntest_model_fit(F1_B_model1)\nlsmeans(F1_B_model1, pairwise ~ type2)\n\n# test models of mean seeds\nF1_N_model1 <- glmer(n.seeds ~ type2 + (1 | mom) + (1 | dad), family = ""poisson"", data = F1_data)\nF1_N_model2 <- glmer(n.seeds ~ (1 | mom) + (1 | dad), family = ""poisson"", data = F1_data)\nanova(F1_N_model1, F1_N_model2)\ntest_model_fit(F1_B_model1)\nlsmeans(F1_N_model1, pairwise ~ type2)\n\n']","Morning glory species co-occurrence is associated with asymmetrically decreased and cascading reproductive isolation Hybridization between species can affect the strength of the reproductive barriers that separate those species. Two extensions of this effect are: (1) the expectation that asymmetric hybridization or gene flow will have asymmetric effects on reproductive barrier strength and (2) the expectation that local hybridization will affect only local reproductive barrier strength and could therefore alter within-species compatibility. We tested these hypotheses in a pair of morning glory species that exhibit asymmetric gene flow from highly selfing Ipomoea lacunosa into mixed-mating I. cordatotriloba in regions where they co-occur. Because of the direction of this gene flow, we predicted that reproductive barrier strength would be more strongly affected in I. cordatotriloba than I. lacunosa. We also predicted that changes to reproductive barriers in sympatric I. cordatotriloba populations would affect compatibility with allopatric populations of that species. We tested these predictions by measuring the strength of a reproductive barrier to seed set across the species' ranges. Consistent with our first prediction, we found that sympatric and allopatric I. lacunosa produce the same number of seeds in crosses with I. cordatotriloba, whereas crosses between sympatric I. cordatotriloba and I. lacunosa are more successful than crosses between allopatric I. cordatotriloba and I. lacunosa. This difference in compatibility appears to reflect an asymmetric decrease in the strength of the barrier to seed set in sympatric I. cordatotriloba, which could be caused by I. lacunosa alleles that have introgressed into I. cordatotriloba. We further demonstrated that changes to sympatric I. cordatotriloba have decreased its ability to produce seeds with allopatric populations of the same species, in line with our second prediction. Thus, in a manner analogous to cascade reinforcement, we suggest that introgression associated with hybridization not only influences between-species isolation but can also contribute to isolation within a species.",4
Social polyandry shapes sperm morphology,"Sexual selection is a major driver of trait variation, and the intensity of male competition for mating opportunities has been linked with sperm size across diverse taxa. Mating competition among females may also shape the evolution of sperm traits, but the interplay between female-female competition and male-male competition on sperm morphology is not well understood. We evaluated variation in sperm morphology in two species with socially polyandrous mating systems, in which females compete to mate with multiple males. Northern jacanas (Jacana spinosa) and wattled jacanas (J. jacana) vary in their degree of polyandry and sexual dimorphism, suggesting species differences in the intensity of sexual selection. We compared mean and variance in sperm head, midpiece, and tail length between species and breeding stages, because these measures have been associated with the intensity of sperm competition. We found that the species with greater polyandry, northern jacana, has sperm with longer midpieces and tails, as well as marginally lower intra-ejaculate variation in tail length. Intra-ejaculate variation was also significantly lower in copulating males than in incubating males, suggesting flexibility in sperm production as males cycle between breeding stages. Our results indicate that stronger female-female competition for mating opportunities may also shape more intense male-male competition by selecting for longer and less variable sperm traits. These findings extend frameworks developed in socially monogamous species to reveal that sperm competition may be an important evolutionary force layered atop female-female competition for mates.","['# How female-female competition affects male-male competition: \n# insights on post-copulatory sexual selection from socially polyandrous species.    \n\n# 11.23.2021\n\n# Sara E. Lipshutz, Samuel J. Torneo, Kimberly A. Rosvall\n\n# Set path\nsetwd()\n\n# Read in datasheet - you\'ll have to specify the correct path\nsperm <-read.csv(""jacana_sperm_measurements.csv"") \nsperm.avg <-read.csv(""sperm_measurements_averages.csv"")\n\n# subset dataset for each species\nspinosa.sperm.avg = subset(sperm.avg, Species == ""Northern"")\njacana.sperm.avg = subset(sperm.avg, Species == ""Wattled"")\n\n#Examine structure of the data\nstr(sperm)\nstr(sperm.avg)\n\n# Export figures to pptx using officer and rvg \nrequire (officer)\nrequire(rvg)\nrequire(ggpubr)\nrequire(ggplot2)\n\n# Compare species - rough overlook\nlibrary(ggplot2)\nggplot(sperm, aes(x=Tail, color = Species)) + geom_histogram()\n# spionsa have longer tail\nggplot(sperm, aes(x=Midpiece, color = Species)) + geom_histogram()\n# spionsa have longer midpiece\nggplot(sperm, aes(x=Head, color = Species)) + geom_histogram()\n# Head length seems like its the same between species\n\n\n# Check for outliers - decide whether to exclude any samples\nggplot(sperm, aes(x=Individual, y=Tail, fill=Breeding)) + geom_boxplot() # WAM7 looks like an outlier\nggplot(sperm, aes(x=Individual, y=Midpiece, fill=Breeding)) + geom_boxplot()\nggplot(sperm, aes(x=Individual, y=Head, fill=Breeding)) + geom_boxplot()\n\nlibrary(outliers)\ngrubbs.test(jacana.sperm$Tail, type = 10)\n#G = 2.94924, U = 0.87752, p-value = 0.08774\n#alternative hypothesis: lowest value 56.328 is an outlier\n\ngrubbs.test(spinosa.sperm$Tail, type = 10) \n#G = 2.89249, U = 0.92395, p-value = 0.182\n#alternative hypothesis: lowest value 65.37 is an outlier\n\n#### Decided not to exclude WAM7 - outlier test not significant\n\n\n# Validation of 10 sperm per individual\n\n# Dataset: 50-60 sperm samples from 1 individual of each species, \n# with measurements on total length, tail, midpiece, and head, along with \n# 10 sperm samples from those same individuals\n\nmethod <- read.csv(""sperm_sample_comparison.csv"")\n\n# Total length - methods are comparable\nggplot(method, aes(x=Method, y=Total.Length, fill=Species)) + geom_boxplot()\nggboxplot(method, x = ""Method"", y=""Total.Length"", color=""Species"", add = ""jitter"")\nmethod.total.length <- aov(Total.Length ~ Method * Species, data = method)\nanova(method.total.length)\n\n# Assessing technical repeatability of sperm within an individual\n\n# Repeatability using rptr\n#install.packages(""rptR"")\nlibrary(rptR)\n\nrepeatability <- read.csv(""Repeatability samples.csv"")\n\nrpt(Tail ~ (1 | Sample), grname = ""Sample"", data = repeatability, datatype = ""Gaussian"", nboot = 1000, npermut = 0)\n# R  = 0.851, SE = 0.07, CI = [0.669, 0.938], P  = 2.65e-07 [LRT], NA [Permutation]\nrpt(Midpiece ~ (1 | Sample), grname = ""Sample"", data = repeatability, datatype = ""Gaussian"", nboot = 1000, npermut = 0)\n# R  = 0.803, SE = 0.089, CI = [0.574, 0.918]. P  = 7.68e-07 [LRT]. NA [Permutation]\nrpt(Head ~ (1 | Sample), grname = ""Sample"", data = repeatability, datatype = ""Gaussian"", nboot = 1000, npermut = 0)\n# R  = 0.924, SE = 0.04, CI = [0.82, 0.968], P  = 1.12e-11 [LRT]. NA [Permutation]\n\n\n# MANOVA\n# Followed this R tutorial: https://www.datanovia.com/en/lessons/one-way-manova-in-r/\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(car)\nlibrary(broom)\n\nggboxplot(sperm.avg, x = ""Species"", y = c(""Tail"", ""Midpiece"",""Head""),  merge = TRUE, palette = ""jco"")\n\n# Confirm that we do not violate sample size assumption (the n in each cell > the number of outcome variables)\nsperm.avg %>%\n  group_by(Species) %>%\n  get_summary_stats(Tail, Midpiece, Head, type = ""mean_sd"")\n\nsperm.avg %>%\n  group_by(Breeding) %>%\n  get_summary_stats(Tail, Midpiece, Head, type = ""mean_sd"")\n\n#Check for outliers\nsperm.avg %>%\n  group_by(Species) %>%\n  identify_outliers(Tail)\n\n# Univariate normality assumption\nsperm.avg %>%\n  group_by(Species) %>%\n  shapiro_test(Tail, Midpiece, Head) %>%\n  arrange(variable)\n\n# QQ plot\nggqqplot(sperm.avg, ""Tail"", facet.by = ""Species"",\n         ylab = "" Tail"", ggtheme = theme_bw())\n\nggqqplot(sperm.avg, ""Midpiece"", facet.by = ""Species"",\n         ylab = "" Midpiece"", ggtheme = theme_bw())\n\nggqqplot(sperm.avg, ""Head"", facet.by = ""Species"",\n         ylab = "" Head"", ggtheme = theme_bw())\n\n# Linearity assumption\n# Create a scatterplot matrix by group\nlibrary(GGally)\nresults <- sperm.avg %>%\n  select(Tail, Midpiece, Head, Species) %>%\n  group_by(Species) %>%\n  doo(~ggpairs(.) + theme_bw(), result = ""plots"")\nresults\nresults$plots\n# Looks like there are several non linear relationships - we will lose power\n\n# Homogeneity of covariances\nbox_m(sperm.avg[, c(""Tail"", ""Midpiece"",""Head"")], sperm.avg$Species)\n# statistic p.value parameter method                                             \n# <dbl>   <dbl>     <dbl> <chr>                                              \n#   1      4.18   0.652         6 Box\'s M-test for Homogeneity of Covariance Matrices\n# Covariation is homogeneic!\n\n# Homogeneity of variance\nsperm.avg %>% \n  gather(key = ""variable"", value = ""value"", Tail, Midpiece, Head) %>%\n  group_by(variable) %>%\n  levene_test(value ~ Species)\n\n# variable   df1   df2 statistic     p\n# <chr>    <int> <int>     <dbl> <dbl>\n#   1 Head         1    16     0.147 0.707\n# 2 Midpiece     1    16     0.676 0.423\n# 3 Tail         1    16     0.118 0.736\n\n\n# MANOVA for species comparison\n\nmodel <- lm(cbind(Tail, Midpiece, Head) ~ Species, sperm.avg)\nManova(model, test.statistic = ""Pillai"") # recommended for unbalanced design\n\n# Group the data by variable\ngrouped.data <- sperm.avg %>%\n  gather(key = ""variable"", value = ""value"", Tail, Midpiece, Head) %>%\n  group_by(variable)\n# Type II MANOVA Tests: Pillai test statistic\n#         Df    test stat approx F num Df den Df   Pr(>F)    \n# Species  1    0.8373   24.017      3     14   8.78e-06 ***\n\ngrouped.data %>% anova_test(value ~ Species)\n# variable    Effect    DFn   DFd    F      p     `p<.05`   ges\n# 1 Head     Species     1    16  0.195   0.665      """"     0.012\n# 2 Midpiece Species     1    16  24.0   0.000162   ""*""     0.6  \n# 3 Tail     Species     1    16  48.1   0.00000336 ""*""     0.75 \n\n# Between species, there was a significant difference in Tail length (F(1, 16) = 48.1, p < 0.0001 ) \n# and midpiece length (F(1, 16) = 24.0, p = 0.00016 ), but not head length (F(1, 16) = 0.195, p = 0.665 )\n\n# Bonferroni multiple testing correction: divide 0.05 by # tests (3), so significance criteria is p < 0.01666\np <- c(0.665,0.000162,0.00000336)\np.adjust (p, method = ""bonferroni"")\n# 1.000e+00 4.860e-04 1.008e-05\n\n\n# MANOVA for breeding stage comparison\nmodel <- lm(cbind(Tail, Midpiece, Head) ~ Breeding, sperm.avg)\nManova(model, test.statistic = ""Pillai"") # recommended for unbalanced design\n#           Df test stat approx F num Df den Df Pr(>F)\n# Breeding  1   0.12001   0.6364      3     14 0.6039\n\n# Group the data by variable\ngrouped.data <- sperm.avg %>%\n  gather(key = ""variable"", value = ""value"", Tail, Midpiece, Head) %>%\n  group_by(variable)\n\ngrouped.data %>% anova_test(value ~ Breeding)\n# variable Effect     DFn   DFd     F     p `p<.05`   ges\n# 1 Head     Breeding     1    16 1.37  0.26  """"      0.079\n# 2 Midpiece Breeding     1    16 0.424 0.524 """"      0.026\n# 3 Tail     Breeding     1    16 0.033 0.859 """"      0.002\n\np <- c(0.26,0.524,0.859)\np.adjust (p, method = ""bonferroni"")\n#  0.78 1.00 1.00\n\n\n# Avg Tail length  - species and breeding stage comparisons\nggboxplot(sperm, x = ""Species"", y = ""Tail"", add = ""dotplot"", color = ""Individual"")\n\ntail <- ggboxplot(sperm.avg, x = ""Species"", y = ""Tail"", add = ""dotplot"")\ntail + theme(text=element_text(size=rel(2.2))) + theme(axis.title.x = element_text(size=16))\n\nshapiro.test(jacana.sperm.avg$Tail) # W = 0.89545, p-value = 0.3043\nt.test(sperm.avg$Tail ~ sperm.avg$Species) #t = 6.3913, df = 9.7457, p-value = 8.89e-05\n\nggboxplot(sperm.avg, x = ""Species"", y = ""Tail"", color = ""Breeding"", add = ""dotplot"")\n\nspecies.tail.length <- lm(Tail ~ Species + Breeding, data = sperm.avg)\nanova(species.tail.length)\n#           Df  Sum Sq Mean Sq F value    Pr(>F)    \n#Species    1 225.046 225.046 49.2224 4.168e-06 ***\n#Breeding   1   6.281   6.281  1.3739    0.2594    \n#Residuals 15  68.580   4.572 \n\n\n# Plots that go into Figure 2 - ppt version - Tail Length\ntail.length <- ggboxplot(sperm.avg, x = ""Species"", y = ""Tail"", add = ""dotplot"")\neditable_graph <- dml(ggobj = tail.length)\ndoc <- read_pptx()\ndoc <- add_slide(doc)\ndoc <- ph_with(x = doc, editable_graph,\n               location = ph_location_type(type = ""body"") )\nprint(doc, target = ""tail.length.pptx"")\n\n\n\n# Avg Midpiece Length\nggboxplot(sperm, x = ""Species"", y = ""Midpiece"", add = ""dotplot"", color = ""Individual"")\n\nmidpiece <- ggboxplot(sperm.avg, x = ""Species"", y = ""Midpiece"", add = ""dotplot"")\nmidpiece + theme(text=element_text(size=rel(2.2))) + theme(axis.title.x = element_text(size=16))\n\nshapiro.test(jacana.sperm.avg$Midpiece) # W = 0.91635, p-value = 0.4416\nt.test(sperm.avg$Midpiece ~ sperm.avg$Species) #t = 4.7361, df = 11.546, p-value = 0.0005371\n\nggboxplot(sperm.avg, x = ""Species"", y = ""Midpiece"", color = ""Breeding"", add = ""dotplot"")\n\nspecies.midpiece.length <- lm(Midpiece ~ Species + Breeding, data = sperm.avg)\nanova(species.midpiece.length)\n#         Df Sum Sq Mean Sq F value    Pr(>F)    \n#Species    1 5.2092  5.2092 26.6509 0.0001158 ***\n#Breeding   1 0.5456  0.5456  2.7914 0.1154960    \n#Residuals 15 2.9319  0.1955     \n\n# Plots that go into Figure 2 - ppt version - Midpiece Length\nmidpiece.length <- ggboxplot(sperm.avg, x = ""Species"", y = ""Midpiece"", add = ""dotplot"")\nmidpiece.length\neditable_graph <- dml(ggobj = midpiece.length)\ndoc <- read_pptx()\ndoc <- add_slide(doc)\ndoc <- ph_with(x = doc, editable_graph,\n               location = ph_location_type(type = ""body"") )\nprint(doc, target = ""midpiece.length.pptx"")\n\n\n\n# Head\nggboxplot(sperm, x = ""Species"", y = ""Head"", add = ""dotplot"", color = ""Individual"")\n\nhead <- ggboxplot(sperm.avg, x = ""Species"", y = ""Head"", add = ""dotplot"")\nhead + theme(text=element_text(size=rel(2.2))) + theme(axis.title.x = element_text(size=16))\n\nshapiro.test(jacana.sperm.avg$Head) # W = 0.96137, p-value = 0.8304\nt.test(sperm.avg$Head ~ sperm.avg$Species) # t = -0.42472, df = 11.303, p-value = 0.679\n\nggboxplot(sperm.avg, x = ""Species"", y = ""Head"", color = ""Breeding"", add = ""dotplot"")\n\nspecies.head.length <- lm(Head ~ Species + Breeding, data = sperm.avg)\nanova(species.head.length)\n#Df Sum Sq Mean Sq F value Pr(>F)\n#Species    1 0.0880 0.08804  0.2006 0.6607\n#Breeding   1 0.6353 0.63534  1.4474 0.2476\n#Residuals 15 6.5843 0.43895        \n\n# Plots that go into Figure 2 - ppt version - Head Length\nhead.length <- ggboxplot(sperm.avg, x = ""Species"", y = ""Head"", add = ""dotplot"")\nhead.length\neditable_graph <- dml(ggobj = head.length)\ndoc <- read_pptx()\ndoc <- add_slide(doc)\ndoc <- ph_with(x = doc, editable_graph,\n               location = ph_location_type(type = ""body"") )\nprint(doc, target = ""head.length.pptx"")\n\n\n# Do testes differ between the species?\nggboxplot(sperm.avg, x = ""Species"", y = ""Testes.Mass"", add = ""dotplot"", color = ""Breeding"")\n\n# ANCOVA\n# Adequate sample size. Rule of thumb: the n in each cell > the number of outcome variables.\nsperm.avg %>%\n  group_by(Species) %>%\n  get_summary_stats(Testes.Mass, type = ""mean_sd"")\n\nsperm.avg %>%\n  group_by(Breeding) %>%\n  get_summary_stats(Testes.Mass, type = ""mean_sd"")\n\n\n# Also need to test for homogeneity of variance\nlibrary(car)\nleveneTest(sperm.avg$Testes.Mass ~ sperm.avg$Species)\n# Levene\'s Test for Homogeneity of Variance (center = median)\n#       Df F value Pr(>F)\n# group  1   0.7793 0.3904\n#       16    \n# Variance is equal. \n\nancova_model_species <- aov(sperm.avg$Testes.Mass ~ sperm.avg$Species + sperm.avg$Somatic.Mass)\nAnova(ancova_model_species, type = ""III"")\n# Anova Table (Type III tests)\n# \n# Response: sperm.avg$Testes.Mass\n#                         Sum Sq Df F value Pr(>F)\n# (Intercept)               460  1  0.0146 0.9055\n# sperm.avg$Species        6270  1  0.1988 0.6621\n# sperm.avg$Somatic.Mass  10206  1  0.3236 0.5779\n# Residuals              473105 15               \n\nancova_model_breeding <- aov(sperm.avg$Testes.Mass ~ sperm.avg$Somatic.Mass + sperm.avg$Breeding)\nAnova(ancova_model_breeding, type = ""III"")\n# Anova Table (Type III tests)\n# \n# Response: sperm.avg$Testes.Mass\n# Sum Sq Df F value Pr(>F)\n# (Intercept)             15937  1  0.5410 0.4734\n# sperm.avg$Somatic.Mass    344  1  0.0117 0.9154\n# sperm.avg$Breeding      37524  1  1.2739 0.2768\n# Residuals              441850 15   \n\n\n\n# Testes volume\nggboxplot(sperm.avg, x = ""Species"", y = ""Avg.Testes.Vol"", add = ""dotplot"", color = ""Breeding"")\n\nspecies.testes.vol <- aov(Avg.Testes.Vol ~ Species * Breeding, data = sperm.avg,na.action= na.exclude)\nsummary(species.testes.vol)\n# Df   Sum Sq Mean Sq F value Pr(>F)\n# Species           1   464494  464494   0.401  0.541\n# Breeding          1   789882  789882   0.682  0.428\n# Species:Breeding  1  1733443 1733443   1.497  0.249\n# Residuals        10 11579282 1157928               \n# 4 observations deleted due to missingness\n\nspecies.testes.vol <- aov(Avg.Testes.Vol ~ Species + Breeding, data = sperm.avg,na.action= na.exclude)\nsummary(species.testes.vol)\n# Df   Sum Sq Mean Sq F value Pr(>F)\n# Species      1   464494  464494   0.384  0.548\n# Breeding     1   789882  789882   0.653  0.436\n# Residuals   11 13312725 1210248               \n# 4 observations deleted due to missingness \n\n\n\n\n# Coefficient of variation of each trait - variation across breeding season?\n# Intermale CV for both breeding stages and combined\n\nspinosa.sperm = subset(sperm, Species == ""Northern"")\njacana.sperm = subset(sperm, Species == ""Wattled"")\nspinosa.sperm.court = subset(sperm, Species == ""Northern"" & Breeding == ""Copulation"")\nspinosa.sperm.inc = subset(sperm, Species == ""Northern"" & Breeding == ""Incubation"")\njacana.sperm.court = subset(sperm, Species == ""Wattled"" & Breeding == ""Copulation"")\njacana.sperm.inc = subset(sperm, Species == ""Wattled"" & Breeding == ""Incubation"")\n\n# Intermale CV tail-\nsd(spinosa.sperm$Tail, na.rm=TRUE)/\n  mean(spinosa.sperm$Tail, na.rm=TRUE)*100 # spinosa CV =  4.459243\nsd(spinosa.sperm.court$Tail, na.rm=TRUE)/\n  mean(spinosa.sperm.court$Tail, na.rm=TRUE)*100 # spinosa court CV = 3.677851\nsd(spinosa.sperm.inc$Tail, na.rm=TRUE)/\n  mean(spinosa.sperm.inc$Tail, na.rm=TRUE)*100 # spinosa inc CV = 5.030138\n\nsd(jacana.sperm$Tail, na.rm=TRUE)/\n  mean(jacana.sperm$Tail, na.rm=TRUE)*100 # spinosa CV =  5.746834\nsd(jacana.sperm.court$Tail, na.rm=TRUE)/\n  mean(jacana.sperm.court$Tail, na.rm=TRUE)*100 # spinosa court CV =  4.439126\nsd(jacana.sperm.inc$Tail, na.rm=TRUE)/\n  mean(jacana.sperm.inc$Tail, na.rm=TRUE)*100 # spinosa inc CV = 6.43182\n\n# Intermale CV Midpiece\nsd(spinosa.sperm$Midpiece, na.rm=TRUE)/\n  mean(spinosa.sperm$Midpiece, na.rm=TRUE)*100 # spinosa CV =  8.875077\nsd(spinosa.sperm.court$Midpiece, na.rm=TRUE)/\n  mean(spinosa.sperm.court$Midpiece, na.rm=TRUE)*100 # spinosa court CV = 7.498641\nsd(spinosa.sperm.inc$Midpiece, na.rm=TRUE)/\n  mean(spinosa.sperm.inc$Midpiece, na.rm=TRUE)*100 # spinosa inc CV = 9.705709\n\nsd(jacana.sperm$Midpiece, na.rm=TRUE)/\n  mean(jacana.sperm$Midpiece, na.rm=TRUE)*100 # spinosa CV =   8.065452\nsd(jacana.sperm.court$Midpiece, na.rm=TRUE)/\n  mean(jacana.sperm.court$Midpiece, na.rm=TRUE)*100 # spinosa court CV =  6.628148\nsd(jacana.sperm.inc$Midpiece, na.rm=TRUE)/\n  mean(jacana.sperm.inc$Midpiece, na.rm=TRUE)*100 # spinosa inc CV = 9.632959\n\n# Intermale CV Head\nsd(spinosa.sperm$Head, na.rm=TRUE)/\n  mean(spinosa.sperm$Head, na.rm=TRUE)*100 # spinosa CV =  8.43109\nsd(spinosa.sperm.court$Head, na.rm=TRUE)/\n  mean(spinosa.sperm.court$Head, na.rm=TRUE)*100 # spinosa court CV = 8.785489\nsd(spinosa.sperm.inc$Head, na.rm=TRUE)/\n  mean(spinosa.sperm.inc$Head, na.rm=TRUE)*100 # spinosa inc CV = 7.77009\n\nsd(jacana.sperm$Head, na.rm=TRUE)/\n  mean(jacana.sperm$Head, na.rm=TRUE)*100 # spinosa CV =   9.335285\nsd(jacana.sperm.court$Head, na.rm=TRUE)/\n  mean(jacana.sperm.court$Head, na.rm=TRUE)*100 # spinosa court CV =  9.366753\nsd(jacana.sperm.inc$Head, na.rm=TRUE)/\n  mean(jacana.sperm.inc$Head, na.rm=TRUE)*100 # spinosa inc CV = 9.343497\n\n\n\n# CV equality - test for difference in intraspecific + CVs between northern and wattled\n\n#install.packages(""cvequality"")\nlibrary(cvequality)\nlibrary(ggbeeswarm)\nlibrary(ggplot2)\nlibrary(knitr)\n\nkable(head(sperm.avg), caption = ""Preview of first few rows of the sperm data"")\nspinosa.sperm = subset(sperm, Species == ""Northern"")\njacana.sperm = subset(sperm, Species == ""Wattled"")\n\nsperm.court = subset(sperm.avg, sperm.avg$Breeding == ""Copulating"")\nsperm.inc = subset(sperm.avg, sperm.avg$Breeding == ""Incubating"")\n\n# Supplementary Table 1\n# Copulation - species diffs\nggplot(sperm.court, aes(Species, Tail)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\nspecies_court_tail_length_cv_test_MSLR <- with(sperm.court, mslr_test(nr = 1e4,Tail, Species))\nspecies_court_tail_length_cv_test_MSLR # MSLRT = 0.4430512, p =  0.5056534\n\nggplot(sperm.court, aes(Species, Midpiece)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\nspecies_court_midpiece_length_cv_test_MSLR <- with(sperm.court, mslr_test(nr = 1e4,Midpiece, Species))\nspecies_court_midpiece_length_cv_test_MSLR # MSLRT = 0.02178721, p =  0.8826546\n\nggplot(sperm.court, aes(Species, Head)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\nspecies_court_head_length_cv_test_MSLR <- with(sperm.court, mslr_test(nr = 1e4,Head, Species))\nspecies_court_head_length_cv_test_MSLR # MSLRT = 0.2724925, p =  0.6016646\n\n# Incubation- species diffs\nggplot(sperm.inc, aes(Species, Tail)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\nspecies_inc_tail_length_cv_test_MSLR <- with(sperm.inc, mslr_test(nr = 1e4,Tail, Species))\nspecies_inc_tail_length_cv_test_MSLR # MSLRT = 0.7983578, p =  0.3715848\n\nggplot(sperm.inc, aes(Species, Midpiece)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\nspecies_inc_midpiece_length_cv_test_MSLR <- with(sperm.inc, mslr_test(nr = 1e4,Midpiece, Species))\nspecies_inc_midpiece_length_cv_test_MSLR # MSLRT = 0.4125012, p =  0.5207027\n\nggplot(sperm.inc, aes(Species, Head)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\nspecies_inc_head_length_cv_test_MSLR <- with(sperm.inc, mslr_test(nr = 1e4,Head, Species))\nspecies_inc_head_length_cv_test_MSLR # MSLRT = 1.44006, p =  0.2301296\n\n# Bonferroni multiple testing correction: \np <- c(0.5056534,0.8826546,0.6016646,0.3715848,0.5207027,0.2301296)\np.adjust (p, method = ""bonferroni"")\n\n\n# Supplementary Table 2\n# test of difference in CV in breeding stage, for each species\n# Northern Jacana\nggplot(spinosa.sperm, aes(Breeding, Tail)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\n#spinosa_breeding_tail_length_cv_test <-  with(spinosa.sperm, asymptotic_test(Tail,Breeding))\nspinosa_breeding_tail_length_cv_test_MSLRT <- with(spinosa.sperm, mslr_test(nr = 1e4,Tail, Breeding))\nspinosa_breeding_tail_length_cv_test_MSLRT # MSLRT = 5.273314, p =  0.02463542\n\nggplot(spinosa.sperm, aes(Breeding, Midpiece)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\n#breeding_midpiece_length_cv_test <-  with(spinosa.sperm, asymptotic_test(Midpiece,Breeding))\nbreeding_midpiece_length_cv_test_MSLRT <- with(spinosa.sperm, mslr_test(nr = 1e4,Midpiece, Breeding))\nbreeding_midpiece_length_cv_test_MSLRT # MSLRT = 3.444121, p =  0.06347718\n\nggplot(spinosa.sperm, aes(Breeding, Head)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\n#breeding_midpiece_length_cv_test <-  with(spinosa.sperm, asymptotic_test(Head,Breeding))\nbreeding_head_length_cv_test_MSLRT <- with(spinosa.sperm, mslr_test(nr = 1e4,Head, Breeding))\nbreeding_head_length_cv_test_MSLRT # MSLRT = 0.7968582, p = 0.3720344\n\n# Wattled Jacana\nggplot(jacana.sperm, aes(Breeding, Tail)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\n#breeding_tail_length_cv_test <-  with(jacana.sperm, asymptotic_test(Tail,Breeding))\nbreeding_tail_length_cv_test_MSLRT <- with(jacana.sperm, mslr_test(nr = 1e4,Tail, Breeding))\nbreeding_tail_length_cv_test_MSLRT # MSLRT = 4.519644, p = 0.03350781\n\nggplot(jacana.sperm, aes(Breeding, Midpiece)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\n#breeding_midpiece_length_cv_test <-  with(jacana.sperm, asymptotic_test(Midpiece,Breeding))\nbreeding_midpiece_length_cv_test_MSLRT <- with(jacana.sperm, mslr_test(nr = 1e4,Midpiece, Breeding))\nbreeding_midpiece_length_cv_test_MSLRT # MSLRT = 4.572663, p = 0.03248604\n\nggplot(jacana.sperm, aes(Breeding, Head)) + geom_boxplot() + geom_quasirandom(alpha = 0.05) + theme_bw()\n#breeding_midpiece_length_cv_test <-  with(jacana.sperm, asymptotic_test(Midpiece,Breeding))\nbreeding_head_length_cv_test_MSLRT <- with(jacana.sperm, mslr_test(nr = 1e4,Head, Breeding))\nbreeding_head_length_cv_test_MSLRT # MSLRT = 0.0009138081, p =  0.9758842\n\n# Bonferroni multiple testing correction: \np <- c(0.02463542,0.06347718,0.3720344,0.03350781,0.03248604,0.9758842)\np.adjust (p, method = ""bonferroni"")\n# 0.1478125 0.3808631 1.0000000 0.2010469 0.1949162 1.0000000\n\n# Intra-ejaculate Coefficient of Variation\n\n# Tail Intra-ejaculate CV \nggboxplot(sperm.avg, x = ""Species"", y = ""CV.Tail"", add = ""dotplot"", color = ""Breeding"")\n\nCV.Tail <- ggboxplot(sperm.avg, x = ""Species"", y = ""CV.Tail"", add = ""dotplot"", color = ""Breeding"", ylim = c(0,15))\nCV.Tail\neditable_graph <- dml(ggobj = CV.Tail)\ndoc <- read_pptx()\ndoc <- add_slide(doc)\ndoc <- ph_with(x = doc, editable_graph,\n               location = ph_location_type(type = ""body"") )\nprint(doc, target = ""CV.Tail.pptx"")\n# CV for tail length changes with breeding stage, marginally different between species.\n# Tail length CV is lower during copulation than incubation\n\nspecies.CV.tail <- aov(CV.Tail ~ Species * Breeding, data = sperm.avg)\nsummary(species.CV.tail)\n# Df Sum Sq Mean Sq F value  Pr(>F)   \n# Species           1  4.338   4.338   4.294 0.05719 . \n# Breeding          1 12.131  12.131  12.010 0.00379 **\n# Species:Breeding  1  0.565   0.565   0.559 0.46693   \n# Residuals        14 14.141   1.010 \n\n# Interaction not significant so we deleted\n\nspecies.CV.tail <- aov(CV.Tail ~ Species + Breeding, data = sperm.avg)\nsummary(species.CV.tail)\n# Df Sum Sq Mean Sq F value  Pr(>F)   \n# Species      1  4.338   4.338   4.424 0.05272 . \n# Breeding     1 12.131  12.131  12.373 0.00311 **\n# Residuals   15 14.706   0.980    \n\n\n# Midpiece Intra-ejaculate CV \nggboxplot(sperm.avg, x = ""Species"", y = ""CV.Midpiece"", add = ""dotplot"", color = ""Breeding"")\n\nspecies.CV.midpiece <- aov(CV.Midpiece ~ Species * Breeding, data = sperm.avg)\nsummary(species.CV.midpiece)\n# Df Sum Sq Mean Sq F value Pr(>F)\n# Species           1  10.83  10.826   1.701  0.213\n# Breeding          1  11.96  11.957   1.879  0.192\n# Species:Breeding  1   0.04   0.039   0.006  0.939\n# Residuals        14  89.08   6.363 \n\nspecies.CV.midpiece <- aov(CV.Midpiece ~ Species + Breeding, data = sperm.avg)\nsummary(species.CV.midpiece)\n#          Df Sum Sq Mean Sq F value Pr(>F)\n# Species      1  10.83  10.826   1.822  0.197\n# Breeding     1  11.96  11.957   2.012  0.176\n# Residuals   15  89.12   5.941  \n\nCV.midpiece <- ggboxplot(sperm.avg, x = ""Species"", y = ""CV.Midpiece"", add = ""dotplot"", color = ""Breeding"", ylim = c(0,15))\nCV.midpiece\neditable_graph <- dml(ggobj = CV.midpiece)\ndoc <- read_pptx()\ndoc <- add_slide(doc)\ndoc <- ph_with(x = doc, editable_graph,\n               location = ph_location_type(type = ""body"") )\nprint(doc, target = ""CV.midpiece.pptx"")\n# CV for midpiece length does not significantly differ w/ breeding stage or species\n\n\n# Head Intra-ejaculate CV \nspecies.CV.head <- aov(CV.Head ~ Species * Breeding, data = sperm.avg)\nsummary(species.CV.head)\n# Df Sum Sq Mean Sq F value Pr(>F)\n# Species           1   2.03   2.027   0.502  0.490\n# Breeding          1   0.91   0.913   0.226  0.642\n# Species:Breeding  1   9.12   9.125   2.261  0.155\n# Residuals        14  56.50   4.036\n\nspecies.CV.head <- aov(CV.Head ~ Species + Breeding, data = sperm.avg)\nsummary(species.CV.head)\n#           Df Sum Sq Mean Sq F value Pr(>F)\n# Species      1   2.03   2.027   0.463  0.506\n# Breeding     1   0.91   0.913   0.209  0.654\n# Residuals   15  65.62   4.375  \n\nggboxplot(sperm.avg, x = ""Species"", y = ""CV.Head"", add = ""dotplot"", color = ""Breeding"",ylim=c(0,14))\nCV.Head <- ggboxplot(sperm.avg, x = ""Species"", y = ""CV.Head"", add = ""dotplot"", color = ""Breeding"",ylim=c(0,15))\neditable_graph <- dml(ggobj = CV.Head)\ndoc <- read_pptx()\ndoc <- add_slide(doc)\ndoc <- ph_with(x = doc, editable_graph,\n               location = ph_location_type(type = ""body"") )\nprint(doc, target = ""CV.Head.pptx"")\n# CV for head length does not significantly differ w/ breeding stage or species\nCV.Head\n\n\n']","Social polyandry shapes sperm morphology Sexual selection is a major driver of trait variation, and the intensity of male competition for mating opportunities has been linked with sperm size across diverse taxa. Mating competition among females may also shape the evolution of sperm traits, but the interplay between female-female competition and male-male competition on sperm morphology is not well understood. We evaluated variation in sperm morphology in two species with socially polyandrous mating systems, in which females compete to mate with multiple males. Northern jacanas (Jacana spinosa) and wattled jacanas (J. jacana) vary in their degree of polyandry and sexual dimorphism, suggesting species differences in the intensity of sexual selection. We compared mean and variance in sperm head, midpiece, and tail length between species and breeding stages, because these measures have been associated with the intensity of sperm competition. We found that the species with greater polyandry, northern jacana, has sperm with longer midpieces and tails, as well as marginally lower intra-ejaculate variation in tail length. Intra-ejaculate variation was also significantly lower in copulating males than in incubating males, suggesting flexibility in sperm production as males cycle between breeding stages. Our results indicate that stronger female-female competition for mating opportunities may also shape more intense male-male competition by selecting for longer and less variable sperm traits. These findings extend frameworks developed in socially monogamous species to reveal that sperm competition may be an important evolutionary force layered atop female-female competition for mates.",4